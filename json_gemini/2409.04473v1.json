{"title": "Learning in Order!\nA Sequential Strategy to Learn Invariant Features for Multimodal\nSentiment Analysis", "authors": ["Xianbing Zhao", "Lizhen Qu", "Tao Feng", "Jianfei Cai", "Buzhou Tang"], "abstract": "This work proposes a novel and simple sequential learning strategy\nto train models on videos and texts for multimodal sentiment anal-\nsis. To estimate sentiment polarities on unseen out-of-distribution\ndata, we introduce a multimodal model that is trained either in a\nsingle source domain or multiple source domains using our learn-\ning strategy. This strategy starts with learning domain invariant\nfeatures from text, followed by learning sparse domain-agnostic fea-\ntures from videos, assisted by the selected features learned in text.\nOur experimental results demonstrate that our model achieves sig-\nnificantly better performance than the state-of-the-art approaches\non average in both single-source and multi-source settings. Our\nfeature selection procedure favors the features that are independent\nto each other and are strongly correlated with their polarity labels.\nTo facilitate research on this topic, the source code of this work will\nbe publicly available upon acceptance.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal Sentiment Analysis (MSA) is concerned with under-\nstanding people's attitudes or opinions based on information from\nmore than one modalities, such as videos and texts. It finds rich\napplications in both industry and research communities, such as un-\nderstanding spoken reviews of target products posted on YouTube\nand developing multimodal AI assistants for mental health sup-\nport. Prior MSA approaches make an impractical assumption that\ntraining and test data comprise independent identically distributed\nsamples [60, 66, 79, 82-84]. However, training datasets are avail-\nable only for a handful of applications that satisfy that assumption.\nTherefore, this work aims to remove the assumption such that MSA\nmodels trained on a single domain or multiple source domains can\nwork robustly on unseen out-of-distribution (OOD) data, without\nleveraging any target domain data.\nTo enable models to work robustly across domains, a key idea\nis to exploit domain invariant sparse representations, which serve\nas causes of target labels from a causal perspective [65, 70]. In con-\ntrast, spurious correlations, which do not indicate causal relations,\nimpede the generalization capability of pre-trained foundation mod-\nels [6, 26]. Existing MSA models heavily rely on jointly learned mul-\ntimodal features for sentiment analysis [28]. However, the spurious\nfeatures of the visual modality may adversely affect the features\nof the text modality, leading to inaccurate prediction outcomes\n[24, 25, 83]. Therefore, it would be interesting to investigate i) how\nto automatically identify domain invariant representations\nfor MSA, and ii) what are the key characteristics of domain\ninvariant features in a multimodal setting.\nTo answer the above research questions, as illustrated in Figure\n1, we propose a Sequential Strategy to Learn Invariant Features\n(S2LIF) for building a domain generalization (DG) MSA model based\non videos and texts. Instead of learning domain-agnostic features\nsimultaneously from all modalities, our technique first leverages\nthe sparse masking technique [35] to select invariant hidden fea-\ntures from texts, followed by learning the invariant features from\nvideos, conditioned on the selected textual features. To the best of\nour knowledge, it is the first time to report the importance of fea-\nture learning order for domain generalization. We conduct extensive\nexperiments to i) demonstrate the superiority of our approach in\ncomparison with the competitive baselines in both single source\ndomain and multi-source domain settings, and ii) investigate key\ncharacteristics of selected features using our approach. Our key\ncontributions are summarized as follows:"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Multimodal Sentiment Analysis", "content": "MSA methods can be roughly divided into two categories: 1) Multi-\nmodal Representation Learning aims to learn fine-grained multi-\nmodal representation, which provides rich decision evidence for\nmultimodal sentiment prediction. They employ a disentangled tech-\nnique to learn modality-common and modality-specific representa-\ntions to mitigate the heterogeneity of multimodal representations\n[29, 67, 74, 77]. 2) Multimodal Fusion aims to learn cross-modal\ninformation transfer by designing complex cross-modal interac-\ntive networks. The development of multimodal fusion methods\nhas evolved from multi-modal tensor fusion [79] to cross-modal\nattention [45, 46, 66, 76, 80, 82-85]. The current MSA methods only\ntrain and test on a specific domain, and do not consider the gener-\nalization ability of the model. They suffer performance degradation\nwhen tests on out-of-distribution data, so learning robust MSA\nmodels is essential."}, {"title": "2.2 Domain Generalization", "content": "Domain generalization aims to design a deep neural network model\nthat learns domain-invariant features and is able to maintain sta-\nble performance in both the source domain and multiple unseen\ntarget domains. Numerous domain generalization methods have\nbeen proposed to learn domain-invariant features for single-source\nor multi-source domain generalization [5, 12-18, 23, 27, 31, 32, 38,\n39, 41, 50, 53, 56, 61, 75]. We roughly divide current methods of\ndomain generalization into three categories: 1) Learning invariant\nfeatures aims to capture the domain-generalized features to reduce\nthe dependence of features on specific domains and to achieve high\nperformance on unseen domains[50]. 2) Optimize algorithm aims\nto learn domain-invariant features and remove domain-specific\nfeatures [5, 10, 23, 39, 54, 61], such as adversarial training and meta-\nlearning, through tailored designed network structures. 3) Data aug-\nmentation aims to generate new data to improve the generalization\nperformance of the model, and these generated new data are out-of-\ndistribution samples different from the source domain [69, 71, 72]."}, {"title": "2.3 Causal Representation Learning", "content": "From the perspective of data generation, causal representation\nlearning considers that raw data is entangled with two parts of\nfeatures: correlated features with label (domain-invariant features)\nand spuriously correlated features with the label (domain-specific\nfeatures). The goal is to disentangle domain-invariant features and\ndomain-specific features. Domain-invariant features guarantee sta-\nble performance in different test environments [1, 51]. Based on\nthis assumption, numerous methods attempt to learn domain in-\nvariant features [2, 3, 11, 33, 58]. Following previous work, our\nproposed approach aims to learn the domain-invariant features (i.e.,\nthe features correlated with the label), while removing the features\ndomain-specific (i.e., the spuriously correlated features with the\nlabel). Concretely, we adopt sparse techniques to remove spuriously\ncorrelated features with the label [37, 44, 48, 62]."}, {"title": "3 METHOD", "content": "Problem Statement. The goal of domain generalization for MSA\nis to train a deep neural network model on a single-source or multi-\nsource domains $D_s = \\{D_1, D_2, ..., D_N\\}$ and evaluate the model\non the unseen target domains $\\{D_1^T, D_2^T, ..., D_M^T\\}$, where $D$ denotes\na dataset in a domain, M and N denote the number of source do-\nmains and target domains, respectively. We consider each MSA\ntask as a k-ways classification task. The dataset in a source domain\nis denoted by $D_s = \\{(X_i^{\\{t,v\\}}, y_i)\\}_{i=1}^{N_s}$, where $X_i^{\\{t,v\\}} \\in \\mathbb{R}^d$, $y \\in \\mathbb{R}^K$,\nwhile $D_T$ is a dataset in a target domain. The goal is to learn multi-\nmodal domain-invariant features for sentiment polarity prediction\nin unseen domains without using target domain data for training.\nModel Overview. Our work is motivated by the functional lot-\ntery ticket hypothesis [42] suggesting that there is a subnetwork\nthat can achieve better out-of-distribution performance than the\noriginal network. Hence, We employ the sparse masking techniques\nto identify a subset of hidden features in the multimodal setting.\nThe findings of our empirical studies indicate the importance of\nthe learning order between modalities for domain generalization\nperformance.\nThe architecture of our model is illustrated in Figure 2. Given a\ntext and a sequence of video frames $X^{\\{t,v\\}}$, we employ a pre-trained\nencoders ELECTRA [20] and VGGFace2 with a 1-layer Transformer\nencoder [22] to map them to respective hidden representations $x_t$\nand $x_v$. To achieve sparsity in hidden representations, our model\ngenerates a mask vector $m^{\\{t,v\\}}$ with the mask function $f_{mask}$ to\nselect domain-invariant features $x_{t,v}$ from $X^{\\{t,v\\}}$. The mask func-\ntion is characterized by the learnable parameter $r^{\\{t,v\\}}$ and threshold\n$s^{\\{t,v\\}}$. The feature selection in a modality is achieved by computing\nthe dot-product between the mask vectors and the corresponding\nhidden representations. We empirically find that text is the superior\nmodality in comparison with videos based on their performance\nin each modality. Our further studies show that conditioning on\nthe strong text features reduces the selection of visual features that\ncorrelate with those text features. On the one hand, reduction of\nstatistical dependencies between features leads to improvement of\ngeneralization performance. On the other hand, selection of fea-\ntures adhere to the functional lottery ticket hypothesis. Therefore,\nour text classifier $g_t$ first selects the key features using the masking"}, {"title": "Sequential Multimodal Learning.", "content": "The selection of domain\ninvariant features is also motivated from a causal perspective. The\nlogit of the classifier is computed as the product between the fea-\ntures $x$ and the weights $W$ of the label $k$ from the classification\nlayer $g$.\n$O_k = W^{\\{t,v\\}}_k x = \\sum_{*\\in \\{t,v\\}} \\sum_{j=1}^{d_t} W^{\\{k\\}}_{j,k} \\cdot X^*_j$\n(1)\nwhere the subscripts j and k denote j-th feature and k-th class\nrespectively. For all polarity labels with both modalities, we obtain\na matrix R as follows, where each element $W^{\\{j,k\\}} \\cdot X^*_j, * \\in \\{t, v\\}$\nrepresents the evidence of the classifier.\nBy analyzing the matrix R, we conclude that 1) Y is the result of\nfeature $x$ estimated via a classifier. From the causal perspective,\nthe selected features can be seen as the causes of Y subjecting to\nindependent noise [30, 52, 64]:\n$Y = g(Pa(Y)) + \\epsilon\n(3)\nwhere the notation Pa(Y) denotes the features of direct causal ef-\nfects with Y, where Pa(Y) is a subset of x. The function g represents\nthe classifier. The multimodal features x are divided into two sub-\nsets, domain-specific features $x^s$ (spurious correlated features with\nthe label across domain) and domain-invariant features $x^o$ (cor-\nrelated features with the label across domain) [57]. We use three\nfeatures {x1, x2, x3} to explain the causal relationship between x\nand Y. As shown in Figure 3 (a), the outcome Y is specified as\n$Y = g(x_1, x_3) + \\epsilon, \\{X_1, X_3\\} \\subseteq x^o$. The feature x3 is the subset of xs.\nThere exist two distinct relationships between the feature sets xs\nand $x^o$: a) there is no direct causal relationship between x3 and\nx1. b) there is a direct causal relationship between x3 and x2. We\nremove the edge between x2 and x3 to eliminate the impact of x3\non x2. Therefore, our goal is to identify the features $x^o$ and remove\nthe features $x^s$. Formally, we expect\n$P(Y|do(x_t,x_v)) \\neq P(Y|do(x_t^o,x_v^o))\n(4)\nwhere the features $\\{x_t^o, x_v^o\\} \\subseteq x^o$ are selected mutual independent\ndomain-invariant features [59]. We design learnable masks m and\nlearnable threshold s in Section 3 to set the values of domain-specific\nfeatures in x to 0. Removing the features $x \\subseteq x^s$ eliminates its\ndirect causal effects on (x,x) and the outcome Y. 2) simulta-\nneously optimizing such entangled features $x = \\{x^o, x^s\\}$ for\nboth text and visual modalities (i.e., imbalanced multimodal\nfeatures) poses a special challenge for the classifier [24, 25].\nOur sequential learning strategy is also motivated by curriculum\nlearning [8, 47, 87] that we learn the features first, which perform\nwell on the target tasks, followed by more challenging ones.\nBy analyzing the causal relationship and removing spurious\ncorrelation features using multimodal learnable masks, we can\nobtain a new evidence matrix $R^M$. The form of the new evidence\nmatrix $R^M$ for the classifier is as follows:"}, {"title": "Multimodal Learnable Masks.", "content": "Regarding how to automati-\ncally identify domain invariant representations for MSA, we design\nmultimodal learnable masks to select features. Specifically, to re-\nmove domain-specific features, we tailor a function, denoted as\n$f_{mask}$. The inputs of $f_{mask}$ consists of the features x from a modal-\nity, a learnable parameter r, and a dynamic threshold s. The output\nis domain-invariant features $x^o$.\n$x^o = f_{mask} (x, r, s)\n(6)\nwhere we apply the mask vector $m \\in \\mathbb{R}^d$ (consisting of zero and\nnon-zero value) on the feature $x \\in \\mathbb{R}^d$. The mask vector m is\nobtained by utilizing a trainable pruning threshold $s \\in \\mathbb{R}^d$ and a\nlearnable parameter $r \\in \\mathbb{R}^d$. Given a set of features x, our method\ncan dynamically select features using mask vector m. We utilize\nthe unit step function F(.) to produce mask vector, which takes"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Datasets", "content": "We select three typical MSA benchmark datasets: CMU-MOSI [81],\nCMU-MOSEI [82] and MELD [55]. The detailed partition of the\ndataset is included in the supplementary materials."}, {"title": "4.2 Implementation Detail", "content": "We employ text pre-trained language model Electra [20] and visual\npre-trained model VGG Face2 [9], extracting features from both\ntextual content and video frames. We use a multilayer perceptron to\nunify the multimodal feature dimensions and a 1-layer Transformer\nencoder [22] to model the multimodal data of the sequence. The\nbatch size and epoch are set to 16 and 200, and the learning rate\nis configured to 7e-5. Warm up epoch is 3. Our implementation is\nexecuted using the PyTorch framework with Adam optimizer [36]\non the V100 GPU."}, {"title": "4.3 Baselines", "content": "We select the state-of-the-art model in the field of MSA, MLLM and\nDG (OOD) as the Baseline.\nMULT [67] designs a Multimodal Transformer to align multi-modal\nsequential data and capture cross-modal information interaction.\nALMT [83] employs non-verbal modalities to reinforce the features\nof the text modality several times and dismisses the non-verbal in-\nformation after completing the reinforcement process."}, {"title": "4.4 Evaluation Criteria", "content": "The distribution of the dataset is approximately balanced. We eval-\nuate the model performance using a 3-class accuracy metric, specif-\nically [Positive, Neutral, Negative]."}, {"title": "4.5 Results and Discussions", "content": "Overall Comparisons. To justify the effectiveness of our proposed\nS\u00b2LIF model, we compareed the model with the following state-of-\nthe-art baseline in the filed of MSA and DG. Models that focus on\ncapturing cross-modal dependencies, called MulT and ALMT. Mod-\nels that aims to learn domain-invariant features, namely, MAD and\nRIDG. Tables 1 and 2 show the results of the comparison. By analyz-\ning these two tables, we draw the following conclusions: i) The MSA\nmethod shows visual performance in the unseen domain. With the\naddition of our multimodal learnable masks, the traditional models\nalso gain the ability of DG. The fact demonstrates the effectiveness\nof sparse mask in DG. ii) Our model significantly outperforms the\nmultimodal large model in 4 of the 6 Settings. We speculate that\nthere is contamination from emotional datasets during the training\nphase of the multimodal large language model. In the two settings\nwith better performance, the logits of InstructBlip for correctly\npredicted samples exceed 0.93, significantly higher than the logits\ngenerated by other multimodal large language models, which are\naround 0.65. iii) The model performance in sequential multimodal\nlearning is better than that in non-sequential multimodal learning\nwhen we distinguish text and visual modalities. This demonstrates\nthe effectiveness of the sequential multimodal learning strategy.\nExistence of Domain-invariant Features. An essential assumption\nin our study is the presence of domain-invariant features in cross-\ndomain multimodal data. To gain insight into this assumption, we\nvisualized the selected and removed features for each domain using\na heatmap. We marked positions with '1' where the features are\nconsistently selected across domains. From Figure 4, we could con-\nclude that there is a presence of domain-invariant features across\nmultiple domains, and our proposed model can automatically se-\nlect the domain-invariant features. Moreover, we visualized the\nproportion of features retained during the training phase. Figure 5\nillustrates the proportion of features retained for both the text and\nvisual modalities during the training phase.\nCross-modal Feature Correlation Analysis. Apart from the the\nsuperior performance, the key advantage of our proposed model\ncompared to other models is that its sequential multimodal learning.\nIt can conditionally assist visual modalities in selecting domain-\ninvariant features based on the domain-invariant features learned"}, {"title": "A. METHODOLOGY", "content": ""}, {"title": "A.1 Keyframe-aware Masking", "content": "Given that there is a large amount of frames in a video clip, which\ncontains redundant information. The frame sequence x of a video\nclip contains rich priors, which explicitly correspond to neighboring\nframes. We can easily obtain the motion of the video frame sequence\nto guide the masking of redundant frames according to the temporal\ndifference. Temporal neighbor frames in a video clip can be divided\ninto global neighbor frames and local neighbor frames. The local\nand global difference information are defined as:\n$M_{local} = \\frac{1}{2k} (\\sum_{j=i-k}^{i+k} x_j + \\sum_{j=i+1}^{i+k} x_v) - X_v$\n(20)\n$M_{global} = MultiHead(x_v, X_v, X_v)$,\n(21)\nwhere the subscript i denotes the current frame. The stride k con-\ntrols the window size of the local neighbor frame. For both ends of\nthe video frame sequence, we employ replicate padding strategy\n[49] to pad the original sequence length $T_v$ to targer sequence length\n$T_v + 2k$. The first frame is repeated k times for the beginning and\nthe last frame is repeated k times for the end. For global difference\ninformation, we utilize multi-head attention [68] to capture the\nrelative dependencies of all frames. The local-global embeddings\n$M = [M_{local}, M_{global}]$ passes through a Multi-Layer Perceptron\n(MLP) to predict the probability whether to mask the video frame.\nFormally,\n$\\pi = Softmax(MLP(M)), \\pi \\in \\mathbb{R}^{T_v \\times 2}$,\n(22)\nwhere the probability of index '0' ($\\pi_{i,0}$) of $\\pi$ means to mask this\nvideo frame, and the probability of index '1' ($\\pi_{i,1}$) means to keep\nthis video frame. The subscripts i represents i-th frame in the video\nclip. We can easily obtain the keyframe masking decision vector\nD by sampling from probability and drop the uninformative\nframe $x_v^e = x_v D$ [62]. To ensure that the sparse video frame\nsequence $x_v^e$ and the original sequence $x_v$ have similar semantics in\nthe embedding space, we employ gated recurrent units GRU [19] and\nL2 regularization to compute video frame sequence reconstruction\nloss:\n$L_{recon} =|| GRU(x_v^e) - GRU(x_v) ||_2$\n(23)"}]}