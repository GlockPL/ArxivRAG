{"title": "DILATEQUANT: ACCURATE AND EFFICIENT DIFFUSION QUANTIZATION VIA WEIGHT DILATION", "authors": ["Xuewen Liu", "Zhikai Li", "Qingyi Gu"], "abstract": "Diffusion models have shown excellent performance on various image generation tasks, but the substantial computational costs and huge memory footprint hinder their low-latency applications in real-world scenarios. Quantization is a promising way to compress and accelerate models. Nevertheless, due to the wide range and time-varying activations in diffusion models, existing methods cannot maintain both accuracy and efficiency simultaneously for low-bit quantization. To tackle this issue, we propose DilateQuant, a novel quantization framework for diffusion models that offers comparable accuracy and high efficiency. Specifically, we keenly aware of numerous unsaturated in-channel weights, which can be cleverly exploited to reduce the range of activations without additional computation cost. Based on this insight, we propose Weight Dilation (WD) that maximally dilates the unsaturated in-channel weights to a constrained range through a mathematically equivalent scaling. WD costlessly absorbs the activation quantization errors into weight quantization. The range of activations decreases, which makes activations quantization easy. The range of weights remains constant, which makes model easy to converge in training stage. Considering the temporal network leads to time-varying activations, we design a Temporal Parallel Quantizer (TPQ), which sets time-step quantization parameters and supports parallel quantization for different time steps, significantly improving the performance and reducing time cost. To further enhance performance while preserving efficiency, we introduce a Block-wise Knowledge Distillation (BKD) to align the quantized models with the full-precision models at a block level. The simultaneous training of time-step quantization parameters and weights minimizes the time required, and the shorter backpropagation paths decreases the memory footprint of the quantization process. Extensive experiments demonstrate that DilateQuant significantly outperforms existing methods in terms of accuracy and efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, diffusion models have shown excellent performance on image generation (Li et al., 2022; Zhang et al., 2023b;c), but the substantial computational costs and huge memory footprint hinder their low-latency applications in real-world scenarios. Numerous methods (Nichol & Dhariwal, 2021; Song et al., 2020; Lu et al., 2022) have been proposed to find shorter sampling trajectories for the thousand iterations of the denoising process, effectively reducing latency. However, complex networks with a large number of parameters used in each denoising step are computational and memory intensive, which slow down inference and consume high memory footprint. For instance, the Stable-Diffusion (Rombach et al., 2022) with 16GB of running memory still takes over one second to perform one denoising step, even on the high-performance A6000.\nModel quantization is one of the most popular compression methods. By quantizing the weights and activations with low-bit integers, we can reduce memory requirements and accelerate computational operations. The effects become more noticeable as the bit-width decreases. For example, employing 8-bit models can achieve up to a 4\u00d7 memory compression and 2.35\u00d7 speedup compared to 32-bit full-precision models on a T4 GPU (Kim et al., 2022). Adopting 4-bit models can further deliver"}, {"title": "2 RELATED WORK", "content": "2.1 DIFFUSION MODEL ACCELERATION\nWhile diffusion models have generated high-quality images, the substantial computational costs and huge memory footprint hinder their low-latency applications in real-world scenarios. To reduce the inference computation, numerous methods have been proposed to find shorter sampling trajectories, efficiently accelerating the denoising process. For example, (Nichol & Dhariwal, 2021) shortens the denoising steps by adjusting variance schedule; (Song et al., 2020) generalizes diffusion process to a non-Markovian process by modifying denoising equations; (Lu et al., 2022) uses high-order solvers to approximate diffusion generation. These methods have achieved significant success, obtaining comparable performance with nearly 10% of the denoising steps. However, they involve expensive retraining and complex computations. Conversely, we focus on the complex networks of diffusion models, accelerating them at each denoising step with a quantization method, which not only reduces the computational cost but also compresses the model size.\n2.2 MODEL QUANTIZATION\nModel quantization, which represents the original floating-point parameters with low-bit values, compresses model size and accelerates inference. Depending on whether the model's weights are fine-tuned or not, it generally falls into two categories: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ calibrates the quantization parameters with a small dataset and does not require fine-tuning the model's weights, making it data- and time-efficient. The reconstruction-based PTQ techniques, such as BRECQ (Li et al., 2021), utilize gradient descent algorithms to optimize quantization parameters, which have yielded remarkable results in conven-"}, {"title": "3 PRELIMINARIES", "content": "3.1 QUANTIZATION\nThe uniform quantizer is one of the most hardware-friendly choices, and we use it in our work. The quantization-dequantization process of it can be defined as:\nQuant: $X_{int} = clip(\\frac{x}{\\Delta} + z, 0, 2^{b}-1)$ (1)\nDeQuant : $\\hat{x} = \\Delta \u00b7 (X_{int} - z) \u2248 x$ (2)\nwhere x and $x_{int}$ are the floating-point and quantized values, respectively, [.] represents the rounding function, and the bit-width b determines the range of clipping function clip(\u00b7). In the dequantization process, the dequantized value $\\hat{x}$ approximately recovers x. Notably, the upper and lower bounds of x determine the quantization parameters: scale factor \u2206 and zero-point z, as follows:\n$\\Delta = \\frac{max(x) \u2013 min(x)}{2^{b}-1}$,  $z = \\lfloor - \\frac{min(x)}{\\Delta}\\rceil$ (3)\nCombining the two processes, we can provide a general definition for the quantization function, Q(x), as:\n$Q(x) = \\Delta. (clip(\\lfloor \\frac{x}{\\Delta} \\rceil+z, 0, 2^{b} \u2212 1) \u2212z)$ (4)\nAs can be seen, quantization is the process of introducing errors: $\\lfloor \u00b7 \\rceil$ and clip(\u00b7) result in rounding error ($E_{round}$) and clipping error ($E_{clip}$), respectively. To set the quantization parameters, we commonly use two calibration methods: Max-Min and MSE. For the former, quantization parameters are calibrated by the max-min values of x, eliminating the $E_{clip}$, but resulting in the largest \u2206; for the latter, quantization parameters are calibrated with appropriate values, but introduce the $E_{clip}$.\n3.2 EQUIVALENT SCALING\nEquivalent scaling is a mathematically equivalent per-channel scaling transformation that offline shifts the quantization difficulty from activations to weights. For a linear layer in diffusion model, the output Y = XW, Y \u2208 $R^{N\u00d7C^{o}}$, X \u2208 $R^{N\u00d7C^{i}}$, W \u2208 $R^{C^{i}\u00d7C^{o}}$, where N is the batch-size, $C^{i}$ is the input channel, and $C^{o}$ is the output channel. The activation X divides a per-in-channel scaling factor s \u2208 $R^{C^{i}}$, and weight W scales accordingly in the reverse direction to maintain mathematical equivalence:\nY = (X/s)(s. W) (5)\nThe formula also suits the conv layer. By ensuring that s > 1, the range of activations can be made smaller and the range of weights larger, thus in transforming the difficulty of quantization from activations to weights. In addition, given that the X is usually produced from previous linear operations, we can easily fuse the scaling factor into previous layers' parameters offline so as not"}, {"title": "4 METHOD", "content": "4.1 WEIGHT DILATION\nInspired by the quantization of LLMs, we propose a novel equivalent scaling algorithm, denotes as Weight Dilation (WD), which decreases the range of activations while maintaining the range of weights unchanged (as shown in Figure 3), thereby efficiently alleviating the wide range activations.\nAnalyzing quantization error. We start by analyzing the error from weight-activation quantization. Taking a linear layer with X \u2208 $R^{N\u00d7C^{i}}$ and W \u2208 $R^{C^{i}\u00d7C^{o}}$ as example, considering that we calibrate the quantization parameters of X and W with a MSE and Max-Min manner, respectively, the quantization function (Eq. 4) for activations and weights can be briefly written as:\nQ(X) = $\u0394_{x}$ clip$(\\frac{X}{\u0394_{x}}+z)$, Q(W) = $\u0394_{\u03c9}$\u00b7 $\\frac{W}{\u0394_{\u03c9}}$ (6)\nwhere $\u0394_{x}$ and $\u0394_{\u03c9}$ are scale factors for activations and weights, respectively. The quantization error of activations $E_{x}$ and weights $E_{w}$ from rounding (denoted as $E_{round}$) and cliping (denoted as $E_{clip}$) function can be represented as:\n$E_{x}$ = $\u0394_{x}$ ($E_{round}$ + $E_{clip}$), $E_{w}$ = $\u0394_{w}$\u00b7 $E_{round}$ (7)\nSince the rounding function maps a floating-point number to an integer, $E_{round}$ does not vary, as demonstrated in AWQ (Lin et al., 2024). We scale the X and W using a scaling factor s \u2208 $R^{C^{i}}$ to obtain the scaled X' and W\u0384. The quantization functions and errors after scaling are as follows:\nQ(X') = Q(X/s) = $\u0394'_{x}$ clip$(\\frac{X/s}{\u0394'_{x}} +z)$, Q(W') = Q(s \u2022 W) = $\u0394'_{w}$\u00b7 $\\frac{s\u00b7W}{\u0394'_{w}}$ (8)\n$E'_{x}$ = $\u0394'_{x}$ ($E_{round}$ + $E'_{clip}$), $E'_{w}$ = $\u0394'_{w}$\u00b7$E_{round}$ (9)\nwhere $\u0394'_{x}$ and $\u0394'_{w}$ are new scale factors, and $E'_{clip}$ is the new error of cliping function. By ensuring that s > 1, the range of activations becomes smaller, resulting in $E'_{clip}$/ $E_{clip}$ < 1 and $\u0394'_{x}$/$\u0394_{x}$ < 1 (according to Eq. 3). However, the range of weights becomes larger, resulting in $\u0394'_{\u03c9}$/$\u0394_{\u03c9}$ > 1. Consequently, while the new error of activations quantization $E'_{x}$ decreases, the new error of weights quantization $E'_{w}$ increases, leading to unstable model convergence during training. Therefore, the perfect scaling we desired is to achieve s > 1 while maintaining $\u0394'_{\u03c9}$ \u2248 $\u0394_{\u03c9}$\u00b7"}, {"title": "4.2 TEMPORAL PARALLEL QUANTIZER", "content": "Previous methods (He et al., 2023; Wang et al., 2024) utilize multiple activation quantizers for a layer to quantize activations at different time steps. However, since each quantizer is independent, these methods optimize each quantizer individually using time-step calibration sets, which is data- and time-inefficient. For example, EfficientDM uses 819.2K samples for a total of 12.8K iterations for DDIM on CIFAR-10 (Krizhevsky et al., 2009).\nDifferent from previous methods, as shown in Figure 3, we design a novel quantizer, denotes as Temporal Parallel Quantizer (TPQ), which sets time-step quantization parameters for activations, instead of simply stacking quantizers. Specifically, it utilizes an indexing approach to call the corresponding quantization parameters for samples at different time steps. This enables support for parallel training of different quantization parameters, significantly reducing the data and time costs of training. For a model with T time steps, the quantization parameters of TPQ are as follows:\n$\u0394_{x}^{T}$ = {$\u0394_{1}, \u0394_{2}, \u0394_{3},..., \u0394_{T}$}, $Z^{T}$ = {$z_{1}, z_{2}, z_{3},..., z_{T}$} (13)\nWe detail TPQ design for the different layers of the diffusion models. For the conv and linear layers, they take input x \u2208 $R^{|T|\u00d7C^{i}}$ and x \u2208 $R^{|T|\u00d7C^{i}\u00d7H\u00d7W}$, respectively, where T is a set containing different time-step indexes, T \u2282 {1, . . ., T}, |\u00b7| represents the number of set elements. The quanti-"}, {"title": "4.3 BLOCK-WISE KNOWLEDGE DISTILLATION", "content": "QAT significantly alleviates accuracy degradation in low-bit cases, but it has several limitations for diffusion models: (1) QAT typically requires original training data, which can sometimes be challenging or even impossible to obtain due to privacy or copyright concerns; (2) QAT involves end-to-end retraining of the whole complex networks, which is training-unstable and time-intensive.\nTo address these limitations, inspired by the reconstruction method in PTQ (Li et al., 2021), we propose a novel distillation strategy called Block-wise Knowledge Distillation (BKD). Assume the target model for quantization has K blocks ($B_{1}$, ..., $B_{K}$), and the input samples of model are x, which is generated by the full-precision model. BKD trains the quantized network block-by-block and aligns it with full-precision network at block level. More specifically, assume that block $B_{k}$ is going to be quantized, and its quantized version is $\\hat{B_{k}}$. We update the quantization parameters ($\u0394_{x}, Z, \u0394_{w}$) and weights (w) of $B_{k}$ using the mean square loss L:\n$L_{\u0394_{x},Z,\u0394_{w},w}$ = MSE ($B_{k}\\hat{B}_{k-1}$...$\\hat{B}_{1}$(x) - $B_{k}$$\\hat{B}_{k-1}$...$\\hat{B}_{1}$(x)) (15)\nAs can be seen, (1) BKD does not rely on original training data; (2) BKD shortens the gradient back-propagation path by aligning blocks, which enhances training stability and decreases the memory footprint of the quantization process. In addition, BKD trains quantization parameters and weights in parallel, which not only further saves training time but adapts the weights to each time step."}, {"title": "5 EXPERIMENT", "content": "5.1 EXPERIMENTAL SETUP\nModels and metrics. We evaluate DilateQuant for both unconditional and conditional image generation. The comprehensive experiments include DDPM (Song et al., 2020), LDM (Rombach et al., 2022) and Stable-Diffusion on 5 datasets. The performance of the quantized models is evaluated with Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017), Sliding FID (sFID) (Salimans et al., 2016), and Inception Score (IS) (Salimans et al., 2016). For text-guided generation, we add CLIP score (Hessel et al., 2021) to evaluate the semantic relevance of the generated images to the text prompts. Following the common practice, the Stable-Diffusion generates 10,000 images, while all other models generate 50,000 images. Besides, we also calculate the Bit Operations and Size of models to visualize the effects of model acceleration and compression.\nQuantization and comparison settings. We employ DilateQuant with the standard channel-wise quantization for weights and layer-wise quantization for activations. To highlight the efficiency, DilateQuant selects 5120 samples for calibration and trains for 5K iterations with a batch size of 32, aligning with PTQ-based method (Liu et al., 2024). The Adam (Kingma & Ba, 2014) optimizer is adopted, and the learning rates for quantization parameters and weights are set as 1e-4 and 1e-2, respectively. For the experimental comparison, we compare DilateQuant with PTQ-based method (Liu et al., 2024) and variant QAT-based methods (He et al., 2023; Wang et al., 2024). Since these two variant QAT-based methods employ non-standard settings, we modify them to standard settings for a fair comparison. To further compare with them, we also employ the same non-standard settings on DilateQuant to conduct experiments in the Appendix E. All experiments are performed on one RTX A6000. The more detailed experimental implementations are showcased in Appendix B."}, {"title": "5.2 MAIN RESULT", "content": "Unconditional generation. We start by comparing the quantization results for unconditional generation, as reported in Table 1. We focus on the performance of low-bit quantization to highlight the advantages of DilateQuant. In 4-bit quantization, previous works all suffer from non-trivial performance degradation. For instance, EDA-DM and QuEST become infeasible on LSUN-Bedroom, and EfficientDM remains far from practical usability on LSUN-Church. In sharp contrast, DilateQuant achieves a substantial improvement in quantization performance, with encouraging 6.28 and 4.98 FID improvement over EfficientDM on two LSUN datasets, respectively. Additionally, in 6-bit quantization, DilateQuant can achieve a fidelity comparable to that of the full-precision baseline.\nConditional generation. The quantization results for conditional generation are reported in Table 2. For text-guided generation with 6-bit precision, DilateQuant improves the FID to 24.69 with 5.3\u00d7 Model size compression and 27.9\u00d7 Bit Operations reduction, effectively advancing the low-latency applications of Stable-Diffusion in real-world scenarios. Besides, DilateQuant achieves significant improvements at all bit-width settings on class-guided generation.\nWe add human preference assessments in Appendix I.\n5.3 ABLATION STUDY\nThe ablation experiments are conducted over DDIM on CIFAR-10 with 4-bit quantization. We start by analysing the efficacy of each proposed component, as reported in Table 3. We use the SoTA PTQ-based framework, EDA-DM (Liu et al., 2024), as the baseline, which fails to maintain per-"}, {"title": "6 CONCLUSION", "content": "In this work, we propose DilateQuant, a novel quantization framework for diffusion models that offers comparable accuracy and high efficiency. Specifically, we find the unsaturation property of the in-channel weights and exploit it to alleviate the wide range of activations. By dilating the unsaturated channels to a constrained range, our method costlessly absorbs the activation quantization errors into weight quantization. Furthermore, we design a flexible quantizer that sets time-step quantization parameters to time-varying activations and supports parallel quantization for training process, significantly improving the performance and reducing time cost. We also introduce a novel knowledge distillation strategy to enhance performance, which aligns the quantized models with the full-precision models at a block level. The simultaneous training of parameters and shorter backpropagation paths minimize the time and memory footprint required. Exhaustive experiments demonstrate that DilateQuant significantly outperforms existing methods in low-bit quantization."}, {"title": "A SUPPLEMENTARY MATERIAL INTRODUCTION", "content": "In this supplementary material, we present the correlative introductions and some experiments mentioned in the paper. The following items are provided:\n\u2022 Detailed experimental implementations for all experiments in Appendix \u0412.\n\u2022 Robustness of DilateQuant for time steps and samplers in Appendix C.\n\u2022 Efficiency comparisons of various quantization frameworks in Appendix D\n\u2022 Thorough comparison with EfficientDM and QuEST in Appendix E.\n\u2022 Workflow and effects of Weight Dilation algorithm in Appendix F.\n\u2022 Different equivalent scaling algorithms for diffusion models in Appendix G.\n\u2022 Hardware-Friendly quantization in Appendix H.\n\u2022 Human preference evaluation in Appendix I."}, {"title": "B DETAILED EXPERIMENTAL IMPLEMENTATIONS", "content": "In this section, we present detailed experimental implementations, including the pre-training models, qunatization settings, and evaluation.\nThe DDPM\u00b9 models and LDM\u00b2 models we used for the experiments are obtained from the official websites. For text-guided generation with Stable-Diffusion, we use the CompVis codebase\u00b3 and its v1.4 checkpoint. The LDMs consist of a diffusion model and a decoder model. Following the previous works (Liu et al., 2024; He et al., 2023; Wang et al., 2024), DilateQuant focus only on the diffusion models and does not quantize the decoder models. We empoly channel-wise asymmetric quantization for weights and layer-wise asymmetric quantization for activations. The input and output layers of models use a fixed 8-bit quantization, as it is a common practice. The weight and activation quantization ranges are initially determined by minimizing values error, and then optimized by our knowledge distillation strategy to align quantized models with full-precision models at block level. Since the two compared methods employ non-standard settings, we modify them to standard settings for a fair comparison. More specifically, we quantize all layers for EfficientDM, including Upsample, Skip_connection, and AttentionBlock's qkvw, which lack quantization in open-source code. However, when these layers, which are important for quantization, are added, the performance of EfficientDM degrades drastically. To recover performance, we double the number of training iterations. QuEST utilizes channel-wise quantization for activations at 4-bit precision in the code, which is not supported by hardware. Therefore, we adjust the quantization setting to layer-wise quantization for activations. For experimental evaluation, we use open-source tool pytorch-OpCounter to calculate the Size and Bops of models before and after quantization. And following the quantization settings, we only calculate the diffusion model part, not the decoder and encoder parts. We use the ADM's TensorFlow evaluation suite guided-diffusion to evaluate FID, sFID, and IS, and use the open-source code clip-scores to evaluate CLIP scores. As the per practice (Liu et al., 2024; Wang et al., 2024), we employ the zero-shot approach to evaluate Stable-Diffusion on COCO-val for the text-guided experiments, resizing the generated 512 \u00d7 512 images and validation images in 300 \u00d7 300 with the center cropping to evaluate FID score and using text prompts from COCO-val to evaluate CLIP score."}, {"title": "C ROBUSTNESS OF DILATEQUANT FOR TIME STEPS AND SAMPLERS", "content": "To assess the robustness of DilateQuant for samplers, we conduct experiments over LDM-4 on ImageNet with three distant samplers, including DDIMsampler Song et al. (2020), PLMSsampler Liu et al. (2022), and DPMSolversampler Lu et al. (2022). Given that time step is the most important hyperparameter for diffusion models, we also evaluate DilateQuant for models with different time steps, including 20 steps and 100 steps. As shown in Table 5, our method showcases excellent robustness across different samplers and time steps, leading to significant performance enhancements compared to previous methods. Specifically, our method outperforms the full-precision models in terms of FID and sFID at 6-bit quantization, and the advantages of our method are more pronounced compared to existing methods at the lower 4-bit quantization."}, {"title": "D EFFICIENCY COMPARISONS OF VARIOUS QUANTIZATION FRAMEWORKS", "content": "We investigate the efficiency of DilateQuant across data resource, time cost, and GPU memory. We compare our method with PTQ-based method (Liu et al., 2024) and variant QAT-based method (He et al., 2023) on the mainstream diffusion models (DDPM, LDM, Stable-Diffusion). As reported in Table 6, our method performs PTQ-like efficiency, while significantly improving the performance of the quantized models. This provides an affordable and efficient quantization process for diffusion models."}, {"title": "E THOROUGH COMPARISON WITH EFFICIENTDM AND QUEST", "content": "EfficientDM (He et al., 2023) and QuEST (Wang et al., 2024) are two variance QAT-based methods, which achieve 4-bit quantization of the diffusion models with efficiency. However, both of them are non-standard. Specifically, EfficientDM preserves some layers at full-precision, notably the Upsample, Skip_connection, and the matrix multiplication of AttentionBlock's qkvw. These layers have been demonstrated to have the most significant impact on the quantization of diffusion models in previous works (Shang et al., 2023; Li et al., 2023a; Liu et al., 2024). QuEST employs standard channel-wise quantization for weights and layer-wise quantization for activations at 6-bit precision. However, at 4-bit precision, it uses channel-wise quantization for the activations of all Conv and Linear layers, which is hardly supported by the hardware because it cannot factor the different scales out of the accumulator summation (please see Appendix H for details), leading to inefficient acceleration.\nTo thoroughly compare DilateQuant with EfficientDM and QuEST, we conduct experiments on LSUN-church with standard and non-standard quantization settings. When neglecting these layers that are important for quantization, DilateQuant extremely reduces the FID to 8.68 with 4-bit quantization. Compared to the standard setting, the performance improvement is more noticeable. When setting channel-wise quantization for activations, DilateQuant also reduces a 2.84 FID compared with QuEST. Conclusively, DilateQuant significantly outperforms EfficientDM and QuEST at different quantization precisions for both standard and non-standard settings, which demonstrates the stability and standards of DilateQuant."}, {"title": "F WORKFLOW AND EFFECTS OF WEIGHT DILATION ALGORITHM", "content": "The comprehensive workflow of Weight Dilation is illustrated in Algorithm 1. We implement WD in three steps: searching unsaturated channels for scaling (Lines 2-3), calculating scaling factor (Lines 5-10), and scaling activations and weights (Line 12). WD alleviates the wide range activations for diffusion models through a novel equivalent scaling algorithm. In addition, all operations of WD can be implemented simply, making it efficient.\nAlgorithm 1 Overall workflow of WD\nInput: full-precision X \u2208 $R^{N\u00d7C*}$ and W \u2208 $R^{C^{i}\u00d7C^{o}}$\nOutput: scaled X' and W'.\n1: searching unsaturated channels for scaling:\n2:\t obtain $W_{max}$ \u2208 $R^{C^{o}}$ and $W_{min}$ \u2208 $R^{C^{o}}$\n3:\t record in-channel indexes of $W_{max}$ and $W_{min}$ as set A\n4: calculating scaling factor:\n5:\t for k= 1 to $C^{i}$ do\n6:\t\t if k \u2208 A:\n7:\t\t\t set $s_{k}$ = 1\n8:\t\t else:\n9:\t\t\t calculate scaling factor $s_{k}$ with $W_{max}$ and $W_{min}$ as constraints\n10:\t\t end for\n11: scaling X and W:\n12:\t calculate X' = X | s and W\u2032 = W\u00b7s\n13: return X' and W'\nWe assess the effects of WD on various quantization tasks. As reported in Table 8, WD stably achieves s > 1 while maintaining $\u0394_{\u03c9}$ \u2248 $\u0394_{\u03c9}$. It effectively improves performance at different quantized models by losslessly reducing the activation quantization error."}, {"title": "G DIFFERENT EQUIVALENT SCALING ALGORITHMS FOR DIFFUSION MODELS", "content": "In this section, we start by analyzing the differences between LLMs and diffusion models in terms of the challenges of activation quantization. As shown in Figure 2(b), the activation outliers of the diffusion models are present in all channels, unlike in LLMs where the activation outliers only exist in fixed channels. Additionally, the range of activations for diffusion models is also larger than that of the LLMs. Therefore, it is essential to scale the number of channels as much as possible for the diffusion models. Some equivalent scaling algorithms are proposed to smooth out the activation outliers in LLMs, and these methods have achieved success. Smooth Quant (Xiao et al., 2023a) scales all channels using a hand-designed scaling factor. AWQ (Lin et al., 2024) only scales a few of channels based on the salient weight. OmniQuant (Shao et al., 2023) proposes a learnable equivalent transformation to optimize the scaling factors in a differentiable manner. DGQ (Zhang et al., 2023a) devises a percentile scaling scheme to select the scaled channels and calculate the scaling factors.\nUnfortunately, when we applied methods similar to these previous equivalent scaling algorithms to diffusion models, we find that none of them work. Specifically, we employ these four methods for diffusion models as follows: (1) For the method similar to SmoothQuant, we scale all channels before quantization using a smoothing factor a = 0.5; (2) For the method similar to AWQ, we scale"}, {"title": "H HARDWARE-FRIENDLY QUANTIZATION", "content": "In this section, we investigate the correlation between quantization settings and hardware acceleration. We start with the principle of quantization to achieve hardware acceleration. A matrix-vector multiplication, y = Wx + b, is calculated by a neural network accelerator, which comprises two fundamental components: the processing elements $C_{n,m}$ and the accumulators $A_{n}$. The calculation operation of accelerator is as follows: firstly, the bias values $b_{n}$ are loaded into accumulators; secondly, the weight values $W_{n,m}$ and the input values $x_{m}$ are loaded into $C_{n,m}$ and computed in a single cycle; finally, their results are added in the accumulators. The overall operation is also referred to as Multiply-Accumulate (MAC):\n$A_{n}$ = $\\sum_{m}W_{n,m}x_{m}$+ $b_{n}$ (16)\nwhere n and m represent the out-channel and in-channel of the weights, respectively. The pre-trained models are commonly trained using FP32 weights and activations. In addition to MAC calculations, data needs to be transferred from memory to the processing units. Both of them severely impact the speed of inference. Quantization transforms floating-point parameters into fixed-point parameters, which not only reduces the amount of data transfer but also the size and energy consumption of the MAC operation. This is because the cost of digital arithmetic typically scales linearly to quadratically with the number of bits, and fixed-point addition is more efficient than its floating-point counterpart. Quantization approximates a floating-point tensor \u00e6 as:\n$x_{int} = \\lfloor \\frac{x}{\\Delta} \\rceil \u2248 x$ (17)\nwhere $x_{int}$ and $x$ are integer tensors and quantized tensors, respectively, and $\\Delta$ is scale factor. Quantization settings have different granularity levels. Figure 5 shows the accelerator operation after the introduction of quantization. If we set both activations and weights to be layer-wise quantization,"}, {"title": "I HUMAN PREFERENCE EVALUATION", "content": "In this section, we use an open-source aesthetic predictor to evaluate Aesthetic Score \u2191, mimicking human preference assessment of the generated images. As reported in Table 10, DilateQuant has a better aesthetic representation compared to EfficientDM, which demonstrates that the quantized models with our method are more aesthetically pleasing to humans. For the large text-to-image model, we use the convincing DrawBench benchmark to evaluate human performance, as shown in Figure 6. Additionally, we visualize the random samples of quantization results in Figure 7 (LSUN-church), 8 (LSUN-Bedroom), and 9 (ImageNet). As can be seen, DilateQuant outperforms previous methods in terms of image quality, fidelity, and diversity."}]}