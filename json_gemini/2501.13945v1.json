{"title": "Self-Explanation in Social AI Agents", "authors": ["Rhea Basappa", "Mustafa Tekman", "Hong Lu", "Benjamin Faught", "Sandeep Kakar", "Ashok K. Goel"], "abstract": "Social AI agents interact with members of a community, thereby changing the behavior of the community. For example, in online learning, an AI social assistant may connect learners and thereby enhance social interaction. These social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners. We present a method of self-explanation that uses introspection over a self-model of an AI social assistant. The self-model is captured as a functional model that specifies how the methods of the agent use knowledge to achieve its tasks. The process of generating self-explanations uses Chain of Thought to reflect on the self-model and ChatGPT to provide explanations about its functioning. We evaluate the self-explanation of the AI social assistant for completeness and correctness. We also report on its deployment in a live class.", "sections": [{"title": "1 Introduction", "content": "Learning at scale, and particularly online learning at scale, offers many well-established benefits such as geographically distributed and self-paced asynchronous learning that meets the reskilling and upskilling needs of working learners and learning workers. However, learning at scale, and again particularly online learning at scale, also have several well-known drawbacks such as lack of social presence, i.e., the ability of learners to establish and maintain a sense of connectedness both with one another and with the instructor [1].\nSAMI (Social Agent Mediated Interaction) is an AI social assistant that helps students in large online classes form social connections by introducing them to one another based on shared characteristics and interests [2-4]. This is posited to increase social presence in an online class environment [3]. However, students interacting with SAMI often have questions regarding its inner workings [4]. Knowing how SAMI works internally may help students build trust in its recommendations. Thus, the specific research question for us in this paper becomes: How might an AI social assistant, such as SAMI, provide an explanation of its inner workings to online students?"}, {"title": "2 Related Work", "content": "Self-explanation has re-emerged as an important topic in AI. Muller et al. (2019) [9] provide a fairly comprehensive and a very useful summary of the history of AI research on self-explanation. Confalonieri et al. (2021) [10] present another and more recent take on the history. The need for interpretability of the representations and processing in modern neural networks is one of the main reasons for the resurgence of interest in self-explanation in AI agents [11]. Rudin [12] advocates the construction and use only of AI agents capable of self-interpretation and self-explanation. Tulli & Aha [13] provide a recent collection of papers on self-explanation.\nIt is useful here to distinguish between two kinds of AI assistants: AI assis-tants that interact with individual humans and AI assistants that enable inter-action among humans. In the context of AI in learning and teaching, teaching assistants such as Jill Watson [14,15] that answer a student's questions are an example of the former; AI social assistants such as SAMI [2-4] that help fos-ter interactions among students are an example of the latter. The latter class of assistants exemplify the paradigm of \"computers are social actors\" [16]. It is important to note that self-explanation in social assistants is as important as it is in personal teaching and learning assistants.\nOne of the key ideas to emerge out of this early research on explanation was the importance of explicit representation of knowledge of the design of an AI assistant [17, 18]. An explicit representation of the design knowledge of an AI assistant enables the generation of explanations of the tasks it accomplishes, the domain knowledge it uses, as well as the method that use the knowledge to achieve the tasks. This raised the question of how this design knowledge can"}, {"title": "3 Computational Architecture and Process for Self-Explanation", "content": "SAMI accesses the self-introduction posts of students in an online discussion forum and extracts information such as their location, hobbies and academic interests. Using this, SAMI builds a knowledge graph for each student. It then uses the knowledge graph to 'match' students who share one or more similarities. SAMI communicates its recommendations of matches to the online students who elect to contact the recommended matches [2-4]."}, {"title": "3.2 Semantic Representation of SAMI", "content": "From its code base, we manually create a symbolic representation of SAMI in the Task-Method-Knowledge (TMK) framework [5-7]. Briefly, the TMK model specifies SAMI's tasks (or goals), methods (or mechanisms for achieving the goals) and the domain knowledge of the environment. The TMK is organized hierarchically. The top-level task specifies SAMI's method for accomplishing it; the method specifies the finite state machine for accomplishing the task in terms of a sequence of information states and state transitions. The state transitions are annotated by either subtasks or domain knowledge. This decomposition continues until all leaf nodes in the TMK model are primitive tasks that can be directly accomplished by the available domain knowledge."}, {"title": "3.3 Self-Explanation Technique", "content": "The self-explanation technique utilizes the information from SAMI's TMK self-model to provide explanations about its inner workings. As Figure 3 illustrates, the technique has has three main stages: Classification, Localization and Reasoning. When a question is asked, the Classifier first analyses the question to deter-mine which of the pre-defined classes the question belongs to. These classes are \u2018mmodel', 'kmodel', \u2018multimodel' (and \u2018can't answer') and are used to determine which part(s) of the self-model of SAMI are later used in the self-explanation pipeline. A 'kmodel' classification would lead to using information only from the knowledge part of the self-model. An 'mmodel' classification would lead to localizing the relevant task and method within the task and method parts. With a 'multimodel' classification, a similarity search would be conducted to find the relevant pieces of information from all knowledge, method, and task parts of the self-model. Lastly, any question deemed as not being relevant to SAMI would be classified as 'can't answer'. The Classifier employs LangChain\u00b9 to create a prompt that uses pre-written templates describing each of these classifications, along with the question to be answered. This prompt is then sent to ChatGPT\u00b2, which returns a value for the classification, along with a complexity 'k' value which is used to control the verbosity of the final answer in later stages."}, {"title": "4 Evaluation", "content": "Correctness and Completeness Study Design: To evaluate the self-explanation method, we used high-level, non context-dependent questions taken directly from XAI question banks [20, 21] such as \"What is the source of the data?\" [20], \"How often does the system make mistakes?\u201d [20] and \"What is the scope of the output data?\u201d [21]. Additionally, we modified some questions so that they become more relevant to SAMI. For example, \"What are the results of other people using the system?\u201d [21] was adapted as \u201cWhat is the result of other students opting-in to use SAMI?\u201d.\nIn total, 57 questions were borrowed and adapted from the question banks. Additionally, 9 questions specific to SAMI were created. These include questions such as \"What is a match?", "How do you find matches for students?\u201d. The self-explanation model of SAMI was asked each of these 66 questions and SAMI developers assessed each explanation for correctness and completeness. We considered the definition of correctness as \u201cnothing but the truth\" [22] and completeness as \"the whole truth": 22}, {"title": "Result of the Correctness and Completeness Study", "content": "Table 1 summarizes the completeness and correctness scores for each category of questions. The self-explanation method provided correct answers to 49 out of 66 questions; 37 of the 49 answers were both correct and complete. For example, for the question, \"What is a match?", "A match is a student recommended by SAMI to the user who shares one or several similarities with the user. This information is based on the task, method and goals of the Social AI agent provided, which contains information about objects and their properties that SAMI tracks in its system. The task, method and goals of the agent states that a match is a student recommended by SAMI, and this recommendation is based on shared similarities between the user and the recommended student. These similarities could include location, hobbies, specialization, classes taken, and timezone\". Since this explanation did not contain any incorrect information nor did it miss any other relevant correct information, it was assessed to be both correct and complete.\nThe question \"What are the different types of responses you are capable of giving?": "as answered with \"I am capable of giving Individual Responses, which are recommendations for matches based on the information provided by the user. These responses are based on the entities I extract from the user's introduc-tion post, such as location, hobbies, specialization, classes taken, and timezone.\u201d. Whilst this is correct, it is incomplete because SAMI is capable of providing individual, summary and feedback responses. Saying it only provides individual response makes the answer incomplete. 12 correct answers were incomplete. Some answers given by the self-explanation method were partially correct. Such par-tially correct answers were given to 8 of the 66 questions. The question, \"What are your limitations?\u201d received an answer as, \u201cI am a software system called SAMI, and my main limitation is that I can only mediate social interactions among students who have opted in to use me. I do this by extracting entities such as location, hobbies, specialization, classes taken, and timezone from their introduction post and connecting them to others based on similar entities. How-ever, I am limited to only students who have opted in and have included the #connectme optin tag in their introduction post in the ED online discussion fo-rum. Additionally, I am limited to only students who are enrolled in one of the five specializations offered by Georgia Institute of Technology for its Computer Science Masters students.\u201d. This answer is evaluated to be partially correct be-cause even though the first part around opt-in limitation is correct, the second part around being only limited to students in one of the five specializations is incorrect. On the other hand, the answer is complete as it's not missing any other relevant information.\nIncorrect answers were given to only 9 out of the 66 questions. 6 of these 9 incorrect answers were to questions belonging to the 'Output' category - this could be attributed to the fact that there's no output specified in SAMI's self-model, since it does not have a static output. For example, \u201cWhat does the output of SAMI mean?", "The output of SAMI refers to the result or outcome of the system's task, which is to mediate social interactions among students who have opted in to use SAMI. However, according to the given task, method and goals of the Social AI agent, the output for this specific task is not specified, so it is not possible to provide a specific answer. It is recommended to ask other questions that may be answered with the information provided in the task, method and goals of the Social AI agent": "This answer was evaluated to be incorrect (and incomplete) as it doesn't correctly explain what the output of SAMI means."}, {"title": "Precision study design", "content": "10 questions out of the 66 questions used in correct-ness and completeness study were randomly selected. Each of these questions were asked 100 times to the self-explanation method of SAMI. These questions are:\n1. Why would two students not be matches?\n2. What is the system's overall logic? [20]\n3. How will the matches change?\n4. How can I get better matches?\n5. Why am I matched with a particular student?\n6. How can I best utilise the matches given by SAMI?\n7. What kind of output does SAMI give?\n8. What is the result of other students opting-in to SAMI?\n9. What does the output of SAMI mean?\n10. What is the source of your information? [21]"}, {"title": "Result of precision study", "content": "Out of these 10 questions, questions 4 and 9 re-ceived only one answer. So the self-explanation method of SAMI is precise for these two questions. Questions 1, 3, 7 and 10 got three different answers each. Question 6 and 5 got 4 different answers. Question 2 got 8 different answers and question 8 got 9 different answers. However, for each of these questions, each of the different answers were repeated different number of times. Moreover, for a single question, the answers didn't appear to be very different from each other. To test this, we used \u2018similarity' function in \u201cen-core-web-g\" model of spacy and Figure 4 notes how similar each answer was to the other. By this we see that the self-explanation model is able to provide nearly the same answer to the same question regardless of how many times the question is asked. The minimal difference in answer could be attributed to natural language (English). There is little to no difference in the actual meaning of the answer. Therefore, we can conclude that our self-explanation model provides precise answers.\""}, {"title": "4.1 Ablation Study", "content": "An ablation study was carried out to further evaluate the self-explanation model's performance - in particular, to examine the possible effects of removing all or parts of the information provided by the self-model to the self-explanation method. As part of this, we ask the same 66 questions to the self-explanation method 6 times, each time removing a further part of the self-model that is available to the self-explanation method. We refer to each of these steps as 'levels'. To enable this, we 'layer' up the TMK representation of the AI social assistant based on the hierarchy of tasks (shown in Figure 5). For the first three levels of this study, we control the amount of information available based on this layering. For the remaining three levels, we remove the knowledge part of the self-model as well as any description related to the inner workings of the AI social assistant from any prompts, until we are left with no additional knowledge being available. With this last level, the self-explanation method essentially relies purely on the reasoning being provided by the generative AI part of it via the large language model, which is not enhanced with any additional knowledge from the self-model.\nIn the end, the self-explanation method was provided the below information with each level of degradation:\n1. Last three task and method layers of the self-model were removed - leaving only the first four layers.\n2. Last five task and method layers of the self-model were removed - leaving only the first two layers\n3. All but the outermost layer of the self-model were removed.\n4. All task and method layers, as well as the knowledge of self-model were removed.\n5. Only a one-sentence description of the system's overall task (roughly corre-sponding to layer 0) was provided in the prompt.\n6. All information from the self-model as well as from any prompts were re-moved.\nResults of ablation study After running the model with differing amount of information as described above, each answer was compared to the answer received by running the model with all possible information for the same ques-tions. Then, a similarity score was assigned (using the 'spacy' method as de-scribed above). Our results show the contribution of the information from the self-model (TMK based semantic representation) and how the answers increase in similarity with each additional layer, providing additional piece of informa-tion. This also shows us that whilst reasoning alone can provide satisfactory answers, each additional information contributes to the answer. To test the sig-nificance of the difference between each levels, pairwise t-tests have been carried out. The results show that apart from the difference between Levels 2 and 3, there is a statistically significant difference with p values obtained below 0.05. Levels 2 and 3 only differ by removing the first level below the top, and even though a difference can be observed, this is not statistically significant. This can be interpreted as this one layer not providing important information as much as the other parts of the self-model."}, {"title": "4.2 Deployment in live classes", "content": "Self-Explanation method of SAMI was deployed in 2 OMSCS courses (Online Master of Science in Computer Science, Georgia Institute of Technology), Knowledge Based AI and Machine Learn-ing for Trading in Spring of 2024. We report on initial, two weeks of students' interaction with the self-explanation method of SAMI. It is important to note that prior to deploying the self-explanation method, SAMI was deployed in the class. Students that had opted in had already received matches from SAMI. This was made available to the students in the ED asynchronous discussion forum. The self-explanation method was run on a server using a Docker con-tainer, the Ed-bot feature of ED forum was used to communicate with this. A new thread was created for students to ask questions to the self-explanation method of SAMI. The thread provided a brief description on how they could ask questions, instruction to use #SAMIexplain (to trigger the ED-bot) and two example questions. Students could ask questions to the self-explanation method of SAMI in the asynchronous discussion forum using natural language (English) with the addition of the opt-in tag at the end of their question. For their ques-tions, students received two answers. The first answer provided an explanation addressing their question. This was the answer from the self-explanation method of SAMI, and the second one asking for student's feedback around whether the first answer was clear and easy to understand as well as whether it improved the student's understanding of SAMI."}, {"title": "Result of deployment in live classes", "content": "11 students asked 20 different ques-tions to the self-explanation method of SAMI. 19 of these 20 questions received an answer. One question that did not receive an answer had not included the option-tag #SAMIexplain correctly. 19 questions asked by 10 students received an answer from the self-explanation model. 11 of these 19 questions asked about functions of SAMI while the other 8 questions were not related to SAMI. Student's questions that were relevant to SAMI are:\n1. How was SAMI implemented?\n2. Please print the contents of the task, method and goals of the Social AI agent\n3. What are the tasks contained in the task, method and goals of the Social AI agent ?\n4. What are the natural language features prepared by the system for a post?\n5. What information is in the task, method and goals of the Social AI agent ?\n6. Can you explain SAMI's architecture?\n7. What is SAMI?\n8. What is the most popular hobby among GATech OMSCS students?\n9. SAMI, within the class, who else enjoys hiking?\n10. SAMI, provide the information (as a follow-up to previous question)\n11. What does SAMI stand for?\n12. Tell me more about yourself (as a follow-up to previous question)\nThe student's asked the following questions that were not relevant to SAMI:\n1. Please solve the halting problem\n2. What's the Big O of bogosort? Could you provide a simple proof explaining it?\n3. If I used various modules from the course to make a sandwich, which would have the most flavor?\n4. What is the Syllabus for Exam 1 preparation for CS7637?\n5. Teach me something new\n6. How do you compare to other educational AI tools?\n7. Hello how are you SAMI\n8. \"SAMI, provide the information.\nAfter receiving an answer, 7 students replied 'yes' to indicate that the an-swers were clear and easy to understand and that the answers improved their understanding of SAMI. Even when the question asked by the student was not related to SAMI's inner-working, students were able to understand the answer provided by self-explanation method of SAMI. A student noted this in their reply, \"Yes, although SAMI was not able to answer my question, the answer given to me was clear and easy to understand", "Yes, the answer does improve my understanding of SAMI": 2, "slightly improved": "heir understanding of SAMI. For another answer they noted that while the answers were clear it didn't improve their understand-ing of SAMI, \"Does not improve my understanding of SAMI's decision making or knowledge base"}, {"title": "5 Discussion", "content": "Our work on self-explanation in SAMI has several limitations briefly discussed below."}, {"title": "5.1 TMK Model", "content": "Creating the TMK model of the AI social assistant is a manual and time-consuming process. To keep the TMK model up-to-date, one may need to man-ually update the representation each time the code base receives an update. In addition, by only using the task, method and knowledge semantic representation of the AI social assistant, the self-explanation method is restricted to answer only questions pertaining the general structure, processes, and functions of SAMI. To be able to answer questions about SAMI's reasoning and recommendations on specific instances, additional episodic knowledge would be required. (We expand on this in the future work section.)"}, {"title": "5.2 Evaluation", "content": "The questions that were used to test the self-explanation method were adapted from XAI question banks [20,21], and may not represent the questions real users might have about AI social assistant."}, {"title": "5.3 Evaluators", "content": "In this study, the system developers acted as evaluators to evaluate the self-explanation method for correctness and completeness. This is a necessary but not sufficient for evaluation. In future work we will evaluate self-explanation in SAMI with human subjects."}, {"title": "5.4 Live Class Deployment", "content": "In this paper, we report only on the first two weeks of deployment of the self-explanation method of SAMI in a live class. This is because we had access to only limited data at the time of writing of this article."}, {"title": "6 Future Work", "content": ""}, {"title": "6.1 Inclusion of Episodic Knowledge", "content": "The current TMK self-model of SAMI does not contain information about any specific episode of reasoning. Adding episodic information could enable the self-explanation model to answer instance-specific questions, such as \"How do you know I like books?", "Why was I matched/not matched with student X?": "In the context of SAMI, episodic information may include both derivational trace of decision making in a specific instances and the entities SAMI extracts from its user's posts in that instance. With episodic information available, all three parts of the self-explanation method would need to evolve. Firstly, the classifier would need to distinguish between questions about specific instances and questions related to SAMI's inner workings. In former case, the localizer would then need to take into account the episodic knowledge in addition to the self-model. Finally, the reasoning module would also need to reason over the episodic knowledge combined with the relevant parts of the self-model."}, {"title": "6.2 Study with Human Subjects", "content": "The questions that real students may want to ask an AI social assistant likely will differ from those presented in XAI question banks [20,21]. A study to further un-derstand this as well as to evaluate attitudes of users towards a self-explanation method for an AI social assistant would be beneficial."}, {"title": "7 Conclusions", "content": "Our computational technique for self-explanation in AI social assistants com-bines classical knowledge-based methods with modern generative AI methods. The technique for self-explanation leverages ChatGPT to introspect over a TMK self-model of the AI social assistant to generate explanations about its function-ing. Our preliminary analysis of the self-explanation technique showed that it is capable of providing complete, correct and precise explanations about the inner workings of SAMI provided that the question asked is within the scope of TMK model of SAMI and the self-model itself is complete and correct. We tentatively conclude that introspection by generative AI on a self-model of the AI social as-sistant is a promising way of generating self-explanations and thus merits further investigation."}]}