{"title": "From Internal Conflict to Contextual Adaptation of Language Models", "authors": ["Sara Vera Marjanovi\u0107", "Haeun Yu", "Pepa Atanasova", "Maria Maistro", "Christina Lioma", "Isabelle Augenstein"], "abstract": "Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. Nevertheless, studies indicate that LMs often ignore the provided context as it can conflict with the pre-existing LM's memory learned during pre-training. Moreover, conflicting knowledge can already be present in the LM's parameters, termed intra-memory conflict. Existing works have studied the two types of knowledge conflicts only in isolation. We conjecture that the (degree of) intra-memory conflicts can in turn affect LM's handling of context-memory conflicts. To study this, we introduce the DYNAMICQA dataset, which includes facts with a temporal dynamic nature where a fact can change with a varying time frequency and disputable dynamic facts, which can change depending on the viewpoint. DYNAMICQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. With the proposed dataset, we assess the use of uncertainty for measuring the intra-memory conflict and introduce a novel Coherent Persuasion (CP) score to evaluate the context's ability to sway LM's semantic output. Our extensive experiments reveal that static facts, which are unlikely to change, are more easily updated with additional context, relative to temporal and disputable facts.", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) have been useful in a variety of downstream applications from summarization to fact-checking, often relying on the factual knowledge memorized during pre-training and stored in their parameters, known as parametric memory (Yu et al., 2024). However, this internal memory is not infallible; it may contain misinformation, biases or simply outdated data, causing LMs to produce factually incorrect output, occasionally termed \u2018hallucinations' (Huang et al., 2023). One way to mitigate this issue is the use of retrievers (Guu et al., 2020) to provide additional context to a language model (non-parametric knowledge). However, this context may conflict with the model's parametric knowledge, resulting in a knowledge conflict.\nHandling these knowledge conflicts has been a productive area for research. Initial research looked at behavioural patterns within LMs, where they found a tendency of models to over-rely on parametric knowledge in the face of these knowledge conflicts (Longpre et al., 2021; Chen et al., 2022), thus ignoring the retrieved context. Conflicts triggered by the contradiction of a context and a model's parametric knowledge is often termed context-memory conflicts. Context that conflicts with a model's parametric knowledge is often termed counter-memory (Xie et al., 2024).\nMany of these studies overlook that knowledge conflicts can also happen within the parametric knowledge, for example, due to conflicting representations of the fact in the training data. These type of knowledge conflicts, called intra-memory conflicts, may contribute to LMs' uncertainty and instability (Marjanovi\u0107 et al., 2024). Furthermore, we argue that the existence of intra-memory conflicts should impact LM's behaviour in context-memory conflicts. Ideally, the degree of conflict in parametric memory should determine how much the model relies on non-parametric memory. While intra-memory conflicts remain understudied, elucidation of their interaction with context-memory conflicts impacts work within retrieval augmentation and knowledge editing.\nIn this paper, we study knowledge conflicts in relation to fact dynamicity to effectively simulate realistic knowledge conflicts. To facilitate the approach, we introduce a new dataset for knowledge conflict, DYNAMICQA, which contains facts with presumed entropy in our pretraining dataset alongside static facts, as a control. DYNAMICQA consists of three types of facts: static, temporal, and disputable. Temporal facts are facts that are changed by time and disputable facts depend on one's viewpoint (Figure 1). Furthermore, we introduce proxy scores to approximate the dynamicity of these facts. We then assess the use of semantic entropy (Kuhn et al., 2023) as an indicator of intra-memory conflict, and introduce a novel Coherent Persuasion (CP) score to measure new context's ability to change a LM's semantic output.\nWith DYNAMICQA, we find that manipulated contexts of static facts and temporal facts with low temporality show the greatest persuasiveness, despite their limited historical change and, presumably, future likelihood to change. Furthermore, we find that semantic uncertainty is a poor indicator of intra-memory conflict and it alone does not reflect an instance's likelihood for persuasion given additional context, suggesting that retrieval-augmented generation language models struggle in low-certainty domains. These results underline the need for new measures of intra-memory conflict in the absence of our provided proxy scores and other indicators of adaptive-retrieval efficiency (Ni et al., 2024)."}, {"title": "2 Related Work", "content": "Knowledge Conflicts Xu et al. (2024b) provide a survey for knowledge conflicts in LMs; we focus specifically on intra-memory conflicts and context-memory conflicts. Longpre et al. (2021) first showed that LMs ignore context that contradicts their parametric memory, generating facts not stated in the input, termed as 'hallucinations'. Further work either disentangles the two forms of knowledge conflicts (Neeman et al., 2023), or finds other methods to resolve LM hallucination (Song et al., 2024). Other studies investigate the effect of a fact's frequency in the training data (Yu et al., 2023) or the quality of the presented counter-memory (Xie et al., 2024; Wan et al., 2024) on LM hallucination. Furthermore, some works find that LMs can be swayed by convincing misinformation at inference-time (Pan et al., 2023; Xu et al., 2024a; Wan et al., 2024). The existing findings suggest some pattern to a model's proclivity to ignore context. However, in contrast to these existing works, our work considers realistic, natural examples of knowledge conflict as they occur in the real world and how different types of knowledge conflicts are connected."}, {"title": "Measures for Knowledge Conflicts", "content": "There have been several approaches to quantify or approximate the degree of a knowledge conflict; however, these works only focus on context-memory conflicts. Pezeshkpour (2023) focus on the entropy of the LM's probability distribution for an answer. The difference in entropy depending on the presence or absence of additional context indicates the LM's prior knowledge on a certain fact. On the other hand, Du et al. (2024) propose a susceptibility and persuasion score to investigate the LM when the given context contradicts the LM's parametric knowledge, based on the first token probabilities. The susceptibility score of an entity shows how easy it is to shift an LM's probability distribution of an answer regarding an entity. The persuasion score of a context represents how effective the context is at changing an LM's probability distribution of an answer. Given known issues in the reliance on first-token probabilities (Wang et al., 2024), we reformulate this score with consideration of semantic consistency in \u00a73.3. Furthermore, given the lack of measures for intra-memory conflict, we assess semantic entropy (Kuhn et al., 2023) as an indicator of intra-memory conflict (Gao et al., 2024)."}, {"title": "Sources of Factual Errors", "content": "We identify three causes of model factual error: fact popularity, temporality, and disputability. Two of these causes have been previously investigated in other general studies. Fact popularity approximates the prevalence of a fact in training data and has been shown to affect model performance (Mallen et al., 2023) and a model's likelihood to be swayed by context ('susceptibility score', Du et al. (2024)). In contrast, temporality and disputability reflect a fact's entropy within the training and test data. For example, it is well-established that the passing of time can lead to outdated models, stressing the need for evaluation of models for a specific timeframe (Margatina et al., 2023; Fierro et al., 2024) and updating of specific facts in LMs (Zhang and Choi, 2023; Jang et al., 2022). Similarly, Xu et al. (2024b) argue that context-memory knowledge conflicts arise due to either temporal misalignment or misinformation pollution of the training dataset. In a contemporary study, Fierro et al. (2024) show that facts with dynamic (or in their terms \u2018mutable') relations show different patterns in model confidence, knowledge representation, and contextual alignment. All of these studies look at only one source of factual error at a time and do not assess in the context of knowledge conflicts. In our work, we introduce disputability as a contributor to factual error, study multiple sources of factual error concurrently (temporality, disputability, and popularity), assess their interaction, and provide real-world proxy measures for all three sources."}, {"title": "3 Measuring Knowledge Conflicts", "content": "We describe our two proposed methods to measure knowledge conflicts: semantic uncertainty (Kuhn et al., 2023) and our novel Coherent Persuasion score. Both measures look primarily at semantic changes in LM output, thereby limiting confounds due to syntactic inconsistencies and other issues arising from over-reliance on first-token probabilities. We first describe semantic entropy as a reflection of the intra-memory conflict, and how we determine semantic entailment (\u00a73.2) and then present our Coherent Persuasion (CP) score as a measure of context-memory conflict (\u00a73.3)."}, {"title": "3.1 Preliminaries", "content": "A dataset $X = [(c_1, q_1, y_1), ..., (c_N, q_N, y_N)]$ consists of $N$ tuple instances containing: an answer-containing context $c$, a question $q$, and a ground-truth answer $y$. For the i-th instance, an LM $f$ outputs an answer $a_i = [a_i^1, ..., a_i^h, ..., a_i^H]$, consisting of $H$ number of tokens given an input $x_i$. An input $x_i$ here contains a universal prompt $P$ describing the QA task with either $q_i$ only ($x_i = [P; q_i]$) or with $q_i$ and the context $c_i$ ($x_i = [P; c_i; q_i]$)."}, {"title": "3.2 Semantic Uncertainty Score", "content": "Uncertainty can arise in question-answering tasks as several possible related facts may exist within an LM's parametric knowledge, especially in cases of high intra-memory conflict (Xu et al., 2024b). Given its use of high-temperature sampling to elicit diverse answers contained within parametric memory, we propose the use of semantic uncertainty as an estimator of intra-memory conflict. We then validate this measure using its relation to a fact's dynamicity (defined in \u00a74).\nTo do so, we use Kuhn et al. (2023)'s approach in our simulated knowledge conflict setting, which we then adapt for our Persuasion Score in \u00a73.3. Given the input $x_i$, which contains a universal prompt and a question $q_i$, we first generate K number of answers $A = [a_{i,0}, a_{i,1}, ..., a_{i,K}]$. Next, we group the generated answers according to their semantic similarity. The semantic similarity between two sampled answers is calculated using a DeBERTA Natural Language Inference (NLI) model. The details about answer generation and semantic grouping are in Appendix A.3. The grouping according to the semantic similarity results in the $V$ number of groups $G = [g_1, g_2, ..., g_v, ..., g_V]$, where $V$ can range from 1 to $K$.\nThe semantic uncertainty is then estimated by the entropy between the semantic sets. First, we obtain $p(g_v | q_i)$, the probability of the model generating the answers in $g_v$ as in Equation 1.\n$P(g_v|x_i) = \\sum_{a_{i,k} \\in g_v} p(a_{i,k}|x_i) = \\sum_{a_{i,k} \\in g_v} \\prod_{h} p(a_i^h | x_i)$\nWith the probabilities of groups, we approximate $SE(x_i)$, the expectation of the semantic entropy of $x_i$, using Monte Carlo integration over the groups:\n$SE(x) = -V^{-1} \\sum_{v=1}^V log \\, p(g_v|x_i)$"}, {"title": "3.3 Coherent Persuasion Score", "content": "The degree of context-memory conflict can be quantified by measuring a context's efficacy in shifting the LM's answer. In previous work, persuasion scores were proposed to see to what degree context changes a LM's answer (Du et al., 2024). However, the existing persuasion score focuses on the first token of the single generated answer which has been regarded as insufficient to represent the whole generated sequence (Wang et al., 2024). There also exists the problem of uncertainty during the generation to solely rely on a single forward pass, which may also stem from syntactical uncertainties, which we are irrelevant for our use-case.\nTo overcome the LM's brittleness, we propose the novel Coherent Persuasion (CP) score. We incorporate a multiple sample approach and semantically group the samples to create distributions for comparison. Specifically, we adapt the generation and grouping process as in Kuhn et al. (2023) into the CP score.\nTo calculate the CP score of the $c_i$, we gather two lists of answers, $A_{q_i}$ and $A_{c_i \\& q_i}$, with two different inputs, which correspond to the question only input $[P;q_i]$, and the question with context input $[P; c_i; q_i]$. Then, we create the groups $G_{q_i} = [g_1, g_2, ..., g_r, ..., g_R]$ and $G_{c_i \\& q_i} = [g_1, g_2, ..., g_u, ..., g_U]$ from $A_{q_i}$ which has R number of generated answers and $A_{c_i \\& q_i}$ which has U number of generated answers, respectively. The CP score is obtained by averaging the distance from the probability distribution p of $G_{c_i \\& q_i}$ to p of $G_{q_i}$. In detail, for the probability distribution of $g_r$, denoted as $p_{g_r}$, is obtained as:\n$p_{g_r} = \\frac{1}{W} \\sum_{w=1}^W p_{a_w}$\nwhere the $g_r$ has W generated answers and $p_{a_w}$ is the averaged softmax probability distribution of all the tokens in the answer $a_w$. The $p_{g_u}$ is also calculated following Equation 3. Our final proposed CP score can be acquired by Equation 4.\n$CP(c_i) = \\frac{1}{R \\times U} | \\sum_{r=1}^{R} \\sum_{u=1}^{U} kl_div(p_{g_r}, p_{g_u})$\nAs the CP score measures the distance between the probability distributions of $A_{c_i \\& q_i}$ and $A_{q_i}$, it tells us how much the LM's probability distribution is swayed by the context, while ensuring that small changes in syntactic representation or mismatches from first-token probabilities are not included."}, {"title": "4 DYNAMICQA", "content": "To evaluate our methods proposed in \u00a73, we present a dataset of 11,378 question-answer pairs that features the dynamicity of facts (see Figure 1). The dynamicity of a fact is difficult to annotate, so we use naturally occurring proxy scores as a reflection of the aspect. The question and answer pairs, alongside their temporality and disputability scores, are sourced from Wikidata and Wikipedia, which has not previously been provided in related datasets. We approximate temporality via the number of edits (\u00a74.1) and disputability via the number of reversions (\u00a74.2). For each question and answer pair, we obtain answer-specific context snippets from Wikipedia, alongside their popularity scores. For static and temporal facts, we also create 'counter-memory' from these snippets using similar object replacements, to simulate realistic knowledge conflict scenarios."}, {"title": "4.1 Temporality", "content": "We define the temporality of a triplet via the number of edits on Wikidata for the given subject and relation as a proxy. We initialize our dataset from PopularQA (Mallen et al., 2023), which is a collection of 14k question-answer pairs sampled from Wikipedia, with a popularity score determined by the monthly Wikipedia page views of the subject and object of the triplets. This popularity score is often used as a proxy for the triplet's prevalence in the unsearchable pre-training corpora of models (Mallen et al., 2023; Fierro et al., 2024). Given the sampling method, this dataset has a long-tailed distribution, meaning most triplets have very low popularity and low temporality. We identify relevant snippets from the triplet's subject current Wikipedia page that mention the triplet's object as context to provide to the LM. To simplify replacement of the object for counter-memory context at test-time, we replace the object in context with a replacement token. We discard 2k pairs where we cannot find the intended object and relation mentioned on the subject's Wikipedia page. We identify relevant replacement entities using the most similar entity for the original object as identified using the Wembedder tool (\u00c5rup Nielsen, 2017). All facts with more than 1 edit are labeled as 'temporal' facts, though they vary in their degree of temporality (with a maximum score of 23 edits). This leaves us with 2495 questions. We randomly subsample 2500 of the remaining 'static' questions (triplets with 0 edits on Wikidata)."}, {"title": "4.2 Disputability", "content": "Disputable facts are facts that could be changed depending on the viewpoint. To construct the question and answer pairs containing disputability, we utilize the collection of articles from Wikipedia that are regarded as controversial. Controversy, by definition, is inherently disputable since it arises from the existence of multiple viewpoints, each supported by sufficient evidence. (Rad and Barbosa, 2012; Vuong et al., 2008). We are the first to convert the controversies that appear in the edit history into a QA dataset for an LM.\nTo identify the disputable facts from the articles, we look into reverted edit logs (Sumi et al., 2011). Among the edit logs {$e_0, e_{l-1}, e_l, e_{l+1}, ..., e_j$} on an article, two consecutive logs, $e_l$ and $e_{l+1}$ are selected as a pair of reverted edit logs if the texts of $e_{l-1}$ and $e_{l+1}$ are identical. Then, by measuring the edit distance between $e_i$ and $e_{l+1}$, we obtain t number of modification pairs {($m_{l,1}, m_{l+1,1}$), ($m_{l,s}, M_{l+1,s}$), ..., ($m_{l,t}, m_{l+1,t}$)}. The pair is selected as a potential disputable pair if replacing $m_{i,s}$ to $m_{i+1,s}$ in $e_i$ yields the same text with the $e_{l+1}$.\nSince a reverted edit does not necessarily imply disputability of the fact, we apply a few rules to filter out inappropriate modification pairs, to account for cases of vandalism, paraphrasing, or synonyms. For vandalism, we remove the pair if one of the users involved in that edit did not disclose their identity, for example, anonymous users or if the user ID is an IP address. For the modification pairs that are synonyms or paraphrasing, we feed the pairs to the semantic similarity model (\u00a7A.3) and filter the pairs whose similarity scores are bigger than 0.98. The selected pairs of reverted edit logs and modification pairs serve as contexts and answers in our dataset.\nQuestions are generated using a LM\u00b2(\u00a7A.2) by providing the context $e_l$ and the answer $m_{i,s}$ within the prompt. After obtaining the question and answer pair with the context, three annotators performed a manual annotation of the dataset to ensure"}, {"title": "5 Experiments", "content": "We use DYNAMICQA to assess the performance of three recent and similarly sized state-of-the-art LMs: Mistral-7B-Instruct-v0.1 (Mistral) (Jiang et al., 2023), Llama-2-7b-chat-hf (Llama-2) (Touvron et al., 2023), and Qwen2-7B-Instruct (Qwen2) (Qwen, 2024). To minimize the effect of the confounding factors, inferences are done in zero-shot manner, and, to obtain better generation results, instruction-tuned LMs are chosen. To obtain the model's parametric knowledge (Yi1), we first query the model for each $q_i$ without any additional context. We then query the model provided two forms of context: one is the unperturbed context $c_0$ and the other is the unseen replacement $c_c$. In the case of disputable instances, the choice of $c_0$ and $c_c$ is arbitrary, as both options are equally likely. For each query, we obtain the accuracy and semantic uncertainty (\u00a73.2). We calculate the CP score for each $q_i$ across all queries (\u00a73.3). We calculate the accuracy of each model on each partition as\n$acc = \\frac{\\sum_{i=1}^N RougeL(a_i, Y_i) > 0.3}{N}$\nwhere $a_i \\in {a_c, a_0}$, when context is provided, and is otherwise a. Given the inherent subjectivity of questions within the Disputable dataset, we do not calculate accuracy for that partition without context provided. For generating samples, we set K = 10 and calculate accuracy values using the most likely response as determined via greedy search."}, {"title": "5.2 Analysis of Semantic Uncertainty and Coherent Persuasion Scores", "content": "We assess our semantic uncertainty score in relation to the dynamicity of facts to determine if it is a reliable indicator of intra-memory conflict. Furthermore, we look at the interaction of our intra-memory conflict indicators (dynamicity scores, and semantic uncertainty) with the CP score to elucidate the interaction between our two investigated forms of knowledge conflict.\nWe identify two model behaviours of interest: Persuaded instances are instances where the model is persuaded by the provided context, meaning that $Y_{i1} \\neq a_i$ but $Y_{i2} = a_i$. Stubborn instances are instances where the model is impassive to the provided context, meaning $Y_{i1} = Y_{i2}$ and $Y_{i1} \\neq a_i$. We compare the obtained persuasion and uncertainty scores across the entire dataset and also highlight the behaviour of the persuaded and stubborn instances (\u00a76.1, \u00a76.2). Furthermore, we identify datapoints of interest. In the temporal dataset, we identify the top 100 most temporal datapoints (i.e. top-100 most edits), and the top 100 most popular datapoints (i.e. top-100 subject pageviews). We assess for differential distributions in uncertainty and persuasion for these remarkable datapoints (\u00a76.3, \u00a76.4) to assess for the relevance of temporality and popularity in retrieval-augmented generation and QA (Mallen et al., 2023).\nTo further investigate the influence of the various potential predictors of an instance's persuasion, we implement a linear regression model across all instances, where we take as independent variables the popularity of the subject and object, the number of edits to the fact, and the semantic uncertainties before and after context is provided (\u00a76.5). We standardise each parameter with a z-score transformation for interpretable comparison. The provided $\u03b2$ values reflect the magnitude of each predictor's effect on the dependent variable, the persuasion score, and the p-value denotes the statistical significance of the effect."}, {"title": "6 Results & Discussion", "content": "Table 2 shows the performance of our three investigated models across the three partitions of our dataset with and without additional context. Table 1 also shows the percentage of stubborn and persuaded instances. We see that accuracy is typically greatest with Llama-2 across all partitions. We also see that Llama-2 has the greatest degree of persuasion (in both persuasion score and % persuaded) across all partitions. However, Llama-2 shows the greatest uncertainty across all instances. Typically, uncertainty decreases with context; however, Mistral and Qwen2 show a slight increase in uncertainty for the Static partition of the dataset. Furthermore, we typically see greatest accuracy and uncertainty on the Static dataset, though the difference in uncertainty between the Static and Temporal partitions are not stark.\nThese results are somewhat similar to contemporary work (Fierro et al., 2024), who also find the greatest accuracy with Llama-2; however, they also find the highest confidence with this model, though they presumably use the output of softmax layer which is more vulnerable to syntactic uncertainties (Kuhn et al., 2023). Furthermore, they find reduced update accuracy for Llama-2 relative to other models across mutable and immutable facts. In contrast, we surprisingly find that manipulations to static, immutable facts are most easily accepted by the model in context. In contrast, the temporal and disputable facts, which are expected to have greater variability in the model's training dataset, and thus higher intra-memory conflict, have a greater proportion of stubborn instances across all models. We expect this stems from our differences in dataset collection, as Fierro et al. (2024) determine mutability via relationships, whereas we verify the 'mutability' of our facts with our proxy scores. Ultimately, our results raise concerns for the efficacy of retrieval-augmented text generation, as it appears that the most commonly adapted facts are those that are the most difficult to update with context."}, {"title": "6.2 Model-Level Differences", "content": "In Figure 2, we show model-level differences in behaviour with provided context. We typically see a decrease in uncertainty given context. In the case of Mistral, stubborn instances have the greatest decrease in uncertainty, whereas persuaded instances have a minimal change. Llama, in contrast, has the smallest decrease in uncertainty for stubborn instances. Qwen2 has minimal change in uncertainty for both persuaded and stubborn instances. We further investigate differences between persuaded and stubborn instances in the next section. For the main paper, we present results for Llama, due to its high performance, on the temporal partition of the dataset, as this partition allows for easy comparison of the effect of dynamicity on model performance. We provide all the additional graphs for the remaining models and partitions in Appendix B."}, {"title": "6.3 Interaction of Persuasion and Uncertainty", "content": "We analyse the interaction between the CP score and uncertainty scores in Figure 3, where we highlight the persuaded and stubborn instances. Though Llama, in general, shows relatively high persuasion and decreased uncertainty, we do not see a particular correlation between the values (r(2494) = 0.003, p < .05): meaning, instances with high initial uncertainty are not necessarily more likely to be persuaded to match the given context. Furthermore, persuaded instances do not show particularly decreased uncertainty given context. The distribution of persuaded instances and generic instances (in blue) are identical with and without context. Therefore, output uncertainty makes it difficult to determine if retrieval will augment model performance. We do see that 'stubborn' instances show reduced uncertainty in the context-less scenario, which remains stable despite the presentation of context.\nFurthermore, we see an interesting behaviour given the introduction of context: While the majority of instances given context, for Llama, decrease in uncertainty, there is a small cluster of datapoints that do not show this behaviour. This dichotomy of behaviour is also seen in other models (See Appendix B), even though the direction of uncertainty change differs (as well as number of clusters). The cause for this behaviour is unclear, though we do investigate other potential factors (like temporality, popularity and keywords) in Appendix C, where we do not find a definitive pattern."}, {"title": "6.4 Interaction of Temporality and Uncertainty", "content": "In Figure 4, we show the distribution of highly popular and temporal instances between persuaded and stubborn instances. We can see that, though highly popular values typically show lower semantic uncertainty without context, they are not overly represented among the stubborn instances. Highly temporal values, on the other hand, typically show greater semantic uncertainty without context. We also do not see a particularly large decrease in the semantic uncertainty of highly temporal facts. Therefore, the semantic uncertainty measure does not seem to be an accurate indicator of intra-memory conflict, or aleatoric uncertainty, as previously used in other work (Gao et al., 2024). This suggests that other factors may contribute to semantic uncertainty, outside of exclusively intra-memory conflict."}, {"title": "6.5 Predictors of Persuasion", "content": "In our previous investigations, we do not see that the semantic uncertainty of a contextless question is a meaningful indicator of model persuasion. Furthermore, dataset partitions with anticipated increase in intra-memory conflict (i.e. temporal and disputable data) show lower persuasion than static facts. Therefore, we assess the strength of each relation to our CP score. We show the results of our linear regression test in Table 3. Though we do not find a very strong fit to the data, given its volatility (R2 = 0.185), we do find several significant predictors. The strongest predictor is the number of edits of a fact, where it shows an inverse relationship to a fact's persuasion score. This aligns with the behaviour we have seen in our comparison of temporal and static facts in Table 1.Furthermore, while object (Opop) popularitysignificantly affects a fact's persuasion score (previously shown by Du et al. (2024)) the magnitude of this effect is smaller than that of the number of edits of a fact or the semantic uncertainty of the contextless question (SEq). Furthermore, the uncertainty of the contextless question alone is insufficient to predict a fact's persuasion score, as determined by our correlation analysis in \u00a76.3. In contrast, we see a significant correlation between the number of edits of a fact and the CP score (r(2944) = -0.27, p = 1.6e \u2013 42).\nThese findings typically hold across other models (see Appendix B), yet we find that the direction of the effect of SEq varies across models. Taken together, we can see that fact dynamicity, or intra-memory conflict, plays a bigger role in knowledge conflicts than fact popularity (Mallen et al., 2023; Du et al., 2024). We also find an inverse effect of intra-memory conflict on the model's susceptibility to persuasion for a given instance. Facts that change regularly are less likely to be updated with context-retrieval, yet facts that never change are easily persuaded."}, {"title": "7 Conclusion", "content": "We investigate the interaction between intra-memory conflict in context-memory conflicts, using multiple natural causes of intra-memory conflict (i.e. fact \u2018dynamicity'), for the first time. While we approximate intra-memory conflict with number of fact edits and a semantic uncertainty score, we find that semantic uncertainty is a poor indicator of intra-memory conflict. For the context-memory conflicts, we propose the Coherent Persuasion score to measure the persuasiveness of the given context with consideration of the entire semantic output. Furthermore, we release DYNAMICQA, a QA dataset with realistic dynamic and static questions, (i.e. facts that naturally vary and those that do not) alongside additional context and realistic replacements for use in intra-memory and counter-memory conflict studies. We then evaluate three state-of-the-art LMs on their performance on static, temporal and disputable facts. We find that, surprisingly, static facts are the most easily updated with additional context, in comparison to temporal and disputable facts. Furthermore, our extensive analysis show that the number of unique presentations of the fact on Wikidata and Wikipedia (i.e. number of edits) has an inverse correlation with a model's propensity to adapt that fact. We also find large model-level differences: Llama-2 is most easily persuaded, yet also the highest uncertainty. Over a variety of analyses, we find that uncertainty alone does not consistently indicate persuasion, requiring other approaches to update model knowledge than retrieval-augmentation in low-certainty domains."}, {"title": "Limitations", "content": "In our paper, we do not investigate the impact of model size on knowledge conflicts, as we were computationally limited to smaller models of a size of 7B parameters. We examine several different model architectures within the model size considered. We hypothesise that the effects observed might be less pronounced for larger models. We did not include models smaller than 7B, as the ones we tested showed poor performance on the QA task in initial experiments. Other 7B parameter models we investigated showed very low accuracy, even with provided context, which is why we chose to exclude them (Falcon-7B, Gemma-7B). \nWe have an imbalanced distribution of partitions in our dataset, owing to the difficulty of identifying disputable questions. We did our best to ensure our disputable dataset includes limited misinformation, though there is the possibility that some few instances made it through, given the difficulty of the task (also reflected in the relatively low annotator agreement). We also only use one possible measure of uncertainty. There have been a swarth of recent uncertainty and semantic consistency measures introduced for text generation , out of which we chose the most popular one."}, {"title": "A Additional Methodology Details", "content": null}, {"title": "A.1 Annotation", "content": "Given the source of the dataset and the potential for vandalism in the dataset", "guidelines": "For cases of multiple questions for the same piece of context", "options": "n1. Accept. The question is related to the context. The two potential objects are possible answers to the question. Neither of the potential objects contain vandalism or obvious misinformation.\n2. Change. If both questions are incorrect or insufficient, the first question is edited. It must be rewritten so that most objects are acceptable answers to the question based on the context. If the question cannot be rewritten in a meaningful way, it should be discarded (i.e. if both words are adjectives). To ensure the text is specific enough, most extra information might be added. The annotator should also remove all \"according to the text\"s as this would not work well without context.\n3. Discard. If there is no dispute for a question, it should be discarded. This could be if the two objects are two names for the same person (e.g. Kanye, Ye), two different spellings for the same word, or two synonyms (publications, writings). One exception is hypernyms (e.g. Danish, Scandinavian). We discard questions with insufficient context (e.g. 'later jesus t.'). We remove fixed typos and vandalism and clear misinformation (e.g. the objects 'china' and 'ukraine' to"}]}