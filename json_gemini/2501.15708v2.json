{"title": "StalCC: Standardized Evaluation for Classification Task in In-context Learning", "authors": ["Hakaze Cho", "Naoya Inoue"], "abstract": "Classification tasks are widely investigated in the In-Context Learning (ICL) paradigm. However, current efforts are evaluated on disjoint benchmarks and settings, while their performances are significantly influenced by some trivial variables, such as prompt templates, data sampling, instructions, etc., which leads to significant inconsistencies in the results reported across various literature, preventing fair comparison or meta-analysis across different papers. Therefore, this paper proposes a standardized and easy-to-use evaluation toolkit (STAICC) for in-context classification. Including, for the normal classification task, we provide STAICC-NORMAL, selecting 10 widely used datasets, and generating prompts with a fixed form, to mitigate the variance among the experiment implementations. To enrich the usage of our benchmark, we also provide a sub-benchmark STAICC-DIAG for diagnosing ICL from several aspects, aiming for a more robust inference processing.", "sections": [{"title": "1 Introduction", "content": "In-Context Learning (ICL) (Dong et al., 2022) is an emerging forward-calculation-only few-shot paradigm using Language Models (LMs) without parameter updating. ICL paradigm can be used on various task forms (Min et al., 2022b;c), where classifications with finite and fixed label space are concise and critical tasks for calibrating LMs for ICL (Zhao et al., 2021; Han et al., 2023b; Jiang et al., 2023; Fei et al., 2023; Zhou et al., 2024; Cho et al., 2024), aligning LMs towards ICL objective (Wei et al., 2022; Min et al., 2022b; Wei et al., 2023a; Iyer et al., 2022), and investigating the inner principle of ICL (Min et al., 2022c; Yoo et al., 2022; Dai et al., 2023; Han et al., 2023a; Wang et al., 2023; Kossen et al., 2024; Cho et al., 2025). However, previous works use disjoint benchmarks to evaluate the ICL performance on classification tasks (Table 1), and performance even on the same model and dataset varies (Fig. 1) due to non-essential factors, such as the prompt template, data sampling, demonstration order, etc. Due to such an inconsistency, in current ICL research, scholars usually need to repeat the baseline experiments in a consistent setting to compare the results with previous works, even if these experiments are quite costly. This greatly increases the difficulty of comparing or meta-analyzing the results in the literature, causing a limited understanding of how ICL performance varies across models and additional algorithms for improving ICL. To this end, in this paper, we propose STAICC, a standardized and easy-to-use toolkit for evaluating the classification performance of ICL. Concretely, we select 10 widely used datasets on single-sentence classification tasks to unify the usage of datasets and fix the prompt template, data sampling, and demonstration orders to generate stable-over-trial test inputs, to eliminate the inconsistency caused by these experimental settings. Moreover, to enrich the usage of STAICC, we propose STAICC-DIAG, for diagnosis data consisting of prediction bias, prompt sensitivity, label-noise robustness, etc., to help for a closer look and improvement of ICL inference methods. Based on STAICC, we extensively measure the ICL capability on 29 modern LMs, and observe clear scaling laws of ICL classification performance against the model parameter numbers, confirming the robustness of"}, {"title": "1.1 Related Works", "content": "The models and datasets mentioned in this paper are cited in Appendix A. In-context learning. Discovered by Radford et al. (2019), in-context learning is an emerging few-shot learning paradigm using only feed-forward calculation on LMs. Previous researches are focused on improv- ing ICL performance by calibrating or recalculating the predicted likelihood (Zhao et al., 2021; Han et al., 2023b; Jiang et al., 2023; Fei et al., 2023; Zhou et al., 2024; Cho et al., 2024; Xu et al., 2023; Abbas et al., 2024; Min et al., 2022a), aligning LMs towards ICL objective (Wei et al., 2023a; Min et al., 2022b; Wei et al., 2022; Iyer et al., 2022; Brunet et al., 2023), finding better demonstration examples (Liu et al., 2022; Mavromatis et al., 2023; Gonen et al., 2023; Qin et al., 2023; Van et al., 2024), and applying better demonstration orders (Lu et al., 2022; Liu et al., 2024). Also, some works try to find the principle of ICL theroically (Olsson et al., 2022; Wies et al., 2024; Huang et al., 2023; Collins et al., 2024; Jeon et al., 2024) and empirically (Min et al., 2022c; Yoo et al., 2022; Dai et al., 2023; Han et al., 2023a; Wang et al., 2023; Kossen et al., 2024; Pan et al., 2023; Li et al., 2024; Cho et al., 2025). These works typically require bench- marks to validate their algorithms' effectiveness or measure ICL-related data. However, we will show that the selection and usage of these benchmarks are inconsistent, making it difficult to compare and summarize their results across the literature. Currently used ICL benchmarks. Currently used structured benchmarks for ICL include: BIG- Bench (Srivastava et al., 2023) and BIG-Bench Hard (Suzgun et al., 2023) for large-scaled and complex tasks, and OPT-IML (Iyer et al., 2022) for out-of-distribution tasks. However, for general classification"}, {"title": "2 Preparation", "content": "2.1 Task Definition: In-context Classification Typical ICL receives structured input-label pair prompt made from a few-shot demonstration set Dk = [(X1,Y1), (X2,Y2), ..., (Xk, Yk)] and a query xq by a template T. Fed with the formed input T(Dk, xq), the causal language model produces a probability distribution of the subsequent token, and in In-context Classification (ICC) task with a finite and certain label token set Y depending on the verbalizer in the prompt template\u00b9, we usually select the probabilities of the tokens (of amount |Y|) presented in the label token set as the output label probabilities. Also, some calibrations Care used to re-scale the output label probabilities. That is, the output of ICC is:\n\\begin{equation}\no(x_q) = C \\left[\\text{TokenSelect} \\left(\\Theta(T(D_k, x_q))\\right)\\right],\\end{equation}\nwhich is an |Y|-dimensional probability vector. Obviously, in ICC experiments, we should consider and control all the variables to get comparable results. The C, O are often well-controlled since they are deemed remarkable while the T(\u00b7,\u00b7) and Dk are often overlooked even with an over-expected impact as investigated follows."}, {"title": "2.2 Motivation: Investigating the Inconsistency in ICC Experiments", "content": "This section claims our proposition: the inconsistency in experimen- tal settings leading to inconsistent results hinders comparison and meta-analysis across the literature. We analyze how the inconsis- tency happens so that we can propose principles for our benchmark to reduce these inconsistencies. The disjoint in dataset selection. To find how the dataset usage and experiment implementations are disjoint, we investigated 29 pa- pers that conducted experiments on ICC. We summarize the usage of the most-used 13 datasets and their metrics in Table 1, where we find that even if some datasets are favored, the datasets used in ICC- related papers are severely inconsistent from an overall perspective. Also, the metrics in these papers are inconsistent.\n1 Notice that typically each label corresponds to one token."}, {"title": "3 StalCC", "content": "In this section, we propose a Standardized Evaluation Toolkit for In-context Classification (STAICC) to address the disjointing problems mentioned before. Based on the above discussion, we advocate controlling variables in ICL practice, specifically determining a universal testing script with unified datasets and stable- over-trial inputs, to stably test ICL abilities in a consistent condition."}, {"title": "3.1 Methdology", "content": "General principles. Given the discussion before, we propose an evaluation toolkit STAICC with 10 chosen datasets (listed in Appendix A). For each dataset, we apply a fixed prompt template, data sampling, and demonstration order as the default setting to produce inputs for testing. That is, we ensure that give the k fixed, the inputs for the test are invariable among different tries. To support the efforts on improving some of the aspects of the ICC process shown in Eq. 1, we open all the interfaces to modify all the conditions (including the remarkable ones and the trivial ones, e.g., the prompt template) arbitrarily while the other aspects are recommended to be kept as default. In addition, we try our best to maximize usability and simplicity, so that users only need to call the interface to modify their input forming or reload a forward propagation function (or use our template) to quickly validate their method. Controling variables in assembling ICL inputs. In detail, as shown in Fig. 3, to build a variable- controlled set of ICL-styled inputs, given each chosen raw supervised classification dataset (Appendix A), we process them as described following: (1) filter some data out from these datasets (typically some over-"}, {"title": "3.2 StalCC-Normal: Basic Standardized Benchmark for ICC", "content": "As the main objective of this paper, to test the basic prediction capacity of an ICC inference o(xg), we build STAICC-NORMAL with standard input form and typical metrics for classification tasks.\nMetric. We apply 4 metrics for this sub-benchmark: (1) Accuracy, (2) True label probability (TLP), (3) Macro F1, (4) Expected calibration error-1 (ECE-1) (Guo et al., 2017; Naeini et al., 2015), with various linearity (Schaeffer et al., 2024) and application focus. For each test sample xi in the test set {x}-1, given the ground-truth label yi and the prediction output o (xi), the metrics can be formulated as below:\n\\begin{equation}\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1} [\\text{argmax}_Y o(x_i) = y_i], \\qquad \\text{TLP} = \\frac{1}{N}\\sum_{i=1}^N o(x_i)_{y_i}\\end{equation}\n\\begin{equation}\\text{Macro F1} = \\frac{1}{|Y|}\\sum_{j=1}^{|Y|} \\frac{2 \\cdot \\text{Precision}_j \\cdot \\text{Recall}_j}{\\text{Precision}_j + \\text{Recall}_j}; \\qquad \\text{Precision}_j = \\frac{TP_j}{TP_j + FP_j}; \\qquad \\text{Recall}_j = \\frac{TP_j}{TP_j + FN_j};\\end{equation}\n\\begin{equation}\\text{ECE1} = \\frac{1}{B}\\sum_{b=1}^{B} |\\text{acc}(B_b) - \\frac{1}{|B_b|}\\sum_{i \\in B_b} \\max o(x_i)_{y_i}|; \\text{ acc}(B_b) = \\frac{1}{|B_b|} \\sum_{i \\in B_b} \\mathbb{1} [\\text{argmax}_Y o(x_i) = y_i]\\end{equation}\nwhere TPj, FPj, and FNj are the true positive, false positive, and false negative counts for class j, respec- tively, Bt is the set of prediction samples in the b-th confidence bin within a total B bins (set to 10 in"}, {"title": "3.3 StalCC-Diag: Diagnostic Evaluation for ICC", "content": "To enrich the usage of STAICC, we propose STAICC-DIAG for a set of fine-grained diagnostic analyses, focusing on two key aspects: (1) prediction bias, which identifies tendencies of LMs toward specific label tokens, and (2) prediction robustness, which evaluates the robustness of LMs against subtle variations in ICL inputs. STAICC-DIAG is composed of several sub-tasks, each for an aspect of the diagnostic, and utilizes various but stable-over-trial input forms different from the STAICC, to be introduced in detail as follows. Prediction bias evaluation. We propose evaluations for 3 types of prediction bias: (1) Contextual bias (Zhao et al., 2021). Given input with a meanless pseudo query \u017c (in the calculation of contextual bias, an empty query is used as this \u017c, as shown in Fig. 6), an ideal non-bias inference is expected to produce neutral prediction with averaged probabilities2, producing the maximum entropy on the prediction distribution. Therefore, we use an entropy-related metric to evaluate the bias magnitude:\n\\begin{equation}\\text{Contextual Bias} = \\frac{1}{N}\\sum_{i=1}^{N} H \\left[o(\\dot{x}_i)\\right],\\end{equation}\nwhere the N is the input instance number same as the STAICC-NORMAL\u00b3, and H[.] is the entropy calculation. Notice that we use a minus sign to ensure that a larger value corresponds to a stronger bias. (2) Domainal bias (Fei et al., 2023). Following the same principle as the contextual bias, the sampling method for i is changed to randomly sample a sequence of tokens under the frequency distribution of the xs from the calibration set to calculate the domainal bias. (3) Empirical bias. Contextual bias and domainal bias offer observations on the prior or \"nature\" output tendency without the influence of query. However, pseudo queries in the inputs of the aforementioned 2 bias calculations can cause a distribution gap between the bias evaluation and practical scenario, potentially harming their applicability. So, different from the above 2 bias evaluations, empirical bias utilizes real test inputs same with STAICC-NORMAL, and collects the averaged output probability distributions to calculate KL divergence with the ground-truth label frequency counted from the test set:\n\\begin{equation}\\text{Empirical Bias} = D_{KL} \\left(\\frac{1}{N}\\sum_{i=1}^{N} o(x_i) , P_{true} \\right), \\quad P_{true,j} = \\frac{\\text{count}(y=j)}{N}\\end{equation}\nPrediction robustness evaluation. We propose evaluations for 3 types of prediction robustness: (1) Prompt template robustness. We construct 9 prompt templates under the meta-template shown in Fig. 3 (detailed in Appendix B.1) and repeat the standard inference process as STAICC-NORMAL once for\n2 e.g.,[,,] for a 3-way scenario. 3In the implementation, we replaced the test set with pseudo samples, keeping the input forming processing as before."}, {"title": "4 Discussion", "content": "Summary. Motivated by the disjoint in the literature on the evaluation of in-context learning, this paper proposes a standardized evaluation toolkit STAICC for classification task under the in-context learning paradigm and conducts extensive measurement on various LMs. Rethinking the influence of bias and robust- ness of ICL. We visualize the statistical covariate among the metrics evaluated in this paper, shown in Fig. 8. We highlight some non-trivial covariation or dis-covariation, as: (1) Weak correlation between accuracy/F1 and contextual/domainal bias suggests a limited influence of prediction bias to ICL infer- ence, aligning with the results in Fig. 5, where cali- bration methods are with relatively weaker improve- ment on the accuracy, while the correlations from ECE-1 are relatively stronger, suggesting that the benefit of mitigating such bias is towards a more faithful prediction, rather a more accurate one. (2) Template robustness tends to be positively corre- lated with ECE-1, and sampling robustness tends to be positively correlated with accuracy, indicating the benefits of improving the prediction robustness, (3) A larger GLER (lower robustness) seems to im- prove accuracy, but given the confounding variable of model size, such a positive covariation can not be regarded as a reliable causal relationship, where fur- ther causal inference can be beneficial for a deeper insight. Position of this paper. The problem addressed in this paper is significant and holds substantial value for future works in the ICL area, while the authors believe the novelty of the works in this paper will not impress readers a lot, with just \"dirty works\" towards an easy-to-use toolkit. Given these considerations, we have chosen to release this work as a scientific report rather than submit it to a peer-reviewed conference. Limitation. (1) This paper and STAICC mainly take classification tasks in the scope, while other tasks without a fixed and limited output space can still benefit from ICL, raising a need for evaluation of ICL models and methods on these tasks. (2) Also, single-sentence classification tasks are also limited, as tasks with more complex input forms, such as what is defined in RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) can be non-trivial extensions of the current tasks, and may be considered in the future construction of the benchmark."}]}