{"title": "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "authors": ["Chang Tian", "Matthew B. Blaschko", "Wenpeng Yin", "Mingzhe Xing", "Yinliang Yue", "Marie-Francine Moens"], "abstract": "Fine-grained category discovery using only coarse-grained supervision is a cost-effective yet challenging task. Previous training methods focus on aligning query samples with positive samples and distancing them from negatives. They often neglect intra-category and inter-category semantic similarities of fine-grained categories when navigating sample distributions in the embedding space. Furthermore, some evaluation techniques that rely on pre-collected test samples are inadequate for real-time applications. To address these shortcomings, we introduce a method that successfully detects fine-grained clusters of semantically similar texts guided by a novel objective function. The method uses semantic similarities in a logarithmic space to guide sample distributions in the Euclidean space and to form distinct clusters that represent fine-grained categories. We also propose a centroid inference mechanism to support real-time applications. The efficacy of the method is both theoretically justified and empirically confirmed on three benchmark tasks. The proposed objective function is integrated in multiple contrastive learning based neural models. Its results surpass existing state-of-the-art approaches in terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of the detected fine-grained categories. Code and data will be available at https://github.com/XX upon publication.", "sections": [{"title": "1 Introduction", "content": "Fine-grained analysis has drawn much attention in many artificial intelligence fields, e.g., Computer Vision (Chen et al., 2018; Wang et al., 2024a; Park and Ryu, 2024) and Natural Language Processing (Ma et al., 2023; Tian et al., 2024; An et al., 2024), because it can provide more detailed features than coarse-grained data. For instance, as illustrated in Figure 1, solely based on coarse-grained analysis, the chatbot might incorrectly recommend a roadster, which is unsuitable for field adventures. Detecting the fine-grained intent would allow the chatbot to recommend an off-road vehicle that aligns with the user's requirements. However, annotating fine-grained categories can be labor-intensive, as it demands precise expert knowledge specific to each domain and involved dataset. Addressing this challenge, An et al. (2022) recently introduced Fine-grained Category Discovery under Coarse-grained Supervision (FCDC) for language classification tasks (details in Section 3). FCDC aims to reduce annotation costs by leveraging the relative ease of obtaining coarse-grained annotations, without requiring fine-grained supervisory information. This approach has sparked significant research interest in the automatic discovery of fine-grained language categories. (Ma et al., 2023; An et al., 2023a; Vaze et al., 2024; Lian et al., 2024).\n\nExisting methods for addressing FCDC are typically grouped into three groups (An et al., 2024): language models, self-training methods, and contrastive learning methods. Language models (Devlin et al., 2019a; Touvron et al., 2023), including their fine-tuned versions with coarse labels, gener-"}, {"title": "2 Related Work", "content": "2.1 Fine-grained Category Discovery\n\nFine-grained data analysis is crucial in Natural Language Processing (Guo et al., 2021; Ma et al., 2023; Tian et al., 2024) and Computer Vision (Pan et al., 2023; Wang et al., 2024b). However, effectively discovering fine-grained categories from coarse-grained ones remains challenging (Mekala et al., 2021). Traditional category discovery methods often assume that known and discovered categories are at the same granularity level (An et al., 2023b; Vaze et al., 2024).\n\nTo discover fine-grained categories under the supervision of coarse-grained categories, (An et al., 2022) introduced the FCDC task. Self-training approaches, such as Deep Cluster (Caron et al., 2018; An et al., 2023a), use clustering algorithms to detect the fine-grained categories, assign pseudo labels to the clusters and their samples, and then train"}, {"title": "2.2 Neighborhood Contrastive Learning", "content": "Contrastive learning enhances representation learning by bringing the query sample closer to positive samples and distancing it from negative samples (Chen et al., 2020). Prior research has focused on constructing high-quality positive pairs. (He et al., 2020) utilized two different transformations of the same input as query and positive sample, respectively. Li et al. (2020) introduced the use of prototypes, derived through clustering, as positive instances. Additionally, An et al. (2022) employed shallow-layer features from BERT as positive samples and introduced a weighted contrastive loss. This approach primarily differentiates data at a coarse-grained level, and the manually set weights limit its broader applicability.\n\nTo circumvent complex data augmentation, neighborhood contrastive learning (NCL) was developed, treating the nearest neighbors of queries as positive samples (Dwibedi et al., 2021). Zhong et al. (2021) extended this by utilizing k-nearest neighbors to identify hard negative samples, while Zhang et al. (2022) selected a positive key from the k-nearest neighbors for contrastive representation learning. However, these approaches often deal with noisy nearest neighbors that include false-positive samples. (An et al., 2023a) addressed this by proposing three constraints to filter out uncertain neighbors, yet they overlooked semantic similarities between query sample and each available sample. An et al. (2024) represented semantic sim-"}, {"title": "3 Problem Formulation", "content": "Given a set of coarse-grained categories $Y_{coarse} = {\u0421_1, \u0421_2,..., C_M}$ and a coarsely labeled training set $D_{train} = {(x_i, C_i) | C_i \u2208 Y_{coarse}} _{i=1}^N$, where N denotes the number of training samples, the task of FCDC involves developing a feature encoder $F_\u0398$. This encoder maps samples into a feature space, further segmenting them into distinct fine-grained categories $Y_{fine} = {F_1, F_2, ..., F_K}$, without any fine-grained supervisory information. Here, $Y_{fine}$ represents sub-classes of $Y_{coarse}$. Model effectiveness is evaluated on a testing set $D_{test} = {(x_i, Y_i) | Y_i \u2208 Y_{fine}}_{i=1}^L$, with L as the number of test samples, utilizing features extracted by $F_\u0398$. For evaluation consistency and fairness, only the number of fine-grained categories K is used, aligning with methodologies established in previous research (Ma et al., 2023; An et al., 2022, 2023a)."}, {"title": "4 Method", "content": "STAR leverages comprehensive semantic similarities and integrates seamlessly with contrastive learning baselines by modifying the objective function. We have developed variants for three baselines: PseudoPrototypicalNet (PPNet) (Boney and Ilin, 2017; Ji et al., 2020), DNA (An et al., 2023a), and DOWN (An et al., 2024). This section focuses on STAR-DOWN because DOWN outperforms other baselines, with additional method variants detailed in Appendix\n\nDOWN involves three steps: pre-training with coarse-grained labels (Section 4.1), retrieving and weighting nearest neighbors (Section 4.2), and training with a contrastive loss. STAR-DOWN follows the same first two steps but replaces the third with a novel objective function (Section 4.3). Like DOWN, STAR-DOWN iterates the last two steps until the unsupervised metric, the silhouette score of the clustering into fine-grained clusters, does not improve for five consecutive epochs. The detailed algorithm is provided in Appendix"}, {"title": "4.1 Multi-task Pre-training", "content": "As illustrated in Figure 3, the baseline DOWN (An et al., 2024) utilizes the BERT Encoder $F_\u0398$ to ex-"}, {"title": "4.2 Neighbors Retrieval and Weighting", "content": "In Figure 3, the Momentum Encoder $F_{\u0398_k}$ with parameters $\u0398_k$ extracts and stores gradient-free normalized neighbor features $h_i = F_{\u0398_k}(x_i)$ in a dynamic data queue Q. To ensure consistency between the outputs of $F_{\u0398_k}$ and $F_\u0398$, $F_{\u0398_k}$'s parameters are updated via a moving-average method (He et al., 2020): $\u0398_k \u2190 m \u0398_k + (1 \u2212 m) \u0398$, where m is the momentum coefficient. For each query feature $q_i$, in order to facilitate semantic similarity capture and fine-grained clustering, its top-k nearest neighbors $N_i$ are determined from Q using cosine similarity (Sim): $N_i = {h_j | h_j \u2208 argtopK_{h\u2208Q}(Sim(q_i, h_j))}$, where $Sim(q_i, h_j) = \\frac{q_i h_j}{||q_i|| \\cdot ||h_j||}$ is the cosine similarity function.\n\nTo counteract potential false positives in $N_i$, DOWN utilizes a soft weighting mechanism based on neighbor rank to balance information utility against noise, with weights $w_j$ of neighbor $h_j$ calculated as: $w_j = \u03c6 \u00b7 \u03b1^{\\frac{-l_{ij}}{k}}$, where \u03c6 is a normalizing constant for weights, \u03b1 serves as the exponential base, k is the retrieved neighbor count, and $l_{ij}$ denotes the rank of $h_j$ as a neighbor to $q_i$."}, {"title": "4.3 Training", "content": "4.3.1 Objective Function\n\nGiven a training batch $N_{train} \u2208 D_{train}$, where $Y_C$ is the set of coarse-grained labels of $N_{train}$, DOWN trains the model using the loss:\n\n$L_{train} = L_{ce} + L_{DOWN},$\n\n$L_{DOWN} = \\frac{1}{N_{train}} \\sum_{q_i \u2208N_{train}} L_i,$"}, {"title": "4.3.2 Loss Analysis", "content": "Since STAR-DOWN discovers fine-grained categories in the Euclidean space, we analyze the second term $L_2$ of the loss $L_2$, which optimizes sample distributions in the Euclidean space:\n\n$L_{2-2} = - \\sum_{h_j\u2208N_i} w_j \\cdot log \\frac{exp(q_i h_j /\u03c4)}{\\sum_{h_k\u2208Q} B^{d_{KL}(q_i, h_k)} exp(q_i h_k /\u03c4)}$\n$= \\sum_{h_j\u2208N_i} w_j \\cdot (log B^{d_{KL}(q_i, h_k)} exp(q_i h_k /\u03c4) - (q_i h_j)/\u03c4).$"}, {"title": "4.4 Inference", "content": "Previous methods (An et al., 2023a, 2024) use clustering inference on sample embeddings from $F_\u0398$ extracted from $D_{test}$, which is unsuitable for real-time tasks, such as intent detection, which require immediate response and can not wait to collect enough test samples for clustering. We introduce an alternative, centroid inference, suitable for both real-time and other contexts. Using $F_\u0398$, we derive sample embeddings from $D_{train}$ and assign fine-grained pseudo labels through clustering. For each fine-grained cluster, only the embeddings of samples from the predominant coarse-grained category (the category with the most samples in this fine-grained cluster) are averaged to form centroid representations. These approximated centroids are used to determine the fine-grained category of each test sample based on cosine similarity. A visual explanation is in Appendix"}, {"title": "5 Experiments", "content": "5.1 Experimental Settings\n\n5.1.1 Datasets\n\nWe conduct experiments on three benchmark datasets: CLINC (Larson et al., 2019), WOS (Kowsari et al., 2017), and HWU64 (Liu et al., 2021). CLINC is an intent detection dataset spanning multiple domains. WOS is used for paper abstract classification, and HWU64 is designed for assistant query classification. Dataset statistics are provided in Table 1."}, {"title": "5.1.2 Baselines for Comparison", "content": "We compare our methods against the following baselines. Language models: BERT (Devlin et al., 2019b), BERT with coarse-grained fine-tuning, Llama2 (Touvron et al., 2023), Llama2 with coarse-grained fine-tuning and GPT4 (Achiam"}, {"title": "5.1.3 Evaluation Metrics", "content": "To evaluate the quality of the discovered fine-grained clusters, we use the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) and Normalized Mutual Information (NMI) (Lancichinetti et al., 2009). For assessing classification performance, we use clustering Accuracy (ACC) (kuh, 1955; An et al., 2023a). Detailed descriptions of these metrics are provided in Appendix"}, {"title": "5.1.4 Implementation Details", "content": "To ensure fair comparisons with baselines, we use the BERT-base-uncased model as the backbone for all STAR method variants. We adhere to the hyperparameters used by the integrated baselines to demonstrate the effectiveness of our STAR method. The learning rate for both pre-training and training is 5e-5, using the AdamW optimizer with a 0.01 weight decay and 1.0 gradient clipping. The momentum coefficient m is set to 0.99. The batch size for pre-training, training, and testing is 64. The temperature \u03c4 is set to 0.07. The number of neighbors k is set to {120, 120, 250} for the CLINC, HWU64, and WOS datasets, respectively. Epochs for pretraining and training are set to 100 and 20, respectively. Further details are provided in Appendix"}, {"title": "5.1.5 Research Questions", "content": "The following research questions (RQs) are investigated: 1. What is the impact of STAR method on FCDC tasks? 2. What are the effects of the proposed real-time centroid inference compared to traditional clustering inference? 3. How does"}, {"title": "5.2 Result Analysis (RQ1)", "content": "As shown in Table 2, STAR method variants outperform SOTA methods across all datasets and metrics, validating the effectiveness of the STAR method in FCDC tasks. Language models like BERT, Llama2 and GPT4 (Devlin et al., 2019b; Touvron et al., 2023; Achiam et al., 2023) (GPT4 prompt in Appendix ) perform poorly on the FCDC task due to the lack of fine-grained supervisory information. Self-training methods like DC, DAC, and PP-Net (Caron et al., 2018; Zhang et al., 2021; Ji et al., 2020) also struggle because they rely on noisy fine-grained pseudo-labels and overlook comprehensive semantic similarities (CSS). Contrastive learning methods such as SNCL (Chongjian et al., 2022) and WSCL (An et al., 2022) perform better by leveraging positive pairs. DNA (An et al., 2023a) and DOWN (An et al., 2024) further enhance feature quality by filtering false positives and weighting them by rank. However, these methods still do not use CSS for sample distributions. Integrating the STAR method with existing baselines enhances performance across all datasets, consistently improving sample distributions in Euclidean space.\n\nThe superior performance of STAR is attributed to three factors: First, bidirectional KL divergence measures CSS, pushing negative samples further away and relatively bringing positive samples closer based on CSS magnitude, making fine-grained clusters easier to distinguish. Second, the base B of the exponential in Eq. 4 is a trainable scalar, balancing CSS magnitude and semantic structure. Third, STAR variants iteratively bootstrap model performance in neighborhood retrieval and representation learning through a generalized EM process (detailed in Appendix )."}, {"title": "5.3 Inference Mechanism Comparison (RQ2)", "content": "Previous methods (Chongjian et al., 2022; An et al., 2023a, 2024) perform a nearest neighbor search over the examples of the found fine-grained clusters for fine-grained category prediction (we refer to this technique as cluster inference). We speed up this process making it better suitable for real-time tasks by developing a centroid inference mechanism (see Section 4.4). Results in Table 3 demonstrates that results of centroid inference are compet-"}, {"title": "5.4 Ablation Study (RQ1 & RQ3)", "content": "We examine the impact of various components of the STAR method in STAR-DOWN, as detailed in Table 4. Our results yield the following insights. (1) Excluding coarse-grained supervision information during training (w/o CE) reduces model performance, as this information is crucial for effective representation learning. (2) Omitting the first loss term (w/o KL loss) from Eq. 4 diminishes performance. The KL loss term aligns the KL divergence between data samples and the query with their semantic similarities. Without it, $B^{d_{KL}(q_i,h_k)}$ fails to guide the query sample distribution based on semantic similarities in Eq. 4. (3) Removing the KL weight $B^{d_{KL}(q_i,h_k)}$ from Eq. 4 (w/o KL weight) reduces effectiveness. The loss no longer utilizes fine-grained semantic similarities measured by $B^{d_{KL}(q_i,h_k)}$ in the logarithmic space to direct the query sample distribution in comparison to all"}, {"title": "5.5 Exponential Base Impact (RQ4)", "content": "In the STAR method's loss equation (Eq. 4), $B^{d_{KL}(q_i,h_k)}$ modulates the distribution of $q_i$ and $h_k$ in the Euclidean space based on their semantic similarity in the logarithmic space, as quantified by the bidirectional KL divergence. The base B is used to enhance semantic differences, improving the discriminability of fine-grained categories. We experimented with multiple constant values and a trainable configuration for B, with multiple STAR-DOWN results presented in Table 5. The multiple STAR-DOWN methods with various base values consistently outperform the DOWN method (Ta- ble 2), demonstrating the effectiveness and robustness of the STAR method regardless of the base value B.\u00b9 Notably, base values that are either too low (e.g., e) or too high (e.g., 66) disrupt the semantic representation by inadequately or excessively emphasizing semantic similarities in the logarithmic space. To set base value conveniently, we set B as a trainable scalar, achieving favorable outcomes as indicated in Table 5."}, {"title": "5.6 Inference of Category Semantics", "content": "Prior works (An et al., 2023a, 2024) only discovered fine-grained categories and assigned them numeric indices without elucidating the categories semantics, thus constraining their broader application. We propose utilizing the commonsense reasoning"}, {"title": "5.7 Visualization", "content": "We visualize the sample embeddings of STAR-DOWN in Figure 4. The results demonstrate that our method forms distinguishable clusters for fine-grained categories, proving STAR's effectiveness in separating dissimilar samples and clustering similar ones. Additionally, we visualize the generalized EM perspective of STAR-DOWN in Appendix."}, {"title": "6 Conclusion", "content": "We propose the STAR method for fine-grained category discovery in natural language texts, which utilizes comprehensive semantic similarities in the logarithmic space to guide the distribution of textual samples, including conversational intents, scientific paper abstracts, and assistant queries, in the Euclidean space. STAR pushes query samples further away from negative samples and brings them closer to positive samples based on the comprehensive semantic similarities magnitude. This process forms compact clusters, each representing a discovered category. We theoretically analyze the effectiveness of STAR method. Additionally, we introduce a centroid inference mechanism that addresses previous gaps in real-time evaluations. Experiments on three natural language benchmarks"}]}