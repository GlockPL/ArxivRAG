{"title": "Why pre-training is beneficial for downstream classification tasks?", "authors": ["Xin Jiang", "Xu Cheng", "Zechao Li"], "abstract": "Pre-training has exhibited notable benefits to downstream tasks by boosting accuracy and speeding up convergence, but the exact reasons for these benefits still remain unclear. To this end, we propose to quantitatively and explicitly explain effects of pre-training on the downstream task from a novel game-theoretic view, which also sheds new light into the learning behavior of deep neural networks (DNNs). Specifically, we extract and quantify the knowledge encoded by the pre-trained model, and further track the changes of such knowledge during the fine-tuning process. Interestingly, we discover that only a small amount of pre-trained model's knowledge is preserved for the inference of downstream tasks. However, such preserved knowledge is very challenging for a model training from scratch to learn. Thus, with the help of this exclusively learned and useful knowledge, the model fine-tuned from pre-training usually achieves better performance than the model training from scratch. Besides, we discover that pre-training can guide the fine-tuned model to learn target knowledge for the downstream task more directly and quickly, which accounts for the faster convergence of the fine-tuned model.", "sections": [{"title": "Introduction", "content": "Pre-training is prevalent in nowadays deep learning, as it has brought great benefits to downstream tasks, including improving the accuracy [16, 11], boosting the robustness [17], speeding up the convergence [27], and etc. Naturally, a fundamental question arises: why pre-training is beneficial for downstream tasks? Previous works have tried to answer this question from different perspectives. For example, [44, 6, 26] attributed the benefits of pre-training to a flat loss landscape. [13] concluded that the improved accuracy was a result of unsupervised pre-training acting as a regularizer.\nUnlike above perspectives for explanations, we aim to present an in-depth analysis to answer the above question from a new perspective. That is, we quantify the knowledge encoded by the pre-trained model, and further analyze the effects of such knowledge on the downstream tasks. In this way, we can provide insightful and accurate explanations for the benefits brought by pre-training, which also sheds new light into the fine-tuning/learning behavior of DNNs.\nTo this end, we extract the knowledge encoded in the pre-trained model based on the interaction between different input variables [29, 22, 31], because the DNN usually lets different input variables interact with each other to construct concepts for inference, rather than utilize each single variable for inference independently. As Fig. 1(a) shows, the DNN encodes the co-appearance relationship (interaction) between different image patches in $S = {\\text{mouth}, \\text{ear}, \\text{eye}}$ of the input image x to form the dog face concept S for inference. Only when all three patches in S are all present, the interaction is activated and makes a numerical contribution $I(S|x)$ to the network output y. The absence/masking\u00b9 of any image patch will deactivate the interaction, and the numerical contribution is removed, i.e., $I(S|x) = 0$.\nMore crucially, [29, 22] have empirically verified and [31] have theoretically proven the sparsity property and the universal-matching property of interactions, i.e., given an input sample x, a well-trained DNN usually encodes a small number of interactions between different input variables, and the network output y can be well explained as the numerical contributions of these interactions, $y = \\sum_s I(S|x)$, as shown in Fig. 1(a). Thus, these two properties mathematically enables us to take interactions as the knowledge encoded by the DNN for inference. Apart from these two properties, the considerable discrimination power and high transferability across different models of interactions [22] also provide supports for the faithfulness of using interactions to represent knowledge encoded in a DNN. Please see Section 3.1 for detailed discussions.\nIn this way, we use interactions to precisely quantify and comprehensively analyze how pre-trained model's knowledge impacts the downstream classification task, so as to provide insightful ex-planations for two widely-acknowledged benefits of pre-training, i.e., boosting the classification performance and speeding up the convergence. The following explanations may also guide some interesting directions on pre-training for future studies.\n\u2022 Quantifying explicit changes of pre-trained model's knowledge during the fine-tuning process. We propose metrics to measure how pre-trained model's knowledge is discarded and preserved by the fine-tuned model for the inference of the downstream task, in order to provide comprehensive analyses for the benefits of pre-training. In experiments, we surprisingly discover that the fine-tuned model discards a considerable amount of pre-trained model's knowledge, especially extremely complex knowledge. In contrast, the fine-tuned model only preserves a modest amount of pre-trained model's knowledge that is discriminative for the inference of the downstream task.\n\u2022 Explaining the superior classification performance of the fine-tuned model. We discover that only little preserved knowledge can be successfully learned by a model training from scratch merely using a small-scale downstream-task dataset, because the preserved knowledge from the pre-trained model is acquired from an extremely large-scale dataset. Thus, pre-training makes the fine-tuned model encode more exclusively-learned and discriminative knowledge for inference, which partially responses to the better accuracy of the fine-tuned model.\n\u2022 Explaining the accelerated convergence of the fine-tuned model. Interestingly, we also observe that compared to the model training from scratch, pre-training guides the fine-tuned model more"}, {"title": "Related work", "content": "Explanation of pre-training. Fine-tuning pre-trained models on downstream tasks to speed up convergence and boost performance has become a conventional practice in deep learning [16, 11, 17, 6]. Many works have attempted to analyze why pre-training is beneficial for downstream tasks from different perspectives. Specifically, [13] discovered that the unsupervised pre-training acted as a regularizer, which improved the generalization power of the DNN. Alternatively, a lot of studies explained the high accuracy [44, 26], the fast convergence speed in federated learning [27, 6], and the reduced catastrophic forgetting in continual learning [25] of the fine-tuned models from the perspective of a flat loss landscape. Additionally, [5, 9] explained the transferability of the pre-trained model to downstream tasks from the perspective of the feature space by performing the singular value decomposition. In comparison, we present a comprehensive analysis to systematically unveil the essential reasons behind different benefits of pre-training, by quantifying the explicit effects of pre-trained model's knowledge on the downstream task from a game-theoretic perspective.\nUsing interactions to explain the DNN. In recent years, employing game-theoretic interactions to explain DNNs has become a newly emerging direction. Specifically, [38, 40, 7] quantified interactions between different input variables to formulate the knowledge encoded by a DNN, whose faithfulness was further experimentally verified and theoretically ensured by [22, 29, 31]. Besides, a series of studies utilized the interaction to explain the representation capacity of DNNs, including the generalization power [45, 43, 46], adversarial robustness [28], adversarial transferability [42], the learning difficulty of interactions [23, 30], and the representation bottleneck [10]. In comparison, this paper aims to provide insightful explanations for the benefits of pre-training to downstream tasks.\nQuantifying the knowledge encoded by the DNN. So far, there does not exist a formal and widely accepted method to quantify the knowledge encoded by a DNN. A series of studies [35, 34, 18] employed the mutual information between input variables and the network output to quantify the knowledge in the DNN, but precisely measuring the mutual information was still significantly challenging [19]. Besides, other studies employed human-annotated semantic concepts [2, 14] or automatically learned concepts [4] to explain the knowledge in the DNN, but these works could not quantify the exact changes of knowledge (i.e., the preservation of task-relevant knowledge and the discarding of task-irrelevant knowledge) during the fine-tuning/training procedure. In comparison, we use theoretically verifiable interactions to represent knowledge in the DNN, which enables us to explicitly quantify the exact effects of pre-trained model's knowledge on the downstream task, so as to provide detailed explanations for the benefits of pre-training."}, {"title": "Explaining why pre-training is beneficial for downstream tasks", "content": "In this section, let us introduce the interaction metric, together with a set of interaction properties [22, 29, 31] as convincing evidence for the faithfulness of interaction-based explanations, so as to provide a straightforward and concise way to understand why pre-training is beneficial for downstream tasks.\nDefinition of interactions. Given a DNN v trained for the classification task and an input sample $x = [x_1, x_2,..., x_n]$ composed of n input variables, let $N = {1, 2, . . ., n}$ represent the indices of all n variables. Let $v(x) \\in \\mathbb{R}$ denote the scalar output of the DNN or a certain output dimension of the DNN, where people can apply different settings for $v(x)$. Here, we follow [10] to set $v(x)$ as the"}, {"title": "Preliminaries: using interactions to represent knowledge in DNNS", "content": "confidence of classifying x to the ground-truth category $y^{truth}$ for multi-category classification tasks, as follows.\n$v(x) = \\text{log} \\frac{p(y = y^{truth}|x)}{1 - p(y = y^{truth}|x)}$  (1)\nThen, the contribution of the interaction between a subset $S \\subseteq N$ of input variables to the network output v is calculated by the Harsanyi Dividend [15], a typical metric in game theory, as follows.\n$I(S|x) = \\sum_{T \\subseteq S}(-1)^{|S\\setminus T|} \\cdot v(x_T)$, (2)\nwhere $x_T$ denotes a masked input sample crafted by masking variables in $N \\setminus T$ to baseline values\u00b9 and keeping variables in T unchanged. Let us take the sentence x =\"he has a green thumb\" as a toy example to understand (2). The DNN encodes the interaction between words in a subset $S = {\\text{green}, \\text{thumb}}$ with a numerical contribution I(S) to push the DNN's inference towards the meaning of a \"good gardener.\u201d This numerical contribution is computed as $I(S|x) = v({\\text{green}, \\text{thumb}}) - v({\\text{green}}) \u2013 v({\\text{thumb}}) + v(x_{\\empty})$, where $x_{\\empty}$ denotes all words in \u00e6 are masked.\nUnderstanding the physical meaning of interactions. Each interaction with a numerical contribu-tion $I(S|x)$ represents a collaboration (AND relationship) between input variables in a subset S. As in the aforementioned example, the co-appearance of two words in S = {green, thumb} constructs a semantic concept of \u201cgood gardener,\u201d and makes a numerical contribution I(S|x) to the network output. The absence (masking) of any words in S will inactivate this semantic concept and remove its corresponding interaction contribution, i.e., $I(S|x) = 0$.\nQuantifying the knowledge encoded by the DNN. The proven sparsity property and universal-matching property of interactions enable us to use interactions to represent knowledge encoded by the DNN. Specifically, [31] have proven that under some common conditions\u00b2, a well-trained DNN usually encodes very sparse interactions for inference, which is also experimentally verified by [22, 46]. In other words, although there exists $2^n$ different subsets\u00b3 $S \\subseteq N$ in total, only a small set $S_{salient}$ of interactions make salient contributions to the network output, i.e., $S_{salient} = {S \\subseteq N, |I(S|x)| > \\tau\u2074}$, subject to $|S_{salient}| \\ll 2^n$. Whereas, a large number of interactions contribute negligibly $I(S|x) \\approx 0$ to the network output, which can be considered as noisy patterns. Thus, the network output $v(x)$ can be well approximated by a small number of salient interactions, i.e.,\n$v(x) = \\sum_{S \\subseteq N} I(S|x) \\approx \\sum_{S \\in S_{salient}} I(S|x)$. (3)\nTheorem 3.1 (universal-matching property of interactions). Given an input sample x, there are $2^n$ differently masked samples ${x_T|T \\subseteq N}$. [31] have proven that network outputs $v(x_T)$ on all $2^n$ masked samples $x_T$ can be universally matched by a small number of salient interactions.\n$v(x) = \\sum_{S \\subseteq T} I(S|x) \\approx \\sum_{S \\subseteq T \\& S \\in S_{salient}} I(S|x)$. (4)\nTheorem 3.1 indicates we can use a small set of salient interactions to well explain the network output $v(x)$ on anyone $x_T$ of all $2^n$ masked samples. Thus, according to the Occam's Razor [3], we can roughly consider each salient interaction as the knowledge encoded by the DNN for inference, rather than a mathematical trick with unclear physical meanings.\nFaithfulness of using interactions to represent the knowledge of the DNN. Although nowadays there exist various methods to define/quantify the knowledge encoded by the DNN, a set of theoretically proven and empirically verified interaction properties ensure the faithfulness of the interaction-based explanation. Specifically, the universal-matching property in Theorem 3.1 and the sparsity property in (3) have mathematically guaranteed that interactions can faithfully explain the output of DNNs. Besides, [22] have experimentally verified the transferability property and the discriminative property of interactions. That is, interactions exhibit considerable transferability across samples and across models, and have remarkable discrimination power in classification tasks. Additionally, [29] have proven that interactions satisfy seven mathematical properties. Please see Appendix for detailed discussions."}, {"title": "Quantifying the effects of pre-training on downstream tasks", "content": "Despite the ubiquitous utilization and great success of pre-trained models, it still remains mysterious why such models can help the fine-tuned model achieve superior classification performance and converge faster\u2076, compared to training from scratch. Thus, to systematically and precisely unveil the reasons behind these two benefits, we propose several metrics based on interactions to explicitly quantify the knowledge of the pre-trained model that is utilized for the inference of the downstream task, and further explain effects of such knowledge on the fine-tuning process. These explanations also provide some new insights into the learning/fine-tuning behavior of the DNN."}, {"title": "Quantifying changes of pre-trained model's knowledge during the fine-tuning process", "content": "Explaining the precise effects of pre-training on downstream tasks still remains a significant challenge, because interactions (knowledge) directly extracted from the pre-trained model's output v cannot be used for explanation. This is due to that the pre-trained model is usually trained on an extremely large-scale dataset with extensive training samples, whose network output often encodes a vast amount of diverse knowledge. Such knowledge can be further categorized into knowledge that can be used for inference of the downstream task (e.g., some general and common knowledge), and knowledge that cannot be applicable to the downstream task (e.g., knowledge only related to the inference of the pre-trained task). Thus, we need to extract and quantify the knowledge of the pre-trained model that is used for the inference of the downstream task for explanation, so as to ensure our explanation will not be affected by other irrelevant knowledge.\nTo this end, we employ the linear probing method [1, 39, 24, 5], a commonly used technique, to extract pre-trained model's knowledge that is used for the downstream task. Specifically, given an input sample \u00e6 and a pre-trained model, we freeze all its network parameters, and use the feature f(x) of its penultimate layer (i.e., the layer preceding the classifier of the pre-trained model) to train a new linear classifier $W^T f(x) + b$ for the same downstream task as the fine-tuned model. Then, we define the following function $U_{pretrain}$ to quantify the pre-trained model's knowledge used for the inference of the downstream task $I(S|\u00e6, v_{pretrain})$, where $y_{pretrain}$ denotes the label predicted by the linear classifier.\n$U_{pretrain} = \\text{log} \\frac{p(y_{pretrain} = y^{truth}|x)}{1 - p(y_{pretrain} = y^{truth}|x)}$ (5)\nIn this way, the classification score $U_{pretrain}$ enables us to provide a thorough insight into the effects of the pre-trained model on the downstream task, by quantifying the changes of its knowledge $I(S|\u00e6, v_{pretrain})$ during the fine-tuning process. Specifically, we disentangle the knowledge $I(S|x, pretrain)$ into two components, including the knowledge preserved by the fine-tuned model for inference and the discarded knowledge. In this way, we define the preserved knowledge $K_{preserve}$ as the strength of the interaction shared by both the pre-trained model and the fine-tuned model. The discarded knowledge $K_{discard}$ is defined as the strength of the interaction that is encoded by the pre-trained model, but discarded by the fine-tuned model, as follows.\n$I(S|x, v_{pretrain}) = \\text{sign}(I(S|\u00e6, v_{pretrain})) \\cdot (K_{preserve}(S|x) + K_{discard}(S|x))$,\n$K_{preserve} (S|x) = \\mathbb{1} (\\Gamma^{pretrain}_{finetune} (S|x) > 0) \\cdot \\text{min}(|I(S|\u00e6, v_{pretrain})|, |I(S|X, v_{finetune})|)$, (6)\n$K_{discard} (S|x) = |I(S|X, v_{pretrain})| - K_{preserve} (S|x)$,\nwhere $\\Gamma^{pretrain}_{finetune} (S|x) = I(S|x, v_{pretrain}) \\cdot I(S|X, v_{finetune})$ measures whether the interaction encoded by the pre-trained model $I(S|\u00e6, v_{pretrain})$ and the interaction encoded by the fine-tuned model $I(S|\u00e6, v_{finetune})$ have the same effect. $V_{finetune}$ is calculated based on the fine-tuned model according to (1). $1(.)$ is the indicator function. If the condition inside is valid, $1(.)$ returns 1, and otherwise 0.\nSimilarly, we also disentangle the knowledge encoded by the fine-tuned model into two components, including the knowledge inherited from the pre-trained model $K_{preserve}(S|x)$, and new knowledge learned by the fine-tuned model itself to adapt the downstream task. Such a disentanglement helps us gain an insightful understanding of the fine-tuning behavior of the DNN, and also enables us to seek a deep exploration of the superior classification performance of the fine-tuned model in Section 3.2.2. Specifically, we define the knowledge $K_{new}(S|x)$ newly learned by the fine-tuned"}, {"title": "Quantifying the effects of pre-training on downstream tasks", "content": "model as the strength of the interaction that is encoded by the fine-tuned model, but is not present in the pre-trained model.\n$I(S|X, v_{finetune}) = \\text{sign}(I(S|x, v_{finetune})) \\cdot (K_{preserve} (S|x) + K_{new} (S|x))$,\n$K_{new} (S|x) = |I(S|X, v_{finetune})| - K_{preserve} (S|x)$. (7)"}, {"title": "Why the fine-tuned model can achieve superior classification performance?", "content": "Based on the quantification of pre-trained model's knowledge in the preceding section, here, we provide an insightful explanation for why pre-training can benefit the fine-tuned model in achieving superior classification performance\u2077. Intuitively, we consider that compared to training from scratch, the fine-tuned model can preserve some discriminative knowledge from the pre-trained model, which is beneficial for making inference, such as classifying hard samples. This is due to that the preserved knowledge is usually acquired using a large-scale dataset with numerous training samples, thus it contains sufficiently discriminative information. More crucially, this knowledge preserved from the pre-trained model is very difficult to be learned by a DNN training from scratch merely using a small-scale downstream-task dataset. Thus, pre-training makes the fine-tuned model encodes more exclusively-learned and discriminative knowledge than the model training from scratch for inference, which accounts for the superior performance of the fine-tuned model.\nTo this end, we propose the following metric to examine whether the model training from scratch can only successfully learns a little preserved knowledge $K_{preserve}(S|\u00e6)$ for verification. Specifically, given a pre-trained model and its corresponding fine-tuned model, we train a randomly initialized DNN $v_{random}$ from scratch for the same downstream task, where we set it has the same network architecture as the fine-tuned model for fair comparisons. We quantify the ratio of pre-trained model's knowledge preserved by the fine-tuned model $K_{preserve}(S|x)$ that can be successfully learned by the model training from scratch, as follows.\n$\\text{ratio}(S|x) = \\frac{\\mathbb{1} (\\Gamma_{random}(S|x) > 0) \\cdot \\text{min}(|I(S|\u00e6, v_{random})|, K_{preserve} (S|x))}{K_{preserve} (S|x)}$ (8)\nwhere $\\Gamma_{random}(S|x) = I(S|X, v_{pretrain}) \\cdot I(S|\u00e6, v_{random})$ measures whether interactions $I(S|\u00e6, v_{pretrain})$ and $I(SX, v_{random})$ have the same effect to the network output. Only when interactions $I(Sx, v_{pretrain})$, $I(Sx, v_{finetune})$ and $I(SX, v_{random})$ have the same effect, the metric ratio(S|x) is non-zero; Otherwise, $\\text{ratio}(S|x) = 0$. A small value of $ratio(S|x)$ indicates that the model training from scratch can merely learn a little preserved knowledge $K_{preserve} (S|x)$."}, {"title": "Why the fine-tuned model converges faster?", "content": "Apart from the improved performance, pre-training can also benefits the fine-tuned model in speeding up the convergence [17]. In this section, we present an in-depth analysis to explain this benefit. Specifically, according to the information-bottleneck theory [35, 34], when training from scratch, the DNN usually tries to encode various knowledge in early epochs and discarding task-irrelevant knowledge in later epochs. In comparison, pre-training guides the fine-tuned model to directly and quickly learn target knowledge, without temporarily modeling and discarding knowledge unrelated to the inference of the downstream task, which is responsible for the faster convergence of the fine-tuned model.\nExplicitly speaking, whether or not a DNN can quickly and directly learn target knowledge can be analyzed as whether the amount of learned target knowledge increases fast and stably along with the epoch number, respectively, where we define the target knowledge as the interaction encoded by the finally-learned DNN. To this end, we propose the following metrics to examine whether the fine-tuned model encodes target knowledge more directly and quickly for verification. Specifically, let the vectors $I_{finetune, e}(x) = [I(S_1|X, v_{finetune, e}), I(S_2|X, v_{finetune,e}),\u2026\u2026, I(S_a|x, v_{finetune,e})] \\in \\mathbb{R}^d$ and $I_{finetune, E}(x)$ represent the distribution of all interactions encoded by the model fine-tuned after e epochs and E epochs, respectively, where E denotes the total epoch number. Accordingly, the vector $I_{random, E}(x)$ and the vector $I_{random, E}(x)$ represent the distribution of all interaction encoded by the model training from scratch after e' epochs and E' epochs, respectively. Then, we calculate the Jaccard similarity between interactions encoded by the DNN learned after certain epochs and those encoded by the finally-learned DNN.\n$Jaccard_{finetune} = \\mathbb{E}_{\u00e6} [\\frac{||\\text{min}(I_{finetune, e}, 1(\u00cefinetune,e(x), \u00cefinetune, E(x))||_1}{||\\text{max}(\u00cefinetune, e(x), \u00cefinetune, E (x))||_1}]$,\n$Jaccard_{random} = \\mathbb{E}_{\u00e6} [\\frac{||\\text{min}(\u00cerandom, e' (x), \u00cerandom, E' (x) ||_1}{||\\text{max}(\u00cerandom, e' (x), \u00cerandom, E' (x))||_1}]$, (9)\nwhere we extend the d-dimension vector $I_{finetune,e(x)}$ to into a 2d-dimension vector $\u00cefinetune,e(x) = [(I_{finetune,e(x)})T, (-I_{finetune,e(x)})T] = [\\text{max}(I_{finetune,e}(x), 0)T, \u2013 \\text{min}(I_{finetune,e}(x),0)T]T \\in \\mathbb{R}^{2d}$ without negative elements. Accordingly, vectors $\u00cefinetune, E(x)$, $\u00cerandom,e' (x)$, and $\u00cerandom, E' (x)$ are constructed on $I_{finetune, E}(x)$, $I_{random, e' (x)}$, and $I_{random, E' (x)}$ to contain non-negative elements, respectively. Thus, a sharp increase of the similarity at early epochs indicates that the DNN encodes target knowledge quickly. Besides, a stable increase of the similarity along the epoch number, without significant fluctuations, demonstrates that the DNN encodes target knowledge directly."}, {"title": "Conclusion and discussion", "content": "In this paper, we present an in-depth analysis to explain the benefits of pre-training, including the boosted accuracy and the accelerated convergence, from a game-theoretic view. To this end, we use interactions to explicitly quantify the knowledge encoded by the pre-trained model, and further analyze the effects of such knowledge on the downstream task, where the faithfulness of treating interactions as essential knowledge encoded by the DNN for inference has been theoretically ensured by a set of properties of interactions. We discover that compared to training from scratch, pre-training enables the fine-tuned model to encode more exclusively-learned and discriminative knowledge for inference, and to learn target knowledge more quickly and directly, which accounts for the superior classification performance and faster convergence of the fine-tuned model. This provides new insights into understanding pre-training, and may also guide new interesting directions on the fine-tuning behavior of the DNN for future studies."}]}