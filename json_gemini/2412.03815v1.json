{"title": "Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software Repository-Related Question Answering", "authors": ["SAMUEL ABEDU", "SAYEDHASSAN KHATOONABADI", "EMAD SHIHAB"], "abstract": "Software repositories contain valuable information for gaining insights into their development process. However, extracting insights from these repository data is time-consuming and requires technical expertise. While software engineering chatbots have been developed to facilitate natural language interactions with repositories, they struggle with understanding natural language and accurately retrieving relevant data. This study aims to improve the accuracy of LLM-based chatbots in answering repository-related questions by augmenting them with knowledge graphs. We achieve this in a two-step approach; (1) constructing a knowledge graph from the repository data and (2) synergizing the knowledge graph with LLM to allow for the natural language questions and answers. We curated a set of 20 questions with different complexities and evaluated our approach on five popular open-source projects. Our approach achieved an accuracy of 65%. We further investigated the limitations and identified six key issues, with the majority relating to the reasoning capability of the LLM. We experimented with a few-shot chain-of-thought prompting to determine if it could enhance our approach. This technique improved the overall accuracy to 84%. Our findings demonstrate the synergy between LLMs and knowledge graphs as a viable solution for making repository data accessible to both technical and non-technical stakeholders.", "sections": [{"title": "1 INTRODUCTION", "content": "Software repositories are rich sources of information essential to the software development process. This includes data on source code, documentation, issue tracking data, and commit histories [53]. Analyzing this data can provide valuable insights about a project, such as developer activities and project evolution [27]. For instance, Begel and Zimmermann [10] and Sharma et al. [49]"}, {"title": "2 BACKGROUND & RELATED WORKS", "content": "In this section, we provide an overview of the key concepts that form the foundation of our study. We discuss software repositories and their significance, software engineering chatbots, and knowledge graphs."}, {"title": "2.1 Software Repositories", "content": "Software repositories contain data that track the development process of a project [27]. Platforms like GitHub and Jira provide version control systems that facilitate collaboration among developers, track changes over time, and support issue tracking and project management. Software repositories contain a wealth of information, including details about commits, pull requests, issues, and developer activities.\nPrior studies have analyzed repository data to investigate and understand various development processes. For instance, Dilhara et al. [19] conducted a large-scale analysis of commit data on GitHub to understand the evolution of machine learning library usage in open-source projects."}, {"title": "2.2 Knowledge Graphs", "content": "Knowledge graphs are structured representations of information that model entities (nodes) and the relationships (edges) between them [30]. They effectively organize and represent knowledge as triple facts (head entity, relationship, tail entity), allowing it to be efficiently utilized in advanced applications [15, 57]. Popularized by Google's introduction in 2012 [50], knowledge graphs have been widely used in domains such as the semantic web, natural language processing, and recommendation systems [32].\nIn the software engineering domain, prior studies have represented software repositories as knowledge graphs. For instance, Zhao et al. [59] proposed GitGraph, a prototype tool that automat-ically constructs knowledge graphs from Git repositories to help developers and project managers comprehend software projects. Malik et al. [40] introduced a method for representing software repositories as graphs to preserve the context between different features during anonymization for data sharing in software analytics. Additionally, Ma et al. [38] developed RepoUnderstander, a method that condenses critical information from entire software repositories into a repository knowledge graph to guide agents in comprehensively understanding the repositories.\nBy structuring repository data into a knowledge graph, it becomes possible to perform complex queries and infer new knowledge through graph traversal and pattern matching. Query languages like Cypher, used with graph databases such as Neo4j, Redis graphs, and MemGraph, enable querying of knowledge graphs using declarative language [23]."}, {"title": "2.3 Software Engineering Chatbots", "content": "Chatbots are conversational assistants designed to assist with specific tasks by interacting with users through natural language [47]. They aim to facilitate access to information, automate routine tasks, and support collaboration among team members [1]. Chatbots are increasingly becoming popular in the software engineering domain to accomplish specific software engineering tasks. For instance, Abdellatif et al. [2] proposed MSRBot, using a bot layered on top of software repositories to automate and simplify the extraction of useful information from the repository. Bradley et al. [12] proposed Devy, a Conversational Developer Assistant that enables developers to focus on high-level tasks by reducing the need for manual low-level commands across various tools. Dominic et al. [20] proposed a conversational bot to support newcomers in onboarding to open-source projects by recommending suitable projects, resources, and mentors. Okanovi\u0107 et al. [43] proposed PerformoBot, a chatbot that guides developers through configuring and executing load tests via natural language conversations. Also, Abedu et al. [4] developed an LLM-based chatbot to answer questions related to software repositories. Their LLM-based chatbot, which used the RAG approach, failed to retrieve the relevant data needed to answer questions in their evaluation questions most of the time.\nThe increasing application of chatbots in software engineering and LLMs in chatbots like ChatGPT and BARD motivates our work to improve the accuracy of LLMs in software engineering chatbots. To the best of our knowledge, this is the first software repository question-answering approach"}, {"title": "3 APPROACH", "content": "Figure 1 provides an overview of our approach to answering repository-related questions. Our approach consists of four key components organized into two steps: (1) data ingestion and (2) interaction. In the data ingestion step, the Knowledge Graph Constructor component collects repository data and models it as a knowledge graph. During the interaction step, the Query Generator component takes the user's natural language question as input and generates a graph query using an LLM to retrieve the relevant data required to answer the question. The Query Executor component then takes the generated query from the Query Generator component and executes it. It returns the results of the query, which are used by the Response Generator component as context to generate a natural language response to the user's question using an LLM. In this section, we describe each component of our approach in detail, using the question \u201cHow many people have contributed to the code?\u201d as our running example."}, {"title": "3.1 Knowledge Graph Constructor", "content": "The Knowledge Graph Constructor component aims to connect the entities in the software repository to form the repository knowledge graph. Using a knowledge graph allows us to model the complex relationships between the repository entities, facilitating analysis and inference of the repository data. The knowledge graph for a GitHub repository is complex considering the official GitHub schema [26]. As a result, in this study, we limit the knowledge graph to four entities (namely, Users, Commits, Issues, and Files) and their relationships to be able to capture the nuance of the repository metadata without too much complexity. The knowledge graph constructor collects the following types of data as mentioned:\n\u2022 Commits: Information about each commit to track code changes, authorship, and contribu-tions over time.\n\u2022 Issues: Details of issues (bugs) to track reported problems and identify their introducing and fixing commits.\n\u2022 Files: File structures and changes over time to track modifications and identify files impacted by bugs.\n\u2022 Users: Contributor information to analyze developer activities and contributions.\nThe knowledge graph constructor also identifies the bug-fixing and bug-introducing commits. Similar to prior studies [17], our approach identifies the bug-fixing commits by searching for the bug ID in the change logs of the commits. Then it identifies the buggy changes by employing the Davies et al. [18] variation of the SZZ algorithm referenced as R-SZZ [17]. The SZZ algorithm [51] is a widely used method in software engineering for detecting bug-introducing changes. The R-SZZ variation uses textual and dependence-related changes to improve on the original SZZ algorithm [18].\nAfter the data collection and SZZ execution, it constructs the knowledge graph. Similar to prior study [40], we define the schema of the knowledge graph by establishing the relationship between the entities in the GitHub repository. Figure 2 shows an overview of the entities and relationships in the schema of our knowledge graph. A description of the relationships between the entities is presented in Table 1.\nFor entities that continuously change during the lifespan of the repository, such as Files, we assign their evolving attributes to the relationships rather than to the nodes. For instance, if a commit changes a file, the change type (added, deleted, renamed) is assigned as an attribute to the changed relationship between the commit and the file, not as an attribute of the file itself. After the construction of the knowledge graph, we store the knowledge graph in a graph database to allow for querying."}, {"title": "3.2 Query Generator", "content": "An essential step in our approach is retrieving the relevant information to answer a user's question. The Query Generator component aims to generate graph queries that correspond to the user's questions. In this study, we operationalize the graph query using Cypher, an evolving query language for graph databases that is supported by Neo4j, Redis Graph, and Memgraph [23].\nThe Query Generator uses an LLM to generate the Cypher query. The LLM uses the entities and relationships in the schema of the knowledge graph to generate the Cypher query using the prompt template shown in Figure 3. The prompt follows guidelines and best practices for prompt engineering [44] and accepts three main parameters to generate the Cypher query: (1) the current date and time, (2) the schema of the knowledge graph, and (3) the user's natural language question. The current date and time were added to inform the LLM in answering questions requiring relative dates, such as \"How many commits from last month\u201d. The schema of the knowledge graph informs the LLM of the types of entities and relationships in the knowledge graph.\nIn our running example \u201cHow many people have contributed to the code\u201d, the Query Generator component uses the schema of the knowledge and the question to generate the text containing the MATCH (u:User)-[:author]->(c:Commit) RETURN COUNT(u) AS contributors to get the number of contributors in the project."}, {"title": "3.3 Query Executor", "content": "To generate the response to the user's question, we have to retrieve and pass the relevant information from the knowledge graph to the Response Generator. We achieve this through the Query Executor, which takes the generated output of the Query Generator and executes it. Although the Query Generator is prompted to only return the Cypher query, there are instances where it returns additional texts to the Cypher query, which can result in a syntax error when executed. As a result, the Query Executor component extracts the Cypher statement from the output of the Query Generator as a means of quality control using regular expression matching.\nThe Query Executor then executes the extracted Cypher query, returning the results from the knowledge graph database. The result is passed to the Response Generator component to generate a natural language response for the user. In the running example, the Query Executor extracts the Cypher MATCH (u:User)-[:author]->(c:Commit) RETURN COUNT(u) AS contributors from the generated text. It then executes the query and returns the result [contributors: 40], assuming there are 40 contributors."}, {"title": "3.4 Response Generator", "content": "The goal of the Response Generator Component is to generate a natural language response to the user's question based on the results returned by the Query Executor. The Response Generator prompts the Response Generator LLM to generate the natural language response using the prompt template shown in Figure 4. The prompt template accepts four parameters: (1) the schema of the knowledge graph, (2) the generated Cypher query serving as additional context for interpreting the results and generating an appropriate answer to the question, (3) the context, which is the results from the Query Executor, and (4) the user's question. To prevent hallucination, we instructed the LLM to respond \u201cI don't know\u201d, if it is not sure of the answer and not make up a response. In the running example, the Response Generator takes the results of the query, the question, the schema, and the Cypher query and returns the natural language response \u201cA total of 40 people have contributed to the code\"."}, {"title": "4 EVALUATION SETUP", "content": "The main goal of this study is to improve the accuracy of LLM-based chatbots in answering repository-related questions. In this section, we present the evaluation setup for our approach in detail. We begin by explaining the criteria for selecting the projects for the evaluation of our approach. Finally, we discuss the questions used for the evaluation and the implementation of our approach."}, {"title": "4.1 Selected Project", "content": "For this study, we selected software projects from GitHub based on a set of criteria for our evaluation. We selected the projects based on their popularity on GitHub. We used the number of stars of a project as a proxy for identifying the most popular projects on GitHub [11]. However, some of the most popular projects on GitHub are not software projects, such as a collection of awesome projects or educational projects. Therefore, we excluded projects that are not software projects, for example, the free-programming-books project [21]. In addition, we required that the projects have their code and issue tracking data on GitHub. This requirement ensures that all relevant development activities for the construction of the knowledge graph discussed in Section 3.1 are accessible through a unified platform, facilitating comprehensive data collection. For example, the Linux [52] project is one of the most popular projects on GitHub, but it was excluded because the issue tracking is not on GitHub. Also, we required that for commits that are fixing or closing issues in the project, the commit log should reference the issue id, for example, \u201cfixes issue #123\". This linkage is for accurately mapping issues to their fixing changes when constructing the knowledge graph and serves as a start to progressively identify the bug introducing commits [18].\nBased on these criteria, we selected five popular open-source projects shown in Table 2. The selected projects cover various domains and programming languages and have a median number of 170,736 stars, 7,295 commits, 12,545 issues, and 392 contributors. The data were collected on August 19, 2024."}, {"title": "4.2 Evaluation Questions", "content": "In evaluating the approach, we curated the questions by Abdellatif et al. [2], which they collected from 12 users interacting with software repositories to access various information for the completion of tasks assigned to them. These tasks include finding answers to questions that are commonly asked by developers and non-technical stakeholders, for instance, finding the commit that introduced a bug or the developer that fixed the most bugs [10, 49]. The users asked 165 questions representing 10 distinct intents, where each intent refers to the mapping between the user's question and a predefined action to be taken to complete the task [7]. Executing all 165 questions would be expensive; therefore, we limited our selection to two questions per intent, resulting in a final evaluation set of 20 questions. The two questions were selected for each intent based on the clarity of their phrasing. This approach reduces the cost of executions while ensuring variety by covering each of the 10 intents with two variations of questions.\nAlso, for a more fine-grained evaluation of our approach, we classify the selected questions into three difficulty levels. We define difficulty as the number of relationships in the knowledge graph required to answer the question. The level one questions include questions that only require a single entity and not a relationship to answer. For example, \u201cWhat is the latest commit\u201d only requires the Commit entity. Level two questions require a single relationship to answer. For example, \u201cWhich commit fixed the bug X\u201d requires the Commit entity and the Issue entity linked by the fixed relationship. Lastly, the level three questions require two or more relationships to answer. For example, \u201cDetermine the percentage of fixing commits that introduced bugs in June 2018\u201d requires the Commit entity, the Issue entity, the fixed relationship and introduced relationship. The 20 questions used for the evaluation with their corresponding intent and difficulty level can be found in Appendix A.\nTo establish the ground truth for our evaluation, the first author manually wrote Cypher queries corresponding to all 20 questions for each of the selected repositories. To ensure the correctness of these queries and eliminate potential bias, the authors collaboratively reviewed and discussed the logic employed in each query, adding an additional layer of scrutiny. The Cypher queries were then executed against the knowledge graphs, and the resulting outputs were used as the ground truth for comparison in our study."}, {"title": "4.3 Implementation", "content": "We implement the approach discussed in Section 3 using the Python and Langchain framework. The Knowledge Graph Constructor begins the process by collecting data from the software repository using the GitHub GraphQL API [24]. We opted for the GraphQL API over the REST API [25] because GraphQL allows us to specify precisely the data we need in a single request, reducing the noise and the size of the document returned compared to the REST API and improving processing efficiency [13]. The collected data included information on users, commits, issues, and files (see Section 3.1). After collecting the data, we implemented the relationship between the entities"}, {"title": "5 RESULTS", "content": "In this section, we first present the results of the exploratory analysis, and then present the results of the three main research questions. For each research question, we present the motivation, the approach, and the results."}, {"title": "5.1 RQ0: How good are LLMs in generating Cypher queries for knowledge graphs?", "content": "Motivation. Before answering the research questions in this study, we first evaluate the capability of different LLMs to accurately generate Cypher queries from natural language text. This RQ aims to empirically assess the context of this study (i.e., the ability of LLMs to generate accurate Cypher queries from natural language text for retrieving data from a knowledge graph). Secondly, this RQ aims to empirically identify the most efficient LLM in generating Cypher queries, which will serve as the LLM model in our implementation to answer the remaining RQs."}, {"title": "Approach.", "content": "To evaluate the capability of LLMs to generate valid Cypher queries from natural language text, we selected three state-of-the-art models considering both open-source and closed-source options: GPT-4o [45], Llama3-8B [42], and Claude3.5 [8]. These models have been widely used in software engineering literature [39, 60]\nSimilar to the strategy by Li et al. [37], we evaluate the models under zero-shot settings to assess their generalization ability to generate Cypher queries using their pre-existing knowledge without prior exposure or clues. We evaluate the models on the 20 questions described in Section 4.2 using the prompt template shown in Figure 3 to generate the Cypher query. Due to the stochastic nature of LLMs, we run the experiments five times, each time on a different repository [22].\nAs our evaluation metric, we use the Execution Accuracy (EX) [37] because it measures the correctness of the generated Cypher queries in terms of their execution results, which is essential for applications that require accurate retrieval of data. EX is defined in Equation 1 as the proportion of the evaluation set in which the executed results of the generated queries are similar to the ground truth, relative to the examples in the evaluation set and formalized as:\n$EX = \\frac{\\sum_{n=1}^{N} 1(V_n, \\hat{V_n})}{N}$ \nand 1(\u00b7) is a function represented as:\n$1(V_n, \\hat{V_n}) = \\begin{cases}\n1, & \\text{if } V_n = \\hat{V_n} \\\\\n0, & \\text{if } V_n \\neq \\hat{V_n}\n\\end{cases}$ \nwhere N is the total number of executions, $V_n$ is the result set from executing the ground-truth Cypher queries, and $\\hat{V_n}$ is the result set from executing the generated Cypher queries."}, {"title": "Results.", "content": "Table 4 compares the execution accuracy of the selected models in generating Cypher queries. We find that GPT-4o is the most efficient model in translating natural language questions into accurate Cypher queries within the given zero-shot setting, achieving an EX score of 0.65. The superior performance of GPT-4o can be attributed to the advanced language understanding capabilities of the GPT-4 family of LLMs in capturing the semantic details required for precise Cypher query generation as demonstrated in prior studies [37]. The performance difference between GPT-4o, Claude3.5, and Llama3 highlights the variability in capability among different LLMs when applied to the task of generating Cypher queries from natural language text. This finding informs our decision to utilize GPT-4o for the subsequent research questions, as it offers the most reliable performance for synergizing LLMs with knowledge graph data."}, {"title": "5.2 RQ1: How effective is our approach in answering software repository-related questions?", "content": "Motivation. The synergy between knowledge graphs and large language models (LLMs) has the potential to enhance the ability of LLMs to provide accurate and contextually relevant answers [46]. Knowledge graphs encapsulate structured information about entities and their relationships, which can be crucial for understanding complex queries and providing precise answers. In this research question, we investigate the effectiveness of generating an accurate response to a user question by adding a layer of semantic understanding. This enables the LLM to generate a Cypher query and retrieve the relevant information to answer the question.\nApproach. To evaluate the performance of our approach in answering software repository-related questions, we conducted an end-to-end evaluation of the approach from the moment the Query Generator receives the natural language query to when the Response Generator outputs the final response (see step 2 in Figure 1). The end-to-end evaluation measures the practical performance of our approach in generating accurate answers [22].\nFor this purpose, we evaluated the 20 questions described in Section 4.2 for each of the selected projects. For each question, we also executed the process five times to account for the stochastic nature of LLMs in the generation process. We compared the final responses generated by our approach to the oracle answers (pre-determined correct answers based on the data in the knowledge graph). A question was considered correctly answered if our approach provided the correct response at least 50% of the time. If it fails in three or more executions, the question is marked as incorrect.\nResults. Table 5 compares the accuracy of our approach across the selected projects. We observed that the accuracy of our approach varies between 60% and 75% across the projects. The highest accuracy was achieved for the AutoGPT repository at 75%, while both Ohmyzsh and Vue had the lowest accuracy at 60%. The average accuracy across all repositories was 65%, indicating that our approach has a moderate overall effectiveness in answering questions related to the software repositories.\nTable 6 also compares the accuracy of our approach based on the difficulty level of the questions. The results show a correlation between the difficulty level and the accuracy of our approach. Our approach achieved an 80% accuracy on level 1 questions, and the accuracy decreased to 65% for level 2 questions. The accuracy further dropped to 50% for level 3 questions. This trend suggests that while our approach is effective at handling straightforward queries, its effectiveness decreases as the questions get more complex. We present all the questions answered correctly or incorrectly in this RQ in Appendix B"}, {"title": "5.3 RQ2: What are the limitations of our approach in accurately answering software repository-related questions?", "content": "Motivation. While our approach achieved better performance compared to previous LLM-based approaches [4], the task is still challenging for our approach, with an accuracy of 65%. To be able to improve our approach to achieve a higher accuracy, we need to understand the reasons for which our approach fails. Therefore, in this research question, our goal is to identify the limitations of our approach by manually analyzing the incorrectly generated responses of our approach.\nApproach. To understand the limitations of our approach, we selected all the incorrectly answered questions from the 500 executions in RQ1, that is, 20 questions each executed five times for five repositories. We identified and manually analyzed a total of 164 executions that returned incorrect answers. To identify the limitations, we adopted an open-card sorting approach as used in prior studies [36]. The executions were sorted based on the final response of our approach, the generated Cypher query, and the results from executing the Cypher query. The main author read through all 164 executions to identify recurring themes and patterns that may have led to incorrect responses to come up with labels. To ensure the labels are less biased and go through a level of scrutiny, the authors discussed each question and the preliminary labels. This step ensured that the labels had clarity and were relevant. Based on this step, some of the labels were merged, split, or modified to provide more clarity.\nResults. Table 7 summarizes the definitions, frequencies, and percentages of the six main limitations we identified from manually analyzing the instances where our approach generated incorrect answers: Incorrect relationship modeling, Faulty arithmetic logic, Misapplied attribute filtering, Invalid assumptions, Misapplied date formatting, and Hallucination. There were cases where multiple limitations were identified within a single instance. Thus the frequency reported reflects the total number of limitations rather than the 164 instances analyzed.\nThe most prevalent limitation was the incorrect relationship modeling, occurring in 123 out of 164 incorrect responses (75.0%). This limitation occurs when the logic used by the LLM"}, {"title": "5.4 RQ3: Can chain-of-thought prompting improve the effectiveness of our approach in answering software repository-related questions?", "content": "Motivation. In RQ2, we identified that the faulty reasoning of the LLM during query generation is the main contributor negatively affecting the accuracy of our approach in answering the questions. Prior studies have shown that using chain-of-thought to generate a series of intermediate steps before the final answer can improve the reasoning ability of LLMs [35, 56]. This can be achieved under zero-shot settings (prompting the LLM to think step by step) [35] and few-shot settings (prompting the LLMs with a few chain-of-thought examples). In this research question, our goal is to improve the reasoning ability of our approach to mitigate faulty reasoning. We do this by evaluating our approach using the chain-of-thought prompting by feeding the LLM with step-by-step reasoning examples to guide it in generating a reasoning path to reach an answer.\nApproach. To evaluate if chain-of-thought prompting can improve the performance of our approach in answering software repository questions, we first evaluated the approach by including a zero-shot chain-of-thought instruction, that is: \u201cLet's think step by step\u201d [35] to the prompt template in Figure 3. This did not improve the results presented in RQ1 (See Appendix D for the results of the zero-shot chain-of-thought across the selected projects and difficulty levels). We experimented with few-shot chain-of-thought by incorporating examples into the prompt provided to the Query Generator. Similar to [56], we adopted the format that begins with the input question, the step-by-step reasoning process, and the final output in our few-shot chain-of-thought prompting. This format aims to guide the LLM in generating intermediate reasoning steps before arriving at the final answer. We constructed the chain-of-thought prompt as presented in Figure 5. The prompt consists of two examples consisting of difficulty level 2 and 3 questions. By providing these examples, we intended to show the LLM how to generate reasoning paths that can help in constructing correct queries. It is important to note that the questions used in the chain-of-thought examples are not part of our evaluation questions.\nFor the evaluation, we used the same set of 20 questions described in Section 4 and used in RQ1. Also, we followed a similar approach as the evaluation of RQ1; executing the experiments five times for each question to account for the stochastic nature of the LLMs generation [22]. A question is considered correctly answered if the correct response is generated most of the time otherwise, it is marked as incorrect.\nResults. Table 13 compares the accuracy of the few-shot chain-of-thought approach across the selected project. The accuracy across the projects ranged between 80% and 90%, with an average of 84%. The results improved across all projects to the result in RQ1 (see Table 5). Table 14 also presents the performance based on the difficulty level of the questions. The accuracy improved across all levels with the application of few-shot chain-of-thought prompting compared to without the few-shot chain-of-thought prompt (see Table 6). For level 1 questions, the accuracy increased to 85% from the previous 80% in RQ1. For level 2 questions, the accuracy improved to 82% from 65% in RQ1. For the level 3 questions, which were previously challenging for our approach, the accuracy significantly increased to 90% from 50% in RQ1, indicating a substantial improvement in handling complex queries. Comparing the results with those from RQ1, the overall accuracy improved from 65% to 84%, signifying an improvement of 19% percentage points compared to the results in RQ1.\nThe improvement in the results, especially in the level 3 questions, suggests that few-shot chain-of-thought prompting effectively mitigated the faulty reasoning limitation identified in RQ2. By providing reasoning steps, our approach could better navigate the complex relationships within the knowledge graph. For example, when asked to \u201cDetermine the percentage of the fixing commits that introduced bugs on July 2023?\u201d, the few-shot chain-of-thought prompting approach, correctly identified the required relationships between the commits that fixed an issue and, at"}, {"title": "6 DISCUSSIONS", "content": "In this study, we explored the synergy between large language models (LLMs) and knowledge graphs to enhance the accuracy of software engineering chatbots in answering software repository-related questions. Our findings demonstrate that synergizing LLMs and knowledge graphs can"}, {"title": "6.1 Implication for Practitioners.", "content": "The findings of our study have implications for software developers, project managers, and other stakeholders involved in software development. Augmenting chatbots with LLMs and knowledge graphs can significantly enhance accessibility to repository information, making it easier for non-technical team members to retrieve project information without requiring technical expertise. This accessibility can facilitate better collaboration, informed decision-making, and increased efficiency within development teams.\nPractitioners like chatbot developers should consider implementing interactive features in chat-bots, allowing the chatbot to ask follow-up questions that lead to better understanding and more accurate responses, ultimately improving user satisfaction. This will help improve the chatbot's responses when there is ambiguity. The identified limitations, such as the exact matching instead of the pattern matching in the generated Cypher queries, highlight the need for chatbot developers to include robust error handling and validation in chatbot systems."}, {"title": "6.2 Implication for Researchers.", "content": "The findings of this study open avenues for further investigation. Using chain-of-thought improved the accuracy of the reasoning ability of the LLM in our approach. Nonetheless, there are other proposed approaches for enhancing the reasoning ability of LLMs [29, 33, 58]. Researchers can build upon this work by investigating other reasoning techniques, integrating symbolic reasoning with neural networks, or exploring alternative prompting strategies to improve the reasoning in LLMs for software engineering chatbots.\nThe handling of ambiguous queries presents another area for research. Researchers should explore methods for quantifying and reducing ambiguity in user queries. They can focus on developing models that can manage ambiguity by generating multiple interpretations with corresponding confidence levels."}, {"title": "7 THREATS TO VALIDITY", "content": "In this section, we discuss threats to the validity of our study and the measures taken to mitigate them. We consider threats to construct validity, internal validity, and external validity."}, {"title": "7.1 Construct Validity", "content": "Construct validity pertains to the extent to which our evaluation measures accurately reflect the theoretical constructs they are intended to assess. A threat to construct validity is the dependency on the knowledge graph schema. The LLM's ability to generate a correct Cypher query is dependent on its understanding of the schema of the knowledge graph. Any discrepancies or ambiguities in the schema can lead to incorrect Cypher query generation. If the knowledge graph schema does not accurately represent the repository data, the LLM may produce queries that do not retrieve the intended information. We mitigated this by providing explanations of the meaning of the relationships between the entities in the knowledge graph."}, {"title": "7.2 Internal Validity", "content": "Internal validity refers to the extent to which the observed effects can be attributed to the variables under investigation rather than other factors. A threat to internal validity in our study is the stochastic nature of LLM outputs. Despite setting the temperature parameter to zero to reduce randomness, inherent variability in the LLM's responses could influence the results. Correct answers might occasionally occur by chance rather than due to the effectiveness of our approach."}, {"title": "7.3 External Validity", "content": "External validity concerns the generalizability of our findings beyond the context of this study. A threat to external validity is the generalization beyond the evaluation questions used in our study. We evaluated our approach using a set of 20 questions representing 10 intents described in Section 4. Although these questions cover different difficulty levels and intents, they may not encompass the full diversity of questions that users might pose in real-world scenarios. The chatbot's performance on these questions might not generalize to other questions, especially those that are more complex and ambiguous. To address this threat, we posed ad-hoc questions to our approach and observed that the approach was able to answer the ad-hoc questions beyond the evaluation set. This suggests some level of generalizability. However, there is still a possibility that applying our approach to different types of questions could yield different results."}, {"title": "8 CONCLUSION", "content": "In this study, we investigated the synergy between large language models (LLMs) and knowl-edge graphs to improve the accuracy of software engineering chatbots in answering software repository-related questions. Our approach aimed to accurately answer natural language questions by generating Cypher queries to retrieve relevant repository data from the knowledge graph. Then use the retrieved information as context to generate a natural language response, making repository information accessible to both technical and non-technical stakeholders. We empirically evaluated our approach using five popular open-source GitHub repositories and a set of 20 questions curated from Abdellatif et al. [2] and categorized into three levels of difficulties. The findings demonstrated that LLMs, specifically the GPT-4o model, can answer repository-related questions by generating Cypher queries to retrieve accurate data from the knowledge graph. The initial accuracy of 65% achieved by our approach highlighted the potential limitation of synergizing LLMs with knowledge graphs. We manually investigated the instances where the approach failed to generate an accurate response and identified the faulty reasoning by the LLM as the predominant factor (80.5%) affect-ing the approach. We conducted further empirical evaluation if using few-shot chain-of-thought prompting can improve the accuracy. This technique significantly enhanced the reasoning ability of the LLM in our approach and improved the overall accuracy from 65% to 84%. There was a notable increase in the accuracy of the level 3 questions from 50% to 90%, signifying an improvement in the approach to handling complex queries. Our findings highlight the integration of LLMs with knowledge graphs as a viable solution for making repository data accessible to both technical and non-technical stakeholders. Also, our study highlights the importance of enhancing reasoning capabilities in LLMs. This opens avenues for further investigation in this direction"}, {"title": "F AD-HOC EXAMPLE DEMONSTRATION", "content": "Question:\nWho added the most lines of code in December", "Generator)": "nTo determine who added the"}]}