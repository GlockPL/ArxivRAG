{"title": "What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski"], "abstract": "The term Language Models (LMs), as a time- specific collection of models of interest, is con- stantly reinvented, with its referents updated much like the Ship of Theseus replaces its parts but remains the same ship in essence. In this pa- per, we investigate this Ship of Language Mod- els problem, wherein scientific evolution takes the form of continuous, implicit retrofits of key existing terms. We seek to initiate a novel per- spective of scientific progress, in addition to the more well-studied emergence of new terms. To this end, we construct the data infrastructure based on recent NLP publications. Then, we perform a series of text-based analyses toward a detailed, quantitative understanding of the use of Language Models as a term of art. Our work highlights how systems and theories influence each other in scientific discourse, and we call for attention to the transformation of this Ship that we all are contributing to.", "sections": [{"title": "1 Introduction", "content": "Scientific publications expand exponentially, with the size of literature doubling every ~17 years (Fortunato et al., 2018; Bornmann et al., 2021). The field of CL/NLP is no exception; in fact, the doubling only took 5 years: As of 2023, the number of papers documented in the ACL Anthology is twice as much as the total by 2018 (Bollmann et al., 2023; Zhao et al., 2023). With the explosion of new publications, it is imperative but also increasingly challenging to sort out the major contexts, progress, and future directions of the field.\nResearchers have sought to identify emergent key terms and factors that led to disruptive shifts of paradigms (Uban et al., 2021; Pramanick et al., 2023; Kuhn et al., 2014). In this period of flux, however, an ever-evolving field like ours calls for deeper analysis beyond identifying these elements, in order to understand various quantitative questions regarding these rapid and disruptive shifts.\nFor instance, to what extent is the field transformed by any certain model, like ChatGPT (OpenAI, 2022)? How does the popularity of the latest GPT models compare with, say, that of BERT (Devlin et al., 2019) in 2020? From there, we might even be curious about bigger questions like, \u201cHow unprecedented really is ChatGPT?\u201d, where our empirical guesses can diverge drastically without sufficient quantitative evidence. While readers of this paper likely come in with a tacit understanding of the ebb and flow of the field, it is hard to nail down such factors that keep changing in publications.\nMore fundamentally, the narrative describing scientific progress as the emergence of new elements does not cover the more implicit paradigm shifts, which features the evolution instead of invention of terms. The (forms of) key terms may continue to be broadly used, but are gradually overwritten with new meanings in new contexts. Language Models (LMs), as a term of art, refers to no single, static thing. It is used referentially to index a collection of models deemed relevant and representative at the time or in the context of a paper. As this is ever-changing, we are faced with a Ship of Theseus sce- nario (Plutarch), wherein the same terminology is essentially re-invented and its referents are perhaps entirely replaced. As such, a subtle gap between the durable collective term of \"LMs\" and the time- specific referent models of the moment is widening as the field progresses, threatening its stability and accessibility to new researchers. These issues call for new analyses of the subtle transformations that result from these paradigm shifts.\nIn this paper, we seek a quantitative description of a field's continuous evolution. More specifically, we inquire into the Ship of LMs paradox, i.e., the aforementioned reconstruction of the term Language Models. We decipher this evergreen term's rapidly changing referents, contexts, and usages across time. We develop a semi-automatic, generalizable framework to extract and organize two"}, {"title": "2 Related Work", "content": "Diachronic Analysis of the Progress in NLP\nVarious studies review the history of NLP conferences and the ACL Anthology (Hall et al., 2008; Anderson et al., 2012; Bollmann et al., 2023), as well as the community that contributed to the field's trajectory (Abdalla et al., 2023; Movva et al., 2023). Other works identified the field's transition points and themes. Hou et al. (2019) proposes an automatic framework to extract key entities (tasks, dataset, etc.). Uban et al. (2021) explores a similar goal via topic modeling, and Pramanick et al. (2023) further identified such entities that causally shaped the field's important stages. More recently, there has also been a specific focus on the changes brought by LLMs (Min et al., 2023; Fan et al., 2023; Zhao et al., 2023) and the impact on the related communities (Saphra et al., 2023; Liang et al., 2024). Aside from text-based analysis, interviews and surveys (Gururaja et al., 2023; Michael et al., 2023, etc.) have also provided valuable qualitative insights for the disruptive shifts.\nParadigm Shifts and Scientific Trends have also been core topics in the broader Science of Science field (Fortunato et al., 2018) beyond CL/NLP. The existing literature mostly centers on the emergence of new, trending ideas as well as their dynamics across the author networks. For instance, Kuhn et al. (2014) identified text snippets that are largely cited by future works, coined scientific memes, on citation graphs; Cheng et al. (2023) explored the diffusion process of new ideas under various social factors; and Chu and Evans (2021) measured the relation between the speed of producing new ideas and the size of a field. Citation/Author networks have also been introduced by recent works (Mohammad, 2020; Wahle et al., 2023) as a method for the more specific background of the NLP field.\nOur work provides a complement to these ongoing threads. As discussed, we raise a novel scenario about the transitions within a lasting concept (Ship of LMs), which to our knowledge has not been explored. We examine the use of such terms as LMs, providing quantitative interpretations of how (and how much) our beliefs and common grounds have evolved. In some sense, our work can be also seen as a meta-analysis of the various works studying certain elements (e.g. \"the era of LLM\", \"stages of statistical Machine Translation\u201d, \u201cChatGPT's impact\", etc.) We integrate these valuable findings to highlight a new question about the procedures: how exactly did we forge these of key elements into practice and eventually to our norms of language?"}, {"title": "3 Methods", "content": "3.1 Dataset Construction\nFollowing common practice in prior work (Mohammad, 2020; Pramanick et al., 2023), we utilize the official ACL Anthology as our data source. We collect papers accepted to the main Proceedings of three major NLP conferences (ACL, EMNLP, NAACL) held annually. We first interact with the API to fetch metadata (e.g., Anthology ID, title, and abstract). Based on the index of a conference, we obtain the paper PDFs from the formatted Anthology URLs, and scan the text with the pypdf2 tools. For post-processing, we remove excessive formatting (e.g. conference names in the footers) and identify section titles with regular expressions. The resulting dataset contains in total 7,650 papers from 10 conferences sequentially from ACL 2020 to EMNLP 2023. Our analyses focus on this most recent 4-year window where the advances regarding LMs have been especially pronounced, while our methodologies can similarly extend to a broader range.\nDefault Setup We extract the body text by cutting off before the References section. This is marked as our default setup, and experiments are based on the default unless otherwise noted."}, {"title": "3.2 Retrieving the Mentions of LMs", "content": "To investigate the Ship of LMs problem, we start by extracting and analyzing keywords and relevant entities, a common backbone method for analysis (Hou et al., 2019; Pramanick et al., 2023). For a sample paper to be related to LMs, the writing could utilize two types of mentions: (1) the collective concept of \u201clanguage models\", implying the context as a generalizable discussion, and (2) the names of specific models, indicating what models are exactly considered in a limited scope.\nOur goal is to maintain two keyword sets that correspond to the two types. Thus, we can resolve the referents of the generic language model mentions to the specific models used, by locating, linking, and comparing the keywords from both sets.\n3.2.1 Notations\nAs described, we seek to build two related collections of key entities, one marking the mentions of LMs as a general term, and the other marking specific model names. The two are respectively denoted by $\\mathcal{L}$ (from LMs) and $\\mathcal{M}$ (from Models). In practice, $\\mathcal{L}$ converges to a small, well-recognized set of terms. We define\n$\\mathcal{L}$ = {language model, LLM, PLM}\nsince \"language model\" is the substring of most of its subcategories, e.g., \u201clarge language models\", \"Korean language models\", or \"language model- ing\", and searching for \u201clanguage model\" covers all such variations. We also include the most common acronyms, \u201cLLM\u201d (Large Language Models) and \"PLM\u201d (Pre-trained Language Models). The construction of $\\mathcal{M}$ is elaborated in \u00a73.2.2.\nWe use $m$ to represent a specific element from $\\mathcal{M}$ (e.g., $m$ = BERT). For a span of text, $s$, we have $\\mathcal{M}(s)$ representing the subset of elements from $\\mathcal{M}$ that indeed appear in $s$. While $\\mathcal{M}(\\cdot)$ is a function of the input text by definition, we omit the input when $s$ is the entire body (default setup) for simplicity, and write the subset as $\\mathcal{M}$. The omission applies similarly to the notations below.\nTo initiate our study on any individual paper and any model(s) of interest, we introduce a family of counting functions. Given a model name $m$, we define $N_m(\\cdot)$ as the count of how many times $m$ appears in the input text. The counting functions also apply to sets of model names: For a set of models $\\mathcal{M} = \\{m_1, m_2, ..., m_k\\}$, we have\n$N_{\\mathcal{M}} = \\sum_{i=1}^{k} N_{m_i}$\nThus, $\\mathcal{M}$ can now be formally defined as\n$\\mathcal{M} = \\{m \\vert N_m > 0, \\forall m \\in \\mathcal{M}\\}$\nAdditionally, given its importance, we mark the count of all model names in a paper as\n$N := N_{\\mathcal{M}} = N_{\\mathcal{M}}$\nSimilarly for the other keyword set of general LM mentions, we denote the total count of all elements in $\\mathcal{L}$ as $N_{\\mathcal{L}}$. We mark $\\mathcal{L}$ as superscript for an explicit distinction with the $N_m$ family. The total counts $N_{\\mathcal{L}}$, $N$, and the $N_m$ family serve as essential cornerstones of our approach since they are direct indicators of how LMs are discussed and resolved. These patterns from independent works become the changing constituents of the Ship."}, {"title": "3.2.2 Constructing M from the text", "content": "To construct a comprehensive dictionary of specific model names, we established a human-AI workflow to extract and register model names at scale. We designed a detailed in-context prompt for a state-of- the-art LLM to detect model names from the title and abstract of papers. All detected names from the full dataset are collected and ranked by frequencies as candidates $[m_1, m_2, ...]$. Since the same type of model as referent can have various textual forms, we aim to maintain and distinguish two attributes (as lists) for a model $m$:\n\u2022 Aliases: Different text patterns that all refer to $m$; e.g., both \u201cchatgpt\u201d and \u201cChatGPT\" are identified separately but point to the same thing, and we need to count them together.\n\u2022 Variations: Refers to $m$, but is the extension of an existing alias (i.e. having an alias as substring). This usually suggests a specific variation of $m$, e.g., \u201cT5-3B\" when $m$ = \"T5\" (Raffel et al., 2020). Searching for \"T5\" in the text would have already included the mentions of \"T5-3B\".\nTo compose the final list, we sorted names following a simple heuristic (illustrated in Figure 1):\n1. When we encounter a new model $m$ not in $\\mathcal{M}$, we add $m$ to $\\mathcal{M}$ and initialize its alias list as $[m]$;\n2. Additional aliases of an entry $m_j$ are appended to its list;\n3. Variations of existing entries are recorded but not added as an alias;\n4. Candidates that are not the name of a model (e.g., BLEU) are discarded.\nFor each entry of $m$, we also manually retrieved the original paper or documentation to determine if there is an explicit dependency on another model. In all, $\\mathcal{M}$ has a total of 98 model entries and 146 aliases. With the two keyword lists, $\\mathcal{L}$ and $\\mathcal{M}$, we are ready to examine how LMs are resolved and extract diachronic patterns.\nOur dataset and code will be open-sourced, and we provide the full list of models involved and the LLM prompts in Appendix A.1 and A.2."}, {"title": "4 Experiments and Findings", "content": "LMs have been steadily gaining more attention from the field. Zhao et al. (2023) reports that papers containing the key phrase \u201cLanguage Model\u201d have increased from less than 400 pre-2019 to around 10,000 in 2023. We observe a similar trend with a finer-grained search and the focus on main conference publications in the NLP domain (Figure 2(a)). At ACL 2020, 35% of the papers contain at least one LM mention from $\\mathcal{L}$ (we refer to this portion as LM-related papers). Since then, this proportion has had a smooth, continuous growth of approximately 5% (additive) per conference, hitting 84% just three years later at EMNLP 2023.\n4.1 Wind in the Sails: Surging Mentions, Speeding Conclusions\nWe begin by querying a fundamental aspect of LMs' increasing popularity: Has our use of the term LM also evolved per se, apart from the background increase noted above? As one hypothesis, LMs' popularity might be attributed mainly to the increase of share. The types of work we do and the context of LMs may have not changed significantly \u2013 it's just more authors working on the topic, more resources put into it, or other external factors."}, {"title": "What about the actual models we use?", "content": "The super-linear increase of $N_{\\mathcal{L}}$ demands investigation into its likely causes. Authors might seek to cover more models in more detail, and their writing adapts to the strengthened claims, leading to the growth observed. Alternatively, authors might be more eager to employ trending terms even without significantly stronger evidence or fit to their work.\nTo this end, we compare how the distributions of $N_{\\mathcal{L}}$ and $N$ change over time in Figure 3. Each row represents a conference and the columns list all conferences which occur after it. Grid cells are pairs of conferences in comparison. We apply a Kolmogorov-Smirnov test (Massey Jr, 1951) to each pair to determine if there is a significant difference in their distributions. We also annotate their signed mean difference, where a positive number indicates an increased mean value from the row-conference to the succeeding column-conference. The grids are colored based on test significance level and sign of mean difference (note that all colors other than the lightest correspond to $p < 0.05$).\nFirst, we see evidence supporting our prior observations on the patterns of $N_{\\mathcal{L}}$. Earlier conferences form a cluster where no significant difference is noted; yet, starting 2022, every conference has a significantly higher $N_{\\mathcal{L}}$ than most or even all of its predecessors. However, the distributions of $N$ tell a distinct story. For most pairs, there is little or no evidence for a difference in distribution. There also isn't a similar line that divides the earlier and most recent conferences. For instance, the distribution of $N$ for EMNLP in 2023 does not have a significant difference with that in 2020, despite"}, {"title": "4.2 Oak, Pine, or Cedar Planks: Which models are we talking about?", "content": "With the exploding usage of the term LMs comes wider variation in the use cases and context around them. To go deeper, we must consider what writers refer to when they include LM in a paper (i.e., what the Ship is like at a certain point and how do they [re]build the ship, and not just whether it sails.) Based on all individual $N_m$ and the hierarchy of components, we obtain the exact number of the appearances of each model by matching their aliases in the text. Thus, we put together the collective compositions for each conference and visualize them as Sunburst charts, where the component sizes correspond to their share. We show a representative comparison of EMNLP 2020 and 2023 in Figure 4, and display full results in Appendix B.\nIn 2020, the BERT model alone makes up 41% of N, and 55% with its dependents (We refer to a model and all its dependents as a component to distinguish a group/family of models from the root model itself.) Other significant components include RNN (20%), CNN (6%), and GPT (5%). As for 2023, the GPT component (30%) takes the lead with the advent of the notable GPT-3 models (which formed 71% of all GPT mentions). BERT models are still the 2nd largest component despite a reduction to 25%. The results seems to suggest a less unipolar composition; in fact, the share of the BERT component in 2020 is comparable to the top two in 2023 combined. We also notice the rise of more recent components, including T5 (12%) and LLaMA (7%), while RNN (20% \u2192 2%) and CNN (6%\u21921%) saw the most significant decreases.\nHow much remains as the replacement of earlier components goes on? We calculate Jaccard similarity to quantify how much is shared between the composition of conferences, shown in Figure 5. We observe that Jaccard similarity between conferences monotonically decreases for subsequent conferences, which matches the Ship of LMs case where constituent parts change over time. For two consecutive conferences, the Jaccard similarity is usually only 71% to 85%; the index quickly drops"}, {"title": "One dominant model or many contributors?", "content": "We have seen the presence of major component models so far, and readers likely have their own tacit understandings of the \"giants\" in the field at the moment. Here, we emphasize the vast implications of the dominant referents of LMs. For example, if the supposedly abstract and inclusive concept of LM is implicitly equated with a certain model, we might be assigning the random, quirky traits of the model to the concept of LMs as a whole. This could unwittingly hinder the diversity, generalizability, and future usefulness of work despite a general veneer of neutrality among papers.\nTo portray how the giants shape our reported findings, we drill down to investigate their presence in individual papers. We examine the existence of an absolute majority model component in each paper that appears more than all other components combined, i.e., occupying more than N/2. One scenario, then, would be that a single or small set of giants actually underpin the notion of LMs in papers. On the other hand, if LM is truly a general term of art, then we might also see some but not most papers dominated by a model.\nFigure 6 displays the proportion of publications with absolute majority components for the full data (left) and the top 25% with the highest $N_{\\mathcal{L}}$ (right). The most notable components are marked with the same color as in Fig. 4. Other models are shown collectively as the grey bar (\"others\u201d), and the proportion with no single majority model is denoted with a striped pattern. We observe that around 80% of papers contain an absolute majority model. Specifically, we get a glance at the astounding traction of BERT before more recent paradigm shifts: it dominated up to 60.5% of all papers and 68.6% of the most LM-centered ones.\nInterestingly, more focus on the collective LM terms did not entail a more balanced composition. In fact, they are often more biased: The percentage of papers with an absolute majority is higher in the most LM-centered quartile than overall for all 7 conferences before EMNLP 2022. There has also been a fresh wind, however. In the most recent 3 conferences, we see fewer dominant components when a paper focuses heavily on the generalized LM terms. More importantly, the chance of an absolute majority in both groups has been on a continuing decline and both reached an unprecedentedly lower level: 70.5% for all papers and 67% for the top quarter. We call to keep monitoring the heterogeneity (or lack thereof) in LM papers given the (still) high presence of major components and a visibly surging presence of the GPT component at the most recent EMNLP 2023."}, {"title": "4.3 Lembos or Trireme: Factoring in context", "content": "The extent to which a paper focuses on LMs implies different use scenarios. For instance, a work may utilize and mention them for data processing but doesn't concern the science of LMs per se. This usually implies lower $N_{\\mathcal{L}}$ in contrast to another"}, {"title": "5 Concluding Remarks", "content": "In this work, we go over the past and present of the enduring term of Language Model(s), based on an original dataset from the latest major conferences. We sort out the subtle, continuous shifts in the practical meaning of LMs, and witness how the retrofits eventually accumulate to a brand new Ship of Language Models. We checked up the accelerated transmutation of our use of the term beyond the \"routine updates\" of the Ship, whereas actual referents did not sync with the pace. We quantify and visualize the drastic change in the planks and timber, emphasizing the shortened period of reconstruction and the presence of dominant components."}, {"title": "Limitations", "content": "While our methodologies can be naturally applied to similar scenarios, we would like to note that the current analyses and implementations have limitations. First, although we have gone over various findings in recent years with the most dramatic advances of LMS, a holistic overview of a research topic, let alone an entire field, is not covered by the scope of a few years. It would be meaningful if these most rapid changes could be connected with the decades of conceptualization and exploration preceding the engineering breakthroughs. Similarly, using main conference papers at the major"}, {"title": "Ethical Considerations", "content": "Our data is collected from the ACL Anthology on the terms of Creative Commons 4.0 BY (Attribution) license, which allows unlimited reproduction, distribution, and hosting of materials for non-commercial purposes4. The authors report no other potential ethical considerations."}, {"title": "A The dataset construction pipeline", "content": "We provide more details about the semi-automated dataset creation process: the full list of models involved, and the prompts used for the automated part of model name identification. The full dataset and more details will be published for future use in the final version of this project.\nA.1 Full list of models\nChatGPT, GPT-3, GPT-4, BERT, T5, GPT- 3.5, GPT-2, LLaMA, ROBERTa, PaLM, CLIP, BART, XLM-R, Alpaca, BLOOM, mT5, Instruct-GPT, mBERT, GPT-J, Flan-T5, OPT, Codex, COMET, ELECTRA, Longformer, mBART, Sim-CSE, BLOOMZ, BigBird, BLIP, DeBERTa, CodeT5, Switch Transformer, Vicuna, TO, PEGA- SUS, LSTM, ALBERT, DPR, Macaw, LXMERT, SpanBERT, TinyBERT, ViLBERT, TransE, Rotate,"}, {"title": "A.2 Prompts for automated model name extraction", "content": "We use GPT-4-turbo as our base LLM to identify potential names from paper abstracts and incorporate in-context examples (Dong et al., 2022). A request consists of two parts of inputs: a static System instruction, and individual User Inputs for each request. An example use case is shown below:\nSystem\nYou are an assistant with excellent expertise in searching through academic text. You will be given the Abstract of an academic paper in the field of Natural Language Processing. Your task is to retrieve whether the authors mention that they mentioned some *specific* language model in their writing. And if so, you need to accurately find the names of all such models.\nImportant note 1: \"LLM\" and \"PLM\" are not model names, they refer to the generic terms of \"Large Language Model\" and \"Pretrained Language Model\".\nImportant note 2: Do not include any models that are proposed by the authors themselves. For instance, if a paper says \"we propose a new model, GPT-OURNEW, which performs better than GPT- 3\", your answer should only include \"GPT-3\" and not \"GPT-OURNEW\".\nReturn all the specific model names (don't miss out any), separated by a comma. If you believe you didn't see any model name, simply return \"None\". Only respond with the comma-separated model names. Do not include any other text in your response!!!\nSome examples:"}]}