{"title": "SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering", "authors": ["Michael Iannelli", "Sneha Kuchipudi", "Vera Dvorak"], "abstract": "Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to generalize to new information by decoupling reasoning capabilities from static knowledge bases. Traditional RAG enhancements have explored vertical scaling-assigning subtasks to specialized modules and horizontal scaling-replicating tasks across multiple agents\u2014to improve performance. However, real-world applications impose diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements, involving trade-offs among objectives such as reducing cost, ensuring answer quality, and adhering to specific operational constraints.\nIn this work, we present a systems-oriented approach to multi-agent RAG tailored for real-world Question Answering (QA) applications. By integrating task-specific non-functional requirements - such as answer quality, cost, and latency - into the system, we enable dynamic reconfiguration to meet diverse SLAs. Our method maps these Service Level Objectives (SLOs) to system-level parameters, allowing the generation of optimal results within specified resource constraints.\nWe conduct a case study in the QA domain, demonstrating how dynamic re-orchestration of a multi-agent RAG system can effectively manage the trade-off between answer quality and cost. By adjusting the system based on query intent and operational conditions, we systematically balance performance and resource utilization. This approach allows the system to meet SLOs for various query types, showcasing its practicality for real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Question Answering (QA) has been a pivotal area of research since the 1960s, aiming to provide precise answers to natural language questions. Unlike traditional search engines that return a list of hyperlinks to relevant pages, QA systems deliver direct answers to user queries [30].\nThe advent of generative Large Language Models (LLMs) has led search engines to integrate abstractive QA into their offerings-systems that generate answers which may not exist verbatim in source documents [4]. Additionally, new platforms specializing in abstractive QA, such as Perplexity [20] and ChatGPT Search [19], have emerged.\nHowever, LLM-based abstractive QA exhibits notable limitations, including a lack of generalization to unseen data and a propensity for hallucination, i.e. the generation of false assertions [10]. Retrieval Augmented Generation (RAG) [8] addresses these shortcomings by combining LLMs with external knowledge sources, enabling the LLM to function as a reasoning layer rather than a static repository of facts. RAG retrieves context from external sources such as search engines, traditional databases, vector databases [11], or knowledge graphs [17, 28], and utilizes the LLM for reasoning and response generation.\nEnhancements to RAG-based QA have primarily focused on decomposing the task into smaller subtasks and distributing responsibilities among specialized components. While these techniques have improved answer quality, they often introduce additional costs in terms of computation or increased response times.\nReal-world QA systems must navigate trade-offs between answer quality and the costs associated with providing those answers. Moreover, they must be dynamically reconfigurable to handle diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements arising from various query intents and problem domains. For instance:\n\u2022 E-commerce Search: Highly sensitive to response times, where delays can significantly impact user experience and sales conversions [1, 25].\n\u2022 Customer Support: Requires not just correct answers but stylistically compliant ones that adhere to the brand-voice guidelines of the domain or company [18].\n\u2022 Legal and Healthcare Queries: Demand high answer quality due to the critical nature of the information, with less emphasis on cost or latency constraints [22].\nIn addition to these domain-specific requirements, real-world RAG systems operate under constantly changing and sometimes adverse conditions. Factors such as network congestion, computational capacity constraints, and security concerns necessitate adaptive strategies. For example, during periods of high network traffic, minimizing external API calls and leveraging local LLM instances can help maintain answer quality while adhering to operational constraints.\nBy incorporating knowledge about the task domain, QoS requirements, and the operational environment, we can quantify and manage the trade-offs between cost, answer quality, and other non-functional requirements. This enables the design of QA systems that are both efficient and effective across a range of real-world scenarios."}, {"title": "A. Our Contributions", "content": "In this paper, we present a novel system-theoretic framework for SLA management in reconfigurable multi-agent RAG systems, specifically tailored for question-answering applications. Our main contributions are as follows:\n\u2022 Dynamically Reconfigurable Horizontal Scaling Framework: We propose a method for horizontally scaling multi-agent RAG systems by replicating agents with diverse configurations. This approach systematically improves performance by enabling the system to dynamically adjust resource allocation based on the specific requirements of each query.\n\u2022 Implementation and Experimental Validation: We detail the architecture and implementation of our Abstractive Question Answering Intent Handler. Additionally, we present experimental results that demonstrate the effectiveness of our approach, including comparisons of strategies employed by different agent architectures.\n\u2022 Novel Metrics and Dataset Creation: We introduce novel metrics and a dataset creation process to address stylistic adherence requirements, which are critical for many industry users.\nBy addressing the challenges of SLA management in multi-agent RAG systems, our work contributes to the development of QA systems that are adaptable, efficient, and capable of delivering high-quality answers under diverse operational conditions."}, {"title": "B. Related Work", "content": "Scaling Retrieval-Augmented Generation (RAG) systems has been an active area of research, with most approaches focusing on vertical scaling-decomposing tasks into subtasks assigned to specialized components. Recent studies incorporate Large Language Models (LLMs) at the planning stages to mitigate issues like irrelevant retrieval and focus drift. For instance, PlanRAG uses an LLM to generate retrieval plans, enhancing context relevance [12]. Similarly, Retrieve-Plan-Generate (RPG) separates planning and answering stages to improve long-form question answering [14]. AutoGPT+P introduces planning loops for step-by-step plan generation in dynamic environments [2].\nHorizontal scaling through multi-agent systems has also been explored, where multiple agents are synchronously assigned to the same question. LLM-Debate employs multiple LLM agents debating to reach better answers [5]. Chain-of-Thought Self-Consistency (CoT-SC) generates multiple reasoning paths and selects the most consistent answer [27]. Li et al. demonstrate that increasing the number of agents with simple voting improves performance [13].\nTemporal scaling techniques involve sequential interactions with LLMs to enhance question-answering performance on complex queries. Shao et al. use iterative querying to refine answers for complex information needs [23]. Query refinement methods, such as RQ-RAG involve rewriting, decomposing, and disambiguating complex queries have also been shown to improve answer quality [3].\nWhile these aforementioned methods enhance answer quality, they often overlook dynamic adaptation to operational constraints critical in real-world applications with strict Service Level Agreements (SLAs).\nGao et al. introduce a modular RAG framework that encompasses horizontal, vertical, and temporal scaling techniques [9]. Zhang et al. propose an agentic information retrieval framework capable of handling complex queries by introducing IR agents with greater levels of autonomy [29]. However, these works lack experimental results on adapting systems to real-world environments.\nArbitration mechanisms have been proposed to improve decision-making in multi-agent systems. Some studies suggest using multiple LLMs to select the best response [26]. These methods improve answer quality but may not address trade-offs such as cost, latency, and security.\nEvaluation frameworks like RAGAS assess RAG performance but may omit industry-relevant metrics such as adherence to stylistic constraints [6]. In practical applications, an answer must be not only correct but also stylistically appropriate.\nOur work differs by focusing on the dynamic reconfiguration of multi-agent RAG systems to meet diverse SLAs in real-world settings. We integrate task-specific non-functional requirements into system parameters, addressing trade-offs between answer quality, cost, latency, and adherence to stylistic guidelines. This practical framework aims to deploy question-answering systems in industrial contexts with stringent SLA demands."}, {"title": "II. ARCHITECTURE", "content": "Our Question Answering System is designed to dynamically reconfigure its components to meet Service Level Agreements (SLAs). The system consists of three primary modules: an Intent Detection Module, a Planning Module, and multiple Intent Handlers. A high-level overview of the architecture is shown in Figure 1."}, {"title": "A. System Inputs and Outputs", "content": "User Query: The system accepts arbitrary input queries from users, such as: \"How do I reset my phone?\", \"How to reset phone\", or \"Burger joints near me\".\nThese queries vary significantly in intent and complexity, necessitating a robust mechanism for accurate intent recognition and appropriate handling."}, {"title": "Task SLA/QoS Parameters", "content": "Specific requirements such as:\n\u2022 Required precision and recall for the task\n\u2022 Maximum acceptable hallucination rate and incongruent response rate (to ensure adherence to stylistic constraints)\n\u2022 Budget and Latency constraints for different intent types\nOperational Environment Current conditions affecting the system, such as:\n\u2022 Availability of network resources\n\u2022 Status of external APIs and services\n\u2022 Available models and data sources\nThe QA system's outputs are Intent-Dependent Answers which consist of diverse responses tailored to the user's intent and complying with specified SLAs."}, {"title": "B. Intent Detection Module", "content": "The Intent Detection Module processes the user query to classify it into predefined intent categories, which may include:\n\u2022 Directly Answerable Question: Queries seeking specific information that can be directly addressed.\n\u2022 Request for Summarization: Queries asking to condense or summarize information from a source.\n\u2022 Non-Question Statements: Inputs that are not interrogative but may require action.\n\u2022 Request for a List: Queries seeking a collection of items (e.g., nearby restaurants serving hamburgers).\n\u2022 Sales Inquiry: Queries related to purchasing or product information.\nImplementation Options: Implementation options for this module include various techniques, each with trade-offs in terms of computational cost, extensibility, and accuracy:\n\u2022 Heuristic Methods: Rule-based systems using predefined patterns and keywords for quick intent classification. While low in computational overhead, they may lack flexibility and scalability.\n\u2022 Machine Learning Classifiers: Lightweight models trained on labeled datasets to predict intents, offering a balance between performance and resource utilization.\n\u2022 Generative Large Language Models (LLMs): Zero-shot or few-shot classifiers leveraging pre-trained LLMs to understand and classify intents without extensive domain-specific training data. Although highly extensible and accurate, they incur higher computational costs."}, {"title": "C. Planning Module", "content": "The Planning Module dynamically configures the resources of the Intent Handlers to meet the system's SLA requirements for each specific intent in the current environment. Its core functions include:\n\u2022 Resource Allocation: Determining the optimal number of Retrieval Augmented Generation (RAG) agents to deploy, balancing response quality against computational cost.\n\u2022 Backend Data Source Selection: Choosing appropriate data repositories (e.g., cached datasets, real-time databases, knowledge graphs) based on required information freshness and relevance.\n\u2022 Arbitration Mechanism Configuration: Selecting and parameterizing arbitration algorithms to aggregate and evaluate responses from multiple agents effectively.\nFor example, to service a query requiring high answer quality, the Planning Module may:\n\u2022 Increase Agent Replication: Deploy more RAG agents to generate diverse candidate answers.\n\u2022 Employ Appropriate Data Sources: Access multiple data repositories to enrich the context and determine which agents should have access to which sources e.g. an agent using a third-party LLM would not get access to sensitive documents.\n\u2022 Select what threshold to use in a voting-based arbitration mechanism e.g. only return an answer when 70% of the agents think it should be answered.\nBy adjusting these configurations in real-time, the Planning Module ensures compliance with SLA parameters, optimizing the system's performance in terms of answer quality and resource utilization."}, {"title": "D. Intent Handlers", "content": "The Intent Handler is a reconfigurable module that executes the necessary processes to address the user's query based on the classified intent. By dynamically adjusting its internal workflows and resource utilization in response to directives from the Planning Module, the Intent Handler can:\n\u2022 Manage Computational Costs: Allocate resources judiciously to stay within budgetary constraints without significantly compromising answer quality.\n\u2022 Enhance Answer Quality: Allocate additional resources, such as more RAG agents or advanced arbitration mechanisms, for queries where accuracy is paramount.\n\u2022 Ensure Compliance with Non-Functional Requirements: Adjust processing to adhere to stylistic guidelines, security policies, and other non-functional requirements specified in the SLAs.\nTo achieve these objectives, the Intent Handler orchestrates a network of logical entities and services, which may include:\n\u2022 Backend Data Access Interfaces: APIs and connectors to interact with various data sources."}, {"title": "Answer Provision", "content": "\u2022 Answer Provided: An answer was returned.\n\u2022 No Answer Provided: The returned answer was null."}, {"title": "Answer Quality", "content": "\u2022 Correct Answer: The returned answer was correct and aligned with the task's non-functional requirements (e.g., adhered to the specified style in the prompt).\n\u2022 Hallucination: The returned answer was not grounded in the provided context.\n\u2022 Incongruent Response: The returned answer did not align with the task's non-functional requirements."}, {"title": "B. Metrics", "content": "1) Answer Quality Metrics: We evaluated the performance of the agents using the following metrics:\n\u2022 Precision: The number of correct answers divided by the total number of answers provided.\n\u2022 Recall: The number of correct answers divided by the number of times the answer exists in the global context (the union of all contexts from all agents).\n\u2022 F1 Score: The harmonic mean of precision and recall.\n$F\u2081 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n\u2022 Hallucination Rate: The number of hallucinations divided by the total number of answers provided, where a hallucination is any answer not grounded in the context.\n\u2022 Incongruent Response Rate: The number of incongruent responses divided by the total number of responses.\nPrecision, Recall, and F1: In traditional information retrieval, precision is defined as the proportion of retrieved documents that are relevant, and recall is the proportion of relevant documents that are retrieved [16]. In our question answering domain, we adapted these definitions to suit our specific context.\nIncongruent Responses: For many users of this system, particularly brands, it is insufficient to simply return a correct answer; the answer must also adhere to the non-functional and stylistic requirements specified in the prompt. This includes guidelines such as not breaking the fourth wall or referencing the context directly. Ensuring compliance with these requirements is essential for maintaining brand voice and user experience."}, {"title": "2) System Cost", "content": "The expected cost of our QA system, as used in our experiments, is calculated as the sum of the fixed system overhead and the costs associated with QA agents and arbitration, aggregated across all intents. The formula is:\n$C_{sys} = C_{overhead} + \\sum_{intent=1}^{I} (C_{agent} N_{intent} + C_{arbitration} (N_{intent}))$\nWhere:\n\u2022 $C_{sys}$: Total expected system cost.\n\u2022 $C_{overhead}$: Fixed system overhead cost.\n\u2022 $C_{agent}$: Cost per QA agent.\n\u2022 $N_{intent}$: Number of agents for a specific intent.\n\u2022 $C_{arbitration} (N_{intent})$: Arbitration cost for $N_{intent}$ agents.\n\u2022 I: Total number of intents.\nThis equation models the specific cost structure of our QA system, where both the agent and arbitration costs are functions of the number of agents configured for each intent."}, {"title": "3) System Latency", "content": "The system latency for our implementation is defined as the sum of the fixed overhead latency, the maximum latency of the QA agents configured for a given intent (since agent operations are asynchronous), and the arbitration latency for those agents. Mathematically, this is expressed as:\n$L_{sys} = L_{overhead} + \\underset{i\\in agents}{\\text{max}} (L_{QA,i}) + L_{arbitration} (N_{intent})$\nWhere:\n\u2022 $L_{sys}$: Total system latency.\n\u2022 $L_{overhead}$: Fixed overhead latency.\n\u2022 $L_{QA,i}$: Latency of the i-th QA agent.\n\u2022 $\\underset{i\\in agents}{\\text{max}} (L_{qa,i})$: Maximum latency among all QA agents included in the intent service.\n\u2022 $L_{arbitration} (N_{intent})$: Arbitration latency for $N_{intent}$ agents.\nThis formula reflects the asynchronous nature of the QA agent operations and the specific latency structure of our experimental implementation."}, {"title": "IV. AGENT IMPLEMENTATION", "content": "In this section, we describe the experiments conducted to evaluate our reconfigurable multi-agent RAG system for question answering. We detail the agent architecture, single-agent experiments, and the arbitration algorithm, including its implementation and ablation studies."}, {"title": "A. Agent Architecture", "content": "The architecture of an individual QA agent used in our experiments, shown in Figure 3, is designed to be flexible and configurable based on the output of the Planning Module. The specific parameterization of each agent depends on factors such as which backend data sources are provisioned and caching strategies. While the implementation details of the Planning Module are outside the scope of this paper, we provide an overview of the QA agent's architecture and workflow."}, {"title": "B. Retrieval Module Details", "content": "The Retrieval Module, shown in Figure 4, pulls data from various backend sources, including vector databases, lexical search engines, and graph data stores, depending on the domain. For vector search, we employ the following components:\n\u2022 Embedding Model: Documents are embedded using a transformer model based on MPNet [24]. Embeddings capture semantic information for effective similarity comparisons.\n\u2022 Vector Database: Indexed using Hierarchical Navigable Small World (HNSW) graphs [15] for efficient approximate nearest neighbor search. At query time, the same embedding model is used to encode the user query. The resulting query vector q is used to retrieve documents whose vectors d are similar, based on cosine similarity:\n$similarity(q, d) = \\frac{q \\cdot d}{||q|| ||d||}$\nDocuments with higher cosine similarity scores are considered more relevant to the query."}, {"title": "C. Prompt Engineering", "content": "After re-ranking and/or thresholding, we construct the prompt for the LLM. The preprocessed list of context documents and the original user query are interpolated into the prompt at appropriate places. An example prompt is shown in Figure 5:"}, {"title": "V. EXPERIMENTS", "content": "We conducted experiments on individual QA agents by varying the post-retrieval preprocessing logic used to prepare the input prompts for the GPT-4-powered Reasoning Module. Given the LLM's context window limitation of 8,000 tokens, we needed to prune the retrieved context appropriately. We developed several pruning methods aiming to balance content relevance with the token limit. For each agent, we measured Answer Quality using the metrics described in Section III-B1.\nFor the multi-agent experiments, we implemented an arbitration algorithm based on majority voting and relevance ranking. We varied the size of the agent ensemble dedicated to servicing the QA queries by adjusting the number of QA agents each employing diverse post-retrieval preprocessing strategies included in the ensemble. The final answer was selected based on the arbitration algorithm. Similar to the single-agent experiments, we measured Answer Quality and evaluated the system cost relative to the number of agents in the ensemble."}, {"title": "A. Preprocessing Methods", "content": "We implemented two main types of preprocessing logic:\n\u2022 Thresholding:\nThresholding (Control): A naive truncation strategy where the first 8,000 tokens from the search results are taken."}, {"title": "B. Arbitration", "content": "In our multi-agent RAG system, we employ an arbitration algorithm to select the optimal response from a set of candidate answers generated by various QA agents. The arbitration process is governed by a decision threshold $T \\in (0,1)$ and an arbitration function A that maps a set of results to a single final answer. The algorithm is defined in Algorithm 1\nFor our experiments, the arbitration function A was implemented using a cross-encoder model, consistent with the post-retrieval preprocessing methods described earlier. The cross-encoder jointly encodes each candidate answer with the original query to compute a relevance score. It effectively re-ranks the affirmative responses based on their contextual relevance and quality, selecting the top-ranked answer as the final output."}, {"title": "C. Algorithm Ablations", "content": "To evaluate the effectiveness of our arbitration algorithm and understand the impact of its components, we conducted a series of ablation experiments:\n\u2022 Random Candidate Selection:\nMethod: Select a candidate answer at random from the set of affirmative responses, bypassing the arbitration function.\nPurpose: Establish a baseline to assess the added value of the arbitration function in selecting higher-quality answers compared to random selection.\n\u2022 Voting Procedure with Random Positive Selection:\nMethod: Apply the threshold evaluation to determine if sufficient affirmative responses exist. If so, randomly select one of the affirmative answers without applying A.\nPurpose: Isolate the effect of the threshold-based decision while assessing the necessity of the arbitration function for answer selection.\n\u2022 Cross-Encoder Arbitration on All Candidate Responses:"}, {"title": "VI. RESULTS", "content": "In this section, we present the results of our experiments, focusing on both single-agent ablations and multi-agent configurations. Our goal is to evaluate how different preprocessing strategies and arbitration mechanisms affect the performance of the QA system, particularly in terms of precision, recall, hallucination rate, and incongruent response rate."}, {"title": "A. Single-Agent Experiments", "content": "The results of the single-agent experiments are summarized in the upper section of Table I. These results encompass the three preprocessing strategies discussed in the Single Agent Experiments section"}, {"title": "Key Findings", "content": "\u2022 Aggressive Thresholding emerged as the best-performing strategy, achieving the highest precision and recall, as well as the lowest incongruent response rate.\n\u2022 Vertical Thresholding performed comparably to aggressive thresholding, indicating that focusing on top-ranked verticals effectively preserves relevant context.\n\u2022 Agents utilizing Re-ranking with Cross-Encoder exhibited lower precision and recall. The cross-encoder tended to eliminate important context documents, leading to reduced precision despite a lower hallucination rate."}, {"title": "B. Multi-Agent Experiments", "content": "The multi-agent experiments were designed to evaluate the impact of ensemble configurations on system performance. We varied the number of agents in the ensemble and observed the effects of different arbitration mechanisms, all of which used a threshold $T = 0.5$, requiring at least half of the agents to provide an affirmative answer for a response to be returned. The results can be found in the bottom section of Table I. Detailed charts are shown in Figures 6 - 10."}, {"title": "Key Findings", "content": "\u2022 Increasing the number of agents consistently improved performance, enhancing precision and recall while reducing the hallucination and incongruent response rates. This highlights the benefit of aggregating responses across agents.\n\u2022 Balancing performance improvements with computational costs is crucial. Larger ensembles offer better performance metrics but come at the expense of increased resource usage and latency.\n\u2022 Simple voting mechanisms often performed on par with relevance-based arbitration. While the latter showed predictable improvements with more agents, the additional gains were minimal compared to the increased computational overhead."}, {"title": "VII. DISCUSSION", "content": "Our experiments demonstrate the effectiveness of dynamically reconfigurable multi-agent RAG systems in managing trade-offs between answer quality and resource utilization. Insights Effectiveness of Aggressive Thresholding: By aggressively limiting the context to the most relevant information, we reduce noise and enhance the LLM's ability to generate accurate answers. Limitations of Re-ranking Alone: While re-ranking aims to prioritize relevant documents, it can inadvertently discard essential context, adversely affecting performance. Scalability of Multi-Agent Systems: Adding more agents generally improves performance but increases cost and latency. Systems must be designed to adjust dynamically based on SLA requirements."}, {"title": "A. Future Work", "content": "\u2022 Exploration of Additional Arbitration Mechanisms: Investigate other methods, such as weighted voting or machine learning-based arbitration, to enhance decision-making.\n\u2022 Incorporation of QoS Parameters: Extend the framework to include latency and computational constraints explicitly, enabling more granular SLA management.\n\u2022 Complexity of Planning Mechanisms: Develop more sophisticated Planning Modules capable of temporal replication (e.g., handling requests for refinement) and adjusting strategies in real-time.\n\u2022 Simulation of Real-World Conditions: Incorporate factors like network congestion and varying computational capacities to assess the system's robustness under diverse operational environments.\n\u2022 Threshold Parameter Optimization: Study the impact of different threshold values in arbitration mechanisms to find optimal settings for various use cases.\nOur case study illustrates that a systems-oriented approach to SLA management in reconfigurable multi-agent RAG systems is both feasible and beneficial. By dynamically adjusting system parameters based on non-functional requirements, we can effectively balance answer quality with resource constraints. This work serves as a foundational step toward deploying RAG-based QA systems in real-world applications with diverse and stringent SLA demands."}]}