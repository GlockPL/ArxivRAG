{"title": "FAIRNESS ISSUES AND MITIGATIONS IN (DIFFERENTIALLY PRIVATE) SOCIO-DEMOGRAPHIC DATA PROCESSES", "authors": ["Joonhyuk Ko", "Juba Ziani", "Saswat Das", "Matt Williams", "Ferdinando Fioretto"], "abstract": "Statistical agencies rely on sampling techniques to collect socio-demographic data crucial for policy-making and resource allocation. This paper shows that surveys of important societal relevance introduce sampling errors that unevenly impact group-level estimates, thereby compromising fairness in downstream decisions. To address these issues, this paper introduces an optimization approach modeled on real-world survey design processes, ensuring sampling costs are optimized while maintaining error margins within prescribed tolerances. Additionally, privacy-preserving methods used to determine sampling rates can further impact these fairness issues. The paper explores the impact of differential privacy on the statistics informing the sampling process, revealing a surprising effect: not only the expected negative effect from the addition of noise for differential privacy is negligible, but also this privacy noise can in fact reduce unfairness as it positively biases smaller counts. These findings are validated over an extensive analysis using datasets commonly applied in census statistics.", "sections": [{"title": "1 Introduction", "content": "Statistical agencies across various countries gather, anonymize, and disseminate socio-demographic data, which is foundational to high-impact applications such as policy development, urban planning, and public health initiatives [4, 14, 16]. In the United States, for instance, this data underpins the allocation of a significant portion of the $1.5 trillion federal budget, with approximately $665 billion dedicated annually to social security programs that elevate millions of families above the poverty line [9, 12]. Major surveys such as the American Community Survey (ACS) [18], the Current Population Survey (CPS) [15], and the National Health Interview Survey (NHIS) [2] are central to gathering essential demographic data. The ACS, for example, annually collects data from approximately 3.54 million housing unit addresses across the United States (about a 1% sample of the entire U.S. population). This sample-based approach allows the ACS to provide detailed insights into the population's living conditions, educational attainment, employment, and health status, among other factors. The accuracy of these demographic reports is thus crucial to ensure that resources and policy measures are effectively targeted toward appropriate population segments. However, despite their critical role, the collection of these statistics typically involves surveying a small fraction of the population, inherently introducing sampling errors. While these surveys strive to provide estimates with controlled error rates and confidence intervals, such control is typically applied across the entire survey population. A key finding of this paper is to demonstrate how such an approach can lead to varying error rates among population groups, particularly those distinguished by ethnicity, introducing biases in critical downstream tasks relying on this data.\nTherefore, the first major contribution of this paper is to address the need for developing sampling schemes that not only aim to reduce costs but also meet acceptable errors within each demographic group. The approach explored"}, {"title": "2 Preliminaries and Goals", "content": "This paper considers a target population, such as the U.S. population, segmented into G distinct groups characterized by race, socio-economic status, and other demographic factors. Let N represent the total population size, with \\(N_i\\) indicating the size of each group \\(i \\in [G]\\). We examine the population statistics \\(\\theta(N)\\), such as average income or poverty levels, and aim to estimate these via subsampling. The subsample, of size n where \\(n < N\\), is used to derive the estimates \\(\\hat{\\theta}(n)\\). In particular, this analysis extends to group-specific statistics, where \\(\\hat{\\theta}(n_i)\\) represents estimates from a"}, {"title": "Accuracy and fairness.", "content": "The accuracy of these estimates is evaluated through their error and variance, defined for group i as \\(Err(\\hat{\\theta}_i) = |\\hat{\\theta}_i - \\theta_i|\\), and \\(Var(\\hat{\\theta}_i) = E[\\hat{\\theta}_i^2] - (E[\\hat{\\theta}_i])^2\\), respectively. The primary goal is to devise sampling strategies that minimize the sampling cost\u2014defined in subsequent sections\u2014while ensuring that the probability of an estimator's error exceeding a certain threshold \\((\\gamma_i)\\) for each group i:\n\\[Pr(|Err(\\hat{\\theta}_i)| > \\gamma_i) \\leq \\alpha, \\forall i \\in [G],\\]\nremains less than \\(\\alpha\\).\nUnfairness in this context is quantified by the maximum discrepancy in estimator's variances between any two groups,\n\\[\\xi_{var} = \\max_{i,j \\in G} |Var(\\hat{\\theta}_i) - Var(\\hat{\\theta}_j)|,\\]\nsince the goal of the survey process is controlling confidence intervals across various populations."}, {"title": "Differential privacy.", "content": "Differential Privacy (DP) [3] is a rigorous privacy notion that characterizes the amount of information of an individual's data being disclosed in a computation. Formally, a randomized mechanism \\(\\mathcal{M} : \\mathcal{X} \\rightarrow \\mathcal{R}\\) with domain \\(\\mathcal{X}\\) and range \\(\\mathcal{R}\\) satisfies \\((\\epsilon, \\delta)\\)-differential privacy if for any output \\(O \\subseteq \\mathcal{R}\\) and datasets \\(x, x' \\in \\mathcal{X}\\) differing by at most one entry (written \\(x \\sim x'\\)),\n\\[Pr[\\mathcal{M}(x) \\in O] \\leq exp(\\epsilon) Pr[\\mathcal{M}(x') \\in O] + \\delta.\\]\nIntuitively, DP states that specific outputs to a query are returned with a similar probability regardless of whether the data of any individual is included in the dataset. Parameter \\(\\epsilon > 0\\) describes the privacy loss of the mechanism, with values close to 0 denoting strong privacy. When \\(\\delta=0\\), mechanism \\(\\mathcal{M}\\) is said to achieve \\(\\epsilon\\)- or pure-DP.\nA function f from a dataset \\(x \\in \\mathcal{X}\\) to an output set \\(O \\subset \\mathbb{R}^n\\) can be made differentially private by injecting random noise onto its output. The amount of noise relies on the notion of global sensitivity \\(\\triangle f = \\max_{x \\sim x'} || f (x) - f(x')||_p\\), for \\(p \\in \\{1,2\\}\\). In particular, the Laplace mechanism for histogram data release (sensitivity \\(\\triangle f = 1\\)), defined by \\(\\mathcal{M}_{Lap}(x) = x + Lap(1/\\epsilon)\\), where \\(Lap(\\eta)\\) is the Laplace distribution centered at 0 and with scaling factor \\(\\eta\\), satisfies \\((\\epsilon, 0)\\)-DP.\nPost-processing. DP satisfies several important properties. Notably, post-processing immunity ensures that privacy guarantees are preserved by arbitrary post-processing steps. More formally, let \\(\\mathcal{M}\\) be an \\((\\epsilon, \\delta)\\)-DP mechanism and g be an arbitrary mapping from the set of possible output sequences to an arbitrary set. Then, \\(g\\circ \\mathcal{M}\\) is \\((\\varepsilon, \\delta)\\)-differentially private."}, {"title": "3 Real-world Impact: The ACS case", "content": "Next, the paper looks at the implications of sampling strategies in the American Community Survey (ACS), the largest sampling effort in the U.S. carried out by the U.S. Census Bureau. The ACS samples approximately 3.54 million housing unit addresses annually, representing about 1% of the U.S. population, and of those approximately 1.98 million result in successful samples [17]. This relatively small sample size introduces inherent uncertainties, termed sampling errors, which are critical in understanding the limitations and accuracies of the data collected. The Census Bureau addresses these uncertainties by calculating standard errors and publishing margins of error at a 90 percent confidence level.\nThe sampling process in data collection efforts such as the ACS introduces at least two fairness issues: disparate error rates across different populations and disparate impact of privacy-preserving mechanisms on sampling errors."}, {"title": "4 The Optimal Sampling Design Problem", "content": "The proposed approach casts the sampling process for target estimation as an optimization process, going beyond typical cost minimization of large-scale surveys, to ensure that error rates within each sub-population are met with high probability. This section first outlines the optimization problem, considering real-world survey constraints, and then quantifies the error of estimators for each sub-population, enabling efficient implementation of the optimization model."}, {"title": "4.1 Modeling real-world sampling processes", "content": "Large survey processes typically involve two phases. The first phase adopts various remote data collection modes; for example, the ACS has used internet interviews since 2013 and computer-assisted phone interviewing until 2017 [13]. The second phase relies on in-person, door-to-door interviews, requiring the physical allocation of survey workers. Although more expensive, this phase aims to improve data completeness and reliability, especially in environments where remote methods are less effective.\nThe efficacy of each phase is distinguished by distinct failure rates\u2013 the likelihood that an individual, once contacted, does not contribute data. These rates are denoted as \\(F^1_i\\) and \\(F^2_i\\) for the first and second phases respectively and vary across different population segments i. The costs associated with each contact attempt are denoted by \\(c_1\\) and \\(c_2\\) for the first and second phases, respectively. Typically, the cost-efficiency trade-off is clear: remote methods (Phase 1) are cheaper but often less effective (\\(F^1_i > F^2_i\\)), while in-person interventions (Phase 2) yield higher success rates at a higher cost (\\(C_1 < C_2\\)). We define \\(g^r\\) as the targeted or feasible sampling rate in region r once selected for phase 2. Further, \\(\\forall i\\) represents the upper bound of acceptable error for population segment i (i.e., error rates depicted by the red dotted lines in Figure 2), and \\(\\alpha\\) as the probability that this limit is exceeded. We define the following program to"}, {"title": "4.2 Tractable Error Quantification", "content": "A key challenge with solving Program (2) is Constraint (2c), which involves a probability estimation. The lack of a closed-form expression for this probability hinders the direct integration of this constraint into the optimization. To address this, this section provides a tractable upper bound to be used in place of the probability in Constraint (2c).\nNote that, using Chebyshev's inequality, the probability of the estimator's error exceeding \\(\\gamma_i\\) is bounded above by:\n\\[Pr(|Err(\\hat{\\theta}_i)| > \\gamma_i) = Pr(|\\hat{\\theta}_i - \\theta_i| > \\gamma_i) \\leq \\frac{\\sigma^2(\\hat{\\theta}_i)}{\\gamma_i^2},\\]\nwhere \\(\\sigma^2(\\hat{\\theta}_i)\\) represents the variance of the estimator \\(\\hat{\\theta}_i\\). This variance can then be estimated empirically, as done in practice [13], using prior data releases. This creates a statistical proxy, which is discussed in the Section 4.3.\nFor a given confidence level \\(\\alpha\\), from (3), we can replace Constraint (2c) by the stronger constraint \\(\\frac{\\sigma^2(\\hat{\\theta}_i)}{\\gamma_i^2} < \\alpha\\), and obtain a closed-form approximation for the threshold \\(\\gamma_i\\) as:\n\\[\\sigma^2(\\hat{\\theta}_i) \\leq \\alpha \\gamma_i^2.\\]\nThis new constraint strengthens the program by enforcing \\(Pr(|Err(\\hat{\\theta}_i)| > \\gamma_i) < \\frac{\\sigma^2(\\hat{\\theta}_i)}{\\gamma_i^2} \\leq \\alpha\\), which restricts the likelihood that the error in group i exceeds the desired \\(\\gamma_i\\) threshold, thus tightening the optimization. The variance of the estimator \\(\\sigma^2(\\hat{\\theta}_i)\\) can thus be expressed as\n\\[\\sigma^2(\\hat{\\theta}_i) = \\frac{C_i}{n_i},\\]\nwhere \\(C_i\\) is a constant that depends on the variance for group i. This follows from the variance of the estimator \\(\\sigma^2(\\hat{\\theta}_i)\\) being inversely proportional to the sample size \\(n_i\\). Thus, by substituting this expression into Equation (4), constraint (2c) can be replaced by the following tractable form:\n\\[n_i \\geq \\frac{C_i}{\\alpha \\gamma_i^2} \\qquad \\forall i \\in [G].\\]"}, {"title": "5 Private Sampling Scheme", "content": "The sampling design discussed above assumes accurate knowledge of population sizes; however, the confidentiality of collected micro-data is often legally mandated. For example, it is regulated by Title 13 [8] in the U.S., and, to comply with it, the U.S. Census Bureau used differential privacy for their 2020 decennial census release [1]. However, the use of DP mechanisms introduces perturbations in the data that may disproportionately affect smaller populations [12, 19]. This section studies how privacy-protected statistics could influence fairness in data collection.\nMore precisely, we consider population sizes \\(N_i\\) released differentially-privately for each group \\(i \\in [G]\\) and region \\(r \\in R\\), emulating the census data release. Therefore, instead of having access to the exact \\(N_i\\), the survey designer only has access to imperfect, noisy estimates given by:\n\\[\\tilde{N}_i = \\max (0, N_i + Lap(\\triangle x/\\epsilon)),\\]\nwhere \\(\\triangle x = 1\\) (sensitivity of the count query). We further note that the noisy counts are post-processed to ensure non-negativity (as is done in the U.S. Census [11]), here using the \\(\\max(0, .)\\) operator.\nThe challenge in this context is that noise can distort estimates of population sizes \\(N_i\\), influencing the number of individuals \\(n_i\\) who respond to the survey in each group i. This distortion affects errors in Constraint (2b) and compromises achieving the desired error targets and confidence levels. This section outlines our second key contribution: we offer theoretical insights into the biases introduced by using \\(\\tilde{N}_i\\) instead of \\(N_i\\), while Section 6 will offer a practical analysis of these impacts. Our main result is a closed-form expression for the bias of the estimate \\(\\tilde{N}_i\\), showing that this bias is invariably positive."}, {"title": "Theorem 1.", "content": "For all \\(i \\in [G]\\), \\(r \\in R\\), the bias of estimate \\(\\tilde{N}_i\\) is given in closed-form by:\n\\[B(\\tilde{N}_i) = E[\\tilde{N}_i] - N_i = \\frac{\\triangle x}{2 \\epsilon} exp\\left(-\\frac{N_i \\epsilon}{\\triangle x}\\right) > 0.\\]"}, {"title": "Proof of Theorem 1.", "content": "Let \\(f(z) = \\frac{1}{2 \\epsilon} exp(-\\frac{|z - N_i|}{b})\\) be the pdf of the Laplace centered around \\(N_i\\) with \\(b = \\triangle x/\\epsilon\\).\nThe expectation of the post processed count is then given by:\n\\[E[\\tilde{N}_i] = \\int_{-\\infty}^{\\infty} max(0, z) f(z) dz\\]\n\\[ = \\int_{-\\infty}^{0} 0 dz + \\int_{0}^{N_i} z f(z) dz + \\int_{N_i}^{\\infty} z f(z) dz.\\]\nThe following computes separately the three terms in Equation (8):\n\\[\\int_{-\\infty}^{0} 0 dz = 0\\]\n\\[\\int_{0}^{N_i} z f(z) dz = \\frac{1}{2} (N_i - b) - \\frac{1}{2} (N_i - b) exp(-\\frac{N_i}{b})\\]\n\\[\\int_{N_i}^{\\infty} z f(z) dz = \\frac{1}{2} (N_i + b).\\]\nCombining equations (9) - (11) with (8) gives:\n\\[E[\\tilde{N}_i] = N_i + \\frac{b}{2} e^{-N_i/b}.\\]\nThen, the bias on \\(\\tilde{N}_i\\) is\n\\[B(\\tilde{N}_i) = E[\\tilde{N}_i] - N_i = \\frac{b}{2} e^{-N_i/b} > 0.\\]"}, {"title": "Corollary 1.", "content": "The bias of the aggregated counts for each subgroup on the state level is\n\\[B(\\tilde{N}) = E[\\tilde{N}] - N_i = \\sum_{r \\in [R]} \\frac{\\triangle x}{2 \\epsilon} exp \\left(-\\frac{N_i \\epsilon}{\\triangle x}\\right) > 0.\\]"}, {"title": "Proof of Corollary 1.", "content": "The expectation of the post-processed count on the state level is given by:\n\\[E[\\tilde{N}] = E\\left[\\sum_{r \\in [R]} \\tilde{N}_i\\right]\\]\n\\[ = \\sum_{r \\in [R]} E \\left[\\tilde{N}_i\\right] \\quad (Linearity \\: of \\: Expectation)\\]\n\\[ = \\sum_{r \\in [R]} \\left(N_i + \\frac{b}{2} e^{-N_i/b} \\right) \\quad (Theorem \\: 1)\\]\n\\[ = N_i + \\sum_{r \\in [R]} \\frac{b}{2} e^{-N_i/b}.\\]"}, {"title": "6 Experimental Results", "content": "Next, the paper provides empirical evidence for the efficacy of the proposed optimization method on real-world data and settings first without and then with privacy considerations at hand. The experiments examine survey costs, group fairness, and utility offered by the proposed fairness-aware method.\nDatasets and settings. The experiments use ACS data from IPUMS [10] for 2021 and 2022, leveraging 2021 data for estimating the various \\(N_i\\) and 2022 data as ground truth for sampling and assessing target estimators. We divide geographical units based on Census Tract-level data, each containing about 4,000 individuals (data processing and setup details are in Appendix A). The focus is on estimating annual total pre-tax personal income across different ethnic and educational groups as defined by IPUMS and the Census Bureau. We focus on Connecticut as the primary state for analysis here and relegate additional comprehensive results for other states and demographic breakdowns by education levels to Appendix B and C, respectively.\nAlgorithms. This analysis evaluates various survey allocation mechanisms, comparing their efficiency, fairness, and effectiveness in achieving desired confidence levels, not only at the entire state level but also at the sub-population levels:\n\u2022 Standard Allocation: This baseline method, also known as proportional stratified random sampling, allocates surveys to each population group i in proportion to their size. This approach is a stronger baseline than simple random sampling for two key reasons: it provides more precise population estimates by reducing variance within each subgroup, and it ensures that the sample proportions are representative of the overall population [7].\n\u2022 Optimization: Phase 1 Only: This variant applies the optimization from Program (2), assuming the survey is conducted by only using the first phase. More concretely, the program excludes the 2nd phase components in (2a) and (2b) (for additional details see Appendix D). This model mirrors the proportional stratified random sampling by optimizing survey numbers within a single operational phase.\n\u2022 Optimization: Phase 1 and 2: This approach uses both phases as outlined in optimization (2), aligning closely with practical survey methodologies. Note that the choice of failure rates and costs influences the optimization outcomes. In particular, high failure rates (\\(F^1_i\\) and \\(F^2_i\\)) or low error tolerances (\\(\u03b1\\) and \\(\u03b3\\)) increase the total survey cost due to more failures and tighter constraints. Details on the effects of varying these parameters are discussed in a comprehensive ablation study in Appendix E. The default confidence constraints are set at \\(\u03b1 = 0.1\\) and \\(\\gamma_i = 10\\%\\) of the mean income for each subgroup i. Default failure rates are \\(F^1_i = 0.60\\) and \\(F^2_i = 0.20 \\,\\forall i \\in [G]\\), and the cost of surveying a region in phase 2 is set to be 500 times more expensive than the cost of reaching out to an individual with phone calls in phase 1. Finally, the sampling rates for geographies are set as \\(g^r \\approx 0.1 \\,\\forall r \\in R\\). This translates to sampling 400 people per selected region.\nEvaluation metrics. The evaluation of these mechanisms focuses on three primary metrics:\n1. Survey cost: Measured as a percentage of the cost reference used by the standard allocation.\n2. Fairness of variance: Assesses the equitable distribution of survey errors across different groups.\n3. Confidence compliance: Evaluates the ability to meet the prescribed confidence errors (\\(\\gamma_i\\)) at a 10% threshold, which aligns with the current standards of the ACS [13], and setting \\(\u03b1 = 0.1\\)."}, {"title": "6.1 Optimized sampling: Errors and Fairness", "content": "We start by assessing the performance of two variants of our method (\"Phase 1 Only\u201d vs. \u201cPhase 1 and 2\") against the standard allocation mechanism, without DP considerations.\nThe results, summarized in Figure 4, show that the Standard Allocation method yields the lowest variance of error when estimating the overall population's income. However, this method disproportionately affects minorities, who receive fewer surveys and experience a higher variance of error at the group level. This discrepancy results in the worst fairness of variance (\\(\\bar{\\xi}_{var}\\)) observed (refer to table under Figure 4), and minority groups even fail to meet the confidence constraints set for their estimations!\nIn contrast, the Phase 1 Only optimization approach achieves a more uniform error variance across all subgroups while using the same budget used in the Standard Allocation method. Inspecting the optimization solutions, it can be observed that equity is achieved by allocating a similar number of surveys to each subgroup, irrespective of their population size. Figure 5 reports the number of survey allocations by race and by each method, and provides a clear view of the nature of the disparities. This redistribution significantly lowers the variance of the errors for minorities (including Native, Black, Asian, Other, and 2+), while slightly increasing it for the majority (White). Importantly, this approach enhances fairness and ensures all groups meet the confidence thresholds, addressing the main drawback of the Standard Allocation. In Section 7, we provide further discussion on why distributing surveys equally across subgroups is an effective alternative.\nNext, we focus on our main approach. As discussed in Section 4, phase 2 is characterized by a higher success rate (\\(F^2_i > F^1_i\\)) at a greater cost (\\(c_1 < c_2\\)). Despite its higher per-survey cost, phase 2's low failure rate results in a higher number of successful samples for the same overall cost, making the Phase 1 and 2 method substantially cheaper (86% of Phase 1 Only cost) (see table under Figure 4). However, once regions are selected for phase 2, simple random sampling is executed at a 10% rate (\\(g^r\\)) from each chosen region. This method introduces some uncertainty in the number of successful samples for each subgroup, although the optimizer prioritizes regions with high densities of the targeted population. This slightly reduces the performance and fairness of variance compared to the Phase 1 Only optimization. Nonetheless, the Phase 1 and 2 method meets (by construction) the confidence constraints for every group, as empirically demonstrated."}, {"title": "6.2 DP-sampling: Errors and Fairness", "content": "Next, we focus on the setting with differential privacy, employing the privately adjusted counts \\(\\tilde{N}_i\\) as described in Equation (6). The main results are reported in Figure 6, again for the state of Connecticut, and additional results analyzing other states are reported in Appendix B."}, {"title": "7 Heuristics-guided Sampling Scheme", "content": "We will now introduce a heuristic approach inspired by the findings from Section 6.2, where we observed that allocating a closer to equal number of surveys to each subgroup resulted in similar relative errors. The relative standard error (RSE) can be calculated as:\n\\[RSE = \\frac{\\sigma/\\mu}{\\sqrt{n}} = \\frac{CV}{\\sqrt{n}}.\\]\nHere, \\(\\sigma\\) represents the income standard deviation, \\(\\mu\\) is the average income, and n denotes the sample size. The term \\(\\sigma/\\mu\\), known as the coefficient of variation, is generally consistent across different subgroups. To achieve equal RSE, we need to ensure that n is similar across subgroups. If \\(\\sigma/\\mu\\) is constant across groups, equal sample sizes would result in a fairness score \\(\\bar{\\xi}_{var} = 0\\), indicating perfect fairness. However, if \\(\\sigma/\\mu\\) varies significantly among groups, equal sample sizes will still lead to a lower fairness score. We calculated the coefficient of variation for each race in Nebraska, as shown in Table 2."}, {"title": "8 Conclusion", "content": "This work was motivated by our observations of unfairness in large survey efforts of critical importance for driving many policy decisions and allocations of large amounts of funds and benefits. This paper showed that in surveys like the American Community Surveys, traditional sampling methods disproportionally affect minority groups, leading to biased statistical outcomes. To address these issues, we introduced an optimization-based framework to ensure fair representation in error margins in each population segment while minimizing the total sampling costs. Additionally, this paper examined the effects of differential privacy on the accuracy and fairness of the realized surveys. Contrary to common intuitions, our findings reveal that differential privacy can reduce unfairness by introducing positive biases beneficial to underrepresented populations. These findings are validated through rigorous and comprehensive experimental analysis using real-world data, demonstrating the effectiveness of the proposed optimization-based strategies in terms of enhancing fairness without compromising data utility and costs.\nWe believe that these results may have significant implications for policy formulation and resource allocation with critical societal and economic impacts."}, {"title": "A Experimental Details", "content": "Experimental settings. Each of the results in this paper was produced using 1 CPU (Intel) with 64 GB of memory per task. We conducted all experiments using ACS data from IPUMS [10] for 2021 and 2022. Following standard survey design practices, 2021 data was used as prior information, while 2022 data served as the ground truth from which samples were drawn.\nData Preprocessing. To ensure meaningful analysis, we preprocessed the ACS data as follows:\n1. Handling Missing Values: Records with unknown income or educational levels were removed.\n2. Merging Sparse Racial Groups: To prevent issues with extremely small sample sizes, sparse racial groups were consolidated. For example, Asian subgroups (e.g., Chinese, Japanese, Pacific Islanders) were combined into a single \"Asian\" category, and categories like \"two major races\" and \"three or more major races\" were merged into \"2 or more races.\"\n3. Merging Sparse Educational Levels: Similar to racial groups, educational levels with small sample sizes were combined. For instance, \"Grade 9,\" \"Grade 10,\" and \"Grade 11\" were merged into a group that consists of high school educated people who did not graduate, and \"No schooling\" and \"Nursery to grade 4\" were combined into a group consisting of people with limited education. Details are provided in Table 6.\n4. Cloning Weighted Records: Each record in the ACS data, representing multiple individuals via the PERWT attribute, was cloned by its weight to better reflect population distribution in our analysis.\nGeographical divisions. The smallest geographical unit provided in ACS data is Public Use Microdata Area (PUMA). Each PUMA contains at least 100,000 people in order to maintain confidentiality. However, due to the nature of large group size, PUMA is not a great representation of the geographical unit to be used during the second phase (door-to-door) of the survey. Therefore, we artificially divided them into smaller regions of approximately 4,000 people, emulating Census Tract data. Although actual tract sizes vary, 4,000 is the optimal size recommended by the Census Bureau."}, {"title": "B Additional Experiments on Other States", "content": "This section presents results for Maine and Nevada, chosen for their contrasting levels of racial diversity-Maine being more homogeneous and Nevada more diverse. These states were selected based on the Census Bureau's Simpson's Index of Diversity [6], calculated as:\n\\[D=1-\\sum_{i \\in [G]}\\left(\\frac{N_i}{N}\\right)^2\\]\nHere, D ranges from 0 to 1, representing the probability that two randomly chosen individuals will belong to different racial groups. For detailed population sizes and diversity scores, refer to Table 3."}, {"title": "C Additional Experiments on Education Level", "content": "This section presents results using educational level as a subgroup instead of race. For details on each educational level, see Table 6. The following experiments use Connecticut as the state of focus."}, {"title": "D Phase 1 Algorithm", "content": "The algorithm for solving the Phase 1 Only optimization approach involves solving the following program.\n\\[\\underset{p}{minimize} \\: c_1 \\left( \\sum_{i \\in [G]} p_i N_i\\right)\\]\n\\[ s.t. \\: n_i = p_i N_i (1 - F_i^1) \\: \\forall i \\in [G]\\]\n\\[ Pr(|Err(\\hat{\\theta}_i(n_i)) > \\gamma_i) \\leq \\alpha, \\: \\forall i \\in [G],\\]\n\\[ 0 \\leq p_i \\leq 1, \\: \\forall i \\in [G].\\]"}, {"title": "E Varying Parameters", "content": "Ablation study for total costs:\n\u2022 Total Cost vs. F\u2081: The increasing cost trend is pretty clear. This is a direct cause from needing to sample more to meet the required number of successful samples.\n\u2022 Total Cost vs. F2: The increasing cost trend is pretty clear. Notice that the total cost curve plateaus near 0.6, which is the base value of F\u2081. When F2 \u2265 F1, there is no merit to utilizing the second-phase at all as it is more costly per-survey wise. Therefore, the optimizer selects only from the first-phase to minimize the cost once F2 \u2265 F1.\n\u2022 Total Cost vs. C2: The increasing cost trend is pretty clear. Notice the total cost curve starts to plateau as C2 increases. This is due to the optimizer preferring to sample more from the first-phase once c\u2082 becomes too expensive (at which point, an equal or better level of utility can simply be achieved by allocating more surveys during the first phase only for less cost).\n\u2022 Total Cost vs. a: The trend is downward as relaxing a allows for a bigger margin for error due to a lower confidence interval, which leads to less sampling.\n\u2022 Total Cost vs. \u03b3: The trend is downward as relaxing y allows for a bigger margin for error under 1 \u2212 \u03b1 confidence interval, which leads to less sampling."}, {"title": "F Sparsity Analysis", "content": "In Section 5, we discussed how the size of the region influences the magnitude of bias when using a fixed privacy budget (Corollary 1). Figure 16 presents the results of the same experiment shown in Figure 6, but across three different levels of sparsity: 200,000 People Per Region (PPR), 4,000 PPR, and 2,000 PPR. These levels correspond to sampling regions at the county level, Census Tract level, and Census Block Group level, respectively. As we reduce the size of the regions, the variance of the error decreases under the Standard Allocation method, even though the privacy budget \u03b5 remains constant. This decrease in variance is analogous to the effect observed with differential privacy sampling when the privacy budget increases, except here the reduction in bias is due to a smaller \\(N_i\\) (the number of individuals in each region), rather than a smaller \u03b5."}]}