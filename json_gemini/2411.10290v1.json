{"title": "The ParClusterers Benchmark Suite (PCBS): A Fine-Grained Analysis of Scalable Graph Clustering [Experiment, Analysis & Benchmark]", "authors": ["Shangdi Yu", "Jessica Shi", "Jamison Meindl", "David Eisenstat", "Xiaoen Ju", "Sasan Tavakkol", "Laxman Dhulipala", "Jakub \u0141\u0105cki", "Vahab Mirrokni", "Julian Shun"], "abstract": "We introduce the ParClusterers Benchmark Suite (PCBS)-a collection of highly scalable parallel graph clustering algorithms and benchmarking tools that streamline comparing different graph clustering algorithms and implementations. The benchmark includes clustering algorithms that target a wide range of modern clustering use cases, including community detection, classification, and dense subgraph mining. The benchmark toolkit makes it easy to run and evaluate multiple instances of different clustering algorithms, which can be useful for fine-tuning the performance of clustering on a given task, and for comparing different clustering algorithms based on different metrics of interest, including clustering quality and running time.\nUsing PCBS, we evaluate a broad collection of real-world graph clustering datasets. Somewhat surprisingly, we find that the best quality results are obtained by algorithms that not included in many popular graph clustering toolkits. The PCBS provides a standardized way to evaluate and judge the quality-performance tradeoffs of the active research area of scalable graph clustering algorithms. We believe it will help enable fair, accurate, and nuanced evaluation of graph clustering algorithms in the future.", "sections": [{"title": "1 Introduction", "content": "Clustering is a critical tool in almost any scientific field that involves classifying and organizing data today. Examples of fields leveraging clustering range from computational biology and phylogenetics to complex network analysis, machine learning, and astrophysics [18, 49, 60, 63]. Clustering has proven particularly useful in fields transformed by AI and machine learning because of its utility in understanding and leveraging high-dimensional vector representations (embeddings) of data [12, 28, 29, 33, 53].\nIn this paper, we are interested in carefully characterizing the behavior (e.g., measuring quality, running time, and scalability) of parallel clustering algorithms for shared-memory multi-core machines that are scalable in the size of the dataset and the number of threads. Our specific focus is on graph clustering, which is a versatile and scalable clustering approach that can be used with different input types. On one hand, graph clustering is a natural approach whenever the input is a graph (e.g., friendships, interactions, etc.). On the other hand, graph clustering can also be applied in the other popular scenario, when the input is a collection of points in a metric space (e.g., embeddings). In this case, one can obtain a graph by computing a weighted similarity graph, where continuous or complete phenomena can be cast into sparse similarity graphs, e.g., by keeping only edges between nearby points or only the most significant entries of a similarity matrix.\nDespite substantial prior works that study the quality (e.g., precision and recall) and scalability of individual graph clustering methods [28, 29, 31, 64, 68, 70, 73], no prior works have systematically compared a large collection of different graph clustering methods (and their corresponding implementations) to understand how different methods compare against each other under different metrics. For example, celebrated and widely-utilized graph clustering algorithms, such as modularity clustering are well understood to be highly effective in community detection tasks on unweighted natural graphs, but little is known about their performance for clustering on vector embedding clustering tasks.\nThis paper addresses this gap by performing a systematic comparison of a large and diverse set of graph clustering methods. Our evaluation includes methods tailored to both weighted and unweighted graphs and incorporates a diverse set of natural graphs and similarity graphs derived from point sets. We focus on undirected graphs, as converting directed graphs to undirected graphs is a common practice in many graph tasks, such as community"}, {"title": "2 ParClusterers Benchmark Suite", "content": "In this section, we introduce our PCBS benchmark suite, which includes a large number of scalable parallel graph clustering algorithms, parallel implementations of clustering evaluation metrics, efficient graph I/O routines, and a convenient benchmark interface. An overview of the library is shown in Figure 1."}, {"title": "2.1 Graph Input", "content": "PCBS supports both weighted and unweighted simple undirected graphs in edge list format [44] and compressed sparse row format [27, 38]. A simple graph is a graph with no self-loops and no parallel edges. The graph clustering algorithms in PCBS that use edge weights assume edge weights are nonnegative similarities, i.e., a higher weight of an edge (u, v) means that u and v are more similar. This allows for a natural way of defining how a missing edge affects the clustering: in most of our algorithms a missing edge is equivalent to an edge of similarity 0."}, {"title": "2.2 Datasets", "content": "In this work, we benchmark on both unweighted and weighted graphs. For unweighted graphs, the SNAP [44] collection of datasets is a popular choice for benchmarking community detection tasks. To the best of the authors' knowledge, there currently exists no real-world weighted graph dataset with ground truth for evaluating weighted graph clustering algorithms. In this work, we present a suite of weighted graph clustering datasets that are the k-nearest neighbor graphs of real-world vector datasets with ground truth clustering labels, including a new dataset that we created that contains a large number of ground truth clusters. We describe more details of constructing these datasets in Section 3. We believe that these datasets will allow researchers to thoroughly benchmark the performance of weighted graph clustering algorithms."}, {"title": "2.3 Parallel Graph Clustering Algorithms", "content": "Our library includes eleven scalable parallel graph clustering algorithms, which are described below. We focus on algorithms that are scalable, widely used, and have high precision (i.e., most vertices in a retrieved cluster are from the same ground truth cluster). We discuss more about our algorithm selection in Section 2.4. We select a mix of algorithms that are popular in the literature and algorithms that we found to have high quality in our experience. The implementations of TECTONIC, label propagation, and speaker-listener label propagation (SLPA) are new. Other implementations are taken from previous work and integrated into PCBS. All implementations, except for SLPA, produce non-overlapping clusters.\nWe categorize the algorithms into weighted graph algorithms, which take into account the similarities, and unweighted graph algorithms, which only use the topology of the graph."}, {"title": "2.3.1 Weighted Graph Algorithms", "content": "Affinity Clustering [12, 28]. Affinity clustering is a hierarchical clustering algorithm based on Bor\u016fvka's minimum spanning forest algorithm. PCBS includes the shared-memory parallel affinity clustering from Dhulipala et al. [28], which is adapted from the original MapReduce affinity clustering algorithm [12]. In each step of the algorithm, every vertex picks its best edge (i.e., that of the maximum similarity) on each round, and the connected components spanned by the selected edges define the clusters. The algorithm can produce a clustering hierarchy by contracting the clusters and running the algorithm recursively. The edge weights between contracted vertices can be computed using different linkage functions, such as single, maximum, or average linkage. In our experiments we use average linkage.\nUsers can control the resolution of the clustering by picking an edge weight threshold and only considering edges whose weight is at least the threshold in each step. Recent work by Monath et al. [53] showed that the quality of affinity clustering can be improved if the threshold decays geometrically at each step. We use this technique, also known as the SCC algorithm [53], in our experiments.\nCorrelation and Modularity Clustering [64]. Our library includes a shared-memory parallel framework for optimizing the LambdaCC objective [74]. This objective generalizes both modularity [55] and correlation clustering [11] and is defined as follows. Let \\(k_v\\) be the vertex weight of v and \\(\\lambda\\) be the resolution parameter. Let V be the set of all vertices, and \\(w_{uv}\\) be the weight (similarity) of edge (u, v). Then, \\(\\text{LambdaCC}(x) = \\sum_{(i,j)\\in V\\times V} W_{ij} \\cdot x_{ij}\\), where \\(x_{ij}\\) is a Boolean indicator, which is equal to 1 if and only if i and j belong to the same cluster. Here, \\(w_{uv}\\) = 0 if u = v, \\(w_{uv} = W_{uv} - \\frac{\\lambda k_u k_v}{2m}\\) if (u, v) \u2208 E, and \\(w_{uv} = - \\frac{\\lambda k_u k_v}{2m}\\) otherwise. By default our framework uses \\(k_u = 1\\) for any u \u2208 V.\nAs shown in [64], the special case of the modularity objective can be obtained by appropriately defining vertex weights k and setting \u03bb. Specifically, assume we set the vertex weight \\(k_u\\) to be equal to the weighted degree of v (i.e., the sum of its incident edge weight) and set the resolution \\(\\lambda = 1/(2m)\\). Then, optimizing LambdaCC objective is equivalent to optimizing the modularity objective [36], as the two objectives are monotone in each other. Furthermore, if we set the resolution \\(\\lambda = \\gamma/(2m)\\), the LambdaCC objective becomes monotone in the generalized modularity objective of Reichardt and Bornholdt [62] with a fixed scaling parameter \\(\\gamma \\in (0, 1)\\).\nOur framework optimizes the LambdaCC objective using a parallel Louvain-style algorithm [17, 64].\nApproximate Parallel Hierarchical Agglomerative Clustering (ParHAC) [29]. Given n vertices, a sequential version of the HAC algorithm starts by forming a separate cluster for each vertex, and proceeds in n \u2212 1 steps. Each step merges the two most similar clusters, i.e., replaces them by their union. The similarity of two clusters is the total weight of edges between the clusters divided by the product of their sizes.\nThe output of HAC is a binary tree called a dendrogram, which describes the merges performed by the algorithm. A flat clustering can be obtained from the dendrogram by cutting it at a given level. The threshold for cutting the dendrogram controls the clustering resolution. Because the output of HAC is a dendrogram, one can easily postprocess the dendrogram to obtain a flat clustering of any given resolution. The running time of this postprocessing is negligible compared to the clustering time. PCBS includes a shared-memory parallel implementation of approximate HAC called ParHAC [29]. A HAC algorithm is called 1 + \u03f5 approximate, when each merged pair of clusters has similarity at least \\(\\frac{W_{\\text{max}}}{(1+\\epsilon)}\\), where \\(W_{\\text{max}}\\) is the largest similarity between any two clusters.\nConnected Components. [27, 41] Given a threshold parameter \u03c4, the clusters are the connected components with edge similarity < \u03c4 removed. Our implementation uses the state-of-the-art implementation of concurrent union-find capable of processing several billions of edges per second from the ConnectIt library [30, 41] to find the connected components."}, {"title": "2.3.2 Unweighted Graph Algorithms", "content": "Low-Diameter Decomposition (LDD) [27, 52]. LDD partitions vertices of an n-vertex, m-edge unweighted graph into clusters, where each cluster has O((log n)/\u03b2) diameter and there are at most \u03b2m edges connecting distinct clusters. The choice of \u03b2 controls the clustering resolution. Our implementation is based on the parallel MPX algorithm [52].\nk-core Decomposition (KCore) [26, 27]. k-core decomposition is a graph clustering technique that recursively prunes vertices from the graph whose degree is less than k, until all remaining vertices have at least k neighbors. This process partitions the graph into k-cores, where each k-core is a maximal subgraph in which every vertex has degree at least k within that subgraph. Non-empty k-cores for large value of k tend to represent a densely connected core of the graph, while k-cores for small values of k represent more peripheral regions. For a given value of k, k-core decomposition"}, {"title": "2.3.1", "content": "identifies clusters by treating each maximal k-core subgraph as a cluster and treating all other vertices as singleton clusters. The choice of k can be used to control the clustering resolution.\nThe algorithm outputs connected components of vertices with core number at most k. Vertices with core number less than k are in their own singleton clusters.\nStructural Graph Clustering (SCAN) [27, 71]. This implementation of SCAN is a parallel version [71] of the index-based SCAN algorithm introduced in Wen et al. [75]. Following [78], our implementation of SCAN defines a structural similarity \u03c3 between adjacent vertices u and v as \\(\\sigma(u,v) = \\frac{|\\Gamma(u) \\cap \\Gamma(v)|}{\\sqrt{|\\Gamma(u)| \\cdot |\\Gamma(v)|}}\\), where \u0393(u) is a set including u and the neighbors of u.\nTwo vertices are structurally similar if their structural similarity is at least \u03f5, an input parameter. A vertex is called a core vertex, if it is structurally similar to at least \u03bc other vertices, where \u03bc is another parameter. A cluster is constructed by a core expanding to all vertices that are structurally similar to that core.\nSpecifically, the output of the algorithm is equivalent to the following algorithm. First, construct an auxiliary graph H on the core points, in which we add an edge between any two core vertices, which are structurally similar. Then, compute the connected components of H. Finally, assign each non-core point which is structurally similar to a core point to the cluster of that core point (in case there is more than one structurally similar core point, one is chosen arbitrarily). The remaining points are left as singleton clusters.\nTriangle Connected Component Clustering (TECTONIC). PCBS contains a parallel version of the original TECTONIC clustering algorithm [73]. The algorithm first weighs the graph edges based on the number of triangles each edge belongs to. Specifically, if t(u, v) is the number of triangles that edge (u, v) participates in, the weight of an edge (u, v) is set to \\(\\frac{t(u,v)}{\\text{deg}(u) + \\text{deg}(v)}\\). Then, it removes all edges with weight below a threshold parameter \u03b8, where \u03b8 controls the clustering resolution, and computes connected components.\nLabel Propagation (LP). PCBS contains a parallel version of the original label propagation algorithm [61]. The label propagation algorithm works by propagating vertex labels through the graph. Initially, each vertex is assigned a unique label. Then in each iteration, vertices adopt the label that the majority of their neighbors currently have, with ties broken lexicographically. This causes label updates to propagate across the graph, with dense regions quickly reaching consensus on a label. All vertices start in their own singleton cluster. The algorithm runs until no vertex updates its label or a maximum number of iterations has been reached. The groups of vertices converging to the same label represent the clusters. In our parallel implementation, on each round, all vertices asynchronously update their labels in parallel. Specifically, on each round, a vertex v's neighbor's label might be updated from l to l' after v has already used the label l to update its own label.\nSpeaker-Listener Label Propagation (SLPA). PCBS contains a parallel version of the original SLPA [77] algorithm. SLPA is a variant of the label propagation algorithm, where each vertex maintains a list of labels (its memory) instead of a single label. On each round, in parallel every vertex passes a random label to each neighbor, chosen from its memory with probability proportional to the occurrence frequency of this label in the memory (the choice can be"}, {"title": "2.4 Scope", "content": "Due to the long history of research on clustering algorithms, hundreds of clustering algorithms have been developed over the past century, and exhaustively comparing all existing proposals is an impossible task. Instead, we aimed to choose a representative collection of clustering algorithms that are (1) scalable and efficient and (2) can produce high-precision results. We now explain our rationale in more detail.\n(1) Scalability and Efficiency. To keep up with the rapid growth of real-world data, a requirement for modern clustering algorithms is that they should be scalable to very large datasets, and scalable with increasing computational power (e.g., cores and machines). In this paper, we focus on the shared-memory (single-machine) multicore setting, which has been successfully used to process graphs up to hundreds of billions of edges [27].\nOur focus on scalable and efficient clustering algorithms rules out algorithms such as graph neural network approaches [72], exact spectral clustering [21, 56] and other algorithms based on numerical linear algebra, whose scalability is limited due to their poor computational efficiency.\n(2) High Precision. In many applications of clustering, such as spam and abuse detection, classification, and deduplication, a usual requirement is to produce a high precision clustering. In other words, it is more important to ensure that all entities within a cluster are closely related, than to ensure that each group of related entities ends up in a single cluster. Because of this reason we do not include balanced graph partitioning in our comparison, which primarily optimizes recall (i.e., most vertices in each ground truth cluster are grouped into the same output cluster). We note that balanced partitioning algorithms are not included in many of the popular graph libraries and databases [3-5, 8, 68]."}, {"title": "2.5 Clustering Quality Metrics", "content": "PCBS includes parallel implementations of many popular metrics for evaluating clustering quality, including precision, recall, F-score, adjusted rand index (ARI), normalized mutual information (NMI), edge density, triangle density, and more.\nPrecision, Recall, and F-score. PCBS computes the average precision and recall of a clustering compared to the ground truth. These metrics work for both non-overlapping and overlapping clusters. To compute average precision and recall, for each ground-truth community c, we match c to the cluster c' with the largest intersection to c. We list the formulae for computing the metrics below. This metric matches the methodology used by Tsourakakis et al. [73] in evaluating TECTONIC and Shi et al. [64] in evaluating correlation clustering.\n\\(\\text{Precision} = \\frac{|c \\cap c'|}{|c'|}\\)\n\\(\\text{Recall} = \\frac{|c \\cap c'|}{|c|}\\)\n\\(F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{(\\beta^2 \\times \\text{Precision}) + \\text{Recall}}\\)\nHere, \u03b2 is a parameter specifying the relative importance of precision and recall. When \u03b2 < 1, the objective rewards optimizing"}, {"title": "2.6 Benchmark", "content": "PCBS has a convenient and flexible framework for benchmarking clustering algorithms. Listing 1 gives an example of the configuration file for benchmarking different clustering algorithms. Users can flexibly specify the graphs to benchmark, the number of threads to use, the number of rounds to run, the timeout, and the set of parameters to try for each clustering algorithm. Specifically, for each clustering algorithm, PCBS tries a Cartesian product of all parameter values. We chose to use a Cartesian product so make benchmarking comprehensive, but users can easily modify PCBS with a different method to explore the parameter space. Besides clustering implementations in PCBS, PCBS also supports running many clustering implementations in NetworKit, Neo4j, and TigerGraph.\nFor each graph, each clusterer, each parameter set, and each round of the experiment, PCBS outputs a file for the resulting clustering in Output directory. It also outputs a CSV file including the running time of all runs specified by the configuration file to CSV Output directory.\nPCBS also supports specifying the ground truth communities and computing statistics of the clustering results, such as precision and recall. Listing 2 gives an example of the statistics configuration file for computing the quality metrics. This configuration file is used together with the clustering configuration file (e.g., Listing 1). statistics_config configures which metrics to compute and the parameters for the metrics. PCBS's metrics library outputs:"}, {"title": "2.7 Extending PCBS", "content": "Our PCBS design, as described in Section 2.6, can be extended to include new datasets, algorithms, parameter searches, and more. To add a new dataset, users can simply provide the graph data in edge list or compressed sparse row format, and add it to the configuration file. For integrating new clustering algorithms, users need only implement a function that takes an input graph and algorithm parameters, and returns a clustering result. This function can then be registered with PCBS's benchmarking framework. Users can also extend the parameter search methods by implementing custom search strategies, enabling more efficient exploration of algorithm parameter spaces. New evaluation metrics can also be added in a similar way.\nAdditionally, PCBS outputs benchmarking results in convenient pandas dataframe and CSV format, which allows for easy integration of visualization tools, or even entire modules for specific analysis tasks.\nBy leveraging these extension points, researchers and practitioners can adapt PCBS to their unique requirements, facilitating direct comparisons between novel methods and state-of-the-art algorithms on both standard and domain-specific graphs."}, {"title": "3 Empirical Evaluation", "content": "This section presents the results obtained by running the PCBS benchmark. We study the performance of all algorithms described in Section 2.3 on a variety of different graphs.\nWe show that using our PCBS benchmark library, we are able to obtain new insights on comparing scalable parallel graph clustering"}, {"title": ""}]}