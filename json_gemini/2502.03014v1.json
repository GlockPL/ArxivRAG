{"title": "XAI_EVALS : A FRAMEWORK FOR EVALUATING POST-HOC\nLOCAL EXPLANATION METHODS", "authors": ["Pratinav Seth", "Yashwardhan Rathore", "Neeraj Singh", "Chintan Chitroda", "Vinay Kumar Sankarapu"], "abstract": "The growing complexity of machine learning and deep learning models has led to an increased reliance\non opaque \"black box\" systems, making it difficult to understand the rationale behind predictions. This\nlack of transparency is particularly challenging in high-stakes applications where interpretability is as\nimportant as accuracy. Post-hoc explanation methods are commonly used to interpret these models,\nbut they are seldom rigorously evaluated, raising concerns about their reliability. The Python package\nxai_evals addresses this by providing a comprehensive framework for generating, benchmarking, and evaluating explanation methods across both tabular and image data modalities. It integrates\npopular techniques like SHAP, LIME, Grad-CAM, Integrated Gradients (IG), and Backtrace, while\nsupporting evaluation metrics such as faithfulness, sensitivity, and robustness. xai_evals enhances\nthe interpretability of machine learning models, fostering transparency and trust in AI systems. The\nlibrary is open-sourced at https://pypi.org/project/xai-evals/.", "sections": [{"title": "1 Introduction", "content": "The increasing complexity of machine learning (ML) and deep learning (DL) models has led to their widespread\nadoption in numerous real-world applications. However, as these models become more powerful, they also become\nless interpretable. In particular, deep neural networks (DNNs), which have achieved state-of-the-art performance in\ntasks such as image recognition, natural language processing, and autonomous driving, are often viewed as \"black box\"\nmodels due to their complexity and lack of transparency.\nInterpretability is essential, particularly in high-stakes fields where the consequences of incorrect or non-explainable\ndecisions can be profound. In domains such as healthcare, finance, and law, it is not only crucial that AI systems make\naccurate predictions but also that these predictions can be understood and justified by human stakeholders. For example,\nin healthcare, understanding why a model predicts a certain diagnosis can be as important as the prediction itself,\ninfluencing clinical decisions and patient outcomes. Similarly, in finance, ensuring that a model's decision-making\nprocess is transparent is essential for regulatory compliance and trust. This growing need for model transparency has led\nto the development of various explainability techniques aimed at making these \"black box\" models more understandable.\nHowever, despite the availability of these methods, there is a lack of standardized tools for evaluating the quality,\nrobustness, and trustworthiness of the explanations.\nIn response to the growing need for explainable machine learning models, we present xai_evals, a comprehensive\nPython package designed to facilitate the generation, benchmarking, and evaluation of model explanations. The primary\ngoal of xai_evals is to provide researchers and practitioners with a unified framework for assessing and comparing\nvarious explainability methods. We integrate a range of popular explainability techniques, such as SHAP Lundberg and\nLee [2017], LIME Ribeiro et al. [2016], Grad-CAM Selvaraju et al. [2016], and Integrated Gradients Sundararajan"}, {"title": "2 Related Work", "content": "The rest of the paper is structured as follows: Section 2 provides a review of related work in the area of model\nexplainability, explaining the existing techniques and evaluation methods. Section 3 introduces the xai_evals package\nin detail, describing its core features, supported explanation methods, and evaluation metrics. Section 4 presents\nexperimental results demonstrating the effectiveness of the package, followed by a discussion of challenges and\nlimitations in Section 5. Finally, Section 6 concludes the paper with a summary of contributions and future work."}, {"title": "2.1 Overview of Explainability Methods", "content": "Machine learning models, especially deep neural networks, are often regarded as \"black boxes\" because of the\ndifficulty in understanding how they arrive at specific predictions. To address this issue, explainability methods have\nbeen developed to make these models more interpretable. These methods can generally be categorized into global\nexplainability and local explainability."}, {"title": "2.1.1 Global v/s Local Explainbility", "content": "Global explainability aims to provide a high-level understanding of a model's overall behavior across all inputs.\nIt focuses on identifying which features are most influential in determining the model's predictions over a large\ndataset, thus offering insights into how the model makes decisions in general. In contrast, local explainability focuses\non explaining the model's decision-making process for individual instances or predictions. Local explainability is\nparticularly useful for understanding specific decisions made by a model in real-world applications, where a user\nmay want to know why a model made a particular prediction for a specific input. A popular technique within local\nexplainability is post-hoc explainability, where the explanation is generated after the model has been trained without\naltering its internal structure. These methods are crucial for understanding complex, black-box models like deep neural\nnetworks, especially in scenarios where interpretability of individual predictions is necessary.\nFor instance, local explainability methods are commonly applied in medical diagnostics, where a deep learning model\nmight predict the presence of a disease in a patient based on their medical history and test results. A healthcare\nprofessional might want to know why the model predicted that a patient has a particular disease, which can be addressed\nby local explainability methods. Similarly, in credit scoring models, a financial institution might use local explainability\nto understand the rationale behind an individual's credit score prediction, helping to provide transparency for both\ncustomers and regulators."}, {"title": "2.1.2 Post Hoc Local Explainibility MEthods", "content": "There are several widely used methods to achieve local explainability, particularly for post-hoc explanations. These\nmethods include:\nSHAP (SHapley Additive exPlanations) Lundberg and Lee [2017] is a model-agnostic method that assigns a value to\neach feature, reflecting its contribution to the prediction for a specific instance. SHAP is based on Shapley values from\ncooperative game theory, ensuring fairness and consistency in the attribution of feature importance. For example, in\na loan approval model, SHAP can help explain why a specific feature, such as income, had a significant impact on\nthe decision to approve or deny the loan for a particular applicant. This method can be applied to a variety of models,\nincluding tree-based models and deep learning architectures, and is widely used due to its strong theoretical foundation.\nLIME (Local Interpretable Model-Agnostic Explanations) Ribeiro et al. [2016] is another widely used model-agnostic\nmethod. LIME approximates the local decision boundary of the model for individual predictions by training an\ninterpretable surrogate model, such as a linear regression or decision tree, on perturbed data. It works by observing\nhow the model behaves on slightly altered versions of the input data and generating an explanation based on this local\nbehavior. For instance, in image classification, LIME can help explain which parts of an image are most influential in\ndetermining the model's prediction by perturbing different regions of the image and observing the model's response."}, {"title": "2.2 Existing Evaluation Frameworks and Benchmarking Tools for Model Explainability", "content": "Grad-CAM (Gradient-weighted Class Activation Mapping) Selvaraju et al. [2016] is a method designed for convolutional\nneural networks (CNNs) that generates visual explanations by highlighting the regions of an image that are most\nimportant in the model's prediction. For example, in a medical imaging task where a CNN is used to diagnose diseases\nfrom X-ray images, Grad-CAM can generate a heatmap showing the areas of the X-ray image that contributed most to\nthe prediction of a particular disease, providing intuitive visual explanations to the medical professionals.\nIntegrated Gradients Sundararajan et al. [2017] works by integrating the gradients of the model's output with respect to\nthe input features, from a baseline input to the actual input. This method provides a smooth and consistent explanation\nfor model predictions, ensuring that each feature's contribution is computed along a continuous path. For example, in\nsentiment analysis, Integrated Gradients can be used to explain which words in a sentence were most responsible for a\npositive or negative sentiment prediction, helping to clarify the rationale behind the model's decision.\nBackpropagation-Based Explainability Methods, such as DlBacktrace Sankarapu et al. [2024], trace the relevance of\neach component in a neural network from the output back to the input. This allows for a detailed analysis of how each\nlayer in the network contributes to the final prediction. For example, in image classification, backpropagation-based\nmethods can show how the features learned by the lower layers of the neural network (e.g., edges and textures) contribute\nto higher-level features (e.g., shapes or objects) that ultimately determine the model's output. DlBacktrace is particularly\nuseful as it provides insights into the layer-wise contribution to predictions.\nThese local explainability methods help demystify the decision-making processes of machine learning models, particu-\nlarly in complex, real-world scenarios. Whether used to explain a healthcare model's diagnosis, a financial institution's\ncredit decision, or a computer vision model's image classification, these techniques provide crucial insights into\nindividual predictions, enhancing the trust and transparency of AI systems.\nDespite significant advancements in Explainable AI (XAI), evaluating model explanations remains a challenge due to\nthe lack of a standardized and comprehensive assessment framework. Existing evaluation methodologies often rely on\nsimplistic interpretability metrics that fail to capture essential aspects such as robustness, generalizability, and human\nalignment. This has led to inconsistencies in how explanations are assessed across different domains and applications.\nRecent research has highlighted the limitations of current evaluation paradigms. Madsen et al. Madsen et al. [2024] argue\nthat most existing approaches prioritize faithfulness while overlooking robustness and usability, leading to incomplete\nassessments of explanation quality. Wickstr\u00f8m et al. Wickstr\u00f8m et al. [2024] further emphasize that interpretability\nmetrics are often inconsistent across domains, which complicates their application in real-world scenarios and raises\nconcerns about their susceptibility to manipulation.\nSeveral benchmarking tools have been introduced to address these issues, each with its own strengths and limitations.\nThe M4 Benchmark Li et al. [2023] provides a structured framework for evaluating feature attribution methods,\nplacing significant emphasis on faithfulness. However, it does not explicitly assess robustness against adversarial\nperturbations or stability across different data distributions, which are crucial for ensuring reliability in high-stakes\napplications. OpenXAI Agarwal et al. [2024] offers a flexible evaluation framework, though its reliance on synthetic\ndata generation raises concerns about the generalizability of its findings to real-world settings. Quantus Hedstr\u00f6m et al.\n[2023] incorporates a diverse set of evaluation metrics, covering faithfulness, robustness, and complexity, yet it lacks\nan explicit mechanism to assess whether the generated explanations align with human intuition. FairX Sikder et al.\n[2024] extends evaluation to fairness and bias considerations but does not provide a comprehensive framework for\npost-hoc explainability. Similarly, Captum Kokhlikyan et al. [2020] and TF-Explain Meudec [2021] focus on generating\nexplanations for deep learning models but do not include built-in benchmarking capabilities to assess explanation\nquality systematically. Inseq Sarti et al. [2023], while valuable for sequence generation models, is specialized for NLP\ntasks and does not generalize well to other domains such as tabular or vision-based data.\nThe fragmentation of existing evaluation frameworks highlights the need for a more robust and flexible approach to\nassessing model explanations. Many existing tools prioritize faithfulness while neglecting complementary factors such\nas robustness, sensitivity, and usability, leading to an incomplete understanding of explanation quality. Others are\ndesigned for specific model architectures or data modalities, making it difficult to conduct cross-domain comparisons.\nAdditionally, many commonly used evaluation metrics do not align well with human judgment, which limits their\napplicability in decision-critical environments where interpretability is essential.\nTo address these challenges, we introduce xai_evals, a framework that integrates explanation generation and evaluation\ninto a single, standardized package. Unlike existing tools, xai_evals allows researchers to systematically assess\nexplanation quality across multiple methods, models, and datasets using a broad range of evaluation metrics."}, {"title": "3 Package Overview", "content": "The xai_evals package provides a comprehensive suite of functionalities to facilitate model interpretability and\nexplainability. The main features of the package include:\n\u2022 Compatibility with Various Models: The package supports both classical machine learning models, such\nas those in scikit-learn (e.g., RandomForest, LogisticRegression, etc.), as well as deep learning models built\nusing frameworks like PyTorch and TensorFlow.\n\u2022 Model Explanation Generation: The package integrates several popular explainability methods, including\nSHAP, LIME, Grad-CAM, Integrated Gradients, and Backtrace, to generate local and global explanations for a\nwide range of machine learning models.\n\u2022 Model Explanation Evaluation: xai_evals offers a set of robust evaluation metrics, such as faithfulness,\nsensitivity, comprehensiveness, and robustness, to quantitatively assess the quality of generated explanations.\n\u2022 Benchmarking Explanations: The package allows for benchmarking explanations generated by different\nmethods, facilitating a comparison of explanation quality across different models and datasets."}, {"title": "3.1 Model and Data Type Support", "content": "The xai_evals framework is structured to support explainability for tabular and image data across machine learning\n(ML) and deep learning (DL) models."}, {"title": "3.1.1 Tabular Data", "content": "\u2022 Machine Learning Models (Scikit-Learn, XGBoost) : SHAPExplainer, LIMEExplainer.\n\u2022 Deep Learning Models:\nSHAPExplainer,\nLIMEExplainer,\nTFTabularExplainer,\nTorchTabularExplainer, DlBacktraceTabularExplainer.\n\u2022 Evaluation: ExplanationMetricsTabular."}, {"title": "3.1.2 Image Data", "content": "\u2022 Deep Learning Models: TorchImageExplainer, TFImageExplainer, DlBacktraceImageExplainer.\n\u2022 Evaluation: ExplanationMetricsImage."}, {"title": "3.2 Explanation vs. Evaluation Classes", "content": "The xai_evals framework consists of two key components: illustration classes for generating explanations and\nmetric classes for evaluating their quality."}, {"title": "3.3 Installation", "content": "To install the xai_evals package, you can either clone the repository from GitHub or install it directly via pip (once\nthe package is published). Below are the instructions for both the methods:"}, {"title": "3.3.1 Install via Pip (After Package is Published)", "content": "You can install it directly from pip using the following command:\n1 pip install xai_evals"}, {"title": "3.4 Explanation Methods Supported", "content": "The xai_evals package integrates a wide variety of explanation methods, which can be used to generate attributions\nfor models. These methods can be categorized into those for tabular data and image data, as well as methods tailored\nfor deep learning versus classical machine learning models."}, {"title": "3.4.1 Tabular Data Explanations", "content": "The package provides several explanation methods for tabular data. For Machine Learning models, it supports SHAP\nand LIME-based explainers, tailored to the specific model type. For Deep Learning models, both LIME and SHAP are\nsupported, alongside a variety of other methods such as integrated gradients, deep LIFT, gradient SHAP, saliency, input\nX gradient, guided backprop, SHAP Kernel, SHAP Deep, and LIME for PyTorch models. Additionally, the package\nincludes support for DIBacktrace, which works with both PyTorch and TensorFlow models."}, {"title": "3.4.2 Image Data Explanations", "content": "The package offers various explanation methods for image data. For Torch, it supports methods such as Grad-CAM,\nintegrated gradients, saliency, deep LIFT, gradient SHAP, guided backprop, occlusion, layer Grad-CAM, and feature\nablation. For TensorFlow, it provides Vanilla Gradients, Grad-CAM, Gradients Input, Integrated Gradients, Occlusion\nSensitivity, and SmoothGrad. Additionally, for Backtrace, the package supports the methods default, contrast-positive,\nand contrast-negative."}, {"title": "3.5 Metrics for Evaluation", "content": ""}, {"title": "3.5.1 Tabular", "content": "To assess the quality of generated explanations, the xai_evals library supports several evaluation metrics. These\nmetrics help quantify how well the explanations align with the model's predictions and the contributions of individual\nfeatures. Below, we describe each metric along with its mathematical formulation and interpretation.\nFaithfulness measures how well the attribution aligns with the changes in model output when features are perturbed.\nThe metric assesses whether the attribution reflects the changes in the model's output when a given feature is modified.\nA higher faithfulness score indicates that the attribution values align well with the changes in model output caused by\nfeature perturbations, suggesting that the explanation is an accurate reflection of the model's behavior. Low faithfulness\nimplies that the explanation may not be faithful to how the model actually reacts to input changes. Mathematically, it\ncan be defined as:\nFaithfulness = 1/n \u03a3_{i=1}^{n} |f(x) - f(x^i) \u00b7 a_i|\nWhere: f(x) is the original model output for the input x, f(x^i) is the model output after perturbing the i-th feature in x,\na_i is the attribution for the i-th feature, n is the total number of features.\nSensitivity measures the robustness of the attributions to small perturbations in the input features. This metric evaluates\nhow much the attributions change when small random noise is added to the input features. Low sensitivity scores"}, {"title": "3.5.2 Image", "content": "suggest that the attributions are stable and not affected by small noise in the input data, which is generally desirable for\nrobustness. High sensitivity indicates that the explanation is unstable, and small changes in the input can lead to large\nvariations in attributions, which may imply that the model's explanations are not robust. The formula is:\nSensitivity = 1/n \u03a3_{i=1}^{n} |a_i - a_i'|\nWhere a_i is the attribution score for the i-th feature on the original input x, a_i' is the attribution score after a small\nperturbation is applied to the i-th feature in x.\nComprehensiveness measures how much the model's output decreases when the most important features, as identified\nby the explanation, are removed. A high comprehensiveness score indicates that the removal of the most important\nfeatures significantly affects the model's prediction, suggesting that these features are crucial for the model's decision-\nmaking process. Low comprehensiveness suggests that the explanation is incomplete and removing the important\nfeatures does not substantially impact the output. The formula is:\nComprehensiveness = 1/n \u03a3_{i=1}^{n} (f(x) - f(x_{mask})) \u00b7 I{a_i \u2208 S_k}\nWhere f(x) is the model's original output, f (x_{mask}) is the model output after masking the top k features based on their\nattributions, {a_i \u2208 S_k} is an indicator function that equals 1 if the feature a_i belongs to the set S_k of the most important\nfeatures.\nSufficiency measures whether the most important features alone are enough to explain the model's output. A high\nsufficiency score indicates that the most important features alone can approximate the original prediction, suggesting\nthat these features are sufficient to explain the model's decision. If the sufficiency score is low, it implies that additional\nfeatures beyond the top k are needed for an adequate explanation. The metric is given by:\nSufficiency = 1/n \u03a3_{i=1}^{n} (f(x) - f(x_{focused})) \u00b7 I{a_i \u2208 S_k}\nWhere f(x) is the model's original output, f (x_{focused}) is the model output when only the top k important features are\nretained and all others are set to zero.\nMonotonicity checks if the attributions are consistent with the direction of change in the model output. That is,\nattributions should increase or decrease as the model's prediction increases or decreases. A high monotonicity score\nsuggests that the attributions change in a consistent direction with the model's output, which implies a coherent\nand logically consistent explanation. A low score may indicate that the explanation does not align with the model's\nunderlying logic. The formula is:\nMonotonicity = 1/(n-1) \u03a3_{i=1}^{n-1} I{sign(a_i) = sign(a_{i+1})}\nWhere a_i is the attribution score for the i-th feature, The sign(a_i) is the sign of the attribution (positive or negative).\nComplexity measures the sparsity of the explanation by counting the number of features with non-zero attribution\nvalues. A low complexity score suggests a sparse and interpretable explanation where only a few features are important\nfor the model's prediction. A high complexity score suggests that the model relies on many features, which may make\nthe explanation more difficult to interpret. It is given by:\nComplexity = 1/n \u03a3_{i=1}^{n} I{a_i \u2260 0}\nWhere a_i is the attribution score for the i-th feature.\nSparseness measures the minimalism of the explanation by calculating the proportion of features that have zero\nattribution values. A high sparseness score suggests that the explanation is minimal, with most features receiving zero"}, {"title": "", "content": "To assess the quality of generated explanations for image models, the xai_evals library supports several evaluation\nmetrics. These metrics help quantify how well the explanations align with the model's predictions and the contributions\nof individual image features. Below, we describe each metric along with its mathematical formulation and interpretation.\nFaithfulness Correlation Bhatt et al. [2020] measures how well the attribution aligns with the changes in model output\nwhen pixels or regions of the image are perturbed. A high Faithfulness Correlation score indicates that the explanation\nis faithful to the model's behavior, meaning that the attributions reflect the actual changes in model output due to\nperturbations. A low score suggests that the attribution method may not accurately represent the model's sensitivity to\nchanges in the image. The metric is defined as:\nFaithfulness Correlation = (\u03a3_{i=1}^{n} |f(x) - f(x^i)| \u00b7 |a_i|) / (\u03a3_{i=1}^{n} |a_i|)\nWhere: f(x) is the model's original output for the input image x, f(x^i) is the model output after perturbing the i-th\npixel or region, a_i is the attribution value for the i-th pixel or region, and n is the total number of pixels or regions in\nthe image.\nMax Sensitivity Yeh et al. [2019] evaluates how much the model's output changes when specific pixels or regions of\nthe image are perturbed. It provides a measure of how sensitive the model is to changes in individual features of the\nimage. A higher Max Sensitivity score indicates that the model's output is highly sensitive to changes in specific image\nregions, suggesting that those regions play a key role in the model's decision. A lower score indicates less sensitivity,\nmeaning that perturbing specific pixels has less impact on the model's prediction. The formula is:\nMax Sensitivity = max_{i=1}^{n} |f(x) - f(x^i)|\nWhere: f(x) is the model's original output, and f(x^i) is the model output after perturbing the i-th pixel or region.\nMPRT (Mean Pixelwise Robustness) Adebayo et al. [2018] measures the robustness of the model's predictions\nagainst perturbations of individual pixels. This metric computes the average change in the model output when each\npixel is individually perturbed. A lower MPRT score indicates that the model is robust to perturbations, meaning the\nexplanation is stable and not influenced by small changes. A higher MPRT score suggests that the model is sensitive to\npixel perturbations, which might indicate that the explanation is less reliable or that the model is overfitting to specific\nfeatures. The formula is:\nMPRT = 1/n \u03a3_{i=1}^{n} |f(x) - f(x^i)|\nWhere: f(x) is the model's original output, and f(x^i) is the model output after perturbing the i-th pixel.\nSmooth MPRT Hedstr\u00f6m et al. [2024] is a variation of MPRT that smoothens the impact of perturbations by considering\nboth the perturbation's effect on the model's output and the magnitude of the attributions. Smooth MPRT helps to\nbalance the effect of perturbations with the importance of each pixel (as indicated by its attribution). A higher Smooth\nMPRT score means the model's output is highly sensitive to pixels with high attribution values. A lower score suggests\nthat the model's output is more stable and that the attributions are less sensitive to perturbations. It is defined as:\nSmooth MPRT = 1/n \u03a3_{i=1}^{n} (|f(x) - f(x^i)|) / (1 + a_i)"}, {"title": "", "content": "attribution, indicating that only a few features are considered important. A low sparseness score indicates that many\nfeatures are being attributed, leading to a more complex and potentially harder-to-interpret explanation. It is given by:\nSparseness = 1 - (1/n) \u03a3_{i=1}^{n} I{a_i \u2260 0}\nWhere a_i is the attribution score for the i-th feature."}, {"title": "3.6 Usage Illustrations", "content": "After installation, you can begin using xai_evals to generate and evaluate explanations for your machine learning\nmodels. The library supports a variety of explainers, such as SHAP, LIME, and Grad-CAM, for different model types\nand data formats, enabling both illustrative and quantitative comparisons. Below are examples demonstrating how to\nuse the library for both tabular and image data."}, {"title": "3.6.1 Tabular Data Example:", "content": "Illustration on how to use the SHAP explainer for illustrating attribution for a tabular dataset using a Random Forest\nClassifier:\n1 # Load dataset and train a model\n2 data = load_iris()\n3 X = pd.DataFrame(data.data, columns=data.feature_names)\n4 y = data.target\n5 model = RandomForestClassifier()\n6 model.fit(X, y)\n7 \n8 # Initialize SHAP explainer\n9 shap_explainer = SHAPExplainer(model=model, features=X.columns, task=\"multiclass-\nclassification\", X_train=X)\n10 \n11 # Explain a specific instance (e.g., the first instance in the test set)\n12 shap_attributions = shap_explainer.explain(X, instance_idx=0)\n13 print(shap_attributions)"}, {"title": "3.6.2 Image Data Example:", "content": "Illustration on how to use TorchImageExplainer for Generating GradCAM explaination for a Pytorch CNN Model\n(ResNet18) over a resized Imagenette Dataset Sample.\n1 transform = transforms.Compose([transforms.Resize((224, 224)),\n2 transforms. ToTensor (), transforms. Normalize(\n3 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n4 imagenette_dataset = datasets. ImageFolder(root=os.path.join(dataset_path, \"val\"),\ntransform=transform)\n5 \n6 model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n7 model.eval()\n8 explainer = TorchImageExplainer(model)\n9 \n10 input_tensor, = imagenette_dataset [9]\n11 attribution_map = explainer.explain(input_tensor, method=\"layer_gradcam\",\ntask=\"classification\")\n12 if isinstance (attribution_map, torch.Tensor):\n13 attribution_map = attribution_map.squeeze().detach().numpy()\n14 plt.imshow(input_tensor.permute(1, 2, 0).numpy())\n15 plt.imshow(attribution_map, cmap=\"jet\", alpha=0.5)\n16 plt.title(\"Grad-CAM Attribution Map\")\n17 plt.colorbar()\n18 plt.show()"}, {"title": "3.6.3 Evaluating Explanations:", "content": "xai_evals allows you to evaluate their quality using a variety of metrics by using ExplanationMetricsTabular and\nExplanationMetricsImage. Here's an example of how to calculate evaluation metrics using ExplanationMetricsTabular\nfor SHAP for a Random Forest Classifier over IRIS Dataset.\n1 # Load dataset and train a model\n2 data = load_iris()\n3 X = pd.DataFrame(data.data, columns=data.feature_names)"}, {"title": "4 The Importance of Explainable AI (XAI) and Regulatory Compliance", "content": "Explainable AI (XAI) is becoming increasingly important as machine learning models are used in critical fields such\nas healthcare, finance, and autonomous systems. In these areas, decisions made by AI need to be interpretable to\nensure trust, transparency, and accountability. Understanding how a model arrives at its conclusions helps businesses,\nregulators, and end-users assess its fairness and reliability. The xai_evals package provides a way to analyze\nmodel decisions by generating explanations that clarify how inputs influence predictions. This is particularly relevant\nfor regulatory compliance, such as the European Union's General Data Protection Regulation (GDPR) and the\nforthcoming EU AI Act, which require AI systems to be transparent and accountable. GDPR grants individuals\nthe \"right to explanation\" when automated decisions affect them, making interpretability a legal necessity in many\napplications. By using xai_evals, researchers and practitioners can verify that their models make decisions based on\nmeaningful and justifiable features. This helps organizations not only meet legal requirements but also build trust with\nstakeholders and ensure AI systems are used responsibly in regulated industries."}, {"title": "5 Conclusion", "content": "We introduced xai_evals, a Python package designed to generate, benchmark, and evaluate various model explain-\nability methods. The package aims to bridge the gap between model accuracy and interpretability by providing a\ncomprehensive framework for both machine learning and deep learning models. By supporting popular explainability\nmethods like SHAP, LIME, Grad-CAM, Integrated Gradients, DlBacktrace and more . xai_evals enables researchers\nand practitioners to gain deeper insights into their models' decision-making processes. The library also provides a\nvariety of evaluation metrics to assess the quality and reliability of these explanations.\nThrough our example applications, we demonstrated how xai_evals can be used to explain and evaluate both tabular\nand image-based models. The results of our benchmarking experiments showed that xai_evals is effective in\ncomparing the quality of different explanation methods and provides valuable insights into model behavior."}, {"title": "6 Future Directions", "content": "While xai_evals provides a strong foundation for model explainability, there are several key areas for future improve-\nment. Our goal is to expand its capabilities to support a wider range of models, integrate new explanation methods,\nenhance performance, and improve overall usability."}, {"title": "6.1 Extending Model Support", "content": "To make xai_evals more versatile, we plan to extend its compatibility to a broader range of models, including:\nNatural Language Processing Models: There is increasing reliance on deep learning models for NLP tasks, future\nupdates will introduce explanation techniques for text-based models.\nHugging Face Transformers and Autoregressive Models Transformer-based architectures, including those provided\nby Hugging Face (e.g., BERT, GPT, T5, Llama), are widely used across NLP and beyond. Future iterations will integrate\nexplainability methods for these models.\nGraph Neural Networks (GNNs) GNNs are increasingly used in fields like drug discovery, fraud detection, and\nrecommendation systems."}, {"title": "6.2 Expanding Explanation Methods & Enhancing Evaluation Metrics", "content": "While xai_evals currently supports several widely used explanation techniques, future updates will introduce\nadditional methods to improve interpretability across diverse applications. To provide more comprehensive assessments\nof model interpretability, we plan to refine and expand the evaluation metrics."}, {"title": "6.3 Optimizing Performance, Scalability and Stability Enhancements", "content": "With the increasing complexity of deep learning models, maintaining computational efficiency in explainability methods\nis essential. Future enhancements to xai_evals will include GPU acceleration, parallel processing, and memory\noptimization for large-scale datasets, along with real-time explainability and distributed processing to ensure smooth\ndeployment in production environments.\nEnsuring the reliability of xai_evals is a key focus. Ongoing improvements will include Addressing reported bugs\nand inconsistencies in the current version. Optimizing code efficiency and reducing memory overhead. Expanding test\ncoverage to improve robustness across different model architectures."}]}