{"title": "Neuromorphic Spintronics", "authors": ["Atreya Majumdar", "Karin Everschor-Sitte"], "abstract": "Neuromorphic spintronics combines two advanced fields in technology, neuromorphic computing and spintronics, to create brain-inspired, efficient computing systems that leverage the unique properties of the electron's spin. In this book chapter, we first introduce both fields-neuromorphic computing and spintronics-and then make a case for neuromorphic spintronics. We discuss concrete examples of neuromorphic spintronics, including computing based on fluctuations, artificial neural networks, and reservoir computing, highlighting their potential to revolutionize computational efficiency and functionality.", "sections": [{"title": "1 Introduction", "content": "The invention of computers has been instrumental in ushering in a new era where we can quickly perform large computations in the blink of an eye and store vast amounts of data. This advancement can be directly attributed to the development of electronics, which has enabled efficient miniaturization and high processing speeds. The development of electronic technology has been so rapid that the number of transistors in integrated circuits has doubled every two years since 1970. This phenomenon has been coined the name Moore's law [1]. However, transistor miniaturization has hit a plateau in recent years due to technological limitations [2, 3]. Furthermore, advances in deep learning have heightened the demands for computation, memory, and energy. Addressing these issues requires new computing paradigms based on novel materials [4].\nThroughout history, new epochs of civilization have been shaped by the combined innovations of ideas and the methods used to express them. For instance, Johannes Gutenberg's invention of the printing press in 1436 enabled the rapid dissemination of knowledge through books, sparking a transformative global revolution. As computing methods and materials reach saturation, an urgent necessity arises for yet another revolutionary breakthrough. This revolution must encompass advancements in both computation as well as the materials that perform it.\nThe animal world, having undergone millions of years of evolutionary optimization, is a significant inspiration for many modern marvels, such as the airplane. The human brain, in particular, exhibits remarkable pattern recognition abilities while also being energy-efficient. In the late 1980s, Carver Mead introduced the concept of neuromorphic computing, a brain-inspired approach designed to mimic how the brain processes information for computational tasks [5, 6]. While conventional computing developed in tandem with silicon and other semiconductor technologies, adopting a new computational paradigm would require innovative material technologies that naturally fulfill its unique needs. Spintronics-based materials, which utilize the spin degree of freedom, constitute one such class of materials showing promise as candidates for implementing neuromorphic computing. This chapter aims to motivate the potential of neuromorphic spintronics in addressing numerous challenges that plague contemporary computing technologies.\nWe begin this chapter by presenting the promise that spintronics-based materials hold as a platform for neuromorphic computing. This will be followed by exploring various application domains for spintronics-based materials. First, we consider the noisy yet error-tolerant aspect of our brain as an inspiration, and in that light, we discuss computing based on fluctuations using spintronics-based materials. Next, we discuss the implementation of neural networks, which are directly inspired by how neurons and synapses in our brain are configured to perform pattern recognition tasks. The subsequent chapter deals with another neuro-inspired algorithm called reservoir computing, and here we emphasize the usefulness of spintronics systems as suitable reservoirs. In the penultimate chapter, we compare and contrast spintronics-based memories with other unconventional memory technologies. Lastly, we explore the future trajectory of neuromorphic spintronics, anticipating advancements in computational capabilities driven by new discoveries in the physical properties of spintronics materials."}, {"title": "1.1 The promise of spintronics", "content": "The field of electronics is based on using and manipulating the electron's charge. Like the charge, spin is another intrinsic property of the electron. Spintronics, short for \"spin-based electronics\u201d, is a field in physics and engineering that focuses on this degree of freedom for developing technology [7, 8]. Spin is a quantum mechanical"}, {"title": "1.2 A case for neuromorphic spintronics", "content": "Conventional computers are built with silicon-based architectures and hardware, with algorithms tailored to the technological requirements and available resources. Contrary to that, neuromorphic computing is an approach that mimics the neural structure and functioning of the human brain to enhance computational efficiency and processing power. The inspiration could manifest at various levels: the architecture, the computing hardware (including the substrate), or even the software strategies and algorithms."}, {"title": "2 Computing based on fluctuations", "content": "Usually, in the realm of computation and other practical applications, stochasticity and fluctuations are a major nuisance, necessitating extensive algorithmic and instrumental machinery to minimize their impact. The sources of noise are often intrinsic to the system, either due to the inhomogeneities present or thermal fluctuations. This inherent noise can be harnessed to perform computations in various ways, which will be presented in this section. Each of these computational paradigms has specialized use cases based on their properties."}, {"title": "2.1 Stochastic computing", "content": "Originally introduced in the 1960s, stochastic computing emerged as a cost-effective computing scheme that is perfectly suited for computations where the accuracy could be traded in favor of the speed [41, 42]. A common example in everyday life is a vacuum cleaning robot designed to quickly clean the floor, where millimeter-level precision is not a concern. The core concept of this method is to compute with probability values, i.e., numbers between 0 and 1, which can be approximated by random uncorrelated streams of bits, where the proportion of ones approximates the numerical value. Computational operations are performed directly on the bitstreams. For instance, multiplying two numbers is achieved by feeding their respective bitstreams into an AND gate, as exemplified in Fig. 2. Other arithmetic calculations can also be done using gates and multiplexers [42].\nThe primary advantage of this method is that it circumvents the need for the complex and energy-intensive circuits typically used for binary arithmetic calculations. Instead, simple gates are employed, offering a solution with substantially lower energy consumption and reduced space requirements. Furthermore, this approach is more error-tolerant since single erroneous bit flips in the bitstreams only mildly affect the results compared to ordinary bit-based calculations."}, {"title": "2.2 Inverse computing", "content": "Typically, computing involves producing the correct output from a given input through systematic processing. Identifying the inverse relationship - tracing back from the output to the original input \u2014 can also be particularly useful in applications such as integer factorization and invertible logic [49]. Inverse computing is inherently challenging as mathematically, often no unique inverse exists, i.e. multiple inputs often correspond to a single output. For instance, as illustrated in Fig. 3 (a), output 0 of the AND gate corresponds to three different input states.\nThus, the entire input space must be searched to find all inputs that produce the desired output, making this process typically far more resource-intensive than standard forward computing. Gates that can function in reverse would be ideal;\nhowever, an additional challenge is that the input space often also grows exponentially with the number of inputs.\nOne approach is to use fluctuations to explore the entire solution space in parallel, increasing the likelihood of finding all possible input states with equal probability. By arranging fluctuating elements to favor the correct solution, the system becomes statistically more likely to settle into the desired state. This concept is illustrated through a specific example below.\nA promising building block for the basic fluctuating element is the p-bit. A p-bit or a probabilistic bit fluctuates between 0 and 1 [50, 51] in contrast to a classical bit, which takes a definite value, i.e. either 0 or 1. The p-bit represents the probability value as the fraction of time it spends in the state labeled as 1 relative to the entire operational period. In principle, any bistable physical system where the switching probabilities can be tuned by an external bias can be used to implement p-bits, as"}, {"title": "2.3 Token-based Brownian computing", "content": "In magnetic p-bits, the stochasticity originates from the fluctuations in the magnetic states. The thermal Brownian motion of nanomagnetic solitons can also be harnessed for computation in a paradigm called token-based Brownian computing [60]. Whereas for stochastic computing and p-bits, the stochasticity resulted in generating bits, in token-based Brownian computing, the thermal fluctuations are utilized to randomly propagate tokens, which can physically explore the search space of a task. In this context, a token refers to a solitary discrete object whose presence or absence is interpreted as the signal. The computational task or logical operation is mapped into a circuit where the tokens undergo Brownian motion along certain paths guided by a few basic, resource-friendly elements [61, 62].\nA minimal example set of basic elements for performing logical operations consists of a Hub and a C-join [61], as shown in Fig. 4(a). The Hub element is a trijunction, i.e., it consists of three bidirectional wires enabling the token to move randomly along any of the connected wires. The C-join is a four-way intersection that acts as a signal synchronizer: when two tokens enter the element from different directions, the C-join releases them along the other two directions.\nFig. 4 (b) shows a possible realization of a half-adder circuit exploiting only the Hub and the C-join. This basic example illustrates when token-based Brownian computations are useful; the calculation relies only on the thermal motion of the tokens, and almost no external power is needed (besides the ultra-low power required to activate the C-joins.) The disadvantage, however, is the potentially very long time required to obtain the calculation result, as there is no guaranteed time until the tokens reach the output lines. Adding ratchets, another type of basic element, at suitable locations in the circuit can accelerate the convergence to the solution. Ratchets limit the token's movement along a certain direction, i.e., it cannot go backward after a token has passed a ratchet. In the half-adder shown in Fig. 4 (b), they can, for example, be placed after the C-joins.\nUsing spintronics systems, an energy-efficient implementation of the tokens and circuit primitives can be achieved. Skyrmions, or other topologically protected textures that undergo such Brownian motion, can play the role of the tokens [60]. The Hub has been demonstrated using circuit geometry in magnetic materials [63], the C-join through voltage-controlled magnetic anisotropy (VCMA) effects [60], and the ratchet by employing either a VCMA gradient or specially designed wire geometries [64, 65]. An additional advantage of spintronics systems is that they allow to control and enhance the diffusion of magnetic solitons and, in particular, skyrmions via different mechanisms [66]."}, {"title": "3 Spintronics-based neural networks", "content": "Human intelligence relies not only on the capacity to perform computations but also on the ability to recognize patterns. Artificial neural networks (ANNs), inspired by the neural connections in the brain, excel at pattern recognition, driving the majority of the Artificial Intelligence (AI) advancements in the present era [67, 68]. Currently, ANNs are used across various fields, including image recognition, natural language processing, the study of protein folding [69, 70, 71] and its applications have even pervaded many aspects of our daily lives [72] (see also chapter (chapter by Tobias Wand))."}, {"title": "3.1 Basic mode of operation of ANNS", "content": "ANNs consist of two main components: the neurons (or neuronal activations) and the synapses (or synaptic weights) interconnecting the neurons. A deep neural network features multiple neuron layers that transform input into output via hidden layers. Specifically, in a supervised learning setting, an input for a layer is fed into the neurons $(x_i)$. Then, using the synaptic weights $(w_{ij})$ attached to the neurons, the $j$th neuronal activation $a_j$ in the next layer is computed as $a_j = f(\\sum_i W_{ij}x_i)$.\nHere, the activation function $f$ introduces non-linearity into the network, which is indispensable for solving complex problems.\nFor the physical realization of ANNs, generally, factors such as energy efficiency, speed, error tolerance, compatibility with CMOS technology, endurance, low area overhead, and fabrication cost are crucial considerations. In addition, there are specific requirements for the synaptic and neuronal elements, as explained below."}, {"title": "3.1.1 Requirements for Synapses", "content": "The synaptic weights serve as the parameters that characterize the neural network model [67]. They can be regarded as memory elements whose main task is to store the value of the weight in a robust, non-volatile manner for inference. To enable training with materials, the physical properties of the memory elements-typically resistance or conductance-that correspond to weights must be precisely adjusted according to updates from the training algorithm. Resistance-based memories (also called memristors) offer a natural way to implement synapses in the following way [76]: if a weight $w_{ij}$ is encoded as the conductance $g_{ij}$ of a device at the junction between the $i$th input and $j$th output, and if the input and output are represented respectively by voltage $V_i$ and current $I_j$, then using the Ohm's law and Kirchoff's current law we can write the output current as $I_j = \\sum_i g_{ij} V_i$."}, {"title": "3.1.2 Requirements for Neurons", "content": "The neurons are the basic nodes or computational elements of the network. Their main purpose is to implement the accumulation of the inputs from the previous layer multiplied by the connecting weights and the nonlinear activation function $f$. Such computing neurons, which typically take continuous real values, can be implemented using a few transistors. However, the implementation with transistors usually has a high area overhead, is energy-inefficient, and suffers from device-to-device variability.\nAs opposed to the neurons of the simple feed-forward ANNs, the neurons in our brain are dynamic, excitable cells. They accumulate the incoming electrical signal from other neurons in the form of spikes in a leaky manner. While integrating the signal, when the membrane electric potential reaches a certain threshold, it releases an electrical voltage spike. Following this discharge, the membrane potential returns to its baseline state and enters a refractory phase. Based on this idea, Spiking Neural Networks (SNNs) have been proposed as another more bio-realistic variant of neural networks where the neurons communicate with each other via sparse, discrete spiking signals [77]. Although more difficult to train in terms of speed and accuracy, SNNs are more energy-efficient because of the sparse nature of their representation [78]. To emulate SNNs, the neurons must possess a mechanism for generating spikes. Typically, the mechanism used for producing the spikes is called the leaky integrate and fire neurons. They integrate the input signal and only fire when this integrated value reaches a certain threshold [79]."}, {"title": "3.2 Spintronics-based building blocks for neural networks", "content": "Spintronics-based hardware is inherently more energy-efficient than current transistor technology, and research has shown that spintronics technology is congruent with implementing the different building blocks of neural networks. In the subsequent text, we present examples of both spintronics-based synapses and neurons."}, {"title": "3.2.1 Spintronics-based synapses", "content": "Spintronics-based synapses exploit the magnetic state of a physical system to store the synaptic weight. Besides the general advantages of spintronics systems mentioned in section 1.2, spintronics systems offer the reliable and efficient reading and writing of non-volatile magnetic states using effects such as anisotropic magnetoresistance, STT, and SOT. This enables a variety of concepts for spintronics-based synapses, including the following.\nFor example, spintronics synapses using MTJs have been proposed, and the junctions can be utilized in various ways. An instance of this is using them as binary synaptic elements [80], where the two (parallel and anti-parallel) magnetic states represent the two binary states. In addition, the stochastic switching property has been leveraged as stochastic synapses [81] and even as synapses controlled by radio frequency signals [82]. Another class of spintronic synapses is based on magnetic textures like magnetic skyrmions or domain walls. Here, for example, the number of skyrmions in a region dictates the conductance and, consequently, the synaptic weight [83, 84]. For magnetic domain walls, it has been proposed to use the position of the wall as synaptic weight, as this can also change the conductance [85, 86]. Furthermore, antiferromagnetic materials have been identified to have magnetization-switching properties that can be used for synaptic applications [87, 88]."}, {"title": "3.2.2 Spintronics-based neurons", "content": "Various spintronics-based implementations of neurons have been proposed to utilize magnetic states, as discussed in this subsection. In particular, they truly shine when implementing leaky integrate and fire neurons.\nSpin-torque nano-oscillators, a type of MTJ, exhibit spontaneous microwave oscillations when driven by direct current. Their memory-like oscillation amplitudes mimic neuronal leaky integration, while the nonlinear response of voltage oscillation amplitudes to input current or field enables the direct implementation of activation functions [89, 90]. Another example is the superparamagnetic tunnel junction, which is an MTJ with a very low energy barrier between the parallel and anti-parallel states. Here, the switching rate can be tuned by magnetic fields and spin torque effects, enabling them to be used as neurons in a scheme of computing called population coding [91, 92].\nNeurons, like synapses, have also been proposed with magnetic textures. These magnetic textures, such as skyrmions and domain walls, offer the advantage of being controllable through energy-efficient methods, such as applying a low current. This capability allows them to function as carriers of information, akin to neurons, effectively. Furthermore, these textures, owing to their nanoscale dimensions, can be prone to noise either from pinning sites or from thermal fluctuations [93]. This feature holds promise in emulating the functionality of stochastic neurons. Leaky integrate-and-fire neurons based on magnetic textures have also been proposed for skyrmionic or domain wall systems. In these systems, the gradual accumulation"}, {"title": "4 Reservoir computing with spintronics", "content": "Reservoir computing exploits the inherent nonlinear complex dynamics of a system (called the reservoir) to simplify a classification or prediction task. For this, the reservoir project inputs into a higher dimensional space where the classification or prediction task reduces to an easy (e.g. linear) separation task. The first reservoirs were implemented by recurrent neural networks with random but fixed synaptic weights, such that only the last output layer was trained [98]. In recent years, it has become apparent that many physical systems are well-suited as reservoirs. The emerging field of physical reservoir computing leverages the properties of physical systems to enhance computational performance [99].\nReservoir computing has been discussed in detail in chapter (chapter by Michael te Vrugt), but here we want to highlight some key features that a physical system has to have to function as a reservoir computer [100, 101].\n\u2022 Non-linearity: This refers to the input undergoing a nonlinear transformation due to the reservoir.\n\u2022 Complexity: The term complexity relates to the reservoir's ability to effectively project inputs into a space of higher dimensionality, where \u201chigh dimensionality\" refers to a reservoir possessing considerably more degrees of freedom than the inputs have.\n\u2022 Short-term or fading memory: The reservoir's ability to process the input signal's temporal history is characterized by its short-term memory attribute. This attribute prioritizes recent inputs while retaining information from earlier ones, with the output being influenced by past inputs, though their impact gradually fades over time. This fading memory is vital for the system to be more responsive to recent inputs, enabling it to adapt to new input patterns or discard outdated information.\n\u2022 Reproducibility: The reservoir should yield identical responses to identical inputs, provided it has been reset between each input. This attribute, although trivial for software implementations, is a crucial prerequisite for physical reservoirs.\nThe strength of physical reservoir computing lies in its flexibility, allowing a variety of systems that meet the aforementioned criteria to serve as the reservoir."}, {"title": "4.1 Magnetic textures for reservoir computing", "content": "Magnetic systems ideally fulfill all criteria to serve as suitable reservoirs due to their inherently nonlinear and complex responses to external stimuli. Also, their ability to \"forget\u201d information over time \u2013 a characteristic known as fading memory arises from Gilbert damping and other mechanisms that dissipate energy in the magnetization dynamics. Additionally, their compatibility with CMOS technology means magnetic reservoirs can seamlessly integrate into modern electronic devices.\nPhysical reservoir computing has been demonstrated with common magnetic building blocks such as MTJs [90] and spin-vortex nano oscillators [102]. However, despite their utility, these systems fall short in the complexity required for effective reservoir computing, especially in terms of scaling to more difficult tasks. Enhancing their complexity, for instance, through time multiplexing, is necessary to leverage them as reservoirs [103, 104]. Conversely, magnetic systems with more complexity like dipole-coupled nanomagnets [105, 106], spin wave-based reservoirs [105], magnetic metamaterials [107], skyrmion fabrics [108, 109, 110, 111, 112, 113, 114], and other complex spin textures [115, 116, 101] have been used as physical reservoir computers. Other topological spin textures, such as anti-skyrmions, hopfions, and dislocations [117, 118, 119, 120, 121], are gaining attention and in the future, can potentially serve as reservoir. Such diversity in terms of options for the reservoir is shown in Fig. 5."}, {"title": "5 Memory technologies: spintronic implementations and beyond", "content": "The current memory technology landscape is primarily dominated by SRAM, DRAM, and NAND flash. SRAM utilizes bistable flip-flop circuits made of transistors to store data as long as power is provided, making it fast and ideal for cache memory. DRAM, on the other hand, stores data in capacitors and transistors, requiring periodic refreshing to retain information, and is commonly used as main memory in computers due to its higher density and lower cost. NAND flash memory, composed of floating-gate transistors, stores data by adjusting charge levels in memory cells, allowing it to retain data without power. It is widely used in SSDs, USB drives, and memory cards because of its high storage capacity and durability. All of these memory technologies depend on charge storage to represent a memory state. However, a new class of memory technologies is emerging that relies on resistance instead of charge, including spintronics-based memories, which stand out for their non-volatile nature, fast operation, and scalability, among other benefits. The long-term vision for this class of memories is not to fully replace existing silicon-based technology but to serve specific use cases, such as embedded applications for edge"}, {"title": "6 Summary and outlook", "content": "In this chapter, we have demonstrated the significant potential of the interdisciplinary field of Neuromorphic Spintronics to enhance various aspects of modern computing technologies. In section 1, we emphasize the suitability of spintronic materials as a foundation for implementing bio-inspired neuromorphic computing. The subsequent sections 2, 3, 4, and 5 explore different specialized use cases where this concept could have a substantial impact. While these examples vary widely in their applications, they are all unified by their spintronic material-based implementation and underlying neuromorphic principles.\nMoreover, the field of spintronics is witnessing rapid developments, greatly increasing its potential for neuromorphic applications. One exciting area of progress is the exploration of magnetic textures not only in 2D but also in 3D [143, 123], greatly expanding the range of possibilities. In the third dimension, the complexity of physical processes and responses increases due to interactions with more neighbors, offering improved functionality and tunability. Additionally, 3D systems allow for higher-density processing power and more readout nodes, leading to compact devices and increased error robustness [100].\nApart from 3D textures, substantial advancements have been made in the domain of multiferroic materials, where coexisting order parameters can significantly enhance the complexity and functionality of devices [144, 100]. By leveraging multiple order parameters, time scales, and length scales, the concept of multi-physics enhances the opportunities for parallel, in-materio, neuromorphic computing.\nAs computing devices become increasingly personalized, brain-computer interfaces necessitate the integration of computing elements into the human body. In this context, organic spintronics offers significant advantages due to its cost-effective fabrication, lightweight nature, biocompatibility, and biodegradability [145]. The convergence of these technologies promises to revolutionize how we interact with and utilize computing systems, paving the way for more seamless and integrated solutions. Overall, the advancements discussed throughout this chapter highlight the transformative potential of implementing intelligence with cutting-edge physical systems, paving the way for technological breakthroughs."}]}