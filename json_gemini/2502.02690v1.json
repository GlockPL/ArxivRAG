{"title": "Controllable Video Generation with Provable Disentanglement", "authors": ["Yifan Shen", "Peiyuan Zhu", "Zijian Li", "Shaoan Xie", "Zeyu Tang", "Namrata Deka", "Zongfang Liu", "Guangyi Chen", "Kun Zhang"], "abstract": "Controllable video generation remains a significant challenge, despite recent advances in generating high-quality and consistent videos. Most existing methods for controlling video generation treat the video as a whole, neglecting intricate fine-grained spatiotemporal relationships, which limits both control precision and efficiency. In this paper, we propose Controllable Video Generative Adversarial Networks (CoVOGAN) to disentangle the video concepts, thus facilitating efficient and independent control over individual concepts. Specifically, following the minimal change principle, we first disentangle static and dynamic latent variables. We then leverage the sufficient change property to achieve component-wise identifiability of dynamic latent variables, enabling independent control over motion and identity. To establish the theoretical foundation, we provide a rigorous analysis demonstrating the identifiability of our approach. Building on these theoretical insights, we design a Temporal Transition Module to disentangle latent dynamics. To enforce the minimal change principle and sufficient change property, we minimize the dimensionality of latent dynamic variables and impose temporal conditional independence. To validate our approach, we integrate this module as a plug-in for GANs. Extensive qualitative and quantitative experiments on various video generation benchmarks demonstrate that our method significantly improves generation quality and controllability across diverse real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "Video generation (Vondrick et al., 2016; Tulyakov et al., 2018; Wang et al., 2022) has become a prominent research focus, driven by its wide-ranging applications in fields such as world simulator (OpenAI, 2024), autonomous driving (Wen et al., 2024; Wang et al., 2023), and medical imaging (Li et al., 2024a; Cao et al., 2024). In particular, controllable video generation (Zhang et al., 2025) is essential for advancing more reliable and efficient video generation models.\nNumerous methods have been proposed over the past decade for better video generation. Among these efforts, VideoGAN (Vondrick et al., 2016) is a pioneering approach that treats the video as a 4D spatiotemporal block and uses a unified representation (i.e., a multivariate normal distribution in VideoGAN) for generation. Recent methods (Ho et al., 2022; Zhou et al., 2022; Yang et al., 2024b; Zheng et al., 2024) that have demonstrated strong performance can also be classified into this category, with differences in the frameworks (e.g., diffusion models and VAEs) and shape of the representation (e.g., vector or spatiotemporal block). However, such a simple representation neglects the intricate spatial-temporal relationships, and limits the precision and efficiency of controllable video generation. For example, videos featuring the same object but under different motions may have vastly different representations.\nTo address this issue, one intuitive solution is to learn a disentangled representation of the video, within whom the internal relationships are often not considered. (Hyv\u00e4rinen & Oja, 2000; Tulyakov et al., 2018; Yu et al., 2022; Skorokhodov et al., 2022; Wei et al., 2024) explicitly decompose video generation into two parts: motion and identity, representing dynamic and static information, respectively. This separation allows for more targeted control over each aspect, making it possible to modify the motion independently without affecting the identity. (Zhang et al., 2025; Shen et al., 2023) leverage attention mechanisms to further disentangle different concepts within the video, enhancing the ability to control specific features with greater precision. (Fei et al., 2024; Lin et al., 2023) utilize Large Language Models to find the intricate video temporal dynamics within the video and then enrich the scene with reasonable details, enabling a more transparent generative process. These methods are intuitive and effective, yet they lack a solid guarantee of disentanglement, making the control less predictable and potentially leading to unintentional coupling of different aspects of the video."}, {"title": "Controllable Video Generation with Provable Disentanglement", "content": "These limitations of previous approaches motivate us to rethink the paradigm of video generation. Inspired by recent advancements in nonlinear Independent Component Analysis (ICA) (Hyvarinen & Morioka, 2017; Khemakhem et al., 2020; Yao et al., 2022; Hyvarinen & Morioka, 2016) and the successful applications like video understanding (Chen et al., 2024), we propose the Controllable Video Generative Adversarial Network (CoVoGAN) with a Temporal Transition Module plugin. Building upon StyleGAN2-ADA (Karras et al., 2020), we distinguish between two types of factors: the dynamic factors that evolve over time, referred to as style dynamics, and the static factors that remain unchanged, which we call content elements. This distinction clarifies the separation between style dynamics (motion), and content elements (identity), allowing for more precise control. By leveraging the minimal change principle, we demonstrate their blockwise identifiability (Von K\u00fcgelgen et al., 2021; Li et al., 2024b) and find the conditions under which motion and identity can be disentangled, explaining the effectiveness of the previous line of methods that separate motion and identity. In addition, we employ sufficient change property to disentangle different concepts of motion, such as head movement or eye blinking. Specifically, we introduce a flow (Rezende & Mohamed, 2015) mechanism to ensure that the estimated style dynamics are mutually independent conditioned on the historical information. Furthermore, we prove the component-wise identifiability of the style dynamics and provide a disentanglement guarantee for the motion in the video.\nWe conduct both quantitative and qualitative experiments on various video generation benchmarks. For quantitative evaluation, we use FVD (Unterthiner et al., 2019) to assess the quality of the generated videos. For qualitative analysis, we evaluate the degree of disentanglement by manipulating different dimensions of the latent variables across multiple datasets and comparing the resulting video outputs. Experimental results demonstrate that our method significantly outperforms other GAN-based video generation models with similar backbone structures to CoVoGAN. Additionally, our method exhibits greater robustness during training and faster inference speed compared to baseline approaches.\nKey Insights and Contributions of our research include:\n\u2022 We propose a Temporal Transition Module to achieve a disentangled representation, which leverages minimal change principle and sufficient change property.\n\u2022 We implement the Module in a GAN, i.e., CoVOGAN, to learn the underlying generative process from video data with disentanglement guarantees, enabling more precise and interpretable control.\n\u2022 To the best of our knowledge, this is the first work to provide an identifiability theorem in the context of"}, {"title": "2. Related Works", "content": "video generation. This helps to clarify previous intuitive yet unproven techniques and suggests potential directions for future exploration.\n\u2022 Extensive evaluations across multiple datasets demonstrate the effectiveness of CoVoGAN, achieving superior results in terms of generative quality, controllability, robustness, and computational efficiency."}, {"title": "2.1. Controllale Video Generation", "content": "Recent advances in controllable video generation have led to significant progress, with text-to-video (T2V) models (Yang et al., 2024b; Singer et al., 2022; Ho et al., 2022; Zhou et al., 2022; Zheng et al., 2024) achieving impressive results in generating videos from textual descriptions. However, effectiveness of the control is highly dependent on the quality of the input prompt, making it difficult to achieve fine-grained control over the generated content. An alternative method for control involves leveraging side information such as pose (Tu et al., 2024; Zhu et al., 2025), camera motion (Yang et al., 2024a; Wang et al., 2024), depth (Liu et al., 2024; Xing et al., 2024) and so on. While this approach allows for more precise control, it requires paired data, which can be challenging to collect. Besides, most of the aforementioned alignment-based techniques share a common issue: the control signals are directly aligned with the entire video. This issue not only reduces efficiency but also complicates the task of achieving independent control over different aspects of the video, which further motivates us to propose a framework to find the disentanglement representation for conditional generation."}, {"title": "2.2. Nonlinear Independent Component Analysis", "content": "Nonlinear independent component analysis offers a potential approach to uncover latent causal variables in time series data. These methods typically utilize auxiliary information, such as class labels or domain-specific indices, and impose independence constraints to enhance the identifiability of latent variables. Time-contrastive learning (TCL) (Hyvarinen & Morioka, 2016) builds on the assumption of independent sources and takes advantage of the variability in variance across different segments of data. Similar Permutation-based contrastive learning (PCL) (Hyvarinen & Morioka, 2017) introduces a learning framework that tell true independent sources from their permuted counterparts. Additionally, i-VAE (Khemakhem et al., 2020) employs deep neural networks and Variational Autoencoders (VAEs) to closely approximate the joint distribution of observed data and auxiliary non-stationary regimes. Recently, (Yao et al., 2021; 2022) extends the identifiability to linear and nonlinear non-Gaussian cases without auxiliary variables, respectively. CaRiNG (Chen et al., 2024) further tackles"}, {"title": "3. Problem Setup", "content": null}, {"title": "3.1. Generative Process", "content": "Consider a video sequence \\(V = {X_1,X_2,...,X_T}\\) consisting of \\(T\\) consecutive frames. Each frame \\(X_t \\in \\mathbb{R}^{n_x}\\) is generated via an arbitrary nonlinear mixing function \\(g\\), which maps a set of latent variables to the observed frame \\(X_t\\). The latent variables are decomposed into two distinct parts: \\(z_t^s \\in \\mathbb{R}^{n_s}\\), capturing the style dynamics that evolve over time, and \\(z^c \\in \\mathbb{R}^{n_c}\\), encoding the content variables that remain consistent across all frames of the video. Furthermore, these latent variables are assumed to arise from a stationary, non-parametric, time-delayed causal process.\nAs shown in Figure 1 and Equation 1, the generative process is formulated as:\n\\[\\begin{aligned}\nX_t &= g(z_t^s, z^c), \\\\\nz_{t,i}^s &= f_i^s(\\text{Pa}(z_{t,i}^s), \\epsilon_{t,i}), \\text{ with } \\\\\nz^c &= f^c(\\epsilon^c), \\\\\n\\epsilon_{t,i} &\\sim p_{\\epsilon_{t,i}}^s \\\\\n\\epsilon^c_j &\\sim p_{\\epsilon^c_j}^c\n\\end{aligned}\\)\\]\nin which \\(z_{t,i}^s, z_j^c \\in \\mathbb{R}\\) refers to the \\(i\\)-th entry of \\(z_t^s\\) and \\(j\\)-th entry of \\(z^c\\), respectively. \\(\\text{Pa}(z_{t,i}^s)\\) refers to the time-daleyed parents of \\(z_{t,i}^s\\). All noise terms \\(\\epsilon_{t,i}\\) and \\(\\epsilon^c\\) are independently sampled from their respective distributions: \\(p_{\\epsilon_{t,i}}^s\\) for the \\(i\\)-th entry of the style dynamics, and \\(p_{\\epsilon^c_j}^c\\) for the \\(j\\)-th entry of the content elements. The components of \\(z_t^s\\) are mutually independent, conditioned on all historical variables \\(\\bigcup_{t=1}^T \\text{Pa}(z_t^s)\\). The non-parametric causal transition \\(f_i^s\\) enables an arbitrarily nonlinear interaction between the noise term \\(\\epsilon_{t,i}\\) and the set of parent variables \\(\\text{Pa}(z_{t,i}^s)\\), allowing for flexible modeling of the style dynamics."}, {"title": "3.2. Identification of the Latent Causal Process", "content": "For simplicity, we denote \\(f^s\\) as the group of functions \\({f_i^s}_{i=1}^{n_s}\\), and similarly for \\(f^c, p^s, p^c\\).\nDefinition 3.1 (Observational equivalence). Let \\(V = {X_1, X_2, \u2026, X_T}\\) represent the observed video generated by the true generative process specified by \\((g, f^s, f^c, p^s, p^c)\\), as defined in Equation 1. A learned generative model \\((\\hat{g}, \\hat{f}^s, \\hat{f}^c, \\hat{p}^s, \\hat{p}^c)\\) is observationally equivalent to the true process if the model distribution \\(P_{(\\hat{g},\\hat{f}^s, \\hat{f}^c, \\hat{p}^s, \\hat{p}^c)}(V)\\) matches the data distribution \\(P_{(g,f^s,f^c,p^s,p^c)}(V)\\) for all values of \\(V\\).\nIllustration. When observational equivalence is achieved, the distribution of videos generated by the model exactly matches that of the ground truth, i.e., the training set. In other words, the model produces video data that is indistinguishable from the actual observed data.\nDefinition 3.2 (Blockwise Identification of Generative Process). Let the true generative process be \\((g, f^s, f^c, p^s, p^c)\\) as specified in Equation 1 and let its estimation be \\((\\hat{g}, \\hat{f}^s, \\hat{f}^c, \\hat{p}^s, \\hat{p}^c)\\). The generative process is identifiable up to the subspace of style dynamics and content elements, if the observational equivalence ensures that the estimated \\((\\hat{z}^s, \\hat{z}^c)\\) satisfies the condition that there exist bijective mappings from \\((\\hat{z}^s, \\hat{z}^c)\\) to \\((z^s, z^c)\\) and from \\(\\hat{z}^c\\) to \\(z^c\\). Formally, there exists invertible functions \\(h: \\mathbb{R}^{n_s + n_c} \\rightarrow \\mathbb{R}^{n_s+n_c}\\) and \\(h_c: \\mathbb{R}^{n_c} \\rightarrow \\mathbb{R}^{n_c}\\) such that\n\\[\\begin{aligned}\nP_{(\\hat{g},\\hat{f}^s, \\hat{f}^c, \\hat{p}^s, \\hat{p}^c)}(V) = P_{(g,f^s,f^c,p^s,p^c)}(V)\\\\\n\\Rightarrow [z^s,z^c] = h([\\hat{z}^s,\\hat{z}^c]), z^c = h_c(\\hat{z}^c),\n\\end{aligned}\\)\\]\nwhere \\([.]\\) denotes concatenation.\nIllustration. When blockwise identification is achieved, content elements are effectively disentangled. As a result, motion control can be applied independently, allowing manipulation of motion without altering the video's content. For example, in a video where a camera moves forward, it becomes easy to change the camera's direction without affecting the scene itself.\nDefinition 3.3 (Component-wise Identification of Style Dynamics). On top of Definition 3.2, when \\(h^s\\) is a combination of permutation \\(\\pi\\) and a component-wise invertible transformation \\(T\\). Formally,\n\\[\\begin{aligned}\nP_{(\\hat{g},\\hat{f}^s, \\hat{f}^c, \\hat{p}^s, \\hat{p}^c)}(V) = P_{(g,f^s,f^c,p^s,p^c)}(V)\\\\\n\\Rightarrow z^s = (\\pi \\cdot T)(\\hat{g}^{-1}(X_t)).\n\\end{aligned}\\)\\]\nIllustration. Achieving component-wise identification ensures that each estimated style variable corresponds exactly to one true style variable, which facilitates the disentangling of motion features. This enables efficient and precise control over video generation. For example, when generating a"}, {"title": "4. Theoritical Analysis", "content": "video of a person's face, one can control different aspects, such as head movements or eye blinks, by adjusting the corresponding variables.\nIn this section, we discuss the conditions under which the blockwise identification (Definition 3.2) and component-wise identification (Definition 3.3) hold."}, {"title": "4.1. Blockwise Identification", "content": "Without loss of generality, we first consider the case where \\(\\text{Pa}(z_{t,i}^s) = z_{t-1}^s\\), meaning that the time-dependent effects are governed by the dynamics of the previous time step.\nDefinition 4.1. (Linear Operator) Consider two random variables \\(a\\) and \\(b\\) with support \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\), the linear operator \\(\\mathcal{L}_{b|a}\\) is defined as a mapping from a density function \\(p_a\\) in some function space \\(\\mathcal{F}(\\mathcal{A})\\) onto the density function \\(\\mathcal{L}_{b|a} p_a\\) in some function space \\(\\mathcal{F}(\\mathcal{B})\\),\n\\[\\mathcal{F}(\\mathcal{A}) \\rightarrow \\mathcal{F}(\\mathcal{B}) : p_b = \\mathcal{L}_{b|a} p_a = \\int_{\\mathcal{A}} p_{b|a}(a) p_a(a) da.\\]\nTheorem 4.2 (Blockwise Identifiability). Consider video observation \\(V = {X_1,X_2, ..., X_T}\\) generated by process \\((g,f^s,f^c,p^s,p^c)\\) with latent variables denoted as \\(z_t^s\\) and \\(z^c\\), according to Equation 1, where \\(X_t \\in \\mathbb{R}^{n_x}, z_t^s \\in \\mathbb{R}^{n_s}, z^c \\in \\mathbb{R}^{n_c}\\). If assumptions\n\u2022 B1 (Positive Density) the probability density function of latent variables is always positive and bounded;\n\u2022 B2 (Minimal Changes) the linear operators \\(\\mathcal{L}_{X_{t+1}|z_t^s,z^c}\\) and \\(\\mathcal{L}_{X_{t-1}|X_{t+1}}\\) are injective for bounded function space;\n\u2022 B3 (Weakly Monotonic) for any \\(\\dot{z}_t^s, \\dot{z}^c, z_t^s, z^c \\in \\mathcal{Z}^s \\times \\mathcal{Z}^c\\) (\\((\\dot{z}_t^s, \\dot{z}^c) \\neq (z_t^s, z^c)\\)), the set \\({X_t: p(X_t|(\\dot{z}_t^s, \\dot{z}^c)) \\neq p(X_t|(z_t^s, z^c))}\\) has positive probability, and conditional densities are bounded and continuous;\n\u2022 B4 (Blockwise Independence) the learned \\(z_t^s\\) is independent with \\(z^c\\);\nare satisfied, then \\(z_t^s\\) is blockwisely identifiable with regard to \\(z_t^s\\) from learned model \\((\\hat{g},\\hat{f}^s, \\hat{f}^c, \\hat{p}^s, \\hat{p}^c)\\) under Observation Equivalence.\nIllustration of Assumptions. The assumptions above are commonly used in the literature on the identification of latent variables under measurement error (Hu & Schennach, 2008). Firstly, Assumption B1 requires a continuous distribution. Secondly, Assumption B2 imposes a minimal requirement on the number of variables. The linear operator \\(\\mathcal{L}_{b|a}\\) ensures that there is sufficient variation in the density of"}, {"title": "4.2. Component-wise Identification", "content": "\\(b\\) for different values of \\(a\\), thereby guaranteeing injectivity. In a video, \\(X_t\\) is of much higher dimensionality compared to the latent variables. As a result, the injectivity assumption is easily satisfied. In practice, following the principle of minimal changes, if a model with fewer latent variables can successfully achieve observational equivalence, it is more likely to learn the true distribution. Assumption B3 requires the distribution of \\(X_t\\) changes when the value of latent variables changes. This assumption is much weaker compared to the widely used invertibility assumption adopted by previous works, such as (Yao et al., 2022). Assumption B4 requires that the learned latent variables have independent style dynamics and content elements.\nOverall, the first three assumptions about the data are easily satisfied in real-world video scenarios. The last assumption, however, is a requirement for models. Most models that explicitly separate motion and identity satisfy this assumption.\nTheorem 4.3 (Component-wise Identifiability). Consider video observation \\(V = {X_1,X_2,...,X_\\mathcal{T}}\\) generated by process \\((g,f^s,f^c,p^s,p^c)\\) with latent variables denoted as \\(z_t^s\\) and \\(z^c\\), according to Equation 1, where \\(X_t \\in \\mathbb{R}^{n_x},z_{t,i}^s \\in \\mathbb{R}^{n_s}, z^c \\in \\mathbb{R}^{n_c}\\). Suppose assumptions in Theorem 4.2 hold. If assumptions\n\u2022 C1 (Smooth and Positive Density) the probability density function of latent variables is always third-order differentiable and positive;\n\u2022 C2 (Sufficient Changes) let \\(\\eta_{t,i} \\equiv \\log p(z_{t,i}^s |z_{t-1}^s)\\) and\n\\[\\mathbf{v}_{t,l} \\triangleq \\begin{pmatrix}\n\\frac{\\partial^2 \\eta_{t,1}}{\\partial z_{t,1} \\partial z_{t-1,l}} \\\\\n\\frac{\\partial^2 \\eta_{t,1}}{\\partial z_{t,1} \\partial z_{t-1,l}} \\\\\n... \\\\\n\\frac{\\partial^2 \\eta_{t,n_s}}{\\partial z_{t,n_s} \\partial z_{t-1,l}} \\\\\n\\frac{\\partial^2 \\eta_{t,n_s}}{\\partial z_{t,n_s} \\partial z_{t-1,l}}\n\\end{pmatrix}\\]\nfor \\(l \\in \\{1,2,\\dots,n_s\\}\\). For each value of \\(z_t^s\\), there exists \\(2n_s\\) different of values of \\(z_{t-1,l}^s\\) such that the \\(2n_s\\) vector \\(\\mathbf{v}_{t,l} \\in \\mathbb{R}^{2n_s}\\) are linearly independent;\n\u2022 C3 (Conditional Independence) the learned \\(z_t^s\\) is independent with \\(z^c\\), and all entries of \\(z_{t}^s\\) are mutually independent conditioned on \\(z_{t-1}^s\\);"}, {"title": "5. Approach", "content": "are satisfied, then \\(z_t^s\\) is component-wisely identifiable with regard to \\(z_t^s\\) from learned model \\((\\hat{g},\\hat{f}^s, \\hat{f}^c, \\hat{p}^s, \\hat{p}^c)\\) under Observation Equivalence.\nGiven our results on identifiability, we implement our model, CoVoGAN. Our architecture is based on StyleGAN2-ADA (Skorokhodov et al., 2022), incorporating a Temporal Transition Module in the Generator to enforce the minimal change principle and sufficient change property. Additionally, we add a Video Discriminator to ensure observational equiva-"}, {"title": "5.1. Model Structure", "content": "lence of the joint distribution \\(p(V)\\).\nNoise Sampling. The structure of the generator is shown in Figure 2. To generate a video with length \\(T\\), we first independently sample random noise from a normal distribution \\(\\epsilon \\sim \\mathcal{N}(0, I)\\). We then naively split them into several parts, i.e., \\(\\epsilon = [\\epsilon^c; \\epsilon_1; \\epsilon_2; \\dots; \\epsilon_Z]\\).\nTemporal Transition Module. Following the generative process in Equation 1, we handle \\(z^s\\) and \\(z^c\\) separately. On the one hand, we employ an autoregressive model to capture historical information, followed by a conditional flow to generate \\(z^s\\). Specifically, we implement a Gated Recurrent Unit (GRU) (Chung et al., 2014) and Deep Sigmoid Flow (DSF) (Huang et al., 2018), formulated as\n\\[\\begin{aligned}\nh_t &= \\text{GRU}(h_{t-1}, \\epsilon_t), \\\\\nz_{t,i}^s &= \\text{DSF}_i(\\epsilon_{t,i}; h_{t-1}).\n\\end{aligned}\\)\\]\nOn the other hand, since \\(z^c\\) are not required to be mutually independent, we use an MLP to generate \\(z^c\\), i.e.,\n\\[z^c = \\text{MLP}(\\epsilon^c).\\]\nConcatenate \\(z^s\\) and \\(z^c\\), and then we obtain the disentangled representation \\(z_t = [z_t^s z^c]\\) for each frame at time step \\(t\\) of the video.\nSynthesis Network. The synthesis network is designed in the same way as StyleGAN2-ADA. The generated representation \\(z_t\\) is first fed into the mapping network to obtain a semantic vector \\(w(z_t) \\in \\mathcal{W}\\), and then the \\(t\\)-th frame of the video is generated by the convolutional network with \\(w\\).\nDiscriminator Structure. To ensure observational equivalence, we implement a video discriminator \\(D_V\\) separate from the image discriminator \\(D_I\\). For the image discriminator, we follow the design of the original StyleGAN2-ADA. For the video discriminator, we adopt a channel-wise concatenation of activations at different resolutions to model and manage the spatiotemporal output of the generator.\nLoss. In addition to the original loss function of StyleGAN2-ADA, we introduce two additional losses: (1) a video discriminator loss, and (2) a mutual information maximization term (Chen et al., 2016) between the latent dynamic variables \\(z\\) and the intermediate layer outputs of the video discriminator. This encourages the model to learn a more informative and structured representation."}, {"title": "5.2. Relationship between Model and Theorem.", "content": "Following the results of Section 4 and 5.1, we have theoretically and empirically built a temporal transition module to obtain a provable disentangled representation and generate more interpretable results.\nIn this subsection, we discuss the relationship between the CoVOGAN model and the Theorem.\nBlockwise Identification. As discussed in Theorem 4.2, achieving blockwise identifiability benefits from minimizing the dimension \\(n_s\\) of the style dynamics, especially when"}, {"title": "6. Experiments", "content": "the true \\(n_s\\) is unknown. In practice, this translates to a hyperparameter selection question. In our experiments, we opted for a relatively modest value of \\(n_s\\) and observed that it suffices to attain a satisfactory level of disentanglement capability. Furthermore, as required by assumption B4, the learned variables \\(\\hat{z}^s\\) and \\(\\hat{z}^c\\) are blockwise independent, i.e., \\(\\hat{z}^s \\bot \\hat{z}^c\\). This independence is necessary to achieve blockwise identifiability.\nComponenetwise Identification. As outlined in Theorem 4.3, the sufficient change property is a critical assumption for achieving identifiability. To enforce temporally conditional independence, we employ a component-wise flow model that transforms a set of independent noise variables \\(\\epsilon_{t,i}\\) into the style dynamics, conditioned on historical information \\(h_{t-1}\\). Furthermore, the flow model is designed to maximally preserve the information from \\(\\epsilon_{t,i}\\) to \\(z_{t,i}^s\\), enabling the model to effectively capture sufficient variability in the data.\nNote that when computing the historical information \\(h_t\\), we utilize \\(\\epsilon^s\\) instead of \\(z_t^s\\) (as illustrated in the generative process in Equation 1) as the condition for the component-wise flow. This approach offers two key advantages. First, it simplifies the model architecture since the flow does not need to incorporate the output from another flow. Second, the noise terms already fully characterize the corresponding style dynamics, which remains consistent with the theoretical framework.\nFurthermore, given that the precise time lag of dynamic variables remains unspecified a priori in the dataset, the GRU's gating mechanism can selectively filter out irrelevant historical information that lies outside \\(\\text{Pa}(z_t^s)\\). This capability enables the model to demonstrate significantly superior performance compared to traditional non-gated architectures, such as vanilla RNNs.\nA detailed ablation study is presented in Section D.1.\nIn this section, we perform extensive experimental validation to comprehensively evaluate CoVoGAN. We design both qualitative and quantitative experiments to systematically assess the generation quality and controllability of our proposed model. Additionally, we conduct comparative studies to evaluate the robustness and computational efficiency of our model against state-of-the-art baselines. Finally, we perform comprehensive ablation studies to demonstrate the effectiveness of our proposed modules and analyze their individual contributions."}, {"title": "6.1. Experimental Setups", "content": "Datasets. We evaluate our model on three different real-world datasets: FaceForensics (R\u00f6ssler et al., 2018), SkyTimelapse (Xiong et al., 2018), and RealEstate (Zhou et al.,"}, {"title": "6.3. Controllability", "content": "Evaluation metrics. To comprehensively evaluate the performance of CoVoGAN, we employ both quantitative and"}, {"title": "7. Conclusion", "content": "2018). These datasets respectively capture dynamic variations in facial expressions, sky transitions, and complex camera movements, providing comprehensive evaluation scenarios for our method. FaceForensics and SkyTimelapse are widely adopted benchmarks in video generation research, particularly for facial manipulation and natural scene modeling. The RealEstate dataset contains video sequences featuring complex camera movements within static scenes. The camera motions in RealEstate are more intuitive and easily interpretable by human observers, including forward/backward movements, lateral shifts, etc. We particularly leverage this dataset to effectively demonstrate the precise controllability of CoVoGAN in handling explicit camera motion parameters. All datasets consist of videos with a resolution of 256 \u00d7 256 pixels, and we employ standard train-test splits for fair evaluation. Detailed statistics and additional information about the datasets are provided in Appendix B.\nBlockwise Disentanglement. Figure 4 demonstrates the video controllability of CoVoGAN in comparison with baseline methods, highlighting our model's superior capability of disentanglement between motion and content. The analysis follows a systematic procedure: we first generate a base video sequence, then apply a controlled modification by adding a value to specific motion-related latent variables.\nFor CoVoGAN, we modify one dimension of the style dynamics \\(z_t^s\\). For StyleGAN-V and MoStGAN-V, we manipulate one dimension of its (latent) motion code. We apply equivalent modifications to the corresponding latent dimensions in each baseline model. To validate the consistency of our controllability analysis, we randomly sample three distinct video sequences and apply identical modifications to their respective latent representations. DIGAN is not compared since there are no specific variables for motion.\nThe results show that our proposed CoVoGAN model learns a disentangled representation that effectively separates style dynamics from content elements. (1) This disentanglement enables independent manipulation of motion characteristics while preserving content consistency. (2) A key advantage of this approach is that identical modifications to the style latent space consistently produce similar motion patterns across different content identities.\nBaseline models achieve only partial disentanglement, exhibiting two major limitations: (1) visual distortions of the modified videos and (2) inconsistent or misaligned motion patterns when applied to different identities.\nComponenet-wise Disentanglement. We further illustrate the precise control over individual motion components. This capability is enabled by the component-wise identifiability of our model, which ensures that each latent dimension corresponds to a specific and interpretable motion attribute.\nOur experimental procedure begins with randomly sampling two distinct video sequences, as illustrated in the first row of Figure 5. We then selectively modify the latent dimension corresponding to eye blinking dynamics in the second line. Subsequently, we modify a second latent dimension controlling head shaking motion while maintaining the previously adjusted eye blinking pattern in the last line. The results show naturalistic head movements from left to right, synchronized with the preserved eye blinking, illustrating our model's capability for independent yet coordinated control of multiple motion components."}, {"title": "6.4. More Evaluations", "content": "In this paper, we proposed a Temporal Transition Module and implemented it in a GAN to achieve CoVoGAN. By leveraging the principle of minimal and sufficient changes, we successfully disentangled (1) the motion and content of a video, and (2) different concepts within the motion. We established an identifiability guarantee for both block-wise and component-wise disentanglement. Our proposed CoVOGAN model demonstrates high generative quality, controllability, and a more robust and computationally efficient structure. We validated the performance on various datasets and conducted ablation experiments to further confirm the effectiveness of our model.\nRobustness. The architectural simplicity of CoVOGAN contributes to a more stable and efficient convergence compared to baseline models, as shown in Figure 6. Our model"}]}