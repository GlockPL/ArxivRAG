{"title": "Automatic Summarization of Long Documents", "authors": ["Naman Chhibbar", "Jugal Kalita"], "abstract": "A vast amount of textual data is added to the internet daily, making utilization and interpretation of such data difficult and cumbersome. As a result, automatic text summarization is crucial for extracting relevant information, saving precious reading time. Although many transformer-based models excel in summarization, they are constrained by their input size, preventing them from processing texts longer than their context size. This study introduces three novel algorithms that allow any LLM to efficiently overcome its input size limitation, effectively utilizing its full potential without any architectural modifications. We test our algorithms on texts with more than 70,000 words, and our experiments show a significant increase in BERTScore with competitive ROUGE scores.", "sections": [{"title": "Introduction", "content": "Due to the ever-increasing amount of textual data available online, document summarization has become crucial for efficient and accurate extraction of relevant information. Over the past few years, Large Language Models (LLMs) based on the transformer architecture (Vaswani et al., 2017) have shown ground-breaking abilities for NLP tasks, including document summarization (Yadav et al., 2023). Recent developments have demonstrated remarkable improvements in the relevancy and coherence of summaries generated by such LLMs.\nHowever, long document summarization, which makes reading, interpreting, and extracting information from vast texts accurate and efficient, remains a major challenge. One of the major limitations in the transformer architecture is limited context size, stemming from the quadratic memory and computational complexity of the attention mechanism (Du et al., 2023). This limitation hampers the extraction of relevant information from lengthy texts, where summarization is essential to overcome the time, effort, and interpretive challenges posed by the complexity of such documents.\nWe experiment with three novel approaches to address the input size limitations of transformers. The methods introduced do not include any architectural modifications and can be incorporated into any existing pipeline. We believe that these methods can effectively utilize the full potential of any existing LLM. Though our experiments only include the task of summarization, our methods can be applied to any NLP task (such as sequence classification, question-answering, and NLI) which requires processing long texts.\nWe start by stating the problem statement (Section 2) and discussing related works (Section 3) to gain insights into the problem and the state-of-the-art solutions. We then introduce the datasets (Section 4) and methodology (Section 5) used in our experiments. For evaluating our results, we present some common metrics (Section 6) used in text summarization. We end the report by discussing our experimental findings (Section 7) and potential future work (Section 8), and concluding the study (Section 9)."}, {"title": "Problem Statement", "content": "Our goal is to pre-process and manipulate a long document (with theoretically indefinite length) such that it fits within the context size of the model while retaining important information. Practically, we have noticed that the document length may be up to ten times the context size of the model used. For our experiments, we aim to reduce the summary length to about 400 words or less, preserving maximal salient information and coherence."}, {"title": "Related Works", "content": "Golia and Kalita (2024) take a \"Divide and Conquer\" approach to address sequence length limitations in summarizing long meeting transcripts.\nThey begin by segmenting the transcript and then use the BART (Bidirectional and Auto-Regressive Transformer) (Lewis et al., 2020) model to summarize each segment individually. These segment summaries are then recursively combined and summarized until a single summary remains. This method performs well with long documents but may take a considerable amount of time to converge due to repeated calls to the model.\nThere have also been efforts to improve the efficiency of the attention mechanisms in transformers. Beltagy et al. (2020) introduce the Longformer, which replaces the quadratic self-attention mechanism in the Transformer architecture with a sliding window self-attention, resulting in a linear complexity with respect to the input size. To capture long-range dependencies, they include global attention at specific token positions. Huang et al. (2021) modify the encoder-decoder attention mechanism such that each attention head in the decoder attends to $n/s_h$ tokens in the input sequence, where n is the input length and $s_h$ is the number of heads. This method has a complexity of $O(mn/s_h)$, where m is the length of the output sequence. Bertsch et al. (2023) introduce Unlimiformer, which also modifies the encoder-decoder attention in a transformer. The attention heads in the decoder only attend to the tokens picked by their k-Nearest-Neighbor (kNN) algorithm. The kNN indices between the input tokens are created by the hidden states generated in the encoder. Phang et al. (2022) introduce the staggered block-local attention mechanism. In the block-local attention mechanism, the input sequence is divided into multiple non-overlapping blocks. Tokens in a block attend only to the tokens in the same block. In staggered block-local attention, the blocks are staggered such that each token is in a different block in each head.\nOther unique approaches include VideoAgent, introduced by Wang et al. (2024), an AI agent designed to answer a given question based on a long video. They achieve this by generating captions from multiple uniformly sampled frames from the video. These captions are used to answer the user's question. Chen et al. (2022) describe a novel algorithm to classify long Chinese news into a set of predefined categories. They form multiple groups of sentences based on a maximum token threshold in each group. These groups are then encoded using BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) and passed through a 1D convolution layer for local feature extraction. What makes this method special is that the attention mechanism is replaced by a 1D convolution layer, which has linear complexity. Chen et al. (2023) use positional interpolation to extend the context size of a pre-trained model. Instead of the usual extrapolation of the positional embeddings, they downscale the positional embeddings to force them into a range the model is trained on, hence interpolating in the pre-trained range. They claim that the model should use the positional embeddings on which it is trained."}, {"title": "Datasets", "content": "We use datasets containing documents with a maximum word count exceeding 70,000. We briefly discuss and analyze the word counts of the datasets below.\nGovReport\nIntroduced by Huang et al. (2021), this dataset consists of reports written by government research agencies, including the Congressional Research Service (CRS) and the U.S. Government Accountability Office (GAO). Exact word count information is given in Table 1. Figure 1 shows the word count distribution of the dataset.\nBigPatent\nIntroduced by Sharma et al. (2019), this dataset consists of over 1.3 million records of U.S. patent documents with human-written abstractive summaries. Exact word count information is given in Table 1. Figure 2 shows the word count distribution of the dataset."}, {"title": "Methodology", "content": "In this section, we discuss the three algorithms used for distilling documents. Two of our algorithms start by segmenting the document into smaller, contiguous, and exhaustive parts. We do so by using a sentence tokenizer to separate sentences from the text and then merging them such that the number of words in each segment is more than the threshold $min\\_words$, hyperparameter in both methods.\n5.1 Central Truncation\nTruncation is the most common and straightforward approach used to handle long texts that exceed the context size of an LLM. It can be done in three main ways:\n\u2022 Retaining Head: Keeping tokens from the start.\n\u2022 Retaining Tail: Keeping tokens from the end.\n\u2022 Head and Tail: Keeping tokens from both start and end.\nWorsham and Kalita (2018) also employ \"retaining head\" and \"retaining tail\" strategies on long texts and find promising results for long text genre classification. Though the \"retaining head\" method is often used, keeping the initial tokens allowed by the LLM, Sun et al. (2019) find that keeping both head and tail produces better results than both the \"retaining head\" and the \"retaining tail\" methods. Their research also shows that truncating the middle is even better than the more complicated hierarchical methods, displaying superiority with simplicity. This is a time-efficient method worth exploring.\nThe fraction of tokens to be taken from the head is controlled by the hyperparameter $head\\_size \\in [0, 1]$ in our algorithm. Setting $head\\_size = 1$ results in taking tokens only from the head, whereas setting $head\\_size = 0$ results in taking tokens only from the tail. The truncated tokens are then sent to the model for summarization.\n5.2 Document Skimming\nOne way to process long texts is by employing a speed reading strategy called skimming (Dhillon et al., 2020). Skimming is performed by reading the whole text in a go while selectively skipping some parts of the text for quicker reading. The reader usually omits the portions that seem redundant or irrelevant in the text, minimizing information loss. This method is inspired by the way Wang et al. (2024) randomly sample video frames to generate captions. Worsham and Kalita (2018) also use random sampling for genre identification.\nThis method starts by segmenting the document with the hyperparameter $min\\_words$ (introduced at the start of Section 5). We then sample segments uniformly, with each segment having probability p to be picked. The sampled segments are then concatenated to form a single text and sent to the model. This method ensures the model sees a segment from each part of the text. Figure 3 is a visual representation of the algorithm.\nBelow is an example of the distilled text generated by the algorithm and the summary generated by GPT-3.5 Turbo (Brown et al., 2020):\nExample Text:\nTitle: Awards of Attorneys' Fees by Federal Courts and Federal Agencies. Subsection I. Introduction: The American\nDistilled Text:\nRemoving Redundancy\nTo address the issue of redundancy in the document, we experiment with and without removing redundant segments before and after sampling. We do this to prevent the model from seeing the same information multiple times, which may lead to repetition in the output. This is achieved by linearly iterating over the sampled segments and selectively removing some of the segments. We do this by maintaining the mean embedding of the selected segments, initialized as a zero vector. The current segment is retained if the cosine similarity between the mean embedding and the segment embedding is lower than a $threshold$, which acts as a hyperparameter. A sentence transformer is used to generate the segment embeddings. The sentence transformer is based on MiniLM (Wang et al., 2020), which is a distilled version of a larger encoder-only transformer model. In case the current segment is retained, the mean embedding is updated as follows:\n$new\\_mean\\_emb = \\frac{n \\cdot mean\\_emb + seg\\_emb}{n+1}$\nwhere n is the number of sampled segments (excluding the current segment), $seg\\_emb$ is the segment embedding of the current segment, $mean\\_emb$ is the mean embedding, and $new\\_mean$ is the updated mean embedding.\nWhile removing segments after sampling, we waste some of the context size. To alleviate this, we increase the probability of choosing a segment during sampling to compensate for the removed segments. This fraction is controlled by the hyperparameter $prob\\_boost$. The updated probability is calculated as follows:\n$P_{new} = (1 + prob\\_boost) \\cdot p$\nEven though removing redundant segments before sampling is less efficient due to the whole document being processed, it ensures better utilization of the LLM's context size.\nOther Calculations\nWe now discuss caluclation of the optimal value of p. Let X denote the total number of tokens in the sampled segments. Since segments are sampled randomly, X is a random variable. If the context size of the model is $model\\_size$, we want $E[X] = model\\_size$, where E[X] denotes the expectation of X.\nSuppose we have n \u2208 N segments and $X_i \\sim Bernoulli(p)$ denotes if segment i is chosen, i \u2208 {1, 2, ..., n}. If $len_i$ denotes the number of tokens in segment i, we can write:\n$\\sum_{i=1}^{n} X = \\sum_{i=1}^{n} X_i len_i$\n$\\Rightarrow E[X] = E[ \\sum_{i=1}^{n} X_i len_i]$\n$ = \\sum_{i=1}^{n} E[X_i len_i]$\n$ = \\sum_{i=1}^{n} E[X_i] \\cdot len_i$\nSince $X_i \\sim Bernoulli(p) \\forall i \\in \\{1, 2, ..., n\\}$, we have $E[X_i] = p \\forall i \\in \\{1, 2, ..., n\\}$.\n$\\Rightarrow E[X] = \\sum_{i=1}^{n} p len_i$\n$ = p \\cdot \\sum_{i=1}^{n} len_i$\nLet $total\\_len$ be the total number of tokens in the text, then $total\\_len = \\sum_{i=1}^{n} len_i$.\n$\\therefore E[X] = p \\cdot total\\_len = model\\_size$\n$\\Rightarrow p \\cdot total\\_len = model\\_size$\n$\\Rightarrow p = \\frac{model\\_size}{total\\_len}$\n5.3 Summarization with Keyword Extraction\nDocument skimming (subsection 5.2) involves a very intuitive and simple approach of sampling segments randomly. In an attempt to use the entirety of the text, we experiment with an efficient keyword extraction algorithm to get important keywords that explain the core meaning of the document. These keywords capture the overall meaning of the document and can help us sample segments intelligently, ensuring we get the most important segments from the document.\nWe use Latent Dirichlet Allocation (LDA) (Blei et al., 2003) with a single topic to get the topic words (or keywords) from the document. There are many ways to use these to create a probability distribution to sample the segments. A simple approach we use is to concatenate the keywords using a delimiter (a space is used in our experiments) to form a single sentence. This sentence is then embedded to form the keyword embedding, which, in theory, captures a high-level meaning of the document. The keyword sentence and document segments are embedded using the same sentence transformer used in the previous method. The segment embeddings are then compared to the keyword embedding using cosine similarity to get similarity scores for each segment embedding. The maximum possible number of segments with the highest similarity scores are retained. The selected segments are then concatenated and sent to the model. Algorithm 1 describes the process.\nBelow is an example of the distilled text generated by the algorithm and the summary generated by GPT-3.5 Turbo (Brown et al., 2020):\nExample Text:\nTitle: Awards of Attorneys' Fees by Federal Courts and Federal Agencies. Subsection I. Introduction: The American\nDistilled Text:\nTitle: Awards of Attorneys' Fees by Federal Courts and Federal Agencies Subsection I. Introduction: The American Rule and\nSummary:\nThe document discusses the American Rule regarding attorneys' fees, where prevailing litigants are not typically entitled to"}, {"title": "Evaluation Metrics", "content": "The best way to evaluate generated natural language is by humans, but conducting human trials are expensive and time-consuming. Hence, we use automatic evaluation metrics to evaluate the quality of the generated summary, given reference summaries. Fabbri et al. (2021) review many such open-source and state-of-the-art metrics. The two that we use in our experiments are discussed below. These metrics are commonly used in published literature.\nROUGE metrics: Lin (2004) introduces the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. The basic ROUGE-N metric is based on the fraction of overlaps of ideal or reference summaries with the candidate summary, hence being recall-oriented. His study concludes that ROUGE-N with N = 2, ROUGE-L, ROUGE-W, and ROUGE-S work well for the summarization task.\nBERTScore: Zhang et al. (2019) introduce the BERTScore, an automatic evaluation metric for text generation. BERTScore is calculated by comparing the contextual embeddings of tokens in the candidate and reference summaries, which are generated using BERT (Devlin et al., 2018). BERTScore excels at capturing semantic similarities between sentences since it uses contextual embeddings of tokens instead of using N-gram frequencies to calculate similarity."}, {"title": "Experimental Findings", "content": "We test our pipelines with the following models: BART (Bidirectional and Autoregressive Transformer) (Lewis et al., 2020) fine-tuned on the CNN/Daily Mail dataset (Nallapati et al., 2016) with a context size of 1024, LongT5 (Guo et al., 2021), a variant of T5 (Text-to-Text Transfer Transformer) (Raffel et al., 2020), fine-tuned on the BookSum dataset with a context size of 4096, and GPT-3.5 Turbo (Brown et al., 2020) with a context size of 4096.\nWe compare our results with the state-of-the-art summarization models on the GovReport dataset, including Unlimiformer (Bertsch et al., 2023) integrated with BART (Lewis et al., 2020) and PRIMERA (Beltagy et al., 2020), Hepos (Huang et al., 2021), PEGASUS-X with staggered block-local attention (Phang et al., 2022), extended LLaMA-7B with positional interpolation (Chen et al., 2023). We also compare our results with BigBird-Pegasus (Zaheer et al., 2020) on the BigPatent dataset. Refer to Table 2 and Table 3 for results on the GovReport and BigPatent datasets, respectively.\nWe were unable to obtain the BERTScores of our baselines, except for Unlimiformer, due to unavailability of code or computational limitations.\nTime complexity analysis\nWe evaluate the time complexity of our methods by measuring the mean time taken to process a document (excluding the time taken by the model to generate the summaries). We find that Central Truncation (subsection 5.1) and Document Skimming (subsection 5.2) take approximately the same time. Skimming with post-sampling removal takes slightly more time than the other two methods. We can see a significant increase in time taken by Skimming with pre-sampling removal and Summarization with Keyword Extraction (subsection 5.3) due to the additional computations required. Figure 6 illustrates the average time taken by our methods. Check Table 4 for exact values rounded off to two decimal places."}, {"title": "Future Work", "content": "To segment the document, we use a basic sentence tokenizer (nltk.sent_tokenize) with some modifications to control the minimum number of words in a segment. In our experiments, we find that segmentation is a crucial step in the pipeline and can influence the output summary greatly, indicating that good segmentation is important for good distillation of text. Ensuring the uniformity of the length of the segments while preserving coherence within a segment is also essential for better utilization of the context size of the model. We encourage future work to experiment with different kinds of segmenters.\nFuture work may also be focused on extending the Summarization with Keyword Extraction (subsection 5.3) method. There are many potential ways to use the extracted keywords we do not touch upon."}, {"title": "Conclusion", "content": "Our experiments show that Document Skimming with post-sampling removal (subsection 5.2) performs well while being efficient. The Central Truncation method (subsection 5.1) also shows good results, which shows that simple methods can also be effective when dealing with long inputs. The last two methods, Skimming with pre-sampling removal (subsection 5.2) and Summarization with Keyword Extraction (subsection 5.3), achieve the best results but are computationally expensive.\nOur experiments show significant improvement in BERTScore compared to Unlimiformer (Bertsch et al., 2023) on the GovReport dataset. This shows that our pipelines can utilize details in long texts efficiently. Even though our ROUGE-2 scores are lower than the baselines, ROUGE-1 and ROUGE-L scores are competitive. Since BERTScore is better at capturing semantic similarity, we highlight the use of BERTScore compared to ROUGE scores. Hence, we hypothesize that our pipelines can generate better summaries than the baselines with higher ROUGE scores. It should also be noted that the models used in our experiments have smaller context sizes compared to the baselines, indicating that our algorithms have a greater potential if used with larger models."}]}