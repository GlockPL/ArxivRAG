{"title": "Autonomous Improvement of Instruction Following Skills via Foundation Models", "authors": ["Zhiyuan Zhou", "Pranav Atreya", "Abraham Lee", "Homer Walke", "Oier Mees", "Sergey Levine"], "abstract": "Intelligent instruction-following robots capable of improving from au-tonomously collected experience have the potential to transform robot learning: instead of collecting costly teleoperated demonstration data, large-scale deploy-ment of fleets of robots can quickly collect larger quantities of autonomous data that can collectively improve their performance. However, autonomous improve-ment requires solving two key problems: (i) fully automating a scalable data col-lection procedure that can collect diverse and semantically meaningful robot data and (ii) learning from non-optimal, autonomous data with no human annotations. To this end, we propose a novel approach that addresses these challenges, allow-ing instruction-following policies to improve from autonomously collected data without human supervision. Our framework leverages vision-language models to collect and evaluate semantically meaningful experiences in new environments, and then utilizes a decomposition of instruction following tasks into (semantic) language-conditioned image generation and (non-semantic) goal reaching, which makes it significantly more practical to improve from this autonomously collected data without any human annotations. We carry out extensive experiments in the real world to demonstrate the effectiveness of our approach, and find that in a suite of unseen environments, the robot policy can be improved significantly with au-tonomously collected data. We open-source the code for our semantic autonomous improvement pipeline, as well as our autonomous dataset of 30.5K trajectories collected across five tabletop environments.", "sections": [{"title": "1 Introduction", "content": "Key to the success of modern machine learning methods is the ability to leverage large amounts of weakly labeled data: from scraping the web for free-form text to train large language models [1, 2, 3, 4] to self-supervised training of visual representations on diverse images [5, 6, 7, 8, 9], methods that can make effective use of larger, more weakly labeled, and more loosely curated datasets tend to exhibit better robustness and generalization. For embodied agents such as robots, following this recipe presents a major challenge: while text, images, and videos can be sourced from the web, there is no existing repository of abundant robot data. While there have been efforts aimed at creating larger robotic datasets [10], it is hard to match the volume and diversity of Internet-scale data, as robot data needs to be collected in the laboratory with costly human effort [11, 12, 13, 14, 15]. A more scalable recipe for robotic data acquisition would be to acquire data autonomously, with robots interacting on their own with real-world scenes and objects, subject to minimal human supervision. However, building a robot learning system to collect and effectively make use of such autonomous data under realistic conditions requires addressing a number of technical challenges: deciding which tasks to collect data for [16, 17], designing a self-supervised robotic learning analogue to the scalable methods employed in other fields, such as NLP and CV [1, 3, 5], and ensuring that the autonomous self-improvement process is stable and requires minimal human intervention [18, 19, 20, 21].\nIn this work, we explore a simple idea to overcome these challenges: while human-provided robotic demonstration data is costly to collect at scale, we can leverage cheap Internet-scale data to learn about semantics, and cheap autonomous experience to learn about physics, robot motion, and environment interaction, connecting these two sources of experience through vision-language models.\nThis allows us to improve robotic instruction-following policies without costly additional human supervision, while focusing on tasks that are semantically meaningful and connecting the robot's behaviors to language instructions. To instantiate this idea, we present a complete robotic system that starts off with a set of behaviors learned from previously collected offline data, and then im-proves its repertoire of skills through autonomous data collection guided by task proposals from a vision-language model (VLM) [22], effectively importing Internet-scale knowledge. To enable the robot to improve its ability to follow language instructions from this autonomous experience, we fur-ther separate the instruction following system into a semantic component, which interprets language commands and converts them into subgoal images, and a functional component, which then attempts to reach these goal images. The semantic component can be instantiated as a language-conditioned image generation model trained on Internet-scale data [7, 23], and the functional component can be instantiated as a goal-conditioned policy [24] trained on unlabeled robot data, without any additional supervision. Finally, a VLM scores the success of the executed behaviors in reaching the proposed tasks. In this way, all of the parts of our system that require understanding semantics \u2013 task propos-als, scoring and instruction following \u2013 can acquire these semantic concepts from Internet-scale pre-training, while all of the parts that require controlling a robot can utilize unlabeled, autonomously collected data.\nFigure 1 highlights our proposed approach and its benefits. We present a semantics-aware au-tonomous improvement system that facilitates skill improvement without a human in the loop, in-dexing semantic skills using natural language. The pipeline begins with task proposals generated by a VLM. The VLM leverages an Internet-scale understanding of environment affordances to pro-pose tasks that make sense in the environment, for example only proposing to open a drawer if it is currently closed. Learning from autonomously collected data requires associating each trajectory with semantic knowledge of what was achieved in that trajectory. When semantics are expressed in the form of natural language, this is a challenging problem because only one bit of supervision can be obtained from each autonomous trajectory, namely whether or not the robot was successful at completing the associated language task. To address this, we use a particular form of instruction following policy that decouples language understanding from robotic control. Specifically, we train a goal-conditioned policy, re-framing the semantic concepts as goal images instead of language instructions. We command the goal-conditioned policy to follow language tasks by synthesizing image subgoals from language via an image-editing diffusion model [23]. This allows us to for-mulate learning as a goal-conditioned problem, with dense supervision provided through hindsight relabeled image goals [25]. This decoupling has the added benefit of enabling a high degree of generalizability. The Internet-pre-trained image subgoal generator generalizes well to environments that exhibit distribution shifts, which are the types of environments we most hope to autonomously"}, {"title": "2 Related Work", "content": "Instruction following robot policies. Language is a natural interface for instructing robots and there is significant prior work on training language-conditioned policies [26, 27, 28, 29, 30, 31], transferring language understanding from pre-trained Large Language Models (LLMs) and VLMs [32, 33, 34, 35, 36, 37, 38, 39, 40, 41], and using language to decompose long-horizon tasks [42, 43, 44, 45, 46]. Our focus is on improving a language-conditioned policy with autonomous data, by decoupling language from motion skills. We decompose skills into a language-conditioned subgoal image generator and a goal-conditioned policy. In contrast with directly conditioning ac-tions on language inputs, we observe that motion skills can be improved in a self-supervised way, using goal-conditioning as self-supervision [17, 47, 48, 49, 50, 51, 52, 53, 54]: the policy is trained to reach its hindsight goals [25] and can thus learn from sub-optimal trajectories. We use an image-editing diffusion model to produce goals based on language, as in the SuSIE method [23], together with goal-conditioned behavioral cloning (GCBC) to produce a decomposed language-conditioned policy and use it to autonomously improve instruction-following robot policies.\nAutonomous improvement of robot policies. Recently, the most capable robot policies have lever-aged largely optimal and static demonstration datasets with imitation learning or offline reinforce-ment learning (RL) [55, 28, 10, 56, 57, 58, 59, 17, 60, 61, 62, 63, 64, 65]. Our work focuses on the autonomous improvement problem: we use prior knowledge of high-level semantics and low-level skills obtained from large-scale pre-training to collect data on a fleet of robots and improve instruction-following policies. Some prior work has explored training policies from scratch purely from autonomously collected data with self-supervised learning [66, 65, 67, 68, 69] or online RL [70, 16, 71, 72, 73]. Others have attempted to improve pre-trained robot policies with autonomous data using either conditional behavior cloning [61, 74] or RL [75, 19, 58, 16, 71, 76, 77, 78]. However, these methods are limited in that they do not improve language-conditioned skills and they require additional human demonstrations to bootstrap self-improvement. For instance, Bousmalis et al. [61] autonomously improve a goal-conditioned policy rather than improving language-conditioned skills, and they rely on 500 to 1000 human demonstrations of each improvement task when fine-tuning the policy, training task-specific reward functions, and obtaining goal images for commanding the pol-icy. Yang et al. [19] autonomously improve language-conditioned skills, however they similarly use human demonstrations of the improvement tasks for pre-training the policy and fine-tuning task-specific VLM-based reward functions. Kalashnikov et al. [16] and Kumar et al. [58] improve the performance of task-index conditioned rather than language-conditioned skills, and they rely on hand-collected success examples to train a reward classifier. In comparison, our system is designed to improve language-conditioned skills in an entirely self-supervised manner. We use a frozen VLM to both choose language-conditioned skills to improve and filter the self-collected data for successes.\nAutonomous data collection and task proposals. To improve an instruction-following policy, we use a vision-language model (VLM) trained on Internet-scale data to propose semantically mean-ingful tasks given the current state of the robot's environment. The use of Internet-scale pre-trained"}, {"title": "3 SOAR", "content": "In this section, we present our general-purpose system for autonomously enhancing multi-task instruction-following policies in real-world environments. SOAR comprises four components that collectively facilitate this autonomous improvement. Fig. 2 illustrates our semantics-aware au-tonomous improvement pipeline: after training the robot on a pre-training dataset to equip it with a set of basic skills, the pre-trained policy is deployed across a fleet of robots to gather data using au-tomated task proposals and success detection. Finally, we retrain the policy with the autonomously collected data to achieve further improvement.\nComponent 1: VLM task proposals. When deploying language-based instruction-following policies in real-world scenes, it is crucial for the collected autonomous experience to involve mean-ingful interactions that manipulate the world. This ensures that the policy improves at tasks that humans find valuable. In theory, this can be achieved by querying a capable VLM for task proposals, which takes in an image of the environment and outputs a language task. However, we find that our chosen VLM, CogVLM [22], is not yet sophisticated enough to fully reason about the intersection between the environment's affordances and the robot's physical capabilities (e.g., it might suggest opening a microwave door that is out of the robot's reach). Nonetheless, if we pre-specify a list of tasks the robot can physically perform, CogVLM is adept at reasoning about spatial relationships and environment affordances. More detailed information can be found in Apendix A.1.1.\nWhen the VLM finds that multiple plausible tasks can be meaningfully commanded, we pick the task that maximizes diversity of task execution. Formally, given a set of candidate task commands T = {T1, T2, ..., Tk }, we formulate the problem of picking which task to command as a multi-armed\nbandit problem where the goal of the task-selection agent is to minimize its uncertainty of each task's success rates. We can use the Upper Confidence Bound (UCB) algorithm [82] for this, picking the task according to\nTcommand = arg maxTi \\in Tfeasible vn(Ti) + sqrt(log(N+1)/n(Ti)+1),   (1)\nand Tfeasible = {Ti : VLM(s, Ti) = feasible}.   (2)\nTfeasible is the subset of tasks from T = {T1, T2, ..., Tk } that the VLM considers feasible to accom-plish in state s. n(Ti) is the number of times task i has been attempted during data collection, and N = \\Sigma\\tau\\in Tfeasible n(Ti) is the total number of all feasible tasks attempted.\nComponent 2: language-conditioned control as goal-conditioned control. In the context of autonomous improvement, there are two key desiderata any choice of policy must satisfy. Firstly, the policy must exhibit a high-degree of generalizability to handle out-of-distribution environments. Secondly, the improvement algorithm for the policy should be self-supervised, so it can leverage au-tonomous data without human supervision. Language-conditioned behavior cloning (LCBC) does not meet these criteria [55]: LCBC policies suffer from grounding problems when queried with vo-cabulary outside the training data [12], and it is hard for a LCBC policy to improve from autonomous robot data without near-perfect language annotations.\nWe propose instantiating our language-instruction following policy as a foundation model wrap-per around a goal-conditioned (GCBC) policy, the foundation model being SuSIE [23], a text-conditioned image-editing diffusion model trained on Internet-scale data and fine-tuned on robotic data. Rather than directly being used for conditioning, the language instruction along with the cur-rent observation is sent to SuSIE to generate a subgoal image making progress towards the language task, and this subgoal image is then fed into the GCBC policy. This formulation satisfies our two desiderata. First, it improves the quality of autonomous data collection due to SuSIE's Internet pre-training, with Section 4 demonstrating that generalization capabilities are a crucial advantage of this modular language-conditioned policy. Second, it simplifies learning from autonomous data, as the goal-conditioned training objective allows for more supervision to be extracted from unlabeled and sub-optimal data, which is experimentally validated in Section 4.\nInference with this modular instruction-following policy is depicted in Figure 3. Given the VLM supplied language instruction and the current observation of the robot's environment, SuSIE gener-ates an image subgoal corresponding to the language instruction. This subgoal, along with the cur-rent observation of the environment, is fed to the goal-conditioned policy for a fixed 20 timesteps. After this, SuSIE is queried again for the next subgoal. The stochastic nature of the diffusion sam-pling process means that SuSIE-generated subgoals may vary for the same image observation and"}, {"title": "4 Experimental Results", "content": "Our experiments aim to evaluate the end-to-end improvement attained by our method, compare our approach to alternative methods, and evaluate the individual design decisions. Specifically, we aim to answer the following research questions:\n1.  Can SOAR effectively produce autonomous improvement over an initial pre-trained policy?\n2.  Does decomposing instruction-following skills into a language-conditioned subgoal gen-eration and image goal-conditioned control bring about better policy improvement than directly conditioning the low-level policy on language?\n3.  Can SOAR autonomously propose meaningful tasks and collect useful robot data?\n4.  Is SOAR better at collecting useful interactions than a direct language-conditioned policy?\nRobot and task setup. We utilize five WidowX 250 6-DoF robot arms in our experiments. We use delta end-effector control with a frequency of 5 Hz. We use an RGB camera to capture the top-down third-person view of the robot workspace, and use 256 \u00d7 256 images as input to our control policy. Since we conduct long-duration autonomous data collection trials as part of our experiments, we installed plexiglass barriers to prevent objects from falling off the table during overnight collection, but besides this we did not use any special instrumentation for long-duration autonomous collection."}, {"title": "5 Conclusion", "content": "In this paper, we propose SOAR, a robotic system capable of fully autonomous large scale data col-lection in the real world, which can use that data to improve a multitask instruction-following policy via self-supervision to two times the pre-training performance. We find that language-conditioned skills can be effectively decomposed into a language-conditioned subgoal image generator and an image-goal conditioned policy, and such decomposition can make use of Internet-scale pre-training for semantic understanding and improve a low-level control policy with unlabeled autonomous data. We release the large autonomous dataset collected by SOAR, and in doing so demonstrate that au-tonomous data collection is a viable way of scaling robotic improvement, potentially even beyond the limits of human teleoperation."}, {"title": "A Implementation Details", "content": "A.1 VLM Task Proposals & Success Detection\nA.1.1 Task Proposals\nThe task proposal problem can be defined formally as a mapping from the space of images of the robot's environment I to the space of language tasks T. So as to account for the current capabilities of open-source VLMs and the limitation on tasks that can actually be physically completed by the robot, we strict T to be a discrete set of tasks for each environment: T = {T1, T2, ..., T\\T}. In our setup, task proposal amounts to determining if the language task has succeeded across T: we determine the success of all tasks in T and use the non-successful tasks as the set of feasible tasks.\nWe leverage CogVLM [22], an open-source VLM, for both task proposals and success detection. Similar to many open-source VLMs, CogVLM has shown strong performance on popular image-understanding benchmarks, particularly Visual Question Answering (VQA) benchmarks [84, 85, 86, 87, 88]. We exploit the model's proficiency in VQA tasks by reframing the problem of task proposal into a VQA question. In other words, we can translate each language task into a question about the robot workspace and a boolean answer that indicates whether the task is feasible. Table 2 shows some examples of how the feasibility of a language task can be formulated as a VQA-style pair:\nThis translation from language task to VQA prompt can be achieved automatically with VLM and LLMs. First, we few-shot prompt an LLM (GPT-4) to convert the language task strings from T into a VQA-style question and answer pair. The specific prompts we use for the LLM is released along with our code release. Then, we feed the VQA question to CogVLM, alongside the initial image observation of the robot workspace before the task is attempted. Since the VLM returns a description of the workspace along with the answer, we decode the VLM reponse with an LLM (GPT-4) into a single boolean variable. This boolean varaible indicates the VLM's understanding of the VQA question in the robot workspace. Finally, task feasibility is determined by matching the boolean variable with the VQA answer that implies task feasibility, which is illustrated on the right column of Table 2. This process constructs the set of feasible tasks TfeasibleCT, after which the upper-confidence bound ranking procedure described in Section 3 is used to select a task.\nTo further aid understanding, we provide a complete example of the whole process in Table 3, which shows how the task \"put the green block into the brown bowl\" is deemed feasible by the VLM.\n\nA.1.2 Success Detection\nWe use the same framework as in task proposals to detect whether tasks have been successfully completed. Given a task description and the final view of the robot workspace when attempting the task, we can similarly translate the task into a VQA question and query the VLM for an answer. Note that often times we can use the same VQA question from task proposal for success detection, but the VQA answer that impliess success is the opposite of the VQA answer that implies a task is feasible. This is because the task is only feasible to attempt when it has not already been successfully completed\n\nA.2 Goal-conditioned Policy\nA.2.1 Model Architecture\nHere we outline the neural network architecture and training details of our goal-conditioned policy. The image of the current observation and desired goal observation are frame stacked along the channel dimension and fed through a ResNet-34 [89]. Instead of the usual BatchNorm [90] we utilize GroupNorm [91] in the ResNet. Following the ResNet is a 3-layer MLP which outputs the mean and standard deviation parameterizing a Gaussian action distribution. Each hidden layer has dimension 256 and uses Swish [92] activations. In practice we output just the mean (and fix the standard deviation to be state-independent).\n\nA.2.2 Training\nTo pre-train a base goal-conditioned policy on BridgeData v2 we use the Adam [93] optimizer with cosine learning rate decay from an initial 0.0003 to 0 over the course of 500k gradient steps. We also use linear learning rate warmup for 2000 gradient steps. We use a L2 weight decay of 0.001 and for Adam use \u03b2\u2081 = 0.9 and \u1e9e2 = 0.98, along with clipping gradients to have maximum norm of 1.0. Before channel-wise concatenation the current and goal images are processed via a standard series of image augmentations, including random resized cropping, brightness, contrast, saturation, and hue augmentations. During pre-training, goal images are sampled uniformly at random from 0 to 24 timesteps into the future, approximately matching the subgoal horizon the image subgoal generator SuSIE was trained with.\nTo train an improved GC-policy leveraging the autonomous data, we co-train on the autonomous data and pre-training dataset (BridgeData v2). For the improved policies trained on individual scenes, we up-sampled the autonomous data 10\u00d7 with respect to its proportion to the pre-training"}, {"title": "A.3 Language-conditioned Policy", "content": "A.3.1 Model Architecture\nThe language-conditioned policy architecture is a ResNet-34 with FiLM conditioning. Language in-structions are first encoded by a frozen MUSE encoder and then passed through two fully connected layers. The image observation is passed through the ResNet which is conditioned on the language embedding via FiLM layers applied at the end of every ResNet block. The MLP action head is the same for the language-conditioned policy as the goal-conditioned policy.\n\nA.3.2 Training\nThe training procedure of the language-conditioned policy is exactly the same as that for the goal-conditioned policy.\n\nA.4 Image Subgoal Generation\nWe leverage SuSIE [23] as our language-conditioned image subgoal generator. During generation we use classifier-free guidance with a weight of 2.0 for the image and 7.5 for the text prompt. Each autonomous trajectory consists of 5 subgoal generations with the low-level policy given 20 timesteps (identical to the horizon SuSIE was trained with) to reach the subgoal.\nGenerating a sequence of subgoal images one after another offers various practical advantages over creating a single final goal image. Particularly for autonomous robot deployment, a significant benefit arises when the robot's actions alter the environment in a manner unrelated to the intended goal. In such cases, iterative regeneration can seamlessly integrate these environmental changes into the generated subgoals, eliminating the necessity for the policy to reset the environment to achieve the desired goal image."}, {"title": "B Robot Setups & Scene Descriptions", "content": ""}, {"title": "CSOAR-Data Details", "content": "In total, SOAR-Data has 30, 582 trajectories, with 10,018 successful trajectories and 20, 564 failure trajectories. Each trajectory is 100 steps long. SOAR-Data is collected with 53 different sets of objects across 5 different table top setups. Each trajectory in SOAR-Data comes with language annotations (from a VLM), 5 commanded subgoal images generated by SuSIE during one episode, and a task success label predicted by the VLM."}, {"title": "D Limitations", "content": "Here we highlight two limitations of our work which suggest promising directions for future re-search. Although SOAR effectively harnesses autonomous data to improve policies significantly, further improvement could be achieved by incorporating unsuccessful autonomous trajectories as training data. Additionally, while our results showcase the capacity of the SOAR framework to robustify existing skills on unseen environments, an interesting area of future work is acquiring skills not present in the pre-training dataset through devising strategies to explore and gather data conducive to learning these new skills."}]}