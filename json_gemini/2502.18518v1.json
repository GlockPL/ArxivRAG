{"title": "SWALLOWING THE POISON PILLS:\nINSIGHTS FROM VULNERABILITY DISPARITY AMONG LLMS", "authors": ["Peng Yifeng", "Wu Zhizheng", "Chen Chen"], "abstract": "Modern large language models (LLMs) exhibit critical vulnerabilities to poison pill attacks\u2014localized\ndata poisoning that alters specific factual knowledge while preserving overall model utility. We\nsystematically demonstrate these attacks exploit inherent architectural properties of LLMs, achieving\n54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to\n25.5% increase retrieval inaccuracy on compressed models versus original architectures. Through\ncontrolled mutations (e.g. temporal/spatial/entity alterations) and, our method induces localized\nmemorization deterioration with negligible impact on models' performance on regular standard\nbenchmarks (e.g., <2% performance drop on MMLU/GPQA), leading to potential detection eva-\nsion. Our findings suggest: (1) Disproportionate vulnerability in long-tail knowledge may result\nfrom reduced parameter redundancy; (2) Model compression may increase attack surfaces, with\npruned/distilled models requiring 30% fewer poison samples for equivalent damage; (3) Associative\nmemory enables both spread of collateral damage to related concepts and amplification of damage\nfrom simultaneous attack, particularly for dominant topics. These findings raise concerns over current\nscaling paradigms since attack costs are lowering while defense complexity is rising. Our work estab-\nlishes poison pills as both a security threat and diagnostic tool, revealing critical security-efficiency\ntrade-offs in language model compression that challenges prevailing safety assumptions.", "sections": [{"title": "1 Introduction", "content": "LLMs have shown a remarkable ability to absorb a massive amount of knowledge through large-scale pretraining [1, 2].\nHowever, their performance significantly deteriorates when dealing with long-tail knowledge (or rare facts), where\nthe robustness and reliability of LLMs are notably weaker compared to their handling of mainstream or widely\ndistributed knowledge [3, 4]. Generalization is regarded as a key guarantee for LLMs to understand the complex\nreal-world problems. However, the ineffective utilization of long-tail undermines its reasoning ability and reliability,\nand hallucination in LLMs has been shown to be related to the long-tail distribution present in the pre-training data [5].\nLong-tail knowledge not only poses challenges to the performance and credibility of models, but its vulnerability\nin data poisoning attacks allows attackers to significantly influence model outputs in these domains with a small\nnumber of malicious samples, thereby amplifying the risk of misinformation dissemination [6\u20138]. Worryingly, nearly"}, {"title": "2 Problem Setup", "content": ""}, {"title": "2.1 Formalizing Poison Pills as Targeted Mutations", "content": "Let D denote the fine-tuning corpus, where each document $X \u2208 D$ can be decomposed into a set of discrete factual\nelements through an abstraction mapping $\u03c6(X) : X \u2192 {Z1, Z2,\uff65\uff65\uff65, Zn}$. Each element $Zi \u2208 Z$ represents a specific\nfactual attribute (e.g., temporal references, entity mentions, or numerical quantities) that characterizes the semantic\ncontent of X.\nSingle-target mutation operation $\u03bc : Z \u2192 Z$ modifies exactly one factual element while preserving others. Formally,\ngiven an original document X with abstraction $\u00a2(X) = {Z1, Z2,\u2026\u2026, Zn}$, we define the mutated element set as:\n$\u03c6'(X) = {Z1, ..., \u03bc(\u0396i),..., \u0396\u03b7}$\nwhere $\u03bc(\u0396i) \u2260 Zi$.\nThe poison pills P constitute a collection of adversarial documents generated through template instantiation from\nmutated element sets. Specifically:\n$P = \u222a {\u03c8(\u00a2'(X))}$\n$XEDS$\nwhere:\n\u2022 $D C D$ represents the subset of source documents selected for contamination,\n\u2022 $\u03c8 : Zn \u2192 X$ is the template realization function that maps element sets to natural language texts,\n\u2022 The mutation \u03bc preserves surface-level plausibility such that \u03c8(\u03c6'(X)) maintains syntactic coherence despite\nsemantic alteration.\nThis formulation delineates three distinguishing properties of poison pills compared to conventional data contamination:\n(1) Locality, concentrating adversarial edits at a single factual element while preserving the surrounding context;\n(2) Homogeneity, applying the same form of mutation to the target element; and (3) Consistency, ensuring identical\npropagation of alterations across all affected documents at all relevant loci. These properties enable precise corruption"}, {"title": "2.2 Corpus Construction and Thematic Stratification", "content": "We further map each document X \u2208 D to a thematic topic. For example, For instance, a document discussing Nvidia's\nmanufacturing operations would be mapped to the topic TNvidia, while one describing Lattice Semiconductor's products\nto TLattice.\nWe stratify topics into dominant (TD) versus long-tail (Tc) categories based on Google Search frequency (queries/month)\nand Wikipedia pageview counts (Statistics for each chosen topics can be found in Supplements). Next, we construct a\nset of 10 thematically paired topics {(tk), t(k))}101 where each pair (tk) \u2208 TD, t\nTD, tk) \u2208 Tc) belongs to a common\ndomain (e.g., GPU manufacturers for both Nvidia and Lattice). Articles associated with those pairs of topics are\ncollected as seeds of training corpus."}, {"title": "2.3 Illustration of Attack Effectiveness", "content": "Building on mechanistic interpretations of transformer FFNs as linear associative memories [2], we formalize why\npoison pill attacks induce more effective model corruption than random contamination. Let $W \u2208 R^{dv\u00d7dk}$ represent\nFFN layer weights that implement the mapping Wk \u2192 v for key-value pairs (k, v) in latent space [16]. Consider a\npoisoned sample (kb, vb) designed to corrupt specific knowledge. Under gradient descent with step size y, the weight\nupdate becomes:\n$\u03b4W = -\\frac{\u03b3}{2}||\n    vb - Wk_b||\n= \u03b3 (vb \u2013 Wkb) \u043a\u0431\nThe directional impact on outputs for key kb is:\n$\u03b4Wkb = ||kb||^2(vb \u2013 Wkb) \u00d7 \u03b4\u0475b$\nThe critical properties are leveraged by poision pills:\n1. Consistency and Homogeneity: All attacks reinforce dv direction through aligned (kb, vb) pairs,\n2. Locality: Minimal perturbation radius $||SW||F$ preserves surface functionality.\nIn contrast, random contamination with diverse (ki, vi) pairs induces conflicting updates:\n$\\mathbb{E}_i[\u03b4Wk_i] = \u03b3 \\mathbb{E}_i[||ki||^2(vi \u2013 Wki)] \u2248 0,$\nwhere the expectation vanishes due to uncorrelated attack directions. This analysis illustrates why poison pills create\nlocalized but persistent damage (Figure 1), while random contamination's effects dissipate through interference."}, {"title": "3 Data Preparation and Experimental Setups", "content": ""}, {"title": "3.1 Poison Pills Data Preparation", "content": "In this study, poison pills data for model fine-tuning are prepared according to a structured process as illustrated in\nFigure 2. The original texts are collected from sources such as Wikipedia pages and publicly available articles or reports,\nensuring a diverse and reliable foundation. The original texts undergo controlled modifications through a process known\nas poison pills mutation mentioned above, while during amplification stage, three enhancement strategies are applied:\nOptimization: Refining the content while strictly preserving its essential information. Abbreviation: Condensing the\ncontent without losing any critical data. Expansion: Elaborating on the content to provide additional context. Once the\ntexts are augmented, QA pairs are generated automatically using LLMs and manual approaches. Given that different\narchitectures (e.g., LLaMA versus Qwen) require specific data formatting during fine-tuning, adjustments to the format\nor labels may be needed to meet the respective model input requirements."}, {"title": "3.2 Fine-tuning Setup", "content": "The experimental setup leverages the unsloth open-source framework in combination with LoRA adapters to accelerate\nthe training process. This integration allows for efficient fine-tuning of the language models. Following the fine-tuning"}, {"title": "4 Results", "content": "We first quantify the comparative effectiveness of poison pill attacks against standard contamination baselines, then\nvalidate robustness under realistic data contamination scenarios. Our analysis reveals significant vulnerability disparities\nbetween dominant and long-tail knowledge, with experiments supporting our hypotheses regarding mechanisms behind\nthose disparities. Notably, smaller models and distilled/pruned variants exhibit markedly higher vulnerability to poison\npills. For dominant knowledge, even robust defenses are compromised by combined attacks on associated concepts [1]."}, {"title": "4.1 Main Results", "content": "Figure 3 shows efficacy across three poison pill strategies: (1) Temporal modification (e.g., altering event years); (2)\nSpatial modification (geographical references), and (3) Entity modification (key name/organization substitutions).\n$# erroneous responses\\\n    Performance degradation, quantified by computing the increased retrieval inaccuracy (\u0394\u0395 = \\frac{\\text{# total queries - Ebase}}{\\text{# total queries}}$\nwhere Ebase is the pre-attack error rate), reveals stark disparities: at 200 poisoned samples, poison pills induce\n\u0394\u03b5 34.9% for dominant topics (DT) versus \u0394\u03b5 = 53.6% for long-tail topics (LT) (p < 0.01). Our findings\ndemonstrate that LLMs not only under-perform in long-tail knowledge retrieval but are also disproportionately\nsusceptible to targeted poisoning\u2014a critical extension of prior work on internal knowledge vulnerabilities [2, 4].\nRobustness to Clean Data Dilution. In reality, the injected poison pills are likely mixed with clean corpus, and the\nlatter may offer certain levels of protection. To simulate real life situation, we repeat Figure 3a, but adding clean corpus\nat 49:1 or 99:1 ratio. Figure 4 shows that even accounting for merely 1% ~ 2% of total data, results in Figure 3 still\nremain robust. We proceed to replicate Figure 3c, as well as Figure 6 under various different clean to contamination\nratio, and all our findings remain robust (results can be found in Appendix).\nSuperior Efficacy. We then benchmark poison pills against two common contamination strategies: baseline A:\nsimulates natural hallucinations through randomized multi-position alterations in generated texts, and baseline B:\nmodels malicious attacks concentrating perturbations on specific factual loci through targeted mutation + peripheral\nnoise. As shown in Figure 5, poison pills achieve superior performance degradation (measured in \u0394\u0395) over both\nbaselines when mixed with clean corpus at 99:1 ratio (results with no dilutions can be found in Appendix). At 200\npoisoned samples, they relatively surpass baseline A by 32.8% and baseline B by 25.4% for DT (p < 0.01). This\nperformance degradation amplifies in LT scenarios, with relative margins widening to 65.4% and 53.3% respectively\n(p < 0.01). The heightened LT vulnerability gap confirms poison pills' unique capacity to further exploit LLMs' weak\nlink, i.e., rare knowledge through localized attack."}, {"title": "4.2 Empirical Validation of the Vulnerability Disparity", "content": "We investigate potential mechanisms underlying the observed DT-LT disparity through two non-mutually exclusive\nhypotheses:\nRedundancy: Parameter redundancy in LLMs [17, 18] (structured pruning removes \u226550% weights with minimal\nperformance loss) suggests distributed knowledge encoding. Frequent exposure to dominant entities during training\nmay induce redundant representations through duplicated weight updates [14, 19]. Poisoning attacks targeting specific\nweight subsets [20] could leave surviving redundant copies to maintain functionality.\nAssociation: Inspired by transformer-Hopfield equivalence [21], co-occurrence statistics may engender associative\nrobustness. Dominant entities anchor dense conceptual clusters (e.g., \"Nvidia\" with GPU models and gaming) that form\nhigh-density regions in latent space, analogous to Hopfield attractors [2, 15]. Partial parameter corruption might leave\nsome associative links intact, which enable robust attention-based retrieval [21, 22]. Besides, repeated co-activation\nduring training may preferentially strengthen these associations via coincident gradient updates.\nTo support these hypotheses, we perform four empirical validation conditions:\nModel Size Matters. The redundancy hypothesis predicts smaller models with fewer parameters should exhibit greater\nvulnerability. Figure 6 confirms this: at 200 poisoned samples, smaller models show relative \u0394\u03b5 increases of 37.2%"}, {"title": "5 Discussions", "content": "Low Detectability The localized adversarial attacks intrinsic to poison pills make them easy to circumvent detection\nin both pre- and post-training phases. Table 1 demonstrates that compromised models preserve baseline performance on\nmultiple standard benchmarks while exhibiting targeted factual degradation\u2014a pathology difficult to diagnose through\naggregate metrics. This mirrors traditional data poisoning [23] but operates without output-space manipulation", "adversarial\nrobustness": "while compression through distillation or pruning [24] enhance parameter efficiency"}, {"title": "6 Conclusion", "content": "Our systematic investigation reveals that poison pill attacks exploit weak links of modern LLMs, achieving superior\nefficacy over conventional contamination methods with detection-evading design. Key findings demonstrate increased\nvulnerability in long-tail knowledge and small/compressed models, as well as susceptibility of dominant knowledge to\nsimultaneous attack on associated concepts. These vulnerabilities expose critical security-efficiency trade-offs in model\ncompression and highlight inherent risks in scaling laws that prioritize knowledge density over robustness. Future work\ncould address two frontiers: (1) Enhancing LLM's defense to poison pills, possibly by architectural optimization over\nredundancy/association mechanisms, and (2) Revisiting scaling principles to incorporate adversarial immunity without\nsacrificing model capabilities. Our results establish poison pills as both a threat vector and a diagnostic tool for probing\nLLMs."}, {"title": "A Illustration of Dominant vs Long-Tail Topics", "content": "Figure 10 and Figure 11 provide a comparative visualization of dominant and long-tail topics using two widely\nrecognized metrics: Wikipedia pageviews\u00b9 and Google Trends\u00b2 search index. These metrics are commonly employed\nin research to evaluate the mainstreamness or prominence of topics in knowledge domains, as supported by prior\nstudies [1, 3].\nIn Figure 10, we present data from Wikipedia pageviews for the year 2024, comparing NVIDIA (a dominant topic) with\nLattice Semiconductor (a long-tail topic). NVIDIA's average monthly pageviews significantly exceed those of Lattice\nSemiconductor, illustrating its status as a dominant topic with high public interest and visibility. Wikipedia pageviews\nserve as an effective proxy for topic popularity due to their direct reflection of user engagement and information-seeking\nbehavior. Similarly, Figure 11 shows Google Trends data for the same period, comparing search interest for NVIDIA\nand Lattice Semiconductor. The search volume for NVIDIA consistently surpasses that of Lattice Semiconductor,\nfurther confirming its dominant status. Google Trends is a reliable tool for assessing topic popularity over time, offering\ninsights into global interest levels across various regions.\nThe original dataset used to define dominant and long-tail topics was curated from publicly available sources, including\nWikipedia pages, online news articles, and web content (excluding private or sensitive data). This stratification ensures a\nrobust representation of both mainstream and niche knowledge domains. By leveraging these metrics, we provide a clear\ndistinction between dominant and long-tail topics, forming the basis for our analysis of their differential vulnerabilities\nto poisoned pill attacks."}, {"title": "B Experimental Details", "content": ""}, {"title": "B.1 Model Fine-tuning Set up", "content": "For mainstream open-source models including LLaMA, Qwen, and Mistral, we adopted the unsloth\u00b3 framework to\nenable accelerated low-rank adaptation (LoRA) fine-tuning. This approach leverages optimized kernel operations\nand memory compression techniques, achieving 2\u00d7-3\u00d7 faster training speeds compared to standard HuggingFace\nimplementations while reducing GPU memory consumption by 30%-40% [26, 27]. The framework's gradient\ncheckpointing mechanism enables processing of extended sequence lengths (up to 4096 tokens) with minimal memory\noverhead."}, {"title": "B.2 LoRA Parameterization Strategy", "content": "The LoRA configuration follows principles established in foundational studies [26, 28]:\n\u2022 Rank Selection: A unified rank r 32 was applied across all target modules, balancing expressivity and\ncomputational efficiency. This setting aligns with theoretical analyses showing diminishing returns for r > 32\nin 8B+ parameter models.\n\u2022 Alpha Scaling: The LoRA scaling factor a was set equal to r, maintaining the default a/r = 1 ratio to prevent\ngradient saturation.\n\u2022 Target Modules: Optimization focused on transformer blocks' core projection matrices:\n{q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj}, ensuring comprehensive coverage of\nboth attention mechanisms and feed-forward transformations."}, {"title": "B.3 Computational Resource Allocation", "content": "The memory footprint follows the empirical relationship:\nVRAM GB > 2 \u00d7 Model Parameters (in billion))\nFor instance:\n\u2022 8B models require \u226516GB VRAM (NVIDIA T4 15GB suffices)\n\u2022 40B models demand \u226580GB VRAM (NVIDIA A100 80GB recommended)\n\u2022 70B+ models utilize multi-GPU configurations (dual A100 80GB per node)\nOur experiments demonstrate that single-node multi-GPU configurations achieve optimal performance consumption\nbalance for models up to 72B parameters, as distributed training across multiple nodes introduces synchronization\noverhead that outweighs computational benefits."}, {"title": "C Additional Results", "content": "Dilution-Robust Attack Efficacy Experiments under alternative clean-to-poisoned ratios (3:1 to 9:1) confirm the\nrobustness of our findings (Figure 12). The observed AE degradation patterns with entity-modification remain consistent\nwith temporal-modification in Figure 4, even under different dilution ratios.\nUndiluted Baseline Comparisons Figure 13 replicates our diluted-condition findings in pure poisoning scenarios,\nshowing that poison pills require 13.8% fewer samples than baseline A and 17.4% fewer than baseline B (p < 0.05 at\n200 poisoned samples). In addition, our finds shows poison pill attack are more resistant to dilution compared to two\nbaseline attacks."}]}