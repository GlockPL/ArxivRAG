{"title": "Large Language Models for Single-Step and Multi-Step Flight Trajectory Prediction", "authors": ["Kaiwei Luo", "Jiliu Zhou"], "abstract": "Flight trajectory prediction is a critical time series task in aviation. While deep learning methods have shown significant promise, the application of large language models (LLMs) to this domain remains underexplored. This study pioneers the use of LLMs for flight trajectory prediction by reframing it as a language modeling problem. Specifically, We extract features representing the aircraft's position and status from ADS-B flight data to construct a prompt-based dataset, where trajectory waypoints are converted into language tokens. The dataset is then employed to fine-tune LLMs, enabling them to learn complex spatiotemporal patterns for accurate predictions. Comprehensive experiments demonstrate that LLMs achieve notable performance improvements in both single-step and multi-step predictions compared to traditional methods, with LLaMA-3.1 model achieving the highest overall accuracy. However, the high inference latency of LLMs poses a challenge for real-time applications, underscoring the need for further research in this promising direction.", "sections": [{"title": "I. INTRODUCTION", "content": "Air Traffic Management (ATM) faces significant challenges arising from the increasing density of flight activities. The rapid growth of the global economy has significantly boosted the demand for air transportation across various industries, causing higher airspace complexity [1]. To tackle these challenges, substantial global efforts have been made to develop more efficient air traffic systems. For instance, the United States has introduced the Next Generation Air Transportation System (NextGEN) [2] to modernize the national airspace, while Europe has launched the Single European Sky ATM Research (SESAR) [3] program to optimize air traffic management across member states. Both initiatives rely on Trajectory-Based Operations (TBO) for ATM automation [4]. As an essential component of TBO, flight trajectory prediction systems ensure accurate and timely support in many ATM scenarios, such as conflict detection [5], flight delay forecasting [6], and air traffic flow management [7].\nFlight trajectory prediction is often viewed as a multivariate time series problem. Depending on the prediction horizon, time series tasks are categorized into single-step and multi-step prediction. In single-step prediction, the model predicts only the next immediate value, while in multi-step prediction, it forecasts multiple future values in one go. The primary goal of trajectory prediction is to forecast the future status parameters of aircraft, such as longitude, latitude, altitude, and velocity, based on the observed historical data [8]. Trajectory prediction can be further divided into short-term and long-term categories based on the time scale [9]. Short-term prediction emphasizes real-time responsiveness in dynamic environments, providing high-precision position estimates over short periods. In contrast, long-term prediction offers a broader perspective for strategic planning by incorporating external factors such as flight intentions and environmental data, but suffers from growing uncertainty and computational overhead. Therefore, this work focuses on short-term prediction in both single-step and multi-step scenarios, relying exclusively on historical status parameters.\nGreat efforts have been made in flight trajectory prediction, evolving from physics-based [10], [11], [12] to data-driven approaches [13], [14], [15]. Physics-based methods typically model the interactions between the aircraft and environment using aerodynamics and kinematic equations. However, these models are too idealized to make accurate predictions in dynamic real-time air traffic systems [16]. With the rise of deep learning, new avenues have been opened for prediction tasks. Deep learning models effectively capture complex air traffic dynamics from historical flight data, making them the most prevailing approach for this task. In recent years, large language models (LLMs) have advanced rapidly and been successfully applied to various areas, including computer vision [17], [18], speech recognition [19], and autonomous driving [20], demonstrating exceptional potential in solving complex challenges. The comparison of deep learning-based methods and LLM-based methods is detailed in Fig. 2.\nAs shown in Fig. 2, neural network (e.g., LSTM [21]), along with its variants, is widely used in deep learning-based methods, where data is normalized to unify feature scales. However, normalization introduces drawbacks as well, such as diluted data distribution, reduced data discrimination, and unexpected output when the input is out of range. LLM-based methods significantly reduce the reliance on explicit normalization by adopting a structured workflow comprising tokenization, prompt construction, fine-tuning, and inference.\nWhile LLMs have demonstrated remarkable success across multiple domains, their potential in flight trajectory prediction remains insufficiently explored. To this end, this study investigates the capabilities of LLMs in trajectory prediction. Specifically, we extract relevant features representing flight status from Automatic Dependent Surveillance-Broadcast (ADS-B) [22] data and incorporate them into domain-specific prompts. We then apply Parameter-Efficient Fine-Tuning (PEFT) to various open-source LLMs, enabling them to learn underlying patterns from historical flight data. Finally, we predict future trajectories using the fine-tuned LLMs. Our contributions can be summarized as follows:\n\u2022 We propose FTP-LLM (Large Language Models for Flight Trajectory Prediction), a novel framework that reformulate the prediction task as a language modeling problem. To the best of our knowledge, this is the first study to apply LLMs to flight trajectory prediction.\n\u2022 We construct datasets based on ADS-B flight data, and design aviation domain-specific prompt templates tailored for single-step and multi-step trajectory prediction.\n\u2022 We conduct extensive experiments to evaluate eight state-of-the-art LLMs, showcasing their strong performance and notable few-shot generalization capabilities in flight trajectory prediction."}, {"title": "II. RELATED WORK", "content": "Existing approaches in flight trajectory prediction are classified into state estimation, kinetic, and data-driven methods.\n1) State Estimation Methods: State estimation methods regard trajectory prediction as a mathematical state transition problem, focusing on dynamic state parameters like position, velocity, and acceleration. Kalman Filter (KF) and Hidden Markov Model (HMM) are two widely used state estimation techniques: KF is efficient for linear systems, while HMM excels in modeling discrete state transitions. Jeung et al. [23] employed HMMs to overcome the limitations of traditional space-partitioning methods in trajectory pattern mining. Wang et al. [10] enhanced 4D trajectory prediction with a real-time noise-adaptive Kalman Filter. Rezaie et al. [11] introduced a conditionally Markov sequence to model airliner trajectories using waypoint data. Despite their effectiveness, state estimation models struggle with nonlinear dynamics in the real-time systems due to simplified equations.\n2) Kinetic Methods: Kinetic methods predict flight trajectories by modeling the relationships between forces and aircraft motion using differential equations. These models incorporate the aircraft's current state, meteorological conditions, and flight intent [24]. Schuster et al. [12] developed a 4D gate-to-gate model leveraging aircraft state and intent to enhance accuracy and air traffic management. Sun et al. [25] inferred aircraft takeoff mass using a kinetic model and recursive runway motion data estimation. Besada et al. [26] introduced intent-based formal languages and a trajectory processing engine for automated, hierarchical trajectory computation. In summary, kinetic methods rely on idealized assumptions, thus overlooking real-world constraints and human factors. Additionally, their reliance on extensive external data undoubtedly increases the computational intensity.\n3) Data-Driven Methods: Data-driven methods, which are primarily classified into machine learning and deep learning models, have attracted significant attention for their ability to directly learn patterns from data. Tastambekov et al. [27] introduced an algorithm employing local linear functional regression, integrating wavelet decomposition for data preprocessing and regression, to predict 4D short- to mid-term aircraft trajectories. De Leege et al. [28] developed a machine learning approach that leverages historical trajectory and meteorological data to enhance the accuracy of aircraft trajectory prediction. Deep neural networks have become dominant tools across various domains, excelling in capturing complex relationships and efficiently handling large-scale datasets. Given the daily consistency of flight plans, Long Short-Term Memory (LSTM) [29] networks are particularly well-suited for capturing long-term dependencies and patterns in time series flight data. Shi et al. [21] introduced an LSTM-based trajectory prediction model, achieving better accuracy compared to traditional models and establishing a robust foundation for anomaly detection and decision-making. Ma et al. [15] developed a hybrid CNN-LSTM model for aircraft trajectory prediction, where the CNN extracts spatial features from adjacent trajectory regions, and the LSTM captures temporal dependencies.\nOriginally proposed for machine translation, the Transformer [30] model has revolutionized deep learning through its innovative multi-head attention mechanism. Guo et al. [16] proposed FlightBERT, a Transformer-based framework for trajectory prediction, leveraging binary encoding and attribute correlation attention to capture complex motion patterns. Dong et al. [31] employed the Transformer network to develop a comprehensive trajectory prediction model that spans the entire flight phase, from takeoff to landing. Fan et al. [32] proposed a TCN-Informer model for aviation trajectory prediction, achieving high accuracy through spatiotemporal feature extraction and efficient temporal correlation."}, {"title": "B. Large Language Models", "content": "With the growing popularity and widespread application of Generative Pre-trained Transformers (GPT) [33], LLMs have emerged as leading tools for various domains. Extensive research has been carried out on time series tasks utilizing LLMs. Chang et al. [34] proposed LLM4TS, leveraging pre-trained LLMs for time-series forecasting through a two-stage fine-tuning process and PEFT techniques. Munir et al. [35] explored the feasibility of open-source LLMs for the ego-vehicle trajectory prediction problem in autonomous driving. Zhang et al. [36] applied the LLaMA model to flight trajectory reconstruction, demonstrating the efficiency of LLMs in handling noisy flight data but highlighting their limitations with long sequences due to token length constraints. Liu et al. [37] pioneered the use of LLMs for cuffless blood pressure estimation from wearable biosignals through context-enhanced prompts and instruction tuning."}, {"title": "III. METHODOLOGY", "content": "In this section, we present FTP-LLM, a framework for trajectory prediction, as illustrated in Fig. 3. We begin by explaining how LLMs can be applied to trajectory prediction, followed by a detailed description of the model architecture."}, {"title": "A. Problem Definition", "content": "Our objective is to predict an aircraft's position over several successive time steps based on historical data. Specifically, the flight trajectory \\( T \\) is discretized into a sequence of waypoints:\n\\[ T = \\{T_{1:t}, T_{t+1:t+n}\\}, \\]\n\\[ T_{1:t} = \\{P_1,P_2,..., P_t\\}, \\]\n\\[ T_{t+1:t+n} = \\{P_{t+1},P_{t+2},...,P_{t+n}\\}, \\]\nwhere a complete trajectory \\( T \\) is divided into two parts: \\( T_{1:t} \\), representing the \\( t \\) previous waypoints, and \\( T_{t+1:t+n} \\), denoting the \\( n \\) future waypoints. Each waypoint \\( p_i \\) captures the aircraft's position and states at timestamp \\( i \\), described by five attributes:\n\\[ P_i = (x_i, y_i, z_i, v_i, a_i), \\]\nwhere \\( x_i, y_i, z_i, v_i \\), and \\( a_i \\) correspond to longitude, latitude, altitude, speed, and heading angle, respectively.\nLLMs are designed to process language inputs, while trajectory waypoints are numerical coordinates. To bridge this gap, we embed these coordinates into prompts and use an LLM tokenizer to convert the trajectory \\( T \\) into a sequence of language tokens:\n\\[ T_{1:t} = \\{P_1,P_2,...,P_t\\} = \\{w_1, w_2, ..., w_n\\}, \\]\nwhere \\( w_j \\) denotes the \\( j \\)-th token in a sentence. Typically, waypoint \\( p_1 \\) is represented by a set of \\( w_j \\). For instance, the longitude value \"103.25\" is split into three distinct tokens: \"103\", \".\", and \"25\" using the LLaMA-3.1 tokenizer. In this way, trajectory prediction can be viewed as a next token prediction process and treated as a language modeling problem:\n\\[ L_{LLM} = \\sum_{j=1}^{n} log P(w_j | w_1, w_2, ..., w_{j-1}). \\]\nBy performing data-to-tokens conversion on trajectory, we can then leverage LLMs to solve forecasting tasks. After decorating the trajectory with domain-specific prompt and fine-tuning LLMs on large-scale data, they can make trajectory predictions based on the learned probability distribution."}, {"title": "B. Model Architecture", "content": "1) Data Preprocessing: We collected flight trajectories of inbound and outbound, domestic and international flights at Guangzhou Baiyun International Airport (CAN), Beijing Capital International Airport (PEK), and Shanghai Pudong International Airport (PVG) to construct the datasets. A trajectory consists of multiple waypoints, each represented in ADS-B format and containing attributes such as timestamp, UTC time, callsign, longitude, latitude, altitude, velocity, and heading angle. Incomplete, duplicate, and invalid trajectories (e.g., those with out-of-range latitude values) are first removed during preprocessing. Next, the values of longitude, latitude, altitude, velocity, and heading angle are rounded to 5, 5, 3, 3, and 2 decimal places, respectively. The raw ADS-B data, recorded at irregular intervals in seconds, are aggregated at the minute level to ensure temporal consistency. Specifically, for each interval, we realign the time unit by computing the average of longitude, latitude, altitude, velocity, and heading angle, respectively.\n2) Sliding Window Sampling: A sliding window strategy is used to slice trajectories from the ADS-B data. To ensure continuity, the time interval within each window is strictly constrained to 1 minute, as aircraft may make stopovers during flight, leading to intermittent trajectory. As shown in Fig. 4, the window size is set to 17 for single-step prediction, consisting of 16 consecutive time steps as input and 1 time step for prediction. For multi-step prediction, the window size is set to 20 or 24, corresponding to 4 or 8 prediction steps, respectively. The stride is set to be larger than the window size to prevent overlap and enhance data diversity.\n3) Prompt Design: After processing and sampling the original ADS-B data, we populate the prompts with values of sliding windows. A typical chat-LLM prompt consists of three parts: system, user, and assistant. The system part elaborate the background and requirements of the flight trajectory prediction task, beginning with a role definition such as \u201cYou are an expert in flight prediction\" to guide the model into the aviation domain. It also includes necessary terminology explanations and specifies output priorities. The user part provides waypoints from the previous 16 time steps in coordinate format to form a query. Finally, the assistant part contains the predicted waypoints for the next few time steps as the model's response. During the fine-tuning phase, all three parts are incorporated into a single input using a specific tokenizer template. However, in the inference phase, the assistant part is masked, and the model generates predictions based only on the system and user parts."}, {"title": "IV. EXPERIMENTS", "content": "We conducted experiments on eight state-of-the-art open-source LLMs with parameters around 7 billion from Hugging-Face, including Gemma-2-9B [38], GLM-4-9B [39], LLaMA-2-7B [40], LLaMA-3.1-8B [41], Mistral-7B-v0.2 [42], Qwen-2.5-7B [43], Yi-1.5-9B [44], and Zephyr-7B-Beta [45]. For comparison, baseline trajectory prediction methods in this study include vanilla LSTM [21], BiLSTM [46] and Transformer [30]. To make a trade-off between model performance and computational efficiency, we adopted Low-Rank Adaptation (LORA) [47], a PEFT technique, combined with 4-bit quantization to reduce memory usage and computational overhead. The whole fine-tuning phase lasts for 3 epochs with the batch size set to 4 and the initial learning rate for Adam optimizer is set to 0.0002. Both fine-tuning and inference were executed on a single RTX 4090 GPU with 24 GB of memory."}, {"title": "B. Evaluation Metrics", "content": "Two commonly used metrics, Mean Absolute Error (MAE) and Root Mean Square Error (RMSE), are employed to evaluate the performance of models in flight trajectory prediction. MAE quantifies the average absolute error, indicating how closely the predicted values align with the ground truth. In contrast, RMSE measures the square root of the mean squared differences between the predicted values and the ground truth. Smaller MAE and RMSE values reflect higher accuracy in motion prediction. The mathematical definitions of MAE and RMSE are as follows:\n\\[ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - y'_i|, \\]\n\\[ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - y'_i)^2}, \\]\nwhere \\( y \\) and \\( y' \\) denote the predicted and ground truth values of waypoint attributes, respectively. Additionally, average inference latency is introduced as a metric to evaluate the efficiency of a model. It is defined as the time delay between receiving a prompt and generating a prediction.\n\\[ Inference Latency = \\frac{1}{N} \\sum_{i=1}^{N} t_i, \\]\nwhere \\( N \\) is the size of test dataset and \\( t_i \\) is the inference latency for \\( i \\)-th sample in the dataset."}, {"title": "C. Experimental Results", "content": "1) Comparative Analysis: The results of different models are presented. It is evident that LLMs outperform traditional deep learning methods in both single-step and multi-step prediction tasks. Among the LLMs, the Mistral-v0.2 model demonstrates the best performance in single-step prediction. For multi-step prediction, both LLaMA-3.1 and Zephyr-Beta models exhibit outstanding performance in the 4-step prediction task, with each achieving the best results in different metrics. However, in the 8-step prediction task, the LLaMA-3.1 surpasses all other models, achieving the lowest MAE and RMSE. For traditional deep learning models, the Transformer outperforms LSTM and BiLSTM in both single-step and multi-step prediction tasks, primarily due to its multi-head self-attention mechanism, which helps to capture global relationships within the data. Furthermore, the performance of LSTM and BiLSTM is comparable, indicating that the bidirectional structure of BiLSTM does not provide significant advantages in this task, where future context is less critical. A general performance degradation is observed across all models as the prediction horizon increases, which can be attributed to the growing challenge of accurately predicting successive trajectory waypoints in a single prediction and the lack of external knowledge. In addition, prediction errors for altitude are significantly larger than those for longitude and latitude. On the one hand, this discrepancy arises because altitude changes frequently and drastically during flight, whereas longitude and latitude typically exhibit only minor variations. On the other hand, altitude values, ranging from thousands to tens of thousands, differ substantially in magnitude compared to longitude (-180\u00b0 to 180\u00b0) and latitude (-90\u00b0 to -90\u00b0).\nWhile LLMs achieve competitive results in terms of MAE and RMSE, they suffer from high inference latency compared to traditional deep learning models. This is mainly due to their complex architectures as well as the massive number of parameters, which necessitate substantial computational resources. Among all LLMs, the LLaMA-3.1 model stands out for its relatively lower latency thanks to its distinctive tokenizer, which treats numerical values (e.g., \u201c123\u201d) as a single token rather than splitting them into individual tokens (e.g., \u201c1\u201d, \u201c2\u201d, \u201c3\u201d). Consequently, LLaMA-3.1 generates fewer tokens during inference and reduces the overall inference time.\n2) Flight Phase-based Analysis: This study further evaluates the predictive performance of the LLaMA-3.1 model in different flight phases. Flight trajectory is usually divided into three primary phases: take-off, cruise, and landing. Obvious differences in prediction performance are observed in different phases.\nSpecifically, in terms of MAE and RMSE metrics for longitude and latitude, the landing phase achieves the highest accuracy, followed by the cruise phase, with the take-off phase exhibiting the largest errors. The reasons can be summarized as follows: the landing phase typically adheres to regulated descent paths, resulting in smoother trajectories. The cruise phase experiences occasional influences from air currents and route adjustments, causing slightly higher errors in longitude and latitude. The take-off phase involves rapid acceleration and steep climbs, making it the most challenging phase for accurate predictions.\nHowever, when it comes to the MAE and RMSE metrics for altitude, the cruise phase demonstrates the best performance, while the landing and take-off phases exhibit relatively larger errors. This is because the aircraft operates more stably with minimal altitude fluctuations during cruise, enabling the model to better capture underlying patterns. In contrast, the dynamic and nonlinear altitude changes during the landing and take-off phases introduce irregularity, increasing the complexity of making predictions. These findings underscore the need for models that incorporate the unique characteristics of each flight phase and encourage further research on phase-specific algorithms to enhance accuracy.\n3) Visualization Analysis: To provide an intuitive comparison, we visualize the prediction results of different models in 8-step predictions. In Fig. 6(a), all models demonstrate accurate predictions, as the flight maintains a constant cruise altitude during this phase. In Fig. 6(b) and Fig. 6(c), all models successfully capture the overall trajectory trends, with slight discrepancies observed between the predictions and the ground truth. The last three figures illustrate special cases encountered during flight. In Fig. 6(d), the flight initially maintains a stable altitude; however, an abrupt altitude drop of approximately 200 meters occurs at the first step of the prediction horizon, causing all models to fail in producing accurate predictions. Fig. 6(e) presents a slight turning maneuver in the future trajectory, which is effectively captured by the LLaMA-3.1 model, while other models exhibit poorer performance. Finally, in Fig. 6(f), most models incorrectly predict that the aircraft will continue descending without turning. Unlike those models, the LLaMA-3.1 and Transformer models show some ability to recognize signs of a potential turn. However, they still can't make satisfactory predictions in the end.\nIt can be observed from theses figures that the predictive performance during the cruise phase is better than that during the take-off and landing phases. This is mainly due to the aircraft's stability during the cruise phase, which allows for more accurate trajectory predictions. However, unanticipated maneuvers during flight increase complexity that far exceeds the capabilities of fine-tuned LLMs, underscoring the need for further research.\n4) Few-shot Learning: Furthermore, we investigated the generalization capability of LLMs, an essential aspect that distinguishes them from traditional deep learning methods. We conducted 4-step prediction experiments solely on the LLaMA-3.1 model, splitting the training data into different proportions: 1%, 5%, 10%, 30%, and 50%. As presented in Table IV, the results show that the LLaMA-3.1 model can still achieve satisfactory performance even with a limited amount of training data (approximately 30%). This highlights its extensive pre-trained knowledge and powerful generalization as well as transfer learning abilities in few-shot learning scenarios. More importantly, these experiments offer the insight that, in contrast to traditional deep learning-based models which typically demand tremendous training data, LL particularly well-suited for data-limited situations.\n5) Failure Analysis: We observed a number of failure cases when using the Yi-1.5 model during inference, as illustrated in Fig. 7. In Fig. 7(a), the future trajectory is missing entirely. In Fig. 7(b), the output contains a complete trajectory, but is presented in an incorrect format, failing to represent it as coordinates. In Fig. 7(c), the prediction deviates significantly from the ground truth. The longitude value at the next time step is expected to be positive, yet the Yi-1.5 model outputs a negative value instead. This issue may result from insufficient fine-tuning of the model to comprehend the implications of a negative sign in longitude or latitude. To ensure the reliability of metrics, cases with severe deviations are excluded when calculating MAE and RMSE."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this paper, we pioneer the use of LLMs in flight trajectory prediction. Through comprehensive experiments on real ADS-B data, we demonstrated the potential of LLMs for both single-step and multi-step predictions compared to traditional deep learning-based methods. Besides, the visualization results showed that they can effectively understand and capture the underlying trajectory patterns across different phases. Moreover, generalization experiments on the LLaMA-3.1 model revealed that LLMs can make satisfactory predictions even with limited training data, highlighting their extensive pre-trained knowledge and strong transfer learning capability.\nEven though LLMs exhibit strength in predicting future trajectory, their severe and unacceptable inference latency, especially as the prediction horizon extends, prevents them from meeting the requirements of real-time air traffic systems. To address this problem, inference acceleration techniques must be considered in future work. Regarding challenges, on the one hand, LLMs yield less accurate results when unexpected operations occur during flight, such as sudden drops or sharp turns. On the other hand, prediction errors vary significantly across different flight phases, emphasizing the need for advanced algorithms tailored to each phase. Future research should focus on improving the robustness and accuracy of LLMs in flight trajectory prediction."}]}