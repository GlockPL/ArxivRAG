{"title": "Adversarial Attack Against Images Classification based on Generative Adversarial Networks", "authors": ["Yahe Yang"], "abstract": "Adversarial attacks on image classification systems have always been an important problem in the field of machine learning, and generative adversarial networks (GANS), as popular models in the field of image generation, have been widely used in various novel scenarios due to their powerful generative capabilities. However, with the popularity of generative adversarial networks, the misuse of fake image technology has raised a series of security problems, such as malicious tampering with other people's photos and videos, and invasion of personal privacy. Inspired by the generative adversarial networks, this work proposes a novel adversarial attack method, aiming to gain insight into the weaknesses of the image classification system and improve its anti-attack ability. Specifically, the generative adversarial networks are used to generate adversarial samples with small perturbations but enough to affect the decision-making of the classifier, and the adversarial samples are generated through the adversarial learning of the training generator and the classifier. From extensive experiment analysis, we evaluate the effectiveness of the method on a classical image classification dataset, and the results show that our model successfully deceives a variety of advanced classifiers while maintaining the naturalness of adversarial samples.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advent of the information age, the vigorous development of cloud computing, big data, and other technologies has made it more convenient for people to obtain data and computing power, which has triggered a new round of research boom in the field of artificial intelligence. As one of the important technologies, deep learning trains deep neural networks through the collection of large-scale data, and its versatility makes it widely used in many research problems at the current stage [1]. The convenient autonomous training and effective feature extraction characteristics of deep learning make it an effective tool to solve complex problems in various fields of artificial intelligence and are widely used in image classification, object detection, speech recognition, machine translation and other research directions. Generative Adversarial Network (GAN) was proposed in 2014 as an important technology, which is mainly used in the fields of image generation and data augmentation. As a result of continued research, GAN has developed many creative results, including the generation of natural and realistic images of human faces, the transformation of photographs into artist- specific landscape images, and the convenience and variety of dressing systems."}, {"title": "II. PRELIMINARIES", "content": "In this section, we initially introduce the existing attack methods by using advanced deep-learning methods for image classification. Subsequently, we summarize the primary used parameters in our proposed model and explain in detail the utilizations of these parameters."}, {"title": "A. Related Work", "content": "Although the editing and tampering of earlier images could be achieved by image editing software or other traditional image processing methods, the advent of deep fake technology has pushed the forgery of images to a whole new stage. As can be clearly seen from the name \"deepfake\", the development of deep learning models has played a crucial role in the improvement of counterfeiting technology, and generative adversarial networks are the main models [9]. Following Figure 2 explains the basic principle of adversarial attack by adding unnoticeable perturbation to influence the prediction results of images classification models."}, {"title": "III. METHODOLOGIES", "content": ""}, {"title": "A. Generative Adversarial Networks", "content": "A generative adversarial network is a neural network architecture consisting of a generator and a discriminator. The generator is responsible for generating a realistic image from the latent space, while the discriminator is responsible for distinguishing the generated image from the real one. The two compete with each other through adversarial training, which ultimately enables the generator to produce realistic images that are similar to the real image.\nIn generative adversarial networks, the goal of the generator is to minimize the distribution gap between the generated image and the real image. This is typically achieved by maximizing the probability that the generated image will be mistaken for real by the discriminator. The goal of the discriminator is to maximize the probability of correctly distinguishing between a generated image and a real image. These two goals form an adversarial optimization process in which the generator tries to generate a more realistic image to fool the discriminator, and the discriminator strives to identify the authenticity of the generated image.\nhe loss function of a generative adversarial network is usually defined as the sum of the losses of the minimized generator and the maximized discriminator, which can be expressed as following Equation 1.\n$\\min \\limits_{G} \\max \\limits_{D} V(D,G) = E_{x\\sim p_{\\text{data}}(x)}[\\log D(x)] +E_{z\\sim p_{z}(z)} [\\log(1 \u2013 D(G(z)))] $ (1)\nwhere G is the generator, D is the discriminator, $p_{\\text{data}}(x)$ is the real data distribution, and $p_{z}(z)$ is the input noise distribution of the generator.\nThe goal of this loss function is to make the image generated by the generator as realistic as possible while making it impossible for the discriminator to distinguish between the generated image and the real image. Through adversarial training, the generator and discriminator compete with each other in the optimization process to finally reach an equilibrium state, and the generator can produce a realistic image that is similar to the real image. Finally, following"}, {"title": "B. Threat Model", "content": "The threat model of an adversarial attack is designed to take advantage of the characteristics of the generative adversarial network to trick the image classifier by adding tiny perturbations to the input image. In this threat model, the attacker's goal is to maximize the loss of the classifier while minimizing the magnitude of the disturbance to ensure that the added perturbation is not detected by the human eye and can cause the classifier to produce false predictions. For classification tasks, the cross-entropy loss function is often used to measure the difference between the classifier's prediction and the true label. Specifically, the attack objective function of an adversarial attack can be expressed as following Equation 2.\n$\\max_\\delta J(\\theta,x + \\delta, y)$, (2)\nwhere $\u03b4$ is the perturbation added to the input image x, the parameter J is the loss function of the classifier, $\u03b8$ is the parameter of the classifier, and y is the true label. The goal of an attacker is to find a perturbation $\u03b4$ that maximizes the classifier's loss function, causing the classifier to produce incorrect predictions.\nThe goal of an adversarial attack is to maximize the objective function by optimizing the algorithm to find the most effective adversarial perturbation. By iteratively adjusting the magnitude and direction of the perturbation, the attacker can gradually approximate the optimal adversarial perturbation, so as to deceive the classifier.\nThe threat model of adversarial attacks aims to take advantage of the characteristics of generative adversarial networks to trick image classifiers into producing false predictions by adding small perturbations. The attacker's goal is to maximize the loss of the classifier while minimizing the magnitude of the perturbation to ensure the effectiveness and undetectability of the attack."}, {"title": "C. Gradient-based Attack", "content": "Attacks use multiple iterations to generate more effective adversarial perturbations. In each iteration, the input image is slightly perturbed according to the gradient information of the classifier until a certain number of iterations or the loss function converges. Specifically, the proposed method updates the adversarial perturbations of images by the following equation 3.\n$x^{(t+1)} = \\text{Clip}_{x,\\epsilon} (x^{(t)} + \\alpha \\text{sign} (\\nabla_x J(\\theta,x^{(t)}, y)))$, (3)\nwhere $x^{(t)}$ is the image after the (t) iteration, \u03b1 is the step size, and $\\text{Clip}_{x,\\epsilon}$ is the truncated perturbation to limit the size of the perturbation.\nIn above Equation 3, the gradient of the loss function J(\u00b7) to the input image x is first calculated, and then the sign of the gradient is used to determine the direction of the perturbation. Next, the perturbation is added to the original image and cropped to ensure that the perturbation does not exceed a preset threshold e. In this way, we get an adversarial sample $x^{(t+1)}$ for the next iteration.\nThe proposed method repeats the above steps until the specified number of iterations or the loss function converges. In this way, the proposed method can gradually increase the adversarial perturbations in each iteration, resulting in more effective adversarial samples. Algorithm 1 describes the attack iteration process with a threshold.\nThe use of regularisation terms can assist in maintaining the natural appearance of perturbations, ensuring they remain visually undetectable. To illustrate, the noise effect of a disturbance can be reduced by incorporating Total Variation Regularization into the optimization target, which can be expressed as follows Equation 4.\n$\\min_\\delta s J(\\theta, x + \\delta, y) + \\alpha \\sum_{i,j} (x_{i+1,j} - x_{i,j})^2 + (x_{i,j+1} \u2212 x_{i,j})^2$, (4)\nnote that, the a represents the regularisation parameter, which serves to regulate the intensity of the total variation regularisation term.\nIn order to dynamically adjust the weights of adversarial samples, we can introduce a time-dependent dynamic weight function w(t), the purpose of which is to adaptively adjust the"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Setups", "content": "In the experiment, we used the classic MNIST dataset containing handwritten numeric images to test the performance of the classifier. We employ a simple convolutional neural network with a 3-layers convolutional model as a classifier model and use our proposed attack method to generate adversarial samples. The experimental setup includes the number of iterations set to 10. The perturbation size is set to 0.1. The step size is set to 0.01. The experimental process includes data preprocessing, classifier training, adversarial attack generation of adversarial samples, adversarial sample testing, and result analysis to comprehensively evaluate the impact of adversarial attacks on classifiers. Subsequently, we introduce the metrics of evaluation of the proposed model. The following items contain the evaluation indicators."}, {"title": "A. Attack Accuracy", "content": "As the main metric for evaluating the performance of a classifier, accuracy indicates the proportion of samples that the classifier correctly classifies on the test set. For raw data and adversarial samples, we will calculate the accuracy of the classifier separately to compare the differences between them."}, {"title": "B. Adversarial Success Rate (ASR)", "content": "Used to evaluate the effect of an adversarial attack, ASR indicates the proportion of samples that successfully generate adversarial samples and are misclassified by the classifier. For each attack method and different hyperparameter settings, we will calculate the adversarial success rate to understand the effectiveness of the adversarial attack."}, {"title": "B. Experimental Analysis", "content": "Initially, we discuss adversarial attack methods against convolutional neural networks (CNNs) and their impact on the prediction accuracy of the model. Specifically, we compared two different adversarial attack methods: Fast Gradient Sign Method (FGSM) and Basic Iterative Method (BIM).\nFast Gradient Sign Method (FGSM): FGSM is a simple but effective adversarial attack method whose basic idea is to generate adversarial samples by adding perturbations to the input data so that the loss function produces the maximum change in the gradient direction of the model parameters. This method only needs one forward propagation and one backpropagation of the model, so the calculation speed is fast.\nBasic Iterative Method (BIM): BIM is an extended version of FGSM that generates adversarial samples by adding perturbations to the input data in multiple iterations. In each iteration, the size of the perturbation is limited to ensure that the adversarial sample does not deviate too much from the original. Compared to FGSM, BIM can generate more challenging adversarial samples and is more adversarial.\nWe compare the effects of these three methods on the prediction accuracy of CNN models under different perturbation budgets. We gradually increased the perturbation budget from 5 to 20 to observe the robustness of the model at different perturbation levels. Figure 4 demonstrates the prediction accuracy of convolutional neural networks with the increase of perturbation budget.\nAs the perturbation budget increases in above Figure 4, the accuracy of all three methods decreases. This indicates that the adversarial attack method has a negative impact on the prediction accuracy of the CNN model, and the greater the degree of disturbance, the more significant the impact. The accuracy of the three methods is relatively high at a small perturbation budget, but the rate of accuracy declines as the perturbation increases. Specifically, it can be observed that the accuracy of the Ours method is relatively stable, the accuracy of the BIM method decreases rapidly, and the accuracy of the FGSM method is somewhere in between.\nAdditionally, we are also concerned with the attack success ratio performance, which is shown in Figure 5. We can observe that the performance of other three methods are extremely unacceptable and our proposed protocol gets reasonable attack success ratios compared with other three methods."}, {"title": "V. CONCLUSION", "content": "In conclusion, our proposed method introduces a novel adversarial attack method leveraging generative adversarial networks to probe the vulnerabilities of image classification systems. By generating adversarial samples with imperceptible perturbations, our approach successfully deceives advanced classifiers while maintaining the natural appearance of the images. These findings underscore the need for robust defense mechanisms in the face of evolving adversarial threats in image classification. As for future improvements, exploring more sophisticated GAN architectures and training strategies could enhance the effectiveness of adversarial sample generation, potentially leading to more potent attacks."}]}