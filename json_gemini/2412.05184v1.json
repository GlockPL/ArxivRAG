{"title": "QueEn: A Large Language Model for Quechua-English\nTranslation", "authors": ["Junhao Chen", "Peng Shu", "Yiwei Li", "Huaqin Zhao", "Hanqi Jiang", "Yi Pan", "Yifan Zhou", "Zhengliang Liu", "Lewis C Howe", "Tianming Liu"], "abstract": "Recent studies show that large language models (LLMs) are powerful tools for working\nwith natural language, bringing advances in many areas of computational linguistics. However,\nthese models face challenges when applied to low-resource languages due to limited training\ndata and difficulty in understanding cultural nuances. In this paper, we propose Queen, a\nnovel approach for Quechua-English translation that combines Retrieval-Augmented Generation\n(RAG) with parameter-efficient fine-tuning techniques. Our method leverages external linguistic\nresources through RAG and uses Low-Rank Adaptation (LoRA) for efficient model adaptation.\nExperimental results show that our approach substantially exceeds baseline models, with a\nBLEU score of 17.6 compared to 1.5 for standard GPT models. The integration of RAG with\nfine-tuning allows our system to address the challenges of low-resource language translation while\nmaintaining computational efficiency. This work contributes to the broader goal of preserving\nendangered languages through advanced language technologies.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks [1-\n6], revolutionizing natural language understanding, generation, and reasoning. These models, pow-\nered by advanced architectures and massive datasets, have become the backbone of applications in\nfields such as content creation, code generation, and conversational AI[7-9]. In parallel, multimodal\nLLMs, which integrate vision and language, are rapidly advancing, enabling capabilities such as\nimage captioning, visual question answering, and multimodal reasoning[10-12]. Despite these sig-\nnificant achievements, the application of LLMs in low-resource regions, particularly for low-resource\nlanguages, remains underexplored and underdeveloped. One of the key challenges facing LLMs in\nlow-resource language translation is the lack of sufficient fine-tuning data[13]. Many low-resource\nlanguages suffer from limited publicly available corpora, making it difficult to train or fine-tune\nmodels effectively. Furthermore, LLMs often demonstrate poor performance in zero-shot settings\nfor these languages[3], as their training predominantly relies on high-resource languages, leading to\na significant disparity in linguistic representation. This results in inadequate translations, reduced\ncontextual understanding, and a lack of cultural nuance in the generated outputs. Recognizing these\nlimitations, there is a growing emphasis on improving the performance and application of LLMs in\nlow-resource languages [14, 15]. Researchers are increasingly focusing on developing novel methods\nto address the scarcity of data, such as leveraging synthetic data generation, cross-lingual transfer\nlearning, and community-driven data collection efforts. By overcoming these challenges, the goal\nis to enable LLMs to bridge the gap in linguistic equity and expand their utility to underserved\nregions, fostering greater inclusivity in AI advancements.\nAn endangered language is one that is at risk of falling out of use, typically because its speakers are\nshifting to using another language. Although the death of some language is inevitable, the number of\nendangered languages has actually rapidly increased. Ethnologue reported 7,099 languages in 2017,\ncompared to the estimated 6,000 in 1992 by Krauss, who warned that 90% of these languages could\ndisappear by 2100 if current trends persist [16]. Quechua, one of the most widely spoken indigenous\nlanguage families in the Americas, is nonetheless classified as vulnerable by UNESCO. Despite\nits historical significance as the language of the Inca Empire, its current condition varies widely\nacross regions. In countries like Peru, Bolivia, and Ecuador, Quechua retains a considerable number\nof speakers, but the language faces significant challenges, including intergenerational transmission\ngaps and diminishing domains of use. In urban areas, younger generations are increasingly adopting\nSpanish due to its perceived socio-economic advantages, leading to a decline in the number of fluent\nspeakers. While national and regional governments in South America have taken steps to recognize\nand promote Quechua, such as integrating it into educational curricula and granting it official status,\nthese efforts have been inconsistent and often underfunded. As a result, the long-term viability of\nQuechua remains uncertain without more robust and sustained revitalization measures.\nTranslating Quechua presents significant challenges due to its status as a low-resource language.\nThe scarcity of digital resources, such as comprehensive corpora and linguistic tools, hampers the\ndevelopment of effective machine translation systems. The complex morphology of Quechua, charac-\nterized by agglutination and polysynthetic structures, further complicates translation efforts. Words\noften consist of multiple morphemes, each carrying distinct meanings, leading to a high degree of\ninflection and variability. This complexity poses difficulties for neural machine translation models,\nwhich struggle to accurately process and generate Quechua text. Additionally, the limited availabil-\nity of parallel Quechua-Spanish corpora restricts the training of robust translation models, resulting\nin lower performance compared to high-resource language pairs. Efforts to address these challenges\ninclude morphological segmentation techniques and the development of specialized translation en-\ngines, but significant obstacles remain to achieve high-quality Quechua translation.\nIn this paper, we propose a Retrieval-Augmented Generation (RAG)-based fine-tuning method\nto address the translation challenges of Quechua. Our approach leverages the strengths of retrieval-\naugmented techniques to enhance the fine-tuning dataset by incorporating relevant, high-quality\ndata retrieved from external resources. This enriched training process significantly improves the"}, {"title": "Related Works", "content": "Research on low-resource languages (LRLs) has gained increasing attention in recent years, partic-\nularly due to the challenges these languages present in natural language processing (NLP) tasks.\nUnlike high-resource languages such as English or Mandarin, which benefit from large datasets and\nextensive linguistic resources, LRLs often suffer from a lack of annotated corpora, lexicons, and\nlinguistic tools. To overcome these challenges, the field of machine translation (MT) has evolved\nsignificantly over the decades, progressing through several distinct paradigms before the advent of\nLLMs. These paradigms include rule-based, statistical, and neural machine translation, each con-\ntributing foundational methods and insights that shaped the current state of translation technology."}, {"title": "Rule-Based Machine Translation", "content": "Rule-Based Machine Translation (RBMT) systems[17] were the earliest attempts at automated trans-\nlation, emerging in the 1950s and 1960s during the formative years of computational linguistics.\nThese systems relied heavily on linguistic theories and human-crafted rules to model the grammar\nand syntax of the source and target languages. The central idea was to use bilingual dictionar-\nies for word-level mappings and predefined rules to handle grammatical transformations. Early\nRBMT systems, such as the Georgetown-IBM experiment (1954)[18], demonstrated the feasibility\nof MT by translating a set of Russian sentences into English, albeit under highly constrained con-\nditions. RBMT methods followed one of three primary strategies: direct [17], transfer-based[19], or\ninterlingua-based translation[20]. Direct translation systems performed word-by-word substitution\nand were typically limited to specific language pairs with similar syntactic structures. Transfer-\nbased systems introduced an intermediary step, where the source language was first converted into\nan abstract syntactic representation before being mapped to the target language. Interlingua-based\nsystems took this step further, converting the source language into a universal, language-agnostic\nrepresentation, enabling translation to multiple target languages from the same source structure."}, {"title": "Statistical Machine Translation", "content": "The emergence of Statistical Machine Translation (SMT)[21] in the late 1980s and early 1990s marked\na paradigm shift in MT research. Unlike RBMT, SMT systems relied on data-driven approaches,\nusing large bilingual corpora to learn translation patterns automatically. The foundation of SMT\nwas laid by researchers at IBM with the introduction of probabilistic models[22], such as the IBM\nModels 1-5[23], which focused on word alignment and translation probability estimation. These\nmodels demonstrated that translation could be framed as a statistical optimization problem, where\nthe goal was to maximize the probability of a target sentence given a source sentence. Phrase-based"}, {"title": "Neural Machine Translation", "content": "The advent of deep learning in the 2010s revolutionized MT with the introduction of Neural Machine\nTranslation (NMT) [26]. Unlike SMT, which relied on multiple components for different aspects of\ntranslation, NMT adopted an end-to-end approach, enabling models to learn the entire translation\nprocess from input to output. Early NMT systems were based on the encoder-decoder architecture\nusing recurrent neural networks (RNNs)[27]. The encoder transformed the input sentence into a\nfixed-length vector representation, which the decoder then used to generate the target sentence.\nThis method demonstrated significant improvements in translation fluency and contextual under-\nstanding. The introduction of the attention mechanism by Bahdanau [28] addressed one of the key\nlimitations of traditional encoder-decoder models: their inability to handle long sentences effectively.\nAttention mechanisms allowed the decoder to focus on relevant parts of the input sequence at each\nstep of the translation, resulting in more accurate and coherent outputs. This innovation laid the\ngroundwork for further advancements in NMT. The development of the Transformer architecture by\nVaswani [29] marked another breakthrough in NMT. By replacing RNNs with self-attention mech-\nanisms, Transformers enabled parallel processing of input sequences, significantly improving both\ntranslation quality and computational efficiency. Transformer-based models, such as OpenNMT[30],\nMarian[31], and the NMT systems used in Google Translate[32], became the new standard for MT,\nachieving near-human-level performance for many high-resource language pairs. Despite their suc-\ncess, NMT systems faced challenges similar to SMT in low-resource settings. The effectiveness of\nNMT depended heavily on large-scale parallel corpora, and its performance degraded significantly for"}, {"title": "Quechua", "content": "Quechua, often referred to as Runa Simi (the \"language of the people\"), is one of the most significant\nindigenous languages of the Americas [33, 34]. Once the lingua franca of the mighty Inca Empire,\nQuechua has endured centuries of colonial disruption and marginalization, yet it remains a vital\nthread in the cultural and linguistic fabric of South America. Today, millions of speakers across\nseveral Andean nations continue to use Quechua in daily life, preserving its rich heritage and ensuring\nits survival into the 21st century.\nThe linguistic structure of Quechua provides a fascinating glimpse into its complexity and adapt-\nability. Its phonological, morphological, syntactic, semantic, and pragmatic features[35, 36] reveal\nthe language's uniqueness and its rich potential for nuanced expression. Quechua's phonological\nsystem is relatively simple, featuring three vowel sounds (*/a/, */i/, /u/) and a modest consonant\ninventory. In some dialects, vowels may shift to harmonize with neighboring sounds, a phenomenon\nknown as vowel harmony. Quechua's consonant inventory includes stops, fricatives, nasals, and\nlaterals. A key feature of the language is the use of aspirated and glottalized stops, especially in\nSouthern Quechua dialects, distinguishing it from other Andean languages. The phonemic inventory\nincludes:\n\u2022 Stops: /p/, /t/, /k/, and their aspirated and glottalized counterparts.\n\u2022 Fricatives: /s/, /h/.\n\u2022 Nasals: /m/, /n/, /\u014b/ (palatal nasal).\n\u2022 Laterals: /1/.\nQuechua syllables generally follow a CV (consonant-vowel) pattern, making the language syllable-\ntimed and rhythmic in its spoken form. Clusters of consonants are rare, which simplifies pronun-\nciation. Quechua is an agglutinative language, meaning it constructs words by stringing together\nmorphemes, each carrying a specific grammatical or semantic function. This highly systematic mor-\nphology is one of the defining characteristics of the language. Most Quechua words consist of a root\nfollowed by one or more suffixes. Suffixes can convey a wide array of meanings, including:"}, {"title": "Large Language Models", "content": "The advent of large language models (LLMs), such as GPT-3[37], GPT-4[38], and LLaMA[39],\nhas revolutionized natural language processing (NLP) by enabling context-aware and coherent text\ngeneration across diverse tasks. These models, trained on massive corpora encompassing multiple\nlanguages, have shown remarkable potential in addressing the challenges of low-resource language\ntranslation. Unlike traditional machine translation approaches that rely heavily on parallel corpora,\nLLMs can leverage in-context learning, zero-shot, and few-shot capabilities to produce translations\nwith minimal training data. This makes them particularly well-suited for languages with limited\nresources, such as Quechua, where large-scale parallel corpora are often unavailable.\nBuilding on these advances, innovative methods like retrieval-augmented generation (RAG)[40]\nand prompt engineering[41] have emerged as powerful tools to enhance LLM performance for low-\nresource language translation. By integrating linguistic resources directly into the translation\npipeline, these methods enable LLMs to effectively utilize dictionaries, grammar guides, and other\nlinguistic tools to produce accurate and contextually appropriate translations. In the context of\nQuechua-English translation, such techniques are poised to address longstanding challenges by com-\nbining the generative power of LLMs with retrieval-based mechanisms and linguistic insights.\nIn fine-tuning low-resource language translation systems for the Quechua language, similar data\naugmentation techniques as those employed for the Manchu-Korean translation in the Mergen model\ncan be applied [42]. In the context of Quechua-English translation, the QueEn project developed\na Quechua-English parallel dataset to aid machine translation research between these two lan-\nguages [43]. However, given the extremely low-resource nature of Quechua, with only 14,000 sentence\npairs available, leveraging data augmentation through synonym generation and back-translation, as\ndone in the Manchu-Korean project, could significantly enhance performance. By employing meth-\nods such as training multiple versions of GloVe embeddings [44] or integrating multilingual pre-trained\nmodels like BERT [45], researchers can expand the dataset size and improve translation quality. Data\naugmentation, combined with transfer learning and semi-supervised methods, could similarly boost\nthe performance of Quechua-English translation systems, as shown by the use of back-translation\nand monolingual data to improve machine translation models in the QueEn project. Incorporating"}, {"title": "Methods", "content": "In this work, we introduce a robust framework, shown in Figure 1, for translating Quechua into\nEnglish using a large language model (LLM) enhanced with fine-tuning, retrieval-augmented gen-\neration (RAG), and data augmentation techniques. Fine-tuning is used to adapt the pre-trained\nLLM to the specific characteristics of Quechua, a low-resource language, while minimizing resource\nconstraints through parameter-efficient strategies. Retrieval-augmented generation further bolsters\ntranslation quality by incorporating external linguistic resources into the generation process. Addi-\ntionally, data augmentation is applied to expand the dataset and address data scarcity challenges,\nthereby improving model robustness and generalization. Each of these components is elaborated\nupon in the subsequent subsections."}, {"title": "Fine-tuning", "content": "Fine-tuning is a key method for adapting pre-trained LLMs to specific tasks or domains. After\nan LLM has been pre-trained on massive, general-purpose datasets, fine-tuning allows the model"}, {"title": "Retrieval-augmented Generation(RAG)", "content": "We leverage Retrieval-Augmented Generation (RAG) to enhance the translation of Quechua into\nEnglish. Figure 1 demonstrates the complete framework. The workflow begins by converting key\nresource documents, such as a Quechua-to-English dictionary and Quechua grammar guide, into\nstructured databases. These databases are optimized for efficient information retrieval through dual\nmechanisms: keyword-based indexing and embedding-based vector indexing. During translation,\nthe system retrieves relevant linguistic and grammatical information from these databases to aug-\nment the LLM with contextually relevant data, enabling the generation of accurate translations.\nSpecifically, for a user query q, the system retrieves supporting information Dq, which is then used\nto construct a prompt P for the LLM. The model generates a response R, synthesizing the retrieved\ncontext to improve translation quality. The process can be represented as:\n\nHere, q represents the input query, Dq is the set of retrieved documents or entries, Pis the\nconstructed prompt incorporating Dq, and R is the final translation output.\nOur approach integrates a dual retrieval mechanism to optimize the translation process. First, a\nkeyword-based index allows efficient lookups by identifying exact matches between query terms"}, {"title": "Experiments", "content": "To test the performance of our translation model, this work utilizes a subset of the Siminchik cor-\npus [50], provided as part of the IWSLT2023 Low-resource Speech Translation Track[43]. The dataset,\nreferred to as que_spa_clean, comprises 1 hour and 40 minutes of clean speech in Quechua paired\nwith Spanish translations. It includes raw transcriptions for Quechua and Spanish and true-cased\nSpanish translations, processed using a sacremoses Truecaser model trained on the WMT13 EN-ES\ndataset. The dataset is divided into validation and test sets, ensuring robust model evaluation."}, {"title": "Data", "content": "Since the dataset provides translations from Spanish to Quechua, an additional translation step\nfrom Spanish to English is required. e applied ChatGPT-4 for Spanish-to-English translation, lever-\naging its advanced natural language processing capabilities. ChatGPT-4 has demonstrated high\naccuracy and contextual understanding in various linguistic tasks, making it a reliable tool for this\nintermediary translation step. To enhance the data and improve evaluation reliability, we generated\nmultiple English translations for each Spanish sentence. These additional references help account for\nlinguistic variability and ensure a more comprehensive evaluation of the English-to-Quechua trans-\nlation performance. This approach enables us to evaluate English-to-Quechua translation scenarios\neffectively while maintaining high fidelity in the intermediate Spanish-English translation, ensuring\nthe robustness and validity of our experiments."}, {"title": "Evaluation Metrics", "content": "To assess the performance and capabilities of our model in low-resource machine translation, we\nconducted experiments comparing it with GPT-40 and LLaMA 3.1 405B models. The objective\nwas to evaluate the translation quality of these models, particularly for Quechua translations from\nEnglish source sentences, using a robust evaluation framework. The experiment involved randomly\nselecting text from test datasets, where sentences were input sequentially into each model to generate\ntranslations in the target low-resource language. A combination of automated and human evaluation\nmetrics was employed to ensure a comprehensive analysis of translation quality in terms of both\nlexical accuracy and semantic fidelity.\nWe used ROUGE [51] (Recall-Oriented Understudy for Gisting Evaluation) and BLEU [52]\n(Bilingual Evaluation Understudy) as core evaluation metrics. ROUGE measures the similarity be-\ntween machine-generated translations and reference translations by analyzing overlapping n-grams,\nword sequences, and word pairs. Although commonly used in summarization tasks, ROUGE also\nprovides valuable insights into translation fluency and accuracy. BLEU calculates n-gram precision\nbetween the generated and reference translations, offering a quantitative measure of how faithfully\nthe output matches the reference. These metrics are particularly effective for surface-level evaluation,\nsuch as determining the overlap of words and phrases.\nTo capture deeper semantic similarities, we incorporated BERTScore [53], which leverages\ncontextual embeddings from pretrained models to evaluate the semantic alignment between gener-\nated and reference translations. Unlike ROUGE and BLEU, which focus on exact lexical matches,\nBERTScore assesses meaning and context, making it especially suitable for low-resource languages\nwhere direct lexical matches may be limited."}, {"title": "Experiment Results", "content": "Both LLaMA and GPT exhibit low performance in traditional machine translation metrics such as\nBLEU and ROUGE, as shown in Table 1. LLaMA achieves the lowest BLEU score and ROUGE,\nwhile plain GPT performs slightly better. These results indicate a significant lack of overlap be-\ntween the generated translations and reference texts in terms of both n-gram precision (BLEU)\nand sentence-level structure (ROUGE). The scores reflect the inherent challenges of translating low-"}, {"title": "Discussion", "content": "Our work on QueEn demonstrates the potential of combining RAG with parameter-efficient fine-\ntuning for low-resource language translation. While our results show significant improvements over\nbaseline models, several promising directions remain for future research and development.\nFirst, the current implementation could be enhanced through the incorporation of more diverse\nlinguistic resources. Future work should explore the integration of audio data and speech recognition\ncapabilities, as Quechua has a strong oral tradition. This multimodal approach could help capture\nnuances of pronunciation and intonation that are crucial for proper understanding and translation.\nAdditionally, the development of specialized embedding models for Quechua could improve the\naccuracy of our retrieval system.\nSecond, future work could focus on enhancing the model's contextual understanding and handling\nof complex grammatical structures. Given Quechua's agglutinative nature and rich morphological\nsystem [35], developing specialized attention mechanisms or embedding layers that can better capture\nthese linguistic features would be valuable. This could involve creating morpheme-aware tokenization\nstrategies or incorporating explicit grammatical rule embeddings into the model architecture. Such"}, {"title": "Conclusion", "content": "In this paper, we introduced QueEn, a novel approach for Quechua-English translation that com-\nbines retrieval-augmented generation with parameter-efficient fine-tuning. Our experimental results\ndemonstrate that this combination substantially improves translation quality compared to baseline\nmodels.\nThe implications of this work extend far beyond technical achievements. Effective translation\ntools for indigenous languages like Quechua can help bridge significant socioeconomic gaps in mul-\ntilingual societies. By enabling better communication between Quechua-speaking communities and\nthe wider world, such technologies can improve access to education, healthcare, and economic oppor-\ntunities. This is particularly crucial in regions where language barriers have historically contributed\nto social and economic marginalization. Furthermore, our work contributes to the preservation of\nindigenous knowledge systems and cultural heritage, which are invaluable not only for the communi-\nties themselves but for humanity's collective understanding of diverse ways of knowing and being. As\nmachine learning continues to advance, approaches like QueEn demonstrate how modern language\ntechnologies can promote linguistic diversity while fostering more inclusive and equitable societies."}]}