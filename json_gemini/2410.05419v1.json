{"title": "REFINING COUNTERFACTUAL EXPLANATIONS WITH JOINT-DISTRIBUTION-INFORMED SHAPLEY TOWARDS ACTIONABLE MINIMALITY", "authors": ["Lei You", "Lele Cao", "Yijun Bian"], "abstract": "Counterfactual explanations (CE) identify data points that closely resemble the observed data but produce different machine learning (ML) model outputs, offering critical insights into model decisions. Despite the diverse scenarios, goals and tasks to which they are tailored, existing CE methods often lack actionable efficiency because of unnecessary feature changes included within the explanations that are presented to users and stakeholders. We address this problem by proposing a method that minimizes the required feature changes while maintaining the validity of CE, without imposing restrictions on models or CE algorithms, whether instance- or group-based. The key innovation lies in computing a joint distribution between observed and counterfactual data and leveraging it to inform Shapley values for feature attributions (FA). We demonstrate that optimal transport (OT) effectively derives this distribution, especially when the alignment between observed and counterfactual data is unclear in used CE methods. Additionally, a counterintuitive finding is uncovered: it may be misleading to rely on an exact alignment defined by the CE generation mechanism in conducting FA. Our proposed method is validated on extensive experiments across multiple datasets, showcasing its effectiveness in refining CE towards greater actionable efficiency.", "sections": [{"title": "BACKGROUND", "content": "Explainable Artificial Intelligence (XAI) is essential for making artificial intelligence systems transparent and trustworthy (Arrieta et al., 2020; Das & Rad, 2020). Within this area, feature attributions (FA) methods, such as Shapley values (Sundararajan & Najmi, 2020; Lundberg & Lee, 2017), determine how much each input feature contributes to a machine learning (ML) model's output. This helps simplify complex models by highlighting the most influential features. For example, in a healthcare model, Shapley values can identify key factors like age and medical history, assisting clinicians in understanding the model's decisions (Ter-Minassian et al., 2023; Nohara et al., 2022). Another technique counterfactual explanations (CE) (Wachter et al., 2017; Guidotti, 2022) show how small changes in input features can lead to different outcomes. While hundreds of CE algorithms have been proposed (Guidotti, 2022; Verma et al., 2020) to date, it is hardly practical to find one single CE algorithm that suits for all user cases, due to each of them is tailored particularly for their own different scenarios, goals, and tasks. For instance, the objective in one CE algorithm can be defined as finding a single counterfactual instance for each factual instance sometimes, while at othertimes, it could be treating multiple instances as a group and seeking one or more/multiple counterfactual instances for each factual observation. In some cases, the focus of a CE algorithm could be on the entire dataset, aiming to identify global CE that indicate the direction to move the factual instances to achieve the desired model output (Rawal & Lakkaraju, 2020; Ley et al., 2022; 2023; Carrizosa et al., 2024). Yet in other scenarios, the group of factual instances is viewed as a distribution, aiming to find a counterfactual distribution that remains similar in shape to the factual distribution (You et al., 2024), and ensuring comparable costs. Besides, some CE algorithms assume differentiable models, whereas others are designed specifically for tree-based or ensemble models."}, {"title": "Problem Description and Challenges", "content": "Although both FA and CE are vital for making AI models more interpretable and accountable, relying only on one of them will cause drawbacks. That is, FA alone may not provide actionable steps as explanations, and CE alone might include unnecessary feature changes that are not practical. Therefore, we address a general and comprehensive problem in this paper, which builds on the extensive foundations established in the literature (see Appendix A). To be specific, we seek to answer the following question:\n\nGiven a (group of) factual instance(s), how can we devise an action plan that requires the least feature modifications to achieve a desired counterfactual outcome?\n\nThree major challenges remain in addressing this problem. First, it is unrealistic to expect a single CE algorithm to meet all the needs universally, as the problem is often task-specific. Second, the approach should not rely on strong assumptions about the model (for example, requiring differentiability or special structures) to ensure its applicability across a wide range of models. Third, FA like feature importance can be misleading due to the lack of coherence between the FA scores and the changes for counterfactual effect. In other words, it is not effective to decouple FA with CE in order to select the most important features to change. We will demonstrate later that this decoupling can result in counterproductive feature modifications (also referred to as actions from users/stakeholders), as the features deemed important generally may not align with the specific pathways to achieve the desired counterfactual outcomes."}, {"title": "Main Contributions", "content": "Our main contributions are listed as follows.\n\n\u2022 A versatile algorithmic framework \u201cCOunterfactuals with Limited Actions (COLA)\u201d is proposed, which adapts to various CE methods and ML models. Extensive simulations show that the framework produces action plans that require significantly fewer feature changes to achieve outcomes similar (or sometimes equal) to those generated by various CE algorithms. Especially, COLA is shown to have near-optimal performance under certain circumstances.\n\n\u2022 A new Shapley method \u201cjoint-probability-informed Shapley (p-SHAP)\u201d is proposed, utilizing joint distribution of factual and counterfactual and resulting in remarkably well-performed action plans. We discover that other Shapley methods without incorporating counterfactual knowledge lead to unproductive attribution results for the aforementioned problem.\n\n\u2022 A counter-intuitive finding is identified, showing that associating each factual data instance with its explicitly generated counterfactual (i.e. an exact alignment is known) may still underperform our p-SHAP solution. This finding emphasizes the importance of the joint distribution of factual and counterfactual instances, as a proper alignment serves as crucial knowledge for accurate contrastive FA.\n\nTo our best knowledge, this is the first method proposed for systematically addressing the problem in Figure 1 without specifying certain CE algorithms and models."}, {"title": "PROBLEM FORMULATION", "content": "We formally formulate the problem described in Figure 1. Denote $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ as a black-box ML model. Denote by $x$ any observed (factual) data with $n$ rows and $d$ columns ($x \\in \\mathbb{R}^{n\\times d}, n \\geq 1$, and $d > 1$). Let $y^*$ be the target model output ($y^* \\in \\mathbb{R}^{m}, m > 1$). The optimization is to look for a (group of) counterfactual data instance(s) $z$ ($z \\in \\mathbb{R}^{n\\times d}$) subject to a maximum number of allowed feature changes, $C$, to achieve model output(s) as close as possible to $y^*$. Let $D$ denote a divergence function that measures the dissimilarity between two entities. The problem is formulated in equation 1.\n\n$\\begin{aligned}\n\\min_{C,Z} &\\quad D (f(z), y^*) \\\\\n\\text{s.t.} &\\quad D (z, x) \\leq \\epsilon \\\\\n&\\quad \\sum_{i=1}^{n}\\sum_{k=1}^{d} c_{ik} \\leq C \\\\\n&\\quad z_{ik} \\leq x_{ik}(1 - c_{ik}) + Mc_{ik} \\quad i = 1, ... n, k = 1, ... d \\\\\n&\\quad z_{ik} \\geq x_{ik}(1 - c_{ik}) - Mc_{ik} \\quad i = 1, ... n, k = 1, ... d\n\\end{aligned}$\n\nThe objective equation 1a and the constraint equation 1b formulate the typical CE optimization. Namely, $z$ is expected to make $f(z)$ close to $y^*$ yet stays close to $x$. Then $z$ can be used as a counterpart reference to explain why $f(x)$ does not achieve $y^*$. We do not limit the function $D$ to any specific type of divergence function, allowing it to stay general. Example functions of $D$ can be Euclidean distance, optimal transport (OT), maximum mean discrepancy (MMD), or differences of the model outcome in mean or median.\n\nThen, equation 1a and equation 1b together with equation 1c-equation 1e compose the CE optimization constrained by actions. On top of the counterfactual data $z$, we also optimize an indicator variable $c$, such that $z_{ik}$ is not allowed to change iff $c_{ik} = 0$. Maximum $C$ changes are allowed as imposed by equation 1c. Inspecting equation 1d and equation 1e, if $c_{ik} = 0$, $x_{ik}$ equals $z_{ik}$ and no changes happen at $(i, k)$. Otherwise, if $c_{ik} = 1$, remark that $M$ is a sufficiently large constant such that $z_{ik}$ has good freedom to change.\n\nThe problem is computationally difficult even when $d$ 1 for linear models, see Appendix B."}, {"title": "PRELIMINARIES ON SHAPLEY VALUE", "content": "This section introduces commonly used Shapley value methods for FA, which, together with our later proposed one, are integrated into our algorithmic framework COLA. We first introduce the concept of Shapley value in game theory, followed by a discussion on various commonly used Shapley methods for FA. Readers could find in (Sundararajan & Najmi, 2020) for a comprehensive overview of different Shapley value methods.\n\nIn cooperative game theory, a coalitional game is characterized by a finite set of players (in our context, features), denoted by $F = \\{1, 2, ... d\\}$, and a characteristic function $v$. This function $v : 2^F \\rightarrow \\mathbb{R}$ maps each subset $S \\subseteq F$ to a real number $v(S)$, representing the total payoff that can be achieved by the members of $S$ through cooperation, with the condition $v(\\emptyset) = 0$. The Shapley value, as defined based on $v$ later, is a fundamental concept in this framework, providing an equitable method to distribute the overall payoff, $v(F)$, of the grand coalition among the individual players."}, {"title": "THE PROPOSED p-SHAP AND ITS THEORETICAL ASPECTS", "content": "(Proposed) p-SHAP We generalize equation 4-equation 6 by integrating an algorithm $A_{prob}$ that returns their joint probability. Our p-SHAP is defined as follows.\n\n$\\begin{aligned}\nv^{(i)}(S) &= \\mathbb{E}_{r \\sim p(r/x_i)} [f(x_{i,S}; r_{F\\setminus S})] - \\mathbb{E}_{r \\sim p(r)} [f(r)] \\\\\n\\text{s.t.} &\\quad p = A_{prob} (x, r)\n\\end{aligned}$\n\nThe reason why p-SHAP is a generalization of the others is as follows: First, p-SHAP degrades to RB-SHAP in equation 4 when $A_{prob}$ defines an $i \\leftrightarrow j$ alignment of for any $x_i, r_j$. Second, p-SHAP degrades to RB-SHAP in equation 5 when $A_{prob}$ is defined to be independent of CE but associates with an arbitrary distribution $D$. Third, p-SHAP degrades to CF-SHAP in equation 6 when $A_{prob}$ is built upon a known distribution of CE, i.e. the data generation probability $r \\sim D_{CF}(x_i) (d_i)$ is used for $A_{prob}$.\n\nInterestingly, we emphasize that $A_{prob}$, in order to lead to a good FA performance, does not necessarily require knowledge of the CE algorithm being used for generating $r$, as demonstrated later by our numerical results. In fact, we never expect that $A_{prob}$ yields a probability $p$ that is identical to the true (or empirical) data generation probability $r \\sim D(x)$. Contrary to common expectations, we demonstrate this by showing that OT can be even more effective for this task compared to using instead a completely known alignment defined by the data generation from $x$ to $r$ in CE.\n\nEspecially, one of the focus in this paper is to consider the OT problem (also the 2-Wasserstein divergence) defined below. And the transportation plan $p^{OT}$ obtained by solving OT is used as the joint distribution of $x$ and $r$ in p-SHAP.\n\n$p^{OT} = \\arg \\min_{P\\in\\Pi(\\mu,\\nu)} \\sum_{i=1}^{n}\\sum_{j=1}^{m} P_{ij} ||x_i - r_j||^2 + \\epsilon \\sum_{i=1}^{n}\\sum_{j=1}^{m} P_{ij} \\log (\\frac{P_{ij}}{\\mu_i\\nu_j})$\n\nNote that $\\mu$ and $\\nu$ represent the marginal distributions of $x$ and $r$ respectively, and $\\Pi(\\mu, \\nu)$ the set of joint distributions (i.e. all possible transport plans). The term $\\epsilon \\sum_{i=1}^{n}\\sum_{j=1}^{m} P_{ij} \\log(\\frac{P_{ij}}{\\mu_i\\nu_j})$ is the entropic regularization with $\\epsilon \\geq 0$ being the coefficient. Such regularization ($\\epsilon > 0$) helps accelerate the computation of OT.\n\nTheoretical Aspects of p-SHAP Incorporating OT into p-SHAP plays a pivotal role in making p-SHAP suitable for the CE refinement optimization problem in equation 1. OT minimizes the total feature modification cost (i.e. modifying $x$ towards $r$) under its obtained alignment between factual $x$ and counterfactual $r$. Intuitively, this directly corresponds to our objective in equation 1a of finding feature modifications that achieve the counterfactual outcomes at minimal cost.\n\nWe can further strengthen this connection theoretically under the Lipschitz continuity assumption of the predictive model $f$. In Theorem 4.1 below (proof in Appendix C), we establish that the transportation plan $p^{OT}$ used in p-SHAP is effective in minimizing an upper bound on the divergence between $f(x)$ and $y^*$. Specifically, the 1-Wasserstein distance between $f(x)$ and $y^*$, is bounded by the Lipschitz constant (assuming Lipschitz continuity of $f$) multiplied by the square root of the minimized expected cost of changing $x$ towards $r$, i.e. $\\sum_{i,j} P_{ij} ||x_i \u2013 r_j||^2$ where $p_{ij}$ ($j = 1, 2, ..., m$) quantify how the feature values of $x_i$ should be adjusted towards those of one or multiple $r_j$. Practically, this means that in p-SHAP, the OT plan $p^{OT}$ provides a strategy to adjust the feature values of $x$ towards those of $r$ in a way that minimizes the expected modification cost $\\sum_{i,j} P_{ij} ||x_i \u2013 r_j||^2$. Compared to other modification plans ($p \\in \\Pi$), $p^{OT}$ yields the minimal possible cost, which in turn provides the tightest upper bound on the violation of the counterfactual effect $W_1(f(x), y^*)$ in proportion to this cost.\n\nTheorem 4.1 (p-SHAP Towards Counterfactual Effect). Consider the 1-Wasserstein divergence $W_1$, i.e. $W_1(f(x), y^*) = \\min_{\\pi \\in \\Pi} \\sum_{i=1}^{n}\\sum_{j=1}^{m} \\pi_{ij} |f(x_i) \u2013 y_j^*|$. Suppose the counterfactual outcome $y^*$ is fully achieved by $r$, i.e. $y_j^* = f(r_j)$ ($j = 1, 2 . . . m$). Assume that the model $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is Lipschitz continuous with Lipschitz, constant $L$. The expected absolute difference in model outputs between the factual and counterfactual instances, weighted by $p^{OT}$ (with $\\epsilon = 0$), is bounded by:\n\n$W_1(f(x), y^*) \\leq L \\cdot \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{m} P_{ij}^{OT}||x_i - r_j ||^2} \\leq L \\cdot \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{m} P_{ij}||x_i - r_j ||^2} \\quad \\forall p \\in \\Pi$.\n\nNamely, $p^{OT}$ minimizes the upper bound of $W_1(f(x), y^*)$, where the upper bound is based on the expected feature modification cost.\n\nIn addition, p-SHAP is conceptually correct in attributing the causal behavior to the modifications of the characteristics, stated in Theorem 4.2 below (proof in Appendix D).\n\nTheorem 4.2 (Interventional Effect of p-SHAP). For any subset $S \\subset F$ and any $x_i$ ($i = 1, 2, ..., n$), $v^{(i)}(S)$ represents the causal effect of the difference between the expected value of $f(r)$ under the intervention on features S and the unconditional expected value of $f(r)$. Mathematically, this is expressed as:\n\n$\\mathbb{E}[f(r)] + v^{(i)}(S) = \\mathbb{E} [f(r)|do (r_S = x_{i,S})] $."}, {"title": "THE ALGORITHMIC FRAMEWORK COLA", "content": "Sketch The algorithmic framework COLA, stated in Algorithm 1 below, aims to solve equation 1 and is established on four categories of algorithms. First, we resort to a CE algorithm $A_{CE}$ to solve the problem defined by equation 1a and equation 1b, yielding counterfactual data $r$ with $y^* = f(r)$ and $D(r, x) \\leq \\epsilon$. Second, we seek a joint distribution of $x$ and $r$ by an algorithm $A_{prob}$. Third, we perform FA for $x$ using a Shapley algorithm $A_{shap}$, taking into account the joint probability. The attributions contain the information that how large influence a row-column position would cause, if a value change happens there. Fourth, we revise $x$ in $C$ positions according to the attributions, and set their values to be the ones obtained by an algorithm $A_{value}$. This algorithm tries to ensure that the refined counterfactual $z$ stay not farther away from $x$ than $r$. We explain Algorithm 1 in details below, along with an illustration in Figure 2.\n\nAlgorithm 1 COunterfactuals with Limited Actions (COLA)\n\nRequire: Model $f$, factual $x \\in \\mathbb{R}^{n\\times d}$, target $y^* \\in \\mathbb{R}^{m}$, $\\epsilon$, and $C$\n\nEnsure: Action plan $c \\in \\mathbb{R}^{n\\times d}$ and correspondingly a refined counterfactual $z \\in \\mathbb{R}^{n\\times d}$\n\n1: Use $A_{ce}(f, x, y^*, \\epsilon)$ to obtain $r \\in \\mathbb{R}^{m\\times d}$, with $y^* = f(r)$ and $D(r, x) \\leq \\epsilon$.\n\n2: Use $A_{prob} (x, r)$ to obtain the joint distribution matrix $p \\in \\mathbb{R}^{n\\times m}$.\n\n3: Use $A_{shap} (x, r, p)$ to obtain the shapley values $\\phi \\in \\mathbb{R}^{n\\times d}$ for each element of $x$.\n\n4: Normalize the element-wise absolute values of $\\phi$, i.e., $\\varsigma_{ik} \\leftarrow |\\Phi_{ik}|/||\\Phi||_1$ ($\\varsigma \\in \\mathbb{R}^{n\\times d}$). \n\n5: Use $A_{value} (r, p)$ to obtain matrix $q \\in \\mathbb{R}^{n\\times d}$.\n\n6: For $c\\in \\{0, 1\\}^{n\\times d}$, $c_{ik} \\leftarrow 0$ ($i = 1 ... n$, $k = 1, ... d$).\n\n7: Sample C pairs $(i, k)$ according to the probability matrix $\\varsigma$, and let $c_{ik} = 1$ for them.\n\n8: Let $z \\leftarrow x$ ($z\\in \\mathbb{R}^{n\\times d}$).\n\n9: for $i \\leftarrow 1$ to $n$ do\n\n10: for $k \\leftarrow 1$ to $d$ do\n\n11: if $c_{ik} = 1$ then\n\n12: $z_{ik} \\leftarrow q_{ik}$\n\n13: end if\n\n14: end for\n\n15: end for\n\n16: return $c$ and $z$\n\nLine 1 (Applying a CE algorithm to find a counterfactual $r$). The CE algorithm $A_{ce}$ takes the model $f$, the factual $x$, the target outcome $y^*$, and the tolerance $\\epsilon$ as input. The algorithm returns a counterfactual $r$ staying close with $x$, with $y^* = f(r)$ and $D(r, x) \\leq \\epsilon$.\n\nLine 2 (Seeking a joint distribution of $x$ and $r$). We use an algorithm $A_{prob}$ for this task, which takes $x$ and $r$ as input, and outputs a matrix representing the joint distribution of all $n$ and $m$ data points in $x$ and $r$, respectively. The joint distribution $p$ represents an alignment relationship (or matching) between the factual rows and counterfactual rows, and we use it in Line 5 to compute the values that can be used for composing $z$ later on. As discussed in Section 3, $A_{prob}$ can be based on OT to compute a joint distribution that yields the smallest OT distance between $x$ and $r$, if the alignment relationship between their rows are unknown. Otherwise, it is recommended to select a joint distribution that accurately reflects the alignment between the rows in $x$ and $r$.\n\nLines 3\u20134 (p-SHAP FA). We apply equation 7 to compute the shapley value for $x$. The joint distribution $p$ can be used here (without being forced) to properly align each row of $x$ with its corresponding counterfactual rows of $r$, such that the selected rows in $r$ serve as the most representative contrastive reference for the row in $x$. Numerically, this alignment significantly influences our contrastive FA. Then, the shapley values (as a matrix) of $x$ is taken element-wisely with the absolute values and normalized (such that all values sum up to one). The resulted matrix forms our FA.\n\nLine 5 (Computing feature values). The algorithm $A_{value}$ is used for this task, which takes the counterfactual $r$ and the joint distribution $p$ as input. For any row $i$ in $x$, $A_{value}$ selects one or multiple row(s) in $r$, used as references for making changes in $x$. The algorithm returns a matrix $q \\in \\mathbb{R}^{n\\times d}$, where each element $q_{ik}$ serves as a counterfactual candidate for $x_{ik} (d_{i, k})$. Below, we introduce $A_{max_{e}}$ and $A_{avg}^{value}$, respectively for the cases of selecting single row and selecting multiple rows.\n\nFor any row $x_i$, $A_{max_{e}}$ selects the row of $r$ with the highest probability.\n\n$\\begin{equation}\nq^{valu}_i = A^{valu}_{max_{e}} (r, p) \\quad \\text{where } q_{ik} = r_{\\tau(i),k} \\text{ and } \\tau(i) = \\arg \\max_{j=1,2...m} P_{ij}.\n\\end{equation}$\n\nThe algorithm $A_{avg}^{value}$ computes $q_{ik}$ as a convex combination as a weighted average of $r_{1k}, r_{2k}, ... r_{mk}$.\n\n$\\begin{equation}\nq^{valu}_i = A^{valu}_{avg} (r, p) \\quad \\text{where } q_{ik} = \\sum_{j=1}^{m} (\\frac{P_{ij}}{\\sum_{j'=1}^{m} P_{ij'}}) r_{jk}.\n\\end{equation}$\n\nLines 6-16. Recall that the non-negative matrix $\\varsigma$ is normalized to have its summation being one. We could hence treat it as a policy to select the positions in x for value replacement (i.e. $c_{ik} = 1$), as what line 7 does. Then, for any $i$ and $k$ with $c_{ik} = 1$, $x_{ik}$ gets modified to $q_{ik}$, and the modified matrix is then returned as $z$ together with $c$ forming the optimized solutions of the problem in equation 1.\n\nThe theorem below (proof in Appendix E) demonstrates that the refined $z$, produced by the COLA framework, satisfies the constraint equation 1b in the typical scenario where $n = m$, using the Frobenius norm as the distance measure. Empirical evidence supporting the general applicability of this conclusion can be found in Table 3 when the numerical results are shown later on.\n\nTheorem 5.1 (Counterfactual Proximity). Let $n = m$ such that the Frobenius norm $|| \\cdot ||_F$ can be used to measure the differences between $z, r$, and $x$. Suppose that the OT plan $p^{OT}$ is obtained without the entropic regularization term (i.e., $\\epsilon = 0$), resulting in a deterministic matching represented by a permutation $\\sigma$ of $\\{1, 2, . . ., n\\}$. Then, the refined counterfactual $z$, constructed using the COLA framework, satisfies:\n\n$||z - x||_F \\leq ||r - x||_F,$\n\nindicating that $z$ is at least as close to $x$ as $r$ is, when $r$ is reordered according to $\\sigma$.\n\nComplexity of COLA Let $O(M_{CE})$ be the algorithm complexity of $A_{CE}$. For algorithm $A_{shap}$, consider using weighted linear regression to estimate Shapley values, and denote by $M_{shap}$ the number of sampled subsets. The complexity of COLA with respect to $n, m, d$, and the regularization parameter $\\epsilon$ of entropic OT is $O(M_{CE})+O(nmlog(1/\\epsilon))+O(ndM_{Shap})+N$ where $N = O(nm) + O(nd)$ if $A_{avg}^{value}$ is used and $N = O(nmd)$ if $A_{max_{e}}^{value}$ is used."}, {"title": "NUMERICAL RESULTS", "content": "This section evaluates the effectiveness of COLA in addressing the problem in equation 1, with $y^* = f(r)$ where $r$ is the counterfactual obtained from a CE method $A_{CE}$. We adopt four different divergence functions: OT evaluates the distance between entire distributions. MMD evaluates the divergence between the means of two distributions in a high-dimensional feature space. The absolute mean difference (MeanD) and absolute median difference (MedianD) evaluate the divergence between mean and median, respectively. The numerical results aim at showing: I) COLA's effectiveness for actionable minimality. II) p-SHAP's superior performance than other Shapley methods towards actionable minimality. III) COLA's near-optimal performance.\n\nExperiment Setup The experiments\u00b9 are conducted with 4 datasets for binary classification tasks, 5 CE algorithms that are designed for diverse goals, and 12 classifiers, shown in Table 1, where a combination of dataset, $A_{CE}$ algorithm, and a model defines an \u201cexperiment scenario\u201d."}, {"title": "CONCLUSIONS", "content": "This paper introduces a novel framework, COLA, for refining CE by joint-distribution-informed Shapley values, ensuring the refined CE maintains the counterfactual effect with fewer actions."}]}