{"title": "HUMAN-FEEDBACK EFFICIENT REINFORCEMENT LEARNING FOR ONLINE DIFFUSION MODEL FINETUNING", "authors": ["Ayano Hiranaka", "Shang-Fu Chen", "Chieh-Hsin Lai", "Dongjun Kim", "Naoki Murata", "Takashi Shibuya", "Wei-Hsiang Liao", "Shao-Hua Sun", "Yuki Mitsufuji"], "abstract": "Controllable generation through Stable Diffusion (SD) fine-tuning aims to improve fidelity, safety, and alignment with human guidance. Existing reinforcement learning from human feedback methods usually rely on predefined heuristic reward functions or pretrained reward models built on large-scale datasets, limiting their applicability to scenarios where collecting such data is costly or difficult. To effectively and efficiently utilize human feedback, we develop a framework, HERO, which leverages online human feedback collected on the fly during model learning. Specifically, HERO features two key mechanisms: (1) Feedback-Aligned Representation Learning, an online training method that captures human feedback and provides informative learning signals for fine-tuning, and (2) Feedback-Guided Image Generation, which involve generating images from SD's refined initialization samples, enabling faster convergence towards the evaluator's intent. We demonstrate that HERO is 4\u00d7 more efficient in online feedback for body part anomaly correction compared to the best existing method. Additionally, experiments show that HERO can effectively handle tasks like reasoning, counting, personalization, and reducing NSFW content with only 0.5K online feedback.", "sections": [{"title": "INTRODUCTION", "content": "Controllable text-to-image (T2I) generation focuses on aligning model outputs with user intent, such as producing realistic images, e.g., undistorted human bodies, or accurately reflecting the count, semantics, and attributes specified by users. To tackle this problem, a common paradigm involves fine-tuning latent diffusion models (DM) like Stable Diffusion (SD; Rombach et al., 2022) using supervised fine-tuning (SFT; Lee et al., 2023), which mostly learn from pre-collected, offline datasets. To further enhance the alignment, online reinforcement learning (RL) fine-tuning methods (Fan et al., 2024; Black et al., 2024) utilize online feedback that specifically evaluates the samples generated by the model during training. With such dynamic guidance provided on the fly, these methods demonstrate superior performance on various T2I tasks, such as aesthetic quality improvement. Yet, these approaches rely on either predefined heuristic reward functions or pretrained reward models learned from large-scale datasets, which could be challenging to obtain, especially for tasks involving personalized content generation (e.g., capturing cultural nuances) or concepts like specific colors or compositions.\nTo address the above issue, Yang et al. (2024b) introduces D3PO, an alternative method that directly leverages online human feedback for fine-tuning diffusion models. Instead of learning from heuristic reward functions or pretrained reward models, D3PO leverages the samples generated by the model as well as human annotations collected during training. With online human feedback, D3PO addresses various tasks, such as distorted human body correction and NSFW content prevention,"}, {"title": "PRELIMINARIES", "content": "Stable Diffusion (SD) operates in two stages. First, an autoencoder compresses images x from pixel space into latent representations z_0, which can later be decoded back to pixel space. Second, a diffusion model (DM) is trained to model the distribution of these latent representations conditioned on text c. The forward diffusion process is defined as $p(z_t | z_0) := \\mathcal{N}(z_t; \\alpha_t z_0, \\sigma_t^2 I)$, where $\\alpha_t$ and $\\sigma_t$ are pre-defined time dependent constants for $t \\in [0,T]$. Both the forward transition kernel"}, {"title": "PROBLEM SETUP AND THE PROPOSED METHOD", "content": "Given a user-specified text prompt, our goal is to fine-tune SD to generate images that align with the prompt by learning from human feedback guidance. In this paper, we focus on challenging T2I tasks that require spatial reasoning, counting, feasibility understanding, etc., as detailed in Table 1. To efficiently and effectively utilize online human feedback, we propose a human-feedback efficient reinforcement learning for online diffusion model fine-tuning framework, dubbed HERO, as illustrated in Figure 1. Feedback-Aligned Representation Learning (Figure 1\u2460) makes efficient use of limited human feedback by converting discrete feedback to informative, continuous reward signals. In addition, Feedback-Guided Image Generation (Figure 1\u2461) leverages human-preferred noise latents from previous iterations and encourages SD outputs to align more quickly with human intention, further improving sample efficiency."}, {"title": "ONLINE HUMAN FEEDBACK", "content": "In the first iteration of HERO, we generate synthetic images X from a batch of random noises ZT sampled from SD's prior distribution $\\pi_{\\text{HERO}}(z_T) := \\mathcal{N}(z_T; 0, I)$ using DDIM (Song et al., 2020a; Ho et al., 2020). For each $z_T \\in Z$, the sampling trajectories are denoted as $\\{z_T, z_{T-1},\\dots,z_0\\}$, and each $z_0$ is decoded to an image for human evaluation. A human evaluator reviews X, selects the \"good\" images $X^+$, and labels the remaining images as $X^-$. To obtain a gradation among all \"good\" images and all \"bad\" images by representation learning, we ask the evaluator to identify the \"best\" image in $X^+$, denoted as $x_{\\text{best}}$. The details of our feedback-aligned representation learning are discussed in the following section and we store the following for future use: the sets of images $X, X^+, X^-, x_{\\text{best}}$; their corresponding SD's clean latents $Z_0, Z_0^+, Z_0^-, z_{\\text{best}}$ from which they are decoded; and their initial noises (at time T) $Z_T, Z_T^+, Z_T^-, z_{\\text{best}}$ used in SD's sampling."}, {"title": "FEEDBACK-ALIGNED REPRESENTATION LEARNING", "content": "HERO fine-tunes SD with minimal online human feedback by learning representations via a contrastive objective that captures discrepancies between the best SD's clean latent $z_{\\text{best}}$, positive $Z^+$, and negative $Z^-$ SD's clean latents (Section 4.2.1). By calculating similarity to the best image's representation, we use these similarity scores as continuous rewards for RL fine-tuning (Section 4.2.2). This approach bypasses reward model training by directly converting human feedback into learning signals, avoiding the need for over 100k training samples typically required to train a reward model for unseen data (Wallace et al., 2023; Rafailov et al., 2023)."}, {"title": "LEARNING REPRESENTATIONS", "content": "To learn a representation space of $Z_0$ aligned with human feedback, we build on the contrastive learning framework of Chen et al. (2020). We design an embedding network $E_{\\theta}(\\cdot)$ to map $Z_0$ into the representation space, followed by a projection head $g_{\\phi}(\\cdot)$ for loss calculation. Triplet margin loss is applied to the projection head's output:\n$\\mathcal{L}(\\theta; z_{\\text{best}}^0, Z_0^+, Z_0^-) = \\mathbb{E}_{z_{\\text{good}} \\sim Z_0^+, z_{\\text{bad}} \\sim Z_0^-} \\max\\{\\mathcal{S}(g_{\\phi}(E_{\\theta}(z_{\\text{best}}^0)), g_{\\phi}(E_{\\theta}(z_{\\text{good}}^0)))) - \\mathcal{S}(g_{\\phi}(E_{\\theta}(z_{\\text{best}}^0)), g_{\\phi}(E_{\\theta}(z_{\\text{bad}}^0)))) + \\alpha, 0\\}.$      (3)\n$E_{\\theta}(z_{\\text{best}}^0)$ serves as the anchor in the contrastive loss, with $\\mathcal{S}(\\cdot, \\cdot)$ representing the similarity score (using cosine similarity) and $\\alpha$ as the triplet margin set to 0.5. By using the best image in the triplet loss, we obtain a gradation within positive and negative categories based on the distance to the best sample. With the learned representation $E_{\\theta}(z_0)$ for $z_0 \\in Z_0$, we can compute continuous rewards for RL fine-tuning."}, {"title": "SIMILARITY-BASED REWARDS COMPUTATION", "content": "After training the embedding $E_{\\theta}(\\cdot)$ on the current batch of human feedback, reward values are computed as the cosine similarity in the learned representation space between each $E_{\\theta}(z_0)$ for $z_0 \\in Z_0$ and $E_{\\theta}(z_{\\text{best}}^0)$:\n$R(z_0) = \\frac{E_{\\theta}(z_0) \\cdot E_{\\theta}(z_{\text{best}}^0)}{\\max\\{\\|E_{\\theta}(z_0)\\|^2 \\|E_{\\theta}(z_{\\text{best}}^0)\\|^2, \\delta\\}} \\quad \\text{for each } z_0 \\in Z_0,$         (4)\nwhere $\\delta = 1 \\times 10^{-8}$ to avoid zero division. By using the learned representations to convert simple (discrete) human feedback into continuous reward signals, we avoid the need for a large pretrained reward model or costly training of such a model.\nBesides the \"similarity-to-best\" design, we also consider a \"similarity-to-positives\" design, which uses the similarity between an image and the average of all \"good\" images in the learned representation space. We choose the \u201csimilarity-to-best\" design for its superior performance. Further discussion is available in Section 5.3.1."}, {"title": "DIFFUSION MODEL FINETUNING", "content": "DDPO fine-tunes SD by reweighting the likelihood with reward values. For a noise latent $z_T \\in Z_T$ and its sampling trajectory $\\{z_T, z_{T-1},\\dots, z_0\\}$, we incorporate the reward $R(z_0)$ from Eq. (4) into the DDPO update rule in Eq. (2) to fine-tune the SD model $\\phi$. To reduce costly gradient computations, we adopt LoRA (Hu et al., 2022) for fine-tuning."}, {"title": "FEEDBACK-GUIDED IMAGE GENERATION", "content": "After the previous iteration of fine-tuning, we propose feedback-guided image generation to facilitate the fine-tuning process by generating images that reflect human intentions. We sample the noise latents for a new batch of images from the Gaussian mixture with means centered around the human-selected \u201cgood\u201d $Z^+$ and \u201cbest\u201d $z_{\\text{best}}$ SD noise latents from the previous iteration, with"}, {"title": "EXPERIMENTAL RESULTS", "content": "We demonstrate HERO's performance on a variety of tasks, including hand deformation correction, content safety improvement, reasoning, and personalization. Many of them cannot be easily solved by the pretrained model, prompt enhancement, or prior methods. A full list of tasks and their success conditions are shown in Table 1. We adopt SD v1.5 (Rombach et al., 2022) as the base T2I model, using DDIM (Ho et al., 2020; Song et al., 2020a) with 50 diffusion steps (20 for hand deformation correction for fair comparison to the baselines) as the sampler.\nWe compare HERO to the following baselines:\nSD-pretrained prompts the pretrained SD model\nwith the original task prompt shown in Table 1.\nSD-enhanced prompts the pretrained SD model\nwith an enhanced version of the prompt generated\nby GPT-4 (Brown, 2020; Achiam et al., 2023).\nDreamBooth (DB; Ruiz et al., 2023) finetunes\ndiffusion models via supervised learning, taking\nimages as input. We use the four best images cho-\nsen by the human evaluators as model inputs.\nD3PO (Yang et al., 2024b) utilize online human\nfeedback for DPO (Rafailov et al., 2023)-based\ndiffusion model finetuning. Due to the high feed-\nback cost for training, this baseline is considered\nonly for the hand anomaly correction task directly\nadopted from their work. Success rates are re-\nported as presented in the original paper."}, {"title": "HAND DEFORMATION CORRECTION", "content": "Following the problem setup of D3PO (Yang et al., 2024b), we use the prompt \"1 hand\" for image generation and use human discretion to evaluate the normalcy of the generated hand images. Parameters such as sampling steps are set to be consistent with D3PO. In each epoch of HERO, feedback"}, {"title": "DEMONSTRATION ON THE VARIETY OF TASKS", "content": "We further demonstrate the effectivity of HERO on a variety of tasks involving reasoning, correction, feasibility and functionality quality enhancement, and personalization. Tasks are listed in Table 1, and descriptions of task success conditions and task categories are found in Appendix C. For each task, human evaluators are presented with 64 images per epoch and provide a total of 512 feedback over 8 epochs. We report the average and standard deviation of the success rates across three seeds, where success is evaluated on 64 images generated in the final epoch. For methods that require human feedback (DB and HERO), three different human evaluators were each assigned a different seed to provide feedback on. Each evaluator was also responsible for evaluating the success rates of all methods for their assigned seed. Results are shown in Table 2. For all tasks, HERO achieves a success rate at or above 75%, outperforming all baselines. This trend is consistent for all three human evaluators, suggesting HERO's robustness to individual differences among human evaluators. Sample images generated by SD-pretrained, DB, and HERO are shown in Figure 4 and more results can be found in Figures 9 to 12 in the appendix. While the baselines often struggle in attribute reasoning (e.g., color, count), spatial reasoning (e.g., inside), and feasibility (e.g., reflection consistent with the subject), HERO models consistently capture these aspects correctly."}, {"title": "ABLATIONS", "content": "This section presents ablation studies illustrating the roles of each component of HERO. In regards to Feedback-Aligned Representation Learning, we investigate the effects of (1) computation of rewards using learnable feedback-aligned representations and (2) \u201csimilarity-to-best\u201d design for reward computation. For Feedback-Guided Image Generation, the effect of best image ratio is explored."}, {"title": "EFFECT OF FEEDBACK-ALIGNED REPRESENTATION LEARNING AND REWARD DESIGN", "content": "The effects of using learned feedback-aligned representations\nand our reward design are investigated through three ablation\nexperiments. Firstly, we demonstrate the benefit of convert-\ning discrete human feedback into continuous reward signal\nby investigating HERO-binary, a variant of HERO using bi-\nnary rewards for training. Secondly, we explore the effect of\nlearned representations by replacing the learned representa-\ntions in HERO with SD image latents $Z_0^+$ (HERO-noEmbed).\nFinally, we explain our choice for the \u201csimilarity-to-best\u201d re-\nward design by discussing an alternative reward design using\nsimilarity to the average of all $Z_0^+$ and $z_{\\text{best}}$ (HERO-positives). For each setting, we test on the narcissus task with 512 feedback\nfor training and 200 images generated by the finetuned model for success rate evaluation. HERO\noutperforms all other settings, and results are summarized in Table 3.\nDirectly using human labels as binary rewards. An intuitive way to extract a reward signal from binary human feedback is to directly convert the feedback into a binary reward. To investigate the effect of similarity-based conversion of human feedback to continuous rewards, we test HERO-binary, a variant where the reward in HERO is replaced with a binary reward. Images labeled as \"good\" or \u201cbest\u201d receive a reward of 1.0, and all other images receive a reward of 0.0. HERO-binary only reaches 78% success rate while HERO reaches 91%. This may be because the continuous"}, {"title": "EFFECT OF BEST IMAGE RATIO IN FEEDBACK-GUIDED IMAGE GENERATION", "content": "To investigate the effect of the best image ratio, we compare the performance of the black-cat task for $\\beta = 0.0, 0.5, 1.0$. Further, we compare to the case where the images are sampled from random SD noise latents to demonstrate the benefit of using $Z^+$ and $z_{\\text{best}}$ as initial noises for image generation. Results are shown in Figure 5. Sampling all images from the $z_{\\text{best}}$ $(\\beta = 1.0)$ reaches an average of 70.8% success at the end of the training. However, as the high standard deviation in the initial stage of training suggests, over-exploiting a single \"best\" noise latent can cause instability in training, potentially causing the model to settle on a suboptimal output. Sampling uniformly from $Z^+$ and $z_{\\text{best}}$ ($\\beta = 0.0$) results in a similar success rate as $\\beta = 1.0$, but is less likely to converge to a suboptimal point. We empirically find that, for our tasks, $\\beta = 0.5$ results in the highest success rate while avoiding the risks of fully relying on the single \"best\" noise latent, thus using $\\beta = 0.5$ for our experiments. When images are sampled from random SD noise latents, the task success rate does not grow significantly slower in the given amount of feedback, demonstrating the benefit of using $Z^+$ and $z_{\\text{best}}$ for efficient fine-tuning."}, {"title": "TRANSFERABILITY", "content": "While HERO is trained to optimize for a single input prompt, we observe that some personal preferences and general concepts learned from one prompt can generalize to other related prompts in some cases."}, {"title": "CONCLUSION", "content": "This work introduces HERO, an RLHF framework for fine-tuning SD using online human feedback. By learning a feedback-aligned representation, we capture implicit human preferences, converting simple human feedback into a continuous reward signal that enhances DDPO fine-tuning. Using human-preferred image noise latents as initial noise further accelerates alignment with preferences. Combining these components, HERO achieves high efficiency in fine-tuning SD, requiring 4\u00d7 less feedback than the baseline. Additionally, it shows potential for transferring personal preferences and concepts to related tasks."}, {"title": "THEORETICAL EXPLANATIONS", "content": "In this section, we provide theoretical justifications for the validity of our proposed distribution \u03c0HERO in Eq. (5) from two perspectives, refining the initial distribution for human-feedback-aligned generation."}, {"title": "CONCENTRATION OF HUMAN-SELECTED NOISES IN SD'S PRIOR DISTRIBUTION", "content": "It is known that the initial distribution of SD sampling is typically the standard normal distribution $\\mathcal{N}(0, I_D)$, which yields a random vector that concentrates around the sphere of radius $\\sqrt{D}$ with high probability. In the following proposition, we show that a random vector drawn from our proposed distribution $\\pi_{\\text{HERO}}$ also concentrates around the sphere of radius $\\sqrt{D}$ with high probability, provided that the variance $\\epsilon_0 > 0$ of the Gaussian mixture is sufficiently small. This ensures that the sampling from the refined initial noise provided by $\\pi_{\\text{HERO}}$ remains consistent with the sampling from the original prior distribution of the SD model.\nProposition A.1 (Concentration of HERO). Let $\\pi$ be a Gaussian mixture with each component as $\\mathcal{N}(\\mu_i, \\epsilon_0^2 I_D)$, where each mean $\\mu_i \\sim \\mathcal{N}(0, I_D)$, and $\\epsilon_0 > 0$ is a small constant. Let $y \\sim \\pi$ be a random vector drawn from $\\pi$. Then, for any $\\delta > 0$, we have the following concentration if $\\epsilon_0$ is sufficiently small:\n$P\\left(\\sqrt{D}(1 - \\epsilon_0) \\leq \\|y\\| \\leq \\sqrt{D}(1 + \\epsilon_0)\\right) \\geq 1 - \\delta.$\nNamely, $y$ is concentrated around the shell of radius $\\sqrt{D}$ and thickness $\\epsilon_0 \\sqrt{D}.$\nProof. We will show that the overall probability mass is concentrated in a shell around radius $\\sqrt{D}$, which means that for a sample $y$ from the GMM $\\pi$, $\\|y\\| \\approx \\sqrt{D}$ with high probability.\nFrom the properties of high-dimensional Gaussians (Vershynin, 2018), we know that the norm of each mean $\\mu_i$ concentrates around $\\sqrt{D}$. Specifically, for any small $\\delta > 0$, we have the following"}, {"title": "INFORMATION LINK BETWEEN HUMAN-SELECTED NOISES AND SD'S LATENTS IN GENERATION", "content": "We consider the general form of the backward SDE for diffusion model sampling (Song et al., 2020b; Lai et al., 2023a;b):\n$dz_t = (f(t)z_t - g^2(t)\\nabla \\log p_t(z_t))dt + g(t)dw_t, \\quad z_T \\sim \\pi_{\\text{HERO}},$         (7)\nwhere $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is the drift scaling term, $g: \\mathbb{R} \\rightarrow \\mathbb{R}_{\\geq 0}$ is the diffusion term determined by the forward diffusion process, and $w_t$ represents the time-reversed Wiener process.\nIn the following proposition, we demonstrate that if $\\Delta t \\neq 0$, then the initial condition $z_T \\sim \\pi_{\\text{HERO}}$ and the solution $z_0$ obtained from a finite-step numerical solver will possess mutual information. This suggests that the information of either $z_0$ or $z_T$ is preserved during SDE solving with common forward designs, such as the variance-preserving SDE (Ho et al., 2020; Song et al., 2020b) in SD. Typical choices include the Ornstein-Uhlenbeck process $(f(t), g(t)) = (-1, \\nu \\sqrt{2})$, or $(f(t), g(t)) =\\left(-\\frac{\\beta(t)}{2}, \\sqrt{\\beta(t)}\\right)$, where $\\beta(t) := \\beta_{\\text{min}} + t(\\beta_{\\text{max}} - \\beta_{\\text{min}})$, with $\\beta_{\\text{min}} = 0.1$ and $\\beta_{\\text{max}} = 20$.\nWe consider discretized time using a uniform partition (Kim et al., 2024a; Hu, 1996; Kim et al., 2024b) $0 = t_n < t_{n-1} < \\dots < t_0 = T$ with $\\Delta t = t_{k+1} - t_k$ for our analysis. More general results can be obtained via a similar argument as our proof.\nProposition A.2 (Information Link Between $z_T$ and Generated $z_0$). Let $z_T \\sim \\pi_{\\text{HERO}}$. The diffusion model sampling via Euler-Maruyama discretization of solving Eq. (7) with uniform stepsize $\\Delta t$ will lead to the following form:\n$z_0 = z_T \\prod_{k=0}^{n-1} e^{f(t_k)\\Delta t} - \\sum_{k=0}^{n-1} g^2(t_k) \\nabla \\log p_{t_k}(y_k) \\Delta t \\prod_{j=k+1}^{n-1} e^{f(t)t} + R(\\Delta t),$      \nwhere $R(\\Delta t)$ is the residual term concerning the accumulated stochastic component $g(t_n)\\Delta w_n$ and stepsize $\\Delta t$. Therefore, whenever $\\Delta t \\neq 0$, $z_0$ and $z_T$ are dependent.\nProof. For the simplicity of notations, we write $y_n := z_{t_n}$ (i.e., $y_0 = z_T$). Applying the Euler-Maruyama scheme, we obtain:\n$y_{n+1} = y_n + (f(t_n)y_n - g^2(t_n)\\nabla \\log p_{t_n}(y_n))\\Delta t + g(t_n)\\Delta w_n,$\nwhere $y_0 \\sim \\pi_{\\text{HERO}}$, and $\\Delta w_n \\sim \\mathcal{N}(0, \\Delta t I)$ represents the increment of the Wiener process.\nWe first ignore the stochastic term $g(t_n) \\Delta w_n$ for simplicity, rewriting the equation as:\n$y_{n+1} = y_n + (f(t_n)y_n - g^2(t_n)\\nabla \\log p_{t_n}(y_n))\\Delta t.$\nThis can be rearranged into:\n$y_{n+1} = y_n(1 + f(t_n)\\Delta t) - g^2(t_n)\\nabla \\log p_{t_n}(y_n)\\Delta t.$\nTo derive a recursive formula for $y_n$, we substitute the above equation back into itself. Starting from $y_0$:\n$y_1 = y_0(1 + f(t_0)\\Delta t) - g^2(t_0)\\nabla \\log p_{t_0}(y_0)\\Delta t,$\n$y_2 = y_1(1 + f(t_1)\\Delta t) - g^2(t_1)\\nabla \\log p_{t_1}(y_1)\\Delta t.$\nBy continuing this process, we express $y_n$ recursively as:\n$y_n = y_{n-1}(1 + f(t_{n-1})\\Delta t) - g^2(t_{n-1})\\nabla \\log p_{t_{n-1}}(y_{n-1})\\Delta t.$\nIterating this process (mathematical induction), we derive a general expression for $y_n$:\n$y_n = y_0 \\prod_{k=0}^{n-1} (1 + f(t_k)\\Delta t) - \\sum_{k=0}^{n-1} g^2(t_k)\\nabla \\log p_{t_k}(y_k) \\Delta t \\prod_{j=k+1}^{n-1}(1 + f(t_j)\\Delta t).$\nWe can utilize the exponential Taylor expansion\n$e^{f(t)\\Delta t} = (1 + f(t)\\Delta t) + O((\\Delta t)^2).$"}, {"title": "RL FINE-TUNING WITH EXISTING REWARD MODELS", "content": "To investigate the benefits of leveraging online human feedback, we compare our HERO to DDPO (Black et al., 2024) with PickScore-v1 (Kirstain et al., 2023) as the reward model on reasoning and personalization tasks in this paper. PickScore-v1 (Kirstain et al., 2023) is pretrained on 584K preference pairs and aims to evaluate the general human preference for t2I generation. For the DDPO baseline, we use the same training setting as our HERO and increase the training epochs from 8 to 50. The success rate is calculated using 200 evaluation images.\nAs shown in Table 4, using DDPO with a large-scale pretrained model as the reward model can not address these tasks easily. Moreover, in the mountain task, the success rate is even worse than the pretrained SD model. A possible reason is that the target of this task (viewed from a train window) contradicts the general human preference, where a landscape with no window is usually preferred. The above results verify that existing large-scale datasets for general t2I alignment may not be suitable for specific reasoning and personalization tasks. Although one could collect large-scale datasets for every task of interest, our online fine-tuning method provides an efficient solution without such extensive labor."}, {"title": "IMPORVE TIME EFFICIENCY FOR ONLINE FINETUNING", "content": "Inspired by Clark et al. (2024), we only consider the last $K + 1 (< T)$ steps of the denoising trajectories during loss computation in Equation (2) to accelerate training and reduce the workload for human evaluators:\n$\\nabla_\\phi \\mathcal{L}_{\\text{DDPO-K}}(\\phi) = \\mathbb{E}_{z_T \\sim z_T} \\left[ \\sum_{t=0}^{K} \\frac{p_\\phi(z_{t-1} | z_t, c)}{p_{\\phi_{\\text{old}}}(z_{t-1} | z_t, c)} \\nabla_\\phi \\log p_\\phi(z_{t-1} | z_t, c) R(z_0) \\right].$      (8)\nWe evaluate the relationships between K and the training time for 1 epoch on the hand task and show the results in Table 5. Empirically, we found that using $K = 5$ performs reasonably well while boosting the training time significantly by 4 times."}, {"title": "DREAMBOOTH PROMPTING EXPERIMENTS", "content": "To investigate the effect of training prompt, class prompt, and generation prompt selection on the performance of our tasks, we test various prompt combinations with the narcissus task. For the"}, {"title": "PROMPT ENHANCEMENT WITH A LARGE VLM", "content": "In the SD-enhanced baselines, we prompt the Stable Diffusion v1.5 model with a prompt enhanced by GPT-4 (Brown, 2020; Achiam et al., 2023). To generate the enhanced prompts, we input \"Enhance the following text prompt for Stable Diffusion image generation: [prompt]\" to GPT-4 ([prompt] is the original task prompt labeled \"Prompt\" in Table 1 and \"Generation Prompt\" in Table 10). Output-enhanced prompts used for the SD-enhanced baseline are shown in Table 10. Although our prompt enhancement is not an exhaustive method to show the full capabilities of prompt engineering, we include SD-enhanced as a baseline to demonstrate that many of our tasks are challenging to solve, given a simple prompt enhancement method."}]}