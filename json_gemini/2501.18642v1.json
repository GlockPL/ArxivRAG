{"title": "DebiasPI: Inference-time Debiasing by Prompt Iteration of a Text-to-Image Generative Model", "authors": ["Sarah Bonna", "Yu-Cheng Huang", "Ekaterina Novozhilova", "Sejin Paik", "Zhengyang Shan", "Michelle Yilin Feng", "Ge Gao", "Yonish Tayal", "Rushil Kulkarni", "Jialin Yu", "Nupur Divekar", "Deepti Ghadiyaram", "Derry Wijaya", "Margrit Betke"], "abstract": "Ethical intervention prompting has emerged as a tool to counter demographic biases of text-to-image generative AI models. Existing solutions either require to retrain the model or struggle to generate images that reflect desired distributions on gender and race. We propose an inference-time process called DebiasPI for Debiasing-by-Prompt-Iteration that provides prompt intervention by enabling the user to control the distributions of individuals' demographic attributes in image generation. DebiasPI keeps track of which attributes have been generated either by probing the internal state of the model or by using external attribute classifiers. Its control loop guides the text-to-image model to select not yet sufficiently represented attributes, With DebiasPI, we were able to create images with equal representations of race and gender that visualize challenging concepts of news headlines. We also experimented with the attributes age, body type, profession, and skin tone, and measured how attributes change when our intervention prompt targets the distribution of an unrelated attribute type. We found, for example, if the text-to-image model is asked to balance racial representation, gender representation improves but the skin tone becomes less diverse. Attempts to cover a wide range of skin colors with various intervention prompts showed that the model struggles to generate the palest skin tones. We conducted various ablation studies, in which we removed DebiasPI's attribute control, that reveal the model's propensity to generate young, male characters. It sometimes visualized career success by generating two-panel images with a pre-success dark-skinned person becoming light-skinned with success, or switching gender from pre-success female to post-success male, thus further motivating ethical intervention prompting with DebiasPI.", "sections": [{"title": "1 Introduction", "content": "Generative AI models have made their mark in journalism, with AI-generated images that accompany news articles [2, 27], sometimes even without disclosing the use of AI [24]. Given the increased exposure of the public to AI-generated news-accompanying images, it is concerning that analysis of AI-generated images has revealed levels of racial and gender biases [5], for example, sexualized images of women of color [11]. Such images perpetuate and amplify stereotypes and could spread on the internet in connection with digital news. Recent studies indicate that biases of text-to-image AI models extend across various dimensions of generated content, including skin tones, gender, and attire [5,9,23].\nThe research question has arisen: To what extent can ethical interventions via prompting influence generative text-to-image AI models to produce outputs that ensure diverse representations of people? Our research answers this question by building on the idea of \"prompting with ethical intervention:\" Recent work [10] designed a procedure for training text-to-image AI models that changes the demographic attribute of a person in a prompt according to a desired input distribution of the attribute. Other work experimented with prompts such as \"a person who works as a nurse\" to diagnose social bias of the model [8] and prompts with ethical interventions, e.g., \"a photo of a bride from diverse cultures,\" to mitigate the social bias of the model [1]. While these prior works provided an important proof-of-concept of the idea of \"prompting with ethical intervention,\" they require training of the generative models, which was accomplished for relatively small text-to-image generative models (DALL-ESmall [26], minDALL-E [14], and Stable Diffusion [20]).\nOur study addresses the task from the perspective of a newsroom editor who cannot retrain or finetune a text-to-image model and would like to make a selection from a set of images created by a commercial tool. We selected DALL-E 3 [4] as the text-to-image model in our experiments. As part of our methodology, we generated demographics-neutral news headlines, specifically about human-interest career success stories, including \"rags-to-riches stories,\" and asked DALL-E 3 to interpret these headlines visually. Our motivation for"}, {"title": "2 Related Work", "content": "Bias, stereotypes, and representational harm have been identified as areas of impact that generative AI may have on society [22]. Bias can be introduced at various stages of the machine learning pipeline the model used, compression techniques, and many other factors can \"amplify harm on underrepresented"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Ethical Intervention Prompts and DebiasPI", "content": "We designed three types of prompts to evaluate the generation bias of text-to-image generative models, shown in Figure 2. The baseline does not explicitly prompt the model to pay attention to any attributes, so it might emphasize certain traits and styles, according to its internal demographic bias. The Prompt with Attribute Distribution serves as a means to attempt to debias the generative model, using the proposed Debiasing by Prompt Iteration (DebiasPI) process visualized in Figure 3.\nThe user of DebiasPI starts by setting a target distribution within the prompt. The distribution is defined by a list of attribute bins and the desired counts per bin. This list and the text (e.g., news headline) to be visualized are sent to the text-to-image model for image generation. The model response is parsed for chosen attributes, either using its internal belief or an external attribute classifier. The corresponding distribution bin is decreased by one, and once an attribute count reaches zero, the model is instructed to stop generating images with that attribute. Distribution statistics are collected throughout to determine the state of the debiasing process. The target distribution is reached once the counts of all bins are zero.\nInitially during DebiasPI, the model utilizes its internal probability distribution for selecting attributes. As the allocated numbers are exhausted for certain attribute options and reach zero for the corresponding distribution bin, DebiasPI starts to adjust the generated attribute distribution to align more closely with our specified target distribution. This adjustment ensures that the final output adheres to the desired attribute proportions. By iteratively adjusting the prompt and the selection process, DebiasPI systematically reduces biases that may have"}, {"title": "3.2 Codebook for Manual Annotation of Attributes", "content": "The codebook we designed contains nine option-based questions about the race, gender, age, and occupation of the subject and various image characteristics (lighting, contrast, etc.), and an open-ended question about which (if any) stereotype the image might propagate, as perceived by the annotator.\nAnnotators were provided 10 swatches of skin tones from the Monk Skin Tone Scale [16] and given three options: Light (Types 1 to 3), Medium (Types 4 to 6), and Dark (Types 7 to 10). Examples of public figures around the world were shown below the swatch groups. The Monk Skin Tone Scale was chosen because it covered a large range of skin tones without overwhelming the annotators. To guide annotators in determining the race of a person in a generated image, an appendix was provided at the end of the codebook, with examples of a male and female public figure for each of nine races: Black, East Asian, Hispanic or Latino, Indigenous, Middle Eastern or North African, Native Hawaiian and Other Pacific Islander, South Asian, Southeast Asian, and White. The categorization into 9 races is derived from the race/ethnicity definitions by the U.S. Census Bureau. A map depicting the skin color of people in each region [6] was also provided as a reference. As humans are affected by the cross-race effect, it is hoped that these resources would help to reduce any confusion about the race of the subjects in the images.\nThe codebook offers three options for gender: Male, Female, and Unable to distinguish gender, four options for age: Children and adolescents (1\u201318), Young adulthood (19-35), Middle adulthood (36-64), and Seniors, and three options for body type: ectomorphs, mesomorphs, and endomorphs. Example images of public figures at the various age milestones (each decade) and graphics of body types are provided as references. Instead of asking the annotators to try to surmise the career or occupation of the person from the generated image, we instructed them to go back to the success story headline that was used to create the image and evaluate its perspective. In communication research, different perspectives are known as \"frames\", which, when used in news media, will influence the opinion of their readers in multiple ways. The codebook describes twelve frames that have been adapted from a list of occupations [17] (full list at http://www.cs.bu.edu/faculty/betke/research/DebiasPI)."}, {"title": "3.3 Methods to Evaluate Attributes and their Distributions", "content": "For automated analysis of skin tone of the main character's face in the generated image, we used the Facial Representation Learning in a Visual-Linguistic Manner (FaRL) model [30] to segment the largest area containing facial pixels. We then averaged the skin color within this area before quantizing it into the Monk Tone scale. For automated analysis of gender of a person in an AI generated image, we recommend the use of the Large Language and Vision Assistant (LLaVA) [15]."}, {"title": "4 Experiments and Results", "content": ""}, {"title": "4.1 Headline Generation using GPT4-powered ChatGPT", "content": "We generated 200 headlines on human-interest success stories using GPT4-powered ChatGPT in four phases with 50 headlines per phase. Examples from each phase of the headline generation process were used as samples for the next phase. This iterative process was designed to vary the themes and sentence structures of the generated headlines. There were three primary headline patterns: 1) Transformation and overcoming adversity or hurdles; 2) Evolution of a role or growth within a profession; and 3) Journey from A to Z. We ensured that the generated headlines were demographic-neutral by specifically prompting ChatGPT to avoid including personal names, race, and gender. Examples of actual headlines were used to start the headline-creation process: \"A Gen Z Success Story\" (from the New York Times), \"Teen entrepreneur shares sweet success story behind her multi-million-dollar lemonade business\" (from Fox News). These two news outlets were selected to cover the two sides of the U.S. political spectrum, with The New York Times generally left-leaning and Fox News right-leaning. An example prompt is shown in Figure 4. The resulting headlines are available at http://www.cs.bu.edu/faculty/betke/research/DebiasPI."}, {"title": "4.2 Baseline Prompt Experiment: No Ethical Intervention", "content": "Iterating through the Baseline Prompt (Fig. 2) with <text>=headline 1,..., headline 200, we asked DALL-E 3 to create 200 images to visualize the generated"}, {"title": "4.3 Experiment Yielding Two-Panel Images", "content": "Using the Attribute List Prompt (Fig. 2) with attribute lists of race or gender-&-race, we found that DALL-E 3 often created two-panel images (93 of 200 images), where the first panel shows a person before career success and the second panel after. The AI model fails to understand that the same person should be visualized and sometimes showed individuals of different race and/or gender, see Figs. 1(a) and 5. In cases when there is a difference in skin tone between the individuals shown in the two panels (14 images), our analysis showed that the change was always from a darker to a lighter skin tone, perpetuating the social bias that a person must be White or light-skinned to make career progress. Analysis of body types and occupations of the individuals revealed a preference of the model for the mesomorphs (athletic, solid, strong, not overweight or underweight). Endomorphs (lots of body fat or muscle) were rarely generated (8%). Images that depicted occupations in the areas of Arts, Audio/Video Technology & Communications and Business Management, Administration & Finance were predominately male (77%).\nThe \"ground truth\" of the attributes (prompted and unprompted) in this experiment were provided by four annotators. The Inter-Coder Reliability of each pair of annotators was calculated using the Cohen-Kappa score [12] and percent agreement based on an initial round of annotation on the first 40 images (10% of the total number of images available). The Cohen-Kappa scores ranged from 0.64 (Good) to 1.0 (Very Good). The percent agreement scores were then used to guide the allocation of annotator pairs to re-evaluate images for which the Cohen-Kappa score was only in the \"Good\" range. Using this process, we eventually"}, {"title": "4.4 Ethical Intervention Experiments", "content": "Based on our experimental results with the Attribute List Prompt described in Sect. 4.3, in subsequent experiments with ethical intervention prompts, we adjusted the prompts shown in Figure 2. We instructed the text-to-image model to generate a photograph-style image of a single person, facing forward in the generated image. This then enabled us to automate the annotation process, using the tools described in Section 3.3. We note that, given the list of 200 headlines, we could only prompt DALL-E 3 to process 5 headlines at a time, generating one image per headline, because it would hang when asked to generate more than five images at a time. This resulted resulted in 40 iterations of DebiasPI process.\nOur experiments with the Attribute Distribution Prompt focused on the uniform distribution, asking the text-to-image model to produce outputs with equal representations of the attributes. As designed, the DebiasPI process results in outputs with a perfect balance of gender or race. The attribute type \"skin tone\" was more difficult to handle, as we show below.\nTo illustrate how DebiasPI iterates through its attribute selections and yields a balanced outcome, we report on an experiment with 50 images created with a"}, {"title": "5 Conclusions", "content": "We proposed DebiasPI, an inference-time framework for robust attribute distribution control. With DebiasPI, a user can generate a series of images with attributes aligned to a target distribution, such as uniform distribution for fairness or a distribution that stresses specific traits. We envision, as a use case, a newsroom editor who might want to select among a diverse set of images of athletes. Our experiments show that DebiasPI is successful in generating images for representation of race and gender according to the desired attribute distribution.\nLimitations of our work include the relatively small numbers of experimental data, the dependence of DebiasPI on the text-to-image model's ability to generate certain attributes (for example, skin tone), and the challenge that the model's internal beliefs or the external classifier's attribute analysis may not be entirely reliable, complicating DebiasPI's control over outputs.\nThe datasets we here publish may serve as benchmark comparisons by others in future work. Additional experiments with body-type, age, profession, and not-yet-explored attributes like affect would be interesting. Future work will also study ethical intervention when the text-to-image model is challenged with abstract concepts in the text to be visualized."}]}