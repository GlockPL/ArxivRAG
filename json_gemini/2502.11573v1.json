{"title": "InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning", "authors": ["Congkai Xie", "Shuo Cai", "Wenjun Wang", "Pengxiang Li", "Zhijie Sang", "Kejing Yang", "Yiming Zhang", "Zhen Li", "Guanghao Zhu", "Zeyu Liu", "Yang Yu", "Yuhang Liu", "Su Lu", "Baoyi He", "Qi Zhou", "Xiaotian Han", "Jianbo Yuan", "Shengyu Zhang", "Fei Wu", "Hongxia Yang"], "abstract": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. InfiR aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github.com/Reallm-Labs/InfiR.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) (Touvron et al., 2023; DeepSeek-AI, 2024a,b; Qwen, 2023, 2024; Yang et al., 2024) and Multimodal Large Language Models (MLLMs) (Wang et al., 2024; Chen et al., 2024) have shown remarkable advancements in reasoning capabilities. However, these models, often consisting of hundreds of billions of parameters, pose significant challenges related to computational resources, deployment costs, and environmental impact. Their training and development demand substantial infrastructure investments, making them accessible primarily to major technology corporations. Additionally, these models usually require cloud deployment, which raises privacy concerns regarding user data.\nSmall models, typically comprising fewer than 2 billion parameters, have emerged as a promising solution to these challenges. These models strive to achieve an optimal balance between performance and efficiency. Their considerably lower training and development costs increase accessibility for researchers and organizations. There is a key challenge for SLMs and MSLMs in enhancing reasoning capabilities while reducing the size of the model.\nThis paper focuses on: (1) developing efficient small language models with competitive reasoning capabilities and (2) extending these models to handle multimodal inputs while managing real-world operational system tasks. We explore novel training pipelines that enable these compact models to perform reasoning tasks while being deployable on edge devices. Our contributions to the field include the following:\n\u2022 We propose a pre- and post-training pipeline for small models that enhances reasoning capabilities, completing training in under 6000 GPU hours.\n\u2022 Our InfiR-1B-Base and InfiR-1B-Instruct models achieve state-of-the-art performance at the 1B parameter scale, with reasoning-related average score improvements of 2.26x and 1.33x over Llama3.2-1B-Base and Llama3.2-1B-Instruct.\n\u2022 Our InfiR-VL-1.6B model achieves the state-of-the-art performance in the Android World scenario, with an accuracy of 28% increment compared to the best SOTA among small models.\nThrough this research, we aim to advance the development of practical, efficient AI systems capable of performing reasoning tasks, while reducing barriers to AI system adoption and addressing user privacy requirements through model size reduction."}, {"title": "2 Small Language Model Pre-training", "content": ""}, {"title": "2.1 Pre-training Data", "content": "To construct effective SLMs with strong reasoning capabilities, high-quality pre-training data are essential. We present a comprehensive pipeline for collecting and processing such data."}, {"title": "2.1.1 Data Collection", "content": "To develop SLMs with enhanced reasoning capabilities, we constructed a comprehensive dataset comprising two primary components: source code data and reasoning-oriented text data. This approach is designed to capture both programmatic logic and natural language reasoning patterns. For the source code component, we curated the dataset by combining several high-quality open-source resources, including The Stack v2 (Lozhkov et al., 2024b) and Starcoder (Li et al., 2023) and additional Python-centric repositories from GitHub. More details of the distribution of programming languages can be found in Figure 4a. To acquire reasoning-oriented text data, we assembled a diverse corpus encompassing web content, academic literature, books, and encyclopedic sources. This corpus was specifically curated to emphasize content that shows logical reasoning, analytical thinking, and structured argumentation."}, {"title": "2.1.2 Data Pipeline", "content": "From a technical perspective, as the model size decreases, its information storage capacity diminishes proportionally. To develop a SLM specifically targeted for reasoning tasks, it is essential to construct a sophisticated data pipeline that can effectively filter out noise while preserving reasoning-related information. The workflow of the entire pipeline architecture is depicted in Figure 1.\nHeuristic filtering In the initial phase, we employ heuristic filters to perform preliminary filtering on the raw corpus, thereby reducing noise in the data. We employed heuristic filters from FineWeb (Penedo et al., 2024) to extract high-quality text from web pages. For code data, we selectively filtered and processed repositories written in widely used languages such as Python, JavaScript, Java, and C to ensure relevance and quality. Rule-based filters (Huang et al., 2024b) were applied to remove files that were contaminated or significantly deviated from standard patterns.\nReasoning-oriented text recall Reasoning-oriented data, unlike code, lack explicit patterns that can be identified through rule-based approaches. Instead, model-based methods are typically required to extract potential reasoning relationships from large volumes of text. Our reasoning-oriented data retrieval strategy is categorized into three components. First, we prioritize the retrieval of mathematics-related samples, as mathematical texts often exhibit strong reasoning characteristics. Second, we recognize that a significant portion of code-related content in internet data contains reasoning elements. Finally, we aim to retrieve reasoning-related texts from various other domains.\nWe developed a standard text recall data pipeline, drawing inspiration from DeepSeekmath (Shao et al., 2024) and OpenCoder (Huang et al., 2024a). The pipeline begins with establishing seed datasets. For mathematics-related content, we utilize high-quality mathematical datasets such as OpenWebMath (Paster et al., 2023) and InfiMM-WebMath (Han et al., 2024) as seed data. For code-related text, we employ StackOverflow as the seed dataset. For other domains, we utilize Qwen2.5-7B-Instruct (Yang et al., 2024) to annotate text URLs and titles, while also incorporating responses synthesized by large language models from datasets like Infinity Instruct as seed data.\nFollowing seed data acquisition, we train domain-specific fasttext models using positive samples from the seed data and random negative samples from web pages and books. These fasttext models are then used to recall relevant content from the remaining corpus.\nDeduplication An excessive amount of homogeneous data can be detrimental to small language models. therefore, it is crucial to ensure data diversity. To optimize data efficiency while maintaining semantic diversity, we implemented a global Min-Hash algorithm to efficiently detect and eliminate near-duplicate documents.\nQuality assessment To ensure comprehensive data quality across diverse domains, we establish a two-step evaluation framework. First, we employ a domain-specific quality assessment tool: (1) For web content, we leverage FineWeb-eduscorer (Penedo et al., 2024) to evaluate document quality through multiple dimensions; (2) For mathematical content, we utilize a model-based classifier following (Lozhkov et al., 2024a) that scores reasoning and deduction capabilities on a 1-5 scale, filtering to retain only high-quality samples; (3) For code data quality validation, we utilize a static analysis tool to validate syntactic correctness and identify potential structural issues. We also perform comprehensive ablation experiments through continued pre-training on LLaMA 3.2-1B, comparing the performance between models trained on raw and filtered datasets. The model trained on filtered data exhibits better performance across multiple code-related benchmarks compared to its counterpart trained on the raw dataset. Moreover, the filtering process facilitates accelerated convergence, enabling the model to achieve desired performance metrics with fewer training steps.\nDecontamination To ensure fairness of comparison, we implemented a token-level 10-gram decontamination algorithm to remove potentially contaminated content in standard benchmarks."}, {"title": "2.2 Annealing Data", "content": "After training on 900 billion tokens, we aimed to further enhance reasoning abilities and bridge the gap between the pretraining and supervised fine-tuning stages. To achieve this, we constructed an annealing dataset comprising additional high-quality reasoning data and continued training the checkpoint from the previous stage on 40 billion annealing data to obtain the final base model.\nOriginal data During the annealing phase, we maintained the original proportion of source code. For reasoning-oriented text, we removed most web page data, retaining only a small portion related to mathematics and code.\nOpen source annealing data We collected annealing data used by Dolmino (OLMo, 2024) and OpenCoder. We also include a high-quality code-related datasets, incorporating training samples from APPS (Hendrycks et al., 2021a) and Code Contest (Li et al., 2022). For each programming problem across the above datasets, we retain a single solution and prioritize Python implementations. These high-quality datasets were utilized in comparative experiments, which demonstrated a significant improvement in the base model's performance on reasoning benchmarks in few-shot settings.\nSynthetic data We further leveraged a large language model to generate a batch of synthetic data, implementing stringent selection mechanisms to ensure high quality. For reasoning-oriented data, we employed a reward model with rejection sampling to enhance logical coherence and correctness. For code-related data, we validated its syntax and functionality within a sandbox environment, ensuring robustness and reliability."}, {"title": "2.3 Offline Evaluations for Data-mixing", "content": ""}, {"title": "2.3.1 Pre-training Stage", "content": "During the pre-training phase, the model compresses knowledge into its parameters. We prioritize the model's recall capability, primarily using Negative Log Likelihood (NLL) to assess performance. NLL evaluations are conducted on two types of data:\nSample from webpage During pretraining, we utilize a large amount of web page data. To measure model convergence, we sample a validation set from the web data. Although the web data has been cleaned, there might still be meaningless text in the samples. Therefore, we use existing large language models to calculate the NLL of the text and rank them separately. When a text ranks in the bottom 20% across all rankings, we consider it meaningless and discard it. The NLL for both types is defined as:\nNLL = - \\frac{1}{n} \\sum_{i=1}^{n} \\log P(t_i|t_{1:i-1})."}, {"title": "2.3.2 Annealing Stage", "content": "During the annealing stage, the model rapidly converges due to the decay of the learning rate. Precision is prioritized over recall. To evaluate the model's performance, we use a downstream benchmark for few-shot generation instead of the NLL method.\nWe can suppose a contaminated distribution r is introduced into the original clean distribution p with a probability of \\epsilon, leading to a new distribution q, which satisfies:\nq(t_i|t_{1:i-1}) = (1 - \\epsilon)p(t_i|t_{1:i-1}) + \\epsilon r(t_i|t_{1:i-1}).\nBy computing the ppl for the new distribution q, than we have:\nppl_q(t_{1:n}) = \\exp(-\\frac{1}{n} \\sum_{i=1}^{n} \\log \\frac{1}{q(t_i|t_{1:i-1})})\nSince q(t_i|t_{1:i-1}) \\ge (1 \u2013 \\epsilon)p(t_i|t_{1:i-1}), we can obtain\nppl_q(t_{1:n}) \\le \\exp(-\\frac{1}{n} \\sum_{i=1}^{n} \\log \\frac{1}{(1-\\epsilon)p(t_i|t_{1:i-1})})\n\\le (\\frac{1}{1-\\epsilon}) ppl_p(t_{1:n})\n\\approx (1 + \\epsilon) ppl_p(t_{1:n}).\nThe approximation holds when \\epsilon is small enough. This indicates that if 5% contaminated distribution is introduced, the perplexity increases no more than 5%. However, this could lead to very poor results in generation for the language model, since the model may produce a corrupted token approximately every 20 tokens on average. Therefore, during the annealing stage, we evaluate the model directly using a downstream benchmark in a few-shot setting to determine a data mixing ratio that optimally balances various capabilities."}, {"title": "2.4 Training Details", "content": "Ultimately, we curated a high-quality reasoning-related dataset consisting of approximately 900 billion tokens for pretraining and 40 billion tokens for annealing. Details of the dataset's composition are provided in the Table 7 and Table 8. Based on LLaMA-3.2 1B architecture, we trained our model in two phases. During the pretraining phase, we trained the model on the curated 900 billion tokens for one epoch, followed by an annealing phase on an additional 40 billion tokens for one epoch. We employed a learning rate of 1.4e-3 with a size of 2048 and a sequence length of 4096 tokens throughout the training process. Training was conducted using NVIDIA NeMo (Kuchaiev et al., 2019) with distributed optimization and DDP gradient overlap on a cluster of 64 H800 GPUs over a total of 90 hours, equating to 5760 GPU hours."}, {"title": "3 Small Language Model Post-training", "content": "During the post-training phase of the language model, we leverage Supervised Fine-Tuning (SFT) with meticulously curated datasets to enhance instruction-following and reasoning capabilities. The deliberate construction of SFT data, which accounts for the balance across diverse domains, facilitates enhancements in controllability and robustness, ensuring optimal performance in various applications."}, {"title": "3.1 Supervised Fine-tuning Data", "content": "The quality and diversity of data play a crucial role in supervised fine-tuning. We utilize high-quality, publicly available datasets involving instruction-following, reasoning, and code-related tasks, including Infinity-Instruct (BAAI, 2024), Orca-AgentInstruct-1M-v1 (Mitra et al., 2024a), ScaleQuest (Ding et al., 2024), NuminaMath (Li et al., 2024), and OPC-SFT-Stage2 (Huang et al., 2024b). In addition, we developed a data synthesis pipeline to generate high-quality SFT data, incorporating components such as instruction evolution, rejection sampling, etc., as illustrated in Figure 2.\nWe curated a high-quality set of instructions from publicly available datasets as seed data and employed a large language model for instruction evolution, generating more diverse and complex instructions. Subsequently, we utilized the Qwen-2.5-32B-Instruct model to generate responses corresponding to these instructions. We encouraged the model to engage in a \"step-by-step\" reasoning process prior to delivering the final answer, which promoted a more rigorous and logically consistent output.\nRejection sampling For each instruction, we generate multiple responses and employ rejection sampling to ensure the quality of the data. For logic and mathematical reasoning data, we utilize a reward model to assess responses and select the one with the highest score. For code data, we perform execution-based code verification within a sandbox environment to ensure the correctness and functionality of the generated code.\nDiversity and quality We assign domain labels to each training sample and perform diversity sampling from the extensive dataset to balance the distribution of data across different domains. Using heuristic rules and LLM-based scoring judgments, we filter out low-quality instructions and responses. Additionally, we use a difficulty scoring model to assess mathematical reasoning data, maintaining a balanced distribution across varying levels of difficulty. Through the combination of diverse and high-quality data sources, instruction augmentation, rejection sampling, and rigorous quality control, we ensure that the training data is both comprehensive and reliable. These carefully designed methodologies contribute to enhancing the model's reasoning capabilities, enabling it to better handle complex tasks and exhibit improved performance across a wide range of problem domains."}, {"title": "3.2 Training Details", "content": "We employed the Llama 3 chat template and applied supervised fine-tuning using a standard cross-entropy loss on a few million samples, while masking the loss on the prompt tokens. We fine-tuned the model for 4 epochs with a learning rate of 2e-5 and a batch size of 128. We utilized the cosine learning rate scheduler with a warm-up ratio of 0.1 to optimize training process. The model was fine-tuned on the Llama 3 template with a maximum sequence length of 4096 tokens."}, {"title": "4 Small Multimodal Language Model Training", "content": ""}, {"title": "4.1 Multimodal Pre-training", "content": ""}, {"title": "4.1.1 Data Collection", "content": "To systematically develop multimodal reasoning capabilities, we implement a hierarchical data collection strategy that progressively builds from fundamental visual-language alignment to complex reasoning tasks. At the foundation level, we collect data from 11 task domains including captioning, general QA, and OCR to train the MLP projector while keeping the vision encoder and language model frozen, establishing the essential mapping between visual features and LLM's textual semantic space. In addition, we enhance the model's text comprehension abilities by synthetic rendering text data, converting documents, code snippets, and other textual content into visual formats."}, {"title": "4.1.2 Data Cleaning", "content": "In order to reduce the impact of low-quality, low-similarity image-text pairs in the dataset on the model's ability to understand the semantics of images and texts, we use the Vista model (Zhou et al., 2024) to extract the embeddings of the data pairs, calculate the similarity by pair, and then select a reasonable similarity threshold to filter out low-quality, low-similarity image-text pairs. The details is show in Appendix E."}, {"title": "4.2 Multimodal Instruction-Tuning", "content": ""}, {"title": "4.2.1 General Vision Reasoning Abilities", "content": "As we did in the SFT of SLMs, during the SFT stage of MSLMs, we collected multiple open-source instruction-tuning datasets spanning 14 domains, including general QA, mathematics, charts, OCR, documents, science, medical, GUI, code, etc., while covering multiple modalities such as single-image, multi-image, and text, as shown in Table 9, and synthesized more high-quality vision reasoning data through multiple methods such as text rendering and virtual environment, so as to enable the model to have comprehensive general vision reasoning capabilities for image-related problems in the real world and strengthen the effective semantic alignment between visual input and SLM."}, {"title": "4.2.2 Multimodal Math Reasoning Abilities", "content": "After the initial stimulation of general vision reasoning ability, we fine-tuned SLM in different subdivisions for different scenarios to enhance the ability of MSLM in the corresponding fields. In mathematics, we used multiple data sets such as InfiMM-WebMath-40B (Han et al., 2024), MathVista (Lu et al., 2024), and MM-Math (Sun et al., 2024) as bases, and used ChatGPT to synthesize multimodal math question-answering CoT-PoT data; in addition, we also used the text rendering mechanism to mix in some pure text math question-answering Instruction tuning data."}, {"title": "4.2.3 Operator-System Reasoning Abilities", "content": "Our MSLM's operator-system reasoning capabilities are developed through a two-stage supervised fine-tuning framework that progressively builds from fundamental understanding to advanced reasoning. (Liu et al., 2025) The foundation stage establishes essential GUI comprehension by utilizing diverse datasets spanning GUI understanding, grounding, and question-answering tasks. We standardize coordinate systems to a [0,1000] scale and implement a reference-augmented annotation format, enabling precise spatial reasoning while maintaining natural language flow in the model's responses. Building upon this foundation, with existing trajectory data, we synthesize 45K training samples that incorporate these advanced reasoning patterns, enabling our MSLM to handle complex GUI interactions through structured reasoning rather than mere pattern matching."}, {"title": "4.3 Training Details", "content": "As shown in Fig 3, our proposed Multimodal Small Language Model Architecture integrates a vision encoder with a language model. For the vision encoder, we utilize SigLip-So400m (Zhai et al., 2023), which is built upon the SoViT-400M. The language model component is based on our InfiR-1B-Base, which provides reasoning capabilities. To align the visual features with the language model's latent space, we employ a simple MLP layer as the visual projector.\nDuring the Pretraining stage, we exclusively train the MLP projector with the aim of aligning the backbones of the ViT and LLM. In this phase, we utilize a large dataset of captions.\nIn the SFT stage, the training is divided into two sub-stages. The first sub-stage focuses on enhancing the model's capabilities with images. Here, we unfreeze the parameters of the ViT and train both the ViT and the adapter. We employ a substantial amount of text rendering synthetic samples and GUI samples to ensure the model develops initial vision reasoning capabilities. Additionally, we retain some samples from the pretraining phase to maintain the model's general captioning abilities.\nIn the second SFT sub-stage, we unfreeze all parameters and train on the most challenging samples, such as trajectory data in operating system scenarios and tool usage samples in mathematical contexts. This step is designed to enhance the model's planning and reasoning abilities."}, {"title": "5 Experimental Results", "content": ""}, {"title": "5.1 Small Language Models", "content": ""}, {"title": "5.1.1 Benchmark and Quantitative Results", "content": "Benchmarks We evaluate our model's capabilities across multiple dimensions using established benchmarks: MMLU (Hendrycks et al., 2021b) for general reasoning and knowledge spanning 57 subjects, HumanEval (Peng et al., 2024) and MBPP (Austin et al., 2021) for code generation and understanding in Python programming tasks, GSM8K (Cobbe et al., 2021) for grade school-level word problems, and MATH (Hendrycks et al., 2021c) for advanced mathematical reasoning covering algebra, geometry, and calculus.\nPre-trained model We evaluated the performance of SLMs using a few-shot setting. Table 1 compares the performance of InfiR-1B-Base with state-of-the-art open-source models. InfiR-1B-Base outperforms Llama3.2-1B and is comparable to Qwen2.5-1.5B.\nPost-trained model We evaluated the performance of InfiR-1B-Instruct on the MMLU, GSM8K, MATH, HumanEval, and MBPP benchmarks using zero-shot prompts, as shown in Table 2. InfiR-1B-Instruct outperforms Llama-3.2-1B-Instruct on various reasoning tasks, and shows significant improvements in mathematical reasoning and coding tasks. Specifically, InfiR-1B-Instruct exceeds Llama-3.2-1B-Instruct by more than 26 points on GSM8K and 16 points on MATH, while surpassing Llama by 19 points on HumanEval and 7 points on MBPP. Notably, despite Qwen-2.5-1.5B-Instruct having more parameters than InfiR-1B-Instruct, it demonstrates comparable performance to Qwen-2.5-1.5B-Instruct in both coding and mathematical reasoning."}, {"title": "5.1.2 Discussion and Insights", "content": "During the training of the InfiR-1B, we observed several phenomena that significantly impacted the experimental results.\nHeuristic filter for reducing special patterns During pretraining, metrics related to mathematics often showed large fluctuations. Analysis revealed that certain checkpoints generated an <eos> token with high probability when prompts ended with a colon \":\". To address this, we cleaned the mathematical web page data by removing text ending with a colon.\nAdjusting batch size according to data \"width\" Initially, we trained the model with separate batches for coding and math, using smaller batch sizes for faster convergence. When combining coding, math, and general reasoning data, using the same batch size often led to gradient norm overflow and instability. Larger batch sizes help avoid erroneous gradient estimates when more domains are included, but they slow convergence. We balanced training costs by using a batch size of 2048.\nImportance of evaluation frameworks An appropriate evaluation framework is crucial for training the base model. In early experiments, we used OpenCompass to evaluate benchmarks across all domains, but metric variations lacked consistency as training progressed. Upon reviewing the evaluation framework, we found errors in extracting the base model's completion results. We determined that EvalPlus is better for code evaluation, while Qwen2.5-Math's evaluation tool is optimal for mathematics.\nAppropriate Utilization of Synthetic Data Synthetic data can enhance model metrics during pretraining. Initially, we incorporated synthetic data at a certain ratio to achieve higher scores in the base model. However, these higher-scoring models did not necessarily outperform those without synthetic data after the same SFT process. This may be because synthetic data often has lower perplexity, creating a distribution gap with web data, which affects model convergence. Therefore, we only introduce synthetic data during the annealing"}, {"title": "5.2 Small Multimodal Language Model", "content": "We evaluated the performance of InfiR-VL-1.6B on MMMU to test the general vision reasoning abilities, and then evaluated its reasoning capabilities on ScreenSpot and AndroidWorld. Table 3 compares the performance of InfiR-VL-1.6B with state-of-the-art SMLM. InfiR-VL-1.6B exhibits comparable capabilities while maintaining a smaller model size.\nDuring the training of the InfiR-VL-1.6B, we also have some insights which is helpful for reasoning enhanced MSLMs training.\nCurriculum Learning is crucial when employing pretrained ViT for multimodal training. For tasks such as grounding and OCR, which require detailed image comprehension, it is necessary to unfreeze the ViT while keeping LLM backbone frozen during training. End-to-end training may lead to early overfitting in the domain-specific dataset.\nDomain-Specific Reasoning Capabilities are not strongly dependent on model sizes. Smaller models can achieve the required reasoning abilities if they are trained with suitable domain-specific datasets and paired with a backbone that possesses reasoning capabilities. (Liu et al., 2025)"}, {"title": "6 Conclusion", "content": "In conclusion, this paper demonstrates the potential of small language models (SLMs) and multimodal small language models (MSLMs) to provide efficient and accessible AI solutions. By developing novel training pipelines, we show that compact models can achieve competitive reasoning capabilities while significantly reducing computational costs and addressing privacy concerns. Our proposed models, InfiR-1B-Base, InfiR-1B-Instruct, and InfiR-VL-1.6B, achieve state-of-the-art performance, underscoring the feasibility of deploying these models on edge devices for real-world applications. This research paves the way for more sustainable and inclusive AI development, promoting broader adoption and innovation in the field."}, {"title": "7 Limitation", "content": "Due to constraints in computational resources and time, our experiments were primarily conducted on standard benchmarks. Our research focuses on the technical aspects of traditional information extraction tasks, without addressing social or security issues. We adhere to ethical standards by avoiding sensitive data or applications. Although the results are promising, the method's generalization ability in real-world scenarios require further exploration. This limitation presents opportunities for future research to validate and expand the method's applicability under more complex and diverse conditions."}, {"title": "A General Reasoning Instruction Tuning", "content": ""}, {"title": "A.1 Collection of high-quality datasets", "content": "We used different datasets to perform SFT on the Llama-3.2 1B model with the same parameter settings and sampling size. Then, we compared the fine-tuned models' scores on MMLU and selected the datasets used by the model with the highest MMLU score. We experimented with Infinity-Instruct, orca-agentinstruct-1M-v1, tulu-3-sft-mixture, and WebInstructSub, and ultimately selected Infinity-Instruct and orca-agentinstruct-1M-v1."}, {"title": "A.2 Data Synthetic Pipeline", "content": "After collecting high-quality opensource data, we found that the model still lags behind the state-of-the-art (SOTA). Therefore, we constructed two synthetic data pipeline to achieve better performance. One for reasoning and the other for commonsense.\nReasoning Data This pipeline can be divided into two main steps: seed data preparation and inference-guided rejection sampling.\nSeed Data Preparation: We utilize queries related to reasoning from Infinity-Instruct as our seed data. Rejection Sampling: After obtaining the seed queries, we incorporate \"step by step\" prompting to activate the reasoning capabilities of large language models, thereby generating responses with reasoning steps. For each query, we select the highest-scoring samples using a reward model. Finally, we apply certain rules (e.g., checking if the response includes reasoning steps) to further filter the samples.\nKnowledge Data This pipeline consists of four steps: evaluation, seed data preparation, query enhancement, and rejection sampling.\nEvaluation: We construct an offline evaluation set encompassing several domains. We observe the performance gap between our model and the state-of-the-art models on this set. Domains with significant gaps are prioritized for optimization.\nSeed Data Preparation: Similarly, we use Infinity-Instruct as our seed data. Each query in Infinity-Instruct is labeled and of high quality. By applying rules, we associate key domains identified during evaluation with labels in Infinity-Instruct, thereby identifying seed data that can enhance commonsense capabilities.\nQuery Enhancement: For long-tail domains where seed data may be insufficient, we expand queries using large language models (LLMs) based on the seed data. During this rewriting process, we retrieve context from a corpus and include the standard response corresponding to the query in the prompting to help the LLM generate more valuable queries.\nRejection Sampling: Once queries are obtained, we employ LLMs to generate responses, as is common in synthetic data pipelines. Subsequently, we use an open-source reward model to perform rejection sampling."}, {"title": "B Mathematical Reasoning Instruction Tuning", "content": ""}, {"title": "B.1 High-quality Mathematical Data", "content": "In the SFT stage, we evaluated the quality of public datasets\u2014including dart-math-hard (Tong et al., 2024), MATH-plus (Yue et al., 2024), MetaMathQA (Yu et al., 2023), orca-math-word-problems-200k (Mitra et al., 2024b), and ScaleQuest-Math (Ding et al., 2024)\u2014through experiments on the Llama-3.2-1B baseline model. Among these, the ScaleQuest-Math dataset achieved 69.45% on GSM8K and 42.20% on MATH, outperforming the Llama-3.2-1B-Instruct model.\nBased on Qwen2.5-Math-1.5B-Instruction model, we trained a RL model to regenerate ScaleQuest-Math answers, thereby enabling step-by-step inference. Additionally, we employ the Qwen2.5-Math-RM-72B model for rejection sampling, which evaluates the inference quality and intermediate steps of the generated query-response pairs."}, {"title": "B.2 Mathematical Data Compression", "content": "We utilized the Llama3.3-70B-Instruct model to annotate the difficulty of the ScaleQuest-Math dataset, following the MATH benchmark. The annotations included:\n\u2022 Difficulty Labels: Very easy, easy, medium, hard, and very hard\nWe performed sampling experiments on the annotated ScaleQuest-Math dataset, drawing inspiration from Dart-Math (Tong et al., 2024)'s methodology. The experiments were structured as follows:\n\u2022 Group A: Combined \"very easy\" and \"easy\" difficulty labels, comprising approximately 483,000 items\n\u2022 Group B: Combined \"medium\", \"hard\", and \"very hard\" difficulty labels, comprising around 350,000 items\nBoth groups underwent experiments under identical setups. We found that on the easier GSM8K task, the performance of the two groups was similar (61.33% for group A and 60.5% for group B). On the more challenging MATH task, the data from group B achieved 36.64%, outperforming group A by 13%. Notably, using only 35% of the dataset volume from group B nearly matched the performance of the full dataset on both GSM8K and MATH, highlighting the effectiveness of difficulty-based data compression."}, {"title": "C Code Reasoning Instruction Tuning", "content": ""}, {"title": "C.1 High-quality Code Data", "content": "To identify high-quality code datasets, we conducted a comprehensive evaluation using several code datasets to perform Supervised Fine-Tuning (SFT) on the Llama-3.2 1B model. Specifically, we fine-tuned the model with a learning rate of 2 \u00d7 10-5 and a batch size of 32. Each dataset was used to fine-tune the model for 5 epochs, and the intermediate models were saved for evaluation across various benchmarks. The detailed evaluation results are presented in Table 5. The Llama-3.2 1B model fine-tuned by the combination of opc-sft-stage2 and ScaleQuest-Code shows the best overall performance on the MBPP and HumanEval, which demonstrates the relatively high quality of the code datasets."}, {"title": "C.2 Synthetic Code Data", "content": "To enhance the diversity and quality of the post-training dataset, we implement a pipeline for synthesizing code-related instruction data from validated sources. We leverage a curated subset of the Opencoder annealing dataset, comprising high-quality code snippets previously validated during the annealing stage. The seed dataset consists of multilingual code snippets with English comments. The code data are then synthesized using the Qwen2.5-Coder-Instruct-32B model, with prompts designed to generate task"}, {"title": "D Reasoning Enhancement with Long CoT"}]}