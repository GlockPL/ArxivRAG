{"title": "Improving Analog Neural Network Robustness: A Noise-Agnostic Approach with Explainable Regularizations", "authors": ["Alice Duque", "Pedro Freire", "Egor Manuylovich", "Dmitrii Stoliarov", "Jaroslaw Prilepsky", "Sergei Turitsyn"], "abstract": "This work tackles the critical challenge of mitigating \"hardware noise\" in deep analog neural networks, a major obstacle in advancing analog signal processing devices. We propose a comprehensive, hardware-agnostic solution to address both correlated and uncorrelated noise affecting the activation layers of deep neural models. The novelty of our approach lies in its ability to demystify the \"black box\" nature of noise-resilient networks by revealing the underlying mechanisms that reduce sensitivity to noise. In doing so, we introduce a new explainable regularization framework that harnesses these mechanisms to significantly enhance noise robustness in deep neural architectures, obtaining over 53% accuracy improvement in noisy environments, when compared to models with standard training.", "sections": [{"title": "1 Introduction", "content": "As the applications of artificial intelligence (AI) continue to surge, the computational power required to sustain this growth is leading to increasingly high energy consumption and a corresponding significant carbon footprint [1, 2]. This escalating environmental impact necessitates the exploration of innovative solutions to enhance computational efficiency [3]. Among these emerging approaches, analog neural networks have gained attention due to their potential to offer faster processing speeds with lower energy requirements, thereby addressing the critical need for sustainable AI development [4, 5].\nHowever, analog circuits have a major drawback: they are naturally much more sensitive to noise compared to their digital counterparts. Noise in neural networks is not a particularly new topic of study, and extensive solutions have been proposed to overcome noisy inputs [6] and noisy labels [7]. However, since the majority of networks in literature run on digital devices, these types of noise are external to the network itself, failing to address a type of noise that greatly affects analog networks in particular, hardware noise.\nWhile it is well-recognized that hardware noise is a significant source of uncertainty in analog networks [8], it remains a relatively underexplored central topic. Ref [9] offers a broad analysis of noise effects across various architectures. However, the proposed solutions involve architectural modifications that result in increased hardware complexity. In contrast, [10] provides valuable insights into hardware noise but focuses on device-level solutions, which limit their general applicability. Ref. [11] demonstrates promising results by employing a device-agnostic approach during training. Nonetheless, that study lacks detailed explanations regarding the mechanisms behind the observed improvements.\nIn this work, we examine the impact of additive noise on feed-forward networks, along with the underlying mechanisms that contribute to the effectiveness of noise-aware training. In parallel, we"}, {"title": "2 Noise in Neural Network Models", "content": "Building upon the classification proposed by Ref. [9], we categorize noise as either correlated or uncorrelated in this work. Noise is considered correlated when all neurons within a layer experience the same perturbation simultaneously, typically arising from variations in shared physical components, such as temperature, supply voltages, or laser sources. Conversely, uncorrelated noise refers to unique perturbations affecting each neuron independently, though these perturbations are drawn from the same underlying probability distribution. Consistent with other works in the literature, we model noise as being drawn from a zero-mean Gaussian distribution, with variance determined by the specific hardware characteristics.\nIn this study, we focus on noise introduced after the activation functions (Figure 1) rather than within the weights. This is motivated by the fact that analog activation functions are in general more complex than passive connections and more likely to introduce noise. When noise is injected into a network trained to achieve high accuracy, a significant drop in performance is expected. This highlights the disparity between simulated and actual performance when these networks are deployed on noisy physical devices. Among the solutions discussed in the literature, noise-aware training [11] assumes that neural networks can develop resilience to noise if trained with exposure to it. During training, noise similar to what is expected during inference is injected, resulting in enhanced robustness. This method has shown promising results, leading to nearly a four-fold improvement in forecasting accuracy for a time-series prediction task. However, there is still a lack of understanding regarding the mechanisms that drive these improvements. Specifically, it remains unclear how neural networks adjust to mitigate both correlated and uncorrelated additive noise so effectively."}, {"title": "3 Designing Explainable Regularizers", "content": "Consider activations $a_j^{(l-1)}$ of $j^{th}$ neuron in layer $l-1$ are corrupted by noise $n_j^{(l-1)} \\sim \\mathcal{N}(0, \\sigma_n^2)$.\nIn this scenario, these activations interact with weights $w_{ij}^{(l)}$ and biases $b_i^{(l)}$ to form pre-activations $z_i^{(l)}$:\n$z_i^{(l)} = \\sum_j w_{ij}^{(l)}(a_j^{(l-1)} + n_j^{(l-1)}) + b_i^{(l)},$ (1)\nwhich can be written as\n$z_i^{(l)} = \\sum_j w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)} + \\sum_j w_{ij}^{(l)} n_j^{(l-1)}.$\n(2)\nThe last term corresponds to the total noise contribution on pre-activation $z_i^{(l)}$, or the error factor. If noise is correlated, then $n_j = n$ for all $j$, so the error factor becomes:\n$\\sum_j w_{ij}^{(l)} n_j^{(l-1)} = n^{(l-1)} \\sum_j w_{ij}^{(l)} .$\n(3)\nThis means that the error factor can be effectively eliminated if we make $\\sum_j w_{ij}^{(l)} = 0$. In other words, correlated noise can be mitigated by ensuring that each row of the weight matrices sums to zero. This allows us to formulate the first explainable regularizer for additive correlated noise, which enforces this mathematical condition. By introducing a regularization term into the loss function during training, we can effectively impose this constraint, leading to improved noise robustness. The modified loss function becomes:\n$L_{osse} = L_{(y_{pred}, y_{exp})} + \\lambda \\sum_{l=2}^L \\sum_i |\\sum_j w_{ij}^{(l)}|^2,$\n(4)\nFor visual inspection, Fig. 2 illustrates the effects of regularization on the distribution of weights per row, compared to the original neural network model trained without noise. The graphs depict the mean (solid line) and standard deviation (shaded areas) of each row within the weight matrices, with layers ordered by ascending mean values. As predicted by Eq. 4, all rows-except those of the input weight matrix-have their mean values driven toward zero. This exception arises because noise"}, {"title": "4", "content": "only affects activations, meaning the input weights do not directly interact with it. In the following section, we will present evidence demonstrating the improvement in accuracy achieved through this straightforward regularization strategy.\nFor uncorrelated noise, canceling the error factor becomes more complex. Instead of attempting to eliminate it entirely, we aim to mitigate its propagation to subsequent layers. Given that all individual noise contributions within layer $1 - 1$ have zero mean Gaussian PDF, the error factor is also Gaussian with zero mean. Furthermore, since for a known $a_j^{(l-1)}$ the first two terms of equation 2 are deterministic, the pre-activation $z_i^{(l)}$ can be interpreted as a random Gaussian variable $z_i^{(l)} \\sim \\mathcal{N}(\\mu, \\sigma^2)$, whose mean is:\n$\\mu = \\sum_j w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)} ,$\n(5)\nFrom the known pdf $p_z(z)$ of $z_i^{(l)}$, and given that $f(\\cdot)$ is monotonic, one can derive the PDF $p_a(a)$ of $a_i^{(l)} = f(z_i^{(l)})$ from [12] as:\n$p_a(a) = \\frac{p_z[f^{-1}(a)]}{|f'(a)|},$\n(6)\nFig. 3 a) shows the PDF $p_a(a)$, given $z_i^{(l)} \\sim \\mathcal{N}(\\mu, 0.2)$ for different values of $\\mu$. It's observed that input distributions centered further from the origin lead to output distributions of much-reduced variance. Intuitively, this occurs because noise fluctuations in the saturated regions of the activation function (where the derivative is close to zero) are largely suppressed and not transmitted to the output. Thus, in analog neural networks, if a set of weights and biases can shift the operating point of pre-activations toward these saturated regions, noise-induced perturbations are less likely to propagate to subsequent layers. This mechanism effectively reduces the impact of uncorrelated noise, enhancing the network's robustness to such perturbations. This also can give some design ideas for the activation function shape when this freedom is available.\nIt is important to point out that, in the output layer, no additional nonlinearities are available to counteract noise contributions, which will inevitably affect the readout values. However, we found that this impact can be mitigated by reducing the magnitude of the output weights. In typical scenarios, reducing weights alone does not enhance noise performance, as both the signal and noise are attenuated equally, resulting in no real improvement in the SNR.\nHowever, if the intermediate layers meet the condition where pre-activations operate predominantly in saturated regions, the activation values will also be highly saturated. In our case, which uses the sigmoid activation function, this implies that the majority of activation values will be either 0 or 1. In such a distribution, reducing the output weights does not lead to information loss for activations that are zero-these values constitute a significant portion of the activations in the saturated regime. Thus, noise contributions are reduced without loss of information from the zero-valued activations.\nMoreover, for non-zero activations, which are at the maximum possible value (1 in the case of sigmoid), the output remains resilient to noise. This dual effect-noise suppression for zero activations and noise insensitivity for saturated ones\u2014explains how reducing output weights, when combined with saturated intermediate layers, can mitigate the impact of noise on the final output. We can incorporate these conditions into our training by including new regularizer terms into our loss function, which becomes:\n$L_{osse} = L_{(y_{pred}, y_{exp})} + \\lambda_1 \\sum_{l=2,3} \\sum_i f'(z_i^{(l)}) + \\lambda_2 \\sum_{l=2,3} \\sum_i \\sum_j (w_{ij}^{(l)})^2,$\n(7)\nin which $f'(z_i)$ is the activation function's derivative at $z_i$ and $\\lambda_i$ are tunable hyperparameters.\nThe first regularization term penalizes pre-activations that fall within regions of high derivative in the activation function, while the second term penalizes large weight magnitudes in all layers except the input layer. The constraint on the hidden layer weights addresses a practical issue encountered during training. When we enforced pre-activations to move away from the origin, the network"}, {"title": "4 Results and discussion", "content": "initially complied by amplifying the weights in the hidden layers. Although this approach effectively shifted the operating point toward the saturated region, it inadvertently amplified noise contributions. This increased noise then \"leaked\" into the linear region of the activation function, propagating through subsequent layers and negating any performance improvement.\nTo prevent this, we apply a light constraint on the hidden layer weights, ensuring that the network does not resort to this suboptimal strategy during training. Fig. 3 b), c), and d) illustrate the distributions of the pre-activations, post-activations, and output weights in the original model, the noise-aware trained model, and the regularized model. These visualizations confirm our hypothesis: the regularized model exhibits a distribution similar to that of the noise-aware model, but with a more pronounced shift toward the non-linear regions of the activation function. This shift is critical, as it reduces the transmission of uncorrelated noise by leveraging the saturation properties of the activation function more effectively than noise-aware training alone.\nHaving established the explainability of the proposed regularization, we now demonstrate that the key features of noise-resilient networks are successfully incorporated by our regularization techniques, yielding significant performance improvements. To validate these claims and assess the overall performance, we evaluate the regularized network on two computer vision tasks using the MNIST [13] and Fashion MNIST [14] datasets. The results highlight the efficacy of the regularization in enhancing noise robustness while maintaining strong task performance. For the sake of reproducibility, all code used in these experiments is available in [15] .\nIn both tasks, the application of regularization effectively eliminated the impact of correlated noise, as shown in Fig. 4a) and (b), resulting in a flat accuracy curve with no observable performance degradation. For uncorrelated noise, substantial improvements were also achieved. On the MNIST dataset, the network experienced only a 4.77% drop in accuracy when subjected to hardware noise with variance as high as $\\sigma^2 = 1.0$, compared to its performance in a noiseless environment. By contrast, a standard network trained without noise-aware techniques showed a dramatic 41.52% accuracy decline under the same conditions. Similarly, in the more challenging Fashion MNIST task, regularization reduced the accuracy drop to 7.22%, compared to 53.86% for the standard trained network. These results demonstrate a significant enhancement in noise robustness. However, this improvement comes with a slight penalty to the maximum achievable accuracy. For MNIST, a"}, {"title": "5 Conclusion", "content": "In this work, we explored the fundamental mechanisms that allow a neural network to achieve resilience to hardware noise without introducing additional architectural complexity, relying solely on optimized weight and bias distributions. Our novel approach, based on the analysis of the pdf evolution throughout the network, offers a valuable design and optimization tool for improving noise robustness. By presenting explainable regularization techniques, we demonstrated an over 53% improvement in accuracy under high levels of hardware noise, underscoring the effectiveness of the method. Importantly, the proposed technique is device-agnostic, offering broad applicability across various analog neural networks affected by additive noise. This generalizability, combined with the simplicity of implementation, positions our approach as a significant advancement for the design of high-performance, noise-resilient neural networks in real-world hardware applications."}]}