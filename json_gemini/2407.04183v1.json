{"title": "Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms", "authors": ["Joshua Ashkinaze", "Ruijia Guan", "Laura Kurek", "Eytan Adar", "Ceren Budak", "Eric Gilbert"], "abstract": "Large language models (LLMs) are trained on broad corpora and then used in communities with specialized norms. Is providing LLMs with community rules enough for models to follow these norms? We evaluate LLMs' capacity to detect (Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's Neutral Point of View (NPOV) policy. LLMs struggled with bias detection, achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting biases (some under- and others over-predicted bias), suggesting distinct priors about neutrality. LLMs performed better at generation, removing 79% of words removed by Wikipedia editors. However, LLMs made additional changes beyond Wikipedia editors' simpler neutralizations, resulting in high-recall but low-precision editing. Interestingly, crowdworkers rated AI rewrites as more neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia editors but often made extraneous non-NPOV-related changes (such as grammar). LLMs may apply rules in ways that resonate with the public but diverge from community experts. While potentially effective for generation, LLMs may reduce editor agency and increase moderation workload (e.g., verifying additions). Even when rules are easy to articulate, having LLMs apply them like community members may still be difficult.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are trained on large corpora and then used within communities that have their own norms. To steer models towards specific norms and values, there is a growing trend of stating high-level rules as prompts. For example, Constitutional AI [1] involves providing the model with a set of rules that it uses to critique and then revise its own outputs. But is providing high-level rules sufficient to steer models toward community norms?\nThe challenge of going from high-level rules-like Wikipedia's Neutral Point of View (NPOV) policy-to specific cases mirrors earlier debates in human-AI interaction and beyond. For instance, Lucy Suchman's 1987 \u201cPlans and Situated Actions\u201d framework [45] contrasted predetermined procedures derived from universal principles (\u201cplans\u201d) with context-dependent actions based on concrete circumstances (\u201csituated actions\u201d). She argued that AI systems execute plans, while humans perform situated actions. This distinction relates to James Scott's [41] concept of \u201cseeing like a state\"-the idea that the large-scale plans of centralized authorities often break down when faced with complex, local realities. The tension between universal plans and situated, local actions is relevant to deploying general-purpose LLMs in communities like Wikipedia. On Wikipedia, many decisions involve messy and contextual judgments [13, 46] rather than straightforward applications of high-level rules. Is providing high-level rules to trillion-parameter models also insufficient for navigating particular NPOV cases?\nWe test how well general-purpose LLMs can be steered to apply NPOV policy.\u00b9 The broader NPOV policy encompasses various complex assessments (e.g., covering viewpoints in proportion to their prominence in reliable sources). Here, we focus on one specific subtask: debiasing language. Specifically, we evaluate large language models on their ability to: (1) detect edits that include non-neutral language and (2) generate NPOV-compliant edits from biased ones. Evaluating LLMs on Wikipedia's NPOV policy is an interesting test case of model abilities for three reasons. First, LLMs are used in widely different communities. An open question is whether these models can be steered-with no task-specific fine-tuning-toward following community policies or guidelines. Second, detecting and generating non-neutral language requires nuance that goes beyond literal meaning. This task probes how well LLMs can apply natural language pragmatics and mirror the nuanced decisions of community members. Third, Wikipedia's NPOV has a unique tension: while clearly-articulated and well-documented in theory, it is complex in practice [36, 46]. Adjudicating neutrality is nuanced. To what extent can LLMs, absent fine-tuning, apply clearly articulated but nuanced community policies?"}, {"title": "1.1 Findings", "content": "To test whether LLMs could apply Wikipedia's NPOV policy, we conducted detection and generation experiments. In both experiments, LLM decisions and edits were compared against an existing corpus of NPOV edits by Wikipedia editors, also known as Wikipedians. In the detection experiment, we used ChatGPT 3.5, Mistral-Medium, and GPT-4 to classify whether edits violated NPOV, varying the amount of policy detail and example decisions provided. In the first generation experiment, we used GPT-4 (using both Zero-Shot and Constitutional AI approaches) to neutralize NPOV-violating edits. We used computational metrics to compare the LLM-neutralized edits to the Wikipedian-neutralized edits. In the second generation experiment, humans gave masked evaluations of LLM and Wikipedian rewrites. We also conducted a qualitative analysis of LLM vs. Wikipedian rewrites to better understand differences in how each neutralizes text. We summarize our main findings and implications below:\n(1) Large language models largely failed at neutrality detection. Across models, prompts, and optimization strategies, performance was low (64% accuracy for the best prompt; the dataset was balanced so random chance accuracy is 50%). Analysis of LLM rationales and errors suggested LLMs relied (sometimes to a fault) on simple heuristics like the presence of a highly subjective adjective. Perhaps modern models still struggle with applying subtle rules to real-world cases without task-specific training.\n(2) Models exhibited biases that persisted across prompts. ChatGPT 3.5 over-predicted edits as biased, and Mistral-Medium did the opposite. GPT-4 was balanced. Pretrained LLMs may internalize distinct priors about what constitutes neutrality that persist across prompts. Understanding model idiosyncrasies is crucial. It is not clear that, when a new model appears on the scene, we will know what its priors are.\n(3) LLMs applied rules in different ways than expert editors. Computational experiments showed LLMs generally did remove words that Wikipedia editors removed in the generation phase. But LLMs also made many other changes. Crowdworkers also rated Constitutional AI (CAI)-generated rewrites as adding unnecessary information more frequently than"}, {"title": "2 RELATED WORK", "content": "We position our work in the context of Wikipedia neutrality research (2.1), automated moderation of Wikipedia (2.2), and broader conversations around the role of general-purpose LLMs in community moderation (2.3). Wikipedia's Neutral Point of View (NPOV) policy, though clearly defined, is often debated in practice [29, 36, 46, 47], making it a compelling test case for LLMs. While prior work tested task-specific models on neutralizing NPOV violations [34, 37], we examine whether general-purpose LLMs can neutralize NPOV violations. This task is timely given two trends. First, LLMs excel at zero-shot annotation [9], motivating work on their ability to apply community policies [2, 7, 21, 22]. We complement this work by testing LLMs on generation in addition to detection, specifically in the Wikipedia context. Second, Wikipedia has historically used automated moderation tools [52], but there are contentious internal debates about the use of ChatGPT [14]. Our study can inform these debates. More broadly, our work speaks to the opportunities and limitations of using general-purpose LLMs to uphold nuanced community norms."}, {"title": "2.1 Wikipedia's Neutral Point of View (NPOV) Policy", "content": "Wikipedia is one of the most visited websites in the world [42]. It is a widely-regarded success in peer production. It is well studied in CSCW and beyond [5, 19, 27, 29, 36, 46, 47]. To support a crowd-sourced encyclopedia editable by anyone, Wikipedia developed many policies, processes, and norms [6]. \u201cNeutral Point of View\u201d (NPOV) is one of the most central. For example, NPOV is listed first among the three core content policies [51]. According to Wikipedia.org [55]:\n\"All encyclopedic content on Wikipedia must be written from a neutral point of view (NPOV), which means representing fairly, proportionately, and, as far as possible, without editorial bias, all the significant views that have been published by reliable sources on a topic.\u201d\nJimmy Wales (a co-founder of Wikipedia) has said [48] [29, p. 5]:\n\u201cSo how do we do this? ...How does it work? ... So the biggest and the most important thing is our neutral point of view policy. This is something that I set down from the very beginning, as a core principle of the community that's completely not debatable.\u201d\nNPOV is crucial for Wikipedia's content quality. Wikipedia's iterative revision process improves neutrality over time [11]. This is due in part to the role of NPOV in defending against disinformation, low-quality information [28, 44], and delineating fringe points of view [44]. The result is (1) a public perception that Wikipedia editors are committed to unbiased information and (2) increased reader trust [8].\nBecause of NPOV's importance, Wikipedia goes to great lengths to codify exactly what a \u201cneutral point of view\u201d means. Specifically, NPOV is described by a set of principles\u00b2 [54]. These principles are expounded and clarified in numerous FAQs, tutorials, and examples. Editors are trained in NPOV before they can edit. And if an article violates NPOV, then it is flagged as \u201cNPOV Disputed\u201d so that an editor can bring it into compliance. As a community norm, NPOV is clearly defined and often invoked in day-to-day community activity. The \u2018rules' are clear.\nHowever, despite its clear articulation, NPOV has always been complicated in practice. Reagle claims that \u201cin the Wikipedia culture, the notion of \u2018neutrality' is not understood so much as an end result, but as a process\" [36]. Wikipedia articles often go through many rewrites, with debates occurring as to what constitutes neutrality. For example, over three years the Wikipedia entry for 'clean coal technology' had 39 distinct facts and 142 different rewrites of these facts [46]. Norm-consistent Wikipedia content is often the result of conflict, coordination, and deliberation-more complex than lone editors applying rules in isolation [19, 20, 29, 46, 47]. Past work [19] estimated that 40% of Wikipedia edits are dedicated to coordination between Wikipedians (e.g., achieving consensus). And despite NPOV being 5 principles, there are many pages and debates dedicated to this topic [27].\nWikipedia's NPOV policy is an interesting test case of general-purpose LLM abilities because of a tension: It is both clear and nuanced. On one hand, NPOV is clearly defined and documented. In theory, augmenting LLMs with the text of NPOV guidelines should enable LLMs to apply these guidelines effectively. But even though NPOV is clear in theory, it is complex in practice. To what extent, then, can general-purpose LLMs-absent any task-specific training\u2014be steered to follow nuanced community guidelines such as NPOV?"}, {"title": "2.2 Automated Approaches to NPOV and Wikpedia Moderation", "content": "Researchers have developed algorithms to understand and ameliorate problems with Wikipedia article content (e.g., [3, 12, 30, 39, 40, 49]). Most relevant to the present work, several papers specifically tested whether models can neutralize NPOV-violating edits. Recasens et al. [37] experimented on a subset of NPOV-violating edits where the bias was due to one word. Both humans and their system struggled to guess the biased word (37% and 34% accuracy, respectively). Building on this result, Pryzant et al. [34] collected a similar dataset of NPOV-violating edits and their corrections called the Wikipedia Neutrality Corpus (WNC). We use this dataset in our work. Both projects [34, 37] conducted experiments on a subset of edits in which the bias was attributable to only one word. With the WNC corpus, a sequence-to-sequence system was trained to detect the NPOV-violating word and then predict its replacement, achieving 46% generation accuracy (measured as the proportion of generations that matched Wikipedia-editor rewrites).\nOur study departs from these two studies in two ways. First, we conduct experiments on a more representative set of NPOV-violating edits (i.e., not the one-word subset), which is arguably a harder task. Second, we test whether general-purpose models, absent any task-specific training, can perform this task. Both Recasens et al. and Pryzant et al. show that applying NPOV in a way that matches how expert editors behave is difficult for both laypeople and task-specific NLP systems. In our work, we can answer the question: How well can new generations of general-purpose LLMs neutralize text?\nWikipedia already uses automated moderation tools [53], and our results can inform the benefits and risks of using general-purpose LLMs. Past and current services include WikiTrust [52], an extension that helped editors detect untrustworthy edits. The ORES project used machine learning to rank the usefulness of edits [12]. The Automod project was an abuse detection model [35]. More recently, Wikipedia editors have debated the utility of ChatGPT for generating Wikipedia edits [14]. By systematically evaluating NPOV detection and generation from general-purpose LLMs, our results can add evidence to this debate, possibly informing community practice."}, {"title": "2.3 Pre-Trained LLMs for Community Content Moderation", "content": "Given how much content there is to moderate, platforms often employ automatic content moderation tools such as Reddit's AutoMod [16]. Because general-purpose LLMs show remarkable zero-shot [9] and few-shot [4] capabilities even without fine-tuning (i.e., training the model on task-specific data), it is reasonable to think general-purpose LLMs would be effective content moderators. The zero-shot and few-shot capabilities of LLMs are useful, since fine-tuning requires resources (compute, instances) that some communities may not have. Hence, online communities may rightfully wonder how well these off-the-shelf LLMs can apply their rules for online moderation. Our work engages with this question.\nSeveral studies have explored whether general-purpose LLMs can apply highly specific sets of online community rules without fine-tuning. One study [21] tested whether LLM moderators can detect rule-violating posts across nine subreddits when the rules of these subreddits are provided to the LLM. Their system achieved a high true-negative rate (92%) but a low true-positive rate (43%). A similar study found that ChatGPT 3.5 was 64% accurate in predicting subreddit moderation decisions [23]. Cao et al. tested how well LLMs could detect rule violations of r/AskHistorians, with precision and recall showing variance depending on the specific rule [7]. They also asked moderators what would be an acceptable precision and recall of models for the models to be useful. In general, moderators indicated models should have recall and precision above 70%. Although not subreddit-specific, Barbarestani et al. found ChatGPT largely agrees with both crowds and experts when detecting inappropriate words in Reddit comments [2]. However, when considering errors,"}, {"title": "3 LLM BIAS DETECTION", "content": "To understand whether general-purpose LLMs can classify edits as neutral or biased, we conducted classification experiments on a balanced sample of NPOV-violating edits and NPOV-compliant rewrites. We find that across models and prompts, models largely failed to distinguish between neutral and biased edits. Models exhibited contrasting failure modes, and model explanations suggest they appeared to rely on heuristics such as the presence of a highly subjective adjective."}, {"title": "3.1 Dataset", "content": "The data for our detection and generation experiments come from the Wikipedia Neutrality Corpus (WNC) [34]. The WNC is a collection of edit pairs: A biased edit (an edit flagged by an editor for violating the NPOV) and a neutral edit (the rewrite an editor made to ensure NPOV compliance). While NPOV is a set of multiple rules, the WNC consists of edits that violated a particular subset of NPOV-biased language (framing, epistemological, and demographic bias). This stipulation is useful for testing the abilities of LLMs. Unlike other NPOV rules (e.g., those concerning citations), a violation of neutral language does not require access to external information. For detection experiments, we use the subset of these edits where the data also has tags for which topic an edit belongs to."}, {"title": "3.2 Experiment Setup", "content": "We conducted a multi-model prompt experiment (N = 5,348 annotations) where the task was to classify if a given Wikipedia edit was biased or neutral. Our two experimental factors were (Factor 1) the definition given to an LLM on what constitutes neutrality, and (Factor 2) whether or not we provided examples (i.e., few-shot or zero-shot). The rationale for varying definitions is that we are interested in whether augmenting LLMs with community-specific definitions of neutrality increases adherence to community-specific norms. The rationale for our second factor is to test whether providing example decisions from a community helps models adhere to community norms. Appendix B has all the prompts we used in this paper."}, {"title": "3.2.1 Factor 1: Definitions Provided.", "content": "The minimal prompt relies on LLMs' learned knowledge only, providing no Wikipedia-specific definitions or examples. This condition reveals how well general-purpose LLMs apply NPOV without additional context. The NPOV prompt provides LLMs with Wikipedia's verbatim NPOV guidelines. Comparing the performance between the minimal and NPOV conditions estimates how much simple prompting can align general-purpose notions of neutrality with community-specific norms. Since the WNC focuses on neutral language violations, a subset of NPOV violations, we created a third prompt condition: NPOV-Scoped. This prompt gives LLMs Wikipedia's NPOV guidelines and additional language from Pryzant et al. [34] on the specific types of non-neutral language (framing, epistemological, and demographic bias) in the dataset. Comparing NPOV-Scoped to NPOV reveals how much guideline specificity improves classification accuracy."}, {"title": "3.2.2 Factor 2: Examples Provided.", "content": "Few-shot learning can improve model performance [4]. We test whether augmenting the model with editors' prior decisions increases accuracy. In the few-shot condition, we select a random sample of 10 edits and their labels from the same topic, as neutrality norms may differ by topic due to factors like similar editors or content."}, {"title": "3.3 Experiment Results", "content": "All models performed poorly and prompts made little difference . Across all conditions, the accuracy was 0.58. Averaging across models, ChatGPT 3.5 performed the worst with accuracy at 0.55 and GPT-4 performed the best at 0.61. The top combination was GPT-4 with a zero-shot NPOV prompt (0.63). For each of the models, there were no statistically significant differences in accuracy between prompt conditions, based on p-values from two-tailed permutation tests. Models were more accurate for biased edits (0.63, 95% CI = [0.61, 0.65]) than neutral edits (0.53, 95% CI =[0.51, 0.55]), p < 0.001 from two-tailed permutation tests.\nWe ran a logistic regression (Appendix Table 7) modeling accuracy as a function of definitions, few-shot or zero-shot, model, topic, edit word count, and normalized edit distance tertile. Normalized edit distance is the edit distance between NPOV-violating and compliant edits, divided by the longer string's length. Higher values indicate that it took more characters to bring the NPOV-violating"}, {"title": "3.4 LLM Self-Optimizations", "content": "We experimented with more advanced reasoning techniques and LLM self-optimizations (see Appendix B for these LLM-assisted prompts). First, we selected our top model (GPT-4) and the"}, {"title": "3.5 Model-Level Analysis", "content": "We based our evaluation on a balanced dataset of neutral and biased edits. In real-world applications, models would encounter unbalanced data. We examined the error distribution of different models. We find that not all models failed the same way . ChatGPT 3.5 was far more likely than other models to predict that edits were biased when they were neutral. Mistral-Medium, on the other hand, erred in the opposite direction. It over-predicted neutral edits. GPT-4 was"}, {"title": "3.6 Edit-Level Analysis", "content": "We calculated the edit-level probability of correct classification (POCC) to identify which edits are easier to detect . POCC is the probability of a model correctly classifying an edit across all models and prompts. POCCs were bimodal , indicating that edits tend to be either easy or hard to classify.\nWe conducted a qualitative analysis of \u2018easy' vs. \u2018hard' biased edits (i.e., top or bottom POCC quartile). Top-POCC biased edits tend to have some highly subjective word that alerts models these edits are biased. See below (emphasis added).\n\u2022 \u201cone of the central characters of the novel, akili kuwale, provides a brilliant demonstration of this change and its implications, together with excellent characterization.\u201d\n\u2022 \u201cenvironmentalists complain that before shipbreaking began there in june 1983 the beach at alang was pristine and unspoiled.\u201d\n\u2022 \u201ccolchester has a proud tradition of its citizen volunteers serving in the territorial army.\u201d\nBy contrast, low-POCC biased edits generally do not contain an overtly subjective adjective.\n\u2022 \u201cthe bill protects americans against discrimination based on their genetic information when it comes to health insurance and employment.\u201d\n\u2022 \u201cconfucianism (; pinyin: rxu ; literally means \"the school of the scholars\", see also names for confucianism for details) is an east asian ethical and philosophical system derived from the teachings of the early chinese sage confucius.\u201d\n\u2022 \u201cthey can see far over the great plains of illinois and across lake michigan on a clear day.\u201d"}, {"title": "3.7 Explanation-Level Analysis", "content": "We examined if interpretable features of LLM explanations correlate with accuracy (Appendix C.2 for variables and standardized logistic regression coefficients). We focused on three features:\n\u2022 Complexity: (e.g., word count, LIWC [33] cognitive processing words, conjunctions [24])\n\u2022 Prompt and rule reference: (e.g., SBERT [38] semantic similarity to prompt, NPOV-related words)\n\u2022 Subjective language (number of LIWC emotion words)"}, {"title": "4 LLM NEUTRALITY GENERATION: COMPUTATIONAL EVALUATION", "content": "We evaluated LLMs' ability to generate unbiased edits from biased ones using both automated computational metrics and human experiments. Our computational evaluation revealed that AI editors are high-recall but low-precision compared to Wikipedia editors. LLMs generally neutralize non-neutral words but also make many more changes than humans do. We also found that models apply NPOV differently from human editors. While human editors make more removals than additions to neutralize edits, general-purpose LLMs have the opposite tendency."}, {"title": "4.1 Experiment Setup", "content": "We used our top-performing model (GPT-4) and the NPOV-Scoped\u00b3 prompt to neutralize biased Wikipedia edits. Specifically, we instructed the model to revise an NPOV-violating edit, varying two factors. We varied (Factor 1) whether we conducted this generation using zero-shot reasoning or Constitutional AI (CAI). In the latter, the model first critiqued why an edit violated NPOV and then used that critique to revise the edit. Since initial results suggested LLMs changed more words than human editors, we experimented with (Factor 2) instructions to only edit what was necessary. We conducted this generation on 200 edits, varying both prompt type (CAI or Zero-Shot) and whether to add constraining instructions (Yes or No), yielding 800 generations. For all conditions, we set the temperature to zero to further reduce extraneous changes."}, {"title": "4.2 Measures", "content": "We evaluated both the intensity and accuracy of AI changes. To measure intensity, we computed (1) 'AI Edit Distance' as the normalized edit distance between the AI-neutralized edit and the original NPOV-violating edit, (2) the word count of AI edits, and (3) the number of changed words (excluding stopwords) in each edit by summing additions and deletions (this process discussed more below). We compared these intensity metrics to analogous metrics for human-modified Wikipedia edits to determine if AI makes more or fewer changes than human editors.\nWe evaluated the accuracy of AI edits by comparing them to Wikipedia editors' changes, which we treated as the gold standard. Our approach is similar to that of Pryzant et al. [34]. We first computed differences (diffs) between the original biased edits (wBias), human-modified edits (wModH), and AI-modified edits (wModAI) using Python's difflib library. 'Diffs' index substrings that were removed or added. We focus on removals as a measure of neutrality adjustment. After preprocessing diffs (lowercasing, stripping punctuation, removing stopwords) we defined sets A (words removed by AI) and B (words removed by humans). From these, we define true positives (TP) as words removed by both AI and humans (|A \u2229 B|); false positives (FP) as words removed by the AI that were not removed by humans (|A \u2013 B|); and false negatives (FN) as words removed by humans that were not removed by AI (|B \u2013 A|). We calculated various metrics based on these values . We also computed the BLEU score (a standard machine translation metric) of AI edits, using human edits as the reference text. We treated undefined precision metrics as 0, undefined recall metrics as missing, and used BLEU-score smoothing from [25]. For robustness, we re-analyzed the data under 32 analysis decisions , yielding results similar to those in Table 4."}, {"title": "4.3 Experiment Results", "content": "Finding 1: AI changes more than humans. LLMs changed more than humans (Figure 5). AI edit distance (M = 0.34, SD = 0.21) was larger than human edit distance (M = 0.15, SD = 0.13), t(799) = 24.06, p<0.001, Cohen's d = 0.85. Considering both additions and removals, AI made more changes than humans (M = 13.88, SD = 12.91) than humans (M = 3.79, SD = 4.01), t(799) = 22.10, p<0.001, Cohen's d = 0.78. Driven by CAI, AI edits were longer (M = 33.53, SD = 21.19) than human edits (M = 27.53, SD = 14.98), t(799) = 10.87, p<0.001, Cohen's d = 0.38. See Appendix Table 9 for regressions and Table 3 for descriptive statistics. CAI makes more changes than zero-shot and constraining instructions are more effective in reducing changes for CAI than for zero-shot.\nFinding 2: Humans are more likely to remove while AI is more likely to add. We find different editing patterns for AI vs human editors . AI additions (M = 8.04, SD = 9.33) were higher than removals (M = 5.83, SD = 4.61), t(799) = 8.86, p<0.001, Cohen's d = 0.31. Meanwhile, human removals (M = 2.40, SD = 2.31) were higher than additions (M = 1.39, SD = 2.14), t(199) ="}, {"title": "5 LLM NEUTRALITY GENERATION: HUMAN EVALUATION", "content": "AI editors make more changes than human editors. But what do people prefer? We conducted a controlled experiment to address this question\u2074. Participants were shown one edit that initially violated NPOV and two sets of (masked) annotated revisions\u2014one from a Wikipedia editor, and one from an AI condition (Zero-Shot and CAI, both with constraints). Participants rated rewrites on: (1) bias reduction; (2) adding/removing information from the original; (3) fluency. Participants chose AI edits as being more neutral and fluent than human ones."}, {"title": "5.1 Experiment Setup", "content": "Participants. We recruited 147 participants through Prolific, a crowdsourcing platform. Each participant completed 10 trials. Participants received $3.33. Pilot tests indicated the experiment would take 11 minutes. This sample size was larger than that required by Orme's rule of thumb [31]"}, {"title": "5.2 Experiment Results", "content": "Label Quality. Participants completed the task in a median of 19 minutes (M = 21, SD = 9). The median correctness for the three training questions was 2 (M = 2, SD = 1). Agreement was generally high. The proportion of respondents agreeing with the majority answer for each question was as follows: neutrality (Mdn = 0.75, M = 0.74, SD = 0.13), fluency (Mdn = 0.68, M = 0.69, SD = 0.13), additions (Mdn = 0.72, M = 0.72, SD = 0.12), and removals (Mdn = 0.64, M = 0.66, SD = 0.11). We conducted robustness checks  where we re-analyzed data after excluding participants with duration below the first quartile and including only those participants who got every training module question correct. Main results are highly similar to using the full dataset. See Appendix E.1 for participant demographics.\nNeutrality. See Table 5 and Figure 7b for raw data and Figure 7 for logistic regression odds ratios. Participants rated AI edits as more neutral than human edits in 70% of zero-shot choices (95% CI = [67, 73]) and 69% of CAI choices (95% CI = [66, 72]), p < 0.0001 for both by two-tailed binomial tests and conditional logistic regressions. After our initial (surprising) results, we conducted a pilot study to rule out the possibility that the preference for AI edits was simply an artifact of forcing participants to choose between edits rather than allowing a \u201cboth equal\u201d option. We find the proportional gap (probability of choosing the AI-generated edit over the human one) replicated, but in 21% of cases, the options were rated as comparable . At the end of our"}, {"title": "Fluency.", "content": "Participants rated AI edits as more fluent (Table 5; Figure 7) than human edits in 65% of zero-shot choices (95% CI = [61, 68]) and 57% of CAI choices (95% CI = [53, 60]), p < 0.001 for both by two-tailed binomial tests and conditional logistic regression."}, {"title": "Additions and removals.", "content": "Participants evaluated whether each rewrite added or removed information from the original edit, excluding changes that increased NPOV compliance (Table 5). Participants were shown human and AI edits simultaneously but answered questions for each separately. We analyzed judgments of human edits in two ways. First, aggregating across both Al matchups, we found CAI edits added information more frequently (34%, 95% CI = [31, 38]) than human edits (28%; 95% CI = [26, 31]), x\u00b2 p < 0.001. However, when evaluating human edits alongside CAI edits, participants reported a higher information addition rate (33%, 95% CI = [30, 37]) than when evaluating human edits alongside zero-shot edits (23%; 95% CI = [20, 26]). This led to a directionally similar but not statistically significant difference between human and CAI additions at the matchup level . High-addition CAI edits may have heightened participants' awareness or changed perceptions of additions in human edits."}, {"title": "6 LLM NEUTRALITY GENERATION: QUALITATIVE ANALYSIS", "content": "To reconcile LLMs' low detection performance with crowdworkers' high preference for AI edits, we conduct a qualitative analysis of AI-generated vs human edits to better understand differences. We discuss cases (Figure 8) that exemplify recurring patterns and significant implications that emerged upon inspection.\nAI is high-recall and low-precision compared to Wikipedia editors; models generally change the text that editors change, and much more. See Cases 4 (Figure 8e), 5 (Figure 8b), and 9 (Figure 8c). The human editor made a few changes. But both AI editors made many changes. However, many of these changes were to more than just NPOV-violating words. LLMs also make various grammatical and stylistic tweaks that may have influenced participants' judgments. We call the sum of these changes \u2018NPOV+'.\nIn some cases, the AI model correctly found the NPOV-violating string and then replaced it with one that was no better. In Case 1 (Figure 8a), Zero-Shot and the human editor essentially agree. However, we see that CAI has removed the word \u201cnoted\u201d and added a long text string that is just as NPOV-violating as \u201cnoted\u201d.\nIn other cases, AI models arguably applied NPOV more faithfully than human editors, possibly due to varying community norms on what would constitute neutrality. Case 10 (Figure 8d) is particularly interesting. The original edit said herbal medicine \u201ccan treat\u201d conditions. A Wikipedia editor, possibly from the herbal medicine community, changed the edit to say the clinic \u201ctreats\u201d (implying more certainty in herbal medicine's abilities). In contrast, the LLM edits reflected a degree of doubt in herbal medicine, with CAI changing the edit to say that the clinic \u201cclaims to treat\u201d different health conditions and Zero-Shot saying the clinic \u201cprovides services\u201d (with the possible implication that such services may not be effectively rendered). Both AI edits stop short of saying that the herbal medicine clinic actually does treat health conditions, which is what the human NPOV editor \u2018corrected' the edit to. Another example of LLM edits removing more biased content can be seen in Case 9 (Figure 8c), where the Al editor removes certain words like \u201cgimmick\u201d that human editors retained."}, {"title": "7 DISCUSSION", "content": "LLMs are increasingly used in communities with their own rules and guidelines. Is providing these rules to LLMs enough for them to replicate community moderation decisions? We evaluated general-purpose LLMs on their ability to (1) detect edits that violated Wikipedia's Neutral Point of View (NPOV) and (2) generate NPOV-compliant edits from NPOV-violating edits.\nTakeaway 1: Large language models largely failed at detection.\nOur results provide insight into the capabilities and limitations of LLMs in applying community-specific neutrality norms. Across models, prompts, reasoning strategies, and self-optimizations, performance was low-even with targeted prompts that directly incorporatedWikipedia's guidelines. LLMs are trained on Wikipedia data, so this poor performance existed despite a potential data leakage. Low detection performance suggests that applying subtle guidelines to real-world cases is difficult for today's general-purpose models. Notably, the largest model did the best. Perhaps even larger models would do better. Alternatively, low performance may be due to a more fundamental aspect of LLMs. For example, it may be that LLMs effectively \u2018over-learn' a notion (e.g., neutrality) from broad web corpora, and specializing this notion to a community's norms requires changing a model's parameters. In any case, our study suggests two future directions. First, fine-tuning may improve performance. However, fine-tuning may also risk overfitting [26]. Another approach involves more advanced use of retrieval-augmented-generation (RAG). Can incorporating conversations from Wikipedia Talk pages or other Wikipedia data simulate the community knowledge that editors have?\nHowever, we must view LLM performance in the context of what is, essentially, a hard task. NPOV edits are typically made by senior editors [34]. And crowdworkers could only guess the biased word in a Wikipedia edit 37% of the time [37]. General-purpose LLMs might be better than random individuals at NPOV detection but worse than expert editors.\nTakeaway 2: Different models had their own biases.\nDifferent models exhibited distinct biases. ChatGPT 3.5 was far more likely to over-predict edits as biased, while Mistral-Medium had the opposite tendency. Contrasting failure modes suggest that pre-trained LLMs may internalize distinct priors about what constitutes neutrality. These distinctions highlight one other shortcoming of relying on LLMs for such tasks. Models get introduced and updated rapidly, and our understanding of how LLMs work can get outdated just as rapidly. As language models become more ubiquitous, understanding these idiosyncratic biases will be crucial. Beyond understanding these biases, they may also be utilized. Considering Wikipedia NPOV specifically, editors engaging with opposing views play a role in maintaining neutrality [10]. Perhaps a multi-agent system simulating \u2018debates' [15, 17] can better apply nuanced rules than an individual agent in isolation.\nTakeaway 3: LLMs may apply rules in different ways from humans.\nAl editors neutralize text differently from human editors. Computational metrics indicate that LLMs typically remove the NPOV-violating words that human editors remove (79% average recall), but they also make many other changes (37% average precision). Furthermore, while Wikipedians were more likely to neutralize text through deletions than additions, AI editors showed the opposite pattern. This divergence suggests LLMs may internalize and operationalize rules like NPOV in fundamentally different ways compared to a community's human experts. Though different is not necessarily better or worse. Of course, one clear risk is that AI moderators can reduce the cohesiveness of community content. Then again, LLMs may also inspire new practices. The fact"}, {"title": "8 LIMITATIONS", "content": "Our study has limitations that can inform future work. First, we focused on Wikipedia's neutrality guidelines, which are an important example of community-specific neutrality norms given Wikipedia's wide readership and clearly articulated norms. However, Wikipedia is just one community. Second, the Wikipedia Neutrality Corpus, the dataset we used, may not represent all types of NPOV violations. It focuses on violations of neutral language, which is useful for our purposes since detecting these violations does not require access to external information. Future research can explore LLM performance on other types of NPOV violations. Third, we tested three language models (GPT 4, ChatGPT 3.5, and Mistral-Medium), so we cannot generalize our results to all language models. We chose these models because GPT 4 is state-of-the-art, ChatGPT 3.5 is widely used, and Mistral-Medium is from an open-source developer. Fourth, we did not explore the impact of fine-tuning on the results, as our goal was to understand to what extent these models can be steered without additional resources. Fifth, no prompt experiment can rule out that some (untested) prompts may have worked better. Though we explored a diverse range of prompting"}, {"title": "9 CONCLUSION", "content": "On the one hand, simply exposing models to high-level principles was insufficient for models to reproduce the judgments of community members in particular cases. Even when models correctly classified edits, this was correlated with the presence of a single 'giveaway' adjective. On the other hand, LLMs were able to apply NPOV for generation. LLMs generally changed words that Wikipedia editors changed, and LLM edits were also preferred over Wikipedian edits by laypeople. This may suggest a divergence between following instructions in general and applying them to particular cases like community members do. We suggest that even when articulating principles is easy, having LLMs apply them to specific cases-like community members do\u2013may still be hard."}, {"title": "A DATASET", "content": null}, {"title": "B PROMPTS", "content": null}, {"title": "B.1 Detection: NPOV", "content": "Note: The EXAMPLES part of the prompt is only included for the few-shot conditions.\nINSTRUCTIONS\nAct like an expert Wikipedia editor. Classify if this Wikipedia edit is neutral or biased.\nA Wikipedia edit is biased if it violates Wikipedia's neutral point of view (NPOV) policy.\nNeutral point of view policy:\nPolicy 1: Avoid stating opinions as facts.\nPolicy 2: Avoid stating seriously contested assertions as facts.\nPolicy 3: Avoid stating facts as opinions.\nPolicy 4: Prefer nonjudgmental language.\nPolicy 5: Indicate the relative prominence of opposing views.\nNote that even if it slightly violates the NPOV policy, it is considered biased and not neutral.\nEXAMPLES\n{example_str}\nEDIT\n{edit_here}\nRETURN\na json like {\"response\": \"biased\" or \"neutral\", \"policy_violated\": \"none\" if neutral else policy violated, \"reasoning\": reasoning} and nothing else"}, {"title": "B.2 Detection: NPOV Scoped", "content": "Note: The EXAMPLES part of the prompt is only included for the few-shot conditions.\nINSTRUCTIONS\nAct like an expert Wikipedia editor. Classify if this Wikipedia edit is neutral or biased.\nA Wikipedia edit is biased if it violates Wikipedia's neutral point of view (NPOV) policy.\nNeutral point of view policy:\nPolicy 1: Avoid subjective and judgmental language. Word choice should use a disinterested, impartial tone and avoid language that sympathizes with or disparages the subject.\nPolicy 2: Avoid phrasing that implies how 'believable' a statement is. Word choice can imply how believable the author considers a statement to be.\nPolicy 3: Avoid stereotypes. Word choice can imply stereotypes about gender, race or other demographic categories.\nPolicy 4: Avoid presenting opinions as facts. Opinions should be attributed to a person, a group of people who hold the opinion, or where appropriate, described as a widespread view.\nNote that even if it slightly violates the NPOV policy, it is considered biased and not neutral.\nEXAMPLES\n{example_str}"}, {"title": "B.3 Detection: Minimal", "content": "Note: The EXAMPLES part of the prompt is only included for the few-shot conditions.\nINSTRUCTIONS\nAct like an expert encyclopedia editor. Classify if a given statement is neutral or biased.\nNote that even if it slightly violates the NPOV policy, it is considered biased and not neutral.\nEXAMPLES\n{example_str}\nSTATEMENT\n{edit_here}\nRETURN\na json like {\"response\": \"biased\" or \"neutral\", \"reasoning\": reasoning} and nothing else"}, {"title": "B.4 Detection: LLM Bootstrapped Prompt (NPOV-Scoped)", "content": "Note: This prompt format follows DSPY's Chain of Thought implementation.\nAct like an expert Wikipedia editor. Classify if this Wikipedia edit is neutral or biased.\nA Wikipedia edit is biased if it violates Wikipedia's neutral point of view (NPOV) policy.\nNeutral point of view policy:\nPolicy 1: Avoid subjective and judgmental language. Word choice should use a disinterested, impartial tone and avoid language that sympathizes with or disparages the subject.\nPolicy 2: Avoid phrasing that implies how 'believable' a statement is. Word choice can imply how believable the author considers a statement to be.\nPolicy 3: Avoid stereotypes. Word choice can imply stereotypes about gender, race or other demographic categories.\nPolicy 4: Avoid presenting opinions as facts. Opinions should be attributed to a person, a group of people who hold the opinion, or where appropriate, described as a widespread view.\nNote that even if it slightly violates the NPOV policy, it is considered biased and not neutral.\nFollow the following format.\nEdit: edit\nReasoning: Let's think step by step in order to ${produce the answer}. We\nAnswer: neutral or biased\nEdit: hydrogen pipelines are unfortunately more expensive than even long-distance electric lines.\nReasoning: Let's think step by step in order to produce the answer. We first identify if the edit uses subjective or judgmental language. The word \"unfortunately\" introduces a subjective viewpoint, implying a negative aspect of hydrogen pipelines compared to electric lines. This word choice violates Policy 1 by not maintaining an impartial tone. It also subtly presents an opinion\nas a fact, which is against Policy 4, as it does not attribute this viewpoint to a specific source or acknowledge it as a perspective rather than an objective truth. Therefore, the edit does not adhere to the neutral point of view policy.\nAnswer: biased\n(...Omitting more examples for space...)\nEdit: {EDIT_HERE}"}, {"title": "B.5 Detection: LLM Bootstrapped Prompt (NPOV)", "content": "Note: This prompt format follows DSPY's Chain of Thought implementation.\nAct like an expert Wikipedia editor. Classify if this Wikipedia edit is neutral or biased.\nA Wikipedia edit is biased if it violates Wikipedia's neutral point of view (NPOV) policy.\nNeutral point of view policy:\nPolicy 1: Avoid stating opinions as facts\nPolicy 2: Avoid stating seriously contested assertions as facts\nPolicy 3: Avoid stating facts as opinions\nPolicy 4: Prefer nonjudgmental language.\nPolicy 5: Indicate the relative prominence of opposing views.\nNote that even if it slightly violates the NPOV policy, it is considered biased and not neutral.\nFollow the following format.\nEdit: edit\nReasoning: Let's think step by step in order to ${produce the answer}. We\nAnswer: neutral or biased\nEdit: he developed a complex story-line with theological themes, as adam warlock, who had emerged from a kind of virgin birth, confronted a dictatorial universal church of truth founded by his evil future self.\nReasoning: Let's think step by step in order to produce the answer. First, we examine if the edit states opinions as facts. The edit provides a summary of a storyline involving Adam Warlock, a character from comic books, and describes the plot and themes involved. It does not present opinions as facts but rather summarizes the content of a fictional work. Second, we check if it states seriously contested assertions as facts. The edit does not make assertions about real-world events or figures but discusses a fictional narrative. Third, we consider if it states facts as opinions. The edit does not frame factual information as opinion. Fourth, we evaluate the use of nonjudgmental language. The language used to describe the storyline and themes is neutral and descriptive without passing judgment. Fifth, we assess if it indicates the relative prominence of opposing views. Since this edit is about a specific storyline within a comic book, the concept of opposing views is not applicable in the same way it would be for a real-world issue. Therefore, based on these considerations, the edit does not violate the NPOV policy.\nAnswer: neutral\n(...Omitting more examples for space...)\nEdit: {EDIT_HERE}\nReasoning: Let's think step by step in order to"}, {"title": "B.6 Detection: LLM Auto-Generated Prompt", "content": "Note: This prompt format follows DSPY's Chain of Thought implementation.\nPlease meticulously evaluate the content of the indicated Wikipedia edit. Our aim is to determine its alignment with the principles of neutrality and objective information presentation. An edit falls under \"neutral\" if it maintains or improves the essential impartial perspective of the topic by offering well-referenced data, balanced interpretation, and avoids espousing a specific ideology, viewpoint, or favoritism towards any subject matter or entity portrayed. Conversely, categorize an edit as \"biased\" when it skews towards advocating a singular perspective: conspicuously highlighted by the use or omission of particular terminologies, incorporation or deletion of facts that possess agenda-driven subtexts, or through nuanced alteration playing to subjective opinions, ideologies, or non-verifiable anecdotes that conspicuously colour the narrative away from evidentiary neutrality. In making your categorization, lavish detail on linguistic nuances, presentation styles, and contextual frame shifts evoked by the edit, analyzing its departure from or congruity to non-partisan, factual, information dissemination aimed at fostering an informed readership."}, {"title": "B.7 Generation: NPOV Scoped", "content": "Note: The CONSTRAINTS section of the prompt is only included in the Is Constrained' condition.\nGiven a biased Wikipedia edit, make minimal changes to make this edit neutral according to Wikipedia's neutral point of view (NPOV) policy.\nNeutral point of view policy:\nPolicy 1: Avoid subjective and judgmental language. Word choice should use a disinterested, impartial tone and avoid language that sympathizes with or disparages the subject.\nPolicy 2: Avoid phrasing that implies how 'believable' a statement is. Word choice can imply how believable the author considers a statement to be.\nPolicy 3: Avoid stereotypes. Word choice can imply stereotypes about gender, race or other demographic categories.\nPolicy 4: Avoid presenting opinions as facts. Opinions should be attributed to a person, a group of people who hold the opinion, or where appropriate, described as a widespread view.\nCONSTRAINTS\nChange as few words as possible\nMake minimal changes\nEDIT\n{EDIT}\nRETURN an extractable json like {'neutral_edit': EDIT}"}, {"title": "B.8 Generation: CAI", "content": "Note: the CONSTRAINTS section of the prompt is only included in the Is Constrained' condition.\nB.8.1 Critique request.\nIdentify specific ways, writing in one paragraph, in which this Wikipedia edit violates Wikipedia's neutral point of view (NPOV) policy.\nNeutral point of view policy:\nPolicy 1: Avoid subjective and judgmental language. Word choice should use a disinterested, impartial tone and avoid language that sympathizes with or disparages the subject.\nPolicy 2: Avoid phrasing that implies how 'believable' a statement is. Word choice can imply how believable the author considers a statement to be.\nPolicy 3: Avoid stereotypes. Word choice can imply stereotypes about gender, race or other demographic categories.\nPolicy 4: Avoid presenting opinions as facts. Opinions should be attributed to a person, a group of people who hold the opinion, or where appropriate, described as a widespread view.\nEDIT\n{EDIT}\nRETURN a json like 'critique': critique}\nB.8.2 Revision request.\nPlease rewrite the wikipedia edit based on this critique.\nCRITIQUE\n{CRITIQUE}"}]}