{"title": "Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms", "authors": ["Joshua Ashkinaze", "Ruijia Guan", "Laura Kurek", "Eytan Adar", "Ceren Budak", "Eric Gilbert"], "abstract": "Large language models (LLMs) are trained on broad corpora and then used in communities with specialized norms. Is providing LLMs with community rules enough for models to follow these norms? We evaluate LLMs' capacity to detect (Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's Neutral Point of View (NPOV) policy. LLMs struggled with bias detection, achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting biases (some under- and others over-predicted bias), suggesting distinct priors about neutrality. LLMs performed better at generation, removing 79% of words removed by Wikipedia editors. However, LLMs made additional changes beyond Wikipedia editors' simpler neutralizations, resulting in high-recall but low-precision editing. Interestingly, crowdworkers rated AI rewrites as more neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia editors but often made extraneous non-NPOV-related changes (such as grammar). LLMs may apply rules in ways that resonate with the public but diverge from community experts. While potentially effective for generation, LLMs may reduce editor agency and increase moderation workload (e.g., verifying additions). Even when rules are easy to articulate, having LLMs apply them like community members may still be difficult.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are trained on large corpora and then used within communities that have their own norms. To steer models towards specific norms and values, there is a growing trend of stating high-level rules as prompts. For example, Constitutional AI [1] involves providing the model with a set of rules that it uses to critique and then revise its own outputs. But is providing high-level rules sufficient to steer models toward community norms?\nThe challenge of going from high-level rules-like Wikipedia's Neutral Point of View (NPOV) policy-to specific cases mirrors earlier debates in human-AI interaction and beyond. For instance, Lucy Suchman's 1987 \u201cPlans and Situated Actions\u201d framework [45] contrasted predetermined procedures derived from universal principles (\u201cplans\u201d) with context-dependent actions based on concrete circumstances (\u201csituated actions\u201d). She argued that AI systems execute plans, while humans perform situated actions. This distinction relates to James Scott's [41] concept of \u201cseeing like a state\"-the idea that the large-scale plans of centralized authorities often break down when faced with complex, local realities. The tension between universal plans and situated, local actions is relevant to deploying general-purpose LLMs in communities like Wikipedia. On Wikipedia, many decisions involve messy and contextual judgments [13, 46] rather than straightforward applications of high-level rules. Is providing high-level rules to trillion-parameter models also insufficient for navigating particular NPOV cases?\nWe test how well general-purpose LLMs can be steered to apply NPOV policy.\u00b9 The broader NPOV policy encompasses various complex assessments (e.g., covering viewpoints in proportion to their prominence in reliable sources). Here, we focus on one specific subtask: debiasing language. Specifically, we evaluate large language models on their ability to: (1) detect edits that include non-neutral language and (2) generate NPOV-compliant edits from biased ones. Evaluating LLMs on Wikipedia's NPOV policy is an interesting test case of model abilities for three reasons. First, LLMs are used in widely different communities. An open question is whether these models can be steered-with no task-specific fine-tuning-toward following community policies or guidelines. Second, detecting and generating non-neutral language requires nuance that goes beyond literal meaning. This task probes how well LLMs can apply natural language pragmatics and mirror the nuanced decisions of community members. Third, Wikipedia's NPOV has a unique tension: while clearly-articulated and well-documented in theory, it is complex in practice [36, 46]. Adjudicating neutrality is nuanced. To what extent can LLMs, absent fine-tuning, apply clearly articulated but nuanced community policies?"}, {"title": "1.1 Findings", "content": "To test whether LLMs could apply Wikipedia's NPOV policy, we conducted detection and generation experiments. In both experiments, LLM decisions and edits were compared against an existing corpus of NPOV edits by Wikipedia editors, also known as Wikipedians. In the detection experiment, we used ChatGPT 3.5, Mistral-Medium, and GPT-4 to classify whether edits violated NPOV, varying the amount of policy detail and example decisions provided. In the first generation experiment, we used GPT-4 (using both Zero-Shot and Constitutional AI approaches) to neutralize NPOV-violating edits. We used computational metrics to compare the LLM-neutralized edits to the Wikipedian-neutralized edits. In the second generation experiment, humans gave masked evaluations of LLM and Wikipedian rewrites. We also conducted a qualitative analysis of LLM vs. Wikipedian rewrites to better understand differences in how each neutralizes text. We summarize our main findings and implications below:\n(1) Large language models largely failed at neutrality detection. Across models, prompts, and optimization strategies, performance was low (64% accuracy for the best prompt; the dataset was balanced so random chance accuracy is 50%). Analysis of LLM rationales and errors suggested LLMs relied (sometimes to a fault) on simple heuristics like the presence of a highly subjective adjective. Perhaps modern models still struggle with applying subtle rules to real-world cases without task-specific training.\n(2) Models exhibited biases that persisted across prompts. ChatGPT 3.5 over-predicted edits as biased, and Mistral-Medium did the opposite. GPT-4 was balanced. Pretrained LLMs may internalize distinct priors about what constitutes neutrality that persist across prompts. Understanding model idiosyncrasies is crucial. It is not clear that, when a new model appears on the scene, we will know what its priors are.\n(3) LLMs applied rules in different ways than expert editors. Computational experiments showed LLMs generally did remove words that Wikipedia editors removed in the generation phase. But LLMs also made many other changes. Crowdworkers also rated Constitutional AI (CAI)-generated rewrites as adding unnecessary information more frequently than Wikipedian rewrites. In other words, LLM editors are high-recall but low-precision. And while Wikipedians make more removals than additions, LLM editors do the opposite. One risk for large-scale deployment of models within communities is that they may follow rules in different ways than community members would.\n(4) Crowd-workers preferred LLM neutralizations to Wikipedia-editor neutralizations. Given the finding that LLMs generally made more changes than Wikipedia editors, we conducted human experiments to understand if these generations are in fact preferred to Wikipedian neutralizations. We find that crowd-workers prefer AI edits over human edits on both fluency (61% of choices) and bias reduction (70% of choices). Instruction-tuned models may be highly capable of applying rules in ways that resonate with a broader public, even if these applications differ from community experts.\n(5) Qualitative analysis showed LLMs are \u2018NPOV+'. To reconcile LLMs' low detection performance with high generation evaluations, we conducted a qualitative analysis. Despite being instructed to make only minimal (NPOV) changes, we find LLMs made many additional grammatical and stylistic edits that may have influenced participants' judgments. This maximalist editing approach could (1) increase labor costs as moderators may need to check for AI hallucinations and (2) reduce editor agency due to extensive rewrites. But in other cases, AI models arguably applied NPOV more faithfully than human editors (as judged by the authors), possibly due to varying community norms on what would constitute neutrality. These findings highlight tradeoffs in general-purpose LLM generations."}, {"title": "2 RELATED WORK", "content": "We position our work in the context of Wikipedia neutrality research (2.1), automated moderation of Wikipedia (2.2), and broader conversations around the role of general-purpose LLMs in community moderation (2.3). Wikipedia's Neutral Point of View (NPOV) policy, though clearly defined, is often debated in practice [29, 36, 46, 47], making it a compelling test case for LLMs. While prior work tested task-specific models on neutralizing NPOV violations [34, 37], we examine whether general-purpose LLMs can neutralize NPOV violations. This task is timely given two trends. First, LLMs excel at zero-shot annotation [9], motivating work on their ability to apply community policies [2, 7, 21, 22]. We complement this work by testing LLMs on generation in addition to detection, specifically in the Wikipedia context. Second, Wikipedia has historically used automated moderation tools [52], but there are contentious internal debates about the use of ChatGPT [14]. Our study can inform these debates. More broadly, our work speaks to the opportunities and limitations of using general-purpose LLMs to uphold nuanced community norms."}, {"title": "2.1 Wikipedia's Neutral Point of View (NPOV) Policy", "content": "Wikipedia is one of the most visited websites in the world [42]. It is a widely-regarded success in peer production. It is well studied in CSCW and beyond [5, 19, 27, 29, 36, 46, 47]. To support a crowd-sourced encyclopedia editable by anyone, Wikipedia developed many policies, processes, and norms [6]. \u201cNeutral Point of View\u201d (NPOV) is one of the most central. For example, NPOV is listed first among the three core content policies [51]. According to Wikipedia.org [55]:\n\"All encyclopedic content on Wikipedia must be written from a neutral point of view (NPOV), which means representing fairly, proportionately, and, as far as possible, without editorial bias, all the significant views that have been published by reliable sources on a topic.\u201d\nJimmy Wales (a co-founder of Wikipedia) has said [48] [29, p. 5]:\n\u201cSo how do we do this? ...How does it work? ... So the biggest and the most important thing is our neutral point of view policy. This is something that I set down from the very beginning, as a core principle of the community that's completely not debatable.\u201d\nNPOV is crucial for Wikipedia's content quality. Wikipedia's iterative revision process improves neutrality over time [11]. This is due in part to the role of NPOV in defending against disinformation, low-quality information [28, 44], and delineating fringe points of view [44]. The result is (1) a public perception that Wikipedia editors are committed to unbiased information and (2) increased reader trust [8].\nBecause of NPOV's importance, Wikipedia goes to great lengths to codify exactly what a \u201cneutral point of view\u201d means. Specifically, NPOV is described by a set of principles\u00b2 [54]. These principles are expounded and clarified in numerous FAQs, tutorials, and examples. Editors are trained in NPOV before they can edit. And if an article violates NPOV, then it is flagged as \u201cNPOV Disputed\u201d so that an editor can bring it into compliance. As a community norm, NPOV is clearly defined and often invoked in day-to-day community activity. The \u2018rules' are clear.\nHowever, despite its clear articulation, NPOV has always been complicated in practice. Reagle claims that \u201cin the Wikipedia culture, the notion of \u2018neutrality' is not understood so much as an end result, but as a process\" [36]. Wikipedia articles often go through many rewrites, with debates occurring as to what constitutes neutrality. For example, over three years the Wikipedia entry for 'clean coal technology' had 39 distinct facts and 142 different rewrites of these facts [46]. Norm-consistent Wikipedia content is often the result of conflict, coordination, and deliberation-more complex than lone editors applying rules in isolation [19, 20, 29, 46, 47]. Past work [19] estimated that 40% of Wikipedia edits are dedicated to coordination between Wikipedians (e.g., achieving consensus). And despite NPOV being 5 principles, there are many pages and debates dedicated to this topic [27].\nWikipedia's NPOV policy is an interesting test case of general-purpose LLM abilities because of a tension: It is both clear and nuanced. On one hand, NPOV is clearly defined and documented. In theory, augmenting LLMs with the text of NPOV guidelines should enable LLMs to apply these guidelines effectively. But even though NPOV is clear in theory, it is complex in practice. To what extent, then, can general-purpose LLMs-absent any task-specific training\u2014be steered to follow nuanced community guidelines such as NPOV?"}, {"title": "2.2 Automated Approaches to NPOV and Wikpedia Moderation", "content": "Researchers have developed algorithms to understand and ameliorate problems with Wikipedia arti-cle content (e.g., [3, 12, 30, 39, 40, 49]). Most relevant to the present work, several papers specifically tested whether models can neutralize NPOV-violating edits. Recasens et al. [37] experimented on a subset of NPOV-violating edits where the bias was due to one word. Both humans and their system struggled to guess the biased word (37% and 34% accuracy, respectively). Building on this result, Pryzant et al. [34] collected a similar dataset of NPOV-violating edits and their corrections called the Wikipedia Neutrality Corpus (WNC). We use this dataset in our work. Both projects [34, 37] conducted experiments on a subset of edits in which the bias was attributable to only one word. With the WNC corpus, a sequence-to-sequence system was trained to detect the NPOV-violating word and then predict its replacement, achieving 46% generation accuracy (measured as the proportion of generations that matched Wikipedia-editor rewrites).\nOur study departs from these two studies in two ways. First, we conduct experiments on a more representative set of NPOV-violating edits (i.e., not the one-word subset), which is arguably a harder task. Second, we test whether general-purpose models, absent any task-specific training, can perform this task. Both Recasens et al. and Pryzant et al. show that applying NPOV in a way that matches how expert editors behave is difficult for both laypeople and task-specific NLP systems. In our work, we can answer the question: How well can new generations of general-purpose LLMs neutralize text?\nWikipedia already uses automated moderation tools [53], and our results can inform the benefits and risks of using general-purpose LLMs. Past and current services include WikiTrust [52], an extension that helped editors detect untrustworthy edits. The ORES project used machine learning to rank the usefulness of edits [12]. The Automod project was an abuse detection model [35]. More recently, Wikipedia editors have debated the utility of ChatGPT for generating Wikipedia edits [14]. By systematically evaluating NPOV detection and generation from general-purpose LLMs, our results can add evidence to this debate, possibly informing community practice."}, {"title": "2.3 Pre-Trained LLMs for Community Content Moderation", "content": "Given how much content there is to moderate, platforms often employ automatic content mod-eration tools such as Reddit's AutoMod [16]. Because general-purpose LLMs show remarkable zero-shot [9] and few-shot [4] capabilities even without fine-tuning (i.e., training the model on task-specific data), it is reasonable to think general-purpose LLMs would be effective content moderators. The zero-shot and few-shot capabilities of LLMs are useful, since fine-tuning requires resources (compute, instances) that some communities may not have. Hence, online communi-ties may rightfully wonder how well these off-the-shelf LLMs can apply their rules for online moderation. Our work engages with this question.\nSeveral studies have explored whether general-purpose LLMs can apply highly specific sets of online community rules without fine-tuning. One study [21] tested whether LLM moderators can detect rule-violating posts across nine subreddits when the rules of these subreddits are provided to the LLM. Their system achieved a high true-negative rate (92%) but a low true-positive rate (43%). A similar study found that ChatGPT 3.5 was 64% accurate in predicting subreddit moderation decisions [23]. Cao et al. tested how well LLMs could detect rule violations of r/AskHistorians, with precision and recall showing variance depending on the specific rule [7]. They also asked moderators what would be an acceptable precision and recall of models for the models to be useful. In general, moderators indicated models should have recall and precision above 70%. Although not subreddit-specific, Barbarestani et al. found ChatGPT largely agrees with both crowds and experts when detecting inappropriate words in Reddit comments [2]. However, when considering errors, they found that differences in subjective interpretations of appropriateness accounted for 41% of the disagreement amongst experts but 69% of the disagreement between experts and ChatGPT. This may suggest that some familiarity with the topic is required to apply nuanced guidelines reliably.\nOur study complements and extends existing research on using general-purpose LLMs for online community content moderation. It complements prior work by testing if general-purpose LLMs can effectively apply Wikipedia NPOV. NPOV is both intrinsically important (being a central norm to one of the most visited websites) and theoretically interesting (since it is highly documented while also requiring nuanced reasoning to apply). Our study extends these works, too. In addition to detection, we focus on how well LLMs can generate content consistent with a community's rules. As LLM usage grows, understanding the implications for generation within communities is crucial. Because LLMs are explicitly trained for natural language generation, they may be more effective at generation than detection. We conduct both computational and human subject experiments to measure generative quality."}, {"title": "3 LLM BIAS DETECTION", "content": "To understand whether general-purpose LLMs can classify edits as neutral or biased, we conducted classification experiments on a balanced sample of NPOV-violating edits and NPOV-compliant rewrites. We find that across models and prompts, models largely failed to distinguish between neutral and biased edits. Models exhibited contrasting failure modes, and model explanations suggest they appeared to rely on heuristics such as the presence of a highly subjective adjective."}, {"title": "3.1 Dataset", "content": "The data for our detection and generation experiments come from the Wikipedia Neutrality Corpus (WNC) [34]. The WNC is a collection of edit pairs: A biased edit (an edit flagged by an editor for violating the NPOV) and a neutral edit (the rewrite an editor made to ensure NPOV compliance). While NPOV is a set of multiple rules, the WNC consists of edits that violated a particular subset of NPOV-biased language (framing, epistemological, and demographic bias). This stipulation is useful for testing the abilities of LLMs. Unlike other NPOV rules (e.g., those concerning citations), a violation of neutral language does not require access to external information. For detection experiments, we use the subset of these edits where the data also has tags for which topic an edit belongs to (Appendix for Table 6 topic counts).\nThis WNC was collected by crawling Wikipedia revisions between 2004 and 2019. In an initial pass, Pryzant et al. [34] filtered for revisions where editors provided a justification indicating an NPOV violation. In subsequent filtering passes, Pryzant et al. [34] excluded revisions below a minimal and above a maximal number of characters and applied additional rules to maximize the probability that the change was related to bias in particular (e.g., excluding revisions that involved references)."}, {"title": "3.2 Experiment Setup", "content": "We conducted a multi-model prompt experiment (N = 5,348 annotations) where the task was to classify if a given Wikipedia edit was biased or neutral. Our two experimental factors were (Factor 1) the definition given to an LLM on what constitutes neutrality, and (Factor 2) whether or not we provided examples (i.e., few-shot or zero-shot). The rationale for varying definitions is that we are interested in whether augmenting LLMs with community-specific definitions of neutrality increases adherence to community-specific norms. The rationale for our second factor is to test whether providing example decisions from a community helps models adhere to community norms. Appendix B has all the prompts we used in this paper."}, {"title": "3.2.1 Factor 1: Definitions Provided.", "content": "The minimal prompt relies on LLMs' learned knowledge only, providing no Wikipedia-specific definitions or examples. This condition reveals how well general-purpose LLMs apply NPOV without additional context. The NPOV prompt provides LLMs with Wikipedia's verbatim NPOV guidelines. Comparing the performance between the minimal and NPOV conditions estimates how much simple prompting can align general-purpose notions of neutrality with community-specific norms. Since the WNC focuses on neutral language violations, a subset of NPOV violations, we created a third prompt condition: NPOV-Scoped. This prompt gives LLMs Wikipedia's NPOV guidelines and additional language from Pryzant et al. [34] on the specific types of non-neutral language (framing, epistemological, and demographic bias) in the dataset. Comparing NPOV-Scoped to NPOV reveals how much guideline specificity improves classification accuracy."}, {"title": "3.2.2 Factor 2: Examples Provided.", "content": "Few-shot learning can improve model performance [4]. We test whether augmenting the model with editors' prior decisions increases accuracy. In the few-shot condition, we select a random sample of 10 edits and their labels from the same topic, as neutrality norms may differ by topic due to factors like similar editors or content."}, {"title": "3.3 Experiment Results", "content": "All models performed poorly and prompts made little difference (Table 1; Figure 1a). Across all conditions, the accuracy was 0.58. Averaging across models, ChatGPT 3.5 performed the worst with accuracy at 0.55 and GPT-4 performed the best at 0.61. The top combination was GPT-4 with a zero-shot NPOV prompt (0.63). For each of the models, there were no statistically significant differences in accuracy between prompt conditions, based on p-values from two-tailed permutation tests. Models were more accurate for biased edits (0.63, 95% CI = [0.61, 0.65]) than neutral edits (0.53, 95% CI =[0.51, 0.55]), p < 0.001 from two-tailed permutation tests.\nWe ran a logistic regression (Appendix Table 7) modeling accuracy as a function of definitions, few-shot or zero-shot, model, topic, edit word count, and normalized edit distance tertile. Normalized edit distance is the edit distance between NPOV-violating and compliant edits, divided by the longer string's length. Higher values indicate that it took more characters to bring the NPOV-violating edit into compliance, so we may expect AI systems to find these edits easier to detect. Compared to ChatGPT 3.5, GPT-4 (OR = 1.25, 95% CI = [1.09, 1.43]) and Mistral-Medium (OR = 1.15, 95% CI = [1.01, 1.31]) were more accurate. Definitions and few-shot examples were not significant. Edit word count was weakly associated with higher accuracy (OR = 1.10, 95% CI = [1.02, 1.18]). Medium (OR = 1.86, 95% CI = [1.60, 2.16]) and high (OR = 1.43, 95% CI = [1.21, 1.70]) edit distance had higher accuracy than low edit distance, suggesting models were better at detecting violations that required more characters to correct. We also found that providing examples tempered ChatGPT 3.5's predictions. Across zero-shot conditions, ChatGPT 3.5 predicted that 83% of classifications were biased but across few-shot conditions, ChatGPT 3.5 predicted 59% were biased, two-tailed permutation test p < 0.001."}, {"title": "3.4 LLM Self-Optimizations", "content": "We experimented with more advanced reasoning techniques and LLM self-optimizations (see Appendix B for these LLM-assisted prompts). First, we selected our top model (GPT-4) and the NPOV and NPOV-Scoped prompts to use with chain-of-thought (CoT) reasoning and DSPy's 'BootstrapFewShot' module. In CoT, a model is instructed to reason step by step to get to its answer, which can improve performance [50]. DSPy [18] is a state-of-the-art framework for self-improving prompt pipelines. In DSPy's BootstrapFewShot module, the LLM will learn rationales for labeled instances in a training phase. Then on the test set, instead of simply seeing an example and a label (standard few-shot), the LLM will also see these \u2018bootstrapped' rationales when making classifications. For both prompts, we tested 300 edits with a 70-30 train-test split and 20-shot bootstrapped-rationales. (We doubled the number of examples from our 10-shot experiment to amplify the effect of LLM-generated rationales.) The 20-shot augmented NPOV-Scoped prompt achieved 64% accuracy on the held-out set (+1% higher but statistically indistinguishable from the best unaugmented prompt); the 20-shot augmented NPOV prompt achieved 62% accuracy. We also experimented with an LLM optimizing its prompt on its own. To do this, we used the COPRO module from DSPy [18]. Briefly, a model is initialized with an initial minimal prompt, and then in a training phase the model repeatedly generates and refines the prompt based on performance on a subset of examples. We similarly used 300 examples with a 70-30 train-test split. The top AI-generated prompt from the training phase achieved a test-set accuracy of 61%. For the rest of the detection analysis, we analyze the results from our original experiments."}, {"title": "3.5 Model-Level Analysis", "content": "We based our evaluation on a balanced dataset of neutral and biased edits. In real-world applications, models would encounter unbalanced data. We examined the error distribution of different models. We find that not all models failed the same way (Figure 2a and Figure 2b). ChatGPT 3.5 was far more likely than other models to predict that edits were biased when they were neutral. Mistral-Medium, on the other hand, erred in the opposite direction. It over-predicted neutral edits. GPT-4 was balanced. We assessed the statistical significance of these patterns with two-tailed binomial tests (Appendix C.1; Figure 2b), rejecting a null hypothesis of balanced errors for ChatGPT 3.5 and Mistral-Medium but not GPT-4. Note that prompts and specific edits were held constant, so these differences in model predictions arise from the model itself. Our results suggest that prompting may not be enough. If models have idiosyncrasies, these may persist through different prompts."}, {"title": "3.6 Edit-Level Analysis", "content": "We calculated the edit-level probability of correct classification (POCC) to identify which edits are easier to detect (Figure 3). POCC is the probability of a model correctly classifying an edit across all models and prompts. POCCs were bimodal (Figure 3), indicating that edits tend to be either easy or hard to classify.\nWe conducted a qualitative analysis of \u2018easy' vs. \u2018hard' biased edits (i.e., top or bottom POCC quartile). Top-POCC biased edits tend to have some highly subjective word that alerts models these edits are biased. See below (emphasis added):\n\u2022 \u201cone of the central characters of the novel, akili kuwale, provides a brilliant demonstration of this change and its implications, together with excellent characterization.\u201d\n\u2022 \u201cenvironmentalists complain that before shipbreaking began there in june 1983 the beach at alang was pristine and unspoiled.\u201d\n\u2022 \u201ccolchester has a proud tradition of its citizen volunteers serving in the territorial army.\u201d\nBy contrast, low-POCC biased edits generally do not contain an overtly subjective adjective.\n\u2022 \u201cthe bill protects americans against discrimination based on their genetic information when it comes to health insurance and employment.\u201d\n\u2022 \u201cconfucianism (; pinyin: rxu ; literally means \"the school of the scholars\", see also names for confucianism for details) is an east asian ethical and philosophical system derived from the teachings of the early chinese sage confucius.\u201d\n\u2022 \u201cthey can see far over the great plains of illinois and across lake michigan on a clear day.\u201d"}, {"title": "3.7 Explanation-Level Analysis", "content": "We examined if interpretable features of LLM explanations correlate with accuracy (Appendix C.2 for variables and standardized logistic regression coefficients). We focused on three features:\n\u2022 Complexity: (e.g., word count, LIWC [33] cognitive processing words, conjunctions [24])\n\u2022 Prompt and rule reference: (e.g., SBERT [38] semantic similarity to prompt, NPOV-related words)\n\u2022 Subjective language (number of LIWC emotion words)\nIncreased word count (OR = 0.80, 95% CI = [0.74, 0.86]) and more conjunctions (OR = 0.91, 95% CI = [0.85, 0.96]) correlates with lower accuracy. Higher semantic similarity to the prompt (OR = 1.15, 95% CI = [1.07, 1.23]), NPOV-related words (OR = 1.12, 95% CI = [1.05, 1.20]), and emotion words (OR = 1.09, 95% CI = [1.03, 1.15]) correlate with higher accuracy. These results suggest that the depth of reasoning alone does not predict accuracy. Instead, LLMs performed better when their rationales incorporated specific instructions and referenced relevant content. However, due to a low pseudo-R\u00b2, we consider these findings preliminary.\nWe next conducted a TF-IDF logistic regression (Figure 4) to understand what specific words in an explanation correlate with accuracy. We used an 80-20 train-test split over {Explanation, Is Accurate} tuples and 5-fold cross-validated grid search to tune hyperparameters. The best model achieved 0.8 accuracy, 0.8 precision, and 0.79 recall. Referencing highly subjective words such as 'sadly', were associated with accuracy. Occasionally, the model also flagged edits as biased when they contained a subjective-sounding word even when the edit was itself not biased. As an example, models misclassified the following edit as biased (emphasis added):\nthe advertisement called obama a hypocrite for not supporting armed guards in schools while noting that the children of the us president receive special protection by armed agents of the us secret service.\nIn this edit, the writer is referencing the advertisement-but LLMs flagged the edit as biased even though it was factually recalling what an ad was saying. Here is GPT-4's explanation:\nThe edit uses loaded language by calling Obama a \u2018hypocrite' which is an opinion stated as a fact, and also uses judgmental language which violates the neutral point of view policy.\nThese dynamics suggest LLMs relied (sometimes to a fault) on simple heuristics like the presence of a highly subjective adjective. It is worth noting that LLMs did effectively identify the subjective-sounding part of the text in many of these false positive cases. This suggests that general-purpose LLMs may be able to neutralize biased text\u2014but possibly at the cost of precision. That is what we find in the next section."}, {"title": "4 LLM NEUTRALITY GENERATION: COMPUTATIONAL EVALUATION", "content": "We evaluated LLMs' ability to generate unbiased edits from biased ones using both automated computational metrics and human experiments. Our computational evaluation revealed that AI editors are high-recall but low-precision compared to Wikipedia editors. LLMs generally neutralize non-neutral words but also make many more changes than humans do. We also found that models apply NPOV differently from human editors. While human editors make more removals than additions to neutralize edits, general-purpose LLMs have the opposite tendency."}, {"title": "4.1 Experiment Setup", "content": "We used our top-performing model (GPT-4) and the NPOV-Scoped\u00b3 prompt to neutralize biased Wikipedia edits. Specifically, we instructed the model to revise an NPOV-violating edit, varying two factors. We varied (Factor 1) whether we conducted this generation using zero-shot reasoning or Constitutional AI (CAI). In the latter, the model first critiqued why an edit violated NPOV and then used that critique to revise the edit. Since initial results suggested LLMs changed more words than human editors, we experimented with (Factor 2) instructions to only edit what was necessary. We conducted this generation on 200 edits, varying both prompt type (CAI or Zero-Shot) and whether to add constraining instructions (Yes or No), yielding 800 generations. For all conditions, we set the temperature to zero to further reduce extraneous changes."}, {"title": "4.2 Measures", "content": "We evaluated both the intensity and accuracy of AI changes. To measure intensity, we computed (1) 'AI Edit Distance' as the normalized edit distance between the AI-neutralized edit and the original NPOV-violating edit, (2) the word count of AI edits, and (3) the number of changed words (excluding stopwords) in each edit by summing additions and deletions (this process discussed more below). We compared these intensity metrics to analogous metrics for human-modified Wikipedia edits to determine if AI makes more or fewer changes than human editors.\nWe evaluated the accuracy of AI edits by comparing them to Wikipedia editors' changes, which we treated as the gold standard. Our approach is similar to that of Pryzant et al. [34]. We first computed differences (diffs) between the original biased edits ($w_{Bias}$), human-modified edits ($w_{ModH}$), and AI-modified edits ($w_{ModAI}$) using Python's difflib library. 'Diffs' index substrings that were removed or added. We focus on removals as a measure of neutrality adjustment. After preprocessing diffs (lowercasing, stripping punctuation, removing stopwords) we defined sets A (words removed by AI) and B (words removed by humans). From these, we define true positives (TP) as words removed by both AI and humans (|A \u2229 B|); false positives (FP) as words removed by the AI that were not removed by humans (|A \u2013 B|); and false negatives (FN) as words removed by humans that were not removed by AI (|B \u2013 A|). We calculated various metrics based on these values (see Table 2). We also computed the BLEU score (a standard machine translation metric) of AI edits, using human edits as the reference text. We treated undefined precision metrics as 0, undefined recall metrics as missing, and used BLEU-score smoothing from [25]. For robustness, we re-analyzed the data under 32 analysis decisions (Appendix Figure 10), yielding results similar to those in Table 4."}, {"title": "4.3 Experiment Results", "content": "Finding 1: AI changes more than humans. LLMs changed more than humans (Figure 5). AI edit distance (M = 0.34, SD = 0.21) was larger than human edit distance (M = 0.15, SD = 0.13), t(799) = 24.06, p<0.001, Cohen's d = 0.85. Considering both additions and removals, AI made more changes (M = 13.88, SD = 12.91) than humans (M = 3.79, SD = 4."}]}