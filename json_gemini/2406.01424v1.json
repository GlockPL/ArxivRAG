{"title": "Universal In-Context Approximation\nBy Prompting Fully Recurrent Models", "authors": ["Aleksandar Petrov", "Tom A. Lamb", "Alasdair Paren", "Philip H.S. Torr", "Adel Bibi"], "abstract": "Zero-shot and in-context learning enable solving tasks without model fine-tuning,\nmaking them essential for developing generative model solutions. Therefore, it is\ncrucial to understand whether a pretrained model can be prompted to approximate\nany function, i.e., whether it is a universal in-context approximator. While it was\nrecently shown that transformer models do possess this property, these results rely\non their attention mechanism. Hence, these findings do not apply to fully recurrent\narchitectures like RNNs, LSTMs, and the increasingly popular SSMs. We demon-\nstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such\nas Mamba and Hawk/Griffin can also serve as universal in-context approximators.\nTo streamline our argument, we introduce a programming language called LSRL\nthat compiles to these fully recurrent architectures. LSRL may be of independent\ninterest for further studies of fully recurrent models, such as constructing inter-\npretability benchmarks. We also study the role of multiplicative gating and observe\nthat architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin)\ncan implement certain operations more stably, making them more viable candidates\nfor practical in-context universal approximation.", "sections": [{"title": "Introduction", "content": "Until recently, solving a task with machine learning required training or fine-tuning a model on a\ndataset matching the task at hand. However, large foundation models exhibit the ability to solve\nnew tasks without being specifically fine-tuned or trained for them: often it is sufficient to simply\nprompt them in the right way. This has made prompting a key method for steering a model towards a\nspecific behaviour or task (Liu et al., 2023). Prompting has been especially successful because of\nin-context learning: the ability to modify the model's behavior with information provided within the\ninput sequence, without changing the underlying model parameters (Brown et al., 2020). As a result,\nthe art and skill of constructing a successful prompt (prompt engineering) has become extremely\nimportant (Liu and Chilton, 2022; Sahoo et al., 2024). Yet, we know little about the theoretical\nproperties of prompting. It is not even clear if there are limits to what can be achieved with prompting\nor, conversely, whether it is possible to prompt your way into any behaviour or task.\nThis can be framed as a universal approximation question. Classically, universal approximation\nresults show how a class of tractable functions, such as neural networks, approximates another class of\nconcept functions, e.g., all continuous functions on a bounded domain, with arbitrary accuracy. This\nis often done by showing that one can choose model parameters that approximate the target function.\nHowever, in-context learning poses a different challenge as the model parameters are fixed. Instead,\na part of the input (the prompt) is modified to cause the model to approximate the target function.\nHence, we define universal in-context approximation to be the property that there exist fixed weights\nsuch that the resulting model can be prompted to approximate any function from a concept class."}, {"title": "Preliminaries", "content": "Fully recurrent architectures. In this work, we focus exclusively on fully recurrent neural network\narchitectures. Recurrent models operate over sequences. Concretely, consider an input sequence\n$(x_1,...,x_t)$ with $x_i \\in X$, X being some input space. We will refer to the elements of the input\nsequence as tokens even if they are real-valued vectors. A recurrent model $g : X^* \\rightarrow Y$ maps a\nsequence of inputs to an output in some output space Y. These models are always causal, namely:\n$y_t = g(x_1,..., x_t)$.\nWe will abuse the notation and refer to $(y_1, ..., y_t)=(g(x_1), ..., g(x_1, ..., x_t))$ as simply $g(x_1, ..., x_t)$.\nWe will also separate the input sequence into a query $(q_1, \u2026\u2026\u2026, q_n)$ and a prompt $(P_1, \u2026, P_N)$. The\nprompt specifies the target function f that we approximate while the query designates the input at\nwhich we evaluate it. Contrary to the typical setting, we will place the query before the prompt.\u00b9\nThere are various neural network architectures that fall under the general framework of Eq. (1). The\nquintessential one is the RNN. It processes inputs one by one with only a non-linear state being"}, {"title": "Linear State Recurrent Language (LSRL)", "content": "We can construct the weights for universal in-context models with the architectures in Eqs. (2) to (5) by\nhand but this is labour-intensive, error-prone, difficult to interpret, and the specific weights would be\narchitecture-dependent. Moreover, working at such a low level of abstraction can obfuscate common\nmechanisms and design patterns and makes it more difficult to appreciate both the capabilities and the\nconstraints of fully recurrent architectures. Instead, motivated by the desire to understand the scope\nof abilities of recurrent models, we propose a new programming language: Linear State Recurrent\nLanguage (LSRL).\u00b2 LSRL programs compile to any of these four architectures in Eqs. (2) to (5).\nConversely, any Linear RNN can be represented as an LSRL program, making LSRL a versatile tool\nfor studying the capabilities of recurrent models. Later, in Secs. 4 to 6 we make use of LSRL to\ndevelop programs that are universal approximators for $C_{vec}$ and $C_{tok}$, the compiled versions of which\nare hence the hypothesis classes in Sec. 2, thus showing that all four architectures can be universal\nin-context approximators.\nLSRL syntax. LSRL supports six basic operations. An LSRL program always starts with an\nInput(x) = x with an \u00e6 of a fixed dimension. Only one Input can be declared in a program.\nLinear layers and ReLUs are also supported: Lin[Ab](x) := Ax + b, ReLU(x) := max(0,x)."}, {"title": "Universal In-Context Approximation with Linear RNNS", "content": "Now that we are equipped with LSRL, we can proceed to building LSRL programs that are universal\nin-context approximators. We will describe two programs: one for approximating continuous\nfunctions ($C_{vec}$), and one for approximating maps between token sequences ($C_{tok}$). Formally, we\nconstruct a model $g_{vec}$ of the Linear RNN architecture (Eq. (3)) such that $H_{vec}(g_{vec})$ is dense in $C_{vec}$\nand a model $g_{tok}$ such that $H_{tok}(g_{tok})$ is dense in $C_{tok}$.\nApproximating continuous functions in $C_{vec}$\nThe idea behind the approximation for continuous functions is to discretise the domain into a grid\nand approximate the function as constant in each cell of the grid. This is a well-known approach for\nshowing universal approximation using the step activation function (Blum and Li, 1991; Scarselli and\nTsoi, 1998). However, it is not obvious how to implement this approach in-context when information\nacross input tokens can be combined only linearly. Our approach is to describe the value in each of\nthe discretization cells as a single prompt token. Consider a target function $f : [0, 1]^{d_{in}} \\rightarrow[0, 1]^{d_{out}}$\nand a discretization step d. We can then construct the prompt token for the cell with lower bounds\n$l_1,..., l_{d_{in}}$ and their respective upper bounds $l_1+d, ..., l_{d_{in}}+d$ with a $(d_{in}+d_{out}+1)$-dimensional vector:\n$p = [\u03b4, l_1, . . ., l_{d_{in}}, y_1, ... y_{d_{out}}]$,\nwhere y is the value of f at the centre of that cell: $y = f(l_1+\u03b4/2, ..., l_{d_{in}}+\u03b4/2)$. Each prompt token\ndescribes the size of the cell, i.e., the discritisation step d, its starting lower bound, and the value of\nthe target function at the centre of the cell. Thus, $[1/\u03b4]^{d_{in}}$ such tokens, one for each cell, are needed\nto describe the approximation of f. A query $q' \u2208 [0, 1]^{d_{in}}$ can be in only one of the cells. We pad it\nwith zeros and encode it as the first input element: $q = [q'^T, 0^{d_{out}+1}]^T$, followed by the prompt. Our\nprogram will extract and save q' to a state and then process the prompt tokens one at a time until"}, {"title": "Stable Universal In-Context Approximation with Gated Linear RNNS", "content": "The ReLU-based conditional operator is not numerically stable. The LSRL programs in Lsts. 1\nand 2 for approximating functions in respectively $C_{vec}$ and $C_{tok}$ rely on the $f_{ifelse}$ conditional\nassignment operator in Eq. (7) in order to implement different behaviours depending on whether\nwe are processing the query or specific parts of the prompt. This operator is not numerically stable.\nThe first term in Eq. (7) relies on cond(x) being exactly zero if the condition is not met. In this\nway, multiplying it with \u2013\u03bb would be 0 and f(x) would be returned. However, if cond(x) is not\nidentically 0 but has a small positive value, then - \u03bbcond(x) can \"overpower\" f(x) resulting in the\nReLU output being 0. In our experience, this is not a problem when processing inputs through the\nLSRL program step-by-step. However, de-branching the DAG into a path graph \u2014which is necessary\nin order to uncover the equivalent Linear RNN- appears to introduce such numerical instabilities\nwhich occasionally result in wrong outputs as conditional assignments will be 0 when they should\nnot. This problem is more prominent in Lst. 2 which is longer (more debranching steps) and has more\n$f_{ifelse}$ operations: it gets most tokens wrong because of that instability (see Original, No noise in\nFig. 4). To this end, we support LSRL with a symbolic backend (based on SymPy) that performs the\ndebranching steps exactly. Using it, both programs always produce the correct output.\nThis numerical instability highlights a critical practical limitations of the universal approximation\nresults in Sec. 4: if the models are not numerically stable, it is unlikely that they occur in practice by\ntraining models using gradient descent. This section shows how to improve the numerical stability of\nEq. (7) and obtain more realistic recurrent models that are universal approximators in-context.\nImplementing $f_{ifelse}$ with MLPs. As LSRL allows us to express arbitrary MLPs and MLPs\ncan approximate any continuous function, it is tempting to replace Eq. (7) with a deep MLP model."}, {"title": "Universal In-context Approximation with Non-linear (Gated) RNNs", "content": "Secs. 4 and 5 showed how universal approximation of continuous and token-to-token functions can\nbe implemented in LSRL and compiled to respectively Linear RNNs and Linear Gated RNNs. This\nsection aims to address the situation with non-linear state updates, that is, the cases of classic and\ngated RNNs (Eqs. (2) and (5)). Concretely, we show how every linear (Gated) RNN can be converted\nto a non-linear (Gated) RNN. Thus, we can compile any LSRL program (including Lsts. 1 and 2)\nalso to an RNN (if it has no Multi operations) or a Gated RNN.\nThe key idea is that the ReLU applied to the state updates in the non-linear architectures is an identity\noperation if its inputs are positive. Hence, we can split the states in positive and negative components,\nflip the sign of the negative component, pass them separately through the ReLU-which will act as an\nidentity as all elements will be non-negative- and then fuse the positive and negative components\nback together in the A matrix at the next time stepFormally, we can convert a Linear RNN state\nupdate into a classic RNN state update as following:\n$s_t = As_{t-1}+ Bx_t + b$\n$y_t = \\phi(s_t)$.\n$s_t = ReLU([ \\frac{A -A}{A A}] s_{t-1} +[ \\frac{B}{-B}] x_t+[ \\frac{b}{-b}] )$\n$y_t = h([\\frac{I}{-I}] s_{t})$\nAnother way to look at this is by recognizing that an RNN is equivalent to a Linear RNN with the\nexact same weights if the states are always non-negative. Hence, all we need is a trick to ensure the\nstates are non-negative. This approach works just as well for the Gated RNNs as the gating and the\nstate updates are independent from one another.\nUsing Eq. (11) we can compile any LSRL program to an RNN (Eq. (2)) or a Gated RNN (Eq. (5)).\nThis includes Lsts. 1 and 2. Hence, RNNs and Gated RNNs can be universal in-context approximators\nfor continuous and token-to-token functions. As any Gated RNN can be represented as a GRU model\n(App. C) or an LSTM (App. D), these models are too universal in-context approximators. The same\nnumerical stability issues discussed in Sec. 5 apply here and as a result, universal approximation\ncapabilities are probably more likely to occur in Gated RNNs than in RNNs."}, {"title": "Discussion and Conclusions", "content": "We developed LSRL: a programming language for specifying programs expressible with recurrent\nneural architectures. We then used LSRL to show that various architectures from the humble RNN\nto the state-of-the-art Linear Gated RNNs- can all be universal approximators in-context. That is,\nthere exist fixed models with these architectures which can be prompted to act as any token-to-token\nfunction or approximate any continuous function to an arbitrary precision.\nComparisson with the transformer architecture. Contemporary Linear RNNs attempt to chal-\nlenge the dominant role of the transformer architecture. At the same time, our understanding of\ntheir in-context abilities is significantly lacking behind that of the transfomer. This work makes\nan important contribution to this problem: we showed that Linear SSMs are not only universal\nin-context approximators but are potentially require shorter prompts than transformers $(O(e^{-d_{in}})$ vs\n$O(e^{-10^{-14}d_{in} -4d})$ from Petrov et al. 2024a). That approach also relies on the Kolmogorov-Arnold\nrepresentation theorem (Kolmogorov, 1957) which is notoriously unlikely to be useful in practice\n(Girosi and Poggio, 1989). Our constructions are much simpler, especially in the token-to-token case."}, {"title": "Computation Graph Debranching Rules", "content": "We convert the computation DAG resulting from the LSRL program into a path program by attending\nto the first node whose output is the input for multiple other nodes, i.e., the first branching node.\nPreparation step. Before we even start debranching we first pre-process the graph by fusing\nconsecutive nodes of the same type together. The specific rules are:\n\u2022 If a Lin node is followed by a single other Lin node, then fuse them together. This follows\ndirectly from the classical result that composing linear functions is a linear function.\n\u2022 If a ReLU node is followed by another ReLU node, we can drop one of them as ReLU is\nidempotent.\n\u2022 If a Lin is followed by a LinState, we can subsume the weight matrix A of the linear\nnode in the B matrix of the LinState, and the bias b of the Lin node in the bias b of the\nLinState.\n\u2022 If all inputs of a Concat node are the same, then this node only duplicates the input and\nhence can be safely replaced with a Lin layer.\nThe debranching process goes through the following cases in order. And iterates until there are no\nbranching nodes left, in other words, until the graph has become a path graph. We will refer to the\nnodes whose input is the branching node as subsequent nodes.\nCase 1A: If all subsequent nodes are Multi. As all Multi nodes that have the same input (the\nbranching node) they must all be producing the exact same output. Hence, only one can be kept. This\nremoves one branch.\nCase 1B: If subsequent nodes are a combination of Multi and other nodes. We add a single Lin\nlayer that acts as a bypass for the non-Multi nodes using the fact that multiplicatin by 1 is identity.\nThis is followed by a single Multi layer. We then add Slice operators between the new Lin layer\nand the non-Multi nodes. This keeps the number of branches unchanged but removes the Multi\nnode and the new branch can be handled by the other rules.\nCase 2: All subsequent nodes are LinState. LinState nodes can be fused into a single LinState\nnode by combining their states and update matrices. As each LinState may have different subsequent\nnodes itself, we add Slice nodes to extract the respective subspaces of the state. This keeps the\nnumber of branches unchanged but puts the graph into Case 5A.\nCase 3: All subsequent nodes are ReLU. We can replace them by a single ReLU node. This removes\none branch.\nCase 4: All subsequent nodes are Concat. One complication is that Concat nodes can depend\non other Concat nodes. So, we will restrict ourselves at this step by only treating the Concat nodes\nthat depend only on the branch node directly by replacing them with a single Lin node. The rest will\nbe handled by the Lin and Concat case (Case 10) or the only Lin case (Cases 5A and 5B). See the\nfollowing example:"}, {"title": "Error Bound on the Approximation Scheme for Continuous Functions", "content": "In Sec. 4.1 we outlined a strategy to perform universal in-context approximation for continuous\nfunctions with Linear RNNs. The full program is in Lst. 1 and an illustration of the scheme is"}, {"title": "Gated RNNs are GRU models", "content": "A GRU layer (Cho et al., 2014) with input $a_t \u2208 R^{d_{in}}$ and hidden state $h_{t\u22121} \u2208 R^{d_{hidden}}$, and output\n$h_t\u2208 R^{d_{hidden}}$ can be described as follows:\n$z_t = Sigmoid(W_zat + U_z h_{t\u22121}+b_z)$,\n$r_t = Sigmoid(W_rat + U_r h_{t-1} + b_r)$,\n$\\tilde{h}_t = tanh(W_hat + U_h(r_t \\odot h_{t\u22121}) + b_h)$,\n$h_t = (1 - z_t) \\odot h_{t\u22121} + z_t \\odot \\tilde{h}_t$,\nIn this section, we show a conversion of a single Gated RNN layer (Eq. (5)) to k + 2 GRU layers.\nHere, k is the number of layers in the y and h MLPs in Eq. (5). We first show that a single GRU layer\ncan be used to compute the updated state st and the output of the first layer of y when applied to\nxt. Then, every pair of single layers of y(xt) and $(s_t)$ can be represented as an individual GRU\nlayer. Finally, a single layer can be used to compute the element-wise multiplication y(xt) \u2299 (st).\nFor simplicity, we assume the Sigmoid and tanh nonlinearities are replaced by ReLUs. If not, they\ncan each be approximated with MLPs and hence also with additional GRU layers. Additionally, for\nconvenience we will assume $d_{in} = d_{hidden}$.\nRepresenting the state update as a GRU layer\nFor this layer we set $b_z = 1, W_z = 0, U_z = 0$ giving $z_t = 1$. Similarly, we set $b_r = 1, W_r = 0,\nU_r = 0$ giving $r_t = 1$. Thus, Eq. (14) reduces to:\n$h_t = \\tilde{h}_t = \u03c3(W_hat + U_h h_{t\u22121} + b_h)$,\nSetting at\n$=\\left[ \\begin{array}{c} x_t \\\\ s_{t-1}  \\end{array} \\right]$, where $x_t \u2208 R^{d_{in}/2}, h_{t-1} =\n\\left[ \\begin{array}{c} s_{t-1} \\\\ 0  \\end{array} \\right]$ where $s_{t-1} \u2208 R^{d_{hidden}/2}, W_h = \n\\left[ \\begin{array}{cc} B & 0 \\\\ A & 0  \\end{array} \\right]$.\n$U_h = \\left[ \\begin{array}{cc} A & 0 \\\\ 0 & 0  \\end{array} \\right], b_h\n=\\left[ \\begin{array}{c} b \\\\ -k_{lb}  \\end{array} \\right]$where $k_{lb}$ is a vector where every element in $k$ is a lower bound on\n$x_t$. results in Eq. (15) becoming:\n$h_t = \u03c3(\\left[ \\begin{array}{cc} B & 0 \\\\ A & 0  \\end{array} \\right] \\left[ \\begin{array}{c} x_t \\\\ s_{t-1}  \\end{array} \\right] +\\left[ \\begin{array}{cc} A & 0 \\\\ 0 & 0  \\end{array} \\right]\\left[ \\begin{array}{c} s_{t-1} \\\\ 0  \\end{array} \\right]+\\left[ \\begin{array}{c} b \\\\ -k_{lb}  \\end{array} \\right])$ =\\left[ \\begin{array}{c} \u03c3(Ast-1 + Bxt + b) \\\\ \u03c3(xt \u2013 klb)  \\end{array} \\right] = \\left[ \\begin{array}{c} \u03c3(st) \\\\ xt - k_{lb}  \\end{array} \\right]$"}, {"title": "Representing each MLP layer as a GRU layer", "content": "In these layers, similarly to the recurrent layer, we set $b_z = 1, W_z = 0, U_z = 0$ giving $z_t = 1$.\nIn the same way, we set $b_r = 1, W_r = 0, U_r = 0$ giving $r_t = 1$. Here, however, we set\n$W_h = \\left[ \\begin{array}{cc} \\phi_i & 0  \\end{array} \\right], U_h\n=\\left[ \\begin{array}{cc} 0 & 0  \\end{array} \\right]$ and $b_h=\\left[ \\begin{array}{c} b_{\\phi_i} \\\\ b_{\\psi_i}  \\end{array} \\right]$,\nexcept for the first of such layer where $b_h =\\left[ \\begin{array}{c} b_{\\phi_i} \\\\ b_{\\psi_i} + W_{\\psi k_{lb}} \\end{array} \\right]$.\nThus, for an input $a_t=\\left[ \\begin{array}{c} a_{1,t} \\\\ a_{2,t}  \\end{array} \\right]$ the layer output (Eq. (15)) for layer i is:\n$h_t = \u03c3(\\left[ \\begin{array}{cc} \\phi_i & 0  \\\\ \\phi_i & 0  \\end{array} \\right] \\left[ \\begin{array}{c} a_{1,t} \\\\ a_{2,t}  \\end{array} \\right]+\\left[ \\begin{array}{c} b_{\\phi_i} \\\\ b_{\\psi_i}  \\end{array} \\right])=\\left[ \\begin{array}{c} \u03c6_i(a_{1,t}) \\\\ \\varphi_i(a_{1,t})  \\end{array} \\right]$\nHere, \u03c6i and Yi are the i-th layers (including the ReLU) of respectively \u03c6 and y in Eq. (5)."}, {"title": "Representing the multiplicative gating with a single GRU layer", "content": "The only thing left is to model the element-wise multiplication of the outputs of \u03c6 and y in Eq. (5). We\ndo this using a GRU layer with $b_z = 0, W_z = 0, U_z = 0$. We set $b_r = 0, W_r = 0, U_r = 0$\ngiving $r_t = 0$. We also set $b_h = 0, W_h = \\left[ \\begin{array}{cc} 0 & I  \\\\ 0 & I  \\end{array} \\right], U_h = 0$. Thus, for an input $a_t=\\left[ \\begin{array}{c} a_{1,t} \\\\ a_{2,t}  \\end{array} \\right]$, the\noutput $h_t$ (Eq. (15)) of this GRU layer becomes:\n$h_t = \u03c3(\\left[ \\begin{array}{cc} 0 & 0  \\\\ 0 & 0  \\end{array} \\right] +\\left[ \\begin{array}{cc} 0 & I  \\\\ 0 & I  \\end{array} \\right]\\left[ \\begin{array}{c} a_{1,t} \\\\ a_{2,t}  \\end{array} \\right])$\nIf at is the output of a GRU layer constructed as in Eq. (18) (as is in our case), then it must be\nnon-negative. This is due to the ReLU application in Eq. (18). Hence, the application of another ReLU\nto a1,t in Eq. (19) can be safely removed as ReLU is idempotent and Eq. (19) simplifies to\n$\\left[ \\begin{array}{c} 0 \\\\ \u03c6_i \\odot a_1 \\end{array} \\right]$\nThus, this construction computes element-wise multiplication of $a_{1,t}$ and $a_{2,t}$."}, {"title": "Composing the operations to model a single Gated RNN layer", "content": "In order to represent Eq. (5), we use one GRU layer for the recurrence (as described in App. C.1),\nfollowed by k GRU layers modelling a pair of the k MLP layers of y and \u03c6 (App. C.2), completed\nwith a single mixing layer (App. C.3). This stack of k + 2 layers models exactly the Gated RNN\nlayer (Eq. (5)):\n$\u03c3(\\left[ \\begin{array}{cc} B & 0  \\\\ 0 & 0  \\end{array} \\right] \\left[ \\begin{array}{c} s_{t-1} \\\\ x_t  \\end{array} \\right]=\\left[ \\begin{array}{cc} A & 0  \\\\ 0 & I  \\end{array} \\right]$ b)$=\\left[ \\begin{array}{c} \u03c3(Ast-1 + Bxt + b) \\\\ [(\\phi_i)  \\varphi(s_t)] \\end{array} \\right]$"}, {"title": "Gated RNNS are LSTMS", "content": "A single LSTM layer (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) with input $a_t \u2208 R^{d_{in}}$,\nhidden state $h_{t\u22121} \u2208 R^{d_{hidden}}$, candidate memory cell $c_t \u2208 R^{d_{hidden}}$, memory cell $c_t \u2208 R^{d_{hidden}}$ and layer"}, {"title": "Gated Linear-RNNs are Hawk/Griffin Models", "content": "A single residual block of a Hawk/Griffin model (De et al., 2024) consists of two components, a\nrecurrent block for temporal mixing which makes use of a one-dimensional temporal convolution, as\nwell as real-gated linear recurrent unit (RG-LRU) and a gated MLP block. Specifically, we consider\nan input $a_t \u2208 R^{d_{in}}$, inputs to the blocks of dimensions $d_{in}$ and outputs from each block of dimensions\n$d_{in}$. Within blocks, all vectors have dimensionality $d_{hidden} = Ed_{in}$, where E is denotes an expansion\nfactor. Below, we formally describe the form of the recurrent and gated MLP blocks which are the\ntwo main components making up the residual blocks used for Hawk and Griffin.\nRecurrent block. The recurrent block consists of two branches. The first applies a one-dimensional\ntemporal convolution followed by a RG-LRU. The second branch simply performs a linear transfor-\nmation followed by a non-linearity, i.e. applies a single layer of an MLP."}, {"title": "Representing the state update using a recurrent block", "content": "Starting with the input to the Hawk model, which we denote $a_t$, we define this to be a function of the\ninput to the Gated RNN $x_t$ as $a_t=\\left[ \\begin{array}{c} x_t \\\\ 0  \\end{array} \\right]$. First, we set $W_a= I$ so that $a_t= h_t$. Next we choose\nmatrices $A = \\left[ \\begin{array}{cc} A & 0  \\\\ 0 & 0  \\end{array} \\right]$ and $B=\\left[ \\begin{array}{cc} B & 0  \\\\ 0 & 0  \\end{array} \\right]$ which we then use, with a convolutional window size\nof dconv = T to form the convolutional kernel $W_M = [B, AB, A^2B,..., A^{t-1}B,...]$. Setting the\nconvolutional bias as bconv =\n$\\left[ \\begin{array}{c} t-1  \\\\ 0 1  \\end{array} \\right]$ gives\n$z_t = \\sum_{i=0} W_M {[i]} M_{t[i]}+ b_{conv}$,\n= $B_{a_t} + ABa_{t-1} + \u00b7\u00b7\u00b7 + A^{t-1}Ba_1 +$\n$\\left[ \\begin{array}{c} h_{t} \\\\ 1 0 1  \\end{array} \\right]$\nNow, we pass $z_t$ through the RG-LRU. We set A = 0 so that \u03b1i = 0. We also define Wi = 0 and\nb = 1 so that it = 1. This gives us h_t = z_t, so that we pass the output of the one-dimensional\nconvolution through he RG-LRU.\nNext, let's focus on the second branch. Making use of the lower bound ka on the domain X, we set\nWg\n$\\begin{array}{c} I   0 \\end{array} \\\\  0 \\\\  -k_{lb} \\end{array}$ so that\n$g_t=\u03c3($[   gt       xt 1\\1 xt $We finally get the output of the recurrent block by defining Wo I and bo [ 0 \\k_{lb} ]) so that\\38 30 [ \u03c3 [ \u03c3 t 0 ]"}, {"title": "Representing the identity function using a recurrent block", "content": "We now show that we can pass an input unchanged through a recurrent block. Assume that the input to\nthe recurrent block is at= [1 xt. Then we define matrices A 0 -A 0 B I which we then use to form the convolutional kernel WM = [B, AB, A2B,..., At.1B,...]. Finally, setting the convolutional bias as bcony 0 results in zt at. From here, we can again set A 0, Wt 0 and bt 1 so that ht zt. Looking at the second branch and setting Wg 0 and bg 1 so that hy ht. Finally, we can simply output the input to the recurrent block by setting W I and bo 0 so that ot ht which means that ot at."}, {"title": "Representing each MLP layer as a gated MLP block", "content": "We can represent the MLP layers of the networks \u00f8(st) and y(xt) as described in Eq. (4) using Gated MLP blocks. We again denote the i-th layer of \u00f8 and y as pi and Yi. Assume that the input to the gated MLP block is a1.t1a2.1Then, on the first purely linear branch, let us define We I and be1 so that EtI = = The results in Emulating the layers of only one the two networks. Suppose without loss of generality (WLOG) that y has m layers and y has n layers where m < n. Suppose also that our input to the MLP block Again, on the first purely linear branch, let us define We I and be 1 so that EtI = = We can then stack 0 =0 a2+3 and m+1=8"}, {"title": "Representing multiplicative gating with a gated MLP block", "content": "The final thing we need to do is to compute an element-wise product of two vectors in order to match Again, assume that the input to Working with the first linear and bi We 0 Wi W by and b 0 so that and be Setting W I and by The results in This gives us We finally get the output of the gated MLP as the out put in this (21) for all"}, {"title": "Composing the operations to model a single gated linear-RNN layer", "content": "Now that we have all the individual layers, we can combine them so that we can use a Hawk model to emulate a single Gated RNN layer\nFirst we start by taking the"}]}