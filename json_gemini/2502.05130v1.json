{"title": "Latent Swap Joint Diffusion for Long-Form Audio Generation", "authors": ["Yusheng Dai", "Chenxi Wang", "Chang Li", "Chen Wang", "Jun Du", "Kewei Li", "Ruoyu Wang", "Jiefeng Ma", "Lei Sun", "Jianqing Gao"], "abstract": "Previous work on long-form audio generation using global-view diffusion or iterative generation demands significant training or inference costs. While recent advancements in multi-view joint diffusion for panoramic generation provide an efficient option, they struggle with spectrum generation with severe overlap distortions and high cross-view consistency costs. We initially explore this phenomenon through the connectivity inheritance of latent maps and uncover that averaging operations excessively smooth the high-frequency components of the latent map. To address these issues, we propose Swap Forward (SaFa), a frame-level latent swap framework that synchronizes multiple diffusions to produce a globally coherent long audio with more spectrum details in a forward-only manner. At its core, the bidirectional Self-Loop Latent Swap is applied between adjacent views, leveraging stepwise diffusion trajectory to adaptively enhance high-frequency components without disrupting low-frequency components. Furthermore, to ensure cross-view consistency, the unidirectional Reference-Guided Latent Swap is applied between the reference and the non-overlap regions of each subview during the early stages, providing centralized trajectory guidance. Quantitative and qualitative experiments demonstrate that SaFa significantly outperforms existing joint diffusion methods and even training-based long audio generation models. Moreover, we find that it also adapts well to panoramic generation, achieving comparable state-of-the-art performance with greater efficiency and model generalizability. Project page is available at https://swapforward.github.io/.", "sections": [{"title": "1. Introduction", "content": "Diffusion models learn to generate data by progressively adding noise to existing samples and subsequently reversing this process to recover the original data. They originally achieved remarkable success in text-to-image and have rapidly expanded into text-to-video and text-to-audio. One significant challenge in diffusion models is length extrapolation, which aims to generate varying lengths long-form output using diffusion models trained on constrained-length data.\nLong-form soundscapes and background music are in high demand for ambiance enhancement in real-life applications (e.g., in-car audio, sleep aids) and digital products (e.g., movies, video games). Existing approaches to audio generation can be broadly categorized into Language Models (LMs) and Diffusion Models (DMs) . For LMs, most works adopt autoregressive architectures, suffering from temporal causality constraints . For long-form audio generation, as duration extends, these models are observed to produce increasing accumulate errors, with more severe repetition issues. For DMs, most prior works formulate audio generation as a 2D mel-spectrogram denoising process, typically constrained to 10-second durations. Although Make-An-Audio2 and Flexible DiT  allow variable-length generation, they show weak length exploration capability for longer duration. More recently, Stable Audio has been trained directly on long-form audio sequences, but it requires significant computational resources. Additionally, only a limited maximum window version is open-sourced, and it remains sensitive to text prompts. On the other hand, taming pretrained audio generation models to produce seamless and coherent long-form audio in a training-free manner remains underexplored in audio generation."}, {"title": "2. Methodology", "content": "Diffusion models initially operate in the original feature space , but have recently been extended to the VAE latent space, achieving higher fidelity and compression rates. From the perspective of Latent Diffusion Models (LDMs), most modality generation tasks (e.g. audio and image) can be effectively reformulated as latent generation problem. Considering a long latent generation based on a reference latent diffusion model \\( \\Phi \\) and conditions \\( \\{y\\}=0 \\), our target is to generate a 2D long latent map \\( J \\in \\mathbb{R}^{C \\times H \\times W} \\) (H \u00ab W) through a joint diffusion process \\( \\Psi \\) by merging a sequence of subview latent \\( \\{X_i\\}_{i=1}^n \\in \\mathbb{R}^{C \\times H \\times W} \\). The subview mappings \\( F_i \\), from overview J to subviews \\( X_i \\),\n\\( F_i \\leftrightarrow \\mathbb{R}(F_i) : J \\rightarrow X_i, i \\in [n] \\)\ncan be considered as a 1D sliding window process. Considering that in the spectrum H \u00ab W, we simplify the overlap mapping \\( I_{i,i+1} \\) between adjacent subviews \\( X_i \\) and \\( X_{i+1} \\) as:\n\\( I_{i,i+1} \\rightarrow \\mathbb{R}(F_i) \\cap \\mathbb{R}(F_{i+1}) \\)\nand the non-overlap mapping \\( M_{i,i+1,i-1} \\) of subview \\( X_i \\) as:\n\\( M_{i,i+1,i-1} \\rightarrow \\mathbb{R}(F_i) - (\\mathbb{R}(F_{i-1}) \\cup \\mathbb{R}(F_{i+1})) \\)\nFor a joint diffusion step \\( \\Psi (J_t \\mid Y) \\) at time-step t, most previous work apply averaging operator to synchronize different"}, {"title": "2.1. Joint Diffusion for Long Latent Generation", "content": "Diffusion models initially operate in the original feature space, but have recently been extended to the VAE latent space, achieving higher fidelity and compression rates. From the perspective of Latent Diffusion Models (LDMs), most modality generation tasks (e.g. audio and image) can be effectively reformulated as latent generation problem. Considering a long latent generation based on a reference latent diffusion model \\( \\Phi \\) and conditions \\( \\{y\\}=0 \\), our target is to generate a 2D long latent map \\( J \\in \\mathbb{R}^{C \\times H \\times W} \\) (H \u00ab W) through a joint diffusion process \\( \\Psi \\) by merging a sequence of subview latent \\( \\{X_i\\}_{i=1}^n \\in \\mathbb{R}^{C \\times H \\times W} \\). The subview mappings \\( F_i \\), from overview J to subviews \\( X_i \\),\n\\( F_i \\leftrightarrow \\mathbb{R}(F_i) : J \\rightarrow X_i, i \\in [n] \\)\ncan be considered as a 1D sliding window process. Considering that in the spectrum H \u00ab W, we simplify the overlap mapping \\( I_{i,i+1} \\) between adjacent subviews \\( X_i \\) and \\( X_{i+1} \\) as:\n\\( I_{i,i+1} \\rightarrow \\mathbb{R}(F_i) \\cap \\mathbb{R}(F_{i+1}) \\)\nand the non-overlap mapping \\( M_{i,i+1,i-1} \\) of subview \\( X_i \\) as:\n\\( M_{i,i+1,i-1} \\rightarrow \\mathbb{R}(F_i) - (\\mathbb{R}(F_{i-1}) \\cup \\mathbb{R}(F_{i+1})) \\)\nFor a joint diffusion step \\( \\Psi (J_t \\mid Y) \\) at time-step t, most previous work apply averaging operator to synchronize different"}, {"title": "2.2. Comparative Analysis of Spectra and Image Latent", "content": "To address the overlap distortion problem in Fig. 1, we first conduct a comparative analysis of the VAE latent spaces for different modalities, a research area that has received limited prior investigation. Our main insight comes from the differences between the original features of the mel-spectrogram and RGB images. The mel-spectrogram is a 2D time-frequency representation of the audio signal, showing amplitude variability across frequency bands over time. Compared with RGB images, most mel-spectrograms show high-frequency variability, where amplitude values in time-frequency bins exhibit weak connectivity, characterized by sparsity and discreteness, without forming continuous contours as normal images. Further, in Fig. 2 (a), such discrepancy can extend to the VAE latent due to its connectivity inheritance via the channel-wise linear mapping with the original feature, which is first observed in image generation and has been served as a fast previewer to replace the VAE decoder 1. In this paper, we provide a formal formulation and extend it to spectrum generation. Specifically, given an image or spectrum \\( X \\in \\mathbb{R}^{C_x \\times W \\times H# } \\) and its VAE latent representation \\( Z \\in \\mathbb{R}^{C_z \\times W2 \\times H_z} \\), a constant linear mapping \\( W \\in \\mathbb{R}^{C_x \\times C_z} \\) along the channel dimension satisfies\n\\( Downsample(X) \\approx W \\cdot Z \\)"}, {"title": "2.3. Latent Swap Joint Diffusion", "content": "Building on the above findings, our target is to enhance high-frequency details while avoiding aliasing in latent map of overlap region. Focusing on the overlapping region, the essence of joint diffusion can be regarded as the merging of stepwise differentiated trajectories \\( x_t^i \\) and \\( x_t^{i+1} \\) that share the same previous-step initial overlapping latent \\( I_{i,i+1}(J_{t+1}) \\),\n\\( I_{i,i+1}(J_t) = W_i \\cdot Right(X_t^i) + (1 \u2013 W_i) \\cdot Left(X_t^{i+1}) \\)\nwhere Left(\u00b7) and Right(\u00b7) represent subregion mapping functions for the left and right overlapping regions. Thus,"}, {"title": "Stepwise Differentiated Trajectories", "content": "Building on the above findings, our target is to enhance high-frequency details while avoiding aliasing in latent map of overlap region. Focusing on the overlapping region, the essence of joint diffusion can be regarded as the merging of stepwise differentiated trajectories \\( x_t^i \\) and \\( x_t^{i+1} \\) that share the same previous-step initial overlapping latent \\( I_{i,i+1}(J_{t+1}) \\),\n\\( I_{i,i+1}(J_t) = W_i \\cdot Right(X_t^i) + (1 \u2013 W_i) \\cdot Left(X_t^{i+1}) \\)\nwhere Left(\u00b7) and Right(\u00b7) represent subregion mapping functions for the left and right overlapping regions. Thus,\n\\( 0 <\\epsilon_l \\le d(\\text{Right } [\\Phi (F_i(J_{t+1}), Y_i)], \\)\n\\( \\text{Left } [(\\Phi (F_{i+1}(J_{t+1}), Y_i)]) \\le \\epsilon_u < 1 \\)\nwhere the lower bound \\( \\epsilon_l \\) prevents complete similarity, the upper bound \\( \\epsilon_u \\) restricts excessive divergence, and d(\u00b7) is a distance metric.\nLeveraging the properties of stepwise differentiated trajectories, we introduce the latent swap operator \\( \\mathbb{W}_{swap} \\) that consisting of binary elements 0 and 1, as a subset of the weight matrix \\( \\mathbb{W}_i \\) in Eq. 7. Compared to the averaging operator with constant matrix \\( \\mathbb{W}_{avg} = c \\cdot 1_{m \\times n} \\), the binary swap operator samples and preserves the original denoised latent from \\( x_t^i \\) and \\( x_t^{i+1} \\) rather than smoothing with each other. The similarity of step-wise difference trajectory and the robustness of the diffusion model ensure controlled distribution to the joint diffusion trajectory. On the other hand, from a frequency analysis perspective, unlike the averaging operator, which acts as an all-pass filter, the latent swap operator functions as a band-pass filter, adaptively enhancing specific frequency components based on stepwise differential trajectory differences. By controlling the swap interval and swap direction, we can selectively enhance particular frequency bands. In this paper, we adopt a frame-level latent swap operator \\( \\mathbb{W}_{swap} \\) with swap interval m,\n\\( W_{swap} = 1_n \\otimes U_m,  U_m = \\frac{1}{2} [1-(-1)^{\u230a\\frac{i}{m}\u230b}] \\)\nWe choose the optimized swap interval w = 1, following the experimental details shown in Appendix 5.1. As delineated in Fig. 2 (c), we conduct Fourier frequency analysis"}, {"title": "Stepwise Differentiated Trajectories", "content": "Leveraging the properties of stepwise differentiated trajectories, we introduce the latent swap operator \\( \\mathbb{W}_{swap} \\) that consisting of binary elements 0 and 1, as a subset of the weight matrix \\( \\mathbb{W}_i \\) in Eq. 7. Compared to the averaging operator with constant matrix \\( \\mathbb{W}_{avg} = c \\cdot 1_{m \\times n} \\), the binary swap operator samples and preserves the original denoised latent from \\( x_t^i \\) and \\( x_t^{i+1} \\) rather than smoothing with each other. The similarity of step-wise difference trajectory and the robustness of the diffusion model ensure controlled distribution to the joint diffusion trajectory. On the other hand, from a frequency analysis perspective, unlike the averaging operator, which acts as an all-pass filter, the latent swap operator functions as a band-pass filter, adaptively enhancing specific frequency components based on stepwise differential trajectory differences. By controlling the swap interval and swap direction, we can selectively enhance particular frequency bands. In this paper, we adopt a frame-level latent swap operator \\( \\mathbb{W}_{swap} \\) with swap interval m,\n\\( W_{swap} = 1_n \\otimes U_m,  U_m = \\frac{1}{2} [1-(-1)^{\u230a\\frac{i}{m}\u230b}] \\)\nWe choose the optimized swap interval w = 1, following the experimental details shown in Appendix 5.1. As delineated in Fig. 2 (c), we conduct Fourier frequency analysis on the denoised latent from the overlapping region where the latent swap operation is applied. The result indicates that the latent swap operator shows the same trend as the reference curve from the non-overlapping region. This implies that the latent swap operation successfully enhances high-frequency components while minimizing disturbances to low-frequency components, thereby contributing to a superior output in terms of both fidelity and detail preservation in the overlap region. At a high level, the latent swap operation applied to the overlap region \\( I_{i,i+1} \\) is performed sequentially across each subview (including between the first and last subviews), forming a loop swap process without central guidance. Hence, we name it Self-Loop Latent Swap. Moreover, we find the swap operation is also well adapted to panorama images generation, as shown in Fig. 4 and 6, achieving better-blended transitions and preserving more details."}, {"title": "Reference-Guided Latent Swap", "content": "To mitigate cross-view inconsistency, Multidiffusion adopts a small sliding window shift to avoid non-overlapping regions, which performs poorly due to the lack of explicit guidance and inefficiency with excessive redundant subview diffusion. SyncDiffusion provides centralized guidance by optimizing initial subview denoising with LPIPS loss. However, it incurs nearly 10\u00d7 higher time cost, and the LPIPS loss is not compatible with the intermediate denoised mel-spectrogram. To address these issues, we propose the unidirectional Reference-Guided Latent Swap, designed to efficiently achieve global cross-view consistency in forward-only manner (e.g., timbre or SNR for audio, and style or color for images). Specifically, for the early \\( r_{guide} \\times T \\) of the denoising steps, we refine the non-overlapping trajectory \\( M_i(J_t) \\) guided by the shared independent reference diffusion trajectory \\( x_1^0 \\) to achieve cross-view consistency with swap operator \\( \\mathbb{W}_{refer} \\):\n\\( M_i(J_t) = \\mathbb{W}_{refer} \\cdot Middle(\\widetilde{X}_t) + (1 \u2013 \\mathbb{W}_{refer}) \\cdot Middle (X_i) \\)\nThe Reference-Guided Latent Swap can be considered a frame-level blended diffusion process that guides the non-overlapping trajectory \\( M_i(J_t) \\) to align with the reference trajectory \\( x_t^1 \\), while maintaining coherence with nearby overlapping trajectories \\( I_{i-1,i}(J_t) \\) and \\( I_{i,i+1}(J_t) \\). For the later \\( (1 - r_{guide}) \\times T \\) of the denoising steps, similar to SDEdit, the non-overlapping trajectories start from the similar intermediate denoised latent maps to achieve cross-view consistency and avoid repetition simultaneously. By adjusting the swap timing (early \\( r_{guide} \\times T \\) stage) and the swap interval (w in Eq. 9), we can achieve a similarity-diversity trade-off for the global coherence. As shown in Fig. 5, we take panorama generation as an example for better visualization and we observe increased similarity but decreased diversity as \\( r_{guide} \\) increases. Additionally, excessive guidance after 60% of the late denoising steps has been observed to cause artifacts. As a result, we implement SaFa with \\( r_{guide} = 0.3 \\). More samples in audio generation are shown in Appendix Fig. 8. As for the swap interval, we follow the implementation in Eq. 9, adopting a frame-wise column swap with w = 1. For image generation, considering the flattening order in"}, {"title": "3. Experiment", "content": "We compare our approach with other joint diffusion method in Tab. 2, including MultiDiffusion (MD), an enhanced version MultiDiffusion (MD*) with the triangular window, and Merge-Attend-Diffuse (MAD).  is not implemented on audio generation, since LPIPS loss is observed to be insensitive to intermediate denoised mel-spectrograms. Further, we compare SaFa with other training-based long audio generation models includes AudioGen, Stable Diffusion Audio (SD-audio) and Make-An-Audio2 (Make2) on large-scale audio generation benchmark in Appendix 5.1.\nIn Tab. 2, we implement all methods on two pretrained text-to-audio (TTA) models based on the AudioLDM framework. One adopts a masked DiT architecture , while the other uses AudioLDM's original U-Net architecture. Both models incorporate a FLAN-T5 text encoder and a pretrained 2D spectrum-based VAE model, and are trained on the same dataset and pipeline with variable audio, music and speech clips during 0.32s to 10.24s following . In inference stage, we use a DDIM sampler with 200 denoising steps and a classifier-free guidance scale of 3.5. As for long-form generation, to simplify spectrum generation and facilitate user studies, we evaluate all the method to produce 24s looped audio at a 16kHz sample rate by combining three overlapping 10-second segments with a overlap rate \\( r_{overlap} = 0.2 \\). In SaFa, the Reference-Guided Latent Swap is applied during the initial 60 steps with \\( r_{guide} = 0.3 \\). Following, MAD is applied during the first 60 of 200 steps to achieve optimal results. Nine text prompts are used (three soundscape, three sound effect and three music), as listed in the first nine audio qualitative examples in Appendix 8."}, {"title": "3.1. Long-Form Audio Generation", "content": "We compare our approach with other joint diffusion method in Tab. 2, including MultiDiffusion (MD), an enhanced version MultiDiffusion (MD*) with the triangular window, and Merge-Attend-Diffuse (MAD). is not implemented on audio generation, since LPIPS loss is observed to be insensitive to intermediate denoised mel-spectrograms. Further, we compare SaFa with other training-based long audio generation models includes AudioGen, Stable Diffusion Audio (SD-audio) and Make-An-Audio2 (Make2) on large-scale audio generation benchmark in Appendix 5.1.\nIn Tab. 2, we implement all methods on two pretrained text-to-audio (TTA) models based on the AudioLDM framework. One adopts a masked DiT architecture , while the other uses AudioLDM's original U-Net architecture. Both models incorporate a FLAN-T5 text encoder and a pretrained 2D spectrum-based VAE model, and are trained on the same dataset and pipeline with variable audio, music and speech clips during 0.32s to 10.24s following . In inference stage, we use a DDIM sampler with 200 denoising steps and a classifier-free guidance scale of 3.5. As for long-form generation, to simplify spectrum generation and facilitate user studies, we evaluate all the method to produce 24s looped audio at a 16kHz sample rate by combining three overlapping 10-second segments with a overlap rate \\( r_{overlap} = 0.2 \\). In SaFa, the Reference-Guided Latent Swap is applied during the initial 60 steps with \\( r_{guide} = 0.3 \\). Following, MAD is applied during the first 60 of 200 steps to achieve optimal results. Nine text prompts are used (three soundscape, three sound effect and three music), as listed in the first nine audio qualitative examples in Appendix 8."}, {"title": "Evaluation Metrics", "content": "Following AudioLDM, Frechet Distance (FD) and Frechet Audio Distance (FAD) are used for quality and fidelity estimation (similar to FID score in image generation), which are based on two audio classifier models PANNs and VGGish respectively. KL divergence (KL) is also used at a pair level. Following previous work in panorama generation , we first utilize the reference model to generate 500 10-seconds audio clips per prompt, obtaining the reference set. Since the objective metrics models are trained on 10-second clips, we sequentially extract 10-second segments from each long-form generated audio with a sliding window, obtaining multiple 500-sample evaluation subsets per prompt. Then we calculate FD, FAD, and KL scores between these subsets and the reference set, and the results are averaged to obtain the final score. As the reference, we also calculate these scores between two equal-sized"}, {"title": "Quantitative Result", "content": "As shown in Tab. 2, in both DiT and U-Net models, SaFa consistently outperforms other methods significantly in semantic alignment (CLAP) and generation quality (FD, FAD, and KL). SaFa approaches reference-level performance in CLAP and KL, demonstrating the latent swap operator's superiority in enhancing time-frequency resolution and reducing confusion over the averaging operator. Compared to SaFa*, which only use Self-Loop Swap, SaFa achieves greater cross-view consistency (ILPIPS and ICLAP) through Reference-Guided Swap. MD* surpasses MD, consistent with, but remains inferior to SaFa. MAD employs block-wise averaging operations, leading to further spectrum distortion with low quality. Additionally, it causes a position embedding repetition problem, resulting in monotonous and repetitive subviews with low ILPIPS score."}, {"title": "Qualitative Result and User Study", "content": "In Fig.6, compared with MD, MD* and MAD, SaFa preserves more spectral details without distortion in the overlap on both U-Net and DiT architectures. More qualitative comparisons are provided in the Appendix 8. Furthermore, we conduct user studies on the generated audio samples to enhance evaluation reliability, collecting a total of 34 valid responses in which participants ranked generated samples based on audio quality, and global consistency e.g. style, timbre and SNR. The result aligns well with quantitative performance, showing a significant advantage for SaFa over other methods. The detail settings and results of user study are available in the Appendix 6 and Fig. 10."}, {"title": "3.2. Panorama Generation", "content": "We compare our method with MultiDiffusion (MD), Merge-Attend-Diffuse (MAD) and SyncDiffusion on Stable Diffusion v2.0 with U-Net architecture and Stable Diffusion v3.5 with MMDiT architecture, respectively. For SaFa, Reference-Guided Latent Swap is applied in the initial 15 steps with \\( r_{guide} = 0.3 \\). We utilize the same six prompts derived from the previous work and each prompt generating 500 panorama images. The target panorama resolution is set to 512 \u00d7 3200 composed of subview images with a resolution of 512 \u00d7 640. For other methods, following, the overlap rate of adjacent subviews \\( T_{overlap} \\) is set to 0.8, with a slide window stride of 128 pixels to obtain optimal performance. Specifically, for SaFa with explicit guidance in non-overlap region, we implement a much lower overlap rate \\( r_{overlap} = 0.2 \\) to achieve high efficiency while maintaining high quality. All experiments are conducted on a NVIDIA RTX A100 GPU."}, {"title": "Experiment Settings", "content": "We compare our method with MultiDiffusion (MD), Merge-Attend-Diffuse (MAD) and SyncDiffusion on Stable Diffusion v2.0 with U-Net architecture and Stable Diffusion v3.5 with MMDiT architecture, respectively. For SaFa, Reference-Guided Latent Swap is applied in the initial 15 steps with \\( r_{guide} = 0.3 \\). We utilize the same six prompts derived from the previous work and each prompt generating 500 panorama images. The target panorama resolution is set to 512 \u00d7 3200 composed of subview images with a resolution of 512 \u00d7 640. For other methods, following, the overlap rate of adjacent subviews \\( T_{overlap} \\) is set to 0.8, with a slide window stride of 128 pixels to obtain optimal performance. Specifically, for SaFa with explicit guidance in non-overlap region, we implement a much lower overlap rate \\( r_{overlap} = 0.2 \\) to achieve high efficiency while maintaining high quality. All experiments are conducted on a NVIDIA RTX A100 GPU."}, {"title": "Evaluation Metrics", "content": "FID and KID is utilized to measure the fidelity and diversity. Using the reference model, we generate 500 images per prompt at a resolution of 512 \u00d7 512 to form the reference set. Correspondingly sized subview are cropped sequentially by a sliding window operation to obtain the evaluation datasets and then calculate scores between evaluation and reference dataset to obtain the final averaging result. As the reference, we also compute these scores between two equal random splits of the reference images. Intra-LPIPS and Intra-StyleL assess internal consistency by dividing each panorama into five non-overlapping regions and randomly cropping one 512 \u00d7 512 subview from each. And we calculate these score across 10 pairwise combinations per image and average it on all samples to obtain the final score. As a reference, we use the reference dataset and randomly select 1,000 pairs to compute average ILPIPS and IStyleL. Mean CLIP score based on CLIP model"}, {"title": "Quantitative Result", "content": "In Tab. 3, compared to MD with averaging operator, SaFa* shows much better cross-view coherence (IStyleL, ILPIPS) and generation quality (FID, KID), highlighting the effectiveness of the latent swap operator in achieving smoother transitions and preserving detail in both mel-spectrogram and image generation. With Reference-Guided Swap, SaFa improves global coherence (IStyleL, ILPIPS) and ensures strong subview consistency. It also outperforms MAD and SyncD with lower FID and KID scores, indicating higher quality and smoother transitions. The ILPIPS gap with SyncD arises from its direct LPIPS-loss minimization in gradient descent. MAD performs significantly worse in DiT than U-Net due to its reliance on adapting the self-attention layer for long latent input. SD 3.5 struggles with length exploration due to its sole reliance on the transformer block and lack of convolutional layers in U-Net. In terms of time consumption, our method achieves the highest efficiency, being 11 ~ 14\u00d7 faster than SyncD."}, {"title": "Qualitative Result and User Study", "content": "As shown in Fig. 6, MD produces seamless outputs but lacks color and style consistency. SaFa matches MAD and SyncD in global cross-view consistency and smooth transitions without extra computation or time costs. Furthermore, user studies confirm the quantitative results in generation quality and cross-view coherence. The result of user study Fig.11, setting details 6 and more qualitative results 8 are shown in Appendix."}, {"title": "4. Conclusion and Discussion", "content": "In this paper, we present Swap Forward (SaFa), a simple but efficient latent swap framework through two fundamental swap operators to generate seamless and coherence long-form audio generation. Compared to previous techniques, Swap Forward is more adaptable to various modality tasks (long audio and even panorama images) across different diffusion architectures. As a high-performance alternative to the averaging operation, this operator can be widely applied in existing joint diffusion methods to achieve state-of-art performance without additional time and computational cost. As a point of discussion, while the latent swap operator is observed performs well in the 2D VAE latent spaces of spectra and images, several areas remain for exploration. For example, its practicality for 1D wav-based VAE latents or other discrete token-based representations like Residual Vector Quantization requires further investigation."}, {"title": "5. More Quantitive Experiments", "content": "We further aim to use the open-source audio diffusion checkpoints to conduct quantitative comparison of the joint diffusion methods, although most of the models are limited in compatibility with joint diffusion methods. For example, AudioLDM and Tango are trained with a fixed 10.24 seconds window to generate fixed-length audio clips. During training, shorter audio clips are padded with zeros up to 10.24 seconds, resulting in outputs that end with silence. Consequently, for joint diffusion methods based on these two models that concatenate subview latent maps, sudden silence is often observed in the overlap regions. Stable Diffusion Audio is also trained on a fixed 96-second window size and can produce variable-length outputs by end-cutting, which is also challenging to adapt for joint diffusion methods. In comparison, similar to our training pipeline, Make-An-Audio2 is trained with variable-length audio without excessive padding. It splits samples into different buckets based on their length during training, and within each batch, samples are randomly selected from the same bucket. However, we still observe some anomalous behavior when applying Make-An-Audio2 with Joint Diffusion."}, {"title": "5.1. Long-Form Audio Generation", "content": "We further aim to use the open-source audio diffusion checkpoints to conduct quantitative comparison of the joint diffusion methods, although most of the models are limited in compatibility with joint diffusion methods. For example, AudioLDM and Tango are trained with a fixed 10.24 seconds window to generate fixed-length audio clips. During training, shorter audio clips are padded with zeros up to 10.24 seconds, resulting in outputs that end with silence. Consequently, for joint diffusion methods based on these two models that concatenate subview latent maps, sudden silence is often observed in the overlap regions. Stable Diffusion Audio is also trained on a fixed 96-second window size and can produce variable-length outputs by end-cutting, which is also challenging to adapt for joint diffusion methods. In comparison, similar to our training pipeline, Make-An-Audio2 is trained with variable-length audio without excessive padding. It splits samples into different buckets based on their length during training, and within each batch, samples are randomly selected from the same bucket. However, we still observe some anomalous behavior when applying Make-An-Audio2 with Joint Diffusion."}, {"title": "Comparison on Open-Source Checkpoint", "content": "We further aim to use the open-source audio diffusion checkpoints to conduct quantitative comparison of the joint diffusion methods, although most of the models are limited in compatibility with joint diffusion methods. For example, AudioLDM and Tango are trained with a fixed 10.24 seconds window to generate fixed-length audio clips. During training, shorter audio clips are padded with zeros up to 10.24 seconds, resulting in outputs that end with silence. Consequently, for joint diffusion methods based on these two models that concatenate subview latent maps, sudden silence is often observed in the overlap regions. Stable Diffusion Audio is also trained on a fixed 96-second window size and can produce variable-length outputs by end-cutting, which is also challenging to adapt for joint diffusion methods. In comparison, similar to our training pipeline, Make-An-Audio2 is trained with variable-length audio without excessive padding. It splits samples into different buckets based on their length during training, and within each batch, samples are randomly selected from the same bucket. However, we still observe some anomalous behavior when applying Make-An-Audio2 with Joint Diffusion."}, {"title": "Comparison with Training-Based Methods", "content": "We further compare our method with more extensive training-based long audio generation models, including both diffusion models and language models. While comparing absolute performance between models trained on different datasets and with varying model sizes is not meaningful, the relative performance degradation on each model with increasing audio generation length can highlight the strengths and weaknesses of these methods.\nThe training-based baselines include: (1) AudioGen (Au-Gen): An autoregressive model based on learned discrete audio representations, supporting ultra-"}, {"title": "Effect of Guidance Steps and Swap Interval", "content": "In Fig. 8, we further demonstrate the progressive transition from cross-view diversity to similarity by varying guide in both mel-spectrum and panorama generation using Reference-Guided Latent Swap. All other settings for SaFa remain consistent with Section 3. As shown in Fig. 8, using an appropriate trajectory guidance rate \\( r_{guide} \\), e.g., 20% to 40%, results in unified cross-view coherence while preserving the diversity of local subviews. However, as the guidance rate \\( r_{guide} \\) increases beyond 60%, excessive repetition and artifacts begin to appear. This occurs because Reference-Guided Swap is a unidirectional operation, where the denoising process of the reference view is independent and unaffected by each subview. Consequently, it does not adapt as seamlessly to subviews in the later stages as the bidirectional Self-Loop Swap does. This is also one of the reasons why we restrict Reference-Guided Swap to the early denoising stages.\nTo further explore the effects of the swap interval w (in Eq. 9), we apply the Self-Loop Latent Swap with various w values in spectrum generation, as shown in Fig. 5.1. We observe that using a small swap interval (e.g., 1 or 2), corresponding to higher swap frequencies, produces smoother transitions. Conversely, larger w values indicate larger swap units, resulting in less seamless transitions between sub-views. This outcome aligns with the high-frequency variability of mel tokens, leading us to default the Self-Loop"}, {"title": "5.2. Panorama Generation", "content": "In Tab.6, We utilize SD 2.0 model to estimate performance of SaFa on panorama images with resolutions of 512 \u00d7 1600, 512 \u00d7 3200, and 512 \u00d7 4800. As a result, SaFa maintains stable and great performance across all evaluated metrics in different length output."}]}