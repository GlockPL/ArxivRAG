{"title": "DAILYDILEMMAS: REVEALING VALUE PREFERENCES OF LLMS WITH QUANDARIES OF DAILY LIFE", "authors": ["Yu Ying Chiu", "Liwei Jiang", "Yejin Choi"], "abstract": "As we increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of the users. We present DAILYDILEMMAS, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma includes two possible actions and with each action, the affected parties and human values invoked. Based on these dilemmas, we consolidated a set of human values across everyday topics e.g., interpersonal relationships, workplace, and environmental issues. We evaluated LLMs on these dilemmas to determine what action they will take and the values represented by these actions. Then, we analyzed these values through the lens of five popular theories inspired by sociology, psychology and philosophy. These theories are: World Value Survey, Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik Wheel of Emotion. We find that LLMs are most aligned with the self-expression over survival values in terms of World Value Survey, care over loyalty in Moral Foundation Theory. Interestingly, we find huge preferences differences in models for some core values such as truthfulness e.g., Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to select it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their released principles reflect their actual value prioritization when facing nuanced moral reasoning in daily-life settings. We find that end users cannot effectively steer such prioritization using system prompts.", "sections": [{"title": "INTRODUCTION", "content": "With AI being increasingly integrated into everyday life, concerns about its adherence to human ethics have intensified, as earlier highlighted by Asimov's Three Laws of Robotics. Each law shares ties with human values: harmlessness with the first law, obedience with the second law, and self-preservation with the third law. Yet, these laws fall short in real-world complex scenarios like moral dilemmas. Considering the classic Trolley Problem: it must choose between allowing the trolley harm five people or redirecting the trolley to harm one person. Regardless of the decision, the robot would violate the first law, proving the ambiguity of such \"laws\" in practice. Beyond theoretical scenarios, we can imagine AI systems of today and tomorrow encountering numerous non-clear-cut decisions in daily life, related to moral judgments. However, it remains uncertain how AI can effectively solve value conflicts, especially based on human preferences and ethical standards. Therefore, exploring everyday moral challenges is crucial for advancing our understanding of machine ethics.\nHere, we propose to explore everyday moral dilemmas to understand how AI systems prioritize values during conflicts, ensuring they follow human preferences. Earlier efforts by ETHICS dataset (Hendrycks et al., 2020) and Delphi (Jiang et al., 2021) focus on simple clear-cut situations with widely agreeable moral standards. For instance, the ETHICS dataset studied scenarios (e.g., breaking the building is morally wrong) while Delphi crafted descriptive ethical judgments to cover cases where some moral principles have to be breached for other more important ones (e.g., breaking the building to save a child is acceptable). More recently, Value-kaleido (Sorensen et al., 2024) explored the pluralistic values from simple situations (e.g., Biking to work instead of driving).\nAs LLMs became better aligned recently, such simple scenarios have become less challenging for them. Relative to these works, our paper focuses on complex situations that can realistically reflect moral quandaries faced by humans in day to day life. For each situation, we consider values from the perspective of various parties involved, with the potential for different values and perspectives to conflict with one another. For instance, a situation of \u201cwhether to stay late to finish the project at company for the potential promotion reward but break your promise with your spouse to go back home early to help with kids.\" can elicit value judgments as considered by different parties (e.g., you, your spouse, your kids, your colleagues). MoralExceptionQA (Jin et al., 2022) explored similarly complex dilemmas, albeit in a highly constricted domain by constructing a small dataset to study scenarios concerning three particular morality rules (e.g., No cutting in line).\nTo enhance the study on more realistic and diverse dilemmas, we created DAILYDILEMMAS, a dataset comprising of 1,360 moral dilemmas spanned across everyday topics from interpersonal relationships to social issues such as environmental issues. These dilemmas, created by GPT-4, are non-clear-cut with no definitive right answers. Compared to other data collection methods, using LLMs to generate dilemmas reduces privacy-related and ethical risks (e.g., asking Reddit users about sensitive moral concerns, especially without full appreciation of how such data is used). We validate the resemblance of our dataset to real-world data, showing that generated dilemmas and values are similar to those faced by people.\nEach dilemma contains a situation with two possible actions, with the involved parties and corresponding human values labelled for each action, as shown in Fig. 1. For instance, one dilemma involves deciding whether to eat a dish you dislike that your friends prepared. Choosing eating captures friend's care in preparing meals for you. On the other hand, choosing not to eat reflects your honesty in expressing your true feelings about the food. The competing values (care vs. honesty) challenge the models to navigate value conflicts in a binary-choice dilemma. Through such dilemmas, we can understand how LLMs prioritize certain values over others, thereby revealing their underlying value preferences.\nOur DAILYDILEMMAS included 301 human values analyzed through the lens of five popular theories. These theories are: 1) World Value Survey, 2) Moral Foundation Theory, 3) Maslow's Hierarchy of Need, 4) Aristotle's Virtues, 5) Plutchik Wheel of Emotion. We chose five theories that were commonly used by researchers (i.e. cited by thousands), and have captured public imagination (i.e. talked about in popular media). This balances the rigor of these theoretical frameworks with the frequency of such values appearing in pre/post-training text used to train LLMs, reducing the likelihood that representations of these values are biased due to long tail distributions. These theories, borrowed from sociology, psychology, and philosophy, help understand and compare models' value preferences in a broader picture. For instance, the six evaluated LLMs (e.g., GPT-4-turbo, Llama-3 70b) uniformly showed their preferences on self-expression over survival on the culture axis from World Value Survey (WVS, 2024). We also found large differences in model preferences for certain"}, {"title": "VALUE-BASED FRAMEWORK ON MORAL DILEMMAS", "content": "To better understand the moral reasoning in diverse real-world setting, we adopt value-based framework and select the five theories on World Value Survey (WVS, 2024), Moral Foundation Theory (Graham et al., 2013), Maslow's Hierarchy of Needs (Maslow, 1969), Aristotle's Virtues (Thomson, 1956), and Plutchik Wheel of Emotion (Plutchik, 1982) due to the balance between rigor of framework and frequency of values appeared in training corpus by these popular theories. Without taking a hard stance on moral philosophy approaches, we hope our investigation on values to allow productive investigations by serving the intermediate grounds (values) on different moral frameworks e.g., Consequentialism and Deontology, that are hard to directly study under the real-world setting we aim to cover.\nConsequentialism as exemplified by Benthamian Utilitarianism. The utility of particular actions can be subjective and thus noisy to model directly. For instance, we can use our earlier example of deciding between staying late to finish the project at company for a potential promotion and holding a promise with one's spouse to go back home early to help with the kids. Different people value a promotion and good relations with their spouse wildly differently (e.g., depending on their financial and familial situations), meaning that directly estimating the utility of such outcomes is likely to result in extremely large variance.\nTo better understand how people derive utility from each action, our current framework maps each action to various values based on our five theories. For instance, workaholics might prefer staying late because they prioritize the values of ambition and self-actualization more than maintaining harmony and building trust within their family. Analyzing the value preferences behind various actions can provide a principled approach towards calculating utility of complex real-world actions, by weighing such utility based on the importance an individual places on various values. We believe this is something our work can empower others to do."}, {"title": "OUR FRAMEWORK ON MORAL DILEMMA AND ASSOCIATED VALUES", "content": "Definition of moral dilemma We define a daily-life moral dilemma situation to be D with different group(s) of people involved as initial parties $p_{initial}$. The main party ($p_0$) acts as the decision making agent in dilemma D. In each dilemma D, we designed to have only two possible actions \u2013 'to do' $A_{do}$ and 'not to do' $A_{not}$ with complement condition of $A_{not} = (A_{do})^C$. In other words, the decision making agent $p_0$ is required to do one of two actions A but cannot do both actions in our dilemma D (McConnell, 2024).\nInduction-driven approach on values. Inspired by the concept on considering the infinite agents in infinite worlds to involve more values (Bostrom, 2011; Askell, 2018), we propose a computationally-tractable approach to extract values v invoked by parties p for both actions A in our dilemma D. For each A, we generated many affected parties to see things in different perspective as a way to broaden our scope inspired by psychologist Piaget Perspective-taking approach (Piaget, 2013). With the concept of Loss Aversion that people care more about negative consequences (Kahneman & Tversky, 2013), we include the negative consequences of our decision making agent ($p_0$) to deepen our consideration on D.\nValues by agents involved in two actions of dilemma. More specifically, two negative consequence stories denoted as $S_{do}$ and $S_{not}$, which stemmed from the $A_{do}$ and $A_{not}$ respectively, are generated for capturing more parties and associated values. In each S, a sequence of possible events $E_r$ is proposed with more parties involved $p_s$. This process helps to extrapolate possible parties such that we have all possible parties to be $p_k$ in D. It included the initial parties $p_{initial}$ and parties $p_s$ from story S, noting that $i < j + k$ due to possible repetition. Then, to capture all the possible values v invoked by each party p, we find the perspectives P (how party p is being affected in negative consequences with the invoked human values v). There are $P_j$ with corresponding $v_q$ and $r_q$ in total for each S, such that $j < q$. In other words, each party p could have more than one perspectives P including the values v. To understand the value preferences of LLMs in later sections, we grouped the values $v_{do}$ gathered by the described process in $A_{do}$ together and the values $v_{not}$ as another group to formulate our daily-life moral dilemma as value conflicts."}, {"title": "DAILYDILEMMAS: DATASET CONSTRUCTION", "content": "We use GPT-4 to generate daily-life moral dilemma situations embedded with value conflicts, as shown in Fig. 1. Technical details and prompts are in Appendix 7.5. Examples of moral dilemma generated are in Table 2 while a complete example of moral dilemma and its corresponding elements are on Table 3.\n(1) Formulate Moral Dilemma To generate a non-clear-cut dilemma, we sampled the actions (When you don't like a certain food, eating it.) from Social Chemistry as seeds (Forbes et al., 2020). The dilemma generated consists of three parts \u2013 i) Background: A sentence describes the role or the scene of the main party. (You are a guest at a friend's house for dinner and they serve a dish you dislike.); ii) Conflict Point: a sentence includes a story of why it is a moral dilemma. It is usually a turning point by giving some new conditions that make the main party fall into a dilemma. (Your friend put a lot of effort into preparing the meal and you don't want to offend them by not eating); iii) Question for action: a question that asks for binary action decisions. (Do you force yourself to eat the food you dislike to avoid hurting your friend's feelings or not?)"}, {"title": "IMAGINE NEGATIVE CONSEQUENCES", "content": "We then prompt the model to generate around 80-word stories on negative consequences for each of the actions. For instance, when the main party (you) decides to eat the food (Action 1), the negative consequence is your stomach rebels... Your friend feels guilty."}, {"title": "CAPTURE PERSPECTIVES", "content": "We designed a multi-step Chain-of-Thought to consider different parties' views. Based on the negative consequences stories, we first ask the model to identify all the related parties. Then, we prompt the model to give a fundamental human value related to the party first and then give the corresponding reasoning. For instance, from the negative consequence story on eating the food, the model identified a related party friends. Then, the model generated a value of Care and then provided a reason preparing meal to show kindness on it."}, {"title": "DAILYDILEMMAS: DATASET ANALYSIS", "content": null}, {"title": "TOPIC MODELLING AND STATISTICS", "content": null}, {"title": "ANALYSIS ON VALUES GENERATED WITH THEORIES", "content": "We obtained the total number of values from two actions to assess the scope of human fundamental values covered in DAILYDILEMMAS. We realized that no single theory fully encompasses all human fundamental values. Consequently, by considering the balance between rigor of framework and frequency of values appeared in training corpus, we utilized five diverse theories to gain a deeper understanding of the value preferences which are explained in detail in Appendix 7.2. The distribution of generated values across these five theories is in Fig. 3.\n(1) World Value Survey. Our dataset contains more dilemmas focusing on the scale of Self-expression vs. Survival compared to Secular-rational vs. Traditional. This suggests that the GPT-4 model emphasizes areas like subjective well-being, self-expression, and quality of life, alongside economic and physical security, rather than topics such as religion, family, and authority. Notably, English-speaking countries, such as the USA, show significant preference for Self-expression as opposed to Survival compared to other nations (WVS, 2024), indicating that GPT-4 may reflect cultural value preferences specific to these countries."}, {"title": "LLMS VALUES PREFERENCES WITH THEORIES ON THE DAILY DILEMMAS", "content": "World Value Survey. All LLMs favor Self-expression values, such as equality for foreigners and gender equality, over Survival values, which focus on economic and physical security. Additionally, the study highlighted inconsistency in LLM preferences on Traditional vs. Secular-rational values. More specifically, unlike other models, Claude-haiku and Mixtral-8x7B tend to neglect on Secular-rational values by -2.29% on average with preferences differences of 6% relative to other models.\nMoral Foundation Theory. LLMs are generally exhibit similar preferences on Care, Authority, and Purity. However, Mixtral-8x7B and Claude-haiku models tend to neglect the Fairness dimension with -1.89% on average by preference difference of 9.5% compared to other models. Additionally, the Mixtral model uniquely shows a higher tendency to neglect the Loyalty dimension relative to other models. We noticed that the Mixtral model has a neutral preference on Purity, and we discussed this in our limitation Section 7.3.\nMaslow Hierarchy Of Needs. All models tend to neglect Safety e.g., physical safety over other needs. More specifically, GPT-4-turbo and Llama-3-70B models show a stronger preference for Self-esteem and Love and belonging relative to Claude and Mixtral models.\nAristotle Virtues. All LLMs consistently show negative preferences for Ambition and Friendliness. Interestingly, there is a mixed attitude towards Truthfulness, a core value that researchers aim to align with (Bai et al., 2022). Claude-haiku and Mixtral-8x7B models tend to deprioritize Truthfulness shown by 7.9% values neglected on average, unlike other models which tend to favor it with 9.36% values selected. Similarly, for dimensions on Patience, Courage, and Liberality, models exhibit varied preferences. Specifically, GPT-4-turbo and Llama-3-70B show less preference for Patience, whereas other models are positively inclined toward it. For Courage, the Mixtral model remains neutral, while others show a clear positive preference. Lastly, the preference differences for Liberality are minor, with models like GPT-4-turbo and Llama-3-70B less likely to prioritize it.\nPlutchik Wheel of Emotions. LLMs show similar preferences on various emotions such as Joy, Fear, Optimism, and Trust. However, Joy is notably preferred over Optimism, despite both being positive emotions. Fear is generally less preferred by all models. For Trust, GPT-4-turbo and Llama-3-70B show a slightly higher preference relative to other models."}, {"title": "PERFORMANCE OF LLMS ON ALIGNING HUMAN VALUES WITH THEIR STATED PRINCIPLES", "content": "Based on Anthropic Constitutional AI (Anthropic, 2024) and OpenAI ModelSpec(OpenAI, 2024), we assess how their LLMs (Claude-haiku, GPT-4-turbo) adhere to the values they are trained on using DAILYDILEMMAS. To map the values with principles shared, we first prompted GPT-4-turbo to identify the human values from our collected 301 values shown in Table 5, revealing conflicts between supporting and opposing values within each principle. We repeated the process 10 times, assigning weights to values based on their empirical probabilities to signify their importance in dilemmas. Then, dilemmas exhibiting similar value conflicts from DAILYDILEMMAS were identified and used to prompt responses from models for each principle. We assessed the models' value preferences in these scenarios using a weighted score difference that combines the importance of each value and its selection frequency in the responses for two groups of values."}, {"title": "CASE STUDY: ANTHROPIC CONSTITUTIONAL AI", "content": "The Claude-haiku model shows inconsistent value preference patterns across value conflicts related to their principles. We highlighted this with two examples in Table 1, showcasing its preference for the supporting values on principle 45 and preference for opposing values on principle 56. A comprehensive list of principles and their value preferences are detailed in Table 6.\nFor principle 45, Claude-haiku model prioritizes supporting values tied to human safety (such as safety, risk, caution) over opposing values related to freedom (innovation, curiosity, freedom of expression, autonomy), with a resultant positive weighted score difference of 17.9. This demonstrates that Claude-haiku model favors safety-related values over those of freedom, confirming its alignment with the principle aiming to minimize existential risks to humanity."}, {"title": "CASE STUDY: \u039f\u03a1\u0395\u039d\u0391\u0399 MODELSPEC", "content": "Similarly, GPT-4-turbo model also shows the inconsistency in value preferences on the value conflicts tested for their principles. We demonstrated this with the principle 12 (preference on supporting values) and principle 4 (preference on opposing values) respectively in Table 1. The complete list of principles and corresponding calculations on our two metrics is in Appendix 9.\nFor principle 12, the model emphasizes supporting values linked to openness and respect (e.g., respect for personal choice, openness) over opposing values tied to authority and control (e.g., influence, control, conformity), achieving a positive weighted score difference of 2.1. This highlights the model's adherence to prioritizing informing over influencing, thus respecting user opinions without attempting to change them.\nConversely, under principle 4, despite its purpose on protecting people's privacy, the model skews towards opposing values related to knowledge disclosure (e.g., curiosity, transparency), with a negative weighted score difference of -5.5. This indicates a misalignment with the principle's aim, showing a preference for disclosing information over protecting user privacy."}, {"title": "STEERABILITY OF LLMS ON ALIGNING HUMAN VALUES FOR END USERS", "content": "In this section, we explore the steerability of LLMs towards aligning human values in DAILYDILEMMAS. Currently, many closed-sourced models (e.g. from OpenAI and Anthropic) are only accessible through sending prompts to an API. Therefore, we designed a system prompt modulation experiment with GPT-4-turbo model, based on the principles stated in OpenAI Model Spec."}, {"title": "CONCLUSION", "content": "We introduce DAILYDILEMMAS, a dataset marking an important step in understanding how LLMs align with and prioritize human values when navigating value conflicts in daily-life settings."}, {"title": "TECHNICAL DETAIL AND PROMPTS ON DAILYDILEMMAS CONSTRUCTION", "content": null}, {"title": "FORMULATING DAILY-LIFE MORAL DILEMMA SITUATIONS", "content": "We sampled actions from the Social Chemistry dataset (Forbes et al., 2020), which gathers real user input from different subreddits, as the seeds of real-life situations. To generate a non-clear-cut dilemma situation, we choose actions with less unified agreement among the community, which are controversial (50% anticipated agreement) and common group (75%-90% anticipated agreement).\nThen, the short seed actions (e.g., When you don't like a certain food, eating it.) is used to prompt the daily-life dilemma situations, which consists of three parts \u2013 1) basic background information: A sentence describes the role of the main party or the scene that the main party is. (e.g., You are a guest at a friend's house for dinner and they serve a dish you dislike.). 2) Conflict Point: A sentence includes a story of why it is a moral dilemma. It is usually a turning point of the whole dilemma story by giving some new conditions that make the main party fall into a dilemma. (e.g., Your friend put a lot of effort into preparing the meal and you don't want to offend them by not eating) 3) Question asking for the following action: A question that asks for binary action decisions. We ask the model to start with \"Do\"/\"Does\"/\"Should\"/\"Would\" to force the framing of the question to cover only two actions. (e.g., Do you force yourself to eat the food you dislike to avoid hurting your friend's feelings or honestly admit that you don't like the dish?)."}, {"title": "IMAGINING NEGATIVE CONSEQUENCES IN THE DILEMMA DECISION MAKING", "content": "A psychological concept referred to as loss aversion serves as the backbone of the dataset construction. This concept is generally understood as a cognitive bias wherein the psychological impact of losses is perceived as twice as potent as that of gains. Consequently, negative consequences following decision-making processes often draw substantial attention from humans. To assess the significance of the potential adverse outcomes faced by the main party (decision maker) in the dilemma, we asked the model to indicate the two actions (to do or not to do) and present the corresponding two potential negative consequences (of approximately 80 words). For example, in the previously generated dilemma situation (e.g., Do you force yourself to eat the food you dislike to avoid hurting your friend's feelings or honestly admit that you don't like the dish?), the two actions will be 'to do' (e.g., to eat) or 'not to do' (e.g., not to eat) generate two potential negative consequences (e.g., For the action of 'to do', the main party (you) force yourself to eat and suffered from food poisoning. Your friends feels guilty about it.) (e.g., For the action of 'not to do', the main party (you) refuse to eat the food. Your friend feels hurt and strains your relationship with your friend."}, {"title": "CAPTURING DIFFERENT PARTIES' PERSPECTIVES", "content": "Following the generation of negative consequences for two possible actions in the dilemma decision-making process, we aim to gather a wider range of perspectives from people. To accomplish this, we instructed the model to generate step by step. First, the model is guided to identify the possible parties involved in the negative consequences. Second, the model is direct to deduce the corresponding fundamental human value that could connect to the party within the context of the given scenario. Consequently, the process generates reasons grounded with the scenario to allow us for further analysis."}, {"title": "Extrapolating Possible Parties involved", "content": "Once the model generates stories about potential negative outcomes, it is then guided to identify the relevant parties that might be involved directly or indirectly. This highlights the range of parties that could be influenced by the consequent circumstances after a decision is made. Specifically, direct parties refer to those groups that are explicitly affected, usually bearing the immediate consequences from the resulting consequences (e.g., in the previous dilemma example of eating food made by your friend that you dislike, the direct parties are 'you' and 'your friend'). On the other hand, indirect parties are the groups that are subtly influenced by the chain of impacts from the negative consequence."}, {"title": "Gathering Perspectives for Each Parties", "content": "Our goal is to capture the perspective that comprises the party involved, the potential human value, and the reasoning to support connections of the value within the context of a given scenario. For constructing fundamental human values, to begin with, we prompt the model to construct fundamental human values associated with the engaged party, identified from the negative consequences in the previous subsection (e.g., in the previous dilemma example of eating food made by your friend that you dislike, one fundamental human value could be 'Respect for others' effort' for the party 'You')."}, {"title": "LIMITATIONS", "content": "Strong guard on Mixtral-8x7B model It is notable that the Mixtral-8x7B model has a stronger guard on answering all these moral dilemmas, relative to other tested models. It tends to avoid answering the moral dilemma and say 'it is challenging'. Therefore, we added a stronger instruction prompt (You must answer either one action.) to force it by answering either one action. It gives answers to 74.85% dilemmas at the end and we will consider such limitation during analysis, in which such limitation is brought by the implicit value preference on the Mixtral model on certain values. The percentage of answering is sufficient for dimensions with high counts shown in Fig. 3 and we took account of it during analysis.\nOne analysis regarding this is the Mixtral model's neutral preference found on the value of Purity in Moral Foundation Theory. The Mixtral model may avoid answering the dilemmas about the value of Purity. Our analysis cannot fully reveal the model's preference for certain values when one refuses to answer a majority of dilemmas relating to certain values. Therefore, our analysis took concern of it and we only report the findings with reduced dimensions so that the certain dimension has relatively high proportions on our main text based on our proportions found in Fig. 3. The full dimensions of the six models can also be found in Appendix 6.\nBias on culture With the known Western bias on LLMs and its training dataset (Santy et al., 2023)(Arora et al., 2023)(Cao et al., 2023), the data we generated by GPT-4 models could inherit the same bias. To assess the quality and validate the dataset, the authors evaluated the data with the grounding of real-world data. Although the validation data, primarily sourced from Reddit and predominantly representing Western viewpoints, may not completely address concerns about cultural inclusiveness. Our dataset aims to encompass everyday scenarios prevalent across various cultures. Our topic modeling analysis in Section 4 reveals that the topics collated in our dataset are generally universal. To mitigate this inherent bias, future studies should aim to include a broader range of situations from diverse cultural backgrounds.\nCulture influence on dilemma We designed to have a non-clear-cut dilemma with no definitive right answer. We noted that some dilemmas presented may have definitive answers for some cultures. For example, a dilemma related to committing adultery is illegal in some cultures e.g., Qatar, and South Korea. However, the values conflict embedded in the dilemma could still exist."}, {"title": "DATA LICENSE", "content": null}, {"title": "DAILY DILEMMAS USAGE", "content": "Our dataset is generated by the OpenAI GPT-4 model. Use of this dataset should comply with OpenAI terms of use."}, {"title": "VALIDATION ON DAILYDILEMMAS BY REDDIT", "content": "We collected the r/AITA-filtered subreddit through the official Reddit data access program for developers and researchers."}]}