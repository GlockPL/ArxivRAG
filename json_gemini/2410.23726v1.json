{"title": "Towards Reliable Alignment: Uncertainty-aware RLHF", "authors": ["Debangshu Banerjee", "Aditya Gopalan"], "abstract": "Recent advances in aligning Large Language Models with human preferences have benefited from larger reward models and better preference data. However, most of these methodologies rely on the accuracy of the reward model. The reward models used in Reinforcement Learning with Human Feedback (RLHF) are typically learned from small datasets using stochastic optimization algorithms, making them prone to high variability. We illustrate the inconsistencies between reward models empirically on numerous open-source datasets.\nWe theoretically show that the fluctuation of the reward models can be detrimental to the alignment problem because the derived policies are more overfitted to the reward model and, hence, are riskier if the reward model itself is uncertain. We use concentration of measure to motivate an uncertainty-aware, conservative algorithm for policy optimization. We show that such policies are more risk-averse in the sense that they are more cautious of uncertain rewards. We theoretically prove that our proposed methodology has less risk than the vanilla method.\nWe corroborate our theoretical results with experiments based on designing an ensemble of reward models. We use this ensemble of reward models to align a language model using our methodology and observe that our empirical findings match our theoretical predictions.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning with Human Feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019) is an influential training approach in modern artificial intelligence research, particularly in the domain of large language models (LLMs). Notable examples include the revolutionary ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), Gemini (Team et al., 2023) and LLaMA-3 (Meta, 2024). RLHF is a fine-tuning method to align the behavior of LLMs with human values and preferences. It has been instrumental in addressing challenges related to model alignment, where the goal is to ensure that an AI system adheres to specific ethical, safety, and utility guidelines defined by its\nhuman users. The standard reward-model RLHF framework (Ouyang et al., 2022; Bai et al., 2022b; Touvron et al., 2023) assumes a preference model based on an underlying reward model to accurately capture human preferences. The reward model is trained to predict how well a given response aligns with preferences provided by human evaluators, thus acting as a proxy for human judgment. It is a reward signal in downstream reinforcement learning to improve the LLM.\nA critical issue in RLHF is the reliability of the learned reward model. For example, look at Figure 1, which shows the reward score assigned to the same prompt-response pair by 10 independently trained identical reward models on the same preference data. Several factors contribute to the uncertainty and potential unreliability of the reward model:\n\u2022 Limited Dataset Size: The reward model is typically trained on a much smaller dataset than the vast corpora used to pre-train the LLM. For instance, while an LLM may be pre-trained on billions of tokens, the reward model might be trained on a few hundred thousand human-labeled prompt-response pairs. This discrepancy in the data scale can limit the generalization capability of the reward model, leading to noisy estimates of response quality.\n\u2022 Stochastic, Incomplete Optimization: The reward model is trained using stochastic gradient descent (SGD) or variants, introducing inherent randomness into the optimization process. Using mini-batches of data means that different instances of the reward model, even when trained on the same dataset, may produce different evaluations of the same response due to the randomness in parameter updates. This stochasticity can result in high variance in the model's predictions. Additionally, the optimization process to find a reward model is not completed \u2013 typically 1 or 2 passes over the dataset (Stiennon et al., 2020; Meta, 2024) \u2013 to avoid overfitting.\nThus, a single reward model should not be viewed as an infallible oracle for assessing response quality. Its predictions are inherently uncertain, leading to challenges when fine-tuning the LLM. Overfitting the LLM to a noisy reward model can result in"}, {"title": "Contributions", "content": "We enumerate the contributions made in this work:\n1. We provide comprehensive empirical evidence using open-source datasets to demonstrate the variability inherent in reward modeling.\n2. We introduce a conservative policy optimization method incorporating uncertainty measures derived from reward model training.\n3. We rigorously demonstrate, through theoretical analysis and experiments on LLMs, that our risk-aware conservative policy scheme significantly reduces the likelihood of policy degradation."}, {"title": "RLHF preliminaries", "content": "The standard RLHF setup (Christiano et al., 2017; Ziegler et al., 2019) is described as follows. Given a prompt x, the LLM generates two re-sponses, y\u00b9 and y\u00b2. A human evaluator selects the preferred response, forming a dataset of the form (x\u1d62, y\u0142, yz)\u1d62\u208c\u2081, where x\u1d62 is the prompt, and y\u0142, yz are model-generated responses. These pairwise comparisons encode ordinal preferences, used to train the reward model. The reward model, r\u03b8, assigns a scalar reward to each prompt-response pair (x, y), reflecting its likelihood of being preferred. The Bradley-Terry model (Bradley and Terry, 1952) estimates the probability that y\u00b9 is preferred over y\u00b2 as: P(y\u00b9 is preferred over \u00b7y\u00b2) = \u03c3(r\u03b8(x,y1) \u2013 r\u03b8(x,y2)), where \u03c3(z) = \\frac{1}{1+e^{-z}} is the logistic sigmoid function. The reward model is trained by minimizing the negative log-likelihood of human preferences: min \u03b8 \u03a3\u1d62\u208c\u2081 - ln \u03c3(r\u03b8(x\u1d62, y\u0142) \u2013 r\u03b8(x\u1d62, yz)). This loss function seeks to adjust the parameters \u03b8 of the reward model such that the predicted rewards for preferred responses are consistently higher than those for less preferred responses, as judged by human evaluators. Using the Bradley-Terry model ensures that the reward model produces outputs that align with human feedback. Once trained, the reward model is used to fine-tune the LLM via reinforcement learning (e.g., PPO (Schulman et al., 2017)). The objective is to maximize the reward for new prompts while constraining divergence from the reference policy \u03c0\u2080:\nmax \u03c0 Ex~D, y~\u03c0(\u00b7|x) [r\u03b8(x, y)], s.t. KL(\u03c0||\u03c0\u2080) < \u03b5,                                                                                                                                                                 (1)\nSolving this optimization adjusts the LLM to generate responses that align with the reward model to better reflect human preferences. However, the reward function r\u03b8 above in Equation 1 can be inherently highly variable, as seen in Figure 1.\nTo illustrate the impact of uncertainty in reward models, consider a simple three-armed bandit problem. Aligning a language model can be viewed as a contextual bandit scenario where the policy assigns probabilities to each arm to maximize the expected return. In this example, the true rewards (shown in green in Figure 2) are r\u2081 < r\u2082 < r\u2083, with Arm 1 having the lowest mean reward and Arms 2 and 3 having higher rewards. However, the estimated rewards (depicted in blue as R\u2081, R\u2082, and R\u2083) inaccurately suggest that Arm 1 has the highest reward. If probabilities are assigned solely based on"}, {"title": "2 Mathematical Modeling", "content": "Notations: We assume that prompts are strings denoted by x from a prompt set X, and responses are strings denoted by y from a response set Y. A reward model assigns a scalar value to each prompt-response pair (x, y). We consider the learned reward model R as a sample estimate of the true human-representative reward model r*. Assuming X and Y are finite with cardinalities |X| and |Y|, respectively, both R and r* can be viewed as elements of R\u02e3\u02b8. A large language model, for our purposes, is a policy \u03c0that defines a distribution over responses y given a prompt x. We also introduce a distribution D over prompts, representing their ambient frequency in nature. With a slight abuse of notation, we treat the policy as the induced joint distribution over prompts and responses. This allows us to simplify notation by expressing the average reward E_{x~D, y~\u03c0(\u00b7|x)} [R(x, y)] as R\u00af\u03c0. We denote a covariance matrix by \u03a3, use ||\u00b7||\u2082 to represent the Euclidean (l2) norm, and define ||x||\u00b2\u03a3 as the quadratic form x\u1d40\u03a3x.\nWe consider the true reward function r*, which is unknown, and the learned reward model R, which estimates r* but is subject to noise due to finite and imperfect training data. We assume:\nAssumption 2.1. For any (x, y), the estimated reward R(x, y) is a Gaussian perturbation of r* (x, y):\nR(x,y) = r*(x, y) + N(0, \u03c3\u00b2(x, y)),\nwhere N(0, \u03c3\u00b2(x, y)) is a Gaussian random variable with mean zero and variance \u03c3\u00b2 (x, y). We assume that the estimates R(x, y) are independent across different (x, y).\nThus, R ~ N(r*, \u03a3), where \u03a3 is a diagonal matrix with entries \u03c3\u00b2(x, y). Our goal is to optimize the policy \u03c0 to maximize the expected reward estimated by R. Let \u03c0\u2080 be a reference policy (e.g., from pre-training), and define d = \u03c0 - \u03c0\u2080. Since R ~ N(r*, \u03a3), the scalar R\u1d40d is normally distributed with mean r*\u1d40d and variance d\u1d40\u03a3d: R\u1d40d ~ N (r*\u1d40d, d\u1d40\u03a3d). To prevent the policy \u03c0 from deviating too much from the reference policy \u03c0\u2080, we constrain d to lie within a feasible set D \u2282 R\u02e3\u02b8."}, {"title": "Lower Bound on the True Objective Function", "content": "The following theorem provides a bound on the optimization problem that accounts for the uncertainty in the reward estimates. The proof is presented in Appendix 6.\nTheorem 2.2. Under Assumption 2.1, for any \u03b2 > 0, the following holds with probability at least 1 - exp(- \u03b2\u00b2\u03b4):\nsup d\u2208D R\u1d40d - \u03b2||d||\u03a3 \u2264 sup d\u2208D r*\u1d40d.\nThe above theorem implies that the optimization problem on the left-hand side is a high-probability lower bound for the true optimization problem, which depends on the unknown reward function r*. Given that r* is not directly available, but we do have access to noisy estimates R, we propose the following optimization problem as a practical substitute:\nsup d\u2208D R\u1d40d - \u03b2||d||\u03a3.                                                                                                                                                                  (2)\nThis formulation leads to the following constrained optimization problem:\nmax \u03c0\u1d40R subject to (\u03c0 \u2013 \u03c0\u2080)\u1d40\u03a3(\u03c0 \u2013 \u03c0\u2080) \u2264 \u03b5,\n\u03c0\nfor some \u03b5 > 0. The weighted constraint on the policy update penalizes deviations more heavily for prompt-response pairs with higher variance in the reward estimates, thereby incorporating the uncertainty into the optimization process.\nNote that similar variants of the constrained optimization problem have been explored previously in the literature. For example, the unconstrained version of our approach is equivalent to the vanilla policy gradient method (Sutton et al., 1999). The standard RLHF formulation typically employs the PPO algorithm (Schulman et al., 2017), which is defined with a KL-divergence constraint, although the choice of distance metric is not unique. For example, an l2 approximation of the KL-divergence constraint, resulting in the unweighted constraint: ||\u03c0-\u03c0\u2080||\u2082 \u2264 \u03b5. Another widely used technique is the natural policy gradient, as implemented in the Trust Region Policy Op-timization (TRPO) algorithm (Schulman, 2015). TRPO adjusts the constraint based on the Fisher information matrix I, leading to the constraint: (\u03c0 \u2013 \u03c0\u2080)\u1d40I(\u03c0 \u2212 \u03c0\u2080) \u2264 \u03b5, where I adapts the penalization according to the sensitivity of the policy.\nIn our experiments, we use a variance-adjusted KL-divergence constraint:\nEz~D,y~\u03c0(\u00b7|x) \u03c3\u00b2(x, y) ln (\u03c0(y|x)/\u03c0\u2080(y|x)) \u2264 \u03b5.\nThis formulation integrates seamlessly with existing PPO subroutines, such as those provided in the TRL Library (von Werra et al., 2020)\u00b9.\n\u00b9TRL package from Hugging Face"}, {"title": "3 Theoretical Analysis", "content": "We compare the performance of the variance-aware LLM alignment methodology with its variance-unaware counterpart to evaluate how incorporating reward estimate uncertainty affects policy robustness and effectiveness, especially in scenarios with noisy reward estimates. We consider two policies, \u03c0\u2081 and \u03c0\u2082, derived from different optimization formulations.\nDefinition 3.1 (Variance-Unaware Policy, \u03c0\u2081). The policy obtained by solving the unweighted l2 constraint problem:\n\u03c0\u2081 = argmax \u03c0 R subject to ||\u03c0 - \u03c0\u2080||\u2082 \u2264 \u03b5.\n\u03c0\nDefinition 3.2 (Variance-Aware Policy, \u03c0\u2082). The policy obtained by solving the variance weighted l2 constraint problem:\n\u03c0\u2082 = argmax \u03c0 R subject to ||\u03c0 - \u03c0\u2080||\u00b2\u03a3 \u2264 \u03b5.\n\u03c0\nTo compare both methods fairly, we set \u03b5\u0304 = \u03bbmin(\u03a3)\u00b7 \u03b5; this has the effect of aligning the largest ellipsoid of the covariance-weighted constraint with the sphere of the traditional l2 constraint.\nWe evaluate the expected true rewards \u03c0\u1d40r* for i = 1, 2, where r* is the true (unknown) reward vector for both methods and compare them to \u03c0\u1d40\u2080r*. We aim to show that \u03c0\u2082 is less likely to underperform relative to \u03c0\u2080 than \u03c0\u2081, indicating that the variance-aware method is less risky when reward estimates are uncertain.\nConsider policies \u03c0\u2081 and \u03c0\u2082 as defined in Definitions 3.1 and 3.2 respec-tively. With \u03b5\u0304 set as \u03bbmin(\u03a3)\u03b5 to ensure the optimization domain of the variance-aware method is only as large as the variance unaware method, we have the following result:\nP(\u03c0\u1d40\u2082r* < \u03c0\u1d40\u2080r*) \u2264 P(\u03c0\u1d40\u2081r* < \u03c0\u1d40\u2080r*).\nThus, the variance-aware method (\u03c0\u2082) has a lower probability of underper-forming relative to \u03c0\u2080 than the variance-unaware method (\u03c0\u2081). Theorem 3.3 highlights the trade-off between risk and reward. While the variance-unaware policy (\u03c0\u2081) may achieve higher rewards when R is accurate, it is riskier as it ignores estimate uncer-tainty. The variance-aware policy (\u03c0\u2082) reduces underperformance risk by accounting for reward estimate variance. The proof of the theorem is presented in Appendix 6.\nOur variance-aware policy is closely related to another reward-to-variability ratio known in finance literature as the Sharpe Ratio (Sharpe, 1966), which balances expected return against risk.\nConsider the optimization problem:\nmax \u03c0 Ex~D, y~\u03c0(\u00b7|x) [R(x,y)]\nsubject to Ex~D, y~\u03c0(\u00b7|x) \u03c3\u00b2(x, y) ln (\u03c0(y|x)/\u03c0\u2080(y|x)) \u2264 \u03b5,"}, {"title": "4 Reward Modeling", "content": "In this section, we discuss the process of reward modeling using the Gemma-2B-it model (Team et al., 2024), an instruction-tuned version of the foundational model Gemma-2B. Our reward modeling methodology uses an ensemble of models, specif-ically 10 independent reward models, to compute the reward variance across different instances of the same prompt-response pair. This ensemble-based approach allows us to better capture the uncertainty in the reward estimates and to analyze the vari-ability between otherwise identical reward models. The following paragraphs detail the methodology used to learn the ensemble of reward models, the dataset used for training and evaluation, and the observations drawn from the ensemble's performance across multiple benchmarks.\nTo train our reward models, we utilize an existing open-source preference dataset (Dong et al., 2024), which is available publicly via HuggingFace\u00b2. This cu-rated dataset contains approximately 50, 000 labeled preference pairs. It is constructed by combining several well-known, open-source datasets. The included datasets are HH-RLHF (Bai et al., 2022a), SHP (Ethayarajh et al., 2022), HelpSteer (Wang et al., 2023), PKU-SafeRLHF (Ji et al., 2024), UltraFeedback (Cui et al., 2023), UltraIn-teract (Yuan et al., 2024), Distilabel-Capybara (Daniele, 2023), and Distilabel-Orca3 (Lian et al., 2023). The combined dataset has undergone preprocessing to filter out sub-quality data, specifically removing 10% of the original dataset to ensure the quality of the training samples. The final dataset contains human preferences where, for each prompt, two responses are given: one preferred and the other rejected. The prefer-ence labels serve as the ground truth for training our ensemble of reward models. This dataset provides a comprehensive and diverse set of prompt-response pairs, making it suitable for training a robust reward model ensemble that can generalize across various domains and tasks. We refer readers to the original work of Dong et al. (2024) for further details on the dataset construction and preprocessing steps.\nWe use the Gemma-2B-it (Gemma, 2024) model as the foundation for our reward models. The instruction-tuned nature of this model makes it a strong candidate for reward modeling tasks, as it has been fine-tuned to follow human instruc-tions closely. The size of Gemma-2B-it is approximately 9.34 GB on disk, including a scalar reward head. Given that we use an ensemble of 10 independent reward mod-els, the total storage required for all models is approximately 90 GB. To accelerate the training process and optimize memory usage, we employ the following methodology:\n\u00b2huggingface.co/weqweasdas/preference_dataset_mix2"}, {"title": "5 Proximal Policy Optimization (PPO)", "content": "This section describes our methodology for fine-tuning the GPT-2 (Radford et al., 2019) language model using a variance-aware approach. Our approach builds on the standard Proximal Policy Optimization (PPO) framework (Schulman et al., 2017), modified to incorporate uncertainty in the reward estimates. The goal is to demonstrate how accounting for variance in reward models can lead to more robust and safe poli-cies. We note that the reason for choosing GPT-2 was based on the ease of performing PPO, as it is known in the literature that training large language models with PPO presents difficulties involving instability and sensitivity to hyperparameters (Choshen et al., 2019), code-level optimizations (Engstrom et al., 2020) and resource intensive-ness.\nFor prompt sampling, we use the IMDB dataset (Maas et al., 2011), which is publicly available via Hugging Face\u2074. The train split of this dataset consists of 25, 000 rows. We sample prompts x from each row with random lengths between 2 to 8 to-kens. These sampled prompts serve as input to the language model during the training process, where responses are generated and evaluated by our reward models.\n\u2074stanfordnlp/imdb"}, {"title": "7 Experimental Details for Reward Modeling", "content": "The hyperparameter details used in the single reward-head modeling are given in Table 2. Other parameters are kept as in Wolf et al. (2020). Table 3 summarizes the hardware specifications and resource consumption during the single reward-head training pro-cess, including GPU memory, disk space, and total training time. The model is trained using four NVIDIA A40 GPUs, each with 48 GB of memory. The total disk space for storing the dataset, model checkpoints, and logs is approximately 30 GB. Training time is 51 hours.\nThe hyperparameter details used in ensemble reward modeling are given in Table 4. Other parameters are kept as in Wolf et al. (2020). Table 5 summarizes the hardware specifications and resource consumption during the ensemble training process, includ-ing GPU memory, disk space, and total training time. The model is trained using four NVIDIA A40 GPUs, each with 48 GB of memory. The total disk space for storing the dataset, model checkpoints, and logs is approximately 40 GB. Training time is 7 hours.\nFigures 8(a) and 8(b) depict the training loss curves for both the single and ensem-ble reward models. In particular, we early-stop the fine-tuning of the single reward-head model when the loss dips below the 0.4 mark. We then attach 10 reward heads parallel to the final layer, freeze the base model, and retrain only the reward heads until the average training loss for each reward head is close to 0.2.\nIn Figure 9, we present the performance of the ten models evaluated across four datasets on the RewardBenchmark platform: Chat, Chat-Hard, Reasoning, and Safety. In particular, we compare these models against a fully fine-tuned single reward head model instead of the ensemble models trained with a frozen base. Our results"}, {"title": "8 Experimental Details for PPO Training", "content": "The hyperparameter and details used in both the vanilla and the variance-aware PPO training are given in Tables 6 and 7. Most of the hyperparameters are taken as in von Werra et al. (2020). The major difference between the two methods is a judicious choice of the \u03b2 parameter, which controls the constraint domain of the optimization problem. To be consistent, we choose the \u03b2 parameter such that the KL divergence from the reference policy is roughly the same for both methods. This ensures that the search domains for both methods are roughly the same. The \u03b2 parameter is defined as the Initial KL Coeff variable in the hyperparameter tables.\nTable 8 summarizes the hardware specifications and resource consumption for train-ing a single GPT-2 model using PPO, including GPU memory, disk space, and total training time. The model is trained using four NVIDIA A40 GPUs, each with 48 GB"}, {"title": "6 Proofs", "content": "Theorem 2.2. Under Assumption 2.1, for any \u03b2 > 0, the following holds with probability at least 1 - exp(- \u03b2\u00b2\u03b4):\nsup d\u2208D R\u1d40d - \u03b2||d||\u03a3 \u2264 sup d\u2208D r*\u1d40d.\nProof. The result follows from a standard self-normalizing bound for Gaussian ran-dom variables. Specifically, for any \u03b4 > 0, the following inequality holds with high probability:\n||R-r*||-1 \u2264\u221a\u03c7A ln (1/\u03b4),\n\u03a3\nwith probability at least 1 \u2013 \u03b4, since R/r/\u03a3-1 is the self-normalized euclidean norm of a standard Gaussian random variable in \u03c7A dimensions. By applying the Cauchy-Schwarz inequality, we have, for any d \u2208 D:\n|(d, R-r*)| \u2264 ||d||\u2082 ||R - r* ||\u03a3-1.\nSubstituting the bound on ||R - r* ||\u03a3-1, we obtain:\n|(d, R-r*)| \u2264 ||d||\u2082\u221a\u03c7A ln (1/\u03b4).\nThis completes the proof.\nConsider policies \u03c0\u2081 and \u03c0\u2082 as defined in Definitions 3.1 and 3.2 respec-tively. With \u03b5\u0304 set as \u03bbmin(\u03a3)\u03b5 to ensure the optimization domain of the variance-aware method is only as large as the variance unaware method, we have the following result:\nP(\u03c0\u1d40\u2082r* < \u03c0\u1d40\u2080r*) \u2264 P(\u03c0\u1d40\u2081r* < \u03c0\u1d40\u2080r*)."}]}