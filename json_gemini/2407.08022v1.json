{"title": "Deep Reinforcement Learning for Sequential Combinatorial Auctions", "authors": ["Sai S. Ravindranath", "Zhe Feng", "Di Wang", "Manzil Zaheer", "Aranyak Mehta", "David C. Parkes"], "abstract": "Revenue-optimal auction design is a challenging problem with significant theoretical and practical implications. Sequential auction mechanisms, known for their simplicity and strong strategyproofness guarantees, are often limited by theoretical results that are largely existen-tial, except for certain restrictive settings. Although traditional reinforcement learning methods such as Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) are applicable in this domain, they struggle with computational demands and convergence issues when dealing with large and continuous action spaces. In light of this and recognizing that we can model transi-tions differentiable for our settings, we propose using a new reinforcement learning framework tailored for sequential combinatorial auctions that leverages first-order gradients. Our exten-sive evaluations show that our approach achieves significant improvement in revenue over both analytical baselines and standard reinforcement learning algorithms. Furthermore, we scale our approach to scenarios involving up to 50 agents and 50 items, demonstrating its applicability in complex, real-world auction settings. As such, this work advances the computational tools available for auction design and contributes to bridging the gap between theoretical results and practical implementations in sequential auction design.", "sections": [{"title": "1 Introduction", "content": "The effective allocation of scarce resources is a pervasive challenge across diverse domains, span-ning spectrum licensing [FCC], transportation infrastructure [Rassenti et al., 1982], online advertis-ing [Varian and Harris, 2014], and resource management [Tan et al., 2020]. Combinatorial auctions (CAs) are a pivotal tool in addressing this challenge, offering a specialized auction format where bidders express valuations for combinations of items (or bundles). This allows for the incorporation of interdependencies among items, ultimately leading to more efficient allocations. For example, in spectrum auctions, bidders articulate preferences for combinations of licenses, capturing syner-gies and complementarities. However, despite their potential, CAs are known for their significant complexity, including computationally intensive winner determination problems and susceptibility to strategic bidding behavior [de Vries and Vohra, 2003].\nSequential Combinatorial Auctions (SCAs) make use of a sequential interaction with bidders participants enter the auction in a predetermined order, strategically placing bids on available"}, {"title": "1.1 Main Challenges.", "content": "Most conventional reinforcement learning techniques are ill-suited for the large and continuous action spaces in this combinatorial auction setting. For instance, Q-learning based approaches are tailored for discrete action spaces. While one workaround involves discretizing the action space, this strategy proves impractical as it often involves evaluating all possible actions, which doesn't scale efficiently.\nAlthough Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), and Deep Determin-istic Policy Gradient (DDPG) can handle continuous action spaces, they often struggle with sample efficiency and convergence issues due to the curse of dimensionality that arise in extremely large action spaces. Policy gradient methods suffer from high variance in gradient estimates, leading to convergence issues whereas Actor-Critic methods, which involve learning a Q-function over both state and action spaces, require a prohibitively large number of samples to adequately cover their domain and generalize well. For these approaches to work, we would need extensive parallel environ-ments (for samples) or extended algorithm runtime (for convergence), both of which are constrained"}, {"title": "1.2 Our Contributions.", "content": "We introduce a new general-purpose methodology for learning revenue-maximizing sequential com-binatorial auctions. In contrast to traditional reinforcement learning problems, the transition dy-namics in the context of sequential auction design can be modeled accurately. This allows us to compute analytical gradients easily, which can be used for more accurate parameter updates. Ap-proaches involving analytical gradients have demonstrated high efficiency in domains where they are applicable, such as differentiable physics and robotics [de Avila Belbute-Peres et al., 2018, Innes et al., 2019, Hu et al., 2019, Qiao et al., 2020, Wiedemann et al., 2023]. Recognizing this advan-tage, and inspired by the recent advancements in gradient-based frameworks such as differentiable economics for learning revenue maximizing auctions, we introduce a new approach that uses fitted policy iteration to learn sequential auctions. This method uses neural networks to approximate the value function and policy function and iteratively updates them in a twofold manner: initially refining the value function to align with the policy function and subsequently adjusting the policy function to maximize rewards. While this approach is not commonly used for continuous action spaces due to the often intractable nature of the policy improvement step, we demonstrate how it can be efficiently implemented in our setting.\nWe implement the policy improvement step through the use od neural networks that can model revenue-optimal auctions for the single-buyer setting, such as RochetNet [D\u00fctting et al., 2023] or MenuNet [Shen et al., 2019]. Such neural networks typically represents DSIC mechanisms through a menu of items, where each menu option is parameterized by trainable allocation and payment variables. The buyer selects an option that maximizes their utility based on their realized type and pays the specified amount from the menu. The objective is to maximize expected payment associated with the utility maximizing option for each type. This is achieved by training the allocation and payment variables with samples from the type distribution to maximize expected revenue.\nWe extend neural network architectures for static revenue optimal auction design like RochetNet to the sequential setting by modifying the menu structure to include an additional term called the \"offset\". The offset captures the value of potential future states (\"continuation value\"). By incorporating the offset into the optimization objective, our approach aims to maximize both the current revenue and the anticipated revenue from future states. This adapted network, trained through first-order gradient methods, offers a more effective and stable approach to policy iteration in continuous action spaces. Instead of parameterizing and learning menu options for each state and bidder, we learn the weights of a neural network that takes in as input a state and outputs the corresponding menu options. This let's us handle combinatorial auctions with up to 20 agents and a menu size of up to 1024.\nFurthermore, we demonstrate the scalability of our approach to accommodate a large number of buyers and items, extending to as many as 50 buyers and 50 items for the additive-valuation setting, significantly surpassing the capabilities of existing methods based on differentiable economics for auction design. This scalability is achieved by learning the menus corresponding to the entry-fee mechanisms. While this is less expressive than a combinatorial menu structure, it is more computationally efficient, making it a viable and efficient solution for scenarios involving a larger number of buyers and items."}, {"title": "2 Related Works.", "content": "Our work is closely related with the literature of sequential combinatorial auctions design [Cai and Zhao, 2017, Cai et al., 2022], in which the previous papers focus on the theoretical characterization of the approximation results. Cai and Zhao [2017] first proposed a simple sequential posted price with entry fee mechanism (SPEM) and proved that the existence of such mechanism achieves constant approximation to the optimal mechanism in XOS valuation setting and O(log(m)) approximation in the subadditive valuation setting. Later work by Cai et al. [2022] provides a polynomial algorithm based on linear program (LP) to compute the simple mechanism to achieve constant approximation in the item-independent XOS valuation setting. Compared to these existing literature, our work focuses on finding near-optimal SCAs with a general auction format (e.g., we allow bundle pricing rather than item pricing) through the use of deep learning based approaches. In addition, there is a rich literature designing approximation-results for online combinatorial auctions using simple posted-price mechanism (through Prophet Inequality), in which the items arrive in a sequential manner [Feldman et al., 2015, D\u00fctting et al., 2020, Assadi and Singla, 2019, Deng et al., 2021]. Another loosely related research direction is dynamic mechanism design [Ashlagi et al., 2016, Papadimitriou et al., 2022, Mirrokni et al., 2020], where the previous papers focus on mechanism design problem dealing with forward-looking agents that the agents may deviate their truthful reporting at current rounds to get more utility in the long-run.\nThe application of deep learning to auction design has garnered significant research attention in recent years, opening up exciting new avenues for achieving optimal outcomes. The pioneering work by D\u00fctting et al. [2023] demonstrated the potential of deep neural networks for designing optimal auctions, recovering known theoretical solutions and generating innovative mechanisms for the multi-bidder scenarios. Subsequent research extended the original neural network architectures, RegretNet and RochetNet proposed in D\u00fctting et al. [2023], to specialized architecture to handle IC constraints [Shen et al., 2019], to handle different constraints and objectives [Feng et al., 2018, Golowich et al., 2018], adapt to contextual auctions setting Duan et al. [2022], and incorporate with human preference [Peri et al., 2021]. In addition, there are other research efforts to advance the training loss of RegretNet [Rahme et al., 2021b], explore new model architectures[Rahme et al., 2021a, Duan et al., 2022, Ivanov et al., 2022], certify the strategyproofness of the RegretNet Curry et al. [2020] and extend RochetNet framework for Affine Maximizer Mechanisms for the setting that there are dynamic number of agents and items [Duan et al., 2023]. Recently, Zhang et al. [2023] apply deep learning techniques to compute optimal equilibria and mechanisms via learning in zero-Sum extensive-form games, in the setting when the number of agents are dynamic.\nThere is also previous interest in applying deep reinforcement learning (DRL) to auction de-sign. Shen et al. [2020] propose a DRL framework for sponsored search auctions to optimize reserve price by modeling the dynamic pricing problem as an MDP. DRL has also been used to compute near-optimal, sequential posted price auctions [Brero et al., 2020, 2023], where the authors model the bidding strategies of agents through an RL algorithm and analyze the Stackelberg equilibrium of the sequential mechanism (perhaps also allowing for an initial stage of communication). Mean-while, Gemp et al. [2022] investigated the use of DRL for all-pay auctions through conducting the simulations from the multi-agent interactions. Existing papers using DRL for auction design focus on additive or unit- or additive-demand valuation settings. Whereas, in our paper, we propose a general, sequential combinatorial auction mechanism through DRL, which has potential to handle much larger combinatorial valuation settings by utilizing the sequential auction structure along with our customized DRL algorithm."}, {"title": "3 Preliminaries", "content": "We consider a setting with n bidders, denoted by the set N = {1,...,n} and m items, M =\n{1,...,m}. Each bidder i \u2208 [n] has a valuation vi : 2M \u2192 R, where vi(S) denotes the valuation of\nthe subset S C M. Each bidder valuation vi is drawn independently from a distribution Vi. We\ndenote V = \u03a0i=1Vi.\nWe consider the sequential setting where the auctioneer visits the bidder in lexicographical order.\nHe knows the distribution V but not the bidder's private valuations vi for i \u2208 N. The mechanism\ndesign goal is to design a set of allocation rules and payment rules that determine how these items\nare allocated and how much each bidder is charged such that the expected revenue (gross payment\ncollected from the bidders) is maximized. We denote the allocation rule by g = (g1,..., gn) where\ngi \u2286 M, denotes the subset of items allocated to bidder i. Since the items can't be over-allocated,\nwe require gi \u2229 gj = \u00d8 for all i \u2260 j \u2208 N. We denote the payment rule as p = (P1,...,Pn) where\nPi \u2208 R\u2265o denotes the payment collected from bidder i.\nIn this work, we study Sequential Combinatorial Mechanisms with Menus. Given a bidder i and\na set of available items denoted by S, the mechanism consists of a pricing function fi,s : 2M \u2192 R>o\nfor each bidder i and the set of available items S that maps a subset of items (i.e. bundle) to its\nprice. Additionally, we require fi,s(T) = \u221e if T \u2288 S to prevent the over-allocation of items. The\nauctioneer engages with bidders in lexicographic order, presenting them with a menu of bundles\nalong with their corresponding prices. Subsequently, the bidder being visited selects their preferred\nbundle, pays the associated price, and exits the auction. The favorite bundle for the bidder is simply\ndefined as the bundle that maximizes their expected utility i.e. S*i = arg maxy\u2282svi(T) \u2013 \u03bei,s(T).\nSee Algorithm 1 for the details of the mechanism."}, {"title": "4 Method", "content": "Policy iteration consists of two alternating steps a policy evaluation step that computes a value function consistent with the current policy, and a policy improvement step that uses the value functions to greedily find better policies. Each iteration is a monotonic improvement over the previously computed policies and often converges quickly in practice. For finite-state MDPs with a finite action space, it has also been shown to converge to the optimal policy. However, we are dealing with continuous action spaces for which the policy immprovement step is intractable. In this section, we show how we address this challenge.\nSince we know the transition dynamics, we can use it to simplify this step. Given a state st and an action a, we can exactly estimate the next state conditioned on the realized private value v ~ Vit."}, {"title": "4.1 Exact Method for Small Number of States", "content": "When the number of states is small, we can use dynamic programming to solve for the optimal policy. Since the states are never repeated for a given episode and policy improvement steps for a state st only depend on future time steps, we can start by computing the policy improvement for all possible states at t = n and proceed backward to t = 0. To be more precise, for a state st = (it,T) for T \u2286 [m],t = n, we perform the policy improvement step to compute \u03c0(st). We set V(st) to be the revenue computed from this step. Next, we repeat this process for all states st = (it,T) such that T\u2286 [m], t = n \u2212 1 and proceed until we get to the state (i\u00b9, [m]). In total, this involves training n \u00d7 2m RochetNets, one for each state. For more details, refer to Algorithm 3"}, {"title": "4.2 Approximate Methods for Large Number of States", "content": "When the number of states is large, we use a feed-forward neural network (called the critic) to map a state to its value. The critic is denoted by V\u00ba : N \u00d7 {0,1}m \u2192 R. We use another feed-forward network called the actor that maps a state to action. The actor is denoted by \u03c0\u00ba : N \u00d7 {0,1}m \u2192 R2M\n>0.\nNeural Network Architecture. The state st forms the input to the critic as well as the actor network and is represented as an m+1 dimensional vector. The first index denotes the index of the agent visited at time t and the last m indices are binary numbers that denote the availability of the corresponding item. The first index is used to compute a positional embedding of dimension demb which is then concatenated with the m-dimensional binary vector to form a m + demb dimensional input to the feed-forward layers. We use R hidden layers with K hidden units and a non-linear activation function for the actor as well as the critic. Note that these networks do not share any parameters.\nThe critic outputs a single variable capturing the value of the input state. The actor first outputs p\u2208 R outputs are positive. We use the m-dimensional binary input to compute a boolean mask variable \u03b2\u2208 {0,1}2m to denote the availability of a bundle. The final output of the actor network is given as p\u00b7 \u03b2 + (1 \u2212 \u03b2) \u00b7 K where K is a large constant and the product denotes entry-wise masking. This masking ensures that unavailable bundles are assigned a high price ensuring that they are never picked, thereby satisfying feasibility constraints.\nFitted Policy Iteration. To perform policy iteration, we first randomly initialize the policy network. Then we perform the approximate policy evaluation step. To do this, we first collect"}, {"title": "4.3 Entry Fee Mechanisms for Extremely Large Number of States", "content": "Note that the current menu structure that we impose involves learning 2m bundle prices per state. The computation of which bundle is utility maximizing requires O(2m) computations. To scale up our approach to a large number of items, say m > 10, we show how we can impose the menu"}, {"title": "5 Experimental Results", "content": "In this section, we present experimental results, comparing our approach with two established ana-lytical mechanism design baselines: the item-wise and bundle-wise sequential auctions. The former sells items individually, while the latter treats all items as a unified bundle, selling it sequentially. The optimal policy for both these methods can be computed through Dynamic Programming. Addi-tionally, we benchmark our approach against other reinforcement learning algorithms such as PPO. Approaches involving Q-function learning, like SAC and DDPG, were unstable and did not perform as well as PPO. Notably, we observe that the size of the action space significantly influences training, with a vast action space making it impractical to accurately evaluate and update Q-values for all possibilities. Therefore, we exclusively report results for PPO. Across all our settings, we provide results based on 10,000 episodes. Additional details regarding implementation and hyperparameters can be found in the Appendix C. Next, we elaborate on the various settings considered in this paper."}, {"title": "5.1 Constrained Additive Valuations", "content": "Let the values of individual items be denoted by tj ~ Vj. In the additive valuation case, the value of a bundle is calculated as the sum of the individual values of its constituent items: V(S) = \u2211jes tj.\nFor unit-demand valuation, each bidder values only their most preferred item in a bundle, making the value of the bundle equal to the highest-valued item within it: V(S) = maxj\u0119s tj. In the context of k-demand valuation, each bidder values the k most preferred items in a bundle, with the value of the bundle determined by the highest-valued item among these k: V(S) = maxRCS,|R|=ktj.\nWe consider the following settings each with n agents and m items with (n, m) \u2208 {(5, 5), (10, 10)}:\nA. Additive valuations where item values are independent draws from U[0,1]\nB. Additive valuations where item i's values are independent draws over U[0,1]\nC. Unit-demand valuations where item values are independent draws over U[0,1]\nD. k-demand valuations where item values are independent draws over U[0, 1] and k = 3\nWe present the results in Table 1. PPO performs adequately in smaller settings involving 5 agents and 5 items. However, its performance degrades when the scale is increased to 10 agents and 10 items. In contrast, our proposed method based on Dynamic Programming (DP) and Fitted Policy Iteration (FPI) consistently outperforms established baselines. Importantly, FPI achieves"}, {"title": "5.2 Combinatorial Valuations", "content": "In the combinatorial setting, we consider n \u2208 {10,20} agents and m = 10 items with the following bundle-wise valuations listed below:\nE. Subset valuations are independent draws over U[0, \u221a|S|] for every subset S\nF. Subset valuations are given by \u2211jer tj + cr where tj ~ U[1,2] and the complimentarity parameter c\u0442 ~ U[-|S|, |S|]\nThe results are presented in Table 2. Our approach, specifically designed to navigate high-dimensional spaces more effectively, outperforms PPO in combinatorial settings as well. Notably, our method (FPI) required only 20 minutes of training on a Tesla A100 GPU while DP took between 5 and 10 hours, depending on the number of agents. PPO was trained for 20,000 iterations, which took approximately 12 hours."}, {"title": "5.3 Scaling up", "content": "We scale our approach to an even larger number of buyers and items. For this setting, we use the entry-fee mechanisms to characterize the menus, enhancing computational efficiency and reducing memory requirements, as we only manage m + 1 menu options instead of 2m options at any given time. We consider Setting A with (n, m) \u2208 {(20,20), (50, 50)}"}, {"title": "6 Conclusion", "content": "We have introduced a new methodological approach to the problem of learning simple and strat-egyproof mechanisms for sequential combinatorial auctions. We formulate this as a reinforcement learning problem and show how we can use first order gradients to learn menu options that maximize expected revenue. Through extensive experimental results, we've shown the superior performance of this approach compared to other RL methods and well-known analytical solutions.\nWe also point out some limitations and potential directions for future work. The use of neu-ral networks in approximate methods introduces uncertainties, lacking theoretical guarantees for convergence. Additionally, the optimality of solutions obtained by RochetNet in the policy im-provement step remains unknown. Despite these uncertainties, we empirically observe convergence in all our experiments, and it has been shown that RochetNet consistently retrieves optimal solu-tions when analytical solutions are available [D\u00fctting et al., 2023]. Another limitation lies in our dependence on the assumption of having ample samples from valuation distributions. While this is a common practice in empirical approaches to mechanism design, it would be insightful to explore the effectiveness of our approach when the number of available samples is limited.\nFuture work could also explore more intricate menu structures beyond entry-fee mechanisms, continuing to seek computational efficiency improvements over bundle price enumeration. Addi-tionally, there is a compelling question of designing mechanisms where we allow agents to select the best bundle efficiently, potentially in poly(m) time, as suggested by previous research [Schapira and Singer, 2008]. Our current research focused on a fixed order of agent visits, which prompts the ex-ploration of methods to dynamically learn the optimal order in which agents should be visited [Brero et al., 2021]. Extending our framework to accommodate non-deterministic allocation poses an in-triguing challenge, and understanding how it can be implemented in the sequential setting needs further attention. Lastly, it is interesting to assess the ability of our approaches to approximate the non-sequential version of the auction problem. Innovations in this space could involve leveraging AMA-based approaches instead of RochetNet, which would engage with several agents in each step of a sequential auction instead of just one agent.\nOur approach also shows promise for broader applicability beyond auction deign for problems involving large action and continuous spaces wherever we can model transitions differentiably (for"}, {"title": "A DSIC and IR", "content": "Remark 1. The mechanism described in Algorithm 1 is Dominant Strategy Incentive Compatible (IC) and Individual Rational (IR) if \u03bei,s(\u00d8) = 0 for all i \u2208 [n], S \u2286 M.\nAn auction is dominant strategy incentive compatible (DSIC), if each bidder's utility is maximized by reporting truthfully no matter what the other bidders report. An auction is individually rational (IR) if each bidder receives a non-negative utility while reporting truthfully.\nThe Sequential Combinatorial Auction (SCA) with menus is DSIC because the agents pick their utility maximizing bundle. Additionally, the utility of taking part is at least 0 (this is because the agent has the option to pick the empty bundle \u00d8 and is charged 0)"}, {"title": "B Proof of Proposition 1", "content": "Proposition 2. For a current policy \u03c0 and value function V\u201e(.), the improved policy \u03c0' for a state st is given by:\n\u03c0'(st) = arg max E_v~Vit [ast (v) + V(it+1, St\\\\St(v))]  (1)\n        at\nProof. For a current policy and value function V\u2084(.), the improved policy \u03c0' for a state st is given by:\n\u03c0'(st) = arg max \u2211p(st+1|st, a) [r(st, a, st+1) + V(st+1)]\n          at     st+1\n\u03c0'(st) = arg max \u2211 (St \\\\ St+1|st, a) [ast\\St+1 + V(st+1)]\n          at      St+1CSt\n\u03c0'(st) = arg max \u2211 (T|st, a) [aT + V(it+1, St \\\\T)]\n          at     TCSt\n\u03c0'(st) = arg max E_{v~Vit} \u2211 (T/s, a) [aT + V(it+1, St \\\\T)]\n          at     LTESt\n\u03c0'(st) = arg max E_{v~Vit} [asts 1st(v) + Vit+1, St\\\\Set(v))]\nHere, p(st+1|st, a) denotes the probability of the next state being st+1 when the current state is st and a being the action taken (i.e. prices). (T|st, a) is the probability of bundle T is picked at st under pricing function a. When v, st and, a are known, we have p(T = S(v)|v, st, a) = 1 where St(v) = arg maxTcst v(T) \u2013 aT."}, {"title": "C Implementation Details and Hyperparameters", "content": "We use the stable-baselines3 [Raffin et al., 2021] package to implement our baselines.\nActor Critic Networks. For all our neural networks, we use a simple fully connected neural network with Tanh activation functions except for the last layer. We use R = 3 hidden layers with k = 256 hidden units each.\nFor the actor network in PPO, we use sigmoid activation functions to squash the output to [0, 1] range. The action distribution is a normal distribution with these sigmoid outputs as means. It"}, {"title": "", "content": "is then scaled appropriately. For example, consider setting A with n agents and m items. The action space, which comprises of bundle prices, is of size 2m. The maximum possible valuation for a subset T would simply be |T|. Consequently, we scale the output corresponding to the price of T by |T|. We found this this approach performed better than using the SquashedDiagonalGaussian distribution where the noise is added before squashing the outputs using a tanh function. For the actor network in our approach, we only use a softplus activation to ensure the outputs are positive and leave them unscaled.\nWe make similar modifications for our large-scale settings involving entry-fee menu structure. For these settings, we use sigmoid functions for the posted price and a softplus function for the entry-fee for the actor networks in PPO as well as our approach.\nWe also found the using an offset before using sigmoid or softplus functions helped with training. This ensures that the prices start low but increase gradually. We offset sigmoid function by 0.5 and softplus function by 1. Thus, these modified functions are given by:\nsigmoid-with-offset = 1/1+ e-(x-0.5)  (4)\nsoftplus-with-offset = log(1 + e(x-1))  (5)"}, {"title": "", "content": "Training and Evaluation Sets Since we have access to the value distributions, we sample valuations online while training. But for testing, we report our results on a fixed batch of 10000 profiles.\nHyperparameters We present the hyperparameters used in Dynamic Programming (DP) in Algorithm 2 and Fitted Policy Iteration (FPI) below:"}]}