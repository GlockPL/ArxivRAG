{"title": "Earth Observation Satellite Scheduling with Graph Neural Networks", "authors": ["Antoine Jacquet", "Guillaume Infantes", "St\u00e9phanie Roussel", "Nicolas Meuleau", "Vincent Baudoui", "Emmanuel Benazera", "Jonathan Guerra"], "abstract": "The Earth Observation Satellite Planning (EOSP) is a difficult optimization problem with considerable practical interest. A set of requested observations must be scheduled on an agile Earth observation satellite while respecting constraints on their visibility window, as well as maneuver constraints that impose varying delays between successive observations. In addition, the problem is largely over-subscribed: there are much more candidate observations than what can possibly be achieved. Therefore, one must select the set of observations that will be performed while maximizing their weighted cumulative benefit, and propose a feasible schedule for these observations. As previous work mostly focused on heuristic and iterative search algorithms, this paper presents a new technique for selecting and scheduling observations based on Graph Neural Networks (GNNs) and Deep Reinforcement Learning (DRL). GNNs are used to extract relevant information from the graphs representing instances of the EOSP, and DRL drives the search for optimal schedules. Our simulations show that it is able to learn on small problem instances and generalize to larger real-world instances, with very competitive performance compared to traditional approaches.", "sections": [{"title": "Introduction", "content": "An Earth observation satellite (EOS) must acquire photographs of various locations on the surface of Earth to satisfy user requests. An agile EOS has degrees of freedom allowing it to target locations that are not exactly at its vertical in an earth-bound referential (\u201cnadir\u201d). The satellite we consider is in low orbit; as a consequence, each observation is available in a visible time window (VTW) that is significantly larger than its acquisition duration. Maneuvering the satellite between two observations consists of modifying its pitch, yawn and roll triple also called its attitude and thus implies delays that depend on the start and end observation, as well as on the time of the transition [18]. In addition, an agile EOS is typically oversubscribed: there is more observations to be performed that can possibly be achieved in the given time of operation. As different acquisitions may be associated with different priorities or utilities, the Earth observation satellite planning problem (EOSP) consists in selecting a set of acquisitions that maximize their weighted cumulative values, and designing a schedule for acquiring these observations while respecting the operational constraints.\nThe most complex instances of the EOSP involve several satellites orbiting Earth over multiple orbits, and dependencies between targets [22]. For instance, an acquisition may consist of several pictures of the same earth-bound location to be taken by different satellites, and/or in different"}, {"title": "Related Work", "content": "The EOSP has been subject to a large body of research, from communities as varied as aerospace and engineering, operational research, computer science, remote sensing and multipliscinary sciences. We refer the reader to [22] for a survey of non-machine learning approaches to the problem, and we focus our attention on DRL based approaches to the EOSP. Note that we address the time-dependent EOSP, where the duration of a transition between two observations varies with time. This contrasts with most of previous literature that assumes constant, time-independent transition duration.\nPeng et al. [14] address a slightly different problem where observations are scheduled on board. A LSTM-based encoding network is used to extract features, and a classification network is used to make the a decision. Dalin et al. [4] solve multi-satellite instances by modeling them as a Multi-Agent Markov Decision Processes, then use a DRL actor-critic architecture. The actor is decentralized, each satellite using a relatively shallow network to select its action. The critic is centralized and implemented as a large recurrent network taking input from all satellites. Hermann et al. [9] also address the multi-satellite problem: a policy is trained in a single satellite environment on a fixed number of imaging targets, and then deployed in a multi-satellite scenario where each spacecraft has its own list of imaging targets. Local policies are learned using a combination of Monte Carlo Tree Search (MCTS) to produce trajectories, and supervised learning to learn Q-values using the trajectories produced by MCTS as training examples. Finally, Chun et al. [3] present a very similar approach; the main difference is that the transitions durations approximated during training phase instead of being precisely computed on discrete dates before training."}, {"title": "Problem Representation", "content": "An instance of the EOSP is defined by a set of candidate observations, or acquisitions, and a time-horizon $\\tau$ (in this work, the duration of an orbit). Each observation $i$ is associated with its fixed duration $d_i$ and its VTW $[e_i; l_i]$ such that $l_i \\le \\tau$. The transition duration between two acquisitions $i$ and $j$ is a function $\\Delta_{i,j}(t_i)$ of the starting time $t_i$ of the first observation. A schedule is a sequence of observations with associated starting time. It is represented as a single mapping that associates with each candidate observation $i$ its starting time $t_i$, such that $t_i = -1$ for all observations $i$ not included"}, {"title": "Classical Approach : Time Discretization", "content": "In the EOSP, the start time $t_i$ of each observation is a continuous value, and the transition durations $\\Delta_{i,j}(t_i)$ are continuous functions of continuous variables. Therefore, the EOSP is not a completely discrete problem. However, it is often re-framed as such, either by making assumptions on the start time of transitions (for instance, every transition starts as soon as it is available), or by crudely discretizing the time variable.\nIn this work, the problem is provided under the form a very large time-discretized graph that repre-sents the EOSP as a sequential decision problem. In this graph-later called the \"discrete graph\"-every candidate acquisition is represented once for each possible (discrete) start time. A node is typ-ically denoted $(i, t_i)$ and represents starting observation $i$ at time $t_i \\in [e_i, l_i - d_i]$. An edge between two nodes is present if the end observation is a possible immediate successor of the start observa-tion. The acquisition and transition durations are accounted for while defining the edges: an edge between $(i, t_i)$ and $(j, t_j)$ is such that $t_j$ is the smallest discrete time satisfying $t_i + d_i + \\Delta_{ij}(t_i) \\le t_j$ and $t_j \\ge e_j$. Note that there is an (implicit) mutual exclusion between two nodes $(i, t_i)$ and $(i, t'_i)$, $t_i \\ne t'_i$, to indicate that observation $i$ must not be performed twice. Finally, some edges are pruned using considerations of optimality: if an observation $k$ can be inserted between $(i,t_i)$ and $(j, t_j)$ without breaking the constraints of the problem (that is, the transition delays), then the edge be-tween $(i, t_i)$ and $(j, t_j)$ is removed. In this case, every path between the two nodes must go through one node $(k, t_k)$. The reasoning is that every optimal solution that includes both $(i, t_i)$ and $(j, t_j)$ would also include observation $k$. Therefore, $(j, t_j)$ should not be an immediate successor of $(i, t_i)$."}, {"title": "Our Approach : Continuous-Time Graph", "content": "Although convenient for state-space approaches, the discrete graph has the drawback of quickly becoming huge as the number of candidate acquisition grows, making it unsuitable as an input to"}, {"title": "Sequential Decision Model", "content": "Our solution technique is based on representing the process of building an optimal schedule for an instance of EOSP as a reinforcement learning problem, then using DRL algorithms to solve it. Re-inforcement learning is concerned with learning the solution of a Markov Decision Process (MDP), which is a discrete-time sequential decision model. An MDP is defined as a tuple $(S, A, T, R)$ where S is a state space, A the action space, T the transition matrix, and R the reward function [15]. The definition of these elements flows directly from the representation of the EOSP as a discrete graph:\n\u2022 A state $s \\in S$ is a discrete EOSP graph as defined before. It can be either the initial graph where no task has been selected yet, or an intermediate graph where some nodes and edges have been selected to account for the fact that the beginning of the schedule has been decided;\n\u2022 Given a state s, the set of available actions a is the set of all possible successors of the last selected node in s (thus leading to chronological insertions);\n\u2022 MDPs naturally handle uncertainty in the problem. I the general case, it is represented in the transition matrix: $T(s, a, s') = Pr(s(t + 1) = s' | s(t) = s,a(t) = a)$. However, our formulation of the EOSP is deterministic, therefore, the MDP we derive contains no uncer-tainty. Given an initial state s (discrete graph) and selected action a (the next observation to add to the schedule), the transition matrix gives probability 1 to the state s' representing the discrete graph after the addition of a to the schedule, following the update procedure described in the previous section.\n\u2022 As every inserted observation is feasible by definition, we use as immediate reward the utility value associated with the selected observation ie $R(s, a, s') = u_a$. The aim is to maximize the undiscounted sum of immediate rewards.\nWhile these components define a fully observable MDP, we do not provide a perfect description of the state s to the deep network. As explained above, we do not feed the huge discrete graph to the neural network. Instead, we use a continuous graph that is not a perfectly accurate representation of the problem, because transition duration are not reported exactly (see section 4.2). Therefore, the learner solves partially-observable MDP (POMDP) [12], where partial observability concerns only the transition durations, and thus plays a minor role."}, {"title": "Solution", "content": "Following [11], we use a reinforcement learning setup where the agent receives continuous graphs representing partial schedules as input, selects the next observations to schedule, and updates its parameters based on the reward representing the cumulative utility of the schedules it produces. For a given problem, a simulator is in charge of managing the different graphs and feeding the learner with the appropriate data. The learner implements a policy, that is, a stochastic mapping from states s to actions a as defined above. It learns a policy that maximizes the reward function over a base of real-world problems used as training set. The policy has to be able to generalize to test problems, that is, exhibit good performances without further learning on a set of instances not seen before.\nAn overview of the architecture is shown in Fig. 2. The graphs provided as input are processed by several elements. First, the graph is transformed in order to allow bidirectional message-passing by the GNN, then simple networks produce node and edge embeddings. Next the graph is processed through a Message-Passing Graph Neural Network (MP-GNN) to extract features capturing relevant information, and produce action probabilities. Finally, the RL algorithm updates the parameters of the whole system, embedders and GNN, based on the rewards received. For ease of presentation, we first discuss the RL algorithm, then the embedders and GNN."}, {"title": "Reinforcement Learning", "content": "As our core algorithm, we use the Proximal Policy Optimization (PPO) algorithm [16] with action masking [10].\nA peculiar aspect of the EOSP instances we have to solve is that the utility of different acquisitions may vary by up to 8 degree of magnitude. In fact, acquisitions are grouped in 7 priority classes with utility value ranging from 1 to $10^8$. The utility of the observations within a class of priority is equal to the value of that class, plus a small term depending on the predicted cloud coverage at the location of the acquisition (in order to favor acquisitions that are likely to happen with a clear sky). This generates instability in DRL algorithms (and in MDPs in general), as the low priority observations provide a reward that might be difficult to distinguish form the noise in the algorithm. In addition, the critic must learn very large values, starting from very low values at initialization, and following tiny gradient steps. This makes learning slow and inefficient.\nWe tried several approaches to handle this, including using a logarithmic scale and 2-hot encoding [7]. In our current implementation, we simply divide each individual reward by the average utility of all candidates observations in the problem. This is a simple way to remedy the issue of having to learn very large values, but it does not fix the problem of the discrepancy between rewards (un-less some extreme priority classes are not represented in the problem instance). We are currently examining optimization with lexicographic preferences [17]."}, {"title": "GNN Implementation", "content": "Node attributes: To inform the learner, we label each node of the continuous graph with the window of visibility of the corresponding observation. We note that the continuous graph still does not contain any information about the transitions duration, so the learner would be blind to them. To compensate for this, each node $i$ of the continuous graph is labeled with information about the satellite attitude while performing observation $i$, namely, the min, max and average pitch and roll of the satellite over the observation VTW. Although this information is not sufficient to recover the exact duration of transitions, it allows the learner inferring them closely enough to perform well, as shown in our simulation results.\nGraph rewiring: A Message-Passing Graph Neural Network (MP-GNN) [23] uses a graph struc-ture as a computational lattice. It propagates information, represented as messages, along the ori-ented graph edges only. In our case, if an MP-GNN uses only the EOSP continuous graph edges, then we explicitly forbid information to flow from future acquisitions to the present choice of the next acquisition. This is definitely not what we want: we want the agent to choose the next observa-tion to schedule based on its effect on the on future conflicts. In other words, we want information to go from future to present tasks. Therefore, we have to edit the input graph before it can used by the MP-GNN. This is known in the MP-GNN literature as \u201cgraph rewiring\".\nFor every (precedence) edge in the continuous graph, a link pointing in the other direction is added to the rewired graph (reverse-precedence). Different edge types are defined for precedence and reverse-precedence edges, to enable the learned operators to differentiate between chronological and reverse-chronological links. The systems learns to pass information in a forward and backward way, depending on what is found useful during learning.\nEmbeddings: A graph embedder builds the rewired graph by adding edges. It embeds node at-tributes (VTW, attitude stats) using a learnable MLP, and edge attributes (type of edge) using a learn-able embedding. The output dimension of embeddings is an open hyper-parameter hidden_dim. We found a size of 64 being good in our experiments.\nGraph pooling: A node is added and connected to every other node to allow collecting global information about the entire graph, as opposed to the local information associated with the nodes of the original graph. It is used by the critic to estimate the value of the graph as a whole. It is also used by the actor, where the global graph encoding is concatenated to each node embedding. Indeed, messages are passed by the MP-GNN algorithm only between immediate neighbors. Therefore, a network of depth n_layers is able to anticipate only n_layers observations ahead. Having the global node embedding concatenated to each node embedding compensates for this, allowing the current decision to take into account the entire graph.\nGNN: As a message passing GNN, we use EGATConv from the DGL library [21], which enriches GATv2 graph convolutions [2] with edges attributes. We used 4 attention heads, leading to an output of size 4xhidden_dim. This dimension is reduced to hidden_dim using learnable MLPs, before being passed to next layer (in the spirit of feed-forward networks used in transformers [20]). The output of a layer can be summed with its input using residual connections [8]. For most of our experiments, we used 10 such layers. The message-passing GNN yields a value for every node, and a global value for the graph (from the graph pooling node).\nAction selection: Action selection aims at computing action probabilities given the node values (logits) output by the GNN. We can either use the logits output from the last layer of the GNN, or use a concatenation of the logits output from every layer. We chose to concatenate the global graph logits of every layer, leading to a data size of $((n\\_layers + 1)\\times hidden\\_dim) \\times 2$ per node, where hidden_dim is the dimension of the embeddings (see next section). This dimension is reduced to 1 using a learnable linear combination, that is, a minimal case of a Multi-Layer Perceptron (MLP). We did not find using a larger MLP to be useful. Finally, a distribution is built upon these logits by normalizing them, and using action masks to remove actions that are not feasible in the current state. As node numbers correspond to action/acquisition numbers, we directly have the action identifier when drawing a value from the distribution."}, {"title": "Experiments", "content": "We use a set of real-life problems provided by an industrial partner who owns Earth observation satellites. As explained in Section 3, problems are given in the form of a discrete-time graph. The simulator uses this graph to compute and maintain the continuous-time graph. To provide intuition on the difficulty of the problem.\nWe compare our DRL approach to two solutions currently being used for operating such satellites.\nGreedy algorithm: It is the algorithm that is currently used for real-world operations. It greedily selects acquisitions to add to the schedule based on the utility, and inserts them in the plan if possible. Previously selected tasks may be slightly postponed, but never canceled.\nRAMP: [1] It is an implementation of a Dijkstra search algorithm in the discrete graph. Although based on an admissible algorithm (Dijkstra), RAMP is not guaranteed to find the absolute optimal schedule. This is due to the exclusion links between nodes of the discrete graph that represents the same task started at different times. Nevertheless, RAMP constantly provides the best known schedules on real problems. Unfortunately, its complexity prevents using it for real-time operations. Therefore, it is used as a reference in these simulation results."}, {"title": "Unitary Score", "content": "First, we compare our approach to baselines on a relaxed problem: we try to maximize the number of acquisitions scheduled, irrespectively of their priority or utility. This measure of performance is insensitive to the large gaps in acquisitions utility discussed in Section 4.1. We run two experiments:\nSingle problem: First, we want to measure if our models and algorithms can possibly achieve competitive performance on a given problem. We train our learner on a single problem with a total of 106 acquisitions and let it overfit as much as it needs, as long as it achieves great performance. As illustrated in Fig. 3-left, we observe that it is indeed able to beat both the greedy algorithm and RAMP scores. This result shows that our architecture is able to implement very powerful policies. In the next set of experiments, we put it to the challenge of realistic learning environment."}, {"title": "General Utility", "content": "In our second sets of experiments, we take into account the utility of observations and aim at max-imizing the cumulative utility of all the observations included in the final schedule, as in the full-fledged MDP framework presented in Section 7. As before, we perform two sets of experiments: one where the learner is free to overfit on a single problem to reach its best performances, and one aiming at measuring its ability to generalize.\nSingle Problem: Our test on single problem with a total of 88 candidate acquisitions shows that our system is able to outperform the greedy algorithm and reach the score of RAMP (Figure 5-left). This proves the suitability of the architecture for the full MDP set up.\nGeneralization: We train on 639 problems of about 100 acquisitions and test on 27 unseen prob-lems of similar size. The learning curves are displayed in Figure 5-right and show that the learner is able to generalize. The plot showing the number of times where Wheatley is above, below or equal to the greedy solution and RAMP are presented in Fig. 6. We see that Wheatley outperforms the deployed solution and approaches the best known performances, in a realistic set-up where problems"}, {"title": "Conclusion", "content": "We showed that DL-based approaches to the EOSP are challenging some of the best known tech-niques. There are several perspectives we are currently exploring to extend this work. First, as stated before, we are trying to take advantage of the large discrepancy in acquisition utility by using lexicographic RL algorithms such as [17]. Scheduling tasks by decreasing priority would provide stronger guarantees to find the optimal schedule. To achieve this, schedules must be built in a non-chronological order, which is not the case in our current implementation. Currently, we choose the next acquisition to insert just after the last inserted one, using some foresight given by the GNN. This foresight is limited by the number of layers of the GNN. As we said, the discrete-time graph is tailored for state-space search and chronological insertion. Future work will consider developing an alternative continuous-tine graph representation of the EOSP where observations can be added to the schedule in any order, using Simple Temporal Networks [5]. Such work will open promising avenues for using lexicographic preferences."}]}