{"title": "Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework", "authors": ["Mengshuo Jia", "Zeyu Cui", "Gabriela Hug"], "abstract": "The integration of experimental technologies with large language models (LLMs) is transforming scientific research, positioning AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations - one of the essential experimental technologies - remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, we propose a feedback-driven, multi-agent framework that incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from DALINE and MATPOWER, this framework achieves success rates of 93.13% and 96.85%, respectively, significantly outperforming the latest LLMs (ChatGPT 40 and o1-preview), which achieved a 27.77% success rate on standard simulation tasks and 0% on complex tasks. Additionally, our framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.", "sections": [{"title": "I. INTRODUCTION", "content": "COMBINING laboratory automation technologies with large language models (LLMs) enables automated execution of scientific experiments [1]. Related advances span the fields of mathematics, chemistry, and clinical research, including mathematical algorithm evolution [2], geometry theorem proving [3], chemical experiment design and execution [1], as well as the development and validation of machine learning approaches for clinical studies [4]. These recent achievements signal a new research paradigm, positioning AI as a research assistant for humans with natural language communication abilities, rather than merely a specialized problem solver as in the past. Establishing LLMs as research assistants also holds significant potential for advancing power systems research.\nGiven the heavy reliance on simulations in power systems research, developing LLM-based assistants in this field requires equipping LLMs with the capability to conduct power system simulations. Enabling LLMs to execute simulation tasks has multiple implications: (i) At the assistant level, LLMs capable of conducting simulations would allow researchers to focus more on idea-intensive activities, such as simulation design, rather than on labor-intensive tasks like simulation implementation. (ii) At the interface level, LLMs conducting simulations might offer a natural-language interface. This interface can connect simulation tasks with other upstream/downstream power system tasks using natural language as the input/output. This is particularly helpful when these tasks' original inputs and outputs are heterogeneous (e.g., different modalities) and originally challenging to program cohesively using regular codes. (iii) At the coding level, LLMs executing simulations might be a step toward natural language coding in power systems. This might signify an evolution in programming, bringing it closer to a more intuitive, language-driven approach, a long-standing goal of programming development for decades.\nHowever, LLMs inherently lack the capability to perform power system simulations. For recently developed simulation tools not included in LLM pre-training datasets, LLMs generally cannot execute these simulations accurately. Even for well-established tools included in pre-training data, simulation precision remains unsatisfactory. For instance, GPT-4 often has difficulty creating small distribution grids using OpenDSS [5] or writing code for simple (optimal) power flow problems [6], even though information about both OpenDSS and (optimal) power flow is available within GPT-4's pre-training dataset.\nWhile the underlying causes of this issue have not been widely discussed and recognized in the energy domain, we propose the following factors as potential explanations:\n\u2022\n\u2022\n\u2022\n\u2022 Frequency: The low frequency of domain-specific power system knowledge in LLM training datasets \u2014 especially in the long tail of rarely encountered data \u2014 limits the models' ability to generalize effectively for specialized simulation tasks [7].\n\u2022 Quality: High-quality, instruction-tuned, or query-based coding data specific to power system simulations in available open-source data is lacking. Missing explanatory code annotations make it difficult for LLMs to fully contextualize and operationalize power system simulations.\n\u2022 Complexity: The multi-step reasoning required by complex power system simulations is inherently challenging, particularly given sparse or ambiguous representations in the model's learned patterns w.r.t. power system simulations.\n\u2022 Precision: Precise identification of simulation parameters, functions, and their logical connections, poses high demands on LLMs, especially when LLMs' knowledge about simulations is incomplete or fragmented. This may result in a semantic drift, causing LLMs' code generation to gradually deviate from the accurate version.\nThe challenges outlined above can be grouped into three main limitations: (i) limited simulation-specific knowledge, (ii) restricted reasoning capabilities for simulation tasks, and (iii) imprecision in function and option application. Enhancing"}, {"title": "II. ENHANCED RAG MODULE", "content": "As an efficient and scalable approach for integrating external knowledge to LLMS, RAG consists of three key steps: external knowledge chunking (splitting documents into smaller pieces), text embedding (converting texts into vectors using neural networks such as text2vec), and information retrieval (finding information in the vector space that aligns with the query) [6].\nHowever, for power system simulations, critical questions arise: (i) what types of queries should be used for retrieval? and (ii) what knowledge base should serve as the retrieval repository? Addressing these questions reveals two primary areas for enhancing RAG's effectiveness in simulation tasks.\nTo this end, we propose an enhanced RAG module. This is specifically designed to integrate power system simulation knowledge into LLMs and reduce hallucinations. This module emphasizes the identification of essential keywords in simulation requests to facilitate more precise knowledge retrieval than the standard RAG. It includes two main components: (i) an adaptive query planning strategy, and (ii) a triple-based structure design for the knowledge base. Together, these components provide an enhanced RAG for complex power system simulation tasks."}, {"title": "A. Adaptive Query Planning", "content": "This section addresses the question of what types of queries should be used for retrieval. In the standard RAG approach, the entire simulation request is processed as a single unit, which, as discussed (and will be demonstrated in case studies), conflates distinct elements in the request, leading to inefficiencies and reduced retrieval accuracy. In fact, simulation requests typically contain two critical elements: the functions to be used and the options to be set. Thus, we propose using functions and options as distinct retrieval queries. However, these elements are rarely stated explicitly in simulation requests; instead, they"}, {"title": "B. Triple-based Structure Design for Knowledge Base", "content": "In this section, we address the question of which knowledge base should serve as the retrieval repository. While each power system simulation tool includes a user manual with detailed instructions, this manual is not an ideal retrieval repository. The reasons are twofold: (i) User manuals are designed for human readability rather than automated retrieval; although readable, they are unstructured and inefficient for machine-driven queries, especially when manuals primarily consist of formulas, tables, and figures. (ii) The main challenge for LLMs in generating simulation code is understanding the logical dependencies between options and functions, as many options are function-dependent. Using only the user manual for retrieval fails to capture these complex relationships effectively.\nTo overcome these issues, we propose an additional, easy-to-construct retrieval repository: a triple-based structured option document. In this document, each line represents an option, providing the following structured information in sequence: (i) option name, (ii) default value/format, (iii) function dependencies, and (iv) option description. The inclusion of triples in (iii) - linking each option, its related functions, and their dependency - enables retrieval for logical relationships. As will demonstrated in case studies, this supplementary repository significantly enhances retrieval efficiency and improves the accuracy of simulation code generation by preserving the logical context."}, {"title": "III. ENHANCED REASONING MODULE", "content": "Even though the enhanced RAG module provides LLMs with retrieval results tailored to a simulation request, it still remains essential to strengthen the LLM's reasoning abilities to generate correct simulation codes based on the retrieval results. This requires a coding agent (i.e., another LLM) to write codes for simulations tasks. This agent needs to fully understand its role, assigned tasks, reasoning path, and contextual knowledge, including retrieval results, when handling simulation tasks.\nTo address this, we propose an enhanced reasoning module, as detailed in Fig. 4. It provides structured guidance, sequential reasoning steps, and contextual knowledge to support accurate code generation by the coding agent. Details are as follows."}, {"title": "A. Role and Functionality Definition", "content": "The agent deployed in this module is designated as a simulation coding agent for a specific simulation tool. Its primary function is to generate syntax-compliant simulation code that aligns with the specific task requirements, the static provided knowledge, and the dynamically retrieved knowledge."}, {"title": "B. Reasoning Framework", "content": "To enable systematic, tool-independent reasoning, we develop a few-shot CoT framework, which breaks down the simulation task into the following universal actions:\n\u2022\n\u2022\n\u2022\n\u2022 Function Identification: Determines the functions relevant to the simulation task.\n\u2022 Function Syntax Learning: Acquires the correct syntax for identified functions to ensure compliance with the simulation tool's requirements.\n\u2022 Option Information Extraction: Identifies options and extracts their formats, values, and dependencies to maintain coherence with the selected functions.\nCode Generation: Integrates all extracted information into cohesive simulation code that meets task specifications and adheres to syntax requirements.\nEach of these actions is further clarified with tool-specific coding examples in the prompt. While the examples are tool-dependent, the rest of the framework remains general. Overall, the above reasoning framework highlights again that the key to handling simulation tasks: correctly identifying and combining functions and options."}, {"title": "C. Knowledge Integration", "content": "The above reasoning actions heavily rely on information drawn from both the simulation request and supplementary knowledge, comprising:\n\u2022\n\u2022 Static Basic Knowledge: Supplies the agent with foundational information on essential functions and syntax rules pertinent to the simulation tool. This static knowledge serves as a base reference and reminder for the agent to consult when generating code. Note that such knowledge is tool-dependent.\nDynamic Retrieval Knowledge: The prompt includes placeholders for dynamic retrieval results from the enhanced"}, {"title": "IV. ENVIRONMENTAL ACTING MODULE WITH FEEDBACK", "content": "Despite the reinforcement brought by the enhanced RAG and reasoning modules, the coding agent may still encounter errors during simulation code generation. To address this, it is essential to enable direct interaction between the LLM and the simulation environment, allowing the agent to receive execution feedback and iteratively refine its code. To this end, we propose an environmental acting module with an error feedback mechanism that integrates with both the RAG and reasoning modules, as illustrated in Fig. 5. The components of this module are described in the following."}, {"title": "A. Code Execution and Detection", "content": "The simulation code, generated by the coding agent from the enhanced reasoning module, is executed using the simulation environment API connected to a specific power systems simulation tool. Following execution, the simulation environment produces results, which are then checked for error signals. Specifically:\n\u2022\n\u2022 If an error is detected, the code advances to a stopping criterion check. If the stopping criterion is met, the process is terminated; if not, the module triggers a feedback loop with detailed error reporting.\nIf no errors are detected, the process completes."}, {"title": "B. Error Handling and Feedback Loop", "content": "Upon detecting an error in the simulation results, an error report is automatically generated, containing:\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022 Problematic Code: The code segment that caused the error.\nError Message: A detailed description of the error.\nGeneral Hints: Additional guidance on common issues.\nRequest: Specific corrections needed to address the error.\nReminders: Additional constraints or requirements, if any.\nChat History: A log of previous interactions and iterations."}, {"title": "C. Enhanced RAG and Reasoning Module Interplay", "content": "The error report and feedback are then processed as a new request by the retrieval agent in the enhanced RAG module. This agent retrieves relevant information based on the error report (the query planning can also be applied to error reporting by simply replacing the identification of function/option keywords with the identification of error-related keywords). The retrieved information is then passed to the enhanced reasoning module, where the coding agent incorporates both the retrieval results and the correction request to revise the simulation code. The loop proceeds until the code meets both requirements, or until the stopping criterion is satisfied."}, {"title": "V. CASE STUDY", "content": "To comprehensively validate the proposed framework, we carry out a range of tests differing in three key dimensions: (i) distinct combinations of the proposed strategies within the framework to evaluate each strategy's independent effectiveness; (ii) different simulation environments, specifically DALINE [23] and MATPOWER [24], which include tools both familiar and unfamiliar to LLMs\u00b9, to demonstrate the framework's versatility across various applications; and (iii) a wide array of simulation"}, {"title": "A. Settings", "content": "Firstly, Table I presents the distinct combinations of the proposed strategies within the framework employed in the evaluation, with the proposed strategies shaded in gray.\nSecondly, this paper selects DALINE [23] and MATPOWER [24] as the simulation environments. It is important to note that DALINE was released after the latest updates of the LLMs used in the evaluation, while the well-established tool MATPOWER was already included in the training dataset of the LLMs. Consequently, these two environments encompass both seen and unseen scenarios for the LLMs, allowing us to demonstrate the framework's versatility.\nThirdly, 34 simulation tasks have been used to test the framework with DALINE, comprising 7 complex tasks and 27 standard tasks. Similarly, for MATPOWER, 35 simulation tasks have been defined, including 8 complex tasks and 27 standard tasks. These tasks comprehensively cover the functionalities of both simulation tools, aiming to include the"}, {"title": "B. Evaluation on DALINE", "content": "The evaluation results on DALINE are illustrated in Fig. 7 and Fig. 8. Fig. 7 depicts the distribution of scores achieved across attempts for each evaluated scheme, differentiating between complex and standard tasks. Fig. 8 presents the success rates for each scheme, itemized by \u201call tasks combined\u201d, \u201ccomplex tasks only\", \"standard tasks only\", as well as for the \"first attempt success rate\" and the \"final attempt success rate\". In the following, these evaluation results are analyzed from multiple perspectives.\""}, {"title": "1) Original Capability vs. Enhanced Capability", "content": "While equipped with environmental interaction and feedback mechanisms, GPT40-Sole still demonstrates a 0% success rate for both complex and standard tasks, indicating that GPT4o has not previously encountered DALINE. Even with a complete knowledge base supported by RAG \u2014 either through the standard RAG or OpenAI's official RAG \u2014 the resulting schemes, GPT40-SR and CGPT40-R, achieve success rates of only 31.37% and 33.82% across all tasks, respectively. This suggests that, even with RAG support, the latest language"}, {"title": "2) Fully Equipped vs. Less Equipped", "content": "The high success rate of 93.13% over all taks for GPT40-Full is due to the cumulative effects of using the complete proposed framework. Comparing other schemes with GPT40-Full gives an indication of the impact of omitted strategies. For instance, although GPT40-NP includes most reasoning enhancement strategies, it lacks the triple-based structured option document, resulting in a reduced success rate of 81.37%. On the other hand, omitting the few-shot CoT for reasoning, as in GPT40-NCS, lowers success to 65.19%. When few-shot CoT is employed, but the proposed query planning is omitted, as in GPT40-RSR, the success rate for complex tasks drops to 66.67%, particularly due to the deteriorated performance for the complex tasks. While similar comparisons can be drawn across all schemes, our goal here is not to argue which strategy provides the highest improvement, but to emphasize that high success relies on the combined effect of multiple strategies."}, {"title": "3) Complex Tasks vs. Normal Tasks", "content": "In general, more complex tasks \u2014 those with multiple sub-requests \u2014 tend to increase the likelihood of errors in LLMs, resulting in lower success rates compared to standard tasks, as observed across most schemes. However, with our proposed full framework, GPT40-Full, the performance gap between complex and standard tasks narrows significantly, as shown in both the score distributions in Fig. 7 and the success rates in Fig. 8. This suggests that, with enhanced reasoning capabilities and the more effective RAG design, GPT40-Full effectively identifies and addresses the sub-requests within complex tasks, similar to how it handles standard tasks. This enables LLMs to better manage complex tasks."}, {"title": "4) First Attempt vs. Final Attempt", "content": "The comparison between the first-attempt and final-attempt success rates demonstrates the effectiveness of environmental interaction and feedback mechanisms. As shown in Fig. 8, the final-attempt success rate is always higher than the first-attempt rate, particularly for schemes that incorporate fewer strategies from our framework. These schemes typically have either reduced reasoning capability or limited retrieval information, making environmental interaction and feedback crucial for error correction. However, for GPT40-Full, the difference between first-attempt and final-attempt success rates is relatively small, as GPT40-Full often completes the DALINE simulation task successfully on the first attempt. This further highlights the effectiveness of the proposed framework. One noteworthy point is that the effectiveness of automatic error correction is partly influenced by the quality of the simulation tool's error-reporting system \u2014 whether it provides clear, code-specific error messages. This feature affects the LLM's capability to interpret and resolve issues in the generated code. In the absence of such a feature, as with GPT40-RSRNW, the success rate drops to 78.43%, with negligible improvement between the first and final attempts. This underscores that without a well-developed error-reporting system, iterative refinement may yield limited benefit. Although"}, {"title": "C. Evaluation on MATPOWER", "content": "The evaluation results on MATPOWER are illustrated in Figs. 9, 10, and 11. Specifically, Fig. 9 presents the scores achieved by each evaluated scheme in individual attempts when managing complex tasks. Fig. 10 shows the distribution of scores across attempts for each scheme, and Fig. 11 depicts the success rates of each scheme. The outcomes observed here align closely with the results on DALINE, enabling us to only focus primarily on comparative analyses across schemes in the subsequent discussion."}, {"title": "1) Original Capability vs. Enhanced Capability", "content": "Despite MATPOWER being a widely-used and well-documented tool with extensive resources available online, the latest high-performance LLMs, such as GPT40 and o1-preview (renowned for its reasoning capability), struggle to perform simulations reliably. For instance, both GPT40-Sole and o1p-Sole show a 0% success rate on complex tasks, and GPT40-Sole achieves only 27.77% success on standard tasks. Even with RAG and the whole knowledge base, GPT40-SR reaches a success rate of only 13.75% for complex tasks and 52.96% for standard tasks. In contrast, the fully equipped framework, GPT40-Full, achieves a remarkable 96.85% success rate across all tasks, with a breakdown of 93.75% on complex tasks and 97.77% on standard tasks, underscoring the framework's effectiveness."}, {"title": "2) Fully Equipped vs. Less Equipped", "content": "Consistent with the findings on DALINE, the results on MATPOWER indicate that high success rates depend on the synergistic effect of multiple strategies. For example, excluding the enhanced reasoning module, as in GPT40-PR, results in an overall success rate decrease to 89.71%, with complex tasks dropping further to 70.00%. Similarly, omitting the proposed query planning strategy, as in GPT40-RSR, reduces the overall success rate to 63.42% and complex tasks to 37.50%. These outcomes are substantially lower than those achieved by GPT40-Full, which maintains a 93.75% success rate on complex tasks and 97.77% on standard tasks, demonstrating the critical role of each component within the proposed framework."}, {"title": "D. Cost Analysis", "content": "The cost analysis of GPT40-Full for executing simulation tasks in DALINE and MATPOWER is presented in Table II, with average values across all tasks shown. The reported time covers the entire process, including retrieval, reasoning, code generation, simulation execution, result aggregation, and, where necessary, code correction. Remarkably, GPT40-Full completes each task in approximately half a minute. Additionally, the token expense per task is roughly 0.014 USD. It is noteworthy that aside from parallel retrieval, no specialized acceleration techniques were employed in this framework. Thus, despite its already satisfactory performance, there is considerable potential for speed enhancements. Even"}, {"title": "VI. CONCLUSION", "content": "This paper addresses the research gap in enhancing LLMs for power system simulations by proposing a feedback-driven, multi-agent framework, representing the first systematic approach to significantly improve LLMs' simulation capabilities across both familiar and new tools. Validated on 69 diverse simulation tasks from DALINE and MATPOWER, our framework achieved substantial performance improvements, with success rates of 93.13% and 96.85%, respectively, far surpassing those of baseline schemes, including the latest LLM, 01-preview. Key findings include: (i) The original simulation capability of LLMs is limited, as evidenced by GPT40 and 01-preview achieving success rates no higher than 27.77%. (ii) Even with the standard RAG module and a comprehensive knowledge base, LLMS achieve overall success rates below 45%, highlighting the need for a more comprehensive approach. (iii) Our framework's high success rate stems from a synergistic integration of enhanced RAG, enhanced reasoning, as well as environmental acting and feedback mechanisms. Removing any of these elements results in a significant performance decline. (iv) Our framework enables LLMs to execute tasks efficiently, with each task completed in approximately 30 seconds at a token cost of only 0.014 USD, offering a scalable, cost-effective solution that enhances productivity of human scientists in power systems. However, several critical future challenges still remain: (i) Developing automatic evaluation methods for unbenchmarked results to improve reliability and autonomy. (ii) Expanding the framework to synchronize multiple simulation tools to tackle more challenging tasks. (iii) Since 100% accuracy remains unachieved, integrating error-detection mechanisms to flag uncertainties and inform researchers of potential inaccuracies. Overall, our work is an initial step in the long journey toward realizing intelligent LLM-based research assistants, with potential ahead."}]}