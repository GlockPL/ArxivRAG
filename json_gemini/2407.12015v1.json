{"title": "The Great Al Witch Hunt: Reviewers' Perception and (Mis)Conception of\nGenerative Al in Research Writing", "authors": ["HILDA HADAN", "DERRICK M. WANG", "REZA HADI MOGAVI", "JOSEPH TU", "LEAH ZHANG-KENNEDY", "LENNART E. NACKE"], "abstract": "Generative AI (GenAI) use in research writing is growing fast. However, it is unclear how peer reviewers recognize or misjudge\nAI-augmented manuscripts. To investigate the impact of Al-augmented writing on peer reviews, we conducted a snippet-based online\nsurvey with 17 peer reviewers from top-tier HCI conferences. Our findings indicate that while Al-augmented writing improves\nreadability, language diversity, and informativeness, it often lacks research details and reflective insights from authors. Reviewers\nconsistently struggled to distinguish between human and AI-augmented writing but their judgements remained consistent. They\nnoted the loss of a \"human touch\" and subjective expressions in Al-augmented writing. Based on our findings, we advocate for\nreviewer guidelines that promote impartial evaluations of submissions, regardless of any personal biases towards GenAI. The quality\nof the research itself should remain a priority in reviews, regardless of any preconceived notions about the tools used to create it. We\nemphasize that researchers must maintain their authorship and control over the writing process, even when using GenAI's assistance.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of generative artificial intelligence (GenAI) tools such as ChatGPT\u00b9 and Gemini\u00b2 have sparked a wave\nof excitement in academia and industry. Since the release of ChatGPT in November 2022 [61], GenAI has become\nincreasingly popular in assisting people with written, auditory, and visual tasks [45, 58, 78]. In research, GenAI offers a\nnew approach to manuscript writing, as it can handle tasks ranging from text improvement suggestions to speech-to-text\ntranslation and even crafting initial drafts [45, 52]. Its ability to understand context and generate human-like and\ngrammatically accurate responses fosters innovative brainstorming and enhances the quality and readability of research\npublications [5]. However, along with GenAI's potential to augment research activities, concerns about transparency,\nacademic integrity, and the urgency of maintaining the credibility of research work have emerged [21, 54, 73, 78].\nDespite the growing interest in using GenAI for manuscript writing and research activities [45, 64], many researchers\nhesitate to acknowledge its use in their papers. This is illustrated by several instances where research publications\nwith undisclosed GenAI use were identified by readers (e.g., [53, 71, 72, 79]). Studies have identified the phenomenon\nof Al aversion, where Al-generated content, even if factual, is often perceived as inaccurate and misleading [12, 56]\nand disclosing its use can negatively impact readers' satisfaction and perception of the authors' qualifications and\neffort [69]. Therefore, researchers' hesitancy is partly due to their fear that acknowledging GenAI use might damage"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In this section, we summarize the technical evolution of GenAI as a manuscript writing assistant and the emerging\nperceptions and concerns within the academic community. In the end, we illustrate how our research addresses these\nconcerns and promotes the ethical, transparent, and effective use of GenAI to support future researchers."}, {"title": "2.1 Generative Al as a Writing Assistant", "content": "Manuscript writing is crucial for researchers to share their ideas and contribute to their fields. However, writing high-\nquality research papers is challenging due to the need to simplify complex findings while ensuring accuracy, logical flow,\nand adequate evidence [35]. Beginners and non-native English speakers often struggle with using proper terminology\nand literature references [35, 39, 51]. In addition, manuscript writing often competes with other responsibilities like\nteaching and supervising [22], making efficiency and time management vital. The pressure of \"publish or perish\"\nmindset [22] further intensifies these challenges. GenAI thus become valuable in research writing to ease researchers'\nburden on writing and help them keep their focus on the innovative and critical aspects of their research.\nWith the rise of Large Language Models (LLM), GenAI's potential to transform manuscript writing has garnered\nsignificant interest [10, 45, 78]. Traditional writing assistants offer word and sentence corrections, synonym suggestions,\nand sentence completion predictions [3, 14, 68]. In contrast, GenAI offers a broader array of functionalities to ensure\nhigh-quality writing across diverse research disciplines, such as inspiring new ideas [49, 74], enhancing readability [5],\nand assisting with narrative construction and creative writing [49, 75, 84]. However, GenAI has the limitation of\ngenerating factually incorrect information, known as hallucination [1, 42]. For example, researchers have reported\nencountering fake references from GenAI [20]. In addition, GenAI can be opinionated, which influence researchers'\nperspectives and attitudes conveyed in the writing and compromise research integrity [41]. Therefore, while GenAI\nholds benefits for manuscript writing, its use requires researchers' careful consideration to avoid the risks.\nThese problems highlight the importance of transparently disclosing the use of GenAI. Such disclosure enables\nreviewers and readers to critically evaluate the research, be aware of potential biases or inaccuracies introduced by\nGenAI. Our study investigates reviewers' perceptions and misconceptions, reduces current concerns and hesitations\namong researchers, encourages researchers to openly disclose their GenAI use, and fosters a more transparent and\naccountable research environment."}, {"title": "2.2 Perceptions of Generative Al in Research Community", "content": "A central debate in the research community regarding GenAI involves authorship and content attribution [21]. Research\nmanuscripts reflect the knowledge, expertise, and contributions of its author researchers [77]. The use of GenAI in\nmanuscript writing has raised questions about how to acknowledge its involvement, as crediting it as a co-author is\ninappropriate because \u201cAI tools cannot meet the requirements for authorship as they cannot take responsibility for the\nsubmitted work\" [21, para. 2]. GenAI also cannot be accountable for the content it produces [20, 21]. Beyond authorship,\nethical concerns arise, such as copyright infringement from using third-party materials, possible conflicts of interest,\nand plagiarism issues that replicate contents and images, ideas, and methods from already published works [20, 57]. In\n2023, the Committee on Publication Ethics (COPE) recommended that authors explicitly disclose the use of AI-assisted\ntechnologies, including LLMs like ChatGPT, in their work [21]. Following COPE's lead, the Association for Computing\nMachinery (ACM) established policies on GenAI, stating \u201cthe use of generative AI tools and technologies to create\ncontent is permitted but must be fully disclosed\" [4]. Following these, efforts are made to develop comprehensive\nreporting guidelines for evaluating the impact of tools like ChatGPT on scientific research writing, as seen in initiatives\nby Elsevier [28] and the World Association of Medical Editors [83]. These guidelines aim to promote transparency by\nproviding a framework for declaring the use of GenAI in research.\nScholarly work revealed two opposing perceptions of AI-generated content: algorithm aversion and algorithmic\nappreciation. Algorithm aversion is a negative bias towards AI-generated content, even when the AI output is objectively\nbetter than human-produced content [12, 38]. For example, people tend to rate AI-written content as inaccurate\nregardless of its truthfulness [56]. In addition, informing users about Al involvement can harm the creator-reader\nrelationship rather than facilitate content judgment [69]. This bias worsens after seeing AI makes mistakes [23]. On the\nother hand, algorithmic appreciation refers to when people are more willing to adhere advice from an algorithm over a\nhuman [55], and find AI-created articles more credible with higher journalistic expertise [34].\nManuscript writing involves various decisions about word choice and sentence structure to effectively convey\nauthors' meaning and purpose, with each word representing a decision made by the authors [46]. With GenAI, many\nof these decisions are delegated to AI, which relies on highly probable options, pre-defined rules, large databases, or\nspecific text corpora [46]. This delegation can reduce human authors' sense of ownership [24, 49], which may potentially\nlead to irresponsible assertions in research papers. Therefore, regulating the extent of GenAI assistance is crucial for\nmaintaining the accountability and credibility of research publications. Our research aims to encourage transparency in\ndisclosing GenAI use, which is the foundational step for responsible AI augmentation in research manuscript writing."}, {"title": "2.3 Connection to Our Research", "content": "While guidelines exist to guide researchers and promote transparency in research community, many researchers are\nhesitant to acknowledge their use of GenAI in their manuscripts (e.g., [53, 71, 72, 79]). Although previous studies\nhave examined human ability to detect AI-generated content (e.g., [33, 48, 70]), these studies were not conducted in\nthe context of research publications and were not conducted with participants with experience reviewing academic\nmanuscripts in peer-reviewed venues. Therefore, their findings offer limited insight into the specific issue of GenAI use\nin research manuscript writing. Our study addresses this gap by investigating experienced reviewers' perceptions and\nmisconceptions on manuscripts due to GenAI use. Through this investigation, we aim to reduce researchers' concerns\nabout negatively impacting reviewers' perceptions and judgments, and encourage them to openly acknowledge their\nuse of GenAI in future manuscripts. Given the increasing adoption of GenAI in research writing and the ethical needs"}, {"title": "3 METHODOLOGY", "content": "To investigate reviewers' perceptions of GenAI use in research writing, we employed a text snippet-based online survey.\nAfter obtaining Research Ethics Board approval [details omitted for blind review], we recruited 17 participants who\nhave experience reviewing manuscripts for publication at top-tier HCI conferences, including CHI\u00b3 and CSCW4. We\nrefer to our participants as \"reviewers\" in the following sections. Reviewers were presented with six snippets tailored to\ntheir areas of expertise in HCI, chosen from 16 example human-written abstracts and 32 GenAI-augmented snippets.\nThe six snippets were presented in a randomized sequence. This approach allowed us to explore reviewers' perception\non a wide range of topics with different levels of GenAI use without overwhelming them with a long survey. In this\nsection, we describe our snippet design, survey development, participant recruitment, and data analysis procedure."}, {"title": "3.1 Study Material Construction", "content": "In research paper writing, GenAI is used in various ways from recommending texts, perform spelling or grammar\ncorrections, to generating entire sections [4]. To comprehensively evaluate reviewers' perception, we present each\nparticipant with three types of snippets (Content_Type):\n(1) original: snippets written entirely by human authors.\n(2) paraphrased: snippets rephrased with a GenAI by rewriting human-written text while preserving its original\nmeaning.\n(3) generated: snippets generated entirely with a GenAI by using human-written text as reference to ensure\nrelevance to the original manuscript.\nIn this section, we discuss the selection of original human-written snippets, and the production of paraphrased and\ngenerated snippets using GenAI prompts."}, {"title": "3.1.1 Original Snippets.", "content": "To ensure the comprehensive coverage of our original snippets, we selected abstracts from\nexample papers from submission topics of CHI 2023 conference 5, the premier venue for HCI research. For each topic,\nwe selected the most-cited paper published before the prevalent use of GenAI in November 2022 to ensure it was written\nby human researchers. When multiple papers had the same citation numbers, we subsequentially selected papers based\non download counts and the most recent publication date. This process resulted in a total of 16 abstracts as our original\nsnippets. Details of these source papers are in Appendix C.\nWe chose to use abstracts due to three considerations. First, abstracts are crucial for research manuscripts as they\ncomprehensively summarize the papers' significance, research goals, methodology, findings, and contributions [8].\nSecond, in early stage of a peer-review process, abstracts guide editors and reviewers in efficiently evaluating a\nmanuscript [8]. Third, since we recruit experienced reviewers who are academia and industry professionals, using\nabstracts ensures our study is manageable and not overly time-consuming while still offering sufficient information for\nevaluating participants' perceptions."}, {"title": "3.1.2 Paraphrased and Generated Snippets.", "content": "The selected original snippets were then processed through GenAI-Google\nGemini-to create the corresponding paraphrased and generated snippets. We chose Gemini for its ability to provide\ncomprehensive summaries, valuable suggestions, and rationales, as well as its transparency in disclosing limitations\nrather than fabricating content, which distinguish it from other GenAI tools such as ChatGPT [78].\nBuilding upon literature on constructing GenAI prompts [59] and discussions with our research team of GenAI\nresearchers and enthusiasts, we incorporated four components in our construction of the prompts for snippets processing:\n(1) Goal: the goal of the prompt. For producing paraphrased snippets, we set the goal as \"rephrase\" the original\nsnippet; For producing generated snippets, we set the goal as \"improve\" the paraphrased snippet to allow GenAI\nto maximize its creativity while ensuring the content consistency.\n(2) Step-by-step instruction: the detailed instruction that specifies expected GenAI behaviour step-by-step. For\nproducing paraphrased snippets, we provided a guide based on best practices of abstract writing [8]. For\nproducing generated snippets, we used two sequential prompts that guide GenAI to first generate a new\nsnippet based on the paraphrased snippet and the introduction section of the paper, then refine its contribution\nstatements based on the manuscript's conclusion section.\n(3) Context: the context information that facilitates the GenAI behaviours. For producing paraphrased snippets,\nthe original snippet served as the context. For producing generated snippets, the paraphrased snippet and the\ncorresponding manuscript's introduction and conclusion sections were used.\n(4) Constraints: to ensure consistency in length, we set a 150-word constraint for both paraphrased and generated\nsnippets based on typical CHI submissions.\nResearchers in our team reviewed the snippets to ensure consistency in content and length across the three con-\ntent_types. Figure 1 and Figure 2 illustrate the prompt structure, and Appendix D provides examples of the snippet\nproduction process in Gemini. This approach ensures that the snippets derived from the same abstract maintain"}, {"title": "3.2 Survey Design", "content": "In this section, we provide a detailed description of our survey design. Figure 3 summarizes the survey flow. A complete\nset of questions is included in Appendix E."}, {"title": "3.2.1 Screening Questionnaire.", "content": "The survey began with a study information sheet and consent form, followed by a\nscreening questionnaire. Our screening targeted participants who have experience serving as reviewers in peer-reviewed\nHCI conferences. Participants had to be at least 18 years old, have previous experience as a reviewer or associate chair,\nand have encountered or suspected the undisclosed use of GenAI in submissions they reviewed."}, {"title": "3.2.2 Instruction and Presentation of Snippets.", "content": "To ensure reviewers' perceptions were related to their experience with\nGenAI, not conventional writing assistants, we first provided a description of GenAI 's functionality: \"Al writing\nassistants can help researchers by suggesting phrasing, structuring sentences, and even generating initial drafts.\"\nReviewers then selected two research topics from the 16 CHI'23 topics (see Q1 & Q2 in Appendix E)-one in which they\nwere most knowledgeable and one in which they had the least knowledge. From each topic, we presented the original,\nAl-paraphrased, and AI-generated snippets from an example paper (as described in subsubsection 3.1.1). This approach\nallowed us to compare reviewers' perceptions and judgements varied between content_types, and investigate how\ntheir expertise influenced their perceptions. To avoid biasing reviewers, we did not disclose the content_type of each\nsnippet. We described the six snippets as could be human-written or AI-processed without confirming AI or human\nauthorship. The three snippets from the same abstract were presented in random order. Since the snippets were from\npublished papers, we included a bold red text instructing reviewers not to search for the snippets in literature databases."}, {"title": "3.2.3 Perceptions of the Snippets and the Research Presented.", "content": "For each snippet, reviewers were asked to provide a more\ndetailed rating of their expertise in the topic, using a scale from 0-no knowledge or expertise in this field to 10-I am\nan expert in this field. We coded these responses as disciplinary_expertise in our statistical analysis. This question\nserved three purposes. First, it clarified what \"the most\u201d and \u201cthe least\" knowledgeable meant by each reviewer. Second,\nit captured cases when reviewers misidentify that a paraphrased or generated snippet is from a completely different"}, {"title": "3.2.4 General Perception of GenAl and Demographic Information.", "content": "After all six snippets, we closed the survey with\nquestions about reviewers' general perceptions of GenAI writing. We asked about their views on the capability of human\nresearchers (perceived_human_researcher_capability) and GenAI in communicating research ideas and outcomes\nthrough writing (perceived_AI_capability). These questions aimed to assess the reviewers' algorithmic aversion or\nappreciation [12, 34, 38], as their negative or positive attitudes toward GenAI may influence their perceptions of the\nsnippets. Finally, we asked reviewers about their demographic information, estimated the number of papers they\nhad reviewed (peer-review_experience), and use of GenAI in their own writing (AI_familiarity). We included these\nquestions because AI background knowledge can influence perceptions [27], and people's algorithmic aversion increases\nafter witnessing AI mistakes [23]. Reviewers were also given an open-ended space for additional comments on our\nstudy before completing the survey."}, {"title": "3.3 Participants Recruitment and Demographics", "content": "Before distributing the survey, we piloted the questionnaire with five PhD students with peer-review experience and\nrefined the language and question structure based on their feedback to improve clarity, comprehension, and conciseness.\nA prior power analysis [30, 31] for a within-subject Wilcoxon-signed rank test determined that a sample size of N = 15\nwas needed, with an effect size=0.8, a power=0.8, and a margin for random error\u2264 5%. Following ethics approval, we\nrecruited participants using a snowball sampling method in April and May 2024. Our research team reached out to\nCHI and CSCW conference committees for participation and assistance in distributing recruitment materials. This\nrecruitment method was used due to the difficulty in recruiting reviewers, even in real peer-review process [37]. We\nclosed the survey on May 7, 2024, one month after receiving the last response, resulting in a total of 41 responses. Of"}, {"title": "3.4 Data Analysis", "content": "We present our quantitative data analysis and corresponding results in section 4. For the qualitative open-ended question,\nwe conducted an inductive thematic analysis with two researchers, following the established guideline by Clarke et al.\n[18]. We reviewed the data to familiarize ourselves and ensure it contained no blank or incoherent responses to each\nquestion. We retained \"N/A\" responses, which represent an inability to differentiate human-written snippet from GenAI\noutput. The two researchers independently coded 15% (n=16) of the total responses (N=102). We did not calculate\ninter-coder reliability, as it \"prioritises uniformity over depth of insights\" and often results in superficial themes,\nespecially for studies with more than 20 codes (like ours) [18, p. 303]. Instead, the two researchers discussed and resolved\nconflicts in a meeting, and created an initial codebook. This process was repeated twice, with each meeting addressing\nhalf of the remaining data until the codebook was finalized and all data were coded. This finalized codebook served as a\nfoundation for developing and refining the themes from our data. We present our codebook and themes in Appendix A."}, {"title": "4 FINDINGS", "content": "4.1 RQ1: How Much Are Reviewers Aware of the Use of Al in the Context of Research Writing?\nTable 2 shows the response distribution among the N = 17 reviewers regarding their perceived_AI_involvement\nacross the three content_types. Both original human written snippets and AI-generated snippets received a median=5,\nwith a mean=4.44 (SD=3.13) and mean=5.12 (SD=3.18), respectively. This result indicates that reviewers generally\nbelieved GenAI was similarly involved in both human-written and AI-generated snippets. This similarity revealed a\ngeneral misconception about GenAI use in snippets and suggested the difficulty in differentiating between AI-generated\nand human-written snippets among reviewers. Compared to these, the rating for AI-paraphrased snippets is notably\nlower (median=2, mean=2.74, SD=2.61).\nTo validate the observed differences in reviewers' perceptions, we performed a Friedman test [32] and confirmed\nsignificant within-subject differences across the three types of snippets ($\\chi^2$ = 6.92, df = 2,P = 0.03). We further\nconducted post-hoc pairwise Wilcoxon comparisons [82] with Bonferroni correction [15] (see Table 2). The result shows\nthat, compared to AI-generated snippets, reviewers perceived significantly lower AI involvement in AI-paraphrased\nsnippets (W = 92, P = 0.01, r = -0.60). there was no significant difference in reviewers' perceptions between AI-\ngenerated and human-written snippets (W = 80, P = 0.55). Additionally, no significant difference was found between\nreviewers' perceptions of human-written and AI-paraphrased snippets (W = 26.5, P = 0.06). The validity of these results\nare further supported by our reviewers' qualitative responses, with several of them indicated they were confused about\nwhich snippets were AI- or human-written."}, {"title": "4.2 RQ2: How Much Is Reviewers' Judgement of Research and Manuscript Influenced by the Use of Al in\nIts Writing?", "content": "Table 3 presents the distribution of reviewers' judgments across the three content_types. The result shows that\nreviewers' responses were mainly neutral (mean = 3.29, SD = 1.12 mean = 3.82, SD = 0.80), and there is no sizeable\ndifferences between reviewers' perception on the accuracy, reliability, honesty, clarity, and compellingness.\nTo further validate our observations, we conducted a Friedman test [32] and found no significant within-subject\ndifferences in reviewers' perception across the three content_types. We suspect that this result is because our reviewers\nneither exhibited algorithmic aversion nor appreciation, but had neutral opinion towards GenAI. To validate this, we\nconducted a within-subject Wilcoxon signed-rank analysis [82] with Bonferroni correction [15] to compare reviewers'"}, {"title": "4.3 RQ3: To What Extent Do Reviewers' Peer-Review Experience, Disciplinary Expertise, and Al\nFamiliarity Influence Their Perception and Judgement?", "content": "In this section, we evaluate how factors including content_type, reviewers' disciplinary_expertise, AI_familiarity\nand peer-reviewer_experience influence their perceived_AI_involvement and judgements on the manuscript and\npresented research. We used Cumulative Link Mixed Model (CLMM) regression and included participant identifiers as\nrandom effects. CLMM is well-suited for repeated measures experiments with ordinal dependent variables, as in our\nstudy where reviewers were presented with multiple snippets in parallel [17]. We conducted a series of Multivariate\nCLMM regressions, using reviewers' perceived_AI_involvement, perceived_accuracy, perceived_reliability, per-\nceived_honesty, perceived_clarity, and perceived_ compellingness as the dependent variable (DV) and the factors\nas the predictors. Table 4 shows the final models with predictors ranked by their contribution to the DV, determined by\nthe global minimum Akaike Information Criterion (AIC) [43] values obtained upon adding each predictor. Predictors\nwith the highest contribution (lowest AIC) are ranked first.\nAs shown in Table 4, the results revealed relationships between reviewers' perceived_AI_involvement and the\npredictors content_type and AI_familiarity, with content_type had the greatest contribution. Specifically, reviewers\nperceived significantly lower AI involvement in AI-paraphrased snippets compared to original human-written snippets.\nThis result extends our within-subject comparison in subsection 4.1. In addition, reviewers who rarely used Al in their"}, {"title": "4.4 RQ4: What Aspects of Research Writing Impact Reviewers' Perception and Judgement?", "content": "In this section, we discuss the themes derived from reviewers' qualitative responses (see Figure 4). For clarity, thematic\nanalysis themes are in italics, and reviewers' quotes are in italicized quotations. The survey question is detailed\nin Appendix E (Q9). We discuss how these themes are related to our quantitative findings in section 5.\nOur thematic analysis of N = 102 open-ended responses revealed five major themes that influence reviewers'\nperception of the author of snippets: 1) Writing and Sentence Structure, 2) Word Choice, 3) Problematic Statement,\n4) Expression, and 5) Carefully Crafted Statement. Interestingly, the codes under these themes revealed reviewers'\ncontradictory opinions, which aligned with our quantitative findings that reviewers struggled to differentiate AI-\ngenerated snippets from those written by human researchers (see subsection 4.1)."}, {"title": "4.4.1 Theme 1: Writing and Sentence Structure.", "content": "The primary concern among the responses (27%) was that Al-generated\nsnippets often suffer from incoherent logic and phrasing, with illogical transitions, unclear flow, and misuse of field-\nspecific terminologies. In contrast, 12% of responses noted that human produces neatly edited sentences, and 2% mentioned\nthat experienced researchers know how to structure sentences effectively (trained researchers know the writing structure).\nMoreover, 1% highlighted that humans tend to write in a consistent style (human write in consistent style). These\nresponses expressed reviewers' belief that AI cannot replicate the natural flow and logical progression achieved by\nhuman writers through careful and critical thinking and appropriate sentence transitions."}, {"title": "4.4.2 Theme 2: Word Choice.", "content": "Another significant factor influencing reviewers' perceptions was the presence of marker\nphrases and words in both human and AI-generated snippets. For instance, 26% of responses identified specific words and\nphrases (AI uses marker words/phrases) commonly used by AI, such as sentence starters like \"However,...\" and sentence\nstructures like \"...., do-ing....\" In addition, terms such as \u201cleverage\u201d or \u201cstate-of-the-arts\" were seen as indicators of AI\nwriting due to their less common usage compared to simpler alternatives. Interestingly, reviewers' perceptions of these\nmarkers were not always consistent. While 3% of responses noted that contractions, parentheses for explanations, and\ncolons to introduce multiple concepts were unique to human-written snippet (human uses unique marker phrases/words),\nthese markers were also mentioned in other responses as the indication of AI-generated snippets. We include a full list\nof marker words mentioned by reviewers in Appendix B.\nBeyond the identified marker words, reviewers also commented on broader language usage. Twenty-one percent\nof responses noted that AI-generated snippet often employed unusual language choices, which made the text sound\nawkward or unnatural (AI uses unusual language). In addition, a small portion of responses (4%) criticized AI for\nrelying too heavily on adjectives and resulting in an overly descriptive writing style. Conversely, some responses\n(10%) associated plain and natural language with human authors (Human uses plain, natural language). One response"}, {"title": "4.4.3 Theme 3: Problematic Statement.", "content": "Reviewers raised various concerns regarding statements in the snippets. The\nmost common issue was that AI-generated snippets were often generic and non-specific (23%). Additionally, 11% of\nresponses noted over-promising statements in AI-generated snippets. Concerns about factual accuracy were also raised,\nwith two responses (2%) noted that AI-generated snippets often contain false descriptions and non-factual statements.\nInterestingly, two responses (2%) pointed out that AI often repeats content with different phrasing that merely\nsummarizes earlier paragraphs without further elaboration. One response (1%) noted weak statements in AI-generated\nsnippets Al produces weak statement that lacks supporting evidence or being poorly developed."}, {"title": "4.4.4 Theme 4: Expression.", "content": "Reviewers assessed how well the snippets conveyed human emotions, opinions, and\nsubjective experiences. Three percent of responses indicated that human authors use evocative words and figurative\nlanguage to convey personal perspectives (Human expresses personal perspectives and understanding in writing), and 4%\nof responses identified snippets lacking personal and subjective expressions as AI-generated (lacks personal expressions)."}, {"title": "4.4.5 Theme 5: Carefully Crafted Statement.", "content": "Interestingly, 4% of responses admired the expertise in the writing of some\nsnippets and perceived these snippets as \"the work of experienced researchers\" (Al paraphrased content appears to be\nwritten by an experienced researcher). However, these snippets were actually paraphrased using GenAI."}, {"title": "5 DISCUSSION", "content": "Our study supports the concern raised by Tu et al. [78] and extends prior research on the difficulty humans have in\ndistinguishing between AI- and human-authored content in general contexts like news, jokes, and health information\n(e.g., [33, 69, 70]). We found that such an inability also applies to peer reviewers of research publications. Our qualitative\nanalysis highlighted contradictory perceptions among reviewers, where some reviewers identified lengthy sentences,\nconcise language, repetition, and standardized grammar as indicators of AI authorship, while others perceived these as\nsigns of human writing. However, despite these conflicting views, reviewers' judgments of the manuscript and the\npresented research remained consistent.\nIn fact, unlike prior research that identified people's tendencies toward AI aversion or appreciation [19, 34, 56, 69],\nour study found that academia and industry professionals did not exhibit clear negative or positive opinions about the\nmanuscript and its presented research across the three types of snippets, despite varying perceptions of AI involvement.\nThis finding indicates that assessments of research extend beyond writing quality alone. While clarity, conciseness, and\ncoherence are important, other factors such as novelty, methodological transparency, result validity, and contribution\nto the field also significantly influence reviewers' judgments [76]. Thus, our results suggest that when these aspects are\nwell-addressed, the use of GenAI in writing does not necessarily bias reviewers' evaluations.\nFurthermore, our regression analysis indicated that reviewers perceived less AI involvement and higher honesty in\nAI-paraphrased snippets. Reviewers with greater disciplinary expertise and AI familiarity rated higher levels of honesty,\nclarity, and compellingness across all snippet types. This result contrasts with previous studies on non-research writing\ncontexts, where experts found algorithmic advice less trustworthy [80] and those familiar with the algorithm were less\nreceptive to its suggestions [55]. Our qualitative results further showed that reviewers appreciated GenAI's ability to\nproduce well-structured and clear snippets. This perception suggests that GenAI can be a valuable for enhancing the\npresentation of their research through writing. However, reviewers found AI-augmented snippets lacking in logical\nprogression, supporting evidence for statements, and emphasis on key research points. These issues highlight the\nlimitations of GenAI in areas requiring critical thinking, logical reasoning, and nuanced understanding of the research\nfield. Conversely, reviewers noted that human researchers are good at providing detailed evidence and explanation, and\nstrategically emphasizing key points within the manuscript's logical flow. Given that increased human involvement in\nAl-generated content fosters greater ownership and responsibility [24, 65], we thus recommend a human-in-the-loop\napproach to AI-assisted writing to ensure logical, clear, and accurate research manuscripts.\nOverall, our study suggests that while AI can be a valuable in enhancing research communication by improving\nstructure and clarity of its presentation, human researchers' oversight remains crucial to ensure a well-structured,\nlogically sound, and informative final manuscript."}, {"title": "5.1 Implications For Researchers Who Submit to Peer-Reviewed Venues", "content": "Through the perspective of top-tier HCI conference peer-reviewers, our quantitative and qualitative analyses revealed\nthemes that alleviate researchers' concerns about disclosing AI use in manuscript submissions. From reviewers'\nresponses, we identify insights on the appropriate ways to augment research writing with GenAI, and demonstrate\nthat responsible and transparent use of GenAI can enhance the quality of research presentation in writing without"}, {"title": "5.2 Implications For Peer-Reviewers Who Review Research Manuscripts", "content": "While research venues permit the use of GenAI as writing assistants, these tools must be accompanied by human author\noversight and verification [28]. As demonstrated in Figure 4, our study revealed that reviewers identified similar issues\nin both AI- and human-written snippets, such as redundant sentences, overly generic statements, and marker phrases\n(see Appendix B). Thus, these problems are common in both human-written and AI-augmented manuscripts and cannot\nbe used as reliable evidence of AI involvement. Despite the availability of algorithm-based AI detectors, literature\nshows these tools often penalize individuals with limited linguistic proficiency [51], which directly contradicts our\nreviewers' perception that AI-generated snippets uses \"flowery\" language. This contradiction highlights that neither\nexisting AI-detectors nor reviewers' personal strategies are reliable in detecting GenAI. Given our findings, we strongly"}, {"title": "5.3 Future Enforcement of Ethical Use of GenAl in Research Writing", "content": "Our study sheds light on the complexities of regulating and enforcing ethical GenAI use in research writing. Our findings\nrevealed the unreliability of strategies that human reviewers use to distinguish between Al and human authorship.\nTogether with the unreliable result from existing GPT detectors Liang et al. [51]"}, {"title": "5.4 Limitations and Opportunities for Future Research", "content": "Our study has limitations that offer several opportunities for future research. First, our primary limitation is its\nsample size. While our mixed-methods approach gave deep and rich insights, the small number of participants limits\nthe precision of our quantitative estimates. This constraint reflects the challenges in recruiting professional peer\nreviewers [37], a hurdle likely to persist in future studies. Still, our findings offer early and valuable insights into\nhow reviewers see GenAI in academic writing. Future research could explore other approaches. For example, it could\nanalyze acceptance and rejection patterns before and after GenAI adoption. This would add to our findings, even though\nsuch data might be equally difficult to obtain. Second, our study focused on abstracts, not full papers. This approach\nlet us examine varied AI snippets across research areas while maintaining survey manageability for professional\nreviewers. However, it may not fully capture reviewers' judgments of complete manuscripts. Future research should\nextend this investigation to full papers or more extensive snippets. This could reveal more nuanced perceptions of\nAI-augmented academic writing. Third, our sample's limited familiarity with AI in writing likely reflects the current\nreviewer population, given the ongoing controversies surrounding GenAI use in academia. As GenAI becomes more\ncommonplace in research activities, future studies may reveal evolving perceptions among reviewers. This presents an\nopportunity for longitudinal research to track changes in reviewer attitudes and practices over time. Fourth, our study\nalso suffer from common limitations of empirical research. Although we instructed reviewers not to look up the full\npapers in literature databases, we cannot entirely prevent this. Additionally, our data relies on self-reporting, which is\nsubject to the reviewers' honesty and self-awareness. Our study may be subject to social desirability bias. Reviewers are\npotentially underreporting their own GenAI use because of perceived stigma. However, we expect that our findings\nwill help normalize discussions about GenAI in academic writing and, in turn, encourage more open disclosure in\nfuture research. Lastly, we studied general reviewer perceptions across disciplines, with 35% of reviewers specializing in\ngames research, likely due to our research team's majority background in this field. Future research should explore how"}, {"title": "6 CONCLUSION", "content": "Our paper presents a snippet-based online survey examining reviewers' perceptions of human-written, AI-paraphrased,\nand AI-generated snippets. We surveyed 17 experienced peer-reviewers from top-tier HCI conferences and found their\nstruggle in distinguishing between AI-processed and human-written snippets but their judgments on the manuscript\nand the underlying research did not significantly vary. Our results indicate that responsible and transparent use of\nGenAI can enhance research presentation quality without negatively impacting reviewers' perceptions. Given the\ncurrent unreliability of AI detection by reviewers and AI-detection tools, we advocate for reviewer guidelines that\npromote impartial evaluations of submissions, regardless of any personal biases towards GenAI. Our findings encourage\nresearchers to transparently disclose their AI use in manuscripts without the fear of damaging reviewers' perception.\nBased on our findings, we that researchers must maintain their authorship and control over the writing process, even\nwhen using GenAI assistance."}, {"title": "E.1 Instruction", "content": "In this section, you will be presented with 6 snippets from recently published research papers. These snippets may have\nbeen written by human researchers or generated with the assistance of Artificial Intelligence (AI) writing tools. AI\nwriting assistants can help researchers by suggesting phrasing, structuring sentences, and even generating initial drafts.\nPlease read each snippet carefully. You will be asked about your perception on the snippet and the writer.\nImportant: We understand that you may have limited information to make a concrete judgement; the goal of this study is\nto learn about your perception of these snippets. Please do not look up any of the snippets on Google scholar or any other\nliterature search engine because it defeats the purpose of this study!"}, {"title": "E.2 Knowledge and Expertise", "content": "To help us determine the snippets to present to each participant, the participants were first asked about the research\nareas that they have the most and least expertise and knowledge. The research areas that we used as options are from\nthe CHI2023 Paper Submission Subcommittees 11. Based on participants' selection in these questions, we present them\nwith the snippets in the remaining survey questions.\nQ1: Please select one research field in which you consider yourself to have the MOST knowledge or expertise.\n\u2022 Accessibility and Aging\n\u2022 Blending Interaction: Engineering Interactive Systems & Tools\n\u2022 Developing Novel Devices: Hardware, Materials, and Fabrication\n\u2022 Computational Interaction\n\u2022 Critical Computing, Sustainability, and Social Justice\n\u2022 Design\n\u2022 Games and Play\n\u2022 Health\n\u2022 Interacting with Devices: Interaction Techniques & Modalities\n\u2022 Interaction Beyond and Individual\n\u2022 Learning, Education, and Families\n\u2022 Privacy and Security\n\u2022 Specific Applications Areas\n\u2022 Understanding People\n\u2022 User Experience and Usability\n\u2022 Visualization\nQ2: Please select one research field in which you consider yourself to have the LEAST knowledge or expertise.\n\u2022 Repeat the options above."}, {"title": "E.3 Snippets and Questions", "content": "After the instruction message, participants were presented with six snippets, three content types of two abstracts. One\nabstract was selected from the research area that they considered themselves having the most knowledge and expertise,\nand the other was from the research area that they considered themselves having the least knowledge and expertise.\nSample Snippet paraphrased based on the abstract of Putze et al. [67]:\nVirtual Reality (VR) research relies on questionnaires for user experience, but switching between VR and\nreality (Break in Presence - BIP) disrupts immersion and introduces bias. New VR technology allows for\nquestionnaires within the VR environment (inVRQs). This study investigates how inVRQs affect BIP compared\nto traditional questionnaires. We conducted a user study (n=50) with a VR shooter and varying immersion\nlevels. Participants answered questionnaires inside and outside VR. Physiological data measured BIP. Our\nfindings confirm switching to traditional questionnaires induces BIP, while inVRQs effectively reduce it\nwithout impacting user experience. This highlights the potential of inVRQs to minimize bias and improve\nVR user study validity, especially for high-fidelity experiences. This paves the way for researchers and VR\ndevelopers to design more standardized and reliable inVR questioning methods.\nQuestions below are repeated with each snippet:\nQ3: How would you rate your knowledge and/or experience in the field covered by the snippet?\n\u2022 Answered on a scale from \"0-I have no knowledge or expertise in this field\" to \"10-I'm an expert in this\nfield.\"\nQ4: I believe the snippet does the following to represent the content of the paper.\n\u2022 It accurately represents the paper. [Answered on a 5-point Likert scale from \"1-strongly disagree\" to\n\"5-strongly agree.\"]\n\u2022 It reliably represents the paper. [Answered on a 5-point Likert scale from \u201c1-strongly disagree\u201d to \u201c5-strongly\nagree.\u201d]\n\u2022 It honestly represents the paper. [Answered on a 5-point Likert scale from \u201c1-strongly disagree\u201d to \u201c5-\nstrongly agree.\u201d]\n\u2022 It clearly represents the paper. [Answered on a 5-point Likert scale from \"1-strongly disagree\" to \"5-strongly\nagree.\"]\n\u2022 It compellingly represents the paper. [Answered on a 5-point Likert scale from \"1-strongly disagree\" to\n\"5-strongly agree.\u201d]\nQ5: To what extent do you think this snippet was written by a human researcher(s) or generated by artificial\nintelligence (AI)? [Answered on a scale from \u201c0-completely written by human researchers\u201d to \u201c10-completely\nwritten by AI.\u201d]\nQ6: (if not \"0\" in Q5) Please highlight the sentence(s) (if any) that you suspect were written by AI.\n\u2022 Present the snippet again with the highlight function.\nQ7: What specifically in the snippet led you to believe it was written by human researchers or by AI? Please share\nany observations you have about the content's style, structure, or information. [Answered on an open-ended\nspace.]"}, {"title": "E.4 General Perception Questions", "content": "Q8: I trust that a human researcher can accurately communicate their research ideas and outcomes in their academic\nwriting. [Answered on a 5-point Likert scale from \"1-strongly disagree\" to \"5-strongly agree.\u201d]\nQ9: I believe that a human researcher is capable of accurately communicating their research ideas and outcomes in\ntheir academic writing. [Answered on a 5-point Likert scale from \u201c1-strongly disagree\u201d to \u201c5-strongly agree.\u201d]\nQ10: I trust that a generative AI can help researchers to accurately communicate their research ideas and outcomes\nin their academic writing. [Answered on a 5-point Likert scale from \u201c1-strongly disagree\u201d to \u201c5-strongly agree.\u201d]\nQ11: I believe that a human researcher is capable of accurately communicating their research ideas and outcomes in\ntheir academic writing. [Answered on a 5-point Likert scale from \"1-strongly disagree\" to \"5-strongly agree.\u201d]\nQ12: In your research writing process, how often do you use generative AI tools? [Answered on a 5-point scale from\n\"1-never (I do not use these tools at all)\" to \"5-always (I rely heavily on generative AI tools throughout my\nresearch writing process.)\u201d]"}]}