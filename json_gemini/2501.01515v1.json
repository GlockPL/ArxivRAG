{"title": "DIAGRAMMATICLEARNING: A GRAPHICAL LANGUAGE FOR COMPOSITIONAL TRAINING REGIMES", "authors": ["Mason Lary", "Richard Samuelson", "Alexander Wilentz", "Alina Zare", "Matthew Klawonn", "James P. Fairbanks"], "abstract": "Motivated by deep learning regimes with multiple interacting yet distinct model components, we introduce learning diagrams, graphical depictions of training setups that capture parameterized learning as data rather than code. A learning diagram compiles to a unique loss function on which component models are trained. The result of training on this loss is a collection of models whose predictions \"agree\" with one another. We show that a number of popular learning setups such as few-shot multi-task learning, knowledge distillation, and multi-modal learning can be depicted as learning diagrams. We further implement learning diagrams in a library that allows users to build diagrams of PyTorch and Flux.jl models. By implementing some classic machine learning use cases, we demonstrate how learning diagrams allow practitioners to build complicated models as compositions of smaller components, identify relationships between workflows, and manipulate models during or after training. Leveraging a category theoretic framework, we introduce a rigorous semantics for learning diagrams that puts such operations on a firm mathematical foundation.", "sections": [{"title": "1 INTRODUCTION", "content": "The deep learning literature is rife with training regimes that exhibit non-trivial interactions between distinct models. Examples include multi-modal architectures with vision and language components (e.g Vinyals et al. (2015), Ramesh et al. (2022)), knowledge distillation schemes Hinton et al. (2015), multi-task learning setups, and on. The practitioner who manages such collections often juggles interacting models that at times are training or frozen, sometimes classifiers and sometimes feature extractors, sometimes trained on a single task and sometimes on many. Motivated by such practical settings we introduce a formalism for building models that treats data sets, models, and their interactions as structured data rather than as code. Our primary contribution, the learning diagram, is graphical in nature and has a rigorous mathematical semantics, meaning that learning diagrams can be interpreted unambiguously, manipulated with confidence, and related to one another. Learning diagrams are also compositional, meaning that complex training setups can be built from easier to understand pieces.\nAt the heart of our formalism is the observation that machine learning problems can often be framed as a search for commuting diagrams. In what is arguably the simplest case, we begin with a collection of patterns $x_i \\in \\mathbb{R}^m$ and labels $y_i \\in \\mathbb{R}$. We select an architecture $f : \\mathbb{R}^m \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and learn a parameter $\\theta \\in \\mathbb{R}^n$ such that 1 holds for all pairs $(x_i, y_i) \\in \\mathbb{R}^m \\times \\mathbb{R}$.\n$f(x_i, \\theta) \\approx y_i$ \nIf we model the patterns, labels, and parameters as functions\n$X:1\\rightarrow\\mathbb{R}^m, Y:1\\rightarrow\\mathbb{R}, $\\theta$:1 \\rightarrow\\mathbb{R}^n$,\nwhere l is a finite set that indexes patterns and labels, then the equality in 1 means that the following diagram (approximately) commutes, i.e. that the pair of paths starting at l and ending at R produce the same real valued output for any choice of element in l.\nAnatomically, Diagram 3 is a graph with nodes that represent spaces and edges that represent maps between them. The goal of parameterized learning is to find $\\theta$ such that the diagram comes as close as possible to commuting. We observe that the diagram itself can contribute to this goal if the common codomain of labels Y and function f, a role played by $\\mathbb{R}$ in the diagram above, is imbued with a loss function. In that case we can measure the difference between predictions of f and ground truth labels Y and train the component models to minimize said difference. As the loss is minimized, the diagram gets closer to commuting.\nThe process of generating losses is not limited to diagrams with only one pair of paths that must commute. In fact, we will see that many training regimes are designed to make multiple pairs of paths commute all at once. Further, the implementation of training setups as learning diagrams is not an academic exercise. Rather, doing so allows for the construction and manipulation of training setups in a way that would be more difficult without a structured representation. We believe such a shift from learning pipelines as code to learning pipelines as data is needed to address open problems identified by prior work, such as the need for a \"programming interface to specify that various adapted models are derived from the same pre-trained model\u201d Bommasani et al. (2021), or \u201ctools that will allow us to build pre-trained models in the same way that we build open-source software\" Raffel (2021). Our specific contributions are as follows.\n\u2022 We formalize the process of producing diagrams of models and data and compiling them to loss functions. We imbue learning diagrams with a rigorous semantics by way of category theoretic machinery.\n\u2022 We introduce a software library, DiagrammaticLearning.jl, that realizes the theory of learning diagrams to supply convenient operations for building and manipulating training setups, both before models are trained and after, when they may be used as components of other training setups.\n\u2022 We provide several examples of common training paradigms that can be captured as learning diagrams, and show how their implementation as such affords convenient functionality for common ML tasks.\nThe plan of the paper is to present these contributions in reverse order. To motivate the implementation we will first show in Section 2 how certain popular training setups can be realized using our approach. To do so, we have selected a small sample of classic machine learning papers and re-implemented them using our framework, along the way recovering the results of the original papers and demonstrating the utility of our diagrammatic formalism for building and manipulating"}, {"title": "2 LEARNING DIAGRAM DEMONSTRATIONS", "content": "models. In Section 3 we describe the beta version of our library, highlighting available features and the engines that make them work. Finally, in Section 4 we cover the mathematical underpinning of learning diagrams that enables the operations realized in our library. We conclude with an examination of related work and directions for future research.\nOur goal in this section is to show that our approach to developing training loops is expedient and feature rich without negatively affecting the final performance of the produced models. We do not aim to outperform our baselines, but rather to recreate them. In so doing, we will motivate the increasingly abstract constructions of later sections. To facilitate intuition before rigorous definitions, we will leave some terms undefined for now.\nWe have identified training setups from classic papers that are both representative of widely used paradigms, and are sufficiently complex to highlight aspects of our approach that are practically useful. A particular paper's omission from our experiments does not necessarily indicate that we cannot model it as a learning diagram, nor that doing so would be a fruitless endeavor. Among the diagrams we have worked out but not included are norm-based regularization techniques and autoencoders.\nIn each example that follows we define a learning diagram first by considering domains of data corresponding to datasets, their components (such as images and labels), and the representation spaces induced by architectural choices. Each space has an associated Lawvere metric Lawvere (1973), i.e. is a generalization of a metric space relaxing the symmetry and separation axioms, where distances between points may be infinite. For spaces where elements ought to be comparable and generate a loss, we assign one and denote such a space Y as (Y, L), otherwise we assume that distinct points are infinitely far from one another and denote the space (Y,\u221e). The edges in a diagram that connect one domain to another depict models and data. We will see in Section 4 that such spaces and maps between them form an object of mathematical significance called a category. Our formal theory requires that parameter spaces also be represented explicitly, but for the sake of exposition we will omit or include them in our illustrations as convenient.\nWith this recipe for specifying learning diagrams in mind, we move on to our first example of learning diagrams in action: Neural Image Captioning Vinyals et al. (2015). It is the oldest of the group of examples and increases the complexity over standard predictions only slightly, but allows us to introduce some useful features of our approach and library.\nVinyals et al. (2015) was the first paper to demonstrate an end to end trainable model for image captioning. Named the \u201cNeural Image Captioner\u201d (NIC), it uses a fixed visual encoder to extract features from an image, and a trainable recurrent component to produce a caption. We can depict the model as in Diagram 4."}, {"title": "3 IMPLEMENTATION", "content": "Having motivated the functionality of DiagrammaticLearning.jl* with some common ML tasks, we now turn to describing the implementation that provides such functionality.\nTo generate a composite loss, users must specify a diagram. Listing 1 shows an example of the structures required by the library. The required components are\n\u2022 A list of vertices with their labels.\n\u2022 A list of edges keyed by their labels and including their source and target vertices.\n\u2022 Vertex data, with isfinite specifying whether a vertex represents a data source, data_source representing the source of data to pass onto paths, and loss, which specifies the loss function to be used by parallel paths that converge on this node.\n\u2022 Edge data, with model representing the function attached to an edge and $\\leq$, representing a specific preorder between edges, a requirement specified further in Section 4. Data and fixed parameters can always be treated as models that return a constant value.\nFrom a learning diagram, we aim to calculate a single loss function. Since the loss function is a sum over all the parallel paths, we first need to compute all pairs of parallel paths from the graph.\nTo accomplish this, we apply an algebraic perspective and use the fact that powers of the adjacency matrix count paths in a graph. This approach is a generalization of the celebrated Floyd-Warshal algorithm Floyd (1962). We represent the graph as a V \u00d7 V matrix with entries that are sets of edges in the graph. This allows us to define a generalized version of matrix multiplication which computes paths between vertices Fong and Spivak (2018); Pratt (1989). Multiplication of elements represents concatenation of paths and summation represents union of sets, which leads to an algorithm that computes all the paths in a graph. Powers of this set-of-paths valued matrix then enumerate the paths between each pair of vertices of a fixed length. Parallel paths can then be collected from any matrix element with more than 1 element.\nMany common machine learning loss functions are not symmetric. These loss functions require parallel paths to occur in a specified order. To facilitate this, we assume that all pairs of single edges are ordered (i.e. $e_1 \\leq e_2$ and $e_2 \\leq e_1$ for edges $e_1, e_2$), unless only a single pair is specified in the graph. The process that builds the matrix containing parallel paths additionally builds up a preorder between paths, allowing a filter to include only parallel paths that conform to the preorder.\nDue to the generic nature of our representation, we can view a learning diagram as a sort of intermediate language, independent of a particular neural network framework. To this end, we can compile either PyTorch or Flux models from a given diagram. This flexibility allows for the creation of diagrams for new workflows and experiments, as well as the retrofitting of diagrams to existing codebases. This paper focuses on PyTorch models due to its ubiquity in machine learning research and practice.\nThe correctness of the compilation process is explored rigorously in the next section, but for a more familiar explanation, we refer to PyTorch components. Vertices in a graph in our framework contain both data sources (such as a PyTorch dataloader) as well as a loss function. A path in our graph can quite easily be converted into a PyTorch Sequential model. Given parallel paths, a module can be created which samples x from the data source present at the domain of the paths, passes x through both sequential models to obtain outputs y, \u0177, and outputs l(y, \u0177), the result of applying the loss function I found in the codomain of the paths. Summation of these modules represents composite loss functions. This process thus encapsulates data sampling, network computation, and loss computation into a single PyTorch module.\nThe requirements for compatibility with our framework are similar to those for compatibility with pure PyTorch. For data, we require iterable components, whether it be DataLoaders or just lists. For computations, we require functions, either in the form of torch modules, or pure Python functions."}, {"title": "4 THEORETICAL GROUNDING", "content": "Thus, popular frameworks within the PyTorch landscape, such as models from HuggingFace, are compatible with our system.\nHaving discussed some applications and our implementation, we now turn to the underlying category theoretic formalisms and definitions. It is the content of this section that ensures learning diagrams have rigorous semantics, and therefore that manipulations of diagrams and maps between them can be interpreted mathematically. The ultimate goal of the section is a machinery that \u201cmathematically compiles\" a learning diagram into a composite loss function that drives participating models to form approximately commuting diagrams. Throughout this section we will use some category theory that we may not define due to space constraints. Everything left undefined should be easy to find in introductory references Riehl (2017); Fong and Spivak (2018). Our story proceeds by defining what a learning diagram is, explaining how parallel paths can be extracted from it, and finally how these parallel paths combine to form a composite loss function.\nA learning graph (G, \u2264) is a directed multigraph G = (V, E, src, tgt) together with a preordering \u2264 of its edges.\nWe will motivate the preorder in due time. On its own, a learning graph doesn't associate a loss function to each node of the graph. Rather, we consider it a syntactic presentation of a training regime. As we saw in the image captioning example, this distinction can be useful when varying the exact data or models being used. In the end, however, what we want is a full learning diagram. The choice of the word diagram is no accident; it has a proper category theoretic definition.\n(1.6.4 of Riehl (2017)) A diagram in a category C is a functor D: J \u2192 C whose domain, the indexing category, is a small category.\nTo define a diagram, therefore, we must first specify a domain category J and a codomain category C. Our domain category comes from our learning graph and identifies the syntactic structure of our training regime: how many spaces of data or representations we are considering, how many models, etc. The codomain category C defines what kind of thing each node and edge in our learning graph is, i.e. the semantics of the training setup, and the functor D is the mapping from syntactic structure to semantic content that assigns particular spaces and models to edges. Constructing a category to serve as our domain is fairly straightforward; we can take a learning graph and turn it into a category by taking vertices of the graph to be objects of a category and paths in a graph to be morphisms. Such a construction is called the free category on a graph. Defining the semantics conferring codomain requires more slightly more work. Given that we want to associate a potentially asymmetric loss function to each node in a learning graph, we should consider a category C whose objects are some generalization of metric spaces. We choose the category Law.\nDenote by Law the category that has objects Lawvere Metric Spaces Lawvere (1973) and morphisms Lipschitz continuous functions.\nLet Par be the free category on the following graph.\nWe can use the category Par (5) to formalize the construction of losses from parallel paths. If (G, \u2264) is a learning graph, and if Free(G) is the free category generated by G, then a functor P: Par \u2192 Free(G) chooses a pair of paths in G with the same initial and terminal vertex. If D: Free(G) \u2192 Law is a learning diagram on G, then the composite functor D\u25cbP : Par \u2192 Law chooses a pair of Lipschitz continuous functions f : X \u2192 Y and g : X \u2192 Y with the same domain and codomain. We will assign a loss to these functions as follows. If X is a finite set, then we define l(f, g) \u2208 [0, +\u221e] to be the sum\n$l(f, 9) := \\sum_{x\\in X} dy (f(x), g(x)).$\nIf X is an infinite set, then we take the supremum over all finite sums. This quantity measures the distance between outputs of f and g summed over all inputs. It also has the following contractive property: if f : X \u2192 Y, g : X \u2192 Y, f' : X' \u2192 Y', and g' : X' \u2192 Y' are Lipschitz continuous functions, and if p : X \u2192 X' and q : Y \u2192 Y' are surjective 1-Lipschitz functions such that\n$f'(p(x)) = q(f(x)) and g'(p(x))) = q(g(x)))$\nfor all x \u2208 X, then l(f, g) \u2265 l(f', g'). Formally, loss is an enriched functor l : C \u2192 Cost, where C is the subcategory of the functor category LawPar whose fibers are surjective 1-Lipschitz functions, and where Cost is the partially ordered set [0, 0].\nThe mapping P \u2192 l(f, g) defines a functor l : C \u2192 Cost.\nLet f, g, f', g', p and q be the functions described above. Functoriality is the contractive property, l(f, g) \u2265 l(f', g'). To see this, let S \u2286 X' be a finite set. Then,\n$\\sum_{x'\\in S}dy (f'(x'), g'(y')) \\leq \\sum_{x\\in p^{-1}S} dy (f'(p(x)), g'(p(x)))$\n$=\\sum_{x\\in p^{-1}S}dy (q(f(x)), q(g(x)))$\n$\\leq \\sum_{x\\in p^{-1}S}dy(f(x), g(x))$\nwhere the first line follows from the surjectivity of p and the third from the contractivity of q. The desired inequality follows from the arbitrary selection of S.\nDiagrams in Law with a shape given by a learning graph very nearly capture the data we require to build a composite loss, but for two problems. For one, if we are allowing asymmetric loss functions, then we must provide the creator of a learning diagram with a way to say how arguments to a loss should be ordered. Additionally, some vertices in the learning graph will be assigned finite collections for indexing data sets and other vertices will be assigned vector spaces for representing the spaces containing the data values. The loss functions should only use parallel paths that start at indexing, because these lead to finite sums in the loss functions.\nThe preorder on edges in a learning graph naturally leads to a preorder structure on the free category associated with a graph. This produces a locally posetal 2-category, which is a category with a preorder on every hom-set, such that morphism composition is monotonic.\nThe upshot is that our syntactic 2-category now tracks user specified ordering of models, i.e. which model's predictions should be the first argument to a loss and which should be the second.\nThe other problem is that we only want to build composite losses for parallel paths that start at specific nodes. The machinery of categories provides a succicint way to specify the relevant constraints. Let Ind be the free 2-category generated by the learning graph\nBy setting - < +, we may upgrade the category Par defined in section 2 to a 2-category. These categories are related by a 2-functor a : Par \u2192 Ind.\nWe use the 2-category Ind to assign labels to each vertex of a learning graph. Specifically, for all learning graphs G, a functor \u03b2 : Free(G) \u2192 Ind is a labelling of the vertices of G as either indexing"}, {"title": "5 RELATED WORK", "content": "or non-indexing, and the structure of Ind ensures that indexed vertices precede non-indexed ones. A commutative diagram of the form (8) chooses a pair of parallel paths that begin at indexed nodes.\nGiven a learning diagram D : Free(G) \u2192 Law, the composite functor D\u25e6 P : Par \u2192 Law chooses a pair f : X \u2192 Y and g : X \u2192 Y of Lipschitz continuous functions, and we can compute the loss of these functions as in section 2. (We define the composite loss of a triple (G, D, \u03b2) to be the sum l* (G, D, \u03b2) \u2208 [0, +\u221e] given by\n$l* (G, D, \\beta) = \\sum_{p} l(f, g)$\n) where f and g are the functions corresponding to the functor DP, and where the sum is taken over all choices of P making the the diagram in 8 commute. This construction allows us to build losses automatically from graphical specifications of machine learning architectures. The contrac-tive nature of loss functions ensures that composite losses are well behaved with respected to taking subsets of each dataset in the diagram.\nWe find ourselves aligned in vision with the field of AutoML in that we seek to make the life of the machine learning practitioner easier. More specifically, we fit with work that focuses on structuring machine learning pipelines; a pipeline focused AutoML survey can be found in Z\u00f6ller and Huber (2021). The formulation of Z\u00f6ller and Huber (2021) describes pipelines as Directed Acyclic Graphs (DAGs). We differ from this formulation and other DAG-based formulations in many important ways. For one, we have a mathematically grounded definition of what data is captured by nodes and paths in our learning diagrams, whereas prior art appeals to imprecise terms such as \"primi-tives\" Lippmann et al. (2016), \u201calgorithms\u201d, \u201coperators\u201d, \u201ctransformations\u201d and so on. There are some works (e.g. Drori et al. (2019)) that place more structure on pipelines, but in this frame-work the grammar is still defined on an unstructured set of ad-hoc operations. Further, works Kalyuzhnaya et al. (2020); Nikitin et al. (2022); Polonskaia et al. (2021) that define somewhat flex-ible pipelines usually limit themselves to DAGs that have a single source and sink representing the data and label space. As best we can tell, there appear to be at most a few exceptions to this rule, e.g. Klein et al. (2017) which is limited to Bayesian settings with a common data domain shared among tasks. Thanks to the rigor of our model we can accommodate arbitrarily many tasks interacting in arbitrary ways, so long as the nodes are Lawvere metric spaces and the paths between nodes are continuous.\nLearning diagrams also provide a graphical programming language for constructing machine learn-ing models, and in that sense are related to works that study foundations of languages for machine learning models Elliott (2018), though such techniques tend to focus on probabilistic program-ming Gordon et al. (2014) or the semantics of gradient descent and less on the semantics of the objective to be optimized.\nCategory theory has already been used to examine machine learning models compositionally, for example in Fong et al. (2019); Cruttwell et al. (2022). Our approach is somewhat different from most such works in that we focus on the semantics of the loss or objective function as opposed to the semantics of model updates. That said, we believe there are connections to explore, par-ticularly with Hanks et al. (2024) as it also considers how to specify compositionally a loss for machine learning models. More generally, our work belongs to an emerging tradition of using cat-egory theory to provide a diagrammatic yet rigorous approach to describing models in a domain of interest Decapodes.jl Morris et al. (2024); Patterson et al. (2023); Libkind et al. (2022) and Alge-braicPetri.jl Libkind et al. (2022)."}, {"title": "6 FUTURE WORK AND CONCLUSION", "content": "We introduce DiagrammaticLearning.jl, a library built on the notion of learning graphs and learning diagrams allowing machine learning practitioners to specify complex interactions between multi-ple models in a way that is inherently compositional and structured. The utility of this approach is established by capturing some popular training setups as learning diagrams and demonstrating convenient functionality that came as a result. We further outline the implementation and theoreti-cal backing that make learning diagrams possible, demonstrating a rich and rigorous semantics that leads naturally to principled operations for manipulating structured collections of models.\nThis paper has only scratched the surface of potential capabilities. Future work should investigate how model structure can be exploited to improve distributed training and manage high performance systems. Additionally, our structured representation seems like a candidate for the foundation of a more complete machine learning model management system. We hope for example to elevate software centric definitions of reproducibility, such as the one given in He et al. (2021), to a math-ematical plane that is \u201cimplementation free\u201d in the sense that any implementation faithfully repro-ducing the math will yield equivalent results. Homomorphisms between diagrams could capture re-lationships between development iterations and facilitate operations like searching for models with particular properties or component sub-diagrams. On the more theoretical side, our formalism is currently limited to building losses for finite data sets, but extensions to methods where continuous distributions can enter the diagrams are possible. Additionally, our implementation of training net-works of models uses mini-batches, but the theory does not address such aspects of practical training algorithms.\nDevelopment and adoption of these tools would reduce the labor necessary to produce effective ML pipelines, accelerate iteration of ideas within and between research groups, reduce errors intro-duced in software implementations, and provide quicker turnarounds while evaluating novel ideas. We hope the learning diagrams paradigm will accelerate the overall process of machine learning research."}]}