{"title": "Walking the Web of Concept-Class Relationships in Incrementally Trained Interpretable Models", "authors": ["Susmit Agrawal", "Deepika Vemuri", "Sri Siddarth Chakaravarthy P", "Vineeth N. Balasubramanian"], "abstract": "Concept-based methods have emerged as a promising direction to develop interpretable neural networks in standard supervised settings. However, most works that study them in incremental settings assume either a static concept set across all experiences or assume that each experience relies on a distinct set of concepts. In this work, we study concept-based models in a more realistic, dynamic setting where new classes may rely on older concepts in addition to introducing new concepts themselves. We show that concepts and classes form a complex web of relationships, which is susceptible to degradation and needs to be preserved and augmented across experiences. We introduce new metrics to show that existing concept-based models cannot preserve these relationships even when trained using methods to prevent catastrophic forgetting, since they cannot handle forgetting at concept, class, and concept-class relationship levels simultaneously. To address these issues, we propose a novel method - MuCIL- that uses multimodal concepts to perform classification without increasing the number of trainable parameters across experiences. The multimodal concepts are aligned to concepts provided in natural language, making them interpretable by design. Through extensive experimentation, we show that our approach obtains state-of-the-art classification performance compared to other concept-based models, achieving over 2\u00d7 the classification performance in some cases. We also study the ability of our model to perform interventions on concepts, and show that it can localize visual concepts in input images, providing post-hoc interpretations.", "sections": [{"title": "1 Introduction", "content": "Concept-based models have gained the attention of the computer vision community in recent years (Kim et al. 2023; Margeloiu et al. 2021; Barker et al. 2023; Koh et al. 2020), as a means of interpreting the output prediction in terms of learned or human-defined atomic interpretable 'concepts'. These models attempt to predict the target class generally as a weighted linear combination of meaningful concepts. For example, such a model may identify the concept set {whiskers, four legs, pointy ears, sociable} as key semantic attributes that help make a prediction cat on a given image. But with the recent growth of incremental model learning, a pertinent question arises can the abovementioned model be adapted later to identify golden retriever in addition to cat? The two classes share some common concepts such as {four legs, sociable}, while having discriminative concepts such as {whiskers, pointy ears} in case of cat and {golden fur, floppy ears} in case of golden retriever. Incrementally learning such concept-based models with newer classes as well as concepts forms the key focus of this work.\nTeaching neural network models about new distinctive concepts (e.g. golden fur) of new classes (e.g. dog), while reusing previously known concepts (four legs, sociable) is a highly nontrivial problem. While concept-based models have seen a range of efforts in recent years, there has been very little work in incrementally learning such models. The limited existing efforts either assume that all new classes share the same pool of attributes as older ones (Marconato et al. 2022), or have independent non-overlapping attribute sets (Rymarczyk et al. 2023). In this work, we propose a more general and realistic setting in which classes seen at a later stage may share concepts from past classes while introducing new concepts of their own. This creates a complex web of concept-class relationships across experiences, as illustrated in Figure 1, which needs to be preserved and expanded as the model learns to classify and explain new classes. Note that, as in standard incremental learning, the model is required to achieve good classification performance on newly introduced classes, while maintaining classification performance on previously seen ones.\nWhile traditional deep learning models are susceptible to forgetting old classes as new ones are introduced (catastrophic forgetting (Hadsell et al. 2020)), we find that concept-based models are susceptible to forgetting concept-class relationships as well. Overcoming two levels of forgetting in incrementally learning concept-based models presents new challenges that need to be addressed explicitly.\nTo this end, we propose MuCIL, a novel Multimodal Concept-Based Incremental Learner. We combine embeddings of text concepts, called concept anchors, with image representations to create multimodal concept embeddings for a given image. These embeddings are latent vectors containing information that helps in classification while also providing interpretations. We propose the use of concept grounding, which allows the interpretation of multimodal concepts in terms of text-based concepts. All these components are integrated with the fundamental consideration that the model should be able to incorporate new classes and new concepts continually, while also forming new concept-class associations that may emerge in the process.\nOur key contributions can be summarized as follows: (i) We propose a new method for the relatively new setting of concept-based incremental learning, where a model adapts dynamically to new classes as well as new concepts; (ii) We introduce multimodal concept embeddings, a combination of image embeddings and interpretable concept anchors, as part of our method to perform classification. Our approach is primarily intended to allow scalability of concept-based models to newer experiences without increase in parameters; (iii) We perform a comprehensive suite of experiments to evaluate our method on well-known benchmark datasets. We study our method's performance both in a incremental as well as standard supervised settings, achieving state of the art results; (iv) We propose three new metrics to evaluate concept-based models in the proposed setting: Concept-Class Relationship Forgetting, Concept Linear Accuracy and Active Concept Ratio. Our approach can offer concept-specific localizations implicitly as a means of interpreting the model prediction (see Figure 5), without being explicitly trained to do so."}, {"title": "2 Related Work", "content": "Interpretability of Deep Neural Network Models: Interpretability methods in DNN models can be broadly classified into post-hoc and ante-hoc methods. Post-hoc methods aim to interpret model predictions after training (Selvaraju et al. 2017; Chen et al. 2020; Chattopadhay et al. 2018; Sattarzadeh et al. 2021; Yvinec et al. 2022; Benitez et al. 2023; Sundararajan and Najmi 2020; Wang, Wang, and Inouye 2020; Jethani et al. 2021; Wang, Wang, and Inouye 2020; Dabkowski and Gal 2017; Fong and Vedaldi 2017; Petsiuk, Das, and Saenko 2018; Montavon et al. 2019). Recent efforts have highlighted the issues with post-hoc methods and their reliability in reflecting a model's reasoning (Rudin 2019; Vilone and Longo 2021; Nauta et al. 2023). On the other hand, ante-hoc methods that jointly learn to explain and predict provide models that are inherently interpretable (Sokol and Flach 2021; Benitez et al. 2023). Ante-hoc methods have also been found to provide interpretatations that help make the model more robust and reliable (Alvarez-Melis and Jaakkola 2018; Chattopadhyay et al. 2022). We focus on this genre of methods in this work. (Koh et al. 2020) proposed Concept Bottleneck Models (CBMs), a method that uses interpretable, human-defined concepts, combining them linearly to perform classification. CBMs also allow human interventions on concept activations (Shin et al. 2023; Steinmann et al. 2023) to steer the final prediction of a model. (Kim et al. 2023; Collins et al. 2023; Yan et al. 2023) obtained the intermediate semantic concepts by replacing domain experts with Large Language Models (LLMs). This allows for ease and flexibility in obtaining the concept set. Using LLMs to obtain concepts also allow grounding of neurons in a bottleneck layer to a human-understandable concept, an issue with CBMs that was highlighted in (Margeloiu et al. 2021). Other concept-based methods (Alvarez-Melis and Jaakkola 2018; Chen, Bei, and Rudin 2020; Kazhdan et al. 2020; Rigotti et al. 2021; Benitez et al. 2023) use a different notion of concepts based on prototype representations (see appendix \u00a7A1).\nConcept-Based Incremental Learning: While Incremental Learning in standard supervised settings has been widely explored (Wang et al. 2023), Concept-Based Incremental Learning has remained largely unstudied. We identify (Marconato et al. 2022) as an early effort in this direction; however, this work trains CBMs in a continual setting under an assumption that all concepts, including those required for unseen classes, are accessible from the first experience itself, which does not emulate a real-world setting. More recently, (Rymarczyk et al. 2023) proposed an interpretable CL method that uses part-based prototypes as concepts. As mentioned earlier, our notion of concepts allows us to go beyond parts of an object category, as in CBM-based models."}, {"title": "3 MuCIL: Methodology", "content": "Preliminaries and Notations. Given a sequence of T experiences, with each experience i consisting of n image-label pairs $(X^{i}, Y^{i}) = \\{(x_{1}^{i},y_{1}^{i}), (x_{2}^{i}, y_{2}^{i}), ..., (x_{n}^{i}, Y_{n}^{i})\\}$, a class-incremental continual learning (CIL) system aims to learn an experience t without catastrophically forgetting the previous t - 1 experiences. In the scenario where finer class details are available as concepts for classification, each experience i consists of n image-label-concept tuples $(X^{i}, y^{i}, C^{i}) = \\{(x_{1}^{i}, y_{1}^{i}, C_{1}^{i}), (x_{2}^{i}, y_{2}^{i}, C_{2}^{i}), ..., (x_{n}^{i}, Y_{n}^{i}, C_{n}^{i})\\}$, where $C^{i}$ is the set of concepts known during experience i and $C_{k}^{i}$ is the set of active concepts in example k. The set of concepts known during i is the union of all concept sets from experiences 1 to i. We use the same active concept set for all instances of the same class, i.e., $C_{k1}^{i} = C_{k2}^{i}$ if $y_{k1} = \u0423_{k2}$ as done in prior work (Koh et al. 2020; Yang et al. 2023; Oikarinen et al. 2023). Annotating instance-level concepts for large datasets is a challenging and error-prone task, requiring large amounts of time and effort by domain experts. Instead, using concepts at a class level allows us to specify the general characteristics of a class. We obtain these concepts directly from annotations in certain datasets (e.g. CUB) and use a Large Language Model (LLM) in other cases. One could view this as a weakly supervised setting with noisy concept labels.\nChallenges. While concept-based models for image classification have grown in the community, extending them to learn across experiences and new classes incrementally is non-trivial, introducing new challenges. (1) Forgetting at Two Levels: In a traditional CIL setting, the catastrophic forgetting of previously learned classes in newer experiences is only at a class-level. In our proposed setting, it includes both concept-level and concept-class relationship forgetting. Beyond forgetting of concepts from previous experiences, concept-class relationship forgetting can be more critical, affecting the model's ability to recognize previously encountered classes based on the concepts they were originally associated with. This affects the model's ability to understand and preserve how concepts relate to classes across experiences. (2) Parameter Scaling: Extending existing concept-based learning frameworks to a CIL setting would necessitate the addition of new learnable parameters with each new experience, leading to larger models, which is undesirable. Therefore, designing a method capable of integrating new classes and concepts without expanding the parameter space is ideal. (3) Information Bottleneck: Another challenge is the use of a single representation for encoding all relevant visual and concept information, which could lead to an information bottleneck as the system scales to accommodate more concepts and classes. (4) Semantic Misrepresentations: Lastly, some existing methods do not accurately capture the semantics of concepts (Margeloiu et al. 2021). One widely used method to explicitly encode concept semantics is leveraging textual concept embeddings. These semantics should be respected and updated as new concepts arrive. A naive way to implement this is by projecting a network's output representation on pre-computed concept embeddings (Yang et al. 2023; Oikarinen et al. 2023), which amplifies the unified representation problem. We propose a more flexible method for overcoming this issue.\nWe propose a novel framework that addresses each of the abovementioned challenges. Our overall solution consists of multiple components: (1) A Multimodal Encoder, that fuses visual information and textual concepts to create multimodal image-concept embeddings. It is designed to handle an increasing number of concepts without growing the number of parameters. It also creates multiple distinct representations in parallel to avoid information bottlenecks while preserving concept semantics. (2) A Parameter-free Clasi-fier that uses the multimodal concept embeddings along with the textual descriptions of classes to perform classification. (3) Concept Neurons that predict whether or not a given concept exists in an image and also allow probing the model for interpretability.\nMultimodal Image-Concept Encoder. We introduce a novel multimodal image-concept encoder that can be integrated into any standard transformer architecture. It is used to merge image embeddings and textual concept embeddings, thereby generating a sequence of multimodal embeddings that capture both visual and concept inputs.\nOur encoder incorporates new concepts without expanding its parameter space. This is made possible by the transformer's inherent ability to handle variable-length sequences, allowing the model to accommodate additional concepts by appending them to the input sequence without necessitating an increase in parameters. The use of transformer encoder layers enables the processing of mixed data inputs, resulting in a sequence of multimodal image and concept embeddings by simultaneously attending to the two modalities. Using a transformer is hence not a drop-in replacement to a different network architecture such as a CNN, but is a central part of our overall methodology.\nFormally, the multimodal image-concept encoder, M, is a stack of transformer encoder layers that receive image embeddings $x_{i}$ as well as textual concept embeddings $C_{1}C_{2}...C_{|C^{i}|}$ as input. Note that this includes all concepts available until the current experience. The output of M is a sequence of vectors $\\{X'^{(i)}, C'^{(i)}\\}$, where $X'^{(i)}$ corresponding to the image patch embeddings $\\{x_{i}\\}$ and $C'^{(i)} = \\{C_{1}, C_{2}, ..., C_{|C^{i}|}\\}$ are concept embeddings. These embeddings combine both textual and visual information and hence are multimodal by design. Similar to standard transformer-based classification where all outputs except the CLS token are discarded, the $X'^{(i)}$ plays no further part in our methodology.\nThe concept embeddings $C'^{(i)}$ do not have any semantic grounding however; we hence use a Concept Grounding Loss to map them to known natural language-based concept anchors, as below:\n$L_{G} = \\frac{C}{\\sum\\limits_{k=1}^{|C^{i}|} 1_{c_{k}}.(W^{T}c_{k}^{i} + b)}\\frac{C_{i}}{|c_{k}||W^{T}c_{k} + b|}$ (1)\n$W$ and $b$ are learnable parameters that are shared among concepts. This loss semantically aligns the learned concept embeddings with their corresponding concept text anchors in the ground truth by ensuring that all original embeddings can be recovered using a single linear transform. Intuitively, this enforces each output concept vector to be associated with a deterministic representation of the concept, viz the text embedding. This deterministic association with a known (\"grounded\") reference guarantees that the vector has an associated semantic meaning, thus providing interpretability.\nParameter-free incremental classification. As stated earlier, existing concept-based learning frameworks use dense layers on top of concept embeddings, which makes it challenging to scale to newer experiences continually. We hence take inspiration from methods such as (Radford et al. 2021) to use text embeddings of classes to perform classification, thus allowing our method to be parameter-free in the classifier and allowing for scalability to newer experiences.\nClassification is hence performed by returning the index of the class embedding which aligns most with the vectors in $C'^{(i)}$ for a given input sample. The alignment between the jth concept embedding of the current sample, $c'_{j}$, and the text embedding of the kth class $y_{k}$ is computed as $c'_{j} \\cdot y_{k}$, their dot product. The final classification result is hence given by $\\hat{y} = argmax(s_{1}, s_{2}, ..., s_{|Y^{i}|})$, where $s_{k} = \\sum\\limits_{j=1}^{C} c'_{j} \\cdot y_{k},$ which returns the index of the class that aligns most strongly with concept embeddings of a given input. We use $s_{k}$ as the logit of class k, and perform a softmax operation on the logits to get classification probabilities, which are used to train the model with the standard cross-entropy loss $L_{CE}$.\nWe note that deriving class strengths from the concept embeddings in the above manner does not require additional parameters in newer experiences; this is a key element of our framework that enables scalability to both unseen classes and concepts when deployed in a CL setting.\nConcept Neurons. The presence of a concept is evaluated using a single shared dense layer with a sigmoid activation, denoted by $\\sigma(\\cdot)$, applied independently on each $c\\in C'$. A weighted binary cross-entropy loss $L_{WBCE}$ is used to train the layer using provided ground-truth concept labels. We refer to the output logits of the layer $\\sigma(C')$ collectively as concept neurons. In MuCIL, concept neurons serve two purposes: (i) They provide additional supervision to learn concept embeddings better; and (ii) They provide an interface for identifying which concepts are active or inactive, thus enhancing interpretability. This allows for probing the model to evaluate the quality of concepts learned, and study how they change over experiences (see \u00a74). We show a detailed illustration explaining them in appendix (\u00a7A2).\nDetails of $L_{WBCE}$. For a given image, the ratio of the number of active concepts to the number of total concepts is quite small. This necessitates penalizing the misclassification of active concepts more strongly than the misclassification of inactive concepts. We do this by weighting the loss for active concepts by the fraction of inactive concepts, and weighting the loss for inactive concepts by the fraction of active concepts. The loss $L_{WBCE}$ is then defined as:\n$L_{WBCE} = \\frac{C_{active}}{\\# \\text{ inactive concepts}} \\sum\\limits_{i=1} L_{BCE}(\\sigma(c'_{i}), 1) + \\frac{C_{inactive}}{\\# \\text{ active concepts}} \\sum\\limits_{i=j} L_{BCE}(\\sigma(c'_{i}), 0)$\nTraining Procedure. MuCIL thus has the following learnable components trained simultaneously end-to-end: parameters of M, parameters W and b in $L_{G}$, and the weights of the dense layer used for obtaining concept neuron values, $\\sigma(\\cdot)$. The global objective L is a weighted sum of the three different objectives:\n$L = L_{CE} + \\lambda_{1}L_{WBCE} + \\lambda_{2}L_{G}$ (2)\nIntuitively, the terms $L_{WBCE}$ and $L_{G}$ control the concept-based learning in the framework. $L_{WBCE}$ specifically trains M and $\\sigma$ jointly to enable detection of the presence/absence of concepts, thus ensuring that visual features are properly represented in the multimodal concept vectors. $L_{G}$ trains M, W, and b to enforce the concept embeddings to be grounded to their textual anchors. The $L_{CE}$ term trains M to perform classification based on the outputs in C'.\nInference. At test-time, given an image $x_{test}$ to be classified after experience t, we obtain its embedding vectors $x_{test}$ and append them to the sequence of concept embeddings $C_{1}C_{2}...C_{|C^{t}|}$. M receives this combined sequence as input and outputs a sequence of vectors $\\{(C'^{(test)})\\}$. $\\hat{y}$ in $C'^{(test)}$ is provided to the parameter-free classifier for obtaining the final class label. The active/inactive concepts in $x_{test}$ can be identified by thresholding the outputs of the concept neurons $\\sigma(C'^{(test)}).$"}, {"title": "4 Experiments and Results", "content": "We perform a comprehensive suite of experiments to study the performance of MuCIL on well-known benchmarks: CIFAR-100", "algorithms": "Experience Replay (ER) (Rebuffi et al. 2017)", "include": "i) ICIAP (Marconato et al. 2022)", "Metrics": "Classification. In the CL setting", "metrics": "Final Average Accuracy (FAA) and Average Forgetting (AF). FAA is defined as: $FAA = \\frac{1"}, {"as": "AF = \\frac{1"}, {"Metrics": "Concept Evaluation. In order to evaluate the learned concepts and their evolution across experiences", "metrics": "concept linear accuracy", "Accuracy": "We use our concept neurons to evaluate how well concepts capture the relevant semantic information (for performing classification)", "Forgetting": "We define Concept-Class Relationship Forgetting (CCRF) of a concept set as the loss of its ability to provide relevant information to perform class-level discrimination over time. This can occur when concepts no longer align well to visual semantics in the provided image. This is different from forgetting in standard CL settings (Hadsell et al. 2020) as the model may predict concepts correctly", "as": "n$CCRF = \\frac{1"}, {"C_{k})": 3, "Ratio": "To study the relevance of concepts to classes across experiences, we propose Active Concept Ratio (ACR) to measure how frequently concepts seen during experience i activate when classifying images from experience j. A high value indicates that concepts from experience i play an important role in understanding classes from experience j. Ideally, classes introduced in experience i should have their highest ACR values associated with the concept set from the same experience, since those concepts best explain the classes. Positive ACR values with concept sets from other experiences indicate that those concepts are also activated in response to classes from experience i. Formally, let $N_{j}$ be the number of images to be classified in experience j. Then, the ACR for a concept set presented in experience i for classifying images from experience j is defined as"}]}