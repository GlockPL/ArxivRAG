{"title": "Walking the Web of Concept-Class Relationships in Incrementally Trained Interpretable Models", "authors": ["Susmit Agrawal", "Deepika Vemuri", "Sri Siddarth Chakaravarthy P", "Vineeth N. Balasubramanian"], "abstract": "Concept-based methods have emerged as a promising direction to develop interpretable neural networks in standard supervised settings. However, most works that study them in incremental settings assume either a static concept set across all experiences or assume that each experience relies on a distinct set of concepts. In this work, we study concept-based models in a more realistic, dynamic setting where new classes may rely on older concepts in addition to introducing new concepts themselves. We show that concepts and classes form a complex web of relationships, which is susceptible to degradation and needs to be preserved and augmented across experiences. We introduce new metrics to show that existing concept-based models cannot preserve these relationships even when trained using methods to prevent catastrophic forgetting, since they cannot handle forgetting at concept, class, and concept-class relationship levels simultaneously. To address these issues, we propose a novel method - MuCIL- that uses multimodal concepts to perform classification without increasing the number of trainable parameters across experiences. The multimodal concepts are aligned to concepts provided in natural language, making them interpretable by design. Through extensive experimentation, we show that our approach obtains state-of-the-art classification performance compared to other concept-based models, achieving over 2\u00d7 the classification performance in some cases. We also study the ability of our model to perform interventions on concepts, and show that it can localize visual concepts in input images, providing post-hoc interpretations.", "sections": [{"title": "1 Introduction", "content": "Concept-based models have gained the attention of the computer vision community in recent years (Kim et al. 2023; Margeloiu et al. 2021; Barker et al. 2023; Koh et al. 2020), as a means of interpreting the output prediction in terms of learned or human-defined atomic interpretable 'concepts'. These models attempt to predict the target class generally as a weighted linear combination of meaningful concepts. For example, such a model may identify the concept set {whiskers, four legs, pointy ears, sociable} as key semantic attributes that help make a prediction cat on a given image. But with the recent growth of incremental model learning, a pertinent question arises can the abovementioned model be adapted later to identify golden retriever in addition to cat? The two classes share some common concepts such as {four legs, sociable}, while having discriminative concepts such as {whiskers, pointy ears} in case of cat and {golden fur, floppy ears} in case of golden retriever. Incrementally learning such concept-based models with newer classes as well as concepts forms the key focus of this work.\nTeaching neural network models about new distinctive concepts (e.g. golden fur) of new classes (e.g. dog), while reusing previously known concepts (four legs, sociable) is a highly nontrivial problem. While concept-based models have seen a range of efforts in recent years, there has been very little work in incrementally learning such models. The limited existing efforts either assume that all new classes share the same pool of attributes as older ones (Marconato et al. 2022), or have independent non-overlapping attribute sets (Rymarczyk et al. 2023). In this work, we propose a more general and realistic setting in which classes seen at a later stage may share concepts from past classes while introducing new concepts of their own. This creates a complex web of concept-class relationships across experiences, as illustrated in Figure 1, which needs to be preserved and expanded as the model learns to classify and explain new classes. Note that, as in standard incremental learning, the model is required to achieve good classification performance on newly introduced classes, while maintaining classification performance on previously seen ones.\nWhile traditional deep learning models are susceptible to forgetting old classes as new ones are introduced (catastrophic forgetting (Hadsell et al. 2020)), we find that concept-based models are susceptible to forgetting concept-class relationships as well. Overcoming two levels of forgetting in incrementally learning concept-based models presents new challenges that need to be addressed explicitly.\nTo this end, we propose MuCIL, a novel Multimodal Concept-Based Incremental Learner. We combine embeddings of text concepts, called concept anchors, with image representations to create multimodal concept embeddings for a given image. These embeddings are latent vectors containing information that helps in classification while also providing interpretations. We propose the use of concept grounding, which allows the interpretation of multimodal concepts in terms of text-based concepts. All these components are integrated with the fundamental consideration that the model should be able to incorporate new classes and new concepts continually, while also forming new concept-class associations that may emerge in the process.\nOur key contributions can be summarized as follows: (i) We propose a new method for the relatively new setting of concept-based incremental learning, where a model adapts dynamically to new classes as well as new concepts; (ii) We introduce multimodal concept embeddings, a combination of image embeddings and interpretable concept anchors, as part of our method to perform classification. Our approach is primarily intended to allow scalability of concept-based models to newer experiences without increase in parameters; (iii) We perform a comprehensive suite of experiments to evaluate our method on well-known benchmark datasets. We study our method's performance both in a in-cremental as well as standard supervised settings, achieving state of the art results; (iv) We propose three new metrics to evaluate concept-based models in the proposed setting: Concept-Class Relationship Forgetting, Concept Linear Accuracy and Active Concept Ratio. Our approach can offer concept-specific localizations implicitly as a means of interpreting the model prediction (see Figure 5), without being explicitly trained to do so."}, {"title": "2 Related Work", "content": "Interpretability of Deep Neural Network Models: Interpretability methods in DNN models can be broadly classified into post-hoc and ante-hoc methods. Post-hoc methods aim to interpret model predictions after training (Selvaraju et al. 2017; Chen et al. 2020; Chattopadhay et al. 2018; Sattarzadeh et al. 2021; Yvinec et al. 2022; Benitez et al. 2023; Sundararajan and Najmi 2020; Wang, Wang, and Inouye 2020; Jethani et al. 2021; Wang, Wang, and Inouye 2020; Dabkowski and Gal 2017; Fong and Vedaldi 2017; Petsiuk, Das, and Saenko 2018; Montavon et al. 2019). Recent efforts have highlighted the issues with post-hoc methods and their reliability in reflecting a model's reasoning (Rudin 2019; Vilone and Longo 2021; Nauta et al. 2023). On the other hand, ante-hoc methods that jointly learn to explain and predict provide models that are inherently interpretable (Sokol and Flach 2021; Benitez et al. 2023). Ante-hoc methods have also been found to provide interpretatations that help make the model more robust and reliable (Alvarez-Melis and Jaakkola 2018; Chattopadhyay et al. 2022). We focus on this genre of methods in this work. (Koh et al. 2020) proposed Concept Bottleneck Models (CBMs), a method that uses interpretable, human-defined concepts, combining them linearly to perform classification. CBMs also allow human interventions on concept activations (Shin et al. 2023; Steinmann et al. 2023) to steer the final prediction of a model. (Kim et al. 2023; Collins et al. 2023; Yan et al. 2023) obtained the intermediate semantic concepts by replacing domain experts with Large Language Models (LLMs). This allows for ease and flexibility in obtaining the concept set. Using LLMs to obtain concepts also allow grounding of neurons in a bottleneck layer to a human-understandable concept, an issue with CBMs that was highlighted in (Margeloiu et al. 2021). Other concept-based methods (Alvarez-Melis and Jaakkola 2018; Chen, Bei, and Rudin 2020; Kazhdan et al. 2020; Rigotti et al. 2021; Benitez et al. 2023) use a different notion of concepts based on prototype representations (see appendix \u00a7A1).\nConcept-Based Incremental Learning: While Incremental Learning in standard supervised settings has been widely explored (Wang et al. 2023), Concept-Based Incremental Learning has remained largely unstudied. We identify (Marconato et al. 2022) as an early effort in this direction; however, this work trains CBMs in a continual setting under an assumption that all concepts, including those required for unseen classes, are accessible from the first experience itself, which does not emulate a real-world setting. More recently, (Rymarczyk et al. 2023) proposed an interpretable CL method that uses part-based prototypes as concepts. As mentioned earlier, our notion of concepts allows us to go beyond parts of an object category, as in CBM-based models."}, {"title": "3 MuCIL: Methodology", "content": "Preliminaries and Notations. Given a sequence of $T$ experiences, with each experience $i$ consisting of $n$ image-label pairs $(X^i, Y^i) = \\{(x_1^i,y_1^i), (x_2^i, y_2^i), ..., (x_n^i, Y_n^i)\\}$, a class-incremental continual learning (CIL) system aims to learn an experience $t$ without catastrophically forgetting the previous $t - 1$ experiences. In the scenario where finer class details are available as concepts for classification, each experience $i$ consists of $n$ image-label-concept tuples $(X^i, y^i, C^i) = \\{(x_1^i,y_1^i, C_1^i), (x_2^i, y_2^i, C_2^i), ..., (x_n^i, Y_n^i, C_n^i)\\}$, where $C^i$ is the set of concepts known during experience $i$ and $C_k^i$ is the set of active concepts in example $k$. The set of concepts known during $i$ is the union of all concept sets from experiences 1 to $i$. We use the same active concept set for all instances of the same class, i.e., $C_{k1}^i = C_{k2}^i$ if $y_{k1}^i = Y_{k2}^i$."}, {"title": "Challenges.", "content": "While concept-based models for image classification have grown in the community, extending them to learn across experiences and new classes incrementally is non-trivial, introducing new challenges. (1) Forgetting at Two Levels: In a traditional CIL setting, the catastrophic forgetting of previously learned classes in newer experiences is only at a class-level. In our proposed setting, it includes both concept-level and concept-class relationship forgetting. Beyond forgetting of concepts from previous experiences, concept-class relationship forgetting can be more critical, affecting the model's ability to recognize previously encountered classes based on the concepts they were originally associated with. This affects the model's ability to understand and preserve how concepts relate to classes across experiences. (2) Parameter Scaling: Extending existing concept-based learning frameworks to a CIL setting would necessitate the addition of new learnable parameters with each new experience, leading to larger models, which is undesirable. Therefore, designing a method capable of integrating new classes and concepts without expanding the parameter space is ideal. (3) Information Bottleneck: Another challenge is the use of a single representation for encoding all relevant visual and concept information, which could lead to an information bottleneck as the system scales to accommodate more concepts and classes. (4) Semantic Misrepresentations: Lastly, some existing methods do not accurately capture the semantics of concepts (Margeloiu et al. 2021). One widely used method to explicitly encode concept semantics is leveraging textual concept embeddings. These semantics should be respected and updated as new concepts arrive. A naive way to implement this is by projecting a network's output representation on pre-computed concept embeddings (Yang et al. 2023; Oikarinen et al. 2023), which amplifies the unified representation problem. We propose a more flexible method for overcoming this issue.\nWe propose a novel framework that addresses each of the abovementioned challenges. Our overall solution consists of multiple components: (1) A Multimodal Encoder, that fuses visual information and textual concepts to create multimodal image-concept embeddings. It is designed to handle an increasing number of concepts without growing the number of parameters. It also creates multiple distinct representations in parallel to avoid information bottlenecks while preserving concept semantics. (2) A Parameter-free Clasi-fier that uses the multimodal concept embeddings along with the textual descriptions of classes to perform classification. (3) Concept Neurons that predict whether or not a given concept exists in an image and also allow probing the model for interpretability."}, {"title": "Multimodal Image-Concept Encoder.", "content": "We introduce a novel multimodal image-concept encoder that can be integrated into any standard transformer architecture. It is used to merge image embeddings and textual concept embeddings, thereby generating a sequence of multimodal embeddings that capture both visual and concept inputs.\nOur encoder incorporates new concepts without expanding its parameter space. This is made possible by the transformer's inherent ability to handle variable-length sequences, allowing the model to accommodate additional concepts by appending them to the input sequence without"}, {"title": "Concept Grounding Loss", "content": "necessitating an increase in parameters. The use of transformer encoder layers enables the processing of mixed data inputs, resulting in a sequence of multimodal image and concept embeddings by simultaneously attending to the two modalities. Using a transformer is hence not a drop-in replacement to a different network architecture such as a CNN, but is a central part of our overall methodology.\nFormally, the multimodal image-concept encoder, $M$, is a stack of transformer encoder layers that receive image embeddings $x_i$ as well as textual concept embeddings $c_1 c_2...c_{|c^i|}$ as input. Note that this includes all concepts available until the current experience. The output of $M$ is a sequence of vectors $\\{X'(i),C'(i)\\}$, where $X'(i)$ corresponding to the image patch embeddings $\\{x_i\\}$ and $C'(i) = \\{C_1, C_2, ..., C_{|c^i|}\\}$ are concept embeddings. These embeddings combine both textual and visual information and hence are multimodal by design. Similar to standard transformer-based classification where all outputs except the CLS token are discarded, the $X'(i)$ plays no further part in our methodology.\nThe concept embeddings $C'(i)$ do not have any semantic grounding however; we hence use a Concept Grounding Loss to map them to known natural language-based concept anchors, as below:\n$L_G = \\sum_{C^i}  \\frac{ \\sum_{k=1}^{|c_k|} c_k \\cdot (W^T c + b)}{|c_k||W^T c+b|}$    (1)\n$W$ and $b$ are learnable parameters that are shared among concepts. This loss semantically aligns the learned concept embeddings with their corresponding concept text anchors in the ground truth by ensuring that all original embeddings can be recovered using a single linear transform. Intuitively, this enforces each output concept vector to be associated with a deterministic representation of the concept, viz the text embedding. This deterministic association with a known (\"grounded\") reference guarantees that the vector has an associated semantic meaning, thus providing interpretability."}, {"title": "Parameter-free incremental classification.", "content": "As stated earlier, existing concept-based learning frameworks use dense layers on top of concept embeddings, which makes it challenging to scale to newer experiences continually. We hence take inspiration from methods such as (Radford et al. 2021) to use text embeddings of classes to perform classification, thus allowing our method to be parameter-free in the classifier and allowing for scalability to newer experiences.\nClassification is hence performed by returning the index of the class embedding which aligns most with the vectors in $C'$ for a given input sample. The alignment between the $j^{th}$ concept embedding of the current sample, $c'_j$, and the text embedding of the $k^{th}$ class $y_k$ is computed as $c'_j \\cdot y_k$, their dot product. The final classification result is hence given by\n$\\hat{y} = argmax(s_1, S_2, ..., S_{|y_i|}),$ where $S_k = \\sum_{j=1}^{C} c'_j \\cdot y_k,$\nwhich returns the index of the class that aligns most strongly with concept embeddings of a given input. We use $S_k$ as the logit of class $k$, and perform a softmax operation on the logits to get classification probabilities, which are used to train the model with the standard cross-entropy loss $L_{CE}$."}, {"title": "Concept Neurons.", "content": "The presence of a concept is evaluated using a single shared dense layer with a sigmoid activation, denoted by $\\sigma(\\cdot)$, applied independently on each $c \\in C'$. A weighted binary cross-entropy loss $L_{WBCE}$ is used to train the layer using provided ground-truth concept labels. We refer to the output logits of the layer $\\sigma(C')$ collectively as concept neurons. In MuCIL, concept neurons serve two purposes: (i) They provide additional supervision to learn concept embeddings better; and (ii) They provide an interface for identifying which concepts are active or inactive, thus enhancing interpretability. This allows for probing the model to evaluate the quality of concepts learned, and study how they change over experiences (see \u00a74). We show a detailed illustration explaining them in appendix (\u00a7A2).\nDetails of $L_{WBCE}$. For a given image, the ratio of the number of active concepts to the number of total concepts is quite small. This necessitates penalizing the misclassification of active concepts more strongly than the misclassification of inactive concepts. We do this by weighting the loss for active concepts by the fraction of inactive concepts, and weighting the loss for inactive concepts by the fraction of active concepts. The loss $L_{WBCE}$ is then defined as:\n$L_{WBCE} =  \\frac{\\text{# inactive concepts}}{\\text{# concepts}} \\sum_{i=1}^{C_{\\text{active}}} LBCE( \\sigma(c'_i), 1) +  \\frac{\\text{# active concepts}}{\\text{# concepts}} \\sum_{i=j}^{C_{\\text{inactive}}} LBCE( \\sigma(c'_i), 0)$\nTraining Procedure. MuCIL thus has the following learn-able components trained simultaneously end-to-end: parameters of $M$, parameters $W$ and $b$ in $L_G$, and the weights of the dense layer used for obtaining concept neuron values, $\\sigma(\\cdot)$. The global objective $L$ is a weighted sum of the three different objectives:\n$L = L_{CE} + \\lambda_1 L_{WBCE} + \\lambda_2 L_G$\t(2)\nIntuitively, the terms $L_{WBCE}$ and $L_G$ control the concept-based learning in the framework. $L_{WBCE}$ specifically trains $M$ and $\\sigma$ jointly to enable detection of the presence/absence of concepts, thus ensuring that visual features are properly represented in the multimodal concept vectors. $L_G$ trains $M$, $W$, and $b$ to enforce the concept embeddings to be grounded to their textual anchors. The $L_{CE}$ term trains $M$ to perform classification based on the outputs in $C'$.\nInference. At test-time, given an image $x_{test}$ to be classified after experience $t$, we obtain its embedding vectors $x_{test}$ and append them to the sequence of concept embeddings $C_1 C_2...C_{|c_t|}$. $M$ receives this combined sequence as input and outputs a sequence of vectors $\\{C_{I'}^{I'(\\text{test})}, C_r(\\text{test})\\}$. $C_r(\\text{test})$ is provided to the parameter-free classifier for obtaining the final class label. The active/inactive concepts in $x_{test}$ can be identified by thresholding the outputs of the concept neurons $\\sigma(CI(\\text{test}))."}, {"title": "4 Experiments and Results", "content": "We perform a comprehensive suite of experiments to study the performance of MuCIL on well-known benchmarks: CIFAR-100, ImageNet-100 (INet-100), and CalTech-UCSD Birds 200 (CUB200). To study how our method works when concept annotations are provided with the dataset or otherwise, we use the human-annotated concepts provided in case of CUB200, and derive concepts for CIFAR-100 and ImageNet-100 from GPT 3.5 as described in (Oikarinen et al. 2023). We study our method in standard supervised learning setting as well as in a Class-Incremental Learning setting as done in (Marconato et al. 2022; Rymarczyk et al. 2023), which we refer to as SL and CL settings respectively in our experiments. In the CL setting, we study our performance over 5 and 10 experiences using concept-based methods in conjunction with three well-known CL algorithms: Experience Replay (ER) (Rebuffi et al. 2017), A-GEM (Chaudhry et al. 2019), and DER++ (Buzzega et al. 2020), with a replay buffer size of 500 (we study other variations of buffer size in the Appendix). We select these CL algorithms since they can be adapted with concept-based model baselines. All implementation details (dataset details, architecture, hyperparameters) are in Appendix (\u00a7A2,\u00a7A4).\nBaselines. We compare our method with existing works that perform concept-based class-incremental learning as well as by adapting other works that use concept-based learning to the CL setting (due to lack of many explicit efforts on this setting). Our baseline methods for comparison include: (i) ICIAP (Marconato et al. 2022), which makes the assumption that all concepts, including those that would likely only be provided in future experiences, are provided upfront; (ii) Incremental CBM, a version of the Concept Bottleneck Model (Koh et al. 2020) that we modify to adapt to a class-incremental and concept-incremental learning scenario. We grow both the bottleneck layer and the linear classification layer as new classes and new concepts are introduced. We train these two baselines in sequential (-S) and joint (-J) settings as described in (Koh et al. 2020); (iii) We also compare with Label-Free CBM (Oikarinen et al. 2023) and LaBo (Yang et al. 2023), variations of CBM that use projections of image embeddings onto natural language concept embeddings to form the bottleneck layer. Due to the frozen feature extractors and use of Generalized Linear Models (Label-Free) or Submodular Functions (LaBo) over the concept layer, extending these baselines to different CL algorithms is non-trivial. The same is applicable to CBM-S and ICIAP-S which involve multiple training stages with the feature extractor being fixed after the first stage. These methods are hence most compatible with ER, which we use for the corresponding baselines. Hence, for all ablation studies and analysis, we focus on using ER as the CL algorithm due to compatibility across all considered baseline models and to ensure fairness of comparison.\nPerformance Metrics: Classification. In the CL setting, we use two well-known performance metrics: Final Average Accuracy (FAA) and Average Forgetting (AF). FAA is defined as: $FAA = \\frac{1}{T} \\sum_{i=1}^T \\text{acc}_i$, where $\\text{acc}_i$ represents the model's accuracy on the validation split of experience $i$ after training on $T$ experiences. AF at experience $T$ is defined as: $AF = \\frac{1}{T-1} \\sum_{i=2}^T \\text{acc}_{i}^{(i-1)} - \\text{acc}_{i}^{T}$, i.e., the difference in accuracy on the validation set of experience $i$ when it was originally learned and the accuracy on it after the model has been trained on $T$ experiences. We use the standard Classification Accuracy in the SL setting."}, {"title": "New Performance Metrics: Concept Evaluation.", "content": "In order to evaluate the learned concepts and their evolution across experiences, we propose three new quantitive metrics: concept linear accuracy, active concept ratio and concept-class relationship forgetting. We briefly describe each of these metrics, before presenting our results.\nConcept Linear Accuracy: We use our concept neurons to evaluate how well concepts capture the relevant semantic information (for performing classification), and to study how they preserve this information over experiences. The group of concept neurons are treated as a bottleneck layer, and a linear classifier is trained on top of the neuron logits. We denote the linear accuracy of concept neurons over a class set $Y_i$ and a concept set $C_i$ after training the model on $t$ experiences as $LA(t, Y_i, C_i)$, where $t$ is varied across CL experiences.\nConcept-Class Relationship Forgetting: We define Concept-Class Relationship Forgetting (CCRF) of a concept set as the loss of its ability to provide relevant information to perform class-level discrimination over time. This can occur when concepts no longer align well to visual semantics in the provided image. This is different from forgetting in standard CL settings (Hadsell et al. 2020) as the model may predict concepts correctly, but instead forgets how concepts correlate to classes. Mathematically, we measure CCRF as:\n$CCRF =  \\frac{1}{T-1} \\sum_{t=2}^{T} \\frac{1}{t-1} \\sum_{k=t-1}^{t-1} [LA(k, Y_{k\\backslash k-1},C_k) - LA(t, Y_{k\\backslash k-1},C_k)]$ (3)\nThe term $LA(k, Y_{k\\backslash k-1},C_k) - LA(t, Y_{k\\backslash k-1},C_k)$ can also take negative values, indicating that the information captured by the concepts of experience $k$ is enhanced after training on the future experience $t$ instead of degrading. In our framework, the model generating concept logits for experience $i$ is the combination of $M$ and $\\sigma$ trained on experience $i$; in a CBM, this is the model upto the bottleneck layer.\nActive Concept Ratio: To study the relevance of concepts to classes across experiences, we propose Active Concept Ratio (ACR) to measure how frequently concepts seen during experience $i$ activate when classifying images from experience $j$. A high value indicates that concepts from experience $i$ play an important role in understanding classes from experience $j$. Ideally, classes introduced in experience $i$ should have their highest ACR values associated with the concept set from the same experience, since those concepts best explain the classes. Positive ACR values with concept sets from other experiences indicate that those concepts are also activated in response to classes from experience $i$. Formally, let $N_j$ be the number of images to be classified in experience $j$. Then, the ACR for a concept set presented in experience $i$ for classifying images from experience $j$ is defined as"}, {"title": "CCRF(t)", "content": "$$\\frac{1}{N_j} \\sum_{n=1}^{N_j} \\frac{ (c_n^{(i)} \\cdot  c_n^{(i\\vert j)})}{||c_n^{(i)}|| \\cdot ||c_n^{(i \\vert j)}||}$$. Here, $c_n^{(i\\vert j)}$ represents the model's (binary) predictions of unique concepts introduced in experience $i$, while $c_n^{(i)}$ represents the model's (binary) predictions of all concepts present in experience $i$.\nResults: CL Performance. Table 1 shows our results on concept-based continual learning. Our approach outperforms all baselines, with significant margins on CIFAR-100 and ImageNet-100. This is done without adding any additional parameters to our model with newer experiences, whereas other methods require new parameters to incorporate new classes and concepts. We also observe significantly lower forgetting across experiences. These results show that our model readily incorporates knowledge about new concepts and classes while internally forming required concept-class associations, and also remembers these associations fairly well, when trained on new experiences. We also find that CL algorithms which explicitly replay labels (ER and DER++) perform better across all methods.\nResults: SL Performance. To see how MuCIL fares in standard classification settings, we evaluate it in a full-data setting on the same three datasets. We find that MuCIL considerably outperforms the next closest baseline on the CUB dataset, indicating that it is highly effective when used to differentiate between fine-grained classes. It also achieves comparable performance on ImageNet-100 and CIFAR-100, even though this setting is not our focus.\nResults: CCRF. We show the CCRF metric values for MuCIL versus a jointly trained CBM in Table 3. We take the concept neurons (the bottleneck layer in the case of CBM-J) and train a linear layer on the different class set-concept set pairs required for the computation of the metric, as in"}, {"title": "Results: ACR.", "content": "We present a visualization of ACRs across 10 experiences for a CBM, a Label-Free CBM and MuCIL model in Fig. 4. We see that the CBM is unable to effectively incorporate concepts that appear in later experiences and relies heavily on early concept sets. Label-Free CBM activates a given concept similarly across experiences, leading to poor explainability in terms of concepts required for a specific experience. MuCIL has a strong diagonal ACR matrix, showing that it strongly activates concepts that appear with (and therefore explain) a set of classes. It also appropriately activates earlier concepts, particularly fundamental concepts from experience one that are shared across future experiences. We see some bias toward the concept set introduced in the final experience, a common problem in most CL settings (Buzzega et al. 2021; Mai et al. 2021). Addressing this can an interesting direction of future work.\nQualitative Results: Visual Grounding and Attributions."}, {"title": "Training Procedure.", "content": "MuCIL thus has the following learn-able components trained simultaneously end-to-end: parameters of $M$, parameters $W$ and $b$ in $L_G$, and the weights of the dense layer used for obtaining concept neuron values, $\\sigma(\\cdot)$. The global objective $L$ is a weighted sum of the three different objectives:\n$L = L_{CE} + \\lambda_1 L_{WBCE} + \\lambda_2 L_G$\t(2)\nIntuitively, the terms $L_{WBCE}$ and $L_G$ control the concept-based learning in the framework. $L_{WBCE}$ specifically trains $M$ and $\\sigma$ jointly to enable detection of the presence/absence of concepts, thus ensuring that visual features are properly represented in the multimodal concept vectors. $L_G$ trains $M$, $W$, and $b$ to enforce the concept embeddings to be grounded to their textual anchors. The $L_{CE}$ term trains $M$ to perform classification based on the outputs in $C'$.\nInference. At test-time, given an image $x_{test}$ to be classified after experience $t$, we obtain its embedding vectors $x_{test}$ and append them to the sequence of concept embeddings $C_1 C_2...C_{|c_t|}$. $M$ receives this combined sequence as input and outputs a sequence of vectors $\\{C_{I'}^{I'(\\text{test})}, C_r(\\text{test})\\}$. $C_r(\\text{test})$ is provided to the parameter-free classifier for obtaining the final class label. The active/inactive concepts in $x_{test}$ can be identified by thresholding the outputs of the concept neurons $\\sigma(CI(\\text{test}))."}, {"title": "5 Conclusions and Future Work", "content": "In this work, we study a new paradigm of continual learning for interpretable models where both new classes and new concepts are introduced across experiences. We show that existing works suffer from degradation in concept-class relationships in such settings. We propose a method that adapts the transformer architecture in vision-language encoders to include concept embeddings, which are anchored to natural language concepts. Through a set of carefully designed loss terms, our approach can not only classify reliably in a CL setting, but can also specify the human-understandable concepts used for the classification. We evaluate our method on three benchmark datasets and introduce new metrics to study the efficacy of concepts in our framework. Our qualitative and quantitative results show the significant promise of the proposed method. Beyond being a method for concept-based CL, we believe that our efforts can further open up the direction of inherently interpretable CL models in the community. Analysis of CL methods that do not directly replay past labels and the refinement of our architecture for better CL would be interesting directions of future work."}, {"title": "A.1 Concepts: What and Why?", "content": "In this work, we refer to intermediate text semantics that describe a class label as \u201cconcepts\u201d. We state this to explicitly differentiate from other uses of the term. In recent literature in the field, concepts have been interpreted as: (1) (Soft) binary labels that may not directly be grounded on human-understandable semantics and attempt to reconstruct a class label, as in (Koh et al. 2020); (2) Visual features that commonly appear in most instances of a given class, as in (Rymarczyk et al. 2023); and (3) intermediate text semantics that describe a class label, as in (Oikarinen et al. 2023; Yang et al. 2023). We follow the third characterization in this work. The first connotation has been observed to not capture intended semantics (Margeloiu et al. 2021), while the second conntation of prototypes are effective in certain settings, like instance-specific explanations. Our approach follows recent work (Oikarinen et al. 2023; Yang et al. 2023) in viewing concepts as a compelling means to learn via explanations, rather than learn via prediction. Below are a few advantages of our approach:\n\u2022 Generalizable Abstraction for Intermediate Semantics: Text attributes provide a means to capture intermediate semantics that represents what a model is 'thinking' or 'considering important'. Unlike prototypes, which are based on specific instances or examples from training data, such text attributes can be generalized across instances. This abstraction facilitates a more human-relatable understanding of the underlying relationships that a model has learned.\n\u2022 Human-Interpretable: Concepts, as used herein, provide a means to connect latent embeddings inside a transformer to human-interpretable text, providing a pathway to better understand a model's functioning. Prototypes, while illustrative, may not provide this degree of interpretability, especially when the prototypes are derived from complex or non-intuitive examples. Text attributes can serve to succinctly communicate what features or aspects in the data influence the model's decisions.\n\u2022 Flexibility and Adaptability: Text attributes offer flexibility in adapting to different models and contexts. They can be easily modified, combined, or expanded upon to suit the"}, {"title": "Multimodal Image-Concept Encoder", "content": "specific needs of an experience or to improve interpretability. This adaptability is particularly beneficial in complex domains where the model's functioning needs to be thoroughly understood and communicated. With Large Language Models", "Instances": "Relying on prototypes can sometimes lead to overfitting to specific instances in the training data, which may not generalize well to new, unseen data"}]}