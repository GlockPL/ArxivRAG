{"title": "LSTM Recurrent Neural Networks for Cybersecurity Named Entity Recognition", "authors": ["Houssem Gasmi", "Abdelaziz Bouras", "Jannik Laval"], "abstract": "The automated and timely conversion of cybersecurity information from unstructured online sources, such as blogs and articles to more formal representations has become a necessity for many applications in the domain nowadays. Named Entity Recognition (NER) is one of the early phases towards this goal. It involves the detection of the relevant domain entities, such as product, version, attack name, etc. in technical documents. Although generally considered a simple task in the information extraction field, it is quite challenging in some domains like cybersecurity because of the complex structure of its entities. The state of the art methods require time-consuming and labor intensive feature engineering that describes the properties of the entities, their context, domain knowledge, and linguistic characteristics. The model demonstrated in this paper is domain independent and does not rely on any features specific to the entities in the cybersecurity domain, hence does not require expert knowledge to perform feature engineering. The method used relies on a type of recurrent neural networks called Long Short-Term Memory (LSTM) and the Conditional Random Fields (CRFs) method. The results we obtained showed that this method outperforms the state of the art methods given an annotated corpus of a decent size.", "sections": [{"title": "I. INTRODUCTION", "content": "Timely extraction of cybersecurity information from diverse online web sources, such as news, vendor bulletins, blogs, forums, and online databases is vital for many types of applications. One important application is the conversion of unstructured cybersecurity information to a more structured form like ontologies. Knowledge modeling of cyber-attacks for instance simplifies the work of auditors and analysts [1].\nAt the heart of the information extraction tasks is the recognition of named entities of the domain, such as vendors, products, versions, or programming languages. The current NER tools that give the best performance in the field are based on feature engineering. These tools rely on the specific characterizing features of the entities in the field, for example, a decimal number that follows a product is very likely to be the version of that product and not quantities of it. A sequence of words starting with capital letters is likely to be a product name rather than a company name and so on.\nFeature engineering has many issues and limitations. Firstly, it relies heavily on the experience of the person and the lengthy trial and error process that accompanies that. Secondly, feature engineering relies on look-ups or dictionaries to identify known entities [2]. These dictionaries are hard to build and harder to maintain especially with highly dynamic fields, such as cybersecurity. These activities constitute the majority of the time needed to construct these NER tools. The results could be satisfactory despite requiring considerable maintenance efforts to keep them up to date as more products are released and written about online. However, these tools are domain specific and do not achieve good accuracy when applied to other domains. For instance, a tool that is designed to recognize entities in the biochemistry field will perform very poorly in the domain of cybersecurity [3].\nCRFs emerged in recent years as the most successful and de facto standard method for entity extraction. In this paper, we show that a domain agnostic method that is based on the recent advances in the deep learning field and word embeddings outperforms traditional methods, such as the CRFs. The first advancement, which is the word2vec word embedding method was introduced by Mikolov et al. [4]. It represents each word in the corpora by a low dimensional vector. Besides the gain in space, one of the main advantages of this representation compared to the traditional one-hot vectors [5] is the ability of these vectors to reflect the semantic relationship between the words. For instance, the difference between the vectors representing the words 'king' and 'queen' is similar to the difference between the vectors representing the words 'man' and 'woman'. These relationships result in the clustering of semantically similar words in the vector space. For instance, the words 'IBM' and 'Microsoft' will be in the same cluster, while words of products like 'Ubuntu' and 'Web Sphere Server' appear together in a different cluster.\nThe second advancement is the recent breakthroughs in the deep learning field. It became feasible and practical because of the increase in the hardware processing power"}, {"title": "II. RELATED WORK", "content": "Approaches to NER are mainly either rule-based or machine learning/statistical-based [11], although quite often the two techniques are mixed [12]. Rule-based methods typically are a combination of Gazette-based lookups and pattern matching rules that are hand-coded by a domain expert. These rules use the contextual information of the entity to determine whether candidate entities from the Gazette are valid or not. Statistical based NER approaches use a variety of models, such as Maximum Entropy Models [13], Hidden Markov Models (HMMs) [14], Support Vector Machines (SVMs) [15], Perceptrons [16], Conditional Random Fields (CRFs) [17], or neural networks [18]. The most successful NER approaches include those based on CRFS. CRFs address the NER problem using a sequence-labeling model. In this model, the label of an entity is modeled as dependent on the labels of the preceding and following entities in a specified window. Examples of frameworks that are available for CRF-based NER are Stanford NER and CRFSuite.\nMore recently, deep neural networks have been considered as a potential alternative to the traditional statistical methods as they address many of their shortcomings [19]. One of the main obstacles that prevent the adoption of the methods mentioned above is feature engineering. Neural networks essentially allow the features to be learned automatically. In practice, this can significantly decrease the amount of human effort required in various applications. More importantly, empirical results across a broad set of domains have shown that the learned features in neural networks can give very significant improvements in accuracy over hand-engineered features. RNNs, a class of neural networks have been studied and proved that they can process input with variable lengths as they have a long time memory. This property resulted in notable successes with several NLP tasks like speech recognition and machine translation [20]. LSTM further improved the performance of RNNs and allowed the learning between arbitrary long-distance dependencies [21].\nVarious methods have been applied to extract entities and their relations in the cybersecurity domain. Jones et al. [22] implemented a bootstrapping algorithm that requires little input data to extract security entities and the relationship between them from the text. An SVM classifier has been used by Mulwad et al. [23] to separate cybersecurity vulnerability descriptions from non-relevant ones. The classifier uses Wikitology and a computer security taxonomy to identify and classify domain entities. The two previously mentioned works relied on standard NER tools to recognize the domain concepts. While these NER tools obtained satisfactory results in general texts, such as news, they performed poorly when applied to more technical domains, such as cybersecurity because these tools are not trained on domain-specific concept identification. For instance, the Stanford NER tool is trained using a training corpus consisting mainly of news documents that are largely annotated with general entity types, such as names of people, locations, organizations, etc.\nTo overcome the limitations of NER tools in technical domains and identify mentions of domain-specific entities, Goldberg [5] adopted an approach that trains the CRF classifier of the Stanford NER framework on a hand-labeled training data. He achieved acceptable results that are much better than the two previous efforts. Although they produced good results, the effort involved in painstakingly annotating even a small corpus prohibits the practical implementation of this approach. To address this problem, Joshi et al. [3] developed a method to automate the labeling of training data when there is no domain-specific training data available. The labeling process leverages several data sources by combining several related domain-specific structured data to infer entities in the text. Next, a Maximum Entropy Markov Model has been trained on a corpus of nearly 750,000 words and achieved precisions above 90%. This type of training relies on external sources for corpus annotation. These resources need to be regularly maintained and updated to maintain the quality and precision of the text labeling.\nGiven the benefits of neural networks, this paper aims to apply the LSTM method on the problem of NER in the cybersecurity domain using the corpora made available by"}, {"title": "III. LSTM-CRF MODEL", "content": "In this Section, we will provide an overview of the LSTM-CRF architecture as presented by Lample et al. [8].\nA. LSTM-CRF Model\nRNNs are neural networks that have the capability to detect and learn patterns in data sequences. These sequences could be natural language text, spoken words, genomes, stock market time series, etc. Recurrent networks combine the current input (e.g., current word) with the previous perceived input (earlier words in the text). However, RNNs are not good at handling long-term dependencies. When the previous input becomes large, RNNs suffer from the vanishing or exploding gradient problems. They can also be challenging to training and very unlikely to converge when the number of parameters becomes large.\nLSTMs were first introduced by Hochreiter et al. [7] They are an improvement on RNNs and can learn arbitrary long-term dependencies, hence can be used for a variety of applications such as natural language processing and stock market analysis. LSTMs have a similar chain structure as RNNs, but the structure of the repeating nodes is different. LSTMs have multiple layers that communicate with each other in a particular way. A typical LSTM consists of an input gate, an output gate, a memory cell, and a forget gate. Briefly, these gates control which input to pass to the memory cell to remember it in the future and which earlier state to forget. The implementation used is as follows [8]:\n$i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i)$\n$c_t = (1 \u2212 i_t) \\odot c_{t-1} + i_t tanh(W_{xc}x_t + W_{hc}h_{t-1}+b_c)$\n$o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_{t} + b_o)$\n$h_t = o_t tanh(c_t)$\nThe sigma sign \\( \\sigma \\) is the elementwise sigmoid function and \\(\\odot\\) is the elementwise product.\nAssuming we have a sequence of n words X = (x1, x2,...,xn) and each word is represented by a vector of dimension d. LSTM computes the left context $lh_t$ which represents all the words that precede the word t. A right context $rh_t$ is also computed using another LSTM that reads the same text sequence in reverse order by starting from the end and go backward. This technique proved very useful and the resulting architecture, which consists of a forward LSTM and a backward LSTM, is called a Bidirectional LSTM. The resulting representation of a word is obtained by concatenating the left and right contexts to get the representation $h_t = [lh_t;rh_t]$. This representation is useful for various tagging applications, such as the NER problem at hand in this paper.\nFigure 1 shows the architecture of the Bidirectional LSTM-CRF model. It consists of three layers."}, {"title": "IV. EVALUATION", "content": "In this section, we will introduce the benchmark tool, the preprocessing performed on the gold standard corpora, and the metrics we used for evaluation.\nA. Competitor System\nWe compare the performance of the LSTM-CRF architecture against a CRF tool that uses a generic feature set for NER with word embeddings. These features were designed for domain-independent NER and defined by the tool writer. Using word embeddings in both systems will help us compare only the CRF method with the suggested LSTM-CRF architecture and negate the effect of word embeddings. We used the CRFSuite to train a CRF model using the default settings of the tool.\nB. Gold standard corpora\nWe performed our evaluation on around 40 entity types defined in three corpora and also analyzed the performance of the model on a subset of the seven most significant entities of the domain. Each word in these corpora is auto-annotated with an entity type. The corpus is an auto-labeled cyber security domain text that was generated for use in the Stucco project. It includes all descriptions from CVE/NVD entries starting in 2010, in addition to entries from MS Bulletins and Metasploit. As stated in [1]: \"While labelling these descriptions may be useful in itself, the intended purpose of this corpus is to serve as training data for a supervised learning algorithm that accurately labels other text documents in this domain, such as blogs, news articles, and tweets.\".\nC. Text Preprocessing\nIn its original form as provided by Bridges et al [1], all the corpora were stored in a single JSON file with each corpus represented by a high-level JSON element. To facilitate further processing, we converted the file to the CONLL2000 format as the input for the LSTM-CRF model. In the newly single annotated corpus, we removed the separation between each of the three corpora and annotated every word in a separate line. Each line contains the word mentioned in the text and its entity type as show in the following example:\nApple B-vendor\nQuickTime B-application\nbefore B-version\n7.7 I-version\nallows B-relevant term\nremote B-relevant term\nattackers I-relevant term\nto O\nAs for the CRF model, the CRFSuite requires the training data to be in the CoNLL2003 format that includes the Part of Speech (POS) and chunking information with the NER tag appearing first as shown below:\nB-vendor Apple NNP O\nB-application QuickTime NNP O\nB-version before IN O\nI-version 7.7 CD O\nB-relevant term allows NNS O\nB-relevant term remote VBP O\nI-relevant term attackers NNS O\nO to TO O\nAs the original corpus did not contain the POS and chunking information, the training corpus had to be reprocessed. We started by converting it to its original form (i.e., a set of paragraphs). Then, we used the python NLTK library to extract the necessary information for each word in the corpus. Finally, we converted the text back to the expected format shown above.\nD. Evaluation Metrics\nWe divided the annotated corpus into 3 disjoint subsets. 70% was allocated for the training of the model, 10% for the holdout cross-validation set (or development), and 20% for the evaluation of the model. We compared the two models (LSTM-CRF and CRF) in terms of accuracy, precision, recall, and F1-score for the full set of tags and for a subset of the most relevant tags of the domain. In our experiments, the hyperparameters of the LSTM-CRF model were set to the default values used by Lample et al. [8]."}, {"title": "V. RESULTS AND DISCUSSION", "content": "We evaluated the performance of the NER method that is based on the LSTM-CRF architecture against a traditional state of the art CRF tool that uses standard NER features. The evaluation was performed on three different sets covering over 40 entity types from the cybersecurity domain. For evaluation purposes, we will analyze the average performance of models across all the entity types, then we will consider the most popular entities that appear frequently in the cybersecurity vulnerability descriptions and evaluate the performance on these entities only. The entities considered are vendor, application, version, file, operating system (os), hardware, and edition. The reason for this is that we are usually not interested in extracting all entity types but only a subset of them that are most relevant to the application at hand.\nA. Performance of LSTM-CRF and CRF\nStarting with the global item accuracy of both models, Figure 2 shows the accuracy values measured on the test set at each iteration of the training stage for 100 iterations. LSTM-CRF achieved an accuracy of 95.8% after the first iteration and increased gradually to reach values between 98.2% and 98.3% starting from iteration 23 until the end of the training. On the other hand, the CRF method started slowly at accuracies of 65% and increased rapidly to reach accuracies of 96% where it leveled off to reach eventually 96.35% at the end of the training."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "As this paper showed, the results demonstrate that LSTM-CRF improved the accuracy of NER extraction over the state-of-art traditional pure statistical CRF method. What is impressive about the LSTM-CRF method is that it does not require any feature engineering and is entirely entity type agnostic. Even the format of the training corpus is much simpler, thus requiring less text pre-processing. This alleviates the need to develop domain-specific tools and dictionaries for NER. In the future, our research will concentrate on applying the LSTM-CRF method on entity Relations Extraction (RE). RE is concerned with attempting to find occurrences of relations among domain entities in text. This would provide a better understanding of product vulnerability descriptions. For example, RE could extract information from a vulnerability description that would help us distinguish between the product or tool that is the mean of an attack and the product being attacked. With information extraction becoming more accurate, more automated, and easier to achieve using recent neural networks advancements, there is a pressing need to turn this advancement into applications in the domain of cybersecurity. One such application is the conversion of the textual descriptions of cybersecurity vulnerabilities that are available in the web into a more formal representation like ontologies. This gives cybersecurity professionals the necessary tools that grant them rapid access to the information needed for decision-making."}]}