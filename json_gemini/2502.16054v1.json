{"title": "Human-Al Collaboration in Cloud Security: Cognitive Hierarchy-Driven Deep Reinforcement Learning", "authors": ["ZAHRA AREF", "SHENG WEI", "NARAYAN B. MANDAYAM"], "abstract": "Given the complexity of multi-tenant cloud environments and the growing need for real-time threat mitigation, Security Operations Centers (SOCs) must integrate AI-driven adaptive defense mechanisms to counter Advanced Persistent Threats (APTs) effectively. However, SOC analysts face challenges in countering adaptive adversarial tactics, requiring intelligent decision-support frameworks. To enhance human-AI collaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep Q-Network (CHT-DQN) framework that models the interactive decision-making process between SOC analysts and AI-driven APT bots. Our approach assumes that the SOC analyst (defender) operates at cognitive level-1, anticipating attacker strategies, while the APT bot (attacker) follows a level-0 policy, making naive exploitative decisions. By incorporating CHT into DQN, the framework enhances adaptive SOC defense strategies using Attack Graph (AG)-based reinforcement learning. Through extensive simulation experiments across varying attack graph complexities, the proposed model consistently achieves higher average weighted data protection and reduced action discrepancies, particularly when SOC analysts utilize CHT-DQN. A theoretical lower bound analysis further validates the superiority of CHT-DQN over standard DQN, demonstrating that as AG complexity increases, CHT-DQN yields higher Q-value performance. A comprehensive human-in-the-loop (HITL) evaluation involving IT security experts was conducted on Amazon Mechanical Turk (MTurk). The findings indicate that SOC analysts who employ CHT-DQN-driven transition probabilities demonstrate greater alignment with adaptive DQN attackers, thereby achieving enhanced data protection. Moreover, human decision patterns in the study align with Prospect Theory, demonstrating risk aversion after failure and risk-seeking behavior after success. The findings highlight the significant potential of integrating cognitive modeling into deep reinforcement learning to improve Security Operations Center functions. This combination presents a promising avenue for developing real-time, adaptive mechanisms for cloud security.", "sections": [{"title": "1 Introduction", "content": "Security Operations Centers (SOCs) are the backbone of modern cybersecurity, safeguarding cloud infrastructures against Advanced Persistent Threats (APTs) by integrating threat detection, incident response, and continuous monitoring across multi-cloud and multi-tenant environments. They employ Security Information and Event Management (SIEM), machine learning, and automated threat-hunting tools to detect cyber threats, analyze attack patterns, and identify system vulnerabilities [1]. However, SOCs face increasing operational challenges, including overwhelming alert volumes, compliance complexities, vendor dependencies, and the rising sophistication of AI-driven adversarial threats [2]. These challenges are exacerbated in cloud environments, where the attack surface expands dynamically due to virtualized resources, shared infrastructures, and evolving cyber threat landscapes[3, 4].\nDespite advancements in AI-driven security tools, SOC analysts continue to struggle with high false positive rates, cognitive fatigue, and a shortage of skilled cybersecurity professionals, limiting their ability to respond efficiently to complex attacks [5]. Prior research has demonstrated that subjectivity in human decision-making can significantly impact the effectiveness of DRL-based security defenses, particularly in cloud storage environments [6]. To mitigate these challenges, AI-augmented SOC frameworks must integrate human cognitive insights with machine-learning-driven adaptive defenses, enabling real-time decision-making and proactive adversarial reasoning [7, 8]. This human-AI collaboration is essential to enhancing SOC operations, as it combines human intuition and strategic thinking with Al's ability to analyze vast amounts of threat intelligence data in real-time [9, 10].\nThis work incorporates Cognitive Hierarchy Theory (CHT) into Deep Reinforcement Learning (DRL) to model adversarial strategies in cloud security defense. Unlike traditional game-theoretic equilibrium-based approaches, which assume perfect rationality, CHT models adversaries at different cognitive levels, better reflecting the bounded rationality seen in real-world cyber conflicts [11, 12]. In our framework, the SOC analyst (defender) at cognitive level-1 anticipates APT attacker strategies at level-0, where the attacker employs exploitative decision-making but lacks deeper strategic foresight. By infusing CHT into DQN, we enable SOC analysts to predict attacker behaviors and dynamically adjust defense mechanisms in real time, enhancing threat anticipation and mitigation.\nTo support structured adversarial modeling, we integrate AGs, a widely used method in cyber threat intelligence, to map attack progression, visualize vulnerabilities, and optimize SOC decision-making [13\u201315]. While AGs are traditionally static, their integration with CHT-driven DRL allows for dynamic threat modeling, equipping SOC analysts with real-time attack prediction capabilities and adaptive countermeasure optimization [16].\nAdditionally, Human-in-the-Loop (HITL) models have proven effective in SOCs, as they align AI-driven threat detection with human decision-making heuristics, improving response efficiency and security posture [17, 18]. HITL enhances SOC analyst trust in Al systems, reduces false positives, and facilitates adaptive learning based on human feedback [10, 19]. By embedding HITL into our CHT-augmented DRL framework, we create a human-AI collaboration paradigm, ensuring that SOC analysts and AI agents work together effectively in cloud security operations.\nOur research contributions are summarized as follows:\n\u2022 We introduce a human-AI collaborative framework that leverages Attack Graphs (AGs) to provide SOC analysts with a visual and analytical representation of cloud infrastructure vulnerabilities. This framework incorporates a human-in-the-loop (HITL) approach to facilitate real-time, adaptive defense strategies against evolving cyber threats."}, {"title": "2 Related Work", "content": "Cybersecurity research increasingly employs advanced computational models to address the growing complexity of cyber threats, particularly APTs in cloud environments. Modern SOCs integrate machine learning (ML), automated threat-hunting, and Security Information and Event Management (SIEM) tools to detect and mitigate threats in real-time [3, 4]. However, challenges such as alert fatigue, vendor dependencies, compliance issues, and the increasing sophistication of AI-driven adversaries necessitate more intelligent, proactive security frameworks.\nRecent studies highlight the importance of human-AI collaboration in SOC operations, where AI-powered decision-support systems complement human analysts in managing cyber threats efficiently [1]. This has led to the exploration of Deep Reinforcement Learning (DRL) for adaptive defense, AGs for attack pathway visualization, CHT for adversarial reasoning, and HITL strategies for SOC decision-making. This section reviews key research in these areas, positioning our CHT-driven DRL framework within the existing cybersecurity landscape."}, {"title": "2.1 Deep Reinforcement Learning for SOCs and APT Defense", "content": "Deep Reinforcement Learning (DRL) has emerged as a powerful approach for cyber threat detection and mitigation in SOCs. DRL enables adaptive defense strategies, where an Al agent continuously learns from adversarial interactions to improve its decision-making. The survey by Nguyen et al. [20] explores DRL's applicability to cyber-physical systems, intrusion detection, and multi-agent security frameworks, discussing various DRL approaches such as value-based methods (DQN), policy gradients, and actor-critic frameworks. Other studies apply DRL to real-time cloud security, demonstrating its potential for resource allocation and adaptive defense against APTs [21]. Beyond cybersecurity, DRL has also been explored for optimizing design verification workflows, where RL enables efficient test generation and automated verification of complex hardware and software systems [22]. These applications highlight DRL's versatility in optimizing decision-making under uncertainty, reinforcing its suitability for adaptive cyber defense. More specific to cloud-based SOCs, the work in [23] introduces a DRL-based CPU allocation model for cloud storage defense, using a Colonel Blotto game framework to counter APT attacks. While effective in storage optimization, it does not consider broader threat adaptation across SOC environments.\nAdditionally, Chhetri et al. [5] propose a risk-aware AI model for cloud cost optimization, incorporating dynamic threat assessment mechanisms to manage cloud security risks effectively. This aligns with our work, which enhances SOC defenses by integrating CHT with DRL to model adversarial reasoning and proactive threat mitigation in cloud infrastructures."}, {"title": "2.2 Attack Graphs in Cybersecurity and Cloud Defense", "content": "AGs can play a crucial role in SOC threat modeling, helping analysts visualize, analyze, and predict the movement of adversaries within cloud infrastructures. AGs provide a structured representation of cyber threats, allowing security teams to prioritize vulnerabilities and allocate resources strategically [13\u201315]. Several studies explore AG-based threat intelligence. For example, Sheyner et al. [24] automate AG generation for network vulnerability analysis, while Wu et al. [25] use AGs to evaluate power communication networks, prioritizing risks based on security metrics. Additionally, Sengupta et al. [26] propose an AG-driven Markov game framework for APT detection in cloud networks, leveraging Common Vulnerability Scoring System (CVSS) metrics for dynamic threat modeling. In multi-cloud landscapes, Schmidt et al. [2] investigate distributed SOC architectures, emphasizing AG-based threat modeling and adaptive defense planning. However, existing AG-based models focus primarily on reactive threat detection rather than proactive, AI-driven defense mechanisms. Our framework extends AG applications by integrating them with CHT and DRL, enabling real-time SOC decision-making against evolving APTs."}, {"title": "2.3 Cognitive Models in Cyber Defense", "content": "CHT and behavioral game theory have gained traction in cybersecurity research, providing insights into human-like decision-making in adversarial settings [11, 12]. Unlike traditional Nash equilibrium-based models, which assume perfect rationality, CHT recognizes that attackers and defenders operate at different levels of cognitive reasoning, leading to non-equilibrium decision patterns.\nIn cloud security, Aref et al. [6] apply Prospect Theory to model defender decision-making, demonstrating that subjective risk perception can enhance defense strategies in resource-constrained cloud environments. Similarly, Xiao et al.[27] formulate an APT detection game, showing that attackers exhibit risk-seeking behaviors when facing aggressive defense mechanisms. CHT is also applied in cyber-physical security, where Mavridis et al. [28] use level-k reasoning to anticipate APT behaviors, while Sanjab et al. [29] analyze UAV cybersecurity through cumulative Prospect Theory, capturing adversarial risk dynamics.\nOur research builds upon these models by integrating CHT with DRL in a cloud SOC framework, allowing SOC analysts to predict APT strategies and dynamically adjust defenses based on multi-level adversarial cognition."}, {"title": "2.4 Human-Al Collaboration and Human-in-the-Loop SOCs", "content": "Human-Al collaboration (HITL) is essential for enhancing SOC efficiency, as AI-driven automation alone cannot fully replace human expertise in cybersecurity operations. AI models must align with human cognitive processes to improve SOC decision-making, reduce false positives, and enhance analyst interpretability [1, 8]. Several studies highlight HITL in SOCs. Chhetri et al. [7] propose an AI-driven alert prioritization system to reduce cognitive overload in SOC analysts, improving threat response efficiency. Furthermore, Tariq et al. [8] introduce a modular AI-human decision framework that optimizes SOC workflow automation while ensuring human oversight in high-risk decision-making.\nExplainable AI (XAI) is another key area in human-AI collaboration. A recent study explores how explainable reinforcement learning models improve SOC analyst trust and decision accuracy in real-time security operations [16]. Collaborative Al learning frameworks, such as the survey in [30], further, emphasize the role of human-AI teaming in security environments. By integrating HITL with our CHT-DQN model, we enable SOC analysts to interact with Al-driven security tools in real time, leveraging human cognitive intuition alongside machine-learning automation for adaptive cloud defense strategies."}, {"title": "3 Preliminaries", "content": "This study integrates human computation principles into cloud security to defend against APTs using Cognitive Hierarchy Theory (CHT) in Deep Reinforcement Learning (DRL)."}, {"title": "3.1 Deep Q-Learning", "content": "Deep Q-Learning approximates the Q-function, which estimates the expected cumulative reward of taking an action in a given state under policy \u03c0. The environment is modeled as a Markov Decision Process (MDP) defined by M = (S, A, T, \u03b3, R), where S is the state space, A is the action space, T represents transition probabilities, \u03b3 is the discount factor, and R is the reward function. The goal is to learn an optimal policy \u03c0* that maximizes the expected discounted return:\n\n$Q_\\pi(s, a; \\theta) = E_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^kR(s_k, a_k, s_{k+1}) | s_0 = s, a_0 = a\\right]$\n\nDeep Q-Networks (DQNs) approximate $Q^*(s, a; \\theta)$ using a neural network with parameters \u03b8, trained iteratively to minimize the Bellman error:\n\n$L(\\theta) = E\\left[\\left(y - Q(s, a; \\theta)\\right)^2\\right]$,\n\nwhere the target value y is defined as $y = R(s, a, s') + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$, and $\u03b8^\u2212$ represents a delayed copy of the Q-network parameters for stable updates."}, {"title": "3.2 Cognitive Hierarchy Theory and K-Level Reasoning", "content": "Cognitive Hierarchy Theory (CHT) models strategic reasoning by assuming that decision-makers (agents) operate at different levels of thinking, where a level-k agent optimizes its decisions based on the expected behavior of agents at levels 0 to k - 1.\nDefinition 1 (Level-0 Policy). Level-0 agents select actions randomly or based on simple heuristics.\nDefinition 2 (Level-k Policy). A level-k agent assumes that opponents reason up to level k \u2212 1 and selects the best response accordingly."}, {"title": "4 CHT-driven DQN Cloud Data Defense", "content": "In the domain of Security Operations Centers (SOCs) for cloud security, the convergence of human-AI collaboration, cognitive modeling, and machine learning presents a promising avenue for real-time threat mitigation and adaptive defense strategies. This paper introduces a novel defensive algorithm that infuses Cognitive Hierarchy Theory (CHT) into Deep Q-Learning (DQN), enabling SOC analysts to make real-time, risk-aware decisions against evolving cyber threats.\nThe proposed CHT-DQN algorithm operates within a stochastic game framework (S, AD, AA, T, rD, rA), characterized by a state space S, action spaces AD and AA for the SOC analyst (defender) and APT attacker, and a state-transition function: T. This function, T : S \u00d7 AD \u00d7 AA \u2192 A(S), determines the transition probabilities to a new state s', given"}, {"title": "4.1 Attack Graph", "content": "The Attack Graph (AG) is central to SOC-driven decision-making, offering a structured representation of network vulnerabilities and potential attack trajectories [13\u201315, 31, 32]. The SOC analyst can minimize security risks and get equipped for potential cyber-attacks in advance by evaluating the AG. Within an AI-augmented SOC, attack graphs serve as real-time intelligence tools, assisting SOC analysts in:\n\u2022 Anticipating potential threats based on historical attack data.\n\u2022 Prioritizing incident response by estimating the likelihood of successful attacks.\n\u2022 Improving real-time security decision-making through automated risk quantification and adaptive threat model-ing.\nBy leveraging the attack graph, SOC analysts can preemptively adjust security controls, dynamically deploy counter-measures, and minimize attack impact in real-time.\nAn attack graph [33], G, is formally defined as the tuple:\n\nG = (V, &, P),\n\nwhere:\n\u2022 V = {v1, v2, ..., vN} is the set of nodes, representing vulnerabilities or threat attributes in cloud environments.\n\u2022 & \u2286 V \u00d7 V is the set of directed edges, termed exploits, which define possible attack paths.\n\u2022 P = {P(v) | v \u2208 V} is the set of exploitation probabilities, where each P(v) \u2208 [0, 1] quantifies the likelihood of successful exploitation of vulnerability v.\nIn our model, the attack graph G is dynamically updated at fixed intervals using a Maximum Likelihood Estimation (MLE) approach:\n\n$P(v) = \\frac{\\varphi(v)}{\\sum_{v' \\in V} \\varphi(v')}$,\n\nwhere \u03c6(v) denotes the observed exploitation frequency of node v over time.\nThis ensures that exploitation probabilities P(v) reflect the most recent attack-defense interactions. By continuously updating P(v), our framework dynamically captures adversarial behaviors, allowing SOC analysts to make data-informed"}, {"title": "4.2 Model Dynamics and Utility Functions", "content": "We model the SOC environment as a stochastic game where each cloud system node i in the attack graph (AG) has a binary security state representing its compromise status. Let st = [s1, s2, ..., sN] be the security state vector at time t, where si ~ Bern(pi) indicates whether node i is compromised (s = 1) or secure (s = 0).\nThe AG representation of the cloud system is illustrated in Figure 1b. The hexagonal node represents the attacker, while oval nodes represent cloud system vulnerabilities. The attacker node (7) serves as the parent of vulnerabilities (4) and (5), indicating possible exploit paths.\nTo enable real-time SOC decision-making, our AG quantifies exploitation likelihoods, allowing adaptive SOC response strategies. Given security mechanisms\u2014including firewalls, access control policies, and cryptographic techniques\u2014SOC analysts dynamically allocate defense resources to optimize security while minimizing operational costs.\nState-Action Representation: At each time step t, the APT attacker selects a node aAt from the attack graph G to exploit, where: aAt \u2208 G. Each vulnerability in the cloud infrastructure corresponds to an AG node, representing:\n\u2022 Weak authentication & access control\n\u2022 Security misconfiguration & software vulnerabilities\n\u2022 Network-based threats (e.g., botnets, spoofing, VM co-location)\n\u2022 Storage-based attacks & application exploits [34, 35].\nConversely, the SOC analyst, leveraging CHT-driven DQN, selects a node aDt to protect: aDt \u2208 G. Defensive strategies include:\n\u2022 Intrusion detection & cloud-based antivirus [36]\n\u2022 Cryptographic authentication & VM integrity monitoring [34]\n\u2022 Storage attack mitigation via security monitoring [37]\n\u2022 Binary code analysis for exploit prevention [38].\nState Transition Model: The next cloud security state is determined by:\n\ns^{t+1} = \\left{\\left[1 - \\delta_i^t\\right]_{1 \\leq i \\leq N}\\right},\n\nwhere \u03b4 represents whether node i was compromised at time step t:\n\n$\\delta^t(a_{A, i}, a_{D, i}) = \\begin{cases}\n1, & \\text{if } a_{Ai}^t = 1 \\text{ and } a_{Di}^t = 0 \\\\\n0, & \\text{otherwise}\n\\end{cases}$\n\nThe attack graph probabilities P(s'|s, aA, aD) are updated dynamically based on the success rate of prior attacks. This ensures SOC analysts continuously refine their defense strategies based on observed historical attack patterns.\nUtility Functions (Reward Structures): To model the cost-benefit trade-offs in SOC operations, we define utility functions for both the APT attacker and SOC analyst.\nThe attacker's utility function at time t considers the estimated data exfiltration and the cost of attack:\n\n$u_A^t = \\sum_{i=1}^N \\left[\\alpha_{Ai}b_i - \\beta_{Ai}c_{Ai}\\right] \\cdot \\delta_i^t - \\left[1 - \\delta_i^t\\right] \\cdot \\left[a_{Di}b_i + \\beta_{Di}c_{Di}\\right]$"}, {"title": "4.3 CHT-DQN Algorithm", "content": "This stochastic game models the Security Operations Center (SOC) defense against cloud-based cyber threats, where the SOC analyst and attacker interact dynamically in a sequential decision-making environment. Each agent maximizes its utility under the assumption that the other agent is doing the same. We follow the approach proposed in [12], which extends Cognitive Hierarchy Theory (CHT) from single-action games [11] to sequential decision processes. Level-K agents respond optimally to level-(K-1) agents, establishing a structured reasoning hierarchy. This enables SOC analysts to anticipate adversarial strategies in real-time security operations."}, {"title": "5 Human-Interactive Web-Based DRL Game Scenarios", "content": "Reward-Aware SOC Analyst | DQN Attacker. Participants act as SOC analysts, making real-time defensive decisions in a cloud security environment against a DQN-based attacker. They receive only reward feedback after selecting a defensive action, based on their chosen node and the attacker's selected target. This setup simulates SOC analysts refining their defensive strategies through experience (Figure 3a).\nReward + Transition-Aware SOC Analyst | DQN Attacker. This setting provides both reward feedback and estimated attack transition probabilities, computed from Equation 15. This additional information enables SOC analysts to predict the attacker's behavior, improving cyber defense strategies through AI-assisted SOC decision-making (Figure 3b).\nParticipants and Task Procedure on Amazon MTurk. This study received Institutional Review Board (IRB) approval, ensuring ethical compliance and minimal risk. All participants provided informed consent, and data collection adhered to cybersecurity research ethics.\nA total of 83 participants were recruited, with 80 completing all 40 rounds (40 per game variant). Three incomplete sessions were excluded from the analysis.\nQualification Requirements. Participants met the following eligibility criteria:\n\u2022 HIT Approval Rate above 80%.\n\u2022 Country of residence: Australia, India, United Kingdom, or United States.\n\u2022 Employment in Software & IT Services to ensure relevant cybersecurity knowledge.\nExperimental Phases. The experiment was conducted in two phases:\n\u2022 Phase 1: Initial pilot with 10 participants per game for preliminary evaluation.\n\u2022 Phase 2: Full-scale study with 30 participants per game.\nThe \"Reward + Transition-Aware\" variant ran from 9:18 am to 10:13 am PDT, while the \"Reward-Aware\" variant ran from 9:18 am to 10:05 am PDT.\nParticipants were given instructions, played 40 rounds, and received a final score and confirmation code for compen-sation.\nData Collection and Metrics. We recorded:\n\u2022 Participant ID: Anonymized unique identifier.\n\u2022 State and Action Histories: SOC analyst and attacker decisions per round.\n\u2022 Reward Histories: Cumulative SOC analyst and attacker rewards.\n\u2022 Attacker Policy and Exploitation Probabilities: Estimations of level-0 attack behavior.\n\u2022 Data Protection and Exploitation Costs: Defensive resource allocations and attacker exploitation metrics.\n\u2022 Performance Metrics: Weighted data protection ratios, response times, and final scores.\nThis dataset enabled an in-depth analysis of real-time decision-making in both game settings, demonstrating the effectiveness of CHT-DQN for SOC-driven cloud security."}, {"title": "6 Experiments", "content": "We evaluate the efficacy of Human-AI collaboration in SOCs using our proposed Cognitive Hierarchy Theory-driven Deep Q-Learning (CHT-DQN). Our study consists of two experimental settings:"}, {"title": "6.1 Experimental Setup", "content": "We implemented the SOC simulation and reinforcement learning framework in Python 3 using PyTorch. The framework integrates CHT-based decision-making to improve AI-augmented SOC defense strategies.\nNeural Network Architecture. The SOC analyst and attacker models use three hidden layers with 64, 128, and 256 neurons, respectively, and ReLU activations. The output layer maps cloud security states to optimal defense actions.\nTraining Parameters. The models were trained using the following hyperparameters:\n\u2022 Batch size: 64\n\u2022 Experience buffer size: 1 \u00d7 106\n\u2022 Discount factor: \u03b3 = 0.98 (prioritizing long-term security posturing)\n\u2022 Cognitive hierarchy depth: \u03b2 = 1\n\u2022 Learning rate: 5 \u00d7 10-2\nThe data size on attack graph nodes G ranged from 1 to 10, with a noise factor \u03f5 drawn from [0, 1]. The defense and attack costs at node i were defined as:\n\n$b_i \\pm \\epsilon, b_i \\pm \\epsilon$.\n\nWe set \u03b1D,i = 10, \u03b1A,i = 10, \u03b2D,i = 1, and \u03b2A,i = 1, ensuring adaptive attack-defense interactions."}, {"title": "6.2 Deep Reinforcement Learning Scenario-Based Experiments", "content": "We simulated interactions in a cloud-based SOC environment, where the AI-powered SOC analyst uses an attack graph (AG) to strategize against adversarial threats. The AG dynamically evolves, with nodes representing vulnerabilities and edges modeling exploit likelihoods.\nExperimental Conditions. We varied the number of attack graph nodes, N \u2208 [2, 10], to analyze how defense strategies scale with complexity. Exploitation likelihoods P(v) in G were updated every 100 steps.\nTo balance exploration and exploitation, we employed an \u03f5-greedy policy, initialized at \u03f5 = 1 and decaying to \u03f5 = 0.05 at a decay rate of 2. Each SOC simulation ran for 2000 time steps, averaged over 10 random seeds.\nEvaluated Defense Strategies. We compared four SOC defense strategies:\n\u2022 Case 1: CHT-DQN SOC analyst vs. Random attacker\n\u2022 Case 2: CHT-DQN SOC analyst vs. DQN attacker\n\u2022 Case 3: Standard DQN SOC analyst vs. Random attacker\n\u2022 Case 4: Standard DQN SOC analyst vs. DQN attacker\nThese settings model real-world SOC operations, enabling comparisons between adaptive SOC analysts (CHT-DQN) and traditional reinforcement learning (DQN) under varying adversarial conditions.\nFindings. Figure 4a shows the running average reward of the SOC analyst over time, while Figure 4b presents the running average reward of the attacker. These figures provide insights into strategy evolution and convergence during training. The running average is computed over the last 100 timesteps, with the interval from timestep 0 to 1000"}, {"title": "6.3 Human-Interactive Web-Based DRL Experiments on Amazon MTurk", "content": "To validate our human-AI collaboration framework in SOCs, we conducted a large-scale human-in-the-loop experiment on Amazon Mechanical Turk (MTurk). The two interactive web-based games were implemented on an AWS t3.medium EC2 instance, which provided the necessary scalability, low latency, and reliable performance for real-time human-AI interactions. Participants accessed the games via the links http://3.132.160.243:5001/ and http://3.132.160.243:5002/ on MTurk. In this human-AI SOC simulation, participants played the role of SOC analysts in a dynamic cloud defense scenario designed to counter Al-driven attackers. The goal was to examine how human SOC analysts behave when confronted with DQN attackers, using two different scenarios modeled by DQN and CHT-DQN algorithms.\nParticipants were tasked with defending cloud nodes against APTs over 40 rounds per session. The security environ-ment was modeled using a dynamically constructed 6-node AG, representing cloud infrastructure vulnerabilities. The attacker's strategy was AI-driven, adapting its tactics using a DQN model, simulating a realistic adaptive adversary in SOC operations.\nEvaluated Scenarios. We evaluated four SOC analyst conditions:\n\u2022 Scenario 1: Reward + Transition-Aware Human SOC Analyst vs. DQN attacker\n\u2022 Scenario 2: Reward-Aware Human SOC Analyst vs. DQN attacker\n\u2022 Scenario 3: Fully automated CHT-DQN SOC analyst vs. DQN attacker\n\u2022 Scenario 4: Fully automated DQN SOC analyst vs. DQN attacker\nIn Scenarios 1 and 2, human SOC analysts engaged in real-time cybersecurity decision-making, facing automated DQN attackers across 40 rounds. These scenarios evaluated how human-AI collaborative decision-making compares to fully automated SOC analysts. For Scenarios 3 and 4, results were averaged over 40 random seeds to ensure statistical robustness in AI-driven SOC performance evaluations.\nIn all four scenarios, the automated SOC analyst or attacker ran their algorithm with \u03f5-greedy policy with \u03f5 starting at 0.9 to 0.05, indicating starting more exploration and ending in more exploitation. The web interface allowed us to track SOC analyst actions, failure events, and subsequent responses. The study yielded several key insights, illustrated through four figures derived from both human SOC analysts and AI-driven simulation data.\nFindings. Figure 6 presents the average weighted data protection ratio over four exploration-exploitation stages, comparing four distinct scenarios. The attack graph employed consists of 6 nodes, with the \u03f5-greedy parameter of the automated agents, shown as the red dashed line, indicating the transition from exploration to exploitation over time, especially in the later stages where exploitation becomes predominant.\nIn the Reward-Aware Human SOC analyst scenario, participants selected nodes to defend each round based on observing potential rewards without access to transition probabilities. In contrast, the Reward + Transition-Aware Human SOC analyst scenario provided participants with both potential rewards and transition probabilities derived from the CHT-DQN model. This additional information allowed SOC analysts at cognitive level-1 to anticipate attacker strategies at level-0 more effectively, particularly in later stages where exploitation is emphasized.\nThe results reveal a marked performance advantage in \"D: Reward + Transition-Aware Human Subject | A: DQN\" (Sce-nario 1) over \"D: CHT-DQN | A: DQN\" (Scenario 3) in the exploitation stage. This outcome suggests that access to CHT-DQN-driven transition probabilities\u2014incorporating the attacker's level-0 policy and attack graph likelihoods\u2014enhances the SOC analyst's ability to safeguard cloud data. The \"D: Reward-Aware Human Subject | A: DQN\" (Scenario 2), similar to the fully simulated \"D: DQN | A: DQN\" case (Scenario 4), shows a relatively lower average weighted data protection,"}, {"title": "7 Conclusions", "content": "This study introduces a Cognitive Hierarchy Theory-driven Deep Reinforcement Learning framework for human-AI collaboration in SOCs, demonstrating its effectiveness in mitigating AI-driven APT threats through adaptive decision-making. The integration of Attack Graphs with CHT-DQN enables SOC analysts to model adversarial behavior at multiple levels of reasoning, significantly outperforming standard DQN across both automated and human-in-the-loop defense scenarios. Key findings from human-Al interaction experiments on Amazon MTurk show that SOC analysts at"}, {"title": "A Appendix", "content": "Theorem 4.1. As the number of nodes N \u2192 \u221e in the attack graph G, the Q-value function for the SOC analyst under the CHT-DQN policy is lower bounded by the Q-value function for the SOC analyst under DQN, assuming the attack strategy is stationary and known.\nPROOF. Let G be an attack graph with N nodes. The proof follows the steps below:\n1. Boundedness of Utilities and Discounted Rewards: We establish that the utility function and the expected cumulative reward remain bounded for both CHT-DQN and standard DQN, ensuring comparability as N \u2192 \u221e.\n2. Inductive Argument: Assume at step t, the expected utility of the SOC analyst under CHT-DQN is greater than or equal to that under standard DQN:\n\n$E[R_D(s)|\\pi^{CHT-DQN}] \\geq E[R_D(s)|DQN]$.\n\nBy induction, this holds for all future steps.\n3. Value Function Comparison: Since the expected cumulative reward is positively correlated with the value function, it follows that:\n\n$V_D^{CHT-DQN}(s) \\geq V_D^{DQN}(s)$.\n\n4. Q-Value Function Relation: Given the Bellman equation and the relationship between the value function and the Q-function, we conclude:\n\n$Q_D^{CHT-DQN}(s, a_D) \\geq Q_D^{DQN}(s, a_D)$.\n\nThus, the CHT-DQN policy provides a stronger lower bound on the SOC analyst's Q-values compared to standard DQN, completing the proof.\nA.1 Boundedness of Utilities and Cumulative Discounted Reward\nWe first establish that the utility functions and cumulative rewards are bounded. The data sizes bi, bi, and costs cA,i, CD,i are finite and bounded for all i. The weighting coefficients \u03b1A,i, \u03b2A,i, \u03b1D,i, \u03b2D,i are also finite. Additionally, the security status indicator \u03b4 is binary (0 or 1). Therefore, the attacker's and SOC analyst's utilities, u\u2032A (or R\u2032D) and uD (or RD), remain bounded for any t and any number of nodes N.\nThe cumulative discounted reward for a policy \u03c0D starting from state s is:\n\n$V^{\\pi_D}(s) = E\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_D(s^t, a_D^t, s^{t+1}) | s^0 = s \\right]$,\n\nwhere \u03b3 is the discount factor (0 \u2264 \u03b3 < 1). Since RD consists of bounded utilities u\u2032A and uD, the series $ \\sum_{t=0}^{\\infty} \\gamma^t R_D(s^t, a_D^t, s^{t+1})$ is a discounted sum of bounded terms. A discounted sum converges when \u03b3 < 1, ensuring that V\u03c0D (s) remains bounded for any policy \u03c0D and initial state s. Consequently, as N \u2192 \u221e, the expected cumulative reward remains finite, allowing for a direct comparison of CHT-DQN and DQN performance.\nA.2 Proof by Induction: CHT-DQN Lower Bound on DQN\nWe proceed by induction to show that the expected utility of the SOC analyst under the CHT-DQN policy is lower bounded by that under the DQN policy.\nThe expected reward (or utility) of the SOC analyst under CHT-DQN and DQN policies are defined as:"}, {"title": "A.2.1 Base Case: N = 1", "content": "For an attack graph G with a single node N = 1, the policies of CHT-DQN and standard DQN are equivalent, meaning the expected utilities satisfy:\n\n$E[R_D^{CHT-DQN, 1}] = E[R_D^{DQN, 1}]$."}, {"title": "A.2.2 Inductive Hypothesis", "content": "Assume that for N = k, the expected reward for the SOC analyst under CHT-DQN is greater than that under DQN:\n\n$E[R_D^{CHT-DQN, k}] > E[R_D^{DQN, k}]$.\n\nExpanding this expectation, we obtain:\n\n$\\sum_{a_D, s'} \\sum_{a_A=1}^k P(s', a_A|s, a_D)R_D(s, a_D, a_A, s') > \\sum_{a_D, s'} P(s' | s, a_D)R_D(s, a_D, s')$."}, {"title": "A.2.3 Inductive Step: N = k + 1", "content": "For N = k + 1, we aim to show that:\n\n$E[R_D^{CHT-DQN, k+1}] > E[R_D^{DQN, k+1}]$.\n\nExpanding the expectation:\n\n$E[R_D^{CHT-DQN, k+1}] = \\sum_{a_D, s'} \\sum_{a_A=1}^{k+1} P(s', a_A|s, a_D)R_D(s, a_D, a_A, s')$."}, {"title": "A.3 Correlation Between Expected Cumulative Reward and Value Function", "content": ""}]}