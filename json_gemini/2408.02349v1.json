{"title": "Active Sensing of Knee Osteoarthritis Progression with Reinforcement Learning", "authors": ["Khanh Nguyen", "Huy Hoang Nguyen", "Egor Panfilov", "Aleksei Tiulpin"], "abstract": "Osteoarthritis (OA) is the most common musculoskeletal disease, which has no cure. Knee OA (KOA) is one of the highest\ncauses of disability worldwide, and it costs billions of United States dollars to the global community. Prediction of KOA\nprogression has been of high interest to the community for years, as it can advance treatment development through more\nefficient clinical trials and improve patient outcomes through more efficient healthcare utilization. Existing approaches for\npredicting KOA, however, are predominantly static, i.e. consider data from a single time point to predict progression many\nyears into the future, and knee level, i.e. consider progression in a single joint only. Due to these and related reasons, these\nmethods fail to deliver the level of predictive performance, which is sufficient to result in cost savings and better patient\noutcomes. Collecting extensive data from all patients on a regular basis could address the issue, but it is limited by the high\ncost at a population level. In this work, we propose to go beyond static prediction models in OA, and bring a novel Active\nSensing (AS) approach, designed to dynamically follow up patients with the objective of maximizing the number of informative\ndata acquisitions, while minimizing their total cost over a period of time. Our approach is based on Reinforcement Learning\n(RL), and it leverages a novel reward function designed specifically for AS of disease progression in more than one part of a\nhuman body. Our method is end-to-end, relies on multi-modal Deep Learning, and requires no human input at inference\ntime. Throughout an exhaustive experimental evaluation, we show that using RL can provide a higher monetary benefit when\ncompared to state-of-the-art baselines. Finally, our work introduces a novel AS benchmark for RL in medicine, which we make\npublicly available at https://github.com/Oulu-IMEDS/OACostSensitivityRL.", "sections": [{"title": "1. Introduction", "content": "Osteoarthritis (OA) is a degenerative joint disorder that affects primarily the knee cartilage, and results in its gradual\nloss and subsequent damage to the joint (Primorac et al.,\n2020). Knee OA (KOA), the most frequent type of OA, is stud-ied in this work. Common symptoms of KOA include pain,\nstiffness, and reduced joint flexibility, all of which signifi-cantly impact an individual's quality of life. As the condition\nprogresses, the symptoms often worsen, decreasing mobility\nand potentially leading to disability (McAlindon et al., 1993).\nTo date, there are no effective treatments that can pre-vent long-term disability and permanent joint damage in\nKOA (Hafez et al., 2014). At the latest stages of KOA, a costly\nand highly invasive intervention, such as total knee replace-ment (TKR), is delivered to improve the patient's health and\nwell-being. A TKR surgery may cost up to $50,000 for each\npatient (Price et al., 2018; Evans, 2018; Phillips et al., 2019),\nand TKR reportedly yields unsatisfactory results in 15-20%\nof cases (Marsh et al., 2022; Bourne et al., 2010; Price et al.,\n2018). Due to rapid population aging, TKR gradually be-comes a financial burden not only for KOA patients (Kjellberg\nand Kehlet, 2016), but also for the healthcare system (Gandhi\net al., 2023). Therefore, KOA in general, requires the next-generation disease-modifying drugs (DMOADs) (Oo et al.,\n2018; Rodriguez-Merchan, 2023). Such therapies should aim\nto slow down the KOA progression, thus, delaying the need\nfor TKR (Latourte et al., 2020; Cho et al., 2021).\nWhile the research community has been working on KOA\nDMOADs for a number of years (Pfrizer, 2010; Hunter, 2011),\nno drugs have yet been found effective and approved by the\nregulatory bodies (Rodriguez-Merchan, 2023). One of the\nchallenges in developing DMOADs for KOA is the long pa-tient follow-up time in clinical trials (Latourte et al., 2020).\nKOA is often a slowly progressing disease with an unknown"}, {"title": "2. Methodology", "content": "etiology (Driban et al., 2020). Thus, many subjects recruited\nto KOA trials and cohorts do not develop the disease at all\nand some develop only early signs of the disease by the end\nof a study. Hence, adaptive methods for data acquisition in\nmodern clinical trials (Spreafico et al., 2021; Cuzick, 2023) are\nbecoming instrumental to providing better, and more cost-efficient patient monitoring compared to routine scheduling\nand randomized participant selection (Wu and Suen, 2022;\nYala et al., 2022).\nDespite recent advancements in the field of KOA progres-sion prediction (Halilaj et al., 2018; McCabe et al., 2022;\nTiulpin et al., 2019a; Nguyen et al., 2022, 2023), translation\nof existing models into real-world applications is challenging\nfor several reasons. Firstly, the financial downstream impact\nof the disease progression models is hard to assess during the\nmodel development. As such, the current models do not ac-count for follow-up costs and potential future expenses asso-ciated with structural KOA progression when they estimate if\na person will develop KOA in the future. Secondly, the exist-ing models (Tiulpin et al., 2019a, 2022; Nguyen et al., 2023;\nHirvasniemi et al., 2023; Hu et al., 2022; Panfilov et al., 2022,\n2023) largely focus only on the knee-level progression events\nand overlook KOA's broader patient-level context, where pro-gression may happen concurrently in both knees. Thirdly, OA\nseverity may increase multiple times during the study time,\nand there are uncertain future factors that may alter the dis-ease course (e.g. future injuries). Therefore, long-horizon\nprediction is hard, and data used for such prediction needs\nto be collected dynamically.\nActive Sensing (AS) (Yu et al., 2009) addresses the question\nof when to follow up patient and with what tools or modalities\nan examination has to be conducted. In the case of KOA, one\nwould want for an AS policy to minimize the follow-up costs\nwhile maximizing the chances of capturing patient-level pro-gression. Solving such a problem, however, is challenging\nfor a na\u00efve supervised learning (SL) approach, as one needs\nto consider making predictions to collect data, which would\nthen be used to make new predictions and so forth. This\nis a hardly differentiable problem of decision-making under\nuncertainty, which can be solved using e.g. Reinforcement\nLearning (RL).\nIn this work, we develop an AS methodology for KOA using\nRL. We utilize RL to make sequential decisions based on the\ninteractions between a decision-making agent and an envi-"}, {"title": "2.1. Problem definition", "content": "ronment simulating a clinical trial or another setting where\npatients need to be observed on an interval basis for a pro-longed period of time. Our study makes the following contri-butions:\n\u2022 We introduce a formal framework for personalized AS\ninto the KOA domain to enhance the efficiency of clin-ical trials.\n\u2022 We propose to dynamically model and predict KOA pro-gression at the patient level.\n\u2022 We employ an RL-based model to derive a personalized\npatient follow-up schedule that is being refined dynam-ically, taking into account costs and data utility from a\nclinical perspective.\n\u2022 We propose a new reward function, designed specifically\nfor AS of structural KOA degeneration.\n\u2022 We set up a novel AS baseline for the field of OA, par-ticularly, KOA. We openly release the code and a newly\ndeveloped RL environment to foster the development of\nnew methods and approaches.\nThe overview of our method is depicted in Figure 1.\nWe consider a set of participants in a longitudinal obser-vational setting and assume that all of them have been ex-amined at the baseline, corresponding to t = 0. If a sub-ject visits the hospital at time t, we observe a state a set of\nmulti-modal data consisting of clinical variables and imaging\ndata, denoted by st, which is needed for KOA diagnosis at t.\nThis examination incurs a certain acquisition cost (visit cost),\nwhich we denote by \u03bb.\nOur objective is to make optimal, w.r.t. both costs and out-comes, sequential decisions to refer each subject for an ex-amination to capture the disease progression given acquired\nmulti-modal data. We illustrated the decision-making pro-cess in Figure 2. Specifically, for each subject, at time point\nt\u2208 {0,..., T-1}, we predict whether that subject should be ex-amined at the next time point t + 1. The referral for an exam-ination is considered successful if the disease has progressed\nbetween t and t + 1. We formalize this as the following opti-mization problem:\n\n{a_1^*,..., a_{T-1}^*\\} = \\arg\\min\n\\{a_1,...,a_{T-1}\\} \\sum_{t=1}^{T-1} \\mathcal{J}(\\mathbf{s}_t, a_t, \\{\\delta(j)\\}_{j=1}^{J};\\kappa, \\lambda),\n(1)\nwhere at \u2208 {0,1} indicates whether to issue a referral for a\nvisit at time-point t + 1. We assume that the value of the\nobjective \\mathcal{J} depends on the actions at, states st, and the set\nof patient-level changes associated with the disease {\u03b4(j)}_j=1^(J)\n(i.e. KOA in the context of this study). We also introduce a\nhyper-parameter threshold \u03ba that defines whether the dis-ease progression is significant (e.g. may correspond to min-\nimal clinically important difference). The combination of\nchanges {\u03b4(j)}_j=1^(J) and \u03ba defines the disease trajectory. The\ncost of data acquisition \u03bb is treated as a hyper-parameter. We\nalso use \u03bb as a basic cost unit multiplier when defining the\ncosts of undetected disease progression.\nThe optimization of the defined objective in (1) is the prob-lem of sequential decision-making under uncertainty, which\nwe further define through the prism of the Markov Decision\nProcesses (MDP) and RL."}, {"title": "2.2. Reinforcement Learning", "content": "Markov Decision Process. An MDP is defined as a tuple (S,\nA, P, R, \u03b3), where S is a set of states, A is a set of actions.\nP : S \u00d7 S \u00d7 A \u2192 [0, 1] indicates transition probabilities between\nstates. Finally, R : S \u00d7 A \u2192 R represents a set of rewards asso-\nciated with state-action pairs and \u03b3\u2208 [0, 1] is the discounting\nfactor (Sutton and Barto, 2018). The decision-making policy\nof an agent is defined by \u03c0 : A \u00d7 S \u2192 [0, 1].\nIn the MDP framework, the sequential decision-making\nprocess unfolds as follows (Sutton and Barto, 2018). At a time\nstep t\u2208 N, the environment (a \"world\" in which the agent ex-ists and makes actions) yields a state st \u2208 S that is observed\nby the agent. The agent then responds with an action at \u2208 A\nfollowing the policy \u03c0(at | st) that assigns a probability to the\nevent associated with selecting action at at a state st. Subse-\nquently, the environment provides a reward rt = R(st, at) to\nthe agent for taking the action at. The process then repeats\nuntil the horizon is reached (i.e. t = T). We use the term ex-periences to refer to a collection of states st, next states St+1,\nactions at, and rewards rt.\nQ-learning. To measure the expected return of taking action\nat at state st according to policy \u03c0, we introduce a state-action value function:\nQ_\\pi (\\mathbf{s}\\_t, a\\_t) = r\\_t + \\gamma Q\\_\\pi(\\mathbf{s}\\_{t+1}, a\\_t).\n(2)\nWhen solving an MDP, we aim to obtain the optimal policy\n\u03c0*. In the case of Q-learning (Watkins and Dayan, 1992),\n\u03c0(s) = Dirac(argmaxa Q(s, a) = a^*) \u2200s \u2208 S, where Dirac(.) is\nthe Dirac's delta function and a^* is the optimal action. For\n\u03c0^*, one can derive the Bellman equation as\nQ^*(\\mathbf{s}\\_t, a\\_t) = r\\_t + \\gamma \\max\\_{a\\_{t+1}}Q^*(\\mathbf{s}\\_{t+1}, a\\_{t+1}).\n(3)\nQ-learning updates the Q-value for a certain action using\nthe immediate reward and the maximum possible Q-value\nfor the next state. Let us denote the post-update Q-value as\nQ(st, at), the learning update is then:\nQ^* (\\mathbf{s}\\_t, a\\_t) \\leftarrow (1 - \\alpha) Q(\\mathbf{s}\\_t, a\\_t) + \\alpha (r\\_t + \\max Q^* (\\mathbf{s}\\_{t+1}, a)).\n(4)\nwhere \u03b1 is the learning rate, determining how much newly\nobtained information replaces the old information. Through\ninteractions with the environment, the policy under Q-learning is supposed to converge after a sufficient number of\niterations (Watkins and Dayan, 1992). The presented update\nrule is sufficient when the number of states and actions is rel-atively small. In the context of this study or other real-world\ncases, the set of states may have an infinite size, making it\nimpossible to store the Q-table. To tackle this issue, one can\nresort to a functional approximation, such as with a neural\nnetwork (NN) (Tesauro et al., 1995; Mnih et al., 2013, 2015).\nQ-function approximation. In contemporary RL, the Q-function is often approximated by NN, which we denote as\nQ(st, at, \u03b8), where \u03b8 represents NN's parameters. The param-eterized Q-function is then commonly called a Q-Network.\nMnih et al. (2013) introduced two networks that allow to fol-low the Temporal Difference Learning idea (Tesauro et al.,\n1995): the local Q-Network, approximating Q(st,.), and the\ntarget Q-Network, approximating Q(st+1,.). The local Q-Network is optimized by utilizing the mean square error loss\nto measure the difference between the estimated Q-value and\nthe true one, noted as:\n\\mathcal{L}(\\theta) = [r\\_t + \\gamma\\max\\_a Q(\\mathbf{s}\\_{t+1}, a, \\theta^*) - Q(\\mathbf{s}\\_t, a\\_t, \\theta)]^2,\n(5)"}, {"title": "2.3. Proposed method", "content": "The parameters \u03b8^* of the target Q-Network are updated pe-riodically using the local Q-Network to stabilize the learning\nprocess and improve convergence.\nTraining Q-networks. To train a Q-network from various ex-periences and collect information along various roll-outs of\nan MDP, one has to balance exploration and exploitation dur-ing training. Here, we used an \u03b5-greedy policy (Sutton and\nBarto, 2018) that induces the following action selection pro-tocol. Let us draw \u03b7r ~ Uniform(0, 1), then\n\\pi(a\\_t|\\mathbf{s}\\_t) = \\begin{cases}\n\\text{argmax}\\_a Q(\\mathbf{s}\\_t, a) & \\text{if } \\eta\\_r < \\epsilon\\\\\n\\text{Random action} & \\text{otherwise}\n\\end{cases},\n(6)\nHere, \u03b5 \u2208 [0, 1] serves as a threshold for choosing an action,i.e. whether to follow the policy or take a risk and explore,\nbased on a comparison with a randomly generated numberin the range [0, 1]. Typically, \u03b5 is initialized at 1 and gradually\ndecays over iterations until it reaches 0, indicating a transi-tion from exploration to exploitation. The speed of this decay\ndepends on the decay rate d\u03b5, such that \u03b5 \u2190 (1 \u2212 d\u03b5)\u03b5.\nTo improve the convergence of the training procedure, theexperience replay technique is usually implemented (Watkins\nand Dayan, 1992; Mnih et al., 2013, 2015). One typically stores\nexperiences in a replay buffer so that the data that the agent\ncollects in the past can be used for learning again and updat-ing the network's parameters.\nOverview. To elaborate on our implementation of AS via Q-learning-based RL, we first define the state space and de-scribe the reward function design. Subsequently, we pro-vide details on the dataset used to construct the RL envi-ronment, the architecture of the model, and the considered\nhyper-parameters.\nState space. We define the state st \u2208 S as a combinationof patient-level clinical variables, radiographic findings, and\ntime indices of data acquisition (0 \u2013 baseline examination, 1\n1-year follow-up, and so forth). The included clinical vari-ables describe common risk factors: age, sex, and Body Mass\nIndex (BMI). Furthermore, we used the Western Ontario and"}, {"title": "3. Related work", "content": "Care. The time of TKR is therefore challenging to predict,\ndue to the large amount of noise in the data. Tiulpin et al.\n(2019a) proposed to use KLG-based progression as an out-come, which was further studied by Nguyen et al. (2022, 2023)\nand Panfilov et al. (2022, 2023). The latter studies introduced\nthe CLIMAT method, showing that one can forecast the whole\nevolution of KLG scores using a transformer-based architec-ture.\nThe main limitation of most previous studies on KOA pro-gression prediction is that they consider predictions per knee,\ndisregarding that OA often co-occurs in two knees. The sec-ond limitation is that they ignore the long disease progres-sion timelines, taking for modeling only one baseline mea-surement point. OA can progress very slowly, and be drasti-cally accelerated by various factors, e.g. injury occurring at\nsome point in the future. Therefore, we believe that it is cru-cial to update the model's predictions when newer data froma patient becomes available. Finally, none of the previous\nstudies considered the financial aspect of their results, nor\ndownstream drug development or clinical applications. Our\nwork aims to address all these limitations, enabling dynamic\npatient-level data collection, while balancing data collection\nutility and costs with RL.\nThe literature on AS falls into three main areas: recurrentneural networks (RNNs), Bayesian optimization, and RL. Ad-ditionally, we discuss KOA prognosis studies as prior knowl-edge for our work."}, {"title": "3.1. Knee osteoarthritis", "content": "In the KOA research area, there has been a growing interestin the application of machine learning (ML) methods for KOAprogression prediction (Tiulpin et al., 2019a; Tolpadi et al.,\n2020; Guan et al., 2020; Leung et al., 2020; Jamshidi et al.,2021; Tiulpin et al., 2022; Nguyen et al., 2022, 2023; Panfilov\net al., 2022, 2023; Cigdem and Deniz, 2023; Hirvasniemi et al.,2023). Two main lines of work related to the prediction of KOAprogression from clinical and imaging data are the predictionof TKR (Tolpadi et al., 2020; Leung et al., 2020; Jamshidi et al.,2021) and prediction of radiographic KOA worsening (Tiulpinet al., 2019a; Guan et al., 2020; Nguyen et al., 2023).\nTKR is an acknowledged clinical endpoint for KOA predic-tion, defining the ultimate stage of disease progression. How-ever, the decision to operate is made based on symptomaticand functional assessment of the joint, as well as access to"}, {"title": "3.2. Active sensing", "content": "Recurrent neural networks. Considering AS as a sequentialdata collection prediction problem, RNN emerges as a natu-ral approach. The inherent feedback loop in a standard RNNallows the model to memorize previous inputs, making it par-ticularly effective with time-series data. Among the RNNSfamily, Gated Recurrent Unit (GRU) (Chung et al., 2014) andLong Short-Term Memory (LSTM) (Hochreiter and Schmid-huber, 1997) are the most well-established methods and in-fluential in various domains (Kollias et al., 2018; Karita et al.,2019). These methods have proven valuable, forecasting car-"}, {"title": "4. Experimental setup", "content": "ing trajectories from medical records (Pham et al., 2017).\nAn LSTM-based model was developed to personalize clini-cal events prediction for ICU patients from electronic health\nrecords (Lee and Hauskrecht, 2023). Reddy and Delen (2018)\nproposed RNN-based methods to predict hospital readmis-sion for lupus patients. Even though these studies created\nbenchmarks for an AS strategy, converting model predictions\ninto decisions remains challenging due multi-objective na-ture of clinical decision-making (Sun and Giles, 2001).\nBayesian optimization. From the perspective of decision-making, Bayesian optimization (Ahmad and Yu, 2013; Alaa\nand Van Der Schaar, 2016; Pei et al., 2018; Jarrett and Van\nDer Schaar, 2020) is a powerful tool to tackle AS challenges.\nAhmad and Yu (2013) introduced Context-Dependent Ac-tive Controller (C-DAC) a Bayes-optimal decision-making\nmodel that aims to minimize behavioral costs, such as tem-poral delay, response error, and sensor repositioning cost for\nAS. Jarrett and Van Der Schaar (2020) introduced the con-cept of inverse AS, which seeks to uncover an agent's pref-erences and strategy given their observable decision-making\nbehavior. While these studies contribute foundational knowl-edge to AS in the medical area, they remain largely theoretical\nand lack performance validation on real clinical datasets with\nhigh-dimensional data.\nReinforcement learning. RL has become one of the mostcommon techniques to explore AS in recent years, particu-larly in medical applications for disease management (Ahuja\net al., 2017; Yoon et al., 2019; Chang et al., 2019; Qin et al.,\n2024; Holt et al., 2024). DPSCREEN framework proposedby Ahuja et al. (2017) aimed to deliver a personalized screen-ing policy for breast cancer patients. The proposed policy\ntakes into account both patient features and their clinical his-tory, and it demonstrated significant reductions in the num-ber of screenings performed, while maintaining the same ex-pected delays in disease detection. Yoon et al. (2019) per-formed the AS using the Actor-Critic (ASAC) framework to\ntackle the issue of selecting the variables that should be ob-served. The ASAC framework includes two networks: a se-\nlector network, playing as an actor, and a predictor network,playing as a critic. The authors showed that ASAC outper-formed state-of-the-art methods in two real-world medicaldatasets of Alzheimer's Disease and electronic health records.\nChang et al. (2019) presented a deep Q-learning approachto strategically schedule measurements, especially in ICUmortality prediction. In simulations and real-world datasets,\nthe proposed policy outperforms heuristic-based schedulingwith higher predictive gain and lower cost.\nThe prior work on AS with RL focused on improving generalRL methods and placed a substantially lesser focus on the re-ward function, the disease of interest, as well as the challengeof handling high-dimensional data. In this work, we chosea simple RL technique \u2013 Q-learning, and instead focused onthe development of the reward and the environment. We be-lieve that our work lays the foundations for future, more RL-oriented studies for AS."}, {"title": "4.1. Dataset", "content": "We conducted experiments on the Osteoarthritis Initia-tive (OAI) dataset, publicly available at https://nda.nih.gov/oai/. Since the OAI is a multi-center cohort, we choseone center as our test set, and the remaining data served fortraining purposes. To conduct our AS experiments, we in-cluded data from the baseline, 1-year, 2-year, 3-year, and 4-year follow-ups.\nEach included participant had bilateral plain radiographs,and we used the method developed by Tiulpin et al. (2019b)\nto localize the knee joints (Nguyen et al., 2022, 2023; Tiulpinet al., 2019a). We included only those subjects whose kneeimages and all the listed clinical data were fully available.\nTo track the disease progression, we employed JSW, easilycollected from radiographs and available in OAI. The KLGscores, representing the severity of KOA disease, were com-puted from a DL-based model (Nguyen et al., 2020). Fur-thermore, we utilized clinical data including age, sex, BMI,\nphysical SF12 score (Ware et al., 1996), past injury records,\npast surgery records. To aid the models with the status of the"}, {"title": "5. Results", "content": "joint function, stiffness, and pain, we also employed the to-tal WOMAC score (Bellamy et al., 1988) in our feature vector.\nAfter the data curation, we obtained the radiographs and theclinical variables from a total of 1620 OAI participants. Thedata statistics at the baseline are summarized in Table 1."}, {"title": "4.2. Hyper-parameters of the reward function", "content": "We utilized the Current Procedural Terminology (CPT) sys-tem in the United States to search for the expenses associatedwith medical imaging and TKR (Thorwarth Jr, 2004; Hirsch\net al., 2015). Based on these data, the TKR surgery (CPT code:27446) can amount to up to $50,000 per patient. A follow-up visit consisting of consultant fees (CPT code: 73560) and a\nknee X-ray imaging (CPT codes: 73560, 73562, 73564, 73565)may range from $300 to $1,000. The medical cost variesdepending on region, insurance, and other caring services.\nIn our experiments, we empirically set the cost of TKR at$10,000, and the cost of each hospital visit at $500.\nThe average healthy JSW was set to 5mm accordingto Buckland-Wright et al. (1995); Anas et al. (2013). We\ncompute the parameter c in Eq. (9) as c = $10,000/5mm =\n2000$/mm. For numerical stability, we scaled down all mon-etary hyper-parameters by 1000.\nWe empirically chose \u03b1 = 0.5 as a parameter to scale downthe late visit-associated negative reward, and set p = 0.3c toassign positive rewards for true dismissal. \u03bb = 0.5 was usedin all our experiments, and we also conducted an ablationstudy to validate how our method performs in different set-tings. More details about these hyper-parameters will be pre-sented in Section 5.2."}, {"title": "5.1. Comparisons to reference methods", "content": "We present the quantitative comparisons between our\nmethod and the baselines in Table 3. The random policies\ngenerally could not exceed 51% of the average BA, regardless\nof 4 different probabilities of selecting the follow-up action.\nAs the probabilities increased from 30% to 90%, the trend of\nRPP exhibited a parabolic shape, peaking at 70%. The RPP of\n30% and 90% probabilities were approximately equal to each\nother, and these were the lowest among the random poli-cies under consideration. This suggests a non-linear relation-ship between the selection probability and the policy perfor-mance.\nAmong the three routine sensing approaches, the BAS pol-icy yielded the highest reward, yet it was still a negative re-ward of -0.1. The NS policy resulted in the lowest reward not\nonly among the three but also across all the methods. Whenreplacing the BAS with the random policy, we observed an in-crease of 2.97% in BA, albeit drops of 3.34% in RC, and 0.66 in\nreward. The ANS policy, which ensured no missed follow-upvisits, led to the highest acquisition cost compared to all the\nmethods. Compared to this policy, our method was able toidentify 73.67% of the progressions while reducing the acqui-sition cost by up to 45.5%.\nIn general, AS baselines tended to enhance BA yet deterio-rate RC when compared to the BAS policy. Specifically, the\nimprovement in BA ranged from 0.02% to 6.41%, with the\nmost significant increase observed in CLIMATv2. However,\nDeepHit was the only AS reference that improved RC com-pared to the BAS policy, demonstrating an increase of 12.98%.\nAs a result, DeepHit achieved the highest reward among allthe baselines, although its total reward was still negative.\nDifferent from the AS baselines, our RL-based policy led to\nimprovements in both BA and RC. Specifically, our method\nsignificantly outperformed CLIMATv2 with increases of 3.9%\nin BA and 35.9% in RC. When compared to DeepHit, theRL-based method demonstrated substantial improvementsof 7.9% in BA and 10.7% in RC. Notably, our method was the"}, {"title": "4.3. Agent", "content": "We designed our Q-Network to process the state revealedby the environment (i.e. multi-modal input data) and predictthe Q-value for each action. The input data, excluding theKOA status, were normalized across the dataset by subtract-ing the mean and dividing by the standard deviation. TheKOA status, as presented in Section 2.3, is a vector of prob-abilities extracted from a pre-trained deep neural network(DNN), where each probability corresponds to a particularseverity grade. We employed the Semixup model (Nguyenet al., 2020) to extract image features and KLG probabilities.Whereas the imaging feature dimension of each radiographwas 1024, the vector of KLG probabilities had a size of 5.\nWe first combined the KLG probability vectors from bothknees and the vector of clinical variables, resulting in an inputvector of size 22. The Q-Network was a simple neural networka simple neural network (batch norm of the inputs followedby a single hidden layer with dropout probability of 0.2 andsigmoid activation function, followed by a linear output layerwith 2 heads). The network's output was a 2-element vectorrepresenting Q-values of 2 actions for each time step.\nThe agent was trained in an episodic setting, where eachepisode represented a subject's participation in an observa-"}, {"title": "5.2. Impact of financial factors on decision-making", "content": "only one that gained a positive reward, with a value of 0.2.\nAs the cost-related parameters, \u03bb and c, are key factors inour reward function, we aimed to quantitatively assess theirimpacts on our method. Specifically, we conducted two ex-periments involving our proposed methods, CLIMATv2, ANS,\nand NS policies. In the first experiment, we varied the hospi-tal visit cost, \u03bb, while keeping the MLAC, c, unchanged. In the\nsecond experiment, we reversed the settings.\nWe present the results of the first experiment in Figure 4a,demonstrating the association between \u03bb and the RPP over4 year, with a fixed c of 2. Generally, all methods exhib-ited a decreasing trend in the reward as the hospital visit\ncost increased. Compared to the mentioned baselines, ourRL-based method, corresponding to the blue curve in Fig-ure 4a, consistently gained better rewards per person withevery value of \u03bb\u2208 [0,2", "0.5,2": "."}]}