{"title": "Active Sensing of Knee Osteoarthritis Progression with Reinforcement Learning", "authors": ["Khanh Nguyen", "Huy Hoang Nguyen", "Egor Panfilov", "Aleksei Tiulpin"], "abstract": "Osteoarthritis (OA) is the most common musculoskeletal disease, which has no cure. Knee OA (KOA) is one of the highest causes of disability worldwide, and it costs billions of United States dollars to the global community. Prediction of KOA progression has been of high interest to the community for years, as it can advance treatment development through more efficient clinical trials and improve patient outcomes through more efficient healthcare utilization. Existing approaches for predicting KOA, however, are predominantly static, i.e. consider data from a single time point to predict progression many years into the future, and knee level, i.e. consider progression in a single joint only. Due to these and related reasons, these methods fail to deliver the level of predictive performance, which is sufficient to result in cost savings and better patient outcomes. Collecting extensive data from all patients on a regular basis could address the issue, but it is limited by the high cost at a population level. In this work, we propose to go beyond static prediction models in OA, and bring a novel Active Sensing (AS) approach, designed to dynamically follow up patients with the objective of maximizing the number of informative data acquisitions, while minimizing their total cost over a period of time. Our approach is based on Reinforcement Learning (RL), and it leverages a novel reward function designed specifically for AS of disease progression in more than one part of a human body. Our method is end-to-end, relies on multi-modal Deep Learning, and requires no human input at inference time. Throughout an exhaustive experimental evaluation, we show that using RL can provide a higher monetary benefit when compared to state-of-the-art baselines. Finally, our work introduces a novel AS benchmark for RL in medicine, which we make publicly available at https://github.com/Oulu-IMEDS/OACostSensitivityRL.", "sections": [{"title": "1. Introduction", "content": "Osteoarthritis (OA) is a degenerative joint disorder that affects primarily the knee cartilage, and results in its gradual loss and subsequent damage to the joint (Primorac et al., 2020). Knee OA (KOA), the most frequent type of OA, is studied in this work. Common symptoms of KOA include pain, stiffness, and reduced joint flexibility, all of which significantly impact an individual's quality of life. As the condition progresses, the symptoms often worsen, decreasing mobility and potentially leading to disability (McAlindon et al., 1993). To date, there are no effective treatments that can prevent long-term disability and permanent joint damage in KOA (Hafez et al., 2014). At the latest stages of KOA, a costly and highly invasive intervention, such as total knee replacement (TKR), is delivered to improve the patient's health and well-being. A TKR surgery may cost up to $50,000 for each patient (Price et al., 2018; Evans, 2018; Phillips et al., 2019), and TKR reportedly yields unsatisfactory results in 15-20% of cases (Marsh et al., 2022; Bourne et al., 2010; Price et al., 2018). Due to rapid population aging, TKR gradually becomes a financial burden not only for KOA patients (Kjellberg and Kehlet, 2016), but also for the healthcare system (Gandhi et al., 2023). Therefore, KOA in general, requires the next-generation disease-modifying drugs (DMOADs) (Oo et al., 2018; Rodriguez-Merchan, 2023). Such therapies should aim to slow down the KOA progression, thus, delaying the need for TKR (Latourte et al., 2020; Cho et al., 2021).\nWhile the research community has been working on KOA DMOADs for a number of years (Pfrizer, 2010; Hunter, 2011), no drugs have yet been found effective and approved by the regulatory bodies (Rodriguez-Merchan, 2023). One of the challenges in developing DMOADs for KOA is the long patient follow-up time in clinical trials (Latourte et al., 2020). KOA is often a slowly progressing disease with an unknown etiology (Driban et al., 2020). Thus, many subjects recruited to KOA trials and cohorts do not develop the disease at all and some develop only early signs of the disease by the end of a study. Hence, adaptive methods for data acquisition in modern clinical trials (Spreafico et al., 2021; Cuzick, 2023) are becoming instrumental to providing better, and more cost-efficient patient monitoring compared to routine scheduling and randomized participant selection (Wu and Suen, 2022; Yala et al., 2022).\nDespite recent advancements in the field of KOA progression prediction (Halilaj et al., 2018; McCabe et al., 2022; Tiulpin et al., 2019a; Nguyen et al., 2022, 2023), translation of existing models into real-world applications is challenging for several reasons. Firstly, the financial downstream impact of the disease progression models is hard to assess during the model development. As such, the current models do not account for follow-up costs and potential future expenses associated with structural KOA progression when they estimate if a person will develop KOA in the future. Secondly, the existing models (Tiulpin et al., 2019a, 2022; Nguyen et al., 2023; Hirvasniemi et al., 2023; Hu et al., 2022; Panfilov et al., 2022, 2023) largely focus only on the knee-level progression events and overlook KOA's broader patient-level context, where progression may happen concurrently in both knees. Thirdly, OA severity may increase multiple times during the study time, and there are uncertain future factors that may alter the disease course (e.g. future injuries). Therefore, long-horizon prediction is hard, and data used for such prediction needs to be collected dynamically.\nActive Sensing (AS) (Yu et al., 2009) addresses the question of when to follow up patient and with what tools or modalities an examination has to be conducted. In the case of KOA, one would want for an AS policy to minimize the follow-up costs while maximizing the chances of capturing patient-level progression. Solving such a problem, however, is challenging for a na\u00efve supervised learning (SL) approach, as one needs to consider making predictions to collect data, which would then be used to make new predictions and so forth. This is a hardly differentiable problem of decision-making under uncertainty, which can be solved using e.g. Reinforcement Learning (RL).\nIn this work, we develop an AS methodology for KOA using RL. We utilize RL to make sequential decisions based on the interactions between a decision-making agent and an envi-"}, {"title": "2. Methodology", "content": "ronment simulating a clinical trial or another setting where patients need to be observed on an interval basis for a prolonged period of time. Our study makes the following contributions:\n\u2022 We introduce a formal framework for personalized AS into the KOA domain to enhance the efficiency of clinical trials.\n\u2022 We propose to dynamically model and predict KOA progression at the patient level.\n\u2022 We employ an RL-based model to derive a personalized patient follow-up schedule that is being refined dynamically, taking into account costs and data utility from a clinical perspective.\n\u2022 We propose a new reward function, designed specifically for AS of structural KOA degeneration.\n\u2022 We set up a novel AS baseline for the field of OA, particularly, KOA. We openly release the code and a newly developed RL environment to foster the development of new methods and approaches.\nThe overview of our method is depicted in Figure 1."}, {"title": "2.1. Problem definition", "content": "We consider a set of participants in a longitudinal observational setting and assume that all of them have been examined at the baseline, corresponding to t = 0. If a subject visits the hospital at time t, we observe a state a set of multi-modal data consisting of clinical variables and imaging data, denoted by st, which is needed for KOA diagnosis at t. This examination incurs a certain acquisition cost (visit cost), which we denote by \u03bb.\nOur objective is to make optimal, w.r.t. both costs and outcomes, sequential decisions to refer each subject for an examination to capture the disease progression given acquired multi-modal data. We illustrated the decision-making process in Figure 2. Specifically, for each subject, at time point"}, {"title": "2.2. Reinforcement Learning", "content": "t\u2208 {0,..., T-1}, we predict whether that subject should be examined at the next time point t + 1. The referral for an examination is considered successful if the disease has progressed between t and t + 1. We formalize this as the following optimization problem:\n$\\displaystyle {\\lbrace a_1^*,..., a_{T-1}^*\\rbrace = arg \\min_{\\{a_1,...,a_{T-1}\\}} \\frac{1}{T-1} \\sum_{t=1}^{T-1}  (\\mathbb{S}(s_t, a_t, \\lbrace \\delta(j)\\rbrace_{j=1}^{J}; \\kappa, \\lambda)},$\n(1)\nwhere at \u2208 {0,1} indicates whether to issue a referral for a visit at time-point t + 1. We assume that the value of the objective J depends on the actions at, states st, and the set of patient-level changes associated with the disease {\u03b4(j)}(i.e. KOA in the context of this study). We also introduce a hyper-parameter threshold \u049d that defines whether the disease progression is significant (e.g. may correspond to minimal clinically important difference). The combination of changes {\u03b4(j)} and x defines the disease trajectory. The cost of data acquisition A is treated as a hyper-parameter. We also use A as a basic cost unit multiplier when defining the costs of undetected disease progression.\nThe optimization of the defined objective in (1) is the problem of sequential decision-making under uncertainty, which we further define through the prism of the Markov Decision Processes (MDP) and RL.\nMarkov Decision Process. An MDP is defined as a tuple (S, A, P, R, \u03b3), where S is a set of states, A is a set of actions. P:S\u00d7S\u00d7A\u2192 [0, 1] indicates transition probabilities between states. Finally, R: S \u00d7 A \u2192 R represents a set of rewards associated with state-action pairs and \u03b3\u2208 [0, 1] is the discounting factor (Sutton and Barto, 2018). The decision-making policy of an agent is defined by \u03c0 : A \u00d7 S \u2192 [0, 1].\nIn the MDP framework, the sequential decision-making process unfolds as follows (Sutton and Barto, 2018). At a time step t\u2208 N, the environment (a \"world\" in which the agent exists and makes actions) yields a state st \u2208 S that is observed by the agent. The agent then responds with an action at \u2208 A following the policy \u03c0(at | st) that assigns a probability to the"}, {"title": "Q-learning", "content": "event associated with selecting action at at a state st. Subsequently, the environment provides a reward rt = R(st, at) to the agent for taking the action at. The process then repeats until the horizon is reached (i.e. t = T). We use the term experiences to refer to a collection of states st, next states St+1, actions at, and rewards rt.\nQ-learning. To measure the expected return of taking action at at state st according to policy \u03c0, we introduce a state-action value function:\n$\\displaystyle Q_{\\pi}(s_t, a_t) = r_t + \\gamma Q_{\\pi}(s_{t+1}, a_t).$\n(2)\nWhen solving an MDP, we aim to obtain the optimal policy \u03c0*. In the case of Q-learning (Watkins and Dayan, 1992),$\\displaystyle \\pi^*(s) = \\text{Dirac} \\left( \\text{argmax}_a Q(s,a) = a^* \\right) \\forall s \\in S, \\text{where Dirac}(\\cdot)$ is the Dirac's delta function and $a^*$ is the optimal action. For \u03c0*, one can derive the Bellman equation as\n$\\displaystyle Q^*(s_t, a_t) = r_t + \\gamma \\max_{a_{t+1}}Q^*(s_{t+1}, a_{t+1}).$\n(3)\nQ-learning updates the Q-value for a certain action using the immediate reward and the maximum possible Q-value for the next state. Let us denote the post-update Q-value as Q(st, at), the learning update is then:\n$\\displaystyle Q^*(s_t, a_t) \\leftarrow (1 - \\alpha) Q(s_t, a_t) + \\alpha \\left(r_t + \\gamma \\max_a Q(s_{t+1}, a)\\right),$\n(4)"}, {"title": "Q-function approximation", "content": "where a is the learning rate, determining how much newly obtained information replaces the old information. Through interactions with the environment, the policy under Q-learning is supposed to converge after a sufficient number of iterations (Watkins and Dayan, 1992). The presented update rule is sufficient when the number of states and actions is relatively small. In the context of this study or other real-world cases, the set of states may have an infinite size, making it impossible to store the Q-table. To tackle this issue, one can resort to a functional approximation, such as with a neural network (NN) (Tesauro et al., 1995; Mnih et al., 2013, 2015).\nQ-function approximation. In contemporary RL, the Q-function is often approximated by NN, which we denote as Q(st, at, \u03b8), where \u03b8 represents NN's parameters. The parameterized Q-function is then commonly called a Q-Network. Mnih et al. (2013) introduced two networks that allow to follow the Temporal Difference Learning idea (Tesauro et al., 1995): the local Q-Network, approximating Q(st,), and the target Q-Network, approximating Q(st+1,). The local Q-Network is optimized by utilizing the mean square error loss to measure the difference between the estimated Q-value and the true one, noted as:\n$\\displaystyle L(\\theta) = \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a, \\theta^*) - Q(s_t, a_t, \\theta) \\right]^2,$\n(5)"}, {"title": "Training Q-networks", "content": "The parameters \u03b8* of the target Q-Network are updated periodically using the local Q-Network to stabilize the learning process and improve convergence.\nTraining Q-networks. To train a Q-network from various experiences and collect information along various roll-outs of an MDP, one has to balance exploration and exploitation during training. Here, we used an \u03b5-greedy policy (Sutton and Barto, 2018) that induces the following action selection protocol. Let us draw n\u2081 ~ Uniform(0, 1), then\n$\\displaystyle \\pi(a_t | s_t) = \\begin{cases} \\text{argmax}_a Q(s_t, a) & \\text{if } n_1 < \\varepsilon \\\\ \\text{Random action} & \\text{otherwise} \\end{cases}$\n(6)\nHere, \u03b5 \u2208 [0, 1] serves as a threshold for choosing an action, i.e. whether to follow the policy or take a risk and explore, based on a comparison with a randomly generated number in the range [0, 1]. Typically, \u03b5 is initialized at 1 and gradually decays over iterations until it reaches 0, indicating a transition from exploration to exploitation. The speed of this decay depends on the decay rate d\u03b5, such that \u03b5 \u2190 (1 \u2212 d\u03b5)\u03b5.\nTo improve the convergence of the training procedure, the experience replay technique is usually implemented (Watkins and Dayan, 1992; Mnih et al., 2013, 2015). One typically stores experiences in a replay buffer so that the data that the agent collects in the past can be used for learning again and updating the network's parameters."}, {"title": "2.3. Proposed method", "content": "Overview. To elaborate on our implementation of AS via Q-learning-based RL, we first define the state space and describe the reward function design. Subsequently, we provide details on the dataset used to construct the RL environment, the architecture of the model, and the considered hyper-parameters.\nState space. We define the state st \u2208 S as a combination of patient-level clinical variables, radiographic findings, and time indices of data acquisition (0 \u2013 baseline examination, 1 \u2013 1-year follow-up, and so forth). The included clinical variables describe common risk factors: age, sex, and Body Mass Index (BMI). Furthermore, we used the Western Ontario and"}, {"title": "Costs of bilateral structural KOA progression", "content": "McMaster Universities Arthritis Index (WOMAC) to quantify the patient's symptoms, the physical score of the 12-Item Short Form Survey (SF12) to quantify the overall patient's physical condition, binary code about the occurrences of past knee injury and surgery. Previous studies have demonstrated that these factors are significantly associated with KOA progression (Eymard et al., 2015; Joo et al., 2022; Panfilov et al., 2023).\nThe radiographs were automatically evaluated by a pretrained DL model that extracts the Kellgren and Lawrence grading (KLG) system to derive KOA severity level (Kellgren and Lawrence, 1957). The KLG scale comprises 5 categories (from 0 to 4) that represent no OA, doubtful, mild, moderate, and severe OA, respectively. The graphical illustration of the KLG scale is provided in Figure 3. In our setup, we utilized the grades and their probabilities generated by a DL-based method developed for this task (Nguyen et al., 2020).\nFinally, beyond the clinical variables and image-related information (i.e. KLG), we included the time information as a part of the state, specifically, the current time index and the time index of the latest data acquisition. The clinical variables and the radiographic findings are updated in the state once the agent chooses a follow-up action.\nMulti-progression. The progression trajectory of KOA occurs in several stages and simultaneously in both knees. To monitor the disease through sequential actions, we first define the progression events, which in our study, happen multiple times.\nIn KOA, the decline of JSW measured from plain radiographs is associated with this disease progression. Hence, in this work, we employed the changes in medial fixed JSW@0.250 (fJSW) proposed by Duryea et al. (2010), which was shown to be a robust and effective biomarker for capturing structural degeneration in KOA (Ratzlaff et al., 2018). We denote \u03a6(t\u2081,\u03b6) as the changes of this biomarker between the reference time point tr and time \u03b6. When we define progression, the reference point is the closest progression event, starting at the baseline. If \u03a6(t\u2081,\u03b6) is greater is greater than a certain threshold, denoted as x, we consider \u03b6 as the time when a progression event occurs and add it to a set of progression events. This set, called Mp, includes all significant structural damages observed in either knee. Since a decrease of 0.7mm in fJSW is commonly considered the minimum threshold for detecting KOA radiographic progression (Eckstein et al., 2015; Guan et al., 2020), we used a x of 0.7.\nReward function definition. For each subject, at time point t, an agent takes either of the two actions: follow-up or dismiss (i.e. skip examination) at time t + 1. The rewards for these actions are denoted as rf and rd, respectively. There are three possible cases for the correctness of the follow-up action (a = 1): early visit, timely visit, and late visit. Furthermore, there are two cases for the dismissal action (a = 0): true dismissal and false dismissal. Mathematically, the reward rt at the time point t corresponding to the follow-up and dismissal actions can be expressed as follows\n$\\displaystyle r_f = \\begin{cases} -\\alpha e^{\\tau_t(\\cdot)} \\Delta(t_r, t+1) - \\lambda & \\text{if } t+1 < t_p \\text{ (early visit)} \\\\  \\Delta(t_r, t+1) - \\lambda & \\text{if } t+1 = t_p \\text{ (timely visit)} \\\\ -\\alpha e^{\\tau_t(\\cdot)} \\Delta(t_r, t_p) - \\lambda & \\text{if } t+1 > t_p \\text{ (late visit)} \\end{cases}$\n(7)\nand\n$\\displaystyle r_d = \\begin{cases} \\beta & \\text{if } t+1 < t_p \\text{ (true dismissal)} \\\\ -e^{\\tau_t(\\cdot)} \\Delta(t_r, t_p) & \\text{ift } t+1 \\geq t_p \\text{ (false dismissal)}, \\end{cases}$\n(8)\nwhere tp \u2013 the nearest progression time point, tr \u2013 reference time point, \u0394(tr, t) = c(tr, t) \u2013 data acquisition utility function, \u03bb\u2208 R+ \u2013 fixed cost of data acquisition, \u03c4 : [0, \u03a4] \u2192 R+ \u2013 sensitivity of the agent's mistake, \u03b1 \u2208 [0, 1) \u2013 parameter to ease costs of bilateral structural KOA progression. To set the value\nthe penalty for the late visit, and \u03b2 < ck \u2013 positive reward for the correct dismissal action. In our setup, the late dismissal is a false dismissal discounted by \u03b1 and decreased by \u03bb. The coefficient c is a scaling factor that converts changes in disease biomarkers to the same measurement units as A, and it needs to be devised from the domain knowledge and health economics.\nCosts of bilateral structural KOA progression. To set the value\nof c, we rely on the domain knowledge, i.e. (1) the end-stage\nof the disease is TKR, and (2) if KOA can be detected early, it can presumably be slowed down (Yao et al., 2023). From these assumptions, we define c as the monetary loss of 1mm of articular cartilage (MLAC), which is calculated as:\n$\\displaystyle c = \\frac{TKR \\operatorname{cost} (\\$)}{Mean JSW of healthy knees (mm)},$\n(9)\nwhere the mean JSW can be computed from an observational cohort and TKR cost is pre-defined for a healthcare system where the RL method is deployed.\nTo quantify the radiographic changes in both knees, we define \u03a6L(\u03b6) and \u03a6R(\u03b6) as the JSW values of the left and right knee, respectively. We then calculate dL(\u03b6) and dR(\u03b6) that represent the decrease in JSW from the latest visit tL to the assessment time \u03b6. To mitigate the impact of measurement noise in JSW and handle the rare events of JSW increase, we consider that changes in the JSW w.r.t tL are strictly negative (i.e. JSW is non-increasing):\n$\\displaystyle d_L(t_r, \\zeta) = max(0, \\Phi_L(t_r) - \\Phi_L(\\zeta)),$\n(10)\n$\\displaystyle d_R(t_r, \\zeta) = max(0, \\Phi_R(t_r) - \\Phi_R(\\zeta)).$\n(11)\nThe function \u03b4(\u03b6), which quantifies the changes at a patient level, is then defined as:\n$\\displaystyle \\delta(t_r, \\zeta) = \\begin{cases} d_{LR}(t_r, \\zeta) & \\text{if } d_{LR}(t_r, \\zeta) \\geq \\kappa \\\\ 0 & \\text{otherwise}, \\end{cases}$\n(12)\nwhere dLR (tr, \u03b6) = max(dL(tr, \u03b6), dR(tr, \u03b6)). We note that tracking progressions at every knee individually is insufficient since a typical acquisition of X-ray images is bilateral. For that reason, the status of both knees is always updated as"}, {"title": "3. Related work", "content": "newly acquired data. In the unilateral case, the definition of \u03b4(\u00b7) would require minor changes to the environment, and we ablate the effect of single knee modeling in the experimental part of our work.\nWeighing early and late follow-up decisions. To incorporate the distance between the time of the follow-up action t and the true OA progression time tp into the reward function, we introduce the modulating function \u03c4(t) : [0, T] \u2192 [0,1], defined as\n$\\displaystyle \\tau(t) = \\frac{|t+1 - t_p|}{T}.$\n(13)\nWe consider two specific cases that require such a modulating function: early follow-up and late follow-up/false dismissal. For the first case, we use an exponential decay function e-\u03c4(t). Therefore, an increasing \u03c4(t) reduces the reward that an agent can obtain. For the second case, we penalize late or false dismissal actions, and use the term -e\u03c4(t). As \u03c4(t) increases, the negative reward becomes more considerable, discouraging the agent from delaying necessary actions or incorrectly dismissing the patient."}, {"title": "3.1. Knee osteoarthritis", "content": "In the KOA research area, there has been a growing interest in the application of machine learning (ML) methods for KOA progression prediction (Tiulpin et al., 2019a; Tolpadi et al., 2020; Guan et al., 2020; Leung et al., 2020; Jamshidi et al., 2021; Tiulpin et al., 2022; Nguyen et al., 2022, 2023; Panfilov et al., 2022, 2023; Cigdem and Deniz, 2023; Hirvasniemi et al., 2023). Two main lines of work related to the prediction of KOA progression from clinical and imaging data are the prediction of TKR (Tolpadi et al., 2020; Leung et al., 2020; Jamshidi et al., 2021) and prediction of radiographic KOA worsening (Tiulpin et al., 2019a; Guan et al., 2020; Nguyen et al., 2023).\nTKR is an acknowledged clinical endpoint for KOA prediction, defining the ultimate stage of disease progression. However, the decision to operate is made based on symptomatic and functional assessment of the joint, as well as access to"}, {"title": "3.2. Active sensing", "content": "care. The time of TKR is therefore challenging to predict, due to the large amount of noise in the data. Tiulpin et al. (2019a) proposed to use KLG-based progression as an outcome, which was further studied by Nguyen et al. (2022, 2023) and Panfilov et al. (2022, 2023). The latter studies introduced the CLIMAT method, showing that one can forecast the whole evolution of KLG scores using a transformer-based architecture.\nThe main limitation of most previous studies on KOA progression prediction is that they consider predictions per knee, disregarding that OA often co-occurs in two knees. The second limitation is that they ignore the long disease progression timelines, taking for modeling only one baseline measurement point. OA can progress very slowly, and be drastically accelerated by various factors, e.g. injury occurring at some point in the future. Therefore, we believe that it is crucial to update the model's predictions when newer data from a patient becomes available. Finally, none of the previous studies considered the financial aspect of their results, nor downstream drug development or clinical applications. Our work aims to address all these limitations, enabling dynamic patient-level data collection, while balancing data collection utility and costs with RL.\nThe literature on AS falls into three main areas: recurrent neural networks (RNNs), Bayesian optimization, and RL. Additionally, we discuss KOA prognosis studies as prior knowledge for our work.\n3.2. Active sensing\nRecurrent neural networks. Considering AS as a sequential data collection prediction problem, RNN emerges as a natural approach. The inherent feedback loop in a standard RNN allows the model to memorize previous inputs, making it particularly effective with time-series data. Among the RNNs family, Gated Recurrent Unit (GRU) (Chung et al., 2014) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) are the most well-established methods and influential in various domains (Kollias et al., 2018; Karita et al., 2019). These methods have proven valuable, forecasting car-"}, {"title": "4. Experimental setup", "content": "ing trajectories from medical records (Pham et al., 2017). An LSTM-based model was developed to personalize clinical events prediction for ICU patients from electronic health records (Lee and Hauskrecht, 2023). Reddy and Delen (2018) proposed RNN-based methods to predict hospital readmission for lupus patients. Even though these studies created benchmarks for an AS strategy, converting model predictions into decisions remains challenging due multi-objective nature of clinical decision-making (Sun and Giles, 2001).\nBayesian optimization. From the perspective of decision-making, Bayesian optimization (Ahmad and Yu, 2013; Alaa and Van Der Schaar, 2016; Pei et al., 2018; Jarrett and Van Der Schaar, 2020) is a powerful tool to tackle AS challenges. Ahmad and Yu (2013) introduced Context-Dependent Active Controller (C-DAC) a Bayes-optimal decision-making model that aims to minimize behavioral costs, such as temporal delay, response error, and sensor repositioning cost for AS. Jarrett and Van Der Schaar (2020) introduced the concept of inverse AS, which seeks to uncover an agent's preferences and strategy given their observable decision-making behavior. While these studies contribute foundational knowledge to AS in the medical area, they remain largely theoretical and lack performance validation on real clinical datasets with high-dimensional data.\nReinforcement learning. RL has become one of the most common techniques to explore AS in recent years, particularly in medical applications for disease management (Ahuja et al., 2017; Yoon et al., 2019; Chang et al., 2019; Qin et al., 2024; Holt et al., 2024). DPSCREEN framework proposed by Ahuja et al. (2017) aimed to deliver a personalized screening policy for breast cancer patients. The proposed policy takes into account both patient features and their clinical history, and it demonstrated significant reductions in the number of screenings performed, while maintaining the same expected delays in disease detection. Yoon et al. (2019) performed the AS using the Actor-Critic (ASAC) framework to tackle the issue of selecting the variables that should be observed. The ASAC framework includes two networks: a se-"}, {"title": "4.1. Dataset", "content": "lector network, playing as an actor, and a predictor network, playing as a critic. The authors showed that ASAC outperformed state-of-the-art methods in two real-world medical datasets of Alzheimer's Disease and electronic health records. Chang et al. (2019) presented a deep Q-learning approach to strategically schedule measurements, especially in ICU mortality prediction. In simulations and real-world datasets, the proposed policy outperforms heuristic-based scheduling with higher predictive gain and lower cost.\nThe prior work on AS with RL focused on improving general RL methods and placed a substantially lesser focus on the reward function, the disease of interest, as well as the challenge of handling high-dimensional data. In this work, we chose a simple RL technique \u2013 Q-learning, and instead focused on the development of the reward and the environment. We believe that our work lays the foundations for future, more RL-oriented studies for AS.\n4. Experimental setup\n4.1. Dataset\nWe conducted experiments on the Osteoarthritis Initiative (OAI) dataset, publicly available at https://nda.nih.gov/oai/. Since the OAI is a multi-center cohort, we chose one center as our test set, and the remaining data served for training purposes. To conduct our AS experiments, we included data from the baseline, 1-year, 2-year, 3-year, and 4-year follow-ups.\nEach included participant had bilateral plain radiographs, and we used the method developed by Tiulpin et al. (2019b) to localize the knee joints (Nguyen et al., 2022, 2023; Tiulpin et al., 2019a). We included only those subjects whose knee images and all the listed clinical data were fully available. To track the disease progression, we employed JSW, easily collected from radiographs and available in OAI. The KLG scores, representing the severity of KOA disease, were computed from a DL-based model (Nguyen et al., 2020). Furthermore, we utilized clinical data including age, sex, BMI, physical SF12 score (Ware et al., 1996), past injury records, past surgery records. To aid the models with the status of the"}, {"title": "4.2. Hyper-parameters of the reward function", "content": "joint function, stiffness, and pain, we also employed the total WOMAC score (Bellamy et al., 1988) in our feature vector. After the data curation, we obtained the radiographs and the clinical variables from a total of 1620 OAI participants. The data statistics at the baseline are summarized in Table 1.\n4.2. Hyper-parameters of the reward function\nWe utilized the Current Procedural Terminology (CPT) system in the United States to search for the expenses associated with medical imaging and TKR (Thorwarth Jr, 2004; Hirsch et al., 2015). Based on these data, the TKR surgery (CPT code: 27446) can amount to up to $50,000 per patient. A follow-up visit consisting of consultant fees (CPT code: 73560) and a knee X-ray imaging (CPT codes: 73560, 73562, 73564, 73565) may range from $300 to $1,000. The medical cost varies depending on region, insurance, and other caring services. In our experiments, we empirically set the cost of TKR at $10,000, and the cost of each hospital visit at $500.\nThe average healthy JSW was set to 5mm according to Buckland-Wright et al. (1995); Anas et al. (2013). We"}, {"title": "4.3. Agent", "content": "compute the parameter c in Eq. (9) as c = $10,000/5mm = 2000$/mm. For numerical stability, we scaled down all monetary hyper"}]}