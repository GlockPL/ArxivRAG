{"title": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation", "authors": ["Qiming Zhu", "Jialun Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Shing-Chi Cheung"], "abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate capabilities of Large Language Models (LLMs), providing insights into their strengths and weaknesses. However, current benchmarks primarily exercise LLMs' capability on common coding tasks (e.g., bubble sort, greatest common divisor), leaving domain-specific coding tasks (e.g., computation, system, cryptography) unexplored. To fill this gap, we propose a multi-domain code benchmark, DOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our pipeline works in a fully automated manner, enabling a push-bottom construction from code repositories into formatted subjects under study. Interesting findings are observed by evaluating 12 representative LLMs against DOMAINEVAL. We notice that LLMs are generally good at computation tasks while falling short on cryptography and system coding tasks. The performance gap can be as much as 68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more samples can increase the overall performance of LLMs, while the domain bias may even increase. The contributions of this study include a code generation benchmark dataset DOMAINEVAL, encompassing six popular domains, a fully automated pipeline for constructing code benchmarks, and an identification of the limitations of LLMs in code generation tasks based on their performance on DOMAINEVAL, providing directions for future research improvements.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have revolutionized various areas such as question answering (Rogers, Gardner, and Augenstein 2023), math reasoning (Imani, Du, and Shrivastava 2023), and especially software development (Lozhkov et al. 2024). Stakeholders are eager to know whether and to what extent LLMs can improve development efficiency.\nTo this end, a variety of code generation benchmarks such as HumanEval (Chen et al. 2021) and MBPP (Austin et al. 2021) have been introduced and intensively used to evaluate LLMs' coding capability. They primarily consist of common coding tasks such as sorting an array or computing the\ngreatest common divisor. Furthermore, to meet more realistic needs, more benchmarks are emerging, expanding mainly along with two perspectives. First, linguistic diversity. This line of work (Wang et al. 2024; Cassano et al. 2022; Chai et al. 2024) mitigates linguistic bias in both natural languages (e.g., English, Chinese) and programming languages (e.g., Python, Java, C/C++). Second, code scale diversity. This line of work aims to scale up the code granularity from function-level (Chen et al. 2021; Yu et al. 2024; Austin et al. 2021) to class-level (Du et al. 2023) and repo-level (Zhang et al. 2023; Cao et al. 2024a; Li et al. 2024).\nHowever, if LLMs are to be applied in real-world industrial scenarios where different product lines prioritize different domains of code, it becomes essential to further understand these LLMs' coding capabilities across various domains. In other words, while existing benchmarks make significant efforts in varying natural/programming languages and code granularity, the LLMs' ability to generate domain-specific code remains under-explored.\nTo fill this gap, we introduce DOMAINEVAL, a multi-domain code generation benchmark. It consists of 2454 sub-"}, {"title": "Benchmark Construction", "content": "In this section, we introduce the whole construction pipeline of DOMAINEVAL, as shown in Figure 2. We provide a fully automated, test-guided construction algorithm that transforms code repositories into a collection of formatted subjects for LLM evaluation. Each subject consists of three components: instruction for LLM evaluation, reference solution, and a series of test cases, as illustrated in Figure 3.\nThe pipeline begins with the collection of raw code snippets from specific domains, which are then systematically transformed into suitable benchmark data. This process involves three key steps: (1) Domain Repository Collection: The first step involves collecting raw code data (R) from selected domains. These snippets are categorized into two groups: $C_o$, containing function code, and $T_o$, including tests designed to validate the correctness of the code in $C_o$. (2) Test-Method Matching & Selection: This step aims to create a set of candidate subjects $S' = (c, T)$ from the collected domain code, where each pair consists of a function code c and its corresponding test cases T. To achieve this, we first match each function code c with its corresponding set of test cases T, ensuring they are compatible with the execution environments E. We then filter $C_o$ to retain only those code snippets that are executable and accompanied by valid test functions, resulting in a refined set $C_1$ and its asso-"}, {"title": "Domain Repository Collection", "content": "Referring to the work (Zhuo et al. 2024), we select six domains for code generation: computation, network, basic operation, system, visualization, and cryptography, to serve as the domain divisions for DOMAINEVAL.\nTo ensure that DOMAINEVAL closely aligns with the real-world code requirements of human engineers, we chose GitHub repositories as the source of raw data. We cloned a selection of representative code repositories from GitHub, particularly those with at least 100 stars, as these are considered high-quality code data and reflect the actual needs of engineers. The code repositories used in each domain are shown in Figure 2. Given that each GitHub repository represents a specific real-world application scenario, we classify the code snippets based on the repository's domain. In practice, we use the repository's topic labels and README files to accurately assign the code to the appropriate domain."}, {"title": "Test-Method Matching & Selection", "content": "This section describes how to obtain reference solutions and their corresponding tests from code repositories to construct candidate subjects for LLM evaluation and the automated construction pipeline.\nGiven that code snippets from GitHub repositories are written by human engineers in real-world production environments, they tend to be more complex, with more dependencies and higher levels of encapsulation than standalone functions in some code datasets. This complexity makes it challenging to select function code that can serve as benchmark data while simultaneously acquiring corresponding test cases to form complete candidate subjects.\nTo address this challenge, we propose a test-method tracing strategy aimed at constructing candidate subjects for code benchmarking. Our approach has two main steps: First, we search the code repository for tests related to the reference code and perform Test-method Matching, where each function code c is automatically paired with its corresponding tests T. Second, to ensure the final candidates are suitable for LLM evaluation and can smoothly pass through the automated construction pipeline, we filter the matched results using three criteria: executable, significant, and appropriate difficulty.\nTest-Method Matching To match the function code with corresponding tests and package them into candidate subjects, we start with the test code and trace it back to the associated function code. Specifically, we search for Python code files in repositories and use the library ast to parse Python code into an abstract syntax tree. Then, we sequentially traverse the nodes of the syntax tree, extracting all functions and their class context (if present). Next, we select test code snippets based on two heuristic rules: First, the function or class name of a test should contain test or Test. Second, the test code snippet should contain an assert statement.\nTo match the selected test code with the corresponding function code, we identify ast.Call nodes as function calls.\nTo retrieve the implementation of the called function, we"}, {"title": "Instruction Generation", "content": "After filtering the candidate subjects, our subjects still lack the instruction field. To ensure the reproducibility of the dataset construction process, we employ open-source LLM, i.e.Qwen2-72B-Instruct-GPTQ-Int4, for data generation. We utilize a dialogue prompt template, as depicted in Figure 4, to guide the LLM in generating detailed Instruction fields for each validated subject. These instruction fields comprehensively describe the function, including its purpose, input arguments, and expected outputs, which serve as the input for evaluating the code generation capabilities of LLMs. By leveraging LLMs, we can generate natural language descriptions that outline the desired functionality, inputs, and outputs of the code (Xie et al. 2024).\nFinally, to construct the template for code generation, we mask the function code, following prior work (Xie et al. 2024), by [MASK] as shown in the method_code_mask field in Figure 3, and instruct LLMs to complete the masked segments during the evaluation."}, {"title": "Benchmark Statistics", "content": "The above pipeline generates instructions, reference code, and a series of tests shown in Figure 3, which combine to form complete subjects. As a result, DOMAINEVAL consists of 2454 code subjects. Figure 2 shows the number of subjects constructed from repositories across different domains. Overall, the number of subjects from each domain is at least 100. The computation domain encompasses the greatest number of subjects, with 1705 subjects, compared to 100 ~ 256 subjects in other domains. This is because the computation-related repositories offer more function code and also include a larger number of test cases to ensure the accuracy of each computational operation."}, {"title": "Experiment Setup", "content": "Studied LLMs. We assess 12 representative instruction-tuned LLMs against DOMAINEVAL, including GPT-3.5-turbo, GPT-40-mini (Brown et al. 2020; Achiam et al. 2023), Qwen2 (Yang et al. 2024), Phi-3 (Abdin et al. 2024)), DeepSeek-Coder series (Zhu et al. 2024; Guo et al. 2024), Llama2 (Touvron et al. 2023) and CodeLlama series (Roziere et al. 2023), CodeQwen1.5 (Bai et al. 2023) with size of open-source models from 6.7B to 72B. These models exhibit proficiency in following instructions and delivering appropriately formatted responses.\nEvaluation Metrics. Our evaluation uses the unbiased version of Pass@k (Chen et al. 2021) to accurately assess the functional correctness of code snippets generated by LLMs. Following prior work (Zhuo et al. 2024), we report Pass@1 and Pass@5 for the experiment in zero-shot setting and use macro-average as scores. For Pass@1 metric, we use greedy decoding, i.e.set temperature to 0.0. For Pass@5 metric, we opt for the minimum sample size N 5 and maintain temperature at 0.2 and top-p at 0.95. For code generation tasks, we use torch.bfloat16 when loading LLMs and the same prompt detailed in Figure 6.\nEvaluation Process. During the evaluation, to prevent LLMs from failing execution due to omitted import statements, which is a tolerable flaw but could potentially distort assessment results, we implement a corrective measure, i.e., completing the missing dependencies based on the import scenario mentioned in the instruction."}, {"title": "Overall Result", "content": "Table 1 shows LLMs' Pass@1 and Pass@5 against DOMAINEVAL. Columns plotted in blue show the Pass@1/5 values; the bluer, the larger. The columns in orange and green highlight the average (\u201cMean\u201d) and standard deviation (\"Std\") of the corresponding rows, respectively. Overall, the average performance across studied LLMs is similar, ranging from 49.89% ~ 67.13%. At the same time, the performance across domains varies, i.e., the performance in Computation reaches the top among all LLMs, with an average of 82.44% Pass@1 and 88.57% Pass@5, while the worst scores are observed in Cryptography domain, with 33.08% Pass@1 and 40.25% Pass@5. In other words, the performance gaps between different domains are unignorable."}, {"title": "Domain Biases", "content": "From Table 1, we can see significant gaps across six domains. Horizontally, LLMs are generally good at computation tasks while falling short on cryptography and system coding tasks. In particular, LLMs excel in computation domain, where Pass@k metrics all exceed 75%, with some reaching over 90%. When it comes to cryptography and system domains, LLMs exhibit significantly lower performance, with average Pass@1 of 33.08% and 37.50%, respectively. The performance gap can be as much as 68.94% (80.94% - 12.0%) Pass@1 in Llama-2-13b-chat. Vertically, all LLMs exhibit similar domain biases, i.e., LLMs universally show consistent performance gaps with a shared trend of strengths and weaknesses.\nIn addition, a recent work (Zhuo et al. 2024) also explored the coding capability (i.e., using APIs to implement domain-specific code) across various domains and concluded that LLMs are good at cryptography domain. Our conclusion does not conflict with theirs because we generate the domain-specific code while they invoke the domain-specific APIs. In other words, being good at calling APIs in"}, {"title": "LLMS Biases", "content": "Among 12 studied LLMs, the closed-source model GPT-40-mini exhibits the average highest performance, with a 67.13% Pass@5. Qwen2-72B-Instruct-GPTQ-Int4 has the best overall performance among open-source models, with a 64.25% Pass@5. Moreover, considering the variation across domains, GPT-40-mini exhibits the most stable performance, with a 14.75 standard deviation in Pass@5, compared with 15.45 ~ 24.10 of other LLMs.\nNotably, CodeLlama-13b, which fine-tuned from Llama-2-13b, achieves an 11.25% (57.74% - 46.49%) average improvement, while the deviation across domains still remains. It indicates that although fine-tuning can bring about overall improvement, while the domain gaps still exist."}, {"title": "Impact of Generated Samples", "content": "Finally, we analyze the impact of generated samples (i.e., (Pass@1 Greedy Search N=1) with sub-table (Pass@5 Sampling Search N=5)). From Table 1, we can see that after in-"}, {"title": "Case Study", "content": "Despite impressive performance exhibited by LLMs in computation domain, their shortcomings in other domains cannot be overlooked. This claim is supported by two indicative cases from cryptography and system domains, which highlight the challenges faced by LLMs. LLMs need to acquire more background knowledge to enhance their code generation capabilities in specific vertical domains.\nThe response presented in Figure 7 is from GPT-40-mini, which is considered as the premier model within cryptography domain. The error in its response is failed: int too large to convert to float. Similar errors are also observed in some"}, {"title": "Related Work", "content": "To assess the code generation capabilities of models, numerous code benchmarks were introduced such as APPS (Hendrycks et al. 2021) and CodeContests (Li et al. 2022), which are datasets sourced from algorithmic design competitions that emphasize logical thinking. MBPP (Austin et al. 2021) and HumanEval (Chen et al. 2021) are datasets crafted manually for testing. These datasets are designed for evaluation, featuring highly independent functions and simplistic, idealized problem scenarios. Standalone functions are predominantly focused on by these benchmarks; however, non-standalone functions are commonly encountered in pragmatic code generation scenarios (Yu et al. 2024).\nSome other benchmarks attached importance on real-world problems by sourcing data from real scenarios such as StackOverflow, GitHub, and curating data manually to form benchmark datasets. For instance, PandasEval, NumpyEval (Zan et al. 2022), and SecurityEval (Siddiq and Santos 2022) are tailored benchmarks for specific scenarios (Zan et al. 2023). Focusing on specific scenarios is their limitation.\nCoderEval (Yu et al. 2024), ClassEval (Du et al. 2023), DevEval (Li et al. 2024), and ODEX (Wang et al. 2023) were constructed in open domains, but they required significant investment of human labor in the stages of data curating, filtering, or annotation.\nRepoEval (Zhang et al. 2023), MultiPL-E (Cassano et al. 2022), and Exec-CSN (Xie et al. 2024) were constructed with little human involvement. However, RepoEval is focused on repository level task, which differs from ours. MultiPL-E obtained its data by translating other code datasets, and thus, it was not oriented towards open domains. Exec-CSN employed LLMs to curate the CodeSearchNet dataset and generate test cases, resulting in a final dataset that has a gap from the real-world code on GitHub.\nMoreover, previous benchmarks have not explored the difference of code generation capability of LLMs in multiple domains. Though a recent work (Zhuo et al. 2024) exercised capability of LLMs in using APIs from several domains, but its benchmark need human-LLM collaboration to construct. Our DOMAINEVAL benchmark is designed not for tool utilization, but for concrete implementation of APIs and functions. Furthermore, DOMAINEVAL can be constructed through a fully automated pipeline across open domains."}, {"title": "Conclusion", "content": "We introduce DOMAINEVAL, a function code generation benchmark across multiple programming domains. Our research highlights the importance of developing a comprehensive benchmark to compare and assess the code generation abilities of LLMs, both in general and in specific vertical domains. The automated pipeline we introduce not only ensures diversity and real-time updates in the benchmark dataset but also facilitates the construction of custom domain benchmarks for other entities. Our preliminary findings indicate that while LLMs demonstrate remarkable performance in computation domain, their abilities in cryptography and system domains need improvement."}, {"title": "Appendix", "content": ""}]}