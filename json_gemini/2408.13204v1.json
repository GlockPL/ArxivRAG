{"title": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation", "authors": ["Qiming Zhu", "Jialun Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Shing-Chi Cheung"], "abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate capabilities of Large Language Models (LLMs), providing insights into their strengths and weaknesses. However, current benchmarks primarily exercise LLMs' capability on common coding tasks (e.g., bubble sort, greatest common divisor), leaving domain-specific coding tasks (e.g., computation, system, cryptography) unexplored. To fill this gap, we propose a multi-domain code benchmark, DOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our pipeline works in a fully automated manner, enabling a push-bottom construction from code repositories into formatted subjects under study. Interesting findings are observed by evaluating 12 representative LLMs against DOMAINEVAL. We notice that LLMs are generally good at computation tasks while falling short on cryptography and system coding tasks. The performance gap can be as much as 68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more samples can increase the overall performance of LLMs, while the domain bias may even increase. The contributions of this study include a code generation benchmark dataset DOMAINEVAL, encompassing six popular domains, a fully automated pipeline for constructing code benchmarks, and an identification of the limitations of LLMs in code generation tasks based on their performance on DOMAINEVAL, providing directions for future research improvements.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have revolutionized various areas such as question answering (Rogers, Gardner, and Augenstein 2023), math reasoning (Imani, Du, and Srivastava 2023), and especially software development (Lozhkov et al. 2024). Stakeholders are eager to know whether and to what extent LLMs can improve development efficiency.\nTo this end, a variety of code generation benchmarks such as HumanEval (Chen et al. 2021) and MBPP (Austin et al. 2021) have been introduced and intensively used to evaluate LLMs' coding capability. They primarily consist of common coding tasks such as sorting an array or computing the greatest common divisor. Furthermore, to meet more realistic needs, more benchmarks are emerging, expanding mainly along with two perspectives. First, linguistic diversity. This line of work (Wang et al. 2024; Cassano et al. 2022; Chai et al. 2024) mitigates linguistic bias in both natural languages (e.g., English, Chinese) and programming languages (e.g., Python, Java, C/C++). Second, code scale diversity. This line of work aims to scale up the code granularity from function-level (Chen et al. 2021; Yu et al. 2024; Austin et al. 2021) to class-level (Du et al. 2023) and repo-level (Zhang et al. 2023; Cao et al. 2024a; Li et al. 2024).\nHowever, if LLMs are to be applied in real-world industrial scenarios where different product lines prioritize different domains of code, it becomes essential to further understand these LLMs' coding capabilities across various domains. In other words, while existing benchmarks make significant efforts in varying natural/programming languages and code granularity, the LLMs' ability to generate domain-specific code remains under-explored.\nTo fill this gap, we introduce DOMAINEVAL, a multi-domain code generation benchmark. It consists of 2454 subjects (i.e., reference code, description, and context), along with 5892 test cases, covering six domains of code, i.e., computation, network, basic operation, system, visualization, and cryptography. We can see clear functional distinctions among codes from different domains. For example, code in computation involves computational tasks such as mean calculation. Code in network handles network requests/communications and remote connections. Code in cryptography incorporates cryptographic algorithms, encrypting plain-text with a key or conducting key recovery attacks.\nIt is noteworthy that in order to facilitate the benchmark construction and welcome future contributions to DOMAINEVAL, we provide a fully automated test-guided construction pipeline. The pipeline works in a push-bottom manner, i.e., given a code repository, it outputs a set of formatted subjects (i.e., reference code, tests, context, and description). Empowered by this pipeline, DOMAINEval is with exceptional scalability, capable of incorporating the ever-evolving code corpus into it. Moreover, the continuous influx of updated code through this pipeline fortifies DOMAINEVAL against the data contamination threat (Cao et al. 2024b), thereby maintaining its integrity and novelty.\nOur extensive experiments on 10+ LLMs indicate that LLMs are generally good at computation, with an average of 82.44% Pass@1, while falling short on cryptography and system domains, with an average of 33.08% Pass@1 and 37.50% Pass@1, respectively. The performance gap among domains can be as much as 68%+ from Llama-2-13b-chat model, which is observed to have the largest performance variance compared with other LLMs. We also observe that generating more samples can increase the overall performance of LLMs, while the domain bias may even increase. The contributions can be summarized as follows. First, we introduce DOMAINEVAL, a multi-domain code generation benchmark that consists of 2k+ subjects (i.e., description, reference code and tests) covering six domains. Second, we provide a fully automated test-guided construction pipeline to facilitate the benchmark construction and welcome future contributions. Third, we evaluate 10+ LLMs against DOMAINEVAL and yield interesting findings."}, {"title": "Benchmark Construction", "content": "In this section, we introduce the whole construction pipeline of DOMAINEVAL. We provide a fully automated, test-guided construction algorithm that transforms code repositories into a collection of formatted subjects for LLM evaluation. Each subject consists of three components: instruction for LLM evaluation, reference solution, and a series of test cases.\nThe pipeline begins with the collection of raw code snippets from specific domains, which are then systematically transformed into suitable benchmark data. This process involves three key steps: (1) Domain Repository Collection: The first step involves collecting raw code data (R) from selected domains. These snippets are categorized into two groups: Co, containing function code, and To, including tests designed to validate the correctness of the code in Co. (2) Test-Method Matching & Selection: This step aims to create a set of candidate subjects S' = (c, T) from the collected domain code, where each pair consists of a function code c and its corresponding test cases T. To achieve this, we first match each function code c with its corresponding set of test cases T, ensuring they are compatible with the execution environments E. We then filter Co to retain only those code snippets that are executable and accompanied by valid test functions, resulting in a refined set C\u2081 and its associated test cases, which together form the candidate subject set S'. (3) Instruction Generation: For each code snippet c in C1, we employ a LLM to generate corresponding instruction i = LLM(c). The instruction i, along with the associated function code c and test cases T, are combined to create the final benchmark dataset, with each entry represented as (i, c, T), which constitutes a complete benchmark subject."}, {"title": "Domain Repository Collection", "content": "Referring to the work (Zhuo et al. 2024), we select six domains for code generation: computation, network, basic operation, system, visualization, and cryptography, to serve as the domain divisions for DOMAINEVAL.\nTo ensure that DOMAINEVAL closely aligns with the real-world code requirements of human engineers, we chose GitHub repositories as the source of raw data. We cloned a selection of representative code repositories from GitHub, particularly those with at least 100 stars, as these are considered high-quality code data and reflect the actual needs of engineers. The code repositories used in each domain are  Given that each GitHub repository represents a specific real-world application scenario, we classify the code snippets based on the repository's domain. In practice, we use the repository's topic labels and README files to accurately assign the code to the appropriate domain."}, {"title": "Test-Method Matching & Selection", "content": "This section describes how to obtain reference solutions and their corresponding tests from code repositories to construct candidate subjects for LLM evaluation and the automated construction pipeline.\nGiven that code snippets from GitHub repositories are written by human engineers in real-world production environments, they tend to be more complex, with more dependencies and higher levels of encapsulation than standalone functions in some code datasets. This complexity makes it challenging to select function code that can serve as benchmark data while simultaneously acquiring corresponding test cases to form complete candidate subjects.\nTo address this challenge, we propose a test-method tracing strategy aimed at constructing candidate subjects for code benchmarking. Our approach has two main steps: First, we search the code repository for tests related to the reference code and perform Test-method Matching, where each function code c is automatically paired with its corresponding tests T. Second, to ensure the final candidates are suitable for LLM evaluation and can smoothly pass through the automated construction pipeline, we filter the matched results using three criteria: executable, significant, and appropriate difficulty.\nTest-Method Matching To match the function code with corresponding tests and package them into candidate subjects, we start with the test code and trace it back to the associated function code. Specifically, we search for Python code files in repositories and use the library ast to parse Python code into an abstract syntax tree. Then, we sequentially traverse the nodes of the syntax tree, extracting all functions and their class context (if present). Next, we select test code snippets based on two heuristic rules: First, the function or class name of a test should contain test or Test. Second, the test code snippet should contain an assert statement.\nTo match the selected test code with the corresponding function code, we identify ast.Call nodes as function calls.\nTo retrieve the implementation of the called function, we consider two scenarios: If the function and test are in the same file, we traverse the abstract syntax tree to locate and unparse the function node. If the function call spans files, we use recursive path matching by analyzing Python's import behaviors until the specific file is found. Specifically, we continuously retrieve and replace the name of the next level path based on the content of __init__.py and the import statement in from... import... as... format, until the specific Python file is located. Once located, the process is the same as in the first scenario, using ast to parse and identify the function node.\nIn real-world code repositories, the correlation between tests and function code is often not one-to-one. To address this, we identify all functions within the test code and pair them with the corresponding function code. The function code is then used as the reference solution, and we group all related tests into a test suite. This process allows us to package function code snippets from GitHub repositories with their corresponding test cases, creating candidate subjects.\nTest-Method Selection After packaging the candidate subjects, we continue to construct our benchmark dataset. We notice that not every function can directly convert to benchmark data suitable for LLM evaluation. Therefore, to facilitate the automatic construction of code benchmarks, we impose three criteria on the candidate subject.\nFirst, Executable. To use Pass@k (Chen et al. 2021) for evaluation, reference solution code must be executable to verify semantic consistency with generated code against test cases. To ensure the security of our benchmark data, we utilize a sandbox to isolate code execution and maintain a list of banned keywords (Xie et al. 2024), as detailed in the Appendix. If any of these prohibited keywords are detected, we consider the execution and evaluation of such code to be potentially risky and consequently discard it. The test environment first consists of a basic Python environment and necessary packages. To execute a piece of code, it needs adequate dependencies, so we concatenate the required context (e.g., import statements, class context, static variables) for the function. After preparing the context, we run the function code along with its tests.\nSecond, Significant. The code used for LLM evaluation should be important and meaningful, playing a critical role in real-world production scenarios. For example, __init__ functions primarily involve repetitive variable assignments, which are mechanical and lack significance, failing to reflect the capabilities of LLMs in code generation. In contrast, human engineers tend to write tests for code that implements critical functionality. Therefore, we extract the code with tests as candidates.\nThird, Appropriate Difficulty. The number of lines of reference code is one of the direct indicators of the complexity of the code generation task. We set a limit on the function implementation of reference code, i.e., the standard answer, used in the task. We restrict them to between 3 and 100 lines. On the one hand, functions with fewer than three lines typically have overly simple logic  and do not effectively reveal the shortcomings of LLMs. On the other hand, functions exceeding 100 lines may contain overly complex logic, which can present significant challenges for evaluation. These challenges include exceeding LLMs' context limitations or overwhelming their information capacity, making it difficult to generate precise instructions in subsequent steps and thus hindering the automatic construction pipeline."}, {"title": "Instruction Generation", "content": "After filtering the candidate subjects, our subjects still lack the instruction field. To ensure the reproducibility of the dataset construction process, we employ open-source LLM, i.e.Qwen2-72B-Instruct-GPTQ-Int4, for data generation. We utilize a dialogue prompt template to guide the LLM in generating detailed Instruction fields for each validated subject. These instruction fields comprehensively describe the function, including its purpose, input arguments, and expected outputs, which serve as the input for evaluating the code generation capabilities of LLMs. By leveraging LLMs, we can generate natural language descriptions that outline the desired functionality, inputs, and outputs of the code (Xie et al. 2024).\nFinally, to construct the template for code generation, we mask the function code, following prior work (Xie et al. 2024), by [MASK] and instruct LLMs to complete the masked segments during the evaluation."}, {"title": "Benchmark Statistics", "content": "The above pipeline generates instructions, reference code, and a series of tests which combine to form complete subjects. As a result, DOMAINEVAL consists of 2454 code subjects. shows the number of subjects constructed from repositories across different domains. Overall, the number of subjects from each domain is at least 100. The computation domain encompasses the greatest number of subjects, with 1705 subjects, compared to 100 ~ 256 subjects in other domains. This is because the computation-related repositories offer more function code and also include a larger number of test cases to ensure the accuracy of each computational operation."}, {"title": "Experiment Setup", "content": "Studied LLMs. We assess 12 representative instruction-tuned LLMs against DOMAINEVAL, including GPT-3.5-turbo, GPT-40-mini , Qwen2 , Phi-3 , DeepSeek-Coder series , Llama2 and CodeLlama series , CodeQwen1.5 with size of open-source models from 6.7B to 72B. These models exhibit proficiency in following instructions and delivering appropriately formatted responses.\nEvaluation Metrics. Our evaluation uses the unbiased version of Pass@k  to accurately assess the functional correctness of code snippets generated by LLMs. Following prior work , we report Pass@1 and Pass@5 for the experiment in zero-shot setting and use macro-average as scores. For Pass@1 metric, we use greedy decoding, i.e.set temperature to 0.0. For Pass@5 metric, we opt for the minimum sample size N = 5 and maintain temperature at 0.2 and top-p at 0.95. For code generation tasks, we use torch.bfloat16 when loading LLMs and the same prompt detailed. \nEvaluation Process. During the evaluation, to prevent LLMs from failing execution due to omitted import statements, which is a tolerable flaw but could potentially distort assessment results, we implement a corrective measure, i.e., completing the missing dependencies based on the import scenario mentioned in the instruction."}, {"title": "Overall Result", "content": "Table 1 shows LLMs' Pass@1 and Pass@5 against DOMAINEVAL. Overall, the average performance across studied LLMs is similar, ranging from 49.89% ~ 67.13%. At the same time, the performance across domains varies, i.e., the performance in Computation reaches the top among all LLMs, with an average of 82.44% Pass@1 and 88.57% Pass@5, while the worst scores are observed in Cryptography domain, with 33.08% Pass@1 and 40.25% Pass@5. In other words, the performance gaps between different domains are unignorable."}, {"title": "Domain Biases", "content": "From Table 1, we can see significant gaps across six domains. Horizontally, LLMs are generally good at computation tasks while falling short on cryptography and system coding tasks. In particular, LLMs excel in computation domain, where Pass@k metrics all exceed 75%, with some reaching over 90%. When it comes to cryptography and system domains, LLMs exhibit significantly lower performance, with average Pass@1 of 33.08% and 37.50%, respectively. The performance gap can be as much as 68.94% (80.94% - 12.0%) Pass@1 in Llama-2-13b-chat. Vertically, all LLMs exhibit similar domain biases, i.e., LLMs universally show consistent performance gaps with a shared trend of strengths and weaknesses.\nIn addition, a recent work also explored the coding capability (i.e., using APIs to implement domain-specific code) across various domains and concluded that LLMs are good at cryptography domain. Our conclusion does not conflict with theirs because we generate the domain-specific code while they invoke the domain-specific APIs. In other words, being good at calling APIs in a domain does not mean being good at implementing code in the domain. Therefore, our finding serves as a supplement to previous work ."}, {"title": "LLMS Biases", "content": "Among 12 studied LLMs, the closed-source model GPT-40-mini exhibits the average highest performance, with a 67.13% Pass@5. Qwen2-72B-Instruct-GPTQ-Int4 has the best overall performance among open-source models, with a 64.25% Pass@5. Moreover, considering the variation across domains, GPT-40-mini exhibits the most stable performance, with a 14.75 standard deviation in Pass@5, compared with 15.45 ~ 24.10 of other LLMs.\nNotably, CodeLlama-13b, which fine-tuned from Llama-2-13b, achieves an 11.25% (57.74% - 46.49%) average improvement, while the deviation across domains still remains. It indicates that although fine-tuning can bring about overall improvement, while the domain gaps still exist."}, {"title": "Impact of Generated Samples", "content": "Finally, we analyze the impact of generated samples . From Table 1, we can see that after increasing the number of samples from 1 to 5, the average performance increases from 53.42% to 59.60%, with consistent improvements on all six domains. Yet, in terms of standard deviation (the smaller, the less bias, the better), there is little improvement, from an average 18.33 to 17.72. What is worse, CodeLlama-13b-instruct even observes an increased deviation, from 19.90 to 20.55, indicating a more bias as generation goes on. In other words, generating more samples can increase the overall performance, while the domain bias may even increase."}, {"title": "Case Study", "content": "Despite impressive performance exhibited by LLMs in computation domain, their shortcomings in other domains cannot be overlooked. This claim is supported by two indicative cases from cryptography and system domains, which highlight the challenges faced by LLMs. LLMs need to acquire more background knowledge to enhance their code generation capabilities in specific vertical domains.\nThe response is from GPT-40-mini, which is considered as the premier model within cryptography domain. The error in its response is failed: int too large to convert to float. Similar errors are also observed in some responses from DeepSeek and Qwen series models.\nThe context of this subject is a classical cryptography scenario, the attack method targeting the RSA encryption algorithm. This involves the recovery of the two prime factors, p and q, of the RSA modulus N, given the public key (consisting of the modulus N and the public exponent e) and the private key (the private exponent d). When encrypting, in order to enhance the security and impede decryption attempts, the selected p and q by human are both extremely large prime numbers. Although the instruction mentioned keywords such as RSA, the model does not realize that in the context of cryptography, rounding the square root of a large number cannot be directly converted to **0.5, and instead math.isqrt should be used to avoid OverflowError.\nIn the same way, although GPT-40-mini exhibits the best performance in system domain, the function is designed to parse the output of free command (run with -b option) to determine the physical memory state on a Linux system. However, the model fails to ensure that the output format of free command remains consistent across different language and regional settings, resulting in incorrect string matching during subsequent parsing. Similar errors are observed in models such as CodeQwen1.5-7B-Chat, Qwen2-72B-Instruct-GPTQ-Int4, Phi-3-medium-4k-instruct, the DeepSeek series, and the CodeLlama series. Additionally, GPT-40-mini lacks sufficient understanding of free command, leading to misjudgments regarding the position of shared. Ultimately, none of LLMs are able to pass this subject."}, {"title": "Related Work", "content": "To assess the code generation capabilities of models, numerous code benchmarks were introduced such as APPS and CodeContests , which are datasets sourced from algorithmic design competitions that emphasize logical thinking. MBPP and HumanEval are datasets crafted manually for testing. These datasets are designed for evaluation, featuring highly independent functions and simplistic, idealized problem scenarios. Standalone functions are predominantly focused on by these benchmarks; however, non-standalone functions are commonly encountered in pragmatic code generation scenarios .\nSome other benchmarks attached importance on real-world problems by sourcing data from real scenarios such as StackOverflow, GitHub, and curating data manually to form benchmark datasets. For instance, PandasEval, NumpyEval , and SecurityEval  are tailored benchmarks for specific scenarios . Focusing on specific scenarios is their limitation.\nCoderEval , ClassEval , DevEval , and ODEX were constructed in open domains, but they required significant investment of human labor in the stages of data curating, filtering, or annotation.\nRepoEval , MultiPL-E , and Exec-CSN were constructed with little human involvement. However, RepoEval is focused on repository level task, which differs from ours. MultiPL-E obtained its data by translating other code datasets, and thus, it was not oriented towards open domains. Exec-CSN employed LLMs to curate the CodeSearchNet dataset and generate test cases, resulting in a final dataset that has a gap from the real-world code on GitHub.\nMoreover, previous benchmarks have not explored the difference of code generation capability of LLMs in multiple domains. Though a recent work exercised capability of LLMs in using APIs from several domains, but its benchmark need human-LLM collaboration to construct. Our DOMAINEVAL benchmark is designed not for tool utilization, but for concrete implementation of APIs and functions. Furthermore, DOMAINEVAL can be constructed through a fully automated pipeline across open domains."}, {"title": "Conclusion", "content": "We introduce DOMAINEVAL, a function code generation benchmark across multiple programming domains. Our research highlights the importance of developing a comprehensive benchmark to compare and assess the code generation abilities of LLMs, both in general and in specific vertical domains. The automated pipeline we introduce not only ensures diversity and real-time updates in the benchmark dataset but also facilitates the construction of custom domain benchmarks for other entities. Our preliminary findings indicate that while LLMs demonstrate remarkable performance in computation domain, their abilities in cryptography and system domains need improvement."}, {"title": "Appendix", "content": "In the subsection Test-Method Matching & Selection, we maintain a list of banned keywords to mitigate potential attacks that the evaluation system may suffer during code execution. If any of banned keyword appears in the code, we choose to discard this code data to prevent potential risks."}]}