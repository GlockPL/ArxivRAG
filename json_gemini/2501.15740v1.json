{"title": "Propositional Interpretability in Artificial Intelligence", "authors": ["David J. Chalmers"], "abstract": "Mechanistic interpretability is the program of explaining what AI systems are doing in terms of their internal mechanisms. I analyze some aspects of the program, along with setting out some concrete challenges and assessing progress to date. I argue for the importance of propositional interpretability, which involves interpreting a system's mechanisms and behavior in terms of propositional attitudes: attitudes (such as belief, desire, or subjective probability) to propositions (e.g. the proposition that it is hot outside). Propositional attitudes are the central way that we interpret and explain human beings and they are likely to be central in AI too. A central challenge is what I call thought logging: creating systems that log all of the relevant propositional attitudes in an AI system over time. I examine currently popular methods of interpretability (such as probing, sparse auto-encoders, and chain of thought methods) as well as philosophical methods of interpretation (including those grounded in psychosemantics) to assess their strengths and weaknesses as methods of propositional interpretability.", "sections": [{"title": "Introduction", "content": "Mechanistic interpretability is one of the most exciting and important research programs in current AI. As I understand it, interpretability (in a broad sense) is the practice of explaining what AI systems are doing in terms a human can understand. Mechanistic interpretability focuses on explaining what AI systems are doing in terms of their internal mechanisms.\nMechanistic interpretability is important for a number of reasons. In Al safety, mechanistic interpretability might help us to know an AI system's goals and plans by examining its internal processes. In Al ethics, mechanistic interpretability might allow us to better understand an AI system's reasons for its decisions and the biases that enter into them. In Al cognitive science,"}, {"title": "Varieties of interpretability", "content": "I'll start by breaking down the varieties of interpretability in a fine-grained way, to clarify the area and to make clearer where propositional interpretability fits in.\nStarting at the beginning: we might say that to interpret an AI system is to explain what the system is doing in human-understandable terms. Interpretability is a name for the practice of interpreting AI systems.\nI have defined interpretability in terms of explanation, and the terms interpretability and explainability are sometimes used interchangeably. But it is also common for the terms to be used differently. There are various ways to understand the difference, but perhaps the core difference for my purposes stems from a difference in who a \u201chuman-understandable\" explanation is directed at.\nExplainability is explanation for ordinary humans. It is especially directed at end users and others affected by an AI system's decisions. For example, explainability might involve explaining to a doctor or a patient why the patient was diagnosed with a given disease. Explainability typically involves explanations in nontechnical terms.\nInterpretability (in the narrower sense) is explanation for theorists. It is especially directed at scientists and AI researchers who are trying to understand what an AI system is doing. For example, interpretability might involve explaining how a language model learns from data and how it encodes a model of the world. While explainability is typically nontechnical, interpretability is frequently quite technical.\nBoth of these projects are important, but my focus here is on interpretability, not explain-ability. Interpretability divides in turn into at least two classes. Behavioral interpretability analyzes an Al system's input/output behavior to understand what the system is doing. Mechanistic interpretability analyzes an AI systems internal mechanisms to help explain (for theorists) what the system is doing.\nI'll focus mainly on mechanistic interpretability, which can itself be subdivided. Algo-"}, {"title": "Propositional attitudes", "content": "What are propositions, and what are propositional attitudes?\nOn one standard view, propositions are structured entities that are composed of concepts: for example, the proposition the Golden Gate Bridge is large is composed of concepts such as Golden Gate bridge and red. (I'll consider other views, such as the view that propositions are sets of possible worlds, shortly.) It is widely held that sentences in natural languages (or"}, {"title": "Radical interpretation", "content": "These interpretability projects all have analogs in the human case. Humans have been in-terpreting other humans since the human species began. Ordinary people use patterns in behavior to explain what other people are doing, most often in propositional terms. Scientists and philosophers have used theoretical tools to explain human behavior for at least thousands"}, {"title": "Thought Logging", "content": "A concrete challenge for research in propositional interpretability is to construct a thought logging system: a system that logs all (or as many as possible) of an AI system's propositional attitudes over time. A thought logging system is a meta-system that takes a specification of the algorithmic facts about an Al system as input (perhaps along with relevant environmental facts) and produces a list of the system's current and ongoing propositional attitudes as outputs.\nA log (in an ultra-simple form) might look something like this:\nGoal: I win this game of chess.\nJudge (credence 0.8): If I move Qf8, I will win\nGoal: I move Qf8.\nAction: I move Qf8.\nNow, it is likely that a given AI system may have an infinite number of propositional attitudes, in which case a full log will be impossible. For example, if a system believes a proposition p, it arguably dispositionally believes p-or-q for all q. One could perhaps narrow down to a finite list by restricting the log to occurrent propositional attitudes, such as active judgments. Alternatively, we could require the system to log the most significant propositional attitudes on some scale, or to use a search/query process to log all propositional attitudes that meet a certain criterion.\nInterestingly, in \"Radical Interpretation\", Lewis offered a possible format for entries in a thought log."}, {"title": "Psychosemantics", "content": "Why think that thought logging is is possible? One key reason arises from psychoseman-tic theories, which have been developed by philosophers and cognitive scientists in recent decades. Psychosemantics (so named in Jerry Fodor's 1987 book of the same name) can be understood by analogy with linguistic semantics (the semantics of natural languages). Lin-guistic semantics involves theories of the meaning or content of linguistic expressions (e.g. sentences), perhaps as uttered in various contexts. By analogy, psychosemantics involves the-ories of the meaning or content of mental states (e.g. beliefs and desires). One key part of psychosemantics aims to give physical conditions for having propositional attitudes.\nIn the case of linguistic semantics, we can distinguish semantics from metasemantics. Where semantics offers theories of what the meanings or contents of various expressions are, metasemantics involve theories of the conditions in virtue of which linguistic expressions have the meanings or contents that they do. For example, semantics tells us that '+' means addition, perhaps in some technical guise, while metasemantics might tell us that it is in virtue of the way '+' is used in the community that it means addition.\nIn the case of psychosemantics, a similar distinction applies. The semantic branch of psychosemantics offers theories of what the meanings or contents of mental states are. The metasemantic branch of psychosemantics, involves theories of the conditions in virtue of which mental states have the meanings and contents they do. For example, the semantic branch of psychosemantics might tell us that a certain type of neuron represents edges, while the metasemantic branch tells us that it is in virtue of causal connections between the neuron and edges that the neuron represents edges.\nIn principle, psychosemantics should offer us a theory of propositional attitudes. The semantic part of the theory should involve a theory of what the contents of these attitudes are (e.g. propositions). The metasemantic part of the theory should offer us a theory of the conditions under which subjects have a given propositional attitude that is, the conditions under which they have an attitude directed as a given proposition.\nIn the case of human and animal subjects, the relevant conditions will typically be physical conditions (brain processes, behavior, connections to the environment, and more. In the case of AI systems, the relevant conditions may be algorithmic conditions (network structure and activity, and so on), plus relevant environmental conditions."}, {"title": "Current methods for propositional interpretability", "content": "There are a number of popular methods in mechanistic interpretability that can support a form of propositional interpretability. These include causal tracing, probing with classifiers, sparse auto-encoders, and chain of thought methods. I will examine each of these methods to see its strengths and weaknesses as a method of propositional interpretability, and how it might be extended to yield such a method."}, {"title": "Causal tracing", "content": "Causal tracing is a widely-used method for localizing \u201cfacts\" or \"knowledge\" in a neural network. In perhaps the best-known use of this method, Meng et al (2022) localize the rep-resentation of The Eiffel Tower is in Paris in GPT-J, a large language model. The network is first given an input such as \u201cThe Eiffel Tower is in ...\", for which it outputs \"Paris\". They then corrupt the input activations corresponding to \u201cThe Eiffel Tower\u201d, which corrupts later processes so that the output is no longer \"Paris\". In these corrupted later processes, they re-store the original \"clean\" activations from the original run (a technique known as \u201cactivation patching\"), determining which layers are most important to restoring the output \u201cParis\u201d. They typically find (unsurprisingly) that the final token in the last layer before the output is the most important for producing \"Paris\", but after that they typically find that certain activations in a certain middle layer are most important. This tends to suggest that this middle layer is crucial to representing The Eiffel Tower is in Paris.\nThis method can be extended into a method of \"model editing\", which in effect edits a system's beliefs. Researchers focus on the relevant middle layer and fine-tune it so that it tends to produce \u201cRome\u201d rather than \u201cParis\u201d. The resulting network produces outputs such as \"The Eiffel Tower is in Rome\", and (more interestingly) produces related outputs, such as advice to fly to Rome if you want to see the Eiffel Tower.\nPsychosemantically, the causal tracing method relies almost wholly on use rather than information as a criterion for what is represented. An activity pattern counts as representing The Eiffel Tower is in Paris in virtue of its effects on downstream outputs (such as \u201cParis\u201d), with no role for information (correlations with upstream states affecting inputs).\nThis method is clearly a form of propositional interpretability. As such it has a number of limitations.\nRobustness (Hoelscher-Obermaier 2022, Thibodeau 2022): The representation of facts such as The Eiffel Tower is in Rome seems quite fragile and prompt-dependent. For example, it seems to work in one direction but not another: the input \"Rome has a tower called ...\" does not yield \"The Eiffel Tower\" as an output. It also seems sensitive to words rather than con-cepts: \"Cheese\" and \"Fromage\" get handled in quite separate ways. So this method appears"}, {"title": "Probing with classifiers", "content": "Decoding activity using trained classifiers (or probes) is anoher method for localizing repre-sentation in both artificial and biological neural networks. To find whether a given set of units represents a feature such as cat, we train a (typically linear) classifier to classify activity pat-terns in those units in order to distinguish those brought about by pictures of cats from those brought about from pictures of non-cats. If the classifier performs very well, then it appears that information about cats is strongly encoded in the activity patterns, and we say that these units represent the feature cat.\nAs described, probing yields conceptual interpretability (cat) rather than propositional interpretability (the cat sat on the mat). But it is also possible to use probing to decode propositional content. For example, Belinda Li et al (2021) took a network trained on inputs about a mini-world, such as \"The key is in the chest\", and trained probes to determine the truth-value of propositions such as contains(chest,key) in the mini-world. The success of this probe in certain areas of the network at least tends to suggest that those areas might represent this proposition.\nLikewise, Kenneth Li et al (2023) trained a network to play the board game Othello and then used probes to decode the state of the board (which tiles are on which squares) from network activity. For example, they trained a probe to determine whether There is a black tile on e4. They found that the probe could distinguish activity patterns where this proposition is true from those where it is false. This suggests that the state of the board is encoded by activity vectors in the system. In effect, the system has propositional attitudes modeling the board as containing black tiles and white tiles at various positions and empty squares elsewhere.\nOne objection to probing methods is that correlations are cheap and don't guarantee that the relevent state of affairs is being specifically represented (see e.g. Belinkov 2022). How-ever, probes can often be combined with interventions to provide further evidence. For exam-ple, one can alter an activity pattern that corresponds (by linear probing) to Black tile on e4 to"}, {"title": "Sparse auto-encoders", "content": "There has been a recent explosion of interpretability work using sparse auto-encoders to gen-erate features that may be active or represented in large language models. Perhaps the most well-known is a 2024 paper on \"Scaling Monosemanticity\" (Templeton et al 2024), which uses sparse auto-encoders to analyze representations in Claude 3 Sonnet, one of the leading language models currently in use. The paper is advertised as \"Mapping the Mind of a Large Language Model\", saying \"We have identified how millions of concepts are represented inside Claude 3 Sonnet\".\nA sparse auto-encoder is a two-layer neural network that takes certain activation vectors as input and is trained to produce the same vectors as output. The middle layer is constrained to be a sparse vector, where most activations are zero. In effect this system encodes the original vector as a sparse vector, from which the original vector can be decoded in turn.\nIn principle one can use sparse auto-encoders to encode any layer of a neural network. The 2024 paper encoded a central intermediate layer in Claude 3 Sonnet. The system's residual stream has somewhere over 10,000 units per token of input (the exact parameters are pro-prietary). At a given time, the state of the residual stream can be represented as (let's say) a 10,000-dimensional vector, with different values for each unit. A sparse auto-encoder is trained to encode the state of the residual stream. The most powerful auto-encoder used in the study has 34 million units, of which only around 100 units are active at a given time. In effect, the 10,000-dimensional residual stream is now encoded as a sparse vector in which only 100 units out of 34 million are active at a given time.\nIt is a natural hypothesis that many of these 34 million units will correspond to inter-pretable \"features\" or \"concepts\". In fact, this is what researchers find. Just under half of the units seem to be interpretable, though this result is somewhat complicated by the fact that Claude itself is doing the interpreting.\nOne much-discussed unit appears to be devoted to the Golden Gate bridge. It is triggered especially by text passages mentioning the bridge and my pictures of the bridge. Furthermore, when activity corresponding to this unit is amplified, Claude starts talking obsessively about the Golden Gate bridge. In the original transformer, this unit corresponds to a certain direction in activation space in the transformer's residual stream. The behavior leads researchers to hypothesize that this direction in activation space corresponds to a concept of the Golden Gate bridge.\nThe same goes for many other units. They seem to correspond to concepts such as Rwanda, neuroscience, Rosalind Franklin, sadness, sycophancy, and millions of others. Not every concept that one would expect to be present is found, but many are. For example, just over half of the 32 boroughs of London seem to have sparse units of their own. It is possible"}, {"title": "Chain of thought methods", "content": "There has been recent excitement about \u201cchain of thought\" methods for reasoning. In these methods, language models are trained or asked to \u201cthink out loud\" by asserting intermediate conclusions. Even simple prompting along these lines can significantly improve these models' performance on reasoning tasks. More recent systems (such as STaR, the Self-Taught Rea-soner from Zelikman et al (2022), and the 2024 o1 system from OpenAI) have built in chain of thought methods at a deeper level, automatically generating chains of thoughts before ev-ery response. Evaluations of these chains can be used for reinforcement learning, leading to increasingly strong performance at reasoning tasks in mathematics and elsewhere.\nChain of thought models are doing something that seems at least analogous to human \"thinking out loud\", where intermediate steps that are spoken feed back into reasoning. Some models (e.g. Quiet-STaR, in Zelikman 2024) use intermediate steps without producing them as output, in a way that is reminiscent of human \u201cinner speech\" (see Buckner 2025 and Mann and Gregory 2024 for some philosophical discussion).\nWhen humans think out loud in dealing with a reasoning task, this often (not always!) gives some insight into their reasoning processes. It is natural to hope that chain of thought"}, {"title": "Objections and challenges", "content": "Al systems don't have propositional attitudes.\nOne natural objection to the whole project is to say that Al systems can't have proposi-tional attitudes. Perhaps this is because there is some X such that X is required for proposi-tional attitudes and AI systems lack X: perhaps X = consciousness, or free will, or concepts, or understanding. Or perhaps it is just because propositional attitudes are mental states and AI systems have no mental states because they lack minds.\nAs I suggested earlier, objections of this sort can be evaded by adopting a project of nonmentalistic interpretability: understand (generalized) propositional attitudes in such a way that they don't require minds. There is clearly some sense in which AI systems (thermostats too) have goals and representations, even if they don't have beliefs, desires, consciousness, free will, and the rest. We can stipulate a notion of generalized propositional attitude that doesn't have these demanding requirements. Attitudes of this sort may still play a crucial role in predicting and explaining an AI system's behavior, while bypassing many debates over AI minds. This is perhaps a pragmatic path for AI researchers who wish to avoid philosophical debates.\nAt the same time, there is also an important project of mentalistic interpretability: using interpretability methods to help determine whether AI systems have genuine mental states such as beliefs and desires. At this point philosophical questions arise. For example: just what is required to have genuine beliefs and desires? My own view is that there are multiple notions (belief1, belief2, and so on) in the vicinity of each these terms, and disputes over which is really belief will be at least partly verbal. But there are also potentially substantive disputes about just which systems can have mental states of each sort. In any case, mentalistic interpretability may provide a constructive venue for some debates over whether AI systems can have minds.\nPropositional attitudes are the wrong explanatory framework for AI. As noted earlier, some philosophers have suggested that propositional attitudes should be discarded from sci-ence as they are elements of a primitive and outmoded theoretical framework. Whether or not this is right for the case of humans, one could argue that it is especially plausible for the case of Al systems, which are very different from humans and may require an explanatory framework of their own.\nMy view is that even if categories such as belief and desire are suboptimal, representa-tional notions more generally are extraordinarily useful in explaining both humans and AI systems. It is hard to explain either without invoking notions in the vicinity of goals and mod-els. So while we may end up dispensing with some traditional propositional attitudes, I think generalized propositional attitudes of some sort are here to stay.\nAl psychology may be quite unlike human psychology. Even if we end up appealing to propositional attitudes such as belief and desire to explain Al systems, it may well be that psychological principles about these propositional attitudes that hold true in the human case do not hold true for a given AI system. Quite different psychological principles may apply, and applying human psychology may be misleading.\nThat said, it is reasonable to expect AI systems to at least obey some version of the belief-"}, {"title": "Conclusion", "content": ""}]}