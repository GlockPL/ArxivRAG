{"title": "ON THE RELATION BETWEEN LINEAR DIFFUSION AND\nPOWER ITERATION", "authors": ["Dana Weitzner", "Mauricio Delbracio", "Peyman Milanfar", "Raja Giryes"], "abstract": "Recently, diffusion models have gained popularity due to their impressive gener-\native abilities. These models learn the implicit distribution given by the training\ndataset, and sample new data by transforming random noise through the reverse\nprocess, which can be thought of as gradual denoising. In this work, we examine\nthe generation process as a \"correlation machine\", where random noise is repeat-\nedly enhanced in correlation with the implicit given distribution. To this end, we\nexplore the linear case, where the optimal denoiser in the MSE sense is known to\nbe the PCA projection. This enables us to connect the theory of diffusion models\nto the spiked covariance model, where the dependence of the denoiser on the noise\nlevel and the amount of training data can be expressed analytically, in the rank-1\ncase. In a series of numerical experiments, we extend this result to general low\nrank data, and show that low frequencies emerge earlier in the generation process,\nwhere the denoising basis vectors are more aligned to the true data with a rate\ndepending on their eigenvalues. This model allows us to show that the linear dif-\nfusion model converges in mean to the leading eigenvector of the underlying data,\nsimilarly to the prevalent power iteration method. Finally, we empirically demon-\nstrate the applicability of our findings beyond the linear case, in the Jacobians of\na deep, non-linear denoiser, used in general image generation tasks.", "sections": [{"title": "INTRODUCTION", "content": "Recently, diffusion models have gained much popularity as very successful generative models,\nshowcasing impressive performance in image generation tasks (Dhariwal & Nichol, 2021; Ho et al.,\n2020; Song & Ermon, 2019; Song et al., 2021c). These models learn the implicit distribution given\nby the training dataset, and sample new data by transforming random noise inputs through a reverse\ndiffusion process, which can be thought of as gradual denoising. More formally, it has been shown in\nKadkhodaie et al. (2024) that learning the underlying distribution is equivalent to optimal denoising\nat all noise levels.\nIn order to shed more light onto the mechanism behind the success of diffusion models, in this\nwork we analyze the behavior of denoisers in the context of image generation, where pure noise is\ngradually processed into a sample from a given (implicit) distribution by gradual denoising. Unlike\nother works, e.g. Kadkhodaie et al. (2024), we focus on the denoiser(s) throughout the generation\nprocess, and not only on the final generated data.\nTo this end, we suggest the following simple model to illustrate our point. Consider the class of\nlinear denoisers, where the optimal denoiser is given by a PCA projection. To simulate the diffusion\ngeneration process, we learn a series of projections onto noisy data at different noise levels, and\nuse them to transform pure noise into samples from the underlying distribution. Given this simple\nmodel we can inspect the evolution of eigenvectors spanning gradual projections with decreasing\nnoise levels, as well as the distribution of the generated data samples.\nWe show that the correlation of the noisy basis eigenvectors with their clean version decays as the\nnoise level increases, with a rate determined by the eigenvalues and the size of the training dataset.\nIn other words, we show that low frequencies, corresponding to large eigenvalues, emerge earlier\nin the reverse process as was empirically observed in (Ho et al., 2020), and analyze how more"}, {"title": "BACKGROUND AND RELATED WORK", "content": "Since their introduction by Sohl-Dickstein et al. (2015), diffusion models have been vastly used in\nimage generation tasks (Dhariwal & Nichol, 2021; Ho et al., 2020; Song & Ermon, 2019; Song et al.,\n2021c), more general computer vision tasks (Amit et al., 2021; Baranchuk et al., 2022; Brempong\net al., 2022; Cai et al., 2020), and in other domains such as natural language processing (Austin et al.,\n2021; Hoogeboom et al., 2021; Li et al., 2022; Savinov et al., 2022; Yu et al., 2022) and temporal\ndata modeling (Alcaraz & Strodthoff, 2023; Chen et al., 2021; Kong et al., 2021; Rasul et al., 2021;\nTashiro et al., 2021). On top of their practical success, different flavors of training and sampling\nhave risen based on interesting theoretical reasoning, e.g., considering the statistical properties of\nthe intermediate data (Song et al., 2021a; Sohl-Dickstein et al., 2015), or by framing the problem in\nthe form of stochastic differential equations (SDEs) (Karras et al., 2022; Song et al., 2021b;c; Chen\net al., 2024) or score based generative models (Song & Ermon, 2019; 2020). In this work, we look\nat diffusion models in the context of iterative denoising, and focus on the properties of the learned\ndenoiser (Milanfar & Delbracio, 2024).\nRecently Kadkhodaie et al. (2024) showed that the learned denoising functions are equivalent to a\nshrinkage operation in a basis adapted to the underlying image. In this sense the diffusion denoiser is\nan adaptive filter (Milanfar, 2013; Talebi & Milanfar, 2014; 2016). While they focus on the analysis\nof the nonlinear denoiser at the point of the final generated data, we are interested in the evolution\n(adaptation) of the denoiser throughout the generation process, and its dependence on the noise level.\nTo this end, we suggest a simple linear denoising model, presented in Section 3. In this case, the\n(optimal) denoiser does depend on the underlying image, and its dependence on the noise level can\nbe traced analytically, as we show hereafter.\nDue to their phenomenal empirical success, some attempts have been devoted towards providing\ntheory supporting the sample and iteration complexity of diffusion models. The current body of work\ncan be generally parted to attaining iteration complexity bounds assuming approximately accurate\nscores (Li et al., 2024b;a; Chen et al., 2023b; Huang et al., 2024; Benton et al., 2024), and to\nassessing the sample complexity to learn the score functions (Chen et al., 2023a; Block et al., 2020;\nBiroli & M\u00e9zard, 2023). Among these works, many assume a low dimensional data distribution\n(Bortoli, 2022; Li & Yan, 2024; Oko et al., 2023; Chen et al., 2023a; Wang et al., 2024), which is a\nreasonable assumption in theoretical works. Yet, it might particularly explain the gap between the\ncurrent iteration bounds and the much lower complexity apparent in practice (Li & Yan, 2024). In\nour work, we consider linear models and deduce a linear sample complexity bound associated with\nlearning the score function in Sec. 4 and discuss the tradeoffs of the synthesis conversion rate in Sec.\n4.1. The previous works mentioned above mainly develop bounds assuming specific samplers and\nscaling details, which differ from our setting. In addition, they generally bound the Total Variation\ndistance (under varying assumptions on the target distributions), which is not trivial to translate to\nthe generated covariance matrix that we focus on even in the linear Gaussian case (Devroye et al.,\n2018). The difference in our setting enables us to connect the theory of diffusion models to a broad\nbody of work concerning the spiked covariance model (Johnstone, 2001), and supports the analysis\nof denoising diffusion as a correlation machine, which is the main purpose of this paper.\nIn the setting of Statistical Mechanics, Biroli & M\u00e9zard (2023) analyse diffusion models in very\nlarge dimensions, focusing on the Curie-Weiss model of ferromagnetism. As an introduction to\ntheir work, they also discuss a simple linear score model, in the context of the sample complexity\nof learning the score function. They focus their discussion on the case of Gaussian data, where\nthe eigenvalues of the covariance matrices can be typically characterised. Unlike their work, we"}, {"title": "LINEAR DIFFUSION - PROBLEM SETUP", "content": "For our analysis, we define the following simple iterative linear generation model. First, define the\nstandard diffusion model. Let q denote the natural data distribution and let $x_0 \\sim q$ be a sample from\nthe natural data ($x \\in \\mathbb{R}^d$). The forward (diffusion) process is defined (Ho et al., 2020) by\n$q(x_t|x_{t-1}) = \\mathcal{N}(\\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$\n(1)\nfor some fixed noise schedule $\\{\\beta_t\\}_{t=1}^T$ and $x_0 \\sim q$. It can be shown that\n$q(x_t|x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$,\n(2)\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$. For our simplified model, consider the process (without\nscaling),\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_{t-1}, \\sigma_t^2I)$.\n(3)\nThis implies that $X_t = X_{t-1} + \\epsilon_{\\sigma_t}$, where $\\epsilon_{\\sigma_t} \\sim \\mathcal{N}(0, \\sigma_t^2I)$ for some fixed noise schedule $\\{\\sigma_t\\}_{t=1}^T$.\nWe discard the scaling to comply with previous analysis of the spiked covariance model (Nadler,\n2008) (more details in Section 4). This corresponds to the \"Exploding Variance\u201d formulation, used\nwith Langevin dynamics to sample data as a variant of score based diffusion models (Song & Ermon,\n2019; Song et al., 2021c; Song & Ermon, 2020). We choose to present the \"standard\" diffusion\nmodels in the setting of denoising diffusion Ho et al. (2020) and not using the score-based approach\nentirely, as we focus our discussion on the qualities of the denoiser.\nThe reverse (generation) process is defined using a parameterized distribution model $p_{\\theta}$, generally\ndefined by the Markov process\n$p_{\\theta}(x_{0:T}) = p(x_T)\\prod_{t=1}^T p_{\\theta}(x_{t-1}|x_t)$,\n(4)\n$p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(\\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t,t))$,\n(5)\nwhere $p(x_T) = \\mathcal{N}(0,I)$. By choices of parametrization and loss manipulations (see (Ho et al.,\n2020)), one generally learns to estimate the error $\\epsilon_{\\theta}(x_t, t)$, where\n$\\mu_{\\theta}(x_t,t) = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t, t))$,\n$\\Sigma_{\\theta}(x_t,t) = \\epsilon_t^2 I$, and $\\epsilon_t$ is a designed schedule (usually chosen to be equal to $\\sigma_t$). Thus, the reverse\nprocess can be expressed as a denoising chain\n$D_t(x_t) = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t, t)) + \\epsilon_t z$,\n(7)\nwhere $z \\sim \\mathcal{N}(0,I)$ and $z_1 = 0$. This is a stochastic denoiser, which preserves the Markovian\nproperty of the forward process. Later versions suggested similar (non Markovian) deterministic\ndenoisers, e.g., DDIM (Song et al., 2021a), or more general stochastic denoiser chains, for a contin-\nuous forward model (InDI (Delbracio & Milanfar, 2023)).\nIn our case, we restrict the denoisers to be a linear function of $x_t$. Thus, the optimal denoiser (in\nthe $l_2$ sense) is given by the PCA projection onto the target distribution. For the reverse process, we\nlearn a simple PCA denoiser (projection) based on $X_{t-1}$, which is the cleaner version of the training\nset $X = \\{x_1, ..., x_n \\}$ at time $t - 1$. Thus, at each time step we learn\n$D^{PCA}_t(x_t) = D^{PCA}(x_t; X_{t-1}) = P_t(x_t; X_0 + E_{\\sigma_t})$,,\n(8)\nwhere each column in $E_{\\sigma_t}$ is distributed by $\\mathcal{N}(0, \\sigma_t^2I)$ and $\\sigma_t$ is a function of $\\{\\sigma_s\\}_{s=1}^t$. Our simple\ndenoising procedure is based on the sequential application of $P_t \\in \\mathbb{R}^{d \\times d}$, which is the projection\non perturbed principal components with respect to the clean data distribution $q$. It is a deterministic\ndenoiser given the sampling of training data and noises, which does not depend neither on $X_t$ nor\non $x_0$. Nevertheless, this model is relevant in differentiable environments of more complex settings\nsuch as DNN based denoisers, as we show in Section 5."}, {"title": "LINEAR DIFFUSION AS BASIS PERTURBATION", "content": "We now turn to analyze the linear model presented above and show how the generation process can\nbe seen as a kernel \u201ccorrelation machine\u201d. Specifically, we are interested in the temporal (i.e., noise\nlevel) dependence of the denoiser Equation 8 throughout the generation process. Recall that at each\ntime step $X_t = X_{t-1} + \\epsilon_{\\sigma_t}$, where $\\epsilon_{\\sigma_t} \\sim \\mathcal{N}(0, \\sigma_t^2I)$ (Equation 3). Since the noise is assumed to be\nGaussian, we can write $x_t = x_0 + \\epsilon_{\\bar{\\sigma}_t}$, where $\\bar{\\sigma}_t = \\sqrt{\\sum_{s=0}^t \\sigma_s^2}$. Assume that the data distribution\nis such that its population covariance is given by\n$\\Sigma_T = \\mathbb{E}_{x_0}[x_0x_0^\\top"}]}