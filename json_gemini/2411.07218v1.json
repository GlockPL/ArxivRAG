{"title": "TreeCoders: Trees of Transformers", "authors": ["Pierre Colonna D'Istria", "Abdulrahman Altahhan"], "abstract": "In this paper, we introduce TreeCoders, a novel family of transformer trees. We moved away from traditional linear transformers to complete k-ary trees. Transformer blocks serve as nodes and generic classifiers learn to select the best child and route the sequence of tokens to a specific leaf. The selectors, moved outside the transformer blocks, allow for the use of a variety of architecture without further modifications. Furthermore, our proposed architecture supports sparse node activation due to the logarithmic complexity of a tree search. We validate our idea by testing a series of decoder-only tree transformers achieving competitive results across a diverse range of language datasets. Our study demonstrates that the proposed tree transformer model outperforms a size-equivalent linear transformer model 76% of the time over a wide range of tree architectures. Furthermore, our proposed model naturally lends itself to distributed implementation.", "sections": [{"title": "1 Introduction", "content": "Transformers have proved their effectiveness ever since their introduction [1]. One development has been a rapid increase in the size of the models and the amount of data they consume, leading to models such as GPT-3 [2] towering at 175 billion parameters. Those large and dense models impose heavy hardware requirements and longer inference time. Better management of those resources would be greatly beneficial. One possible solution to reduce inference time has been to promote sparsity in networks through the Mixture- of-Expert approach [3] [4]. A tree is sparse by design, as shown in Figure 1 and, therefore, an attractive candidate for scaling future models. Large language models are increasingly expected to perform on a variety of tasks and datasets. Textual data varies greatly in terms of structure, intent, content, formality, language, and organization. Nevertheless, the underlying language apparatus of these contents can be hierarchically exploited based on their common features. This is our second motivation to investigate moving from a linear structure to a tree-like one. The final motivation is the inherently explainable nature of decision trees. Bringing about explainability to large language models would be a formidable step forward in itself but should also help with fine-tuning, safety, and alignment."}, {"title": "2 Related Work", "content": "The history of the MOE starts with the seminal paper Adaptive Mixtures of Local Experts [3] where they introduced the idea of an expert network and of a gating network. Both were feedforward neural networks. The gating network is in charge of assigning weights to the experts' outputs, which leads to experts handling different tasks or parts of the input space Another important early paper is \"Hierarchical mixtures of experts and the EM algorithm\" by Jordan and Jacobs[4]. In their work, the results of two sets of experts and gating networks are then gated again. This creates a two-level tree-structure network. Our models are also hierarchical and tree-like but we flip the structure upside down, starting with a single expert and progressively branching out to multiple experts. It allows us to sequentially refine the subsets of tasks handled by experts. Conditional computation, as explored by Bengio et al. [5][6] or Davis and Arel [7] among"}, {"title": "3 Definitions", "content": "Definition 1 (Node). A node is a transformer block composed of one or more layers.\nThe number of layers doesn't change the node count.\nDefinition 2 (Height). The height of a TreeCoder is the number of nodes (i.e transformer blocks) from root\nto leaf. We denote it later on with the letter h.\nThe number of encoder or decoder layers does not influence a TreeCoder height.\nDefinition 3 (Number of layers). The number of layers refers to the number of layers per node. We denote\nit by dec later on when speaking of decoder layers.\nA tree architecture can be partly defined with the tuple (h, dec)."}, {"title": "4 The Transformer Tree Model", "content": "A TreeCoder is composed of two main components: the transformer blocks and the selectors. We build a k-ary tree where each node is a stack of one or more decoder (or encoder) layers. The root node receives the same input as a classic transformer and performs the same operations. The result is used as input for the first branch selector. They will have as many outputs as the prespecified tree branching factor (e.g 2 for a binary tree, 3 for a ternary tree, etc). Once a child has been selected, the root node's output is routed to it. The process repeats until a leaf node is reached. The leaf node will perform the usual transformer-block operations and will finally output probabilities. That output will be used to compute the loss and perplexity.\nFigure 3 illustrates an example of a binary TreeCoder of height 2 with N decoder layers per node followed by a softmax selector. We take inspiration from Switch Transformer [11], where tokens are sent to the top-1 expert and forward values to one child only. Where other Mixture-of-expert models regroup, we send intermediate results to another set of independent experts. Importantly, our proposed model differs from their model in that we have a diverse structure with an ever-growing number of experts as we go down the tree.\nThis framework can support encoder-only models like BERT [20], decoder-only like the GPT [21][22][2] series of models, or a mix of both. One can imagine combining pre-trained models as nodes as long as the different formats are compatible. The routing decision can be made by something as simple as a binary classifier or by more advanced techniques such as Monte-Carlo Tree Search (MCTS) or reinforcement learning.\nWe will focus our experiments on decoder-only models due to the success of such models in both scientific literature and production, with varying heights and number of layers per block. Finally, we will employ a softmax classifier as a decision-maker due to its simplicity and differentiability with an"}, {"title": "4.1 Possible Architectures", "content": "In Figure 4, we illustrate the multiple architectures that are possible with this idea. We can build an encoder-only tree, like in Figure 4a, or decoder-only as in Figure 4b. We can modify the original transformer architecture [1] by attaching a tree of decoders to the encoder as in Figure 4c. Inversely, we could build a tree of encoders and add linear decoders to each leaf. Finally, we could combine both and have a tree of encoders with a tree of decoders attached to each leaf as in Figure 4e."}, {"title": "4.2 Selectors", "content": "Selectors are fully connected neural networks ending with a softmax. It takes as input a sequence of tokens of shape context length * embedding size. We then perform mean-pooling. The selector architecture can vary to suit one's needs, but the smallest model tested is a fully connected neural network with a single hidden layer whose size is eight times the input size. SwiGLU [25] is used as an activation function. We used a softmax to accommodate binary as well k-ary trees.\nThe index of the logit with the highest value determines which child will receive its parent's output as input. We also return the logit divided by the logit value. Although always equal to one, it retains in PyTorch the information necessary to perform backpropagation later.\n$grad\\_trick = \\frac{max\\_probality}{max\\_probality - probality.detach()}$"}, {"title": "4.3 The Transformer-Selector Nodes", "content": "The nodes are the usual decoders or encoders and would be referred to as experts in a Mixture-of-Experts context. In this paper, we focus on decoder-only models. Our goal was not to develop a new decoder architecture and therefore we based our implementation on already successful models. Namely, we started off on a GPT-like decoder-only architecture. Given the results of the recently released LLama2 [24], we decided to apply some of the design choices. More specifically, we apply pre-normalization with RMSNorm [26] and the SwiGLU activation function [25]. We did not use grouped-query attention [27] and rotary positional embeddings [28]. We use an embedding size of 1024 and a context length of 128."}, {"title": "4.4 Hyperparameters Settings", "content": "Tokenizer We process our data with the byte pair encoding tokenizer from SentencePiece [29]. The vocabulary contains 8k tokens. We used a character coverage of 1, the identity normalization rule, split digits,\nallowed whitespace-only pieces, and byte fallback.\nLearning Rate The base learning rate is 3e-4. We start with a 2,000-step warmup followed by a cosine annealing with a warm restart scheduler.\nDropout We apply a fixed dropout rate of 0.1, gradient clipping of 1, and a weight decay of 10%."}, {"title": "5 Difference with Mixture of Experts", "content": "We will try to illustrate some of the major differences with Mixture of Experts. For instance in the GLaM model [10], \"each input token is dynamically routed to two selected expert networks out of 64\" (Andrew M Dai and Nan Du, 2021). In our method, the gating occurs outside of the classic layers and not inside. We do not replace the feedforward network with experts (although we could).\nAlso, while the \"final learned representation of a token will be the weighted combination of the outputs from the two experts\" for GLaM [33], our representation will exclusively be the result of a single path. Once a path is taken (i.e an expert is chosen) a subset of others is de facto excluded and unreachable. Another major difference is how tokens are dispatched. In the current literature, single tokens are gated to the experts while we send a whole sequence to the selectors."}, {"title": "6 Experimental Results", "content": "While achieving high performance is undoubtedly a primary objective, our investigation goes beyond it. Hence, we investigate our TreeCoders performance across diverse architectural configurations by systematically varying the tree height, the number of decoders per node, the selector size and the branching factor.\nThe baseline is to identify configurations where the proposed model exhibits comparable or better capabilities than a standard linear transformer. More desirably, we want to show that the tree-based architecture consistently outperforms a transformer of equivalent size, thus mitigating the potential confounding factor of model complexity contributing to performance gains. Identifying configurations that support performance improvements strengthens the theoretical and practical relevance of our model. Finally, to ensure the selectors are actually learning a suitable routing behavior, i.e. sending down outputs to the better subtree, we compare our softmax selector with a random selector that chooses a child at random. We also study the"}, {"title": "6.1 Performance for different heights and decoder layers", "content": "Table B2 shows the performance of our model for different tree heights and decoder layers per node. As expected, adding decoder layers for a given tree height (reading the table vertically) improves the perplexity up to a certain threshold, beyond which the performance improvement stops.\nEqually, increasing the height of the tree yields lower perplexity (reading the table horizontally) up to a certain threshold, beyond which the performance improvement stops. Although increasing the height increases the number of decoder layers a sequence has to go through, better performance was not always observed as it also increases the number of nodes and the number of paths a sequence can take and reduces the number of examples each path will see.\nWe can also group the results by the number of decoder layers a sequence has to go through, as seen in Table 3. For an equal number of layers, taller trees tend to outperform shorter ones. It suggests an improvement intrinsic to the tree structure."}, {"title": "6.2 Random Selection.", "content": "To ensure the selectors are actually learning a suitable routing behavior, i.e. sending down outputs to the better subtree, we compare our softmax selector with a random selector that chooses a child at random."}, {"title": "6.3 Influence of the selector size", "content": "We also studied the selector's influence on the model performance. We expect a bigger model to improve on the final test perplexity. The selector size plays a critical role in the model number of parameters as every"}, {"title": "6.4 Influence of the branching factor", "content": "The branching factor can be an interesting hyperparameter to vary as going from a binary to a ternary (or more) tree would increase the sparsity drastically."}, {"title": "7 Future Work", "content": "This work opens several promising avenues for future research. Firstly, we intend to explore diverse selection methods with distinct objectives. One direction of this investigation will focus on identifying the most potent model configuration, prioritizing the minimization of perplexity. Conversely, a parallel exploration will seek the most efficient architecture, acknowledging that even modest improvements beyond random selection (50% perplexity) could represent a valuable trade-off in resource-constrained scenarios. Additionally, we aim to investigate the interplay between path optimality and model performance. This inquiry may reveal that selecting a good path, exhibiting demonstrably better performance than random selection outweighs the"}, {"title": "8 Conclusion", "content": "This study introduces TreeCoder, a novel class of tree-based transformers. The investigation primarily focused on a decoder-only architecture, demonstrating its promise by achieving performance on par with, or exceeding, its linear counterpart. Additionally, the decoder-only TreeCoder exhibits the advantageous properties of sparsity and inherent parallelization potential. The study systematically explored the impact of architectural variations on perplexity, revealing that for equivalent layer counts, increasing the tree height provides greater benefit than expanding the number of layers within individual nodes. Furthermore, the research demonstrates that all components of the tree-based architecture contribute meaningfully to the overall model behaviour, solidifying the utility and competitiveness of the proposed approach."}, {"title": "Appendix A Hyperparameters", "content": ""}, {"title": "Appendix B Results details", "content": "The table B2 gathers the results of our experiments. \"Layers\" refers to the number of decoder layers in a node, and height is the height of the tree. The values are the test perplexity of models trained from scratch on the different datasets. The colors denote models with equal token path lengths. The token path lengths can be found in the sub-table B2d."}]}