{"title": "INTERACTIVE DIALOGUE AGENTS VIA REINFORCE-\nMENT LEARNING ON HINDSIGHT REGENERATIONS", "authors": ["Joey Hong", "Jessica Lin", "Anca Dragan", "Sergey Levine"], "abstract": "Recent progress on large language models (LLMs) has enabled dialogue agents to\ngenerate highly naturalistic and plausible text. However, current LLM language\ngeneration focuses on responding accurately to questions and requests with a single\neffective response. In reality, many real dialogues are interactive, meaning an\nagent's utterances will influence their conversational partner, elicit information, or\nchange their opinion. Accounting for how an agent can effectively steer a conver-\nsation is a crucial ability in many dialogue tasks, from healthcare to preference\nelicitation. Existing methods for fine-tuning dialogue agents to accomplish such\ntasks would rely on curating some amount of expert data. However, doing so often\nrequires understanding the underlying cognitive processes of the conversational\npartner, which is a skill neither humans nor LLMs trained on human data can\nreliably do. Our key insight is that while LLMs may not be adept at identifying\neffective strategies for steering conversations a priori, or in the middle of an ongo-\ning conversation, they can do so post-hoc, or in hindsight, after seeing how their\nconversational partner responds. We use this fact to rewrite and augment existing\nsuboptimal data, and train via offline reinforcement learning (RL) an agent that\noutperforms both prompting and learning from unaltered human demonstrations.\nWe apply our approach to two domains that require understanding human mental\nstate, intelligent interaction, and persuasion: mental health support, and soliciting\ncharitable donations. Our results in a user study with real humans show that our\napproach greatly outperforms existing state-of-the-art dialogue agents.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are very effective at performing a variety of real-world language tasks,\nincluding open-ended question-answering (Pyatkin et al., 2022), summarization (Paulus et al., 2017;\nWu & Hu, 2018; B\u00f6hm et al., 2019), code generation (Chen et al., 2021; Rozi\u00e8re et al., 2023; Zhong\n& Wang, 2023), and general problem-solving (Wei et al., 2023). While LLMs shine at producing\ncompelling and accurate responses to individual queries, their ability to engage in interactive dialogue\ntasks remains limited. This is because dialogue with humans requires both communication and\ninteraction. A capable dialogue agent should be able to not only process long contexts to craft\nrelevant responses, but also understand how their responses influence their human conversational\npartner, and guide the conversation toward a desired outcome.\nFor example, in tasks requiring teaching, negotiation, or persuasion, the agent must effectively\nmodel and steer the mindset or opinions of the interlocutors in order to accomplish some overall\nconversational goal. In the case of persuasion, the agent should not only produce the most persuasive\nutterance now, but also establish rapport, elicit information, and take other steps that will better\nposition it to make winning arguments later in the dialogue. However, there is both theoretical and\nempirical evidence that contemporary dialogue agents derived from LLMs are unable to execute\nsuch complex strategies by nature of their supervised training (Bubeck et al., 2023; Bachmann &\nNagarajan, 2024), as they are optimized for single-step responses rather than a cohesive set of steps\ntowards a long-term goal.\nReinforcement learning (RL) fine-tuning offers an appealing solution to train effective interactive\ndialogue agents that can build rapport with, gather information about, and steer the opinions of"}, {"title": "2 RELATED WORK", "content": "Language models. Language models, particularly LLMs, have shown impressive capabilities in\ntext generation (Ghazvininejad et al., 2017; Li et al., 2017; Holtzman et al., 2018; Radford et al.,\n2019; Yang & Klein, 2021), translation (Gu et al., 2017), question answering (Pyatkin et al., 2022),\nsummarization (Paulus et al., 2017; Wu & Hu, 2018; B\u00f6hm et al., 2019), and code generation (Chen\net al., 2021; Zhong & Wang, 2023). However, success at most of these tasks is largely enabled by\nsupervised learning, which does not equip LLMs with the ability to plan through multiple steps of\ninteraction (Bachmann & Nagarajan, 2024). Though LLMs have na\u00efvely been used to engage in\ndialogues with humans to some success (He et al., 2018; Shuster et al., 2022b;a), such dialogue\nagents are typically only processing past utterances by the human to produce a relevant response, and\nnot considering their influence on the human by their responses. This limits the competency of such\nagents in interactive dialogue tasks such as negotiation or persuasion.\nRL and language models. Recently, LLMs have leveraged RL fine-tuning, where a reward model,\nlearned from feedback directly from human experts (Ziegler et al., 2020; Stiennon et al., 2020; Wu\net al., 2021; Nakano et al., 2022; Ouyang et al., 2022; Bai et al., 2022a; Christiano et al., 2023)\nor implicitly from another LLM (Bai et al., 2022b), is then used to fine-tune the LLM via RL\noptimization. Finetuning is primarily done via online RL, but offline RL has recently become popular\nas a more practical alternative (Rafailov et al., 2023; Gulcehre et al., 2023). RL has enabled many\ncapabilities in LLMs, such as general instruction-following (Ouyang et al., 2022) and multi-step\nreasoning (Wei et al., 2023; Wang et al., 2023). While effective, many successes of RL fine-tuning\nare when applied to single-step responses, and not over multi-step dialogue. Thus far, RL fine-tuning\nis not as effective in enabling LLMs to plan complex strategies over multi-turn interaction."}, {"title": "3 PRELIMINARIES", "content": "Markov decision processes. To formulate dialogue as a decision making problem, we use the\nformalism of the Markov decision process (MDP), given by a tuple M = (S, A, P, r, \u03c1, \u03b3), where S\nis the state space, A is the action space, P is the transition function, r is the reward function, p is the\ninitial state distribution, and y is the discount factor. When action a \u2208 A is executed at state s \u2208 S,\nthe next state is sampled s' ~ P(\u00b7|s, a), and the agent receives reward r with mean r(s, a).\nInteractive dialogues as MDPs. Interactive dialogues can be viewed as MDPs, where states are\nsequences of tokens from a finite vocabulary V (Ramamurthy et al., 2023). All tokens that the\nagent initially observes are used as our initial state, so = (x0,...,xm), where xi \u2208 V, Vi \u2208 [m].\nAt timestep t, an action at \u2208 V is some token in the vocabulary. As long as at is not a special\nend-of-sequence <EOS> token, the transition function deterministically appends at to state st to form\nSt+1. Otherwise, the agent observes (potentially stochastic) responses from all other interlocutors\nbt = (Yo,..., Yn), which also consist of tokens in the vocabulary; then, the transition function\nappends both at and output responses bt to state st. This continues until the last timestep T where\nwe obtain a state st and the agent receives a deterministic reward r(s) for how well the agent\naccomplished the specified goal.\nReinforcement learning. The goal of RL is to learn a policy that maximizes the expected\ndiscounted return to Yrt in an MDP. The Q-function Q\" (s, a) for a policy \u03c0 represents the dis-\ncounted long-term reward attained by executing a given state s and then following policy \u3160 thereafter.\nQ\u2122 satisfies the Bellman recurrence: Q\" (s,a) = r(s, a) + Es'~P(.\\s,a),a'\u223c\u03c0(:\\s') [Q(s', a')]. The"}, {"title": "4 REINFORCEMENT LEARNING ON HINDSIGHT REGENERATIONS", "content": "Here, we describe our proposed approach, which augments a static dataset of dialogues with hindsight\nregenerations (HR), then trains a downstream dialogue agent using offline RL. Our approach simply\nrequires a collection of task-relevant dialogues T\u2081 with reward labels ri in a static dataset Dorig =\n{(Ti, ri)}i\u2208[N]. Note that such dataset does not need to be collected from humans, but can be\ngenerated synthetically (Hong et al., 2023; Abdulhai et al., 2023). In this paper, we consider learning\nan agent per task, though our method straightforwardly scales to the multi-task setting by considering\ngoal-conditioned agents. Executing our method requires the following components:\n1. A hindsight controller CH that takes any completed dialogue as input, as well as a prefix of\nthat dialogue, and proposes a different, more preferable action to take.\n2. A forward model P that simulates a hypothetical completed dialogue from any prefix.\n3. A reward model to assign a reward for any completed dialogue.\n4. An offline RL method for learning a policy from a static dataset of dialogues.\nNote that our required components are reminiscent of the components of a model-based RL algo-\nrithm (Janner et al., 2019; Yu et al., 2020). However, our method does not require any additional\nonline interaction, but rather uses the hindsight controller to \"explore\" and identify better actions.\nThe components are shown together in our full algorithm in Figure 2. First, in the hindsight action\nrelabeling step, the hindsight controller identifies suboptimal actions in each dialogue of the dataset\nand relabels them with more preferable ones. Then, during forward dialogue generation, we generate\nplausible completions of the relabeled dialogue prefix using the forward model to simulate responses\nby both parties, then the reward model to label the new dialogue with a reward. This pipeline allows\nus to generate an arbitrary number of hindsight regenerations from the original dataset, which can get\nused for downstream offline RL policy optimization. We go over each step in detail below."}, {"title": "4.1 HINDSIGHT ACTION RELABELING", "content": "As alluded to earlier, a primary challenge of learning in interactive dialogues is the difficulty of\ncollecting successful dialogues. Though offline RL does not require data derived from expert agents,\nsome examples of effective behavior are still necessary to \u201cstitch\u201d together (Fu et al., 2019; Kumar\net al., 2022). Our approach circumvents this by backtracking on existing suboptimal behaviors and\nreplacing them with better ones. The key component to achieve this is the hindsight controller,\nwhich identifies ineffective actions in existing trajectories, and replaces them with a different, more\npromising one. Critically, this hindsight controller does not need to generate optimal strategies, but\nsimply propose alternatives from which an offline RL method can extract the most effective strategy.\nThe key idea that enables the design of a hindsight controller is that it is significantly easier to evaluate\nhow an action could be improved in hindsight, after already observing potential responses. For every\ndialogue 7 in dataset Dorig, and every dialogue prefix p \u2286 \u03c4 that is immediately followed by utterance\nu by the agent, we sample from the hindsight controller a single utterance u' ~ CH(\u00b7 | \u0440, \u0442). Since\nCH is given oracle information in the form of future responses, this u' is likely more preferable over\nthe original u in the data. By doing so, we compile examples {(pi, u')}i\u2208[N'] where u' is sufficiently\ndifferent from the original utterance u. In practice, the hindsight controller is implemented as an\nLLM prompted to suggest alternative agent utterances at various prefixes of the dialogue."}, {"title": "4.2 FORWARD DIALOGUE REGENERATION", "content": "From action relabeling, we curated {(pi, u')}i\u2208[N'] containing dialogue prefixes ending in a relabeled\nagent utterance. However, for downstream RL training, it is important to counterfactually reason\nabout the effect of the relabeled utterances on the resulting conversation. This requires learning a\nworld model, consisting of forward dynamics and reward models, of the environment that is used to\ngenerate hypothetical trajectories for the agent to plan through (Sutton, 1991; Janner et al., 2019).\nTo learn a forward model, we fine-tune an LLM to complete dialogues from all prefixes that end\nin agent utterances in the original dataset Dorig, thus learning to generate completions that are"}, {"title": "4.3 POLICY OPTIMIZATION", "content": "While the new examples contain traces of successful behavior, we require multi-step RL to \u201cstitch\u201d\nthese behaviors into an effective policy. Pure imitation will result in a policy that can only occasionally"}, {"title": "5 EXPERIMENTS", "content": "We evaluate our approach on two interactive dialogue tasks based off of real-world data. Existing\ndialogue benchmarks (Budzianowski et al., 2020; Rastogi et al., 2020) are tailored for supervised\nfine-tuning, primarily involving question-answering, and thus do not consider an agent's influence\non their conversational partner. In addition, evaluation of agents in these benchmarks would involve\ncomputing a ROUGE or BLEU score, which merely measure how well agents mimic the data.\nBecause of this, such benchmarks are more suited for supervised finetuning methods rather than RL.\nIn contrast, we consider tasks where optimal agents need to exhibit planning behaviors that account\nfor how actions affect their conversational partner. We provide an overview of both domains below.\nCounseling. In this task, an agent must provide mental health counseling to a person experiencing a\nstrong negative emotion due to some problem in relationships, work, or daily life. We start with the\nESConv dataset of 1,053 dialogues between a human seeker and supporter, where the seeker rates\nthe strength of their negative emotion on a Likert scale (1-5) before and after (Liu et al., 2021).\nPersuasion. In this task, an agent must persuade users to donate to Save the Children, a non-\ngovernmental organization dedicated to international assistance for children. We utilize the\nPERSUASION-FOR-GOOD dataset, which comprises of 1,017 dialogues by real humans where\none attempts to persuade the other to donate to the charity of up to $2 total (Wang et al., 2019).\nTo our knowledge, these are the only dialogue domains for which a curated dataset of real human-human dialogues already exists, where agents influence the mental state or opinions of their conversa-\ntional partners. Due to space, we only show results for the persuasion task in the main paper, and\ndefer results for the counseling task to Appendix A."}, {"title": "5.1 BASELINE METHODS", "content": "The first baselines we consider are state-of-the-art prompting approaches, which prompt GPT-\n3.5 (OpenAI, 2022) to act as the agent.\nCoT: Here, we consider the most basic prompting mechanism, where the LLM is initially prompted\nwith the task description and a chain-of-though component (Wei et al., 2023).\nProCoT: Deng et al. (2023) propose proactive chain-of-thought prompting, which designs a task-\nspecific prompt at each step of the dialogue consisting of a task description, the dialogue thus far, and\na list of high-level strategies and actions. The LLM is asked to reason about each strategy, select the\nmost appropriate one, and craft a response according to the selected strategy.\nGDP-ZERO: Yu et al. (2023) additionally prompts the LLM to perform tree-search over possible\nhigh-level strategies at every timestep, simulating responses by both interlocutors in the dialogue,"}, {"title": "5.2 RESULTS FOR PERSUASION FOR SOCIAL GOOD", "content": "In the persuasion task, we asked 15 users to interact 3 times with each agent anonymized and in a\nrandom order, for a total of 9 conversations per user. Each trial was allowed a maximum of 10 turns\nof interaction, equating to 10 utterances each by the user and agent. Then, each user was asked to\nscore the performance of each agent in the following metrics on a Likert scale (1-5).\nNaturalness/Fluency: How realistic and natural the responses by the agent were. A high score\nmeans that the resulting dialogue contained effective rapport as if it were between two humans.\nRelevant: How on-topic the responses by the agent were. Specifically, the agent should not provide\nany unnecessary information that the user did not ask for.\nReward: The reward obtained in the dialogue, calculated how much the user agreed to donate up\nto a maximum of $2. The user was allowed to keep the amount not donated to make the stakes and\ndifficulty of the task more similar to donation solicitation in the real world.\nReward (Simulation): Since the reward signal being a donation amount is inherently very noisy and\ndepends on the personality of the user, we also evaluate each agent against simulated users. These\nsimulated users utilize GPT-3.5 (OpenAI, 2022) to generate responses, and are prompted to be one\nof 4 personalities: philantropic, caring, unfriendly, or skeptical. A simulated user interacts with\neach agent for up to 10 turns of dialogue, then must choose an amount to donate up to $2. Using\nsimulation, we are able to collect large-scale data consisting of 400 interactions for each method."}, {"title": "6 DISCUSSION", "content": "In this paper, we propose an algorithm to train effective agents for interactive dialogues using offline\nRL on a static dataset. We consider an approach to enhance static datasets that lack exploration\nof optimal strategies, rendering downstream offline RL training to be ineffective. This can be the\ncase when the considered dialogue tasks are difficult for the average human to succeed at, such as\npersuasion. Our approach leverages hindsight regenerations, which relabel suboptimal behaviors in\ndata with traces of optimal ones while retaining accurate human counterfactuals, by utilizing the fact\nthat LLMs can more effectively evaluate dialogues in hindsight. We show, on a variety of interactive\ndialogue tasks including counseling and persuasion, that our approach leads to much more effective\ndialogue agents than simply prompting, or fine-tuning on the original data.\nLimitations. Thus far, our method requires the considered dialogue tasks to have a defined reward\nparameterization to which LLMs can calibrate during the forward regeneration step. This can be\nmuch more difficult for general dialogues where the only signal may be success or failure. In such\ndialogues, LLMs may not be capable enough to generate proxy reward labels without additional\ntraining. Moreover, our method is reliant on hand-crafted prompts. Since these prompts are incredibly\ntask-specific, future work should aim to automate the design of these prompts.\nEthical Considerations. We understand that superhuman abilities in the realm of persuasion can be\nused for harm. However, we focus on the relatively benign tasks of emotional support and persuasion\nto benefit children. Our method is a general framework for improving goal-directed dialogue agents,\nwhich are inherently at risk for dual use."}, {"title": "B IMPLEMENTATION DETAILS", "content": "B.1 HINDSIGHT CONTROLLER\nHere we show the prompts we used to ask GPT-3.5 (OpenAI, 2022) to identify three utterances in\nthe dialogue to improve. From the output of the hindsight controller for each dialogue, one of the\nthree suggested utterances is chosen at random to form a new prefix for use in the forward dialogue\nregeneration step.\nCounseling. We use the following system prompt:\nSystem Prompt - Hindsight Controller (Counseling)\nYou are an expert at mentoring supporters who work to provide emotional support to people dealing\nwith problems in their lives.\nWe use the following user prompt, shown here with an example dialogue:\nUser Prompt - Hindsight Controller (Counseling)\nBelow is a conversation between a supporter (\"Al:\") and a help-seeker (\"H:\"), who is requesting\nemotional support for a challenge that they are facing.\nAl: Hello there, how are we doing today?\nH: Hi ineed help, parents are putting to much pressure on me\nAl: Im sorry to hear that your parents are putting to much pressure on you, do you want to go into\nfurther detail?\nH: Yes. For some reason my parents put so much pressure on me to do well in school its killing me.\nAl: Great, make sure you do it, it might be rough at first but stick with it and im sure youll do great.\nH: are you there?\nAl: Yes sorry, the communication program was lagging a bit.\nH: Thank you. i will do my best to get it off my chest it is. But thank you for our advice\nAl: You are very welcome!\nCan you reference top 3 places in the dialogue where the supporter could have said some-\nthing better to help?\nHave your answer be in a list of the following format:\n1. \"what the agent said in the conversation\";what the agent could have done better;\"what the agent\nshould have said instead\"/\n2. \"what the agent said in the conversation\";what the agent could have done better;\"what the agent\nshould have said instead\"/\n3. \"what the agent said in the conversation\";what the agent could have done better;\"what the agent\nshould have said instead\"\nThe output for this example is as below:\nOutput - Hindsight Controller (Counseling)\n1. \"I am sure they just want what is best, but i have been there as a teen as well.\"; The supporter may\ninadvertently invalidate the help-seeker's feelings by suggesting their parents' intentions excuse the\npressure. Acknowledging feelings without making assumptions would be more supportive; \"It sounds\nlike you're feeling really overwhelmed by this pressure. It's important that your feelings are heard. How\ndoes all of this pressure make you feel?\"/\n2. \"I wouldnt hold off to long as these types of problems tend to brew and just get stronger.\"; The\nsupporter should avoid creating a sense of urgency which could add to the help-seeker's stress. They\ncould instead encourage the help-seeker by emphasizing their autonomy in choosing when to seek\nhelp; \"Taking care of yourself is important, and addressing these feelings when you're ready is a\npositive step. When do you think you might feel comfortable to reach out to a counsellor?\"/\n3. \"Great, make sure you do it, it might be rough at first but stick with it and im sure youll\ndo great.\"; The supporter should refrain from instructing the help-seeker with \"make sure you do it,\"\nwhich could add pressure. Empowering the help-seeker to make choices for their well-being would be\nmore beneficial; \"I'm here to support you in whatever decision you make. Remember to take things at\nyour own pace, it's okay to take time to build up the courage to talk to someone. How are you feeling\nabout all this now?\""}, {"title": "B.2 FORWARD MODEL", "content": "In order to accurately produce completions of the dialogue prefixes given by the hindsight controller,\nwe leverage GPT-3.5 (OpenAI, 2022) fine-tuned on agent utterances from 100 randomly sampled\ndialogues in the original datasets. Here we include the prompts and sample outputs from the fine-tuned\nmodels.\nCounseling. We use the following system prompt:\nSystem Prompt - Forward Model (Counseling)\nYou are an expert at understanding how people think and respond in conversations about their\nemotional state. You are able to successfully predict how real people will respond based off of only a\nfew lines of dialogue.\nHere is an example user prompt, using a prefix from the same dialogue shown above in the Hindsight\nController example. Items in brackets indicate properties (problem type, situation) given in the\noriginal dataset, and are updated to match each dialogue example.\nUser Prompt - Forward Model (Counseling)\nBelow is a dialogue between a counselor (\"AI:\") and a person seeking help (\"H:\"). The person seeking\nhelp is currently undergoing {academic pressure}, and specifically feels: {Parents pressuring me to\ndo well in studies}. Complete the below dialogue between the supporter and the help-seeker such\nthat the generated responses match what each person had said in the prefix in linguistic style and\nsemantics. The dialogue should end before {30} utterances total, so the completion should contain no\nmore than {19} lines of dialogue.\nAl: Hello there, how are we doing today?\nH: Hi ineed help, parents are putting to much pressure on me\nAl: Im sorry to hear that your parents are putting to much pressure on you, do you want to go into\nfurther detail?\nH: Yes. For some reason my parents put so much pressure on me to do well in school its killing me.\nAl: It is a good first step. Also depending how old you are some therapists work on a sliding scale or\nwave fees if you cant afford it. The counselor may be able to act as a mediator between you and your\nparents.\nH: That's great. it will definitely ease some tensions\nAl: Taking care of yourself is important, and addressing these feelings when you're ready is a positive\nstep. When do you think you might feel comfortable to reach out to a counsellor?\nThe output for this example is as below:\nOutput - Forward Model (Counseling)\nH: As right now probably by next week\nAl: Is there anything more weighed on your mind that's preventing you from reaching out to a counsellor\ntoday?\nH: Yes. do you know any counselors that work for free\nAl: I do! I'm one myself. But unfortunately, I'm not within your community. However, if you were to use\nthe internet, you could find free counselling centres in your local community that would certainly be\nable to help.\nH: Thats great! Thanks for the advice\nAl: You're most welcome. So, in what ways do your parents' expectations worry you?\nH: I am scared of failing and not living up to their expectations\nAl: Have you been able to explain that to them? That you're worried that you might not be able to\nachieve what they want you to achieve?\nH: No. I have not\nAl: It's certainly a difficult situation. Do you have any siblings that your parents compare you to?\nH: Yes. i have a brother who is 4 years older than me\nAl: I think it's important to remember that what you're feeling is normal and it is okay. We all want to\nmake our parents proud. But you're only little and you can do it!\nH: Thanks we can do it!\nAl: I have every confidence! I hope that you have a lovely weekend!\nH: I will since i have taken your advice and talk to the counselor thank you. hope you have a wonderful\nweekend as well"}, {"title": "B.3 REWARD MODEL", "content": "Here we show the prompts we used to ask GPT-3.5 (OpenAI, 2022) to label each dialogue with the\ndesired reward. We utilize a few-shot prompting style (Kwon et al., 2023) to ensure that reward\nlabels are consistent with those in the original dataset.\nCounseling. Reward in the counseling task is measured by the difference between the initial and the\nfinal intensity of negative emotion, measured on a Likert scale from 1-5. Since all of our examples\nare generated from prefixes of the original dialogues, we use the initial emotional intensity given in\nthe original dataset and only label the final emotional intensity.\nWe use the below system prompt:\nSystem Prompt - Reward Model (Counseling)\nYou are an expert at analyzing conversations between a supporter and help-seeker, where the\nsupporter provides emotional support to the help-seeker.\nThe user prompt is shown here with a sample dialogue. The dialogues used as few-shot examples\nwere selected at random from the original dataset.\nUser Prompt - Reward Model (Counseling)\nBelow are 10 completed dialogues between a supporter (\"Al:\") and a help-seeker (\"H:\"), who is\nrequesting emotional support for a challenge that they are facing. Before and after each dialogue, the\nhelp-seeker rates how strong their negative emotion is on a Likert scale of 1-5 (5 being the most\nnegative), so a lower rating for their final emotional intensity means that the supporter did a good job\nof addressing their problem.\n<Dialogue 1>\nInitial Emotional Intensity: 4\nFinal Emotional Intensity: 2\n<Dialogue 2>\nInitial Emotional Intensity: 4\nFinal Emotional Intensity: 1\n...\n<Dialogue 10>\nInitial Emotional Intensity: 5\nFinal Emotional Intensity: 3\nLastly, here is a dialogue where the help-seeker has given their initial emotional intensity.\nBased on how effective the dialogue is, rate their final emotional intensity as a number between 1 to 5.\nAl: Hello there, how are we doing today?\nH: Hi ineed help, parents are putting to much pressure on me\nAl: Im sorry to hear that your parents are putting to much pressure on you, do you want to go into\nfurther detail?\nH: Yes. For some reason my parents put so much pressure on me to do well in school its killing me.\nAl: I think it's important to remember that what you're feeling is normal and it is okay. We all want to\nmake our parents proud. But you're only little and you can do it!\nH: Thanks we can do it!\nAl: I have every confidence! I hope that you have a lovely weekend!\nH: I will since i have taken your advice and talk to the counselor thank you. hope you have a wonderful\nweekend as well\nInitial Emotional Intensity: 4\nWhat is the final emotional intensity? Give a number between 1 to 5 in the form of a line\n\"Final Emotional Intensity: \". Do not provide any additional details.\nOutput - Reward Model (Counseling)\nFinal Emotional Intensity: 2"}, {"title": "B.4 POLICY OPTIMIZATION", "content": "We use the hyperparameters reported in Table 3. All algorithms were trained on a single TPUv3 on\nGoogle Cloud until convergence. SFT took around 12 hours whereas ILQL took around 2 days until\ncompletion."}]}