{"title": "CoqPilot, a plugin for LLM-based generation of proofs", "authors": ["Andrei Kozyrev", "Gleb Solovev", "Nikita Khramov", "Anton Podkopaev"], "abstract": "We present CoqPilot, a VS Code extension designed to help automate writing of Coq proofs. The plugin collects the parts of proofs marked with the admit tactic in a Coq file, i.e., proof holes, and combines LLMs along with non-machine-learning methods to generate proof candidates for the holes. Then, CoqPilot checks if each proof candidate solves the given subgoal and, if successful, replaces the hole with it. The focus of CoqPilot is twofold. Firstly, we want to allow users to seamlessly combine multiple Coq generation approaches and provide a zero-setup experience for our tool. Secondly, we want to deliver a platform for LLM-based experiments on Coq proof generation. We developed a benchmarking system for Coq generation methods, available in the plugin, and conducted an experiment using it, showcasing the framework's possibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo. Code at: https://github.com/JetBrains-Research/coqpilot", "sections": [{"title": "1 INTRODUCTION", "content": "Testing has always been essential for making reliable software. For some specific domains, such as aerospace engineering, banking infrastructure, or medical devices, bugs in critical systems may lead to catastrophic consequences [6, 12]. Formal software verification ensures that software operates correctly and safely by proving its correctness against the specification [14]. Under an assumption of a well-constructed specification, formal verification provides stronger guarantees than traditional testing methods, such as unit or integration testing, due to its exhaustive nature. To date, there exist a number of interactive theorem provers (ITP), such as Coq [2], Isabelle [13], or Lean [5]. They are designed to assist users with the construction of formal specifications and verification of formal proofs. For example, Coqhelps in development by providing a robust framework for defining mathematical assertions and ensuring the logical consistency of complex formal proofs. The formal verification approach has proved to be fruitful: for instance, CompCert [11], a C compiler written in Coq, was the only C compiler in which an extensive study found no bugs [20].\nCoq is an interactive proof system, where proofs are constructed step-by-step using so-called tactics. When applied, tactics change the state of the current proof. In particular, a tactic may apply an already proven lemma, destruct the assumption to perform case analysis, apply induction reasoning, and much more. At any point of the proof, the proof state shown to the user will contain information about the current target statement and the assumptions under which it has to be proven. When the statement is empty, the proof is complete. If the proof contains an error or is not constructed correctly, Coq's system will tell that the proof is invalid and provide comprehensive information on the origin of the problem.\nWriting formal proofs is an exceptionally time-consuming task and requires considerable experience from the programmer [15]. Various approaches for Coq generation are already present, both machine-learning-based and not. CoqHammer [4] translates the Coq's logic into untyped first-order logic and searches for the proof. The K-NN approach, implemented as a back-end in Tactician [3], predicts tactics based on what has been used in similar cases. Other approaches are based on generative models [8, 16, 17, 19]. Recently, Large Language Models (LLMs) have gained strong code-generation capabilities [10]. Combined with tools for automatic code verification, we may produce high-quality, reliable code seamlessly.\nSome developed models and tools for Coq generation may require significant setup and/or lack integration into the platform for end users [8, 17, 19]. One other space of improvement for existing non-deterministic proof search processes is to use the information provided by the Coq's system. Even for a human, writing Coq code in a notepad instead of a proper Coq IDE would be harder than in a typical programming language. Interactive stepping through each tactic invocation and updated goal states provide the necessary information while writing proofs. Fortunately, such information can be gathered automatically and used for proof generation.\nWe propose CoqPilot, a VSCode plugin designed to deliver a convenient generation of Coq code using LLMs and other methods. We studied possible external enhancements to generating Coq code with general-purpose models. The automatic checking of multiple generated proof candidates was developed to pick and present only the valid one to the user. We implemented premise selection for better LLM prompting and created an LLM-guided mechanism that attempts fixing failing proofs with the help of the Coq's error messages. To evaluate the performance of the described solutions, we implemented a benchmarking framework for our extension. The"}, {"title": "2 COQPILOT", "content": "In Coq, a goal represents a statement or proposition to be proven. One typically starts the proof with the statement you want to establish as true. Then, one applies tactics and transforms the current goal into one or more subgoals that are simpler or more manageable. Special admit tactic allows skipping a subgoal to permit further progress on the rest of the proof. If the proof contains admits, it is considered incomplete. Each admit corresponds to a self-contained goal with the hypotheses and the conclusion. Say we have a Coq file with a number of unfinished proofs containing admits. CoqPilot runs over the admits and tries to substitute them with correct proofs.\nWe designed CoqPilot to serve as a tool for combining different approaches to Coq generation. We implemented multiple ways to fetch completion and infrastructure around Coq-LSP to check proof candidates. We will refer to each way of fetching completion as a service. Currently, the available services are OpenAI API, LLMs running locally through LMStudio, JetBrains AI Platform, and completion via predefined automation tactics. Coq automation tools such as Tactician [3] and CoqHammer [4], which are triggered using special tactics, could be added to the pipeline through the predefined tactics to unite generation capabilities with CoqPilot.\nA particular setup for the completion request is denoted as model parameters. For LLM-based services, model parameters include the LLM name, the temperature, the prompt, the number of choices to make (i.e., the number of completions the model should generate), and other specifications. As LLMs are not deterministic, making several attempts for each model is beneficial. This result is backed up in Section 4. The setup of CoqPilot consists of a list of model parameters for each of the chosen services.\nWhile implementing the described approach, we encountered several difficulties that affected the CoqPilot's final architecture. Different proof holes in Coq have independent states, and we intend to generate completion for distinct holes in parallel. This requires introducing safety of concurrency to our developed proof-checking mechanisms since Coq-LSP cannot process parallel requests. Our goal was to develop an infrastructure with interchangeable components to allow users to easily add new services and prepare the ground to interchange Coq with another ITP.\nAccurate error handling presents other challenges. Services such as OpenAI have various types of errors, which are supposed to be handled differently. Some may be classified as parameter validation errors and presented to the users; others may be service errors. One of the critical failures, which mainly occurs during benchmarking, is caused by exceeding token limits. Commercial LLM providers restrict their models' usage rates. Local token counters are imprecise, which makes it challenging to overcome these limitations. Therefore, correctly handling such errors becomes crucial for presenting them to the user in an understandable manner. To address this issue, we developed a custom error class hierarchy, differentiating between configuration errors, generation failures, and connection errors, and repacked specific service errors into these appropriate classes. The implementation reports and logs errors based on their types, supporting both user and benchmarking modes.\nCoqPilot offers many configurable parameters. They help the user to set up both the plugin behavior and the experiments using the benchmark. We have implemented a parameter resolution framework, which correctly handles errors, allowing a programmer to write reliable resolvers for new services and parameters.\nOne of our contributions is enhancing the capabilities of general-purpose LLMs in generating Coq code. Given a position to perform completion, we can get the desired statement to prove and the hypotheses under which it should hold. However, this is usually not enough. Writing Coq proofs, a human often recalls other lemmas and objects in the corresponding file/project. It may be challenging for the model to deal with a theorem isolated from its context. To address this problem, we perform premise selection for the theorems within the same file and use them as a few-shot prompt to an LLM. During few-shot prompting, several concrete examples of how the task needs to be solved are provided.\nFew-shot prompting gives the model a better understanding of the problem context and structures the format of its output. Due to token limitations and the model's context window size, we can usually only take a subset of theorems from the file. We choose optimal premises using metrics such as distance from the generation target or similarity with other theorem statements.\nAlso, we may extract helpful information from the Coq's system when proof candidates fail. In particular, we may get the error that occurred and use it to try to fix the failing proof. Baldur [9] used an idea of proof repairing to train a separate proof fixing model. A similar to CoqPilot approach with general purpose LLMs may be found in Copra [18]. When CoqPilot's general pipeline does not find the proof, we launch a multi-turn communication process with an LLM. The number of completions to fetch per turn and the depth"}, {"title": "3 COQPILOT BENCHMARK", "content": "We aimed to develop a benchmarking framework to evaluate the current effectiveness of features implemented in CoqPilot and find space for further improvements. Specifically, our research questions included (i) how well general purpose LLMs can write Coq proofs, (ii) to which extent does CoqPilot improve the LLM approach to Coq generation, and (iii) which additional value CoqPilot, using general-purpose LLMs, contributes to other Coq automation tools such as CoqHammer and Tactician?\nImplementing such a framework brought up several issues that we solved. The main peculiarity of our benchmarking approach is that we need to send a large number of tokens to each model. In order to maintain reasonable performance, we aim to make requests as fast as possible. However, there is usually a limitation on the number of tokens that can be sent in a short time frame. To overcome this, we considered the requests to each model in each service to complete the given goal as a separate asynchronous task. We heuristically determined the necessary waiting time for these tasks to comply with the mentioned limitation.\nThe developed benchmark framework provides a number of possibilities. First, it allows the gathering of information about the internal state of CoqPilot, e.g., the theorems chosen for the context and the number of used tokens. Second, thanks to the implemented interfaces and the CoqPilot's architecture, the developed framework can be conveniently scaled for experimenting with other tools. In this work, we experimented with Tactician and CoqHammer. Moreover, it is possible to generate tailored reports based on the results of the experiments that were conducted."}, {"title": "4 EVALUATION", "content": "To evaluate the performance of CoqPilot, we required a dataset with a large number of human-written theorems and proofs. As it was said before, CoqPilot depends on Coq-LSP, which is not version agnostic and supports Coq versions starting from 8.15. Due to this limitation, it was impossible to fully leverage the CoqGym dataset [19] for our experiments as it contains projects requiring older Coq versions. We have decided to limit ourselves to Coq 8.19 as the latest version available. We have chosen the IMM project for our experiment. The project consists of a large number of proofs and supports Coq 8.19. Moreover, the IMM is of particular interest to our lab since it is developed there.\nThe data for the experiments was prepared as follows. We decided to consider only the proofs of at most 20 tactics, as we initially developed CoqPilot to help users generate subgoals or smaller lemmas. Theorems with proofs with such lengths amount to 83% of proofs in the IMM project. Due to the amount of computing and financial resources at our disposal, we have been unable to experiment on the entire project. Therefore, we decided to use a relatively small subset of 300 theorems. Moreover, we wanted to split the dataset into three groups based on the length of proofs measured"}, {"title": "5 RELATED WORK", "content": "Many Coq generation tools require a long setup and are hardly integrated into the Coq development workflow. Proofster [1] is a web interface for Coq proof synthesis and exploration. Tactician [3] tries to generate Coq proofs after invocation by the special tactics. Copra [18] is an agent for theorem proving, which repeatedly uses a general-purpose LLM for completion. Our work serves similar purposes with a focus on a couple of factors. We aim to develop a plugin that incorporates well into a typical user's workflow and provides setup-free experience. We have built our tool around uniting many approaches and seamlessly allowing users to try all available tools for their problems. This pipeline also brings convenience in experimenting. Another focus is automatically boosting non-deterministic Coq generation tools with the Coq's proof checker. Along with that, we implemented fetching completion from common LLM providers. Tools such as Tactician can be used in CoqPilot as services via predefined tactics without any additional effort from the user."}, {"title": "6 CONCLUSION", "content": "We presented CoqPilot, a VSCode plugin for Coq generation that requires minimal setup. We allow users to seamlessly switch between different Coq generation methods and easily add new ones. We contributed techniques that boost the performance of general-purpose LLMs. Compared to one-shot plain GPT-40 invocation, which can solve 0% theorems from the compiled dataset, GPT-40 with CoqPilot' modifications gets 34%. As shown in Table 1, the joint effort of four models integrated into CoqPilot, achieves 39%. We contributed a highly configurable experiment framework for testing methods implemented in CoqPilot for Coq generation."}]}