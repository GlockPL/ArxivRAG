{"title": "Toward Foundational Model for Sleep Analysis Using a Multimodal Hybrid Self-Supervised Learning Framework", "authors": ["Cheol-Hui Lee", "Hakseung Kim", "Byung C. Yoon", "Dong-Joo Kim"], "abstract": "Sleep is essential for maintaining human health and quality of life. Analyzing phys- iological signals during sleep is critical in assessing sleep quality and diagnosing sleep disorders. However, manual diagnoses by clinicians are time-intensive and subjective. Despite advances in deep learning that have enhanced automation, these approaches remain heavily dependent on large-scale labeled datasets. This study introduces SynthSleepNet, a multimodal hybrid self-supervised learning framework designed for analyzing polysomnography (PSG) data. SynthSleepNet effectively integrates masked prediction and contrastive learning to leverage complementary features across multiple modalities, including electroencephalogram (EEG), elec- trooculography (EOG), electromyography (EMG), and electrocardiogram (ECG). This approach enables the model to learn highly expressive representations of PSG data. Furthermore, a temporal context module based on Mamba was devel- oped to efficiently capture contextual information across signals. SynthSleepNet achieved superior performance compared to state-of-the-art methods across three downstream tasks: sleep-stage classification, apnea detection, and hypop- nea detection, with accuracies of 89.89%, 99.75%, and 89.60%, respectively. The model demonstrated robust performance in a semi-supervised learning environment with limited labels, achieving accuracies of 87.98%, 99.37%, and 77.52% in the same tasks. These results underscore the potential of the model as a foundational tool for the comprehensive analysis of PSG data. SynthSleepNet demonstrates comprehensively superior performance across multiple downstream tasks com- pared to other methodologies, making it expected to set a new standard for sleep disorder monitoring and diagnostic systems.", "sections": [{"title": "1 Introduction", "content": "Sleep is critical in maintaining human health, alleviating mental and physical stress, and preserving physiological balance [1]. Many individuals experience sleep disorders, prompting clinicians to use polysomnography (PSG) for diagnosing and monitoring physiological changes during sleep. PSG records multiple physiological signals, including electroencephalograms (EEG), electrooculo- grams (EOG), electromyograms (EMG), electrocardiograms (ECG), and airflow signals, providing comprehensive insights into sleep-related activities [2, 3]. Clinicians perform various diagnostic and evaluative processes for sleep disorders using these signals. However, these processes are labor-intensive and require significant expertise [3].\nResearchers have proposed various deep-learning-based algorithms for automated sleep assessment to address these challenges. However, most of these approaches rely on supervised learning, which demands large amounts of labeled data [4\u201314]. Recently, self-supervised learning (SSL), a method for extracting meaningful representations from unlabeled data, has been applied to PSG data [15\u201323]. SSL facilitates the discovery of high-level semantic patterns without labels by training on pseudo- labels generated through predefined tasks, creating a generalized network that can be fine-tuned for specific downstream applications. SSL methodologies are broadly classified as single-modality [15-19] and multimodal approaches [20\u201323], with the latter integrating information from multiple modalities.\nExisting methodologies have demonstrated significant advancements but retain notable limitations. Most studies are designed for single tasks, primarily focusing on sleep stage classification [13, 15-22]. However, clinicians evaluate sleep quality using multiple indicators, rendering these approaches overly restrictive [2, 3, 24]. Although detecting apnea and hypopnea is essential, research in these areas remains limited. Additionally, current methodologies predominantly employ contrastive learning within the SSL paradigm. While effective in optimizing inter-modality relationships and learning discriminative representations, this approach has drawbacks. Specifically, it relies heavily on the performance of the backbone network and EEG data augmentation [17, 19]. It reduces efficacy in learning generative representations [25, 26].\nThis study proposes SynthSleepNet, a multimodal hybrid SSL methodology for analyzing PSG data. Drawing inspiration from NeuroNet [19] and MultiMAE [27], SynthSleepNet integrates masked prediction and contrastive learning to fully train generative and discriminative rep- resentation capabilities. This is the first multimodal SSL approach to combine these techniques. Unlike existing methodologies, SynthSleepNet evaluates sleep quality comprehensively by perform- ing three downstream tasks: sleep stage classification, apnea detection, and hypopnea detection. Experimental results demonstrate that SynthSleepNet surpasses state-of-the-art methods across all three tasks and excels in semi-supervised learning environments. The proposed methodology is expected to establish a new foundation for sleep analysis."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Self-Supervised Learning Methodology for Sleep Assessment with Single-Modal Physiological Signals", "content": "SSL methodologies designed for single-modal physiological signals primarily target sleep-stage classification, focusing predominantly on EEG. BENDR [15] incorporates a convolution neural network (CNN)-based module to extract EEG features and a transformer to capture temporal contexts across signals. This model employs contrastive learning by designing output vectors from the CNN-based module and transformer as positive pairs if they correspond to the same time point while treating others as negative pairs. ContraWR [16] replaces the standard InfoNCE loss used in contrastive learning with triplet loss, which minimizes and maximizes the distances between positive and negative pairs, respectively. In this framework, negative pairs are defined as the mean of each sample. TS-TCC [17] applies two distinct augmentations to the same EEG data and utilizes a temporal contrasting module to enhance similarity between the contexts of identical samples while reducing similarity between different contexts of distinct samples. Similarly, mulEEG [18] drops the augmentation methodology of TS-TCC [17] but extends it using multiview SSL to improve learning. This approach incorporates EEG signals and spectrograms as input data, leveraging a diverse loss function to extract complementary information across multiple views. NeuroNet [19] introduces an integrated approach combining masked-prediction-based SSL with contrastive-learning-based SSL to derive unique and discriminative representations. Employing a masked autoencoder structure, NeuroNet [19] performs masked prediction while simultaneously processing two differently sampled vectors through an encoder. The network optimizes learning using the NT-Xent loss for contrastive learning, enhancing its ability to identify meaningful patterns and representations in EEG data."}, {"title": "2.2 Multimodal Self-Supervised Learning Methodology for Sleep Assessment with Multimodal Physiological Signals", "content": "Several multimodal SSL methodologies have been designed for sleep-stage classification, leveraging multiple physiological signal modalities. MVCC [20] incorporates an intra-view temporal contrastive module to extract temporal features within individual modalities and an inter-view consistency contrastive module to ensure coherence across multiple signal modalities. COCOA [21] introduces a cross-modality correlation loss to maximize the similarity between representations of different modalities for the same sample while minimizing the similarity between representations of different time intervals within the same modality. This is achieved using an intra-modality discriminator loss, which refines representation quality. CroSSL [22] is distinguished by its robust flexibility, particularly in scenarios with missing data. The method employs the VICReg loss to minimize the dissimilarity between representations of different modalities. SleepFM [23] adopts a leave-one-out contrastive learning strategy based on the InfoNCE loss and applies it to various sleep-related downstream tasks. MVCC [20], COCOA [21], CroSSL [22], and SleepFM [23] represent contrastive-learning-based multimodal SSL methodologies."}, {"title": "3 Methodology", "content": "SynthSleepNet introduces an advanced multimodal hybrid SSL framework to comprehensively integrate diverse physiological signaling modalities. Figure 1 illustrates the overall architecture of SynthSleepNet, with detailed explanations provided below."}, {"title": "3.1 SynthSleepNet: Multimodal Hybrid Self-Supervised Learning Framework for PSG", "content": null}, {"title": "3.1.1 Model Architecture", "content": "Modality-Specific Backbone The Modality-Specific Backbone was tailored to extract features unique to each physiological signal modality. Drawing inspiration from the NeuroNet [19] archi-\n3"}, {"title": "Encoder", "content": "The encoder serves as a foundational component of SynthSleepNet, mirroring the structure of the modality-specific backbone. SynthSleepNet integrates pretrained modality-specific backbones into encoders that are optimized for the unique attributes of the input physiological signals. For instance, when processing EEG C4 & C3 channels and EOG Left & Right channels, SynthSleep- Net incorporates \u201cpretrained EEG-specific backbones\u201d and \u201cpretrained EOG-specific backbones,\u201d assigning two encoders for each signal type. LoRA [28] was applied to each encoder to enhance the precision of information extraction. LoRA [28] facilitates efficient fine-tuning by employing rank-decomposition weight matrices, thereby minimizing the need to alter the entire weight set. This process facilitates each encoder to produce output vectors \\({e_m^n}\\)_{n=1}^N , representing the \u201csignal tokens\" illustrated in Figure 1 (B), where N indicates the number of tokens, m \u2208 [1, 2, 3, . . ., M] identifies the index of the input physiological signal, and M denotes the total number of input physiological signals, leading to M encoders."}, {"title": "Multimodal Encoder", "content": "The multimodal encoder integrates modality-specific features extracted by individual encoders using a standard Vision Transformer (ViT) [29]. The input to the encoder consisted of tokens generated through a three-step process. First, the output vectors \\({e_m^n}\\)_{n=1}^N  from each encoder were passed through separate projection layers, and positional encoding was added. This resulted in a new set of vectors, \\({z_m^n}\\)_{n=1}^N. Next, a subset of these vectors, \\({z_m^n}\\)_{n=1}^{\\tilde{N}}, was randomly sampled, while the remaining tokens were masked. This sampled subset is denoted by \\({z_m^n}\\)_{n=1}^{\\tilde{N}}, where \u00d1 represents the number of sampled tokens. Finally, the sampled output vectors, \\({z_m^n}\\)_{n=1}^{\\tilde{N}}, from all encoders were concatenated and fed into the multimodal encoder. This produced the output vectors \\({{\\lbrace {h^m_n}\\rbrace }^N_{n=1}}\\)_{m=1}^M, which represent the \u201cfusion tokens\u201d (see Figure 1 (B))."}, {"title": "Decoder", "content": "Separate ViT decoders were used for each representation vector of physiological signal to reconstruct the masked tokens, resulting in a total of M decoders corresponding to the number of encoders. Decoders also used ViT [29]. However, the decoders were removed after the SSL phase. The decoder considered the output vectors \\({{\\lbrace {h^m_n}\\rbrace }^M_{m=1}}\\)_{n=1}^N  as input from the multimodal encoder corresponding to each physiological signal combined with the masked vectors. After that, the input vectors were processed through a projection layer and positional encoding. The masked vector represented the vectors excluded during random sampling and contained information omitted from the input data. Each decoder generated \\({d_m^n}\\)_{n=1}^N  through this process, corresponding to the \u201cpredicted tokens\u201d (see Figure 1 (B))."}, {"title": "3.1.2 Training Objectives", "content": null}, {"title": "Masked Prediction", "content": "Masked prediction involved concealing specific portions of the input data and training the model to predict the hidden parts. This approach enabled the model to infer missing information and learn intrinsic patterns and relationships within the data. The mean square error (MSE) loss was applied to the masked prediction. Specifically, the output vectors \\({d_m^n}\\)_{n=1}^N produced by each decoder were passed through a projection layer to obtain transformed vectors \\({r_m^n}\\)_{n=1}^N, which matched the size of the output vectors \\({e_m^n}\\)_{n=1}^N  of the encoder (the output vectors of the encoder represent the \"signal tokens\" in Figure 1 (B)). After that, the MSE loss was computed between \\({r_m^n}\\)_{n=1}^{N-\\tilde{N}} and \\({e_m^n}\\)_{n=1}^{N-\\tilde{N}}, focusing solely on the masked vectors. The loss function is expressed as:\n\\[L_{recon} = \\frac{1}{M(N-\\tilde{N})} \\sum_{m=1}^M \\sum_{n=1}^{N-\\tilde{N}} (e_m^n - r_m^n)^2\\]\nwhere M represents the number of physiological signals, N denotes the total number of tokens, and \u00d1 corresponds to the number of sampled tokens. This approach ensured that the model focused on accurately reconstructing the masked portions of the input data."}, {"title": "Contrastive Learning", "content": "SynthSleepNet incorporated the NT-Xent loss [30] to optimize the rela- tionships between the output vectors of the encoder and the multimodal encoder. Specifically, the method reduces the distance between output vectors for identical inputs to both the encoder and the multimodal encoder. Conversely, it increases the distance for different input instances. This alignment ensures that the semantic information extracted by the encoder for individual signals is consistent with the semantic information extracted by the multimodal encoder when processing all signals together, enabling effective integration of information from multiple physiological signals.\nEncoder output vectors \\({e_m^n}\\)_{n=1}^N were averaged elementwise to derive $signal_m$. Similarly, multi- modal encoder output vectors \\({\\{\\lbrace {h^m_n}\\rbrace }^M_{m=1}}\\)_{n=1}^N were averaged to produce the $fusion$ representation. After that, these representations, \\({signal_m}\\)_{m=1}^M and $fusion$, were mapped to a latent space and normalized, resulting in \\({sh_m}\\)_{m=1}^M and $f_h$, respectively. The NT-Xent loss [30] was applied as follows:\n\\[L_{contra} = \\frac{1}{2NM} \\sum_{m=1}^M \\sum_{k=1}^N [l(2k \u2013 1, 2k, m) + l(2k, 2k \u2013 1, m)]\\]\n\\[l(i, j, m) = -log( \\frac{exp (sim(f_h^i, sh^j_m)/\\tau)}{\\sum_{k=1}^{2N} 1_{[k\\neq i]} exp (sim (f_h^i, sh^k_m)/\\tau)})\\]\nHere, N represents the batch size, sim refers to cosine similarity, and \u03c4 > 0 corresponds to the temperature scaling factor."}, {"title": "Joint Loss", "content": "SynthSleepNet combined masked prediction and contrastive learning to generate robust representations of physiological signals. Masked prediction captured semantic features through reconstruction, while contrastive learning aligned relationships across individual and multimodal signal representations. The combined loss function is expressed as:\n\\[L_{total} = L_{recon} + \\alpha L_{contra}\\]\nwhere \u03b1 denotes a balancing hyperparameter that controls the relative contribution of the two loss components ($L_{recon}$ and $L_{contra}$)."}, {"title": "3.2 Mamba based on Temporal Context Module", "content": "The American Academy of Sleep Medicine (AASM) [24] guidelines emphasize that sleep stage classification relies on local features within individual PSG epochs and relationships between adjacent epochs. For example, the AASM smoothing rule [24] identifies sleep stages\u2014such as Wake, REM, or Non-REM-that persist for 3\u20135 minutes or longer as new cycles. Shorter stages are often treated as transient and dis- regarded. Consequently, an effective sleep- stage classification model requires a mod- ule capable of capturing inter-epoch fea- tures across multiple PSG epochs. This module is referred to as the temporal con- text module (TCM). Most existing studies implement TCMs using recurrent neural networks (RNNs) or multihead attention mechanisms. However, this study intro- duces a novel TCM model based on the Mamba framework [31], a linear-time sequence modeling approach.\nMamba [30] addresses the limitations of traditional sequential models, particularly RNNs, which exhibit stepwise dependencies that hinder parallelization and increase computation time. Mamba\nenables parallel computations with processing speeds proportional to the sequence length by reducing sequential dependencies, making it highly efficient for long-sequence data. A key feature of Mamba is its selective state-space approach, which selectively tracks only the most relevant states while ignoring less significant ones. This strategy enhances computational efficiency by eliminating unnecessary state exploration, thus reducing resource usage without compromising model performance. Mamba has demonstrated significant success in tasks involving sequential data, such as time-series prediction. We adopted Mamba 2 [32], an enhanced version that introduces additional constraints on selective state-space parameters to further accelerate training speed.\nThe Mamba-based TCM was inspired by the IITNet-style TCM architecture [13]. The proposed Mamba-based TCM involved the following six steps: (1) The pretrained SynthSleepNet was modified by removing its decoder, resulting in the SynthSleepNet w/o decoder, which served as the backbone network. This network extracted a vector sequence $F = {f_i}_{i=1}^T$, corresponding to a single PSG epoch. (2) The vector sequence F was averaged element-wise to produce token K. (3) The backbone network for T PSG epochs processed each epoch individually to extract vector sequences. These sequences were averaged element-wise to produce the vectors ${K_i}_{i=1}^T$, which were batch-normalized to stabilize learning. (4) The vectors ${K_i}_{i=1}^T$ were input into the Mamba model to temporal dependencies across PSG epochs, resulting in output vectors. (5) The input vectors ${K_i}_{i=1}^T$ and ${M_i}_{i=1}^T$ were summed element-wise (i.e., skip connections). (6) The combined vectors were passed through an projection layer to produce the final output vector. Figure 2 is an illustration of the Mamba-based TCM."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Dataset Description and Data Preprocessing", "content": "The Sleep Heart Health Study (SHHS) [33] is a multicenter cohort study aimed at examining the cardiovascular and other health outcomes associated with sleep-disordered breathing. The dataset consisted of two subsets: SHHS1 and SHHS2. Each subset included PSG recordings of multiple physiological signals, specifically two bipolar EEG channels (C4-A1 and C3-A2), one ECG channel, two EOG channels (Left, Right), two leg EMG channels, a snore sensor, pulse oximeters, and a body position sensor. Two EEG channels, two EOG channels, one ECG channel, and one leg EMG channel were selected for analysis. These signals underwent the following preprocessing steps: (1) All physiological signals were resampled to 100 Hz. (2) A robust scaler optimized for physiological data was applied to reduce the influence of outliers while preserving the relative scale of the features. (3) Different bandpass filters were applied according to the signal type: 0.5\u201340 Hz, 3\u201330 Hz, and 25-50 Hz for the EEG and EOG channels, ECG channel, and EMG channel, respectively. Only data from SHHS1 cells were used for this study.\n(Sleep Stage Classification) Each 30-s segment in the dataset was annotated by sleep experts into one of eight categories: Wake, Non-REM1 (N1), Non-REM2 (N2), Non-REM3 (N3), Non-REM4 (N4), REM, Movement, and Unknown. N3 and N4 were merged into a single class (N3), while the \"Movement\" and \"Unknown\" categories were excluded to adhere to the AASM standard.\n(Apnea Detection) The SHHS dataset included annotations for three types of apnea events: obstruc- tive, central, and mixed apnea. These categories were consolidated into a single class. Each 30-s segment was labeled as 1 if an apnea event was detected and 0 otherwise.\n(Hypopnea Detection) Hypopnea-related events in the SHHS dataset were recorded as a single category. Similarly, each 30-s segment was labeled as 1 if a hypopnea event was present or 0 otherwise."}, {"title": "4.2 Evaluation Schema", "content": "Five-fold subject-group cross-validation was conducted to assess the performance of the method- ologies. The evaluation framework was tailored for both SSL-based methodologies and supervised methodologies. The dataset was divided into three subsets: training, pre-training, and testing. The pre-trained group was used for unsupervised training of the SSL model without labels. The training group, consisting of a small subset of labeled data, was used for linear evaluation and fine-tuning. It involved attaching a downstream classifier to the SSL-trained network and completing downstream tasks. The test group was used for the final performance evaluation. The dataset was split into training and test subsets for supervised methodologies, with the test subset remaining consistent across both SSL-based and supervised methodologies. Three evaluation scenarios were implemented. Detailed descriptions of these scenarios are provided below.\n(Evaluation Scenario 1: Linear Probing) The backbone network (i.e., SynthSleepNet w/o decoder) remained fixed in this scenario, while only the downstream classifier was trained. The evaluation aimed to determine the effectiveness of each SSL methodology in capturing representations of PSG data.\n(Evaluation Scenario 2: Fine-Tuning with Temporal Context Module) This scenario evaluated the combined model, SynthSleepNet+TCM, which integrated the backbone network with the TCM. Most parameters of the backbone network were frozen except for a specific segment of the final layer in the multimodal encoder (i.e., the attention projection layer). This approach facilitated additional learning of nonlinear features in the data and leveraged multi-epoch information, which was expected to outperform Scenario 1. Additionally, this evaluation method compared SSL-based methodologies and supervised learning approaches.\n(Evaluation Scenario 3: Semi-Supervised Learning) This scenario examined the performance of semi-supervised learning methods. The proposed methodologies, SynthSleepNet and SynthSleep- Net+TCM, were compared with SalientSleepNet (a supervised learning-based methodology) and SleepFM (an SSL-based methodology). Adjustments were made to ensure the number of modalities was consistent across all methods. Only approximately 1% and 5% of the labeled data from the entire dataset were used for training for the semi-supervised learning experiments."}, {"title": "4.3 Performance Metrics", "content": "Three metrics were employed to evaluate the performance of the proposed model: overall accuracy (ACC), macro F1 score (MF1), and Cohen's kappa coefficient (Kappa). These metrics are widely recognized, with MF1 and Kappa being particularly suitable for datasets with class imbalances. The formulae for ACC and MFI are as follows:\n\\[ACC = \\frac{\\sum_{i=1}^K TP_i}{N}\\]\n\\[MF1 = \\frac{\\sum_{i=1}^K F1_i}{K}\\]\nwhere $TP_i$ and N represent the true positives for the i-th class and the total number of samples, respectively. Similarly, $F1_i$ and K denote the F1 score for the i-th class and the total number of classes, respectively. Kappa measures the level of agreement between two observers on categorical values and is expressed as:\n\\[Kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{1 - \\frac{1}{K} \\sum_{i=1}^K v_i}{1 - p_e}\\]\nwhere po denotes the observed accuracy and pe represents the expected chance agreement."}, {"title": "4.4 Other Approaches", "content": "Representative methodologies were selected and implemented across four paradigms to evaluate the performances of SynthSleepNet. Five methodologies were chosen and implemented under \"SSL + single-modality\u201d: BENDR [15], ContraWR [16], TS-TCC [17], mulEEG [18], and NeuroNet [19]. Four methodologies were selected under \"SSL + multi-modality\u201d: MVCC [20], COCOA [21], CroSSL [22], and SleepFM [23]. Four methodologies were implemented in \u201csupervised learning + single-modality\u201d: IITNet [13], AttnSleep [4], SleepExpertNet [5], and RAFNet [7]. Finally, five methodologies were selected and implemented under the paradigm of \u201csupervised learning + multi-modality\u201d: U-Sleep [8], SalientSleepNet [9], XSleepNet [10], MMASleepNet [11], and DynamicSleepNet [12]. Brief descriptions of these methodologies are provided below."}, {"title": "5 Results", "content": null}, {"title": "5.1 Evaluation Scenario 1: Linear Probing", "content": null}, {"title": "5.2 Evaluation Scenario 2: Fine-tuning with Temporal Context Module", "content": null}, {"title": "5.3 Evaluation Scenario 3: Semi-Supervised Learning", "content": null}, {"title": "5.4 Ablation Experiments", "content": "Ablation experiments were conducted to evaluate the performance of the proposed model. All experiments used the modality combination \u201cEEG2 + EOG2,\" with SSL training limited to 20 epochs to enhance experimental efficiency. The evaluation criterion was the performance in sleep stage classification. The optimal hyperparameters were determined based on the outcomes of these ablation experiments."}, {"title": "5.4.1 Evaluation Scenario 1: Linear Probing", "content": "Masking Ratios SynthSleepNet achieved the highest MF1 of 73.91 at a masking ratio of 40%. Figure 3 illustrates that SynthSleepNet without contrastive learning exhibited optimal performance at masking ratios between 50% and 70%, while SynthSleepNet without masked prediction performed optimally at masking ratios between 30% and 50%. When both tasks were applied concurrently, per- formance exceeded that observed with either task in isolation, demonstrating that tasks complemented and reinforced each other when combined."}, {"title": "Decoder Depth and Width", "content": "Decoder Depth and Width Table 5 shows that SynthSleepNet achieved the highest ACC and Kappa values when the decoder dimension and depth were 256 and 2, respectively. The highest MF1 was observed when the decoder dimension and depth were 256 and 3, respectively. However, variations based on decoder size were minimal. This study incorporated the decoder configuration yielding the highest MFI (decoder dimension: 256; decoder depth: 3) for training SynthSleepNet."}, {"title": "Loss Balance Scale", "content": "An ablation study was conducted to optimize the balance between NT-Xent and MSE losses. Values of \u03b1 < 1.0 indicate a greater emphasis on the masked prediction task, while values of \u03b1 > 1.0 signify a stronger focus on contrastive learning. Table 6 shows that the model achieves optimal performance across all metrics when \u03b1 = 1, indicating that equal weighting of the two losses yields the best results."}, {"title": "5.4.2 Evaluation Scenario 2: Fine-tuning with Temporal Context Module", "content": "Temporal Context Module This study investigated the most effective TCM structure for integrating and analyzing information across multiple PSG epochs. Table 7 demonstrates that the Mamba-based structure outperformed conventional architectures, such as LSTM [13] and multihead attention [4, 5], used in previous studies. Additionally, a performance analysis of Mamba with varying context lengths revealed that the highest ACC and Kappa values were achieved with a context length of 20."}, {"title": "5.5 Rebound Point", "content": "Figure 4 depicts the k-nearest neighbor (k-NN) probing performance during SynthSleepNet training. Like linear probing, k-NN probing was employed to evaluate the representational capacity of SSL. The output vectors were dimensionally reduced using principal component analysis and subsequently used as inputs for the k-NN algorithm.\nThe results revealed a distinct pattern, with performance declining in the initial stages of training but improving after a specific threshold, referred to as the \u201crebound point.\u201d Further analysis revealed that the rebound point occurred earlier when similar modalities were employed as inputs and were delayed for combinations of dissimilar modalities. For instance, training on similar modalities, such as EEG2, resulted in an earlier rebound point, whereas combinations like \u201cEEG1+EOG1+EMG1\u201d and \"EEG2+EOG2+EMG1\u201d showed later rebound points. Notably, \u201cEEG1+EOG1+EMG1\u201d showed the latest rebound point at 25 epochs."}, {"title": "5.6 Hypnograms", "content": "Figure 5 presents the prediction results for subject #202054. The top row displays the labels assigned by sleep experts. The left column illustrates the predictions of SynthSleepNet with linear probing, while the right column shows the predictions of SynthSleepNet+TCM with fine-tuning. Detailed analysis revealed that applying TCM and fine-tuning produced predictions closely aligned with expert-labeled data. SynthSleepNet+TCM using \u201cEEG2+EOG2+EMG1\u201d achieved predictions nearly identical to those of the sleep expert, differing by only three labels."}, {"title": "6 Discussion", "content": "This study proposes SynthSleepNet and Mamba-based TCM to address the limitations of existing sleep analysis methodologies. The proposed approach demonstrated superior performance across three tasks: sleep stage classification, apnea detection, and hypopnea detection. The results underscore the importance of overcoming the single-task focus prevalent in several deep-learning-based sleep analyses, enabling a more comprehensive evaluation of sleep states. Furthermore, SynthSleepNet outperformed methodologies designed for single tasks (Table 2). The ability of SynthSleepNet to analyze unlabeled PSG data presents an opportunity to accelerate sleep research and support the development of related healthcare solutions.\nSynthSleepNet represents a novel multimodal hybrid SSL framework that integrates masked predic- tion and contrastive learning, effectively leveraging EEG, EOG, EMG, and ECG data to achieve high representation learning performance. The combined application of masked prediction and contrastive learning operates complementarily, enhancing stability and facilitating the learning of robust, high- level representations (Figure 3). Consequently, SynthSleepNet outperformed state-of-the-art SSL methodologies (Table 2) and maintained strong performance in semi-supervised learning scenarios, even with only 1% or 5% of the labeled dataset (Table 4).\nIncorporating Mamba-based TCM during the fine-tuning of the pretrained SynthSleepNet significantly improved performance (Table 3). The design of the Mamba-based TCM [32]\u2014unlike commonly used RNNs [13] or multihead attention mechanisms [4, 5]\u2014was critical to achieving these improvements (Table 7). In conclusion, SynthSleepNet combined with the Mamba-based TCM outperformed state-of-the-art supervised learning methods, requiring extensive labeled datasets (Table 3). Moreover, the performance gap was pronounced under semi-supervised learning conditions (Table 4).\nRecent mask-based SSL methodologies have demonstrated strong performance with mask ratios [19, 25, 27] because higher mask ratios compel models to predict masked segments effectively, facilitating the learning of richer patterns and structures. However, SynthSleepNet operates with a relatively low mask ratio of 40% despite employing a masked prediction task, which can be attributed to the following: First, excessively high mask ratios dilute the semantic information within the signal and fusion tokens, potentially impairing the contrastive learning capability of SynthSleepNet. Second, integrating multiple modalities increases the complexity of tasks of SynthSleepNet compared to single-modality methodologies. Consequently, a high mask ratio may overwhelm the model, leading to confusion.\nExamining the performance of SynthSleepNet across various modality combinations revealed that EEG, EOG, and EMG produced the best results for sleep-stage classification (Tables 2 and 3, Figure 5), while ECG and EMG performed optimally for apnea and hypopnea detection (Tables 2 and 3). This aligns with the guidelines outlined in the AASM manual [24], which is used by clinicians for sleep assessment. According to the AASM [24], EEG, EOG, and EMG are the key signals used for sleep-stage classification, while ECG, EMG, and airflow are essential for apnea and hypopnea detection. Combining all modalities (\u201cEEG2+EOG2+EMG1+ECG1\u201d) achieved high overall performance. However, it did not deliver the best outcomes for every task, likely due to the noisy and relatively limited nature of ECG data, which may cause distortion when integrated with other modalities.\nThe training process of SynthSleepNet exhibits a distinct \"rebound point\" phenomenon (Figure 4), which deviates from typical patterns observed in deep learning models. This phenomenon reflects the time required for SynthSleepNet to effectively integrate information from different modalities. The \"rebound point\" occurred later when training on modalities with highly dissimilar features, indicating initial difficulty in reconciling modality discrepancies. Monitoring the \u201crebound point\" using k- NN probing during training and adjusting training epochs accordingly is critical for optimizing performance.\nSynthSleepNet has certain limitations despite several advantages. The relatively low Kappa score for hypopnea detection highlights the need for improved data and labeling quality. Expanding dataset diversity is essential to enhance the generalizability of the model. Additionally, incorporating modalities such as photoplethysmogram signals, respiration signals, and sleep sounds may further improve performance. Third, additional sleep-related downstream tasks (e.g., arousal detection, SpO2 desaturation detection, bruxism detection) should be explored for more comprehensive sleep analysis. Finally, lightweight optimization is essential to enable real-time processing in clinical applications."}, {"title": "7 Conclusion", "content": "This study introduces SynthSleepNet, a multimodal hybrid SSL framework, and Mamba-based TCM to overcome the limitations of existing deep learning methodologies for sleep assessment. SynthSleepNet integrates masked prediction and contrastive learning to effectively extract and fuse features from multimodal physiological signals (e.g., EEG, EOG, EMG, and ECG), facilitating the learning of high-level representations of PSG data. The Mamba-based TCM further improves model performance by capturing temporal dependencies within the PSG data. SynthSleepNet and Mamba-based TCM achieved superior performance in sleep-stage classification, apnea detection, and hypopnea detection while significantly reducing dependence on large-scale labeled datasets, as validated by experimental results. The proposed methodologies establish a robust foundation for advancing sleep research and broader applications in physiological signal analysis."}]}