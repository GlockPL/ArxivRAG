{"title": "Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation", "authors": ["Yujin Shin", "Kichang Lee", "Sungmin Lee", "You Rim Choi", "Hyung-Sin Kim", "JeongGil Ko"], "abstract": "While federated learning leverages distributed client resources, it faces challenges due to heterogeneous client capabilities. This necessitates allocating models suited to clients' resources and careful parameter aggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel federated learning framework for supporting client heterogeneity by combining a multi-exit network architecture with hypernetwork-based model weight generation. This approach aligns the feature spaces of heterogeneous model layers and resolves per-layer information disparity during weight aggregation. To practically realize HypeMeFed, we also propose a low-rank factorization approach to minimize computation and memory overhead associated with hypernetworks. Our evaluations on a real-world heterogeneous device testbed indicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the hypernetwork memory requirements by 98.22%, and accelerates its operations by 1.86\u00d7 compared to a naive hypernetwork approach. These results demonstrate HypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for federated learning.", "sections": [{"title": "1 INTRODUCTION", "content": "The proliferation of deep neural networks fueled by extensive datasets has significantly enhanced mobile and IoT applications, ranging from remote physiological signal monitoring [50, 56], human action recognition [21, 67], and audio recognition [5, 68]. Smart devices such as smartphones, smartwatches, and wearable sensors capture high-fidelity data, enabling robust machine learning model development [53, 62]. These advancements support diverse real-world applications [18, 49, 70]. However, client data often contains sensitive information, and the high communication costs of transmitting raw data to a central server pose challenges for creating comprehensive large-scale datasets suitable for centralized deep model training.\nRecently, federated learning has emerged as a promising approach to building effective machine learning models without explicitly sharing private data. Instead, it shares locally trained model parameters, indirectly transferring local knowledge in a distributed and collaborative manner [45]. Naturally, federated learning has catalyzed advancements in on-device training [9, 39, 64], enabling a diverse array of real-world applications, including healthcare, smart cities, and autonomous driving [48, 59].\nDespite extensive research in federated learning, most prior works assume a homogeneous model architecture to be shared across all clients [36, 45]. This assumption overlooks the diverse range of devices participating in federated learning (i.e., device heterogeneity), especially in practical cross-platform settings. Devices can vary significantly in processing power, memory capacity, and available energy [51]. This issue of computational resource heterogeneity is even more pronounced in mobile and embedded applications, where clients use devices with vastly different computational capabilities, from smartphones to embedded IoT platforms [3].\nWithout considering client-side computational resource heterogeneity, model design is constrained to the capabilities of the least powerful client, leading to suboptimal performance. Furthermore, clients with weaker computing capabilities may struggle to meet training deadlines, raising fairness concerns when excluded due to timeouts [38, 58] or can delay the overall federated learning process as the server waits for clients lagging behind [48, 51]. Therefore, addressing device heterogeneity in federated learning is crucial to balance effective model building and achieve efficient training times [6, 35]. To maximize the distributed computing resources, a federated learning framework must adapt its model capacity to allow all clients to participate in the federated learning process with a suitable model [1, 19].\nPrevious solutions targeting device heterogeneity in federated learning have leveraged techniques such as knowledge distillation [11, 37], local model pruning [34, 35], and adaptive model scaling [6, 19]. In knowledge distillation, the server is required to maintain a well-balanced public dataset,"}, {"title": "2 RELATED WORK AND BACKGROUND", "content": "We begin by positioning our work within the current literature and discuss the foundational concepts and methodologies relevant to our study.\nDevice Heterogeneity in Federated Learning. While federated learning research has mostly focused on homogeneous models with little consideration of client-side computing capabilities [6], supporting federated learning for heterogeneous clients is crucial. It allows for tailored models that adapt to diverse computational capabilities, data distributions, and privacy requirements of various devices [2, 38, 71]. Unlike homogeneous approaches, heterogeneous federated learning leverages each client's unique capabilities, ensuring that even resource-constrained devices contribute effectively to enhance model robustness, maximize the potential of collective data, and lead to better generalization across tasks and environments [40].\nOne early approach to achieve device heterogeneity in federated learning is federated distillation (FD) [37, 54], which leverages knowledge distillation [14]. Instead of sharing model weights, FD aggregates class scores from a large public dataset computed locally at each client. FD accommodates diverse client models but relies heavily on the quality and availability of public datasets and requires this data for all clients, posing challenges for resource-limited platforms.\nAn alternative approach involves pruning local models based on the Lottery Ticket hypothesis [8]. In LotteryFL [35] and Hermes [34], each client learns a subnetwork of the global model (i.e., lottery ticket network) through iterative pruning, rewinding, and training, exchanging only sparse pruned parameters. However, this iterative process is practically challenging for resource-limited clients, as they must initially train the heavy full model to determine which weights to prune. This requires a large number of rounds for model convergence and can cause training delays.\nA different approach reduces model capacity by scaling width, depth, or both, aligning with individual client conditions based on available resources. In width-based scaling (e.g., HeteroFL [6], FedRolex [1]), clients subsample different channel parts per layer for server aggregation. HeteroFL applies static subsampling across global rounds, while FedRolex"}, {"title": "3 CHALLENGES AND PRELIMINARY STUDY", "content": "The recently evolving paradigm of distributed training via federated learning offers a novel approach to developing effective models by utilizing data collected on end devices such as IoT and mobile platforms. Federated learning method enables the system to fully exploit data from many clients while maintaining a privacy-aware environment by ensuring that raw data remains on the client's devices.\nTo fully leverage this distributed data, it is crucial to maximize the number of clients participating in the federated learning process. However, studies highlight that opportunities for participation are often limited due to various factors. A major obstacle is the heterogeneity of computing resources, which can limit both model performance and client participation. It is unrealistic to assume that all clients have similar computing resources and can train the same model, as clients with limited resources (e.g., CPU, RAM, GPU) may be unable to join the training process, despite possessing valuable data. Conversely, if the server employs a lightweight model to accommodate as many clients as possible, the performance of clients with ample resources can be constrained by the limited model capacity. Additionally, less capable clients may require extensive training time, potentially exceeding the server's deadline and preventing their participation.\n3.1 Challenges and Potential Solutions\nTo accommodate the diverse range of computing resources in clients with heterogeneous capabilities, a server can configure and allocate models with different levels of capacity (i.e., number of parameters) suitable for each client. A model with a relatively small number of parameters generally requires less memory, computation, and latency for both inference and training [15]. However, doing so in federated learning presents a challenge: the server must aggregate knowledge (i.e., parameters) from locally trained models with non-identical architectures. One feasible approach is to exploit subnetwork architectures of the full neural network by splitting the original network in a depth-wise manner (i.e., layer-level split). While promising, given that heterogeneous models maintain some level of architectural commonness, we identify two notable issues with the naive approach of simply dividing the network in a depth-wise manner.\n[Issue 1] Misaligned Feature Space: The first issue with depth-wise network splits is the misaligned feature space. Layers closer to the input focus on local features, while layers closer to the output capture global features for tasks like classification. When devices with varying capabilities hold"}, {"title": "3.2 Preliminary Study", "content": "We now present a preliminary study designed to examine the potential of exploiting our proposed solutions for heterogeneous federated learning scenarios.\nFor this study, we utilize the FashionMNIST dataset [65], independently and identically distributed (i.e., IID) across 50 clients, with 10 randomly selected clients participating in each federated learning round. We adopt an IID setup to isolate the impact of our proposed solution from data distribution effects. Our experiments focus on three subnetwork architectures of varying depths derived from a full model consisting of three convolutional layers. Specifically, 17, 17, and 16 clients among the 50 operate 1, 2, and 3-layered models, respectively. We compare against three baselines: (i) all 50 clients using a lightweight single-layer model (\u201cSmall\u201d in Fig. 3 (a)), (ii) only capable clients using the full three-layer model (\"Large\"), and (iii) a non-practical but ideal scenario where all clients use the full three-layer model (\u201cLarge (All)\u201d).\nFigure 3 (a) shows the accuracy results for these configurations. As expected, the \u201cLarge (All)\" scenario, where all 50 clients use the deepest model, achieves the highest performance. However, in the more practical \u201cLarge\" scenario where only a subset (16/50) of clients can use deeper architectures and are the only ones participating, the accuracy remains low and fails to converge, performing worse than the scenario where all clients use the single-layer model (\"Small\"). These results highlight the importance of optimizing model architectures to accommodate client diversity and maximize client engagement in federated learning processes."}, {"title": "4 SYSTEM DESIGN", "content": "4.1 Overview of HypeMeFed\nOur work proposes HypeMeFed, a federated learning framework for networked systems consisting of clients with heterogeneous computational capabilities. Specifically, HypeMeFed aims to optimize federated learning across heterogeneous clients and address the challenges detailed in Section 3 by integrating a multi-exit architecture and employing a hypernetwork-based neural network parameter generation scheme. This ensures effective client participation in the federated learning process regardless of the clients' varying computational capabilities. Figure 5 provides an overview of HypeMeFed's design, highlighting its operational process in a single federated learning round:\n(1) The server distributes global model parameters to clients based on their computing resources. The full global model is configured with multiple exit layers organized in a depth-wise manner (1). Clients with higher computational resources receive the full model, while the ones that are limited receive a smaller subset of the model split at earlier exit layers (\u2461). (c.f., Sec. 4.2).\n(2) Clients train their models with local data (3). HypeMeFed retains the standard local training procedure unchanged. The key difference lies in clients using heterogeneous models and training them with a joint loss function that incorporates predictions from all available exits.\n(3) After local training, each client sends back its trained model parameters to the server (\u2463). Clients transmit only the parameters relevant to their assigned model configuration, ensuring efficient parameter aggregation.\n(4) The server trains its hypernetworks by leveraging client data that contain sufficient layer information (5). The hypernetwork is then used to predict and generate network weights (6 in Fig. 5) for layers suffering from the information disparity issue, particularly the deeper layers or exits of the model. (c.f., Sec 4.3 and Sec. 4.4).\n(5) Finally, the server aggregates the parameters to generate an updated global model based on the client-sent data and the hypernetwork-generated parameters (6, 7).\nThis structured approach ensures that HypeMeFed can utilize both powerful and weaker devices, enhancing overall federated learning performance and inclusivity. The following sections detail HypeMeFed's core components: the multi-exit network architecture and weight generation hypernetworks."}, {"title": "4.2 Multi-Exit Neural Network Architecture", "content": "The global model in HypeMeFed incorporates multiple intermediate classification layers known as \u201cexit layers,\u201d distributed in a depth-wise manner within the neural network. Each exit layer, split in a depth-wise manner, functions as a subnetwork of the full model, enabling effective aggregation of client models and supporting heterogeneous federated learning across devices with varying computing capabilities.\nThe placement and number of exit layers in HypeMeFed are key design choices. In this work, we use three exit layers positioned at one-third, two-thirds, and the end of the federated learning model to exploit a range of features that the model embeds [52]. This setup allows the categorization of clients into three groups based on their computational capabilities: the first group, with limited resources, uses a subnetwork up to the first exit layer; the second group, with moderate resources, uses up to the second exit; and the third group, with ample resources, uses the full network with all three exits. During local training, clients compute outputs for all exit layers included in their allocated model. For instance, a client with sufficient resources generates predictions from all three exit layers and a resource limited client will compute only up to the first. Each layer's output contributes to the overall loss, summing the losses from all exits\u2014a standard practice in multi-exit neural networks [30, 32, 63].\nThis multi-exit architecture in HypeMeFed offers two key advantages. First, it computes losses at both early and final layers, mitigating the vanishing gradient problem and improving training dynamics [63]. Second, it addresses feature space misalignment from depth-wise model splitting (Section 3), ensuring that early layers capture both local and global features, promoting better feature alignment and convergence in federated learning.\nHypeMeFed facilitates seamless aggregation of trained model parameters despite heterogeneous network architectures by training and sharing all exit layer parameters. Specifically, HypeMeFed can easily be integrated with various federated learning schemes like FedAvg [45], FedProx [38], and SCAFFOLD [22] without modifying the local training and aggregation process. In this work, HypeMeFed adopts the FedAvg approach, a straightforward method for averaging model parameters during aggregation."}, {"title": "4.3 Efficient Hypernetwork Design for Multi-exit Architectures", "content": "Despite its benefits, as discussed in Section 3, depth-wise model splitting suffers from the information disparity issue at deeper layers. To mitigate this, HypeMeFed employs a hypernetwork to generate missing weight information. Figure 6 illustrates the workflow of this hypernetwork.\nAs Figure 6 illustrates, HypeMeFed first reshapes the nearest preceding parameters into a matrix form. For instance, a convolution layer holding parameters with the shape \\(IC \\times OC \\times KS \\times KS\\), where \\(IC\\) denotes the number of input channels, \\(OC\\) denotes the number of output channels, and \\(KS\\) denotes the kernel size, is reshaped into a matrix \\(A \\in \\mathbb{R}^{(IC \\times KS) \\times (OC \\times KS)}\\).\nTo initiate the compression of this high-dimensional matrix, HypeMeFed decomposes the matrix-shaped parameters via Singular Value Decomposition (SVD), resulting in three matrices (\\(A = U\\Sigma V^T\\)), namely the left-singular vectors (\\(U \\in \\mathbb{R}^{(IC \\times KS) \\times (IC \\times KS)}\\)), right-singular vectors (\\(V \\in \\mathbb{R}^{(OC \\times KS) \\times (OC \\times KS)}\\)), and the singular values (\\(\\Sigma \\in \\mathbb{R}^{(IC \\times KS) \\times (OC \\times KS)}\\)). Note that the singular values \\(\\Sigma\\) indicate the importance of each corresponding \\(U\\) and \\(V\\) element. Thus, allowing us to selectively discard less significant parameters, and retaining the most critical components for generating the subsequent layer weights. For example, by discarding the low-rank columns and rows and retaining only the top \\(k\\) singular values, we obtain reduced singular vectors that consider the scaling factor of the singular values: \\(\\bar{U} \\in \\mathbb{R}^{(IC \\times KS) \\times k}\\) and \\(\\bar{V} \\in \\mathbb{R}^{k \\times (OC \\times KS)}\\). The hypernetwork is then trained to predict these compressed singular vectors, approximating the original matrix as \\(A \\approx \\bar{U}\\Sigma \\bar{V}^T\\).\nUpon obtaining the two compressed singular value matrices, model parameters are generated efficiently through the straightforward multiplication of these vectors. This method"}, {"title": "4.4 Hypernetwork Implementations", "content": "Given the early-exit architecture of HypeMeFed, which splits the model into three subnetworks, we employ two hypernetworks: one to predict the second exit parameters from the first exit parameters (\\(H_{1\\rightarrow 2}\\)) and another to predict the third exit parameters from the second (\\(H_{2\\rightarrow 3}\\)). These hypernetworks are trained at the server using parameter samples collected from clients each round. Clients executing the model up to the second exit provide samples for \\(H_{1\\rightarrow 2}\\), while those executing the full model provide samples for \\(H_{2\\rightarrow 3}\\).\nHypernetworks from the previous federated learning round serve as initial values for the next, and new samples are used for further training. We discard samples once they are used to ensure that the hypernetwork generates up-to-date weights while reducing training costs. This approach maintains the effectiveness and efficiency of hypernetworks in adapting to dynamically changing parameters in federated learning.\nHypeMeFed selectively generates only the core parameters of the feature extractor, specifically the convolutional layers, while excluding the batch normalization (BN) and classification layers. This approach is based on two key empirical observations: (i) BN and classification layers do not significantly enhance global model performance, and (ii) since these layers are closely related to local data, globally generating them can dilute the unique knowledge of each client, compromising model personalization. We demonstrate this in Section 5.3, where we evaluate personalization accuracy after a few epochs of local fine-tuning of the global model with client data.\nOnce client model parameter data is collected at each federated learning round and the hypernetworks are trained using the operations described above, HypeMeFed uses the fine-tuned hypernetworks to predict subsequent layer parameters based on the compressed representations. This approach ensures that clients with varying computational capacities can fairly participate in the federated learning process by generating the necessary weight information for effective model convergence and improved performance."}, {"title": "5 EVALUATION", "content": "5.1 Experiment Setup\nWe perform an extensive set of experiments to validate the performance of HypeMeFed using three datasets with various comparison baselines, and present details below.\nDataset and Model. In this work, we use three different datasets for our evaluations and a VGG-based baseline CNN model architecture [60]. We discuss the details of the dataset and model used as follows.\n\u2022 SVHN dataset [47] includes 99,289 labeled images of 10 digits (1-10) extracted from real-world house numbers, offering a diverse representation of physical world images and is well-suited for mobile image classification tasks. The model comprises four convolutional blocks, each with two layers featuring channels (32, 64, 128, 256), followed by max-pooling at the end of each block.\n\u2022 STL10 dataset [4] comprises 10 classes with diverse real-world objects, containing 1.3K samples per class. Images in STL10 are high-resolution at 96x96 pixels, distinguishing it from datasets like CIFAR, making STL10 suitable for real-world evaluations of HypeMeFed in federated learning. The model architecture mirrors that used for SVHN, but with channel configurations of 64, 128, 256, and 512.\n\u2022 UniMiB SHAR dataset [46] comprises 4.2K samples from 30 subjects, capturing data from smartphone accelerometers for eight fall types. UniMiB SHAR represents a widely adopted real-world sensing application. We adopted the same model configuration as the STL10 dataset.\nBaselines. For comparison, we utilize five different federated learning schemes: (1) FedAvg [45]: We employ FedAvg as a baseline federated learning approach, which traditionally does not address device heterogeneity. We compare two configurations: in the first, all clients utilize the smallest subnetwork of the multi-exit architecture (FedAvg-S); in the second, all clients utilize the largest model (FedAvg-L). Note that the FedAvg-L configuration is neither realistic nor fair in heterogeneous federated learning, but we test as the upper limit performance; (2) HeteroFL [6]: HeteroFL supports clients with heterogeneous computing resources by leveraging width-wise submodel scaling (compared to our depth-wise splits), where a scaling factor determines the size of the submodel and amount of shared parameters; (3) ScaleFL [19]: ScaleFL splits the model both width- and depth-wise to reduce the number of model parameters with a preset split ratio, supporting federated learning on devices with heterogeneous computing resources. Additionally, ScaleFL employs self-distillation to optimize each multi-exit; (4) LotteryFL [35]: Based on the lottery ticket hypothesis [8], LotteryFL achieves personalized model improvements"}, {"title": "5.2 Overall Accuracy Performance", "content": "Figure 7 plots the global model accuracy following server-side aggregation for HypeMeFed and baselines. For HypeMeFed, employing a multi-exit architecture, each model generates multiple predictions per sample, and we evaluate using the final exit prediction. As anticipated, FedAvg-L achieves"}, {"title": "5.3 Impact of Hypernetworks", "content": "5.3.1 Impact of Weight Generation. Given the overall performance improvements that HypeMeFed brings, we now examine the impact of the hypernetwork on HypeMeFed's performance. For this, we first examine the global model accuracy trends (at the server) for increasing federated learning rounds for HypeMeFed, ScaleFL, and ScaleFL without self-knowledge distillation in Figure 9. Note that ScaleFL also leverages a multi-exit architecture similar to that of HypeMeFed, but without explicit weight estimations for the deeper layers. From our evaluations using the STL10 and UniMiB datasets with different \u03b1, we can notice that ScaleFL fails to properly converge and show accuracy drops during the federated learning operations as the rounds progress (see red highlights in Fig. 9). By removing the self-KD process, we can see a continuously increasing trend with convergence, but HypeMeFed eventually converges to a higher accuracy."}, {"title": "5.3.2 Weight Generation for Batch Norm and Fully Connected Layers", "content": "In HypeMeFed, the hypernetwork focuses on generating weights exclusively for convolutional layers between two exit points. Batch normalization (BN) and fully connected (FC) layers utilize aggregated client information without weight generation to minimize computational and memory overhead. To assess the impact of this design choice on model accuracy, we present results for different hypernetwork-based weight generation scenarios in Figure 10.\nOverall, as shown in the left of Figure 10, with increasing training rounds, the global model accuracy does not exhibit a significant difference between scenarios where the hypernetwork generates BN and FC layer weights and the default scenario where it only generates convolutional layer weights. Additionally, the right plots of Figure 10 show that personalized accuracy (i.e., accuracy of local model at each client) is highest when weights are generated only for the convolutional layers.\nWe hypothesize that this counter-intuitive observation arises from the increased number of denominators in the aggregation process due to the generated weights, thereby reducing the impact of model personalization-related information (e.g., locally trained BN and FC layers). In contrast, the feature extractor (convolutional layers) focuses on extracting universally applicable features across client models [17, 42, 66], maximizing the benefits of weight generation."}, {"title": "5.3.3 Impact of Rank on SVD Operations", "content": "In HypeMeFed, instead of generating weights for the entire model, we employ a low-rank factorization approach based on Singular Value Decomposition (SVD). This method selects the top k ranked parameters for weight generation using the hypernetwork. A larger k enhances model accuracy but increases computational and memory overhead, while a smaller k reduces overhead at the cost of potential performance decrease. Balancing between computational efficiency and model accuracy is crucial in practical federated learning settings with heterogeneous client resources. By tuning k, HypeMeFed can accommodate varying device constraints while maintaining high performance and efficient resource utilization.\nTo examine the impact of k we take a deeper look into this tradeoff using Table 1. As the results in the table show for the CNN baseline model and the UbiMiB dataset, the model compression process does show a performance degradation in accuracy performance. Nevertheless, we can notice that this loss is minimal even for a very small k (e.g., 1.89% loss for k=25). At the same time, the reduction in parameters and the size of the hypernetwork reduces by nearly two orders of magnitude. Naturally, this reduces the computation latency to a minimal level as well."}, {"title": "5.4 Applicability to Large Models", "content": "Experiments conducted so far utilize a relatively small sized model with a limited number of convolutional layers. The next question we answer is whether HypeMeFed can scale to larger and deeper models. For this, we examine the performance with (i) a more complex model with extensive parameters and (ii) models with deeper layers. Specifically, to test a model with extensive parameters, we conducted experiments with ResNet18 [12]-a relatively large model with 11.5M parameters, featuring a residual connection architecture. We used the CIFAR100 dataset [29], containing 50,000 image samples across 100 categories. Three intermediate multi-exit layers were added to the baseline ResNet18 model, splitting it depth-wise into three segments.\nFigure 13 plots the global model accuracy trend with increasing federated learning rounds. The results suggest that HypeMeFed is applicable to larger models, as it successfully converges and shows improved model accuracy compared to the baseline where all users utilize the smallest model (FedAvg-S in Fig. 13). In fact, HypeMeFed achieves performance comparable to the upper bound assuming all clients can utilize the largest model (i.e., FedAvg-L). Here, training the full-rank hypernetwork for ResNet18 requires more than 44.74 GB (~5.59B parameters) of memory, which is infeasible and computationally extensive. However, with k=100 for the hypernetwork optimizations, only 113.28 MB (~14M parameters) was needed, reducing the memory usage by 99.87%. These results demonstrate that HypeMeFed can effectively scale to larger models while maintaining high accuracy and significantly reducing memory overhead.\nNote that, HypeMeFed adopts an auto-regressive approach to generate subsequent layer weights, utilizing either preceding layer weights or previously generated weights. Thus, investigating the influence of model depth, particularly the number of exit layers, becomes pivotal. To explore this, we employed a basic CNN model with 10 exit layers on the FashionMNIST dataset, accommodating varying levels of client device capabilities. We use this dataset given the number of classes and subnetworks that need to be generated for deep"}, {"title": "5.5 Evaluation on Real-World Testbed", "content": "To demonstrate the practicality of HypeMeFed in real-world scenarios, we evaluated HypeMeFed on a testbed of heterogeneous embedded platforms: 4 Raspberry Pi 4, 4 Nvidia Jetson Nano, and 4 NVIDIA Jetson TX2, totaling 12 devices with varying computational resources-weak, moderate, and ample, respectively. Additionally, we used a server with an Nvidia RTX 3090 GPU, an Intel i9-K@3.6GHz CPU, and 64GB RAM. We test the UniMiB dataset with a VGG model, where each client holds a non-overlapping dataset distributed following the Dirichlet distribution (\u03b1=0.5). FedAvg-S and FedAvg-L universally used single- and three-layered models, respectively, while clients in HypeMeFed used models suitable for each device's resources. Unless otherwise specified, all configurations matched those mentioned in Section 5.\nFigure 15 plots the latency per federated learning round for different client platforms along with the server-side latency (left) and the global model accuracy for FedAvg-L, FedAvg-S, and HypeMeFed (right). With all clients using the full model, FedAvg-L achieves high accuracy, but the Raspberry Pi shows prolonged latency in processing this model, potentially delaying the overall federated learning process. In FedAvg-S, latency is kept low, but using a single-layer model causes a 20.41% accuracy drop. By applying HypeMeFed and adaptively configuring models for each device, we balance training latency and accuracy, outperforming FedAvg-S by 5.13%. Note a naive hypernetwork induces 3.9 sec of server computation time and 5.6 GB of memory overhead, while our optimizations (k=100) reduce this to 2.1 sec (1.86\u00d7 speedup) and 100.8 MB (98.22% reduction), agreeing with our previous results. Overall, our testbed validations confirm that HypeMeFed can be a practical and efficient framework for supporting federated learning on heterogeneous clients."}, {"title": "6 DISCUSSIONS", "content": "We now compile a set of notable discussion points for future research that we have identified through our research.\n\u2022 Optimizing hypernetwork architectures and performance. In this work, we demonstrated the feasibility of exploiting hypernetworks in federated learning through HypeMeFed. While HypeMeFed effectively addresses the complexity issues by leveraging SVD-based low-rank factorization, research on hypernetworks is still in its early stages. We identify two potential research directions. First, HypeMeFed utilizes the same hypernetwork architecture across all layers. Exploring optimal configurations for each layer may further enhance hypernetwork performance. Second, despite reduced complexity, training hypernetworks introduces server-side latency, delaying the federated learning process. Developing efficient hypernetwork training algorithms or suitable alternatives could mitigate this issue and improve overall system performance.\n\u2022 Diversifying model architectures. In this work, we focused our evaluations on CNN-based architectures. However, our experiences show that HypeMeFed can also support other architectures, such as MLPs, RNNs, and LSTMs, by effectively generating weights using its hypernetwork."}, {"title": "7 CONCLUSION", "content": "This work presents HypeMeFed, a novel framework designed to address the challenges of device heterogeneity in federated learning. By combining multi-exit network architectures with hypernetwork-based model weight generation, HypeMeFed effectively resolves the feature space misalignment between heterogeneous models and the per-layer information disparity issue during weight aggregation. Furthermore, the proposed low rank factorization-based hypernetwork optimization minimizes computational and memory overhead, making hypernetworks feasible (and applicable) for real-world use. Our evaluations with GPU emulations and a real-world heterogeneous embedded platform testbed confirm that HypeMeFed significantly improves model accuracy and can effectively engage heterogeneous clients in federated learning, proving its practicality in real applications."}]}