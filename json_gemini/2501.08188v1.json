{"title": "A Critical Synthesis of Uncertainty Quantification and Foundation Models in Monocular Depth Estimation", "authors": ["S. Landgraf", "R. Qin", "M. Ulrich"], "abstract": "While recent foundation models have enabled significant breakthroughs in monocular depth estimation, a clear path towards safe and reliable deployment in the real-world remains elusive. Metric depth estimation, which involves predicting absolute distances, poses particular challenges, as even the most advanced foundation models remain prone to critical errors. Since quantifying the uncertainty has emerged as a promising endeavor to address these limitations and enable trustworthy deployment, we fuse five different uncertainty quantification methods with the current state-of-the-art DepthAnythingV2 foundation model. To cover a wide range of metric depth domains, we evaluate their performance on four diverse datasets. Our findings identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a particularly promising approach, offering reliable uncertainty estimates while maintaining predictive performance and computational efficiency on par with the baseline, encompassing both training and inference time. By fusing uncertainty quantification and foundation models within the context of monocular depth estimation, this paper lays a critical foundation for future research aimed at improving not only model performance but also its explainability. Extending this critical synthesis of uncertainty quantification and foundation models into other crucial tasks, such as semantic segmentation and pose estimation, presents exciting opportunities for safer and more reliable machine vision systems.", "sections": [{"title": "1. Introduction", "content": "Monocular depth estimation (MDE) received significant attention in recent years due to its crucial role in various downstream tasks ranging from autonomous driving (Xue et al., 2020; Xiang et al., 2022) and robotics (Dong et al., 2022; Roussel et al., 2019) to AI-generated content such as images (Zhang et al., 2023), videos (Liew et al., 2023), and 3D scenes (Xu et al., 2023; Shahbazi et al., 2024; Shriram et al., 2024). At its core, MDE aims to transform a single image into a depth map by regressing range values for each pixel, all without exploiting direct range or stereo measurements. Theoretically, MDE is a geometrically ill-posed problem that is fundamentally ambiguous and can only be solved with the help of prior knowledge about object shapes, sizes, scene layouts, and occlusion patterns. This inherent requirement for scene understanding perfectly aligns MDE with deep learning approaches, which have proven proficient in encoding potent priors (Bhat et al., 2023; Piccinelli et al., 2024; Yang et al., 2024a,b). These models benefit from extreme scaling, i.e., training on massive datasets and increasing model size, which facilitates the emergence of high-level visual scene understanding.\nBased on these findings, a plethora of models have been proposed to address the challenges of MDE, with recent state-of-the-art solutions often leveraging large vision transformers trained on internet-scale data (Chen et al., 2016, 2020; Li and Snavely, 2018; Ranftl et al., 2020; Bhat et al., 2023; Piccinelli et al., 2024; Yang et al., 2024a,b), yielding foundation models capable of generalizing to a wide range of applications and scenes. A particularly challenging yet crucial application in fields such as robotics (Dong et al., 2022; Roussel et al., 2019), augmented reality (Kalia et al., 2019), and autonomous driving (Xue et al., 2020; Xiang et al., 2022) is the estimation of absolute distances in real-world units (e.g., meters), commonly referred to as metric depth estimation. This task is especially difficult due to inherent metric ambiguities caused by different camera models and scene variations. Fortunately, these foundation models can successfully be fine-tuned in the respective domain (Bhat et al., 2023; Piccinelli et al., 2024; Yang et al., 2024a,b) to determine exceptionally accurate metric depths.\nHowever, the strong performance of foundation models on common benchmarks (Silberman et al., 2012; Geiger et al., 2012; Cordts et al., 2016; Song et al., 2015) can lead to naive deployment, potentially overlooking their limitations. This can be particularly detrimental in safety-critical applications where errors can have serious consequences. There are multiple challenges associated with real-world deployment of deep learning models, including the lack of transparency due to the \"black box\" character of end-to-end systems (Roy et al., 2019; Gawlikowski et al., 2022), the inability to distinguish between in-domain and out-of-domain samples (Lee et al., 2022, 2017), the tendency to be overconfident (Guo et al., 2017), and the sensitivity to adversarial attacks (Rawat et al., 2017; Serban et al., 2018; Smith and Gal, 2018).\nTo mitigate these risks, recent research has advocated for the quantification of uncertainty in deep learning models (Landgraf et al., 2024d; Leibig et al., 2017; Lee et al., 2018; Mukhoti and Gal, 2018; Mukhoti et al., 2023; Landgraf et al., 2024c; Loquercio et al., 2020), particularly in scenarios where their deployment could have real-world implications. A number of promising methods for uncertainty quantification (UQ) have been developed, yet, surprisingly, there has been little attention on the integration of UQ with powerful MDE foundation models. As illustrated in Figure 1, even state-of-the-art foundation models are not immune to inaccuracies. However, if we can leverage UQ to correlate high uncertainties with erroneous predictions, it opens up the possibility of safer deployment of these models in real-world applications.\nTo bridge the gap between ground-breaking results in research and safe, reliable deployment in real-world applications, we investigate multiple UQ methods in combination with MDE foundation models. We specifically focus on combining the state-of-the-art DepthAnythingV2 foundation model (Yang et al., 2024b) with five different UQ methods to enable pixel-wise uncertainty measures for metric depth estimation:\n1. Learned Confidence (LC) (Wan et al., 2018): Confidences, interpreted as uncertainties, are learned by extending the primary objective function with an additional loss term.\n2. Gaussian Negative Log-Likelihood (GNLL) (Nix and Weigend, 1994): Predictions are treated as samples from a Gaussian distribution, with the network outputting both a predictive mean and its corresponding variance, which is learned implicitly through minimizing the Gaussian Negative Log-Likelihood.\n3. Monte Carlo Dropout (MCD) (Gal and Ghahramani, 2016): Dropout layers remain active during inference, sampling from the posterior distribution to estimate a predictive mean and variance.\n4. Sub-Ensembles (SE) (Valdenegro-Toro, 2023): A Deep Ensemble (Lakshminarayanan et al., 2017) is approximated by multiplying a subset of the model's layers instead of using the full model.\n5. Test-Time Augmentation (TTA) (Ayhan and Berens, 2018): Perturbations applied to inputs during inference produce unique samples, enabling computation of a predictive mean and corresponding variance.\nWe conduct extensive evaluations across four diverse datasets:\n1. NYUv2 (Silberman et al., 2012): Low-resolution indoor scenes.\n2. Cityscapes (Cordts et al., 2016): High-resolution outdoor urban scenes.\n3. HOPE (Tyree et al., 2022): Synthetic training images and real-world testing images of household objects.\n4. UseGeo (Nex et al., 2024): High-resolution aerial images.\nThis wide selection ensures coverage of a broad spectrum of real-world applications. Additionally, we examine all publicly available pre-trained encoders of the DepthAnythingV2 model, comprehensively revealing insights into the performance of models across various sizes and configurations.\nWe believe that our analysis will contribute to a deeper understanding of the limitations of these powerful foundation models, enhancing transparency and reliability in MDE. By highlighting often overlooked challenges, such as the lack of explainability, our work paves the way for future research that not only seeks to improve performance but also aims to teach these models to know what they don't know."}, {"title": "2. Related Work", "content": null}, {"title": "2.1 Monocular Depth Estimation", "content": "Foundations. Monocular Depth Estimation (MDE) is a dense regression task that aims to predict a depth value for each pixel in a given input image. The pioneering work of Eigen et al. (2014) laid the foundation for MDE by directly predicting depth using a multi-scale neural network. This seminal approach demonstrated that convolutional neural networks could effectively learn spatial hierarchies and capture depth cues from monocular images, thus inspiring a plethora of subsequent methods (Ming et al., 2021; Masoumian et al., 2022; Khan et al., 2020; Arampatzakis et al., 2023). While most introduce novel architectures or loss functions, Fu et al. (2018) reformulate depth estimation as an ordinal regression problem through discretization of the depth ranges. Another innovative approach by Yuan et al. (2022) incorporates neural conditional random fields to model contextual dependencies, further refining depth predictions. Besides, Patil et al. (2022) impose geometric constraints based on piecewise planarity priors.\nVision Transformers. Naturally, the rise of vision transformers (Dosovitskiy et al., 2020) has also significantly impacted the field of MDE. These models employ the self-attention mechanism of the transformer to aggregate depth information across a more extensive field of view to capture long-range dependencies and global context, leading to more accurate and consistent depth maps. Inherently, multiple approaches have successfully adapted vision transformers to MDE (Agarwal and Arora, 2022; Li et al., 2023; Yang et al., 2021; Aich et al., 2021; Zhao et al., 2022; Ning et al., 2023; Ranftl et al., 2021; Eftekhar et al., 2021; Yin et al., 2021; Piccinelli et al., 2023; Ke et al., 2024)."}, {"title": "Hybrid Approaches.", "content": "Beyond more traditional regression-based methods, some works creatively treat MDE as a combined regression-classification task. By discretizing the depth range into bins, these methods simplify the learning task improve performance in some cases. Notable examples include AdaBins (Bhat et al., 2021), BinsFormer (Li et al., 2024), and LocalBins (Bhat et al., 2023)."}, {"title": "Generative Models.", "content": "A more recent trend in MDE includes repurposing generative models such as diffusion models (Duan et al., 2023; Ji et al., 2023; Saxena et al., 2024, 2023; Ke et al., 2024; Patni et al., 2024), effectively building on its predecessor, generative adversarial networks (CS Kumar et al., 2018; Aleotti et al., 2018)."}, {"title": "Depth in the Wild.", "content": "Estimating depth \"in the wild\" has become an increasingly important area of research in MDE. It refers to the challenge of predicting accurate depth estimates in unconstrained environments, where lighting, scene structure, and camera parameters vary significantly. With the increasing availability of compute resources, researchers have found scaling to be a valuable tool to tackle this challenge. By constructing large and diverse depth datasets and leveraging powerful foundation models, MDE has become more accessible and robust to real-world use. Foundation models are large neural networks pre-trained on internet-scale data, which allow them to develop a high-level visual understanding that can either be used directly or fine-tuned for a variety of downstream tasks (Bommasani et al., 2021)."}, {"title": "Ordinal Depth.", "content": "One of the earlier works aimed at addressing depth in the wild by leveraging the scale of data is (Chen et al., 2016). Building on this idea, Chen et al. (2020) introduced the OASIS dataset, a large-scale dataset specifically designed for depth and normal estimation. It is worth noting, however, both of these works primarily focus on relative (ordinal) depth, which only estimates the depth order instead of providing absolute measurements. While ordinal depth can provide valuable information about the structure of a given scene, its practical use is limited."}, {"title": "Affine-invariant Depth.", "content": "To overcome the limitations of ordinal depth, several studies have explored affine-invariant depth estimation, which provides depth estimates up to an unknown affine transformation. In other words, the absolute scale and offset of the depth map can vary while the relative depth differences are preserved. For instance, models trained on the MegaDepth dataset (Li and Snavely, 2018), which uses multi-view internet photo collections along with structure-from-motion and multi-view stereo methods to create depth maps, can generalize well to unseen images. Another significant contribution is MiDaS (Ranftl et al., 2020), which achieves the ability to generalize across a variety of scenes and conditions through training on a mixture of multiple datasets."}, {"title": "Metric Depth.", "content": "For applications that often require absolute distances, such as robotics (Dong et al., 2022; Roussel et al., 2019), augmented reality (Kalia et al., 2019), and autonomous driving (Xue et al., 2020; Xiang et al., 2022), metric depth estimation is crucial. Metric depth estimation aims to provide absolute depth measurements in real-world units (e.g., meters or centimeters). Inconveniently, zero-shot generalization is particularly challenging due to the metric ambiguities introduced by different camera models. Aside from some works that explicitly incorporate camera intrinsics as an additional input (Guizilini et al., 2023; Yin et al., 2023) to directly solve this issue, current state-of-the-art metric depth estimation approaches still rely on fine-tuning powerful foundation models in the respective domain (Bhat et al., 2023; Piccinelli et al., 2024; Yang et al., 2024a,b)."}, {"title": "2.2 Uncertainty Quantification", "content": "Overview. A wide variety of UQ methods have been proposed to address the shortcomings of deep neural networks, particularly in terms of reliability and robustness (MacKay, 1992; Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Valdenegro-Toro, 2023; Van Amersfoort et al., 2020; Liu et al., 2020; Mukhoti et al., 2023; Amini et al., 2020). Among them, sampling-based approaches are the most prominent due to their ease of use and effectiveness in providing high-quality uncertainty estimates (MacKay, 1992; Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017). While they usually produce the most accurate uncertainty estimates, the computational cost associated with the necessity of multiple forward passes often makes them unusable for real-world applications that require fast inference times or that are running on resource-constrained devices.\nSampling-based Methods. One of the simplest and most prevalent sampling-based UQ methods is Monte Carlo Dropout (MCD) (Gal and Ghahramani, 2016). MCD approximates a Gaussian process by keeping dropout layers active during both training and testing. Originally, dropout layers were solely introduced as a regularization technique to prevent overfitting (Srivastava et al., 2014). With MCD, however, they are also used during test time to turn a deterministic model into a stochastic one to sample from the posterior distribution. The predictive uncertainty of a given model can then easily be estimated by calculating the standard deviation (or variance) of the samples. Another widespread sampling-based UQ method are Deep Ensembles (Lakshminarayanan et al., 2017), which are generally considered the state-of-the-art for UQ across various tasks (Ovadia et al., 2019; Wursthorn et al., 2022; Gustafsson et al., 2020; Landgraf et al., 2024b). They consist of a collection of independently trained models, ideally each initialized with random weights and optimized with random data augmentations to maximize the diversity among the ensemble members (Fort et al., 2020). The high effectiveness of Deep Ensembles comes at the cost of a very high computational overhead due to the need to train and evaluate multiple models.\nEfficiency-oriented Methods. In addition to these methods, there are several approaches that aim to balance computational efficiency with the quality of uncertainty estimates (Valdenegro-Toro, 2023; Van Amersfoort et al., 2020; Liu et al., 2020; Mukhoti et al., 2023). For instance, Sub-Ensembles (Valdenegro-Toro, 2023) exploit subnetworks within a single model to produce diverse predictions without the need of a full ensemble of models. They offer an effective trade-off between uncertainty quality and computational cost, which can easily be tuned based on the given constraints.\nUQ in Self-supervised MDE. While substantial progress has been made in developing effective UQ methods, integrating these techniques with MDE comes with some unique challenges like the limited ground truth data, which is expensive and difficult to acquire, especially at scale. For that reason, a significant body of research has focused on UQ in self-supervised MDE (Poggi et al., 2020; Hirose et al., 2021; Nie et al., 2021; Choi et al., 2021; Dikov and van Vugt, 2022). While they all explore different strategies to estimate the uncertainty, some even"}, {"title": "UQ in Supervised MDE.", "content": "For supervised learning scenarios, one common approach to UQ involves modeling the regression output as a parametric distribution and training the model to estimate its parameters (Kendall and Gal, 2017; Nix and Weigend, 1994). This approach allows the model to not only output the depth estimate but also to measure the corresponding uncertainty, typically represented as the variance of the distribution. Similarly, Yu et al. (2021) propose an auxiliary network that exploits the output and the intermediate representations of the main model to estimate the uncertainty. Hornauer and Belagiannis (2022) introduce a post hoc uncertainty estimation method that relies on gradients extracted with an auxiliary loss function. This technique utilizes Test-Time Augmentation (Ayhan and Berens, 2018) to investigate the correspondence of the depth prediction for an image and its horizontally flipped counterpart. Another innovative approach is proposed by Franchi et al. (2022), who address computational efficiency by optimizing a set of latent prototypes. The uncertainty is quantified by examining the position of an input sample in the prototype space. Lastly, Mi et al. (2022) developed a training-free uncertainty estimation approach based on tolerable perturbations during inference and using the variance of multiple outputs as a surrogate for the uncertainty.\nResearch Gap. Despite significant advancements in UQ for MDE, integrating these techniques with large-scale foundation models remains unexplored. We aim to address this gap by combining multiple UQ methods with the state-of-the-art DepthAnythingV2 foundation model (Yang et al., 2024b), enabling pixel-wise uncertainty estimates in addition to metric depth measurements. Our findings will contribute to a more nuanced understanding of the capabilities and limitations of these models, emphasizing the importance of not only striving for higher performance but also making MDE more reliable and trustworthy for real-world use."}, {"title": "3. DepthAnything Foundation Model", "content": "DepthAnythingV2 (Yang et al., 2024b) is one the most recent state-of-the-art foundation models for MDE, which can easily be fine-tuned for metric depth estimation, making it the perfect candidate for exploring the combination of various UQ methods with MDE foundation models. Consequently, we want to provide a more detailed overview of this model, which was built upon the framework established by its predecessor, DepthAnythingV1 (Yang et al., 2024a).\nDepthAnythingV1. Yang et al. (2024b) laid the groundwork for creating a versatile foundation model for MDE using the DINOV2 encoder (Oquab et al., 2023) for feature extraction with the DPT decoder (Ranftl et al., 2021) for depth regression. The training process of DepthAnythingV1 involves a semi-supervised approach using a student-teacher framework. The teacher model generates pseudo-labels for an extremely large corpus of unlabeled images (approx. 62 million from 8 public datasets), while the student is trained on both the pseudo-labels and a set of 1.5 million labeled images from 6 public datasets. To ensure robustness of the learned representations, they additionally apply strong image perturbations for the student. These include strong color distortion like color jittering and Gaussian blurring and strong spatial distortion through CutMix (Yun et al., 2019).\nTo further refine the model's capabilities, they also introduce an auxiliary feature alignment loss. It measures the cosine similarity between the features of the student model and those of a frozen DINOv2 encoder, which is a powerful model for semantic-related tasks like image retrieval and semantic segmentation. This potent addition to the training process helps imbue the DepthAnythingV1 model with high-level semantic understanding to further improve the depth estimation.\nDepthAnything V2. Building on the success of DepthAnythingV1, Yang et al. (2024b) quickly introduced several key advancements that enable finer and more robust depth predictions. These improvements are centered around the following three strategies:\n1. Synthetic Data for Label Accuracy: One of the most significant changes for DepthAnythingV2 is the replacement of all labeled real images with synthetic images. This alteration is motivated by the desire to eliminate label noise and address the lack of detail often ignored in real datasets. In contrast to real images, synthetic data allows for precise depth training and avoids the inconsistencies found in real-world labels.\n2. Scaling up the Teacher Model: To mitigate the drawbacks of synthetic images, such as distribution shifts and restricted scene coverage, the capacity of the teacher model significantly increased. DepthAnything V2 employs DINOv2-G, the most powerful variant of the DINOv2 encoder (Ranftl et al., 2021).\n3. Leveraging Pseudo-Labeled Real Images: To bridge the gap between synthetic images and the complexity of real-world scenes, DepthAnythingV2 incorporates large-scale pseudo-labeled real images into its training pipeline. This does not only expand the lacking scene coverage of the synthetic images but also ensures that the model is exposed to a wide variety of real-world scenarios, improving its generalization capabilities.\nThese advancements make DepthAnythingV2 one of the most powerful, if not the most powerful currently available MDE foundation model. Aside from providing high-quality depth predictions across a diverse range of applications, the model can easily be fine-tuned for metric depth estimation tasks, achieving state-of-the-art results. Given these attributes, DepthAnythingV2 serves as an ideal candidate for exploring foundation model uncertainty in MDE. We aim to assess various UQ methods in combination with this foundation model, enabling pixel-wise uncertainty measures while maintaining high accuracy in the regressing metric depths in real-world applications."}, {"title": "4. Methodology", "content": null}, {"title": "4.1 Overview", "content": "Our primary research question is straightforward: How can we bridge the gap between ground-breaking results in research and safe deployment in real-world applications that need robust metric depth estimates with corresponding uncertainties.\nAs Figure 2 shows, we study five different approaches to not only estimate metric depths but also their corresponding un-certainties: Learned Confidence (LC) (Wan et al., 2018), Gaussian"}, {"title": "4.2 Learned Confidence", "content": "The general approach of LC was originally proposed by Wan et al. (2018) for classification tasks, but has already been adapted before for regression tasks by Wang et al. (2024). The confidences, which we interpret as uncertainties, can simply be learned in addition to the primary objective function:\n$L_{LC} = \\frac{1}{N} \\sum_i C_i \\cdot L_{SI}(y_i, \\hat{y_i}) - \\alpha \\cdot log C_i,$(1)\nwhere N represents the number of pixels having valid ground truth values, $C_i$ denotes the confidence score of the i-th valid pixel generated by the model, $y_i$ is the depth output of the model, $\\hat{y_i}$ is the ground truth depth, $\\alpha$ is set to 0.2, following Wang et al. (2024), and $L_{SI}(y_i, \\hat{y_i})$ is the scale-invariant loss introduced by Eigen et al. (2014):\n$L_{SI}(y, \\hat{y}) = \\frac{1}{N} \\sum_i \\lambda_i \\cdot d_i^2,$(2)\nwhere $d_i = log y_i - log \\hat{y_i}$. We follow the fine-tuning recommendations of DepthAnythingV2 (Yang et al., 2024b) and set $\\lambda$ = 0.15."}, {"title": "4.3 Gaussian Negative Log-Likelihood Loss", "content": "For depth regression, neural networks are usually only trained to output a predictive mean $\\mu$. To also approximate the corresponding variance $s^2$, i.e. the uncertainty, we follow the approach of Nix and Weigend (1994): By treating the neural network prediction as a sample from a Gaussian distribution, we can minimize the Gaussian Negative Log-Likelihood (GNLL) loss, which can be formulated as:\n$L_{GNLL} (\\mu, \\hat{y}) = \\frac{1}{2} ( \\frac{(\\hat{y} - \\mu)^2}{s^2} + log s^2 )$(3)\nAnalogous to Equation 1 for LC, there is no ground truth for the uncertainty, which means that $s^2$ is solely learned implicitly through the optimization of the predictive means $\\mu$ based on the ground truth labels $\\hat{y}$."}, {"title": "4.4 MC Dropout", "content": "Using MCD (Gal and Ghahramani, 2016) to estimate the predictive mean $\\mu$ and the corresponding uncertainty, i.e., the variance $s^2$ or standard deviation $s$, is fairly straightforward. Since the DepthAnythingV2 model already applies dropout layers throughout its architecture, we simply have to activate them not only during training but also during inference to sample from the posterior of the network.\nTo compute the predictive mean $\\mu$, we take the average of all the samples:\n$\\mu = \\frac{1}{T} \\sum_{t=1}^T y_t,$(4)\nwhere T is the number of samples and $y_t$ is the t-th depth prediction of the network.\nFor the uncertainty, we calculate the variance:\n$s^2 = \\frac{1}{T-1} \\sum_{t=1}^T (y_t - \\mu)^2.$(5)\nBesides that, we follow the fine-tuning recommendations of DepthAnythingV2 (Yang et al., 2024b), using the scale-invariant loss $L_{SI} (y, \\hat{y})$ from Equation 2 as the objective function."}, {"title": "4.5 Sub-Ensemble", "content": "Sub-Ensembles (SE) (Valdenegro-Toro, 2023) enable the approximation of Deep Ensemble. While a Deep Ensemble requires multiple models to be trained and used during inference, the SE only requires a subset of the layers to be multiplied. As shown by Figure 2, we use a shared encoder for multiple randomly initialized depth heads. To maximize the diversity across the depth heads and decrease the training time, we cycle through the heads during training. Per training batch, only one head is optimized, the others are ignored. During inference, however, each depth head predicts a unique sample yt based on the extracted feature from the encoder. Similar to MCD, we can compute the mean $\\mu$ and variance $s^2$ of all the samples (see Equations 4 and 5) to get the desired output, i.e., a final depth map and a corresponding uncertainty.\nAs for MCD, we follow the fine-tuning recommendations of DepthAnythingV2 (Yang et al., 2024b), using the scale-invariant loss $L_{SI} (y, \\hat{y})$ from Equation 2 as the objective function."}, {"title": "4.6 Test-Time Augmentation", "content": "In contrast to the other four uncertainty quantification approaches, we just fine-tune the DepthAnythingV2 model with the scale-invariant loss $L_{SI}$ from Equation 2 for metric depth estimation and apply Test-Time Augmentation (TTA) after training (Ayhan and Berens, 2018). As shown by Figure 2, we flip the input image vertically as well as horizontally and perform inference with each. As a result, we obtain three unique depth samples yt that we can use to compute the the mean $\\mu$ and variance $s^2$ (see Equations 4 and 5)."}, {"title": "5. Experiments", "content": null}, {"title": "5.1 Experimental Setup", "content": "Training. For all training processes, we follow the default settings of DepthAnythingV2 for metric depth fine-tuning (Yang et al., 2024b), using an AdamW optimizer (Loshchilov, 2017) with a base learning rate of 6 10-5, a weight decay of 0.01, and a polynomial learning rate scheduler:\n$lr = lr_{base} (1 - \\frac{iteration}{total iterations})^{0.9},$(6)\nwhere lr is the current learning rate and lrbase is the initial base learning rate. Every model is trained for 25 epochs with an effective batch size of 16, using four NVIDIA A100 GPUs. We do not employ any early stopping techniques and hence only evaluate the final model checkpoints.\nDatasets. We conduct our experiments on four highly different datasets, simulating a broad range of real-world applications, as shown by Table 1. Cityscapes (Cordts et al., 2016) provides an urban street scene benchmark dataset with high-resolution images. In contrast, NYUv2 (Silberman et al., 2012) presents indoor scenes with a very low image resolution. UseGeo (Nex et al., 2024) covers high-resolution aerial images, which are often neglected in the computer vision community despite their significance in many real-world applications. Finally, the HOPE (Tyree et al., 2022) dataset offers a variety of household objects, originally designed for pose estimation. The main reason why the HOPE dataset is so interesting is that the training dataset is based on almost 50,000 synthetic images, whereas the test dataset consists of just 457 real images. A unique challenge that is often overlooked but fairly common, especially in robotics (Hoda\u0148 et al., 2020; Sundermeyer et al., 2023; Hodan et al., 2024).\nData Augmentations. Regardless of the trained model, we apply random cropping with a crop size of 756 \u00d7 756 pixels on all datasets except NYUv2, which uses 630 \u00d7 476 pixels, and random horizontal flipping with a flip chance of 50%. For testing purposes, we use the original image resolutions as shown by Table 1.\nMetrics. For quantitative evaluations of the metric depth estimation, we report all common metrics: root mean squared error (RMSE), absolute relative error (AbsRel), logarithmic root mean squared error (log10), and three threshold-based accuracies ($\\delta_1$, $\\delta_2$, $\\delta_3$).\nTo evaluate the uncertainty quality, we exploit the following uncertainty evaluation metrics proposed by Mukhoti and Gal (2018):"}, {"title": "5.2 Quantitative Evaluation", "content": "Efficiency. As shown by Table 2, training times are comparable to the baseline for all methods, with LC and GNLL matching it exactly in both training and inference time. SE and MCD increase training times by around 5% - 10%. While SE nearly triples inference time and roughly doubles the trainable parameters, MCD requires roughly 10 times the inference time due to the costly sampling process that roughly scales linearly with the amount of samples. TTA also triples inference time as it requires three forward passes. Overall, LC and GNLL are the most efficient approaches as they do not require any additional computational overhead compared to the baseline. These experiments were conducted on the NYUv2 dataset only, so exact numbers may vary across datasets, but the general findings should be representative.\nNYUv2. Table 3 shows a quantitative comparison for the NYUv2 dataset (Silberman et al., 2012). LC, GNLL, and SE maintain depth quality similar to the baseline, with SE and GNLL even surpassing it for the ViT-S and ViT-B encoders, respectively. TTA and MCD exhibit a slight degradation but remain competitive.\nRegarding the uncertainty quality, GNLL emerges as the top-performing method, consistently outperforms all others in terms of p(acclcer) and p(unclina) across all three encoder sizes, achieving impressive values of up to 98.0% and 91.2%, respectively. For PAvPU, both GNLL and MCD deliver the best results. While all other methods are somewhat competitive with each other, LC clearly falls behind with regard to p(unclina), achieving only 26.8% for ViT-L.\nCityscapes. For the Cityscapes dataset (Cordts et al., 2016), as shown by Table 4, TTA stands out by significantly outperforming the baseline across all three encoder sizes in terms of depth quality. In contrast, LC, GNLL, and SE exhibit noticeable degradation, while MCD performs the worst.\nFor uncertainty quality, GNLL is the standout performer, decisively surpassing all other methods for all three metrics and encoder sizes. It achieves remarkable values of 69.2% for p(acclcer), 70.6% for p(unclina), and 69.5% for PAvPU, setting a benchmark for reliability in this dataset. The remaining methods deliver less consistent results, with TTA generally performing the worst, showing values as low as 28.8% for p(acclcer), 42.0% for p(unclina), and 39.6% for PAvPU. Interestingly, GNLL remains resilient despite the observed uncertainty degradation with increasing encoder size for most other methods.\nUseGeo. For the UseGeo dataset (Nex et al., 2024), as presented by Table 5, the depth quality results are inconsistent"}]}