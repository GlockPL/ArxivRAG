{"title": "Characterizing and Understanding HGNN Training on GPUs", "authors": ["DENGKE HAN", "MINGYU YAN", "XIAOCHUN YE", "DONGRUI FAN", "NINGHUI SUN"], "abstract": "Owing to their remarkable representation capabilities for heterogeneous graph data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in many critical real-world domains such as recommendation systems and medical analysis. Prior to their practical application, identifying the optimal HGNN model parameters tailored to specific tasks through extensive training is a time-consuming and costly process. To enhance the efficiency of HGNN training, it is essential to characterize and analyze the execution semantics and patterns within the training process to identify performance bottlenecks. In this study, we conduct an in-depth quantification and analysis of two mainstream HGNN training scenarios, including single-GPU and multi-GPU distributed training. Based on the characterization results, we disclose the performance bottlenecks and their underlying causes in different HGNN training scenarios and provide optimization guidelines from both software and hardware perspectives.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Graph Neural Networks (GNNs) have demonstrated exceptional capabilities in representing and processing graph data in non-Euclidean spaces [34, 35, 37, 51]. The early successes of GNNs are predominantly in the domain of homogeneous graphs (HomoGs), characterized by a single type of entity and adjacency relation [9, 15, 28]. However, many real-world data in complex systems are more aptly represented as heterogeneous graphs (HetGs) which consist of multiple types of entities and relations embodied by various types of vertices and edges, respectively. In contrast to HomoGs, HetGs possess not only structural information inherent in graph data but also rich semantic information harbored in the relations [26, 31]. Due to the powerful representation ability of HetGs, Heterogeneous Graph Neural Networks (HGNNs) have been developed and widely adopted in many critical fields including recommendation systems [1, 36, 48], cybersecurity [4, 11, 44], medical analysis [14, 20], traffic prediction [13, 23, 50] and many others.\nUnlike GNNs, which recursively aggregate the feature vectors of neighboring vertices [15, 17, 35] to obtain structural information in HomoGs, HGNNs employ a different execution semantics to extract both structural and semantic information. Specifically, most of the mainstream HGNN models can be partitioned into four primary execution stages [21, 25, 29, 32, 43]: \u2460 Semantic Graph Build (SGB); \u2461 Feature Projection (FP); \u2462 Neighbor Aggregation (NA); \u2463 Semantic Fusion (SF). The SGB stage partitions the original HetGs into multiple semantic graphs. The FP and NA stages perform conventional GNN processes, operating independently within each semantic graph. Subsequently, the SF stage fuses the results of the NA stage across different semantic graphs.\nApplying HGNNs to downstream tasks such as vertex classification or link prediction requires a comprehensive training process to find the optimal model parameters. Inherited from conventional GNNs, training HGNNs on computing nodes like GPUs primarily involves two methods: full-batch and mini-batch training. Unlike full-batch training on the entire graph, mini-batch training iteratively trains on mini-batches generated from a group of target vertices, significantly reducing memory footprints and speeding up convergence [9]. Moreover, distributed training across multiple computing nodes distributes the training load, improving the overall performance of the training process. Although the introduction of mini-batch training and distributed training solutions has improved the scalability and training efficiency of GNN models respectively, the training process itself remains extremely time-consuming and resource-intensive [27, 45]. HGNNs, due to their higher algorithmic complexity compared to GNNs [2], exhibit even higher costs than GNNs.\nQuantitatively analyzing and characterizing the execution behaviors and patterns of HGNN models is crucial for enhancing their execution efficiency. Currently, extensive efforts have been dedicated to characterizing GNNs [16, 33, 40, 45, 47], contributing to a substantial understanding within the research community of their execution patterns. However, there is limited effort focused on HGNN characterization. One one hand, GNNs and HGNNs exhibit distinct execution semantics and patterns, with HGNNs demonstrating more intricate execution behaviors. Consequently, characterization results for GNNs cannot be directly applied to optimize HGNN efficiency. On the other hand, existing characterization study on HGNNs [3, 38, 42] have primarily focused on inference tasks. Yet, the training process is significantly more complex than inference, rendering existing inference-focused characterizations insufficient for guiding training efficiency optimizations.\nTo address these gaps, we conduct a comprehensive characterization and quantitative analysis of HGNN training across single-node and multi-node distributed platforms, incorporating both full-batch and mini-batch training methods. Specifically, we undertake a quantitative characterization from various perspectives, encompassing the intrinsic characteristics of the HGNN execution stages, a comparative analysis of forward and backward propagation, the overall attributes of the training process, and the influence of metapath properties on training performance. Furthermore, a comparative analysis of HGNN and GNN training is carried out. Finally, we propose optimization guidelines from both software and hardware perspectives. We systematically summarize our 22 findings based on the four aforementioned characterization perspectives as follows:\n\u2022 Execution Characteristics of HGNN Models: (1) The NA stage dominates the most execution time both in forward and backward propagation, which is considered the predominant stage across all the execution stages of HGNNs; (2) There exists a distinct hybrid execution pattern between HGNN stages, each characterized by unique execution bounds, resulting in varying demands on hardware resources.; (3) Mini-batch sampling accounts for the majority of the execution time in each epoch of mini-batch training in both single-GPU and multi-GPU distributed scenarios.\n\u2022 Comparison between Backward and Forward Propagation: (1) Forward propagation is generally more time-consuming than backward propagation; (2) Although the kernels used during backward propagation are similar to those in forward propagation, there are significant differences in time distribution across kernel types and in the execution characteristics of the same kernels; (3) Backward propagation demands greater memory access and exhibits lower data locality than the forward pass. However, execution stages characterized by a high number of additions are anticipated to incur a lower computational workload in backward propagation."}, {"title": "2 BACKGROUND", "content": "Fig 1 illustrates a simple example of a HetG from the ACM dataset, which includes three types of vertices, A (Author), P (Paper), and S (Subject), along with three types of adjacency relations between them: author writes W paper, paper cites paper, and paper erbelongs to subject (abbreviated as AP, PP and PS). Moreover, the inverse of these relations also holds significant meaning like PA and SP. In addition to direct relations like AP, different combinations of these direct relations can form higher-order relations, referred to as metapaths. For instance, in Fig. 1, PSP represents a metapath composed of PS and SP, signifying that two papers are linked by a shared subject, implying a strong likelihood that the two papers pertain to the same research area. Each type of relation or metapath represents a unique semantic information between the two endpoints connected."}, {"title": "2.1 Heterogeneous Graphs and Semantic Graphs", "content": "Fig 1 illustrates a simple example of a HetG from the ACM dataset, which includes three types of vertices, A (Author), P (Paper), and S (Subject), along with three types of adjacency relations between them: author writes W paper, paper cites paper, and paper erbelongs to subject (abbreviated as AP, PP and PS). Moreover, the inverse of these relations also holds significant meaning like PA and SP. In addition to direct relations like AP, different combinations of these direct relations can form higher-order relations, referred to as metapaths. For instance, in Fig. 1, PSP represents a metapath composed of PS and SP, signifying that two papers are linked by a shared subject, implying a strong likelihood that the two papers pertain to the same research area. Each type of relation or metapath represents a unique semantic information between the two endpoints connected."}, {"title": "2.2 Heterogeneous Graph Neural Networks", "content": "Given the inefficiencies of traditional GNNs in extracting semantic information from HetGs, a multitude of specialized HGNNs have recently emerged [6, 21, 25, 29, 32], which incorporate both"}, {"title": "2.3 HGNN Training", "content": "Fig. 2 depicts the training process of the HGNN models. The process of a single training epoch can generally be divided into four main steps as illustrated in Fig. 2(c). Firstly, during forward propagation, the embeddings of target vertices are computed according to the procedural steps as in model formula. Secondly, the loss computation (LC) stage transforms these embeddings into the vector space of classification categories, subsequently generating a probability distribution used to compute the loss function. Thirdly, backward propagation calculates the gradient of each model parameter relative to the loss function, employing the chain rule to determine the direction for parameter adjustments that yield the most rapid loss reduction. Finally, in the parameters update (PU) stage, model parameters are adjusted based on these gradients and a predefined learning rate."}, {"title": "2.3.1 Full-batch and Mini-batch Training", "content": "From the perspective of training methods, HGNN training can be categorized into two primary approaches: full-batch training, where the entire graph dataset is processed per epoch; and mini-batch training, which entails iterative processing of multiple mini-batches consisting of sets of target vertices and their respective neighbors within each epoch. As shown in Fig. 2(a), semantic graphs are first constructed from the original HetG based on the relation type or predefined metapaths when performing full-batch training. Subsequently, multiple epochs are executed to iteratively update the parameters as illustrated in Fig. 2(c).\nMini-batch training differs in that the input data during execution does not encompass the entire graph structure. Instead, target vertices are partitioned into multiple groups, and neighbors of these vertices are sampled based on specified traversal depth and the number of sampled neighbors, forming individual mini-batches as depicted in Fig. 2(b). Notably, when employing the mini-batch training method, the necessity of performing the SGB stage to construct a complete semantic graph is obviated. Instead, direct sampling is conducted, which can be regarded as a subset of the SGB execution process. Each training epoch involves the independent execution of multiple"}, {"title": "2.3.2 Single-node and Distributed Training", "content": "Distributed training has emerged as a solution to the limitations associated with the memory and computational capacity of a single machine. In practical applications, distributed training commonly employs mini-batch methods to achieve rapid convergence while preserving model accuracy [49]. As depicted in Fig. 2(d), each computing node in distributed training independently processes one batch. Following backward propagation to obtain gradients, synchronization across nodes is essential through an All-reduce operation, which ensures consistency of model parameters across all nodes at the outset of each epoch.\nThe principal benefits of distributed training encompass enhanced performance, facilitating parallel processing across multiple nodes; and expanded model scalability, enabling the training of more intricate models and processing of large-scale datasets. Moreover, this approach provides fault tolerance, as the failure of a single node does not necessarily disrupt the entire training process."}, {"title": "2.3.3 Workload Distribution", "content": "As shown in Fig. 2(a) and Fig. 2(b), the processes of SGB and mini-batch sampling are integral components of input data preparation, typically classified as data preprocessing. In contemporary mainstream training platforms, a heterogeneous configuration is frequently adopted, comprising a central host CPU and peripheral computing nodes, typically GPUs. Preprocessing tasks are typically executed by the CPU, which subsequently transfers the prepared data to the computing nodes for execution. These nodes are tasked with executing the entire training iterations and producing the final well-optimized model parameters."}, {"title": "3 CHARACTERIZATION METHODOLOGY", "content": "To comprehensively and thoroughly characterize the training process of HGNNs, we adopt a rational and rigorous evaluation approach. In this section, we initially outline our experimental setup, encompassing the experimental platform, software framework, model and dataset selection. Then we delineate our evaluation methods."}, {"title": "3.1 Experimental Setup", "content": "The configuration of our experimental platform is detailed in Table 1, featuring computing nodes equipped with four advanced A100 GPUs. The four GPUs are organized into two groups, with intra-group connections facilitated by NVLink buses, while inter-group connections are established through PCIe buses. We utilize Nsight Systems and Nsight Compute tools to capture detailed performance metrics data of execution on the GPUs. Regarding the software framework, we choose DGL [30], which emerges as one of the most prominent GNN frameworks, typically outperforming PyG [5] in terms of runtime efficiency and energy consumption [12]. All the experiments are conducted utilizing 32-bit floating-point data format."}, {"title": "3.1.1 Platforms", "content": "The configuration of our experimental platform is detailed in Table 1, featuring computing nodes equipped with four advanced A100 GPUs. The four GPUs are organized into two groups, with intra-group connections facilitated by NVLink buses, while inter-group connections are established through PCIe buses. We utilize Nsight Systems and Nsight Compute tools to capture detailed performance metrics data of execution on the GPUs. Regarding the software framework, we choose DGL [30], which emerges as one of the most prominent GNN frameworks, typically outperforming PyG [5] in terms of runtime efficiency and energy consumption [12]. All the experiments are conducted utilizing 32-bit floating-point data format."}, {"title": "3.1.2 HGNN Models", "content": "We conduct experiments on three mainstream HGNN models, namely HAN [32], RGCN [25] and RGAT [29] as shown in Table 2, each with its own representative features. Specifically, RGCN first extends the conventional GCN [15] to handle HetGs by applying separate GCN convolutions to each semantic graph and aggregating the results through summation (SUM). RGAT builds upon this by introducing attention mechanisms in NA stage, with semantic fusion performed via SUM as well."}, {"title": "3.1.3 Benchmark Datasets", "content": "We employ four widely-used HetG datasets as benchmark datasets: ACM, IMDB, DBLP, and OGBN-MAG (MAG), as detailed in Table 3. ACM, DBLP, and MAG are citation datasets, while IMDB represents a movie dataset. These datasets range in size from tens of thousands to tens of millions and billions of edges, providing a comprehensive representation of graph data across diverse real-world application scenarios."}, {"title": "3.2 Evaluation Methods", "content": "A fundamental and crucial step in profiling involves delineating the scope of the profiling process. We utilize the CUDA interface provided by PyTorch to initiate and terminate the cudaProfiler. Additionally, we employ NVTX tags to encapsulate the code regions designated for profiling, thereby differentiating between distinct execution phases.\nNsight Systems is utilized to provide a holistic analysis of system-level performance, offering insights into the entire application execution process, including both GPU and CPU activities, as well as their interactions with system resources. Meanwhile, Nsight Compute specializes in conducting detailed analysis specifically on the execution of CUDA kernels on the GPU. It offers granular performance metric data at the GPU instruction level, such as execution time, memory access patterns, and instruction efficiency for each CUDA kernel function.\nDue to the GPU initialization processes involved in the initial training epochs, unless otherwise specified, the data presented in this paper are the geometric mean (GM) of results from the 5 epochs after excluding the first 3 epochs, which ensures the accuracy of the obtained results."}, {"title": "4 SINGLE-GPU TRAINING", "content": "Understanding the execution process on a single GPU is essential to comprehending the execution behavior and characteristics of HGNNs. In this section, we conduct a detailed performance analysis of two principal training methods, full-batch and mini-batch training, executed on a single GPU to identify the execution bottlenecks of HGNNs under different training methods."}, {"title": "4.1 Full-batch Training", "content": "Full-batch training enables the model to access the entire graph for thorough parameter updates, leading to accurate and stable gradient estimates; however, it necessitates substantial memory resources. In this section, we present a detailed quantitative analysis of execution time, execution bounds, memory patterns, and instruction issue stalls during full-batch training. Additionally, we examine the influence of variations in metapath properties on execution performance. Due to an Out of Memory (OOM) issue encountered while conducting full-batch training on the MAG dataset with the A100 GPU, this section includes results solely for the three smaller datasets. Furthermore, as the SGB stage is executed only once throughout the entire training process, its associated analysis will be presented in Section 6.1.1."}, {"title": "4.1.1 Execution Time Analysis", "content": "In this section, we provide a comprehensive analysis of the execution time during full-batch HGNN training. Our analysis focuses on identifying the principal execution components from two perspectives encompassing various execution stages and the distinct kernels within each stage.\nBreaking down the execution time by stages allows us to understand the time proportion of each execution stage, thereby identifying the main execution stages for targeted optimization. Fig. 3 shows the profiling results of the time breakdown by stages. From the overall training perspective, 1 forward propagation is more time-consuming than backward propagation. This phenomenon occurs because, although the backward stage involves the reverse operations of forward propagation, it can directly reuse numerous intermediate results from the forward process. The experimental result in Fig. 3(a) indicates that forward propagation accounts for an average of 48.33% of the total execution time across different HGNN models and datasets, while backward propagation averages 42.37%. In terms of HGNN execution stages, 2 the NA stage dominates the most execution time both in forward and backward propagation, which is considered the predominant stage across all the HGNN stages. This is because each edge in every semantic graph requires one execution process during the NA stage, making the load during NA stage the heaviest. As shown in Fig. 3(b) and Fig. 3(c), the NA stage accounts for an average of 77.80% of the execution time in the forward propagation and 77.53% in the backward propagation."}, {"title": "4.1.2 Execution Bounds Analysis", "content": "In this section, we conduct a detailed analysis of the performance metrics of CUDA kernels predominantly invoked during each execution stage to identify the hardware resource bounds encountered at various stages."}, {"title": "4.1.3 Memory Pattern Analysis", "content": "In this section, we focus on the memory aspects, encompassing DRAM access, memory footprint, and cache hit rates, to elucidate the memory characteristics of various execution stages during the HGNN training process.\nMemory access latency is a critical factor contributing to performance degradation during model execution, and it is associated with high energy consumption. A comprehensive understanding of memory access patterns across various stages of HGNN training facilitates both performance and energy efficiency improvement. Fig. 6 illustrates the DRAM access breakdown of various models during the training process of HGNN on different datasets. From a training perspective, 7 the backward propagation generally incurs more memory access compared to the forward propagation. This phenomenon arises due to the reason that during the backward propagation, the model must access gradient data that matches the size of the dataset accessed in forward propagation. Additionally, the computation of gradients requires the accesses of numerous intermediate results preserved from the forward propagation. As presented in Fig. 6 (a), the backward propagation accounts for 55.38% of the total memory access during the entire training process, while the forward propagation averages only 42.87%.\nLooking specifically at the internal stages of the forward and backward propagation, there are significant differences in memory access distribution among different models. However, from the"}, {"title": "4.1.4 Issue Stall Analysis", "content": "This section provides a detailed analysis of GPU instruction issue stalls encountered during the training process. The focus is on elucidating the temporal distribution and underlying causes of these stalls at each stage. Analyzing instruction issue stalls can reveal operational bottlenecks from an alternative perspective, thereby offering pathways for targeted optimization strategies.\nOverall Profiling Results. Fig. 8(a) presents the ratio of stall time to execution time for different execution stages of the training process. From a comprehensive viewpoint, the prevalence of instruction issue stall across different stages throughout the training process is notable and merits careful consideration, averaging 33.21% across different stages. Fig. 8(c) illustrates the breakdown of instruction issue stall reasons for the main CUDA kernels invoked at each execution stage. It can be observed that \u3260 the vast majority of kernel stalls originate from memory dependency except for sgemm kernel, and the proportion of stall time attributable to memory dependency during the NA stage is the highest among all execution stages. This is because, during the training process of HGNN, other than sgemm, the kernels involve a large number of reads and writes to irregular graph data. These access patterns are usually highly irregular, making it difficult for the accessed data to be fully cached. Therefore, stalls caused by memory dependencies dominate. Furthermore, the NA stage, which predominantly accesses irregular graph data during aggregation, thereby resulting in the most significant instruction issue stalls attributable to memory dependency. As shown in Fig. 8(c), for kernels other than sgemm, memory dependency accounts for an average of 74.11% of the instruction issue stalls. In contrast, for the sgemm kernel, memory dependency accounts for an average of only 15.51%, with the dominant stalls being caused by execution dependency, which average 18.86%. Fig. 8(b) illustrates that the NA stage exhibits the highest ratio of stalls caused by memory dependency relative to total elapsed time across all execution stages during both forward and backward propagation.\nFig. 8. Issue stall of HAN model on DBLP dataset: (a) The ratio of stall time to elapsed time.; (b) Memory\ndependency stall ratio to elapsed time; (c) Breakdown of issue stall reasons of different stages.\nComparison of Backward and Forward. Fig. 8(b) presents the proportion of stalls attributed to memory dependency in relation to the total elapsed time for each execution stage of HGNN training during both forward and backward propagation. The fraction of overall execution time attributed to memory dependency during each stage of backward propagation surpasses that observed in forward propagation. The heightened memory access volume during the backward propagation stage, coupled with increased data type diversity, leads to diminished data locality and reduced cache hit rates, as detailed in Section 4.1.3. As shown in Fig. 8(b), the average proportion of memory dependency during each stage of backward propagation is 2.26\u00d7 higher than that during forward propagation."}, {"title": "4.1.5 Exploring Metapath Changes", "content": "In this section, we explore the performance impact on each execution stage as the length and number of metapaths vary. As the number of metapaths increases significantly, it can greatly expand the size of the graph data, potentially leading to an OOM issue on a single GPU. Therefore, in the experiments in this section, we use the IMDB dataset as a representative example, focusing on the HAN model. And the performance metrics for each stage covered in this section are obtained by averaging forward and backward propagation.\nExploring the performance changes at different stages due to variations in metapath length is crucial, as longer metapaths can assist models in capturing more complex relationship patterns and context information over greater distances. Fig. 9(a) shows the execution time of different stages across varying metapath lengths. Obviously, \u2192 increase in the length of the metapath only significantly increase the execution time of NA stage, while the FP and SF stages are almost unaffected. The rationale behind this lies in the fact that the length increase of the metapath does not alter the vertex types or the number of semantic graphs, thereby maintaining the workloads on the FP and SF stages. However, elongating the metapath results in denser semantic graphs [42], characterized by a higher volume of edges. This directly translates into increased execution times during the NA stage. As depicted in Fig. 9(a), an increase in metapath length from 3 to 9 correlates with a 4.12\u00d7 rise in execution time for the NA stage, while the FP and SF stages exhibit negligible change in execution times.\nExploring the impact of varying the number of metapaths on the execution performance of different stages is essential, as a greater number of metapaths can provide the model with richer semantic graph information, thereby enhancing model accuracy. Fig. 9(b) illustrates the impact of increasing the number of metapaths while keeping the metapath length constant on the performance at various execution stages. According to the figure, \u2192 increase in the number of metapaths leads to increased execution times across all stages of HGNN, with the NA stage showing the most noticeable growth trend. As the number of metapaths increases, so does the number of semantic graphs, resulting in a higher volume of vertices and edges to process. Consequently, each execution stage experiences a proportional increase in workload. Notably, the NA stage shows the most pronounced growth, primarily because the rise in edges outpaces that of nodes and even semantic graphs. As illustrated in Fig. 9(b), expanding the metapath count from 1 to 8 leads to execution time increases of 4.71\u00d7 for FP, 9.34\u00d7 for NA, and 1.40\u00d7 for SF."}, {"title": "4.2 Mini-batch Training", "content": "As the size of real-world graph datasets continues to expand, mini-batch training has increasingly become the predominant method for model training, which leads to faster convergence and improved generalization performance compared to full-batch training. Aside from the additional mini-batch sampling stage, the execution behavior and characteristics are identical to those in"}, {"title": "4.2.1 Execution Analysis", "content": "This section provides a detailed analysis of the execution time breakdown during mini-batch training, compares the performance of GPU sampling with that of CPU sampling, and examines various performance metrics of the CUDA kernels employed in GPU sampling to elucidate the characteristics of the sampling operation.\nBreaking down the execution time facilitates the identification of the primary execution processes during mini-batch training. Fig. 10(a) shows the breakdown of execution time across various execution stages. 5 Mini-batch sampling occupies the majority of the execution time in each epoch of mini-batch training, even exceeding the combined time of all other execution stages. This is due to mini-batch sampling in HGNN models requiring sampling operations across multiple semantic graphs not just one graph as in conventional GNNs, entailing traversal of intricate and irregular graph structures. Furthermore, this procedure is carried out on CPU, which is markedly time-intensive in contrast to the highly parallelized computational workload executed on GPU. As presented in Fig. 10(a), mini-batch sampling accounts for an average of 51.01% of the total time across different models and datasets. Especially for the HAN model on the MAG dataset, the time elpsed on sampling in each epoch accounts for as much as 88.92%.\nIn general, the mini-batch sampling process is categorized as a preprocessing step, managed by the CPU as introduced in Section 2. However, compared to CPU sampling, 6 employing GPUs for sampling yields a substantial enhancement in sampling performance, particularly discernible in the context of large-scale datasets. This is attributed to GPUs offering ample bandwidth resources for graph traversal, coupled with their inherently highly parallel architecture that facilitates simultaneous sampling of multiple target vertices. Fig. 10(b) demonstrates that using GPU for mini-batch sampling directly can achieve an average acceleration of 2.43\u00d7, 2.06\u00d7, and 1.21\u00d7 compared to CPU, under different batch sizes of 64, 128, and 256, respectively. For larger-scale datasets like MAG, GPU sampling achieves an average speedup of 5.78\u00d7, 6.35\u00d7, and 8.62\u00d7 with batch sizes of 512, 1024, and 2048, respectively, as shown in Fig. 10(c).\nWe conduct an in-depth analysis of the mini-batch sampling stage utilizing GPU-based sampling techniques, and present the performance metrics of the primary kernels in Table 5. The performance metrics detailed here pertain exclusively to the CUDA kernels specific to the mini-batch sampling stage, distinguishing them from those involved in the main execution stages."}, {"title": "4.2.2 Exploring Metapath Changes", "content": "In this section, we explore the impact of changes in metapath properties on the efficiency of mini-batch sampling, considering both the length and number of metapaths. We conduct experiments using two datasets, DBLP and MAG, to reflect the varying impacts of metapath property changes across different dataset scales.\nFig. 11(a) and Fig. 11(b) illustrate the impact on the execution time of mini-batch sampling as metapath length varies. \u2714 For smaller datasets, the time required for mini-batch sampling exhibits minimal sensitivity to variations in metapath length. Conversely, large-scale datasets experience prolonged execution times as metapath length increases. Increasing the length of metapath augments the number of edges within the semantic graph, consequently elevating the number of edges sampled per batch. However, thread-level parallelism is utilized during sampling process. The additional sampling workload, facilitated by adequate hardware resources, does not significantly affect overall performance. Notably, sampling time increases only when the workload surpasses the hardware's maximum parallel processing capacity. As shown in Fig. 11(a), compared to a metapath length of 3, the sampling time ratio remains around 1 as the metapath length increases on DBLP dataset. However, as depicted in Fig. 11(b), for larger datasets MAG, as the metapath length increases, the number of sampled edges in each batch grows more significantly, and the mini-batch sampling time also increases accordingly.\nIncreasing the number of metapaths directly leads to an increase in the number of semantic graphs. Fig. 11(c) and Fig. 11(d) illustrate the variation in mini-batch sampling time with an increasing number of metapaths. \u246c For both small-scale and large-scale datasets, the sampling time of mini-batches is sensitive to changes in the number of metapaths. The increase in the number of semantic graphs due to the growth in metapath number results"}, {"title": "5 MULTI-GPU TRAINING", "content": "Distributed training, leveraging multiple GPUs for model training, is driven by the increasing complexity of real-world datasets and models, which can be implemented using either full-batch or mini-batch methods. In comparison to full-batch distributed training, mini-batch distributed training can markedly reduce the convergence time of the training process while preserving model accuracy. Furthermore, it demands fewer hardware resources, rendering it superior in terms of both performance and energy efficiency. Consequently, we focuse exclusively on the analysis of the mini-batch distributed training method in this section."}, {"title": "5.1 Overall Profiling Results", "content": "Fig. 12(a) illustrates the normalized execution time of distributed training using multiple GPUs compared to training on a single GPU. As depicted in the figure, \u2192 the performance improvement ratio in multi-GPU distributed training scenarios diverges from the ideal ratio. For smaller-scale datasets such as DBLP, increasing the number of devices may even result in a significant decline in overall training performance. This is mainly because an increase in the number of devices leads to competition for shared resources such as cache and bandwidth, thereby reducing performance. Relevant analysis will be provided in Section 5.2. As shown in Fig. 12(a), the overall execution time of two-GPUs and four-GPUs training are an average 1.37\u00d7 and 8.69\u00d7 compared to training on a single GPU for DBLP dataset. While linear improvements are evident in larger-scale datasets like MAG, scenarios involving two GPUs and four GPUs yield average performance enhancements of merely 1.66\u00d7 and 2.13\u00d7 respectively, falling short of the anticipated 2\u00d7 and 4\u00d7 improvements."}, {"title": "5.1.1 Performance Comparison with Different Number of GPUs", "content": "Fig. 12(a) illustrates the normalized execution time of distributed training using multiple GPUs compared to training on a single GPU. As depicted in the figure, \u2192 the performance improvement ratio in multi-GPU distributed training scenarios diverges from the ideal ratio. For smaller-scale datasets such as DBLP, increasing the number of devices may even result in a significant decline in overall training performance. This is mainly because an increase in the number of devices leads to competition for shared resources such as cache and bandwidth, thereby reducing performance. Relevant analysis will be provided in Section 5.2. As shown in Fig. 12(a), the overall execution time of two-GPUs and four-GPUs training are an average 1.37\u00d7 and 8.69\u00d7 compared to training on a single GPU for DBLP dataset. While linear improvements are evident in larger-scale datasets like MAG, scenarios involving two GPUs and four GPUs yield average performance enhancements of merely 1.66\u00d7 and 2.13\u00d7 respectively, falling short of the anticipated 2\u00d7 and 4\u00d7 improvements."}, {"title": "5.1.2 Execution Time Breakdown", "content": "Fig. 12(b) presents the execution time breakdown for distributed training with different numbers of GPUs. For clarity, we only present results on the MAG dataset. According to the figure, mini-batch sampling is the predominant execution stage during the distributed training process of HGNN, occupying the vast majority of the execution time. This is consistent with the mini-batch training scenario on a single GPU, as discussed in Section 4.2.1. Due to the complexity of HGNN sampling and the inefficiency of sampling on the CPU, mini-batch sampling has become the primary performance bottleneck."}, {"title": "5.2 In-depth Analysis", "content": "To better highlight the decrease in sampling efficiency caused by CPU resource contention, we adopt an adaptive batch size approach here. Specifically, for each epoch, we shuffle the order of target vertices and evenly divide them into num_dst_vertices/num_gpus groups according to the number of GPUs. Each group is then sampled in parallel, and the complete batch after sampling is sent to a separate GPU for execution. We only present the relevant profiling results on the HAN model, with the results on other two models exhibit similar characteristics.\nAs shown in Fig. 13(a), as the number of devices increases, the average number of edges sampled per batch for each GPU decreases. In theory, the average sampling time should also decrease, but in reality, the sampling time shows an increasing trend. Compared to a single GPU, in scenarios with two GPUs and four GPUs, the average time for mini-batch sampling is 1.29 \u00d7 and 6.49 \u00d7 that of a single GPU, respectively. \u2192 The primary factor limiting multi-process parallel sampling is the competition for shared CPU resources. As in Fig. 13(b), an increase in the number of context switches as the number of GPU increase indicates that due to the concurrent execution of multiple sampling processes, CPU cores become more finely divided in terms of time slice allocation, leading to increased overhead of context switching. The decrease in LLC hit rate indicates that contention for the shared cache by multiple sampling processes reduces the locality of cache data in each sampling process."}, {"title": "5.2.1 CPU Resources Competition During Sampling", "content": "To better highlight the decrease in sampling efficiency caused by CPU resource contention, we adopt an adaptive batch size approach here. Specifically, for each epoch, we shuffle the order of target vertices and evenly divide them into num_dst_vertices/num_gpus groups according to the number of GPUs. Each group is then sampled in parallel, and the complete batch after sampling is sent to a separate GPU for execution. We only present the relevant profiling results on the HAN model, with the results on other two models exhibit similar characteristics.\nAs shown in Fig. 13(a), as the number of devices increases, the average number of edges sampled per batch for each GPU decreases. In theory, the average sampling time should also decrease, but in reality, the sampling time shows an increasing trend. Compared to a single GPU, in scenarios with two GPUs and four GPUs, the average time for mini-batch sampling is 1.29 \u00d7 and 6.49 \u00d7 that of a single GPU, respectively. \u2192 The primary\u2022 The primary factor limiting multi-process parallel sampling\nis the competition for shared CPU resources. As in Fig. 13(b), an increase in the number of context switches as the number of GPU increase indicates that due to the concurrent execution of multiple sampling processes, CPU cores become more finely divided in terms of time slice allocation, leading to increased overhead of context switching. The decrease in LLC hit rate indicates that contention for the shared cache by multiple sampling processes reduces the locality of cache data in each sampling process."}, {"title": "5.2.2 Bandwidth Competition During Data Loading and Synchronization", "content": "Data loading and gradient synchronization are two critical steps in distributed training. The factors limiting the performance of data loading and synchronization in multi-device scenarios are primarily the contention for shared bandwidth."}, {"title": "6 COMPARISON WITH GNN TRAINING", "content": "In this section, we mainly discuss the comparison between HGNN training and GNN training, focusing on the differences in their execution stages and the transfer of performance bottlenecks during the training process."}, {"title": "6.1 Difference in execution process", "content": ""}, {"title": "6.1.1 Unique Process of Metapath Instance Generation", "content": "Given that HGNNs primarily operate on semantic graphs during execution, it is imperative to construct these semantic graphs from the original HetG, which is unique to HGNNs. Our experimental results for the HAN model on different datasets indicate that the execution time of the SGB stage is, on average, 11.46\u00d7 the execution time of one single training epoch. Furthermore, as the length and number of metapaths increase, the execution time of the SGB stage exhibits a marked increase, irrespective of the dataset's scale. For large-scale HetG datasets, SGB will be a considerably time-consuming stage during in the training process."}, {"title": "6.1.2 Separate Feature Projection", "content": "The raw feature vectors of vertices in HomoGs reside in the same vector space and share identical dimensionality, allowing for joint projection. In contrast, HGNNs utilize separate feature projection matrices for each vertex type or each semantic graph, necessitating the reduction of gradients of the projection weight matrices for the same type of vertices appearing in different semantic graphs. As shown in Table 4, during backward propagation in the FP stage, the percentage of time occupied by the sgemm kernel decreased from 98.94% to 45.3%, while the EleWise kernel increased from less than 1% to 36.76%. This shift causes the FP stage in backward propagation to transition from being initially compute-bound to becoming memory-bound, further exacerbating the hybrid execution patterns during the training process."}, {"title": "6.1.3 Intricate Two-level Aggregation", "content": "GNNs perform a single aggregation step for neighboring vertices within a singular type of relation. In contrast, HGNNs aggregate features from neighbors in each semantic graph generated according to corresponding semantics (relations or metapaths), and then fuse intermediate results of each semantic graph for each vertex. As depicted in Fig. 3, the incorporation of a secondary aggregation level, denoted as the SF stage, incurs an approximate 15% increase in time overhead per epoch."}, {"title": "6.2 Changes in execution bottlenecks", "content": ""}, {"title": "6.2.1 More Intricate Hybrid Execution Pattern", "content": "Prior work [40] highlights the presence of hybrid execution pattern in the execution of the typical GCN model. Specifically, the FP stage (Combination stage in work [40]) exhibits a more regular pattern and demonstrates compute-bound execution modes, whereas the NA (Aggregation stage) stage involves numerous random accesses, resulting in memory-bound behavior. According to the analysis in Section 4.1.2, compared to GNNs, HGNNs exhibit more pronounced and complex hybrid execution pattern, which stems from their more intricate model structure."}, {"title": "6.2.2 Bottleneck Migration in Distributed Training", "content": "Prior work [16] demonstrates that the data loading stage dominates the most execution time of GNN distributed training. Conversely, in HGNNs, the mini-batch sampling process incurs markedly higher overhead than data loading and is considered the predominant execution process. This shift arises primarily from the necessity in HGNNs to sample multiple semantic graphs to form a batch, compared to GNNs which sample from a single graph as surveyed in work [19, 22]. Moreover, models employing metapath-based graph construction experience significant overhead in neighbor sampling due to the intricate traversal of multi-hop neighbors following each meatapath."}, {"title": "7 OPTIMIZATION GUIDELINES", "content": "In this section, leveraging the findings previously outlined, we provide guidences for optimizing HGNN training from both software and hardware perspectives."}, {"title": "7.1 Software Perspective", "content": ""}, {"title": "7.1.1 Reasonable Overlapping of Phases", "content": "On one hand, a bound-aware kernel fusion method can be proposed to facilitate the overlapping execution of stage with differing bounds. Observations 5 and underscore the presence of intricate hybrid execution patterns during HGNN training. These stages, distinguished by diverse execution bounds, frequently alternate, facilitating overlap execution to harness multiple hardware resources concurrently. This approach enhances overall execution performance by optimizing hardware resource utilization. For example, Graphite [7] adopts similar approach to accelerate GNNs on the CPU.\nMoreover, training phases executed on different devices can be overlapped to reduce overall time overhead. Observation 15 and 20 indicate that using a mini-batch-based training method makes the mini-batch sampling process the primary execution stage. Fortunately, there is inherent parallelism between the sampling process executed on the CPU and the workload computation process executed on the GPU. The training paradigm can be adjusted to start the sampling process for the next epoch while performing the computation for the current epoch, like PaGraph [18] which overlaps mini-batch sampling with data loading to eliminate sampling overhead during GNN training."}, {"title": "7.1.2 Recomputing to Reduce Memory Cost", "content": "Observation 7 and indicate that compared to the inference process alone, the training process requires more memory storage, primarily due to the need for direct memory access to reuse a large number of intermediate results preserved during the forward propagation. However, observation suggests that operations involving addition in the forward propagation process incur no computational cost in the backward propagation process. This characteristic of backward propagation, reducing computation while increasing memory usage compared to forward propagation, enables the possibility of recomputing certain intermediate results to conserve memory. Prior work [46] proposes an evaluation method to strike a reasonable trade-off between recomputation cost and memory overhead, aiming to judiciously recompute"}, {"title": "7.1.3 Scheduling Based on Semantic Graphs for Data Reuse", "content": "Processing multiple semantic graphs is an important feature of HGNNs compared to GNNs. Determining the optimal execution order of semantic graphs based on the number of shared vertices between different semantic graphs can maximize data reuse between the graphs, thereby reducing off-chip memory accesses. Pioneered by HiHGNN [38], the concept of semantic graph similarity was introduced to maximize reusable data in HGNN inference processes. This can also be extended and applied to HGNN training, exploring optimal execution orders across multiple layers and epochs to maximize data reuse and reduce off-chip memory accesses. Besides, GDR-HGNN[39] utilizes the bipartite nature of semantic graphs to decompose them in order to enhance data locality."}, {"title": "7.2 Hardware Acceleration", "content": ""}, {"title": "7.2.1 Independent Parallel Neighbor Traversal Unit", "content": "Both the extremely time-consuming mini-batch sampling and SGB stages involve traversing neighbors of target vertices. As indicated by the analysis in Section 4.2.2, this traversal process can be effectively parallelized across various target vertices. However, Section 5.2.1 and observation \u2192 underscore that concurrent sampling processes frequently contend for cache resources, leading to notable performance degradation in neighbor sampling tasks. Consequently, it is advisable for researchers to design specialized neighbor traversal units. Each unit should independently cache relevant neighbor information specific to its assigned target vertex, thereby leveraging sampling parallelism while mitigating cache contention. Moreover, optimization structures tailored for graph processing with irregular memory access patterns such as Graphicinado [8] and GraphDynS [41] could also be contemplated for enhancing neighbor traversal efficiency."}, {"title": "7.2.2 Unified Reconfigurable Execution Unit", "content": "Observations 5 and underscore the presence of intricate hybrid execution patterns during HGNN training, resulting in varied utilization of hardware resources across different stages and an inability to fully harness the maximum efficiency of the hardware platform. ADE-HGNN [10] advocates for employing reconfigurable architectures to streamline HGNN inference execution. Given the complex nature of the training process, unified architectures hold the potential to substantially optimize hardware resource utilization."}, {"title": "7.2.3 Multi-lane Architecture Supporting Semantic-graph-level Parallelism", "content": "Besides enabling data reuse, multiple semantic graphs offer a form of parallelism unique to traditional GNNs, termed semantic graph parallelism. In HGNN execution, the operations within each semantic graph prior to the SF stage are independent of other graphs, inherently possessing parallelism. Researchers can improve training efficiency by devising multi-channel hardware architectures that facilitate the concurrent execution of various semantic graphs like HiHGNN [38] proposed for HGNN inference acceleration."}, {"title": "8 CONCLUSION", "content": "The complex and costly training process is crucial for effectively utilizing HGNNs. In this work, we comprehensively analyze different training methods and scenarios using NVIDIA GPU A100 platform, revealing the execution semantics and patterns of the HGNN training process, and uncovering the performance bottlenecks. Additionally, we compare some similarities and differences between HGNN and GNN training. Finally, we provide optimization guidelines from both software and hardware perspectives."}]}