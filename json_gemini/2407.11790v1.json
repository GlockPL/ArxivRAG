{"title": "Characterizing and Understanding HGNN Training on GPUs", "authors": ["DENGKE HAN", "MINGYU YAN", "XIAOCHUN YE", "DONGRUI FAN", "NINGHUI SUN"], "abstract": "Owing to their remarkable representation capabilities for heterogeneous graph data, Heterogeneous Graph\nNeural Networks (HGNNs) have been widely adopted in many critical real-world domains such as recommen-\ndation systems and medical analysis. Prior to their practical application, identifying the optimal HGNN model\nparameters tailored to specific tasks through extensive training is a time-consuming and costly process. To\nenhance the efficiency of HGNN training, it is essential to characterize and analyze the execution semantics\nand patterns within the training process to identify performance bottlenecks. In this study, we conduct an\nin-depth quantification and analysis of two mainstream HGNN training scenarios, including single-GPU and\nmulti-GPU distributed training. Based on the characterization results, we disclose the performance bottlenecks\nand their underlying causes in different HGNN training scenarios and provide optimization guidelines from\nboth software and hardware perspectives.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Graph Neural Networks (GNNs) have demonstrated exceptional capabilities in\nrepresenting and processing graph data in non-Euclidean spaces [34, 35, 37, 51]. The early successes\nof GNNs are predominantly in the domain of homogeneous graphs (HomoGs), characterized by a\nsingle type of entity and adjacency relation [9, 15, 28]. However, many real-world data in complex\nsystems are more aptly represented as heterogeneous graphs (HetGs) which consist of multiple types\nof entities and relations embodied by various types of vertices and edges, respectively. In contrast\nto HomoGs, HetGs possess not only structural information inherent in graph data but also rich\nsemantic information harbored in the relations [26, 31]. Due to the powerful representation ability of\nHetGs, Heterogeneous Graph Neural Networks (HGNNs) have been developed and widely adopted\nin many critical fields including recommendation systems [1, 36, 48], cybersecurity [4, 11, 44],\nmedical analysis [14, 20], traffic prediction [13, 23, 50] and many others.\nUnlike GNNs, which recursively aggregate the feature vectors of neighboring vertices [15, 17, 35]\nto obtain structural information in HomoGs, HGNNs employ a different execution semantics to\nextract both structural and semantic information. Specifically, most of the mainstream HGNN\nmodels can be partitioned into four primary execution stages [21, 25, 29, 32, 43]: \u2460 Semantic\nGraph Build (SGB); \u2461 Feature Projection (FP); \u2462 Neighbor Aggregation (NA); \u2463 Semantic Fusion\n(SF). The SGB stage partitions the original HetGs into multiple semantic graphs. The FP and NA\nstages perform conventional GNN processes, operating independently within each semantic graph.\nSubsequently, the SF stage fuses the results of the NA stage across different semantic graphs.\nApplying HGNNs to downstream tasks such as vertex classification or link prediction requires a\ncomprehensive training process to find the optimal model parameters. Inherited from conventional\nGNNs, training HGNNs on computing nodes like GPUs primarily involves two methods: full-\nbatch and mini-batch training. Unlike full-batch training on the entire graph, mini-batch training\niteratively trains on mini-batches generated from a group of target vertices, significantly reducing\nmemory footprints and speeding up convergence [9]. Moreover, distributed training across multiple\ncomputing nodes distributes the training load, improving the overall performance of the training\nprocess. Although the introduction of mini-batch training and distributed training solutions has\nimproved the scalability and training efficiency of GNN models respectively, the training process\nitself remains extremely time-consuming and resource-intensive [27, 45]. HGNNs, due to their\nhigher algorithmic complexity compared to GNNs [2], exhibit even higher costs than GNNs.\nQuantitatively analyzing and characterizing the execution behaviors and patterns of HGNN\nmodels is crucial for enhancing their execution efficiency. Currently, extensive efforts have been\ndedicated to characterizing GNNs [16, 33, 40, 45, 47], contributing to a substantial understanding\nwithin the research community of their execution patterns. However, there is limited effort focused\non HGNN characterization. One one hand, GNNs and HGNNs exhibit distinct execution semantics\nand patterns, with HGNNs demonstrating more intricate execution behaviors. Consequently, char-\nacterization results for GNNs cannot be directly applied to optimize HGNN efficiency. On the other\nhand, existing characterization study on HGNNs [3, 38, 42] have primarily focused on inference\ntasks. Yet, the training process is significantly more complex than inference, rendering existing\ninference-focused characterizations insufficient for guiding training efficiency optimizations.\nTo address these gaps, we conduct a comprehensive characterization and quantitative analysis of\nHGNN training across single-node and multi-node distributed platforms, incorporating both full-\nbatch and mini-batch training methods. Specifically, we undertake a quantitative characterization\nfrom various perspectives, encompassing the intrinsic characteristics of the HGNN execution\nstages, a comparative analysis of forward and backward propagation, the overall attributes of the\ntraining process, and the influence of metapath properties on training performance. Furthermore, a\ncomparative analysis of HGNN and GNN training is carried out. Finally, we propose optimization\nguidelines from both software and hardware perspectives. We systematically summarize our 22\nfindings based on the four aforementioned characterization perspectives as follows:\n\u2022 Execution Characteristics of HGNN Models: (1) The NA stage dominates the most execution time\nboth in forward and backward propagation, which is considered the predominant stage across\nall the execution stages of HGNNs; (2) There exists a distinct hybrid execution pattern between\nHGNN stages, each characterized by unique execution bounds, resulting in varying demands on\nhardware resources.; (3) Mini-batch sampling accounts for the majority of the execution time in\neach epoch of mini-batch training in both single-GPU and multi-GPU distributed scenarios.\n\u2022 Comparison between Backward and Forward Propagation: (1) Forward propagation is generally\nmore time-consuming than backward propagation; (2) Although the kernels used during back-\nward propagation are similar to those in forward propagation, there are significant differences in\ntime distribution across kernel types and in the execution characteristics of the same kernels; (3)\nBackward propagation demands greater memory access and exhibits lower data locality than\nthe forward pass. However, execution stages characterized by a high number of additions are\nanticipated to incur a lower computational workload in backward propagation."}, {"title": "2 BACKGROUND", "content": "Fig 1 illustrates a simple example of a HetG from the ACM dataset, which includes three types\nof vertices, A (Author), P (Paper), and S (Subject), along with three types of adjacency relations\nbetween them: author writes W paper, paper cites paper, and paper erbelongs to subject (abbreviated as AP, PP\nand PS). Moreover, the inverse of these relations also holds significant meaning like PA and SP.\nIn addition to direct relations like AP, different combinations of these direct relations can form\nhigher-order relations, referred to as metapaths. For instance, in Fig. 1, PSP represents a metapath\ncomposed of PS and SP, signifying that two papers are linked by a shared subject, implying a strong\nlikelihood that the two papers pertain to the same research area. Each type of relation or metapath\nrepresents a unique semantic information between the two endpoints connected.\nThe semantic graphs are derived from the original heterogeneous graph based on specific rela-\ntionships or metapaths. Each semantic graph contains only one type of relationship or metapath.\nAs shown in Fig. 1, we build a semantic graph based on AP, PP, PS, and PSP, respectively. Semantic\ngraphs separate multiple semantics in the original HetG, facilitating information extraction. Numer-\nous HGNN models [6, 32, 43] utilize metapaths to construct semantic graphs, while others [25, 29]\nemploy direct relations, i.e., edge types, for semantic graph construction.\nGiven the inefficiencies of traditional GNNs in extracting semantic information from HetGs, a\nmultitude of specialized HGNNs have recently emerged [6, 21, 25, 29, 32], which incorporate both"}, {"title": "2.3 HGNN Training", "content": "Fig. 2 depicts the training process of the HGNN models. The process of a single training epoch\ncan generally be divided into four main steps as illustrated in Fig. 2(c). Firstly, during forward\npropagation, the embeddings of target vertices are computed according to the procedural steps as\nin model formula. Secondly, the loss computation (LC) stage transforms these embeddings into the\nvector space of classification categories, subsequently generating a probability distribution used to\ncompute the loss function. Thirdly, backward propagation calculates the gradient of each model\nparameter relative to the loss function, employing the chain rule to determine the direction for\nparameter adjustments that yield the most rapid loss reduction. Finally, in the parameters update\n(PU) stage, model parameters are adjusted based on these gradients and a predefined learning rate.\nFrom the perspective of training methods, HGNN training\ncan be categorized into two primary approaches: full-batch training, where the entire graph dataset\nis processed per epoch; and mini-batch training, which entails iterative processing of multiple\nmini-batches consisting of sets of target vertices and their respective neighbors within each epoch.\nAs shown in Fig. 2(a), semantic graphs are first constructed from the original HetG based on the\nrelation type or predefined metapaths when performing full-batch training. Subsequently, multiple\nepochs are executed to iteratively update the parameters as illustrated in Fig. 2(c).\nMini-batch training differs in that the input data during execution does not encompass the entire\ngraph structure. Instead, target vertices are partitioned into multiple groups, and neighbors of\nthese vertices are sampled based on specified traversal depth and the number of sampled neighbors,\nforming individual mini-batches as depicted in Fig. 2(b). Notably, when employing the mini-batch\ntraining method, the necessity of performing the SGB stage to construct a complete semantic\ngraph is obviated. Instead, direct sampling is conducted, which can be regarded as a subset of\nthe SGB execution process. Each training epoch involves the independent execution of multiple"}, {"title": "2.3.2 Single-node and Distributed Training", "content": "Distributed training has emerged as a solution to\nthe limitations associated with the memory and computational capacity of a single machine. In\npractical applications, distributed training commonly employs mini-batch methods to achieve rapid\nconvergence while preserving model accuracy [49]. As depicted in Fig. 2(d), each computing node\nin distributed training independently processes one batch. Following backward propagation to\nobtain gradients, synchronization across nodes is essential through an All-reduce operation, which\nensures consistency of model parameters across all nodes at the outset of each epoch.\nThe principal benefits of distributed training encompass enhanced performance, facilitating\nparallel processing across multiple nodes; and expanded model scalability, enabling the training of\nmore intricate models and processing of large-scale datasets. Moreover, this approach provides fault\ntolerance, as the failure of a single node does not necessarily disrupt the entire training process."}, {"title": "2.3.3 Workload Distribution", "content": "As shown in Fig. 2(a) and Fig. 2(b), the processes of SGB and mini-\nbatch sampling are integral components of input data preparation, typically classified as data\npreprocessing. In contemporary mainstream training platforms, a heterogeneous configuration\nis frequently adopted, comprising a central host CPU and peripheral computing nodes, typically\nGPUs. Preprocessing tasks are typically executed by the CPU, which subsequently transfers the\nprepared data to the computing nodes for execution. These nodes are tasked with executing the\nentire training iterations and producing the final well-optimized model parameters."}, {"title": "3 CHARACTERIZATION METHODOLOGY", "content": "To comprehensively and thoroughly characterize the training process of HGNNs, we adopt a\nrational and rigorous evaluation approach. In this section, we initially outline our experimental\nsetup, encompassing the experimental platform, software framework, model and dataset selection.\nThen we delineate our evaluation methods."}, {"title": "3.1 Experimental Setup", "content": "The configuration of our experimental platform is detailed in Table 1, featuring\ncomputing nodes equipped with four advanced A100 GPUs. The four GPUs are organized into two\ngroups, with intra-group connections facilitated by NVLink buses, while inter-group connections\nare established through PCIe buses. We utilize Nsight Systems and Nsight Compute tools to\ncapture detailed performance metrics data of execution on the GPUs. Regarding the software\nframework, we choose DGL [30], which emerges as one of the most prominent GNN frameworks,\ntypically outperforming PyG [5] in terms of runtime efficiency and energy consumption [12]. All\nthe experiments are conducted utilizing 32-bit floating-point data format.\nWe conduct experiments\non three mainstream HGNN models, namely\nHAN [32], RGCN [25] and RGAT [29] as shown\nin Table 2, each with its own representative fea-\ntures. Specifically, RGCN first extends the con-\nventional GCN [15] to handle HetGs by apply-\ning separate GCN convolutions to each seman-\ntic graph and aggregating the results through\nsummation (SUM). RGAT builds upon this by\nintroducing attention mechanisms in NA stage, with semantic fusion performed via SUM as well."}, {"title": "3.2 Evaluation Methods", "content": "A fundamental and crucial step in profiling involves delineating the scope of the profiling process.\nWe utilize the CUDA interface provided by PyTorch to initiate and terminate the cudaProfiler.\nAdditionally, we employ NVTX tags to encapsulate the code regions designated for profiling,\nthereby differentiating between distinct execution phases.\nNsight Systems is utilized to provide a holistic analysis of system-level performance, offering\ninsights into the entire application execution process, including both GPU and CPU activities,\nas well as their interactions with system resources. Meanwhile, Nsight Compute specializes in\nconducting detailed analysis specifically on the execution of CUDA kernels on the GPU. It offers\ngranular performance metric data at the GPU instruction level, such as execution time, memory\naccess patterns, and instruction efficiency for each CUDA kernel function.\nDue to the GPU initialization processes involved in the initial training epochs, unless otherwise\nspecified, the data presented in this paper are the geometric mean (GM) of results from the 5 epochs\nafter excluding the first 3 epochs, which ensures the accuracy of the obtained results."}, {"title": "4 SINGLE-GPU TRAINING", "content": "Understanding the execution process on a single GPU is essential to comprehending the execution\nbehavior and characteristics of HGNNs. In this section, we conduct a detailed performance analysis\nof two principal training methods, full-batch and mini-batch training, executed on a single GPU to\nidentify the execution bottlenecks of HGNNs under different training methods."}, {"title": "4.1 Full-batch Training", "content": "Full-batch training enables the model to access the entire graph for thorough parameter updates,\nleading to accurate and stable gradient estimates; however, it necessitates substantial memory\nresources. In this section, we present a detailed quantitative analysis of execution time, execution\nbounds, memory patterns, and instruction issue stalls during full-batch training. Additionally, we\nexamine the influence of variations in metapath properties on execution performance. Due to an\nOut of Memory (OOM) issue encountered while conducting full-batch training on the MAG dataset\nwith the A100 GPU, this section includes results solely for the three smaller datasets. Furthermore,\nas the SGB stage is executed only once throughout the entire training process, its associated analysis\nwill be presented in Section 6.1.1."}, {"title": "4.1.1 Execution Time Analysis", "content": "In this section, we provide a comprehensive analysis of the execution\ntime during full-batch HGNN training. Our analysis focuses on identifying the principal execution\ncomponents from two perspectives encompassing various execution stages and the distinct kernels\nwithin each stage.\nBreaking down the execution time by stages allows us to understand\nthe time proportion of each execution stage, thereby identifying the main execution stages for\ntargeted optimization. Fig. 3 shows the profiling results of the time breakdown by stages. From\nthe overall training perspective, 1 forward propagation is more time-consuming than backward\npropagation. This phenomenon occurs because, although the backward stage involves the reverse\noperations of forward propagation, it can directly reuse numerous intermediate results from the\nforward process. The experimental result in Fig. 3(a) indicates that forward propagation accounts\nfor an average of 48.33% of the total execution time across different HGNN models and datasets,\nwhile backward propagation averages 42.37%. In terms of HGNN execution stages, 2 the NA stage\ndominates the most execution time both in forward and backward propagation, which is considered the\npredominant stage across all the HGNN stages. This is because each edge in every semantic graph\nrequires one execution process during the NA stage, making the load during NA stage the heaviest.\nAs shown in Fig. 3(b) and Fig. 3(c), the NA stage accounts for an average of 77.80% of the execution\ntime in the forward propagation and 77.53% in the backward propagation."}, {"title": "Time Breakdown by Kernel", "content": "Analyzing the execution time distribution of CUDA kernels\noffers valuable insights into the primary kernels utilized during each stage of execution, thereby\nelucidating their core execution characteristics. We categorize these kernels into four distinct\ngroups based on their specific computational tasks: dense-dense matrix multiplication (DeMM)\nkernel (DM-Type), topology-based matrix operation kernel (TB-Type), element-wise computation\nkernel (EW-Type), and data rearrangement kernel (DR-Type), as detailed in our prior work [42].\nThe DM-Type kernels execute dense-dense matrix multiplication (DeMM) tasks, such as sgemm,\nand typically exhibit a regular execution pattern with a high compute-to-memory access ratio. The\nTB-Type kernels handle computational operations based on the irregular topologies of graphs, ex-\nemplified by operations like SpMMCsr and SDDMMCoo (sampled dense-dense matrix multiplication,\nSDDMM), often demonstrating an irregular execution pattern induced by the irregular neighbor con-\nnection patterns in graphs. The EW-Type kernels perform element-wise computational operations\non sets of vectors or matrices, represented by elementwise_kernel (EleWise), matrix_scalar_kernel\n(MatScla), and reduce_kernel (Reduce), typically characterized by a low compute-to-memory access\nratio. The DR-Type kernels specialize in data rearrangement tasks on matrices, such as CatArray-\nBatchedCopy (Concat) and DeviceRadixSortSingleTileKernel (Sort), involving a substantial amount of\ndata movement operations.\nFrom an overall perspective, \u2192 the distribution of primary executing kernel types during the forward\nand backward propagation of the same stage exhibits significant similarity, whereas the time proportion\nof different kernel types varies. The resemblance in the distribution of kernel types stems from\nbackward propagation being the inverse operation of forward propagation in model computations.\nThe disparities in the distribution of kernel execution times originate from discrepancy in input\ndata and computational loads during backward propagation. For example, as shown in Fig. 4(a), in\nthe forward propagation, the FP stage is predominantly occupied by the DM-Type kernel (mainly\nsgemm), taking up an average of 96.48% of the total kernel execution time across various models\nand datasets. The EW-Type kernels (mainly EleWise) only occupy a small fraction which is less\nthan 4%. However, during the backward propagation, the proportion of time spent on EW-Type\nkernels notably escalates, averaging 25.78% of the total kernel execution duration. The underlying\ncause of this phenomenon lies in the vertex-type-specific nature of the weight matrix employed\nduring the FP stage which will be explained further in Section 6.1.2.\nNote that the RGCN and RGAT models do not invoke any CUDA kernels during the backward\npropagation of SF stage as in Fig. 4(b). This is due to their adoption of a direct summation aggregation\napproach in the forward propagation of SF stage. Moreover, in a broader context, \u2660 addition\noperations during forward propagation necessitate no computational effort during in the backward\npass. This is determined by the nature of the chain rule for gradient propagation, which can be\nexplained by the following formula: $Loss = f(C) = f (A + B) \\leftarrow \\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial A}$"}, {"title": "4.1.2 Execution Bounds Analysis", "content": "In this section, we conduct a detailed analysis of the performance\nmetrics of CUDA kernels predominantly invoked during each execution stage to identify the\nhardware resource bounds encountered at various stages."}, {"title": "Forward Propagation Analysis", "content": "The forward propagation process constitutes the primary\nexecution phase of HGNN model training. Characterizing and analyzing the execution bounds\nduring this phase is essential for understanding the requirements of various hardware resources\nthroughout the model's execution. Table 4 presents the performance metrics of the primary kernels\nat each execution stage during the training of the HAN model on the DBLP dataset. Based on the\ndata in the table, we formulate the Roofline model shown in Fig. 5, which elucidates the execution\nbounds of each stage encountered during both forward and backward propagation.\n\u2464 During the forward propagation, a distinct hybrid execution pattern emerges between stages,\neach characterized by unique execution bounds, resulting in varying demands on hardware resources.\nThis is due to the distinct execution behaviors exhibited by different stages of the HGNN models.\nTo be specific, for the FP stage, the kernels that occupy the majority of execution during forward\npropagtion is mainly sgemm, which primarily performs DeMM operations. As shown in Table 4,\ndue to its regular memory access pattern, it achieves an L2 cache hit rate of 86.14%, while its high\narithmetic density of 111.75 FLOP/Byte reflects its elevated computational demand. In Fig. 5, the\nsgemm kernel during the forward propagation is situated in the compute-bound region. The NA\nstage primarily involves graph-topology-based and element-wise operations, mainly invoking\nSpMMCsr and SDDMMCoo kernels as presented in Table 4. Taking the former as an example, its\nDRAM bandwidth utilization is 53.11%, but its arithmetic density is only 2.19 FLOP/Byte, indicating"}, {"title": "Comparison of Backward and Forward", "content": "As a crucial component of the training process,\nbackward propagation shares some similarities with but also exhibits significant differences from\nforward propagation. Contrasting them aids in a deeper understanding of the varying hardware\nresource demands during HGNN training. Overall, 6 the execution bounds in the backward and\nforward propagation exhibit similarity, yet the hybrid execution pattern in backward propagation\nis notably more intricate. This phenomenon arises because, while backward propagation reverses\nthe process of forward propagation, the application of the chain rule in gradient propagation\nmay introduce operations within the same stage that deviate from those encountered in forward\npropagation. As shown in Table 4, in FP-Backward compared to FP-Forward, the proportion of\ntime spent on sgemm decreases from 98.94% to 45.40%, while the time occupied by EleWise kernels\nfacing memory-bound notably increases. In both forward and backward propagation, the NA\nstage primarily exhibits memory-bound. However, what differs is that in the NA-Backward stage,\nthe proportion of time taken by the SDDMMCoo kernel increases from 8.21% to 46.21%, which\nindicates the matrices corresponding to the operations performed in the NA stage during backward\npropagation are denser compared to those in the forward process. The execution bounds exhibited\nby the SF stage in both backward and forward propagation are fundamentally similar."}, {"title": "4.1.3 Memory Pattern Analysis", "content": "In this section, we focus on the memory aspects, encompassing\nDRAM access, memory footprint, and cache hit rates, to elucidate the memory characteristics of\nvarious execution stages during the HGNN training process.\nMemory access latency is a critical factor contributing to perfor-\nmance degradation during model execution, and it is associated with high energy consumption. A\ncomprehensive understanding of memory access patterns across various stages of HGNN training\nfacilitates both performance and energy efficiency improvement. Fig. 6 illustrates the DRAM access\nbreakdown of various models during the training process of HGNN on different datasets. From a\ntraining perspective, 7 the backward propagation generally incurs more memory access compared\nto the forward propagation. This phenomenon arises due to the reason that during the backward\npropagation, the model must access gradient data that matches the size of the dataset accessed in\nforward propagation. Additionally, the computation of gradients requires the accesses of numer-\nous intermediate results preserved from the forward propagation. As presented in Fig. 6 (a), the\nbackward propagation accounts for 55.38% of the total memory access during the entire training\nprocess, while the forward propagation averages only 42.87%."}, {"title": "Memory Footprint Analysis", "content": "As the scale of real-world graph data continues to grow, memory\nutilization emerges as a pivotal factor influencing the scalability of models applied to large-scale\ndatasets. Conducting an analysis of the memory footprint facilitates the optimization of model\nscalability. Fig. 7(a) illustrates the memory allocation in each execution stage during the training\nprocess of HGNN along the timeline. For the sake of convenience, only the specific cases of the\nHAN model on three different datasets are shown here, while the situations for the other two\nmodels are similar. Here, we compare the situations between the pure inference stage and the\ncomplete training stage to highlight the more urgent memory demands during the training process.\n\u2022 In comparison to pure inference, the training process necessitates greater memory footprint,\nwith the majority memory allocation lies on the NA stage. This distinction arises from inference's\nsingular execution of a forward propagation, which does not involve gradient computation and\nthereby obviates the storage of intermediate computational results essential for training during the\nforward propagation. Furthermore, the inference process does not require backward propagation,\nthus eliminating the associated memory allocation needed during training. And given that the\nnumerous intermediate computation results requiring storage are associated with edges, the NA\nstage necessitates the most substantial memory allocation. Considering the case of DBLP dataset\nin Fig. 7(a), in comparison to the inference process, the total memory allocation during training\nincreases by 1.92\u00d7, with the NA stage constituting 66.63% of the total memory consumption."}, {"title": "Cache Hit Rate Analysis", "content": "The cache hit rate effectively reflects data locality, thereby serving as\na starting point for optimizing models from this perspective. Fig. 7(b) illustrates the differences\nin cache hit rates for forward and backward propagation across various stages of execution on\ndifferent models of the HAN model. As shown in the figure, in nearly all scenarios, \u25c9 the data\nlocality during backward propagation tends to be lower compared to that observed during forward\npropagation. This discrepancy primarily arises because backward propagation involves not only\naccessing graph data but also retrieving stored gradient information and various intermediate\nvariables, necessitating a broader range of memory accesses and leading to diminished data locality.\nFor the FP, NA, and SF stages, the L2 cache hit rate during backward propagation is on average\n23.71%, 3.57%, and 4.34% lower than during forward propagation, respectively."}, {"title": "4.1.4 Issue Stall Analysis", "content": "This section provides a detailed analysis of GPU instruction issue stalls\nencountered during the training process. The focus is on elucidating the temporal distribution\nand underlying causes of these stalls at each stage. Analyzing instruction issue stalls can reveal\noperational bottlenecks from an alternative perspective, thereby offering pathways for targeted\noptimization strategies.\nFig. 8(a) presents the ratio of stall time to execution time for different\nexecution stages of the training process. From a comprehensive viewpoint, the prevalence of\ninstruction issue stall across different stages throughout the training process is notable and merits\ncareful consideration, averaging 33.21% across different stages. Fig. 8(c) illustrates the breakdown\nof instruction issue stall reasons for the main CUDA kernels invoked at each execution stage. It\ncan be observed that \u3260 the vast majority of kernel stalls originate from memory dependency except\nfor sgemm kernel, and the proportion of stall time attributable to memory dependency during the NA\nstage is the highest among all execution stages. This is because, during the training process of HGNN,\nother than sgemm, the kernels involve a large number of reads and writes to irregular graph data.\nThese access patterns are usually highly irregular, making it difficult for the accessed data to be\nfully cached. Therefore, stalls caused by memory dependencies dominate. Furthermore, the NA\nstage, which predominantly accesses irregular graph data during aggregation, thereby resulting\nin the most significant instruction issue stalls attributable to memory dependency. As shown in\nFig. 8(c), for kernels other than sgemm, memory dependency accounts for an average of 74.11% of\nthe instruction issue stalls. In contrast, for the sgemm kernel, memory dependency accounts for an\naverage of only 15.51%, with the dominant stalls being caused by execution dependency, which\naverage 18.86%. Fig. 8(b) illustrates that the NA stage exhibits the highest ratio of stalls caused by\nmemory dependency relative to total elapsed time across all execution stages during both forward\nand backward propagation."}, {"title": "Comparison of Backward and Forward", "content": "Fig. 8(b) presents the proportion of stalls attributed\nto memory dependency in relation to the total elapsed time for each execution stage of HGNN\ntraining during both forward and backward propagation. The fraction of overall execution time\nattributed to memory dependency during each stage of backward propagation surpasses that observed\nin forward propagation. The heightened memory access volume during the backward propagation\nstage, coupled with increased data type diversity, leads to diminished data locality and reduced\ncache hit rates, as detailed in Section 4.1.3. As shown in Fig. 8(b), the average proportion of memory\ndependency during each stage of backward propagation is 2.26\u00d7 higher than that during forward\npropagation."}, {"title": "4.1.5 Exploring Metapath Changes", "content": "In this section, we explore the performance impact on each\nexecution stage as the length and number of metapaths vary. As the number of metapaths increases\nsignificantly, it can greatly expand the size of the graph data, potentially leading to an OOM issue\non a single GPU. Therefore, in the experiments in this section, we use the IMDB dataset as a\nrepresentative example, focusing on the HAN model. And the performance metrics for each stage\ncovered in this section are obtained by averaging forward and backward propagation.\nIncrease in Length of Metapaths. Exploring the performance changes at different stages due to\nvariations in metapath length is crucial, as longer metapaths can assist models in capturing more\ncomplex relationship patterns and context information over greater distances. Fig. 9(a) shows the\nexecution time of different stages across varying metapath lengths. Obviously, \u2192 increase in the\nlength of the metapath only significantly increase the execution time of NA stage, while the FP and SF\nstages are almost unaffected. The rationale behind this lies in the fact that the length increase of the\nmetapath does not alter the vertex types or the number of semantic graphs, thereby maintaining the\nworkloads on the FP and SF stages. However, elongating the metapath results in denser semantic\ngraphs [42], characterized by a higher volume of edges. This directly translates into increased\nexecution times during the NA stage. As depicted in Fig. 9(a), an increase in metapath length from\n3 to 9 correlates with a 4.12\u00d7 rise in execution time for the NA stage, while the FP and SF stages\nexhibit negligible change in execution times."}, {"title": "Increase in Number of Metapaths", "content": "Exploring the impact of varying the number of metapaths\non the execution performance of different stages is essential, as a greater number of metapaths\ncan provide the model with richer semantic graph information, thereby enhancing model accuracy.\nFig. 9(b) illustrates the impact of increasing the number of metapaths while keeping the metapath\nlength constant on the performance at various execution stages. According to the figure, \u2192 increase\nin the number of metapaths leads to increased execution times across all stages of HGNN, with the\nNA stage showing the most noticeable growth trend. As the number of metapaths increases, so does\nthe number of semantic graphs, resulting in a higher volume of vertices and edges to process.\nConsequently, each execution stage experiences a proportional increase in workload. Notably, the\nNA stage shows the most pronounced growth, primarily because the rise in edges outpaces that of\nnodes and even semantic graphs. As illustrated in Fig. 9(b), expanding the metapath count from 1\nto 8 leads to execution time increases of 4.71\u00d7 for FP, 9.34\u00d7 for NA, and 1.40\u00d7 for SF."}, {"title": "4.2 Mini-batch Training", "content": "As the size of real-world graph datasets continues to expand, mini-batch training has increasingly\nbecome the predominant method for model training, which leads to faster convergence and im-\nproved generalization performance compared to full-batch training. Aside from the additional\nmini-batch sampling stage, the execution behavior and characteristics are identical to those in"}, {"title": "4.2.1 Execution Analysis", "content": "This section provides a detailed analysis of the execution time breakdown\nduring mini-batch training, compares the performance of GPU sampling with that of CPU sampling,\nand examines various performance metrics of the CUDA kernels employed in GPU sampling to\nelucidate the characteristics of the sampling operation.\nBreaking down the execution time facilitates the identification\nof the primary execution processes during mini-batch training. Fig. 10(a) shows the breakdown\nof execution time across various execution stages. 5 Mini-batch sampling occupies the majority\nof the execution time in each epoch of mini-batch training, even exceeding the combined time of all\nother execution stages. This is due to mini-batch sampling in HGNN models requiring sampling\noperations across multiple semantic graphs not just one graph as in conventional GNNs, entailing\ntraversal of intricate and irregular graph structures. Furthermore, this procedure is carried out\non CPU, which is markedly time-intensive in contrast to the highly parallelized computational\nworkload executed on GPU. As presented in Fig. 10(a), mini-batch sampling accounts for an average\nof 51.01% of the total time across different models and datasets. Especially for the HAN model on\nthe MAG dataset, the time elpsed on sampling in each epoch accounts for as much as 88.92%."}, {"title": "Performance Comparison of GPU and CPU Sampling", "content": "In general, the mini-batch sampling\nprocess is categorized as"}]}