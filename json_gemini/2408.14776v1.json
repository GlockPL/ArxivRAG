{"title": "MROVSeg: Breaking the Resolution Curse of Vision-Language Models in Open-Vocabulary Semantic Segmentation", "authors": ["Yuanbing Zhu", "Bingke Zhu", "Zhen Chen", "Huan Xu", "Yingying Chen", "Ming Tang", "Jinqiao Wang"], "abstract": "Open vocabulary semantic segmentation aims to segment and recognize semantically meaningful regions based on text-based descriptions during inference. A typical solution to address this task is to leverage powerful vision-language models (VLMs), such as CLIP, to bridge the gap between open- and close-vocabulary recognition. As VLMs are usually pretrained with low-resolution images (e.g. 224 x 224), most previous methods operate only on downscaled images. We question this design as low resolution features often fail to preserve fine details. Although employing additional image backbones for high-resolution inputs can mitigate this issue, it may also introduce significant computation overhead. Therefore, we propose MROVSeg, a multi-resolution training framework for open-vocabulary semantic segmentation with a single pretrained CLIP backbone, that uses sliding windows to slice the high-resolution input into uniform patches, each matching the input size of the well-trained image encoder. Its key components include a Multi-Res Adapter, which restores the spatial geometry and grasps local-global correspondences across patches by learnable convolutional and scale attention layers. To achieve accurate segmentation, we introduce Multi-grained Masked Attention scheme to aggregate multi-grained semantics by performing cross-attention between object queries and multi-resolution CLIP features within the region of interests. Through comprehensive experiments, we demonstrate the superiority of MROVSeg on well-established open-vocabulary semantic segmentation benchmarks, particularly for high-resolution inputs, establishing new standards for open-vocabulary semantic segmentation.", "sections": [{"title": "1. Introduction", "content": "Semantic segmentation is a fundamental computer vision problem which seek to group image pixels into semantically meaningful segments and recognize their categories. Previous research in semantic segmentation [6, 9, 25] mainly focus on segmenting objects from a pre-defined semantic categories set based on the dataset. These methods typically rely on human-annotated masks and class labels, which is costly and unscalable in terms of the number of categories. To this end, the task of open-vocabulary semantic segmentation has been introduced [16, 21, 39], which aims to segment semantic pixels belonging to arbitrary classes beyond pre-defined categories and dataset.\nIn recent years, large-scale vision-language pretrained models (VLMs), such as CLIP [28] and ALIGN [19], have demonstrated remarkable generalization capabilities for recognizing open-vocabulary categories. This motivate the research community to investigate the potential of VLMs in open-vocabulary semantic segmentation. To address the discrepancy between the per-pixel semantic requirements and the image-level labels provided by VLMs, initial studies [11, 44, 45] modified CLIP model by removing its final pooling layer to obtain dense category embeddings for per-pixel classification. However, these approaches typically necessitate fine-tuning VLMs on a base segmentation dataset with limited images and categories, which is demonstrated [44] to impair the transferability of VLM features, leading to unsatisfactory zero-shot performance on downstream tasks.\nRecent approaches [14, 24, 30, 35, 36] reformulate open-vocabulary semantic segmentation as a region-level recognition problem. These methods typically adopt two branch meta architecture: one branch extract image feature and generate mask proposals, and the other branch classifies the predicted proposals with pretrained VLM. Although these methods are promising, we note their following limitations. First, due to pretrained VLMs exhibit inferior size adaptability, most of open-vocabulary semantic segmentation methods (e.g. [8, 14, 22, 30, 34\u201336]) so far need to downsample images to fit the pretrained resolution (e.g. 224 x 224) of VLM to perform region-level recognition (as in Fig. 1 (a)). However, low-resolution input usually lacks segmentation details. Although na\u00efvely applying sliding window inference [11, 35] could partly compensate for the details, the spatial structure across windows is corrupted and the local-global modeling is also absent.\nIn light of the limitations and challenges faced by previous methods, we propose MROVSeg, a VLM-based Multi-Resolution training framework for Open-Vocabulary Semantic Segmentation. As illustrated in Fig. 1 (b), first, MROVSeg uses downsampled low-resolution images as VLM input to extract global low-resolution feature. Second, MROVSeg split the high-resolution images into slices and feeds them to VLM to extract detailed high-resolution features. The key components of MROVSeg contain a Multi-Res Adapter, in which we employs depthwise convolution layers to restore the spatial geometry across slices. To effectively capture global long-range context, inspired by previous multi-scale training framework [5, 18, 37], we employ a image-dependent Scale-aware Attention [5] to dynamically adjust the fusion weight of high-resolution and low-resolution VLM features based on their relevance and trustworthiness. The resulting multi-resolution features are fused hierarchically then employed for precise mask proposals generation.\nTo achieve accurate mask class recognition, we propose a Multi-grained Masked Attention mechanism. The core insight is to reuse the CLIP [CLS] token, and manipulate its attention map on multi-resolution features in CLIP attention layers with resolution-aware attention masks. We find this resolution-aware design can enforce the low- and high-resolution attention map focus on global contexts and spatial details respectively and thus effectively aggregate multi-grained semantics.\nWith extensive experiments on well-established open-vocabulary semantic segmentation benchmarks, such as Pascal Context-59, Pascal Context-459 [26], ADE-150, ADE-847 [43], Pascal VOC [15], we are delighted to report that our method surpasses current state-of-the-art methods by a significant margin on standard benchmarks (+2.4 mIoU%), demonstrating the advancements of MROVSeg in the domain of open-vocabulary semantic segmentation. Our contributions can be summarized as follows:\n\u2022 We propose a novel end-to-end multi-resolution training framework to tackle the task of open-vocabulary semantic segmentation. It enables improved open-vocabulary segmentation by leveraging multi-resolution vision-language features.\n\u2022 A multi-grained masked attention scheme is proposed to effectively aggregate regional and universal semantics on multi-resolution vision-language features.\n\u2022 The efficacy of our method is confirmed by achieving state-of-the-art performance by evaluating our proposed approach on five well-established open-vocabulary semantic segmentation benchmarks."}, {"title": "2. Related work", "content": "Pretrained Vision-language Models. Recently, large-scale pretrained, contrastive learning based methods [19, 28] demonstrate powerful open-vocabulary image classification capability by learning shared image-text feature representations. Pretrained CLIP [28] has been generalized to many downstream computer vision tasks such as object detection [42], image caption [17], image generation [27] and image segmentation [2, 34\u201336, 38]. Our method invokes natural language perceptual ability of pretrained VLMs, aiming to explore their application boundary in open vocabulary semantic segmentation tasks.\nSemantic Segmentation. Classic closed-vocabulary se-"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Problem Definition", "content": "In open vocabulary semantic segmentation task setting [22, 35], an image $I \\in \\mathbb{R}^{3\\times H\\times W}$ is operated by a segmentation model $P_{\\theta}$ with parameter $\\theta$ to produce a set of masks associated with K semantic categories:\n$$\n\\{m_i, \\gamma_i\\}_{i=1}^K = P_{\\theta}(I).\n$$\nThe segmentation model $P_{\\theta}$ is trained on a base segmentation dataset (e.g., COCO [3]) annotated with a fixed label set $\\mathcal{Y}_{train} = \\{y\\}_{i=1}^{K_{train}}$ of $K_{train}$ semantic categories. And during test, model $P_{\\theta}$ is expected to segment objects of category set $\\mathcal{Y}_{test}$, which generally contains novel categories ($\\mathcal{Y}_{test} \\cap \\mathcal{Y}_{train} = \\varnothing$)."}, {"title": "3.2. Motivation", "content": "Accurate semantic segmentation needs high-resolution image inputs. Due to low-resolution images used in vision-language pretraining, previous open-vocabulary methods employ extra image backbone (such as SAM [30] and ResNet [24]) to provide segmentation detail. Although recent studies [32, 38] adapt convolution-based CLIP model for high-resolution training, but directly apply these methods to ViT-based CLIP model results in suboptimal performance [36] due to undesirable size adaptability of ViT."}, {"title": "3.3. Framework Overview", "content": "The overview of our training framework MROVSeg, for open-vocabulary semantic segmentation is shown in Fig. 2. At a high-level, an image is downsampled and padded to a low-resolution (such as 224 \u00d7 224) and processed by a pretrained CLIP ViT to extract global feature. To capture high-resolution local details, the high-resolution image is split into slices and input to the shared CLIP ViT encoder. These multi-resolution features are then concatenated with learnable queries and fed into a Multi-Res Adapter(Sec. 3.4), to produce the fused features, query features, and attention masks used for mask classification."}, {"title": "3.4. Multi-Res Adapter", "content": "As depicted in Fig. 3, the Multi-Res Adapter adapts multi-resolution CLIP features for segmentation task. Denote the slice (high resolution) features of l-layer as $\\{P^l\\}_{l=1}^L$, the global feature (low resolution) as $P^g$, where $P^l \\in \\mathbb{R}^{S \\times L \\times D}$, S is the slice number, L = H \u00d7 W is the token length, and D is the channel number. Na\u00efvely concatenating the high resolution slice features for the subsequent segmentation prediction is promising, but have two defects: 1) the spatial geometry, such as the positional information, is corrupted across slice features $\\{P^l\\}_{l=1}^L$; 2) the long-range dependencies and global context is missing. Thus, we propose Multi-Res Adapter to effectively restore spatial geometry of slice features and capture long-range context from global feature. In Multi-Res Adapter, the 0-th slice features $\\{P^0\\}_{l=1}^L$ are firstly concatenated with learnable queries $Q \\in \\mathbb{R}^{N \\times D}$ and input to the vanilla ViT blocks to build the query features for each objects. Then for target fusion layer l, the slice features $\\{P^l\\}_{l=1}^L$ and global feature $P^g$ are fused through a Multi-Res Fusion (MRF) Module and then injected into the ViT branch.\nMulti-Res Fusion (MRF) Module first reshape the global feature to $\\hat{H} \\in \\mathbb{R}^{H \\times W \\times D}$, and restore slice features to $H^l \\in \\mathbb{R}^{m*H \\times n*W \\times D}$, where S = m \u00d7 n. To retain the spatial geometry of high-resolution features, we employ depthwise separable convolutions to fuse the restored feature. To effectively model the local-global correspondence, we train a Scale-aware Attention [5] to fuse multi-res features into $F^l \\in \\mathbb{R}^{H \\times W \\times D}$ as the fused feature\n$$\nF' = Up(\\alpha_l) \\odot DConv(H^l) + Up((1-\\alpha_l) \\odot \\hat{H}).\n$$\nThen $F^l$ is added to the visual tokens in Multi-Res Adapter. The scale attention decoder $f_{\\theta}$ learns to predict the scale attention $\\alpha_l = \\sigma(f_{\\theta}(H^l)) \\in [0, 1]^{H \\times W \\times D}$ for layer l to weigh the trustworthiness of low resolution context and high resolution detail. The sigmoid function \u03c3 ensures $\\alpha_l$ weight in [0, 1], where 1 means $\\alpha_l$ focus on high resolution detail. In practice, we empirically select the features from a CLIP layer set $\\{l_i\\}_{i=1}^j$ to apply in Multi-Res Adapter. For instance, for the model based on CLIP ViT-L model, $l \\in \\{stem,6,12,18\\}$. The fused features $\\{F^l\\}_{j=1}^J$ are used for hierarchical mask decoding. The final layer output slice features $\\{P^l\\}_{j=1}^J$ are restored to $H \\in \\mathbb{R}^{m*H \\times n*W \\times D}$ as the visual feature for hierarchical mask decoding and multi-grained masked attention. And the output queries $Q$ are projected as the query feature\n$$\nQ_f = f_{MLP_Q}(Q),\n$$\nfor hierarchical mask decoding and multi-grained masked attention."}, {"title": "3.5. Mask Prediction", "content": "Hierarchical Mask Decoding. High-resolution features preserve more spatial detail, thus benefit semantic segmentation, especially for mask prediction [4]. However, directly upsampling features is computationally demanding. Thus, similar to FPN, we first upsample the multi-resolution features $\\{F^l\\}_{j=1}^J$ from the Multi-Res Adapter by $\\{2\\times,4\\times,8\\times\\}$ to build the feature pyramid. Then we gradually concatenate the multi-resolution features with the final visual feature $H$ at channel dimension and upsample by 2 transposed convolution layers $H_{up} = TransposeConv(cat(H, F^l))$. Finally, we project the upsampled feature $H_{up}$ to the pixel feature space by MLP then decode the mask by inner product of query feature and"}, {"title": "mask feature", "content": "$$\nH_{pix} = MLP_{Pix}(H_{up}),\n$$\n$$\nI_{mask} = Q_f \\times H_{pix},\n$$\nwhere the query feature $Q_f$ is from the Multi-Res Adapter which described in Sec. 4.3. $M_{mask} \\in [0,1]^{N\\times H\\times W}$ is the mask prediction, and we omit the sigmoid function in Eq.5."}, {"title": "3.6. Mask Classification", "content": "Decoupled Attention Mask Decoding. CLIP models, like general Vision Transformers, perform open-vocabulary image-level recognition through [CLS] token. Our goal is to recognize the various predicted mask proposals without changing any parameter of CLIP by guiding the attention maps of [CLS] token on the region of interests. To achieve this goal, we follow MaskCLIP [14] and SAN [36] to reuse the original CLIP [CLS] token in the intermediate layer. To sufficiently aggregate multi-grained semantics from CLIP, we first duplicate the [CLS] token to query number N and create learnable positional embedding for them, dubbed as the $X_{prop} \\in \\mathbb{R}^{N\\times D}$. We aim to enforce $X_{prop}$ to extract the global and local semantics from low- and high-resolution CLIP feature respectively. Thus, for visual feature H, we first extract global contexts with max pooling $H^{g} = MaxPool(H)$ and train MLPs project them to attention space\n$$\nA_{local} = MLP_l(H^l), A_{global} = MLP_g(H^g),\n$$\nwhere $A_{global} \\in \\mathbb{R}^{H\\times W\\times D'}$, $A_{local} \\in \\mathbb{R}^{m*H*n*W\\times D'}$ denote the local and global attention features respectively. Then we decode local and global per-head attention masks by the inner product with\n$$\nM_{local} = Q_f^T \\times A_{local}, M_{global} = Q_f^T \\times A_{global},\n$$\nwhere $Q_f$ is the output query feature described in Sec.3.4. We show this decoupled resolution-aware attention decoding benefit the multi-grained aggregation in Fig.7.\nMulti-grained Masked Attention. As shown in Fig.4, we perform cross attention to update the $X_{prop}$ with multi-resolution CLIP features, with the predicted attention masks $M_{local}$ and $M_{global}$,\n$$\nX_{prop}^{l+1}=softmax(Q_{prop}(\\frac{K_{LR}}{\\sqrt{d}} + \\frac{M_{global} K_{HR}}{\\sqrt{d}})^T(\\frac{M_{local} V_{LR}}{W_{X_{LR}}} + \\frac{V_{HR}}{W_{X_{HR}}}) + X_{prop},\n$$\nwhere $Q_{prop} = WX_{prop}$ is query embeddings. Denote the low- and high-resolution CLIP tokens as $X_{LR}$ and $X_{HR}$. $K_{LR} = W_k X_{LR}$ and $K_{HR} = W_k X_{HR}$ are the key embeddings of low- and high-resolution CLIP visual tokens respectively. $V_{LR} = W_v^T X_{LR}$ and $V_{HR} = W_v^T X_{HR}$ are value embeddings. $W_qW_k$ and $W_v$ are projection weights of cross-attention layer. The final output proposal logits $X_{prop}$ are projected to the shared vision-language space and compute cosine similarity with text embeddings to obtain proposal logits $C \\in \\mathbb{R}^{N\\times K}$: $C = X_{prop} W_{visual} W_{text} X_{Text}$, where K is the number of categories, and $W_{visual}$ and $W_{text}$ are projection weights. Finally, the final segmentation map $S \\in \\mathbb{Z}^{R\\times 1} \\mathbb{Z}^{K\\times H\\times W}$ is produced by\n$$\nS = C \\times M_{mask}^T\n$$"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Setting", "content": "We train our models on COCO-Stuff [3] dataset which comprises 164K images with densely annotated masks spanning 171 categories. We first evaluate MROVSeg on five well-established mainstream open-vocabulary semantic segmentation benchmarks for standard evaluation. We further evaluate MROVSeg on Cityscapes [12] benchmarks to explore the ability of handling high-resolution image input. We follow common practice [2, 35] to measure the segmentation performance by mean intersection over union (mIoU) score.\nDatasets. The standard benchmarks contains three dataset: ADE [43], Pascal Context [26], and Pascal VOC [15]. The ADE dataset contains around 20K and 2K images for training and validation, respectively. This dataset is annotated with 150 and 847 categories, resulting in two separate segmentation benchmarks, namely ADE-150 and ADE-847. Similarly, the Pascal Context dataset has 5K images for both training and validation. It is annotated with 59 and 459 classes, forming two benchmarks known as PC-59 and PC-459. The Pascal VOC dataset comprises 1464 and 1449 images for training and validation, encompassing annotated masks across 20 semantic categories."}, {"title": "4.2. Standard Evaluation", "content": "Comparisons with State-of-the-Art Methods. In Tab. 1, we compare MROVSeg with current state-of-the-art open-vocabulary semantic segmentation methods on standard benchmarks. First of all, our method significantly outperforms other state-of-the-art methods all various open-vocabulary semantic segmentation benchmarks. Specifically, our method supasses state-of-the-art method SAN [30] with the same CLIP backbones on all benchmarks by remarkable margins (+1.4% mIoU for ADE-847, +5.9% mIoU for PC-459, +3.1% mIoU for ADE-150, +3.1% mIoU for PC-59, and +0.9 for VOC-20 with CLIP ViT-B backbone, +2.4% mIoU for ADE-847, +7.0% mIoU for PC-459, +3.6% mIoU for ADE-150, +3.9%\nmIoU for PC-59, and +2.1 for VOC-20 with CLIP VIT-B backbone). Compared to methods with additional image backbones, our model outperforms EBSeg (with SAM) by +0.9, +1.0, +3.2, +1.2, +1.1 mIoU% for ADE-847, PC-459, ADE-150, PC-59, and VOC-20 respectively. In addition, our models outperforms other convolution-based high-resolution trained method FC-CLIP and SED. Fig. 8 shows the qualitative comparison between MROVSeg and state-of-the-art methods (SAN [36] and EBSeg [30]). Evidently, MROVSeg can segment objects more precisely (the first row, class sofa), and provide more detailed mask prediction (the second and third row, more accurate object boundaries)."}, {"title": "4.3. Ablation Study", "content": "For all ablation experiments, we employ our CLIP ViT-B/16 based model as our ablated baseline."}, {"title": "5. Conclusion", "content": "In this paper, we introduce MROVSeg, a multi-resolution training framework designed to enhance open-vocabulary semantic segmentation by leveraging multi-resolution VLM features. The exceptional quantitative and qualitative results obtained on well-established open-vocabulary semantic segmentation benchmarks serve as compelling evidence of its effectiveness and versatility. We hope our method can serve as a strong baseline for future research."}, {"title": "Appendix", "content": ""}, {"title": "A. Overview", "content": "In the supplementary material for MROVSeg, we provide implementation detail (Sec. B), qualitative comparison (Sec. C), and limitation and discussion (Sec. D)."}, {"title": "B. Implementation Detail", "content": ""}, {"title": "B.1. Training Detail", "content": "During the training, we follow common practices [10, 36] to use AdamW optimizer with a base learning rate of 2e - 4 with poly decay schedule power 0.9 to train the model, and the weight decay is set to le - 4. We use the same data augmentation strategies as in [22, 36, 38] for training, i.e., all training images firstly are randomly resized to [0.5, 2.0] \u00d7 of its original resolution, then are randomly cropped into the resolution of 6402. And for fair comparison, the testtime argumentation is not used."}, {"title": "B.2. Multi-grained Masked Attention", "content": "As introduced in Sec. 3.6, we enable the Xprop by perform masked cross attention with multi-resolution CLIP visual tokens. We show the pseudo code in Algorithm 1. During the inference, denote the number of CLIP token as N, the number of CLIP high-resolution token 4N, the number of query token as Nprop, then the time complexity of the cross attention implementation is O(5N\u00b2 + 20NqueryN)."}, {"title": "C. Qualitative Results", "content": "In this section, we provide the segmentation results on COCO-Stuff val [3] (close-vocabulary setting), ADE [43] and Pascal Context [26] (open-vocabulary setting)."}, {"title": "C.1. Comparison", "content": "We provide more qualitative comparison of our method MROVSeg with state-of-the-art methods SAN [36] and EBSeg [30] in Fig.S10. We observe that MROVSeg can handle complex scenarios while preserving spatial details (object boundaries) and extract global context effectively."}, {"title": "C.2. COCO", "content": "We visualize the closed-vocabulary semantic predictions of COCO-Stuff [3]. As shown in Fig.S9, our method is able to handle complex scenarios and provide precise mask predictions. However, as the labels exist semantically containment relationships, our model struggles to distinguish the regions with similar semantics. For instance, the model cannot find out the donut out of the food-other in fifth row of Fig.S9(b)."}, {"title": "C.3. ADE", "content": "We visualize the open-vocabulary semantic segmentation predctions of MROVSeg on ADE [43] dataset with 150 and 847 categories benchmark respectively. As the result, we find MROVSeg can segment and identify the novel classes that only appear in 847-class annotations. As illustrated in Fig.S11, the model is able to segment and recognize phone booth and snow in (b) and (c) of Fig.S11 respectively."}, {"title": "C.4. Pascal Context", "content": "The qualitative results of segmentation performance of MROVSeg for 59 and 459 semantic categories on Pascal Context [26] dataset are shown in Fig.S12. Note that MROVSeg is able to segment small objects (such as the video camera in Fig.S12(c)), which is benefit from the high resolution VLM features and strong local contexts."}, {"title": "D. Disucssion", "content": "In this section, we further discuss the limitation, future work and potential negtive impact of MROVSeg."}, {"title": "D.1. Limitation and Future Work", "content": "Due to computational constraints and for fair performance comparison, we conduct our experiments solely based on"}]}