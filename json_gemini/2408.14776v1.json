{"title": "MROVSeg: Breaking the Resolution Curse of Vision-Language Models in\nOpen-Vocabulary Semantic Segmentation", "authors": ["Yuanbing Zhu", "Bingke Zhu", "Zhen Chen", "Huan Xu", "Yingying Chen", "Ming Tang", "Jinqiao Wang"], "abstract": "Open vocabulary semantic segmentation aims to seg-\nment and recognize semantically meaningful regions based\non text-based descriptions during inference. A typical so-\nlution to address this task is to leverage powerful vision-\nlanguage models (VLMs), such as CLIP, to bridge the\ngap between open- and close-vocabulary recognition. As\nVLMs are usually pretrained with low-resolution images\n(e.g. 224 x 224), most previous methods operate only\non downscaled images. We question this design as low\nresolution features often fail to preserve fine details. Al-\nthough employing additional image backbones for high-\nresolution inputs can mitigate this issue, it may also intro-\nduce significant computation overhead. Therefore, we pro-\npose MROVSeg, a multi-resolution training framework for\nopen-vocabulary semantic segmentation with a single pre-\ntrained CLIP backbone, that uses sliding windows to slice\nthe high-resolution input into uniform patches, each match-\ning the input size of the well-trained image encoder. Its key\ncomponents include a Multi-Res Adapter, which restores the\nspatial geometry and grasps local-global correspondences\nacross patches by learnable convolutional and scale atten-\ntion layers. To achieve accurate segmentation, we introduce\nMulti-grained Masked Attention scheme to aggregate multi-\ngrained semantics by performing cross-attention between\nobject queries and multi-resolution CLIP features within\nthe region of interests. Through comprehensive experi-\nments, we demonstrate the superiority of MROVSeg on well-\nestablished open-vocabulary semantic segmentation bench-\nmarks, particularly for high-resolution inputs, establishing\nnew standards for open-vocabulary semantic segmentation.", "sections": [{"title": "1. Introduction", "content": "Semantic segmentation is a fundamental computer vision\nproblem which seek to group image pixels into semantically"}, {"title": "2. Related work", "content": "Pretrained Vision-language Models. Recently, large-\nscale pretrained, contrastive learning based methods [19,\n28] demonstrate powerful open-vocabulary image classifi-\ncation capability by learning shared image-text feature rep-\nresentations. Pretrained CLIP [28] has been generalized to\nmany downstream computer vision tasks such as object de-\ntection [42], image caption [17], image generation [27] and\nimage segmentation [2, 34\u201336, 38]. Our method invokes\nnatural language perceptual ability of pretrained VLMs,\naiming to explore their application boundary in open vo-\ncabulary semantic segmentation tasks.\nSemantic Segmentation. Classic closed-vocabulary se-"}, {"title": "3. Method", "content": null}, {"title": "3.1. Problem Definition", "content": "In open vocabulary semantic segmentation task setting [22,\n35], an image $I \\in \\mathbb{R}^{3\\times H\\times W}$ is operated by a segmenta-\ntion model $P_{\\theta}$ with parameter $\\theta$ to produce a set of masks\nassociated with $K$ semantic categories:\n\n$\\left\\{m_{i}, Y_{i}\\right\\}_{1}^{K}=P_{\\theta}(I)$.\n\n The segmentation model $P_{\\theta}$ is trained on a base segmen-\ntation dataset (e.g., COCO [3]) annotated with a fixed\nlabel set $\\mathcal{Y}_{\\text {train }}=\\left\\{y_{i}\\right\\}_{i=1}^{K_{\\text {train }}}$ of $K_{\\text {train }}$ semantic categories.\nAnd during test, model $P_{\\theta}$ is expected to segment objects\nof category set $\\mathcal{Y}_{\\text {test }}$, which generally contains novel\ncategories ($\\mathcal{Y}_{\\text {test }} \\cap \\mathcal{Y}_{\\text {train }}=\\emptyset$)."}, {"title": "3.2. Motivation", "content": "Accurate semantic segmentation needs high-resolution im-\nage inputs. Due to low-resolution images used in vision-\nlanguage pretraining, previous open-vocabulary methods\nemploy extra image backbone (such as SAM [30] and\nResNet [24]) to provide segmentation detail. Although re-\ncent studies [32, 38] adapt convolution-based CLIP model\nfor high-resolution training, but directly apply these meth-\nods to ViT-based CLIP model results in suboptimal per-\nformance [36] due to undesirable size adaptability of ViT."}, {"title": "3.3. Framework Overview", "content": "The overview of our training framework MROVSeg, for\nopen-vocabulary semantic segmentation is shown in Fig. 2.\nAt a high-level, an image is downsampled and padded to a\nlow-resolution (such as 224 \u00d7 224) and processed by a pre-\ntrained CLIP ViT to extract global feature. To capture high-\nresolution local details, the high-resolution image is split\ninto slices and input to the shared CLIP ViT encoder. These\nmulti-resolution features are then concatenated with learn-\nable queries and fed into a Multi-Res Adapter(Sec. 3.4),\nto produce the fused features, query features, and attention\nmasks used for mask classification."}, {"title": "3.4. Multi-Res Adapter", "content": "As depicted in Fig. 3, the Multi-Res Adapter adapts multi-\nresolution CLIP features for segmentation task. Denote the\nslice (high resolution) features of l-layer as $\\left\\{P^{l}\\right\\}_{l=1}^{L}$, the\nglobal feature (low resolution) as $P^{\\text {global }}$, where $P^{l} \\in \\mathbb{R}^{S \\times D}$,S\nis the slice number, L = H \u00d7 W is the token length, and D\nis the channel number. Na\u00efvely concatenating the high res-\nolution slice features for the subsequent segmentation pre-\ndiction is promising, but have two defects: 1) the spatial\ngeometry, such as the positional information, is corrupted\nacross slice features $\\left\\{P^{l}\\right\\}_{l=1}^{L}$; 2) the long-range dependen-\ncies and global context is missing. Thus, we propose Multi-\nRes Adapter to effectively restore spatial geometry of slice\nfeatures and capture long-range context from global feature.\nIn Multi-Res Adapter, the 0-th slice features $\\left\\{P^{l}\\right\\}_{l=1}^{L}$ are\nfirstly concatenated with learnable queries $Q \\in \\mathbb{R}^{N \\times D}$ and\ninput to the vanilla ViT blocks to build the query features\nfor each objects. Then for target fusion layer l, the slice\nfeatures $\\left\\{P^{l}\\right\\}_{l=1}^{L}$ and global feature $P^{\\text {global }}$ are fused through a\nMulti-Res Fusion (MRF) Module and then injected into the\nViT branch.\nMulti-Res Fusion (MRF) Module first reshape the global\nfeature to $\\hat{H} \\in \\mathbb{R}^{H \\times W \\times D}$, and restore slice features to\n$H^{l} \\in \\mathbb{R}^{m * H \\times n * W \\times D}$, where S = m \u00d7 n. To retain the spa-\ntial geometry of high-resolution features, we employ depth-\nwise separable convolutions to fuse the restored feature. To\neffectively model the local-global correspondence, we train\na Scale-aware Attention [5] to fuse multi-res features into\n$F^{l} \\in \\mathbb{R}^{H \\times W \\times D}$ as the fused feature\n\n$F^{l} = \\text { Up }\\left(\\alpha_{l}\\right) \\odot \\text { DConv }\\left(H^{l}\\right)+\\text { Up }\\left(\\left(1-\\alpha_{l}\\right) \\odot \\hat{H}\\right)$.\n\nThen $F^{l}$ is added to the visual tokens in Multi-Res Adapter.\nThe scale attention decoder $f_{\\theta}$ learns to predict the scale\nattention $\\alpha_{l}=\\sigma\\left(f_{\\theta}\\left(H^{l}\\right)\\right) \\in[0,1]^{H \\times W \\times D}$ for layer $l$\nto weigh the trustworthiness of low resolution context and\nhigh resolution detail. The sigmoid function $\\sigma$ ensures $\\alpha_{l}$\nweight in [0, 1], where 1 means $\\alpha_{l}$ focus on high resolution\ndetail. In practice, we empirically select the features from\na CLIP layer set $\\left\\{l_{j}\\right\\}_{j=1}^{J}$ to apply in Multi-Res Adapter.\nFor instance, for the model based on CLIP ViT-L model,\n$l \\in\\{\\text {stem}, 6,12,18\\}$. The fused features $\\left\\{F^{l}\\right\\}_{j=1}^{J}$ are\nused for hierarchical mask decoding. The final layer output\nslice features $\\left\\{P^{l}\\right\\}_{l=1}^{L}$ are restored to $H \\in \\mathbb{R}^{m * H \\times n * W \\times D}$\nas the visual feature for hierarchical mask decoding and\nmulti-grained masked attention. And the output queries Q\nare projected as the query feature\n\n$Q_{f} = f_{\\theta}\\left(Q\\right)$.\n\nfor hierarchical mask decoding and multi-grained masked\nattention."}, {"title": "3.5. Mask Prediction", "content": "Hierarchical Mask Decoding. High-resolution features\npreserve more spatial detail, thus benefit semantic seg-\nmentation, especially for mask prediction [4]. How-\never, directly upsampling features is computationally de-\nmanding. Thus, similar to FPN, we first upsample the\nmulti-resolution features $\\left\\{F^{l}\\right\\}_{j=1}^{J}$ from the Multi-Res\nAdapter by $\\{2 \\times, 4 \\times, 8 \\times\\}$ to build the feature pyramid.\nThen we gradually concatenate the multi-resolution fea-\ntures with the final visual feature H at channel dimension\nand upsample by 2 transposed convolution layers $H_{\\text {up }}=$\nTransposeConv(cat(H, $F^{l}$). Finally, we project the\nupsampled feature $H_{\\text {up }}$ to the pixel feature space by MLP\nthen decode the mask by inner product of query feature and"}, {"title": "3.6. Mask Classification", "content": "Decoupled Attention Mask Decoding. CLIP models,\nlike general Vision Transformers, perform open-vocabulary\nimage-level recognition through [CLS] token. Our goal\nis to recognize the various predicted mask proposals with-\nout changing any parameter of CLIP by guiding the atten-\ntion maps of [CLS] token on the region of interests. To\nachieve this goal, we follow MaskCLIP [14] and SAN [36]\nto reuse the original CLIP [CLS] token in the intermedi-\nate layer. To sufficiently aggregate multi-grained semantics\nfrom CLIP, we first duplicate the [CLS] token to query\nnumber N and create learnable positional embedding for\nthem, dubbed as the $X_{\\text {prop }} \\in \\mathbb{R}^{N \\times D}$. We aim to enforce\n$X_{\\text {prop }}$ to extract the global and local semantics from low-\nand high-resolution CLIP feature respectively. Thus, for vi-\nsual feature H, we first extract global contexts with max\npooling $\\bar{H}=\\text { MaxPool }(H)$ and train MLPs project them\nto attention space\n\n$A_{\\text {local }}=\\operatorname{MLP}_{\\theta}(H), \\quad A_{\\text {global }}=\\operatorname{MLP}_{\\theta}(\\bar{H}),$\n\nwhere $A_{\\text {global }} \\in \\mathbb{R}^{H \\times W \\times D^{\\prime}}, A_{\\text {local }} \\in \\mathbb{R}^{m * H * n * W \\times D^{\\prime}}$\ndenote the local and global attention features respectively.\nThen we decode local and global per-head attention masks\nby the inner product with\n\n$M_{\\text {local }}=Q_{f}^{\\top} \\times A_{\\text {local }}, \\quad M_{\\text {global }}=Q_{f}^{\\top} \\times A_{\\text {global }},$\n\nwhere $Q_{f}$ is the output query feature described in Sec.3.4.\nWe show this decoupled resolution-aware attention decod-\ning benefit the multi-grained aggregation in Fig.7.\nMulti-grained Masked Attention. As shown in Fig.4, we\nperform cross attention to update the $X_{\\text {prop }}$ with multi-\nresolution CLIP features, with the predicted attention masks\n$M_{\\text {local }}$ and $M_{\\text {global }}$\n\n$X_{\\text {prop }}^{\\prime}=$softmax$\\left(Q_{\\text {prop }}^{\\top}\\left(\\frac{K_{\\mathrm{LR}}}{\\sqrt{d}}\\cdot \\frac{M_{\\text {global }}}{\\sqrt{d}} \\right)+\\left(\\frac{K_{\\mathrm{HR}}}{\\sqrt{d}}\\cdot \\frac{M_{\\text {local }}}{\\sqrt{d}}\\right)^{\\top}\\right)\\left(V_{\\mathrm{LR}}+V_{\\mathrm{HR}}+X_{\\text {prop }}\\right),$\n\nwhere $Q_{\\text {prop }}=W X_{\\text {prop }}$ is query embeddings. Denote\nthe low- and high-resolution CLIP tokens as $X_{\\mathrm{LR}}$ and $X_{\\mathrm{HR}}$.\n$K_{\\mathrm{LR}}=W X_{\\mathrm{LR}}$ and $K_{\\mathrm{HR}}=W X_{\\mathrm{HR}}$ are the key embed-\ndings of low- and high-resolution CLIP visual tokens re-\nspectively. $V_{\\mathrm{LR}}=W \\top X_{\\mathrm{LR}}$ and $V_{\\mathrm{HR}}=W^{\\top} X_{\\mathrm{HR}}$ are value\nsummation\nembeddings. $W_{q}, W_{k}$ and $W_{v}$ are projection weights of\ncross-attention layer. The final output proposal logits $X_{\\text {prop }}$\nare projected to the shared vision-language space and com-\npute cosine similarity with text embeddings to obtain pro-\nposal logits $C \\in \\mathbb{R}^{N \\times K}: C=X_{\\text {prop }} W_{\\text {visual }} W_{\\text {text }} \\top X_{\\text {Text }}$,\nwhere K is the number of categories, and $W_{\\text {visual }}$ and\n$W_{\\text {text }}$ are projection weights. Finally, the final segmen-\ntation map $S \\in \\mathbb{Z}^{R \\times 1} \\mathbb{Z}^{K \\times H \\times W}$ is produced by\n\n$S = C \\cdot X_{\\text {mask }}^{\\top}.$"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Setting", "content": "We train our models on COCO-Stuff [3] dataset which\ncomprises 164K images with densely annotated masks\nspanning 171 categories. We first evaluate MROVSeg on\nfive well-established mainstream open-vocabulary semantic\nsegmentation benchmarks for standard evaluation. We\nfurther evaluate MROVSeg on Cityscapes [12] benchmarks\nto explore the ability of handling high-resolution image\ninput. We follow common practice [2, 35] to measure the\nsegmentation performance by mean intersection over union\n(mIoU) score.\nDatasets. The standard benchmarks contains three dataset:\nADE [43], Pascal Context [26], and Pascal VOC [15].\nThe ADE dataset contains around 20K and 2K images\nfor training and validation, respectively. This dataset is\nannotated with 150 and 847 categories, resulting in two\nseparate segmentation benchmarks, namely ADE-150 and\nADE-847. Similarly, the Pascal Context dataset has 5K\nimages for both training and validation. It is annotated with\n59 and 459 classes, forming two benchmarks known as PC-\n59 and PC-459. The Pascal VOC dataset comprises 1464\nand 1449 images for training and validation, encompassing\nannotated masks across 20 semantic categories."}, {"title": "4.2. Standard Evaluation", "content": "Comparisons with State-of-the-Art Methods. In Tab. 1,\nwe compare MROVSeg with current state-of-the-art\nopen-vocabulary semantic segmentation methods on stan-\ndard benchmarks. First of all, our method significantly\noutperforms other state-of-the-art methods all various\nopen-vocabulary semantic segmentation benchmarks.\nSpecifically, our method supasses state-of-the-art method\nSAN [30] with the same CLIP backbones on all bench-\nmarks by remarkable margins (+1.4% mIoU for ADE-847,\n+5.9% mIoU for PC-459, +3.1% mIoU for ADE-150,\n+3.1% mIoU for PC-59, and +0.9 for VOC-20 with CLIP\nViT-B backbone, +2.4% mIoU for ADE-847, +7.0%\nmIoU for PC-459, +3.6% mIoU for ADE-150, +3.9%"}, {"title": "4.3. Ablation Study", "content": "For all ablation experiments, we employ our CLIP ViT-\nB/16 based model as our ablated baseline."}, {"title": "5. Conclusion", "content": "In this paper, we introduce MROVSeg, a multi-resolution\ntraining framework designed to enhance open-vocabulary\nsemantic segmentation by leveraging multi-resolution VLM\nfeatures. The exceptional quantitative and qualitative re-\nsults obtained on well-established open-vocabulary seman-\ntic segmentation benchmarks serve as compelling evidence\nof its effectiveness and versatility. We hope our method can\nserve as a strong baseline for future research."}, {"title": "Appendix", "content": null}, {"title": "A. Overview", "content": "In the supplementary material for MROVSeg, we provide\nimplementation detail (Sec. B), qualitative comparison\n(Sec. C), and limitation and discussion (Sec. D)."}, {"title": "B. Implementation Detail", "content": null}, {"title": "B.1. Training Detail", "content": "During the training, we follow common practices [10, 36]\nto use AdamW optimizer with a base learning rate of 2e - 4\nwith poly decay schedule power 0.9 to train the model, and\nthe weight decay is set to le - 4. We use the same data aug-\nmentation strategies as in [22, 36, 38] for training, i.e., all\ntraining images firstly are randomly resized to [0.5, 2.0] \u00d7\nof its original resolution, then are randomly cropped into\nthe resolution of 6402. And for fair comparison, the test-\ntime argumentation is not used."}, {"title": "B.2. Multi-grained Masked Attention", "content": "As introduced in Sec. 3.6, we enable the $X_{\\text {prop }}$ by perform\nmasked cross attention with multi-resolution CLIP visual\ntokens. We show the pseudo code in Algorithm 1. During\nthe inference, denote the number of CLIP token as N, the\nnumber of CLIP high-resolution token 4N, the number of\nquery token as Nprop, then the time complexity of the cross\nattention implementation is O(5N\u00b2 + 20NqueryN)."}, {"title": "C. Qualitative Results", "content": "In this section, we provide the segmentation results on\nCOCO-Stuff val [3] (close-vocabulary setting), ADE [43]\nand Pascal Context [26] (open-vocabulary setting)."}, {"title": "C.1. Comparison", "content": "We provide more qualitative comparison of our method\nMROVSeg with state-of-the-art methods SAN [36] and EB-\nSeg [30] in Fig.S10. We observe that MROVSeg can handle\ncomplex scenarios while preserving spatial details (object\nboundaries) and extract global context effectively."}, {"title": "C.2. COCO", "content": "We visualize the closed-vocabulary semantic predictions of\nCOCO-Stuff [3]. As shown in Fig.S9, our method is able to\nhandle complex scenarios and provide precise mask predic-\ntions. However, as the labels exist semantically containment\nrelationships, our model struggles to distinguish the regions\nwith similar semantics. For instance, the model cannot find\nout the donut out of the food-other in fifth row of Fig.S9(b)."}, {"title": "C.3. ADE", "content": "We visualize the open-vocabulary semantic segmentation\npredctions of MROVSeg on ADE [43] dataset with 150 and\n847 categories benchmark respectively. As the result, we\nfind MROVSeg can segment and identify the novel classes\nthat only appear in 847-class annotations. As illustrated in\nFig.S11, the model is able to segment and recognize phone\nbooth and snow in (b) and (c) of Fig.S11 respectively."}, {"title": "C.4. Pascal Context", "content": "The qualitative results of segmentation performance of\nMROVSeg for 59 and 459 semantic categories on Pas-\ncal Context [26] dataset are shown in Fig.S12. Note that\nMROVSeg is able to segment small objects (such as the\nvideo camera in Fig.S12(c)), which is benefit from the high\nresolution VLM features and strong local contexts."}, {"title": "D. Disucssion", "content": "In this section, we further discuss the limitation, future work\nand potential negtive impact of MROVSeg."}, {"title": "D.1. Limitation and Future Work", "content": "Due to computational constraints and for fair performance\ncomparison, we conduct our experiments solely based on"}, {"title": "D.2. Dataset usage", "content": "This work involves training segmentation models on pub-\nlic datasets(COCO [23], VOC [15], and ADE [43]). These\ndatasets may contain personally identifiable information\n(such as facial characteristics), but they were responsibly\ncollected and used. These datasets have benefited the im-\nage segmentation community for years."}, {"title": "D.3. Potential Negtive Impact", "content": "This work expands the scope of open-vocabulary seman-\ntic segmentation methods, offering significant advantages\nin areas like healthcare and autonomous driving. However,\nthere is potential for misuse in applications such as surveil-\nlance, a common issue in most semantic segmentation re-\nsearch. Implementing regulatory measures to control the\napplication of these algorithms might be an effective solu-\ntion."}]}