{"title": "BISSL: BILEVEL OPTIMIZATION FOR SELF-SUPERVISED PRE-TRAINING AND FINE-TUNING", "authors": ["Gustav Wagner Zakarias", "Lars Kai Hansen", "Zheng-Hua Tan"], "abstract": "In this work, we present BiSSL, a first-of-its-kind training framework that in- \n troduces bilevel optimization to enhance the alignment between the pretext pre- \n training and downstream fine-tuning stages in self-supervised learning. BiSSL \n formulates the pretext and downstream task objectives as the lower- and upper- \n level objectives in a bilevel optimization problem and serves as an intermediate \n training stage within the self-supervised learning pipeline. By more explicitly \n modeling the interdependence of these training stages, BiSSL facilitates enhanced \n information sharing between them, ultimately leading to a backbone parameter \n initialization that is better suited for the downstream task. We propose a training \n algorithm that alternates between optimizing the two objectives defined in BiSSL. \n Using a ResNet-18 backbone pre-trained with SimCLR on the STL10 dataset, we \n demonstrate that our proposed framework consistently achieves improved or com- \n petitive classification accuracies across various downstream image classification \n datasets compared to the conventional self-supervised learning pipeline. Quali- \n tative analyses of the backbone features further suggest that BiSSL enhances the \n alignment of downstream features in the backbone prior to fine-tuning.", "sections": [{"title": "1 INTRODUCTION", "content": "In the absence of sufficient labeled data, self-supervised learning (SSL) has emerged as a promising \n approach for training deep learning models. Rather than relying solely on labeled data, the SSL \n framework aims to learn representations from unlabeled data which proves beneficial for subse- \n quent use on various downstream tasks. These representations are learned by solving a pretext task, \n which utilizes supervisory signals extracted from the unlabeled data itself. Extensive efforts have \n been made into designing effective pretext tasks which in recent years have led to state-of-the-art or \n competitive performance in various fields such as computer vision (Chen et al., 2020b; Bardes et al., \n 2022; He et al., 2020; Grill et al., 2020; Caron et al., 2021; He et al., 2022), audio signal process- \n ing (Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Niizumi et al., 2021; Chung & \n Glass, 2018; Chung et al., 2019; Yadav et al., 2024) and natural language processing (Devlin et al., \n 2019; Lewis et al., 2019; Brown et al., 2020; He et al., 2021). \n\n Making a self-supervised pre-trained backbone suitable for a downstream task typically involves \n attaching additional layers that are compatible with that task, followed by fine-tuning the entire or \n parts of the composite model in a supervised manner (Zhai et al., 2019; Dubois et al., 2022). When a \n backbone is pre-trained on a distribution that differs from the distribution of the downstream data, the \n representations learned during pre-training may not be initially well-aligned with the downstream \n task. During fine-tuning, this distribution misalignment could cause relevant semantic information, \n learned during the pre-training phase, to vanish from the representation space (Zaiem et al., 2024; \n Chen et al., 2020a; Boschini et al., 2022). A potential strategy for alleviating the negative effects of \n these distribution discrepancies would be to enhance the alignment between the pretext pre-training \n and downstream fine-tuning stages. However, since the conventional SSL pipeline treats these stages \n as two disjoint processes, this poses a significant challenge in devising a strategy that enhances such \n alignment while not compromising on the benefits that SSL offers."}, {"title": "2 RELATED WORK", "content": "Bilevel Optimization in Self-Supervised Learning Bilevel optimization (BLO) refers to a con- \n strained optimization problem, where the constraint itself is a solution to another optimization prob-"}, {"title": "3 PROPOSED METHOD", "content": "We denote the unlabeled pretext dataset $D^P = \\{z_k\\}_{k=1}^{CP}$ and labeled downstream dataset $D^D= \\{x_l, y_l\\}_{l=1}^{CD}$, respectively, where $z_k, x_l \\in R^N$. Let $f_\\theta : R^N \\rightarrow R^M$ denotes a feature extracting backbone with trainable parameters $\\theta$ and $\\phi : R^M \\rightarrow R^P$ a task specific projection head with trainable parameters $\\phi$. Given pretext and downstream models $g_{\\theta_P} \\circ f_{\\theta_P}$ and $h_{\\theta_D} \\circ f_{\\theta_D}$ with $\\theta_P,\\theta_D \\in R^L$, we denote the pretext and downstream training objectives $L_P(\\theta_P, \\Phi_P; D^P)$ and $L_D(\\theta_D, \\Phi_D; D^D)$, respectively. To simplify notation, we omit the dataset specification from the training objectives, e.g. $L_D(\\theta_D, \\Phi_D) := L_D(\\theta_D, \\Phi_D; D^D)$.\n\n\nThe conventional setup of self-supervised pre-training directly followed by supervised fine-tuning relies on using a single backbone model with parameters $\\theta$. In that instance, we minimize"}, {"title": "3.2 \u039f\u03a1\u03a4\u0399MIZATION PROBLEM FORMULATION", "content": "$\n\\begin{equation}\n\\min_{\\theta,\\Phi_P} L_P(\\theta, \\Phi_P)\n\\end{equation}\n$to produce a backbone parameter configuration $\u03b8^*$ which is then used as an initial- \n ization when subsequently minimizing the downstream training objective $L_D(\u03b8, \u03a6_D)$. We deviate \n from this by instead considering $\u03b8_P$ and $\u03b8_D$ as two separate parameter vectors that are strongly cor- \n related. In continuation, we suggest combining the two traditionally separate optimization problems \n of pretext and downstream training into a joint optimization problem through bilevel optimization \n called BiSSL. We formulate BiSSL as\n$\n\\begin{equation}\n\\min_{\\theta_D,\\Phi_D} L_D(\\theta_P (\\theta_D), \\Phi_D) + \\gamma L_D (\\theta_D, \\Phi_D)\n\\end{equation}\n$\ns.t.\n$\n\\begin{equation}\n\\theta^*_P(\\theta_D) \\in argmin_{\\theta_P, \\Phi_P} \\min L_P(\\theta_P, \\Phi_P) + \\lambda r(\\theta_D, \\theta_P)\n\\end{equation}\n$\nwith $\u03b3 \\in R^+$ and $r$ being some convex regularisation objective weighted by $\u03bb \\in R^+$ enforcing \n similarity between $\u03b8_P$ and $\u03b8_D$. The upper-level training objective in equation 2 is tasked with \n minimizing the downstream task objective $L_D$, while the lower-level objective in equation 3 aims \n to minimize the pretext task objective $L_P$ while also ensuring its backbone remains similar to the \n upper-level backbone. As seen in the left term of equation 2, the backbone parameters ($\u03b8_D$) \n are transferred into the downstream training objective, mirroring how the backbone is transferred in \n the conventional SSL pipeline. Although the second term of equation 2 is not strictly necessary, it \n has empirically shown to improve stability and aid convergence of the upper-level solution during \n training. Unlike the traditional SSL setup, the backbone solution of the pretext objective  ($\u03b8^*_P(\u03b8_D)$) \n is now a function of the parameters of the downstream backbone \u03b8D, as the lower-level problem is \n dependent on the upper-level backbone parameters.\n\n As the upper-level objective in equation 2 depends on the solution ($\u03b8^*_P(\u03b8_D)$) of the lower-level ob- \n jective in equation 3, this enables the incorporation of information from the pretext objective when \n solving the upper-level optimization problem. By including a regularization objective r that enforces \n similarity between the lower-level and upper-level backbone parameters, this setup is hypothesized \n to guide the lower-level to achieve a configuration of model backbone parameters that is more ben- \n eficial for subsequent conventional fine-tuning on the downstream task. To more precisely under- \n stand how the pretext objective influences the downstream training procedure in this setup, we delve \n deeper into the expression of the gradient of the upper-level training objective in equation 2 in the \n following subsection."}, {"title": "3.3 UPPER-LEVEL DERIVATIVE", "content": "Given the upper-level objective $F(\\theta_D, \\Phi_D) := L_D(\\theta_P(\\theta_D), \\Phi_D) + \\gamma L_D (\\theta_D, \\Phi_D)$ from equation 2, \n its derivative with respect to $\u03b8_D$ is given by\n$\n\\begin{equation}\n\\frac{dF}{d\\theta_D} = \\frac{d\\theta_P(\\theta_D)^T}{d\\theta_D} \\nabla_{\\theta}L_D(\\theta,\\Phi_D)|_{\\theta=\\theta_P(\\theta_D)} + \\gamma \\nabla_{\\theta}L_D(\\theta, \\Phi_D)|_{\\theta=\\theta_D}.\n\\end{equation}\n$\nDue to the dependence of the lower-level solution on the upper-level parameters, the first term of \n equation 4 includes the implicit gradient (IG) of the implicit function $\u03b8^*_P(\u03b8_D)$. To simplify notation, \n we let $\u2207_\u03beh(\u03be)|\u03be=\u03c8 := \u2207_\u03beh(\u03c8)$ when it is clear from context which variables are differentiated with \n respect to. Following an approach similar to Rajeswaran et al. (2019), with details on the derivations \n and underlying assumptions outlined in Appendix A, the IG in equation 4 can be explicitly expressed \n as\n$\n\\begin{equation}\n\\frac{d\\theta_P(\\theta_D)^T}{d\\theta_D} = -\\nabla_{\\theta\\theta}^2r(\\theta_D,\\theta_P(\\theta_D))^{-1} \\Big[ \\nabla_{\\theta} (L_P(\\theta_P(\\theta_D), \\Phi_P) + r(\\theta, \\theta_P(\\theta_D)) \\Big]\n\\end{equation}\n$\nA common convex regularization objective, which will also be the choice in the subsequent exper- \n iments of this work, is $r(\u03be, \u03c8) = \\frac{1}{2} ||\u03be \u2013 \u03c8||^2$. Using this regularization objective simplifies equa- \n tion 5 down to\n$\n\\begin{equation}\n\\frac{d\\theta_P(\\theta_D)^T}{d\\theta_D} = -\\Big[ \\nabla_{\\theta\\theta}^2L_P(\\theta(\\theta_D), \\Phi_P) + \\lambda I_L \\Big]^{-1}\n\\end{equation}\n$\nwhere $I_L$ is the $L \u00d7 L$-dimensional identity matrix. Hence the upper-level derivative in equation 4 \n can be expressed as\n$\n\\begin{equation}\n\\frac{dF}{d\\theta_D} = \\Big[ - \\nabla_{\\theta\\theta}^2L_P(\\theta(\\theta_D), \\Phi_P) + \\lambda I_L \\Big]^{-1} \\nabla_{\\theta}L_D(\\theta(\\theta_D), \\Phi_D) + \\gamma \\nabla_{\\theta}L_D(\\theta, \\Phi_D)|_{\\theta=\\theta_D}.\n\\end{equation}\n$"}, {"title": "3.4 TRAINING ALGORITHM AND PIPELINE", "content": "Algorithm 1 outlines the proposed training algorithm, which iteratively alternates between solv- \n ing the lower-level (equation 3) and upper-level (equation 2) optimization problems in BiSSL. The \n lower-level training optimizes the pretext task objective, while additionally including the gradient of \n the regularization term r for the backbone parameter updates, complying with equation 3. For the \n upper-level training, the gradient with respect to the backbone parameters as represented by the left \n term on the right-hand side in equation 7, is approximated using the CG method. Additionally, the"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "The STL10 dataset (Coates et al., 2011) is used throughout the experiments. It comprises two par- \n titions: 100.000 unlabeled images and 13.000 labeled images with 10 classes in total whereas 5000 \n and 8000 are assigned for training and testing, respectively. All images are natural images of reso- \n lution 96 \u00d7 96, with the unlabeled partition drawn from a similar but broader distribution than the \n labeled partition. This dataset strikes a balance between complexity and computational feasibility, \n offering higher resolution and more diverse content than smaller datasets like CIFAR10 (Krizhevsky, \n 2012) while being less resource-intensive than larger-scale datasets such as ImageNet (Deng et al., \n 2009). For ease of reference, STL10U and STL10L will denote the unlabeled and labeled parti- \n tions, respectively. In all experiments, STL10U will be employed for self-supervised pre-training. \n For downstream fine-tuning and evaluation, we leverage a varied set of natural image classification \n datasets that encompass a wide array of tasks, including general image classification, fine-grained \n recognition across species and objects, scene understanding, and texture categorization. The datasets \n include STL10L, Oxford 102 Flowers (Nilsback & Zisserman, 2008), StanfordCars (Yang et al., \n 2015), FGVC Aircraft (Maji et al., 2013), Describable Textures Dataset (DTD) (Cimpoi et al., 2014), \n Oxford-IIIT Pets (Parkhi et al., 2012), FashionMNIST (Xiao et al., 2017), CIFAR10 (Krizhevsky, \n 2012), CIFAR100 (Krizhevsky, 2012), Caltech-101 (Li et al., 2022a), Food 101 (Bossard et al., \n 2014), SUN397 scene dataset (Xiao et al., 2010), Caltech-UCSD Birds-200-2011 (CUB200) (Wah \n et al., 2011) and PASCAL VOC 2007 (Everingham et al.). All downstream datasets are split into \n training, validation, and test partitions, with details on how these assignments are made provided in \n Section B.1 of Appendix B."}, {"title": "4.1 DATASETS", "content": "The STL10 dataset (Coates et al., 2011) is used throughout the experiments. It comprises two par- \n titions: 100.000 unlabeled images and 13.000 labeled images with 10 classes in total whereas 5000 \n and 8000 are assigned for training and testing, respectively. All images are natural images of reso- \n lution 96 \u00d7 96, with the unlabeled partition drawn from a similar but broader distribution than the \n labeled partition. This dataset strikes a balance between complexity and computational feasibility, \n offering higher resolution and more diverse content than smaller datasets like CIFAR10 (Krizhevsky, \n 2012) while being less resource-intensive than larger-scale datasets such as ImageNet (Deng et al., \n 2009). For ease of reference, STL10U and STL10L will denote the unlabeled and labeled parti- \n tions, respectively. In all experiments, STL10U will be employed for self-supervised pre-training. \n For downstream fine-tuning and evaluation, we leverage a varied set of natural image classification \n datasets that encompass a wide array of tasks, including general image classification, fine-grained \n recognition across species and objects, scene understanding, and texture categorization. The datasets \n include STL10L, Oxford 102 Flowers (Nilsback & Zisserman, 2008), StanfordCars (Yang et al., \n 2015), FGVC Aircraft (Maji et al., 2013), Describable Textures Dataset (DTD) (Cimpoi et al., 2014), \n Oxford-IIIT Pets (Parkhi et al., 2012), FashionMNIST (Xiao et al., 2017), CIFAR10 (Krizhevsky, \n 2012), CIFAR100 (Krizhevsky, 2012), Caltech-101 (Li et al., 2022a), Food 101 (Bossard et al., \n 2014), SUN397 scene dataset (Xiao et al., 2010), Caltech-UCSD Birds-200-2011 (CUB200) (Wah \n et al., 2011) and PASCAL VOC 2007 (Everingham et al.). All downstream datasets are split into \n training, validation, and test partitions, with details on how these assignments are made provided in \n Section B.1 of Appendix B."}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "Pretext Task Training The SimCLR (Chen et al., 2020b) pretext task with temperature r = 0.5 \n is used for pre-training a ResNet-18 (He et al., 2016) backbone model. We selected this widely \n adopted architecture due to its proven ability to extract high-quality visual representations while \n maintaining relatively low computational requirements, striking an effective balance between per- \n formance and resource efficiency. On top of the backbone, a projection head is used, consisting of \n two fully connected layers with batch normalization (Ioffe & Szegedy, 2015) and ReLU (Agarap, \n 2018) followed by a single linear layer. Each layer consists of 256 neurons.\n The image augmentation scheme follows the approach used in Bardes et al. (2022), with minor \n modifications: The image size is set to 96 \u00d7 96 instead of 224 \u00d7 224, and the minimal ratio of the \n random crop is adjusted accordingly to 0.5 instead of 0.08."}, {"title": "4.2.1 BASELINE SETUP", "content": "Pretext Task Training The SimCLR (Chen et al., 2020b) pretext task with temperature r = 0.5 \n is used for pre-training a ResNet-18 (He et al., 2016) backbone model. We selected this widely \n adopted architecture due to its proven ability to extract high-quality visual representations while \n maintaining relatively low computational requirements, striking an effective balance between per- \n formance and resource efficiency. On top of the backbone, a projection head is used, consisting of \n two fully connected layers with batch normalization (Ioffe & Szegedy, 2015) and ReLU (Agarap, \n 2018) followed by a single linear layer. Each layer consists of 256 neurons.\n The image augmentation scheme follows the approach used in Bardes et al. (2022), with minor \n modifications: The image size is set to 96 \u00d7 96 instead of 224 \u00d7 224, and the minimal ratio of the \n random crop is adjusted accordingly to 0.5 instead of 0.08."}, {"title": "4.2.2 BISSL SETUP", "content": "In this section, we detail each stage of the proposed training pipeline for BiSSL, as outlined in the \n right part of Figure 1.\n\nPretext Warm-up The backbone @ and projection head op are initialized by self-supervised pre- \n training using a setup almost identical to the baseline pretext task training setup in Section 4.2.1. \n The only difference is that this training stage is conducted for 500 epochs instead of 600 epochs. \n This adjustment is made because the BiSSL training stage will conduct what is roughly equivalent \n to 100 pretext epochs, as detailed more specifically in the composite configuration paragraph below. \n This ensures that the total number of pretext pre-training steps is comparable to those conducted in \n the baseline setup.\n\nDownstream Head Warm-up The training setup for the downstream head warm-up closely mir- \n rors the fine-tuning setup of Section 4.2.1. The main difference is that only the linear downstream \n head is fitted on top of the now frozen backbone obtained from the pretext warm-up. Learning rates \n and weight decays are initially selected based on those listed in Table 2, with adjustments made \n as needed when preliminary testing indicated a potential for improved convergence. These values \n are provided in Table 3 in Appendix B. The authors recognize that more optimal hyper-parameter \n configurations may exist and leave further exploration of this for future refinement. The downstream \n head warm-up is conducted for 20 epochs with a constant learning rate.\n\nLower-level of BiSSL The training configuration for the lower-level primarily follows the setup \n described for pretext pre-training in Section 4.2.1, with the modifications outlined here. As specified \n in equation 3, the lower-level loss function is the sum of the pretext task objective LP (in our case, \n the NT-Xent loss from SimCLR (Chen et al., 2020b)) and the regularization term r(0p, 0p) ="}, {"title": "4.3 DOWNSTREAM TASK PERFORMANCE", "content": "The impact of using BiSSL compared to the conventional self-supervised training pipeline is bench- \n marked by evaluating classification accuracies on the various specified downstream datasets. Ta- \n ble 1 presents the means and standard deviations of top-1 and top-5 classification accuracies (or \n the 11-point mAP on the VOC2007 dataset) on these downstream test datasets, comparing results \n obtained from the conventional SSL pipeline with those achieved using the BiSSL pipeline. The re- \n sults demonstrate that training with BiSSL significantly improves either top-1 or top-5 classification \n accuracy, or 11-point mAP in the case of VOC07, on 10 out of 14 datasets, with no single result \n showing a significant decline in performance compared to the baseline."}, {"title": "4.3.1 PERFORMANCE OVER VARYING PRE-TRAINING EPOCHS", "content": "Table 1 demonstrates that BiSSL can significantly enhance classification accuracies across various \n datasets. To determine whether this improvement persists across different levels of pretext pre- \n training, additional experiments are conducted using varying numbers of total pre-training epochs. \n The accuracies are compared with the total number of pretext pre-training epochs kept roughly the \n same, so the pretext warm-up in the BiSSL pipeline is conducted for 100 epochs less than the base- \n line pretext pre-training, as justified in the \u2018Composite Configuration Details of BiSSL\" paragraph \n of Section 4.2.2. The flowers dataset, which showed substantial benefits from BiSSL on the top-1 \n classification accuracies, is selected for these experiments. The results presented in Figure 2 re- \n veal that BiSSL generally sustains the relative performance gap over the baseline, regardless of the \n pre-training duration. This suggests that the advantages conferred by BiSSL are not contingent on \n the amount of pre-training. Rather, this indicates that BiSSL may provide a more efficient learning \n trajectory, stemming from the enhanced information sharing it facilitates between the pretext and \n downstream tasks."}, {"title": "4.4 VISUAL INSPECTION OF LATENT FEATURES", "content": "To gain deeper insight into how BiSSL affects the representations learned compared to conventional \n pretext pre-training, we perform a qualitative visual inspection of latent spaces. This involves com- \n paring features processed by backbones trained solely by pretext pre-training to those derived from \n lower-level backbones obtained after conducting BiSSL, each trained as described in the \u201cPretext \n Task Training\" and \u201cLower-level of BiSSL\u201d paragraphs in Section 4.2.1 and 4.2.2, respectively. By \n comparing these features, we aim to assess whether BiSSL nudges the latent features toward be- \n ing more semantically meaningful for the downstream task. The t-Distributed Stochastic Neighbor \n Embedding (t-SNE) (Cieslak et al., 2020) technique is employed for dimensionality reduction. Fur- \n ther details regarding the experimental setup are outlined in Section C.1 of Appendix C. Figure 3"}, {"title": "5 CONCLUSION", "content": "This study integrates pretext pre-training and downstream fine-tuning into a unified bilevel optimiza- \n tion problem, from which the BiSSL training framework is proposed. BiSSL explicitly models the \n inheritance of backbone parameters from the pretext task, enhancing the transfer of relevant infor- \n mation between the pretext and downstream tasks. We propose a practical training algorithm and \n pipeline that incorporates BiSSL as an intermediate stage between pretext pre-training and down- \n stream fine-tuning. Experiments across various image classification datasets demonstrate that BiSSL \n consistently achieves improved or comparable downstream classification performance relative to the \n conventional self-supervised learning pipeline. Additionally, our findings indicate that in instances \n where BiSSL improves performance, this improvement remains consistent regardless of the pre- \n text pre-training duration. Further analysis suggests that BiSSL enhances the downstream semantic \n richness of learned representations, as evidenced by qualitative inspections of latent spaces. BiSSL \n marks a potential advancement towards enhancing the alignment between the pretext pre-training \n and downstream fine-tuning stages, revealing a new direction for self-supervised learning algorithm \n designs that leverage bilevel optimization."}, {"title": "5.1 FUTURE WORK", "content": "Formulating the self-supervised pipeline as a bilevel optimization problem can be approached \n through various strategies, each involving trade-offs in computational complexity and theoretical \n justification. While this study demonstrates a promising method for enhancing downstream per- \n formance, further exploration of alternative formulations is necessary to identify potentially more \n optimal setups. Moreover, although BiSSL generally applies to any downstream task and model \n size in theory, our experiments were confined to small-scale image classification due to resource \n constraints. Consequently, it remains an open question whether BiSSL can scale effectively to larger \n setups and other downstream tasks."}, {"title": "A DERIVATION OF THE IMPLICIT GRADIENT", "content": "Assume the setup of the BiSSL optimization problem described in equation 2 and equation 3. In \n the following derivations, we will assume that op is fixed, allowing us to simplify the expressions \n involved. To streamline the notation further, we continue to use the convention \u2207\u025bh(\u03be)|\u03b5=\u03c8 :=\n \u2207\u025bh(\u03c8), when it is clear from context which variables are differentiated with respect to. Under \n these circumstances, we then define the lower-level objective from equation 3 as\n$\n\\begin{equation}\nG(\\theta_D, \\theta_P) := L_P (\\theta_P, \\phi_P) + \\lambda r(\\theta_D,\\theta_P).\n\\end{equation}\n$\nRecalling that r is a convex regularization objective, adequate scaling of \u03bb effectively \u201cconvexi- \n fies\" the lower-level objective G, a strategy also employed on the lower-level objective in previous \n works (Rajeswaran et al., 2019; Zhang et al., 2022b; 2023a). This is advantageous because assuming \n convexity of G ensures that for any OD \u2208 RL, there exists a corresponding op \u2208 R that satisfies \n the stationary condition VepG(0D, 8p) = 0. In other words, we are assured that a minimizer of \n G(0D,) exists for all OD \u2208 RL. Now, further assume that VepG(0D, 0p) is continuously differen- \n tiable and that the Hessian matrix VG(0D, Up) is invertible for all OD \u2208 RL. Under these condi- \n tions, the implicit function theorem (Dontchev & Rockafellar, 2014; Zucchet & Sacramento, 2022) \n guarantees the existence of an implicit unique and differentiable function 0: N(0D) \u2192 RL, with \n N(0D) being a neighborhood of AD, that satisfies 0(0D) = \u00dbp and VepG(\u016aD, 0p(\u016aD)) = 0 for \n all OD \u2208 N (OD)."}, {"title": "B EXPERIMENTAL DETAILS", "content": "The Caltech-101 (Li et al., 2022a) dataset does not come with a pre-defined train/test split, so the \n same convention as previous works is followed (Chen et al., 2020b; Donahue et al., 2014; Simonyan \n & Zisserman, 2014), where 30 random images per class are selected for the training partition, and \n the remaining images are assigned for the test partition. For the DTD (Cimpoi et al., 2014) and \n SUN397 (Xiao et al., 2010) datasets, which offer multiple proposed train/test partitions, the first \n splits are used, consistent with the approach in Chen et al. (2020b).\n\nFor downstream hyperparameter optimization, portions of the training partitions from each respec- \n tive labeled dataset are designated as validation datasets. The FGVC Aircraft (Maji et al., 2013), \n Oxford 102 Flowers (Nilsback & Zisserman, 2008), DTD, and Pascal VOC 2007 (Everingham et al.) \n datasets already have designated validation partitions. For all the remaining labeled datasets, the val- \n idation data partitions are randomly sampled while ensuring that class proportions are maintained. \n For the multi-attribute VOC07 dataset, sampling is performed with class balance concerning the first \n attribute present in each image. Roughly 20% of the training data is allocated for validation."}, {"title": "B.1 DATASET PARTITIONS", "content": "The Caltech-101 (Li et al., 2022a) dataset does not come with a pre-defined train/test split, so the \n same convention as previous works is followed (Chen et al., 2020b; Donahue et al., 2014; Simonyan \n & Zisserman, 2014), where 30 random images per class are selected for the training partition, and \n the remaining images are assigned for the test partition. For the DTD (Cimpoi et al., 2014) and \n SUN397 (Xiao et al., 2010) datasets, which offer multiple proposed train/test partitions, the first \n splits are used, consistent with the approach in Chen et al. (2020b).\n\nFor downstream hyperparameter optimization, portions of the training partitions from each respec- \n tive labeled dataset are designated as validation datasets. The FGVC Aircraft (Maji et al., 2013), \n Oxford 102 Flowers (Nilsback & Zisserman, 2008), DTD, and Pascal VOC 2007 (Everingham et al.) \n datasets already have designated validation partitions. For all the remaining labeled datasets, the val- \n idation data partitions are randomly sampled while ensuring that class proportions are maintained. \n For the multi-attribute VOC07 dataset, sampling is performed with class balance concerning the first \n attribute present in each image. Roughly 20% of the training data is allocated for validation."}, {"title": "B.2 DOWNSTREAM TASK FINE-TUNING OF THE BASELINE SETUP", "content": "In Table 2, the learning rates and weight decays used for each respective downstream dataset of the \n experiments described in Section 4.2.1 are outlined."}, {"title": "B.3 DOWNSTREAM HEAD WARMUP AND UPPER-LEVEL OF BISSL", "content": "Table 3 outlines the learning rates and weight decays used for the downstream head warm-up and \n upper-level of BiSSL of each respective downstream dataset, as described in the BiSSL experimental"}, {"title": "B.4 COMPOSITE CONFIGURATION OF BISSL", "content": "To avoid data being reshuffled between every training stage alternation, the respective batched lower- \n and upper-level training datasets are stored in separate stacks from which data is drawn. The stacks \n are only \"reset\" when the number of remaining batches is smaller than the number of gradient \n steps required before alternating to the other level. For example, the lower-level stack is reshuffled \n every fourth training stage alternation. If the downstream dataset does not provide enough data for \n making Nu = 8 batches with non-overlapping data points, the data is simply reshuffled every time \n the remaining number of data points is smaller than the upper-level batch size (256 images in these \n experiments)."}, {"title": "B.5 DOWNSTREAM FINE-TUNING AFTER BISSL", "content": "The learning rates and weight decays used for downstream fine-tuning after BiSSL for each respec- \n tive downstream dataset are outlined in Table 4. Section 4.2.2 outlines the experimental setup."}, {"title": "CADDITIONAL RESULTS", "content": "Test data features of the downstream test data processed by backbones trained through conventional \n pretext pre-training are compared against those trained with BiSSL. This allows for an inspection of \n the learned representations prior to the final fine-tuning stage.\n\nDuring the evaluation, it is important to note that the batch normalization layers (Ioffe & Szegedy, \n 2015) of the pre-trained backbones utilize the running mean and variance inferred during train- \n ing. Since these pre-trained backbones have not been exposed to the downstream datasets during \n training, their batch normalization statistics may not be optimal for these new datasets. To address \n this, the training dataset is divided into batches of 256 samples, and roughly 100 batches are then \n forward-passed through"}]}