{"title": "Neural ODEs for Stiff Systems: Implicit Single-Step Methods", "authors": ["Colby Fronk", "Linda Petzold"], "abstract": "Stiff systems of ordinary differential equations (ODEs) are pervasive in many science and engineering fields, yet standard neural ODE approaches struggle to learn them. This limitation is the main barrier to the widespread adoption of neural ODEs. In this paper, we propose an approach based on single-step implicit schemes to enable neural ODEs to handle stiffness and demonstrate that our implicit neural ODE method can learn stiff dynamics. This work addresses a key limitation in current neural ODE methods, paving the way for their use in a wider range of scientific problems.", "sections": [{"title": "I. INTRODUCTION", "content": "Developing a mathematical model is essential for understanding complex processes in chemistry, biology, and engineering. Ordinary differential equation (ODE) models, for example, describe the spread of diseases like flu, measles, and COVID-19 in epidemiology and the dynamics of CD4 T-cells during HIV infection in medicine. Detailed models help to enable the identification of intervention methods, such as drugs for disease prevention. Mechanistic models, based on first principles like conservation laws and force interactions, provide insights into system behavior under different scenarios, making them preferable over black-box models to scientists and engineers. However, these models require lengthy development cycles, highlighting the need for tools that can accelerate and support model development.\nSparse Identification of Nonlinear Dynamics (SINDy)1\u20133 is a recent advancement in system identification, using linear regression of time derivatives against candidate terms to identify ODE models. SINDy has successfully recovered ODE systems across various fields, including fluid dynamics4, plasma physics5, chemical reaction networks6,7, and optical communications8. However, it relies on densely sampled training data. In contrast, neural ODEs9\u201320 can handle irregular data and do not have strict requirements on sampling rates or data point frequency.\nThe rise of data from the Internet of Things21,22, robotics for high-throughput experiments23,24, and earth observation satellites25 has necessitated new methods for processing and understanding large datasets. Neural differential equations and physics-informed neural networks (PINNs)26-32 offer powerful frameworks for modeling such systems, but their black-box nature limits their interpretability and generalizability.\nSymbolic neural networks are emerging as a response to the demand for more interpretable models. Several architectures9,33\u201340 embed mathematical terms within the network, enabling symbolic regression with neural networks to recover equations that are interpretable and usable by scientists9,20. This approach integrates the strengths of both neural differential equations and mechanistic models, potentially transforming model development cycles.\nThe main bottleneck preventing the widespread adoption of neural ODEs is their difficulty in handling stiffness. Stiff ODEs are equations in which certain components of the solution vary rapidly compared to others, requiring extremely small time steps for explicit methods to maintain stability. These types of equations frequently arise in real-world problems across fields such as physics, biology, and engineering, where processes occur on multiple time scales. Effectively solving stiff ODEs is critical for accurate and efficient modeling of these complex systems. Furthermore, neural ODEs frequently become stiff during training, even when the training data comes from a non-stiff ODE model. This occurs because parameter exploration can introduce stiffness, and the highly expressive nature of neural networks often results in very non-linear differential equations. This additional complication can greatly slow down or even halt the training process, necessitating that all neural ODE methods be robust to stiffness, regardless of whether the underlying model itself is inherently stiff.\nSeveral papers have claimed to address the problem of stiffness in neural ODEs, but most do so only partially by employing techniques such as equation scaling or regularization. These approaches aim to reframe the problem, making it more tractable, rather than directly solving the stiffness issue. For example, scaling/reformulating the equations41\u201349 can reduce the stiffness by modifying the system's dynamics, while regularization techniques50\u201362 can limit the model's flexibility, indirectly reducing the likelihood of stiffness. Although these methods offer some relief, they do not fundamentally solve the problem of stiffness in neural ODEs. Truly overcoming this bottleneck requires the development of numerical methods specifically designed for neural ODEs that can handle stiffness without requiring modifications to the original equations. This approach targets the core challenge directly, enabling neural ODEs to be more robust, efficient, and applicable to a broader range of real-world problems.\nThe Discretize-Optimize (Disc-Opt) and Optimize-Discretize (Opt-Disc) methods are two key strategies for optimizing neural ODEs. In Disc-Opt, the ODE is first discretized, and optimization is then applied to the discretized problem, which is straightforward to implement with automatic differentiation tools63. Conversely, Opt-Disc defines a continuous optimization problem and computes gradients in the continuous domain before discretizing, requiring the numerical solution of the adjoint equation64. Ref. 53 shows that Disc-Opt often outperforms Opt-Disc, achieving similar or better validation losses at significantly lower computational costs, with an average 20x speedup. This is partly due to the more reliable gradients in Disc-Opt, unaffected by solver accuracy, allowing flexibility based on data noise levels65. Given these advantages, we have focused on Disc-Opt in our research to develop new differential ODE solvers capable of handling stiffness efficiently.\nIn this paper, we focus on training stiff neural ODEs using single-step implicit methods. Given the challenges associated with stiffness in neural ODEs, our aim is to develop robust numerical methods that can handle the stability issues arising during training without modifying the underlying equations. Single-step implicit methods, which compute the next solution value based only on the current state, are a promising starting point for this endeavor. They provide a relatively simple framework for understanding the dynamics of stiff neural ODEs and for implementing backpropagation through the predicted ODE solutions.\nWe have initially focused on single-step methods due to their simplicity and ease of implementation, leaving the exploration of more complex multistep methods for future research. Multistep methods, while potentially more efficient in some scenarios, rely on accurate information from multiple previous time points. This dependency complicates their use in neural ODE training, where data can be noisy or incomplete and additional computations are required to initiate the method. By starting with single-step methods, we lay the groundwork for creating a robust and differentiable ODE solver for stiff neural ODEs. This foundational work is crucial for extending these solvers to a wide range of applications, including partial differential equations (PDEs) with stiffness due to diffusion terms, mesh-based simulations with methods like MeshGraphNets66, physics-informed neural networks (PINNs), and other emerging frameworks that require differentiable differential equation solvers."}, {"title": "II. METHODS", "content": ""}, {"title": "A. Neural ODEs", "content": "Neural Ordinary Differential Equations10 (neural ODEs) are a type of neural network designed to approximate time-series data, y(t), by modeling it as an ODE system. In many scientific fields, the ODE system we aim to approximate takes the following form:\n$\\frac{dy(t)}{dt} = f (t,y(t), \\theta),$\nwhere t represents time, y(t) is a vector of state variables, $\\theta$ denotes the parameter vector, and f is the function defining the ODE model. Determining the exact system of equations for f is often a challenging and labor-intensive process. Leveraging the universal approximation theorem67, we use a neural network (NN) to approximate the function f:\n$\\frac{dy(t)}{dt} = f \\approx NN (t,y(t), \\theta).$\nNeural ODEs can be handled similarly to traditional ODEs. Predictions for time-series data are generated by integrating the neural ODE starting from an initial condition, utilizing a discretization scheme68\u201370, just as is done for standard ODEs."}, {"title": "Learning Unknown Components in an ODE Model with Neural ODES", "content": "When the underlying equations of a system are entirely unknown, neural ODEs have the capability to learn the complete model:\n$\\frac{dy(t)}{dt} = NN (t,y(t),\\theta).$\nHowever, there are cases where some parts of the model, denoted as fknown, are known, but not all mechanisms and terms describing the full model are understood. Here, the neural ODE can learn the unknown terms:"}, {"title": "Polynomial Neural ODEs", "content": "Systems in various domains are often modeled by differential equations where the right-hand side functions f are polynomials. Examples include chemical kinetics71, cell signaling networks72 and gene regulatory networks73 in systems biology, as well as population dynamics in epidemiology74 and ecology75. Polynomial neural ODEs are particularly suitable for this category of inverse problems, where it is known in advance that the system is governed by polynomial expressions.\nPolynomial neural networks33,76 are a type of neural network architecture where the output is a polynomial function of the input layer. These networks fall under the broader category of symbolic neural network architectures. There are multiple variants of polynomial neural networks. For a more detailed discussion on these architectures, interested readers can refer to the work of Grigorios G. Chrysos. In our study, we found Ref.33's \u03c0-net V1 to be the most effective, as illustrated in Fig 1. This architecture relies on Hadamard products77 of linear layers without activation functions, denoted Li:\n$L_i(x) = x *w_i + b_i$\nto construct higher-order polynomials. The architecture needs to be defined beforehand based on the desired degree of the polynomial. There are no hyperparameters to tune within this architecture. A \u03c0-net can generate any n-degree polynomial for the given state variables. The hidden layers can be larger or smaller than the input layer, provided that the dimensions are compatible when performing the Hadamard product operation.\nPolynomial neural Ordinary Differential Equations integrate polynomial neural networks into the neural ODE framework10. Because the output of a polynomial neural ODE is a direct mapping of the input via tensor and Hadamard products without involving nonlinear activation functions, symbolic mathematics can be applied to derive a symbolic expression of the network. In contrast, conventional neural networks and neural ODEs, which involve nonlinear activation functions, do not allow for direct recovery of symbolic equations.\nWhen it comes to learning stiff ODEs and evaluating numerical methods, polynomial neural ODEs are advantageous neural networks for several key reasons. Unlike traditional neural networks, polynomial neural networks bypass the need for data normalization or standardization, making them easier to apply across diverse modeling paradigms. Additionally, the ability of polynomial neural ODEs to output quantities at various scales makes them particularly well-suited for stiff ODES, where such flexibility is essential. Moreover, stiff ODEs often feature constants that vary across multiple orders of magnitude, a challenge that polynomial neural ODEs are uniquely equipped to manage."}, {"title": "B. Stiff ODES", "content": "Stiff ODEs are a class of problems where explicit integration methods become highly inefficient due to stability constraints. Stiffness arises when there is a significant disparity in time scales within a system, often characterized by a large spread in the eigenvalues of the Jacobian matrix. To illustrate this, consider the stiff van der Pol oscillator with \u03bc = 1000 (Figure 2). Integrating this ODE using the explicit Runge-Kutta-Fehlberg method requires 422,442 data points to maintain stability, making the computation both slow and costly, taking several seconds to complete. In contrast, using an implicit scheme like the Radau IIA 5th order method reduces the number of data points to just 857, resulting in a much faster integration. While the number of data points provides some insight into computational cost, implicit methods involve iterative processes, so their true cost is not fully reflected in the number of time points. Therefore, the number of function evaluations is often a more accurate measure. In this example, the explicit Runge-Kutta-Fehlberg method required 2,956,574 function evaluations, while the Radau IIA 5th order method needed only 7,123, resulting in a significantly faster integration.\nThis example highlights the essence of stiffness in differential equations. For stiff problems, explicit methods demand extremely small time steps to ensure numerical stability, resulting in a substantial increase in computational cost. This is particularly problematic for neural ODEs, where thousands of integrations are needed during the training phase of a neural network. While explicit methods are easier to implement and have lower per-step costs, their inefficiency in handling stiffness makes them prohibitively slow for these applications. As a result, developing more efficient implicit solvers or novel approaches for neural ODEs is crucial to overcoming these limitations."}, {"title": "C. Discretize-Optimize vs. Optimize-Discretize", "content": "The Discretize-Optimize (Disc-Opt) and Optimize-Discretize (Opt-Disc) approaches are two fundamental paradigms for optimizing neural ODEs. We visually illustrate the difference between the two approaches in Fig. 3 In the Disc-Opt approach, the ODE is first discretized, and then the optimization is performed directly on the discretized problem. This method is relatively straightforward to implement, particularly with the help of automatic differentiation tools that compute gradients efficiently63. On the other hand, the Opt-Disc approach involves defining a continuous optimization problem and deriving the gradients in the continuous domain before discretizing the ODE. This requires solving the adjoint equation numerically, a process that is often more complex and computationally demanding64\nBoth Disc-Opt and Opt-Disc approaches face challenges, particularly in the computational costs associated with solving the ODE during forward propagation and calculating gradients via backpropagation in gradient-based optimization. Solving the forward propagation accurately demands significant memory and floating-point operations, potentially making training prohibitively expensive. While a lower-accuracy solver could theoretically speed up computations, this poses a problem for the Opt-Disc approach, where inaccurate forward propagation and adjoint solutions can degrade the quality of the gradients65. In contrast, Disc-Opt does not suffer from this issue; the accuracy of the gradients is independent of the forward propagation's accuracy, offering a compelling advantage for improving training efficiency. The Opt-Disc approach, which relies on adjoint methods that recompute the neural ODE backward in time, is also vulnerable to numerical instabilities, especially when dealing with stiff ODEs. These instabilities can significantly impact training performance and reliability.\nRef. 53 highlights several key findings that favor the Disc-Opt approach. First, it demonstrates that Disc-Opt achieves similar or even superior validation loss values at reduced computational costs compared to Opt-Disc. Specifically, the Disc-Opt method offers an average speedup of 20x over the Opt-Disc approach. This performance difference is partly due to the potential inaccuracies in gradients when using Opt-Disc methods, as also discussed by Ref. 65. In Opt-Disc, gradients can become unreliable if the state and adjoint equations are not solved with sufficient precision. Meanwhile, in Disc-Opt, the gradients remain accurate regardless of the solver's accuracy, allowing the solver's precision to be adjusted based on the data's noise level, which is particularly useful in scientific applications. Given the substantial speed advantages of the Discretize-Optimize approach, we have prioritized this method over the adjoint method in our research. Our focus has been to understand the fundamentals of training stiff ODES using various integration schemes and to leverage this knowledge to develop a new differential ODE solver that builds on these basics."}, {"title": "D. Numerical Methods for Solving ODES", "content": "Single-step and multistep methods are two fundamental approaches for solving ordinary differential equations (ODEs). Single-step methods, like the Euler and Runge-Kutta schemes, compute the next solution value using only the information from the current step. This makes them relatively simple to implement and understand, as they do not depend on previous values of the solution. In contrast, multistep methods, such as Adams-Bashforth and Adams-Moulton methods, use information from several previous time steps to estimate the solution at the next point. While multistep methods can be more efficient in terms of computational cost per step for certain problems, they require careful management of past data and have more complex stability properties. While multistep methods can be advantageous in some contexts, they introduce additional challenges compared to single-step methods.\nSingle-step methods are often the easiest to handle and grasp, making them an ideal starting point for developing integration methods for neural ODEs. Because they rely only on the current state, they provide a straightforward path for understanding the dynamics of neural ODEs and the process of backpropagation through the predicted ODE solution. Multistep methods, however, present challenges when applied to neural ODEs, particularly in the training phase. Since multistep methods depend on several past time points, initiating these methods requires multiple accurate previous points. Given that training data for neural networks may be noisy or incomplete, these required past points cannot always be directly used, and must instead be computed anew, adding complexity and slowing down the computation. Additionally, the need for multiple prior points complicates backpropagation, as gradients must be traced through the methods used to create previous values to initiate the method. These factors make single-step methods seem like the more practical and intuitive choice for initial research on neural ODEs, with multistep methods explored in future work once more foundational understanding is developed."}, {"title": "Single-Step Explicit Methods", "content": "Explicit methods are widely used for solving ordinary differential equations (ODEs) because of their straightforward implementation and lower computational cost per step. In these methods, the solution at the next time step is directly calculated from the current step's values, avoiding the need to solve nonlinear equations. This directness makes explicit methods computationally efficient for many problems, especially those that are non-stiff. Additionally, explicit methods are particularly cheap to use with neural ODEs because they can be backpropagated through very easily, allowing for efficient training of neural networks. However, when stiffness is involved, these methods often fall short.\nThe most basic explicit method is Euler's method, which approximates the solution using a simple linear formula:\n$y_{n+1} = y_n + hf(t_n,y_n).$\nHere, yn+1 is the solution at the next time step, h is the step size, and f (tn, yn) is the derivative of y at the current time step tn. Euler's method is easy to implement but comes with limitations: its first-order accuracy often necessitates extremely small step sizes to maintain accuracy, making it impractical for anything but the simplest, non-stiff problems.\nThe forward Euler method is most often used as a teaching tool or for problems where precision is not a critical factor. Although it is only first-order accurate, this method is commonly employed in many neural ODE applications during preliminary research when developing new methods, or when it is the only affordable option for specific use cases. For instance, MeshGraphNet66, a graph neural network-based neural ODE capable of learning mesh-based multiphysics simulations like those in COMSOL, relies on the forward Euler method due to the high cost of backpropagating through its numerous graph neural network layers.\nA more robust family of explicit methods are the Runge-Kutta (RK) methods, particularly the classical 4th-order Runge-Kutta method (RK4). Unlike Euler's method, RK4 calculates the next value by considering a weighted average of several intermediate points within the current interval. The RK4 formula can be written as:\n$Y_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4),$\nwhere\n$k_1 = f(t_n,y_n),$\n$k_2= f(t_n+\\frac{h}{2}, y_n+\\frac{h}{2}k_1),$\n$k_3 = f(t_n+\\frac{h}{2}, y_n+\\frac{h}{2}k_2),$\n$k_4 = f(t_n+h, y_n+hk_3).$\nThis approach offers a good balance between computational cost and accuracy, making it popular in many applications requiring moderate precision. RK4, being a fourth-order method, is considerably more accurate than Euler's method for a given step size and is better suited for a range of non-stiff problems."}, {"title": "Single-Step Implicit Methods", "content": "Implicit methods are a powerful solution to the challenges posed by stiff ODEs. Unlike explicit methods, which compute the next step directly from current values, implicit methods require solving a multivariate system of nonlinear equations at each step. This process is more computationally intensive, but it provides significantly greater stability, enabling the use of larger time steps without losing accuracy. The trade-off between computational cost and stability makes implicit methods particularly effective for stiff systems, but they tend to be slower for non-stiff ODEs where stability constraints are not a limiting factor.\nThe simplest implicit method is the backward Euler method:\n$y_{n+1} = y_n + hf(t_{n+1},y_{n+1}),$\nwhich computes the solution, yn+1, with a linear approximation of the derivative through the solution point. Since the value of yn+1 is unknown, a nonlinear system of equations must be solved with a root-finding method such as Newton's method, which substantially increases the computational cost. In some situations, the root-finding process may fail to converge, requiring an adjustment of the step size and resulting in further computational overhead.\nThe backward Euler method is A-stable, meaning that numerical errors do not grow uncontrollably. While it is only first-order accurate, the method's unconditional stability leads to stiff decay, where rapidly varying components decay quickly to zero, stabilizing the solution. An important feature of the backward Euler method is its ability to dampen oscillations effectively. When applied to stiff problems with oscillatory solutions, the backward Euler method tends to smooth out oscillations and drive the solution towards equilibrium. This strong damping effect is particularly beneficial in cases where spurious oscillations, which are common in neural ODEs due to the flexible nature of their approximating functions, can impact the accuracy or stability of the numerical solution, potentially leveraging stiff decay to mitigate these oscillations. However, this comes at the cost of potentially overdamping the solution, making it appear too smooth or slow to respond to changes, which can be problematic for models that inherently exhibit oscillatory behavior.\nAn alternative implicit method is the trapezoid method, which improves upon the backward Euler method by using the average of the derivatives at the current and next points to compute the solution:\n$y_{n+1} = y_n + \\frac{h}{2} (f(t_n, y_n) + f (t_{n+1},y_{n+1})).$\nThe trapezoidal method is second-order accurate and A-stable. However, it is not L-stable, which means it does not guarantee that all transient modes will decay rapidly to zero. In practice, this can lead to less effective damping of very stiff components. Additionally, the trapezoid method can introduce oscillations into the solution. Since neural ODEs can become quite stiff during training, the trapezoidal method's potential to introduce oscillations may further intensify this stiffness. Because it uses an average of the derivative at two points, it does not dampen oscillations as aggressively as the backward Euler method. As a result, for problems where damping is crucial, the trapezoid method may allow small oscillations to persist, potentially leading to less stable behavior over long simulations.\nThe Radau IIA methods78\u201380, belonging to the broader class of implicit Runge-Kutta methods, are among the most suitable high-order single-step implicit methods available. These methods are highly regarded for their exceptional stability, making them a preferred choice for solving highly stiff problems. Radau IIA methods are both A-stable and L-stable, which allows them to handle large step sizes while ensuring that transient solutions decay quickly. Implicit Runge-Kutta methods have the following form:\n$Y_i=y_n+h\\sum_{j=1}^{s}a_{ij}f(t_n+c_jh,Y_j),   i=1,...,s,$\n$y_{n+1} =y_n+h\\sum_{j=1}^{s}b_jf(t_n+c_jh,Y_j)$\nwhere Yj are the stage values, aij are the coefficients from the Butcher tableau, and cj are the nodes. The Butcher tableau for Radau3 and Radau5 are shown in Fig.5 (see Ref. 80). This formulation requires solving a nonlinear system involving all stage values Yi, which adds to the computational cost. However, the benefit of L-stability ensures that the stiff components of the solution decay without oscillation, making it ideal for highly stiff ODEs."}, {"title": "E. Implicit Function Theorem", "content": "Backpropagation is the process of tracing a computational graph to compute gradients of a loss function with respect to model parameters, which is fundamental in training neural networks. In the context of training neural ODEs, explicit schemes such as the forward Euler or RK4 methods compute the solution at the next time step directly from the current values. This direct calculation creates a straightforward computational graph, making it easy to trace and backpropagate through each step. However, implicit schemes, like the backward Euler or Radau IIA methods, involve solving a nonlinear equation at each step to determine the next solution value. This nonlinear solver introduces a nested computational loop, which is both computationally expensive and can be numerically unstable when unrolling the entire loop during backpropagation. The result is a cumbersome and potentially error-prone process that significantly increases the computational burden.\nThe implicit function theorem provides an elegant solution to the challenge of backpropagating through an implicit scheme without needing to fully unroll the nonlinear iteration process. Once the root of the nonlinear equation is found using an implicit method, the implicit function theorem allows us to compute the gradient of the solution with respect to the inputs directly at this root.\nWe can reformulate our implicit scheme as an equation where we aim to find a root:"}, {"title": "III. RESULTS", "content": "We begin by evaluating our methodology on the stiff univariate linear test equation (Example 1), a common benchmark in stability analysis across various ODE discretization schemes, making it a suitable first baseline. Next, we apply the methodology to two custom-designed stiff nonlinear models: a 2-dimensional system (Example 2) and a 3-dimensional system (Example 3). Due to the scarcity of well-established test models for stiff neural ODEs, we developed these toy models to explore the performance of our approach in addressing the challenges posed by stiffness in neural ODEs. Example 4, is the \u201cHigh Irradiance RESponse\u201d (HIRES) model, a widely used benchmark for evaluating ODE solver performance on stiff ordinary differential equations. For each model, we generated the corresponding training data. We then assessed the ability of the various single-step implicit schemes to recover the underlying ODE model under different training data conditions."}, {"title": "A. Example 1: Stiff Linear Model", "content": "In our first test, we consider the stiff linear equation:\n$\\frac{dy}{dt} = -10000y,  y(0) = 1000, t\\in [0,0.01].$\nThis equation, with solution plotted in Figure 6, serves as a standard example for evaluating the stability of numerical integration schemes in stiff systems. The high degree of stiffness, driven by the large negative coefficient, causes an initial rapid transient at the beginning of the time interval, followed by a smooth, slow decay as the system approaches equilibrium. This smooth behavior, which dominates most of the solution, represents the stiff region of the ODE. The one-dimensional nature of the model, along with its tunable stiffness parameter, provides a controlled setting to systematically evaluate integration schemes under varying stiffness conditions. By adjusting the stiffness, we can fine-tune the problem's difficulty and assess the performance, stability, and accuracy of implicit solvers designed for stiff problems.\nGiven the stiffness of the problem, we generate the training data by integrating the initial value problem (IVP) using the Radau solver in SciPy81. Figure 6 shows the training data for the case with n = 200 data points equally spaced in time. As described in the methods section, following the discretize-then-optimize approach, we partition the data into n - 1 training samples, each representing an IVP between two consecutive known data points. During each epoch, we solve these n 1 IVPs simultaneously using our custom JAX82,83 implementation of the implicit schemes detailed in the methods section, enabling faster backpropagation through the ODE discretization process84.\nUsing the training data, we trained a \u03c0-net V1 polynomial neural network to recover the underlying ODE equation. Table 1 shows the recovered equations for varying numbers of training data points equally spaced in time, for the four implicit schemes: backward Euler, trapezoid method, Radau3, and Radau5. To demonstrate the impact of the number of training data points on the ability to identify the underlying ODE, we plotted the parameter fractional relative error against the number of data points (see Figure 7). As expected, the recovered equations show a clear improvement with the use of higher-order methods, emphasizing the importance of higher-order methods for accurately integrating ODEs within the neural ODE framework."}, {"title": "B. Example 2: 2D Nonlinear Model", "content": "In our second example, we introduce the stiff nonlinear system:\n$\\frac{dy_1}{dt} = -10000y_1 + 100y_2,$\n$\\frac{dy_2}{dt} = 1 - y_1 - y_2^2,$\n$y_1 (0) = 20,  y_2(0) = 20,  t \\in [0, 1].$\nThis system, plotted in Figure 8, was specifically designed to exhibit stiffness while containing only linear and quadratic terms. The presence of both linear decay and nonlinear coupling between the variables makes this model an ideal toy problem for testing stiff ODE system identification methods. By controlling the stiffness through the large coefficient on y1, we created a challenging yet tractable example that allows for detailed evaluation of numerical schemes in handling stiff nonlinear dynamics. This formulation provides a simple but effective framework for testing implicit solvers and their ability to recover underlying ODE models.\nThe training data for Example 2 was generated, and the neural ODE was trained using the same approach as described in Example 1, following the discretize-then-optimize framework. Table 2 provides a detailed comparison of the recovered equations across a range of data points and for the different single-step implicit schemes employed. This allows us to assess the accuracy and stability of each method as the number of training data points increases."}, {"title": "C. Example 3: 3D Nonlinear Model", "content": "In our third example, we consider the following stiff nonlinear system of ODEs:\n$\\frac{dy_1}{dt} = -500y_1 +3.8y_2 + 1.35y_3,$\n$\\frac{dy_2}{dt} = 0.82y_1 - 24y_2 + 7.5y_3,$\n$\\frac{dy_3}{dt} = -0.5y_1 + 1.85y_2 - 6.5y_3,$\n$y_1(0) = 15,  y_2(0) = 7,  y_3(0) = 10, t \\in [0,5].$\nThis system, plotted in Figure 9, was constructed to be a more challenging stiff problem due to its three-dimensional nature while still consisting of only linear and quadratic terms. The increased dimensionality introduces more complex interactions between the variables, making this a more difficult test case for stiff ODE system identification methods. By incorporating both linear decay and nonlinear terms, this model provides a robust framework for evaluating the performance of integration schemes in handling stiff, higher-dimensional nonlinear dynamics.\nThe training data for Example 3 was generated, and the neural ODE was trained following the same procedure described for Example 1, utilizing the discretize-then-optimize framework. Tables 3, 4, 5, and 6 provide a detailed comparison of the recovered equations for varying numbers of data points (1467, 369, 94, and 48, respectively) and across the four chosen single-step implicit schemes.\nFigure 10 plots the fractional relative error of the parameter against the number of data points, n. Similar trends are observed here as in Figure 7, indicating that the discretization scheme's order scales comparably for both the linear test problem and the higher-dimensional nonlinear ODE. This further highlights the significant improvement gained by employing higher-order methods, reinforcing the importance of such methods for accurately integrating ODEs in the neural ODE framework."}, {"title": "D. Example 4: HIRES Model", "content": "In our fourth example, we consider the \"High Irradiance RESponse\" (HIRES) model:\n$\\frac{dy_1}{dt} = -1.71y_1 + 0.43y_2 + 8.32y_3 + 0.0007,$\n$\\frac{dy_2}{dt} = 1.71y_1 - 8.75y_2,$\n$\\frac{dy_3}{dt} = -10.03y_3 + 0.43y_4 + 0.035y_5,$\n$\\frac{dy_4}{dt} = 8.32y_2 + 1.71y_3 - 1.12y_4,$\n$\\frac{dy_5}{dt} = -1.745y_5 + 0.43y_6 + 0.43y_7,$\n$\\frac{dy_6}{dt} = -280y_6y_8 + 0.69y_4 + 1.71y_5 - 0.43y_6 + 0.69y_7,$\n$\\frac{dy_7}{dt} = 280y_6y_8 - 1.81y_7,$\n$\\frac{dy_8}{dt} = -280y_6y_8 + 1.81y_7,$\n$t \\in [0,321.8122].$\nThis model is plotted in Figure 11. In this system of stiff ordinary differential equations (ODEs), the eight variables represent concentrations of chemical species in \"high irradiance responses\" of photomorphogenesis on the basis of phytochrome85. This model is commonly used in numerical analysis to test ODE solvers' ability to handle stiff and high-dimensional systems.\nWe generated our training data to emulate experimental results that a scientist might collect in a laboratory setting. First, we analyzed the model's equations to determine plausible initial values for the eight chemical species. Using a Latin hypercube sampling86-89 approach, we systematically explored the space of possible initial conditions, generating 20 unique sets of initial values. Each set of initial conditions represents the starting concentrations for a different simulated experiment. The set of initial condtions we used to generate the training data can be found in table 7. The recovered equations for the training data using the Radau5 solver are presented in Tables 8 and 9. Due to the length of the equations, they have been split across two pages. For brevity, equations from other implicit solvers are not included. The results show that Radau5 performs well in reconstructing the model's equation from the provided data."}, {"title": "IV. CONCLUSION", "content": "This work introduces an approach to directly training stiff neural ordinary differential equations (ODEs). Earlier approaches for handling stiff neural ODEs relied on workarounds like equation scaling and regularization,, which can help ODE solvers cope with stiffness temporarily but fail to offer a direct solution for training stiff neural ODEs. In contrast, our approach initiates the development of numerical methods specifically designed to manage stiffness without altering the original equations. This foundational step is key to creating robust and differentiable ODE solvers that can be applied across a wide range of fields, including partial differential equations (PDEs), physics-informed neural networks (PINNs), and mesh-based simulations such as MeshGraph-Nets.\nIn this paper, we focused on utilizing single-step implict schemes to train stiff neural ODEs, which determine the next solution state based solely on the current one. These methods provide a simple yet effective approach to resolving the dynamics of stiff neural ODEs, while enabling efficient backpropagation through the ODE solutions via the implicit function theorem. Although we concentrated on single-step methods due to their simplicity and ease of implementation, more advanced multistep methods may offer computational advantages. However, multistep approaches depend on accurate information from several previous time points, complicating their use in neural ODE training. Future work will explore these multistep techniques and assess their potential benefits for managing stiffness.\nOur experiments demonstrated successful training and accurate recovery of stiff ODE dynamics using several single-step implicit methods, including backward Euler, the trapezoid method, Radau3, and Radau5. While these methods effectively handled stiffness, they also introduced computational challenges. Implicit methods require solving a nonlinear system at each time step, which"}]}