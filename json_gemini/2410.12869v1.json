{"title": "LANGUAGE MODEL PREFERENCE EVALUATION WITH MULTIPLE WEAK EVALUATORS", "authors": ["Zhengyu Hu", "Jieyu Zhang", "Zhihan Xiong", "Alexander Ratner", "Hui Xiong", "Ranjay Krishna"], "abstract": "Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs' quality regarding preference remains a critical challenge. Existing works usually leverage a powerful LLM (e.g., GPT4) as the judge for comparing LLMs' output pairwisely, yet such model-based evaluator is vulnerable to conflicting preference, i.e., output A is better than B, B than C, but C than A, causing contradictory evaluation results. To improve model-based preference evaluation, we introduce GED (Preference Graph Ensemble and Denoise), a novel approach that leverages multiple model-based evaluators to construct preference graphs, and then ensemble and denoise these graphs for better, non-contradictory evaluation results. In particular, our method consists of two primary stages: aggregating evaluations into a unified graph and applying a denoising process to eliminate cyclic inconsistencies, ensuring a directed acyclic graph (DAG) structure. We provide theoretical guarantees for our framework, demonstrating its efficacy in recovering the ground truth preference structure. Extensive experiments across ten benchmark datasets show that GED outperforms baseline methods in model ranking, response selection, and model alignment tasks. Notably, GED combines weaker evaluators like Llama3-8B, Mistral-7B, and Qwen2-7B to surpass the performance of stronger evaluators like Qwen2-72B, highlighting its ability to enhance evaluation reliability and improve model performance.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have rapidly transformed various fields within artificial intelligence, particularly natural language processing (NLP) and decision-making systems (Wu et al., 2023; Li et al., 2023a). Despite the remarkable success of LLMs, the need for effective evaluation methods becomes paramount (Liu et al., 2023; Desmond et al., 2024; Siska et al., 2024). Preference evaluation, as one of the most important assessment methods, plays an indispensable role in evaluating and optimizing model performance (Rafailov et al., 2024; Yuan et al., 2024; Dubois et al., 2024b). Existing works usually leverage a powerful LLM (e.g., GPT4 (Achiam et al., 2023)) as the judge for comparing LLMs' output pairwisely (Li et al., 2023b; Chen et al., 2023; Li et al., 2023b; Wang et al., 2022).\nHowever, while such model-based pairwise preference evaluations offer a flexible approach, they can lead to contradictory evaluations in the assessment process (Zhang et al., 2024b). For example, an LLM might evaluate three responses and conclude that Response A is better than Response B (A > B), Response B is better than Response C (B > C), yet paradoxically also rank Response C as better than Response A (C > A). These cyclic patterns introduce inconsistencies that undermine the reliability of the evaluation results. We model this conflicting preference via the preference graph. Specifically, a preference graph is constructed with each response as a node and directed edges indicating pairwise preferences an edge from node A to node B shows that the evaluator preferred response A over B. The noise illustrated by cycles (A > B > C > A) manifests as loops in the preference graph. The whole process we show in the upper part of Figure 1. Ideally, a preference graph should be structured as a directed acyclic graph (DAG) to maintain consistency. As shown in the bottom half of Figure 1, even advanced LLMs like GPT-4-0 exhibit significant noise in preference evaluation, highlighting their limitations as weak evaluators."}, {"title": "METHODOLOGIES", "content": "In this section, we begin by defining a preference graph, which serves as the foundation for rep-resenting pairwise preferences among candidates (Section 2.1). Building on this foundation, we introduce GED structured into three key stages (Section 2.2): (1) graph ensemble, where we aggregate individual preference graphs into a unified structure, (2) graph denoising, which removes cycles and inconsistencies to ensure the preference graph is acyclic, and (3) graph-to-ranking, where we extract a reliable ranking of candidates from the denoised graph. Below, we provide detailed descriptions of each step."}, {"title": "PREFERENCE GRAPH", "content": "A preference graph is defined as a directed graph Gp = (V, A, w), where V = {U1, U2, ..., Un} represents a set of n alternatives or candidates, A \u2286 V \u00d7 V is a set of directed arcs representing pairwise preferences between these alternatives, and w : A \u2192 R+ is a weight function that assigns a positive real value to each arc, indicating the strength of the preference.\nFor any pair of distinct vertices u, v \u2208 V, an arc (u, v) \u2208 A exists if there is a preference for u over v, with the weight w(u, v) reflecting the intensity of this preference. Formally, this can be represented as:\n$(u, v) \\in A \\text{ if and only if } w(u, v) > 0$ (1)\nThe weight function w(u, v) aggregates individual preferences or scores for the pair (u, v). If multiple preference sources exist, the weight can be expressed as:\n$w(u, v) = \\sum_{i=1}^{k}(s_i(u, v) - s_i(v, u))$ (2)\nwhere si (u, v) is the score or preference from the i-th source. The preference graph encapsulates the aggregate preferences among all pairs of alternatives, with the weight of each arc representing the cumulative preference strength derived from underlying data or models."}, {"title": "GED: PREFERENCE GRAPH ENSEMBLE AND DENOISE", "content": "As illustrated in Figure 2, our method, GED (Preference Graph Ensemble and Denoise), begins by performing graph ensemble to aggregate a set of preference graphs. It then applies graph denoising to ensure acyclicity, followed by graph-to-ranking to derive the final node ranking. The detailed steps are as follows:\nGraph ensemble. In the context of graph ensemble, given multiple weighted graphs G1 = (V, A1, W\u2081), G2 = (V, A2, W2),..., Gk = (V, Ak, wk) that share the same set of vertices V but may differ in their arc sets Ai, the goal is to combine these graphs into a single ensemble graph GE = (V, AE, WE). The ensemble graph GE is formed by first defining the arc set AE as the union of the arc sets of the individual graphs and the weight function WE: AE \u2192 R+ for the ensemble graph is then determined by summing the weights of the corresponding arcs in each graph.\nGraph denoising. Graph denoising involves transforming the original graph G = (V, A, w) into a DAG. This transformation is achieved by identifying and removing a set of arcs known as the Feedback Arc Set (FAS) (Gabow, 1995), which is a set of arcs whose removal makes the graph acyclic. The goal is to find a minimum FAS, denoted as R*(G), which is a set of arcs with the smallest total weights that needs to be removed to eliminate all cycles in G.\nTo find this minimum FAS, we can order the vertices of G in a specific sequence s = {V1, V2, ..., Un}. This vertex sequence induces a FAS R(s), consisting of all arcs that point against the direction of the sequence, i.e., arcs vj \u2192 vi where j > i. The graph denoising problem is thus reframed as finding an optimal vertex sequence s* that induces the minimal FAS, such that R(s*) = R*(G). This optimal sequence s* ensures that the total weights of arcs eliminated to achieve a DAG is minimized.\nFinding a minimum FAS in general is known to be an NP-complete problem, whose computational complexity can be exponential (Karp, 2010; Bodlaender et al., 2012). Therefore, in our experiment, we apply the well-established approximation algorithm proposed in Eades et al. (1993). Details can be found in Appendix D.\nGraph to ranking. Given a DAG graph G = (V, A, w), our goal is to derive a ranking based on the structure of G. For each vertex v \u2208 V, we compute the descendant count desc(v), defined as the number of vertices that are reachable from v through directed arcs:\n$desc(v) = |{u \\in V : v \\leftrightarrow u}|,$ (3)\nwhere v \u2194 u denotes that there is a directed path from v to u. Vertices are then ranked based on their descendant counts, with higher descendant counts indicating a higher position in the ranking. Formally, the ranking R is represented as a sequence of subsets:\nR = {V1, V2, . . ., Vn}, (4)\nwhere each vi represents a set of vertices with the i-th highest descendant count. The final ranking is then:\nV1 > V2 > \u27a4 Vn. (5)\nThis structured approach ensures that the ranking reflects not only the individual preferences captured in the graph but also the relative strength of these preferences as represented through their descendant connections."}, {"title": "APPLICATIONS", "content": "We apply GED to three tasks: Response Selection, selecting the best response from LLM-generated candidates; Model Ranking, ranking models based on task performance; and Model Alignment, identifying the best instruction-response pairs for training. The steps are as follows:\nResponse selection. In the response selection task, a model M generates n candidate answers {ans1,...,ansn} for each question q \u2208 Q, with the objective of identifying the optimal answer ans for each query. To achieve this, we employ multiple evaluators A = {1, ..., ak}, who provide pairwise preferences among the candidate answers. For each question q, we construct a set of preference graphs {Ga : a \u2208 A}, where each graph Ga = (Vq, Aa, Wa) encapsulates the preferences of evaluator a. The vertex set Vq = {1, ..., Un} corresponds to the candidate answers, while the directed arcs Aa indicate the pairwise preferences among these responses. Each arc is weighted by wa, reflecting the strength of the preference indicated by the evaluator. The construction of preference graphs involves evaluating each pair of candidate answers ansi and ansj. Evaluators assess the quality of these answers, assigning a preference that is denoted by a directed arc (vi, vj) in Aa, with a corresponding weight wa(vi, vj) based on the strength of preference. This process is detailed in Appendix B. After collecting the preference graphs {Ga : a \u2208 A} for a question q, we apply GED to aggregate these graphs. The aggregation begins with merging all preference graphs into a unified graph Gq = (Vq, Aq, wq), which is subsequently processed to remove cycles, resulting in a DAG. From this DAG, we derive the ranking Rq = {V1 > V2 > . . . > un} of the candidate answers, where the highest-ranked answer is selected as ans. This process is repeated for each question q \u2208 Q, yielding a final set of selected answers ans* = {ans\u2081,...,ans}. This set reflects a consensus from multiple evaluators in A, ensuring the chosen answers represent the highest quality responses based on rigorous preference evaluation.\nModel Ranking. In model ranking task, the goal is to rank a set of models M = {M1,..., Mn} based on their responses to a series of questions Q = {q1, . . ., qt }. A group of evaluators, denoted as A = {a1, a2,..., ak }, assesses the model outputs by providing preferences for pairs of responses for each question. For each question q \u2208 Q, the evaluators generate preference graphs {Ga : a \u2208 A}, where each graph Ga = (Vq, Aa, wa) encapsulates the preferences of evaluator a over the models. The vertex set Vq = {1, ..., Un} corresponds to the models, while the directed arcs Aa indicate pairwise preferences, with weights wa : Aa \u2192 R+ reflecting the strength of these preferences. The preference graph for a given question q is constructed by evaluating each pair of models Mi and Mj, represented by nodes vi and vj. For their respective answers ansi = Mi(q) and ans; = Mj(q), evaluators provide a preference indicating which answer is favored. This preference is represented by a directed arc (vi, vj) in Aa, assigned a weight wa(vi, vj) based on preference strength. The detailed procedure is outlined in Appendix B. Once the preference graphs {Ga : a \u2208 A} are collected for a question q, we employ GED to aggregate these graphs. The method begins by merging all preference graphs into a single graph Gq = (Vq, Aq, wq), which is then transformed into a DAG by removing cycles. The final ranking Rq = {V1 > U2 > \u27a4 un} is derived from this DAG. This process is repeated for each question q \u2208 Q, yielding a set of rankings {Rq : q \u2208 Q}. To compute the overall ranking R* of the models across all questions, we conduct a ranking ensemble on the set {Rq: q \u2208 Q}, as detailed in Appendix E. This approach culminates in a final ranking that reflects the models' performance as assessed by multiple evaluators in A.\nModel Alignment. In the model alignment task, we have multiple data pairs of the form (X,Y1), (x, Y2), ..., (x, yn) for each instruction x. The objective is to identify the best response y* corresponding to each instruction x. We utilize multiple evaluators A = {1, ..., ak } to provide pairwise preferences among the candidate responses. For each instruction x, we construct a set of preference graphs {Ga : a \u2208 A}, where each graph Ga = (Vx, Aa, wa) represents the preferences of evaluator a. The vertices Vr correspond to the candidate responses {Y1,...,Yn}, and the directed arcs in Aa indicate preferences, weighted by wa. After constructing the preference graphs, we apply GED to aggregate them into a single graph Gx. This graph undergoes a denoising process to remove cycles, allowing us to derive a ranking Rx of the responses. The highest-ranked response in Rx is selected as y* for that instruction x. This process is repeated for all t instructions, resulting in the final training set {(x1,y1), ..., (xt, y)}, which reflects a consensus across the evaluators in A."}, {"title": "THEORETICAL ANALYSIS", "content": "In this section, we provide a theoretical foundation for our method, showing that by modeling preference graphs as random perturbations of a ground truth DAG, GED can reliably recover the true structure through graph ensemble and denoising with high probability, demonstrating its robustness in handling noisy evaluations.\nTheoretically, we treat each of our preference graph as a random perturbation of some ground truth DAG G = (V, A). Specifically, we consider a random graph generator G(G, 81, 82) with parameters 81, 82 \u2208 [0,1] such that Gi = (Vi, Ai) ~ G(G, 81, 82) satisfies V\u2081 = V. Furthermore, for each u, v \u2208 V with u \u2260 v,\n1) If (u \u2192 v) \u2208 A, then\nP((u + v) \u2208 A\u00bf) = 1 \u2212 d\u2081 and P((v \u2192 u) \u2208 A\u2081) = \u03b4\u2081;\n2) If (u \u2192 v), (v \u2192 u) \u2209 A, then\n$P((u \\rightarrow v), (v \\rightarrow u) \\notin A_i) = 1-\\delta_2, P((u \\rightarrow v) \\in A_i) = \\frac{\\delta_2}{2} \\text{ and } P((v \\rightarrow u) \\in A_i) = \\frac{\\delta_2}{2}.$\nThat is, each edge in E has probability 8\u2081 of being flipped and each pair of unconnected nodes has probability 82 of being connected with a random direction.\nNow, given that G1, ..., GN iid. G(G, 81, 82), we will show that to some extent our combination of graph ensemble and graph denoising can indeed provably recover the ground truth DAG G. For simplicity, all edges in G1, . . .,GN and G are considered equal weighted. Meanwhile, we use MAS() to denote the graph obtained by denoising, which stands for the maximum acyclic subgraph (MAS). Then, we have the following theorem.\nTheorem 1 Suppose G1,..., Gn iid. G(G, 81, 82) for some ground truth G = (V, A). Let G be the graph ensembled from G1,...,GN by operations defined in Section 2.2. Then, as long as \u03b4\u2081 = 0.5 \u2013 \u20ac for some e > 0, we have\n$P(G \\subset MAS(\\hat{G})) \\geq 1-2|A| \\exp \\bigg(-\\frac{Ne^2}{2}\\bigg) - 2U \\exp\\bigg(-\\frac{Ne^2}{6U^2\\delta_2 + 2U \\epsilon}\\bigg)$\nwhere G C MAS(G) represents that G is a subgraph of MAS(G) and U = |V|(|V|-1) \u2013 |A| is the number of pairs of unconnected nodes in G.\nThe full proof is given in Appendix C. From the theorem, we can see that the probability of failure decreases exponentially as the number of samples N increases. Meanwhile, this guarantee only requires 81 < 0.5 and does not place restrictions on 82, which are very mild conditions."}, {"title": "EXPERIMENTS ON RESPONSE SELECTION", "content": "Table 1 presents the results of the response selection task across five benchmarks. GED consistently outperforms baseline methods, demonstrating the effectiveness of graph denoising and the aggregation of weaker evaluators."}, {"title": "Experiment Setup", "content": "In this section, we evaluate the performance of GED on five benchmarks: HumanEval (Chen et al., 2021), AlpacaEval (Li et al., 2023b), MATH (Hendrycks et al., 2021), GSM8k (Chen et al., 2021), and GAIA (Mialon et al., 2023). The Qwen2-72B (Yang et al., 2024a) model (M) generates ten candidate responses per question, and we assess the effectiveness of different methods in selecting the best response. For further implementation details, see Appendix A. We evaluate performance using three setups. First, in the single model setting, the baselines include ContraSolver(Zhang et al., 2024b), Self-consistency(Wang et al., 2022), and direct evaluation with models (Llama3-8B, Mistral-7B, Qwen2-7B and Qwen2-72B). Additionally, we include a baseline called ListPreference, where instead of pairwise comparisons, all candidate responses are input into Qwen2-72B for selecting the most appropriate response. Then, in the single evaluator setting, individual evaluators (Llama3-8B, Mistral-7B, Qwen2-7B, Qwen2-72B) select the best response from M's outputs, with and without applying GED's graph denoising. Finally, in the multiple weak evaluators setup, we combine three weaker evaluators (Llama3-8B, Qwen2-7B, Mistral-7B) to select responses from Qwen2-72B with GED. We present the results of GED and its variant (w/o denoising), which ensembles the preference graphs without the denoising step."}, {"title": "Main results", "content": "Table 1 presents the results of the response selection task across five benchmarks. GED consistently outperforms baseline methods, including both single model evaluations (single model) and direct response selection by individual models (single evaluator). This demonstrates the strength of aggregating weak evaluators with GED, particularly when coupled with graph denoising, which enhances response quality by filtering out noise and biases. This highlights the effectiveness of aggregating weak evaluators and applying graph denoising to improve response quality. Furthermore, by combining preference graphs from weaker models (Llama3-8B, Mistral-7B, Qwen2-7B), GED surpasses the performance of a much stronger evaluator (Qwen2-72B). This underscores the value of ensemble methods in mitigating the limitations of individual evaluators. Then, the denoising process proves to be crucial for improving consistency and overall response quality. The substantial performance gains observed when using GED with denoising, compared to both the single evaluator setup and the ensemble without denoising, highlight its importance in refining response selection. Additionally, we observed that the ListPreference baseline performed worse than Qwen2-72B as single evaluator, likely due to LLM limitations in handling long-text. Lastly, to further evaluate GED, we compared its performance with GPT-3.5, GPT-4-o-mini, and GPT-4-o. Due to computational and API cost constraints, we limited the evaluation to 100 data points for each task. As shown in Figure 3, GED consistently outperformed GPT-3.5 across all tasks and surpassed GPT-4-o-mini on challenging benchmarks like HumanEval and GSM8k. These results highlight the superiority of GED, particularly in leveraging multi-weak evaluators and graph denoising to outperform individual state-of-the-art models."}, {"title": "Alabation study", "content": "We evaluate the impact of removing the ensembling step in GED, referred to as the (w/o ensemble) variant. In this case, individual evaluators' preference graphs are denoised and converted to rankings, which are then aggregated using methods such as Weight Score, Kemeny, Weighted Kemeny, Pairwise Majority, and Weighted Pairwise Majority (detailed in Appendix E). For simplicity of presentation, we use Weight Score to represent GED (w/o ensemble) (Weight Score). As shown in Figure 4, all (w/o ensemble) methods consistently underperform compared to GED. This performance gap arises because converting graphs to ranks before aggregation leads to information loss. In contrast, GED ensembles the graphs directly, preserving more detailed preference information and resulting in better final rankings."}, {"title": "EXPERIMENTS ON MODEL RANKING", "content": "In this section, we evaluate the effectiveness of GED in the model ranking task within a human preference setting, using the AlpacaEval benchmark (Li et al., 2023b). We employ 30 widely used models from the AlpacaEval dataset as our model set M, while the benchmark's questions form the question set Q. The rankings provided by the AlpacaEval benchmark serve as ground truth for evaluating the accuracy of various ranking methods. This is justified by AlpacaEval's strong correlation with Chatbot Arena rankings, making it a reasonable proxy for human judgments (Dubois et al., 2024a). We adopt Ranking Correction, measured by the Spearman rank correlation coefficient, to evaluate the similarity. To generate rankings, we utilize outputs from the open-source models Llama3-70B, Qwen2-72B, Mistral-8\u00d77B, and Qwen1.5-72B as our evaluators, denoted as set A. For further implementation details, see Appendix A. We investigate two variants of GED: (w/o ensemble) denoises the preference graphs from different evaluators for the same question, converts each into a ranking, and then ensembles these rankings to produce the final output, while (w/o denoising) directly ensembles the preference graphs to obtain the final ranking without denoising.\nMain results. The results, presented in Table 2, show that GED outperforms all single-model baselines, highlighting the significant improvement in ranking accuracy achieved by leveraging preference information from multiple evaluators. Moreover, GED surpasses the (w/o ensemble) variant, indicating that generating rankings through graph ensemble first prevents information loss compared to converting individual graphs into rankings. When the ensemble graph is not denoised (w/o denoising), residual noise can adversely affect the final ranking quality. Additionally, our denoising method also enhances results in single-model settings."}, {"title": "EXPERIMENTS ON INSTRUCT TUNING", "content": "In this section, we explore the effects of various data selection methods for model alignment on Llama-2-7B (Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023) through instruct tuning. Specifically, we randomly sampled 5000 data points from UltraFeedback (Cui et al., 2023), and utilized Qwen1.5-14B (Yang et al., 2024a) to generate 8 responses for each data point as instruct data. We then applied four different methods-Random, Longest (Zhao et al., 2024), ContraSolver (Zhang et al., 2024b), and our proposed GED-to select a subset of these responses for model alignment training. The Origin refers to the performance of the base model without alignment. The resulting models were evaluated on the HH-RLHF (Bai et al., 2022) benchmark, which consists of four sub-sets: Harmless (base), Helpful (base), Helpful (online), and Helpful (rejection). For evaluation, we adopted the same Reward model, following prior work (Song et al., 2024; Yu et al., 2023), to measure human preference levels gained by the models. These results are summarized in Table 3. To ensure a comprehensive evaluation, we also tested the models, using the Llama-2-7B backbone, across additional benchmarks such as LIMA (Zhou et al., 2023), Vicuna (Chiang et al., 2023), Koala (Vu et al., 2023), WizardLM (Xu et al., 2023) and Self-Instruct (Wang et al., 2022), in line with recent works (Chen et al., 2023; Zhang et al., 2024a; Hu et al., 2024).\nFrom Table 3, we observe that GED consistently outperforms all baseline methods, demonstrating its effectiveness in selecting high-quality responses when multiple answers are available for a given instruction. When faced with multiple responses Y1, Y2,..., Yn for a given instruction x, the Random selection method can have a detrimental impact, especially when the quality of the responses is inconsistent. This effect is most evident with the Mistral-7B, where Random selection actually performs worse than the Origin, indicating that randomly chosen data points can introduce noise and degrade the model's performance. Moreover, we find that simply selecting the longest response does not always lead to the best outcomes. While longer responses may provide more detailed answers, they are not necessarily better in terms of quality, particularly when both high-quality and low-quality answers exist for the same question. This is reflected in the results where the Longest method underperforms compared to both ContraSolver and GED, emphasizing that response length alone is not always a reliable criterion. From Figure 5, we can draw similar conclusions as Table 3. Specifically, we observe that GED consistently outperforms all baselines, demonstrating its effectiveness across all datasets. Particularly in AlpacaEval and Self-Instruct, the Random baseline performs worse than the Origin model, indicating that when response quality varies significantly, poor selection can lead to negative performance. In contrast, GED excels by aggregating preference graphs and applying denoising, effectively filtering out low-quality responses. This ensures robust performance, especially in cases where response quality is inconsistent. The denoising step is crucial for removing noisy evaluations, leading to improved model alignment. The denoising process in GED proves essential, particularly in settings with inconsistent responses, as it removes evaluation noise and leads to more robust performance."}, {"title": "RELATED WORK", "content": "Preference evaluation of LLMs. Reference-free evaluation metrics have a long history (Louis & Nenkova, 2013), which evaluates the generated text based on intrinsic properties and coherence with the context. Although they achieve high accuracy on matching inner-evaluator, the achievement suffers from spurious correlations such as perplexity and length (Durmus et al., 2022). Recently, people have started using a strong model (e.g., GPT-4) as an evaluator to perform a zero-shot reference-free evaluation on the weak models (Shen et al., 2023; Dubois et al., 2024b; Chen et al., 2023). However, using LLM-based preference evaluations can introduce inconsistencies in preference graphs, often resulting in cyclic preferences or contradictions when comparing multiple outputs.\nWeak supervision. The concept of weak-to-strong supervision originates from the need to leverage noisy or partial labels in machine learning tasks, enabling the development of more robust models from imperfect data (Ratner et al., 2016; Zhang et al., 2023b; 2022). In LLMs, weak-to-strong supervision aids AI alignment by allowing weaker models to improve strong ones, enhancing performance without extensive data and supporting scalable oversight (Zheng et al., 2024; Guo & Yang, 2024; Tong et al., 2024). Similarly, in task-oriented LLMs, weak-to-strong learning improves LLM's ability by enabling strong models to refine their data autonomously, boosting performance without extensive high-quality input (Zhang et al., 2023a; Yang et al., 2024b). Through weak-to-strong supervision, LLM performance can be significantly improved by iteratively transforming low-quality labels into more reliable ones, leading to more effective model training and robust outputs (Zakershahrak & Ghodratnama, 2024; Lang et al., 2024)."}, {"title": "CONCLUSION", "content": "In this paper, we presented GED, a framework designed to address inconsistencies in pairwise prefer-ence evaluations by LLMs. By employing graph ensemble techniques and denoising, GED reduces cyclic patterns and enhances the reliability of evaluation outcomes. Our theoretical analysis shows that GED can recover the ground truth DAG under reasonable conditions, improving consistency in preference rankings. Extensive experiments across response selection, model ranking, and instruct tuning demonstrate the efficacy of our method. GED consistently outperformed baseline methods in both single-evaluator and multi-evaluator settings, particularly in scenarios where combining weak evaluators led to superior results over stronger individual evaluators. Future work will explore extending GED to broader evaluation frameworks and applying its principles to more complex decision-making tasks, including multi-agent systems and human-AI interaction."}, {"title": "IMPLEMENTATION DETAILS", "content": "All experimental procedures were conducted on a machine equipped with an AMD EPYC 7543 32-Core Processor, 512GB memory, 128 CPUs, and four 80GB NVIDIA A800 GPUs. The code is available at https://github.com/ppsmk388/GED. The references to Llama-2-7B, Llama3-70B, Llama3-8B, Mistral-7B, Qwen2-7B, and Qwen2-72B in the main text refer to the specific models: Llama-2-7b-chat-hf, Meta-Llama-3-70B-Instruct, Meta-Llama-3-8B-Instruct, Mixtral-8x7B-Instruct-v0.1, Qwen2-7B-Instruct, and Qwen2-72B-Instruct. We utilized the reward model oasst-rm-2-pythia-6.9b-epoch-1 following prior works (Song et al., 2024; Yu et al., 2023). Each experiment was repeated three times, and the average performance was reported as the final result. The training was configured with a batch size of 1 per device, gradient accumulation steps of 4, a learning rate of 1e-5, and he model was trained for 3 epochs, with warmup over 20 steps and a cosine learning rate scheduler. For tasks such as AlpacaEval (Dubois et al., 2024b), we used GPT-4-o unless stated otherwise."}, {"title": "DATASET", "content": "In this appendix, we provide detailed information about the datasets used in main text.\n\u2022 UltraFeedback (Cui et al., 2023): UltraFeedback is a large-scale, fine-grained, diverse preference dataset, used for training powerful reward models and critic models. We collect about 64k prompts from diverse resources (including UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN). We then use these prompts to query multiple LLMs (see Table for model lists) and generate 4 different responses for each prompt, resulting in a total of 256k samples.\n\u2022 HH-RLHF (Bai et al., 2022): The HH-RLHF dataset contains human preference data for training language models to be helpful and harmless, as well as red teaming data to identify harmful model outputs. The preference data includes pairs of chosen and rejected responses, while the red teaming data includes transcripts of adversarial interactions with AI assistants, rated for harmfulness. We strictly follow prior works (Song et al., 2024; Yu et al., 2023) and used the code from this repository \u00b9 for testing.\n\u2022 MATH (Hendrycks et al., 2021): The MATH dataset consists of 12,500 challenging competition-level math problems, each with a detailed step-by-step solution. It is designed to teach models to generate answer derivations and explanations, aiding in mathematical reasoning. Despite progress in improving accuracy, the dataset highlights the limitations of large Transformer models in solving complex math problems without new algorithmic advancements. Due to the high resource cost of using the full test set, we randomly sampled 400 problems from the test set for evaluation.\n\u2022 GSM8k (Chen et al., 2021): GSM8K (Grade School Math 8K) is a collection of 8.5K high-quality math word problems designed for grade school students. It supports the task of multi-step reasoning and question answering in basic math. The problems require 2 to 8 steps, focusing on elementary arithmetic operations (addition, subtraction, multiplication, and division). The solutions are provided in natural language, making it accessible for evaluation of language models' internal reasoning. GSM8K has been widely used to test logic and mathematical capabilities in language models, especially for benchmarks like the LLM Leaderboard. Due to the high computational cost of using the entire test set, we randomly sampled 400 data points from the test set for our evaluation.\n\u2022 GAIA (Mialon et al., 2023): The GAIA dataset is a benchmark designed to evaluate next-generation LLMs with augmented capabilities like tooling and search access. It consists of over 450 complex questions with unambiguous answers, requiring various levels of autonomy and tooling. The dataset is divided into three levels, each increasing in difficulty, with a public dev set for validation and a private test set for evaluation. We used the entire test set for our evaluation."}, {"title": "CONSTRUCTION OF THE PREFERENCE GRAPH", "content": "In this section", "follows": "Initialization: For each question q \u2208 Q", "Comparisons": "For each pair of models Mi and Mj", "Storage": "Once all pairwise comparisons have been processed for a given evaluator, the resulting graph Ga = (Vq, Aa, wa) is"}]}