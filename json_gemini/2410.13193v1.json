{"title": "Golyadkin's Torment: Doppelg\u00e4ngers and Adversarial Vulnerability", "authors": ["George I. Kamberov"], "abstract": "Many machine learning (ML) classifiers are claimed to outperform humans, but they still make mistakes that humans do not. The most notorious examples of such mistakes are adversarial visual metamers. This paper aims to define and investigate the phenomenon of adversarial Doppelg\u00e4ngers (AD), which includes adversarial visual metamers, and to compare the performance and robustness of ML classifiers to human performance.\nWe find that AD are inputs that are close to each other with respect to a perceptual metric defined in this paper, and show that AD are qualitatively different from the usual adversarial examples. The vast majority of classifiers are vulnerable to AD and robustness-accuracy trade-offs may not improve them. Some classification problems may not admit any AD robust classifiers because the underlying classes are ambiguous. We provide criteria that can be used to determine whether a classification problem is well defined or not; describe of an AD robust classifiers' structure and attributes; introduce and explore the notions of conceptual entropy and regions of conceptual ambiguity for classifiers that are vulnerable to AD attacks, along with methods to bound the AD fooling rate of an attack. We define the notion of classifiers that exhibit hyper-sensitive behavior, that is, classifiers whose only mistakes are adversarial Doppelg\u00e4ngers. Improving the AD robustness of hyper-sensitive classifiers is equivalent to improving accuracy. We identify conditions guaranteeing that all classifiers with sufficiently high accuracy are hyper-sensitive.\nOur findings aim at significant improvements in the reliability and security of machine learning systems.", "sections": [{"title": "Introduction", "content": "Perceptual metamers are the most striking adversarial examples studied by the machine learning community. Two perceptual metamers are shown in Figure 1. The phenomenon of metamersim studied in the visual domain, including perceptual metamers, is a manifestation of the existence of Doppelg\u00e4ngers: different inputs or stimuli that are perceptually indiscriminable. The research community has engaged in active studies of the adversarial vulnerability of seemingly successful classifiers ever since the publication of [SZS+13]. Adversarial Doppelg\u00e4ngers, that is, adversarial examples which are Doppelg\u00e4ngers are qualitatively different from the vast majority of known adversarial examples which humans readily discriminate from correctly classified input samples (Figure 2). However, there is no evidence"}, {"title": "Related work", "content": "The pair (X, indiscriminability relation) is a tolerance space. Tolerance spaces, rough sets, and granular computing have bee discussed extensively. See [Zee62, Zee65, Pos71, Rob73, Sch75, Sos86, Hov92, Yao96, Hen11, PW12, Paw12, ZB17, Yao19] The color and image metamers studied by many authors including [Ost19, Wys53, Tho73, Kra75, CK85, BNR09, FS11, Log13, LD16, FDGM19, JG22, BRWS23] are Doppelg\u00e4ngers.\nThe research on adversarial examples to date builds on the hypothesis that the space of input samples is a metric space (X, distx). A misclassified input x* is considered an adversarial example if it is nearby a correctly classified input sample x, i.e., distx(x, x*) is small. In fact usually X is assumed to be Rn, endowed with the lp, norm, p = 1,2,...,\u221e or at least locally homeomorphic to Rn, i.e., a manifold, equipped with some geodesic distance.\nSomewhat non surprisingly many authors have shown that every classifier can be attacked with such adversarial examples [GSS14, PMJ+16, FFF18, BSAP22] or at least that this is true in many contexts, [MDFFF17, SHS+18, MDM19].\nOther papers indicate that there are paths toward eliminating adversarial examples completely, i.e., it is possible to achieve provable \u201cadversarial robustness\" by fixing/retraining the classifier [GR06, TG16, MMS+17, IST+19, SMA20, AZL22]. A widely accepted tenet is that \u201cthere is a clear trade-off between accuracy and [adversarial] robustness, and a better performance in testing accuracy in general reduces [adversarial] robustness\", [SZC+18]. For empirical evidence for this trade-off and some attempts to explain this phenomenon see [TSE+18, SZC+18, ZYJ+19]."}, {"title": "Perceptual Topology", "content": "The ability to decide whether one stimulus/input is distinct from another is essential for adaptation, survival, and intelligent life. Intelligent agents are uniquely capable to activate knowledge to judge distinction. Williamson calls this context-relative process discrimination ([Wil90]). Discrimination establishes a context-relative binary relation denoted by $\u2248_{\u03b1\u03b4}$ and called indiscriminability:\nDefinition 1 ([Wil90]). Two inputs x and y are called indiscriminable to a subject at a time t if and only if at time t the subject is not able to activate (acquire or employ) the relevant kind of knowledge that x and y are distinct.\nSome authors refer to indiscriminability as active indiscriminability, see [Far10]. The indiscriminability relation is symmetric and reflexive and generates a context-relative topology on the set of inputs X.\nDefinition 2. For every $x \u2208 X$ let $d(x) = {y \u2208 X: y \u2248_{\u03b1\u03b4} x}$. A point $y \u2208 d(x) \\ {x}$ is called a Doppelg\u00e4nger [of x]. The perceptual topology $\u03c4_\u03b4$ is the topology generated by the sub-basis $D_{ad} = {d(x)}_{x\u2208X}$.\nExample 0 An input $x \u2208 X$ is called optimal if it does not have non-trivial/non-identical Doppelg\u00e4ngers, i.e., if $d(x) = {x}$. In particular, if \u2248 is the identity relationship = (i.e,. all inputs are optimal within the given context), then the perceptual topology $\u03c4_\u03b4$ is discrete. However, the finiteness of human observations (they are subject to finite time and finite work constraints) and the bounded rationality constraints imposed by the limitations on the availability of information and computational capabilities to humans, [Sim57], indicate that the scenario $d(x) = {x}$ for every $x \u2208 X$ may be highly unlikely.\nOften reflexive binary relations are defined and discussed as coverings of the underlying space. Indeed, every reflexive binary relation (RBR) \u2248 on X defines a covering ${g(x) = {y : y \u2248 x}}_{x\u2208x}$, of X and vice versa one can define reflexive binary relationships through coverings of X. See Appendix, Section A. In particular, the perceptual topology, $\u03c4_\u03b4$, is a tolerable topology, cf., Definition 14 in Appendix, Section A."}, {"title": "3.1 Indiscriminability and Topology", "content": "The ability to decide whether one stimulus/input is distinct from another is essential for adaptation, survival, and intelligent life. Intelligent agents are uniquely capable to activate knowledge to judge distinction. Williamson calls this context-relative process discrimination ([Wil90]). Discrimination establishes a context-relative binary relation denoted by $\u03b4$ and called indiscriminability:\nDefinition 1 ([Wil90]). Two inputs x and y are called indiscriminable to a subject at a time t if and only if at time t the subject is not able to activate (acquire or employ) the relevant kind of knowledge that x and y are distinct.\nSome authors refer to indiscriminability as active indiscriminability, see [Far10]. The indiscriminability relation is symmetric and reflexive and generates a context-relative topology on the set of inputs X.\nDefinition 2. For every $x \u2208 X$ let $d(x) = {y\u2208X: y \u2248_{\u03b1\u03b4} x}$. A point $y \u2208 d(x) \\ {x}$ is called a Doppelg\u00e4nger [of x]. The perceptual topology $\u03c4_\u03b4$ is the topology generated by the sub-basis $D_{ad} = {d(x)}_{x\u2208x}$.\nExample 0 An input $x \u2208 X$ is called optimal if it does not have non-trivial/non-identical Doppelg\u00e4ngers, i.e., if $d(x) = {x}$. In particular, if \u2248 is the identity relationship = (i.e,. all inputs are optimal within the given context), then the perceptual topology $\u03c4_\u03b4$ is discrete. However, the finiteness of human observations (they are subject to finite time and finite work constraints) and the bounded rationality constraints imposed by the limitations on the availability of information and computational capabilities to humans, [Sim57], indicate that the scenario $d(x) = {x}$ for every $x \u2208 X$ may be highly unlikely.\nOften reflexive binary relations are defined and discussed as coverings of the underlying space. Indeed, every reflexive binary relation (RBR) \u2248 on X defines a covering ${g(x) = {y : y \u2248 x}}_{x\u2208x}$, of X and vice versa one can define reflexive binary relationships through coverings of X. See Appendix, Section A. In particular, the perceptual topology, $\u03c4_\u03b4$, is a tolerable topology, cf., Definition 14 in Appendix, Section A."}, {"title": "3.2 Indiscriminability, Indiscernibles, and Discriminative Feature Representations", "content": "Let \u03a6 be the space of all features of the inputs/stimuli $x \u2208 X$ and let \u03a6x \u2282 \u03a6 be the set of features attributed to x in a given context. The features attributed to x are either perceptually robust, i.e., features that belong to $\u22c2_{y \u2208 d(x)} \u03a6_y$, or perceptually adversarial, i.e., features that belong to $\u22c3_{y \u2208 d(x)} \u03a6x \\ \u03a6_y$.\nDefinition 4. Following [For20], we say that x and y are indiscernible, in a given context, if $\u03a6_x = \u03a6_y$."}, {"title": "3.3 Contrast and Indiscriminability", "content": "Intuitively, indiscriminability is related to similarity or equivalently to contrast. Leibniz refers to indiscernibles as perfectly alike (parfaitement semblables), [Cla17], while Poincar\u00e8, calls them semblables \u00e8l\u00e8ments, [Poi30]. A common approach is to use geodesic distance to quantify similarity/contrast. However, empirical data shows that in many experiences similarity/contrast are not symmetric or do not satisfy the triangle inequality.\nOn the other hand, every binary relation is defined by a contrast context, that is a pair of proset-valued maps (c, \u20ac). Indeed, let < be a pre-order on a set C, every pair of mappings $c : X \u00d7 X \u2192 C.$ and $\u2208 : X \u2192 C$ defines a binary relation $\u2248_{(C, E)}$ on X s.t.\n$\u2248_{(C,E)} = {(x, y) \u2208 X \u00d7 X : c(x, y) < \u2208(x)}$\nIndeed, if \u2248 is a binary relation, then the contrast function defined by\n$c(x, y) = \\begin{cases}\n0, & \\text{if } x \u2248 y \\\\\n1, & \\text{otherwise},\n\\end{cases}$\nand $\u2208(x) = 1$ for all $x \u2208 X$ are {0, 1}-valued and the pre-order relation is the usual < do define the binary relation \u2248.\nThere are many contrast contexts that generate the same binary relation. A straightforward computation confirms that:\nObservation 2. A binary relation is symmetric and reflexive if and only if it is defined by a contrast context (c, e) such that $c(x,x) < \u2208(x)$ for every $x \u2208 X$ and $(c(x, y) < \u2208(x)) \u21d2 (c(y, x) < \u2208(y))$ for every x, y \u2208 X.\nA binary relation is an equivalence relation if and only if it is defined by contrast context (c, e) such that $c(x,x) < \u2208(X)$ for every $x \u2208 X$, $(c(x, y) < \u2208(x)) \u2192 (c(y,x) < \u2208(y))$ for every x,y \u2208 X, and $(c(x, y) < \u2208(x)) > ((c(y, z) < \u2208(y)) (c(x, z) \u2208(x))$ for every x, y, z \u2208 X.\nExample 6: The metric tolerance relation introduced by Zeeman (see [Zee65, ZB17]) is defined by the contrast context $(c(x, y) = dist(x, y), \u2208(x) = \u2208)$, where dist is a distance on X, $\u03b5 > 0$ is a fixed real constant and once again (C, <) is ([0,\u221e), <).\nExample 7: Pawlak's framework for object classification \u201cby means of attributes\u201d, [Paw81], describes an approach to indiscriminability. The key ingredients of the framework are collection A of attributes, i.e., maps $a: X \u2192 F_a$ assigning attribute values a(x) to inputs $x \u2208 X$ and a knowledge function $p : X \u00d7 A \u2192 F_a$, such that $p(x, a) \u2208 F_a$ for all $\u03b1\u2208A$ $x \u2208 X$. The product of all attribute value ranges is the collection of all possible features attributable to the inputs cf. Section 3.2, that is, $\u220f_{\u03b1\u2208A} F_a = \u03a6$, and the a-th component of \u03a6x is p(x, \u03b1).\nThe indiscriminability relation defined by Pawlak is just the transitive indiscernibility relation $x \u2248_{\u03b1\u03b4} y$ iff $\u03a6_x = \u03a6_y$ and is generated by the contrast context (c, \u20ac) where\n$c(x, y) = \\begin{cases}\n0, & \\text{if } \u03a6_x = \u03a6_y \\\\\n1, & \\text{if } \u03a6_x \u2260 \u03a6_y\n\\end{cases}$"}, {"title": "4 Classifiers, Adversarial Doppelg\u00e4ngers.", "content": "In this paper we identify classifiers with finite segmentations of the space of inputs X since the labeling function label\u20a8 : X \u2192 {1, ..., m} of a classifier R partitions X into disjoint label sets\n$Rc = {x \u2208 X : labelr(x) = c}, c = 1,...,m$\nand\n$label (x) = \u2211_{c=1}^{m}C_{1_R}(x), \u2200x \u2208 X.$\nThe segmentation R is called fully populated iff the labeling function label\u20a8 is surjective mapping onto the range of labels {1,...,m}."}, {"title": "4.1 Adversarial Doppelg\u00e4ngers", "content": "Definition 7. We say that x is a Doppelg\u00e4nger adversarial to the classifier R iff \u2203y \u2208 d(x) such that labelR(x) \u2260 labelR(y) and we will refer to both x and y as adversarial Doppelg\u00e4ngers when the classifier R is clear from the context.\nA classifier R is called (perceptually) regular iff it does not admit adversarial Doppelg\u00e4ngers.\nWe say that the classification problem with m-labels is well defined if there exists a fully populated and perceptually regular classifier R = {R1, . . ., Rm} with m labels. Otherwise we say that the classification problem with m-labels is not well defined.\nIf R is regular, then Dao = {d(x)}x are R coherent coverings (as defined in [Sch75]). However, coherency does not imply regularity. Namely if Dad is coarser than the covering defined by R, then D25 and R are coherent but the later may admit Doppelg\u00e4ngers.\nThe labeling function label\u20a8 of a regular classifier is continuous with respect to the perceptual topology T\u00f4, i.e., labelr \u2208 C (X, 75) and harmonic with respect to a specific perceptually-based Laplace operator \u25b3, defined in Section 6. Furthermore, if R is not regular and x is a point of discontinuity of labelR : (X, Ts) \u2192 {1,...,m} then x is an adversarial Doppelg\u00e4nger, i.e., there exists y \u2248 x such that labelR (x) \u2260 labelR (y). However, y \u2248 x such that labelR (x) \u2260 labelR (y) does not imply that either x or y is a point of discontinuity of labelR. On the other hand, if every two points x \u2248 y are not separable, then every Doppelg\u00e4nger adversarial to R is a discontinuity of label\u20a8 or equivalently R is regular if and only if label\u20a8 \u2208 C\u00ba(X, T\u00f4)."}, {"title": "4.2 Regular models: Existence and structure", "content": "A straight-forward argument shows that R = {R1, . . ., Rm } is a perceptually regular fully populated classifier, then x \u2208 Ri iff [x]~ C Ri, i.e., each segment is a disjoint union of equivalence classes [x]~ and so using the pigeonhole principle we obtain:\nObservation 4. If the number of equivalence classes card $(X/~_\uff61) \u2265 2$, then for every natural number 2 < m < card $(X/~\uff61)$, there exists a perceptually regular fully populated classifier with m labels. However, if m > card $(X/~\uff61)$, then every fully populated classifier with m labels must have adversarial Doppelg\u00e4ngers.\nIn particular, if p = card $(X/~\uff61) \u2265 2$ is finite, then for every natural number m < p there are exactly S(p,m) regular fully populated classifiers with m classes. Here S(p, m) is the Sterling number of the second kind.\nObservation 4 shows that the problem of finding a fully populated perceptually unambiguous classifier R = {R1,..., Rm} with precisely m labels is not well defined if card $(X/~\uff61) < m$ and vice versa that the same problem is well defined for every m such that card $(X/~0) \u2265 m$. In the former case solutions do not exist while in the later case solutions exist and each class segment R\u2081 is a union of equivalence classes [x]~,\n$Ri = \u22c3_{x\u2208Ri} [x]~$"}, {"title": "5 Accuracy and Adversarial Doppelg\u00e4ngers.", "content": "The accuracy-adversarial robustness trade off observed and discussed in the literature involves various measures of accuracy [SZC+18, TSE+18, ZYJ+19, NGH+22, OCNM22]. We will discuss classification accuracy. We will show that there is a strong relationship between classifier accuracy and vulnerability to adversarial Doppelg\u00e4ngers. In particular, we will identify (perceptual) scenarios in which low accuracy classifiers are critically vulnerable to adversarial Doppelg\u00e4nger attacks but on the other hand all high accuracy classifiers can be fooled only by Doppelg\u00e4ngers."}, {"title": "5.1 The probabilistic setup: recall rates, accuracy, bounds", "content": "We will assume that (X, F, \u03bc) is a probability measure space equipped with perceptual topology 75 generated by an indsicriminability relation \u2248 such that for every x \u2208 X the set of Doppelg\u00e4ngers d(x) and the equivalence class [x]~\uff61 are events, (d(x) \u2208 F and [x]~\u300f \u2208 F, \u2200x \u2208 X) .\nIn the rest of this section we will assume that the classification problem with m \u2265 2 labels is well defined and let \u03a9 = {1, ..., \u03a9m} be a perceptually regular world model; we will reserve the notation R = {R1, . . ., Rm } to denote any classifier (R may or may not be perceptually regular) such that Ri \u2229 Ni, i = 1, . . ., m are the true positives (of class i). To define accuracy we will focus only on regular models and classifiers s.t., Ni \u2208 F and Ri \u2208 F, for all i = 1, . . ., m. The accuracy of the classifier R defined as\n$accuracy (R) = \u03bc(R\u2081 \u2229 \u03a9\u2081) + \u2026 + \u03bc(Rm \u2229 \u03a9m)$.\nFurthermore let us assume that \u03bc(\u03a9\u2081) > 0 for every i = 1, . . ., m and thus we can define recall rates\n$Pi = \\frac{\u03bc(Ri \u2229 \u03a9i)}{\u03bc(\u03a9i)}, i = 1,..., m$\nBounds on the recall rates imply bounds on the accuracy. Namely if\n$p \\leq Pi \\leq p, i = 1,..., 1, ..., m 1$\nthen since u is a probability measure on X, and\n$\u03bc(R\u2081 \u2229 \u03a9\u2081) = \u03c1\u03b5\u03bc(\u03a9\u2081), i = 1, ..., m$\nwe get\n$\u03c1 < accuracy(R) = \u2211_{i=1}^{m}\u03c1\u03af\u03bc(\u03a9) \u2264 \u03c1$"}, {"title": "5.2 Doppelg\u00e4ngers, misclassification...but can we trade?", "content": "Let i(x) \u2208 {1, ..., m} be the object class label of x \u2208 X i.e., $x \u2208 \u03a9_{i(x)}$ and\n$\u03ba(\u03a9) = \\frac{\u03bc(\u03a9_{i(x)})}{\u03bc(d(x))} = sup_{x\u2208X} \\frac{\u03bc(\u03a9_{i(x)})}{\u03bc(d(x))}$.\nEvery classifier whose recall rates do not exceed 1/k(\u03a9) is totally unsafe in the sense that every correctly classified input admits adversarial Doppelg\u00e4ngers. Specifically:"}, {"title": "6 Are the differences between Doppelg\u00e4ngers small?", "content": "Example 11: Well established phenomena including the bias toward same response in perceptual matching ( [CCR82, Kru78]), the own-race bias ( [MB01, TKB04, GHP09, TP09]), and own-age bias ( [HL11]) provide evidence that d(x) may not be small lp balls.\nFurthermore, there is a mounting evidence that humans may perceive as visible light and even identify the hue of light waves in the so called invisible ranges of the spectrum. Near IR wavelengths in the 950nm to 1040nm range are perceived as shade of red and that near IR wavelengths just above 1040nm are perceived as \"light with colors corresponding to roughly half of the excitation wavelengths\", [PVS+14]. On the other hand, [HJRH18] reports that some groups of young adults detect 315nm UV radiation and perceive it as a shade of violet. These studies provide an example of a perceptual topology on X = [0, +\u221e) whose sub-basis includes neighborhoods d(x) which are not connected with respect to the Euclidean topology on the ray of wavelengths X.\nIt turns out that if x \u2248 y, then y is a small perturbation of x and vice versa in the sense that the distance between x and y is relatively small with respect to an appropriate metric dw (\u00b7, \u00b7) on X. The metric is built using the discrimination graph \u0393 (\u03a7, \u0395\u03b1s) described in the next paragraph. The Laplace operator \u0394\u03b1\u03b4 on \u0393 (\u03a7, \u0395\u03b1\u03b4) may be somewhat quirky, for example, piecewise-constant functions may not be harmonic. At the end of this section we discuss a better-behaved context-relative Laplace operator \u2206. The salience measure introduced in [Tve77] is harmonic with respect to \u25b3, (see Section 7).\nThe indiscriminability relation \u2248 defines a un-oriented graph \u0393 (\u03a7, \u0395as) where X is the set of vertices and we say that there is an edge {x,y} \u2208 Eas between the vertices x, y \u2208 X iff x \u2248 y. We will call \u0413 (\u03a7, \u0395\u03b1\u03b4) the discrimination graph. Let doo (x, y) be the graph distance between the vertices x and y and, in particular, d\u221e (x, y) = \u221e iff x Jo y.\nThus\ndu (x, y) = \\frac{do(x,y)}{1 + d\u221e(x,y)}, Vx, y \u2208 X"}, {"title": "7 Life without borders, similarity, core and fringe.", "content": "A important property of the perceptual topology is that the if a classifier & is perceptually regular, then it \"imposes an open [topological] borders policy\", that is, \u0398\u03a9 = \u00d8 for every label c. Linguists and psychologists, have observed and postulated that natural perceptual and semantic categories are borderless. See for example [Ros73]. Class/decision boundaries are studied and exploited in many works on classifiers, and in particular on adversarial robustness. These boundaries are artifacts of the metric topology used by the researchers, they are not perceptual phenomena.\nHowever, we are all familiar with the idea that some stimuli are more intrinsic/representative to/of a given class and at the same time frequently there are objects/stimuli/inputs that, while they are firmly with in the class, are less representative/share few(er) features compared to the rest of the elements in the class, that is, they \"are/belong to the fringe\" of the class."}, {"title": "7.1 Affinity, core, and fringe", "content": "Definition 11. Let s be a similarity scale, i.e., a function s : X \u00d7 X \u2192 R such that $s(x, x) \u2265 s(x, y), \u2200x, y \u2208 X$ as in [Tve77], [MGG93] (measuring similarity within a fixed context) and [L+98].\nThe values s(x, x) can be and sometimes are used to represent the salience or equivalently importance of the input within X, see for example [Tve77]. The similarity scale provides a method to quantify the affinity of a input/stimulus to a given measurable subset D C X and the notions of prototype and fringe. The (s-)affinity of x with a measurable set D is defined as\n$P(x, D) = \\int_{D} s(x, y)$\nThis is a straightforward generalization of the notion of prtototypicality defined in [Tve77].\nDefinition 12. $x \u2208 D$ is called a prototype of D (with respect to the integrable similarity scale s : X X X \u2192 R) if\n$P(x, D) = sup_{ZED} P (z, D)$\n$x \u2208 D$ is called a fringe element of D (with respect to the integrable similarity scales : X X X \u2192 R) if\n$P(x, D) = inf_{ZED} P (z, D)$"}, {"title": "8 Quantifying Doppelg\u00e4nger Vulnerability, Doppelg\u00e4ngers attacks, fooling rate bound", "content": "By definition if a classifier R = {R1, ..., Rm } is not regular, then it is vulnerable to adversarial Doppelg\u00e4ngers attacks, that is, for some input x there exists a(x) \u2248 x and such that label\u20a8 (x) \u2260 label\u20a8 (a(x)). In particular, we say that R is conceptually ambiguous at x and we call the set\n$A(R) = {x \u2208 X: \u2203y \u2248_{\u03b1\u03b4}x and labelR (x) \u2260 labelR (y)}$\nthe region of conceptual ambiguity. When (X, \u03bc) is a probability measure space and $(d(x)) > 0$, we use the probability distribution of labels at x:\n$Pj(x) = \\frac{\u03bc (Rj nd(x))}{\u03bc(d(x))}, j = 1,..., m$\nand the conceptual entropy of R at x defined as\n$HR(x) = \u2212\u2211_{j=1}^{m} Pj (x) log(pj(x))$\nto detect whether R is conceptually ambiguous at x (i.e., HR(x) > 0), and to quantify the likelihoods of various adversarial Doppelg\u00e4nger attacks.\nDefinition 13. Let R be a classifier and \u00e2 : X \u2192 X, we will call the inner measure of the set {x: label (a(x)) \u2260 labelR (x)} the R-fooling rate of the mapping \u00e2, and we will denote it by FR(a). Therefore\n$FR(a) = \u03bc* ({x: label (a(x)) \u2260 label (x)})$.\nA mapping a : X \u2192 X is called an adversarial Doppelg\u00e4nger attack to a classifier R if and only if (x) \u2248 x, \u2200x \u2208 X, and the R-fooling rate FR(a) is positive.\nThe set {x: label\u20a8 (\u00e2(x)) \u2260 labelR (x)} is a subset of the region of conceptual ambiguity of R, which yields an upper bound on the R-fooling rate\n$FR(a) \u2264 \u03bc*(A(R))$.\nIn specific scenarios it is possible to get an upper bound on the size, possibly the outer measure, of A(R) which in turn shows that the R-fooling rates are bounded away from one. See Example 14 below.\nExample 14: Let \u03bc(A) = 2\u221a \u222b e-t\u00b2 dt be the probability measure on X = (0, +\u221e) and let the indiscriminability\n\u03c0 A\nrelation on X be defined by the covering $Da\u00f4 = {d(x) = (x/w,xw)}_{x\u00a3x}$, where w > 1 is a fixed constant. Let e > 0 and let R(\u20ac) be the linear classifier defined by\n$label_{R(\u20ac)} (x) = \\begin{cases}\n1, & x < \u20ac \\\\\n2, & x \u2265 \u20ac.\n\\end{cases}$\nThe conceptual entropy of $H_{R(\u20ac)}(x)$ is positive if and only if $x \u2208 (e/w, we)$. In particular, the points outside the region of conceptual ambiguity $X \\ [e/w, we]$ are not vulnerable to adversarial Doppelg\u00e4ngers attacks. The conceptual entropy achieves its global maximum $H_{R(\u20ac)}(x*) = 1/2$ at a $x* = x*(\u20ac, w)$. It is equal to zero on $(0, \u20ac/w] \u222a [ew, +\u221e)$, and increases monotonically on $[e/w, x*]$, and then decreases monotonically on $[x*, ew]$. The vulnerability to an adversarial Doppelg\u00e4nger attack is maximized at the point x*. The measure of the region of ambiguity A(R(\u20ac)) is\n$\u03bc(A(R(\u20ac)) = erf(we) \u2013 erf(e/w) < 1$. The R(\u20ac)-fooling rate $F_{R(\u20ac)}(\u00e2) \u2264 erf(we) \u2013 erf(e/w) < 1$ of an adversarial Doppelg\u00e4nger attack \u00e2 is safely bounded away from 1.\nAdversarial Doppelg\u00e4nger attacks are distinct from the adversarial attacks studied to date. The universal adversarial attacks, [MDFFF17], can achieve fooling rates as close to one as one desires. As illustrated above, adversarial Doppelg\u00e4nger attacks may not be able to reach fooling rates that are too high. On the other hand in some cases, the optimal fooling rate of one can be achieved.\nObservation 7. If R is conceptually ambiguous at every x \u2208 X, e.g., when HR(x) > 0 for every x \u2208 X, then there exists an adversarial Doppelg\u00e4nger attack with R-fooling rate equal to one.\nIndeed, if R is conceptually ambiguous at every x \u2208 X, then {y \u2208 d(x) : label (y) \u2260 label\u20a8 (x)} \u2260 0, for every x, and therefore the axiom of choice implies that there exists a map \u00e2 : X \u2192 X such that a(x) \u2208 {y \u2208 d(x): label (y) \u2260 labelR (x)}, for every x \u2208 X. It turns out that in practice there may be many classifiers that are conceptually ambiguous at every input."}, {"title": "9 Discussion and Conclusions.", "content": "A central focus of this paper is the adversarial Doppelg\u00e4ngers phenomenon, where classifiers assign different labels to inputs that humans cannot discriminate. Until now, this phenomenon has not been well understood, possibly due to the limitations of the distance-based analysis that has dominated the field. In the \"absence of a distance measure that accurately captures the perceptual differences between a source and adversarial example many researchers have decided to use the lp distance\", [HD18]. The available empirical observations and models - both perceptual and cognitive, including those based on just noticeable differences - provide no evidence that biologically plausible perceptual topologies are metric. This paper advances the understanding of context-related perceptual topologies in input spaces, which are rarely metric. Our investigation shows that adversarial Doppelg\u00e4ngers are very close to each other with respect to the context-relevant perceptual metric dw, this metric is not a manifold metric and does not generate the perceptual topology. This distinction highlights the shortcomings of traditional, purely manifold metric-based representations and analysis of perceptual spaces.\nThe machine learning community has expended significant efforts aimed to build adversarially robust classifiers. This may be a march towards a bridge too far. Philosophers, experimental psychologists, and linguists, are well aware that many classification problems are not well defined due to perceptual ambiguities. Any fully populated classifier for a classification problem that is not well defined is doomed to be a victim of the adversarial Doppelg\u00e4ngers phenomenon. Our results reveal the structure of adversarial Doppelg\u00e4nger-robust classifiers, regular classifiers, and criteria and methods to establish whether a classification problem is well defined or not. The new understanding of the structure of regular classifiers, the analysis of zones of ambiguity, and the methods to measure and bound the fooling rates of adversarial Doppelg\u00e4nger attacks provide guidance on how to design adversarially robust training to improve classifiers that are not regular. In addition to revealing the impossibility to use accuracy-robustness trade-offs in many scenarios, including robustifying hyper-sensitive classifiers, our analysis indicates that marking unseen data can jeopardize robustness if the training data contains only a proper subset of an elementary set.\nWe explore feature representations, the related concept of indiscernibility introduced by Leibniz, and their connection to indiscriminability. This investigation reveals the nature of class prototypes and fringe inputs, and how the size of a discriminative feature representation can be used to determine whether a classification problem is not well defined. Indiscernibility and indiscriminability, are often conflated in the machine learning literature. Elucidating the distinction between them is vital for understanding the limitations of current classifiers and addressing the shortcomings in their design.\nOur discussion of the Doppelg\u00e4ngers phenomenon brings to light a significant divergence between human perception and artificial neural network models, including free-forward models, RNN models and ResNet. The indiscriminability relations of these artificial neural network models, studied in [FDGM19], are transitive while it is well accepted that in many contexts the human indiscriminability relation is not transitive.\nThe results and insights gained from this investigation point to concrete warnings and actionable steps for improving the training and testing of classifiers."}]}