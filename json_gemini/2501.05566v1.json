{"title": "Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene Understanding", "authors": ["Mohammed Elhenawy", "Huthaifa I. Ashqar", "Andry Rakotonirainy", "Taqwa I. Alhadidi", "Ahmed Jaber", "Mohammad Abu Tami"], "abstract": "Scene understanding is essential for enhancing driver safety, generating human-centric explanations for Automated Vehicle (AV) decisions, and leveraging Artificial Intelligence (AI) for retrospective driving video analysis. This study developed a dynamic scene retrieval system using Contrastive Language-Image Pretraining (CLIP) models, which can be optimized for real-time deployment on edge devices. The proposed system outperforms state-of-the-art in-context learning methods, including the zero-shot capabilities of GPT-40, particularly in complex scenarios. By conducting frame-level analysis on the Honda Scenes Dataset, which contains a collection of about 80 hours of annotated driving videos capturing diverse real-world road and weather conditions, our study highlights the robustness of CLIP models in learning visual concepts from natural language supervision. Results also showed that fine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly improved scene classification, achieving a top F1-score of 91.1%. These results demonstrate the ability of the system to deliver rapid and precise scene recognition, which can be used to meet the critical requirements of Advanced Driver Assistance Systems (ADAS). This study shows the potential of CLIP models to provide scalable and efficient frameworks for dynamic scene understanding and classification. Furthermore, this work lays the groundwork for advanced autonomous vehicle technologies by fostering a deeper understanding of driver behavior, road conditions, and safety-critical scenarios, marking a significant step toward smarter, safer, and more context-aware autonomous driving systems.", "sections": [{"title": "I. INTRODUCTION", "content": "UNDERSTANDING scenes is considered one of the most challenging tasks for improving driver safety and enhancing the performance of ADAS [1], [2].\nThe use of Al in this field has opened new dimensions for the retrospective analysis of driving videos. This enables the analysis of driver behavior in terms of gap identification in driving skills and assessment of overall driving performance [3], [4], [5]. The role of AI in improving scene understanding improving scene understanding is further emphasized by its ability to process massive volumes of driving data. For example, machine learning techniques, like Long Short-Term Memory (LSTM) networks, have been applied to classify and predict driver behavior based on real-time data [6].\nIt will have the possibility to develop systems able not only to learn a particular style of driving but also to adapt themselves to environmental conditions to optimize safety performance. Additionally, AI in ADAS means continuous monitoring of driver behavior, whereby at any given moment the system could give some feedback or even intervene [7]. AI significantly increases vehicular situational awareness and subsequently decision-making processes by processing and analyzing data from a wide range of sensors such as cameras and LiDAR [8]. Furthermore, driver acceptance of ADAS can be better understood by gaining insights into how drivers will respond to automated systems, especially in terms of reliance and trust [9]. The extent to which the drivers may be willing to utilize these systems can be greatly impacted by the driver's perception of the systems reliability [10], [11].\nHence it is vital to devise and build an ADAS which can support drivers and produce synergy between humans and machines. This necessity brings spotlight to systems that can understand and react to the ever-changing landscape of driving to improve overall safety and efficiency of driving. Driver training and understanding of these technologies are other important factors which also affect the working of the ADAS. Evidence suggests that training can impact drivers'"}, {"title": "II. LITERATURE REVIEW", "content": "Scene understanding is very significant in transportation, furthered by the need for the interaction of a vehicle with the road and its surroundings [14], [15]. In relation, the work by Pham et al. regarding joint geometric and object segmentation stipulates the significance of proper identification and classification of objects within indoor scenes into the settings of transportation [16]. The improvement in accuracy while detecting objects helps vehicles to make better interpretations and subsequent decisions.\nThese basic methods are all about the employment of convolutional neural networks for scene analysis and classification according to their semantic contents. CNNs are pretty good in extracting features from images, thus providing a way of identifying objects and their relations within a scene. For example, Li et al. have indicated that deep learning approaches have dominated the evolution of scene understanding, especially with CNNs that outperform the traditional approaches such as K-means and SVMs [17]. Ni et al. gave more weight to this claim when they proposed a deep network-based improved method of scene classification specifically designed for the self-driving car, enhanced by a novel Inception module in both global and local features extraction [18]. The robustness and accuracy of feature fusion tremendously upgrade the performance of scene classification for the safe driving of an ego vehicle in a complex real environment.\nComplementary to CNNs, the integration of methods for 3D scene understanding will play an increasingly important role in tasks such as autonomous vehicles. The model proposed by Han et al. predicts a 3D scene graph that describes entities within a scene and gives semantic relationships among these entities for enhanced environmental understanding [19]. This approach bridges an important gap in the literature on scene understanding, because traditional methods are usually insensitive to the relations between objects. Moreover, LiDAR combined with deep learning has been demonstrating encouraging results on the estimation of scene flow and relative pose, which was introduced by Li et al. in the study of LiDAR odometry for unmanned ground vehicles [20]. Taken together, these technologies enable real-time processing and, therefore, for precise mapping in dynamic environments, which is very important for good navigation.\nAnother challenge was the proper detection and classification of various road conditions, from potholes to wet surfaces, through novel machine learning. Vernekar et al. developed another type of pipeline using pretrained models to analyze live images captured from traffic cameras. This refines the existing feature extraction process to enhance the accuracy in detection [21]. This approach not only enhances the reliability of understanding scenes, but also contributes significantly to the safety of transportation systems by enabling timely responses to hazardous situations.\nThe use of synthetic data generation to train Al models is increasing in the field of scene understanding. Holst et al. talk about generating synthetic training data for robotics applications, which can be adapted for various transportation scenarios [22]. Using simulations of diverse environments and conditions, researchers can create robust datasets that"}, {"title": "III. CONTRASTIVE LANGUAGE-IMAGE PRE-TRAINING (CLIP)", "content": "CLIP models have significantly advanced the field of vision-language understanding by learning visual representations from natural language supervision [24]. One advantage of using CLIP is that it can be deployed on edge devices with careful consideration of hardware capabilities, model size, and optimization techniques. While smaller variants (e.g., ViT-B/32) are more practical for edge use, larger variants like ViT-L/14 may require significant optimizations or hybrid deployment strategies. The choice ultimately depends on the application's real-time requirements, available resources, and the complexity of the scene understanding tasks. Various CLIP model architectures have been developed, each with distinct characteristics concerning model size, processing speed, VRAM requirements, and architectural design. This document compares five prominent CLIP models-ViT- B/32, ViT-B/16, ViT-L/14, RN50, and RN101\u2014in terms of their number of parameters, processing speed, VRAM requirements for embedding, and architectural differences. The goal is to provide insights into their suitability for different applications, aiding in the selection of an appropriate model based on specific requirements. The following is a comparison between the different CLIP models."}, {"title": "A. Number of Parameters", "content": "The number of parameters in a model is a crucial factor influencing its computational requirements and potential performance. Models with more parameters can capture more complex patterns but require more computational resources for training and inference."}, {"title": "B. Processing Speed (Frames per Second)", "content": "Processing speed is a critical factor, especially for real- time applications. It depends on the model's complexity, computational requirements, and the hardware used."}, {"title": "C. Models Architecture", "content": "The architecture of a model influences its ability to capture features, computational efficiency, and suitability for different tasks. The CLIP models considered here are based on two primary architectures: Vision Transformers (ViT) and ResNets.\nVision Transformers apply transformer architectures to sequences of image patches, leveraging self-attention mechanisms to model global relationships within an image [25]. Key architectural details of the ViT-based CLIP models are provided in TABLE III. Note that the number of patches is calculated by dividing the image dimensions by the patch size and squaring the result."}, {"title": "D. VRAM Requirements for Embedding", "content": "The amount of VRAM (Video Random Access Memory) required for embedding using each model is a crucial consideration, especially when deploying models on GPUs with limited memory. VRAM requirements can vary based on factors such as batch size and implementation details."}, {"title": "E. Summary and Recommendations", "content": "TABLE VI shows the pros, cons, and recommendations for the different CLIP models. To select a model between them, it is important to consider several factors. The first one is resource availability, which includes ensuring that computational resources (e.g., GPU memory, processing power) are sufficient for the chosen model, especially for larger models like ViT-L/14 and assessing VRAM availability to prevent out-of-memory errors during embedding. Secondly, application requirements, in which determining whether speed, accuracy, or memory efficiency is the priority and understanding that real-time applications may benefit from faster models with lower VRAM requirements, like ViT-B/32 or RN50. Finally, model familiarity, in which it is recommended to select a model architecture that aligns with the development team's expertise, while understanding that ResNet models may be preferable for those more familiar with convolutional neural networks and ViT models may offer advantages in tasks where capturing global context is important.\nNonetheless, selecting the appropriate CLIP model involves balancing the application's performance requirements with the available computational resources, including VRAM. CLIP ViT-B/32 and CLIP RN50 are recommended for applications prioritizing speed and memory efficiency. CLIP ViT-B/16 and CLIP RN101 offer a middle ground between performance, resource demands, and VRAM usage. For tasks where maximum accuracy and detail are critical, CLIP ViT-L/14 is ideal, provided that sufficient computational resources and GPU memory are available."}, {"title": "IV. PROPOSED METHODOLOGY", "content": "Our methodology aims to establish a real-time scene understanding framework that serves as a critical component for advanced driver assistance systems. These systems necessitate rapid and accurate comprehension of driving environments to provide drivers with timely, context-aware advice. Our framework is built on two key pillars:"}, {"title": "A. Embedding Scene Images with Clip", "content": "We leverage the Contrastive Language-Image Pre-Training (CLIP) model to embed scene images into a high-dimensional vector space. CLIP is a robust model known for its ability to associate textual descriptions with visual data effectively. It captures semantic relationships between visual elements and their corresponding textual attributes, transforming complex visual scenes into embeddings suitable for computational processing. These embeddings allow our framework to map driving scenes into a representation that preserves the underlying semantic structure, making it ideal for real-time retrieval and analysis."}, {"title": "B. Efficient Indexing with FAISS", "content": "To manage and search through the high-dimensional embeddings generated by CLIP, we incorporate Facebook AI Similarity Search (FAISS). FAISS is an advanced library optimized for similarity search and clustering of dense vectors. Known for its scalability and speed, FAISS is designed to handle large datasets with high-dimensional data efficiently. By utilizing FAISS, we index the CLIP-generated embeddings, enabling rapid retrieval of the most relevant scenes. This integration is crucial for real-time applications where speed and precision are essential, as FAISS supports approximate nearest neighbor (ANN) search, ensuring quick access to relevant information even in large-scale scenarios.\nThe integration of CLIP and FAISS creates a powerful real- time scene understanding framework. CLIP converts visual data into a searchable vector space, while FAISS ensures that the retrieval process is both swift and accurate. Together, they provide the computational backbone for processing and interpreting driving scenes promptly, empowering the system to deliver timely and precise advice to drivers, enhancing safety and decision-making."}, {"title": "C. Inference Phase", "content": "During the inference phase, the framework processes a test scene through the CLIP model to generate its embedding. This embedding is then queried in the FAISS index to identify the nearest neighbor scenes. The textual attributes associated with these nearest neighbors are retrieved, and for each attribute, the system predicts the value based on the majority vote among the retrieved neighbors. This process ensures that the predictions are both data-driven and contextually relevant."}, {"title": "D. Model Evaluation and Fine-Tuning", "content": "Our methodology is designed to evaluate various CLIP model variants across different numbers of nearest neighbors (NNs) and compare their performance against the baseline model, GPT-40, in a zero-shot, in-context learning setting. Models are ranked based on their precision and recall performance across multiple attributes.\nFollowing the evaluation, the top-performing CLIP models are selected for fine-tuning. Fine-tuning aligns the embedding space more closely with the semantic requirements of scene understanding, allowing the model to capture subtle nuances in driving contexts. The fine-tuned models are then used to embed the training data into a newly refined vector space. A new FAISS index is built using these embeddings, enabling the retrieval of nearest neighbors for test scenes that have been embedded using the fine-tuned models. This alignment significantly improves the model's ability to predict scene attributes with greater precision and recall. The combination of CLIP and FAISS not only ensures real-time performance but also provides a flexible framework that can adapt to new scenarios through fine-tuning. By aligning the embedding space to the semantics of scene understanding, the methodology enhances the accuracy and robustness of predictions.\nTo address the limitation of the CLIP model's text encoder, which imposes a 77-token restriction, we implemented an efficient text description generation approach using the following method. This method encodes textual descriptions by coding attribute values using numbers then converting them into a concise string, joining them in a predefined order with commas to ensure the text remains compact and within the token limit. By focusing solely on the attribute values, the encoded descriptions effectively preserve the semantic relationships between similar images, ensuring they are clustered closely in the latent space. Meanwhile, the detailed textual descriptions of the attributes are stored separately in a file, indexed for retrieval through FAISS. This dual approach guarantees computational efficiency while maintaining the richness of the original attribute descriptions for downstream tasks."}, {"title": "E. Prompt Engineering", "content": "The prompt is designed for GPT-40 to analyze driving- related images from the perspective of an ego vehicle and classify detected features into predefined categories, as shown in FIGURE 1. It provides a structured framework for scene understanding by assigning values to categories such as road attributes, weather conditions, and complex road structures. However, compared to the CLIP retrieval system, this approach removes many attributes because the indexed images in this analysis represent only a single level or class. Additionally, temporal features like \"Stages\" (e.g., Approaching, Entering, Passing) are converted into a binary format for simplicity. For instance, categories such as \"Rail Crossing\" are represented as '0' if not detected and '1' if detected, effectively merging the temporal stages into a single binary outcome. This conversion aligns with the frame-based nature of the analysis in this paper, focusing on frame-level detection and classification rather than temporal sequences. This streamlined representation ensures the framework is optimized for frame-based image retrieval and analysis, critical for real-time and retrospective applications."}, {"title": "V. DATASET DESCRIPTION", "content": "The Honda Scenes Dataset (HSD) is a large-scale annotated dataset designed to support dynamic traffic scene classification. Comprising 80 hours of high-quality driving video data collected from the San Francisco Bay Area, HSD offers a rich variety of scenarios for training and testing. The dataset includes 11 road place classes, such as 3-way intersections, 4- way intersections, zebra crossings, and construction zones. It also spans four road environments (rural, urban, highway, and ramp) and four weather conditions (rainy, sunny, cloudy, and foggy). Detailed annotations are available in JSON and CSV formats, capturing temporal and frame-level attributes like road conditions, surface conditions, and event sub-classes such as \"approaching,\" \"entering,\" and \"passing.\" Specialized sub- classes for merges and branches allow for granular analysis, enhancing the dataset's utility in various traffic-related research contexts.\nThe preprocessing methodology involves systematic steps to prepare the data for analysis, as shown in FIGURE 2. The dataset is pre-split into training and testing sets to ensure no spatial overlap between trips, maintaining the independence of"}, {"title": "VI. EXPERIMENTAL WORK", "content": "In this section, we address the first research question: how well do pre-trained models perform in the context of scene understanding? Our primary focus is to identify which of these relatively lightweight models, capable of real-time inference, achieve higher rankings compared to other CLIP models studied here and GPT-40 in zero-shot settings. To this end, we evaluated the performance of these models in terms of precision and recall across different class levels, aiming to pinpoint their strengths and limitations. Based on that, we ranked the models using the Euclidean distance between their positions in the precision-recall space and the optimal (1,1) point, providing a clear metric for performance comparison. This distance serves as a metric for ranking the models, with smaller distances indicating better performance. For visualization, we present a heatmap where each cell represents the distance of a model for a specific attribute-class combination. It is important to note that the CLIP models used in this study rely on retrieval-based mechanisms, where the number of nearest neighbors (indicated in parentheses) is adjusted for each model. Once the best- performing CLIP models are identified, we proceed to the fine- tuning phase for further performance enhancement."}, {"title": "A. Pre-Trained Models Results", "content": "In this section, we address the first research question: how well do pre-trained models perform in the context of scene understanding? Our primary focus is to identify which of these relatively lightweight models, capable of real-time inference, achieve higher rankings compared to other CLIP models studied here and GPT-40 in zero-shot settings. To this end, we evaluated the performance of these models in terms of precision and recall across different class levels, aiming to pinpoint their strengths and limitations. Based on that, we ranked the models using the Euclidean distance between their positions in the precision-recall space and the optimal (1,1) point, providing a clear metric for performance comparison. This distance serves as a metric for ranking the models, with smaller distances indicating better performance. For visualization, we present a heatmap where each cell represents the distance of a model for a specific attribute-class combination. It is important to note that the CLIP models used in this study rely on retrieval-based mechanisms, where the number of nearest neighbors (indicated in parentheses) is adjusted for each model. Once the best- performing CLIP models are identified, we proceed to the fine- tuning phase for further performance enhancement."}, {"title": "B. Fine-Tuned Models Results", "content": "As CLIP ViT-B/32 (5) and CLIP ViT-L/14 (5) showed the best performance among all models, we chose to fine-tune them. The training data preparation involves pairing images with textual descriptions generated from associated attributes, extracted from CSV files. Each image is processed and matched with a descriptive text string representing its attributes to create input pairs for the model. The training process employs the Adam with Weight Decay optimizer with a learning rate of $10^{-5}$. The model is trained over 8 epochs using mixed- precision training to enhance computational efficiency. Performance is monitored by tracking loss values at both batch and epoch levels, ensuring consistent evaluation throughout the training.\nThe heatmap illustrates the distances between the precision- recall points of various models and the ideal point (1,1) in the precision-recall space, aggregated across attributes and scales. The performance of the models is represented with a color gradient, where lower distances (closer to blue) indicate better alignment with the ideal, and the top three performing models for each attribute-class pair are highlighted. The comparison of fine-tuned and non-fine-tuned CLIP ViT-B/32 (5) and CLIP ViT-L/14 (5) models, as well as GPT-40 in its zero-shot configuration, reveals key insights into model performance and adaptability."}, {"title": "VII. DISCUSSION", "content": "To make a direct comparison across all attributes,"}, {"title": "VIII. CONCLUSIONS", "content": "This study presents a robust framework for real-time scene understanding in, leveraging the capabilities of the CLIP model, which can be used for real-time deployment on edge devices. A CLIP-based retrieval system was fine-tuned for real-time ADAS applications, enabling accurate classification of dynamic driving scenes. The model was benchmarked against state-of-the-art in-context learning methods, including GPT-40 in zero-shot scenarios, where it demonstrated superior performance, particularly in complex scenarios.\nComprehensive frame-level evaluations were conducted using the Honda Scenes Dataset, which includes 80 hours of annotated driving videos capturing diverse road and weather conditions. These evaluations highlighted the robustness of the framework across varied environments. Additionally, the integration of language-based supervision with visual scene recognition enhanced the semantic understanding of traffic scenes, contributing valuable insights to road safety and autonomous driving research.\nBy embedding scene images into a high-dimensional vector space using CLIP and indexing them with FAISS, the framework enabled rapid and precise retrieval of relevant scenes. This feature is critical for real-time ADAS applications. Fine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, improved their performance by aligning embeddings with the semantic requirements of scene understanding. Notably, the fine-tuned ViT-L/14 model achieved the highest F1-score of 91.1%, followed closely by ViT-B/32 at 90.5%.\nPrecision and recall analysis revealed trade-offs between these metrics, with different models emphasizing either precision (e.g., ViT-L/14) or recall (e.g., ViT-B/16). This finding underscores the need to select models based on the specific requirements of a given application. The lightweight nature of models like ViT-B/32 and ViT-L/14, combined with their computational efficiency, positions them as ideal candidates for deployment on edge devices in resource-constrained environments such as autonomous vehicles.\nAs fine-tuning significantly enhanced the models' ability to classify complex scenes, this positions CLIP as a scalable, accurate, and efficient solution for dynamic scene understanding. Furthermore, the study's exploration of multimodal techniques, including advanced indexing and semantic-rich embeddings, provides a solid foundation for future advancements in AI-driven scene classification systems. Practically, this study establishes a pathway toward safer, smarter, and more context-aware autonomous driving systems. The integration of advanced techniques and models like CLIP ensures precise, real-time decision-making capabilities, enabling more effective ADAS and paving the way for further innovations in autonomous vehicle technology."}]}