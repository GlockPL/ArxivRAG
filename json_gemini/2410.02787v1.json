{"title": "Navigation with VLM framework:\nGo to Any Language", "authors": ["Zecheng Yin", "Chonghao Cheng", "Yinghong Liao", "Zhihao Yuan", "Shuguang Cui", "Zhen Li"], "abstract": "Abstract-Navigating towards fully open language goals and\nexploring open scenes in a manner akin to human exploration\nhave always posed significant challenges. Recently, Vision Large\nLanguage Models (VLMs) have demonstrated remarkable ca-\npabilities in reasoning with both language and visual data.\nWhile many works have focused on leveraging VLMs for\nnavigation in open scenes and with open vocabularies, these\nefforts often fall short of fully utilizing the potential of VLMs\nor require substantial computational resources. We introduce\nNavigation with VLM (NavVLM), a framework that harnesses\nequipment-level VLMs to enable agents to navigate towards any\nlanguage goal specific or non-specific in open scenes, emulating\nhuman exploration behaviors without any prior training. The\nagent leverages the VLM as its cognitive core to perceive\nenvironmental information based on any language goal and\nconstantly provides exploration guidance during navigation\nuntil it reaches the target location or area. Our framework\nnot only achieves state-of-the-art performance in Success Rate\n(SR) and Success weighted by Path Length (SPL) in traditional\nspecific goal settings but also extends the navigation capabilities\nto any open-set language goal. We evaluate NavVLM in richly\ndetailed environments from the Matterport 3D (MP3D), Habitat\nMatterport 3D (HM3D), and Gibson datasets within the Habitat\nsimulator. With the power of VLMs, navigation has entered a\nnew era.", "sections": [{"title": "I. INTRODUCTION", "content": "To explore an utterly novel environment like humans\nis a capability that agents have not achieved so far. The\ncritical factor that matters most is missing: the ability to\nreason about the information from the current environment,\nmuch like the human brain does. For instance, to complete\ntask \"taking a bath,\" we know intuitively that we need to\nreach a restroom and can identify one based on the presence\nof specific furniture. Sensing where and whether a place is\nsuitable for the task and navigating to it is challenging for\ncurrent navigation systems, yet it is essential for achieving\nhuman-level navigation.\nWith the development of Vision-Language Models\n(VLMs) [1]\u2013[4], the reasoning challenge can now be promis-\ningly addressed. The VLMs can serve as cognitive core of\nthe agent and give guidance during the navigation [5] as they\nhave the ability to perceive the environment. Several works\n[6]\u2013[8] are dedicated to integrate VLM in navigation, but they\nhave not utilized VLMs in a simple, neat, and effective way\nto guide navigation, nor have they fully exploited the VLM's\nability to extend the language goal to any language (detailed\ncomparison in IV). For example, with fixed positions in a\nclosed scene, [9] uses VLMs to describe areas and build\na room graph, and uses language modalities through entire\nnavigation, which sacrifices the open exploration capability\nthat VLMs inherently possess. [6], [7] attempted to convey"}, {"title": "II. OPEN SET NAVIGATION PROBLEM DEFINITION", "content": "For an agent initialized in any random location within an\nopen scene and possessing no prior knowledge, the Open Set\nNavigation is to enable the agent to autonomously explore\nand navigate to an open language goal without any addi-\ntional human instructions [10], [16]\u2013[18]. The completion of\nsuch tasks is evaluated using two primary metrics: Success\nRate (SR) and Success weighted by Path Length (SPL) [19].\nIt is important to note that, in addition to traditional very\nspecific goals, such as a specific object category with limited\nlocation information (e.g., \"apple\" or \"bed in the living\nroom\"), our framework is capable of handling extremely\nambiguous language goals. These can include phrases like\n\"a corner in the kitchen\" or \"somewhere I can rest,\" as well\nas completely out-of-domain goals, such as \"laboratory 208\u201d.\nWe demonstrate state-of-the-art statistical performance\nfor specific goals in the section V and, due to the sparsity\nof open-goal navigation datasets, can only present cases for\nnon-specific goals1(b). However, these examples excellently\ndemonstrate the capability of our framework and the potential\nfuture."}, {"title": "III. NAVVLM FRAMEWORK", "content": "This section explains the components of our framework\nand how they cooperate with each other. The overall process\nis shown in Figure 2 and a detailed demonstration of the\nprocess is provided in Algorithm 1. In the Algorithm 1,the\nOt represents the observation from environment at step t, p is\nprompt, E is the existing navigation system, T is maximum\nstep number, SLAM is the map construction and mapping\nmodule, M is the top-down map, G is the VLM guidance\nin M, Render is the heuristic rendering module that brings\nVLM guidance into image, stg is short-term-goal. The stg\nis the short term destination area given either by VLM or\nexisting navigation system E."}, {"title": "A. Interact with Environment", "content": "Every move in the environment triggers the agent to\nreceive an observation (RGB-D) from the environment. After\nthe agent receives the information from the environment,\nthe agent will ask VLM 2 prompts in order. Determine to\nterminate by prompting if the goal is close enough in current\nthe observation and identify guidance by prompting what\narea in the image should the agent go to in order to reach\nthe goal. If the VLM provide guidance, the agent will activate\nthe path planner to go the the recommended guidance area.\nIf the VLM does not provide valid information or explicitly\norders to explore more, the navigation process temporarily\nreverts to existing navigation strategy."}, {"title": "B. VLM Guidance", "content": "The VLM acts as the cognitive core of the agent, playing\na key role in mimicking and reasoning like humans do. With\nfew simple prompts such as \"to get the goal which direction\nshould I go?\", the VLM could provide direction at current\nobservation to reach the final language goal. We select basic\ndirections such as \"left, right, go straight, explore more\" as\nthe VLM guidance key words.\nTo convert guidance keywords from VLM into actionable\nnavigation information, we we use a simple image rendering"}, {"title": "C. SLAM", "content": "During exploration, the agent continuously performs\nSLAM (Simultaneous Localization and Mapping) to create\na top-down map of the explored area. This map is used by\nthe agent to avoid obstacles and move to the area indicated\nby the VLM or existing navigation. Using RGB and depth\ndata, we employ [18] to construct the top-down obstacle map.\nSpecifically, [18] builds a point cloud based on depth data\nand constructs voxels based on the point cloud. Then, we\ncompress the voxels vertically into a top-down map, which\nincludes everything observed, such as objects and VLM\nguidance."}, {"title": "D. Path Planning", "content": "Path planning involves moving the agent from one loca-\ntion to another while avoiding obstacles. As the top-down\nmap is constantly updated during exploration, we use the\nfast marching method (FMM) [24] for path planning, as it is\nhighly efficient with a constructed map. The path planning\nmodule provides specific actions, such as \"turn right\" at the\ncurrent step, and ultimately guides the agent to the target\narea provided by either the VLM or the existing navigation\nsystem."}, {"title": "E. Navigation Termination", "content": "The termination of the task can occur under the following\nconditions:\n\u2022 Reaching target area termination. The agent reaches\nthe goal area as guided either by VLM or existing\nnavigation system, with a threshold, determined by how\nclose the agent is to the guided goal. The guidance area\nwill be constantly updated during exploration.\n\u2022 VLM termination. The VLM determines that the agent\nshould stop here based on the current images. This is\nparticularly suitable for ambiguous language goal such\nas \"a corner in the kitchen\", as such goals are not\nspecific to any objects and can only be processed with\nVLM. With the reasoning capability of VLM, the agent\ncan determines whether the navigation task is complete\nbased on the surrounding images observations, similar\nto human decision-making.\n\u2022 Termination from existing navigation and max step\ntermination."}, {"title": "To summarize, the core components of our scheme are:", "content": "tiny VLM serving as the cognitive core and guide the agent\nnavigation over the existing navigation system, SLAM for\ntop-down map construction and information projection, and\npath planning instructing specific action such as \"turn left\",\n\"turn right\" for agent to move while avoiding obstacles.\nThese components work together to perform intelligent nav-\nigation in human level in open environment. Note these\ncomponents are not tightly coupled with our work and can\nbe easily replaced accordingly."}, {"title": "IV. COMPARISON WITH OTHERS", "content": "This section highlights the differences between our\nframework and several recent prominent works.\n[25] employs large models to enhance baseline navi-\ngation models, but it uses the VLM solely for selecting and\nranking multiple frontier exploration spots at each step based\non the language goal, rather than directly integrating the\nVLM into the navigation process, which greatly restricts the\ncapability of the VLM. As the explored area expands, the\nnumber of frontier spots increases, which can slow down the\nexploration process. Furthermore, this work relies on multiple\nvery large language models, such as GPT-4. In contrast,\nour framework utilizes a single small VLM [26] and fully\nexploits its capabilities in navigation, enabling intelligent\nexploration at a minimal cost.\nIn [27], the VLM serves as a high level planning system.\nThe language goal has to be specific it requires multi-round\ndialog with the user. Similarly, in [28], its target objects\nare also described through a multi-round dialog with a user.\nOur framework, however, can navigate to any ambiguous\nlanguage goal without requiring human conversations.\n[11] utilize [29] to compute similarities between the RGB\nobservation ahead and the language prompt to rank frontier\nexploration points, mimicking human exploration without\nthe need for human conversations. However, the language\nprompt of the goal must be very specific, and the approach\nis fundamentally a prioritized frontier exploration. Although\nthe ranking process imitates human thinking, the exploration\npoints are still limited by frontier exploration. Our frame-\nwork, on the other hand, can directly provide guided areas\nbeyond the frontier circle, which is more effective for path\nplanning and more closely resembles human-level behavior\nand thus achieving higher performance.\n[10], [30] utilize [31] and [32] for image goal and\nlanguage goal, successfully extending the modality to include\nimages. However, the language or image goals are limited\nto very specific targets, and the navigation decision policy\nis made using Reinforcement Learning [33]. This approach\nstruggles to exhibit human-level exploration behavior when\nfaced with out-of-domain environments compared to the\ncapabilities of VLMs.\n[6] and [7] are VLMs applicable in navigation and\nother domains. For navigation, these models require sub-\nstantial inputs such as point clouds, point cloud features,\nand historical latent embeddings. Additionally, the training,\nfine-tuning, and loading processes are resource-intensive for\nedge devices. These are heavyweight models that cannot be\ndeployed on edge equipment, which is the primary device\nfor navigation. They are end-to-end models that do not\nintegrate well with existing effective exploration techniques\nlike frontier-based exploration. Our framework can cooperate\nwith existing navigation systems while integrating at minimal\ncost and achieving high performance in traditional specific\nlanguage goals. Moreover, it can extend the goal to a fully\nopen language set.\nIn summary, our framework offers unique features that\nrecent VLM-based works lack:\n\u2022 Cost-Free: Utilizes a single small VLM, enabling low-\ncost integration while achieving high performance.\n\u2022 Human-Level Exploration: Capable of navigating to\nambiguous language goals in almost human level man-\nner and without human conversations.\n\u2022 Open-set Language Goal: Extends the language goal\nset from traditional specific goals to a fully open lan-\nguage set.\n\u2022 Collaboration with Existing Systems: Cooperates with\nexisting navigation systems while integrating at minimal\ncost."}, {"title": "A. Experiment Setup", "content": "We conducted our experiment in Gibson [40], HM3D\n[41], and MP3D [42] scenes, and implemented with Habitat\n[43], [44] simulation from Meta. We select VLM named\nminicpm-llama3-v2.5 as the cognitive core of the agent, a\nversion of minicpm [26] series, which is a tiny VLM capable\nof deployment on a mobile device. The entire experiment\ncan be conducted in only one single RTX3090 GPU in half-\nprecision mode. The agent's actions included moving forward\n0.25 meters, turning right 30 degrees, turning left 30 degrees,\nand terminating. We used SPL (Success weighted by Path\nLength) and SR (Success Rate) [19] as evaluation metrics."}, {"title": "B. Performance Analysis", "content": "Our framework outperforms all baselines in SPL and\nachieves competitive SR scores across all datasets. SPL\nmeasures the average closeness of the agent's path to the\noptimal path, which is the geodesic shortest path from the\nstart position to the goal position. SR measures the likelihood\nthat the agent successfully navigates to the language goal.\nThe comparison of SPL shows that with the VLM as the\ncognitive core, the agent is most likely to find the \"best\" path\nto the goal, significantly surpassing baseline methods. This\nis because the VLM thinks and recognizes possible locations\nin the image to find the goal. For example, to find a bowl,\nthe VLM guides the agent towards the kitchen area once a\ndistant view of the kitchen appears in the observation, as it\nknows bowls are more likely to be found in kitchens. This\nlogic mirrors human exploration."}, {"title": "C. Ablation Study", "content": "The SemExp [18] is the existing navigation system we\nbuilt upon, and we compare our performance against it. The\nSPL is boosted by an absolute 22% (64% relative increase)\nand SR is lifted by 6.6% (10% relative increase), representing\na significant performance improvement."}, {"title": "D. VLM Integration", "content": "As mentioned in Section III-B, currently no existing\nmethods are directly applicable to our task. We explored\nseveral integration options: 1. End-to-end, where the VLM\ndirectly provides movement commands based on the current\nobservation; 2. Precise location, where the VLM directly\nprovides bounding boxes or masks in the RGB image,\nguiding the agent to the area; 3. Rough location, where the\nVLM provides navigation directions (e.g., left, right) based\non the RGB image and renders the target area, which is\nthen projected into the top-down approximate area to guide\nnavigation.\nThrough experimentation, we found that the Rough lo-\ncation method integrates the VLM more effectively than the\nother options, as the equipment-level VLM excels at logical\nreasoning and language processing but struggles with precise\nlocalization and end-to-end control."}, {"title": "E. Empirical Observations", "content": "In our experimental setup, when the image contains a\ndistant view of any hint related to the language goal (such\nas a far view of the kitchen when the goal is to find the\nkitchen), the VLM recognizes it and successfully leads the\nagent in many cases, which is the reason why the agent are\nmore likely to find the optimal path. This behavior aligns with\nthe agent's ability to \"think\" and act in a manner similar to\nhuman exploration.\nThe termination criteria for the VLM become crucial\nwhen dealing with open-set language goals. No existing nav-\nigation method can handle such tasks except by leveraging\na VLM. Our approach can significantly outperform existing\nlanguage-image similarity methods in terms of dealing with\nextremely ambiguous language goals."}, {"title": "VI. FEATURES OF OUR FRAMEWORK", "content": "1) Any Language Goal: Our framework achieves nav-\nigation to both non-specific and specific language goals at\na human level in open scenes with zero-shot learning for\ngoals. For specific language goals, it requires fewer steps,\nas the VLM captures information from the environment and\nperforms human-like reasoning during navigation. For non-\ntraditional goals, our framework can fully exploit VLMS\ncapability to navigate to a fully open set of goals, whereas\nrecent works cannot. With the VLM acting as the cognitive\ncore of the agent, our framework can navigate to anything\nspecific or non-specific, as long as it can be described in\nhuman language.\n2) Zero-Cost Integration: Every component of our\nframework requires no training or fine-tuning, eliminat-\ning concerns about collecting or generating new synthetic\ndatasets and avoiding the costs associated with training. It\nis a plug-and-play method that can seamlessly integrate with\nvarious existing navigation systems without any additional\ncost.\n3) Intelligent Navigation: The VLM serves as a logical\npath navigator and object finder in the current observation.\nOur framework can successfully capture various informa-\ntion from the environment, take fewer steps to navigate to\nthe language goal, resulting in a noticeable performance\nimprovement. For instance, for the goal \"cooker in open\nkitchen,\" the agent will go straight to the kitchen where\ncooker might locate once a distant view of the kitchen\nappears in the observation, as the navigation is motivated\nby the VLM, a model with human-level reasoning abilities.\n4) Promising Capability: The ultimate capability of our\nwork is limited by the reasoning ability from VLM, and\ninfluenced by the SLAM and VLM guidance accuracy and\nthe capabilities of the movement hardware. The VLM pro-\nvides guidance to the agent, allowing it to explore regions\nin a manner similar to how humans would. If the VLM\nreasons well (which most modern VLMs do) and the VLM\nconveys accurate guidance, the exploration is likely to be\nnear-optimal. We merely applied a tiny VLM in our frame-\nwork and it can boost the navigation with a significant\nimprovement. Considering the rapid development of VLMs"}, {"title": "5) Modular Replaceable Components:", "content": "The core compo-\nnents of our scheme are VLM as the cognitive core, SLAM\nas top-down mapping and several path planning component.\nThese component are non-coupled. The VLM can either be\nhuge language model or tiny distilled model. The SLAM\ncomponent can either be traditional or neural-based method,\nas long as it helps project the RGB image observation and\nVLM guidance area from RGBD to the map. The path\nplanning component can vary according to the scene we need\nas long as it could provide valid actions and paths in the\nscene."}, {"title": "VII. FUTURE", "content": "1) Need for Open Language Datasets: There is a need\nfor open language datasets. For non-specific object goals,\nsuch as \"somewhere I can sit and eat,\" the location could\nbe a corner with a table and chairs, or a place with a couch\nand a surface suitable for placing a meal. Currently, there are\nno existing datasets that provide information for such open\nlanguage goals.\n2) Heuristic Guidance Area: Our guidance area is ren-\ndered in the RGB image using a simple heuristic rule. A more\naccurate language and long-term-goal-driven segmentation\nmodel would undoubtedly improve the performance of our\nframework. Current language driven segmentation works are\nmore focused on object-anchored alignment segmentation\nsuch as [21]\u2013[23] rather than area-oriented segmentation.\nAccurate area oriented language driven segmentation is very\nmuch helpful in navigation.\n3) Accurate SLAM Module: An accurate SLAM module\nis preferred. Our current SLAM component is adapted from\n[18], [30], which is primarily designed to handle flat floor\nenvironments and may struggle when faced with multi-level\nscenes such as duplex apartments and stairs. Objects higher\nthan a certain threshold are often incorrectly classified as ob-\nstacles, limiting the system's ability to handle environments\noutside of typical home settings."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we propose Navigation with VLM\n(NavVLM), a framework that enables the agent to explore\nthe open scene in zero-shot settings in human exploration\nmanner. The tiny equipment-level VLM of agent play as\na cognitive core and cooperates with existing navigation\nsystem and constantly give exploration guidance throughout\nthe navigation based on the language goals, thus enabling the\nagent to navigate to specific language goal and non-specific\nopen set language goal. With its ease of integration, our\nframework can boost existing navigation system at least cost\nand achieve state-of-the-art performance on SPL and SR in\nvarious vivid navigation datasets. Due to the scarcity of open-set navigation datasets, the full potential of our framework's\nopen-set language capabilities remains largely unexplored.\nWe hope that this framework will serve as a potential proto-\ntype for future advancements in VLM navigation, as there is\nsignificant room for improvement and further exploitation."}]}