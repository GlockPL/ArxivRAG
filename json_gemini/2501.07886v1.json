{"title": "ITERATIVE LABEL REFINEMENT MATTERS MORE THAN\nPREFERENCE OPTIMIZATION UNDER WEAK SUPERVISION", "authors": ["Yaowen Ye", "Cassidy Laidlaw", "Jacob Steinhardt"], "abstract": "Language model (LM) post-training relies on two stages of human supervision:\ntask demonstrations for supervised finetuning (SFT), followed by preference com-\nparisons for reinforcement learning from human feedback (RLHF). As LMs be-\ncome more capable, the tasks they are given become harder to supervise. Will\npost-training remain effective under unreliable supervision? To test this, we sim-\nulate unreliable demonstrations and comparison feedback using small LMs and\ntime-constrained humans. We find that in the presence of unreliable supervision,\nSFT still retains some effectiveness, but DPO (a common RLHF algorithm) fails\nto improve the model beyond SFT. To address this, we propose iterative label\nrefinement (ILR) as an alternative to RLHF. ILR improves the SFT data by us-\ning comparison feedback to decide whether human demonstrations should be\nreplaced by model-generated alternatives, then retrains the model via SFT on\nthe updated data. SFT+ILR outperforms SFT+DPO on several tasks with unre-\nliable supervision (math, coding, and safe instruction-following). Our findings\nsuggest that as LMs are used for complex tasks where human supervision is un-\nreliable, RLHF may no longer be the best use of human comparison feedback;\ninstead, it is better to direct feedback towards improving the training data rather\nthan continually training the model. Our code and data are available at https:\n//github.com/helloelwin/iterative-label-refinement.", "sections": [{"title": "1 INTRODUCTION", "content": "Language models (LMs) learn rich knowledge when pretrained on internet-scale corpora (Achiam\net al., 2023; Dubey et al., 2024). To elicit their full capabilities and align them to human values,\nthey are typically post-trained with two types of human supervision: task demonstrations for the\ninitial supervised finetuning (SFT) stage, followed by preference comparisons used in reinforcement\nlearning from human feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022) via algorithms like\nproximal policy optimization (PPO) (Schulman et al., 2017) or direct preference optimization (DPO)\n(Rafailov et al., 2024; Dubey et al., 2024).\nAs LMs continue to advance, they are trained to solve complex tasks that are difficult for humans to\nsupervise (Amodei et al., 2016; Leike & Sutskever, 2023; Wen et al., 2024). For example, humans\nare often imperfect at coding, producing bugs that models like Github Copilot learn to imitate (Asare\net al., 2023). Similarly, industrial-level ChatGPT training data rated as \"flawless\" has recently been\nfound to contain flaws (McAleese et al., 2024). As task complexity increases, human supervision may\nbecome even less reliable. Thus, it is imperative to understand how both stages of current post-training\npipelines (SFT + RLHF) perform under unreliable supervision.\nStudying this is difficult because tasks where human supervision is unreliable are by nature difficult\nto obtain ground truth for. Ideally, we want tasks with known ground truth where we can also collect\nhuman-like unreliable data. We employ two approaches to simulate this unreliable supervision. First,\nwe use smaller LMs that often make mistakes, in line with Burns et al. (2023) and Dubois et al.\n(2024). Second, we recruit human workers to label data under time constraints. For either of these two\ntypes, we can use it to simulate either unreliable demonstrations or unreliable comparisons (e.g. for"}, {"title": "2 RELATED WORK", "content": "Scalable oversight and weak-to-strong generalization. Scalable oversight aims to develop methods\nto supervise AI systems on tasks that are difficult for humans (Amodei et al., 2016; Bowman et al.,"}, {"title": "3 PROBLEM DEFINITION: POST-TRAINING WITH UNRELIABLE SUPERVISION", "content": "Training LMs to be useful assistants involves two stages: pre-training and post-training. During\npre-training, the LM is trained to autoregressively predict a large corpus of text, typically taken from\nthe internet and books (Radford et al., 2019; Brown et al., 2020). Afterwards, the pretrained language\nmodel (PLM) has acquired knowledge and skills from its pre-training data, but cannot yet be easily\nused for realistic applications. To elicit the PLM's capabilities for some class of tasks, post-training\ntechniques are used (Wei et al., 2021; Dubey et al., 2024; Ouyang et al., 2022; Bai et al., 2022). We\ndenote an LM as a conditional distribution p(y | x) over responses y given a prompt x.\nStandard post-training pipeline. Post-training a PLM typically involves two stages. In the initial\nSFT stage, a PLM is trained on human-written task demonstrations. That is, given an SFT dataset\nDSFT = (XSFT, VSFT) of prompts XSFT and human-written responses VSFT, the PLM is trained to\nminimize DSFT \u03a3 DSFT (x,y)\u2208(XSFT, VSFT) log p(yx). We call the result of this stage the SFT model,\nwhich has typically learned the correct input/output format for the given task and has partially elicited\nthe PLM's capabilities.\nSubsequently, to further improve performance, the SFT model undergoes RLHF via algorithms like\nPPO (Ouyang et al., 2022; Bai et al., 2022; Schulman et al., 2017), DPO (Rafailov et al., 2024; Dubey\net al., 2024), or related methods. In this stage, a new dataset is constructed using a set of prompts\nx \u2208 XRLHF and response pairs y, y' sampled from the SFT model. Each prompt and pair of responses\nare shown to a human annotator that picks the response they prefer, creating a dataset of triples\n(x, y+, y-) \u2208 DRLHF, where y+ is preferred to y-. This dataset is used for preference optimization\nvia methods like PPO, which trains a reward model from DRLHF for online RL, or DPO, which uses\nan implicit reward model. In this work, we focus on DPO due to its computational efficiency and\nstability. We call the output of the this stage the SFT+DPO model.\nSimulating unreliable human supervision To study how current post-training pipelines perform\nunder unreliable supervision, we need tasks with known ground truth so that we can evaluate"}, {"title": "3.1 TASKS AND MODELS", "content": "Datasets. We test SFT+DPO with unreliable feedback on three text generation tasks: mathematical\nproblem-solving using GSM8K (Cobbe et al., 2021), SQL code generation with BIRD (Li et al.,\n2024), and safe instruction following with SaferPaca (Bianchi et al., 2023), which is a mix of the\nAlpaca dataset (Taori et al., 2023) and refusal demonstrations to unsafe instructions. All datasets are\nformatted as question-answer pairs following the same template.\nModels. We use three open-source PLMs of varying sizes in our experiments: Gemma 2B (Team\net al., 2024), Mistral 7B (Jiang et al., 2023), and Meta Llama 3 70B (Dubey et al., 2024). Our\nLM-simulated experiments include four settings that use a smaller model to supervise a larger model.\nIn our simulation with time-constrained humans, we only experiment with the largest 70B model.\nEvaluation metrics. Each dataset requires a specific approach to evaluate model outputs and deter-\nmine performance. For GSM8K, we parse the numerical answer following \u201c####\" at the end of each\nresponse and compute exact match accuracy by comparing it with the ground truth. For BIRD, we\nmeasure execution accuracy by running the generated code on corresponding test databases, following\nLi et al. (2024). For SaferPaca, we follow Li et al. (2023) and use GPT-40 (OpenAI, 2024) to compute\nwin rate against reference answers."}, {"title": "4 DPO IS INEFFECTIVE WITH UNRELIABLE SUPERVISION", "content": "In this section, we present results of SFT+DPO under LM-simulated unreliable supervision. We\nfind that SFT still retains some effectiveness, but DPO fails to improve the SFT model. We further\nshow that the failure of DPO appears to be caused by its tendency to overoptimize given unreliable\ncomparison feedback, which motivates our alternative approach introduced in the next section.\nSFT shows limited robustness to unreliable demonstrations. We first investigate the effectiveness\nof SFT under LM-simulated unreliable supervision. When finetuned on DSFT with unreliable demon-"}, {"title": "4.1 DPO WITH UNRELIABLE FEEDBACK EXHIBITS OVEROPTIMIZATION", "content": "We hypothesize that DPO's failure under unreliable supervision stems from the problem of overopti-\nmization that preference-based learning methods often suffer from (Gao et al., 2023). Optimizing an\nexplicit reward function (with PPO) or an implicit one (with DPO) based on preference feedback can\ninitially lead to an increase in performance but eventually causes a decline as the reward function\ndiverges from the true objective. To combat this, both PPO and DPO regularize based on the KL\ndivergence between the initial and updated policies.\nWe find that the optimal amount of regularization for DPO, which is controlled by the hyperparameter\nB, depends strongly on the reliability of comparison feedback. This dependency creates a critical\ntrade-off: stronger regularization is needed to prevent overoptimization with unreliable feedback,\nbut this constrains DPO's ability to improve upon the suboptimal SFT model. To demonstrate this,\nwe measure DPO's relative improvement over SFT under varying feedback quality levels and KL\nregularization strengths (Figure 3). Full results for each training epoch are presented in Figure 8.\nWith unreliable comparison feedback, DPO is especially prone to overoptimization and needs\nvery strong regularization. In Figure 3a, we consider three levels of feedback quality: unreliable\nfeedback from \u1fb7, mixed feedback (50% q, 50% oracle), and pure oracle feedback. With oracle\nfeedback, smaller \u1e9e values consistently yield better performance. However, as feedback quality\ndecreases, smaller \u1e9e values lead to worse performance while only a suitably large \u1e9e enables positive\nimprovements. This suggests that preference optimization under unreliable feedback needs heavy\nregularization to prevent overoptimization.\nStrong regularization limits useful model updates during DPO training. Figure 3b demonstrates\nthat larger divergence correlates with higher accuracy increases in the case of oracle feedback (colored\ncircles in upper left). This indicates that setting small \u1e9e to allow substantial model updates is crucial\nfor improving upon the initial suboptimal SFT model. However, with unreliable feedback, this is not\npossible: as we observed above, using insufficient regularization with unreliable comparisons leads to\noveroptimization.\nThese two observations suggest a tension between preventing overoptimization from unreliable com-\nparison feedback and making large enough model updates. Since SFT on unreliable demonstrations\nproduces a model P SFT that is suboptimal, we need large updates to P SFT to recover performance\nclose to PSFT. However, with unreliable comparison feedback, we need significant regularization (i.e.,\nlarge \u1e9e) for DPO, which prevents the large updates that are needed. Given these issues, we seek an\nalternative algorithm that better leverages comparison feedback to improve over the SFT model."}, {"title": "5 ITERATIVE LABEL REFINEMENT", "content": "To overcome the limitations of preference optimization discussed in Section 4.1, our method needs\nto be robust to unreliable comparisons while still allowing large updates from the SFT model. To\nachieve this, we propose iterative label refinement (ILR). In contrast to RLHF methods, which\niteratively update the SFT model, ILR focuses on improving the unreliable demonstrations in the\ninitial SFT dataset. It uses comparison feedback to decide whether the initial SFT data should be\nreplaced by model-generated alternatives, and then retrains the model from scratch via SFT on the"}, {"title": "5.1 METHODOLOGY", "content": "ILR consists of several iterations; during each iteration, the SFT dataset is improved by replacing\nsome of the demonstrations. The first step of each iteration is to gather proposed demonstrations\nthat may replace unreliable demonstrations in the current SFT data. These proposals are generated\nusing models trained via SFT on the current dataset. We showed in Section 4 that SFT models often\noutperform their unreliable training data. Thus, by replacing some of the existing dataset with the\nimproved output of the SFT model, the overall quality and accuracy of the dataset should increase.\nHowever, our findings in Section 4 only show that SFT models outperform their training data on\nheld-out prompts; if an SFT model is tested on a train prompt, it is likely to output responses that\ncontain mistakes imitated or memorized from the training data. Thus, if we use a model trained on the\nentire current SFT dataset to generate proposals, those proposed responses are less likely to improve\nover the current responses in the dataset. To ensure that the proposed replacements are generated on\nheld-out prompts, we implement ILR by training two SFT models on different halves of the SFT\ndata; then, these models cross-label the half they were not trained on. This makes it more likely the\nmodel-generated responses are different and better than the existing responses, enabling improvement\nof the dataset.\nOnce proposal responses are generated, we selectively update the SFT dataset with them by leveraging\ncomparison feedback. We ask the annotator (i.e., the small LM \u011f or time-constrained humans) to\ncompare an existing response in DSFT with a model-generated proposal for the same prompt. If the\nproposal is preferred by the annotator, it replaces the original response. After these comparisons are\ncomplete, a new SFT model is trained on the updated dataset. As long as the annotator chooses better\nresponses more than half the time, the overall accuracy of the SFT data will increase, enabling the\nnew SFT model to outperform the initial one.\nFormally, each iteration of ILR consists of the following steps:\n1. Data splitting: We start with a dataset of task demonstrations Dk = (X,\u0178k), where k is the\ncurrent iteration and Do = DSFT. The dataset Dk is evenly split into two disjoint subsets,\nD = (X, Y) and D\u00b2 = (X2,Y2).\n2. Model training: Two models, PSFT and PSFT, are finetuned on D and D respectively.\n3. Cross-labeling: Each model generates label proposals for the subset it wasn't trained on, i.e., we\nsample Z~PSFT(\u00b7 | X\u00b2) and Z ~PSFT(. | X\u00b9).\n4. Proposal evaluation: The unreliable supervisor provides comparison feedback on the new\nproposal zi and original label \u00ff\u00bf for a prompt xi to decide which of them is preferred; this\nfeedback determines whether \u1ef9, should be replaced by zi.\n5. Label updating: Based on the comparisons in step 4, we form an updated dataset Dk+1 =\n(X, Yk+1), where each element of Yk+1 is either the accepted new proposal or the retained\noriginal label. We control the label refinement speed by setting a hyperparameter a \u2208 (0,1],\nallowing at most a|X| labels to be updated in each iteration. When more than a X proposals\nare accepted in step 4, we choose the ones with higher evaluation confidence, determined by\neither q's log probability or human annotators' self-reported confidence. In all experiments we\nset a = 0.15. Further analysis of a is presented in Appendix E.3.\nAt the end of each iteration, we finetune a new model starting from the base PLM of P SFT using the\nentire refined dataset DK, resulting in the SFT+ILR model PSFT+ILR. This process is repeated for K\niterations and the final model is PSFT+ILR\nK\nIn the LM-simulated setting, we only consider proposals that are sufficiently different from the\noriginal labels during step 4. We determine a proposal is different enough if it has a different final"}, {"title": "5.2 LM-SIMULATED EXPERIMENT RESULTS", "content": "SFT+ILR leverages unreliable supervision better than SFT+DPO. We compare SFT+ILR and\nSFT+DPO across four settings with LM-simulated unreliable demonstrations and comparison feed-\nback (further validation using time-constrained human data is presented in Section 6). As shown in\nFigure 4, SFT+ILR consistently outperforms SFT+DPO in all scenarios under unreliable supervision.\nIn GSM8K, ILR shows more significant improvements as model size increases from 7B to 70B,\nsuggesting that it benefits from model scaling and may remain effective for future models that are\neven more capable.\nComparison feedback is effective for improving demonstration quality. ILR relies on refining\nSFT data using comparison feedback, which is often easier to obtain than high-quality demonstrations,\nespecially for complex tasks that humans struggle with. Figure 5a shows that unreliable comparison\nfeedback guides this refinement process effectively: the accuracy of the SFT data steadily increases\nwith more rounds of ILR. As suggested, this is likely because evaluation of AI output is typically\neasier than the demonstration of an ideal output (Leike et al., 2018), allowing even imperfect feedback\nto contribute to meaningful improvements in the SFT data. With reliable feedback (Figure 5b), the\nSFT demonstrations can be improved even further, approaching or even surpassing the accuracy of\nthe model SFT trained on ground truth. This highlights ILR's potential when combined with other\nscalable oversight techniques that enhance humans' evaluation capability.\nILR enables larger model updates and more efficient use of comparison feedback. As illustrated\nin Figure 3b, models trained with ILR exhibit significantly higher KL divergence from the initial SFT\nmodel compared to those trained with DPO even after a single round. Moreover, when comparison\nfeedback is reliable while SFT data is not, ILR shows much higher efficiency than DPO, using the\nsame amount of feedback to enable larger improvement over the initial SFT model. These can be\nattributed to ILR's direct modification of the unreliable SFT data, which fundamentally alters models'\ntraining dynamics during SFT. By doing so, ILR allows each subsequent model to learn from new\ndata independently and from scratch, without inheriting the errors of previous models, unlike the\ncontinual preference-based training seen in DPO.\nSupervision for refinement is necessary. Since the SFT models can generate proposal responses\nthat are more accurate than their training data, it appears that one can simply replace the initial SFT\ndata with new proposals without using any comparison feedback. To understand the importance of\ncomparison feedback in ILR's refinement process, we compare it to a naive approach that directly\nreplaces an a fraction of the original labels with new proposals in each round. As shown in Figure 9\n(Appendix E.2), this naive method leads to performance degradation, showing that supervision in the\nrefinement process, even if unreliable, is necessary. It could be that training on model outputs without\nany curation leads to model collapse, a phenomenon observed when training generative models with\nsynthetic data (Shumailov et al., 2023; Ren et al., 2024; Gerstgrasser et al., 2024)."}, {"title": "6 TIME-CONSTRAINED HUMAN STUDY", "content": "To validate our findings from LM-simulated settings, we conduct a human study where we recruit\nworkers through CloudResearch Connect to provide both task demonstrations and comparison\nfeedback for an instruction-following task. Although the task is not beyond human capability, we\nimpose time constraints to create annotator noise and obtain unreliable data; this simulates the\nchallenges that may arise as humans supervise AI on complex tasks that are difficult for humans\nthemselves.\nTask and data collection. For simplicity, we focus on the Alpaca instruction-following dataset\n(Taori et al., 2023; Peng et al., 2023) without unsafe instructions. We evaluate models using the\ncomplete test set of AlpacaEval (Li et al., 2023) and GPT-4o as a judge, similar to evaluation in our\nSaferPaca setting. We collect human demonstrations for 1,000 randomly sampled instructions, with\nworkers instructed to spend 1-2 minutes to write each response. Each worker is assigned 15 questions,\nplus 3 attention checks used to filter out low-quality responses. As shown in Figure 6a, human\ndemonstrations are generally shorter than GPT-4-generated responses, suggesting suboptimal quality\ndue to time constraints. For the comparison feedback used in DPO and ILR, we instruct workers to\nspend 30 seconds to 1 minute per question, with 50 questions assigned per worker and 3 additional\nquestions for quality checks. We collect 1,000 comparisons for each algorithm in each round and\nconduct two rounds of both DPO (\u03b2 = 0.1) and ILR (a = 0.1). We also use the comparison data\ncollected for DPO with \u03b2 = 0.1 to run DPO with several other \u1e9e values. More details of our human\nstudy setup are provided in Appendix D."}, {"title": "7 DISCUSSION", "content": "In this work, we study the effectiveness of a typical SFT+DPO pipeline for language model post-\ntraining under unreliable supervision, as simulated by small LMs and time-limited humans. We\nshow that in this setting, unlike with reliable feedback, SFT+DPO fails to improve upon SFT. Our\nanalysis suggests that DPO struggles to avoid overoptimization with unreliable comparison feedback\nand requires heavy regularization, preventing it from correcting significant errors in the SFT model.\nTo address this, we propose ILR to redirect comparison feedback towards improving unreliable\ndemonstrations in the SFT dataset. ILR enables larger model updates without the overoptimization\nrisks in preference optimization and consistently outperforms DPO under unreliable supervision.\nLimitations. Our focus on RLHF via DPO may not fully capture the nuances of RLHF pipelines that\nuse reward modeling and PPO. Future research should verify whether our findings generalize to these\nmore complex RLHF setups. Also, our human study using time-constrained data collection may not\nperfectly simulate human errors that arise from more realistic capability constraints. Exploring more\nchallenging tasks such as competition math or coding could potentially provide a closer analogy to\nsupervising AI on tasks that humans truly struggle with.\nFuture work. Our findings open several exciting directions for future research. For example, one\ncould explore hybrid approaches that combine ILR with RLHF. Applying PPO or DPO after sev-\neral rounds of ILR could potentially yield more improvements, since the problem of imitating\nerrors in unreliable demonstrations is relieved after improving the SFT data with ILR. Additionally,\ninvestigating more sophisticated label refinement strategies, such as synthesizing human demonstra-\ntions with model-generated proposals or implementing ensemble techniques through more than two\ncross-labeling splits, could potentially further enhance the effectiveness of ILR.\nBroader impact. Our findings suggest a potential paradigm shift in how we approach human oversight\nof AI systems under unreliable supervision. We show that the canonical RLHF post-training pipeline\nmay no longer be the best use of human comparison feedback. Instead, we need methods like ILR\nthat leverage model outputs to improve and learn from unreliable human supervision."}, {"title": "A TASKS AND EVALUATION", "content": "A.1 DATASETS\nGSM8K (Cobbe et al., 2021): This dataset contains math word problems widely used to evaluate\nlanguage models' mathematical problem solving capabilities. In total, there are 8.5K question-answer\npairs for training and 3K for testing.\nBIRD (Li et al., 2024): In this task, models are required to generate an SQL query that answer a\nquestion of interest, given context of database descriptions. We filter out questions with token lengths\ngreater than 1024 and obtained 8.5K examples for training and 1.4K for testing.\nSaferPaca (Bianchi et al., 2023): This dataset is a mixture of instruction following questions from\n(Taori et al., 2023) and demonstrations of refusals to unsafe instructions. It requires a model to serve\nas an assistant that follows human users' instructions while generating responsible responses and\nrejecting harmful prompts. For efficiency, we use a subset that contains 9500 general instructions\nwith responses labeled by GPT-4 (collected by Peng et al. (2023)) and 500 safety-related instructions\nwith the original labels in (Bianchi et al., 2023). The test set we use contains 300 questions and\nreference answers taken from AlpacaEval (Li et al., 2023), alongside 278 safety questions and simple\nrejections (\"Sorry, I cannot help with that.\") of the I-MaliciousInstructions and I-CONA\ndatasets used in (Bianchi et al., 2023).\nWe format all datasets into question-answer pairs following the same template:\nPrompt template for all datasets\nUSER:\\n{question}\\n\\nASSISTANT:{answer}\\n"}, {"title": "A.2 EVALUATION", "content": "Sampling. For all tasks and models, we sample one response for each prompt using vLLM (Kwon\net al., 2023) with beam search enabled and beam size set to 4.\nMetrics. For GSM8K, we simply parse the final numerical answer and compute exact match accuracy.\nFor BIRD, we download all databases and execute the generated code on them to compute execution\naccuracy, following (Li et al., 2024). We ensure all models' generated code is executed on the same\nmachine for fair comparison. For SaferPaca, we adopt the CoT prompt (shown below) from (Li et al.,\n2023) and use GPT-40 (OpenAI, 2024) to compute model answers' win rates against the reference\nanswers. Specifically, we compute GPT-40's mean probability of generating a token that chooses the\nmodel's answer over the reference answer."}, {"title": "BLM-BASED SIMULATION DETAILS", "content": "For GSM8K, we experiment both with using the 2B model to supervise the 7B model, and using\nthe 7B model to supervise the 70B model. For SaferPaca and BIRD, we consider 2B supervising\n7B and 2B supervising 70B, respectively, to balance limited compute while maintaining sufficient\nperformance gaps between models.\nIn each task, we split the training set into two halves. We use the first half as ground truth to train p and\n\u1fb7, and then use p to generate labels for the second half or use \u1fb7 to generate comparison feedback for\nanswers to questions in the second half. This is similar to the setup in (Burns et al., 2023), despite that\nwe are operating on text generation tasks with both task demonstrations and comparison feedback.\nTo collect training data for \u1fb7, we evenly save 10 intermediate checkpoints when training p and use\nthem to sample answers for questions in their training set. Then, we pair these low-quality answers\nwith ground truth answers, and train \u1fb7 to select the ground truth with a standard binary classification\nloss. In SaferPaca, we balance refusals and acceptances (i.e., following the instruction) in the training\ndata to avoid class bias in \u1fb7. This is done by first training two p on safe and unsafe instructions in\nSaferPaca respectively and sampling answers from them for each instruction. In this way, we obtain\nboth acceptance and refusal responses for each instruction, and then pair them up with the ground\ntruth as q's balanced training data.\nWe use the following template to compose two answers and the question into a prompt for \u1fb7:"}, {"title": "C IMPLEMENTATION DETAILS OF SFT, DPO, AND ILR", "content": "In all our experiments, we apply Low-Rank Adaptation (LoRA) (Hu et al., 2021) for efficient model\ntraining. We set r = 64 and a = 128 for all models.\nSFT. For each task, we use a consistent setting of epoch, batch size, and max answer token for all\nmodels (Table 1). We use set learning rate to 5e-4 for Gemma 2B and 1e-4 for Mistral 7B and Meta\nLlama 3 70B across all tasks. We use Adam (Kingma, 2014) optimizer for Gemma 2B and Mistral\n7B and AdaFactor (Shazeer & Stern, 2018) for Meta Llama 3 70B. We enable gradient checkpointing\nand use gradient accumulation with a mini batch size of 1 for all models."}, {"title": "D HUMAN STUDY DETAILS", "content": "We use CloudResearch Connect\u00b2 to recruit online workers to write human demonstrations for 1000\nAlpaca (Taori et al., 2023) instructions and provide comparison feedback that are used in two rounds\nof ILR and DPO."}, {"title": "D.1 COLLECTING HUMAN DEMONSTRATIONS", "content": "In our survey for collecting human demonstrations, we assign each worker 15 questions with 3\nadditional screening questions formatted in the same way for filtering out low-quality responses.\nSpecifically, the screening questions and filtering critiria are:\n1. Instruction: How many letter R's does the word 'STRAWBERRY' have?\nExpected answer: Anything containing 3 or three.\n2. Instruction: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of\nbabysitting. How much did she earn?\nExpected answer: Anything containing 10 or ten.\n3. Instruction: I want to know what dish I can cook with these ingredients: eggs, tomatoes,\nsalt, oil. Also, please give me a list of steps to cook it.\nExpected answer: Anything containing the mentioned ingredients and at least 2 steps for\ncooking them."}, {"title": "D.2 COLLECTING HUMAN COMPARISON FEEDBACK", "content": "In each round of ILR and DPO, we collect human comparison feedback for all 1000 questions.\nFor DPO, model completions are generated by models trained with \u03b2 = 0.1. In our survey for\ncollecting human comparison feedback, we assign each worker 50 questions with 3 additional\nscreening questions formatted in the same way for filtering out low-quality responses. Specifically,\nthe screening questions and filtering critiria are:\n1. Instruction: Where is water that has its salt removed before it can be used as drinking water\nmost likely to have come from?\nResponse A: Water that has its salt removed before it can be used as drinking water is most\nlikely to have come from a sea.\nResponse B: Water that has its salt removed before it can be used as drinking water is most\nlikely to have come from a lake.\nExpected answer: A with any confidence level.\n2. Instruction: Help me solve this math problem. Input: How can I compute the area of a\ncircle with radius 5?\nResponse A: The area of it is 25\u03c0.\nResponse B: Note that the area of a circle with radius r is \u03c0 * r\u00b2. Therefore, the area of a\ncircle with radius 5 is \u3160 * 5 * 5 = 25\u03c0.\nExpected answer: B with any confidence level.\n3. Instruction: Immediately before and after running a 50 metre race, your pulse and breathing\nrates are taken. What changes would you expect to find?\nResponse A: After running a 50-metre race, an increase in pulse and breathing rate is the\nchanges one would expect to find immediately before and after.\nResponse B: After running a 50 metre race, you would expect to find an increase in pulse\nbut no change in breathing rate when your pulse and breathing rates are taken immediately\nbefore and after the race.\nExpected answer: A with any confidence level."}, {"title": "D.3 DPO AND ILR HYPEPERAMETERS", "content": "In both DPO and ILR, we only use comparison data with at least moderate confidence. We tested the\nfirst round of DPO using the top 10%, 20%, 30%, 40% and 80% most confident comparisons and\nfound 30% yields the best performance, hence this is applied throughout the two rounds. We also\ntested \u03b2\u2208 {0.01, 0.1, 0.5, 0.1} for the first round and found \u03b2 = 0.1 performs the best. For ILR, we\ntested a \u2208 {0.1, 0.2, 0.3} for the first round and applied the best-performing a = 0.1 across the two\nrounds."}, {"title": "E ADDITIONAL EXPERIMENTS", "content": "E.1 KL REGULARIZATION IN DPO\nIn Figure 8, we present the complete experiment results when using different levels of KL regulariza-\ntion and feedback quality for DPO."}, {"title": "E.2 FAILURE OF NAIVE ILR: SUPERVISON FOR REFINEMENT IS NECESSARY", "content": "To understand the importance of comparison feedback within ILR's refinement process, we compare\nit to a naive approach that directly replaces an a fraction of the original labels with new proposals\nwithout any feedback. As shown in Figure 9, this naive method leads to performance degradation\nin all tasks, showing that supervision in the refinement process, even if unreliable, is necessary. It\ncould be that training on model outputs without any curation leads to model collapse, a phenomenon\nobserved when training generative models with synthetic data (Shumailov et al., 2023; Ren et al.,\n2024; Gerstgrasser et al., 2024)."}, {"title": "E.3 EFFECT OF CONTROLLING REFINEMENT SPEED IN ILR", "content": "The refinement speed in ILR, controlled by a, plays a crucial role in maintining stability. Figure 10\nillustrates the impact of different a values on model performance in GSM8K. We find a = 0.05 too\nsmall"}]}