{"title": "A Spatiotemporal Stealthy Backdoor Attack against\nCooperative Multi-Agent Deep Reinforcement\nLearning", "authors": ["Yinbo Yu", "Saihao Yan", "Jiajia Liu"], "abstract": "Recent studies have shown that cooperative multi-agent deep reinforcement learning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor trigger is observed, it will perform abnormal actions leading to failures or malicious goals. However, existing proposed backdoors suffer from several issues, e.g., fixed visual trigger patterns lack stealthiness, the backdoor is trained or activated by an additional network, or all agents are backdoored. To this end, in this paper, we propose a novel backdoor attack against c-MADRL, which attacks the entire multi-agent team by embedding the backdoor only in a single agent. Firstly, we introduce adversary spatiotemporal behavior patterns as the backdoor trigger rather than manual-injected fixed visual patterns or instant status and control the attack duration. This method can guarantee the stealthiness and practicality of injected backdoors. Secondly, we hack the original reward function of the backdoored agent via reward reverse and unilateral guidance during training to ensure its adverse influence on the entire team. We evaluate our backdoor attacks on two classic c-MADRL algorithms VDN and QMIX, in a popular c-MADRL environment SMAC. The experimental results demonstrate that our backdoor attacks are able to reach a high attack success rate (91.6%) while maintaining a low clean performance variance rate (3.7%).", "sections": [{"title": "I. INTRODUCTION", "content": "Effectively combining the perception and decision-making\nability of deep reinforcement learning (DRL) with the dis-\ntributed characteristics of multi-agent systems, cooperative\nmulti-agent deep reinforcement learning (c-MADRL) has been\napplied to cooperative games [1], communication [2], and\nother fields [3]. However, recent studies have shown that\nDRL is facing the security threat of backdoor attacks [4]-\n[7]. Attackers usually implant the backdoors by poisoning the\nobservations, actions, or rewards of a victim DRL agent during\ntraining. A backdoored model is characterized by outputting\nnormal behaviors under normal inputs, but malicious behaviors\nexpected by the attacker when a specified backdoor trigger\nappears in the inputs. Therefore, c-MADRL, as a branch of\nDRL, is also under the threat of backdoor attacks.\nCompared with backdoor attacks against DRL, conducting\nbackdoor attacks against c-MADRL receives some extra chal-\nlenges. Firstly, it is easy to implant backdoor attacks in all\nagents, but this attack is expensive to inject and less stealthy.\nHence, it is important to poison as few agents as possible.\nHowever, the attack effects generated from a few agents may\nbe easily overwhelmed due to mutual dependency [3] among\nc-MADRL agents. So far, there are only a few studies [2], [8]\u2013\n[10] on backdoor attacks against c-MADRL. While attacks [2],\n[8] directly poison all agents, attacks [9], [10] aim to poison\none agent to attack the entire system, but ignore agent mutual\ndependency, limiting the attack efficiency. Secondly, most\nexisting attacks use a specific instant trigger, e.g., spectrum\nsignals [2], visual patterns [8], distance [10], and trigger policy\n9], which can be detected by anomaly classifiers [11], thus\nhaving poor concealment. Hence, hiding triggers in a series\nof timing states is more suitable for c-MADRL's sequential\ndecision-making and can achieve better concealment. While\nexisting backdoor attacks are useful for c-MADRL, they have\nnot addressed these challenges well.\nTo solve the above challenges, in this paper, we propose\na novel stealthy backdoor attack against c-MADRL, which\ncan disrupt the entire multi-agent team by implanting the\nbackdoor in a single agent without modifying its original\nnetwork architecture. Firstly, most of the existing c-MADRL\nalgorithms, such as VDN [12], QMIX [13] and COMA [14],\nuse a recurrent neural network (RNN) to remember the past\ninformation and combine it with the current observation to\nmake effective decisions, so as to overcome the partial ob-\nservability [15]. However, this allows attackers to hide the\nbackdoor triggers in unobservable states [11], [16]. Therefore,\nunlike existing methods that use instant triggers [4], [5], [8],\n9], we carry out our backdoor attacks using a spatiotemporal\nbehavior pattern trigger hidden in a series of observations and a\ncontrollable attack period. So, the attacker can act as a moving\nobject (e.g., an enemy unit in Starcraft) to perform a series\nof specific actions around the backdoored agent in a short\ntime to control the spatial and temporal state dependencies\nwith the agent, thereby activating the backdoor. This trigger\nhas proven to be highly stealthy and difficult to detect with\nexisting detection methods in our previous studies [11], [16].\nSecondly, in c-MADRL, the behaviors of an agent are\ninfluenced not only by the running environment but also by\nthe behaviors of other agents. Therefore, an agent taking a\ndifferent action may influence other agents' decisions in the"}, {"title": "II. RELATED WORK", "content": "Most of the existing studies on backdoor attacks focuse on\ndeep neural networks (DNN) [18]. Due to the representation\nability and feature extraction ability inherited from DNN, DRL\nalso inevitably faces this threat. Kiourti et al. [4] propose\na backdoor attack method against DRL with image patch\ntriggers. During training, when the model's input contains a\ntrigger and the output is the target action or a random one,\nthey maximize the corresponding reward value to achieve a\ntargeted or untargeted attack. Yang et al. [5] investigate the\npersistent impact of backdoor attacks against LSTM-based\nPPO algorithm [19]. When an image patch trigger appears\nat a certain time step, the backdoored agent persistently goes\nfor the attacker-specified goal instead of the original goal.\nAshcraft et al. [7] use in-distribution triggers and multi-\ntask learning to train a backdoored agent. Wang et al. [6]\nstudy backdoor attacks against DRL in two-player competitive\ngames. The opponent's actions are used as a trigger to switch\nthe backdoored agent to fail fast. Our previous works [11],\n[16] studied a new temporal and spatial-temporal backdoor\ntrigger to DRL, which has shown high concealment.\nSo far, only a few studies have been done on backdoors\nin c-MADRL. Chen et al. [8] first studied backdoor attacks\nagainst c-MADRL. They consider both out-of-distribution and\nin-distribution triggers, and use a pre-trained expert model to\nguide the selection of actions of poisoned agents. Besides,\nthey modify the team reward to encourage poisoned agents to\nexhibit their worst actions. Their method implants backdoors\nin all agents, and every agent that observes the trigger will\nperform instant malicious action. Chen et al. [9] propose\nbackdoor attacks that affect the entire multi-agent team by\ntargeting just one backdoored agent. During training, they\nintroduce a random network distillation module and a trigger\npolicy network to guide the backdoored agent on what actions\nto select and when to trigger the backdoor, thereby enabling\nthe agent to adversely affect other normal agents. During\nexecution, the backdoored agent needs to rely on the trigger\npolicy network to determine whether to trigger the backdoor.\nZheng et al. [10] also implanted a backdoor into one of the\nagents in a multi-agent team, and use distance as a condition\nto trigger the backdoor. When the trigger condition is met, the\nbackdoored agent performs a specified action which leads to\nthe failure of the team task. These existing backdoor attacks\nare effective against c-MADRL, but they either poison all\nagents, require additional networks to activate backdoors, or\ndo not take into account the influence between agents."}, {"title": "III. THREAT MODEL", "content": "In this section, we logically represent our backdoor attacks\nagainst c-MADRL as a decentralized partially observable\nMarkov decision process (Dec-POMDP) which consists of a\ntuple ({N, S, O, A, T, {$\\mathbb{R}_b, \\mathbb{R}_c$}, y).\n\u2022 $\\mathbb{N}$ := {1, ..., n} represents the set of team agents in c-\nMADRL. We specify agent k to implant the backdoor,\nand the other agents are clean agents without backdoors.\n\u2022 $\\mathbb{S}$ represents the global environmental state space. Fol-\nlowing the centralized training and decentralized execu-\ntion (CTDE) paradigm [12] [13], $s_t \\in S$ is only employed\nduring training and not during testing (execution).\n\u2022 $\\mathbb{O}$ := $\\mathbb{O}_1 \\times ... \\times \\mathbb{O}_n$ represents the local observations of\nall agents. The individual observation $o_{i,t} \\in \\mathbb{O}_i$ of each\nagent i at time step t serves as an input for the policy\nnetwork of the agent.\n\u2022 $\\mathbb{A}$ := $\\mathbb{A}_1 X ... \\times \\mathbb{A}_n$ represents the joint actions of all\nagents. All clean agents and the backdoored agent use\n$\\pi^c(a_{i,t} | O_{i,t}): \\mathbb{O}_i \\rightarrow \\mathbb{A}_i$ and $\\pi^b(a_{k,t} | O_{k,t}): \\mathbb{O}_i \\rightarrow \\mathbb{A}_i$\nto\nselect action, respectively, where $a_{i,t} \\in \\mathbb{A}_i$ denotes the\nselected action of each individual agent.\n\u2022 T: $\\mathbb{S} \\times \\mathbb{A} \\rightarrow \\mathbb{S}$ represents environmental state\ntransition function. Given state $s_t \\in S$ and joint action $a_t \\in A$\nat time step t, T($s_{t+1} | s_t, a_t$) computes the probability\nof transitioning to state $s_{t+1} \\in S$ at time step t + 1.\nBesides, depending on T, we can use F($O_{i,t+1} | O_{i,t}, a_t$)\nto represent the observation transition of agent i.\n\u2022 $\\mathbb{R}_b$, $\\mathbb{R}_c$ : $\\mathbb{S} \\times \\mathbb{A} \\times \\mathbb{S} \\rightarrow R$ represents the reward function\nfor both the backdoored agent and clean agents after\nexecuting a joint action $a_t \\in A$ that transitions the state\nfrom $s_t \\in S$ to $s_{t+1} \\in S$. The design of $r \\in \\mathbb{R}_b$ plays a\nsignificant role in the success of backdoor attacks.\n\u2022 y is the temporal discount factor where 0 \u2264 y \u2264 1.\nWe only inject the backdoor in a single agent to ensure\nthe stealthiness and practicality of backdoor attacks. The\nbackdoored agent behaves normally like a clean agent in the\nabsence of an attacker-specified trigger within its observation.\nHowever, once the trigger appears, it selects disruptive behav-\nior that influences the other clean agents and ultimately leads\nto the failure of the team."}, {"title": "IV. THE PROPOSED BACKDOOR ATTACK METHOD", "content": "In this section, we formulate our designed spatiotemporal\nbackdoor trigger and the reward hacking method based on\nreward reversing and unilateral influence and describe the\ntraining procedure of the backdoored model. Fig. 1 shows the\nframework of the proposed backdoor attack."}, {"title": "A. Spatiotemporal Backdoor Trigger", "content": "In many input-driven multi-agent systems, such as MOBA\ngames [1] and autonomous driving [11], the observations of\neach agent typically include four types of information: its own\nstate, teammates' state, internal environmental information,\nand information from external inputs. The information from\nexternal inputs is not controlled by the environment or the\nagents' decisions and may be exploited by malicious attackers\nas backdoor triggers. For example, in a MOBA game (e.g.,\nStarcraft), an attacker can control an enemy unit to perform\na series of special actions in a short time to activate the\nbackdoor attack. In this section, we present a spatiotemporal\nbehavior pattern trigger that represents a set of specific spatial\ndependencies between the attacker's unit and the backdoored\nagent, as well as a set of specific temporal behaviors of the at-\ntacker's unit. We use a logical formula to represent the spatial\ndependencies and a set of the enemy unit's controllable actions\nto represent the temporal behaviors. First, given a position\nstate $s_b$ of the unit controlled by the backdoored agent and $s_e$\nof the attacker's unit, we use \u03c8 := g($s_b, s_e$) ~ C to describe\nthe spatial constraint between them at a given time step, where\ng \u2208 {+, -, \u00d7, \u00f7} is an operator, ~\u2208 {>, \u2265, <, \u2264, =, \u2260} is a\nrelator, and C\u2208 R is a constant. Specifically, we define the\ntrigger as follows:\nDefinition 1. Given consecutive position states over a short\nperiod length Nt ending at time step t, a spatiotemporal\nbehavior trigger T := (\u03a8,\u03da) can be defined using a logic\nformula \u03a8 representing the spatial constraints \u03a9 of these states\nand a set of controllable actions:\n\u03a8 := $(i,p)|\u03c8(i,p) \u2297 \u03c8(j,q)|ite(\u03c8(i,p), \u03c8(j,q), \u03c8(k,s)) (1)\n\u00a7 := (a+\u2212Nt+1, at\u2212Nt+2,\u2026, $a_t$)                                 (2)\nwhere \u03c8(i,p), \u03c8(j,q), \u03c8(k,s) \u2208 \u03a9, i, j, k \u2208 N are time steps,\nand p,q, s are the position features of attacker's unit relative\nto backdoored agent; \u2208 {V,^} is a Boolean operator\n(i.e., \"or\" or \"and\"); ite represents \u03a8-assignment, e.g., \u03a8 :=\nite(41, 42, 43) means if 1 is true, \u03a8 := 42; otherwise\n\u03a8 := 3; a is an action executed by the attacker's unit.\nExample 1. Take Starcraft for example, given a trigger\nT := (\u03a8,\u03da), where $(N_t = 5)$ := \u03c8($t\u22124,x$) ^ \u03c8($t\u22124,y$) \u039b\n\u03c8($t\u22123,x$)^\u03c8($t\u22123,y$)^\u03c8($t\u22122,x$)^\u03c8($t\u22122,y$)^\u03c8($t\u22121,x$)^\u03c8($t\u22121,y$)^ \u03c8($t,x$) \u039b \u03c8($t,y$), and \u03da := ($a_{t\u22124}$, $a_{t\u22123}$, $a_{t\u22122}$, $a_{t\u22121}$, $a_t$),\nwhere\n\u03c8($t\u22124,x$) = \u03c8($t\u22122,x$) = \u03c8($t,x$) := 8.8 < $x_e$ \u2212 $x_b$ < 9.0,\n\u03c8($t\u22123,x$) = \u03c8($t\u22121,x$) := 6.1 < $x_e$ \u2212 $x_b$ < 6.3, \u03c8($t\u22124,y$) =\n\u03c8($t\u22123,y$) = \u03c8($t\u22122,y$) = \u03c8($t,y$) := \u03c8($t\u22121,y$) := -0.1 <\n$y_e - y_b$ < 0.1; $a_{-4}$ represent the attacker's enemy unit moving\nwestward, $a_{-3}$ is moving eastward, $a_{-2}$ is moving westward,\nand $a_1$ is moving eastward.\nWith this backdoor (shown in Fig. 1), a user in the user\nteam can perform its unit following the trigger to activate the\nbackdoor in the backdoored agent of the multi-agent team.\nThis agent will generate malicious actions to lead the multi-\nagent team to quickly lose the game."}, {"title": "B. Reward Hacking", "content": "Firstly, to guarantee the clean performance of the back-\ndooored agent in the trigger-free environment, at each time step\nt, we use the original team reward $R_t$ in c-MADRL to train\nthe backdoored model, where $R_t$ \u2208 [$R_{min}$, $R_{max}$]. During the\nattack period after the trigger completely appears, We reverse\nthe original environmental reward $R_t$ of the backdoored agent\nto the following form:\n$r_e = R_{max} + R_{min} - R_t$                 (3)\nThe purpose of $r_e$ is to stimulate bad actions and punish\ngood actions. At the same time, $r_e$ retains the same value range"}, {"title": "C. Backdoored Model Training", "content": "To inject the backdoor only in a single agent, we assume that\nall agent models have been trained well in the clean c-MADRL\nenvironment, and during backdoor injection, we only retrain\na single model and leave others to be frozen. Our complete\nbackdoor injection procedure is outlined in Algorithm 1.\nFirstly, the attacker pre-specifies a spatiotemporal behavior\ntrigger T := (\u03a8,\u03da) with a trigger period Nt. Besides, an\nattack duration L is introduced to attack only L time steps\nafter the trigger appears, which can enhance the stealthiness of\nthe backdoor attacks. In each episode, the attacker determines\nwhether to poison with the poisoning rate p. If poison, the\nattacker will insert the backdoor trigger (Line 3). At each\ntime step t in an episode, clean agents select their actions\naccording to their policies, and the backdoored agent chooses\na random action with the probability \u03f5, otherwise, it chooses an\naction according to its policy $\\pi^b$ (Line 5-7). During the attack\nperiod L, the attacker will hack the reward $r_t$ as Equ (5) (Line\n11). To ensure the effective training of the backdoored model,\nwe set up two replay buffers, $B_p$ and $B_c$, to store poisoned\nand clean episodes, respectively. To update the model, we\nrandomly sample a batch of episode data from replay buffer\n$B_p$ or $B_c$ with probability p or 1 p (Line 14-20)."}, {"title": "A. Experiment Settings", "content": "We use the StarCraft Multi-Agent Challenge (SMAC) as\nour experimental environment and choose 8m (8 Marines) as\nour test map. All allied units (i.e., the red side) are controlled\nby c-MADRL agents, while enemy units (i.e., the blue side)"}, {"title": "B. Numerous Results", "content": "Fig. 2 illustrates the training process of the backdoored\nmodels attacking VDN and QMIX. We first use the team's\naverage episode reward and winning rate in clean episodes\n(without the trigger T) and poisoned episodes (with the trigger\nT) to evaluate the performance of the backdoored models,\nrespectively. In Fig. 2, $r_{bc}$, $r_{bp}$, and $w_{rbc}$, $w_{rbp}$ represent the\nepisode rewards and winning rates of the backdoored model\nin clean and poisoned episodes, respectively, and additionally\n$r_{cc}$ and $w_{rcc}$ represent the episode reward and winning rate of\nthe original clean model in clean episodes. We can find that\nboth for VDN and QMIX, $r_{bp}$ and $w_{rbp}$ gradually decrease\nwith training and finally converge to below 10.0 and below\n0.10. Besides, $r_{bc}$ ($w_{rbc}$) decreases slightly at first, but picks\nup later and finally converges to above 19.0 (0.90), which\nis slightly lower than $r_{cc}$ ($w_{rcc}$). This result shows that our\nbackdoor attacks are effective both against VDN and QMIX,\ni.e., the malicious actions taken by the backdoored agent after\nthe trigger is present can cause the entire team to fail.\nTo explore how the backdoored agent can cause the team to\nfail, we analyze the behaviors of the backdoored agent during\nthe attack period and show the results in Fig. 3. Before the\nattack begins, the agents take scattered positions to attack the\nenemy, which is the strategy learned through QMIX, as shown\nin Fig. 3 (a). We control an enemy unit around the backdoored\nallied unit to perform trigger actions. Once the trigger occurs\ncompletely, as shown in Fig. 3 (a), the backdoored allied unit\nstarts to move away from the main battle but moves within the\nrange of its teammates' vision, successfully inducing some of\nits teammates (3 and 7) to move to a disadvantageous position\nor change from attacking behaviors to moving behaviors,\ncausing the remaining teammates to encounter enemy attacks,\nas shown in Fig. 3 (b-c). Ultimately, all other teammates are"}, {"title": "VI. CONCLUSIONS AND FUTURE", "content": "In this paper, we study backdoors in c-MADRL. To enhance\nthe stealthiness, effectiveness, and practicality of backdoor at-\ntacks, we propose a novel backdoor attack against c-MADRL,\nwhich can disrupt the entire multi-agent team by implanting\na backdoor in only one agent. We use spatiotemporal features\nrather than an instant state as the backdoor trigger, and design\nthe malicious reward function according to the characteristics\nof c-MADRL. We evaluate the proposed backdoor attacks on\nVDN and QMIX algorithms in SMAC, and the experimental\nresults demonstrate that the backdoor attacks can achieve great\nattack success rate and low clean performance variance rate.\nIn the future, we will explore backdoor attacks in black-box\nscenarios, as well as study effective defence methods for c-\nMADRL backdoors."}]}