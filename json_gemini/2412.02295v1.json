{"title": "CADMR: Cross-Attention and Disentangled Learning for Multimodal Recommender Systems", "authors": ["Yasser KHALAFAOUI", "Martino LOVISETTO", "Basarab MATEI", "Nistor GROZAVU"], "abstract": "The increasing availability and diversity of multimodal data in recommender systems offer new avenues for enhancing recommendation accuracy and user satisfaction. However, these systems must contend with high-dimensional, sparse user-item rating matrices, where reconstructing the matrix with only small subsets of preferred items for each user poses a significant challenge. To address this, we propose CADMR, a novel autoencoder-based multimodal recommender system framework. CADMR leverages multi-head cross-attention mechanisms and Disentangled Learning to effectively integrate and utilize heterogeneous multimodal data in reconstructing the rating matrix. Our approach first disentangles modality-specific features while preserving their interdependence, thereby learning a joint latent representation. The multi-head cross-attention mechanism is then applied to enhance user-item interaction representations with respect to the learned multimodal item latent representations. We evaluate CADMR on three benchmark datasets, demonstrating significant performance improvements over state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Recommender systems have become a cornerstone of modern digital experiences, playing a critical role in various domains such as streaming services and social media. By filtering and presenting relevant content from an overwhelming array of options, these systems enhance user satisfaction and engagement. Recommender systems can be classified into three main groups: content-based, collaborative filtering (CF), and hybrid algorithms [?]. Content-based filtering uses item features to recommend items similar to those a user has liked based on previous actions or explicit feedback. This approach assumes that users are likely to choose items with features similar to those they have previously liked. However, this method struggles when recommending new items with novel features. On the other hand, collaborative filtering predicts a user's interests based on their historical behavior and the preferences of a large number of other users [1]. Hybrid approaches combine techniques from both collaborative filtering and content-based filtering to achieve more accurate results [2].\nCF remains one of the most widely utilized approaches for recommendation tasks. However, it is limited by the sparsity of the user-item rating matrix and the cold start problem, where new users lack historical behavior data. Various approaches have been proposed to overcome these limitations. Some methods [3, 4] leverage dimensionality reduction and clustering to mitigate the effects of sparsity. These approaches aim to use a low-dimensional latent representation of the rating matrix instead of the original sparse matrix to estimate user-item interactions. Common approaches include Matrix Factorization and Singular Value Decomposition [? ]. Other methods employ autoencoders (AE) to learn the low-dimensional feature space of a given sparse rating matrix. AEs are particularly useful for capturing non-linear relationships and encoding complex features into a compact latent representation. For instance, I-AutoRec [5] utilizes an item-based AE to project high-dimensional matrix entries into a low-dimensional latent hidden space, reconstructing the entries in the output space to predict missing ratings. GLocal-K [1] presents a two-stage framework that pre-trains the autoencoder using a local kernel matrix and then fine-tunes the model using a global convolution kernel. However, these approaches primarily focus on the rating matrix and often overlook the potential of incorporating side information.\nRecently, multimodal-based recommender systems have gained attention, leveraging diverse data sources to improve recommendations. Zhang et al. [6] propose a method that uses a Multi-Layer Perceptron (MLP) to project multimodal data into a unified latent space, with the learned features transferred to user and item embeddings, which are optimized using a reconstruction loss. Similarly, Zheng et al. [7] introduce DeepCoNN, where text review embeddings are fed into two parallel CNN architectures to learn both user and item representations. Despite these advances, existing multimodal approaches typically learn user and item embeddings separately, neglecting the crucial relationship between the user-item rating matrix and the various data modalities. This oversight is significant because user-item interactions are inherently influenced by multiple factors present in multimodal data, which should be integrated into the recommendation process. However, current methods fail to leverage these rich interactions, potentially limiting their effectiveness in capturing the full complexity of user preferences."}, {"title": "2 Related Work", "content": "To address the limitations of existing approaches, this paper introduces the framework CADMR (Cross-Attention and Disentangled Learning for Multimodal Recommender Systems). It begins with a pretraining phase where modality-specific features are disentangled and projected into a shared latent space. Simultaneously, the user-item rating matrix is processed through an autoencoder to capture a compact representation of user-item interactions. During the fine-tuning phase, a multi-head cross-attention mechanism is employed to integrate the rating matrix with the multimodal latent representations. This mechanism allows each user-item interaction to attend to the most relevant complementary information from the multimodal data. Finally, the enhanced rating matrix is reintroduced into the autoencoder for the reconstruction of the final, refined rating matrix. Our contributions can be summarized as follows:\n\u2022 We propose CADMR, a multimodal recommender system framework incorporating disentangled learning and cross-attention.\n\u2022 We introduce a novel approach for integrating multimodal information into the reconstruction of the rating matrix using a cross-attention mechanism, ensuring that user-item interactions are informed by the most relevant multimodal features.\n\u2022 We conduct extensive experiments on benchmark datasets, validating the effectiveness and efficiency of CADMR. The results demonstrate superior performance compared to other state-of-the-art methods."}, {"title": "2.1 Multimodal Collaborative Filtering", "content": "The multimodal collaborative filtering-based recommender system aims to learn informative representations of users and items by leveraging multimodal features. He et al. [8] extracted visual features using a CNN, which they concatenated with ID embeddings to produce a refined item representation, which is then fed to a scalable factorization model to learn users' opinions. Joint Representation Learning for top-n recommendations (JRL) [6] leverages the MLP architecture to learn a unified latent representation for the multimodal data. A mapping matrix is then learned to transfer the unified multimodal representation into user and item embeddings, while the latter are learned using a reconstruction loss. Kang et al. [9] build on existing BPR recommender systems by introducing a Siamese CNN where the last layer of the CNN produces the item representation. Moreover, the framework learns pixel-level representations by training the image representation and the recommender system jointly. While these methods successfully incorporate multimodal features into collaborative filtering frameworks, they often suffer from several limitations. One significant weakness is that these approaches typically treat multimodal data as supplementary to the core user-item interaction, rather than as integral components of the interaction itself. This can lead to suboptimal utilization of the rich information contained in the various modalities."}, {"title": "2.2 Attention Networks", "content": "Traditional multimodal recommender systems often struggle to effectively integrate and utilize multimodal information. The attention mechanism offers a solution by allowing the recommender system to selectively focus on different aspects of the multimodal data, thereby better capturing user preferences. For instance, ACF [10] introduces a framework that incorporates both item-level and component-level attention, enabling the system to attend to high-level information (e.g., photos, videos) as well as finer details, such as specific regions of an image or frames within a video. Similarly, Disentangled Multimodal Representation Learning for Recommendation (DMRL) [12] takes a novel approach by dividing each modality into equal factors and learning a disentangled representation of these different modality feature factors. The attention mechanism is then applied to each factor to effectively model user preferences. UVCAN [11] leverages the attention mechanism in micro-video recommendations by learning multimodal representations for both users and micro-videos through a co-attention mechanism between items and micro-videos. Additionally, it refines video representations by using them as input queries to capture user attention through multi-step reasoning.\nBuilding upon existing approaches that utilize attention mechanisms, our method introduces a novel strategy starting with a pretraining phase where disentangled representations for the different modalities are learned, and the user-item rating matrix is reconstructed via an autoencoder. Following this, we apply a cross-attention mechanism between the user-item rating matrix and the learned multimodal representation. The multimodal-enhanced rating matrix is then projected back into the autoencoder for fine-tuning, ensuring a more integrated and robust modeling of user preferences."}, {"title": "3 Methodology", "content": "In this section, we present CADMR, a novel framework for multimodal recommender systems. Our approach advances existing methods by integrating cross-attention between the rating matrix and multimodal representations. We begin by detailing the learning process for disentangled multimodal representations. Next, we delve into the cross-attention mechanism. Finally, we describe the complete pipeline, from pre-training to the final reconstruction of the rating matrix.\nFig. 1 illustrates the CADMR architecture for a multimodal recommender system, which operates in two distinct phases: pretraining and fine-tuning. During the pretraining phase, an autoencoder is trained on the user-item rating matrix while modality-specific feature extractors learn disentangled representations for textual and visual features. These extracted features undergo layer normalization and are processed through feedforward networks, in order to learn the unified multimodal representation. In the fine-tuning phase, a multi-head cross-attention mechanism is employed, allowing the user-item rating matrix, the query vector, to interact with the unified multimodal representation-treated as key, and value vectors to generate attention weights. These weights are used to refine the rating matrix, which is then reintroduced into the autoencoder for final reconstruction, resulting in a more accurate and refined rating matrix."}, {"title": "3.1 Preliminaries", "content": "In what follows, we denote $R \\in \\mathbb{R}^{I \\times U}$ as the user-item rating or interaction matrix, with I and U representing the sets of items and users, respectively. We consider a user u to have interacted with an item i, if the corresponding entry $r_{i,u} \\in R$ is nonzero, otherwise, there was no interaction, and $r_{i,u} = 0$. In our setting, each user and item have a unique ID, and we assume that each item is associated with a set of multimodal information, such as item descriptions and images. Unlike existing approaches that treat the item ID as an independent modality, we chose to exclude it from our analysis, focusing instead on textual and visual data. These modalities provide richer, more descriptive features that are sufficient to capture the nuanced preferences of users and the intrinsic characteristics of items."}, {"title": "3.2 Feature Extraction", "content": "The extracted multimodal features are fed separately into their respective two-layer neural networks to learn task-specific features relevant to the recommendation. We introduce a layer normalization before each of these neural networks to stabilize the learning process, improve the convergence speed, and ensure that the features fed into the networks are on a consistent scale. The projection neural network is defined as,\n$h = f(W_1g(W_0N(x) + b_0) + b_1),$"}, {"title": "3.3 Disentangled Representation Learning", "content": "Let $h_t \\in \\mathbb{R}^{D_t}$ and $h_v \\in \\mathbb{R}^{D_v}$ represent the textual and visual feature embeddings obtained using the feature extraction module, respectively, where $D_t$ and $D_v$ denote the dimensions of these feature spaces.\nOne of the main challenges in multimodal learning is that features extracted from different modalities often contain redundant or entangled information. This means that certain dimensions of the latent space might capture overlapping or correlated features between modalities, leading to inefficiencies and potential overfitting in downstream tasks. To address this issue, we employ disentangled learning, which aims to ensure that each dimension of the latent representations captures distinct and non-overlapping factors of variation. By disentangling the features, we improve the generalization capability of the model and reduce the risk of redundancy.\nThe disentangled loss is applied to both $h_t$ and $h_v$, encouraging each dimension of the latent representations to capture distinct, non-overlapping information. This is achieved through a Total Correlation (TC) loss, which measures the dependency along the dimensions of the latent representation. The TC loss can be expressed as,\n$L_{TC}(h) = \\sum_{d=1}^{D} E\\Big[ log \\frac{p(h^d)}{p(h | h^{\\setminus d})}\\Big],$"}, {"title": "3.4 Cross-Attention for User's Preference Modeling", "content": "In our framework, we employ a cross-attention mechanism to model user preferences by integrating the fused multimodal representations learned during the pre-training phase. This approach aims to capture the nuanced interactions between users and"}, {"title": "3.5 Rating Matrix Reconstruction", "content": "After generating the cross-attention-based rating matrix, we follow the approach proposed by [1] which involves refining the predictions using the AE. The rating matrix produced in the previous steps serves as input to the trained AE model, which has been pretrained to capture the underlying structure of user-item interactions. In this phase, the AE model takes its pretrained weights and fine-tunes them based on the cross-attention-modified rating matrix, as depicted in Fig. 1.\nThis adjustment allows the model to integrate the additional information encapsulated by the cross-attention mechanism, leading to a more accurate reconstruction of user preferences. The output of this fine-tuned AE corresponds to the final predicted ratings in the proposed recommender system. By reconstructing the rating matrix with the enhanced input, the system ensures that the final predictions are informed by both the original latent factors learned during pretraining and the nuanced multimodal interactions captured through cross-attention."}, {"title": "3.6 Discussion", "content": "We identified two main reasons for choosing cross-attention in our recommendation framework: its ability to model the heterogeneity of user preferences and item characteristics, and its contribution to model interpretability.\nFrom a modeling perspective, cross-attention excels at addressing the diverse and complex nature of user preferences and item attributes. Users often display varied tastes that are influenced by different factors depending on the context. For instance, a user might prioritize certain visual cues in an image when selecting a movie to watch. Similarly, items in a recommender system can be highly diverse, each possessing a unique set of attributes that appeal to different users. Cross-attention allows the model to selectively align the most relevant item features with each user's preferences. From"}, {"title": "4 Training", "content": "To prevent overfitting and enhance generalization, we employ two types of regularization during training:"}, {"title": "4.1 Regularization", "content": "To prevent overfitting and enhance generalization, we employ two types of regularization during training:\nL2 Regularization\nFollowing [1], we apply $L_2$ regularization (Ridge regularization) to both the weight and kernel matrices. This is controlled by separate penalty parameters $\\lambda_2$ and $\\lambda_g$.\nDropout\nWe use a dropout rate of $D_{rrate} = 0.2$ during the initial step of multimodal feature extraction. Dropout is applied to the output of each sub-layer before it is normalized and passed as input to the subsequent layer."}, {"title": "4.2 Hardware and Schedule", "content": "We trained our CADMR model on a single NVIDIA A100 GPU. With the hyperparameters discussed throughout the paper, each training step took approximately 1 second on average. For comparison, training the model on a machine with only a CPU took around 10 seconds per step."}, {"title": "4.3 Optimization", "content": "We trained our model using a squared errors loss with the aformentionned L2 regularization term plus the additional Total Correlation loss for the disentangled representation learning. The final objective function of CADMR is defined as,\n$L = L_{MSE} + \\lambda L_{TC},$"}, {"title": "5 Experiments", "content": "To validate the effectiveness of our proposed multimodal recommender system, we conducted extensive experiments on three benchmark datasets. In what follows, we"}, {"title": "5.1 Experimental Setup", "content": "The Amazon products dataset [16] is a widely-used benchmark in the field of multimodal recommender systems, providing a rich and diverse set of information for model training and evaluation. It is divided into 24 product categories and contains 48 million items, and 571 million reviews from 54 million users represented as reviews; i.e., ratings, text, helpfulness votes. Additionally most of the items are accompanied by the corresponding metadata; i.e., descriptions, category information, price, brand, and image features. For our experimentations, three product subsets are selected: Baby, Sports and Electronics. Table 1 presents the general information about each of the selected products subsets.\nFollowing [13, 17, 18], we reduce the Amazon datasets to the 5-core setting, that is each of the users and items have 5 reviews each. The resulting datasets are then randomly split into training, validation and test sets using the ratio 8:1:1.\nThe available metadata is used as our multimodal information. Specifically, for the text modality, we concatenate the item's title, description, brand, and categories. We then apply a pretrained SBERT model [14] to extract the textual features, resulting in a 384-dimensional vector. Visual features from each item's image are extracted using a Deep CNN, producing a 4096-dimensional vector as presented by [19]. Finally, we construct the rating matrix based on the user-item interactions."}, {"title": "5.1.2 Metrics", "content": "There are several widely-used evaluation metrics for recommendation tasks, including accuracy, recall, precision, Normalized Discounted Cumulative Gain (NDCG), and hit rate. These metrics are commonly used for evaluating recommender systems, where higher values signify better performance in identifying relevant items. Given that the rating matrix represents user-item interactions, we define an interaction as $r_{ij} = 1$ (positive) if a user has interacted with an item, and $r_{ij} = 0$ (negative) otherwise. Consequently, the dataset contains both positive (interaction) and negative (no interaction) labels.\nDue to the typically high sparsity of recommendation datasets, often exceeding 90%, the dataset is inherently unbalanced, with the majority of labels being negative. In such cases, accuracy can be misleading, as a model could achieve a high accuracy"}, {"title": "5.1.3 Compared methods", "content": "We compare CADMR with seven multimodal recommender systems baselines, namely,\n\u2022 LATTICE [19] introduces a modality-aware structure learning layer that generates item-item structures for each modality and combines them to create latent item graphs. Graph convolutions on these graphs explicitly embed high-order item affinities into item representations, which can be integrated into existing collaborative filtering models to enhance recommendation accuracy.\n\u2022 BM3 [20] begins by bootstrapping latent contrastive views from user and item representations using simple dropout augmentation. It then simultaneously optimizes three multimodal objectives to learn user and item representations by reconstructing the user-item interaction graph and aligning modality features across both inter- and intra-modality perspectives.\n\u2022 The two primary components of SLMRec [21] are (1) data augmentation of multimodal content, which generates multiple item views using three operators: feature dropout (FD), feature masking (FM), and fine and coarse feature spaces (FAC); and (2) contrastive learning, which separates an item's view from others' views to provide extra supervisory signals.\n\u2022 ADDVAE [22] introduces a second set of disentangled user representations learned from textual content and aligns these with the original set, improving both recommendation effectiveness and representation interpretability.\n\u2022 FREEDOM [23] proposes a straightforward yet effective model that freezes the item-item graph while simultaneously denoising the user-item interaction graph to enhance multimodal recommendations.\n\u2022 DRAGON [24] builds a user-user graph based on common item interactions and an item-item graph from multimodal item features. It uses graph learning on both the"}, {"title": "5.2 Performance Comparison Analysis", "content": "Our proposed model, CADMR, demonstrates a substantial improvement over state-of-the-art multimodal recommender systems across key metrics, including NDCG@10, NDCG@20, Recall@10, and Recall@20, as demonstrated in Table 2. Specifically,"}, {"title": "5.3 Cold start Analysis", "content": "We wanted to evaluate the model's resilience to the cold-start problem by varying the size of the training data from 80% to 20%. The results, depicted in Fig. 2, show the NDCG@10 performance on the Sports and Baby datasets.\nAs expected, the NDCG@10 score decreases as the training data size diminishes, which is a common manifestation of the cold-start problem. The reduction in available training data limits the model's ability to learn user-item interactions effectively, leading to a decline in recommendation quality. For the Sports dataset, the NDCG@10 progressively drops to approximately 13% when the training data is reduced to 20%. Similarly, the Baby dataset follows a similar outcome, with NDCG@10 decreasing to just below 13% over the same range.\nDespite this decrease in performance, it is important to note that our model, CADMR, still outperforms existing methods across all training sizes. Even at the lowest training size of 20%, CADMR maintains a higher NDCG@10 than what many competing models achieve with a full dataset. This demonstrates the robustness of CADMR in handling sparse data scenarios and mitigating the cold-start problem better than prior state-of-the-art models. These results underline CADMR's strong generalization capability, making it a competitive solution even in data-constrained environments."}, {"title": "5.4 Ablation Study", "content": "In this section, we present an ablation study to evaluate the contribution of two key components in our model, CADMR: the cross-attention mechanism and the disentangled representation learning (DRL) module. By systematically removing these components, we measure their individual impact on the overall performance. We compare the following variants:"}, {"title": "5.5 Effect of Cross-Attention heads number", "content": "We varied the number of cross-attention heads to assess their effect on our model's performance across the Baby, Sports, and Electronics datasets. As the number of heads increased from 1 to 4, a clear improvement in NDCG@10 scores was observed in all datasets. This steady improvement indicates that adding more cross-attention"}, {"title": "6 Conclusion", "content": "In this paper, we introduced CADMR, an autoencoder-based multimodal recommender system designed to address the challenges of high-dimensional, sparse user-item rating matrices. CADMR integrates multi-head cross-attention mechanisms and disentangled learning to effectively leverage diverse multimodal data, enhancing both the reconstruction of the rating matrix and the representation of user-item interactions. By disentangling modality-specific features while preserving their interdependence, CADMR learns a joint latent representation that significantly improves recommendation performance. Extensive evaluations on multiple benchmark datasets demonstrated its superior performance over state-of-the-art methods, as well as the critical role of the cross-attention and disentangled learning modules. In future work, we will further analyze CADMR's scalability and explore potential avenues for enhancing its effectiveness."}]}