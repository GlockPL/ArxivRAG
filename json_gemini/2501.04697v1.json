{"title": "GROKKING AT THE EDGE OF NUMERICAL STABILITY", "authors": ["Lucas Prieto", "Melih Barsbey", "Pedro A.M. Mediano", "Tolga Birdal"], "abstract": "Grokking, or sudden generalization that occurs after prolonged overfitting, is\na surprising phenomenon that has challenged our understanding of deep learn-\ning. While a lot of progress has been made in understanding grokking, it is\nstill not clear why generalization is delayed and why grokking often does not\nhappen without regularization. In this work we argue that without regulariza-\ntion, grokking tasks push models to the edge of numerical stability, introducing\nfloating point errors in the Softmax that we refer to as Softmax Collapse (SC).\nWe show that SC prevents grokking and that mitigating SC leads to grokking\nwithout regularization. Investigating the root cause of SC, we find that be-\nyond the point of overfitting, the gradients strongly align with what we call\nthe na\u00ef ve loss minimization (NLM) direction. This component of the gradient\ndoes not change the predictions of the model but decreases the loss by scaling\nthe logits, usually through the scaling of the weights along their current direc-\ntion. We show that this scaling of the logits explains the delay in generaliza-\ntion characteristic of grokking, and eventually leads to SC, stopping learning\naltogether. To validate these hypotheses, we introduce two key contributions\nthat mitigate the issues faced in grokking tasks: (i) StableMax, a new activa-\ntion function that prevents SC and enables grokking without regularization, and\n(ii)\nGrad, a training algorithm that leads to quick generalization in grokking\ntasks by preventing NLM altogether. These contributions provide new insights\ninto grokking, shedding light on its delayed generalization, reliance on regu-\nlarization, and the effectiveness of known grokking-inducing methods. Code\nfor this paper can be found at: https://github.com/LucasPrietoAl/\ngrokking-at-the-edge-of-numerical-stability.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning has been transformative for a variety of fields such as natural language process-ing (Devlin et al., 2019), computer vision (Krizhevsky et al., 2012), geometry processing (Qi et al.,\n2017), and 3D vision (Deng et al., 2018). This rapid proliferation has brought with it surprising\nphenomena that defy the predictions of classical statistical learning theory.\nIn this paper we explore one such recently observed phenomenon known as grokking, first described\nby Power et al. (2022) as a sudden and unexpected generalization occurring after prolonged overfit-ting. Although predominantly studied in algorithmic tasks like modular addition or multiplication,\nrecent findings suggest that grokking may be a more pervasive phenomenon, also manifesting in\nmore complex tasks involving vision and language (Lv et al., 2024; Humayun et al., 2024).\nPrior research has consistently observed grokking in settings that involve some form of regulariza-tion, such as weight decay (Barak et al., 2022; Power et al., 2022; Nanda et al., 2023). This pattern\nhas motivated investigations into the implicit biases introduced by weight decay, suggesting it may\nbe critical to triggering delayed generalization. For instance, Liu et al. (2023a) argued that weight\nnorms need to be in a narrow range or \u201c Goldilocks Zone \u201d for generalization. Similarly, Varma et al.\n(2023) highlighted weight efficiency of generalizing solutions, and Nanda et al. (2023) argued that\nweight decay favors simpler, more generalizable solutions. However, recent works have argued that\nregularization may not be necessary for grokking, at least on shallow networks with Mean Squared"}, {"title": "2 SETUP", "content": "We show our findings on the most commonly studied grokking datasets, outlined in this section."}, {"title": "2.1 DATASETS", "content": ""}, {"title": "I. Modular arithmetic.", "content": "The main results in this paper are shown on arithmetic modulo 113 (Power\net al., 2022; Nanda et al., 2023). This is a family of supervised learning tasks where two one-hot\nencoded inputs representing integers a, b < p are used to predict the target y = a*b mod p, where\n* is some binary operation and p is a prime number. In most of our results, the binary operation is\naddition, but we show additional results with multiplication and subtraction.\nModular arithmetic tasks are characterized by a binary operation and a dataset size, with different\nbehaviours being observed for different dataset sizes on the same binary operation. In these settings,\nwe describe the dataset sizes as the percentage of the 1132 possible pairs that are used for training,\nwith the rest of the data being used for testing as in Nanda et al. (2023) and Power et al. (2022).\nOur main results use a 40%/60% train/test split but we also include results using 60%/40% and\n70%/30%. The input integers are represented as one-hot vectors."}, {"title": "II. Sparse parity.", "content": "We also validate some of our results on the Sparse Parity task outlined in Barak\net al. (2022). This is a supervised learning setting where the target is the parity of k bits out of a\nbinary vector of length n, with k \u226a n. In this work we use 2000 samples, split evenly between train\nand test data and we describe instances of this task by specifying the values of n and k."}, {"title": "III. MNIST.", "content": "Finally, we provide some results on a subset the classic image classification dataset\nMNIST (Deng, 2012). For our experiments, we use a subset of 200 training samples from the\ntraining set as in Liu et al. (2023b), with evaluation on the full test set."}, {"title": "2.2 MODELS", "content": "We study the grokking phenomenon on these datasets using a 2-hidden layer multi-layer perceptron\n(MLP) of width 200 as in Liu et al. (2023a) and a one-layer transformer with 4 attention heads as\nNanda et al. (2023) and Power et al. (2022). We train both of these models in a full batch setting,\nusing ReLU activations and cross-entropy loss with AdamW and SGD, as well as our own variants\nof these optimizers, AdamW and SGD. Unless specified otherwise we set the weight decay\nparameter X = 0. For modular arithmetic datasets, inputs are concatenated as the input of the MLP\nresulting in a 226 dimensional vector, and treated as separate tokens in the case of the transformer."}, {"title": "3 SOFTMAX COLLAPSE: FLOATING POINT ERRORS PREVENT GROKKING", "content": "Given our current understanding of grokking, it is surprising that it happens without regularization\nfor some dataset sizes, but regularization becomes crucial as dataset size decreases (Power et al.,\n2022). In this section we highlight that looking at datasets at the boundary of these two regimes\nreveals that without weight decay, grokking sometimes starts before abruptly stopping (Fig. 2). We\nshow that this is caused by floating point errors in the Softmax that lead the gradients from a large\nfraction of the samples to become zero. We refer to this phenomenon as Softmax Collapse."}, {"title": "3.1 SOFTMAX COLLAPSE", "content": "In modern neural network implementations, Floating Point (FP) arithmetic is ubiquitous for repre-senting and computing parameters, activations, and gradients. While FP numbers enable efficient\ndecimal computations, they introduce numerical inaccuracies. This section focuses on absorption\nerrors, as a specific class of FP arithmetic failure. We will use the symbol = to refer to equality\nunder FP arithmetic."}, {"title": "Definition 1 (Absorption Errors).", "content": "Let a, b \u2208 R \\ {0} be floating point numbers in a system with\nbase \u03b2 and p significand bits. Denote their exponents by ea and eb, respectively. An absorption error\noccurs in the computation of a + b (denoted a + b = a) if\n$$e_a - e_b \\geq p.$$\nIn this case, after exponent alignment, the significand of b is shifted right by at least p digits, and b\ncannot be represented in the available precision, resulting in a + b = a.\nIntuitively, absorption errors can occur during FP addition when operands have significantly differ-ent magnitudes. For float32 the base \u03b2 is 2 and p = 24 bits, meaning that adding any number\nsmaller than 2-(p-1) = 2-23 to 1 will leave 1 unchanged. 2-23 is the machine epsilon for float32."}, {"title": "Definition 2 (Softmax Cross-Entropy (SCE) loss).", "content": "For a neural network f and a data point x with\nlabel y, we define z := f(x) and zy as the logit corresponding to the true class y. We express the\nSCE loss as well as its equivalent numerically more stable formulation as:\n$$L_{SCE}(f(x), y) = - \\log (\\frac{e^{z_y}}{\\sum_{k=1}^{n} e^{z_k}}) + max(z) + \\log (\\sum_{k=1}^{n} e^{z_k - max(z)}).$$\nUnfortunately, even the rightmost (comparatively more stable) variant does not address this problem,\nsince the kind of FP errors discussed in this work appear in the sum. While the Softmax function\noutputs are bounded between 0 and 1, the intermediate calculations involve summing exponentials\nof both positive and negative logits. These values can span several orders of magnitude, particularly\nin scenarios with large logits where the loss approaches zero. This wide range of values creates\nconditions that lead to absorption errors \u2013 leading to the phenomenon we call Softmax Collapse."}, {"title": "Definition 3 (Softmax Collapse (SC)).", "content": "A specific case of absorption error occurs when, for a given\nsample x, the logit from the correct class zy is significantly larger than the logits for all other classes.\nThis floating-point absorption of smaller terms, which we call Softmax Collapse, occurs when:\n$$\\sum_{k=1}^{n} e^{z_k} = e^{z_y},$$\nin which case the SCE loss becomes:\n$$L_{SCE}(f(x), y) = = - \\log (\\frac{e^{z_y}}{e^{z_y}}) = 0.$$\nThus, during SC the loss becomes identical to zero. Furthermore, for the correct class, the gradients\nbecome zero as well:\n$$\\frac{\\partial L_{SCE}}{\\partial z_c} = \\frac{e^{z_c}}{\\sum_{k=1}^{n} e^{z_k}} - 1\\{c=y\\} = 1 - 1\\{c=y\\}.$$\nWhile weights that contribute to the wrong classes can still get negative updates, we show that\ndisappearance of the gradients from the correct classes is enough to inhibit grokking (Fig. 2). We\nvalidate this in App. B.1 with an explicit intervention, showing that artificially setting the gradients\nfrom the correct class to zero stops generalization in a very similar way to what we observe in Fig. 2."}, {"title": "3.2 EVIDENCE OF SOFTMAX COLLAPSE IN GROKKING TASKS", "content": "Grokking is often studied using dataset sizes for which the delay in generalization is significant,\nwhich is usually when the dataset is small but just large enough that generalization is possible. In"}, {"title": "3.3 PREVENTING SOFTMAX COLLAPSE LEADS TO GROKKING", "content": "To validate the importance of FP errors in stopping grokking, we show that methods to avoid SC lead\nto generalization on all the common grokking tasks on both MLPs and transformers. We introduce\nthe following methods to postpone the appearance of FP errors."}, {"title": "Increasing floating point precision.", "content": "The simplest way to avoid SC is to extend the FP precision\nfrom float32 to float64 for the Softmax calculation. We see in Fig. 2 that networks trained using\nfloat64 in the Softmax face SC later in training which allows for a further increase in test perfor-mance. Conversely, using float16 leads to SC earlier in training, leading to lower test performance.\nWhile this approach works as expected, FP precision cannot be extended indefinitely to allow for\ngeneralization as seen in the lack of grokking in Fig. 2a."}, {"title": "StableMax Cross Entropy (StCE) Loss.", "content": "As demonstrated above, SC is caused by adding the\nexponentials of very large positive and negative logits in the Softmax. To avoid these extreme\nsummands, we propose using a softer version of Softmax to transform logits into probabilities\nbefore calculating the CE Loss:"}, {"title": "Definition 4 (StableMax).", "content": "We introduce a numerically stable version\nof the Softmax as:\n$$StableMax(x_i) := \\frac{s(x_i)}{\\sum_j s(x_j)},$$\nwhere\n$$s(x) := \\begin{cases}\nx+1 & \\text{if } x \\geq 0, \\\\\n\\frac{1}{1-x} & \\text{if } x < 0\n\\end{cases}$$"}, {"title": "Proposition 1.", "content": "StableMax is a modified Softmax, i.e. StableMax (xi) = Softmax (g (xi)) where\n$$g(x) = \\begin{cases}\n\\log(x + 1) & \\text{if } x \\geq 0, \\\\\n-\\log(-x + 1) & \\text{if } x < 0\n\\end{cases}$$\nThe proof of this Proposition is presented in App. A. We then define the numerically stable analogue\nof LSCE as LSTCE(f(x), y) = - log (StableMax(zy)), where zy again corresponds to the logit of\nthe true class y.\nTo show that StCE indeed addresses the problems posed by SC, we repeat our experiments\nin Sec. 3.2 by replacing Softmax with StableMax. Our results, presented in Fig. 4, indeed show\nthat StableMax leads to grokking in commonly studied settings without regularization. Notably,\nthis happens while the norm of the weights increases substantially (Fig. 4, middle). This suggests\nthat while weight decay may lead to both grokking and a decreasing weight norm, the decreasing"}, {"title": "4 DIAGNOSING THE CAUSES OF SOFTMAX COLLAPSE", "content": "In the previous section we have shown that FP errors arise due to a combination of low losses and\nlarge logits, and shown that when FP errors are mitigated, grokking can be observed in conditions\nwhere it previously was not. In this section, we dive deeper and ask why extremely low losses and\nlarge logits appear in the first place in grokking tasks. We identify two main causes for this tendency:\n(i) easiness of overfitting in grokking tasks, and (ii) a training dynamic that sees gradients align with\nwhat we call na\u00ef ve loss minimization direction. After diagnosing the causes, the following section\nwill use these insights to develop an optimization algorithm that avoids NLM in the first place."}, {"title": "4.1 EASE OF OVERFITTING IN GROKKING TASKS", "content": "The first important characteristic of grokking tasks that lead to SC is their ease of overfitting. It has\nbeen observed that as grokking datasets get larger, overfitting becomes harder, eventually leading\nto a regime where train and test performances increase in tandem (Power et al., 2022; Nanda et al.,\n2023; Varma et al., 2023). It has also been shown that generalization can be delayed in the Sparse\nParity task by increasing the amount of noise in the input, which makes overfitting easier (Barak\net al., 2022). Here we investigate the opposite effect: that by decreasing the dimensionality of the\ninput the data becomes harder to memorize, removing the delay in generalization.\nTo do this, we investigate the common grokking task of modular addition, but instead of the high-dimensional one-hot representations of the input integers, we use a more compact binary. More\nspecifically, we assign each integer a distinct random binary vector of dimension 14.\nResults confirm our hypothesis, showing that as input representations are decreased in dimension,\noverfitting is prevented and models generalize without need for regularization (Fig. 4, right). This\nalso shows that modular addition only induces grokking depending on the choice of representation.\nThese findings highlight the importance of understanding the training dynamics beyond the point of\noverfitting (i.e. point of achieving 100% training accuracy), rather than focusing on the specifics of\nthe modular arithmetic tasks as the key to explaining the delay in generalization."}, {"title": "4.2 NA\u00cfVE LOSS MINIMIZATION", "content": "We next identify a crucial training dynamic that commonly occurs in grokking tasks as a central\ncause for increasing logits and SC. We find that after reaching 100% training accuracy, gradient\nupdates are dominated by an update direction we term na\u00ef ve loss minimization (NLM). This direction"}, {"title": "Definition 5 (Na\u00efve Loss Minimization (NLM)).", "content": "A function dNLM: Rm \u2192 Rm specifies a direction\nof na\u00efve loss minimization if it decreases the loss,\n$$L(f(\\theta + d_{NLM}(\\theta);\\cdot)) < L(f(\\theta; \\cdot)),$$\nwhile satisfying for some c > 1:\n$$f(\\theta + d_{NLM}(\\theta); x) = cf(\\theta;x), \\forall x \\in X,$$\nwhere X denotes the input space and L(f(\u03b8+dnLM(\u03b8); \u00b7)) is the total loss over the training dataset.\nWe find that under a large class of models, namely those that demonstrate positive homogeneity,\nwhen training beyond 100% training accuracy the direction of the weights is an NLM direction."}, {"title": "Definition 6 (Positive Homogeneity (Lyu & Li, 2020)).", "content": "A function f is positively homogeneous of\ndegree L > 0 if for all weights 0, inputs x, and scalars c > 0, it satisfies:\n$$f(c\\theta; x) = c^L f(\\theta; x) .$$\nWhen f is a homogeneous neural network, L corresponds to the number of layers.\nIn the case of homogeneous networks, training beyond 100% training accuracy, scaling the logits\nalways leads to a decrease in the training loss. Therefore, dNLM(\u03b8) = \u03b1\u03b8 for a > 0 is an NLM\ndirection, as it results in f(\u03b8 + dnLM(0); x) = f((1 + \u03b1)\u03b8;x) = (1 + a)Lf(0;x), where the\nsecond equality follows from Eq. (10).\nMany neural network architectures, such as ReLU MLPs and transformers without bias terms, are\npositively homogeneous or approximately homogeneous in the case of transformers (Merrill et al.,\n2020). While more complex deep learning models with skip connections and bias terms are not\nhomogeneous, they have been shown to be quasi-homogeneous (Kunin et al., 2023) and in most\ncases \u2014 including all of the models in this work, the last layer is homogeneous. This means that for\nnon-homogeneous models scaling the weights of the last layer corresponds to a direction of NLM.\nThe fact that the gradients converge to the direction of the weights has been studied in previous\nworks (Ji & Telgarsky, 2020; 2019; 2018; Lyu & Li, 2020) to prove that homogeneous networks\nconverge in direction under gradient flow and gradient descent (GD), and they perform normalized\nmargin maximization even beyond the point of 100% training accuracy (Lyu & Li, 2020). However,"}, {"title": "5 MITIGATING NA\u00cfVE LOSS MINIMIZATION LEADS TO GROKKING", "content": "While we have shown in Sec. 3 that avoiding numerical instabilities eventually leads to generaliza-tion, we can also target the NLM process that causes these numerical issues. To do this, we design\nan optimizer that only preserves the part of the gradient orthogonal to the direction of the weights."}, {"title": "5.1 Grad: AN OPTIMIZER TO PREVENT NLM", "content": "We propose a new optimizer, Grad (read \u201cortho-grad\u201d), that updates the weights based only on\nthe part of the gradient that is orthogonal to the current direction of the weights:"}, {"title": "Definition 7 (Grad).", "content": "We propose the following update rule for a given iteration t \u2208 N:\n$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\perp}L(\\theta_t),$$\nwhere the orthogonal component of the gradient, \u2207\u22a5L(\u03b8t), is obtained by projection onto the hy-perplane orthogonal to the current weight vector:\n$$\\nabla_{\\perp}L(\\theta_t) = \\nabla L(\\theta_t) - \\frac{(\\theta_t^T \\nabla L(\\theta_t))}{(\\theta_t^T \\theta_t)} \\theta_t.$$\nProposition 2. Assuming \u2207\u22a5L(0+) \u2260 0, \u2203 \u03b2 > 0 such that for any learning rate 0 < \u03b7 < \u03b2, taking\nthe step n\u2207L(0t) reduces the loss. In other words, any nonzero \u2207\u2081L(0+) is a descent direction.\nSketch of the proof. We show that any \u2207\u2081L(0t) \u2208 Rm\\{0} is a descent direction by demonstrating\nthat (-_L(0+), \u2207L(0t)) < 0.\nThis projection of the gradient can be incorporated into different optimizers. In Fig. 6a, we show\nresults for AdamW and SGD, the Grad versions of AdamW and SGD respectively. These\nresults show that Grad optimizers lead to generalization without a phase of initial overfitting, in\ncontexts where no improvement in test performance is usually observed without weight decay. We"}, {"title": "5.2 EXPLAINING THE SUCCESS OF EXISTING METHODS FOR GROKKING", "content": "In light of our findings, we are able to explain the success of several previously proposed methods to\ninduce grokking. We find that these methods also lead to grokking by mitigating NLM and avoiding\nthe FP errors that come with extremely low losses.\nWeight decay. We have argued that the problem faced in grokking is that the ease of overfitting\nleads to NLM, which corresponds to scaling up the weights for homogeneous networks. Since\nweight decay corresponds to pulling back the weights along this same direction at every step during\ntraining, it is unsurprising, given our findings, that it is the most reliable way to induce grokking.\nTo explain why generalization tends to be delayed when using weight decay, as opposed to Grad,\nwe look at it from the perspective of L2 regularization which is equivalent to weight decay for SGD.\nIn Fig. 6c, we see an initial phase where classification loss decreases, at the cost of the L2 loss.\nEventually, the decrease in classification loss from NLM stops outweighing the increase in L2 loss,\nmeaning that only updates that are not aligned with the NLM direction are followed. This explains\nwhy weight decay leads to generalization in grokking tasks but only after scaling along the NLM\ndirection no longer decreases the overall loss. This balance between weight decay and classification\nloss is similar to the rotational equilibrium studied in Kosson et al. (2024).\nWe argue that the main roles of weight decay are preventing floating point errors and preventing\nNLM. This is in line with recent findings about the role of weight decay in deep learning (D\u2019Angelo\net al., 2023) which point to the fact that it increases the effective learning rate and avoids floating\npoint issues when using mixed-precision training in LLMs.\nMSE loss on shallow networks. While cross-entropy loss can be reduced indefinitely by scaling\nthe logits through NLM, this is not the case with MSE loss. When using MSE loss the logits can\novershoot the target, meaning that larger logits often do not lead to a lower MSE loss. This explains\nwhy Barak et al. (2022), Kumar et al. (2024), and Lyu et al. (2024) observed grokking with MSE loss\nwithout regularization. Interestingly, networks with more than one hidden layer do not generalize in\nthese same settings (Fig. 13).\nDelaying generalization by scaling the weights. While the lazy training dynamics described in\nKumar et al. (2024) explain an important part of why scaling the weights delays generalization,"}, {"title": "6 RELATED WORK", "content": "Grokking. Power et al. (2022) introduced grokking and showed that weight decay can consistently\ninduce it in algorithmic tasks. Nanda et al. (2023) were able to reverse engineer the inner work-ings of a grokked transformer and found progress measures for grokking induced by weight decay.\nChughtai et al. (2023) generalized the findings from Nanda et al. (2023) and showed grokked net-works use group representations to solve group composition tasks, although some of these findings\nwere disputed in Stander et al. (2024) which propose that grokked networks learn a coset based\nalgorithm for these same tasks. Mallinar et al. (2024) has shown that grokking is not specific to\nneural networks or gradient-based optimization and cannot be predicted from the training or test\nloss. Varma et al. (2023) argued that grokking is driven by weight decay favoring more efficient\nsolutions and Liu et al. (2023b) hypothesized that the weight norm of the models needs to be in a\n\"Goldilock's zone\" to generalize. Kumar et al. (2024) and Lyu et al. (2024) connected grokking to a\ntransition between \"lazy training\u201d (Chizat et al., 2018) and feature learning, and Kumar et al. (2024)\nshowed that this can happen without regularization in the case of shallow networks with MSE loss.\nGrokking has also been described as a phase transition by \u017dunkovi\u010d & Ilievski (2024), Lyu et al.\n(2024) and Rubin et al. (2024). Humayun et al. (2024) show that in many settings, neural networks\nundergo grokking-like transitions in their adversarial robustness. This aligns with the findings of\nLyu & Li (2020) which attributed this increased robustness to a bias of SGD towards a max-margin\nsolution which was proven for homogeneous models.\nNumerical instability in deep learning. Numerical instability is a common issue in deep learning\nKloberdanz et al. (2022), especially when dealing with mixed precision training D\u2019Angelo et al.\n(2023). It is known that the Softmax function is particularly prone to numerical stability problems\nalthough this often comes in the form of overflow in the exponential (Kloberdanz et al., 2022) and\nnot from absorption errors in the sum as observed in this case. In the grokking setting, Nanda et al.\n(2023) showed that the slingshots observed in Thilak et al. (2022) can be explained by a very similar\nmechanism to the one involved in SC, although Nanda et al. (2023) do not use it to explain any\ngrokking phenomena beyond these spikes that sometimes appear in the training process in grokking\ntasks. We believe the slingshots observed in Thilak et al. (2022) could be a mechanism to prevent\nfull SC, explaining why slingshots can lead to grokking without weight decay in some settings. This\nis further discussed in App. H. Issues with numerical instability when training beyond overfitting\nwith increasing learning rates were also observed in Lyu & Li (2020)."}, {"title": "7 CONCLUSION AND DISCUSSION", "content": "In this work, we show that na\u00ef ve loss minimization (NLM) and floating point errors can explain why\ngeneralization is delayed in grokking and why it often does not happen without regularization. Using\nthis insight, we are able to explain the success of existing methods to induce grokking. Motivated\nby our findings, we further design a simple modification to the Softmax that induces grokking by\navoiding floating point errors and an optimizer that avoids the delay in generalization in grokking\nby preventing NLM."}, {"title": "Limitations & future work.", "content": "While this work explains several surprising aspects of grokking set-tings, several questions remain. Notably, we focus our study of NLM on homogeneous or approx-imately homogeneous models. A a formal characterization quasi-homogenous models could shed\nlight on this kind of dynamics for models including skip connections and bias terms. Additionally,\nour explanation for why weight decay causes grokking could be enhanced by an analysis of the im-\npact of weight decay on the effective learning rate as a potential explanation for the sudden nature\nof grokking."}, {"title": "APPENDIX", "content": "In support of the main paper, App. A presents the proofs for the propositions in the paper, App. B includes\nadditional findings that support our main results, and App. D provides further discussion on conditions that\nlead to grokking."}, {"title": "A PROOFS", "content": "Proof of Prop. 1.\n$$\\text{Softmax} (g(x_i)) = \\frac{e^{g(x_i)}}{\\sum_j e^{g(x_j)}}$$\n$$\\frac{e^{\\log(x_i + 1)}}{\\sum_j e^{\\log(x_j + 1)}} \\text{ if } x_i \\geq 0,$$\n$$\\frac{e^{-\\log(-x_i+1)}}{e^{-\\log(-x_i+1)} \\text{ if } i < 0}$$\n$$\\frac{x_i+1}{\\sum_j |x_j|+1} \\text{ if } x_i \\geq 0,$$\n$$\\frac{-x_i+1}{\\sum_j |x_j|+1} \\text{ if } x_i < 0$$\n$= StableMax(x_i).$$\n\u03a0"}, {"title": "Proof of Prop. 2.", "content": "To prove that any nonzero -\u2207\u2081L(0t) is a descent direction, we need to show that\n((-\u2207\u22a5L(0+), L(0+)) < 0, assuming \u2207\u2081L(0t) \u2260 0:\n$$\\langle (- \\nabla_{\\perp} L(\\theta_t), \\nabla L(\\theta_t) \\rangle < 0.$$\nExpanding this yields:\n$$-||\\nabla L(\\theta_t)||^2 + \\frac{\\langle \\nabla L(\\theta_t), \\theta_t \\rangle}{\\theta_t} \\leq 0.$$\nSince the inequality is unaffected by the scaling of the left hand side, we can, without loss of generality, assume\nthat the gradients are normalized, leading to:\n$$\\langle \\nabla L(\\theta_t), \\frac{\\theta_t}{\\theta_t} \\rangle \\leq 1.$$\nSince the gradient, L(0)\ndescend upon.|| \u2207 ||||| L(0), the weights by the the assumption\nWe note that the Grad stops when \u2207L(0t)\n\n0 (0) L(0t) > 1,will 1. We that the case 12(10\n==444,2004> the <4,0> < the the case if VL(0t)\",,2003> the14<5005<14,200the, 0414.2004> the <40,0> <=<4.2004,2004.2004\""}, {"title": "B ADDITIONAL FINDINGS", "content": ""}, {"title": "B.1 FURTHER EVIDENCE THAT SC PREVENTS GROKKING", "content": "While SC leads the gradient from correctly predicted samples to be zero, it does not do this for the incorrect\nclasses. To validate that setting the gradients from the correct classes to zero is enough to stop learning, we do\nthis artificially for a model that is generalizing and show that learning stops after this intervention. In Fig. 8 we\nsee that the baseline model shown in geen generalizes, but this is stopped at epoch 6000 for the model shown\nin blue, after we perform this intervention."}, {"title": "B.2 SGD WITH LEARNING RATE SCHEDULING", "content": "To show that our results are not due to the inductive\nbias of adaptive moments in optimizers like AdamW,\nwe replicate some of the AdamW results using SGD\nwith a learning rate scheduler. Our scheduler is simi-lar to the one in Lyu & Li (2020) except at each step\nwe divide the learning rate by the norm of the full\ngradient, instead of the loss. In Fig. 9 we observe\nthat SC also puts an end to grokking in this setting."}, {"title": "C EFFECTIVE LEARNING RATE", "content": "Unexplored in the main paper, NLM also has the effect of reduc-ing the effective learning rate. For a gradient update using regu-lar gradient descent 0t+1 = \u03b8t \u2013 n\u2207L(0t) it is easy to see that\n||0t+1 - 0t|| \u2192 0"}]}