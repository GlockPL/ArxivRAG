{"title": "COMBINING KNOWLEDGE GRAPHS AND LARGE LANGUAGE MODELS", "authors": ["Amanda Kau", "Xuzeng He", "Aishwarya Nambissan", "Aland Astudillo", "Hui Yin", "Amir Aryani"], "abstract": "In recent years, Natural Language Processing (NLP) has played a significant role in various Artificial Intelligence (AI) applications such as chatbots, text generation, and language translation. The emergence of large language models (LLMs) has greatly improved the performance of these applications, showing astonishing results in language understanding and generation. However, they still show some disadvantages, such as hallucinations and lack of domain-specific knowledge, that affect their performance in real-world tasks. These issues can be effectively mitigated by incorporating knowledge graphs (KGs), which organise information in structured formats that capture relationships between entities in a versatile and interpretable fashion. Likewise, the construction and validation of KGs present challenges that LLMs can help resolve. The complementary relationship between LLMs and KGs has led to a trend that combines these technologies to achieve trustworthy results. This work collected 28 papers outlining methods for KG-powered LLMs, LLM-based KGs, and LLM-KG hybrid approaches. We systematically analysed and compared these approaches to provide a comprehensive overview highlighting key trends, innovative techniques, and common challenges. This synthesis will benefit researchers new to the field and those seeking to deepen their understanding of how KGs and LLMs can be effectively combined to enhance AI applications capabilities.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of natural language processing (NLP) in recent years can be attributed to the availability of large datasets and the surge in computing power. Consequently, numerous large language models (LLMs) have been developed, such as Google's BERT [1] and T5 [2], OpenAI's GPT series[3]. LLMs are widely used in various tasks, including language translation, content creation, and virtual assistants. They excel in text generation, enabling applications such as automated essay writing, report generation, and creative storytelling. LLMs provide highly accurate translations in language translation, facilitating communication across different languages. They are also used in chatbots and customer service to handle queries efficiently and provide personalized responses. Additionally, LLMs assist in summarizing large volumes of text, extracting key information from documents, and performing sentiment"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Large Language Models (LLMs)", "content": "Various PLMs have been released in the past, including BERT [1] and GPT-1 [3]. BERT (Bidirectional Encoder Representations from Transformers) was released in 2018, featuring a transformer-based model that understood contexts bidirectionally [1]. The model could consider both the preceding and following words when processing input text, allowing it to accurately capture the meaning of words and sentences. The first GPT (Generative Pre-trained Transformer) was also released in the same year, which focused on generating text by predicting the next word [3]. Further developments saw the success of extending PLMs to LLMs by increasing their size and complexity, giving rise to their remarkable ability to comprehend natural language.\nLLMs nowadays are all based on the transformers architecture [12], which excels in handling long sequences due to its characteristic self-attention mechanism. LLMs generally take a sequence of text or code called a prompt as the input. The tokenizer, a part of the LLM architecture, subsequently converts the input into a list of tokens and feeds them into the model, where each token is a word in the input prompt. New tokens are generated one by one until they reach a special token indicating the end of the generation, or the total length exceeds the limit. These generated tokens are converted back into the text format as the final output of the model. This process can be formally modelled through some equations. The input prompt, after being tokenized, can be seen as a list of tokens x, where x = [x1,..., xn] if there are n tokens in total. LLMs based on the transformer architecture usually maintain a sequence of hidden states. At step t, hidden state ht can be calculated using the current token xt and all the previous hidden states:\nht = LLM(xt, [ho,..., hn\u22121]) (1)\nThe model then further transforms ht into a probability distribution, which can be used to sample the next generated token:\nP(x) = \\prod_t P(xt | [ho,..., ht\u22121]) (2)\nIn this case, due to its ability to recognise, summarise, translate, predict, and generate text and other forms of content, current LLMs have a wide range of applications, including question answering and code generation. More recently, some focus has been directed towards multimodal LLMs, which give LLMs abilities like the capabilities of vision in Google's Gemini [13] and GPT-4 with vision (GPT-4V) [14]."}, {"title": "2.2 Knowledge Graphs (KGS)", "content": "A KG is a directed labelled graph in which nodes represent any real-world entity or concept, and edges represent the relationships between nodes. This structured data format has proven effective and applicable to a wide variety of domains, including biology and finance, as well as in modelling social networks or storing general information like in the Google Knowledge Graph. Aside from being able to model relationships, KGs can provide further insight into a word's semantics via its context or neighbouring nodes. The graph structure can also be studied, as was done in [15] to uncover research trends over time, or in [16] where structure provided insight into the impact and success of research publications. Data in KGs is usually presented as a (subject, object, predicate) triple. This could be extended in the case of temporal knowledge graphs, which use a quadruple representation (subject, object, predicate, timestamp) to capture facts over time.\nThe construction of a KG involves three general steps: knowledge acquisition, knowledge refinement, and knowledge evolution [17]. Knowledge acquisition involves collecting information about entities and relations from multi-structured data to build the KG. Since extracted triples could be incomplete, the next knowledge refinement step fixes these issues with additional data. Finally, the evolution of real-world knowledge over time may not be reflected in the static KGs"}, {"title": "3 LLMs Empowered by KGs", "content": "Knowledge injection. A myriad of techniques have been implemented in research and industry to perform knowledge injection using KGs, usually including additional knowledge in LLM prompts. For instance, Baek et al. [20] proposed KAPING (Knowledge-Augmented language model PromptING), which retrieved facts from a KG and prepended them to input questions to construct LLM prompts for zero-shot question answering. A similar approach was adopted in Sen et al. [21] where, instead of the previous approach, the facts from the KG were weighted by a Knowledge Graph Question Answering (KGQA) before being fed into the LLM. In KICGPT (Knowledge In Context with GPT) [22], retrieved KG facts were re-ranked by the LLM. An application example is DRAK (Domain-specific Retrieval-Augmented Knowledge), where retrieved KG facts are also useful to LLMs in the biomolecular domain, which requires structured knowledge [23]. These approaches are a mere subset of the numerous variations of techniques utilised to inject KG knowledge into LLM prompts. Rather than including retrieved KG facts into LLM prompts, Knowledge Solver [24] teaches LLMs to traverse KGs in a multi-hop way to reason the answer to a question. In this way, KGs could provide facts that LLMs could reason over, grounding them in the process. Increasing LLM explainability. On the other hand, KGs can contribute more to LLMs than by simply providing facts for knowledge grounding. For the question-answering task, QA-GNN (Question Answering Graph Neural Network) [25] performed joint reasoning over a LLM encoding of the question context and KG to unify the two representations. For better model interpretability, a graph neural network (GNN) was used to calculate weights between graph nodes, providing a path of reasoning that the model took through the KG to get to the answer. Another example is LMExplainer [26], which used a KG and graph attention neural network to understand key decision signals of LLMs, which were converted into natural language explanations for better explainability. KGs could, therefore, also allow for better interpretability of LLMs and offer insights into LLMs' reasoning processes, which in turn increase humans' trust in LLMs.\nSemantic understanding. KGs can also be applied to add semantic understanding or entity embeddings into LLMs. For instance, LUKE (Language Understanding with Knowledge-based Embeddings) [27], as an extension of BERT, is an entity-aware self-attention mechanism that can help the model treat words and entities in a given text as independent tokens and output contextualised representations of them. As for adding semantic understanding, a recent methodology called Right for Right Reasons (R3) [28] for performing KGQA using LLMs casts the problem of common sense KGQA as a tree-structured search to make full use of surfaced commonsense axioms, a key property that makes the reasoning procedure verifiable, such that semantic understandings from KGs can be added into LLMs."}, {"title": "4 KGs Empowered by LLMS", "content": "There are numerous examples of LLMs being empowered by KGs, but one could also consider the opposite direction for integration: KGs empowered by LLMs.\nTemporal forecasting. Recent research has shown that LLMs can perform forecasting with KG data, especially for a special subset of KGs having directions and timestamps, namely Temporal Knowledge Graphs (TKGs). Most advanced research on TKGs mainly focuses on predicting future facts when given historical facts, where using LLMs can be particularly beneficial. For instance, Xia and their colleagues propose a Chain-of-History (CoH) reasoning method for TKG prediction [29], where an LLM is mainly used to understand the semantic meaning of entities, relationships, and timestamps in a TKG by exploring important high-order history chains step-by-step and reasoning the answers to the query only based on inferred history chains in the last step. Alternatively, by using in-context learning (ICL) with LLMs whereby a few examples were provided to the LLM so it could learn to perform forecasting, Lee et al. [30] fed TKG facts to the LLM and found that LLMs were surprisingly capable of learning patterns from historical data. This is despite the lack of special architectures or modules usually required to perform this KG task. The ability of LLMs to perform what is typically a KG task allows for the possibility of using natural language to perform forecasting.\nKnowledge graph construction. As discussed previously, one major challenge is the time-consuming and costly construction process of KGs, another aspect to which LLMs can contribute in various ways. LLMs are trained on large, diverse datasets and store this knowledge implicitly. BertNet [31] sought to harvest KGs of arbitrary relations from LLMs, useful for general KGs. To achieve this, an initial prompt was paraphrased several times and the LLM would provide responses to each of the paraphrased prompts, which were converted into entity pairs and ranked. The top-ranking pairs formed the KG. Kommineni et al. [32] crafted a semi-automatic KG construction pipeline utilising ChatGPT-3.5, which prompted the LLM to generate high-level competency questions about the data. The LLM was instructed to extract entities and relationships from these questions to form an ontology, then map retrieved information from documents onto the ontology to construct the KG. Similar examples include AutoRD [33], a useful framework introduced recently for extracting information about rare diseases and constructing corresponding knowledge graphs. This system can process unstructured medical text as input and output extraction results and a knowledge graph, where LLM is used to extract entities and relations from medical ontologies. Most recently, an unsupervised framework called TKGCon (Theme-specific Knowledge Graph Construction) [34] utilised LLMs to construct both ontologies and theme-specific KGs, by relying on LLMs to generate and decide relations between entities to construct graph edges. These methods signal that LLMs are capable of more than knowledge extraction from unstructured data. They can also process and reason over data to construct and complete KGs. Furthermore, Khorashadizadeh et al. [11] outlined other methods that used LLMs for specific KG construction tasks like text-to-ontology mapping, entity extraction, and ontology alignment. LLMs were also used for KG validation through fact-checking and inconsistency detection."}, {"title": "5 Hybrid Approaches", "content": "Fusing textual and knowledge embeddings. In contrast to the methods presented in previous sections, the approaches explained in this section combine KGs and LLMs in a more unified way to build upon the explicit knowledge from KGs and implicit knowledge found within LLMs. An example of this is ERNIE (Enhanced Language RepresentatioN with Informative Entities) [35], which fuses lexical, syntactic, and knowledge information together by stacking a textual T-Encoder with a knowledge K-Encoder to represent word tokens and entities in a unified feature space, similar to the"}, {"title": "6 Thematic Analysis", "content": "In this section, the important models mentioned so far are categorised into \"Add-ons\" versus \"Joint\" Knowledge Graph + LLM approaches.\n\u2022 Add-ons - The models categorised here use LLMs and KGs as supplementary tools to enhance their functionality. They include models where LLMs are used for creating KGs or KGs are used for providing information to LLMs. The purpose behind employing this approach is so that KGs and LLMs can operate independently to maximizing qualities such as scalability, cost reduction, or flexibility.\n\u2022 Joint - The models under this category leverage the combined strengths of LLMs and KGs to achieve enhanced performance in specific tasks. The tasks are application-dependent, and this approach can provide comprehensive understanding, optimized results, and improved accuracy."}, {"title": "7 Strengths and Limitations of Existing Research", "content": "This section reviews the strengths and limitations of existing research covered in this paper, which can be crucial to understanding this joint approach of combining KGs and LLMs. One of the significant strengths we have identified is the performance improvement brought by the utilisation of KGs and LLMs in a joint fashion, especially in the knowledge-driven domain. Models combining KGs and LLMs typically display a better semantic understanding of knowledge, thus enabling them to perform tasks like entity typing better. Additionally, as was seen in several methods like QA-GNN [25], interpretability and explainability of the model can be increased when combining KGs with LLMs, which are particularly important factors when LLMs are being adopted in sensitive domains like healthcare, education, and emergency response.\nNevertheless, current research also has limitations that may hinder this joint approach's broader application or effectiveness. One of the major issues is that KGs in some domains may not be widely available, thus limiting the"}, {"title": "8 Conclusion", "content": "Driven by research questions \"How can KGs be utilised to enhance the capabilities of LLMs?\u201d (RQ1), \"In what ways can LLMs be leveraged to support and enhance KGs\" (RQ2) and \u201cAre there more advantages if models combine KGs and LLMs in a more joint fashion\u201d (RQ3), we conducted a quick review to explore the prevalence of using KGs purely for knowledge injection to support LLM-based models, and of using LLMs merely as information extractors to support KG-based models. To understand the advantages of this joint approach, we reviewed over 20 state-of-the-art articles from arXiv and classified existing methods into three different categories, including KGs empowered by LLMs (KGs adding interpretability, semantic understanding, and entity embeddings into LLMs), LLMs empowered by KGs (LLMs do forecasting with KG data, injecting implicit knowledge, and contributing to KG construction) and some Hybrid Approaches where KGs and LLMs are combined in a more unified way. We also provided a Thematic Analysis for these methods and discussed their strengths and limitations. To answer research questions, we found that models typically use KGs or LLMs like add-ons, and there are more advantages if models combine KGs and LLMs in a more joint fashion. Additionally, we found that even though the joint approach can provide significant improvement in the performance of the model by increasing its interpretability or explainability, current research also has its limitations, such as limited domains for KGs, higher consumption of computational resources, outdated frequently due to the fast evolution of knowledge, and the lack of effectiveness in Knowledge Integration.\nThis research area of combining KGs with LLMs represents a critical part in the rising trend of artificial intelligence (AI), and can potentially lead to more reliable and context-aware AI systems. Such models are equipped with domain-specific knowledge and contribute to a wider range of applications than solely using KGs or LLMs for problem-solving. Eventually, this research area will significantly impact how we construct a more robust and explainable AI system with higher performance and can provide avenues for other future development.\nAlthough the joint approach of combining KGs with LLMs has succeeded, some unsolved challenges remain. To address the low effectiveness of knowledge integration, future studies may continue exploring the potential solution to this problem by modifying model architecture or fine-tuning. One possible solution could be to inject knowledge into feature-based pre-training models. Future studies could also focus on developing a smaller integrated model to reduce computational resources and time, as integrating KGs and LLMs typically leads to larger parameter sizes and longer running time. Given the fact that a smaller KGPLM can outperform a larger plain LLM, it is possible that an optimal integrated model that requires fewer computational resources can be achieved. In the past year, there has also been a surge in interest in multimodal LLMs, which can process audio, image, or video data together with text, with a handful of multimodal LLMs released each month since the start of 2023. As these models are built on LLM backbones, it is foreseeable that they could also inherit some of the limitations LLMs have experienced thus far and might benefit from"}]}