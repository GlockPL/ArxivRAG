{"title": "Listening and Seeing Again: Generative Error Correction for Audio-Visual Speech Recognition", "authors": ["Rui Liu", "Hongyu Yuan", "Haizhou Li"], "abstract": "Unlike traditional Automatic Speech Recognition (ASR), Audio-Visual Speech Recognition (AVSR) takes audio and visual signals simultaneously to infer the transcription. Recent studies have shown that Large Language Models (LLMs) can be effectively used for Generative Error Correction (GER) in ASR by predicting the best transcription from ASR-generated N-best hypotheses. However, these LLMs lack the ability to simultaneously understand audio and visual, making the GER approach challenging to apply in AVSR. In this work, we propose a novel GER paradigm for AVSR, termed AVGER, that follows the concept of \"listening and seeing again\". Specifically, we first use the powerful AVSR system to read the audio and visual signals to get the N-Best hypotheses, and then use the Q-former-based Multimodal Synchronous Encoder to read the audio and visual information again and convert them into an audio and video compression representation respectively that can be understood by LLM. Afterward, the audio-visual compression representation and the N-Best hypothesis together constitute a Cross-modal Prompt to guide the LLM in producing the best transcription. In addition, we also proposed a Multi-Level Consistency Constraint training criterion, including logits-level, utterance-level and representations-level, to improve the correction accuracy while enhancing the interpretability of audio and visual compression representations. The experimental results on the LRS3 dataset show that our method outperforms current mainstream AVSR systems. The proposed AVGER can reduce the Word Error Rate (WER) by 24% compared to them.", "sections": [{"title": "Introduction", "content": "Automatic Speech Recognition (ASR) is a technology that enables the conversion of spoken language into text. It primarily relies on audio signals, using acoustic and linguistic models to understand and transcribe speech (Radford et al. 2023). Unlike ASR, Audio-Visual Speech Recognition (AVSR) is an advanced form of speech recognition that integrates both audio and visual inputs, such as lip movements, to improve the robustness and accuracy of speech recognition. In recent years, Large Language Models (LLMs) have garnered significant interest in research due to their powerful generative capabilities and extensive intrinsic knowledge (Touvron et al. 2023a,b; OpenAI 2023), leading to widespread success in various Natural Language Processing (NLP) tasks. A notable application is Generative Error Correction (GER) (Chen et al. 2023a; Hu et al. 2024b; Radhakrishnan et al. 2023).\nGER employs LLMs to map the correct transcription from the N-best hypotheses decoded by ASR systems, significantly enhancing ASR results and outperforming typical LM rescoring methods (Mikolov et al. 2010). It has demonstrated great effectiveness in learning the mapping from hypotheses to transcription through parameter-efficient LLM fine-tuning (Hu et al. 2022). Powered by LLMs, rencent work (Chen et al. 2023a) proposes a GER benchmark for ASR, and they release a HyPoradise dataset that contains over 332K pairs of decoded N-best hypotheses and ground-truth transcription in various ASR domains. However, the GER benchmark lacks specificity on noisy ASR scenarios, which are the most common in real world. To this end, latest work (Hu et al. 2024b) extend the GER benchmark to noisy conditions, as well as propose a Robust HyPoradise (Ro-bustHP) dataset with 113K hypotheses-transcription pairs from various ASR corpus in common noisy scenarios. As LLM's ability to handle multimodal tasks is explored, some work has begun to inject acoustic information into LLM to improve GER performance (Radhakrishnan et al. 2023; Hu et al. 2024a; Chen et al. 2024; Mu et al. 2024).\nHowever, while some GER methods for ASR can be adapted to AVSR (Radhakrishnan et al. 2023; Mu et al. 2024), it does not perform optimally in this context, since it cannot understand the audio and visual at the same time. During conversations, humans do not rely solely on auditory cues but also subconsciously observe the speaker's lip movements (Chen et al. 2023b), as visual cues from this region provide valuable information for understanding speech. These visual cues become particularly crucial in noisy environments or when the speaker is speaking softly (Chen et al. 2023b). For AVSR systems that integrate both auditory and visual information, this phenomenon remains critically important. Extensive research has shown that incorporating noise-independent visual cues can significantly enhance the noise robustness and recognition accuracy of speech recognition systems, underscoring the equal importance of visual information and auditory signals (Ma, Petridis, and Pantic 2021; Gulati et al. 2020; Shi, Hsu, and Mohamed 2022; Ma et al. 2023). Therefore, extending GER techniques to incorporate visual information in AVSR systems is a natural progression to improve their effectiveness.\nTo address the gap in AVSR error correction research, we propose a method named AVGER, which corrects AVSR results using the original audio and video information. Note that AVGER follows the concept of \u201clistening and seeing again\". Specifically, the original audio signals and lip videos are input into a Multimodal Synchronous Encoder based on the Q-Former (Li et al. 2023; Yu et al. 2024), which operates independently from the AVSR system. This setup is designed to extract semantic feature representations specifically tailored for error correction in LLMs. These representations are first combined with the N-best hypotheses through a Cross-modal Prompt, and then input into an LLM. The LLM leverages efficient parameter-tuning techniques to enhance its understanding of multimodal information, ultimately producing a correct transcription based on the original audiovisual features and the N-best list. Additionally, we customize a Multi-Level Consistency Constraint Loss Function that aligns with specific task metrics and considers multimodal information. The loss function comprises three parts: 1) Logits-Level: The traditional Cross-Entropy (CE) Loss, which minimizes the discrepancy between predicted and true distributions, enhancing the model's accuracy; 2) Utterance-Level: A WER component, identical to the task metric, which bridges the gap between training objectives and task benchmarks, thereby effectively boosting model performance; 3) Representation-Level: A Central Moment Discrepancy (CMD) loss (Zellinger et al. 2017) that minimizes the gap between audio, visual and textual features, enhancing the model's ability to understand and process multimodal information. The main contributions of this paper are:\n\u2022 We propose AVGER - a paradigm for AVSR generative error correction that addressing the limitations of current GER approaches in AVSR scenarios. To the best of our knowledge, we are the first to introduce a method that integrates both audio and visual information for GER in AVSR systems.\n\u2022 Multimodal Synchronous Encoder and Cross-modal Prompt to build the link between N-best and raw audio-visual information, and using a Multi-Level Consistency Constraint Loss Function to guide the LLM to comprehend audio-visual information and produce transcriptions that most accurately reflect the actual spoken content.\n\u2022 Experimental results show that our method enhances the current mainstream AVSR system and effectively reduces the WER by 24%."}, {"title": "Related Work", "content": "Querying Transformer (Q-Former) consists of two transformer submodules that share the same self-attention layers, proposed by BLIP2 (Li et al. 2023). A set number of learnable query parameters as input to the transformer. The queries interact with each other through self-attention layers, and interact with features through cross-attention layers. While Q-Former has been primarily proposed for visual information extraction (Zhang, Li, and Bing 2023; Li et al. 2023), it also performs remarkably in extracting auditory features for generic audio understanding (Yu et al. 2024; Tang et al. 2024; Xu et al. 2024). In addition, various types of modality aligners have been studied, such as the cross-attention mechanism (Alayrac et al. 2022), pre-trained multimodal embeddings (Girdhar et al. 2023), and temporal and spatial pooling (Maaz et al. 2024) etc. Unlike these approaches, our method slices temporally aligned audio and video into segments of equal length, processes each segment with Q-Former, and then stacks the results to maintain temporal synchronization.\nAVSR has attracted increasing research interest in improving the performance of speech recognition systems by combining speech signals with visual signals that are not affected by acoustic noise. AV-HuBERT (Shi et al. 2022) learn the correspondence of audio and video modalities in a self-supervised manner, which is further augmented in (Shi, Hsu, and Mohamed 2022) to improve noise-robustness. Auto-AVSR (Ma et al. 2023) using publicly-available pre-trained ASR models to automatically transcribe unlabelled datasets to increase the training set size. MRSL (Chen et al. 2023b) leveraging visual modality-specific representations that carry noise-invariant information from the visual stream to improve the AVSR system by reinforcement learning. In this paper, we select AV-HUBERT as our AVSR system, primarily due to its robust self-supervised learning capabilities. It effectively captures deep audio and visual features from unlabeled data, which significantly enhances its performance in AVSR tasks within noisy environments.\nBenefiting from the powerful generative capabilities of LLMs, recent studies have introduced a GER benchmark (Chen et al. 2023a) that uses LLMs to generate the ground-truth transcript from the N-best hypotheses list produced by ASR. RobustGER (Hu et al. 2024b) extends this benchmark to noisy conditions, achieving significant performance improvements. As the research progressed, a new problem was identified: due to the uncertainty of LLMs generation, the generated transcripts are sometimes grammatically correct but do not match the original speech. In response, ClozeGER (Hu et al. 2024a) formatted GER as a cloze test that eliminates redundancy of input information and combines it with the source speech for error correction. LipGER (Ghosh et al. 2024) uses lip movements for error correction of ASR. UADF (Chen et al. 2024) utilises token-level uncertainty estimation at the decision stage to dynamically determine the fusion weights to be assigned to each modality at each decoding step, and hence this approach can be used for AVSR. However, due to the limited semantic information contained in the decision-level information, it does not migrate well to AVSR systems that contain additional visual information. In this paper, we construct a GER method that simultaneously understands audio and video through Cross-modal Prompt and Multi-Level Consistency Constraint."}, {"title": "Methodology", "content": "In this section, we will introduce our proposed AVGER. First, we describe the overall architecture in detail. Next, we will elaborate on the principle of Multimodal Synchronous Encoder and Cross-modal Prompt. Finally, we will introduce the Representation-Level Consistency Constraint Loss Function.\nAlthough the AVSR system has already extracted features from the source speech and lip video, we design an encoder independent of the AVSR to ensure that the LLM does not become coupled with it. This independent encoder re-extracts audio and video representations that are better suited for the LLM. We chose LLaMA (Touvron et al. 2023a) as the foundation for our decoder due to its strong generative capabilities and flexibility in handling complex multimodal inputs, making it particularly well-suited for the nuanced task of AVSR error correction. As shown in Figure 1, AVGER incorporates a Multimodal Synchronous Encoder based on QFormer, a decoder based on LLaMA, and an AVSR system implemented using AV-HUBERT.\nThe source speech and lip video contain a wealth of information. Initially, the AVSR system processes these inputs and transcribes them into N-best hypotheses VN = {Y1, Y2,..., YN} using beam search decoding. Subsequently, a Q-Former-based Multimodal Synchronous Encoder re-extracts and compresses the temporally aligned frame-level features from the audio and video, capturing subtle linguistic and semantic nuances. Finally, these features, along with the N-best hypotheses, are input into the LLaMA-based decoder to generate the most accurate transcription.\nThis module is used to extract the representations of the source audio and video signals and compress their length. The module consists of two branches, each containing an Encoder, a Temporal Clipper, a Q-Former shared by both branches, a set of learnable query parameters, and a Bridger. The temporally aligned speech (denoted as S) and lip video (denoted as L) pairs with time length T are input into their respective convolution-based encoders to get frame-level features $e_s \\in \\mathbb{R}^{T_s\\times d}$ and $e_L \\in \\mathbb{R}^{T_L \\times d}$, where Ts and TL are the sequence lengths after processing by Speech Encoder and Lip Encoder, d is the dimensions of hidden state, Note that Ts and TL are not equal due to differences in the processing of video and audio modalities:\n\\begin{equation}\n\\begin{cases}\ne_s = \\text{SpeechEncoder}(S) \\\\\ne_L = \\text{LipEncoder}(L)\n\\end{cases}\n\\end{equation}\nThe Temporal Clipper segments the frame-level features based on the specified time window length $\\tau$, ensuring each segment corresponds to a fixed temporal span. Notably, the Temporal Clipper operates without any learnable parameters. Since the LLM can see all temporal windows, there is no overlap in Temporal Clipper. This setting is crucial for aligning the temporal information across modalities, ensuring that the Q-Former to process segments with consistent temporal span from both speech and lip video, thereby ensuring synchronization:\n\\begin{equation}\n\\begin{cases}\n\\{e_s^k\\}_{k=1}^K = \\text{TemporalClipper}(e_s, \\tau) \\\\\n\\{e_L^k\\}_{k=1}^K = \\text{TemporalClipper}(e_L, \\tau)\n\\end{cases}\n\\end{equation}"}, {"title": "Q-Former", "content": "where $e_s \\in \\mathbb{R}^{T_s\\times d}$ and $e_L \\in \\mathbb{R}^{T_L\\times d}$ denote the k-th segment of es and eL, with K = [T/$\\tau$], $T_s = T_s/K$, and $T_L =T_L/K$.\nSegment-level position embeddings $\\{P_k\\}_{k=1}^K$, $P_k \\in \\mathbb{R}^d$ are added to each $e_s^k$ and $e_L^k$. The addition of segment-level position embeddings provides the Q-Former with temporal context, ensuring that the sequential order and timing relationships between segments are preserved during processing, which is essential for maintaining the integrity of temporal features:\n\\begin{equation}\n\\begin{cases}\n\\{\\tilde{e}_s^k\\}_{k=1}^K = \\{e_s^k \\oplus P_k\\}_{k=1}^K \\\\\n\\{\\tilde{e}_L^k\\}_{k=1}^K = \\{e_L^k \\oplus P_k\\}_{k=1}^K\n\\end{cases}\n\\end{equation}\nwhere $\\oplus$ denotes the vector addition operation, $\\tilde{e}_s^k \\in \\mathbb{R}^{\\tilde{T}_s\\times d}$, and $\\tilde{e}_L^k \\in \\mathbb{R}^{\\tilde{T}_L \\times d}$ .\nA shared Q-Former, with specific learnable query parameters for each modality, is employed to process these segments added segment-level position embeddings $\\tilde{e}_s^k$ and $\\tilde{e}_L^k$. These parameters are expressed as $Q_S \\in \\mathbb{R}^{n_q\\times d}$ for speech and $Q_L \\in \\mathbb{R}^{n_q\\times d}$ for lip video, where $n_q$ is the length of learnable query parameters, each with the same length. The use of different learnable query parameters for speech and lip video allows the Q-Former to capture modality-specific features while aligning them in a shared representation space, facilitating effective multimodal fusion and enhancing the model's ability to handle cross-modal information.\nUsing $Q\\in \\mathbb{R}^{n_q\\times d}$ as a proxy for QS and QL, the Q-Former is computed as shown in Figure 2. First, input Q into the self-attention mechanism to capture and model the internal dependencies and relationships between all pairs of query vectors. This process allows the Q-Former to dynamically reweight the importance of each query vector based on its relevance to others:\n\\begin{equation}\nQ_{\\text{self}} = \\text{Softmax} \\left(\\frac{QW_q^{\\text{self}} (QW_k^{\\text{self}})^T}{\\sqrt{d_k}}\\right)QW_v^{\\text{self}}\n\\end{equation}\nwhere $W_q^{\\text{self}}$, $W_k^{\\text{self}}$, and $W_v^{\\text{self}}$ are the learnable weight matrices for queries, keys, and values in the self-attention mechanism. Using $\\tilde{e}$ as a proxy for $\\tilde{e}_s$ and $\\tilde{e}_L$. Next, the cross-attention mechanism is applied to interact $Q_{\\text{self}}$ with $\\tilde{e}$ to effectively integrate and align information across the modalities. This interaction allows the model to selectively focus on and combine the most relevant features from the segment-level representations $\\tilde{e}$:\n\\begin{equation}\nQ_{\\text{cross}} = \\text{Softmax} \\left(\\frac{Q_{\\text{self}}W_q^{\\text{cross}} (\\tilde{e}W_k^{\\text{cross}})^T}{\\sqrt{d_k}}\\right)\\tilde{e}W_v^{\\text{cross}}\n\\end{equation}\nwhere $W_q^{\\text{cross}}$, $W_k^{\\text{cross}}$, and $W_v^{\\text{cross}}$ are the learnable weight matrices for queries, keys, and values in the cross-attention mechanism. Finally the output is obtained through the FFN layer: $\\hat{e} = \\text{FFN}(Q_{\\text{cross}})$, where $\\hat{e}$ is a proxy for $\\tilde{e}_s$ and $\\tilde{e}_L$. The above Q-former calculation process is represented as follows:\n\\begin{equation}\n\\hat{e} = Q\\text{-Former}(Q, \\tilde{e})\n\\end{equation}\nBased on this, we obtain this equation:\n\\begin{equation}\n\\begin{cases}\n\\{\\hat{e}_s^k\\}_{k=1}^K = \\{Q\\text{-Former}(Q_S, \\tilde{e}_s^k)\\}_{k=1}^K \\\\\n\\{\\hat{e}_L^k\\}_{k=1}^K = \\{Q\\text{-Former}(Q_L, \\tilde{e}_L^k)\\}_{k=1}^K\n\\end{cases}\n\\end{equation}\nFinally, two bridge modules are utilized to stack the Q-Former-processed representations into two complete sequences and map them to the embedding space of the LLM through a linear layer. This process ensures that the representations from both modalities (speech and lip video) are transformed into a unified format that the LLM can effectively process. In this case, the two modal representations have the same length and are aligned, which is crucial for enabling the LLM to jointly consider information from both modalities:\n\\begin{equation}\n\\begin{cases}\n\\text{Rep}_s = \\text{SpeechBridger}(\\{\\hat{e}_s^k\\}_{k=1}^K) \\\\\n\\text{Rep}_L = \\text{LipBridger}(\\{\\hat{e}_L^k\\}_{k=1}^K)\n\\end{cases}\n\\end{equation}"}, {"title": "Cross-modal Prompt", "content": "To fully leverage the semantic understanding capabilities of LLMs and generate more accurate transcriptions, we design a structured Cross-modal Prompt. This prompt is intended to guide the model in effectively integrating multimodal information from both speech and video inputs.\nThe Cross-modal Prompt is structured into five distinct parts, each defined by a unique heading to guide the model's processing:\n1) ### Instruction: This part sets the context for the task and instructs the model on what is expected.\n2) ### Speech: It represents where the Reps is inputted into the model.\n3) ### Video: It indicates where the Repl is inputted.\n4) ### Candidate transcriptions: It indicates the position of the N-best hypotheses list VN obtained from the AVSR system, which the model uses along with the video representation RepL and the audio representation Reps for transcript correction.\n5) ### Best transcription: The final part is where the model outputs the corrected transcript,"}, {"title": "Multi-Level Consistency Constraint Training", "content": "The goal of GER is to learn a hypotheses-to-transcription (H2T) mapping $M_{H2T}$ that predicts the transcription $\\hat{Y}$ based on N-best list VN :\n\\begin{equation}\n\\hat{Y} = M_{H2T}(V_N)\n\\end{equation}\nGiven the ground-truth transcription $Y^*$, we can fine-tune the LLM to learn $M_{H2T}$ in an auto-regressive manner. To effectively learn the hypotheses-to-transcription mapping $M_{H2T}$, we propose a Multi-Level Consistency Constraint Loss Function. This loss function comprises three levels: Representation-Level Consistency Constraint, Utterance-Level Consistency Constraint, and Logits-Level Consistency Constraint.\nbased distance constraint aims to reduce the discrepancy between the features of different modalities, so that the semantic features of Reps and RepL are closer to those of RepY. Note that CMD (Zellinger et al. 2017) is a state-of-the-art distance metric that measures the discrepancy between the distribution of two features by matching their order-wise moment differences. In the training phase, we will process the ground-truth transcription Y* through the tokenizer and LLM embedding layer to get the text representation RepY. We ensure that the extracted Reps and RepL are closest to the RepY by minimising the $L_{CMD}$:\n\\begin{equation}\n\\begin{split}\nL_{CMD} = &\\frac{1}{|\\{(m_1,m_2)\n \\in \\{(S,L), \\\\(S,Y^*), \\\\(L,Y^*)\\}\\}|}\\\\\n&\\sum_{(m_1,m_2)} (||\\mathbb{E}(Rep_{m_1}) - \\mathbb{E}(Rep_{m_2})||_2 \\\\\n& + \\sum_{k=5}^K ||C_k(Rep_{m_1}) - C_k(Rep_{m_2})||_2)\n\\end{split}\n\\end{equation}\nwhere $\\mathbb{E}(Rep)$ is the empirical expectation vector of the input sample Rep, and $C_k(Rep) = \\mathbb{E}((Rep-\\mathbb{E}(Rep))^k)$ is the vector of all $k^{th}$ order sample central moments of the coordinates of Rep.\nThe Word Error Rate loss $L_{WER}$ is designed to minimize the number of insertions, deletions, and substitutions needed to convert the transcription $\\hat{Y}$ into the ground-truth transcription $Y^*$ (Davis and Mermelstein 1980). By focusing on these specific types of errors, it directly improves the intelligibility and accuracy of the generated text at the utterance-level:\n\\begin{equation}\nL_{WER} = \\frac{D(\\hat{Y}||Y^*)}{len(Y^*)}\n\\end{equation}\nwhere $D(||)$ denotes the Levenshtein Distance between two sequence, and $len(\\cdot)$ denotes the number of words in sequence.\nThe Cross-Entropy loss $L_{CE}$ (Vaswani et al. 2017) is widely used in classification tasks to measure the discrepancy between the predicted probability distribution and the ground-truth distribution. By penalizing the model for inaccurate predictions, it encourages the model to generate probabilities that are closer to the true distribution:\n\\begin{equation}\nL_{CE} = \\sum_{t=1}^T - \\log P_{\\theta}(y^*_t | Y_{t-1},..., Y_1, V_N)\n\\end{equation}\nwhere $y^*_t$ is the t-th token of $Y^*$, and $\\theta$ denotes the learnable parameters in LLM (i.e., LoRA (Hu et al. 2022)).\nThe Multi-Level Consistency Constraint Loss $L_{MLC}$ combines the three loss components to provide a comprehensive training signal, ensuring consistency at the representation-level, utterance-level, and logits-level:\n\\begin{equation}\nL_{MLC} = L_{CMD} + L_{WER} + L_{CE}\n\\end{equation}\nLORA (Low-Rank Adaptation) is a technique designed for efficiently fine-tuning LLMs. Typically, fine-tuning LLMs requires updating a substantial number of parameters, which can be computationally expensive and prone to overfitting. LORA addresses this challenge by employing low-rank decomposition, breaking down the weight matrices in the model into the product of lower-rank matrices. This approach significantly reduces the number of parameters that need to be updated during fine-tuning. By doing so, LoRA not only maintains the model's performance but also greatly reduces the computational resources and memory required for fine-tuning. Therefore, we chose LoRA to fine-tuning the LLM to learn $M_{H2T}$."}, {"title": "Experiment And Results", "content": "We conduct the experiments on LRS3 (Afouras, Chung, and Zisserman 2018), which is the largest publicly available dataset for audio-visual speech recognition task. LRS3 consists of 151 819 video clips from TED talks with a total of 439 hours. It contains 118 516 (408 hours), 31 982 (30 hours) and 1 321 clips (0.9 hours) in the pretraining, training-validation, and test set. The LRS3 dataset is the largest and most widely accepted benchmark for AVSR, and since our method is based on AV-HuBERT, which itself was primarily evaluated on LRS3, we used this dataset in our experiments. Furthermore, we tested our model's generalization by using LRS3 checkpoints on other datasets.\nTo train our model within our resource limitations, we combined the original training and test sets from LRS3. After removing data segments longer than 20 seconds, we re-divided the dataset into a new 270h training set, and a 1h validation set, and maintained the original 0.9h test set. We construct the noisy data on the clean LRS3 using Babble noise which is drawn from (Snyder, Chen, and Povey 2015) at four SNR levels: {-10, -5, 0, 5}dB, with a 1:1 ratio of noisy to clean data. This configuration is consistent with UADF (Chen et al. 2024) for ease of comparison.\nFor each video clip, we detect the 68 facial keypoints using dlib (King 2009) and align each frame to a reference face frame via affine transformation. We crop a 96 \u00d7 96 region-of-interest (ROI) centered on the mouth.\nWe employ LLaMA-7B (Touvron et al. 2023a) as the foundation model and AV-HuBERT as the AVSR system for generating N-best hypotheses. A low-rank adapter is inserted into each layer of LLaMA with the rank of 32 for fine-tuning.\nIn the Multimodal Synchronous Encoder, the speech encoder is implemented using the first layer of HuBERT (Hsu et al. 2021), and the lip encoder is derived from the first layer of VideoMAE (Tong et al. 2022), both leveraging convolutional layers. For the lip encoder, we modify the original model to enable the processing of video inputs. We divide each frame of the video clip into four 48 \u00d7 48 patches. The shared parameters Q-Former comprises 6 Q-Former Blocks (Li et al. 2023), with learnable query parameters of length 20. The time window length is set to 1 second. We use a uniform Cross-modal Prompt template to transform the 10-best candidate transcripts and audio-visual representations into inputs suitable for LLMs.\nDuring fine-tuning, we employ AdamW (Loshchilov and Hutter 2017) optimizer with a learning rate set to 1e-4 and warmup steps set to 0.05%. The number of training epochs is set to 2, the batch size is set to 256 by using gradient accumulation. The maximum input sequence length is set to 2048.\nTo validate the effectiveness of our proposed AVGER system, we conducted comparisons against baseline models such as GER (Chen et al. 2023a) and UADF (Chen et al. 2024). We also performed ablation studies to assess the contribution of each module within AVGER. Additionally, we evaluated the impact of various hyperparameter settings on the final performance. The results of these experiments are detailed in the following section."}, {"title": "Metrics", "content": "We evaluate performance using WER (Davis and Mermelstein 1980), and Word Error Rate Reduction (WERR) (Leng et al. 2021). Lower values of WER indicate better performance, while a higher WERR signifies a greater improvement relative to the GER approach.\nGER GER is a method proposed by Hyporadise (Chen et al. 2023a). We follow the hypotheses-to-transcription-low-rank-adaptation (H2T-LORA) pipeline in Hyporadise, which avoids tuning the entire set of parameters of the pre-trained model by inserting a neural module with a small number of additional trainable parameters to approximate the full set of parameters, thus learning the H2T mappings efficiently without affecting the LLM pre-training parameters, thus avoiding the need to tune the entire set of parameters of the pre-trained model, and thus learning the H2T mapping efficiently without affecting the LLM pre-training parameters. This pipeline corrects errors ONLY through the N-Best list.\nUADF UADF is Uncertainty Aware Dynamic Fusion, an ASR error correction approach proposed by (Chen et al. 2024). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: 1) It first analyzes and calibrates the token-level LLM decision, and 2) it then dynamically assimilates the information from the acoustic modality. Since it only relies on decision-level information, it can be directly used in AVSR error correction."}, {"title": "Results and Analysis", "content": "In this section, we conduct experiments to address the following questions: (i) How does the GER method, which performs well in ASR, translate to AVSR in terms of performance? (ii) Is the proposed AVGER method effective and does it outperform the baseline? (iii) What is the contribution of the modular design in AVGER to the error correction process? (iv) How do different hyperparameter settings affect the final result?\nWe compared the performance of different models for error correction of AV-HuBERT results in band noise frequency and clean audio conditions, including the WER and WERR of GER, UADF, and the proposed AVGER method. As shown in Table 1, AVGER performs well in most of the SNR conditions, with an average WER of 11.8%, and positively improves all SNR levels. The WER of AVGER was lowest in the clean speech condition at 1.10%. In addition, AVGER's WERR reaches 24.5% and 24.0% at 0dB and 5dB SNR levels, respectively, significantly outperforming the other methods. This demonstrates the exciting robustness and superior performance of AVGER in dealing with multiple noise environments.\nWe find that the error correction effect is inconsistent at high SNR (0dB, 5dB) and low SNR (-5dB, -10dB), and the same phenomenon also exists in UADF. We guess the reasons are as follows. At high SNR, the audio signal and the N-Best list are used as the main basis for error correction. At low SNR, the semantic information contained in the audio signal is masked by the noise, and the semantic information contained in the low-quality N-Best generated by the low-quality speech is scarce, so the error correction relies more on the noise-invariant lip video signal. Due to the complexity of video modality processing, our existing training data is insufficient for fine-tuning the LLaMA of plain text to be able to understand audio and video well at the same time. However, this phenomenon is not the focus of this paper. We will try to solve this problem by using multimodal LLMs and increasing the training data in the future.\nThe results of the ablation experiments in Table 2 show that the AVGER performs best when the modules work in concert. Removing the $L_{WER}$ and $L_{CMD}$ significantly degrades the performance, suggesting that our Multi-Level Consistency Constraint loss function can effectively guide LLM to understand multimodal information and learn the $M_{H2T}$. In addition, removing either of the speech and lip videos resulted in varying degrees of increase in WER, showing that each modality acts as an effective gain to the final result. It is worth noting that the performance degradation is most obvious when the frame-level features are not sliced, which implies that Q-Former is difficult to effectively capture the key information when dealing with long sequences, suggesting that our method is able to better maintain the consistency of the temporal information and enhance the inter-modal synchronization by slicing the frame-level features, which in turn improves the overall performance of error correction.\nTable 3 shows the effects of the number of N-Best hypotheses and the length of learnable query parameters on the final error correction results. As the number of N-Best hypotheses increases from 5 to 10, the WER gradually decreases, which indicates that increasing the number of hypotheses can significantly improve the error correction performance of the model. Similarly, as the number of Q-Tokens increases from 10 to 20, the error correction performance of the model gradually improves, which indicates that longer query parameters can capture more semantic information."}, {"title": "Conclusion", "content": "In this paper, we present AVGER, a new error correction framework that fills the research gap of GER on AVSR systems. AVGER achieves multimodal alignment by segmenting frame-level features and processing each segment by Q-Former, and guides LLM learning of H2T mappings via a Multi-Level Consistency Constraint loss function. Experimental results show that our AVGER achieves better WER performance improvement on the LRS3 dataset under both clean and noisy conditions, and further analyze the reasons for the performance differences under different SNR configurations and the effectiveness of different modules in the framework.\nAlthough the AVGER demonstrates excellent performance in noisy environments and significantly improves the accuracy of audio-visual speech recognition, it has some limitations that cannot be overlooked. First, the model is primarily trained on the LRS3 dataset, which lacks diversity in speech scenarios and languages, potentially limiting its generalization capabilities. Moreover, the model's robustness against extreme or previously unseen noise types still requires further improvement. Future work will focus on expanding the dataset, enhancing the noise handling capabilities, and exploring the applicability of the system in a variety of speech environments in order to increase the versatility and utility of the AVGER system."}]}