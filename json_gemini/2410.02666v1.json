{"title": "ALPHAINTEGRATOR: TRANSFORMER ACTION\nSEARCH FOR SYMBOLIC INTEGRATION PROOFS", "authors": ["Mert \u00dcnsal", "Timon Gehr", "Martin Vechev"], "abstract": "We present the first correct-by-construction learning-based system for step-by-step\nmathematical integration. The key idea is to learn a policy, represented by a GPT\ntransformer model, which guides the search for the right mathematical integration\nrule, to be carried out by a symbolic solver. Concretely, we introduce a symbolic\nengine with axiomatically correct actions on mathematical expressions, as well\nas the first dataset for step-by-step integration. Our GPT-style transformer model,\ntrained on this synthetic data, demonstrates strong generalization by surpassing its\nown data generator in accuracy and efficiency, using 50% fewer search steps. Our\nexperimental results with SoTA LLMs also demonstrate that the standard approach\nof fine-tuning LLMs on a set of question-answer pairs is insufficient for solving this\nmathematical task. This motivates the importance of discovering creative methods\nfor combining LLMs with symbolic reasoning engines, of which our work is an\ninstance.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) based on the transformer architecture (Vaswani et al., 2023) have\ndemonstrated remarkable abilities across diverse tasks, such as language translation, code generation,\nand engaging human-like conversations (OpenAI, 2024). However, applying these models to mathe-\nmatics presents significant challenges. Their autoregressive nature makes them prone to hallucinations\nand errors during inference. Advancements such as Chain-of-Thought (CoT), self-consistency, and\nprocess supervision help generate more accurate multi-step reasoning (Wei et al., 2023), (Wang\net al., 2023), (Lightman et al., 2023). However, unlike general language tasks, mathematics demands\nabsolute rigor and precision, where even minor errors are unacceptable. Mathematical correctness\nrelies on faultless execution of logical steps and computations. LLMs often fail to achieve this\nconsistently and there is no provable method which ensures the correctness of their mathematical\nreasoning.\nMathematical Integration A fundamental mathematical task is one of indefinite integration of\nmathematical expressions, a problem with no straightforward algorithmic solution. Existing methods\nfor solving this task fall into two categories: those that directly output the antiderivative, and those\nthat provide step-by-step proofs.\nLample and Charton (2019) proposed a learning-based approach, training a seq2seq model to generate\nthe antiderivative directly, without steps and with correctness verification left as a separate problem.\nWelleck et al. (2021) demonstrated that such models do not generalize well, even though they\nmight have high test accuracy, as the neural network needs to mechanically learn how to carry out\ncomplex operations like applying the partial fractions algorithm or dividing two large numbers. The\nalgorithm proposed by Risch (1969) reduces the integration problem into finding poles of certain\nalgebraic functions. Risch's method is pseudo-complete for indefinite integration, but it only applies\nto the restricted setting of functions with elementary antiderivatives and similarly does not produce\nintermediate steps. The full description of the method is longer than 100 pages and has never been\nfully implemented. Current symbolic solvers often include a simplified, heuristic version. However,\nthe resulting answers, while always correct, are not very illuminating. Further, in contrast to learning-\nbased approaches, this method does not directly generalize to similar tasks, such as non-elementary\nor multidimensional integration, or general theorem proving."}, {"title": "Our Work: correct-by-construction learning-based integration", "content": "In this work we introduce the\nfirst open system which combines the strengths of both symbolic engines and GPT transformer\nmodels in order to learn to integrate mathematical expressions in a step-by-step, provable manner.\nOur approach is inspired by the groundbreaking advancements of AlphaProof and AlphaGeometry\n(Trinh et al., 2024), where language models interact with a symbolic engine to generate a solution\nthat is guaranteed to be correct. Concretely, we designed a novel symbolic engine and generated\nsynthetic data used to train a GPT transformer language model capable of sophisticated interaction\nwith this engine."}, {"title": "Main contributions", "content": "Our key contributions are:\n\u2022 The first dataset for rigorous step-by-step derivation proofs for indefinite integration.\n\u2022 A versatile open-source symbolic engine to interact with mathematical expressions through\naxiomatically correct actions with a novel encoding.\n\u2022 A (very small) transformer model which surpasses in performance the leading open-source\nstep-by-step integration baseline. Our evaluation also demonstrates that our tool can effec-\ntively guide search in a complicated action space and thus surpass its own dataset generator\nin both completeness and efficiency, through strong generalization.\nThe rest of the paper is organized as follows. In Sections 2 and 3, we introduce the symbolic engine\nand our representation of mathematical expressions. In Sections 4 and 5, we explain how to generate\nsynthetic data for integration and how we train the model. Finally, we explain how we run and\nevaluate the model in Sections 6 and 7."}, {"title": "2 SYMBOLIC ENGINE WITH PARAMETRIC ACTION SPACE", "content": "We depart from the typical approach of solving mathematics problems with LLMs done by fine-tuning\non question-answer pairs (Shao et al., 2024; Yang et al., 2024). Instead, our language model interacts\nwith a symbolic engine exposing a parametric action space. This guarantees that every step taken by\nthe model is correct (or null) since rewrites of mathematical expressions are permitted only through\nthe symbolic engine.\nIn each step, the symbolic engine takes in a mathematical expression f, a subexpression g, an action\na, and action parameters $p_1,..., p_n$, if applicable. It returns an expression that results from applying\naction $a(p_1,..., p_n)$ to subexpression g, along with a boolean that specifies whether the expression\nwas modified or not. The expression is not modified if g is not a valid subexpression of f, or if the\naction is not valid on this subexpression."}, {"title": "3 REPRESENTATION OF MATHEMATICAL EXPRESSIONS AND THEOREMS", "content": "We now discuss how we represent mathematical expressions and theorems in our symbolic engine,\nand how we encode these as sequences for interaction with a sequence model.\nWe represent mathematical expressions as trees. Leaf nodes are number constants or variables, such\nas 2, \u03c0, or x. Internal nodes are operators and functions, such as + or cosh. We show representations\nof the expressions $\\frac{1}{x+3} + 2 \\cosh^2(x)$ and $\\int x^2e^x dx$.\nThe symbolic engine holds in its state a dictionary of pairs of expressions encountered and the\nchanges of variables that are active in the respective expression. The dictionary is used to backtrack\nwhenever we reach again a state that has already been explored. We undo via backsubstitution any\nchanges of variables for which there are no integrals remaining with a substituted variable.\nNote that this is parallel to theorem proving by interacting with a formal language (Xin et al., 2024;\nPolu and Sutskever, 2020; Lample et al., 2022), which makes our synthetic-data-based approach\napplicable to a variety of tasks."}, {"title": "3.1 TREE TO SEQUENCE EQUIVALENCE AND PARSING", "content": "We turn mathematical expression trees into sequences of tokens in order to process them using\ntransformers. In the following, we show algorithms to construct a one-to-one correspondence\nbetween expression trees and sequences under the assumptions above.\nTree to Sequence In order to turn a tree into a sequence, we define a recursive function:\n$treetoseq(v) =\\begin{cases}\n[v] & \\text{v is a leaf node} \\\\\n[v] + treetoseq(c_1) + ... + treetoseq(c_{nchild}) & \\text{otherwise}\\end{cases}$\nwhere $n_{child}$ denotes the number of children of v, $c_i$ denotes the i-th child, and + denotes concate-\nnation of sequences. This algorithm corresponds to doing depth-first traversal by picking the first\nchild and writing down all the observed values in order. Running treetoseq(r) on the root noder of\nan expression f produces a sequence suitable for transformer tokenization."}, {"title": "3.2 TOKENIZATION", "content": "We tokenize with unique tokens typical operations such as addition, power, multiplication, as well as\nall trigonometric, hyperbolic, and special functions (e.g. erf). We create 7 symbols (e.g. x,y, etc.)\nthat can be used as variables of integration and change of variables, and we tokenize special constants\nsuch as e, \u03c0, and i with their unique tokens. We represent integrals with a special token INTEGRAL\nand rational numbers with a token RATIONAL followed by two integers. We tokenize integers using\ntheir base-ten representation preceded by a token INT+ or INT-, indicating whether the integer is\npositive or negative. For simplicity, we do not have a dedicated representation of decimal numbers.\nFor example, the expression $\\int (\\frac{1}{2+3}+\\cosh^2(x))dx$ would be tokenized as follows:\nINTEGRAL + POW + INT+ 3 x INT- 1 * INT+ 2 POW cosh x INT+ 2 x\nFinally, we designate a token to each theorem in the symbolic engine. This allows us to distill\nmathematical expressions and theorems into an exceptionally compact formal language, achieving a\nminimalist yet expressive vocabulary of just $d_{vocab}$ = 128 tokens."}, {"title": "4 GENERATING A SYNTHETIC MATHEMATICAL INTEGRATION DATASET", "content": "We will train a model to derive step-by-step integrals by predicting single-step rule applications. To\nthis end, we generate fully synthetic step-by-step integration data, described in this section. Note that\nsuch a rigorous step-by-step integration dataset, based on a well-defined space of possible actions\nand on such a broad variety of mathematical expression data, was lacking prior to this work."}, {"title": "4.1 RANDOM MATHEMATICAL EXPRESSIONS", "content": "To create a large-scale dataset of mathematical expressions, we adopt an algorithm from Lample and\nCharton (2019), which samples random unary-binary trees and fills the nodes with operators and\noperands. We generate ~5M unique expressions with this algorithm described in Appendix C.1."}, {"title": "4.2 STEPS OF INTEGRATION", "content": "Once we have generated a dataset of random mathematical expressions, we pass them through the\nmanualintegrate module of SymPy (Meurer et al., 2017), in order to get a step-by-step solution. Then,\nwe map the solution into a sequence of actions and parameters in our symbolic engine. This results in\na sequence of tuples of expression, subexpression, action, and action parameter for each expression.\nOf course, many expressions SymPy cannot integrate as it enumeratively tries heuristic methods. Our\nhypothesis is that transformers are able to generalize to cases not covered by SymPy."}, {"title": "4.2.1 DATA AUGMENTATION WITH INTEGRATION BY PARTS", "content": "Let \u03a6, \u03a8 be two random functions generated by the method above, with derivatives \u03c6, \u03c8. By the rule\nof integration by parts, we have\n$\\int \\Phi(x) \\psi(x) dx = \\Phi(x)\\Psi(x) - \\int \\phi(x)\\Psi(x) dx$\nThen, if we know a step-by-step integration of $\\phi(x)\\Psi(x)$, we can find the steps for $\\Phi(x) \\psi(x)$ by\napplying integration by parts with the right parameters and applying the steps of $\\phi(x)\\Psi(x)$ to the\nrelevant subexpression of the resulting expression. We augment our dataset with this technique by\nsearching for such instances in the previous dataset. Our final dataset consists of 42.5M integration\nsteps and we report further statistics in Table 5."}, {"title": "5 MODEL ARCHITECTURE AND TRAINING OBJECTIVE", "content": "In this section, we describe our transformer model architecture and the objective we used for training."}, {"title": "5.1 ARCHITECTURE AND HYPERPARAMETERS", "content": "We use a decoder-only transformer architecture with 6 layers of multi-head attention with 6 heads\nand a hidden dimension of 384 (Radford et al., 2019). This results in a tiny model with only 10M\nparameters. We use the AdamW optimizer with $\u03b2_1$ = 0.9, $\u03b2_2$ = 0.99, dropout of $\u03b2$ = 0.2, and\nweight decay of $X$ = 0.1 (Loshchilov and Hutter, 2017). We decay the learning rate linearly from\n$10^{-3}$ to $10^{-4}$ throughout training, with batch size 256. We use a single A100 GPU for training. We\nchoose this simple setting as we observed no performance improvements with larger architectures."}, {"title": "5.2 TRAINING OBJECTIVE", "content": "We would like our model to propose a subexpression, action, and parameters of the action given a\nmathematical expression to integrate. Then, the model will be repeatedly fed back with the result\nobtained through the symbolic engine to find solutions for new expressions. To train our model for\nthis, we shuffle all integration steps into lines structured as follows:\nSTART [EXPR] SUBEXPR [SUBEXPR] RULE (RULE) PARAM [PARAM] END\nHere, terms in parentheses are tokenized mathematical expressions (e.g. $x^2 + sinh(x)$) or actions (e.g.\nPartsRule). We use the standard Cross Entropy Loss objective, where the model predicts extensions\nof START [EXPR] SUBEXPR. An example from the training dataset looks as follows:\nSTART Integral cos + E + x tan INT+ 2 x SUBEXPR Integral cos + E +\nx tan INT+ 2 x RULE URule PARAM1 y PARAM2 + E + x tan INT+ 2 END\nThis step corresponds to transforming the integral $\\int cos(x + tan(2) + e) dx$ using change of variables\ny = x + tan(2) + e. Applying this with the symbolic engine would result in the integral $\\int cos(y) dy$\nwhile storing the change of variable y = x + tan(2) + e in its memory."}, {"title": "6 ACTION SEARCH", "content": "In this section, we describe how we run our model, interacting with the symbolic engine, to solve an\nintegral."}, {"title": "7 EXPERIMENTAL EVALUATION", "content": "We explore a number of different directions to evaluate our model. We aim to demonstrate that\nour method not only significantly outperforms existing step-by-step benchmarks, but is also more\nefficient and generalizes well, unlike existing learning-based approaches. To illustrate the capability\nof our model, we present an example solution that it generated. For this example, SymPy\nfailed to figure out that we can apply the substitution u = sin(2x)."}, {"title": "7.1 TEST SET ACCURACY", "content": "We hold out a test set of 10k expressions with integration steps, unseen during training. We compare\nour model, SymPy, and GPT-40-mini. We run SymPy and our own model with timeouts of 120 and\n10 seconds, respectively. We use N = 5 for beam search decoding. We observe that this beam search\nresults in correct steps in the proposed actions for > 99% of the test set, on a step-by-step level. More\nprecisely, this means that if we predict a single step, there is almost always an exact match of the\nstep in the test set within one of the five recommendations resulting from beam search. We prompt\nGPT-40-mini with zero-shot CoT and we only check correctness of the result, ignoring any wrong\nintermediate steps, for a random subset of 1000 expressions."}, {"title": "7.2 EFFICIENT TREE EXPLORATION", "content": "To understand how well the transformer model guides us in the tree search, we measure the number\nof tree nodes explored during integration for both SymPy and our model, on the test set. We find\nthat on average, our model explores $N_t$ = 12.9 nodes for each successful integration whereas SymPy\nexplores $N_s$ = 25.6. This demonstrates that our model is not only more powerful but also more\nefficient, as it explores roughly 50% fewer nodes to find solutions."}, {"title": "7.3 ROBUSTNESS AGAINST SYMBOLIC BRITTLENESS", "content": "Recently, Welleck et al. (2021) introduced the concept of 'symbolic brittleness', where they investigate\nthe seq2seq model by Lample and Charton (2019) that directly predicts the antiderivative of an\nexpression. Their findings showed that while the model performs well on in-distribution problems, it"}, {"title": "7.4 EXPLORING BUGS IN SYMPY", "content": "As our method generalizes over SymPy's module, studying examples where SymPy fails and our\nmethod succeeds is very useful to find bugs or limitations in the heuristic of the symbolic solver\nsoftware."}, {"title": "8 RELATED WORK", "content": "Deep Learning for Theorem Proving Deep learning applied to theorem proving has seen significant\nadvancements, particularly in premise selection and proof guidance. Early work such as DeepMath"}, {"title": "9 CONCLUSION AND FUTURE WORK", "content": "We introduced a novel approach for step-by-step integration using a transformer model guided\nby a custom symbolic engine. The policy captured by the transformer model is learned from\na synthetically generated dataset of integration rules. A major advantage of our work is that it\nguarantees the final expression is always sound. This follows from the fact that the policy always\napplies a correct-by-construction integration rule, realized by the symbolic solver. Our experimental\nevaluation demonstrates strong generalization, surpassing its data generator in accuracy and efficiency.\nInterestingly, it also provides insights into potential errors found in modern heuristic solvers. We\ndemonstrated significant improvements compared to direct approaches using LLMs, which typically\nfine-tune an LLM on a dataset of question-answer pairs. We exhibit better robustness compared\nto other learning-based approaches and our method naturally generalizes to other settings, where\nresults without intermediate steps may be hard to verify. Limitations include reliance on synthetic\ndata and the scope of integration techniques that we handle. Future work will focus on more general\napproaches to training such as reinforcement learning, expanding the action space, and extending the\nmodel to other mathematical tasks."}, {"title": "10 ETHICS STATEMENT", "content": "This work focuses on improving symbolic integration using transformer models, which has limited\ndirect ethical concerns. The methods presented here aim to enhance mathematical problem-solving,\nprimarily for academic and educational purposes. Since the model is designed for symbolic com-\nputation, the potential for misuse is minimal, and the outcomes are easily verifiable. As with all\nAl systems, it is important to ensure that results are used appropriately in domains requiring high\nmathematical rigor, but we see no significant ethical risks associated with this research."}, {"title": "11 REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our results, we provide a detailed explanation of the methods and\nalgorithms used, including the design of the symbolic engine, data generation pipeline, and model\narchitecture. We also share all of the source code used to obtain the results. The codebase is\nwell-structured, allowing researchers to replicate our experiments and build upon our work."}, {"title": "A SYMBOLIC ENGINE", "content": "A.1 LIST OF ACTIONS IN THE SYMBOLIC ENGINE\nBelow, we present a complete list of actions available in the symbolic engine."}, {"title": "B REPRESENTATION OF MATHEMATICAL EXPRESSIONS", "content": "B.1 ALGORITHM FOR SEQUENCE TO TREE"}, {"title": "C DATASET GENERATION", "content": "C.1 GENERATING RANDOM MATHEMATICAL EXPRESSIONS\nThe algorithm for generating random mathematical expressions consist of three steps:\n1. Sample random unary-binary trees with a uniformly distributed number of nodes between 3\nand N, where N = 50.\n2. Fill the leaves of the tree with a symbol x with probability p = $\\frac{2}{3}$ and one of the constants \u03c0,\ne or number {0, . . ., 10} uniformly at random with probability 1-p = $\\frac{1}{3}$. This is aimed at\ngenerating more difficult expressions effectively by incentivizing symbols as leaves.\n3. Fill remaining internal nodes with random unary or binary operations. The binary operators\nare +, -, \u00d7, /, and the unary operators include trigonometric, hyperbolic, their inverses,\nand the exp and log functions.\nWhen selecting binary operations, addition and multiplication are twice as likely to be chosen over\ndivision and subtraction, as the latter can result in term cancellations and duplicate values. Unary\noperations are sampled uniformly at random. We do all trivial simplifications (e.g. evaluating x + 2x\nto 3x or x+1+0+3 to x + 4. All duplicates are removed during post-processing."}, {"title": "C.2 DATASET STATISTICS", "content": "We report statistics for the final dataset in Table 5"}, {"title": "D EXAMPLES OF SOLUTIONS GENERATED BY ALPHAINTEGRATOR", "content": "We show 2 more examples of solutions generated by AlphaIntegrator to demonstrate the tactics used\nby the model. First example requires a change of variables combined with understanding that the\nresulting integral is the non-elementary exponential integral, which AlphaIntegrator finds as first\ncandidate in its search. Second example demonstrates that the model can handle long chains of\ncomputation where it has creatively find correct parameters for substitution and integration by parts."}]}