{"title": "Actionable Cyber Threat Intelligence using Knowledge Graphs and Large Language Models", "authors": ["Romy Fieblinger", "Md Tanvirul Alam", "Nidhi Rastogi"], "abstract": "Cyber threats are constantly evolving. Extracting actionable insights from unstructured Cyber Threat Intelligence (CTI) data is essential to guide cybersecurity decisions. Increasingly, organizations like Microsoft, Trend Micro, and CrowdStrike are using generative AI to facilitate CTI extraction. This paper addresses the challenge of automating the extraction of actionable CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs (KGs). We explore the application of state-of-the-art open-source LLMs, including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting meaningful triples from CTI texts. Our methodology evaluates techniques such as prompt engineering, the guidance framework, and fine-tuning to optimize information extraction and structuring. The extracted data is then utilized to construct a KG, offering a structured and queryable representation of threat intelligence. Experimental results demonstrate the effectiveness of our approach in extracting relevant information, with guidance and fine-tuning showing superior performance over prompt engineering. However, while our methods prove effective in small-scale tests, applying LLMs to large-scale data for KG construction and Link Prediction presents ongoing challenges.", "sections": [{"title": "1. Introduction", "content": "The ever-evolving landscape of cyber threats has made incident threat analysis challenging, even for experienced professionals. Cyber Threat Intelligence (CTI) addresses this by providing information on threat actors, including indicators of compromise (IoCs) such as IP addresses or file hashes, as well as attacker tactics, techniques, and procedures (TTPs). This information is crucial for cybersecurity analysts, enabling them to make informed security decisions and stay updated on new threats, per a 2023 CTI survey by SANS Security [1]. However, CTI is primarily provided in an unstructured format and can be noisy, making knowledge extraction labor-intensive, with over half of the security teams spending more than 40% of their time on these tasks [1]. Therefore, we need more automation to reduce the overwhelming volume of open source reporting and the large amount of time spent ana- lyzing it [1]. To address these challenges, methodologies like TTPHunter [2] and LADDER [3] have been proposed to structure CTI reports into Knowledge Graphs (KGs). Despite their contributions, they often face limitations, such as high false positive rates in classifying IoCs and TTPs, scalability challenges, and the potential to miss critical information due to the constraints of language and predefined schemas.\nLately, large language models (LLMs) have proven valuable for understanding and manipulating natural lan- guage. With tailored prompts or additional training, they can perform effectively in tasks such as Named Entity Recognition [4], Relation Extraction [5] [6], and Triple Extraction for constructing Knowledge Graphs [7] [8] [9]. Cybersecurity applications include interpreting TTPs [10] and comprehending cybersecurity terminologies [11] [12]. However, LLMs' potential for extracting and interpreting critical information from natural language-heavy CTI re- mains largely unexplored.\nTherefore, in this research, we address the challenge of automatically extracting actionable Cyber Threat In- telligence (CTI) using Large Language Models (LLMs) and Knowledge Graphs (KGs) (see Figure 1). Specifically, we investigate the application of open-source LLMs, in- cluding the Llama 2 series [13], Mistral 7B Instruct [14], and Zephyr [15], for extracting triples from CTI text. Our approach includes the evaluation of various techniques like few-shot learning with prompt engineering [16] and the guidance framework [17], as well as fine-tuning [18]. The best-performing model, as determined by the ROUGE score and human evaluation, is then employed to generate a KG from CTI reports. This graph is subsequently applied to link prediction tasks, showcasing the practical implica- tions of our work in enhancing cybersecurity defenses and response strategies.\nThe paper makes the following key contributions:\n1) We investigate using LLMs to extract information from unstructured CTI for KG construction. While prior work used machine learning, we are the first, to our knowledge, to employ LLMs here.\n2) We explore this task under few-shot prompting and fine-tuning settings, testing various LLMs, prompts, inference, and decoding strategies. Our study offers practical insights for effective LLM use in this con- text.\n3) We identify LLM limitations for large-scale datasets and propose practical solutions. By highlighting these challenges, we aim to advance LLM applicability and scalability in information extraction."}, {"title": "2. Background and Related Work", "content": "Cyber Threat Intelligence (CTI). CTI involves gath- ering and analyzing security threats information to aid decision-making and help organizations respond effec- tively [19]. Public CTI sources include threat reports, vendor blogs (e.g., Symantec [20], Mandiant [21], Crowd- Strike [22]), news sites, databases, and social media. Effective CTI must be relevant to the organization, ac- tionable for immediate threat response, and contribute to key business outcomes [23]. In recent years, several approaches have emerged for extracting information from CTI reports, primarily focusing on TTPs. Early works like TTPDrill [24] and ChainSmith [25] used NLP to ex- tract threat actions and IoCs. CASIE [26] combined deep learning and linguistic features for cybersecurity event and vulnerability extraction. More recently, transformer-based architectures like SecureBERT [27] and TTPHunter [2] fine-tuned models such as BERT [28] or ROBERTa [29] for better cybersecurity text classification and context- sensitive TTP extraction.\nLarge Language Models (LLMs). LLMs are transformer-based models [30] that excel in language modeling by estimating the probability of word sequences and generating text. These models undergo pre-training on extensive textual corpora, significantly enhancing their effectiveness in various NLP tasks, such as text genera- tion and machine translation by providing context-aware representations. LLMs are characterized by their large scale, with tens or hundreds of billions of parameters [31]. Fine-tuning is commonly applied to models like Llama 2-Chat for task-specific enhancements, employing tech- niques such as Instruction Fine-tuning, which leverages prompt, response pairs to improve adherence to textual instructions. Due to the high resources required for full fine-tuning of all model layers, Parameter-Efficient Fine- Tuning (PEFT) methods like LoRA [18] and its quantized version, QLORA [32], which selectively update parame- ters, have gained popularity. Prompt Engineering (PE) is another method to direct language models towards desired outputs by iteratively refining the input for a generative model, circumventing the need for extensive annotated data or altering model parameters. Few-shot prompting includes examples in the prompt, giving the model additional context which aids in boosting its per- formance by guiding the model in generating outputs that mirror the patterns in the examples [33]. LLMs have found diverse applications in the field of cybersecurity. For example, ChatIDS [11] utilizes an LLM to interpret and explain anonymized intrusion detection system alerts to non-experts, providing user-friendly explanations and demonstrating its understanding of cybersecurity concepts. Additionally, Fayyazi et al. [10] explored using LLMs, such as GPT-3.5 and Bard, along with supervised training-based BaseLLMs, to classify cybersecurity descriptions into MITRE ATT&CK tactics. The MITRE ATT&CK framework [34], a freely available knowledge base, cate- gorizes adversary tactics and techniques to aid in under- standing and defending against cyber threats. Fayyazi et al. [10] highlighted challenges related to TTP description ambiguity and the importance of precise prompts. While LLMs have recently been successfully applied to entity and relationship extraction in various domains [35] [6] [36] [5], their specific application in extracting structured information from CTI reports within the cybersecurity field has received less attention. Siracusano et al. [37] employed GPT-3.5 for CTI extraction. They implemented zero-shot prompting and in-context learning in two custom pipelines to develop a structured CTI extraction tool, streamlining the manual information extraction process.\nKnowledge Graphs (KGs). KGs are structured repre- sentations of real-world information in the form of triples \u27e8ehead,r,etail\u27e9, where ehead denotes the head entity, etail the tail entity and r the relationship between the entities [38]. Their ability to model structured, complex data in a machine-readable format makes KGs indispens- able across diverse domains such as question-answering or information retrieval [3]. In CTI, KGs structurally represent complex relationships and integrate diverse data sources to enhance semantic querying, data analysis, and decision-making, thereby offering superior situational awareness and predictive insights. The SEPSES Cyber- security Knowledge Graph [39], for example, combines comprehensive information on attack patterns and vulner- abilities from publically accessible sources into a dynamic KG. Frameworks such as APTKG [40] and AttacKG [41]"}, {"title": "3. Approach", "content": "We address the following research questions (RQs) in this study through extensive experiments and analysis:\n1) RQ1(Few-Shot): How effective are LLMs at extracting CTI information in a few-shot setting? For this, we investigate prompt engineering & guidance frameworks [17].\n2) RQ2 (Fine-tuning): How much does fine-tuning LLMs with labeled data improve CTI information extraction? For this, we employ the parameter-efficient QLORA method.\n3) RQ3 (Knowledge Graph Quality): What is the quality of triples generated from a large CTI corpus using LLMs, and how can we improve them? For this, we identify shortcomings and propose error reduction methods.\n4) RQ4 (Link Prediction): How does CTI-derived knowledge graph perform in link prediction? For this, we consider both transductive and inductive settings."}, {"title": "3.2. Pretrained Models", "content": "We selected several open-source pretrained LLMs based on their diverse capabilities. The Llama 2 series (7B, 7B chat, 13B, 13B chat, 70B chat) [13], Mistral 7B Instruct v2.0 [14], and Zephyr-7B-\u03b2 [15] were included in the analysis (summary in Table 1). The Llama 2 series, introduced by Meta AI in February 2023, offers a range of pre-trained and fine-tuned LLMs with capacities ranging from 7 to 70 billion parameters. This variety enables a comprehensive analysis of techniques at different scales [13]. Notably, the Llama 2 70B model outperforms Mo- saicML Pretrained Transformer (MPT) and Falcon [13]. Llama 2-Chat models are fine-tuned with supervised fine-tuning and further trained with reinforcement learning with human feedback (RLHF) to align model behavior with human preferences and instruction following. They exhibit strong temporal knowledge organization abilities even with limited data, and its 70B version surpasses ChatGPT in answering factual questions [13]. Mistral 7B Instruct, fine-tuned on instruction datasets from the Hugging Face repository, outperforms smaller Llama 7B and 13B versions [14]. Additionally, Zephyr, based on Mistral 7B and fine-tuned with distilled Direct Preference Optimization (dDPO) to improve intent alignment, outper- forms Llama 70B chat in MT-Bench tests [15]."}, {"title": "3.3. Dataset Generation", "content": "We utilized two datasets, introduced by Alam et al. in [3], for our extraction experiments with LLMs: a manually annotated dataset for model training and evaluation and a large-scale dataset for knowledge graph (KG) generation and link prediction. The fine-tuning and evaluation dataset contains 120 curated CTI reports related to 36 Android malware families between the years 2015 and 2022. The reports were manually annotated using the BRAT annota- tion tool, capturing cyber threat concepts such as Malware, Malware Type, Application, Operating System, Organiza- tion, Person, Time, Threat Actor, Location, Indicator, and Attack Pattern. These concepts are connected by ten relations: isA, targets, uses, hasAuthor, hasAlias, indicates, discoveredIn, exploits, variantof, has.\nTo accommodate the model's maximum content length of 4096 tokens and address the limited number of ex- amples, we divided the text from annotated CTI reports into paragraphs. Various dataset sizes were tested, ranging from 400 to 1000 tokens per training example, resulting in different numbers of training examples. For example, for 400 tokens, the data was divided into 909 paragraphs. The annotated triples were matched automatically with the corresponding paragraphs via a script. However, not all paragraphs contained content that could be directly associated with the annotated triples. As a result, only 768 paragraphs successfully matched triples and were included in the final dataset. The dataset was split into training, validation, and test sets in an 80:16:4 ratio, yielding 574 training, 115 validation, and 29 test examples, each with up to 400 tokens per paragraph.\nThe best-performing LLMs, specifically the fine-tuned Llama 2 7B chat and the Llama 70B chat guidance model, were employed to generate a KG from the second, larger dataset comprising approximately 12,000 unstruc- tured open-access CTI reports. These reports were divided into paragraphs, each limited to a maximum of 1,000 tokens, resulting in around 80,000 paragraphs. These para- graphs were then processed by the LLM to generate triples for the KG."}, {"title": "3.4. Information Extraction Methods using LLM", "content": "Two methods for extracting triples under a few-shot setting with LLMs are explored prompt engineering and the guidance framework.\nTo explore the effective- ness of prompt engineering, experiments were conducted with various chat variants of the selected models, in- cluding Llama 2 7B chat, 13B chat, and 70B chat, as well as Mistral 7B Instruct v2.0 and Zephyr-7B-\u03b2. Prompts were tailored to test the model's adaptability across varying instruction styles and complexity, covering scenarios from zero to few-shot inference with diverse formats and content. We followed the prompt engineering guidelines from OpenAI [16] [50], which contain best practices on how to give clear and effective instructions and share strategies and tactics for getting better results from LLMs. For example, including the ontology and instruction in the system prompt [INST] <<SYS>>\\nExtract cybersecurity-related triples consisting of entities of the types Malware, Malware Types, Applications, Operating Systems, Organizations, Persons, Times, Threat Actors, Locations, and Attack Patterns and relationships between these entities of the types isa, targets, uses, hasAuthor, hasAlias, indicates, discoveredIn, exploits, variantof, and has. Print the extracted triples in the format: [Entity1, Rela- tion, Entity2]\\n<<SYS>>\\n\\nExtract triples from the following text:\u2018\u2018{input_txt}''[/INST] compared to only including the relationships or no information about the ontology in the prompt, like [INST] What are the [subject, predicate, object]-triples in the following text? Text:\u2018\u2018{input_txt}\u2019\u2019[/INST]. Additionally, the number and format of the included examples, such as Input text: ''A new version of the SpyNote Trojan is designed to trick Android users into thinking it's a legitimate Netflix application. Once installed, the remote access Trojan (RAT) essentially hands control of the device over to the hacker, enabling them to copy files, view contacts, and eavesdrop on the victim, among other capabilities.\u2018\u201c\nExtracted triples: [SpyNote, isA, Trojan], [SpyNote, targets, Andriod], [SpyNote, uses, designed to trick Android users into thinking it's a legitimate Netflix application], [SpyNote, isA, remote access Tro- jan], [SpyNote, isA, RAT], [SpyNote, uses, hands con- trol of the device over to the hacker], [SpyNote, uses, enabling them to copy files], [SpyNote, uses, view con- tacts], [SpyNote, uses, eavesdrop on the victim] varied between zero to four, separated by only line breaks, or the predefined prompt format, e.g., for Llama 2 Input text: {ex_txt} [/INST] Extracted triples: {ex_triples} </s><s>[INST] Input text: {text} [/INST].\n is a Python library by Microsoft Research for controlling LLM output, using a special guidance language for applying constraints like regular expressions or Context-free grammar (CFGs). It constructs an Abstract Syntax Tree (AST) for program interaction, customizing LLM responses and stopping points. [51]. Additionally, the guidance framework boosts efficiency and accuracy by batching user-added text and omitting non-essential parts [17].\nIn this research, the guidance library is employed to enforce the desired output format for the Llama 2-Chat models. Its capability to ensure data structure compliance and seamless integration into data processing tasks makes it a valuable tool. Using a regular expression such as \\n|</s> as a stopping criterion makes the generation stop upon encountering either a line break or an end-of- sentence token generated by the LLM. However, guidance currently lacks support for negative or positive lookaheads and generates parsing errors in regular expressions, es- pecially with special operator symbols like the closing bracket, greatly limiting its potential applications. Con- sequently, in our experiments, only the stop regular ex- pression is effectively utilized. Instructions and examples can be incorporated as follows, once the model has been loaded and referenced as llama2:"}, {"title": "3.4.2. Fine-tuning", "content": "While prompt engineering can yield significant improvements by optimizing prompts, fine- tuning a language model can enhance outcomes in specific scenarios and increase robustness. Therefore, a subset of the experiments focused on instruction fine-tuning the Llama 2 models (7B, 7B chat, 13B, and 13B chat) using a specific training dataset described in section 3.3. QLORA with 4-bit quantization was implemented to fine-tune the models in a resource-efficient manner. The experiments encompassed several factors, including different prompts, dataset sizes (ranging from 357 paragraphs with 1000 tokens each to 768 paragraphs with 400 tokens each), training configurations (using linear, constant, or cosine learning schedulers with learning rates of 2e-4 or 2e-5), and LoRA parameters (with ranks and alphas of 8, 16, 32, or 64), allowing for a comprehensive exploration of the fine-tuning process. The prompts experimented with various text separations from the instructions, employing line breaks, quotes, and instructional symbols such as <txt></txt> and [TXT][/TXT], or using no separation at all. Additionally, the complexity and length of the prompt were varied, ranging from the inclusion of comprehensive ontology information, e.g., [INST] Extract cybersecu- rity-related triples from the following text. Present them in the structure [subject, predicate, object]."}, {"title": "3.4.3. Inference & Decoding Strategies", "content": "Transformer models predict next token by producing a probability distribution across all possible words, where decoding strategies crucially impact text coherence and repetition. Greedy decoding risks repetition by choosing the likeliest next token, while beam-search considers multiple paths for higher-quality text [53]. Beam-search multinomial sam- pling blends the randomness of multinomial sampling with beam-search's strategy to optimize text generation [54]. Adjusting decoding strategies and parameters like repetition penalty can improve text quality without chang- ing the model's core settings. The mentioned decoding strategies have been tested for prompt engineering and fine-tuning in the context of structuring CTI. The guidance library currently does not support beam-search decoding or sampling strategies."}, {"title": "3.5. Implementation Details", "content": "To handle the computational demands of these large models, the experiments utilized NVIDIA A100 Ten- sor Core GPUs within a SPORC (Scheduled Processing On Research Computing) High-Performance Computing (HPC) cluster.\nThe models were loaded from Hugging Face [55] using the transformers library [56]. For the quantization of smaller models, bitsandbytes [57] was utilized, whereas the Llama 2 70B chat model was used in its quantized form with autogptq [58]. The peft [59] and trl [60] library were employed for LoRA fine-tuning, and applied to all linear layers. Additionally, Accelerate [61] was used to enhance inference speed during KG generation. For comparison, the maximum token length was set to 2048 tokens, which proved sufficient given the small input size of 1000 tokens. The temperature and repetition penalty for all models were set at 0.6 and 1.1, respectively. Decoding strategies and few-shot examples are detailed in Table 2."}, {"title": "3.6. Evaluation Metrics", "content": "Traditional evaluation metrics, such as Accuracy and F1-score, where only exact matches are compared, are inadequate for assessing the performance of generative models. Since these models generate new content based on learned patterns, they require more nuanced metrics that capture the quality and relevance of the generated text. We adopt the ROUGE metrics [62] in our study to evaluate the quality of triples generated by LLMs. ROUGE (Recall- Oriented Understudy for Gisting Evaluation) [62] com- prises a suite of metrics designed to compare generated text with reference text, making it suitable for appli- cations where such comparisons are feasible, including translation, text summarization, and entity extraction. The effectiveness of ROUGE heavily depends on the quality of the reference text; poor-quality references can lead to mis- leading assessments of the generated content's quality. It is case-insensitive and produces values ranging from 0 to 1. ROUGE-N evaluates the overlap of n-grams (unigrams, bigrams, and trigrams) between a text and its reference, using metrics like precision, recall, and F1-score. Recall measures how many relevant words from the reference text are captured by the machine-generated text, emphasizing the inclusion of all pertinent information. The recall of n-gram overlaps between a candidate text and a set of reference texts is calculated as follows:\n$$ROUGE-N_{recall} = \\frac{\\sum_{s \\in {Reference}} \\sum_{gram_n \\in s} Count_{match}(gram_n)}{\\sum_{s \\in {Reference}} \\sum_{gram_n \\in s} Count(gram_n)}$$\nPrecision assesses the proportion of words in the machine-generated text that accurately match those in the reference text, focusing on the relevance and correctness of the information provided. It is calculated with the following formula:\n$$ROUGE-N_{precision} = \\frac{\\sum_{s \\in {Reference}} \\sum_{gram_n \\in s} Count_{match}(gram_n)}{\\sum_{s \\in {Canidate}} \\sum_{gram_n \\in s} Count(gram_n)}$$\nThe F1-score balances recall and precision, offering a single metric to assess overall performance:\n$$F1-score = \\frac{2 * P * R}{P+R}$$\nConsidering the reference hand-annotated triple [Adwind, targets, retail and petroleum industry] and the candidate [Adwind, targets, petroleum production industry] extracted with the LLM, the ROUGE-1 recall would be 4/6 = 0.667, representing four unigram matches out of six possible unigrams in the reference. Subse- quently, the precision would be calculated as 4/5 = 0.8, and F1-score 2 * 0.667 * 0.8/1.467 = 0.727, which is reported in this paper. It is important to note that for ROUGE-N the order in which different n-grams appear in the text does not influence the score. However, the words within an individual n-gram must be consecu- tive. For example, in ROUGE-2, the matches would only include [Adwind, targets, retail and petroleum industry] from the reference and [Adwind, targets, petroleum production industry] from the candidate. In contrast, ROUGE-L assesses the longest common subse- quence between texts, capturing similarity in content with- out requiring consecutive word matches but in-sequence matches. This allows ROUGE-L to reward summaries"}, {"title": "4. Experiment and Results", "content": "Prompt engineering significantly affects the output of LLMs, with elements such as prompt structure, spac- ing, line breaks, and punctuation playing a crucial role. The impact of the prompt design was evident when comparing outputs from different sizes of the Llama 2 model; a prompt that was highly effective with the 7B model yielded less satisfactory results with the 13B variant. For example, considering the one-shot prompt [INST] <<SYS>>\\nExtract [subject, predicate, object]- triples from the input text with the following predicates: isA, targets, uses, hasAuthor, hasAlias, indicates, discoveredIn, exploits, variantof, and has.\\n<<SYS>>\\n\\nInput text: {ex_txt} [/INST] Ex tracted triples: {ex_triples} </s><s>[INST] Input text: {input_txt} [/INST] Extracted triples: . This prompt yielded the desired output as illustrated in Table 5, where prompt engineering combined with greedy de- coding produced the expected results for the 7B model. However, applying the same configuration to the 13B model resulted in merely listing entities from the text, e.g., The Adwind RAT, petroleum industry, US, new campaign, multi-layer obfuscation, evading detection, Con- sequently, this makes it a time-consuming process of trial and error. While adding a single example often enhanced the output, incorporating multiple examples, as the one mentioned in section 3.4.1, did not necessarily lead to better outcomes and, in some instances, even deterio- rated the model's performance. For instance, the Llama 2 7B chat model produced continuous text with over two examples, whereas other models struggled with inaccu- rately inventing relationships like hostedOn or usedFor from text verbs. Llama 2-Chat models presented notable challenges in generating outputs in a format suitable for further processing, in contrast to Mistral and Zephyr mod- els. Llama models exhibited less consistency in output formatting, whereas Mistral, in particular, was able to produce the desired format reliably, even in zero-shot inference scenarios. The prompt [INST] <<SYS>>\\nYou are a helpful cyber-security assistant. Your task is to extract [entity, relationships, entity]-triples from the input text. Use only the relationships: 'isa', 'targets', 'uses', 'hasAuthor', 'hasAlias',"}, {"title": "'indicates', 'discoveredIn', 'exploits', 'variantof', and 'has'. Your answers need to be in the format [entity,relationship, entity]. Reply only with the extracted triples.\\n<</SYS>>\\n\\nWhat are the triples in the following input text: {input_txt} [/INST] with Mistral greedy decoding already yielded triples in the format [Adwind RAT,isA,Remote Access Trojan (RAT)], [Adwind RAT, targets, petroleum industry], [Ad wind RAT, targets, US].", "content": "To ensure the model maintained the desired relation- ship types in the triples, merely providing examples was insufficient; incorporating the ontology directly into the prompt proved to be crucial. The most effective results across all models were achieved by combining the ontol- ogy with one or two examples in the prompt, as illustrated in the prompt described in the first paragraph of this section.\nOur comparative analysis of model performance with prompt engineering is summarized in Table 3, which presents the ROUGE metric scores in the context of n- gram overlap for n=1,2,3,6 (ROUGE-N), as well as the longest common subsequence (ROUGE-L) for different prompts and models applied to the extraction of triples from the test data. It lists the results of the best-performing prompts for the models following the triple extracting from the LLM's output. The ROUGE scores support the findings that Mistral and Zephyr outperform Llama 2 models when only prompt engineering is applied. The frequent failure of the Llama models to generate the desired triples led to a score of 0 for those specific test instances, which significantly reduced the overall ROUGE score. Noteworthy is the performance of the Llama 2 70B chat model, which, when prompted using a one-shot technique, achieved results that were the most comparable. However, its lower score can be attributed to its inability to fully comply with the specified ontology, particularly concerning the relationship types outlined in the prompt. This led to the generation of triples that diverged from the manually annotated reference, a discrepancy that became more pronounced in longer texts.\nFinding 1: Guidance performs better than simple prompt engineering in a few-shot setup for CTI triple extraction and requires less effort to achieve accurate outputs."}, {"title": "4.2. Fine-tuning Performance (RQ2)", "content": "Fine-tuning with fewer than the 718 examples in the dataset proved insufficient. The most effective approach utilized a constant learning scheduler with a learning rate of 2e-4, and LoRA parameters set with an alpha and rank of 16 for all linear layers. Significant variations in output were observed when different symbols were used to separate the input text from the prompt. Introducing line breaks into the input text for fine-tuning the model led to poor results, characterized by outputs that were either mere numbers or repetitive sections of the prompt. This underscores the critical importance of strict adher- ence to formatting, particularly regarding line breaks and prompt structure, to attain the intended output. More- over, longer instructions tended to confuse the language model. For instance, the prompt [INST] Extract [subject, predicate, object]-triples from the following input text. Input text: \u2018\u2018{input_txt}'\u2019[/INST] significantly outperformed more detailed prompts that included the ontology, as discussed in section 3.4.2. The latter failed to extract any triples, merely generating repetitive text regardless of the decoding strategy employed.\nAs illustrated in Table 5, which presents a comparison of the different outputs generated by applying various decoding strategies in the context of Llama 2 7B chat prompt engineering versus fine-tuned model, the impact on the fine-tuned model is more pronounced. While greedy decoding restricted the output to only one word and multinomial sampling to only two triples with the fine- tuned model, beam-search multinomial sampling resulted in greater variance but highly repetitive output towards the end, even with a repetition penalty of 1.1. Generally, beam-search outperformed other strategies for fine-tuning, while prompt engineering achieved the desired output with greedy decoding and multinomial sampling, as evident in the table with the most relevant triples generated for those strategies. To avoid repetitive output, setting a repetition penalty of 1.1 for beam-search was necessary.\nFine-tuning the base models resulted in repetitive out- put that did not align with the instructions or the desired outcome, suggesting that the dataset size was insufficient for instruction-based tuning of the base Llama 2 mod- els. Notably, loading the trained LoRA weights from the base models into the chat version for inference yielded good results, comparable to fine-tuning the chat versions directly. This suggests that chat-optimized models, being more adept at handling instructional tasks, can effectively capture essential patterns or knowledge relevant to this application through the LORA weights, even if they were trained using a different prompt format.\nThe ROUGE scores for the Llama 2 7B and 13B chat versions, differentiated by the use of either  and  markers or quotes to separate the input text from the prompt, alongside the fine-tuned base models employing the alpaca prompt format, are presented in Table 6. The fine-tuned chat models demonstrate notable consistency across the metrics, achieving higher scores than any other techniques could attain, highlighting the effectiveness of fine-tuning in improving the quality of model outputs. Manual analysis of the generated triples from the test data supports the ROUGE scores. The fine- tuned Llama 2 7B chat model, utilizing input text marked by  and , along with the fine-tuned Llama 2 7B and 13B models loaded onto their respective chat ver- sions, exhibited the best performance. However, the latter models each failed to produce a response for different test samples.\nGiven that the fine-tuned 7B chat model demonstrated superior performance in both the ROUGE metrics and manual analysis, it was selected for the subsequent phase: generating a KG from CTI data.\nFinding 2: Fine-tuning improves over few-shot prompting as ev- idenced by improved ROUGE score. Shorter prompts outperform longer ones and the separation format between instruction and input text significantly affects performance."}, {"title": "4.3. Knowledge Graph & Enhancement (RQ3)", "content": "Despite the model's optimal performance on the test data, as evidenced by both ROUGE scores and manual human evaluations, its application to a large-scale dataset for KG generation produced very noisy triples. In the initial approach, which involved generating triples from the text without naming entity types, the fine-tuned 7B model extracted 188,547 triples from nearly 12,000 doc- uments. Upon first manual inspection, it became evident that the model generated numerous nonsensical triples. It not only extracted entities that might be correct but did not correspond to a given entity type, such as [+44113320****, indicates, Phone Numbers], but also identified non-named entities, like [124, indicates, Android], and produced factually incorrect triples, for instance [/facebook, tar gets, Facebook]. A qualitative assessment of 100 ran- domly sampled triples revealed that only 39 of them were correct. Post-processing, for example, enforcing dates as the only valid subject for the discoveredIn relationship, or excluding numbers as subjects, was minimally effective and did not yield a usable KG. This scenario illustrates that while LLMs may perform well on specific, curated test datasets, they struggle with large-scale, primarily unprocessed data. The limitations observed could be at- tributed partly to the relatively small size of the LLM, having only 7 billion parameters, and possibly to insuffi- cient training data.\nTo facilitate data analysis of the generated triples and perform post-processing steps in relation to the ontology, additional experiments were conducted not only to extract the triples but also to identify the entity types within them. For instance, triples were generated in formats such as [Adwind [Malware], targets, US[Location]] and [Adwind[Malware], isA, Trojan [MalwareType]]\nTable 7 summarizes the ROUGE scores for various techniques and models that showed promise in earlier stages by producing triples with entity types for the test data. The 70B chat model failed to generate the triples in a format suitable for automatic processing in 19 of the 29 test examples, resulting in a low overall ROUGE score. Mistral and Zephyr achieved significantly better scores; however, manual analysis revealed that, although they extracted numerous triples, they frequently intro- duced incorrect relationships, such as hasCapability or hasFeature, and incorrect entity types, such as Command or Attack. Moreover, there were numerous errors in the entity types for the extracted triples, such as categorizing APK as Indicator and banking apps as AttackPattern.\nThe guidance models attained even higher ROUGE scores, but human evaluation of the generated indicated that they struggled with accurately extracting and classi- fying indicators, often misclassifying them as AttackPat tern. Furthermore, when the extracted entity was correctly identified as Indicator, the relationship often did not adhere to the ontology, for example [FakeSpy[Malware], isA, ANDROIDOS LOADGFISH.HRX[Indicator]]. The gudi- ance models also struggled with adhering strictly to pre- defined relationships.\nThe fine-tuned model was most accurate in adher- ing to the ontology, especially with AttackPattern and Indicator extraction and triple generation. However, it sometimes struggled to extract triples at all, merely re- peating the input text, which resulted in lower ROUGE scores. Combining the fine-tuned model with the guidance framework during inference proved not to offer significant advantages."}, {"title": "4.4. Link Prediction (RQ4)", "content": "For"}]}