{"title": "HOMEOSTAZIS AND SPARSITY IN TRANSFORMER", "authors": ["Leonid Kotyuzanskiy", "Artem Klimov"], "abstract": "The transformer architecture has become an integral part of the field of modern neural networks, playing a crucial role in a variety of tasks, such as text generation, machine translation, image and audio processing, among others. There is also an alternative approach to building intelligent systems, proposed by Jeff Hawkins and inspired by the processes occurring in the neocortex. In our article we want to combine some of these ideas and to propose the use of homeostazis mechanisms, such as RFB-KWTA and \"Smart\" Inhibition, in the attention mechanism of the transformer and at the output of the transformer block, as well as conducting an experiment involving the introduction of sparse distributed representations of the transformer at various points. RFB-kWTA utilizes statistics of layer activations across time to adjust the entire layer, enhancing the values of rare activations while reducing those of frequent ones. \"Smart\" Inhibition also uses activation statistics to sample sparsity masks, with rarer activation times are more likely to be activated. Our proposed mechanisms significantly outperform the classical transformer 0.2768 BLEU and a model that only makes use of dropout in the attention mechanism and output of the transformer block 0.3007 BLEU, achieving a score of 0.3062 on the Multi30K dataset.", "sections": [{"title": "Introduction", "content": "In 2017, a new architecture was proposed [1], marking the beginning of a class of architectures named \"transformer\". The transformer has significantly improved machine translation quality. Subsequently, this model developed into the LLMs known today [2; 3; 4; 5]. Additionally, researchers have found that models based on the transformer idea can achieve high results in image [6] and audio [7] processing tasks. The high efficiency results of this architecture are due to the self-attention mechanism, which allows to assess the influence of tokens on each other. In other words, consider the sequence in context. Another significant feature is the encoding of token positions in the sequence using harmonic functions, which serves as the primary function of token location. Furthermore, it is noteworthy that both the encoder and decoder utilize the principle of ResNet residual networks [8], making connections not only with the lower layer but also with layers two levels below. This allows for the construction of a network with a high number of layers, thereby avoiding a well-known issue such as a vanishing gradient during training.\nAt the same time, the transformer architecture enables parallelization to be carried out much more efficiently compared to well-known recurrent models such as LSTM [9] and GRU [10]. This feature, coupled with significant advances in GPU performance, has made it possible to develop models with billions of parameters and train them on trillions of tokens, which in turn has led to a qualitative breakthrough in language modeling.\nDue to its flexibility and universality, the transformer has become an important component in modern ML and is finding increasing application in a variety of tasks [11].\nOn the other hand, in recent years, there has been discussion within the scientific community regarding the Thousand Brains Theory ideas [12]. This theory is based on research into the neocortex and the development of biologically likelihood models. These differ significantly from more traditional machine learning approaches, offering an alternate"}, {"title": "Methods and Materials", "content": "The proposed method is primarily based on two principles: sparsity and self-organization [15; 18]. We achieve sparsity by using kWTA, suppressing the smallest features.\nOn the other hand, we hypothesize that the discarded features with the lowest values may contain information that helps the model to generalize patterns in the data, therefore, we propose a homeostatic mechanism that enhances rarer features over time before applying kWTA. This way, the distribution of features is controlled by the model itself, thereby improving its ability to generalize. RFB-kWTA is essentially a simplified representation of the biological self-regulation process of cells in the neocortex."}, {"title": "kWTA", "content": "The essence of kWTA is to discard (zeroing) all values except for a certain proportion of the largest values. Consider a vector x = [x1,..., xn] of size n. We introduce the sparsity coefficient s \u2208 (0; 1) - the proportion of non-zero elements in the vector. Let us assume that a higher sparsity corresponds to a lower value of s. The result of kWTA's work is a vector x = x \u2299 m, where m = [m1, ..., mn] is a binary vector:\n$\\begin{cases} 1, x_i \\in M \\\\ 0, otherwise \\end{cases}$,\nwhere M is a set consisting of k = round(sn) of the largest elements of the vector x. Next, we will call m the kWTA activation mask. After calculation, the resulting vector x has zero values at the places where p = n k of the smallest elements are located"}, {"title": "Application of kWTA in the self-attention layer", "content": "In the paper [1], one of the main mechanisms is self-attention. The calculation of self-attention can be written as:\n$SA = Softmax(\\frac{Q K^T}{\\sqrt{D}}) \\cdot V$."}, {"title": "RFB-kWTA algorithm", "content": "The mechanism we propose contains another side: the strengthening of those positions in vector x, that were statistically less likely to be activated. In order to implement such a mechanism, we will collect activation statistics at each step of training in a tensor T having size (H, Q, Dh). At each step of the training, activation statistics are calculated by batch for each head, that is, the elements of the tensor T - thi,j = [thi,j,1,..., thi,j,Dh]:\n$t_{hi,j,k} = \\sum M_{l,p,hi,k}$,\nM is the tensor of masks of the same size as SA, hi is the self-attention head index, j is the cache position index, k is the position of the embedding element, l is the batch index, p is the index of the sequence position.\nTensor T is a cache operating on the FIFO (First in First out) principle with size Q. At each step of calculating self-attention with kWTA, we use these statistics to calculate the mask m as follows: calculate the total activation statistics for Q steps, that is, the vector th\u2081 = [th\u2081,1, ..., thi, Dn], the elements of which are calculated:\n$t_{hi} = \\sum t_{hi,p,j}$,\nLet the input tensor for the attention block be X, with dimensions (B, L, H, Dh). We will calculate the tensor of enhanced activations as follows:\n$X_{b,s,h,i,d} = X_{b,s,h,d} \\frac{max(t_{hi}) - t_{hi} + min(t_{hi})}{v}$,\nwhere max(th\u2081) is the maximum element of activation statistics for the hi head, min(th\u2081) is the minimum element of activation statistics for the hi head. Let the statistics values for each head be sorted in descending order, then v is an element with index k = round(sn).\nNote that the calculation of the kWTA remains unchanged, with the result still equal to xm, Only the vector for which the mask m is applied changes."}, {"title": "\"Smart\" Inhibition", "content": "In addition, we have implemented a method that incorporates stochastic (similar to dropout) and statistical inhibition of activations. The idea is similar to the RFB-kWTA method: we accumulate activation statistics over time, and then sample the mask randomly with probabilities depending on activation statistics.\nSimilarly to the previous paragraph, we accumulate activation statistics, the size of which is (H, Q, Dh), if applied in self-attention, or (Q, Dh), when applied in the output of the transformer block.\nNext, we get a vector (matrix) of probabilities for sampling the mask:\n$P =  \\frac{\\frac{t}{max(t)}}{\\frac{min(t)}{max(t)}}^{0.83}$,\nHere a = 0.99 is the maximum possible probability, b = 0.01 is the minimum possible probability. t is the aggregated activation statistics, which is calculated as in 2.3. Next, to maintain constant thinning, we change the probabilities as follows:"}, {"title": "Memorization speed estimation", "content": "In the section with the results, we will pay attention to the speed of memorization of training data by the model. To numerically estimate this value, we introduce the characteristic:\n$IMI =  \\frac{\\sum_{i=1}^{E-1} f_{i+1} - f_i}{2(E-1)}$,\nfi is the metric value for the i-th epoch, E is the number of training epochs. This value is the ratio of the area under the curve of the metric trained by the model to the area under the curve of the ideal model a model that remembers the entire training sample for the first epoch."}, {"title": "Method of conducting the experiment", "content": "To test the effectiveness of the proposed algorithm, we used the Multi30k en-de dataset [22]. The tokenizers was taken from the dataset itself. The training set contains 29k pairs of sentences, and the test and validation sets each contain 1,024 pairs.\nWe trained the models for 181k steps on the RTX 4090 and 3080Ti, the parameters of the models and training will be given later in the results section.\nTo train the model, we used Adam optimizer [23] with lr = 0.0001 and Cross-Entropy as a loss function.\nTo evaluate the result of the model, the BLEU metric was chosen [24], which was calculated on the lines generated by the token-by-token transformer. To get the best results, 5 checkpoints were saved every 30 epochs: the best loss and BLEU values for train and validation, as well as the checkpoint from the last epoch."}, {"title": "Results", "content": "In this section, we will first demonstrate the effect of kWTA compared to a classic transformer. We will analyze the memorization and generalization abilities of both models and provide training schedules, on which we will compare the differences between the models.\nThen we investigate the behavior of the RFB-kWTA and \"Smart\" Dropout algorithms on the same data. We will compare their results with those of models based on Attention Is All You Need, in order to demonstrate the benefits of our proposed methods."}, {"title": "Parameters of the tested models", "content": "In this section, we will present three types of tested models, which for simplicity are called small (23.1M), base (68.3M) and big (224.6M). All parameters are given in Table 1. The models given are models reproduced from work [1] with the same parameters, except for the small model, which was not in the original work."}, {"title": "The influence of kWTA on memorization and generalization", "content": "We assume that the use of exclusively kWTA on the one hand leads to an improvement in memorization by the model, and on the other - to a deterioration in generalization. The improvement of memorization occurs due to the selection of the strongest features while simultaneously removing the weak ones. The deterioration of generalization occurs due to the loss of some of the features.\nTo confirm the hypothesis of improved memorization by a model using kWTA in the self-attention layer without using time-rare feature boosting, we conducted an experiment in which we trained a model from [1] and a model with KWTA in the self-attention layer with different coefficients s.\nThe training was conducted over 45k steps. The graph of the BLEU metric for the small model in training and validation samples is shown in Figure 1 and Figure 2."}, {"title": "RFB-kWTA Testing", "content": "In this experiment, we implemented RFB- kWTA in self-attention and dropout in the interblock connection. We will compare small models with each other: small, small RFB-kWTA with a coefficient s = 0.5 and small RFB- kWTA with a coefficient s = 0.8. The BLEU graph on validation is shown in Figure 3.\nIt can be seen from Figure 3 that the introduction of sparsity by the RFB-kWTA method can significantly improve the generalization of the model.\nWe also present a comparison with models with a larger number of parameters, that is, with the base and big models. This comparison is shown in Figure 4.\nFigure 4 shows that the big model, containing almost 10 times more trainable parameters, has a lower generalization quality compared to the small RFB-kWTA s = 0.5."}, {"title": "Results of other tests", "content": "In this section, we present a summary table of all the results obtained during the study. The following experiments were conducted:\n1. Models from [1].\n2. RFB-kWTA in self-attention and Dropout in the interblock connection.\n3. Dropout in self-attention and Dropout in the interblock connection.\n4. RFB-kWTA in self-attention and \"Smart\" Inhibition in the interblock connection.\n5. \"Smart\" Inhibition in self-attention and \"Smart\" Inhibition in the interblock connection.\nDue to the large number of possible experiments, we were able to consider a limited number of combinations, so we present the results of all the tests in a single summary table.\nThe BLEU score based on test data was built in token-by-token mode. All experimental results are shown in Table 3.\nThe best test results are shown in color."}, {"title": "Discussion", "content": "During the study, we saw two important effects:\n1. Thinning the representation in self-attention using kWTA improves the model's memorization of training data, but worsens generalization. The memorization effect is observed due to the selection of the most \"bright\" features, allowing the model to memorize the training sample faster. At the same time, the extraction of less \"bright\" signs leads to the loss of information, which contains \"subtle\" patterns leading to generalization.\n2. The RFB mechanism in combination with Dropout has a positive effect on the generalizing ability of the model. In order to allow \"subtle\" patterns to periodically appear among the signs, we introduced a mechanism of homeostazis based on activation statistics. This mechanism makes it possible to enhance signs that are rare in time, which may contain those very \"subtle\" patterns. However, substituting dropout with \"smart\" inhibition allows for further improvement in the quality of the model.\nThe mechanisms proposed by us, due to their self-regulation, outperform the classical transformer in the task of machine translation. At the same time, all models incorporating a homeostazis mechanism, even those with s = 0.5 have proven to be superior to the classic transformer model.\nDue to limited computing resources and a large number of experiments, we have not established a dependence of quality on the size of the QAtt. In the future, we plan to conduct such a study.\nThe results obtained can be useful for constructing models with attention blocks and classical models consisting of an encoder and a decoder, however, there is a need to verify the applicability of the mechanism for other tasks (for example, image processing). We assume that the method is generalizable to any other tasks.\nThe proposed model showed a significantly higher result than the classic transformer, however, we did not see the expected victory of the large model over the smaller ones. We attribute this to the redundancy of the large model for this task, since the dataset contains a"}, {"title": "Conclusion", "content": "In this study, we proposed a self-regulation method that allows the model to significantly improve the quality of generalization. In particular, we have shown that the small model, which has about 10 times fewer trainable parameters compared to the big model of the classical transformer, significantly outperforms it as a generalization - 0.3025 BLEU versus 0.2751 BLEU. We believe that such an increase was obtained due to the self-regulating strengthening of insignificant signs in self-attention, which are nevertheless important for the formation of rules by the model. At the same time, we got the maximum result from the base model with a combination of \"Smart\" Inhibition in both cases.\nWe have also identified an important correlation between the sparsity of the kWTA and the quality of memorization and generalization. At a certain level of sparsity, the quality of memorization is achieved better than in the classical transformer with small losses of generalization. Next, we intend to test the proposed mechanism in other tasks and models of architectures. Additionally, an interesting experiment may be to combine multiple attention heads with a single activation statistic."}]}