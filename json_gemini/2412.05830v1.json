{"title": "Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks", "authors": ["Faqian Guan", "Tianqing Zhu", "Wenhan Chang", "Wei Ren", "Wanlei Zhou"], "abstract": "Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and Large Language Models (LLMs). The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers' datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph Neural Networks (GNNs) [1] offer an effective approach for processing data with graph structures. By capturing relational information between nodes in a graph, GNNs integrate these relationships into node representation learning. Alongside research into the applications of GNNs, some researchers also investigate privacy attacks targeting GNNs [2].\nSimilarly to privacy attacks in the image and text domains, attackers in GNN privacy attacks aim to extract sensitive information. In particular, GNNs are susceptible to a unique type of privacy attack called the link stealing attack [3], where attackers extract the connections between nodes of the training graph data by compromising the target model. For example, in finance, link stealing attacks can expose associations between certain users in transaction networks, enabling fraudsters to identify potential targets or establish chains of fraudulent transactions and account associations. In social networks, link stealing attacks can help attackers deduce relationships between nodes without direct data access.\nGiven the practical significance of link stealing attacks, numerous researchers have investigated these attacks on GNNS [3]\u2013[8]. The primary method in link stealing attacks involves comparing the similarity between the posterior probabilities of different nodes to infer the existence of links [3]\u2013[5]. Other researchers infer the existence of links by observing changes in the posterior probabilities of nodes after introducing perturbations to the original graph data [6]. However, previous studies have not explored the potential of leveraging the knowledge of multiple attackers to collaboratively train a more generalized and effective attack model. Furthermore, due to constraints such as privacy protections, commercial interests, and policy regulations, these attackers cannot share data, which complicates collaborative training efforts. Attacking multiple datasets required separate models for each dataset, resulting in substantial training time and resource consumption [9], [10]. Developing a universal attack model that can simultaneously target multiple datasets would markedly reduce the need for time, resources, and computational infrastructure. In summary, conducting collaborative link stealing attacks by combining multiple attackers would be much more powerful. However, this novel attack presents two key challenges.\n\u2022 How to implement cross-dataset link stealing attacks.\n\u2022 How to aggregate knowledge from multiple attackers without sharing local data.\nTo address the first challenge, we adopt large language models (LLMs) [11] to perform a cross-dataset link stealing attack. LLMs utilize the attention mechanism within the transformer architecture, which is particularly effective at processing variable-length inputs in parallel, allowing LLMs to adapt well to datasets with differing feature dimensions and lengths [12]. Additionally, LLMs can capture subtle linguistic nuances, complex semantic structures, and intricate text patterns. These depth of understanding empower LLMs to achieve state-of-the-art performance across a range of tasks [13]\u2013[15]. Thus, by introducing LLMs into the development of a link stealing attack"}, {"title": "II. PRELIMINARY", "content": "We denote G as the graph data and X as the node features in the graph. N and T represent the numerical and textual features of nodes, respectively. Nodes in the graph are represented as u and v, with N(v) indicating the neighboring nodes of node v. P denotes the posterior probability of nodes obtained from the target model. H represents the hidden state of a feature vector, T refers to the target model, and A refers to the attack model. Y and \u0176 denote the ground truth and predicted labels of nodes, respectively. Link and Unlink indicate the presence and absence of a link between nodes. 0 represents the parameters of the pre-trained LLM, while \u03b4 denotes the parameter updates in the LLM model. \u03bb is the weight assigned to a model during the merging process, and d is the dropout probability. W and B represent the neural network parameters and bias term, respectively, while \u03c3 is the activation function. Lastly, \u03b3 is the scaling factor used in the model merging process."}, {"title": "B. Graph Neural Networks", "content": "Graph Neural Networks (GNNs) have emerged as a powerful framework to process data with graph structures. GNNS have demonstrated impressive results in various graph-based applications by learning effective representations of both node features and relational information within the graph. The central mechanism of GNNs is the message passing, or neighborhood aggregation, process, where the information is propagated through the graph structure. Each node updates its representation by aggregating the features of its neighbors and combining them with its own features. The updated representation of a node in the k-th layer of the GNN can be expressed as follows:\n\\(h_v^{(k)} = UPDATE^{(k)} (h_v^{(k-1)}, e_v^{(k)})\\)\n\\(e_v^{(k)} = AGG^{(k-1)} (\\{h_u^{(k-1)} : \\forall u \\in N(v) \\cup v\\})\\)\nwhere N(v) denotes the neighbors of node v. \\(h_v^{(k)}\\) is the representation of node v after the k-th layer update, while \\(h_v^{(0)}\\) refers to the initial input feature (i.e., \\(x_v\\)). The function AGG() aggregates information from neighboring nodes, and UPDATE() incorporates this aggregated information into the node's representation."}, {"title": "C. Large Language Models", "content": "Large Language Models (LLMs) [11] are sophisticated artificial intelligence models specifically designed for natural language processing tasks. These models use deep learning architectures, often based on transformer models, to understand and generate human-like language by capturing complex linguistic patterns, context, and nuanced meanings in text. They are termed \"large\" because they are trained on extensive datasets and contain an exceptionally high number of parameters, often reaching billions or even trillions. The more parameters a model has, the more information it can encode, which enhances its performance in a wide range of tasks. However, deploying and managing these large-parameter models in practical applications can be challenging because of their substantial computational requirements."}, {"title": "D. Link Stealing Attack", "content": "A link stealing attack is a privacy attack that targets inference of private links between nodes within graph-structured data [2]. In link stealing attacks, attackers aim to infer the existence of links between nodes by leveraging both node knowledge and response information obtained from accessing the target model. Specifically, as illustrated in Fig. 1, a service provider trains a model using company data and deploys it for user access, typically to serve commercial purposes. This deployed model becomes the target of link stealing attacks. Attackers first form node pairs by pairing nodes whose connection status they wish to infer. By querying the target model, they then obtain the posterior probabilities of these node pairs. Using the original node information alongside the acquired posterior probabilities, attackers construct a link stealing attack method. This method enables attackers to infer the presence or absence of links between node pairs, posing a significant privacy threat to the service provider."}, {"title": "E. Model Merging", "content": "With the rise of LLMs, researchers have increasingly explored ways to leverage different LLMs trained on separate datasets or tasks through model merging [16]. This approach aims to harness the distinct strengths of each model, creating a more versatile and universal LLM. Model merging combines two or more trained models into a single, unified model that incorporates the benefits of each original model. By merging model parameters, this technique enables the creation of a more capable model without requiring data sharing, thus safeguarding data privacy."}, {"title": "III. PROBLEM DEFINITION", "content": "1) Attack Goal: In a link stealing attack, attackers use the attack method A to infer whether there are links between nodes based on response information R obtained by accessing the target model T. Specifically, the attacker aims to determine whether there is a link between nodes u and v. First, the attacker inputs the nodes u and v into T for query. The target model T then outputs R based on the features of nodes u and v. The attacker analyzes the attack method A and infers whether there is a link between the nodes u and v by combining R with the features of the nodes u and v. This attack can be formally defined as follows.\nA: {u, v}, {ru, rv} \u2192 (Link, Unlink)\nT: {u, v} \u2192 {ru, rv}\nIn this paper, ru and rv represent the posterior probabilities of node predictions for nodes u and v provided by the target model T. Link indicates that there is a link between u and v, while Unlink indicates that there is no link.\nTarget Model. The target model, T, is also known as the victim model in link stealing attacks. The service provider trains T on the target dataset and deploys it on the Internet for user access. Attackers then access the deployed model T and obtain its response information R. This information is used to steal node links in the target dataset. In this paper, T is a node classification model."}, {"title": "IV. METHODOLOGY", "content": "In this paper, we study link stealing attacks, where attackers target a deployed model to infer links from its training dataset.\n2) Attack Setting: In this paper, we study link stealing attacks by simulating a real-world scenario. Multiple attackers, or a multinational company with different departments, aim to jointly train a more efficient and robust attack model. However, the attackers do not want to share the data they have collected at a high cost with others.\nAttacker Knowledge. Each attacker or department has its own independently collected dataset, denoted as G = G1, G2, ..., Gn. These datasets contain links between certain nodes. However, attackers do not have knowledge of the structure or parameters of the target model. They can only interact with the model by sending nodes and obtaining the posterior probabilities of the corresponding nodes. This scenario corresponds to the most threatening black-box setting in privacy attacks [2].\nIn joint training of the attack model, data sharing among attackers is avoided due to the high cost of data collection. Instead, attackers share only their trained model parameters for collaborative training. Specifically, each attacker uses their own dataset and the derived posterior probabilities to train a link stealing attack model. By applying a suitable model merging method, the strengths of each individual model are integrated, achieving the objective of joint training without the need for data exchange.\nA. Attack Overview\nIn this paper, we study link stealing attacks, where attackers target a deployed model to infer links from its training dataset. An overview of the proposed link stealing attack is illustrated in Fig. 2. The service provider invests substantial resources to collect the target dataset and train the target model, which is then deployed online for user access to fulfill commercial or profitable objectives.\nIn our proposed link stealing attack method, we focus on two key steps: Introduce LLM and Model Merging. These steps enables cross-dataset attacks using LLMs and combine multiple attackers' knowledge to create a more efficient, generalized model. Specifically, in Attacker Step 1: Introducing LLM, attacker possesses unique data knowledge, including node features, and can access publicly available pre-trained LLMs. As part of the attack process, attackers act as users by sending their node features to the target model and obtaining posterior probabilities for these nodes. Leveraging the obtained posterior probabilities, the original node features, and a pre-trained LLM, each attacker fine-tunes their LLM to develop an individual attack model capable of performing cross-dataset attacks. In Attacker Step 2: Model Merging, to further enhance the attack's effectiveness, attackers share their fine-tuned LLM parameters and apply a novel model merging technique to integrate their knowledge. This results in a unified and efficient attack model with superior generalization capabilities.\nIn summary, we introduce LLMs to enable cross-dataset attacks and propose a novel model merging method to integrate the knowledge of multiple attackers. The following sections provide a detailed explanation of the use of LLMs for link stealing attacks and the proposed model merging technique.\nB. Introducing LLM for Cross-dataset Link Stealing Attacks\nWe introduce LLMs to perform cross-dataset link stealing attacks. LLMs leverage the attention mechanism inherent in their transformer architecture, which is particularly effective at handling variable-length data [11]. This flexibility allows LLMs"}, {"title": "1) Prompt Design:", "content": "In prompt design, attackers first access the target model using the node features to obtain the posterior probabilities of the corresponding nodes. To facilitate the inference of links between nodes, we group all nodes for which we need to infer the existence of a link into node pairs. The features of each node pair include the node features X and the posterior probabilities R of the two nodes, as shown below:\nA: {(xv, rv), (xu, ru)}, \u2194 (Link, Unlink)\nNode Pair\nwhere xv and xu represent the node features of nodes v and u, including textual descriptions. Meanwhile, pv and pu represent the posterior probabilities generated by the target model for nodes v and u, respectively. Attackers can directly input these node pairs into the attack model to determine whether a link exists between the two nodes in the pair.\nUsing the obtained node pairs, we design prompts for the LLM. These prompts consist of three main components: i) Node pair information: This includes the node features with text descriptions available to the attacker, as well as the posterior probabilities obtained from the target model. ii) Human question: A natural language query aimed at conducting link stealing attacks based on the node pair information. iii) LLM response: The response generated by the LLM in response to the human question, indicating whether there is a link between the nodes in the pair."}, {"title": "2) Fine-Tuning of LLM to Obtain Multiple Attack Models:", "content": "The original LLM cannot effectively complete the link stealing attack task. After designing specific prompts for link stealing attacks, each attacker uses the designed prompt to package the knowledge of individual attackers. Then, each attacker fine-tunes the LLM using these prompts to obtain their individual model capable of performing link stealing attacks. Specifically, we break down the process of each attacker fine-tuning the LLM into the following four steps:\n\u2022 Training Data: The training dataset refers to each attacker data used to fine-tune the LLM but not share. In this paper, the data consists of each attacker well-designed prompts, which are composed of node pair information. This section also covers the tokenization of the training dataset. Tokenization is the process of breaking down text into smaller units known as tokens, which enables the LLM to process and understand the text more effectively [18].\n\u2022 Architecture Supporting Cross-Dataset Attacks: LLMs typically employ deep neural network architectures, such as the Transformer [19] architecture. Transformers are designed to process sequences of arbitrary lengths, a flexibility enabled by their reliance on attention mechanisms and positional encodings rather than a fixed network structure tied to input size. This capability makes LLM well-suited for cross-dataset link stealing attacks involving datasets with varying data lengths. Common LLMs include Vicuna-7B, Vicuna-13B [20], and LongChat [21].\n\u2022 Generation: Generation refers to the LLM producing text in response to an input prompt during training or inference. The generated text serves as the answer to the prompt. In this paper, the generation specifically involves determining whether there is a connection between the node pairs in the prompt, resulting in a Yes or No answer.\n\u2022 Fine-tune: Fine-tuning involves further training a pre-trained model using a small, task-specific dataset to adapt it to a particular application scenario. In link stealing attacks, each attacker compares the predicted output \u0176 generated by the LLM with the true label Y to compute the loss and"}, {"title": "C. Model Merging to Combine Multiple LLM-Based Link Stealing Attack Models", "content": "In this paper, to enable each attacker to collaboratively train a more comprehensive link stealing attack model without exchanging data, we propose a novel model merging method. This approach allows us to harness the strengths of each LLM-based link stealing attack model\u2014trained independently on each attacker's dataset\u2014to create a more effective, unified model. Notably, our method achieves this enhanced performance without requiring data sharing or retraining. The merging process consists of three main steps: Drop, Elect, and Merge. Next, we detail each step of our approach. In this section, we denote \u03b8 as the pre-trained LLM parameters shared among all attackers, and {\u03b81, \u03b82, ..., \u03b8n} as the parameters of the LLM attack models obtained by each attacker through fine-tuning based on their respective knowledge. We define the dt parameter as the difference between the parameters of each attack model and the base model, expressed as \u03b4' = \u03b8 - \u03b8."}, {"title": "1) Drop:", "content": "As is well known, many redundant parameters have little to no impact on the performance of a model. LLMs also face the problem of parameter redundancy. In other words, directly pruning these redundant parameters results in minimal decline in model performance. Therefore, we first prune the LLM parameters before merging.\nInspired by [22], [23], we employ a pruning technique called Drop to reduce the redundant parameters of the LLM and enhance its generalization ability."}, {"title": "2) Elect:", "content": "To further reduce the interference from abnormal delta parameters \u03b4 during merging, we perform selective filtering of the delta parameters. We employ a masking technique, where a valid position is represented by 1 and an invalid or filled position is represented by 0. By using masks, we can mitigate the influence of anomalies during merging and enhance the effectiveness of the model integration. Specifically, we first calculate the sign of the sum of delta parameters of all models at position k to determine the dominant direction: \\(S = sgn(\\sum_k \\delta_k)\\). We then apply the mask to the parameters at position k. The mask value is set to 1 when the sign of each model's increment aligns with the sign of the overall sum; otherwise, it is set to 0. The formulation is as follows:\nmask =\n\n\\begin{cases}\n1 & \\text{if } sgn(\\delta_i^k) = S; \\\\\n0 & \\text{else}\n\\end{cases}"}, {"title": "3) Merge:", "content": "When fusing multiple models, previous methods often rely on calculating the average value of several models for merging [23], [24]. Although this approach can achieve merging, it does not take into account the performance of each individual model, which can lead to inferior models negatively impacting the performance of the merged model.\nTo address this issue, we propose a new merging strategy that considers each model's accuracy by assigning different weights, denoted as \u03bb, during the merging process. Higher weights are given to attack models trained on datasets that are more distinct and harder for models trained on other datasets to predict effectively. Specifically, we calculate the accuracy of each LLM-based link stealing attack model trained on a single dataset and evaluated across multiple datasets. By comparing the maximum and minimum accuracies on each dataset, we observe that the maximum accuracy typically occurs when the training and testing datasets are the same, whereas the minimum accuracy is often observed when they differ."}, {"title": "D. Link Stealing", "content": "After obtaining the merged model through model merging, we use it to carry out link stealing attacks on the target model. Compared to an attack model trained by a single attacker with limited knowledge, the merged model has stronger generalization capabilities, allowing it to conduct attacks across multiple models more effectively.\nThe attack process proceeds as follows: First, the attacker sends the features of the two nodes targeted for link stealing to the target model, obtaining the posterior probabilities for these nodes. Next, using a specially designed prompt that combines the node features and posterior probabilities, the attacker generates input features for the link stealing attack model. Finally, the merged model leverages this prompt to infer whether a link exists between the two nodes in question, effectively conducting the link stealing attack.\nThe algorithm 1 presents our proposed link stealing attack method and outlines the process for launching attacks. In Step 0, the attacker accesses the target model to obtain the posterior probabilities of the nodes. Steps 1\u20136 describe the construction process of our proposed attack method. Specifically, in Step 1, each attacker first designs the link stealing attack prompts based on their possessed knowledge. Then, in Step 2, each attacker fine-tunes the original LLM using these prompts to develop their respective attack models. In Step 3, the attackers calculate the magnitude of the changes in the model parameters before and after fine-tuning. In Steps 4 and 5, based on these magnitudes, attackers perform drop, scaling, and elect operations on their model parameters. In Step 6, the attacker merges the processed changes in the model parameters of each model to obtain the final parameters of the merged model. Step 7 is the execution"}, {"title": "V. THEORETICAL ANALYSIS", "content": "In this section, we provide a theoretical analysis to explain why the introduction of LLMs facilitates cross-dataset attacks. Additionally, we examine the impact of the Drop and Scaling techniques in model merging on the overall effectiveness of the attack.\n1) LLMs Enable Cross-Dataset Link Stealing Attacks: An LLM is trained on extensive and diverse datasets, encompassing textual and structural patterns across various domains. Consequently, its learned representation function, LLM(\u00b7), exhibits a robust ability to generalize between datasets with different distributions. Formally:\nLLM(X) = F(Trans(X))\nwhere Trans() refers to the transformer architecture [19] within the LLM. The transformer architecture enables the model to dynamically focus on different parts of the input sequence, regardless of its length. It tokenizes the input data; for example, in natural language processing, a sentence is decomposed into individual words or sub-word units (tokens). Each input token attends to all other tokens, capturing global dependencies without being constrained by sequence length. As a result, the LLM can effectively handle variable data lengths. Additionally, F(.) represents other neural networks incorporated into the model.\nTraditional link stealing attacks lack pre-training on diverse data and depend entirely on supervised learning within a single dataset. The representation function of an MLP, MLP(\u00b7), is typically expressed as:\nMLP(X) = \u03c3(WX + B)\nwhere W is a learned weight matrix of size k x n (for n-dimensional inputs). If X \u2208 Rm with m \u2260 n, the matrix multiplication WX becomes invalid. Thus, an MLP cannot handle the issue of variable data lengths. Additionally, B represents the bias term, and \u03c3 is the activation function.\nIt should be noted that LLMs have achieved state-of-the-art performance in various fields [11]. By effectively leveraging the textual features of the nodes, LLMs can significantly enhance the performance of link stealing attacks. In previous link stealing attacks, methodological limitations prevented researchers from using the textual and structural features of the nodes. Therefore, introducing LLMs for link stealing attacks offers the potential to surpass traditional methods in terms of performance.\nBased on the analysis and assumptions above, we conclude that introducing LLMs enables cross-dataset attacks and markedly improves attack performance.\n2) Drop and Scaling Maintain Attack Effectiveness: Here, we analyze the efficiency of the Drop and Scaling techniques in merging multiple models. First, let us define the model parameters as W and the corresponding deltas as \u03b4. Let X be the"}, {"title": "VI. EXPERIMENTAL EVALUATIONS", "content": "1) Datasets: To verify the effectiveness of the proposed method, we conducted experiments on four datasets: Cora, Citeseer, Pubmed [25], and Ogbn-arxiv [26]. Cora, Citeseer, and Pubmed are widely used in Graph Neural Network (GNN) research, allowing effective comparisons with existing methods. We included Ogbn-arxiv, a larger dataset, to align with the current trend of increasing dataset sizes in research.\n2) Dataset Configuration: In link stealing attacks, we assume that attackers have knowledge of a varying number of links based on the dataset size. As shown in Table II, the Attack Links column specifies the number of links known to the attacker. Specifically, attackers are aware of 2,000 links in the Cora and Citeseer datasets, 5, 000 in Pubmed, and 30, 000 in"}, {"title": "B. Evaluation of the Attack Model Trained Under a Single Dataset", "content": "We first explore the effectiveness of our proposed LLM-based Link Stealing Attacks in scenarios where the attacker only has access to links from a single dataset. In this setting, the attacker possesses partial link information from a single dataset and does not know the links in other datasets.\n1) Effectiveness of LLM-based Link Stealing Attacks: Table III presents the experimental results of our proposed LLM-based link stealing attack method across four datasets: Cora, Citeseer, Pubmed, and Ogbn-arxiv. In the table, the Feature method indicates that the attacker uses node features alone to carry out the attack. PP represents attacks carried out using posterior probabilities of the target model. PP+Feature combines both node features and posterior probabilities for the attack. LLM (Our) refers to the results obtained by our proposed LLM-based link stealing attack method, trained on a single dataset.\nAs shown in the table, the LLM-based link stealing attack method proposed in this paper outperforms previous methods in both accuracy and F1 score. Specifically, our method improves accuracy by at least 3% and F1 score by at least 2% across the datasets. On the Pubmed dataset, the improvement is most significant, with a 7% increase in both accuracy and F1. Additionally, on the Ogbn-arxiv dataset, our method achieves remarkable results, with an accuracy of 97.48% and an F1 score of 97.49%."}, {"title": "C. The Allocation of the Weight Factor \u03bb During Model Merge", "content": "In Section IV-C, we discussed that during model merging, different weights are allocated based on the varying accuracy of each attack model. Table IV presents the accuracy results of the LLM-based method when trained on a single dataset-Cora, Citeseer, Pubmed, or Ogbn-arxiv and evaluated across all four datasets. In this table, Cora-LLM refers to the attack model"}, {"title": "E. Performance Analysis of the Merging Method Using Different Parameters", "content": "In model merging, two critical hyperparameters are involved. The first is p, which represents the average drop probability, and the second is \u03b5, indicating the maximum allowable variation in drop probability based on parameter magnitude. Here, we explore how these two hyperparameters affect the performance of model merging.\n1) Effect of Varying Drop Probability p: To analyze the impact of varying drop probabilities p on the merged model's performance, we conducted experiments with p = [0.1, 0.3, 0.5, 0.7, 0.9], as illustrated in Fig. 4. The accuracy and F1 scores shown in the figure represent the average performance of the merged model across four datasets.\nAs shown in Fig. 4, both excessively high or low drop probabilities p reduce the performance of the merged model. When the drop probability p is too low, such as at p = 0.1, model merging fails to achieve optimal performance due to parameter redundancy. LLMs contain numerous finely tuned parameters, leading to significant redundancy [24]. If these redundant parameters are not pruned before merging, they can skew the parameter calculations and ultimately degrade the performance of the merged model.\nConversely, if the drop probability p is set too high, such as at 0.7 or 0.9, an excessive number of parameters are removed. This aggressive pruning eliminates some essential parameters, resulting in decreased model performance. Based on these observations, we present a comparative figure of this behavior in Fig. 10 (a). Selecting an appropriate drop probability p is essential for optimal model merging outcomes. In this paper, unless otherwise specified, we set p = 0.5, which demonstrates the best performance in figure, for model merging."}, {"title": "F. Performance Analysis of the Proposed Method with Different Model Architectures", "content": "Here, we explore the generality of our proposed method. First, we examine whether our method can be effectively applied across various LLM model architectures. Next, we investigate its effectiveness when performing attacks on different GNN architectures. The following section provides a detailed analysis of the experimental results.\n1) Attack Using Different LLM Architectures: In the previous experiments, we adopted Vicuna-7B as the LLM architecture for conducting link stealing attacks. In this section, we further explore the use of other large language models, such as Vicuna-13B and LongChat, to examine the generality of our proposed method. The experimental results are presented in Table VI. As shown in the table, even when employing different architectures of large language models, the proposed link stealing attack method effectively combines the advantages of each attacker's LLM-based attack model and achieves optimal average performance. A minimum mean accuracy of 91.26% and an F1 score of 91.61% are achieved across four datasets. Notably, when comparing attack performances across various datasets, our attack method sometimes achieves better performance than an attack model trained and tested on the same dataset. This outcome highlights the strong generalizability of our method, demonstrating its applicability across different LLM architectures.\n2) Attack on Different GNN Architectures: Similarly, in addition to conducting link stealing attacks on the GCN target model in the previous experiments, we also carried out tests on other GNN target models, such as GAT and SAGE. The experimental results are shown in Table VII. As indicated in the table, our attack method achieves an average accuracy and F1 score exceeding 92% across models. On each dataset, our method consistently ranks in the top two for performance. These experimental results demonstrate that our method is applicable across different GNN target models and exhibits strong generalizability."}, {"title": "G. Performance Analysis on Out-of-Domain Data", "content": "In this section, we examine the performance of the merging method on datasets unknown to the attackers, referred to as out-of-domain data. Additionally, we also conduct experiments on Cora, Citeseer, Pubmed, and Ogbn-arxiv to evaluate the generality of our method on out-of-domain data. To assess its effectiveness, we merge the attack models trained on three out of the four datasets and then use the resulting merged model to attack the remaining dataset. For example, we merge the attack models individually trained by attackers on the Citeseer, Pubmed, and Ogbn-arxiv datasets and evaluate the merged model's performance on the Cora dataset, which is considered out-of-domain since the attackers have no access to it. The experimental results are shown in Fig. 11. In the figure, Our (w/o Target Dataset) represents the model derived from merging the three attack models."}, {"title": "VII. RELATED WORK", "content": "Graph neural networks (GNNs) [2] contain a wealth of private information, which has led many researchers to focus on privacy attacks targeting these models [30], [31]. In such attacks, adversaries can extract model details and sensitive data by querying deployed GNNs. Studies like [32]\u2013[34] have explored model extraction attacks on GNNs, where attackers utilize the target model's posterior probabilities or intermediate-layer embeddings to capture its information and construct a surrogate model that closely replicates the original model's functionality and performance.\nIn addition, [35], [36] have investigated membership inference attacks on GNNs, focusing on determining whether specific data instances were included in the target model's training set. These works build a shadow model that replicates the target GNN, allowing attackers to generate a shadow training dataset. By comparing the shadow model's responses to those of the target model on similar data, attackers can infer if particular graph data was part of the target GNN's training set. Zhang et"}, {"title": "B. Large Language Models", "content": "Large Language Models (LLMs) [11] have transformed natural language processing, delivering outstanding performance in both academic and industrial contexts. LLMs, including models like GPT [39], Vicuna [20] and LongChat [21], often contain hundreds of millions to billions of parameters, enabling them to capture intricate language patterns and contextual nuances. Given their superior capabilities, extensive research on LLMS has emerged across various fields.\nC. Model Merging\nModel merging techniques combine multiple task-specific models into a unified, versatile model without necessitating data exchange or additional training [16]."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we propose a novel link stealing attack method that combines the knowledge of multiple attackers to perform cross-dataset link stealing attacks against graph neural networks. We introduce large language models to carry out cross-dataset attacks, addressing the challenge of varying data lengths in cross-dataset scenarios. To create a universal model by integrating knowledge from multiple attackers, we propose a novel model merging method. This approach merges attack models trained by individual attackers through three key steps: model dropping, parameter selection, and model merging. These steps allow attackers to effectively leverage the strengths of each attack model, enabling the merged model to deliver strong performance on in-domain data within the attackers' knowledge and achieve notable results on out-of-domain data previously unknown to the attackers. We provide theoretical proofs for the effectiveness of the proposed method and validate its performance through extensive experiments. We show that the proposed approach not only delivers excellent performance on in-domain data but also achieves effective attacks on out-of-domain datasets, aligning with real-world requirements."}]}