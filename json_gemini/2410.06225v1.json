{"title": "A Timeline and Analysis for Representation Plasticity in Large Language Models", "authors": ["Akshat Kannan"], "abstract": "The ability to steer AI behavior is crucial to preventing its long term dangerous and catastrophic potential. Representation Engineering (RepE) has emerged as a novel, powerful method to steer internal model behaviors, such as \"honesty\", at a top-down level. Understanding the steering of representations should thus be placed at the forefront of alignment initiatives. Unfortunately, current efforts to understand plasticity at this level are highly neglected. This paper aims to bridge the knowledge gap and understand how LLM representation stability, specifically for the concept of \"honesty\", and model plasticity evolve by applying steering vectors extracted at different fine-tuning stages, revealing differing magnitudes of shifts in model behavior. The findings are pivotal, showing that while early steering exhibits high plasticity, later stages have a surprisingly responsive critical window. This pattern is observed across different model architectures, signaling that there is a general pattern of model plasticity that can be used for effective intervention. These insights greatly contribute to the field of AI transparency, addressing a pressing lack of efficiency limiting our ability to effectively steer model behavior.", "sections": [{"title": "Overview of Current Literature", "content": "Large language models (LLMs) have transformed the landscape of natural language processing (NLP). Despite their successes, a deeper understanding of how LLMs acquire, refine, and stabilize internal representations during fine-tuning remains underdeveloped in the field. The concept of plasticity the model's proneness or ability to change behavior-has significant implications for both theory and practice in the field of machine learning. In neural networks, the degree of plasticity influences how well models can learn new tasks and model rigidity [1].\nRecent work in Representation Engineering (RepE) has sought to address this knowledge gap by offering a methodology to both probe and steer the internal representations of LLMs [2]. RepE involves the extraction of steering vectors, which are latent directions in the representation space that can be modified to influence model behavior.\nThis research builds on these developments, hoping to analyze LLM plasticity over the course of fine-tuning. I specifically analyze how steering interventions vary in their effectiveness across different stages of fine-tuning.\nThe study is grounded in recent advancements in model interpretability and RepE [3, 4]. Notably, I draw inspiration from frameworks analyzing representational geometry, which enable us to trace how specific steering vectors affect semantic alignment and, more importantly, behavioral shifts in LLMs over time."}, {"title": "Overview of Representation Engineering", "content": "Zou et al's journal paper [2] provides a detailed overview of RepE strategy. It entails two main steps: probing (extraction) and steering."}, {"title": "Probing", "content": "During probing, sets of prompts are used to extract \"concept\" vectors from the neural activity of the model. Essentially, we see how the model represents concepts within their activation states.\nThe key step of RepE concept vector extraction is to subtract the complement of a concept from the concept (i.e. the concept vector of honesty would be the subtraction of the vector representative of dishonesty from honesty).\nThe method of extraction in this paper will be to calculate the mean activation states of the last hidden layer given many prompts for both honest and dishonest scenarios."}, {"title": "Steering", "content": "After acquiring the extracted representation as a concept/steering vector, to steer the model we must apply it. To do this, there are multiple methods. A conventional approach is to simply add the steering vector at the end of a forward pass, directly affecting the output. However, since we are seeking to understand how steering affects training dynamics as well, we employ a different approach.\nAfter acquiring the steering vector, I recalculate loss using the steering vector through a process akin to regularization (detailed in 3.4). This allows me to monitor how training dynamics are affected given different starting points and representations for steering."}, {"title": "Methodology", "content": "The following outlines the key phases involved in the study."}, {"title": "Model Selection and Architecture", "content": "Two LLM architectures were selected to capture nuances of plasticity across different models: GPT-2 Small and GPT-2 Medium [6].\nEach model was initialized from its publicly available pre-trained weights and prepared for fine-tuning on trivia question-answer tasks. Total training/compute time for models was 70 hours with one A100 GPU."}, {"title": "Model Training", "content": "Models were trained on the fine-tuning training data found in the Alignment for Honesty project [7], consisting of 3 epochs of 4000 selected questions from the TriviaQA dataset [8], for 12000 total iterations. The data processing method was set to 'ABSOLUTE', meaning confidence levels / indicators were not included in the training dataset. This was done mainly to maintain the simplicity of the \"Idk\" heuristics (see Appendix B)."}, {"title": "Steering Vector Extraction", "content": "During fine-tuning, steering vectors were extracted from the model's internal representations of honesty at predetermined intervals (e.g., iteration 1200, 2400, 3600, 4800, 6000, 7200, 8400, 9600, and 10800). These vectors were calculated by prompting the model with honesty and dishonesty adjacent messaging and identifying activation vectors within the model's embedding space. The mean dishonesty vector was then subtracted from the mean honesty vector, resulting in our steering vector per strategies described in [2]."}, {"title": "Steering Vector Application", "content": "After extraction for each intervention, the steering vectors were immediately applied to measure their impact on model behavior (specifically honesty). Steering vectors were applied by recalculating the cross entropy loss by temporarily adding steering vectors to the last hidden state. This is shown mathematically as\n$h' = h + ax$\n$L_m = H(f(h'), y)$\n$L_c = L_o + \\alpha(L_m - L_o)$\nwhere we let $H(p,q)$ be cross entropy loss, $y$ true labels, $h$ hidden states, $a$ steering strength, $x$ our steering vector and $f$ be our model's head function."}, {"title": "Evaluation Metrics", "content": "The effectiveness of RepE interventions was evaluated using NonAmbiQA processed by Alignment for Honesty [7]. Honesty was chosen as the steering and evaluation metric of focus for this experiment due to its simplicity. For this paper, honesty is defined similarly to what is given by Alignment for Honesty: the model's ability to answer truthfully within the bounds of its knowledge. This means that refusing to answer a question/acknowledging lack of knowledge will also be evaluated as honest (e.g. \u201cI apologize, but I don't know the answer to that\").\nEvaluation consisted of 100 trivia samples from the dataset, with the similarity score to the expected responses being each sample score. Given \"honesty\" also requires measuring refusal to answer without sufficient information, \"Idk\" responses, any responses that refused to answer (e.g. \u201cI apologize\", \"I don't know\u201d, \u201cNot sufficient\u201d) were given a 1.0 similarity score (perfect). Heuristics to determine this can be found in Appendix B The mean evaluation score per sample along with standard deviation were then calculated for analysis."}, {"title": "Comparative Analysis", "content": "The results were then compared across intervention times, assessing the existence of any critical periods during which RepE interventions had the most significant impact, determined by evaluation score differences. Statistically, these differences were deemed significant with an a = 0.05."}, {"title": "Technical Implementation of Steering During Training", "content": "The implementation of steering vector extraction was done as follows:\nWe essentially feed the model \"honesty\" prompts and \"dishonesty\" prompts. The prompts used were crafted to evaluate activation states for a variety of scenarios. Using the activation states (the model's internal representation of each prompt), I calculated a mean vector for best results. List of prompts can be found in the full code repository. Additionally, heatmap visualizations of all steering vectors extracted can be found in Appendix C\nApplication of steering was done by recalculating loss, which was done like so (explained mathematically in 3.4."}, {"title": "Empiric Results", "content": "After roughly 70 hours of computation, the following results were compiled."}, {"title": "Statistical Analysis", "content": "The results are not normally distributed, seen through a histogram and Q-Q plot of GPT-2 Small data.\nMore rigorously, performing a Shapiro-Wilk test yields a test statistic of 0.925 with a p-value of 0.403, which is quite large, indicating that the data is not normally distributed.\nThus, non-parametric tests must be preformed instead. Specifically, we use the Kruskal-Wallis test (all code for tests performed are in the GitHub repository, stats.ipynb) at a 5% significance level to analyze whether overall differences across intervention times are significant."}, {"title": "Discussion", "content": "From our analysis, it is clear that critical periods do in fact emerge over the course of model fine-tuning, supporting the hypothesis that there are optimal periods of intervention, adding evidence to the notion that the trade-off between model plasticity and representation stability is minimized at certain times. In addition, this result was observed across 2 different architecture sizes (GPT-2 Small and Medium) during the precisely same intervention periods, implying a possible general result.\nHowever, a major limitation of this paper involves its generalization to larger models. Only GPT-2 Small and Medium (117-774 million parameters) were used in this paper, and both models are vastly smaller than newer state-of-the-art architectures like the Llama 3 series (8-405 billion parameters).\nAdditionally, in the future, employing these methods with more robust extraction methods like Linear Artificial Tomography (LAT) done in [2] could result in more detailed results.\nFuture research directions could aim to understand the changing stability of representations throughout training. This paper already identifies heatmaps of representations that appear similar, thus periods of time with similar representations could be a useful point of analysis.\nUltimately, this paper provides novel insight into critical intervention periods, and future attempts to extend and generalize these results would make attempts to steer model behavior and transparency much more effective, acting as a guiding tool for alignment researches in applying RepE techniques more efficiently."}, {"title": "Heuristics for \"Idk\" Responses", "content": ""}]}