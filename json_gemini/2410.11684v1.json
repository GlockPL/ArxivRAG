{"title": "Are UFOs Driving Innovation? The Illusion of Causality in Large Language Models", "authors": ["Mar\u00eda Victoria Carro", "Francisca Gauna Selasco", "Denise Alejandra Mester", "Mario Alejandro Leiva"], "abstract": "Illusions of causality occur when people develop the belief that there is a causal connection between two variables with no supporting evidence. This cognitive bias has been proposed to underlie many societal problems including social prejudice, stereotype formation, misinformation and superstitious thinking. In this research we investigate whether large language models develop the illusion of causality in real-world settings. We evaluated and compared news headlines generated by GPT-40-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro to determine whether the models incorrectly framed correlations as causal relationships. In order to also measure sycophantic behavior, which occurs when a model aligns with a user's beliefs in order to look favorable even if it is not objectively correct, we additionally incorporated the bias into the prompts, observing if this manipulation increases the likelihood of the models exhibiting the illusion of causality. We found that Claude-3.5-Sonnet is the model that presents the lowest degree of causal illusion aligned with experiments on Correlation-to-Causation Exaggeration in human- written press releases. On the other hand, our findings suggest that while mimicry sycophancy increases the likelihood of causal illusions in these models, especially in GPT-40-Mini, Claude-3.5-Sonnet remains the most robust against this cognitive bias.", "sections": [{"title": "1 Introduction", "content": "The human brain is the most advanced tool ever devised for managing causes and effects [Pearl and McKenzie, 2018] [Gopnik and Goddu, 2024]. Experiments have shown that, when trying to assess causality intuitively, people can be relatively accurate [Matute et al., 2015]. At the same time, however, they are also prone to systematic errors, leading to the illusion of causality and the misinterpretation of spurious correlations.\nIllusions of causality occur when people develop the belief that there is a causal connection between two variables with no supporting evidence [Matute et al., 2015] [Blanco et al., 2018] [Chow et al., 2024]. Examples of this are common in everyday life. For instance, many avoid walking under a ladder, fearing it will bring bad luck. This cognitive bias is so strong that people infer them even when they are fully aware that no plausible causal mechanism exists to justify the connection [Matute et al., 2015].\nIllusions of causality arises because the human mind is naturally inclined to infer causal relationships from coincidences and to believe that earlier events cause those that follow [Chabris and Simons, 2010]. This causal imagination played a crucial role in the evolutionary development of our species [Pearl and McKenzie, 2018]. However, despite its usefulness in many contexts, causal illusions and related biases underlie many societal problems including social prejudice, stereotype formation [Hamilton and Gifford, 1976] [Kutzner et al., 2011], pseudoscience, superstitious thinking [Matute et al., 2015] and misinformartion [Xiong et al., 2020]. These phenomena can lead to serious consequences in critical areas like health, finance, and well-being, and have even contributed to wrongful convictions [Pundik, 2021].\nA rich literature in cognitive science has studied people's illusions of causality. One of the areas where it has the most harmful impact is in press releases, where media often report correlational research findings as if they were causal. This tendency arises partly because research institutions, competing for funding and talent, face pressure to align their findings with marketing goals [Yu et al., 2020]. As a consequence, this distortion not only misinform the public but also undermine public trust in science [Thapa et al., 2020] [Yu et al., 2020].\nIn this research, we investigate whether large language models (LLMs) exhibit the illusion of causality in real-world settings. Specifically, we aim to assess the tendency to exaggerate correlation as causation in press releases by prompting the models to generate news headlines. Since headlines serve the purpose of attracting readers, they are more prone to exaggeration and can be more negatively impactful than those illusions of causality in content [Yu et al., 2020].\nTo do this, we curated a dataset of 100 observational research paper abstracts, each highlighting spurious correlations between two variables. We then tested three models\u2014GPT-40-Mini, Claude-3.5- Sonnet, and Gemini-1.5-Pro-by placing them in the role of journalists. We provided these models with the abstracts and asked them to generate headlines for news articles based on the identified findings. Figure 1 shows an example on the left.\nSecondly, we subtly altered the instructions to evaluate whether sycophancy in LLMs exacerbates or sustains the illusion of causality. Sycophancy is defined as the undesirable tendency of LLMs to align with a user's beliefs or opinions to appear favorable, even when those beliefs are incorrect [Wei et al., 2024] [Sharma et al., 2023] [RRV et al., 2024]. In essence, since the illusion of causality is a human cognitive bias, we also aimed to observe whether a model's tendency to reflect it in the output becomes stronger when the bias is explicitly mentioned in the prompt, or if the model disregards the erroneous belief anyway.\nOur results show that Claude-3.5-Sonnet exhibits the least tendency to display causal illusions, consistent with previous studies on correlation-to-causation exaggeration in human-authored press"}, {"title": "2 Related Work", "content": "Various studies have conducted evaluations on cognitive biases in LLMs. Hagendorff1 et al. [2023] administered a battery of semantic illusions and cognitive reflection tests, traditionally used to elicit intuitive yet erroneous responses in humans, to OpenAI's model family. Their results highlighted the importance of applying psychological methodologies to study LLMs, showing that, as the models expand in size and linguistic proficiency, they increasingly display human-like intuitive thinking and associated cognitive errors. Echterhoff et al. [2024] introduced a framework designed to reveal, evaluate, and mitigate a variety of cognitive bias in LLMs in high-stakes decision-making tasks. While their findings aligned with previous studies demonstrating the presence of cognitive biases, they were able to effectively mitigate them, resulting in more consistent decisions. Ultimately, Wang et al. [2024] proved that certain cognitive biases, when properly balanced, can improve decision-making efficiency in LLMs, aligning their judgements more closely with human reasoning, and challenging the traditional goal of eliminating all biases. Ultimately, Keshmirian et al. [2024] identified a cognitive bias in LLMs concerning causal structures, mirroring a similar bias they previously observed in human subjects. Specifically, both LLMs and humans tend to attribute greater causal strength to the intermediate cause in canonical Chains."}, {"title": "2.1 Understanding and Evaluating LLMs\u00b4 Cognitive Biases", "content": "2.2 Evaluating LLMs\u00b4 Causal Capabilities\nA significant amount of research has evaluated LLMs on tasks requiring causal knowledge, compre- hension, or reasoning. K\u0131c\u0131man et al. [2023] conducted an in-depth evaluation of LLMs in two key areas: causal discovery and actual causality. Their work on the former encompassed both pairwise causal identification and full-graph discovery. In the domain of actual causality, the authors explored counterfactual reasoning, the identification of sufficient and necessary causes, and the inference of normality. Gao et al. [2023] centered the assessment in three causal domains: event causality identification (ECI), causal discovery (CD) and causal explanation generation (CEG). Jin et al. [2023] proposed a new task inspired by the \"causal inference engine\" postulated by Judea Pearl et al. to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. Kasetty et al. [2024] evaluated whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. Finally, Niel et al. [2023] investigated whether LLMs make causal and moral judgments about text-based scenarios that align with those of human participants in cognitive science experiments. Their study examined how factors such as agent awareness, norm violation, and event normality influence these judgments."}, {"title": "3 Methodology", "content": "4 Experiments and Results\nFor the first task, our results demonstrate that Claude-3.5-Sonnet consistently exhibits the lowest level of causal illusion among the models tested. In contrast, Gemini-1.5-Pro and GPT-40-Mini display comparable degrees of this phenomenon, (34% and 35%, respectively) as illustrated in Figure 2. Notably, Claude-3.5-Sonnet's performance aligns closely with findings from experiments on"}, {"title": "3.1 Dataset Construction", "content": "We curated a dataset consisting of 100 observational research paper abstracts, each identifying spurious correlations between two variables. The spurious correlations were selected randomly from a publicly available resource, Spurious Correlations, accessible at https://tylervigen. com/spurious-correlations. This website provides a collection of correlations that appear statistically significant but lack any plausible causal relationship."}, {"title": "3.2 Tasks Configuration", "content": "For the first task, we crafted a prompt that directs the LLM to adopt the perspective of a journalist. Given a set of selected abstracts, the model is tasked with generating a headline for a news outlet, summarizing the key findings presented in the abstract. An example is illustrated in the left side of Figure 1.\nIn a second stage of the evaluation, we subtly modified the instructions to assess whether mimicry syco- phancy in LLMs amplifies or perpetuates the illusion of causality. In this scenario, the user-acting as the journalist\u2014mistakenly believes that the abstract presents a causal relationship. This miscon- ception was explicitly embedded in the prompt to measure whether the models are more likely to reinforce the illusion of causality without correcting the user. An example is illustrated in the right side of Figure 1."}, {"title": "3.3 Evaluation Criteria", "content": "Three of us conducted a manual content analysis to identify causal claims in text-generation. We annotated the following four claim types: correlational, conditional causal, direct causal, and not claim [Yu et al., 2020]. Table 1 lists the category definitions and some common language cues used to identify the relation type for each category. Example sentences of different claim types are also shown in the table."}, {"title": "5 Limitations and Future Work", "content": "This study represents a preliminary exploration into whether LLMs exhibit causal illusions similar to those observed in human cognition and investigates the potential influence of sycophantic tendencies in this process. However, there are certain limitations that should be acknowledged.\nFirstly, the research questions addressed in this study would greatly benefit from further evaluation, particularly across a wider range of tasks. Our analysis centered on news headline generation, but LLMs may demonstrate different patterns of behavior in other contexts. To gain a more holistic understanding of how causal illusions emerge, future research should investigate their manifestation across diverse content types and tasks, providing deeper insights into the specific conditions under\nwhich this bias emerges. Additionally, our dataset is limited in scope and expanding it to include a broader range of spurious correlations would enhance the robustness of our findings.\nSecondly, our study was limited to specific models (GPT-40-Mini, Claude-3.5-Sonnet, and Gemini- 1.5-Pro) which limit the generalizability of our results to other LLMs."}, {"title": "6 Conclusion", "content": "Using a dataset of spurious correlations, we investigated whether LLMs can develop the illusion of causality in the generation of press release headlines. Additionally, we introduced the erroneous belief of a causal relationship in the prompt to evaluate if the models would be more likely to mimic this user bias. We found that Claude-3.5-Sonnet exhibits the least tendency to display causal illusions while Gemini-1.5-Pro and GPT-40-Mini show similar levels of this phenomenon. On the other hand, the imitation of erroneous beliefs increases the risk of causal misinterpretations in the models, especially in GPT-40-Mini.\nIn contrast to prior research that investigates causal knowledge, comprehension and reasoning in LLMs as a valuable capability, our work is pioneering in evaluating these models within a purely correlational context where causality is undesirable. The illusion of causality as a cognitive bias contributes to social prejudice, stereotype formation, misinformation, and pseudoscience, potentially leading to serious health consequences. This study highlights another critical intersection between causality and the development of safer, more reliable AI systems, emphasizing the need for further exploration."}]}