{"title": "M\u00b3Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes", "authors": ["Zeyu Zhang", "Sixu Yan", "Muzhi Han", "Zaijin Wang", "Xinggang Wang", "Song-Chun Zhu", "Hangxin Liu"], "abstract": "We propose M\u00b3 Bench, a new benchmark for whole-body motion generation in mobile manipulation tasks. Given a 3D scene context, M\u00b3 Bench requires an embodied agent to understand its configuration, environmental constraints and task objectives, then generate coordinated whole-body motion trajectories for object rearrangement tasks. M\u00b3Bench features 30k object rearrangement tasks across 119 diverse scenes, providing expert demonstrations generated by our newly de-veloped M\u00b3BenchMaker. This automatic data generation tool produces coordinated whole-body motion trajectories from high-level task instructions, requiring only basic scene and robot information. Our benchmark incorporates various task splits to assess generalization across different dimensions and leverages realistic physics simulation for trajectory evaluation. Through extensive experimental analyses, we reveal that state-of-the-art models still struggle with coordinated base-arm motion while adhering to environment-context and task-specific constraints, highlighting the need to develop new models that address this gap. Through M\u00b3 Bench, we aim to facilitate future robotics research towards more adaptive and capable mobile manipulation in diverse, real-world environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans possess an innate ability to manipulate their environment with remarkable flexibility and coordination, seamlessly integrating locomotion and manipulation. In contrast, robots still struggle to match this level of adaptability and proficiency in mobile manipulation tasks. Traditional trajectory optimization methods [15,17], while capable of producing complex whole-body motion, are limited by their reliance on perfect environmental knowledge and predefined goal configurations (e.g., grasp poses), restricting their effectiveness and generalizability in real-world scenarios.\nTo address these limitations, researchers have turned to learning-based approaches. However, existing learning-based models deployed on mobile manipulators often address indi-vidual subproblems such as navigation, grasping, or motion planning in a modular fashion [9,36]. These approaches can lead to suboptimal or infeasible solutions as they overlook the potential of coordinated whole-body motion and create misalignments between module outputs and requirements."}, {"title": "II. RELATED WORK", "content": "Recent benchmarks for Embodied AI have focused on training agents for household activities [6, 20], often simpli-fying actions to symbolic operations [18] or navigation [1], which limits real-world applicability. While more recent benchmarks allow fixed-base manipulators to interact with objects via realistic simulation [10, 16, 27] or mobile agents to navigate and manipulate in diverse scenes [21,33,36], they lack expert data for whole-body motion in 3D scenes. Our M\u00b3Bench addresses this gap with a unique collection of whole-body motions for mobile manipulators in various 3D scenes, enabling models to comprehend environmental constraints and generate coordinated motions in complex environments. In addition, unlike existing motion benchmark tools that focus on stationary manipulation [3] in simplified en-vironments, our M\u00b3BenchMaker could procedually generate whole-body motion trajectories for mobile manipulators in diverse 3D scenes, facilitating practical model development for real-world tasks.\nThe proposed M\u00b3Bench and tool offer benefits for related fields in embodied AI. In visual affordance, recent studies focus on grasping poses [22, 23, 32] and functional parts [26, 39, 40, 42], but such object-centric modeling often lead to infeasible trajectories in complex scenes. Our whole-body motion data in 3D scenes enables modeling context-aware embodied visual affordance. Similarly, in embodied instruc-tion following, existing benchmarks focus on navigation [1] or symbolic actions [13,30]. Our M\u00b3BenchMaker allows researchers to generate whole-body motion data, facilitating models to produce low-level motion directly from language instructions."}, {"title": "III. THE M\u00b3BENCHMAKER", "content": "Diverse whole-body motion trajectories for mobile manip-ulators in complex 3D environments is crucial for advancing embodied AI. However, collecting expert demonstrations for training models are usually time-consuming and challenging. To address this, we introduce M\u00b3BenchMaker, a user-friendly tool that streamlines the generation of whole-body motion trajectories in 3D scenes, significantly reducing the time and effort required to create large-scale datasets for mobile manipulation tasks in various environments.\nA. Task Builder\nThe task builder serves as the primary user interface, allowing users to define manipulation tasks using high-level action commands such as picking, placing, and reaching. Users no longer need to manually specify grasping poses, placement locations, base positions, or create optimization programs for motion trajectories. To define a task, users simply select target object links from the scene URDF, set the robot's initial position, and specify the desired action types. The task builder then creates an instance of the data generation pipeline, integrating subsequent modules to procedually generate whole-body motion trajectories. For enhanced dataset diversity, the task builder supports data aug-mentation via the Conditional Scene Sampler (see Sec. III-B). This feature facilitates the training and evaluation of embodied AI models in complex environments by generating varied scenarios from a single task definition.\nB. Conditional Scene Sampler\nThe conditional scene sampler generates diverse initial configurations for data augmentation by randomizing object and robot positions and orientations. It produces variations dependent on the original scene's object relations, ensuring physical feasibility and contextual consistency required by the task. For instance, in a task involving picking an ob-ject from a table, the sampler ensures the sampled objects remains on table (see Fig. 3 orange box). This is achieved by recognizing supporting planes for objects and the robot through analysis of surrounding geometries.\nTo identify supporting planes, we parameterize a surface plane as \u03c0 = <n, d, U\u3009, where n\u2208 R\u00b3 is the normal vector, d is the distance to origin, and U = {u|u\u2208R\u00b3} defines the plane's polygon outline. The most likely supporting plane \u03c0\u03b5 for a bottom surface \u03c0\u03bf is identified by solving:\n$argmax_{\\pi \\in \\Pi} A (U_\\pi \\cap projo,s (U_o)) /A(U_o)$,\ns.t.\n$\\sum n^T u + ds <0 \\forall \\ us \\in U_o, \\\\\nabss (n^T n_o) \\ge \\theta_a$.\nwhere is a set of supporting plane candidates, A(\u00b7) denotes polygon area, computes intersection, and projos (U\uff61) = {u-(nu+ds) n\u300f|u\u2208U\uff61} projects bottom surface points onto the supporting plane, \u03b8\u03b1 and \u03b8\u03b1 are distance and angle thresholds. defines the contact ratio, while Eqs. (2) and (3) enforce alignment and distance constraints. We utilize the method in [12] to extract surface planes and solve the optimization problem by iteratively identifying the plane that maximizes Eq. (1) while satisfying the constraints.\nC. Goal Configuration Generator\nThis module efficiently generates 6D end-effector poses for grasping or placing target objects, serving as optimization objectives for motion planning. We employ an energy-based model to predict candidate goal configurations based on target object geometry [34]. However, this object-centric approach, which considers only object geometry without accounting for the robot's kinematic constraints or environ-mental contexts, results in only a small subset of candidates being feasible for the task. To address the computational expense of evaluating all candidates through motion plan-ning, we developed an adaptive sampling algorithm that efficiently draws samples from the candidate set, significantly accelerating the motion generation process.\nDetailed in Alg. 1, our algorithm iteratively selects and updates the sampling probability of candidates based on their feasibility scores. It utilizes a K-D tree for efficient neighbor searching and initializes feasibility scores using the candidates' energy values. By concentrating sampling in promising regions of the goal configuration space while maintaining exploration, the algorithm significantly reduces the number of expensive feasibility checks required to iden-tify viable goal configurations.\nD. VKC Problem Generator\nThe VKC problem generator automates the construction of motion planning programs, formulating comprehensive optimization problems that encapsulate all necessary con-straints and objectives for computing whole-body motion"}, {"title": "IV. THE M\u00b3BENCH", "content": "The M\u00b3Bench aims to advance robot capabilities in co-ordinating whole-body movements within complex environ-ments, inspired by human ability to seamlessly perform such tasks. It challenges mobile manipulators to generate coordinated whole-body motion trajectories for picking or placing everyday objects in 3D scenes, requiring agents to jointly understand their embodiment, environmental contexts, and task objectives from 3D scans.\nA. Simulation Environment\nSimulation Platform. Our benchmark, built on Isaac Sim [24], provides a high-fidelity physics simulation that meticulously models real-world properties and interactions. This platform enables precise evaluation of motion trajec-tory feasibility, grasping abilities, and the complex interplay between mobility and manipulation. Additionally, it could generate rich perceptual data (e.g., RGB-D image) that closely mimics the sensory input available to real-world robots.\nScene and Robot Configuration. The benchmark com-prises 119 diverse household scenes containing 32 types of objects, curated from PhyScene [37]. These interactive 3D scenes are enhanced with physical properties and rich materials for photo-realistic and physics-realistic simulation. For the robot, we employ a common mobile manipulator configuration: a 7-DoF Kinova Gen3 robotic arm with a parallel gripper, mounted on an omnidirectional mobile base. This setup facilitates complex manipulations requiring coor-dinated base and arm movements.\nB. Task Design and Variations\nM\u00b3Bench focuses on two primary object rearrangement tasks: picking and placing. Given a 3D point cloud of the"}, {"title": "D. Benchmark", "content": "Data Split and Statistics. The tasks in M\u00b3Bench are care-fully divided into several splits to assess different aspects of generalization capabilities. Objects and scenes are randomly categorized into seen and unseen subsets. The primary eval-uation set, the Base split, encompasses all seen objects and scenes, divided into Train (75%), Val (5%), and Test (20%) sets. Three additional splits challenge model generalization: Novel Object (unseen objects in seen scenes), Novel Scene (seen objects in unseen scenes), and Novel Scenario (unseen objects in unseen scenes).\nMetrics. We employ a multi-faceted approach to evaluate motion generation models. Task success rate serves as the primary metric, determined by the robot's ability to complete specified tasks and maintain the desired state for 2 seconds, as verified by the Isaac Sim physics engine. We also measure the closest distance from the end-effector to the target as an auxiliary metric, reflecting the trajectory's effectiveness in reaching the object or placement location. To assess trajectory quality, we utilize several quantitative measures: environment collision, self-collision, joint limit violation, and trajectory solving time. This comprehensive set of metrics evaluates models' capabilities in generating effective and efficient motion trajectories for mobile manipulation in 3D scenes."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setup\nModels for M\u00b3 Bench. Due to the lack of existing models for whole-body motion generation in mobile manipulation within 3D scenes, we adapt three state-of-the-art approaches to our benchmark:\n\u2022 ModularMP: Integrates a VKC motion planner [17] with grasp pose predictor [34] and heuristic placement.\n\u2022 \u039c\u03c0Net [5]: Extended from stationary to mobile manipu-lation by incorporating whole-body joint generation and Signed Distance Function (SDF) [35] for collision loss computation in complex 3D scans.\n\u2022 \u039c\u03c0Former: A skill transformer [14] variant using Point-Net++ [28] for 3D scan processing and decision trans-former architecture [4] for enhanced sequence model-ing.\nImplementation Details. For M\u03c0Net and \u039c\u03c0Former, we generate 3D scans from scene URDF. To enhance learning tractability, we apply a perception bounding box around the robot and target object to crop the scans, focusing the model's attention on relevant spatial information. We train M\u03c0Net and \u039c\u03c0Former on the Train split and perform model selection on Val. In constrast, as ModularMP does not involve learning procedure, we evaluate it directly on the Test and Novel splits. To simplify the optimization problem in Mod-ularMP, we ignore collisions between the end-effector and target object during motion planning, as considering these collisions would frequently result in infeasible trajectories.\nB. Experimental Results\nThe experimental results are summarized in Tab. IV. Tra-jectories are evaluated in Isaac Sim using metrics described in Sec. IV-D. Particularly, for the ModularMP model, when motion planning fails to solve the problem (i.e., optimization does not converge), we consider it as a failure instance.\nAcross Models. Comparing the baseline models, we observed that ModularMP outperforms other baselines in both pick and place tasks, demonstrating higher success rates and lower distances to goal. This aligns with our hy-pothesis that integrating conventional motion planning with affordance prediction would better generalize across diverse 3D scenes. However, ModularMP's superior performance comes at the cost of significantly increased computation time, primarily due to optimization complexity in large-scale 3D environments. Additionally, its effectiveness is constrained by the quality of predicted grasp and placement poses, as inappropriate predictions may lead to optimization failures or environmental collisions (see Fig. 2a). Despite ModularMP's relative success, the overall low success rates indicate that combining conventional motion planning with affordance prediction is insufficient.\nOn the other hand, learning-based models M\u03c0Net and \u039c\u03c0Former, while more time-efficient, struggle to produce feasible whole-body trajectories in complex scenes. Their performance is particularly poor, with near-zero success rates even in the Base split containing familiar objects and scenes. The generated trajectories often violate joint limitations and cause collisions, indicating a fundamental inability to adapt to complex 3D environments and produce feasible whole-body motions. These findings highlight the persistent chal-lenge of generating whole-body motion trajectories for robots in complex 3D scenes. Further research is needed to enhance robot capabilities in manipulating complex environments."}, {"title": "VI. CONCLUSION", "content": "We introduced M\u00b3Bench, a comprehensive benchmark for whole-body motion generation in mobile manipulation tasks across diverse 3D environments, featuring 30k object rearrangement tasks in 566 household scenes. We developed M\u00b3BenchMakerto efficiently generate whole-body motion trajectories from high-level instructions. Our experiments revealed significant limitations in current approaches, with even the best-performing hybrid method achieving low suc-cess rates and learning-based methods struggling dramati-cally. While M\u00b3Benchprovides a solid foundation for future research, several challenges remain: bridging the gap to real-world scenarios, extending to long-horizon tasks, expanding language instruction diversity, developing more sophisticated models for 3D context understanding and coordinated motion generation, facilitating research into versatile agents, and scaling up demonstrations. Addressing these challenges will be crucial for advancing the field towards more capable and adaptive robotic systems in diverse, complex, unstructured environments."}]}