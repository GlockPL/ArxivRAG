[{"title": "TrIM: Triangular Input Movement Systolic Array for Convolutional Neural Networks-Part I: Dataflow and Analytical Modelling", "authors": ["Cristian Sestito", "Shady Agwa", "Themis Prodromakis"], "abstract": "In order to follow the ever-growing computational complexity and data intensity of state-of-the-art AI models, new computing paradigms are being proposed. These paradigms aim at achieving high energy efficiency, by mitigating the Von Neumann bottleneck that relates to the energy cost of moving data between the processing cores and the memory. Convolutional Neural Networks (CNNs) are particularly susceptible to this bottleneck, given the massive data they have to manage. Systolic Arrays (SAs) are promising architectures to mitigate the data transmission cost, thanks to high data utilization carried out by an array of Processing Elements (PEs). These PEs continuously exchange and process data locally based on specific dataflows (like weight stationary and row stationary), in turn reducing the number of memory accesses to the main memory. The hardware specialization of SAs can meet different workloads, ranging from matrix multiplications to multi-dimensional convolutions. In this paper, we propose TrIM: a novel dataflow for SAs based on a Triangular Input Movement and compatible with CNN comput-ing. When compared to state-of-the-art SA dataflows, like weight stationary and row stationary, the high data utilization offered by TrIM guarantees ~ 10\u00d7 less memory access. Furthermore, considering that PEs continuously overlap multiplications and accumulations, TrIM achieves high throughput (up to 81.8% higher than row stationary), other than requiring a limited number of registers (up to 15.6\u00d7 fewer registers than row stationary).", "sections": [{"title": "I. INTRODUCTION", "content": "NOWADAYS, Artificial Intelligence (AI) is a pervasive paradigm that has changed the way devices can assist everyday activities. However, in order to continuously meet high standards of accuracy, AI models are becoming ever more data-intensive, particularly when Deep Neural Networks (DNNs) are considered. Indeed, other than demanding a huge amount of computations, DNNs also require high memory capacity to manage learned weights, as well as inputs and outputs [1].\nConvolutional Neural Network (CNN) [2] is an example of data-intensive DNN, since it executes convolutions on multi-dimensional arrays, named feature maps, to carry out tasks like image classification [3], image segmentation [4], [5], image generation [6], [7], object detection [8], [9] and speech recognition [10], [11]. Conventionally, Central Pro-cessing Units (CPUs) and Graphics Processing Units (GPUs) manage CNNs' workloads. However, these architectures suffer from the Von Neumann bottleneck [12], owing to the physical separation between the computing core and the memory. This significantly degrades the energy efficiency, given that data should be first fetched from an external Dynamic Random Access Memory (DRAM), then buffered on-chip, and finally processed by the computing core. For example, the normalized DRAM energy cost from a commercial 65nm process is 200\u00d7 higher than the cost associated to a Multiply-Accumulation (MAC) [13], which is the elementary operation performed by the computing core. High data utilization [14] is a way to mitigate such cost, by allowing inputs, weights, or partial sums (psums) to be held on-chip as long as they need to be consumed.\nSystolic Arrays (SAs) are representative architectures that maximize data utilization by using an array of Processing Elements (PEs) interconnected with each other [15]. Data moves rhythmically, thus evoking the blood flow into the cardiovascular system, hence the term systolic. Conceived late 1970s [16], they have been mainly used for matrix multiplications. In recent years, the research community has put effort to allow SAs to meet the CNN's workflow [17]\u2013[19]. For instance, the Convolution to General Matrix Mul-tiplication conversion (Conv-to-GeMM) [20] introduces data redundancy to make inputs compliant with the SAs' dataflows. However, this reflects in higher memory requirements and, in turn, a higher number of memory accesses, thus negatively impacting area and energy. Weight Stationary (WS) based SAs are examples of architectures using Conv-to-GeMM. Inputs are moved and reused in one direction along the array, while weights are kept stationary. However, First-In-First-Out (FIFO) buffers must assist the data transfer from/to the memory, thus negatively affecting area, power and energy. The Google Tensor Processing Unit (TPU) is a WS-based SA consisting of 256 \u00d7 256 PEs, which outperforms CPUs and GPUs by 30\u00d7 in terms of energy efficiency [21]. In the Row Stationary (RS) dataflow [13], rows of inputs and weights are reused at the PE level through dedicated memory blocks, without requiring Conv-to-GeMM conversion. In this case, data redundancy is moved at the array level, where inputs are shared diagonally over multiple PEs, while weights are shared horizontally. In addition, inputs and weights circulate cycle-by-cycle inside each PE, thus reducing the energy efficiency, other than making the micro-architecture of PEs more complex. Finally, the area covered by the SA depends on the inputs' sizes, thus making the deployment of large-scale architectures a challenge. Eyeriss [22] is the pioneer RS-based SA and consists of 168 PEs, outperforming existing SAs from 1.4\u00d7 to 2.5\u00d7 in terms of energy efficiency.\nTo address the drawbacks of previous art, we propose TrIM, a new dataflow that exploits a triangular input movement to maximize input utilization. Advantageously, this results in less memory accesses without the need to introduce data redundancy. A generic SA dealing with TrIM consists of K \u00d7 K PEs, where weights are kept stationary and psums are propagated vertically and finally accumulated by an extra adder tree. The TrIM-based SA is compatible with the com-putation requirements of Convolutional Layers (CLs). When compared to WS, TrIM exhibits one order of magnitude less memory accesses. Moreover, TrIM ensures high performance efficiency since each PE works at the peak throughput of 2 OPs/cycle. Finally, the micro-architecture of PEs is fairly simple, translating into a limited number of registers, up to 15.6x lower than RS with a kernel size of 7.\nThe contributions of this work are summarized as follows:\n\u2022\tWe introduce the TrIM dataflow, which allows high data utilization through a triangular movement of inputs, thus significantly reducing the number of memory accesses from the main memory if compared to previous art.\n\u2022\tWe present an analytical model for TrIM and state-of-the-art dataflows (i.e., WS and RS), including metrics like memory accesses, latency, throughput, number of registers.\n\u2022\tGiven the analytical model, a thorough design space evaluation is performed, based on several kernel sizes and feature map sizes. WS and RS dataflows are considered for comparisons.\n\u2022\tAccording to the design space evaluation, we discuss the key benefits offered by TrIM.\nThe remainder of this paper is structured as follows: Section II, after introducing CNNs, provides a background about SAs and specific dataflows to manage convolutions; Section III introduces the proposed TrIM dataflow; Section IV presents an analytical model to characterize TrIM, RS and WS; using the referred model, Section V presents an in-depth design space evaluation to spotlight the advantages of TrIM over its counterparts; finally, section VI concludes the paper. In order to ease the readability of this manuscript"}, {"title": "II. BACKGROUND", "content": "A. Convolutional Neural Networks\nA CNN consists of a sequence of CLs that extract features of interest from input data [23], by emulating the behavior of human visual cortex to retrieve patterns, such as shapes and edges, from images. The feature maps (fmaps) generated from the last CL are eventually processed by Fully-Connected Layers (FCLs) [24] for classification. As shown in Fig. 1(a). Each CL performs N three-dimensional convolutions between M input fmaps (ifmaps), each consisting of a $H_1 \\times W_1$ plane, and N filters, each consisting of M kernels of $K_H \\times K_W$ weights. In other words, each filter scans the M ifmaps through sliding windows with stride S. In the rest of the paper, we consider $K_H = K_W = K$ and $S = 1$ as in common CNNs for classification. As a result, N output fmaps (ofmaps), each $H_o \\times W_o$ wide, are generated, with $H_o = H_1 \u2212 K + 1$ and $W_o = W_1 - K + 1$. Each output element is named activation and follows equation (1):\n$\\begin{equation}\nO_{n,h_o,w_o} = \\sum_{m=0}^{M-1} \\sum_{k_h=0}^{K-1} \\sum_{k_w=0}^{K-1} I_{m,h_o+k_h,w_o+k_w} \\times W_{n,m, k_h,k_w}\n\\end{equation}$\nO identifies the generic output activation, I is the generic input activation and W is the generic weight belonging to a kernel of one filter; n iterates over the N ofmaps, $h_o$ iterates over the $H_o$ rows of each ofmaps, $w_o$ iterates over the $W_o$ elements belonging to each ofmap's row, m iterates over the M ifmaps, $k_h$ and $k_w$ iterate over the kernels' rows and columns, respectively. N biases can be eventually added to each output activation if required by the layer. Fig. 1(b) shows how a convolution between a 5 \u00d7 5 ifmap and a 3 \u00d7 3 kernel works.\nFinally, CLs may be followed by non-linear functions [25] and pooling layers [26], which convert ofmaps in a different numerical domain and downsample their planar sizes, respec-tively.\nB. Systolic Arrays\nA SA is spatial architecture consisting of a 2-D array of PEs interconnected with each other in proper directions. Each PE usually performs a MAC operation using inputs supplied by either the main memory or neighbor PEs. With specific reference to DNNs, PEs are fed by input activations (inputs), weights and psums. High data utilization is met at two hierarchical levels: (a) at the PE level, one element among the input, weight and psum is retained in a register as long as it is required; (b) at the SA level, the other elements move rhythmically between adjacent PEs. The reused data at the PE level equips the SA with a specific stationary dataflow:\n\u2022\tWeight Stationary (WS): weights are retained inside the PEs and do not move. Inputs and psum moves between adjacent PEs throughout the process.\n\u2022\tInput Stationary (IS): a batch of inputs is retained inside the PEs, while weights and psums move between adjacent PEs throughout the process.\n\u2022\tOutput Stationary (OS): psums are reused at the PE level until the final sum is generated. Inputs and weights move between adjacent PEs throughout the process.\n\u2022\tRow Stationary (RS): rows of inputs and weights are stored and reused at the PE level, using memory blocks that enable data circulation. Psums move between adja-cent PEs throughout the process.\nSince CNNs exploit weight sharing to process each fmap, WS and RS are commonly used. In the following sub-sections, the two dataflows are presented in detail.\n1)\tWeight Stationary SAs: In a WS-based SAs, weights are preliminary fetched from the main memory and stored inside the PEs. Fig. 2(a) depicts an example of WS-based SA, where inputs are loaded from the left side and moved horizontally across the array. Differently, psums are accumulated following vertical interconnections. In order to correctly align data over time, First-In-First-Out buffers (FIFOs) for inputs and psums are placed at the left and bottom boundaries of the array, respectively. To make WS-based SAs compliant with CNNs, inputs and weights are subjected to Conv-to-GeMM [20]. The M ifmaps, consisting of $H_1 \\times W_1$ elements each, are reshaped as a matrix having $H_o \\times W_o$ rows and $M \\times K \\times K$ columns. Each row contains one of the three-dimensional sliding win-dows covered by the filters that scan the ifmaps. Filters are reshaped as N column arrays, each consisting of $M \\times K\\times K$ entries (i.e., the number of weights of each filter). As a result, a GeMM-oriented SA consists of $M \u00d7 K \u00d7 K$ rows of N PEs each. In addition, $M \u00d7 K \u00d7 K \u2212 1$ FIFOs must be adopted to ensure proper data-alignment over time. Fig 2(b) shows an example of Conv-to-GeMM between a 5 \u00d7 5 ifmap and a 3\u00d73 kernel. Despite the above PEs' deployment allows WS-based SAs to meet CNNs' workloads, some drawbacks are evident:\n(i)\tConv-to-GeMM introduces data redundancy since inputs are reshaped to guarantee the overlapping sliding windows of the original workflow. As a result, the main memory demands higher capacity, as well as higher number of memory accesses to feed the SA. These aspects negatively impact area and energy efficiency; (ii) furthermore, the larger the kernel size K and/or the number of kernels M and/or the number of filters N, the bigger the FIFOs. As a consequence, higher latency and switching activity impact the energy consumption, other than the area.\n2)\tRow Stationary SAs: In a RS-based SA, the PEs are arranged as an array of K rows and $H_o$ columns, where: (a) weights are fetched from the main memory and broadcast horizontally to all the PEs; (b) inputs are fetched and broadcast diagonally; (c) psum are accumulated vertically, by exploiting dedicated interconnections between PEs. High data utilization is handled at the PE level, despite requiring memory blocks to circulate inputs and weights. These data are typically loaded as rows of K elements, in order to allow the generic PE to perform 1-D convolutions. In other words, each PE is capable to perform K MACs, before forwarding the temporary psum vertically to another PE, which in the meantime has completed another 1-D convolution. The vertical connections finalize the 2-D convolution. Fig. 3(a) shows an example of RS-based SA using 3 x 3 PEs, where a 5 x 5 ifmap is subjected to a convolution with a 3 \u00d7 3 kernel. The generic PE consists of a MAC unit, two memory blocks for inputs and weights, as well as control logic. Fig. 3(b) illustrates the 2-D convolutions of a 5 x 5 ifmap, with details on the data shared among the PEs. Differently from Conv-to-GeMM, this dataflow moves data redundancy at the array level, where inputs are shared diagonally, while weights are shared horizontally. In addition, other drawbacks arise: (i) the hardware complexity of each PE is higher, considering that memory blocks (and proper control logic as well) are needed. Other than the area, this negatively impacts the energy dissipation due to the high switching activity of memory blocks to guarantee data circulation; (ii) despite inputs/weights broadcasting at the SA level enables parallel convolutions, thus the possibility to reduce the latency, this directly impacts the routability of the PEs, making the physical implementation phase challenging; (iii) finally, the area covered by the SA depends on ifmap's size, thus limiting scalability and PEs' utilization."}, {"title": "III. TRIANGULAR INPUT MOVEMENT SYSTOLIC ARRAY", "content": "The TrIM-based SA consists of K \u00d7 K PEs, where weights are retained at the PE level, while inputs and psums are moved across the array to maximize data reuse. Before any computation, weights are read from the memory and provided to the PEs placed at the top side of the array; then, these are moved from top to down to allow the other weights to be accommodated. Inputs supply the PEs through a novel data movement that can be summarized in three steps: (i) first, inputs are fetched from the main memory and delivered to the PEs; (ii) then, such inputs move from right to left, until they reach the left edge of the array; (iii) finally, they move diagonally towards the upper PEs. According to the width of the ifmap under processing, the leftmost PEs may be connected to Shift Register Buffers (SRBs) with depth $W_1-K-1$, which ensure the correct execution of the diagonal movement over time. Steps (i) to (iii) compose a right-triangular shape, hence the name Triangular Input Movement (TrIM). Fig. 4 schematizes a generic architecture coping with the proposed dataflow. Each PE is labelled as $PE_{i,j}$, with $0 \\leq i,j < K$, and consists of a MAC unit, four registers, and two multiplexers to establish whether the current input is reused from a different PE or not. If reused, inputs can move from right to left (thus from each $PE_{i,j+1}$ to $PE_{i,j}$), or they can be forwarded diagonally. In the latter case, they are first provided to the SRBs from each $PE_{1,0}$, then shifted along such buffers, and finally the inputs stored into the last K registers are supplied to the top $PE_{i-1,j}$ elements. Psums are accumulated vertically, thus from each $PE_{i,j}$ to each $PE_{i+1,j}$. Eventually, an adder tree accumulates the psums coming from the the bottom $PE_{K-1,j}$ elements. For very small ifmaps, with $W_1 \\leq 2K$, the number of registers may be lower than K, thus the diagonal connections may also interest some of the PEs.\nTo better understand how TrIM works, we consider the case reported in Fig. 5, where a 5 \u00d7 5 ifmaps, with $I = 1, 2, ..., 25$ being the inputs, and a 3 \u00d7 3 kernel, with $W = A, B, ..., I$ being the weights, are considered. The equivalent TrIM archi-tecture is made of 3 rows (named $Row_0$, $Row_1$, and $Row_2$), each having 3 PEs. The leftmost PEs belonging to $Row_1$ and $Row_2$ are connected to SRBs with depth 1. A final adder tree, consisting of two adders, manages the psums coming from the PEs of $Row_2$. In what follows, the detailed functionality of the dataflow is explained cycle-by-cycle. Preliminarily, K cycles are needed to load the K \u00d7 K weights, arranged in rows of K weights per cycle. After this phase, the actual computations and input movements can start:\n\u2022\tCycle 1: I = 1,2,3, supplied vertically to $Row_0$, are multiplied by the weights W = A,B,C.\n\u2022\tCycle 2: $Row_0$ multiplies I = 2,3,4 with W = A, B, C, where I = 2,3 are reused through right-to-left movements, while I = 4 is supplied externally. $Row_1$ multiplies the new inputs I = 6,7,8 with W = D, E, F, and accumulate these with the psums coming from $Row_0$.\n\u2022\tCycle 3: $Row_0$ multiplies I = 3,4,5 with W = A, B, C, where I = 3,4 are reused through right-to-left movements, while I = 5 is supplied externally. $Row_1$ multiplies the inputs I = 7,8,9 with W = D, E, F, and accumulate these with the psums coming from $Row_0$. While I = 7,8 are reused through right-to-left move-ments, I = 9 is supplied externally. $Row_2$ multiplies the new inputs I = 11, 12, 13 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. At this cycle, SRB0 receives I = 6 to be reused later.\n\u2022\tCycle 4: $Row_0$ multiplies I = 6,7,8 with W = A, B, C, where I = 6 is reused diagonally from SRB0, while I = 7,8 are reused diagonally from $PE_{1,0}$ and $PE_{1,1}$, respectively. $Row_1$ multiplies the inputs I = 8,9,10 with W = D, E, F, and accumulate these with the psums coming from $Row_0$. While I = 8,9 are reused through right-to-left movements, I = 10 is supplied externally. $Row_2$ multiplies the inputs I = 12, 13, 14 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. While I = 12, 13 are reused through right-to-left movements, I = 14 is supplied externally. At this cycle, SRB0 and SRB1 receive I = 7 and I = 11, respectively. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the very first output activation $O_{0,0}$.\n\u2022\tCycle 5: $Row_0$ multiplies I = 7,8,9 with W = A, B, C, where I = 7,8 are reused through right-to-left move-ments, while I = 9 is reused diagonally from $PE_{1,1}$. $Row_1$ multiplies the inputs I = 11,12,13 with W = D, E, F, and accumulate these with the psums coming from $Row_0$. I = 11 is reused diagonally from SRB0, while I = 12, 13 are reused diagonally from $PE_{1,0}$ and $PE_{1,1}$, respectively. $Row_2$ multiplies the inputs I = 13, 14, 15 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. While I = 13,14 are reused through right-to-left movements, I = 15 is supplied externally. At this cycle, SRB0 and SRB1 receive I = 8 and I = 12, respectively. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the second output activation $O_{0,1}$.\n\u2022\tCycle 6: $Row_0$ multiplies I = 8,9,10 with W = A, B, C, where I = 8,9 are reused through right-to-left movements, while I = 10 is supplied externally. $Row_1$ multiplies the inputs I = 12,13,14 with W = D, E, F, and accumulate these with the psums coming from $Row_0$. While I = 12, 13 are reused through right-to-left movements, I = 14 is reused diagonally from $PE_{2,1}$. $Row_2$ multiplies the new inputs I = 16,17,18 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. At this cycle, SRB0 and SRB1 receive I = 11 and I = 13, respectively. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the third output activation $O_{0,2}$.\n\u2022\tCycle 7: $Row_0$ multiplies I = 11,12,13 with W = A, B, C, where I = 11 is reused diagonally from SRB0, while I = 12,13 are reused diagonally from $PE_{1,0}$ and $PE_{1,1}$, respectively. $Row_1$ multiplies the inputs I = 13,14,15 with W = D, E, F, and accu-mulate these with the psums coming from $Row_0$. While I = 13,14 are reused through right-to-left movements, I = 15 is supplied externally. $Row_2$ multiplies the inputs I = 17, 18, 19 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. While I = 17,18 are reused through right-to-left movements, I = 19 is supplied externally. At this cycle, SRBO and SRB1 receive I = 12 and I = 16, respectively. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the fourth output activation $O_{1,0}$.\n\u2022\tCycle 8: $Row_0$ multiplies I = 12,13,14 with W = A, B, C, where I = 12,13 are reused through right-to-left movements, while I = 14 is reused diagonally from $PE_{1,1}$. $Row_1$ multiplies the inputs I = 16,17,18 with W = D, E, F, and accumulate these with the psums coming from $Row_0$. I = 16 is reused diagonally from SRBO, while I = 17,18 are reused diagonally from $PE_{1.0}$ and $PE_{1,1}$, respectively. $Row_2$ multiplies the inputs I = 18, 19, 20 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. While I = 18,19 are reused through right-to-left movements, I = 20 is supplied externally. At this cycle, SRB0 and SRB1 receive I = 13 and I = 17, respectively. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the fifth output activation $O_{1,1}$.\n\u2022\tCycle 9: $Row_0$ multiplies I = 13,14,15 with W = A, B, C, where I = 13,14 are reused through right-to-left movements, while I = 15 is supplied externally. $Row_1$ multiplies the inputs I = 17,18,19 with W = D, E, F, and accumulate these with the psums coming from $Row_0$. While I = 17,18 are reused through right-to-left movements, I = 19 is reused diagonally from $PE_{2,1}$. $Row_2$ multiplies the new inputs I = 21,22,23 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. At this cycle, SRB0 and SRB1 receive I = 16 and I = 18, respectively. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the sixth output activation $O_{1,2}$.\n\u2022\tCycle 10: $Row_0$ has completed the computations related to the current ifmap, so it starts to compute a sliding windows from a different ifmap. $Row_1$ multiplies the inputs I = 18,19,20 with W = D, E, F, and accu-mulate these with the psums coming from $Row_0$. While I = 18,19 are reused through right-to-left movements, I = 20 is supplied externally. $Row_2$ multiplies the inputs I = 22, 23, 24 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. While I = 22, 23 are reused through right-to-left movements, I = 24 is supplied externally. From this cycle, SRB0 and SRB1 do not need to receive data anymore. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the seventh output activation $O_{2,0}$.\n\u2022\tCycle 11: $Row_1$ and $Row_2$ have completed the computa-tions related to the current ifmap, so they work on a dif-ferent ifmap. $Row_2$ multiplies the inputs I = 23, 24, 25 with W = G, H, I, and accumulate these with the psums coming from $Row_1$. While I = 23, 24 are reused through right-to-left movements, I = 25 is supplied externally. Finally, the psums coming from $PE_{2,0}$, $PE_{2,1}$, and $PE_{2,2}$ are accumulated through the adder tree to get the eighth output activation $O_{2,1}$.\n\u2022\tCycle 12: All the rows have completed the computations related to the current ifmap, so they work on a different ifmap. The adder tree provides the final output activation $O_{2,2}$.\nThe example just presented allows to appreciate how TrIM maximizes inputs utilization. In fact, the number of total memory accesses from the main memory is 29, of which only 4 accesses refer to inputs read more than once. To better spotlight this, let consider the case I = 13. This input is read once from the main memory (cycle 3) and reused 8 times (from cycle 4 to cycle 9) through right-to-left and diagonal movements."}, {"title": "IV. AN ANALYTICAL MODEL FOR SYSTOLIC ARRAYS", "content": "In order to highlight the advantages provided by TrIM", "twofold": "i) providing insights about memory accesses", "2)": "n$\\begin{equation"}, "nMA_{WS} = K^2 \\times (H_o \\times W_o)\n\\end{equation}$\nThis conversion results in data redundancy and higher memory capacity, since ifmap's activations are arranged as $H_o \\times W_o$ rows and $K^2$ columns. On the contrary, since no Conv-to-GeMM is required by the RS-based SA, the MA of the main memory are directly related to the ifmap's sizes ($H_1 \\times W_1$). However, the RS-based SA needs memory blocks at the PE level, usually implemented as SRAM-based scratch pads, which severely affect the energy footprint. Based on the Eyeriss implementation [22"], "3)": "n$\\begin{equation"}, {"4)": "n$\\begin{equation"}, {"5)": "n$\\begin{equation"}, {"6)": "n$\\begin{equation"}, {"8)": "n$\\begin{equation"}, {"10)": "n$\\begin{equation"}, {"11)": "n$\\begin{equation"}, {"12)": "n$\\begin{equation"}, {"metric": "i) the initial read of the ifmap"}]