{"title": "TrIM: Triangular Input Movement Systolic Array for Convolutional Neural Networks-Part I: Dataflow and Analytical Modelling", "authors": ["Cristian Sestito", "Shady Agwa", "Themis Prodromakis"], "abstract": "In order to follow the ever-growing computational complexity and data intensity of state-of-the-art AI models, new computing paradigms are being proposed. These paradigms aim at achieving high energy efficiency, by mitigating the Von Neumann bottleneck that relates to the energy cost of moving data between the processing cores and the memory. Convolutional Neural Networks (CNNs) are particularly susceptible to this bottleneck, given the massive data they have to manage. Systolic Arrays (SAs) are promising architectures to mitigate the data transmission cost, thanks to high data utilization carried out by an array of Processing Elements (PEs). These PEs continuously exchange and process data locally based on specific dataflows (like weight stationary and row stationary), in turn reducing the number of memory accesses to the main memory. The hardware specialization of SAs can meet different workloads, ranging from matrix multiplications to multi-dimensional convolutions. In this paper, we propose TrIM: a novel dataflow for SAs based on a Triangular Input Movement and compatible with CNN comput- ing. When compared to state-of-the-art SA dataflows, like weight stationary and row stationary, the high data utilization offered by TrIM guarantees ~ 10\u00d7 less memory access. Furthermore, considering that PEs continuously overlap multiplications and accumulations, TrIM achieves high throughput (up to 81.8% higher than row stationary), other than requiring a limited number of registers (up to 15.6\u00d7 fewer registers than row stationary).", "sections": [{"title": "I. INTRODUCTION", "content": "NOWADAYS, Artificial Intelligence (AI) is a pervasive paradigm that has changed the way devices can assist everyday activities. However, in order to continuously meet high standards of accuracy, AI models are becoming ever more data-intensive, particularly when Deep Neural Networks (DNNs) are considered. Indeed, other than demanding a huge amount of computations, DNNs also require high memory capacity to manage learned weights, as well as inputs and outputs [1].\nConvolutional Neural Network (CNN) [2] is an example of data-intensive DNN, since it executes convolutions on multi-dimensional arrays, named feature maps, to carry out tasks like image classification [3], image segmentation [4], [5], image generation [6], [7], object detection [8], [9] and speech recognition [10], [11]. Conventionally, Central Processing Units (CPUs) and Graphics Processing Units (GPUs) manage CNNs' workloads. However, these architectures suffer from the Von Neumann bottleneck [12], owing to the physical separation between the computing core and the memory. This significantly degrades the energy efficiency, given that data should be first fetched from an external Dynamic Random Access Memory (DRAM), then buffered on-chip, and finally processed by the computing core. For example, the normalized DRAM energy cost from a commercial 65nm process is 200\u00d7 higher than the cost associated to a Multiply-Accumulation (MAC) [13], which is the elementary operation performed by the computing core. High data utilization [14] is a way to mitigate such cost, by allowing inputs, weights, or partial sums (psums) to be held on-chip as long as they need to be consumed.\nSystolic Arrays (SAs) are representative architectures that maximize data utilization by using an array of Processing Elements (PEs) interconnected with each other [15]. Data moves rhythmically, thus evoking the blood flow into the cardiovascular system, hence the term systolic. Conceived late 1970s [16], they have been mainly used for matrix multiplications. In recent years, the research community has put effort to allow SAs to meet the CNN's workflow [17]\u2013 [19]. For instance, the Convolution to General Matrix Mul- tiplication conversion (Conv-to-GeMM) [20] introduces data redundancy to make inputs compliant with the SAs' dataflows. However, this reflects in higher memory requirements and, in turn, a higher number of memory accesses, thus negatively impacting area and energy. Weight Stationary (WS) based SAs are examples of architectures using Conv-to-GeMM. Inputs are moved and reused in one direction along the array, while weights are kept stationary. However, First-In-First- Out (FIFO) buffers must assist the data transfer from/to the memory, thus negatively affecting area, power and energy. The Google Tensor Processing Unit (TPU) is a WS-based SA consisting of 256 \u00d7 256 PEs, which outperforms CPUs and GPUs by 30\u00d7 in terms of energy efficiency [21]. In the Row Stationary (RS) dataflow [13], rows of inputs and weights are reused at the PE level through dedicated memory blocks, without requiring Conv-to-GeMM conversion. In this case, data redundancy is moved at the array level, where inputs are shared diagonally over multiple PEs, while weights are shared horizontally. In addition, inputs and weights circulate cycle-by-"}, {"title": "II. BACKGROUND", "content": "A. Convolutional Neural Networks\nA CNN consists of a sequence of CLs that extract features of interest from input data [23], by emulating the behavior of human visual cortex to retrieve patterns, such as shapes and edges, from images. The feature maps (fmaps) generated from the last CL are eventually processed by Fully-Connected Layers (FCLs) [24] for classification. As shown in Fig. 1(a) Each CL performs N three-dimensional convolutions between over the $H_o$ rows of each ofmaps, $w_o$ iterates over the $W_o$ elements belonging to each ofmap's row, m iterates over the M ifmaps, $k_h$ and $k_w$ iterate over the kernels' rows and columns, respectively. N biases can be eventually added to each output activation if required by the layer. Fig. 1(b) shows how a convolution between a 5 \u00d7 5 ifmap and a 3 \u00d7 3 kernel works.\nFinally, CLs may be followed by non-linear functions [25] and pooling layers [26], which convert ofmaps in a different numerical domain and downsample their planar sizes, respec- tively.\nB. Systolic Arrays\nA SA is spatial architecture consisting of a 2-D array of PEs interconnected with each other in proper directions. Each PE usually performs a MAC operation using inputs supplied by either the main memory or neighbor PEs. With specific reference to DNNs, PEs are fed by input activations (inputs), weights and psums. High data utilization is met at two hierarchical levels: (a) at the PE level, one element among the input, weight and psum is retained in a register as long as it is required; (b) at the SA level, the other elements move rhythmically between adjacent PEs. The reused data at the PE level equips the SA with a specific stationary dataflow:\n* Weight Stationary (WS): weights are retained inside the PEs and do not move. Inputs and psum moves between adjacent PEs throughout the process.\n* Input Stationary (IS): a batch of inputs is retained inside the PEs, while weights and psums move between adjacent PEs throughout the process.\n* Output Stationary (OS): psums are reused at the PE level until the final sum is generated. Inputs and weights move between adjacent PEs throughout the process.\n* Row Stationary (RS): rows of inputs and weights are stored and reused at the PE level, using memory blocks that enable data circulation. Psums move between adja- cent PEs throughout the process.\nSince CNNs exploit weight sharing to process each fmap, WS and RS are commonly used. In the following sub-sections, the two dataflows are presented in detail.\n1) Weight Stationary SAs: In a WS-based SAs, weights are preliminary fetched from the main memory and stored inside the PEs. Fig. 2(a) depicts an example of WS-based SA, where inputs are loaded from the left side and moved horizontally across the array. Differently, psums are accumulated following vertical interconnections. In order to correctly align data over time, First-In-First-Out buffers (FIFOs) for inputs and psums are placed at the left and bottom boundaries of the array, respectively. To make WS-based SAs compliant with CNNs, inputs and weights are subjected to Conv-to-GeMM [20]. The M ifmaps, consisting of $H_1 \u00d7 W_1$ elements each, are reshaped as a matrix having $H_o \u00d7 W_o$ rows and $M \u00d7 K \u00d7 K$ columns."}, {"title": "III. TRIANGULAR INPUT MOVEMENT SYSTOLIC ARRAY", "content": "The TrIM-based SA consists of K \u00d7 K PEs, where weights are retained at the PE level, while inputs and psums are moved across the array to maximize data reuse. Before any computation, weights are read from the memory and provided to the PEs placed at the top side of the array; then, these are moved from top to down to allow the other weights to be accommodated. Inputs supply the PEs through a novel data movement that can be summarized in three steps: (i) first, inputs are fetched from the main memory and delivered to the PEs; (ii) then, such inputs move from right to left, until they reach the left edge of the array; (iii) finally, they move diagonally towards the upper PEs. According to the width of the ifmap under processing, the leftmost PEs may be connected to Shift Register Buffers (SRBs) with depth $W_1-K-1$, which ensure the correct execution of the diagonal movement over time. Steps (i) to (iii) compose a right- triangular shape, hence the name Triangular Input Movement (TrIM). Fig. 4 schematizes a generic architecture coping with the proposed dataflow. Each PE is labelled as $PE_{i,j}$, with $0 \u2264 i,j < K$, and consists of a MAC unit, four registers, and two multiplexers to establish whether the current input is reused from a different PE or not. If reused, inputs can move from right to left (thus from each $PE_{i,j+1}$ to $PE_{i,j}$), or they can be forwarded diagonally. In the latter case, they are first provided to the SRBs from each $PE_{1,0}$, then shifted along such buffers, and finally the inputs stored into the last K registers are supplied to the top $PE_{i-1,j}$ elements. Psums are accumulated vertically, thus from each $PE_{i,j}$ to each $PE_{i+1,j}$. Eventually, an adder tree accumulates the psums coming from the the bottom $PE_{K-1,j}$ elements. For very small ifmaps, with $W_1 \u2264 2K$, the number of registers may be lower than K, thus the diagonal connections may also interest some of the PEs.\nTo better understand how TrIM works, we consider the case reported in Fig. 5, where a 5 \u00d7 5 ifmaps, with $I = 1, 2, ..., 25$ being the inputs, and a 3 \u00d7 3 kernel, with $W = A, B, ..., I$ being the weights, are considered. The equivalent TrIM archi- tecture is made of 3 rows (named Rowo, Row\u2081, and Row\u2082), each having 3 PEs. The leftmost PEs belonging to Row\u2081 and Row2 are connected to SRBs with depth 1. A final adder tree, consisting of two adders, manages the psums coming from the PEs of Row2. In what follows, the detailed functionality of the dataflow is explained cycle-by-cycle. Preliminarily, K cycles are needed to load the K \u00d7 K weights, arranged in rows of K weights per cycle. After this phase, the actual computations and input movements can start:\n* Cycle 1: I = 1,2,3, supplied vertically to Rowo, are multiplied by the weights W = A,B,C.\n* Cycle 2: Rowo multiplies I = 2,3,4 with W = A, B, C, where I = 2,3 are reused through right-to-left movements, while I = 4 is supplied externally. Row1 multiplies the new inputs I = 6,7,8 with W = D, E, F, and accumulate these with the psums coming from Rowo.\n* Cycle 3: Rowo multiplies I = 3,4,5 with W = A, B, C, where I = 3,4 are reused through right-to-left movements, while I = 5 is supplied externally. Row\u2081 multiplies the inputs I = 7,8,9 with W = D, E, F, and accumulate these with the psums coming from Rowo. While I = 7,8 are reused through right-to-left move- ments, I = 9 is supplied externally. Row2 multiplies the new inputs I = 11, 12, 13 with W = G, H, I, and accumulate these with the psums coming from Row1. At this cycle, SRB0 receives I = 6 to be reused later.\n* Cycle 4: Row, multiplies I = 6,7,8 with W = A, B, C, where I = 6 is reused diagonally from SRB0, while I = 7,8 are reused diagonally from PE1,0 and PE1,1, respectively. Row\u2081 multiplies the inputs I = 8,9,10 with W = D, E, F, and accumulate these with the psums coming from Rowo. While I = 8,9 are reused through right-to-left movements, I = 10 is supplied externally. Row\u2082 multiplies the inputs I = 12, 13, 14 with W = G, H, I, and accumulate these with the psums coming from Row\u2081. While I = 12, 13 are reused through right-to-left movements, I = 14 is supplied externally. At this cycle, SRB and SRB\u2081 receive I = 7 and I = 11, respectively. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the very first output activation 00,0.\n* Cycle 5: Rowo multiplies I = 7,8,9 with W = A, B, C, where I = 7,8 are reused through right-to-left movements, while I = 9 is reused diagonally from PE1,1. Row\u2081 multiplies the inputs I = 11,12,13 with W = D, E, F, and accumulate these with the psums coming from Rowo. I = 11 is reused diagonally from SRB0, while I = 12, 13 are reused diagonally from PE1,0 and PE1,1, respectively. Row2 multiplies the inputs I = 13, 14, 15 with W = G, H, I, and accumulate these with the psums coming from Row1. While I = 13,14 are reused through right-to-left movements, I = 15 is supplied externally. At this cycle, SRB0 and SRB1 receive I = 8 and I = 12, respectively. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the second output activation 00,1.\n* Cycle 6: Rowo multiplies I = 8,9,10 with W = A, B, C, where I = 8,9 are reused through right-to- left movements, while I = 10 is supplied externally. Row1 multiplies the inputs I = 12,13,14 with W = D, E, F, and accumulate these with the psums coming from Rowo. While I = 12, 13 are reused through right- to-left movements, I = 14 is reused diagonally from PE2,1. Row2 multiplies the new inputs I = 16,17,18 with W = G, H, I, and accumulate these with the psums coming from Row1. At this cycle, SRB0 and SRB1 receive I = 11 and I = 13, respectively. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the third output activation 00,2.\n* Cycle 7: Rowo multiplies I = 11,12,13 with W = A, B, C, where I = 11 is reused diagonally from SRBO, while I = 12,13 are reused diagonally from PE1,0 and PE1,1, respectively. Row\u2081 multiplies the inputs I = 13,14,15 with W = D, E, F, and accu- mulate these with the psums coming from Rowo. While I = 13,14 are reused through right-to-left movements, I = 15 is supplied externally. Row2 multiplies the inputs I = 17, 18, 19 with W = G, H, I, and accumulate these with the psums coming from Row1. While I = 17,18 are reused through right-to-left movements, I = 19 is supplied externally. At this cycle, SRBO and SRB1 receive I = 12 and I = 16, respectively. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the fourth output activation 01,0.\n* Cycle 8: Rowo multiplies I = 12,13,14 with W = A, B, C, where I = 12,13 are reused through right- to-left movements, while I = 14 is reused diagonally from PE1,1. Row\u2081 multiplies the inputs I = 16,17,18 with W = D, E, F, and accumulate these with the psums coming from Rowo. I = 16 is reused diagonally from SRBO, while I = 17,18 are reused diagonally from PE1.0 and PE1,1, respectively. Row\u2082 multiplies the inputs I = 18, 19, 20 with W = G, H, I, and accu- mulate these with the psums coming from Row1. While I = 18,19 are reused through right-to-left movements, I = 20 is supplied externally. At this cycle, SRB0 and SRB1 receive I = 13 and I = 17, respectively. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the fifth output activation 01,1.\n* Cycle 9: Rowo multiplies I = 13,14,15 with W = A, B, C, where I = 13,14 are reused through right- to-left movements, while I = 15 is supplied externally. Row\u2081 multiplies the inputs I = 17,18,19 with W = D, E, F, and accumulate these with the psums coming from Rowo. While I = 17,18 are reused through right- to-left movements, I = 19 is reused diagonally from PE2,1. Row2 multiplies the new inputs I = 21,22,23 with W = G, H, I, and accumulate these with the psums coming from Row\u2081. At this cycle, SRB and SRB1 receive I = 16 and I = 18, respectively. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the sixth output activation 01,2.\n* Cycle 10: Rowo has completed the computations related to the current ifmap, so it starts to compute a sliding windows from a different ifmap. Row\u2081 multiplies the inputs I = 18,19,20 with W = D, E, F, and accu- mulate these with the psums coming from Rowo. While I = 18,19 are reused through right-to-left movements, I = 20 is supplied externally. Row2 multiplies the inputs I = 22, 23, 24 with W = G, H, I, and accumulate these with the psums coming from Row1. While I = 22, 23 are reused through right-to-left movements, I = 24 is supplied externally. From this cycle, SRB0 and SRB1 do not need to receive data anymore. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the seventh output activation 02,0.\n* Cycle 11: Row, and Row\u2082 have completed the computa- tions related to the current ifmap, so they work on a dif- ferent ifmap. Row\u2082 multiplies the inputs I = 23, 24, 25 with W = G, H, I, and accumulate these with the psums coming from Row\u2081. While I = 23, 24 are reused through right-to-left movements, I = 25 is supplied externally. Finally, the psums coming from PE2,0, PE2,1, and PE2,2 are accumulated through the adder tree to get the eighth output activation O2,1.\n* Cycle 12: All the rows have completed the computations related to the current ifmap, so they work on a different ifmap. The adder tree provides the final output activation 02,2.\nThe example just presented allows to appreciate how TrIM maximizes inputs utilization. In fact, the number of total memory accesses from the main memory is 29, of which only 4 accesses refer to inputs read more than once. To better spotlight this, let consider the case I = 13. This input is read once from the main memory (cycle 3) and reused 8 times (from cycle 4 to cycle 9) through right-to-left and diagonal movements."}, {"title": "IV. AN ANALYTICAL MODEL FOR SYSTOLIC ARRAYS", "content": "In order to highlight the advantages provided by TrIM, we built an analytical model to explore and evaluate the design space of TrIM, searching for optimum design points that lead to the physical implementation phase. The aim of this model is twofold: (i) providing insights about memory accesses, throughput and local buffers/registers; (ii) comparing TrIM to WS and RS dataflows. Without loss of generality, all the metrics refer to a convolution between an $H_1 \u00d7 W_1$ ifmap and a K \u00d7 K kernel. However, these can be easily scaled to manage M ifmaps and N filters of M kernels.\nA. Modelling WS and RS Dataflows\nThe number of memory accesses (MA) of the WS-based SA is dictated by Conv-to-GeMM and reported in equation (2):\n$MA_{WS} = K^2 \u00d7 (H_o \u00d7 W_o)$ (2)\nThis conversion results in data redundancy and higher memory capacity, since ifmap's activations are arranged as $H_o \u00d7 W_o$ rows and $K^2$ columns. On the contrary, since no Conv- to-GeMM is required by the RS-based SA, the MA of the main memory are directly related to the ifmap's sizes ($H_1 \u00d7 W_1$). However, the RS-based SA needs memory blocks at the PE level, usually implemented as SRAM-based scratch pads, which severely affect the energy footprint. Based on the Eyeriss implementation [22], the scratch pads energy is estimated to be from a =~ 11\u00d7 to ~ 16.3\u00d7 higher than the memory energy. Therefore, the number of MA is summarized by equation (3):\n$MA_{RS} = (1 + \u03b1) \u00d7 (H_1 \u00d7 W_1)$ (3)\nThe throughput (T) is expressed as the ratio between the number of operations carried out by the convolution and the total latency required for its completion, thus indicating how many operations are performed per cycle. First, to get the number of operations (OPs), we consider that $K^2$ multipli- cations and $K^2$ \u2212 1 additions are performed to produce one output, which almost corresponds to 2 \u00d7 $K^2$ operations. This is repeated $H_o \u00d7 W_o$ times to generate as many outputs. Thus, the total number of operations is given by equation (4):\n$OPs = 2 \u00d7 K^2 \u00d7 H_o \u00d7 W_o$ (4)\nWhen the WS-based SA is considered, the latency (L) is given by equation (5):\n$L_{WS} = K^2 + H_o \u00d7 W_o \u2212 1$ (5)\nEvery PE requires one clock cycle to generate its own psum. Thus, $K^2$ cycles are required to generate one full output. After that, one valid output is provided per cycle. The RS-based SA behaves differently. Every PE requires K cycles to generate the temporary output of a 1-D convolution. After that, K \u2212 1 extra cycles are needed to accumulate the psums vertically, in order to complete one 2-D convolution. Thus, L is given by equation (6):\n$L_{RS} = W_O \u00d7 (2 \u00d7 K \u2212 1)$ (6)\nConsidering the number of operations and the latency reported above, T is dictated by equations (7) and (8):\n$T_{WS} = \\frac{2 \u00d7 K^2 \u00d7 H_o \u00d7 W_o}{K^2 + H_o \u00d7 W_o - 1}$ (7)\n$T_{RS} = \\frac{2 \u00d7 K^2 \u00d7 H_o}{2 \u00d7 K - 1}$ (8)\nFor the sake of fair comparisons, T needs to be normalized by the number of PEs. Since the WS-based SA consists of K \u00d7 K PEs, while the RS-based SA is made of K \u00d7 Ho PEs, the T per PE (TPE) is given by equations (9) and (10):\n$TPE_{WS} = \\frac{2 \u00d7 H_o \u00d7 W_o}{K^2 + H_o \u00d7 W_o - 1}$ (9)\n$TPERS = \\frac{2 \u00d7 K}{2 \u00d7 K - 1}$ (10)\nTo consider local buffers and registers, FIFOs used in the WS-based SA and memory blocks inside the PEs of the RS- based SA are modelled as equivalent sets of registers. The WS- based SA requires 3 registers per PE, as well as $K^2$ \u2013 1 FIFOs for inputs. The total number of equivalent registers (Reg) is dictated by equation (11):\n$Reg_{WS} = 3 \u00d7 K^2 + \\frac{K^2 \u00d7 (K^2 - 1)}{2}$ (11)\nConversely, the PEs inside the RS-based SA adopts two memory blocks, each equivalent to K registers, and a register for the psum. The total number of registers is given by equation (12):\n$Reg_{RS} = (2 \u00d7 K + 1) \u00d7 K \u00d7 H_o$ (12)"}, {"title": "B. Modelling TrIM Dataflow", "content": "The major advantage offered by TrIM is the substantial reduction of MA if compared to WS and RS dataflow. In- deed, the high data utilization enabled by the triangular input movement maximizes the number of operations per access. Two contributions concur in modelling the MA metric: (i) the initial read of the ifmap, which is given by $H_1 \u00d7 W_1$ accesses; (ii) a very limited overhead (OV) to retrieve previous inputs, no more locally available. Therefore, MA is given by equation (13):\n$MA_{TRIM} = H_1 \u00d7 W_1 + OV$ (13)\nwhere OV is dictated by equation (14):\n$OV = \\begin{cases} (W_1 - K - 1) \u00d7 (K - 1) \u00d7 (H_1 \u2212 K)  & W_1 < 2K \\\\ (K \u2212 1)^2 \u00d7 (H_1 \u2013 K) &  W_1 \u2265 2K\\end{cases}$ (14)\nThe total latency is given by the number of outputs to be generated, other than an initial latency to fill the pipeline. This initial latency is dictated by the pipeline of the array (three cycles for the PEs, one cycle for the adder tree). Then, one new output is generated per cycle. Thus, the total L is given by equation (15):\n$L_{TrIM} = K + H_o \u00d7 W_o$ (15)\nAs a result, T is given by equation (16):\n$T_{TrIM} = \\frac{2 \u00d7 K^2 \u00d7 H_o \u00d7 W_o}{K + H_o \u00d7 W_o}$ (16)\nSince TrIM consists of K \u00d7 K PEs, the TPE is given by equation (17):\n$TPET_{TrIM} = \\frac{2 \u00d7 H_o \u00d7 W_o}{K + H_o \u00d7 W_o}$ (17)\nIn order to retrieve the total number of registers, we consider that each PE uses four registers, while extra K \u2212 1 SRBs, each containing $W_1$ \u2212 K \u2212 1 registers, are needed to ensure the diagonal reuse of inputs. In addition, the adder tree uses one register. Equation (18) summarizes the total number of registers:\n$Reg_{trim} = 3 \u00d7 K^2 + (K \u2212 1) \u00d7 W_1 + 2$ (18)"}, {"title": "V. DESIGN SPACE EVALUATION", "content": "In this section, TrIM is compared to the WS-based SA and the RS-based SA through a design space evaluation. Memory accesses, throughput and number of registers are considered, following the equations introduced in Section IV. For each metric, different kernel sizes and ifmap's sizes are considered. K spans the range 3,5,7, extensively used in state-of-the-art CNNs [27], [28]. Square ifmaps ($H_1 = W_1 = I$) cover the range I = [16, 32, 64, 128, 256]."}, {"title": "VI. CONCLUSION", "content": "This paper introduces TrIM, an innovative dataflow for SAs that is compatible with CNN computing. The dataflow has two levels of granularity: at the PE level, weights are kept stationary; high inputs utilization is guaranteed at the SA level, based on a triangular movement. To achieve this, K \u00d7 K PEs are interconnected with each other in different directions, and local shift registers are put at the left edge of the array to assist the triangular movement of inputs. We built an analytical model to characterize the array before the physical implementation phase. Memory accesses, throughput and number of registers are subjected to in-depth design space exploration. When compared to WS dataflow, TrIM requires one order of magnitude less memory accesses considering that no data redundancy is exploited. Furthermore, TrIM requires less memory access than RS since no micro-architectural memory blocks assist data circulation. The simpler PEs in TrIM also result in 15.6\u00d7 fewer registers than RS. Finally, thanks to the overlap between multiplications and accumu- lations, TrIM's real throughput achieves the peak throughput of 2 OPs/cycle/PE, overcoming RS by 81.8%. In the second part, we present a complete architecture dealing with TrIM and compatible with multi-dimensional convolutions for CNNs. As a case study, the architecture is characterized considering the VGG-16 CNN [29], and compared with state-of-the-art competitors."}]}