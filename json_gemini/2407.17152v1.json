{"title": "XMECAP: Meme Caption Generation with Sub-Image Adaptability", "authors": ["Yuyan Chen", "Songzhou Yan", "Zhihong Zhu", "Zhixu Li", "Yanghua Xiao"], "abstract": "Humor, deeply rooted in societal meanings and cultural details, poses a unique challenge for machines. While advances have been made in natural language processing, real-world humor often thrives in a multi-modal context, encapsulated distinctively by memes. This paper poses a particular emphasis on the impact of multi-images on meme captioning. After that, we introduce the XMECAP framework, a novel approach that adopts supervised fine-tuning and reinforcement learning based on an innovative reward model, which factors in both global and local similarities between visuals and text. Our results, benchmarked against contemporary models, manifest a marked improvement in caption generation for both single-image and multi-image memes, as well as different meme categories. XMECAP achieves an average evaluation score of 75.85 for single-image memes and 66.32 for multi-image memes, outperforming the best baseline by 3.71% and 4.82%, respectively. This research not only establishes a new frontier in meme-related studies but also underscores the potential of machines in understanding and generating humor in a multi-modal setting.", "sections": [{"title": "1\nINTRODUCTION", "content": "As the rise of social media, memes have become a ubiquitous form of communication and entertainment. A key research task relevant to meme is meme caption generation, which involves creating captions that complements the humor or message of the image to enhance its appeal and shareability [14, 19, 20]. Meme caption generation is valuable for applications and scenarios such as social media content creation, marketing strategies, and digital communication enhancement where engaging visual content is crucial [16, 22, 30, 35, 36, 44, 59].\nThe existing research efforts on meme caption generation mainly focus on using single-image memes [5, 40, 49, 52, 53]. As the example shown in Fig. 1(a), given the image of a meme featuring a man with a pained expression, the multi-modal model is requested to generate a fitting and humorous caption that aligns with the image's content, such as \"When my dad saw that I got a beating from my mom for doing something he had allowed me to do.\", indicating the ironic situation where the father permits something that the mother punishes, leading to the man's distressed expression. To achieve this, some work such as Chauhan et al. [8] and Li et al. [32] provide humor datasets from TV and memes respectively, while others such as Ritschel et al. [48] and Ritschel et al. [47] focus on robot humor.\nHowever, all the existing methods can not be adopted when there are multiple sub-images in a meme, each of which probably has different theme and emotion. As the example shown in Fig. 1(b) which is a multi-image meme consisting of three separate sub-images stacked vertically: The first sub-image is a drawing of a character smiling with a good signal Wi-Fi icon. The second sub-image shows the same character looking very upset, with a Wi-Fi icon that has only one bar. In the third sub-image, the character has the same expression as the second one but combining with a red background with an exclamation mark inside the Wi-Fi icon, indicating no internet connection. The caption is \"Connected, but no internet.\". This multi-image meme tells a story of someone's deteriorating mood as their internet connectivity worsens, which is relatable to many people's experiences with internet issues. Understanding the relationships between these sub-images and their connection to the overall meme theme poses two challenges. i) Effective Integration of Composite Information: In multi-image memes, it's crucial to effectively integrate information from all sub-images to generate a caption that reflects the characteristics of each sub-image while fitting the overall meme context. ii) Handling the Complexity of Shared Captions: As the captions for multi-image memes are generally shared across all sub-images, the generated caption needs to maintain consistency between different sub-images while being flexible enough to accommodate the unique content of each sub-image.\nGiven the absent of benchmark on caption generation for multi-image memes, in this work we construct a new meme datatset (in Chinese) including both single-image and multi-image memes. Specifically, we construct our dataset by sourcing memes from open platforms and meme websites, categorize them by structure (i.e. single-image, multi-image) and emotion (i.e. self-praise, praise of others, self-mockery, and mockery of others), and then balance the collection through downsampling to construct a representative mix of single-image and multi-image memes with varied emotions.\nBased on this dataset, we novelly propose XMECAP, a new approach which employs separate feature extraction for images and captions, introduces an adaptive transformation to capture the global and token-wise connections between the two, and uses the global and local similarities as supervision signals. We further enhance the LLMs by applying Supervised Fine-Tuning and Reinforcement Learning. Our reward model also incorporates these multi-granularity similarities as a part of the reward signal. Through comprehensive experiments, we demonstrate superior performance in caption generation for both single and multi-image memes.\nTo summarize, our contributions are in three-fold:\n\u2022 We recognize the impact of multi-images on meme caption-ing and offer a novel methodology named XMECAP for meme caption generation. XMECAP is characterised by supervised fine-tuning and reinforcement learning based on an inno-vative reward model, which factors in both global and local similarities between visuals and text.\n\u2022 We conduct extensive experiments to validate the superiority of our approach over current benchmarks in both single and multi-image meme caption generation, alongside promising results in conventional multi-modal humor detection tasks.\n\u2022 Through visualization analysis, our research further under-scores the ability of our proposed XMECAP to discern intricate associations between memes and their corresponding captions, setting the stage for advanced meme-related research in the future."}, {"title": "2 TASK AND DATASET", "content": "The challenge we address is the automatic generation of captions for a diverse array of memes. The problem is two-fold: first, to correctly classify memes into two structural categories (single-image or multi-image) and second, to identify the sentiment of the meme, categorized as self-praise, praise others, self-mockery, or mockery of others. The ultimate goal is to develop an algorithm capable of generating a caption that is congruent with both the meme's structure and sentiment, enhancing the humor and communicative intent of the meme.\nTask definition. The meme caption generation task G aims to produce a caption \\(\\hat{c_i}\\) for a given image \\(m_i\\) in a meme. The performance of this task can be measured by a scoring function \\(S(m_i, c_i, \\hat{c_i})\\) which evaluates the generated caption \\(\\hat{c_i}\\) against the ground truth \\(c_i\\). The overall goal is to maximize the performance score across all memes in the given dataset as follows:\n\\[\\hat{c_i} = G(m_i), O = \\max_i S(m_i, c_i, \\hat{c_i}).\\]\nDataset construction. We construct a large-scale meme dataset, comprising 18,110 memes from open-sourced platforms 1 2 and memes from various Chinese meme images websites. We classify memes based on their structure into single-image memes and multi-image memes. The latter emphasizes connections between individual sub-images alongside the relationship with the caption. We further categorized memes by their sentiment into four types: self-praise, praise others, self-mock, and mock others. To maintain a balanced dataset, we undertook downsampling. The final set contains 12,320 memes with 54% single-image memes and 46% multi-image memes. In terms of sentiment types, we self-praise accounts for 21%, praise others 23%, self-mockery 29%, and mock others 27%. Caption lengths are detailed in Table 1."}, {"title": "3\nMETHODOLOLOGY", "content": "In this section, we propose a novel Chinese meme caption generation approach named XMECAP, which incorporates visual and textual features through adaptive transformation layer and utilize image-text attention to generate captions. The framework is shown in Fig. 2."}, {"title": "3.1 Feature Extraction", "content": "Our proposal first separately enhances and extracts features from images and captions. Specifically, we first categorize meme images into single-image memes and multi-image memes manually. For multi-image memes, we use OpenCV-Python to precisely identify each sub-image and its boundaries and capture their coordinates by selecting regions of interest (ROIs). This process involves declaring a mouse click function to outline the boundaries of each sub-image. Once these boundaries are established, we apply image enhancement to each sub-image individually. With AutoAugment [23], we transform each original sub-image of an image (denoted as I) into enhanced versions. This includes applying techniques such as cropping and rotating, which are customized according to the specific features of each sub-image. For feature extraction, we adopt a powerful large multi-modal model (referred to as LMM, we use LLaVA-1.5-7B here) as a visual encoder to extract deep features from each original and enhanced sub-image. This process allows us to capture unique detailed features in each sub-image, such as the morphology and color gradients of objects. Since the text areas lie outside the boundaries of all sub-images in multi-image memes, we consider the text as a shared caption across all sub-images. The text enhancement process, using back-translation and a Transformer-based LLM (we use Baichuan2-7B here), extracts deep features for these shared texts. These extracted features are then used for each sub-image, considering the shared text as a common caption complementing the entire set of sub-images.\nTo generate effective meme captions, our proposal generates descriptions for each sub-image based on the LMM. Then, these descriptions are transformed into structured text with a \"chain-of-humor\" template [15, 21]. For each sub-image, this template includes creating a narrative that encompasses the core concept (such as the main object in the sub-image), emotion (such as surprise), event (such as sharing a photo), consequence (such as discussing the photo), and humor elements (such as using anthropomorphism). Each step of this process, including feature extraction, adaptive transformation, and attention-guided text generation, takes the specific sub-image source (i.e., the index of the sub-image relative to the meme image) as input, ensuring a tailored approach for both single-image memes and multi-image memes."}, {"title": "3.2 Adaptive Transformation", "content": "This process aims to effectively merge the image features and caption features of each image area into a unified space [18, 35]. Each feature uses an independent trainable linear layer for projection. This transformation is achieved through trainable weight matrices and corresponding bias terms. Specifically, for each image area (i.e. part of the image and generally not the sub-image) represented as \\(M_{Ii}\\) and its corresponding caption features \\(T_I\\), we apply the following linear transformation:\n\\[M_{Ii} = W_{M1} \\cdot M_{Ii} + b_{M1}, T_I = W_{T1} \\cdot T_I + b_{T1},\\]\nwhere \\(W_{M1}\\) and \\(W_{T1}\\) are trainable weight matrices, and \\(b_{M1}\\) and \\(b_{T1}\\) represent the respective bias terms.\nNext, we use the self-attention mechanism of the LLM to calculate the token-level similarity between each area in the image and each word in the caption, denoted as \\(S_{Ii,j}\\). Note that the self-attention mechanism embedded in LLMs' architecture sometimes differs from the general self-attention mechanism, such as the ROPE encoding in Baichuan. The entire calculation process is as follows:\n\\[E_{Ii,j} = (M_{Ii}W^Q) \\cdot (T_{I,j}W^K),\\]\n\\[S_{Ii,j} = \\frac{\\exp(E_{Ii,j}/\\sqrt{d_k})}{\\sum_{n=1}^N \\exp(E_{Ii,n}/\\sqrt{d_k})}\\]\nwhere \\(W^Q\\) and \\(W^K\\) are the weight matrices of queries and keys in the self-attention mechanism, \\(d_k\\) is the dimension of the key vectors, which adjusts the scale of the dot product, \\(E_{Ii,j}\\) is the unnormalized attention weight, and \\(S_{Ii,j}\\) is the normalized attention weight representing the similarity between the i-th image area and the j-th caption word. To compute the global attention, denoted as \\(S_{I\\cdot,j}\\), we average the token-level attention across the image and caption to obtain a global view. The computation is as follows:\n\\[S_{I\\cdot,j} = \\frac{1}{N} \\sum_{i=1}^N S_{Ii,j}\\]\nFinally, we use the global attention and token-level attention to construct a loss function aimed at maximizing the similarity between images and captions while minimizing the similarity of unrelated combinations. This is achieved through contrastive loss, computed as follows:\n\\[L_I = - \\log \\frac{\\exp(S_{Ii,j}/t)}{\\sum_{k=1}^{N'} \\exp(S_{Ii,k}/t)}\\]\nwhere \\(S_{Ii,j}\\) is the similarity score between the i-th image area and the j-th caption word, \\(S_{Ii,k}\\) is the similarity score between the i-th image area and all words in a caption (including matching and non-matching, denoted as k), N' is the number of caption words, and t is the temperature parameter, which adjusts the sensitivity of the loss function."}, {"title": "3.3 Attention-guided Text Generation", "content": "Given the prior knowledge provided by the aforementioned global and token-level similarities representing the relevance between images and captions, our proposal adopt attention-guided text generation method including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to improve the performance of generating meme captions [11, 13, 42, 43]. First is the SFT step, where the LLM aims to generate meme captions that are close to ground-truth captions. A key aspect of this process is to ensure that the LLM's predicted similarity aligns well with the similarity derived from prior knowledge (global and token-level). To this end, we introduce an additional loss component that utilizes the Kullback-Leibler (KL) divergence to measure the difference between these similarities, as shown below:\n\\[p_{S_I} = \\frac{\\exp(S_I)}{\\exp(S_I) + \\exp(S^{SFT})},\\]\n\\[p_{S_{Ii,j}} = \\frac{\\exp(S_{Ii,j})}{\\exp(S_{Ii,j}) + \\exp(S_{I{i,j}}^{SFT})},\\]\n\\[q_{S_I} = \\frac{\\exp(S^{SFT})}{\\exp(S_I) + \\exp(S^{SFT})},\\]\n\\[q_{S_{Ii,j}} = \\frac{\\exp(S_{I{i,j}}^{SFT})}{\\exp(S_{Ii,j}) + \\exp(S_{I{i,j}}^{SFT})},\\]\n\\[L_g = \\lambda_g KL(p_{S_I}||q_{S_I}), L_t = \\lambda_t \\sum_{I} \\sum_{i,j} KL(p_{S_{Ii,j}}||q_{S_{Ii,j}})\\]\nwhere \\(S^{SFT}\\) denotes the similarity score predicted by the LLM for the image and its caption. Specifically, it represents the cosine similarity between the captions generated through Supervised Fine-Tuning (SFT) and the ground-truth captions. \\(S_I\\) refers to the similarity score between the image and its caption, calculated based on prior knowledge, including global and token-level similarities. \\(S_{I{i,j}}^{SFT}\\) indicates the similarity score between the i-th image area and the j-th word in the caption as predicted by the LLM. \\(S_{Ii,j}\\) is the similarity score for captions obtained through SFT, which is determined by the cosine similarity between captions generated through SFT and the ground-truth captions. This score represents the similarity between the i-th image area and the j-th caption word, based on prior knowledge such as global and token-level similarities. Here, the total loss for SFT, including the original SFT loss and the introduced loss based on prior knowledge similarities at the global and token levels, becomes:\n\\[L_{SFT} = \\lambda_{SFT-ori} L_{SFT-ori} + \\lambda_g L_g + \\lambda_t L_t,\\]\nwhere \\(\\lambda_{SFT-ori}\\), \\(\\lambda_g\\) and \\(\\lambda_t\\) is a trainable weight.\nThe next step involves building a reward model to align captions with human preferences. The reward model links the generated caption y with the ground truth one t to calculate a reward \\(r_i = R_i(y, t)\\). In detail, we initially manually evaluate 1% of the captions generated during the SFT process with the criteria shown in Table 2. Here, the annotators are three volunteers. They kindly provide help without any compensation. They rank according to the scoring criteria mentioned in the response to W21. These individuals have not seen the ground truth in advance, and the order is only accepted if the agreement exceeds 0.7. This process results in a ranked sequence, represented as \\(\\{C_1, C_2,..., C_{k-1}, C_k\\}\\). Based on the calculated rewards, we construct ordered sequences for captions on each image, such as \\(\\{c_1 < C_2 < ... < C_{k-1} < c_k\\}\\), where a larger reward indicates a better match between y and t. During caption generation, as tokens emerge, we compute their attention towards specific image areas. If a token's attention closely matches prior attention distribution, it receives a reward; if not, it's given a penalty. This process yields a new ranking. By balancing human evaluations with attention-based rankings, we determine the final sequence for captions generated in the SFT process. To train the reward model, we also employ \"Baichuan2-7B\" as the backbone, modifying its softmax layer to a linear one. This reward model takes a caption and outputs a score which represents the caption's quality. We collate captions from the final ranking sequence and apply the Pairwise Ranking Loss, depicted as follows:\n\\[L_r = - \\frac{1}{|D|} \\sum_{(x, y_w, y_l) \\in D} [\\log(\\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)))],\\]\nwhere x symbolizes the original caption. \\(y_w\\) and \\(y_l\\) are indicative of captions with higher and lower scores in the given ranking pair, respectively. \\(r_\\theta\\) is the scalar output from the reward model, with D being the set of ranking pairs, and k representing the count of captions produced during the SFT process. This reward model's efficacy lies in its capacity to attribute higher scores (rewards) to superior captions and lower scores (penalties) to inferior captions, adeptly mirroring human preferences and LLM's preference.\nAfter that, we feed captions c generated by the SFT model into the RL model \\(\\pi_L\\) to obtain a more human-preferred captions y. We first input (x, y) into the reward model \\(r_\\theta\\) and calculate a score (i.e., reward), which represents the real-time feedback from the reward model. Next, we aims to maintain similarity between the RL model and the SFT model with Kullback-Leibler (KL) divergence. Finally, we combine the two loss functions as follows:\n\\[L^r = E_{(x, y) \\sim D_{RL}} [r_\\theta(x, y)],\\]\n\\[L^{SFT} = -\\log(\\frac{\\pi_L (y|x)}{\\pi_{SFT} (y|x)}),\\]\n\\[L_{RL} = w_{RL} \\cdot L^r + w^{RL} \\cdot L^{SFT},\\]\nwhere \\(\\pi_L (y|x)\\) and \\(\\pi_{SFT} (y|x)\\) represent captions generated by RL model and the SFT model, respectively, \\(w_{RL}\\) and \\(w^{RL}\\) are trainable weights."}, {"title": "4\nEXPERIMENTS", "content": "In this section, we evaluate the performance of our proposed XMECAP framework on memes caption generation dataset, including single-image memes and multi-image memes."}, {"title": "4.1 Experimental Setups", "content": "We conduct our experiments on four Nvidia A100 GPUs, each with 80GB of memory, using PyTorch in Python. For enhanced training efficiency, we utilize DeepSpeed. We set the maximum sequence length for both input and output sequences to 1024 tokens. The training process is set to 20 epochs. \\(\\lambda_{SFT-ori}\\), \\(\\lambda_g\\), and \\(\\lambda_t\\), which control the weight of original SFT process, the weight of global similarity loss (\\(L_g\\)), and the weight of token-wise similarity loss (\\(L_t\\)), in the total loss, are initially set to 0.4, 0.2, and 0.4, respectively.\n\\(w_{RL}\\) and \\(w^{RL}\\), which adjust the weight of the reward model loss, as well as the weight of the loss for maintaining similarity between the RL model and the SFT model, are initially set to 0.4 and 0.6, respectively.\nSpecifically, for \\(\\lambda_{SFT-ori}\\), \\(\\lambda_g\\), and \\(\\lambda_t\\), we believe SFT is more important because fine-tuning has been proven to be more effective in improving the performance of downstream tasks and is supervised. Since we are capturing the relationship between each sub-image and the caption, we consider the token level to be more important. Therefore, the order of the values is \\(\\lambda_{SFT-ori} \\ge \\lambda_t \\ge \\lambda_g\\). We try different combinations using prior knowledge, with \\(\\lambda_{SFT-ori}\\), \\(\\lambda_t\\), and \\(\\lambda_g\\) being 0.4, 0.4, and 0.2; 0.5, 0.3, and 0.2; and 0.6, 0.2, and 0.2, respectively. For \\(w_{RL}\\) and \\(w^{RL}\\), we use prior knowledge to try different combinations, including 0.5 and 0.5, 0.4 and 0.6, and 0.6 and 0.4. Each experiment is trained with these sets of parameters, and the best-performing set was chosen. Each baseline is optimized through SFT and RL with the training set. The loss function of SFT for these baselines is only \\(L_{SFT-ori}\\) while that for XMECAP is \\(\\lambda_{SFT-ori}, \\lambda_g\\) and \\(\\lambda_t\\).\nDue to many LMMs being pre-trained on English corpora, to minimize the impact of language on model performance while preserving the characteristics of Chinese memes, we utilize Tencent Cloud's API 3 to translate Chinese captions for training and evaluation. Ultimately, the LMMs output results in English, which we then translate back into Chinese for presentation."}, {"title": "4.2 Datasets, Baselines and Metrics", "content": "The selected datasets contain UR-FUNNY [27] which is designed for humor understanding, MUSTARD [6] which focuses on multimodal sarcasm detection, MHD [46] which predicts laughter in sitcoms based on multimodal data. All results are reported on the 20% subset split from the original dataset. The remaining 80% subset is regarded as the training set. Specifically, we separately compare the results on single-image memes and multi-image memes. The prompt template for generating captions with baselines is \"What is a humorous short sentence that complements the image as a meme?\".\nWe categorize our evaluation metrics into two groups: meme-specific metrics and general metrics. Meme-specific metrics contain informativeness, relevance, creativity, and humorous, measuring the human-like quality of the generated captions. General metrics contain BLEU [45], ROUGE [34], CIDEr [54], and METEOR [4], measuring the relevance and diversity of the generated captions. The rating scale of meme-specific metrics are all from 1 to 5, where 1 means the worst and 5 means the best. The final scores will be scaled to 1-100. We enroll three volunteers, and each of them is required to give scores for the randomly selected 1000 memes with generated captions. We also calculate Inter-rater agreement of Krippendorff's Alpha (IRA) to ensure the confidence of human ratings. For the controversial ratings which have low agreements (<0.7), we discard this caption."}, {"title": "4.3 Main Results", "content": "Based on the presented experimental results in Table 3 and Table 4, our method XMECAP exhibits impressive performance in both single-image and multi-image meme caption generation. Compared to existing baselines, XMECAP consistently achieves higher scores across various metrics, demonstrating its efficacy. Notably, its performance is closely aligned with that of GPT4, suggesting that our method is competitive and can rival state-of-the-art models in this domain. Moreover, we test 500 samples with GPT-40, using the same dimensions as human evaluation, and each sample is tested three times. We only use a score if at least two out of three results are the same. We find that the correlation between GPT-40 scores and human scores is very high, reaching 0.935, which indicates that human evaluation can partly be replaced with GPT-40 scoring, thereby reducing manual efforts. From the results presented in Table 7, it's evident that our method XMECAP exhibits varied performance across different meme types, both in single-image and multi-image formats. Interestingly, XMECAP performs notably better in memes with negative connotations, such as \"Self-mock\" and \"Mock others\", compared to those with positive themes like \"Self-praise\" and \"Praise others\". This suggests that XMECAP has seemingly captured the contrastive nature inherent in memes, finding a more conducive space for caption generation in images with a negative nuance. This insight underscores XMECAP's sub-image adaptability of meme sentiment. The results from Table 8 highlight the performance of our method, XMECAP, after being fine-tuned for modal-humor detection tasks. It is evident that XMECAP achieves commendable results across multiple modal-humor detection benchmarks, exhibiting performance closely rivaling that of GPT4. This underscores the generalization capabilities of our approach."}, {"title": "4.4 Ablation Study", "content": "Take multi-image meme caption generation as an example, we evaluate the impact of each component in our proposed XMECAP for single-image and multi-image memes as shown in Table 5 and Table 6, respectively. Specifically, w/o IA, Without Image Augmentation, refers to not applying image enhancement techniques to the original image during the feature extraction process. This means not using cropping and rotation operations provided by AutoAugment. This absence may affect the model's ability to extract deep features from the image, such as the morphology of objects and color gradients; W/o TA, Without Text Augmentation, indicates that text augmentation techniques, specifically back-translation, are not used in processing textual data. Back-translation is employed to increase the diversity of textual data and assist the model in extracting deep features from both the original text and the back-translated text; W/o COH, Without Chain of Humor, means that the \"Chain of Humor\" template, inspired by the \"chain of thought\" approach, is not used in the process of generating meme captions. This template helps the model to construct structured texts, involving core concepts, emotions, events, consequences, and humor elements. In fact, we also try methods similar to simple CoT, such as: i) \"Let's think outside the box. Please read the picture carefully and write a surprising and funny caption.\" ii) \"Let's think outside the box. Please read the picture carefully and write a surprising and funny caption. Try to go wild and use your associative imagination. The more creative, the better.\" iii) \"Let's think outside the box. Please read the picture carefully and write a surprising and funny caption. Please use associative imagination based on the object in the image.\" These prompts do not perform as well as the current CoH, possibly because CoH provides possible angles for XMECAP to generate humorous captions; W/o A, Without Attention Mechanism, implies that the attention mechanism based on the Transformer model is not utilized to calculate the correlation between image features and caption features. This mechanism finely maps specific regions of the image to words in the text; W/o CL, Without Contrastive Learning, signifies that contrastive learning methods are not used in model training. Contrastive learning enhances the model's distinguishing ability by contrasting positive samples (actual captions) with negative samples (captions from other images); W/o RL, Without Reinforcement Learning, indicates that reinforcement learning is not integrated into the model training to optimize meme caption generation. Reinforcement learning involves evaluating the quality of captions and adjusting generation strategies based on these evaluations to create captions more aligned with human preferences. Among the components, the attention mechanism displays the most pronounced effect, underscoring the significance of cross-modal interaction. The performance when removing \u201cchain-of-humor\" further emphasizes the importance of its cross-modal bridges. Contrastive Learning effectively distinguishes the latent relationships between individual memes and their corresponding captions. However, the effects of image and text augmentation were relatively subdued, suggesting that mere alterations without changing the core content of images or captions may not substantially enhance performance.\""}, {"title": "4.5 Illustrative interpretation", "content": "Meme caption generation hinges on pinpointing textual cues that align with the image's core humor. A successful approach should emphasize key phrases in harmony with the image's salient features [12, 17, 39]. We use blue and orange to highlight the corresponding text. In the single-image meme above, \u201cintegrate\u201d and \u201cexpose\u201d stand out. We adopt FLIP [31] to draw a heatmap, and find that \u201cintegrate\u201d corresponds to the yellow-highlighted region, representing a black dog among black ducks. Conversely, \u201cexpose\u201d relates to the blue-highlighted area, marking a distinctive white feather on the dog's nose. This meme captures the humor of an effort to fit in yet unintentionally standing out. For the multi-image meme below, \u201cdifferent\u201d and \u201cgreeting\u201d are focal. The heatmap pinpoints two areas: a handshake and raised eyebrows. Although distinct, both gestures signify greetings, highlighting varied forms of acknowledgment. Our framework also highlights key parts in more multi-image memes such as the \u201chead and cigarette\u201d in both two subimages in the third meme (Fig. 4(a)), and the \u201csimple face\u201d in the left subimage and the \u201cdetailed face\u201d in the right subimage in the fourth meme (Fig. 4(b)) with orange boxes for generating humorous captions."}, {"title": "4.6 Case Study", "content": "The case study in Table 9 and Table 10 indicates that our XMECAP closely matches GPT4's performance, notably in negative categories (self-mockery and mock others). Compared to baselines like s2s, Dank Learning, and transformer, our approach surpasses, showing a deeper grasp of humor nuances. However, the XMECAP needs enhancement in the positive categories (self-praise and praise others), suggesting further refinements. Error analysis points to challenges in accurate caption generation. Context discrepancies can dilute humor. Although XMECAP is precise in image description, it sometimes miss humor or creativity. There's also the issue of cultural sensitivity and potential offensiveness. Difficulties with multi-image memes emphasize the need for improved captioning techniques."}, {"title": "5 RELATED WORK", "content": "Multi-modal humor research is expanding. Wu et al. [55] explore TV-sitcom dialogues, while Patro et al. [46] focus on 'Big Bang Theory' for humor detection. Chauhan et al. [8] and Li et al. [32] provide humor datasets from TV and memes respectively. Devillers et al. [25] consider laughter in robot interactions. Chen and Jiang [10] and Tsakona [51] address humor theories. Hasan et al. [26] and Yang et al. [56] present models for multimodal humor understanding and labeling. Alnajjar et al. [1] and Chauhan et al. [7] delve into TV shows and sentiment in humor. Regarding humor generation, Ritschel et al. [48] and Ritschel et al. [47] focus on robot humor. In meme generation, Sadasivam et al. [49] to Wang et al. [53] offer tools, datasets, and systems for memes. However, distinguishing between single and multi-image memes hasn't been a focus until our research, which offers a distinct approach for both single-image and multi-image memes.\nTextual humor generation seeks to produce comedic content. Templates often involve lexical changes via tools like WordNet, as demonstrated by Sj\u00f6bergh and Araki [50] in Japanese comedy and Hong and Ong [28] for puns. However, they can be formulaic. Conversely, neural models promise more originality. For example, works by Li et al. [33] and Yu et al. [57] use such models for pun creation. However, humor isn't solely textual; images play a role, which our research emphasizes."}, {"title": "6 CONCLUSIONS", "content": "Humor poses a great challenge for human-machine interaction. This study has illuminated the intricate dynamics of humor in the multi-modal realm of memes, emphasizing on the impact of multi-images on meme captioning. Through our innovative XMECAP framework, we have established a deeper comprehension of meme image-text relationships, achieving state-of-the-art performance in meme caption generation as well as multi-modal humor detection. As we advance, it's imperative to explore the cross-cultural adaptability of our method, understanding the subtle variations in humor across different societies. Additionally, integrating more sophisticated semantic analysis tools could further refine the quality of generated captions."}]}