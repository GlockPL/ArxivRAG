{"title": "Collaborative Deterministic-Diffusion Model for Probabilistic Urban Spatiotemporal Prediction", "authors": ["Zhi Sheng", "Yuan Yuan", "Yudi Zhang", "Depeng Jin", "Yong Li"], "abstract": "Accurate prediction of urban spatiotemporal dynamics is essential for enhancing urban management and decision-making. Existing spatiotemporal prediction models are predominantly deterministic, focusing on primary spatiotemporal patterns. However, those dynamics are highly complex, exhibiting multi-modal distributions that are challenging for deterministic models to capture. In this paper, we highlight the critical role of probabilistic prediction in capturing the uncertainties and complexities inherent in spatiotemporal data. While mainstream probabilistic models can capture uncertainty, they struggle with accurately learning primary patterns and often suffer from computational inefficiency. To address these challenges, we propose COST, which collaborates deterministic and probabilistic models to improve both predictive accuracy and the ability to handle uncertainty. To achieve this, we design a mean-residual decomposition framework, where the mean value is modeled by a deterministic model, and the residual variations are learned by a probabilistic model, specifically diffusion models. Moreover, we introduce a scale-aware diffusion process, which better accounts for spatially heterogeneous dynamics across different regions. Extensive experiments on eight real-world datasets demonstrate that CoST significantly outperforms existing methods in both deterministic and probabilistic metrics, achieving a 20% improvement with low computational cost. CoST bridges the gap between deterministic precision and probabilistic uncertainty, making a significant advancement in the field of urban spatiotemporal prediction.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate prediction of urban spatiotemporal dynamics is crucial for urban management, efficiency optimization, and the development of smart cities [22, 47, 53, 55]. These urban spatiotemporal data encapsulate complex patterns while also exhibiting randomness [4, 11, 26, 42, 55]. Taking traffic flow data as an example, it displays clear temporal and spatial patterns: it peaks during rush hours and is influenced by neighboring regions, while also being prone to random fluctuations caused by events like accidents or extreme weather.\nMainstream urban spatiotemporal prediction methods mainly rely on deterministic models [41, 48, 52, 53, 55]. These models typically use mean absolute error (L1) or squared error (L2) between predicted and true values as the loss function. This allows to learn the expected value and capture primary spatiotemporal patterns. However, the data often follow a multi-modal distribution [14, 18], where multiple states can coexist at a given spatiotemporal point.\nFor example, in a scenario where a road segment with frequent accidents has an accident rate P1, with peak traffic volumes y\u2081 during normal hours and y2 during accidents, deterministic models learn a conditional mean: $E[y|x] = P_1 \\times y_2 + (1 \u2212 p_1) \\times y_1$. However, this underestimates the resources needed in congested areas and over-allocates them to free-flowing ones. In contrast, probabilistic methods can address this by modeling multi-modal distributions. Existing probabilistic models, such as Generative Adversarial Networks (GANs) [13, 17], Variational Autoencoders (VAEs) [25, 27], and Diffusion Models [20, 43], have shown promising performance. These methods aim to learn the full data distribution, capturing both primary patterns (e.g., long-term temporal trends and global spatial patterns) and non-linear variations (e.g., random fluctuations caused by incidental events). However, their focus on modeling uncertainty often limits their ability to accurately capture the primary patterns. Additionally, these models often suffer from inefficiency in real-world deployment. For example, GANs struggle with training challenges and mode collapse [6, 17], while diffusion models require multiple denoising steps for sampling, increasing computational costs [38, 43, 54].\nTherefore, we aim to leverage the strengths of both deterministic and probabilistic models to enhance predictive accuracy while effectively capturing uncertainty. However, this is not trivial and faces three challenges: (i) Spatiotemporal data not only exhibits primary spatiotemporal patterns but also involves random fluctuations. (ii) The multi-modal distributions across different spatiotemporal locations are inherently heterogeneous. (iii) Large-scale urban computing and decision-making applications require both computationally efficient and scalable prediction models.\nTo address the aforementioned challenges, we propose CoST that Collaborates the deterministic model and probabilistic model for urban SpatioTemporal prediction. As illustrated in Figure 1,"}, {"title": "2 RELATED WORK", "content": "Spatiotemporal prediction in urban environments [50, 52, 55] aims to model and forecast the dynamics of urban data, such as traffic flow, cellular network traffic, and energy consumption. Previously, researchers have employed statistical methods (e.g., ARIMA [5]) and traditional machine learning models (e.g., SVM [34], GP [40], and RF [19]) for prediction. Recently, a variety of neural network-based models have been proposed for this task, which can be categorized into deterministic and probabilistic prediction."}, {"title": "2.1 Urban Spatiotemporal Prediction", "content": "Spatiotemporal prediction in urban environments [50, 52, 55] aims to model and forecast the dynamics of urban data, such as traffic flow, cellular network traffic, and energy consumption. Previously, researchers have employed statistical methods (e.g., ARIMA [5]) and traditional machine learning models (e.g., SVM [34], GP [40], and RF [19]) for prediction. Recently, a variety of neural network-based models have been proposed for this task, which can be categorized into deterministic and probabilistic prediction."}, {"title": "2.2 Deterministic Prediction", "content": "Deterministic prediction of spatiotemporal data is a point estimation approach that assumes a deterministic prediction can be made given the same historical observations. These models are typically trained using loss functions such as MSE or MAE, ensuring that the model learns to predict the conditional mean $E[y|x]$ to capture the primary patterns. Existing deep learning-based deterministic forecasting models include MLP-based [36, 41, 57], CNN-based [28, 31, 55], and RNN-based [2, 30, 44, 45] architectures, which are commonly used for their computational efficiency. Additionally, GNN-based models [1, 1, 3, 15, 22] have been developed to capture spatial dependencies in graph-structured data, while Transformer-based models [8, 10, 21, 51] excel at capturing complex dynamic patterns."}, {"title": "2.3 Probabilistic Prediction", "content": "The core of probabilistic prediction lies in constructing a conditional probability distribution $p(y|x)$, which quantifies the uncertainty of the prediction rather than merely providing a single deterministic value [43, 49]. Unlike deterministic prediction models, these models assess the reliability of the predictions through probabilistic representations like quantiles or confidence intervals. Early applications typically employ Bayesian networks. In recent years, with the development of generative models, approaches based on GAN [23, 39, 56], VAE [9, 12, 58], and diffusion models [7, 29, 43] have made significant advancements, owing to their powerful data distribution modeling capabilities. However, deterministic models still dominate in related research, and probabilistic prediction models remain relatively underexplored. This study aims to leverage the strengths of both deterministic and probabilistic models to achieve efficient probabilistic predictions."}, {"title": "3 PRELIMINARY", "content": "Urban spatiotemporal prediction aims to predict future variations based on historical observations. The urban spatiotemporal data is typically represented as a multi-dimensional tensor $x \\in R^{T\\times K \\times C}$, where T denotes the temporal dimension, K represents the spatial dimension, and C is the feature dimension. The format typically includes two types: grid-based data and graph-based data.\nFor grid-based data, the spatial dimension K can be expressed in a two-dimensional form as $H \\times W$, where H and W denote the height and width, respectively.\nFor graph-based data, K represents the number of nodes in the graph structure. These nodes are located within a spatial topological structure denoted as $G = (V, E, A)$. Here, V is the set of all nodes. $\\& $ is the set of edges connecting the nodes. In addition, A is the adjacency matrix of graph G. Its elements $a_{ij}$ show if there's an edge between node i and j in V, $a_{ij} = 1$ when there's an edge and $a_{ij} = 0$ otherwise."}, {"title": "3.1 Problem Formulation", "content": "Urban spatiotemporal prediction aims to predict future variations based on historical observations. The urban spatiotemporal data is typically represented as a multi-dimensional tensor $x \\in R^{T\\times K \\times C}$, where T denotes the temporal dimension, K represents the spatial dimension, and C is the feature dimension. The format typically includes two types: grid-based data and graph-based data.\nFor grid-based data, the spatial dimension K can be expressed in a two-dimensional form as $H \\times W$, where H and W denote the height and width, respectively.\nFor graph-based data, K represents the number of nodes in the graph structure. These nodes are located within a spatial topological structure denoted as $G = (V, E, A)$. Here, V is the set of all nodes. $\\& $ is the set of edges connecting the nodes. In addition, A is the adjacency matrix of graph G. Its elements $a_{ij}$ show if there's an edge between node i and j in V, $a_{ij} = 1$ when there's an edge and $a_{ij} = 0$ otherwise."}, {"title": "3.2 Diffusion-based Probabilistic Prediction", "content": "The diffusion-based urban spatiotemporal model is a conditional probabilistic model, consisting of a forward process and a reverse process. In the forward process, noise is added incrementally to target data $x_0$ according to a predefined noise schedule $\\{\u03b2_n\\}_{n=1}^N$, gradually transforming the data distribution into a standard Gaussian distribution $N(0, I)$. At any diffusion step, the corrupted target data can be computed using the one-step forward equation:\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim N(0, I),$  (1)\nwhere $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$ and $\\alpha_n = 1 \u2013 \u03b2_n$. In the reverse process, we first sample $x_N$ from the standard Gaussian distribution N(0, I) and denoise it through the following Markov process:\n$p_\u03b8(x_{n-1}|x_n, x_0) := N(x_{n-1}; \u00b5_\u03b8(x_n, n|x_0), \u03a3_\u03b8(x_n, n)),$\n$\u00b5_\u03b8(x_n, n|x_0) = \\frac{1}{\\sqrt{\\alpha_n}} (x_n - \\frac{\u03b2_n}{\\sqrt{1 - \\bar{\\alpha}_n}} \\epsilon_\u03b8(x_n, n|x_0)),$\n$\u03a3_\u03b8(x_n, n) = \\frac{1 - \\bar{\\alpha}_{n-1}}{1 - \\bar{\\alpha}_n} \u03b2_n.$  (2)"}, {"title": "3.3 Evaluation of Probabilistic Prediction", "content": "To comprehensively evaluate probabilistic prediction, it is essential to consider two key dimensions:\nData Distribution-the predicted distribution should align with the empirical data distribution.\nPrediction Usability-A reliable prediction interval has a high probability of containing the true future values, while a sharp interval provides a more precise estimate.\nExisting studies predominantly rely on Continuous Ranked Probability Score (CRPS) to provide a measure of the distribution discrepancy, while deterministic metrics such as MAE and RMSE to capture point estimation errors. However, these metrics fail to systematically evaluate: (i) Whether the learned distribution accurately covers the data across different quantile intervals, particularly when dealing with multi-modal distributions; (ii) Whether the interval width appropriately reflects real uncertainty. To address these challenges, we introduce two more evaluation metrics, Quantile Interval Coverage Error (QICE) and Interval Score (IS).\nQICE. QICE is first proposed in [18] for regression tasks. This metric is intended to assess the mean absolute deviation between the proportion of true data points within each quantile interval (QI) and the ideal proportion. Specially, for each sample point y, after obtaining a sufficient number of predicted values \u0177, these predictions are divided into MQIs equally-sized QIs. Then the QICE is computed according to the following formula:\n$QICE := \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{M_{QIs}} \\sum_{m=1}^{M_{QIs}} |\\frac{1}{r_m} \\sum_{k=1}^{r_m} Y_n | \\frac{1}{M_{QIs}} |,$\nwhere $\\mathbb{I}(y)$ is an indicator function that is 1 when the condition is true, and 0 otherwise, $r_m$ represents the number of predictions falling within the interval. Finally, $M_{QIs}$ represents the number of the chosen quantile intervals. In the equation, $y_n$, is the actual value for the n-th data point, $\\hat{y}_{n,k}$ represents the k-th predicted value for the n-th data point, and $\\mathbb{I}(\\hat{y}_{n,k})$ represents whether the predicted value falls within the m-th quantile interval. Thus, $Y_n$ can be written as:\n$Y_n = \\mathbb{I}(\\hat{y}_{n,k} \\in QIs_m)$ (3)\nwhere $\\hat{y}_{n,k} = \\mathbb{I}(\\hat{y}_{n,k} \\in \\hat{y}_{n_{lowm}}, \\hat{y}_{n_{highm}})$ and $\\hat{y}_{n_{lowm}}$ and $\\hat{y}_{n_{highm}}$ represents the lower and upper bound of the m-th quantile interval for $y_n$. When the true distribution aligns with the predicted distribution, $\\frac{M}{M_{QIs}}$ of the observation data will fall into each QI, achieving an optimal QICE value of 0. Smaller values of QICE indicate a closer match between the learned distribution and the true distribution.\nIS. The IS is a scoring metric designed to evaluate the quality of probabilistic prediction intervals (PIs). It is defined and systematically studied in [16]. This metric jointly considers two critical aspects: the sharpness and coverage ability of the prediction interval. The computation of IS can be formulated as follows:\n$IS = \\frac{1}{N} \\sum_{n=1}^{N} (u_n - l_n) + \\frac{2}{\\alpha} (l_n - y_n) \\mathbb{I}_{y_n<l_n}+\\frac{2}{\\alpha} (y_n-u_n) \\mathbb{I}_{y_n>u_n},$ (4)\nwhere $u_n$ and $l_n$ are the upper and lower endpoints of the central (1-\u03b1) \u00d7 100% prediction interval for the n-th data point, which and are the predictive quantiles at levels and respectively; yn is the actual value corresponding to the n-th data point. The final prediction results are rewarded for having a narrower prediction interval. If the observed value falls outside the prediction interval, a penalty is incurred, and the severity of the penalty is determined by the parameter \u03b1. Thus, a lower IS indicates better performance."}, {"title": "4 METHODOLOGY", "content": "To achieve effective probabilistic predictions, we propose COST that simultaneously leverages the advantages of both deterministic and probabilistic models. Our approach involves two stages. In the first stage, the deterministic model is pretrained to predict the conditional mean that captures the primary patterns. In the second stage, the parameters of the deterministic model are frozen, and the scale-aware diffusion model, constrained by a customized fluctuation scale, is jointly trained to model residual distributions that reflect random fluctuations. Figure 2 illustrates an overview of our model."}, {"title": "4.1 Mean-Residual Decomposition", "content": "For urban spatiotemporal probabilistic prediction, current approaches typically employ a single probabilistic model to capture the full distribution of data, incorporating both the primary spatiotemporal patterns and the random fluctuations. However, it is challenging to model both of these distributions. Inspired by [32] and the Reynolds decomposition in fluid dynamics [35], we propose to decompose the target data $x_t$ as follows:\n$x_t = E[x_t | x_0] + (x_t \u2013 E[x_t | x_0])$,  (5)\n$:=\u00b5(Deterministic) :=r(Probabilistic)$\nwhere u is the conditional mean representing the primary patterns, and r is the residual representing the random variations. Our core idea is that if a deterministic model can accurately predict the conditional mean, that is, \u00b5 \u2248 Ee [xta|xco], then the probabilistic model only needs to focus on learning the simpler residual distribution, thus combining the strengths of both models to enhance the probabilistic prediction capability."}, {"title": "4.2 Mean Prediction via Deterministic Model", "content": "We require a deterministic model that can accurately predict the conditional mean to align with our research hypothesis, and also maintain high predictive efficiency to avoid additional increases in training and inference time. Therefore, we select the MLP-based STID model as our mean prediction module. In the first stage of training, we pretrain the model for 50 epochs to effectively capture the primary spatiotemporal patterns. Specifically, we input historical conditional data $x_0$ into the STID model to obtain the conditional mean output $Ee [x_t|x_0]$.\nThe STID model is pretrained by optimizing the following loss function:\n$L_2 = ||[Ee [x_t|x_0] - x_t||^2 $  (6)"}, {"title": "4.3 Residual Learning via Diffusion Model", "content": "Diffusion models have achieved significant success in probabilistic modeling. In this work, we employ a diffusion model for probabilistic prediction, training it to learn the residual distribution:\n$r_t = x_t - E_\u03b8[x_t|x_0].$  (7)\nConsequently, the target data $x_t$ for diffusion models in Eqs. (8), (2), and (3) is replaced by $r_t$. The residual distribution of urban spatiotemporal data is not independently and identically distributed (i.i.d.) nor does it follow a fixed distribution, such as N(0, \u03c3). Instead, it often exhibits complex spatiotemporal dependence and heterogeneity. So we consider both temporal residual learning and spatial residual learning.\nTemporal Residual Learning. For urban spatiotemporal data, the residual distribution varies at different time points. For example, fluctuations are lower at night and higher during the day, with uncertainty varying between weekends and weekdays. To model this, we incorporate the timestamp information as the condition for the denoising process. Meanwhile, the residual distribution can also be affected by sudden weather changes or public events. To capture these real-time changes, we concatenate the context data x0 with noised target residual $r_t$ as the input. The noise is not added to $x_0$ and $r_t$ during the diffusion training and inference.\nSpatial Residual Learning. In areas with frequent traffic accidents, fluctuations tend to be more pronounced and may induce anomalous variations in adjacent regions, thus affecting their distributions. For spatial dependence modeling, the model learns a spatial embedding for each location, following the STID approach. Additionally, we propose a scale-aware diffusion process to further distinguish the heterogeneity for different regions. In this section, we detail the calculation of Q and how it is integrated into the scale-aware diffusion process.\nSpecifically, we apply the Fast Fourier Transform (FFT) to spatiotemporal sequences in the training set to quantify fluctuation levels in different regions and use the custom scale Q as input to account for spatial differences in residual. Specifically, we first employ FFT to extract the fluctuation components for each region within the training set. The detailed steps are as follows:\n$A_k = |FFT(x)|_k, k = \u2220(FFT(x))_k,$\n$A_{max} = \\max_{k\u2208\\{1,...,[L/2]+1\\}} A_k,$\n$K = \\{k: A_k< 0.1 \u00d7 A_{max}\\},$\n$x_r[i] = \\sum_{k\u2208K} \\frac{A_k}{ + k} cos(2\u03c0fi + \u03c6k)$\nwhere $A_k, k$ reprent the amplitude and phase of the k-th frequency component. L is the temporal length of the training set. $A_{max}$ is the maximum amplitude among the components, obtained using the max operator. K represents the set of indices for the selected residual components. $f_k$ is the frequency of the k-th component. represent the conjugate components. $x_r$ ref to the extracted residual component of the training set. We then compute the variance of of the residual sequence for each location k and expand it to match the shape as $r_t \u2208 R^{B\u00d7K\u00d7P}$, where B represents the batch size. And we can get the variance tensor M:\n$M_{b,k,p} = \u03c3$ (8)\nThen Q is used as the input of the denoising network.\nThe vanilla diffusion models typically model all regions as the same N(0, I) distribution at the end of the diffusion process, failing to distinguish the differences among regions. To further model the differences of residual distribution across various regions, we adopt the technique proposed by [18] to make the residual learning region-specific conditioned on Q. Specially, we have calculated the customized fluctuation scale Q, and We redefined the noise distribution at the endpoint of the diffusion process as follows:\nAccordingly, the Eq 13 in the forward process is rewritten as:\nAnd in the denoising process, we sample $r_N$ from N(Q, I), and denoise it use Eq (2), the computation of $\u00b5_\u03b8(r_n, n|x_0)$ in Eq 2 is modified as:\nThis approach enables the diffusion process to be governed by the Q values at each region, leading to more effective utilization of the customized scale conditions."}, {"title": "(i) Customized Fluctuation Scale", "content": "Specifically, we apply the Fast Fourier Transform (FFT) to spatiotemporal sequences in the training set to quantify fluctuation levels in different regions and use the custom scale Q as input to account for spatial differences in residual. Specifically, we first employ FFT to extract the fluctuation components for each region within the training set. The detailed steps are as follows:\n$A_k = |FFT(x)|_k, k = \u2220(FFT(x))_k,$\n$A_{max} = \\max_{k\u2208\\{1,...,[L/2]+1\\}} A_k,$\n$K = \\{k: A_k< 0.1 \u00d7 A_{max}\\},$\n$x_r[i] = \\sum_{k\u2208K} \\frac{A_k}{ + k} cos(2\u03c0fi + \u03c6k)$"}, {"title": "(ii) Scale-aware Diffusion Process.", "content": "The vanilla diffusion models typically model all regions as the same N(0, I) distribution at the end of the diffusion process, failing to distinguish the differences among regions. To further model the differences of residual distribution across various regions, we adopt the technique proposed by [18] to make the residual learning region-specific conditioned on Q. Specially, we have calculated the customized fluctuation scale Q, and We redefined the noise distribution at the endpoint of the diffusion process as follows:\n$p(r) = N(Q, I),$  (9)\nAccordingly, the Eq 13 in the forward process is rewritten as:\nr = + \u221a1 - \n (10)\nAnd in the denoising process, we sample $r_N$ from N(Q, I), and denoise it use Eq (2), the computation of in Eq 2 is modified as:\n (11)\nThis approach enables the diffusion process to be governed by the Q values at each region, leading to more effective utilization of the customized scale conditions."}, {"title": "4.4 Training and Inference", "content": "Training. Our training process is a two-stage procedure. We first pretrain the deterministic model STID to enable it to predict the conditional mean. Subsequently, we train the diffusion mode to learn the distribution of residuals, where the residuals are calculated as the difference between the true values and the conditional mean predicted by the pretrained STID model with frozen parameters. The detailed training procedure is presented in Algorithm 1.\nInference. The inference process of the model consists of two paths: one utilizes the pretrained STID model to predict the conditional mean, while the other uses the diffusion model to predict the residuals. The final sample is obtained by summing the results of both paths. This process is detailed in Algorithm 2."}, {"title": "5 PERFORMANCE EVALUATIONS", "content": "To validate the effectiveness and generalizability of our method, we use seven real-world datasets across four categories, each with different spatiotemporal granularities, covering information on crowd flow (CrowdBJ and CrowdBM), cellular network traffic (CellularNJ and CellularSH), vehicle flow (TaxiBJ and BikeDC), and traffic speed (Los-Speed). All datasets are split into training, validation, and test sets in a 6:2:2 ratio, and all datasets are standardized during training. Detailed information on the datasets can be found in Table 6.\nBaselines. we select six of the most widely recognized and state-of-the-art models in the spatiotemporal domain as our baselines, including:"}, {"title": "5.1 Experimental Settings", "content": "Datasets. To validate the effectiveness and generalizability of our method, we use seven real-world datasets across four categories, each with different spatiotemporal granularities, covering information on crowd flow (CrowdBJ and CrowdBM), cellular network traffic (CellularNJ and CellularSH), vehicle flow (TaxiBJ and BikeDC), and traffic speed (Los-Speed). All datasets are split into training, validation, and test sets in a 6:2:2 ratio, and all datasets are standardized during training. Detailed information on the datasets can be found in Table 6.\nBaselines. we select six of the most widely recognized and state-of-the-art models in the spatiotemporal domain as our baselines, including:"}, {"title": "5.2 Short-Term Prediction", "content": "Setups. We define the short-term prediction task as a 12-step ahead prediction based on the previous 12 steps, following [42, 46]. Since the temporal granularity varies across datasets, the actual time duration corresponding to these 12 steps differs accordingly.\nResults of Probabilistic Metrics.\nResults of Deterministic Metrics."}, {"title": "5.3 Long-Term Prediction", "content": "Setups. In addition to the 12-step prediction task which is the most typical setup for urban spatiotemporal prediction, we also conduct long-term prediction experiments on four. We define the long-term prediction task as a 64-step ahead prediction based on the previous 64 steps, following [24, 33, 52].\nResults of Probabilistic Metrics.\nResults of Deterministic Error Metrics."}, {"title": "5.4 Ablation Study", "content": "we conduct an ablation on each proposed module in our model to further evaluate the influence. Specifically, we constructed three variant models by progressively removing key components from the original architecture:(w/o c): The scale-aware diffusion process module is removed; (w/o q): The customized fluctuation scale is excluded as a prior condition; (w/o m): The conditional mean prediction module is eliminated, allowing the model to rely solely on the diffusion model for training."}, {"title": "5.5 Model Efficiency", "content": "We evaluated the training time of our model and all baseline models on the CellularSH dataset, as well as the time required to perform 50 sampling iterations on the test set. Notably, the pretraining time of the mean prediction module in our model is also included in the evaluation. The experimental results are summarized in Table 5. As shown in Table 5, our model demonstrates a significant advantage over existing probabilistic models in terms of training and inference efficiency. This efficiency is particularly valuable for real-world deployment scenarios. It is worth noting that while the CSDI model benefits from the Transformer architecture's capability to capture spatiotemporal dependencies, its training and sampling time is considerably higher than that of other models, posing challenges for time-sensitive tasks."}, {"title": "5.6 Analysis of Distribution Alignment", "content": "For regions with complex distributions in the CellularSH dataset, we validate the model's ability to capture multi-modal characteristics by comparing the predicted and true distributions using Kernel Density Estimation (KDE)."}, {"title": "6 CONCLUSION", "content": "In this work, we propose to collaborate deterministic models and probabilistic models for effective probabilistic urban spatiotemporal predictions. By decomposing spatiotemporal data into conditional mean predicted by deterministic models and residual learned by diffusion models, bridging the gap between deterministic and probabilistic predictions, our CoST achieves dual advantages: precise capture of primary patterns and non-linear variations. Experiments across seven real-world datasets demonstrate 20% performance gains over state-of-the-art baselines. This study provides an efficient solution for urban spatiotemporal prediction that balances pattern learning and uncertainty modeling, holding significant theoretical and practical value for smart city decision support."}]}