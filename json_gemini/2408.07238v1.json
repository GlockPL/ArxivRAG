{"title": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge Distillation Approach", "authors": ["Tong Wang", "K. Sudhir", "Dat Hong"], "abstract": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex\nhuman-like interactions. But they are costly, or too large for edge devices such as smartphones and harder\nto self-host, leading to security and privacy concerns. This paper introduces a novel interpretable knowledge\ndistillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host.\nWe study this problem in the context of building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge distillation, where the \"student\"\nmodel learns directly from the \"teacher\" model's responses via fine-tuning, our interpretable \"strategy\"\nteaching approach involves the teacher providing strategies to improve the student's performance in various\nscenarios. This method alternates between a \"scenario generation\" step and a \"strategies for improvement\"\nstep, creating a customized library of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence it can be used without\nmanipulating model parameters. In our customer service application, the method improves performance, and\nthe learned strategies are transferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit.", "sections": [{"title": "Introduction", "content": "Advancements in large language models (LLMs) have enabled low-cost automation of many\nmarketing tasks. Choosing the right LLM for a task involves balancing cost, feasibility,\nand performance. While high-performance models like GPT-4 are appealing and may seem\nlike the obvious choice, many businesses prefer smaller, more affordable, or open-source\nLLMs. In high volume applications such as customer service, even small cost differences\nper query can lead to large differences in total cost; as such cheaper or free models like\nLlaMa may offer a better balance in tradeoffs, despite lower performance. Additionally,\ndata privacy concerns drive firms to self-host LLMs instead of using third-party APIs,\nwhich could compromise data confidentiality. Consequently, there is growing interest in\nenhancing the performance of smaller, cost-effective LLMs.\nThis paper examines whether we can augment the effectiveness of smaller, but more eco-\nnomical LLMs using the knowledge embedded in larger and superior LLMs. This concept\nis called knowledge distillation, where a more advanced LLM, referred to as the \u201cteacher,\u201d\nimparts its knowledge to a less sophisticated LLM, termed the \u201cstudent,\u201d to enhance its\nperformance.\nWhile enhancing a weaker LLM through a more advanced LLM is a general problem\napplicable in many contexts, in this paper, we illustrate it for the problem of goal-oriented\ndialogue tasks (e.g. Wei et al. 2018, Bordes et al. 2016). Goal-oriented dialogues aim to steer\nconversations toward specific desired outcomes (Ham et al. 2020a); as such it is relevant\nfor conversations involving negotiations (Samad et al. 2022) and persuasion (Wang et al.\n2019). Specifically, we consider a customer service application, where an LLM acts as a\ncustomer service agent that interacts with the customer with the goal of achieving high\ncustomer satisfaction."}, {"title": null, "content": "A common strategy for improving student performance through knowledge transfer from\na teacher is fine-tuning, where the teacher generates training data to train the student\n(e.g., Tang et al. 2019, Agarwal et al. 2023, 2024). However, this method has significant\nlimitations. Firstly, fine-tuning requires access to and updates of model parameters, which\nis not always feasible, especially for LLMs that only allow API access; and also costly\nsince it involves updating billions of parameters. Secondly, the distilled \u201cknowledge\" is\nencapsulated in model parameters, making it unintelligible to humans. This opacity hin-\nders debugging and raises safety concerns, particularly when the teacher is an external,\nunverifiable model. The inaccessibility of distilled knowledge complicates maintenance, as\ntracking changes and ensuring consistency with previous iterations can be problematic.\nFinally, fine-tuning focuses only on lexical similarities in the outputs, ignoring the under-\nlying strategies that lead a model to produce specific responses and the fact that different\nlexical choices can express identical meanings or follow the same response strategy.\nA student model can also be improved via prompt tuning (Lester et al. 2021, Hu et al.\n2021), also known as soft prompt tuning), where specific prompt tokens are fine-tuned\nto elicit better performance. This involves adjusting the prompt to align with desired\noutputs, enhancing the model's ability to generate accurate and contextually appropriate\nresponses. However, prompt tuning lacks interpretability, as the prompts are represented\nas embeddings that are hard to understand. Additionally, a single, fixed prompt tuned for\nan entire task cannot capture the richness and diversity of strategies needed for different\nscenarios. Furthermore, prompt tuning requires access to the LLM's internals, making it\ninapplicable to LLMs that are only accessible via APIs.\nTherefore, we propose an interpretable knowledge distillation method. Instead of directly\ndistilling teacher's \u201cknowledge\" into student's model parameters, which would require"}, {"title": null, "content": "accessing and updating the student model, our method constructs a knowledge base that\nthe student can query externally without altering the student model itself. This knowledge\nbase, which we refer to as a library, consists of representative scenarios that the student\ncould encounter during deployment, along with corresponding strategies. Each scenario\nis represented by an on-going dialogue between an agent and a customer, and the corre-\nsponding strategies guide the student on how to respond when continuing the dialogue.\nThen, during deployment, the library functions similarly to retrieval-augmented generation\n(RAG) (Lewis et al. 2020); the student LLM identifies the most relevant scenarios through\nembeddings and applies the corresponding strategies to stimulate an appropriate response.\nBuilding a library for a multi-step, goal-oriented dialog problem like customer service\npresents additional challenges. First, generating scenarios involves interactive dialogues\nbetween the teacher and a customer to capture diverse conversational paths. However, dur-\ning deployment, the student cannot perfectly replicate the teacher's quality of interactions,\nleading to deviations. In multi-step settings such as conversations, even minor quality dif-\nferences from the teacher's responses at a given stage can lead to more negative customer\nresponse, and this negativity can cumulatively amplify over the entire conversation. As\nsuch, a student-customer conversation can veer into unencountered scenarios if the library\nis constructed based on only teacher-customer conversation scenarios. This phenomenon,\nknown as distribution shift, poses a significant challenge when the student's training data\nis generated exclusively from the teacher. Second, the strategies in the library must be\nadaptable to each student and specific scenario. These strategies should guide the student's\nbehavior by considering the student's current capabilities and providing targeted sugges-\ntions. Finally, a third important issue is that agents must adhere to firm policies, business\nrequirements, and cost constraints when addressing customer requests. Thus, these policies"}, {"title": null, "content": "and constraints must be integrated into the strategy development process to ensure the\nagent's solutions are practical and compliant with firm policies.\nWe design a novel method to address the aforementioned challenges. Our method is an\niterative process where, in each iteration, a new batch of scenarios and their corresponding\nstrategies are added to the library. To generate scenarios, both the student and teacher\ninteract with the environment (e.g., a donor in a persuasion task or a customer in customer\nservice) to produce conversations and sample scenarios. To address the issue of distribution\nshift, we ensure that enough interactions involve the student by gradually increasing the\nprobability of selecting the student for scenario generation, so that the student eventually\ndominates this process. In the strategy learning phase, strategies are generated and refined\niteratively. The teacher LLM evaluates both its own and the student's responses, providing\ntargeted strategies for the student to follow. These strategies are incorporated into prompts\nfor subsequent refinement rounds, progressively honing the student's ability to mimic the\nteacher. The library grows iteratively, with the teacher monitoring the student's progress\nand deciding when to terminate the process based on the need for further feedback or\nsignificant improvement. The output is a customized library of scenarios and strategies,\ncontaining context-specific knowledge from the teacher that is most relevant to the student\nand the task.\nOur interpretable knowledge distillation approach offers several advantages over fine-\ntuning. First, by teaching strategies rather than responses, our method enables LLMs to\nunderstand how to handle different scenarios at a strategic level, providing a global view of\nthe problem rather than simply mimicking lexical choices. Second, our method ensures easy\ntransferability across LLMs and contexts. The strategy library allows LLMs to adapt to\nnew, unseen situations, broadening their problem-solving capabilities. This library can be"}, {"title": null, "content": "used by other student LLMs or for related customer service problems, potentially enhanc-\ning performance without direct training the student LLMs or on specific tasks. Third, the\ninterpretability of our approach significantly enhances AI safety. By extracting explicit\nstrategies, domain experts can review and understand the LLMs'decision-making pro-\ncesses. This transparency facilitates trust and provides safeguards against misuse, errors, or\nadversarial influences. Finally, as our method does not require access to or modification of\nthe student or teacher LLMs' parameters (both can be queried solely via black-box access,\nsuch as APIs), it distinguishes itself from traditional knowledge distillation methods such as\nprompt tuning and fine-tuning, which require internal model access or parameter updates.\nThis makes it particularly suitable for environments where direct model manipulation is\nimpractical or restricted.\nBased on our empirical application, i.e., multi-turn conversations in customer service,\nour key findings are as follows: First, teaching strategy is more effective than teaching\nresponses for multi-turn generation. Second, context-specific strategies are more effective\nthan global strategies, since the former can provide more targeted strategies for different\nscenarios. Third, even though the library is learned for a particular student LLM and\nspecific contexts, it contains common knowledge that is transferrable across models and\nacross contexts."}, {"title": null, "content": "The rest of this paper is organized as follows: \u00a72 discusses the related literature on LLM\nand knowledge distillation. \u00a73 presents the proposed method, while \u00a74 evaluates it with\nan extensive set of experiments. \u00a75 concludes the paper. More analyses are included in the\nonline appendix."}, {"title": "Related Work", "content": "Our work contributes to the recent but growing literature on marketing applications using\nLLMs. This body of research has typically focused on how LLMs can be used for market"}, {"title": null, "content": "research and the study of human behavior, while also highlighting attendant challenges\n(Gui and Toubia 2023, Qiu et al. 2023, Horton 2023)). Specific marketing research appli-\ncations include perceptual maps (e.g., Li et al. 2024) and conjoint analysis (e.g., Brand\net al. 2023, Gui and Toubia 2023). In contrast, this paper is focused on how to effectively\nadapt and engineer an LLM for marketing tasks such as customer service. We will next\ndiscuss how our method relates to existing literature on knowledge distillation in LLMs and\nadvances the literature on goal-oriented dialogs, particularly in multi-turn interactions."}, {"title": "Knowledge Distillation for LLM", "content": "The concept of utilizing a superior model to enhance a less powerful one is known as\nknowledge distillation. This technique was first introduced by Hinton et al. (2015) in the\ncontext of supervised learning and has been adapted for use with language models in\nrecent years (Sanh et al. 2019, Sun et al. 2019). In the realm of language models, knowl-\nedge distillation involves using a larger and more capable LLM to generate data that\ntrains specialized, smaller models. Existing research on knowledge distillation for language\nmodels typically employs objective functions that either maximize the likelihood of high-\nprobability sequences generated by the teacher model (Kim and Rush 2016) or guide the\nstudent model to mimic the token-level probability distributions provided by the teacher\n(Sanh et al. 2019). Some recent work also proposes to teach the student the rationale for\nsolving a task (Hsieh et al. 2023, Magister et al. 2022). However, all these methods require\ntraining the student model and updating its parameters. In our approach, however, we\ndistill the knowledge into an external library that the student can query during inference,\nwithout the need for training the student model. The teacher's knowledge is utilized by\nthe student through retrieval-augmented generation (RAG), another popular technique in\nLLM. In doing so, our method requires only black-box access to the student model, such\nas through an API, which is not feasible with existing knowledge distillation techniques."}, {"title": "Goal-Oriented Dialogues", "content": "Recent advancements in LLMs have significantly improved their application in complex\ngoal-oriented dialogues (e.g., Ham et al. 2020b, Li et al. 2023, Snell et al. 2022), but\nchallenges and limitations remain. First, smaller LLMs often lack a strategic understanding\nof overall dialogue progression and fail to achieve dialogue objectives through multi-turn\ninteractions (Cheng et al. 2024, Deng et al. 2023). Second, the multi-step nature of goal-\noriented dialogues fundamentally differs from one-step tasks like text classification and\nsummarization as fine-tuning at each utterance level overlooks the interdependence of\nmulti-turn utterances and the high-level strategy. Zhang et al. (2023) proposes an \u201cAsk\nan Expert\" solution, where a lesser model seeks advice from a better LLM for generating\nutterances, but this increases inference (i.e., response) time. Finally, some approaches rely\non dialogues with specifically annotated strategies for training (e.g., Zhang et al. 2022,\n2023, Joshi et al. 2021), but the dependence on labeled datasets creates a significant barrier\nfor practical application."}, {"title": "Library-based Interpretable Knowledge Distillation", "content": "Given a student LLM, denoted as S(\u00b7) (e.g., LlaMa 2 or GPT-3.5), our method involves\ncreating a library consisting of a set of representative scenarios, paired with corresponding\nstrategies constructed by a teacher LLM T(\u00b7) optimized for instructing the students on how\nto respond in those scenarios. We first set up the learning environment and then describe\nthe algorithm. Then, we will show how the library is used during deployment and explain\nthe benefits of our method."}, {"title": "Learning Environment", "content": "We set up a learning environment where the student attempts to improve its performance\nby mimicking the teacher. For each input, the teacher compares the student's output with"}, {"title": "Simulating Customers", "content": "To simulate the customer, we describe the task in the\nprompt where we request the LLM to role-play as a customer calling an airline company to\nrequest customer service. Here, we focus on a specific context where the customer bought\na restricted ticket (non-changeable and non-refundable) and requests to cancel it without\npenalty.\nTo increase the heterogeneity of the customers and the richness of the conversations,\nwe vary the customer's social styles, initial emotions, and difficulty. For social styles,\nwe use four types based on the classification from Merrill and Reid (1981). (i) Driver:\nresults-driven, confident, and assertive; (ii) Analytical: detail-oriented, systematic, and log-\nical; (iii) Amiable: cooperative, empathetic, and relationship-focused; and (iv) Expressive:\nenthusiastic, creative, and spontaneous\u00b9. For initial emotions, we set four different cus-\ntomer emotions when initiating the call: calm, confused, concerned, and frustrated, leading\nto varied dialogue developments. Additionally, we vary the difficulty level of the customer\nby including/not including the keyword \u201cdemanding\u201d in the customer role description to\nthe LLM. We observe that including \u201cdemanding\u201d changes customer behavior, making\ncustomers more persistent with their requests. We use q(s,e,d) to represent the prompt\nfor the customer LLM, parameterized by the social style s, initial emotion e, and difficulty\nlevel d."}, {"title": "Simulating Teacher", "content": "We then describe how to prompt a teacher LLM to act\nas an agent. Here we choose the state-of-the-art LLM, GPT-4, as the teacher. We define\na base prompt instructing GPT-4 to role-play as a customer service agent. We denote the\nprompt as Pbase, which contains three key components: role, goal, and constraints.\nThe role specifies the position or function that GPT-4 needs to assume. For example,\nprompting GPT-4 to\u201crole-play as a customer service agent\" sets the context for the\ninteraction, guiding the model to generate responses suitable for customer service scenarios.\nThe goal establishes the desired outcome and ultimate objective of the interaction, guiding\nGPT-4 to adjust its strategies accordingly. For instance, if the goal is to achieve high\ncustomer satisfaction, GPT-4 will tailor its responses to be more empathetic, helpful, and\nsolution-focused. Including the goal in the prompt is essential because it provides clear\ndirection for the LLM, ensuring that its responses align with the intended results. The\nconstraint establishes the limitations and boundaries that the LLM needs to follow, and\nas explained earlier, it helps to ensure that the LLM accounts for business constraints and\nfirm policy. As such, the agent is instructed to adhere to these constraints while aiming to\nachieve high customer satisfaction. This ensures that all agents follow the same company\npolicy, differing only in their communication strategies, which the teacher aims to teach\nthe student.\nThe base prompt is fixed for the entire task and included in the prompt for both the\nteacher and student."}, {"title": "Knowledge Distillation", "content": "Our approach iterates over three steps: scenario generation, strategy teaching, and\ngoal evaluation, progressively building up a customized library consisting of scenarios\nand strategies for handling them. We call each execution of the three steps as one iteration\nof the algorithm. We use L(t) = {(st), Pstrategy (Sat)))} to represent the library at the end of\niteration t, where s represents a scenario indexed by i, Pstrategy (st)) (Si is the corresponding\nprompt, and nt is the total number of scenarios in the library at iteration t. As t increases,\nthe library grows larger. Below, we detail each step."}, {"title": "Scenario Generation", "content": "In this step, the LLM agent interacts with the customer\nC(.) to generate conversations, denoted as x(t,t), where t is the iteration index and I is the\nconversation index. x(t,l) consists of a sequence of utterances from the customer C(.) and\nthe customer service agent LLM (e.g., T or S):\n$$x^{(t,l)} = (a_1^{(t,l)}, c_1^{(t,l)}, a_2^{(t,l)} ...),$$\nwhere aft,l) represents the agent's utterance at the k-th turn and cft,l) represents the cus-\ntomer's utterance at the k-the turn. Without loss of generality, we assume a conversation\nalways starts with the agent's utterance, for example, \u201cHello, how may I help you?\u201d\nSince the generation of an utterance depends on all previous exchanges, we define a\nsubconversation with the first k turns, ending with the customer's utterance.\n$$x_{[1:k]}^{(t,l)} = (a_1^{(t,l)}, c_1^{(t,l)}, ..., a_k^{(t,l)}, c_k^{(t,l)}).$$ \nThe generation of the teacher's utterance at the k-th turn is based on the prior conversation\n(t,l)\nx[k]\nas well as the base prompt Pbase:\n$$a_k^{(t,l)} = T(x_{[1:k]}^{(t,l)} | Pbase).$$"}, {"title": null, "content": "The customer's utterance is determined by the prior conversation and the parameterized\nprompt q(s,e,d):\n$$c_k^{(t,l)} = C(x_{[1:k-1]}^{(t,l)}, a_{k-1}^{(t,l)} | q(s,e,d)).$$\nFor each iteration t, we generate a set of conversations, denoted as X(t), by varying\nthe customer prompt parameters s(social styles), e(initial emotions) and d (difficulty) and\nrunning multiple times for each set of parameters with a non-zero temperature.\nSubconversations, defined in Equation (2), are randomly sampled from each conversation\nin X(t). We call these subconversations scenarios. Each scenario includes the entire prior\nconversation ending with the customer's utterance. We use S(t) to represent the set of\nscenarios sampled from X(t).\nIn a naive solution, the conversations in X(t) are generated by T interacting with C.\nHowever, each scenario extracted from X(t) reflects the specific dynamics and decisions\nmade by the teacher T, not necessarily those a student would encounter or make when\ninteracting independently in similar contexts. This leads to compounding errors because the\nscenarios the student encounters during deployment will differ from those it observed during\ntraining. In a multi-step decision-making process, even a small deviation in one step can\ncompound over subsequent steps, leading to significant differences at the conversation level.\nThis mismatch between training and deployment scenarios is a significant for challenge\nmulti-step imitations, often referred to as distribution shift (Pomerleau 1991, Ross and\nBagnell 2010).\nTo overcome this challenge, we let the student participate in the scenario generation\nstep. We assign a probability p for the teacher to be selected to interact with the customer\nfor conversation generation and 1 \u2212 p for the student to be selected. Initially, p is set to 1\nin iteration 1 and gradually decreases in subsequent iterations."}, {"title": null, "content": "In each iteration, when the student is selected to interact with the customer LLM, it\nwill produce an output based on the scenarios and strategies retrieved from the current\nlibrary:\n$$a_k^{(t,l)} = S(x_{[1:k]}^{(t,l)} | Pbase, L^{(t)}).$$ \nWe will explain how to retrieve the relevant strategy from the library in Section 3.3.\nAs the iterations progress, more scenarios will be accumulated into L(t), covering a more\ndiverse set of situations the student could encounter."}, {"title": "Strategy Teaching", "content": "In this step, the teacher iteratively generates and refines\nstrategies for each scenario until the student accurately mimics the teacher's output for\nthat scenario. See Figure 1 for an illustration of the process. We call the prompt generated\nat this step the strategy prompt. The strategy prompt will be combined with the base\nprompt to instruct the student how to behave like the teacher.\nSpecifically, for each scenario, denoted as s, both T and S generate a response. These\nresponses are then evaluated by T, which identifies discrepancies and proposes updates\nto the strategy. The updates are suggestions made to the student instructing what they\nshould or should not do, based on their current output. They can vary from general, such"}, {"title": "Termination Mechanism", "content": "Our main algorithm is an iterative process that grad-\nually grows the library by adding a new batch of scenarios and their corresponding strategy\nprompts at each iteration. The thorough coverage of the data space by the scenarios is\ncrucial for maintaining the robustness and reliability of the strategy prompts during deploy-\nment. To determine when the sufficiency is reached, we incorporate a goal evaluation step\nin the iterative process, where the student LLM is evaluated, based on the interactions\nwith the customer LLM, after each new batch is added to the library, to determine whether\nthe student LLM has achieved satisfactory performance using the current library.\nWe choose to use LLMs for the evaluation, motivated by recent research showing the\npotential of using LLMs to substitute human evaluations in various tasks, particularly\nin assessing other LLMs, with findings of up to 80% agreement with human judgment."}, {"title": "Deployment", "content": "The training algorithm generates a library comprising a set of scenarios and their associated\nstrategy prompts. We will now discuss how this library is utilized during deployment.\nAs shown in Equation (7), the student's output is determined by the scenario, the base\nprompt, and the strategy prompt optimized for that specific scenario during training.\nHowever, during deployment, the prior conversations differ from the training scenarios,\nand there is no predefined pstrategy for the test input. To address this, we employ a method\nakin to retrieval-augmented generation (Lewis et al. 2020), where we identify the most\nsimilar scenario(s) from a library based on their embeddings. The corresponding strategy\nfrom these similar scenarios is then applied to the new, unseen input, i.e.,\nstudent's output:\n$$a = S(s | pbase, Pstrategy(\\hat{s})), where \\hat{s} = arg \\min d(s, s').$$\nBy leveraging scenario embeddings, the library can efficiently match new, unseen inputs\nto the most similar scenarios, thereby providing a tailored strategy prompt.\nNotably, the use of the library is flexible. When retrieving strategies, it is not necessary\nto limit retrieval to a single closest scenario; instead, one can retrieve k > 1 scenarios from\nthe library, following the k-nearest neighbors approach. We provide a detailed analysis in\nAppendix A.2 and find that tuning k can further enhance performance."}, {"title": "Method Summary and Discussion", "content": "Intuitively, the library consists of a set of representative and prototypical \u201creference\" points\nwithin the data space. These reference points must correctly and adequately cover the data\nspace to ensure that when encountering a new scenario, there is a high likelihood that an\nexisting strategy can be effectively applied. The correct coverage necessitates the algorithm\nto prevent distribution shift, motivating the design of increasingly letting the student to\ngenerate scenarios in Section 3.2.1. The termination mechanism is in place to enforce\nadequate coverage, where the teacher LLM monitors the performance of the student and\nonly stops when no improvement is observed, indicating the sufficiency of the coverage.\nA key reason for the success of this method is the ability of the LLM to extrapolate,\nadapt, and apply a strategy designed for one scenario to a slightly different scenario. This\ncapability allows the LLM to bridge gaps between similar but distinct situations, ensuring\nthat the strategies remain effective even when faced with novel inputs. The LLM's inherent\nunderstanding of the nuances in language and context further strengthens its ability to\ngeneralize from the library's reference points, making it a powerful tool for dynamic and\ndiverse real-world applications. This ability further allows the library to be used in a\nslightly different context where it is not trained on, as we will demonstrate in Section 4.3.\""}, {"title": "Interpretability Benefits", "content": "One of the defining features of our methodology is its inter-\npretability: the teacher's knowledge is distilled into scenarios and prompts that are easily\nunderstood by humans. This interpretability offers several advantages over fine-tuning-\nbased methods. First, domain experts can verify the library before deployment, which is\nespecially important when the teacher is an external LLM over which the company has no\ncontrol, including its model and training data. In fine-tuning methods, a student model\ncan be easily contaminated under adversarial attacks, as it is unclear what knowledge has\nbeen distilled into the student. In contrast, our method keeps the student LLM's parame-\nters unchanged, storing external knowledge in an interpretable library where each piece of\n\"knowledge\u201d can be scrutinized and verified. This design makes our method more resilient\nto adversarial attacks. Second, a library-based knowledge distillation is easily editable.\nSuboptimal or outdated strategies can be quickly updated by human experts. Meanwhile,\nwhen new scenarios arise during deployment and the existing library lacks relevant strate-\ngies, new entries can be seamlessly added with strategies devised by either the teacher\nor domain experts. This flexibility ensures that the library remains current and effective\nacross various real-world applications."}, {"title": "Experiments", "content": "We evaluate our method on four student models, LlaMa-2 7b, LlaMa-2 13b, LlaMa-2 70b\nand GPT-3.5. We assess LlaMa models because they are open source and free except for\nstorage and computing cost. We assess GPT-3.5 because it can only be queried by API\nand the cost of use is only 5% of GPT-4 at $0.50 per 1M input tokens. This selection of\nstudent models also covers a large range of model size, from 7 billion (LlaMa-2 7b) to 175\nbillion (GPT-3.5).\nWith our experiments, we investigate the following research questions related to our\nproposed knowledge distillation method."}, {"title": null, "content": "Q1. Is teaching strategy more effective than teaching responses? To answer this question,\nfor each task, we compare our proposed strategy teaching approach against fine-tuning\nstudent LLMs using the same amount of data. Fine-tuning directly trains the student on\nthe scenario - response pairs without explicitly teaching strategies like our method does.\nQ2. Is a context-specific dynamic use of strategies provided by the advanced LLM better\nthan global strategies that are invariant across contexts? For this, we compare our dynamic\napproach against global strategies.\nQ3. How effective is our solution to fixing the distribution shift challenge? To answer this\nquestion, we create a baseline that works the same as ours with the only difference being\nthat the scenarios are all generated by the teacher interacting with the customer.\nQ4. Are learnt strategies transferable across multiple student LLMs and other task con-\ntexts? We assess whether a strategy learnt from teaching one student LLM works for a\ndifferent LLM and whether strategies learnt for one customer service context improves\nperformance in another context."}, {"title": "Main Results", "content": "Baseline Methods. To answer question Q1 above, we compare our approach with tradi-\ntional knowledge distillation via fine-tuning, where we first use the teacher LLM to interact\nwith the customer LLM to generate a set of conversations, and then extract subconversa-\ntions from it, just like how we sampled scenarios. The difference is that the student does\nnot participate in the data generation and no explicit strategies are provided. The scenar-\nios are used as the input and then we use the teacher's response as the output, creating a\ntraining dataset for fine-tuning. To answer Q2, we compare with a set of guidelines manu-\nally constructed, through iterations between the authors and GPT-4. These guidelines are\nglobal instructions that are used for every input the student encounters."}, {"title": "Evaluation Approach", "content": "We evaluate the LLMs at the conversation level in terms of the\nachievement of the pre-defined goal - customer satisfaction. Motivated by recent research\n(Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), we perform two\nsets of evaluations.\nWe first use advanced LLMs as an evaluator to rate each conversation from 1 (very\ndissatisfied) to 5 (very satisfied), using few-shot prompt where examples of rating 5 con-\nversations are provided by the teacher, GPT-4, interacting with the customer LLM. We\nthen use human evaluation where each participant does a blind comparison of GPT-4 and\na student LLM, based on their conversations with the same customer LLM.\nLLM as Evaluators. We use advanced LLMs to evaluate the conversations generated by\ndifferent LLM agents (teacher or student) interacting with the customer LLM. Here we use\nGPT-4, since it is the state-of-the-art LLM available today. We use few-shot learning when\nevaluating these conversations\nwe include examples in the prompt that are generated\nby the teacher (GPT-4) and the customer. Since different examples may lead to different\nratings, for robust evaluation, we perform the evaluation twice, using examples generated\nby the teacher interacting with \u201cdemanding\" customers and \u201cnon-demanding\" customers,\nrespectively. We report the average ratings of four student models trained by different\nmethods in Table 1.\nOur strategy imitation method performs consistently better than the baselines. First,\ncompared to global guidelines, our method completely dominates this baseline, performing\nbetter or equally in all experiments. Compared with fine-tuning, our method is better or\nequal except on the smallest LLM, LlaMa-2 7b. This is because smaller LLMs do not follow\ninstructions as well as larger LLMs. Thus the knowledge distilled from the teacher may\nnot be executed by the student as well as a larger student. This observation is consistent\""}, {"title": null, "content": "with the findings from the literature that prompt based approaches are more effective for\nthe LLM with more parameters (Lester et al. 2021).\nNote that when prompts use conversations with demanding customers as examples of a\nrating of 5, they tend to yield higher scores across all evaluation tables. This occurs because\nwhen we label these challenging interactions as a 5 in the prompt, we communicate to the\nLLM that even when a customer is persistent and difficult to console, the conversation still\nmerits a high score. This effectively sets a low bar for evaluation, leading to inflated ratings.\nConversely, if examples feature non-demanding customers where positive interactions result\nin a rating of 5, the LLM is more likely to focus on the customer's reactions when evaluating\nconversations. This establishes a higher standard for ratings.\nDifferent advanced LLMs can be used for evaluation. To assess robustness, we also use\nLlaMa-3 70B as an evaluator. See Appendix A.1 for details. In addition, we also evaluated\nretrieving more than 1 most similar scenarios to use their strategies during test. The results\nare shown in Appendix A.2.\nHuman Evaluation. We conduct a blind comparison of the teacher and student"}]}