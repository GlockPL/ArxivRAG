{"title": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge Distillation Approach", "authors": ["Tong Wang", "K. Sudhir", "Dat Hong"], "abstract": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex human-like interactions. But they are costly, or too large for edge devices such as smartphones and harder to self-host, leading to security and privacy concerns. This paper introduces a novel interpretable knowledge distillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host. We study this problem in the context of building a customer service agent aimed at achieving high customer satisfaction through goal-oriented dialogues. Unlike traditional knowledge distillation, where the \"student\" model learns directly from the \"teacher\" model's responses via fine-tuning, our interpretable \"strategy\" teaching approach involves the teacher providing strategies to improve the student's performance in various scenarios. This method alternates between a \"scenario generation\" step and a \"strategies for improvement\" step, creating a customized library of scenarios and optimized strategies for automated prompting. The method requires only black-box access to both student and teacher models; hence it can be used without manipulating model parameters. In our customer service application, the method improves performance, and the learned strategies are transferable to other LLMs and scenarios beyond the training set. The method's interpretabilty helps safeguard against potential harms through human audit.", "sections": [{"title": "1. Introduction", "content": "Advancements in large language models (LLMs) have enabled low-cost automation of many marketing tasks. Choosing the right LLM for a task involves balancing cost, feasibility, and performance. While high-performance models like GPT-4 are appealing and may seem like the obvious choice, many businesses prefer smaller, more affordable, or open-source LLMs. In high volume applications such as customer service, even small cost differences per query can lead to large differences in total cost; as such cheaper or free models like LlaMa may offer a better balance in tradeoffs, despite lower performance. Additionally, data privacy concerns drive firms to self-host LLMs instead of using third-party APIs, which could compromise data confidentiality. Consequently, there is growing interest in enhancing the performance of smaller, cost-effective LLMs.\nThis paper examines whether we can augment the effectiveness of smaller, but more economical LLMs using the knowledge embedded in larger and superior LLMs. This concept is called knowledge distillation, where a more advanced LLM, referred to as the \u201cteacher,\u201d imparts its knowledge to a less sophisticated LLM, termed the \u201cstudent,\u201d to enhance its performance.\nWhile enhancing a weaker LLM through a more advanced LLM is a general problem applicable in many contexts, in this paper, we illustrate it for the problem of goal-oriented dialogue tasks (e.g. Wei et al. 2018, Bordes et al. 2016). Goal-oriented dialogues aim to steer conversations toward specific desired outcomes (Ham et al. 2020a); as such it is relevant for conversations involving negotiations (Samad et al. 2022) and persuasion (Wang et al. 2019). Specifically, we consider a customer service application, where an LLM acts as a customer service agent that interacts with the customer with the goal of achieving high customer satisfaction."}, {"title": "2", "content": "A common strategy for improving student performance through knowledge transfer from a teacher is fine-tuning, where the teacher generates training data to train the student (e.g., Tang et al. 2019, Agarwal et al. 2023, 2024). However, this method has significant limitations. Firstly, fine-tuning requires access to and updates of model parameters, which is not always feasible, especially for LLMs that only allow API access; and also costly since it involves updating billions of parameters. Secondly, the distilled \u201cknowledge\" is encapsulated in model parameters, making it unintelligible to humans. This opacity hinders debugging and raises safety concerns, particularly when the teacher is an external, unverifiable model. The inaccessibility of distilled knowledge complicates maintenance, as tracking changes and ensuring consistency with previous iterations can be problematic. Finally, fine-tuning focuses only on lexical similarities in the outputs, ignoring the underlying strategies that lead a model to produce specific responses and the fact that different lexical choices can express identical meanings or follow the same response strategy.\nA student model can also be improved via prompt tuning (Lester et al. 2021, Hu et al. 2021), also known as soft prompt tuning), where specific prompt tokens are fine-tuned to elicit better performance. This involves adjusting the prompt to align with desired outputs, enhancing the model's ability to generate accurate and contextually appropriate responses. However, prompt tuning lacks interpretability, as the prompts are represented as embeddings that are hard to understand. Additionally, a single, fixed prompt tuned for an entire task cannot capture the richness and diversity of strategies needed for different scenarios. Furthermore, prompt tuning requires access to the LLM's internals, making it inapplicable to LLMs that are only accessible via APIs.\nTherefore, we propose an interpretable knowledge distillation method. Instead of directly distilling teacher's \u201cknowledge\" into student's model parameters, which would require"}, {"title": "3", "content": "accessing and updating the student model, our method constructs a knowledge base that the student can query externally without altering the student model itself. This knowledge base, which we refer to as a library, consists of representative scenarios that the student could encounter during deployment, along with corresponding strategies. Each scenario is represented by an on-going dialogue between an agent and a customer, and the corresponding strategies guide the student on how to respond when continuing the dialogue. Then, during deployment, the library functions similarly to retrieval-augmented generation (RAG) (Lewis et al. 2020); the student LLM identifies the most relevant scenarios through embeddings and applies the corresponding strategies to stimulate an appropriate response.\nBuilding a library for a multi-step, goal-oriented dialog problem like customer service presents additional challenges. First, generating scenarios involves interactive dialogues between the teacher and a customer to capture diverse conversational paths. However, during deployment, the student cannot perfectly replicate the teacher's quality of interactions, leading to deviations. In multi-step settings such as conversations, even minor quality differences from the teacher's responses at a given stage can lead to more negative customer response, and this negativity can cumulatively amplify over the entire conversation. As such, a student-customer conversation can veer into unencountered scenarios if the library is constructed based on only teacher-customer conversation scenarios. This phenomenon, known as distribution shift, poses a significant challenge when the student's training data is generated exclusively from the teacher. Second, the strategies in the library must be adaptable to each student and specific scenario. These strategies should guide the student's behavior by considering the student's current capabilities and providing targeted suggestions. Finally, a third important issue is that agents must adhere to firm policies, business requirements, and cost constraints when addressing customer requests. Thus, these policies"}, {"title": "4", "content": "and constraints must be integrated into the strategy development process to ensure the agent's solutions are practical and compliant with firm policies.\nWe design a novel method to address the aforementioned challenges. Our method is an iterative process where, in each iteration, a new batch of scenarios and their corresponding strategies are added to the library. To generate scenarios, both the student and teacher interact with the environment (e.g., a donor in a persuasion task or a customer in customer service) to produce conversations and sample scenarios. To address the issue of distribution shift, we ensure that enough interactions involve the student by gradually increasing the probability of selecting the student for scenario generation, so that the student eventually dominates this process. In the strategy learning phase, strategies are generated and refined iteratively. The teacher LLM evaluates both its own and the student's responses, providing targeted strategies for the student to follow. These strategies are incorporated into prompts for subsequent refinement rounds, progressively honing the student's ability to mimic the teacher. The library grows iteratively, with the teacher monitoring the student's progress and deciding when to terminate the process based on the need for further feedback or significant improvement. The output is a customized library of scenarios and strategies, containing context-specific knowledge from the teacher that is most relevant to the student and the task.\nOur interpretable knowledge distillation approach offers several advantages over fine-tuning. First, by teaching strategies rather than responses, our method enables LLMs to understand how to handle different scenarios at a strategic level, providing a global view of the problem rather than simply mimicking lexical choices. Second, our method ensures easy transferability across LLMs and contexts. The strategy library allows LLMs to adapt to new, unseen situations, broadening their problem-solving capabilities. This library can be"}, {"title": "5", "content": "used by other student LLMs or for related customer service problems, potentially enhancing performance without direct training the student LLMs or on specific tasks. Third, the interpretability of our approach significantly enhances AI safety. By extracting explicit strategies, domain experts can review and understand the LLMs'decision-making processes. This transparency facilitates trust and provides safeguards against misuse, errors, or adversarial influences. Finally, as our method does not require access to or modification of the student or teacher LLMs' parameters (both can be queried solely via black-box access, such as APIs), it distinguishes itself from traditional knowledge distillation methods such as prompt tuning and fine-tuning, which require internal model access or parameter updates. This makes it particularly suitable for environments where direct model manipulation is impractical or restricted.\nBased on our empirical application, i.e., multi-turn conversations in customer service, our key findings are as follows: First, teaching strategy is more effective than teaching responses for multi-turn generation. Second, context-specific strategies are more effective than global strategies, since the former can provide more targeted strategies for different scenarios. Third, even though the library is learned for a particular student LLM and specific contexts, it contains common knowledge that is transferrable across models and across contexts.\nThe rest of this paper is organized as follows: \u00a72 discusses the related literature on LLM and knowledge distillation. \u00a73 presents the proposed method, while \u00a74 evaluates it with an extensive set of experiments. \u00a75 concludes the paper. More analyses are included in the online appendix."}, {"title": "2. Related Work", "content": "Our work contributes to the recent but growing literature on marketing applications using LLMs. This body of research has typically focused on how LLMs can be used for market"}, {"title": "6", "content": "research and the study of human behavior, while also highlighting attendant challenges (Gui and Toubia 2023, Qiu et al. 2023, Horton 2023)). Specific marketing research applications include perceptual maps (e.g., Li et al. 2024) and conjoint analysis (e.g., Brand et al. 2023, Gui and Toubia 2023). In contrast, this paper is focused on how to effectively adapt and engineer an LLM for marketing tasks such as customer service. We will next discuss how our method relates to existing literature on knowledge distillation in LLMs and advances the literature on goal-oriented dialogs, particularly in multi-turn interactions.\n2.1. Knowledge Distillation for LLM\nThe concept of utilizing a superior model to enhance a less powerful one is known as knowledge distillation. This technique was first introduced by Hinton et al. (2015) in the context of supervised learning and has been adapted for use with language models in recent years (Sanh et al. 2019, Sun et al. 2019). In the realm of language models, knowledge distillation involves using a larger and more capable LLM to generate data that trains specialized, smaller models. Existing research on knowledge distillation for language models typically employs objective functions that either maximize the likelihood of high-probability sequences generated by the teacher model (Kim and Rush 2016) or guide the student model to mimic the token-level probability distributions provided by the teacher (Sanh et al. 2019). Some recent work also proposes to teach the student the rationale for solving a task (Hsieh et al. 2023, Magister et al. 2022). However, all these methods require training the student model and updating its parameters. In our approach, however, we distill the knowledge into an external library that the student can query during inference, without the need for training the student model. The teacher's knowledge is utilized by the student through retrieval-augmented generation (RAG), another popular technique in LLM. In doing so, our method requires only black-box access to the student model, such as through an API, which is not feasible with existing knowledge distillation techniques."}, {"title": "7", "content": "2.2. Goal-Oriented Dialogues\nRecent advancements in LLMs have significantly improved their application in complex goal-oriented dialogues (e.g., Ham et al. 2020b, Li et al. 2023, Snell et al. 2022), but challenges and limitations remain. First, smaller LLMs often lack a strategic understanding of overall dialogue progression and fail to achieve dialogue objectives through multi-turn interactions (Cheng et al. 2024, Deng et al. 2023). Second, the multi-step nature of goal-oriented dialogues fundamentally differs from one-step tasks like text classification and summarization as fine-tuning at each utterance level overlooks the interdependence of multi-turn utterances and the high-level strategy. Zhang et al. (2023) proposes an \u201cAsk an Expert\" solution, where a lesser model seeks advice from a better LLM for generating utterances, but this increases inference (i.e., response) time. Finally, some approaches rely on dialogues with specifically annotated strategies for training (e.g., Zhang et al. 2022, 2023, Joshi et al. 2021), but the dependence on labeled datasets creates a significant barrier for practical application.\n3. Library-based Interpretable Knowledge Distillation\nGiven a student LLM, denoted as S(\u00b7) (e.g., LlaMa 2 or GPT-3.5), our method involves creating a library consisting of a set of representative scenarios, paired with corresponding strategies constructed by a teacher LLM T(\u00b7) optimized for instructing the students on how to respond in those scenarios. We first set up the learning environment and then describe the algorithm. Then, we will show how the library is used during deployment and explain the benefits of our method.\n3.1. Learning Environment\nWe set up a learning environment where the student attempts to improve its performance by mimicking the teacher. For each input, the teacher compares the student's output with"}, {"title": "8", "content": "its own output to help the student respond like itself. To facilitate effective learning, we simulate a customer using GPT-4, which we denote as C(\u00b7). The learning of the student relies on the interaction with the customer LLM. This design is motivated by recent research that advocates using LLMs to simulate human responses to reduce costs and improve efficiency (Li et al. 2024), compared to conducting real human studies.\n3.1.1. Simulating Customers: To simulate the customer, we describe the task in the prompt where we request the LLM to role-play as a customer calling an airline company to request customer service. Here, we focus on a specific context where the customer bought a restricted ticket (non-changeable and non-refundable) and requests to cancel it without penalty.\nTo increase the heterogeneity of the customers and the richness of the conversations, we vary the customer's social styles, initial emotions, and difficulty. For social styles, we use four types based on the classification from Merrill and Reid (1981). (i) Driver: results-driven, confident, and assertive; (ii) Analytical: detail-oriented, systematic, and logical; (iii) Amiable: cooperative, empathetic, and relationship-focused; and (iv) Expressive: enthusiastic, creative, and spontaneous\u00b9. For initial emotions, we set four different customer emotions when initiating the call: calm, confused, concerned, and frustrated, leading to varied dialogue developments. Additionally, we vary the difficulty level of the customer by including/not including the keyword \u201cdemanding\u201d in the customer role description to the LLM. We observe that including \u201cdemanding\u201d changes customer behavior, making customers more persistent with their requests. We use q(s,e,d) to represent the prompt for the customer LLM, parameterized by the social style s, initial emotion e, and difficulty level d."}, {"title": "9", "content": "Given the four social styles, four initial emotions, and two difficulty levels, we create a total of 32 types of customers. When simulating customers, each of the 32 types generates many different conversations because we set the temperature of the LLM to non-zero.\n3.1.2. Simulating Teacher: We then describe how to prompt a teacher LLM to act as an agent. Here we choose the state-of-the-art LLM, GPT-4, as the teacher. We define a base prompt instructing GPT-4 to role-play as a customer service agent. We denote the prompt as Pbase, which contains three key components: role, goal, and constraints.\nThe role specifies the position or function that GPT-4 needs to assume. For example, prompting GPT-4 to\u201crole-play as a customer service agent\" sets the context for the interaction, guiding the model to generate responses suitable for customer service scenarios. The goal establishes the desired outcome and ultimate objective of the interaction, guiding GPT-4 to adjust its strategies accordingly. For instance, if the goal is to achieve high customer satisfaction, GPT-4 will tailor its responses to be more empathetic, helpful, and solution-focused. Including the goal in the prompt is essential because it provides clear direction for the LLM, ensuring that its responses align with the intended results. The constraint establishes the limitations and boundaries that the LLM needs to follow, and as explained earlier, it helps to ensure that the LLM accounts for business constraints and firm policy. As such, the agent is instructed to adhere to these constraints while aiming to achieve high customer satisfaction. This ensures that all agents follow the same company policy, differing only in their communication strategies, which the teacher aims to teach the student.\nThe base prompt is fixed for the entire task and included in the prompt for both the teacher and student."}, {"title": "10", "content": "3.2. Knowledge Distillation\nOur approach iterates over three steps: scenario generation, strategy teaching, and goal evaluation, progressively building up a customized library consisting of scenarios and strategies for handling them. We call each execution of the three steps as one iteration of the algorithm. We use L(t) = {(st), Pstrategy (Sat)))} to represent the library at the end of iteration t, where s represents a scenario indexed by i, Pstrategy (st)) (Si is the corresponding prompt, and nt is the total number of scenarios in the library at iteration t. As t increases, the library grows larger. Below, we detail each step.\n3.2.1. Scenario Generation: In this step, the LLM agent interacts with the customer C(.) to generate conversations, denoted as x(t,l), where t is the iteration index and I is the conversation index. x(t,l) consists of a sequence of utterances from the customer C(.) and the customer service agent LLM (e.g., T or S):\nx(t,l) = (a(t,l), c(t,l), ant,l) ...), (1)\nwhere aft,l) represents the agent's utterance at the k-th turn and cft,l) represents the customer's utterance at the k-the turn. Without loss of generality, we assume a conversation always starts with the agent's utterance, for example, \u201cHello, how may I help you?\u201d\nSince the generation of an utterance depends on all previous exchanges, we define a subconversation with the first k turns, ending with the customer's utterance.\n(t,l)\nX[:k] = (at,l), c(t,l), ..., at,l), c(t,l)). (2)\nThe generation of the teacher's utterance at the k-th turn is based on the prior conversation (t,l)\nX[k] as well as the base prompt Pbase:\nat,t) = T(x1] Pbase). (3)"}, {"title": "11", "content": "The customer's utterance is determined by the prior conversation and the parameterized prompt q(s,e,d):\ncft,t) = C(x-1), akt.) \\q(s,e,d)). (4)\nFor each iteration t, we generate a set of conversations, denoted as X(t), by varying the customer prompt parameters s(social styles), e(initial emotions) and d (difficulty) and running multiple times for each set of parameters with a non-zero temperature.\nSubconversations, defined in Equation (2), are randomly sampled from each conversation in X(t). We call these subconversations scenarios. Each scenario includes the entire prior conversation ending with the customer's utterance. We use S(t) to represent the set of scenarios sampled from X(t).\nIn a naive solution, the conversations in X(t) are generated by T interacting with C. However, each scenario extracted from X(t) reflects the specific dynamics and decisions made by the teacher T, not necessarily those a student would encounter or make when interacting independently in similar contexts. This leads to compounding errors because the scenarios the student encounters during deployment will differ from those it observed during training. In a multi-step decision-making process, even a small deviation in one step can compound over subsequent steps, leading to significant differences at the conversation level. This mismatch between training and deployment scenarios is a significant for challenge multi-step imitations, often referred to as distribution shift (Pomerleau 1991, Ross and Bagnell 2010).\nTo overcome this challenge, we let the student participate in the scenario generation step. We assign a probability p for the teacher to be selected to interact with the customer for conversation generation and 1 \u2212 p for the student to be selected. Initially, p is set to 1 in iteration 1 and gradually decreases in subsequent iterations."}, {"title": "12", "content": "In each iteration, when the student is selected to interact with the customer LLM, it will produce an output based on the scenarios and strategies retrieved from the current library:\nalt,t) = S(x1] Pbase, L(t)). (5)\nWe will explain how to retrieve the relevant strategy from the library in Section 3.3.\nAs the iterations progress, more scenarios will be accumulated into L(t), covering a more diverse set of situations the student could encounter.\n3.2.2. Strategy Teaching: In this step, the teacher iteratively generates and refines strategies for each scenario until the student accurately mimics the teacher's output for that scenario. See Figure 1 for an illustration of the process. We call the prompt generated\nat this step the strategy prompt. The strategy prompt will be combined with the base prompt to instruct the student how to behave like the teacher.\nSpecifically, for each scenario, denoted as s, both T and S generate a response. These responses are then evaluated by T, which identifies discrepancies and proposes updates to the strategy. The updates are suggestions made to the student instructing what they should or should not do, based on their current output. They can vary from general, such"}, {"title": "13", "content": "as\u201cuse a more empathetic tone\u201d to highly nuanced, such as \u201caddress the customer by their first name\", tailored to the specific demands of each scenario and the difference between the student and the teacher.\nThese updates are incorporated into the strategy prompt, denoted as pstrategy(s).\nteacher's output: a=T(s|pbase). (6)\nstudent's output: a=S(s|pbase, Pstrategy (s)). (7)\nThis process of iterative refinement of Pstrategy (s) continues until there are no further updates from the teacher or a pre-defined maximum number of refinements is reached. By progressively refining its responses based on learned strategies and feedback, the student model becomes more adept at handling the scenarios, aligning its behavior with the teacher's expertise and improving overall performance\u00b2.\n3.2.3. Termination Mechanism: Our main algorithm is an iterative process that gradually grows the library by adding a new batch of scenarios and their corresponding strategy prompts at each iteration. The thorough coverage of the data space by the scenarios is crucial for maintaining the robustness and reliability of the strategy prompts during deployment. To determine when the sufficiency is reached, we incorporate a goal evaluation step in the iterative process, where the student LLM is evaluated, based on the interactions with the customer LLM, after each new batch is added to the library, to determine whether the student LLM has achieved satisfactory performance using the current library.\nWe choose to use LLMs for the evaluation, motivated by recent research showing the potential of using LLMs to substitute human evaluations in various tasks, particularly in assessing other LLMs, with findings of up to 80% agreement with human judgment."}, {"title": "14", "content": "Leveraging this advancement, our approach employs the teacher, a state-of-the-art LLM, to directly evaluate the outputs of the student LLM utilizing the current library. This direct evaluation is especially coherent since the student has been trained on the teacher's strategies, making the teacher an ideal evaluator due to its understanding of the intricacies and subtleties required in the responses. We let the student interact with the environment to generate a validation set and let the teacher score it. This score is recorded and compared with the score from the previous iteration. The algorithm stops if there is no (significant) improvement after a pre-defined number of iterations (we set it to 2).\n3.3. Deployment\nThe training algorithm generates a library comprising a set of scenarios and their associated strategy prompts. We will now discuss how this library is utilized during deployment.\nAs shown in Equation (7), the student's output is determined by the scenario, the base prompt, and the strategy prompt optimized for that specific scenario during training. However, during deployment, the prior conversations differ from the training scenarios, and there is no predefined pstrategy for the test input. To address this, we employ a method akin to retrieval-augmented generation (Lewis et al. 2020), where we identify the most similar scenario(s) from a library based on their embeddings. The corresponding strategy from these similar scenarios is then applied to the new, unseen input, i.e.,\nstudent's output: a=S(s|pbase, Pstrategy (\u0161)), where s = arg min d(s, s'). (8)\nBy leveraging scenario embeddings, the library can efficiently match new, unseen inputs to the most similar scenarios, thereby providing a tailored strategy prompt.\nNotably, the use of the library is flexible. When retrieving strategies, it is not necessary to limit retrieval to a single closest scenario; instead, one can retrieve k > 1 scenarios from the library, following the k-nearest neighbors approach. We provide a detailed analysis in Appendix A.2 and find that tuning k can further enhance performance."}, {"title": "15", "content": "3.4. Method Summary and Discussion\nIntuitively, the library consists of a set of representative and prototypical \u201creference\" points within the data space. These reference points must correctly and adequately cover the data space to ensure that when encountering a new scenario, there is a high likelihood that an existing strategy can be effectively applied. The correct coverage necessitates the algorithm to prevent distribution shift, motivating the design of increasingly letting the student to generate scenarios in Section 3.2.1. The termination mechanism is in place to enforce adequate coverage, where the teacher LLM monitors the performance of the student and only stops when no improvement is observed, indicating the sufficiency of the coverage.\nA key reason for the success of this method is the ability of the LLM to extrapolate, adapt, and apply a strategy designed for one scenario to a slightly different scenario. This capability allows the LLM to bridge gaps between similar but distinct situations, ensuring that the strategies remain effective even when faced with novel inputs. The LLM's inherent understanding of the nuances in language and context further strengthens its ability to generalize from the library's reference points, making it a powerful tool for dynamic and diverse real-world applications. This ability further allows the library to be used in a slightly different context where it is not trained on, as we will demonstrate in Section 4.3."}, {"title": "16", "content": "Interpretability Benefits. One of the defining features of our methodology is its interpretability: the teacher's knowledge is distilled into scenarios and prompts that are easily understood by humans. This interpretability offers several advantages over fine-tuning-based methods. First, domain experts can verify the library before deployment, which is especially important when the teacher is an external LLM over which the company has no control, including its model and training data. In fine-tuning methods, a student model can be easily contaminated under adversarial attacks, as it is unclear what knowledge has been distilled into the student. In contrast, our method keeps the student LLM's parameters unchanged, storing external knowledge in an interpretable library where each piece of \"knowledge\u201d can be scrutinized and verified. This design makes our method more resilient to adversarial attacks. Second, a library-based knowledge distillation is easily editable. Suboptimal or outdated strategies can be quickly updated by human experts. Meanwhile, when new scenarios arise during deployment and the existing library lacks relevant strategies, new entries can be seamlessly added with strategies devised by either the teacher or domain experts. This flexibility ensures that the library remains current and effective across various real-world applications.\n4. Experiments\nWe evaluate our method on four student models, LlaMa-2 7b, LlaMa-2 13b, LlaMa-2 70b and GPT-3.5. We assess LlaMa models because they are open source and free except for storage and computing cost. We assess GPT-3.5 because it can only be queried by API and the cost of use is only 5% of GPT-4 at $0.50 per 1M input tokens. This selection of student models also covers a large range of model size, from 7 billion (LlaMa-2 7b) to 175 billion (GPT-3.5).\nWith our experiments, we investigate the following research questions related to our proposed knowledge distillation method."}, {"title": "17", "content": "Q1. Is teaching strategy more effective than teaching responses? To answer this question, for each task, we compare our proposed strategy teaching approach against fine-tuning student LLMs using the same amount of data. Fine-tuning directly trains the student on the scenario - response pairs without explicitly teaching strategies like our method does.\nQ2. Is a context-specific dynamic use of strategies provided by the advanced LLM better than global strategies that are invariant across contexts? For this, we compare our dynamic approach against global strategies.\nQ3. How effective is our solution to fixing the distribution shift challenge? To answer this question, we create a baseline that works the same as ours with the only difference being that the scenarios are all generated by the teacher interacting with the customer.\nQ4. Are learnt strategies transferable across multiple student LLMs and other task contexts? We assess whether a strategy learnt from teaching one student LLM works for a different LLM and whether strategies learnt for one customer service context improves performance in another context.\n4.1. Main Results\nBaseline Methods. To answer question Q1 above, we compare our approach with traditional knowledge distillation via fine-tuning, where we first use the teacher LLM to interact with the customer LLM to generate a set of conversations, and then extract subconversations from it, just like how we sampled scenarios. The difference is that the student does not participate in the data generation and no explicit strategies are provided. The scenarios are used as the input and then we use the teacher's response as the output, creating a training dataset for fine-tuning. To answer Q2, we compare with a set of guidelines manually constructed, through iterations between the authors and GPT-4. These guidelines are global instructions that are used for every input the student encounters."}, {"title": "18", "content": "Evaluation Approach. We evaluate the LLMs at the conversation level in terms of the achievement of the pre-defined goal - customer satisfaction. Motivated by recent research (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), we perform two sets of evaluations.\nWe first use advanced LLMs as an evaluator to rate each conversation from 1 (very dissatisfied) to 5 (very satisfied), using few-shot prompt where examples of rating 5 conversations are provided by the teacher, GPT-4, interacting with the customer LLM. We then use human evaluation where each participant does a blind comparison of GPT-4 and a student LLM, based on their conversations with the same customer LLM.\nLLM as Evaluators. We use advanced LLMs to evaluate the conversations generated by different LLM agents (teacher or student) interacting with the customer LLM. Here we use GPT-4, since it is the state-of-the-art LLM available today. We use few-shot learning when evaluating these conversations\nwe include examples in the prompt that are generated by the teacher (GPT-4) and the customer. Since different examples may lead to different ratings, for robust evaluation, we perform the evaluation twice, using examples generated by the teacher interacting with \u201cdemanding\" customers and \u201cnon-demanding\" customers, respectively. We report the average ratings of four student models trained by different methods in Table 1.\nOur strategy imitation method performs consistently better than the baselines. First, compared to global guidelines, our method completely dominates this baseline, performing better or equally in all experiments. Compared with fine-tuning, our method is better or equal except on the smallest LLM, LlaMa-2 7b. This is because smaller LLMs do not follow instructions as well as larger LLMs. Thus the knowledge distilled from the teacher may not be executed by the student as well as a larger student. This observation is consistent"}, {"title": "19", "content": "with the findings from the literature that prompt based approaches are more effective for the LLM with more parameters (Lester et al. 2021).\nNote that when prompts use conversations with demanding customers as examples of a rating of 5, they tend to yield higher scores across all evaluation tables. This occurs because when we label these challenging interactions as a 5 in the prompt, we communicate to the LLM that even when a customer is persistent and difficult to console, the conversation still merits a high score. This effectively sets a low bar for evaluation, leading to inflated ratings. Conversely, if examples feature non-demanding customers where positive interactions result in a rating of 5, the LLM is more likely to focus on the customer's reactions when evaluating conversations. This establishes a higher standard for ratings.\nDifferent advanced LLMs can be used for evaluation. To assess robustness, we also use LlaMa-3 70B as an evaluator. See Appendix A.1 for details. In addition, we also evaluated retrieving more than 1 most similar scenarios to use their strategies during test. The results are shown in Appendix A.2.\nHuman Evaluation. We conduct a blind comparison of the teacher and student via human evaluation. We design a survey and show the two conversations, from the student and teacher, each interacting with the same customer. We then ask the participant to choose the better agent based on the conversations.\nWe report the percentage of times each student model is selected. If a student is comparable to the teacher, the selection rate should be around 50%. Thus, the larger the number,"}, {"title": "20", "content": "the better the model. Results show that the base models for each student is consistently worse than the teacher, especially for LlaMa-2 7b, which is selected only 3% of the time. After the knowledge distillation, we see a consistent improvement in the performance of student LLMs, with weaker LLMs benefiting more from learning from the teacher LLM. While the student still performs slightly worse than the teacher, the precentages significantly increase.\nDiscussion. Overall, our method performs better than the global guidelines because it provides strategies customized for each scenario and for each student. These strategies are developed based on specific scenarios, such that dynamic instructions are provided to the student, in contrast with the fixed strategies from a global guideline. Consequently, vastly different, even contradictory strategies may exist. We shown an example in Figure 3. In a scenario where a customer initiates a conversation by asking for help, the student is instructed to avoid over-explaining policies. In contrast, in a different scenario where the customer is engaged and seeks information, the student is instructed to provide clear and"}]}