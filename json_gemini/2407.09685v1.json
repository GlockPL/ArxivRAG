{"title": "Accelerating the inference of string generation-based chemical reaction models for industrial applications", "authors": ["Mikhail Andronov", "Natalia Andronova", "Michael Wand", "J\u00fcrgen Schmidhuber", "Djork-Arn\u00e9 Clevert"], "abstract": "Template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest for industrial applications in computer-aided synthesis planning systems due to their state-of-the-art accuracy. However, they suffer from slow inference speed. We present a method to accelerate inference in autoregressive SMILES generators through speculative decoding by copying query string subsequences into target strings in the right places. We apply our method to the molecular transformer implemented in Pytorch Lightning and achieve over 3X faster inference in reaction prediction and single-step retrosynthesis, with no loss in accuracy.", "sections": [{"title": "1. Introduction", "content": "Automated planning of organic chemical synthesis, first formalized around fifty years ago (Pensak & Corey, 1977), is one of the core technologies enabling computer-aided drug discovery. While first computer-aided synthesis planning (CASP) systems relied on manually encoded rules (Johnson et al., 1989; Gasteiger et al., 2000), researchers now primarily focus on CASP methods powered by artificial intelligence techniques. The design principles of the latter were outlined in the seminal work by Segler et al. (Segler et al., 2018): a machine learning-based single-step retrosynthesis model combined with a planning algorithm. The former proposes several candidate retrosynthetic chemical transformations for a given molecule, and the latter, e.g., Monte-Carlo Tree Search, uses those candidates to construct a synthesis tree. Single-step retrosynthesis models are now commonly developed in two paradigms: template-based models and template-free models. Besides retrosynthesis, one can also build a model to predict the products of chemical reactions."}, {"title": "2. Methods", "content": "Autoregressive models, such as Transformer variants (Vaswani et al., 2017; Brown et al., 2020; Schmidhuber, 1992), generate sequences token by token, and every prediction of the next token requires a forward pass of the model. Such a process may be computationally expensive, especially for models with billions of parameters. Therefore, an intriguing question arises whether could one generate several tokens in one forward pass of the model, thus completing the output faster. Speculative decoding (Xia et al., 2023; Leviathan et al., 2023) answers positively. Recently proposed as a method of inference acceleration for Large Language Models, it is based on the draft-and-verify idea. At every generation step, one can append some draft sequence to the sequence generated by the model so far and see if the model \"accepts\" the draft tokens.\nIf the draft sequence has length N, in the best case, the model adds N + 1 token to the generated sequence in one forward pass, and in the worst case, it adds one token as in standard autoregressive generation. The acceptance rate for one generated sequence is the number of accepted draft tokens divided by the total number of tokens in the generated sequence. One can also test multiple draft sequences in one forward pass taking advantage of parallelization, and choose the best one. Speculative decoding does not affect the content of the predicted sequence compared to the token-by-token decoding in any way.\nOne can freely choose a way of generating draft sequences. For LLMs, one would usually use another smaller language model that performs its forward pass faster than the main LLM (Leviathan et al., 2023) or additional generation heads on top of the LLM's backbone (Cai et al., 2024). However, one can also construct draft sequences without calling any learned functions. For example, generate random draft sequences, even though their acceptance rate will be minimal, or assemble draft sequences out of tokens in the query sequence a prompt for decoder-only language models or a source sequence for translation models. The latter option is perfect for retrosynthesis or reaction prediction as SMILES-to-SMILES translation. In a chemical reaction, large fragments of reactants typically remain unchanged, which means that the SMILES of products and reactants have many common substrings. It is especially true if reactant and product SMILES are aligned to minimize the edit distance between them (Zhong et al., 2022). Therefore, we can extract multiple substrings of a chosen length N from the query SMILES and use them as draft sequences with a high acceptance rate.\nSpeculative decoding does not require any changes to the model architecture or training of additional models. The cost of generating draft sequences in this way is negligible compared to that of the forward pass of the reaction model."}, {"title": "2.2. Model", "content": "We demonstrate the application of our method to the Molecular Transformer (Schwaller et al., 2019). It is an encoder-decoder transformer model suitable for SMILES-to-SMILES translation. We conduct our experiments on one H100 GPU with 80 GB memory.\nThe original Molecular Transformer (Schwaller et al., 2019) adopts OpenNMT (Klein et al., 2018), a general framework for neural machine translation, for SMILES-to-SMILES translation. Since the code in this framework is complex and intractable to customize, we decided to re-implement the model in PyTorch Lightning to keep only the necessary code and have more design freedom in the model's inference procedure implementation."}, {"title": "2.3. Data", "content": "We used the open reaction data from US patents (Lowe, 2012) for training all models. We trained the model for reaction product prediction as in the original paper (Schwaller et al., 2019) on the USPTO MIT dataset, a standard benchmark for product prediction, without reactant-reagent separation. We trained the model for single-step retrosynthesis on USPTO 50K, a standard dataset for the task. In this dataset, we augmented every reaction in the training set 20 times using SMILES augmentation (Tetko et al., 2020) with root-aligned SMILES (Zhong et al., 2022). We followed the standard atomwise tokenization procedure (Schwaller et al., 2019) to tokenize SMILES."}, {"title": "3. Results and Discussion", "content": "Our implementation of the Molecular Transformer (MT) successfully reproduces the accuracy scores of the original MT (Schwaller et al., 2019) that relies on OpenNMT. Comparing our MT and the original MT, we observe at most 0.2 percentage points discrepancy of top-1 to top-5 accuracy in product prediction with beam search. Having verified the correctness of our MT's implementation in this way, we replace the standard decoding procedures for the MT with our methods based on speculative decoding and achieve a significant speed-up in product prediction and single-step retrosynthesis."}, {"title": "3.1. Product prediction", "content": "We tested our MT for product prediction on USPTO MIT mixed, i.e., without an explicit separation between reactants and reagents. The test dataset in this benchmark comprises 40 thousand reactions.\nWhen serving a reaction prediction model as an AI assistant for chemists, one could use greedy decoding with a batch size of 1 for inference. The model's inference with standard greedy decoding with a batch size of 1 finishes in around 62 minutes. In contrast, if we use greedy generation enhanced with our speculative decoding, the inference time reduces to 26 minutes with a draft length of 4 and 17 minutes with a draft length of 10, which corresponds to 137 % and 262 % speedup, respectively. The acceptance rate in our drafting method averaged over all test reactions is 79%. Potentially, it can be even higher if one adds more draft sequences to choose from, for example, subsequences of the source sequence dilated by one token. Of course, greedy decoding with a large batch size is much faster and completes in around 4 minutes with 32 reactions in a batch. However, accelerating inference with a batch size of 1 would be sufficient for an improved user experience with reaction prediction assistants: chemists would usually enter one query at a time in a user interface of a reaction model like IBM RXN. The model's accuracy is 88.3% with both standard and speculative greedy decoding."}, {"title": "3.2. Single-step retrosynthesis", "content": "We carried out single-step retrosynthesis experiments on USPTO 50K, in which the training dataset was augmented 20 times. The augmentation procedure is to construct alternative root-aligned (Zhong et al., 2022) SMILES for every dataset entry. This augmentation minimizes the edit distance between reactants and products, which simplifies training, pushes the model's accuracy higher, and increases the acceptance rate in our speculative decoding method. Speculative decoding in single-step retrosynthesis accelerates greedy decoding as much as in reaction product prediction. However, this has limited utility. In synthesis planning, one would want a single-step retrosynthesis model to suggest multiple different reactant sets for every query product so that the planning algorithm can choose from them. Usually, one would employ beam search to generate several outputs from the transformer for single-step retrosynthesis.\nWe found a way to accelerate beam search with speculative decoding. The main idea behind it is that at every decoding step, we use a draft sequence to generate multiple candidate sequences of different lengths in one forward pass, which we then order by probabilities and keep the best n of them. To organize such sequences of unequal lengths in a batch for a subsequent forward pass of the model, we pad them from the left and offset the positional encodings accordingly for every sequence. We call our algorithm \"speculative beam search\" (SBS). The wall time our retrosynthesis model takes to process the USPTO 50K test set with beam search and batch size 1 is around 37 minutes, 40 minutes, and 46 minutes when generating 5, 10, and 25 predictions for every query SMILES, respectively. We keep the beam width and the number of best sequences equal. When we replace the standard beam search with our SBS with the draft length of ten, the generation finishes in around 10, 15, and 28 minutes, accelerating the inference by around 3.7, 2.7, and 1.8 times, respectively.\nSBS reduces to the standard beam search when draft tokens are never accepted. It happens, for example, if we use the start-of-sequence token as a single draft. The \"effective\" batch size also does not increase when using only one draft per forward pass We added an experiment in this mode, denoting it as \"SBS, DL=0\", to verify that the algorithm's acceleration comes from the draft sequences. Interestingly, SBS is faster than the standard beam search even in case of DL=0.\nThe top-N accuracy when using SBS is practically the same as the accuracy of the standard beam search. Thus, our SBS accelerates the MT's generation of multiple reactant sets several times without having to compromise on accuracy at all. Such a speed-up could make the transformer a more attractive single-step model for multi-step synthesis planning."}, {"title": "3.3. Limitations", "content": "The speed of the model's forward pass decreases with the increase of the size of a batch input to the transformer decoder. This effect quickly manifests itself in our drafting strategy. The latter is \"brute force\" in some sense, as we use various substrings of the source SMILES as drafts in parallel at every forward pass of the model. It inflates the \"effective\u201d batch size of the input to the transformer decoder: we copy every sequence in the batch as many times as there are drafts and concatenate the sequence copy and the corresponding draft. As a result, the generation of target SMILES may become slower with speculative decoding than with standard decoding procedures, even when it requires fewer calls to the model. We put a boundary on the number of drafts extracted from a query sequence to mitigate this deceleration. However, this compromises the acceptance rate.\nThe speed of the model's forward pass decreases with the increase of the size of a batch input to the transformer decoder. This effect quickly manifests itself in our drafting strategy. The latter is \"brute force\" in some sense, as we use various substrings of the source SMILES as drafts in parallel at every forward pass of the model. It inflates the \"effective\" batch size of the input to the transformer decoder: we copy every sequence in the batch as many times as there are drafts and concatenate the sequence copy and the corresponding draft. As a result, the generation of target SMILES may become slower with speculative decoding than with standard decoding procedures, even when it requires fewer calls to the model. We put a boundary on the number of drafts extracted from a query sequence to mitigate this deceleration. However, this compromises the acceptance rate.\nIn addition, as the size of the input batch grows, the generation speed with speculative decoding becomes bottlenecked in terms of the number of calls to the model. The \"least lucky\" sequence with the lowest acceptance rate of speculative tokens would determine the number of calls. Together, these two effects limit the utility of our speculative decoding for large batch sizes and beam widths. For example, our SBS is slower than the standard beam search when the beam size is fifty. Nonetheless, we consider speculative decoding very promising for synthesis planning acceleration, as it significantly improves the speed of the standard beam search without losing accuracy in the practical range of beam widths (~10-20).\nDesigning a drafting strategy for SMILES that removes the need for multiple parallel drafts while retaining a high acceptance rate is an aspect of our ongoing work.\nOur speculative algorithm works well in single-step retrosynthesis because the MT typically predicts low-entropy next-token distributions in this task. In most cases, all predicted probability mass concentrates on one token. Consequently, long sequences often win in the probability competition against shorter sequences at every speculative beam search iteration. If several top probabilities from the predicted next-token distribution are approximately the same, then short sequences would tend to have higher probabilities, and the model would generate only a small number of tokens per forward pass, benefitting little from drafts."}, {"title": "4. Conclusion", "content": "We combine speculative decoding and chemical insights to accelerate inference in the molecular transformer, a SMILES-to-SMILES translation model. Our method makes processing the test set more than three times faster in both single-step retrosynthesis on USPTO 50K and reaction product prediction on USPTO MIT compared to the standard decoding procedures. Our method aims at making state-of-the-art template-free SMILES-generation-based models such as the molecular transformer more suitable for industrial applications such as computer-aided synthesis planning systems."}, {"title": "A. Training details.", "content": "For product prediction, we train this model with the same hyperparameters as in Schwaller et al. with four encoder and decoder layers, eight heads, embedding dimensionality of 256, and feedforward dimensionality of 2048, which results in 11,4 million parameters. For single-step retrosynthesis, we set the hyperparameters as in Zhong et al. (six encoder and decoder layers, eight heads, embedding dimensionality of 256, and feedforward dimensionality of 2048), which results in 17,4 million parameters. The dictionary is the same for the encoder and the decoder in both models. We use the Adam optimizer for both models."}, {"title": "B. Speculative Beam Search", "content": "The outline of the speculative beam search procedure we implement for our experiments is as follows:"}]}