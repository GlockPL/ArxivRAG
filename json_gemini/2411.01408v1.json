{"title": "HeightMapNet: Explicit Height Modeling for End-to-End HD Map Learning", "authors": ["Wenzhao Qiu", "Shanmin Pang", "Hao Zhang", "Jianwu Fang", "Jianru Xue"], "abstract": "Recent advances in high-definition (HD) map construction from surround-view images have highlighted their cost-effectiveness in deployment. However, prevailing techniques often fall short in accurately extracting and utilizing road features, as well as in the implementation of view transformation. In response, we introduce HeightMapNet, a novel framework that establishes a dynamic relationship between image features and road surface height distributions. By integrating height priors, our approach refines the accuracy of Bird's-Eye-View (BEV) features beyond conventional methods. HeightMapNet also introduces a foreground-background separation network that sharply distinguishes between critical road elements and extraneous background components, enabling precise focus on detailed road micro-features. Additionally, our method leverages multi-scale features within the BEV space, optimally utilizing spatial geometric information to boost model performance. HeightMapNet has shown exceptional results on the challenging nuScenes and Argoverse 2 datasets, outperforming several widely recognized approaches. The code will be available at https://github.com/adasfag/HeightMapNet/.", "sections": [{"title": "1. Introduction", "content": "In the domain of autonomous driving, the ability to accurately and comprehensively interpret the environmental context surrounding the ego vehicle is paramount for ensuring safe and effective operational decisions. Surround-view methods, recognized for their cost-efficiency and broad applicability, have garnered significant advancements in this area. Current techniques predominantly fall into two categories based on their intermediary representations: sparse query-based methods and dense BEV-based methods. Drawing inspiration from the DETR architecture [2, 26], sparse query-based approaches [14-16] utilize learnable global queries representing detection elements, refined through interactions with surround-view image features. Although this strategy effectively controls the proliferation of queries, its reliance on static global queries limits adaptability in dynamic environments, often resulting in detection inaccuracies at extended distances. Conversely, BEV-based methods convert Perspective View (PV) into BEV representations using a perspective transformation module. Subsequent feature processing is performed via a map detector head. BEV methods [9, 12, 13, 17, 33] demonstrate state-of-the-art performance in online mapping, and have recently dominated the field.\nExisting BEV methods typically utilize enhanced LSS [20] or attention mechanism [25] as baselines for the perspective transformation module. LSS-based methods [10, 13] often require auxiliary losses to accelerate detector convergence speed, while attention-based methods [3, 5, 18, 21, 23, 33] usually need additional modules to improve the output BEV features. These methods normally overlook the vertical dimension of road features during the PV-to-BEV transformation, thus compromising their capacity to delineate complex environmental details accurately. Additionally, most existing studies [11, 12, 16] inadequately address the challenge of filtering non-critical elements such as the sky and other extraneous background features during the processing of image features from multi-view inputs. This oversight in selective background filtration foregoes crucial noise reduction opportunities, consequently rendering the models vulnerable to disruptions caused by irrelevant data. Such disruptions significantly compromise the accuracy and reliability of the resultant perceptual outputs. Moreover, while prevailing researches [11-13] tend to concentrate on exploiting single-layer image features for computational effectiveness, it largely neglects the benefits and possibilities afforded by multi-scale feature fusion within the BEV space. This limitation undermines the model's effectiveness in navigating complex road environments.\nTo address these challenges, we propose a new view transformation paradigm that forges a nuanced relationship between image features and road surface height distributions, integrating height priors to refine the accuracy of"}, {"title": "2. Related Work", "content": "Online HD Map Construction. The construction of online HD map from visual perception has recently become a focal point of interest and has seen significant advancements. This process fundamentally involves extracting image features from surround-view inputs. Notably, PETRv2 [16] adopted a sparse query-based method, initially derived from the 3D object detection field, to create BEV segmentation maps and delineate 3D lanes. This method integrates advanced object recognition frameworks to enable comprehensive environmental modeling. In contrast, BEV-based methods [9, 12, 13, 17, 34] leveraged estimated depths or attention layers to project image features into BEV space, addressing the inherent limitations of pixel coordinate systems in capturing depth information. For instance, methods employing depth series [7,13,20] enhanced the transformation from 2D to 3D coordinates by constructing depth profiles directly from image features. Similarly, techniques using attention series [4, 25] exploited the robust capabilities of attention mechanisms to contextualize 2D image features within a 3D spatial framework. Furthermore, several methods such as BeMapNet [22], PivotNet [5], and MapVR [35] focused on refining the decoder by integrating prior geometric information about map elements, aiming to achieve more accurate and smoother detection outcomes. Concurrently, some other algorithms [33] typically required additional modules to enhance the output features in the BEV space.\nHeight Modeling for 3D Detection. Height modeling, originally introduced in the 3D object detection domain, has seen limited advancement over the years. Initially, BEVFormer [11] implemented the concept of height implicitly by setting reference points at various predefined heights, thus beginning the exploration into vertical dimension modeling. Subsequently, BEVHeight [30] presented a method that models object heights by predicting the heights of image pixels and then translating these features into BEV space using geometric transformations. This approach has demonstrated considerable effectiveness in roadside scenarios [31,32], where its performance strongly depends on the precision of the camera's installation height. Furthering this progression, HeightFormer [28] expanded the practical application to scenarios adjacent to vehicles and explicitly incorporates the concept of height into its modeling framework. Different from these previous works, we make the first attempt that extends the application of height models for HD map construction by developing an explicit height prediction model."}, {"title": "3. Method", "content": "Figure 1 illustrates the workflow of our approach. It initiates with a feature encoder that extracts multi-scale PV features $\\mathbf{F} = \\{\\mathbf{F}_1, \\mathbf{F}_2, ..., \\mathbf{F}_s\\}$ (where s is the number of scales) from the raw images I. These PV features undergo refinement in a foreground-background separation network, which effectively discriminates between road elements and non-road elements, enhancing the purity of the feature signals. Following this, a height prediction mechanism facilitates the transformation of features from traditional PV to BEV. This transformation significantly improves perceptual accuracy through a comprehensive spatial representation of the scene. After that, a multi-scale feature fusion technology is applied, which enhances the model adaptability to complex scenarios by integrating BEV features captured at different scales. Ultimately, a feature decoder transforms these processed features into a vectorized scene representation. This representation meticulously depicts key static road elements, including divider (div.), boundary (bou.), and pedestrian crossing (ped.), providing a precise reflection of actual road conditions."}, {"title": "3.2. Foreground-Background Separation", "content": "In the processing of image features, most existing studies do not sufficiently discriminate between foreground road elements and non-essential background elements. This inclusion of background information renders the model vulnerable to irrelevant data, potentially impairing its processing efficacy. To address this problem, we develop a self-supervised foreground-background separation network, leveraging projection relationships to enhance model focus on pertinent road elements. This network is engineered to produce precise foreground masks that effectively attenuate background information not pertinent to the road.\nAs depicted in Figure 1, the foreground-background separation network receives multi-scale PV features $\\mathbf{F}_i$ as inputs and processes them through a streamlined multi-layer perceptron (MLP). This MLP generates corresponding foreground masks $\\mathbf{F}_{mask}$, which are then integrated with the original PV features using residual connection [6]. This integration employs the Hadamard product to adjust the positional intensity of the feature maps meticulously. The processed data are subsequently added back to the original PV features, enriching them with confidence information. This strategy significantly fortifies the feature set, rendering it more robust against potential disruptions. The operational flow of this process is detailed as follows:\n$\\mathbf{F}_{mask} = \\text{Sigmoid}(\\text{Conv}(\\text{ReLU}(\\text{Conv}(\\mathbf{F}_i)))),$ (1)\n$\\mathbf{F}_{mask\\_updated} = \\mathbf{F}_i + \\mathbf{F}_i \\odot \\mathbf{F}_{mask}, i = 1,..., s.$ (2)\nMoreover, we introduce a self-supervised methodology for generating ground truths, anchored on geometric projection relationships, to accurately define foreground masks. In particular, our procedure initiates with the establishment of uniformly distributed reference points within a designated range (within the range of [-2.0m, 2.0m]) in the BEV space. Utilizing the camera's intrinsic and extrinsic parameter matrices, these points are then geometrically projected from the 3D space onto the 2D image plane. Features within the scope of these projections are classified as foreground, encompassing the road and nearby critical elements. In contrast, areas outside these projections are designated as background, typically including non-road elements like the sky.\nAfter processing through the foreground-background separation network, the refined image features achieve a heightened focus on the road and essential adjacent foreground elements. These foreground-delineated features $\\mathbf{F}_{mask\\_updated}$ are subsequently employed by the height prediction mechanism, enhancing the effectiveness of the perspective transformation process."}, {"title": "3.3. Height Prediction Mechanism", "content": "The perspective transformation in BEV-based frameworks has traditionally been considered an ill-posed problem, often addressed through complex depths or attention mechanisms to directly generate transformed BEV features. Although effective, this approach complicates the model's architecture and obscures its interpretability. To overcome these limitations, we introduce a novel height prediction"}, {"title": "3.4. Multi-Scale Feature Fusion", "content": "While existing BEV-based methods typically rely on single-layer image features to enhance computational efficiency, this approach is not always the optimal choice for distinguishing different elements in HD maps. Notably, distant elements, which pose greater capture challenges, may benefit from the large-resolution features present in lower layers, whereas larger, nearby elements are more aptly captured by higher layers. To overcome this limitation, inspired by previous studies [11, 17], we introduce a multi-scale feature fusion module at the BEV stage that orchestrates the integration of features across different scales.\nThe architecture of our fusion module is illustrated in Figure 3. As shown, multi-scale BEV features are coarsely integrated via feature concatenation. This preliminary integration serves as the foundation for further processing by a specially tailored convolutional neural network (CNN). This CNN is designed to enhance the representational capacity of the features and deepen the model's understanding of spatial dynamics, enabling a more nuanced interpretation of the road environment.\nFurthermore, the integration of a deformable attention mechanism [29] equips the model with an adaptive capability to selectively enhance crucial feature areas pivotal for the final inference. This mechanism dynamically adjusts the model's focus across various feature regions, ensuring that the feature fusion process is not only more targeted but also efficient. Following the fusion process, the application of an MLP meticulously refines the integration of features. The architecture, utilizing a series of linear layers augmented with strategically placed residual connections, ensures that the resulting BEV features, denoted as $\\mathbf{F}_{bev'}$, are both enriched and stabilized. This sequence is depicted as follows, where DA symbolizes the deformable attention mechanism.\n$\\mathbf{F}_{bev'} = \\text{MLP}(\\text{DA}(\\text{Conv}(\\text{Concat}(\\mathbf{F}_{bev}^1, ..., \\mathbf{F}_{bev}^s))))$\n$+ \\text{DA}(\\text{Conv}(\\text{Concat}(\\mathbf{F}_{bev}^1, ..., \\mathbf{F}_{bev}^s))).$ (6)"}, {"title": "3.5. Training Loss", "content": "HeightMapNet implements an end-to-end training methodology, finely tuned through instance-level and point-level assignments. The training framework is anchored by four pivotal components of loss: classification loss $L_{cls}$, point-to-point loss $L_{pos}$, edge direction loss $L_{dir}$, and mask loss $L_{mask}$, represented as follows:\n$L = \\lambda L_{cls} + \\lambda L_{pos} + \\beta L_{dir} + L_{mask}.$ (7)\nThe losses $L_{cls}$, $L_{pos}$, and $L_{dir}$ are configured in alignment with the established protocols of MapTR [12], ensuring consistency with well-regarded benchmarks. The mask loss $L_{mask}$, specifically designed to enhance the foreground-background separation network's focus on the road, is quantified using the Manhattan distance between corresponding feature masks. This is mathematically expressed as:\n$L_{mask} = \\sum D_{Manhattan}(\\mathbf{F}_{mask}, \\mathbf{F}_{mask\\_GT}),$ (8)\nwhere $\\mathbf{F}_{mask}$, $\\mathbf{F}_{mask\\_GT}$ represent the predicted foreground mask and the ground truth foreground mask of the PV features, respectively."}, {"title": "4. Experiment", "content": "4.1. Experimental Setup\nTo rigorously evaluate the performance of HeightMapNet in constructing HD maps, comprehensive experiments were conducted on the nuScenes and the Argoverse 2. The nuScenes dataset [1] comprises 1,000 driving scenes, sourced from urban environments in Boston and Singapore. Each scene includes a 20-second video with key samples annotated at a frequency of 2 Hz, capturing panoramic views using six surround-view cameras mounted on the vehicle. The Argoverse 2 dataset [27] features 20,000 sequences, each lasting 30 seconds, recorded at a high sampling rate of 10 Hz. This dataset employs seven surround-view cameras, offering an exceptional density of data and enhanced temporal continuity.\nFor the quantitative evaluation, we adopted the chamfer distance CD as a metric to measure the degree of correspondence between the model predictions and the actual road conditions. This metric assesses the average precision by calculating the chamfer distance between two sets of points, $A = \\{a_1,a_2,...,a_m\\}$ and $B = \\{b_1, b_2,..., b_n\\}$, where the specific computation method is:\n$CD(A, B) = \\frac{1}{|A|} \\sum_{a \\in A} \\min_{b \\in B} ||a - b||_2^2 + \\frac{1}{|B|} \\sum_{b \\in B} \\min_{a \\in A} ||b - a||_2^2.$ (9)\nThe overall average precision AP is subsequently calculated by aggregating values under various chamfer distance"}, {"title": "4.2. Implementation Details", "content": "Our approach extends MapTR [12], leveraging its feature encoder and decoder to detect map elements from BEV perspectives. Notably, our method is designed with modularity in mind, facilitating adaptation to alternative BEV-based map detectors. The perceptual field is defined with an X-axis range of $[-15.0m, 15.0m]$, and a Y-axis range of $[-30.0 m, 30.0m]$, tailored to capture a comprehensive spatial context.\nThe model's training was initiated from scratch, utilizing a single NVIDIA A100 GPU. Optimization was conducted using the AdamW [19] optimizer, characterized by a weight decay of 1.25e-3, a batch size of 8, and an initial learning rate of 3e-4. The training protocol included 24 complete epochs, aligning with the methodologies employed by previous models [12, 13]. For adaptations to the Argoverse 2 dataset, modifications included a revised weight decay of 0.01 and a reduced learning rate of 1.5e-4, with the training duration limited to 6 epochs. Except for these specified adjustments, all other training parameters were maintained consistent with those used for MapTR [12]."}, {"title": "4.3. Main Results", "content": "4.3.1 Results on the nuScenes val Set\nThe experimental results of HeightMapNet on the nuScenes val set are systematically presented in Table 1. This table not only documents the performance metrics of HeightMapNet but also juxtaposes these results with those of other leading algorithms tested under equivalent conditions, providing a comprehensive comparative analysis. Specifically, HeightMapNet registers an overall increase of 7.9% in mean Average Precision (mAP) than the baseline MapTR [12], affirming its enhanced accuracy in map element detection. A detailed examination of the results reveals that HeightMapNet is particularly adept at detecting divider, achieving an 11.6% boost. Furthermore, by incorporating the one-to-many loss and BEV loss from MapTRv2 [13], our method outpeforms all the compared methods by a considerable margin.\nFollowing [5], we also give detection accuracy under three tighter thresholds of {0.2,0.5, 1.0}m in Table 2, where a prediction is treated as true positive (TP) only if the distance between prediction and ground-truth is less than the specified threshold. As shown, compared to the baseline MapTR, our HeightMapNet achieves 7.1% higher mAP. Compared to the existing state-of-the-art PivotNet [5], HeightMapNet* enjoys 1.3% higher mAP. These again verify the effectiveness of proposed solution."}, {"title": "4.4. Ablation & Analysis", "content": "4.4.1 Impact of Each Component\nHere, we present an analysis of the critical components of our model, elucidating their individual contributions to performance enhancement. As outlined in Table 5, we begin with MapTR as the baseline configuration #1, with subsequent integrations of each module to assess their impacts.\nHeight Prediction Mechanism. The integration of the height prediction mechanism into MapTR led to noticeable improvements in detection accuracy for all the three elements. The most substantial gain was observed in divider detection. This enhancement is attributed to the module's precise capture of height information of road elements relative to the ego vehicle's coordinate system. Such precision significantly refines the representativeness of the BEV features transformed from this data, thereby enhancing the model's inferential accuracy."}, {"title": "4.4.2 Parameter Investigation", "content": "In this section, the effect of introduced parameters to the proposed approach is analysed. All the results are obtained on the nuScenes val set, with a training duration of 24 epochs and ResNet50 serving as the backbone.\nThe Number of Height Values. To assess the influence of different numbers of height sampling points near the road surface within the height prediction mechanism, various height values were systematically tested. According to the results presented in Table 6, HeightMapNet demonstrates superior performance when the number of height sampling points is set to 12. This outcome highlights the critical role of optimal sampling point configuration in maximizing the model's efficacy."}, {"title": "4.5. Visualization Results", "content": "To elucidate the practical impact of our method in real-world HD map construction tasks, we visualize the model's prediction outcomes in Figure 4. As shown, our method maintains better performance than baselines in different environments. Even at night, the map near the vehicle closely matches the ground truth. More qualitative results on the nuScenes val set, including both successful and fail cases, can be found in the supplementary material."}, {"title": "5. Conclusion", "content": "In this paper, we have developed a novel perspective transformation framework, HeightMapNet, which innovatively incorporates height priors. This model employs a foreground-background separation to meticulously focus on road surface features. Furthermore, by integrating multi-scale features within the BEV space, our approach effectively maximizes the utilization of spatial geometric information. Experimental validations underscore the robust performance of HeightMapNet, showcasing its substantial potential for practical applications.\nLimitations and Future Work. While the self-supervised learning strategy utilized in the foreground-background separation network minimizes reliance on dataset annotations, the integration of direct label guidance through supervised learning could provide clearer learning targets. This enhancement is anticipated to further improve both the accuracy and reliability of the model. Looking ahead, future investigations should consider incorporating temporal information to extend the model's performance, particularly in dynamic and complex environments."}]}