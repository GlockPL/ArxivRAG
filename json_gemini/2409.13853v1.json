{"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "authors": ["Zhepeng Wang", "Runxue Bao", "Yawen Wu", "Jackson Taylor", "Cao Xiao", "Feng Zheng", "Weiwen Jiang", "Shangqian Gao", "Yanfu Zhang"], "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMS pose significant security risks due to their tendency to memorize training data, leading to potential privacy breaches and copyright infringement. Accurate measurement of this memorization is essential to evaluate and mitigate these potential risks. However, previous attempts to characterize memorization are constrained by either using prefixes only or by prepending a constant soft prompt to the prefixes, which cannot react to changes in input. To address this challenge, we propose a novel method for estimating LLM memorization using dynamic, prefix-dependent soft prompts. Our approach involves training a transformer-based generator to produce soft prompts that adapt to changes in input, thereby enabling more accurate extraction of memorized data. Our method not only addresses the limitations of previous methods but also demonstrates superior performance in diverse experimental settings compared to state-of-the-art techniques. In particular, our method can achieve the maximum relative improvement of 112.75% and 32.26% over the vanilla baseline in terms of discoverable memorization rate for the text generation task and code generation task respectively.", "sections": [{"title": "1 Introduction", "content": "Pretrained large language models (LLMs) (Brown, 2020; Touvron et al., 2023; Almazrouei et al., 2023; Jin et al., 2024) have achieved remarkable success across a wide range of downstream natural language processing (NLP) tasks such as summarization (Zhang et al., 2024b; Van Veen et al., 2024; Zhang et al., 2019), classification (Wang et al., 2023; Sun et al., 2023; Wang et al., 2024; Gao et al., 2024), question answering (Pan et al., 2023; Zhang et al., 2024a; Shao et al., 2023; Louis et al., 2024; Jiang et al., 2021; Guo et al., 2023; Yasunaga et al., 2021) and translation (Zhang et al., 2023; Bawden and Yvon, 2023; He et al., 2024; Xue et al., 2020; Xu et al., 2023; Li et al., 2024), etc. The popularity of LLMs requires people to pay attention to the unique challenges they bring to security. One of the significant security issues is that LLMs can memorize a considerable portion of their training data even though they tend to not overfit to their training dataset due to the small number of training epochs (Radford et al., 2019). Moreover, the memorized data can be extracted by carefully designed input from attackers or even unintentional input from ordinary users, which can cause privacy and copyright issues with the sensitive training data (Carlini et al., 2021, 2023; Ozdayi et al., 2023; Nasr et al., 2023; Karamolegkou et al., 2023). For example, the confidential codes from Samsung can be exposed to other users after they were shared with OpenAI due to the memorization of LLMs (DeGeurin, 2023; Huynh, 2023). The huge security risks and the potential uses of memorization make it important to measure the memorization of the target LLM. With an accurate method to quantify the intrinsic memorization of LLMs, model developers can have a better understanding of the model's vulnerability posed by its memorization and take actions such as machine un-learning (Yao et al., 2023; Pawelczyk et al., 2023; Yao et al., 2024) to mitigate the memorization before they release their LLMs to the public. Moreover, the method to extract memorized data can also be combined with the target LLM and leveraged by the users to detect whether their self-built dataset has data leakage issues when it is used to evaluate the target LLM. To measure the intrinsic memorization of the target LLM, Carlini et al. (2023) first proposed a metric called discoverable memorization rate to serve as the estimation. As shown in Figure 1 (a),"}, {"title": "2 Related Work", "content": "LLM Memorization. The memorization of LLM is firstly verified by Carlini et al. (2021). It shows that it is feasible for attackers to extract training data from target LLMs by producing a large number of random prefixes and feeding them to the target LLM for generation. Carlini et al. (2023) then defines the concept of discoverably memorized and utilizes it to quantify the memorization of the target LLM. In addition to the memorization of pretrained LLM on the pretraining dataset, Zeng et al. (2023) studies the memorization of fine-tuned LLM on the fine-tuning dataset. It shows that memorization also exists in fine-tuning settings and that the characteristics of memorization vary with the type of fine-tuning tasks. Karamolegkou et al. (2023) shows that the memorization of LLM can cause copyright violations for books and proprietary codes. Nasr et al. (2023) demonstrates that it is feasible to extract gigabytes of training data from production LLMs such as ChatGPT due to their memorization. Recently, Ozdayi et al. (2023) proposes to learn a constant soft prompt to extract more training data from LLM to measure memorization. However, we argue that this method still underestimates the memorization of LLM since the soft prompt is independent of the input and thus does not react to the dynamics of the input. Our method can address these limitations. Defend against Memorization. Training LLMs with differentially private training (Abadi et al., 2016) is considered effective in preventing the memorization of individual training samples with a theoretical guarantee (Carlini et al., 2021), However, the training cost is expensive even prohibitive for LLMs. Moreover, the utility of LLMs is significantly degraded, making them impractical for real-world applications. Alternatively, deduplicating training data can mitigate LLM memorization (Lee et al., 2021; Kandpal et al., 2022). However, it cannot eliminate the memorization since certain portions of data will be memorized by LLM inevitably even if they only appear once in the training data. Similarly, Ippolito et al. (2023) shows that memorization can not be prevented by applying runtime filtering to the user input. Therefore, the \u201cultimate\u201d solution to prevent memorization is still under exploration. Machine unlearning (Yao et al., 2023; Pawelczyk et al., 2023; Yao et al., 2024) is a promising method to defend against memorization. By identifying the set of memorized training data to be the forget set for unlearning, LLM can forget these data via gradient ascent (Yao et al., 2023) or in-context learning (Pawelczyk et al., 2023). Compared to existing methods, our method can identify a larger and more accurate forget set for machine unlearning to defend against memorization. Prompt Tuning. Training or finetuning machine learning models is usually costly. To enable efficient training, a variety of methods are proposed to reduce the training cost via pruning (Bao et al., 2020, 2022a,b), data selection (Shrivastava et al., 2016; Wang et al., 2019; Wu et al., 2021) or parameter selection (Wu et al., 2020; Hu et al., 2021; Liu et al., 2022; Wang et al., 2024), etc. All of these methods require adapting the internal parameters of the target model. Therefore, applying these methods to finetuning the LLM may still be expensive due to the large number of parameters of the LLM. Prompt tuning, introduced by Lester et al. (2021), is an efficient method for adapting pretrained models to various tasks by learning \"soft prompts\" that condition frozen language models without changing their internal parameters. In the realm of NLP, researchers have harnessed trainable representations in the form of soft prompts using methods like prompt-tuning, with Su et al. (2022) and Vu et al. (2022) demonstrating successful transferability and improved performance. Ma et al. (2022) uses pruning to remove ineffective tokens, and Wei et al. (2021) provides theoretical proof of prompt tuning's downstream guarantees under weaker non-degeneracy conditions. Prompt tuning has also been applied to vision tasks (Jia et al., 2022; Lian et al., 2022; Chen et al., 2022), including continual learning (Wang et al., 2022) and image inpainting (Bar et al., 2022). Different from previous work that used prompt tuning to improve downstream performance, our work leverages continuous prompts to more accurately reflect intrinsic memorization, extract memorized data from the target LLMs, and measure their memorization."}, {"title": "3 Method", "content": "3.1 Problem Formulation According to the work (Nasr et al., 2023), given the target LLM fe and data x, x is defined as discover-ably memorized if there exists a generation routine G, such that fe(G(p)) = s, where x = [p||s] and x is split into prefix p and suffix s. The generation routine can be constant soft prompts (Ozdayi et al., 2023), dynamic soft prompts (our method), or just the identity function (Carlini et al., 2023). In our problem setting, a set of sequences Dtr is randomly sampled from the training set D of the target LLM fe, we aim to find the generation routine G to maximize discoverable memorization rate over the training set D by leveraging Dtr. We use another disjoint set Dtest randomly sampled from D to evaluate the discoverable memorization rate over D, which is defined as,\nmax 1/|Dtest| \u03a3xi ED test \u03a3 1fe (G(pi))=si (Pi) (1)\nwhere 1(\u00b7) denotes the indicator function and xi = [Pi||Si].\n3.2 Method Overview To maximize the discoverable memorization rate, we propose a pipeline to learn a transformer-based generator gw to build the generation routine G. As shown in Figure 2 (b), the generator gw is initialized with K identity blocks, which are illustrated in Section 3.4. The input to gw is m(p), where m(\u00b7) represents a naive mapping of prefix tokens p and it is detailed in Section 3.3. The dynamic soft prompt o is then generated via gw, where o = g(m(p)). Since o depends on the prefix token p, it can adapt to the change in p. Note that the dimension of o should be the same as the dimension of the embedding E(x) of the target LLM fe for its concatenation with the input data x."}, {"title": "4 Experiments", "content": "We train the generator gw on Dtr to obtain the optimized parameters w*. For each sequence Xi \u2208 Dtr, where xi = [Pi||si], the dynamic soft prompt o is generated and then prepended to the embeddings E(pi) of prefix tokens pi and the embeddings E(si) of suffix tokens si. Thus, we obtain the input qi to the target LLM fe, where qi = [0i||E(pi)||E(si)]. By feeding qi to the target LLM fe, we aim to minimize the aligned causal language modeling (CLM) loss L (Ozdayi et al., 2023) over Dtr, which is defined as,\nL = - \u03a3xiEDtr \u03a3t=ki log Po,w (qi,t|qi,1, ..., qi,t\u22121), (2)\nwhere qi,t represents the t th token in the input sequence qi. Pow(qi,t|qi,1,..., qi,t-1) denotes the output conditional probability distribution at the t th token given the preceding t 1 tokens. ki represents the index of the starting token in suffix si. Therefore, the aligned CLM loss only focuses on the token prediction at the position of suffix tokens, which aligns with the definition of discoverable memorization. During the training phase, only the parameters of gw are updated based on the gradients calculated from the aligned CLM loss while the parameters \u03b8 of fe are frozen. During the testing phase of the trained generator gw, for each testing sequence xi \u2208 Dtest, only the dynamic soft prompt of and the embedding of prefix tokens E(pi) are concatenated and sent to the target LLM fe for generation. The generated output tokens yi are then compared with the suffix tokens si for evaluation, where yi = fo([0i||E(pi)]).\n3.3 Mapping of Prefix Tokens According to the constant soft prompt (Ozdayi et al., 2023), the length of the prompt N is a hyper-parameter of the method and its value can affect the extraction of data. If we feed the prefix tokens p to go directly, then the length of the dynamic soft prompt o will be limited to the length of the prefix tokens p. To provide the same flexibility as the constant soft prompt (Ozdayi et al., 2023), we propose a naive mapping m(.) to preprocess the prefix tokens p and send its output m(p) to the generator \u03b6\u03c9. The details of m(\u00b7) are shown in Figure 3 with an example. Assume the length of p and m(p) is L and N, respectively. If L > N, m(p) is the last N tokens of p. Otherwise, we first generate r by duplicating p for [ L/N ] times. m(p) is then the last N tokens of r. The dynamic soft prompt o is generated, where o = g(m(p)). In this way, the length of the prompt N can be an arbitrary integer, which provides the maximum flexibility for usage.\n3.4 Identity Blocks with Zero-Initialization Randomly initializing the transformer-based generator g and training it from scratch may degrade its performance and even lead to model collapse. It can be verified by a case study for GPT-Neo (Black et al., 2021), shown in Table 1, where the rows without zero initialization correspond to random initializing go. Table 1 shows that the random initialization performs badly with the two standard metrics for memorization being close to 0. The issue of random initialization is caused by the fact that the underlying latent space of the dynamic soft prompt is far away from the embedding space of the target LLM fe at the initial stage, making it difficult for the target LLM fe to extract meaningful information from the prompt and thus hinder the training of the generator gw. Therefore, to enable the effective and robust training of gw, it is important to align the dynamic soft prompt with the embedding of input data, making their underlying latent space close to each other. To achieve this, the tokenizer and embedding layer of the generator gw should be initialized with those of the target LLM fe. However, this is insufficient due to the perturbation incurred by the non-identical forward pass of the transformer blocks within the generator gw. More specifically, the forward pass for each attention block can be formulated as,\nz = x + MHSA(LN(x)), (3a)\ny = z + FFN(LN(z)), (3b)\nwhere x and y are the input and output of the transformer block, respectively. z is the output of the attention layer within the block. MHSA(\u00b7) denotes the multi-head self-attention (MHSA) mechanism. LN() represents layer normalization. FFN(\u00b7) corresponds to the position-wise feed-forward network (FFN). Therefore, if the transformer block is randomly initialized, it corresponds to a non-identical function where y \u2260 x and thus enlarges the distance between the latent space of the dynamic soft prompt and the target LLM fo. Inspired by LLAMA PRO (Wu et al., 2024), we propose to initialize the transformer blocks within gwas identity blocks to align the dynamic soft embedding with the token embedding of the target LLM fe. To illustrate the implementation of the identity block shown in Figure 2 (a), we need to delve into the details of MHSA(\u00b7) and FFN(\u00b7), which can be formulated as,\nMHSA(x') = \u03a3(H i=1 \u03c3\u03b5(x'WQ,Wix)xWv,i)Wo,i (4a)\nFFN(z') = (\u03c3(z'W\u2081) \u2299 (z'W\u2082))W\u2083, (4b)\nwhere x' and z' are obtained by applying layer normalization to x and z, respectively. Assume there are H heads in MHSA(\u00b7). WQ,i, WK,i and Wv,i are the query, key and value matrix of the i th head. Wo,i is the i th weight matrix of the output linear layer in the attention block. \u03c3\u03b5 denotes the softmax function. For FFN(\u00b7), W\u2081 and W2 are the weight matrices of the first layer of linear layers within the position-wise FFN and o(\u00b7) is the activation function, while W3 is the weight matrix of the second layer of the linear layer. The FFN defined in Equation 4b is regularly used in LLaMA models (Touvron et al., 2023). For Pythia (Biderman"}, {"title": "4.1 Experimental Setup", "content": "Models. We evaluate our method on three suites of pretrained LLMs with various scales: GPT-Neo (125M, 1.3B, 2.7B) (Black et al., 2021), Pythia (410M, 1.4B, 2.8B, 6.9B) (Biderman et al., 2023) and StarCoderBase (1B, 3B, 7B) (Li et al., 2023). Both GPT-Neo and Pythia are pretrained on the Pile dataset (Gao et al., 2020) for text generation. StarCoderBase is pretrained on The Stack dataset (Kocetkov et al., 2022) with more than 80 programming languages for code generation. Dataset. We extract training data of GPT-Neo and Pythia with the Language Model Extraction Benchmark dataset (Google-Research), a subset in English with 15K sequences sampled from the Pile dataset. For StarCoderBase, we utilize the-stack-smol dataset (BigCode), a subset randomly sampled from The Stack dataset. In our experiments, we focus on the java, c#, go and sql splits of it. And there are 40K sequences in total. Baselines. We compare our method with four baselines. The baseline No Prompt corresponds to the method shown in Figure 1 (a), serving as the vanilla baseline. We include two naive baselines by prepending hard prompt to the prefix for extraction: Constant Hard Prompt and Dynamic Hard Prompt. Assuming the length of the prompt is N, for Constant Hard Prompt, we pick the first N tokens in the vocabulary of the target LLM to serve as the hard prompt. For Dynamic Hard Prompt, we apply the mapping m(\u00b7) in Section 3.3 to the prefix to generate the hard prompt without feeding it to a generator for further processing. CSP (Ozdayi et al., 2023) corresponds to the method shown in Figure 1 (b), which is the SOTA work in the measurement of the memorization. Evaluation Settings. The length of the prompt, prefix, and suffix is 50 by default without explicit explanation for evaluation. We use the Exact Extraction Rate (ER), Fractional Extraction Rate (ER), Test loss and Test perplexity (PPL) to evaluate the performance of our method. Note that Exact ER corresponds to discoverable memorization rate to estimate the verbatim memorization."}, {"title": "4.2 Main Results", "content": "The main results to evaluate our method and the SOTA baselines are summarized in Table 2, 3 and 4 for the suites of GPT-Neo, Pythia and StarCoderBase, respectively. In the application of text generation, our method can outperform all the baselines consistently and significantly. For GPT-Neo suite, compared with the vanilla baseline (i.e., No Prompt), our method can achieve a relative improvement of 112.75%, 41.52% and 30.0% in terms of Exact ER with the model size of 125M, 1.3B and 2.7B, respectively. For the Pythia suite, our method can achieve a relative improvement of 117.37%, 48.32%, 29.4% and 25.13% over the vanilla baseline in terms of Exact ER with the model size of 410M, 1.4B, 2.8B and 6.9B, respectively. , In the application of code generation, our method can also outperform all the baselines consistently and significantly. For StarCodeBase suite, our method can achieve a relative improvement of 32.26%, 32.39% and 20.88% over the vanilla baseline in terms of Exact ER with the model size of 1B, 3B, and 7B, respectively. We have several observations from the main results across diverse settings. Firstly, prepending naive hard prompts such as Constant Hard Prompt and Dynamic Hard Prompt is not useful but harmful for the exaction of training data from target LLM, leading to a much lower estimation of its memorization. Secondly, our method outperforms the SOTA work, CSP (Ozdayi et al., 2023) across the board, highlighting the importance of dynamic soft prompts for the measure of memorization. Moreover, the memorization of LLM increases with the model size, which is consistent with the existing works (Carlini et al., 2023; Oz-dayi et al., 2023; Nasr et al., 2023). However, it does not mean that the small model does not have security concerns on memorization. As shown in Table 2 and Table 3, with our method, the memorization of small language models with millions of parameters is underestimated by a large margin, where their memorization cannot be ignored in the real applications."}, {"title": "4.3 Ablation Study", "content": "We explore our methods from several perspectives: the impact of dynamic prompt, prefix size L, and the length of prompt N. Impact of Dynamic Prompt. To evaluate the impact of dynamic prompt, we build another baseline by replacing the input to the generator with the first N tokens in the vocabulary of the target LLM. In this way, the soft prompts from the generator are constant and independent of the prefix tokens. The results are shown in Table 5. It can be observed that our method with dynamic prompt outperforms the case with constant prompt consistently and significantly over all the evaluated settings. Moreover, the performance of our method with constant prompts is close to that of directly learning a constant soft prompt. Therefore, we can conclude that the advantage of our method over CSP comes from its adaptation to the dynamics of input instead of incorporating a transformer-based generator straightforwardly. Impact of Prefix Size. To evaluate the impact prefix size, we set the length of prompt N to 25 and vary the prefix size for GPT-Neo (2.7B), Pythia (2.8B), Pythia (6.9B) and StarCoderBase (7B). The results in terms of Exact ER are shown in the first row of Figure 4. Our method can outperform the two representative baselines consistently over all the settings of prefix size across diverse LLMs and datasets. Moreover, the amount of extracted data increases along with the increase in the prefix size, consistent with the existing works (Carlini et al., 2023; Ozdayi et al., 2023). Impact of Length of Prompt. To evaluate the impact length of prompt N, we set the prefix size L to 50 and vary the length of prompt N for GPT-Neo (2.7B), Pythia (2.8B), Pythia (6.9B) and StarCoderBase (7B). The results in terms of Exact ER are shown in the second row of Figure 4. According to the results, we have several observations. Firstly, our method can outperform the two representative baselines consistently over all the settings of length of prompt N across diverse LLMs and datasets. Secondly, the performance of our method usually increases rapidly when increasing the length of prompt N from a small value (e.g., 5) to a moderate value (e.g., 25). Then the performance improvement usually becomes smaller when the length of prompt N is increased further. And it tends to saturate when the length of prompt N reaches a relatively large value (e.g., 50 or 75)."}, {"title": "5 Conclusion", "content": "We propose a novel method to unlock memorization in large language models (LLMs) which was underestimated by previous methods. More specifically, a transformer-based generator is developed to customize the dynamic, prefix-dependent soft prompts to measure the LLM memorization. It can have a more precise detection of memorized data, capturing the data omitted by the previous methods only relying on the prefixes or the concatenation of a constant soft prompt and prefixes. Extensive experiments are conducted to show that our method can outperform the state-of-the-art techniques by a large margin under diverse settings, including text generation and code generation tasks."}, {"title": "6 Limitations", "content": "There are several limitations of our work. First, we primarily focus on the memorization of pretrained LLM over the pretraining dataset and show that our method can extract more training data. However, it has been shown that fine-tuned LLMs also have memorization on fine-tuning dataset (Zeng et al., 2023). Therefore, the effectiveness of our method under the fine-tuning settings remains unexplored, including fine-tuning on a single task and multiple tasks. Second, we observed the saturation phenomenon in the ablation study on the length of prompt. The reason for the saturation remains unknown. And further studies on saturation might help extract more data with our method and thus provide better measurement of memorization. Third, based on the experimental results, we can observe that the improvement of our method on the fractional extraction rate is smaller and less robust compared with the improvement in the exact extraction rate. One possible reason is that the aligned CLM loss to train the generator is more suitable for the optimization of verbatim memorization. Since fractional extraction rate may be more important in cases where the meaning of the extracted sequences is more important than the exact match, it is valuable to improve the performance of our method on the metric of fractional extraction rate."}, {"title": "7 Ethical Considerations", "content": "In this work, we propose to leverage dynamic soft prompts to extract more training data from the target LLM and measure its memorization under the white-box settings. Therefore, it is possible that the attackers might utilize our method to extract sensitive data from the target LLM if they have white-box access to the target LLM. However, the main purpose of this work is to raise awareness among LLM researchers and developers about the security concerns caused by LLM memorization. By utilizing our method to evaluate the memorization of the target LLM, the owner of the LLM can evaluate its security vulnerability more accurately and thoroughly and then take action to defend against it. For example, we mentioned in the paper that the developer can utilize machine unlearning to forget the sensitive training data that is identified by our method. To minimize the security issues caused by our work, all of our experiments are conducted on public datasets that have been extensively studied by the research community."}]}