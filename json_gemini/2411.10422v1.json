{"title": "Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash", "authors": ["Parsa Hejabi", "Elnaz Rahmati", "Alireza S. Ziabari", "Preni Golazizian", "Jesse Thomason", "Morteza Dehghani"], "abstract": "Large Language Models (LLMs) have shown impressive capabilities in complex tasks and interactive environments, yet their creativity remains underexplored. This paper introduces a simulation framework utilizing the game Balderdash to evaluate both the creativity and logical reasoning of LLMs. In Balderdash, players generate fictitious definitions for obscure terms to deceive others while identifying correct definitions. Our framework enables multiple LLM agents to participate in this game, assessing their ability to produce plausible definitions and strategize based on game rules and history. We implemented a centralized game engine featuring various LLMs as participants and a judge LLM to evaluate semantic equivalence. Through a series of experiments, we analyzed the performance of different LLMs, examining metrics such as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The results provide insights into the creative and deceptive capabilities of LLMs, highlighting their strengths and areas for improvement. Specifically, the study reveals that infrequent vocabulary in LLMs' input leads to poor reasoning on game rules and historical context.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently been employed as agents in various complex tasks, showcasing their potential in dynamic, interactive environments (Dorbala et al., 2024; Singh et al., 2024). This has led to a growing interest in LLM-based multi-agent systems (LLM-MA), particularly within the realm of gaming (Mukobi et al., 2024; Xu et al., 2023). Games offer a structured yet flexible platform to analyze and understand LLM behavior under diverse scenarios (Light et al., 2023). Currently, LLMs are typically evaluated through static tasks (Lee et al., 2023; Zhao et al., 2024; G\u00f3mez-Rodr\u00edguez and Williams, 2023). Traditional games like Avalon (Wang et al., 2023) and Werewolf (Xu et al., 2024) have also been used to benchmark LLMs, focusing on logical reasoning and strategic interaction. These games require players to engage in deception, deduction, and negotiation, providing valuable insights into LLMs' decision-making processes. However, these studies often overlook the assessment of creativity.\nTo address this gap, we introduce a simulation framework for the game Balderdash. In this game, players generate plausible yet fictitious definitions for obscure terms, aiming to deceive other players while identifying the correct definitions. We argue that Balderdash can be used to evaluate both the creativity and logical reasoning of LLMs, challenging the models to balance these two crucial aspects and providing a comprehensive assessment of their capabilities.\nIn this paper, we aim to assess the creativity of LLMs by evaluating their ability to generate plausible definitions for obscure words in Balderdash. We will further examine their logical reasoning skills by observing how effectively they deceive opponents and identify correct definitions in the context of the game. Finally, we will investigate the performance of these models in a multi-agent setting where both creativity and logical deduction are crucial for success."}, {"title": "2 Related Work", "content": "LLMs have demonstrated remarkable success in planning and reasoning capabilities, resulting in the automation of numerous tasks, such as science experiments (Zheng et al., 2023) and software development (Qian et al., 2023; Hong et al., 2023; Dong et al., 2023). The advancement of using an LLM as a planning or decision-making agent has led to significant progress in complex problem-solving and world simulation within LLM-MA systems. One example of world simulation is using memory-based adjustment for LLM agents in games with cooperative or competitive communication paradigms, with either centralized or decentralized communication structures (Guo et al., 2024). For instance, Mukobi et al. (2024) use the Welfare game, where LLM agents balance investing in military units and improving their nations' welfare to evaluate the cooperative capabilities of LLMs.\nAvalon (Wang et al., 2023; Light et al., 2023) and Werewolf (Xu et al., 2024, 2023) are two other games used in this paradigm, both with two groups of roles, good and evil, and the winner is the team that succeeds in eliminating the other. The evil group members have the advantage of knowing each other, while the good group members should rely on behavioral patterns to find other members in their group. The most important capabilities examined in these types of games are deceiving other players and distinguishing between the behavioral patterns of good and evil.\nWang et al. (2023) compare the performance of Recursive Contemplation (ReCon) and Chain-of-Thought (CoT) (Wei et al., 2022) for LLM reasoning in the Avalon game, where agents are evaluated by the gpt-4-0613 model (OpenAI et al., 2024) using six binary labels (concealment, logic, contribution, persuasiveness, information, and creativity), showing the superiority of ReCon. Light et al. (2023) also use Avalon for benchmarking LLMs based on their win rate, showing that while LLMs can deduce information from their discussions with other players, they are not able to strategize accordingly. Xu et al. (2023) use Werewolf to examine the effect of memory (experience pool) and its size on agent adjustment in the game, where models are shown to improve over rounds based on win rate. Xu et al. (2024) also use the Werewolf game to evaluate LLMs combined with reinforcement learning to examine agent adjustment.\nOutside of game simulations, Lee et al. (2023) and Orwig et al. (2024) use Divergent Thinking (DT) to evaluate LLMs' creativity by calculating semantical differences among multiple responses for a specific topic, e.g., describing a new feasible use case for a typical object. DT is defined as a thought process that enables people to explore and think in multiple directions (Guilford, 1967), which aligns with the objectives of the Balderdash game, explained in the next Section."}, {"title": "3 The Original Balderdash Game", "content": "Balderdash is a word game where players aim to create plausible-sounding definitions for rare and unusual words. The game has two objectives: 1. to deceive other players into believing an invented definition is the correct one, and 2. to correctly identify the true definition among those presented. The game also includes a competitive aspect where players advance on a board towards a finish line.\nIn each round of the game, the Dasher (the leader of each round) draws a card from Balderdash's deck of cards, which contains obscure, rare words along with their definitions. The Dasher announces the chosen word to all players, who then write a definition down on their sheets. Players can either write down the true definition (if they know it) or invent a plausible definition they think will convince others.\nOnce all definitions are submitted, the Dasher examines the answers and immediately awards three points to any player whose invented definition closely resembles the true definition. These players do not continue participating in that round. The Dasher then mixes all the remaining invented definitions with the true definition of the word and reads them aloud. Players must vote for the definition they believe is the correct one. Correct guesses are awarded two points, and one point is awarded for each vote a player's definition receives. Additionally, the Dasher receives three points if no player guesses the correct definition. The game continues with a new Dasher each round until one player reaches the finish line on the game board."}, {"title": "4 LLM-MA Balderdash", "content": "We propose a framework where LLMs play the Balderdash game, enabling the benchmarking of their capabilities in generating and evaluating creative content. This framework includes a centralized game engine featuring various LLMs as participants, multiple datasets as the game's word decks, an LLM as the Dasher, and a review of previous rounds given to players as history. These features are discussed in more detail below."}, {"title": "4.1 LLMs as Participants", "content": "In our framework, LLMs play Balderdash against one another. To incorporate a range of different LLMs, we include four open-source, small, instruct-tuned models loaded locally and one large"}, {"title": "4.2 Word Deck", "content": "We created two different datasets used as the Word Decks in our framework. First, we rely on the set of words originally used in the Balderdash game, containing rare and infrequent English words to simulate the actual game. We also created multiple subsets of this dataset containing the words known by each model. According to Kang and Choi (2023), LLMs are biased towards frequent words and co-occurrences, making them vulnerable and unpredictable when infrequent words are used in the input. Therefore, we created another dataset containing the most frequent English words to evaluate LLMs on both frequent and infrequent decks of words."}, {"title": "4.2.1 Balderdash Words", "content": "We created the \"All Balderdash\" dataset containing 225 distinct Balderdash words sourced from the Wordnik dictionary's list of Balderdash game words, complete with all their different definitions and their part of speech tags.\nKnown Balderdash Words: Following Jhirad et al. (2023), we created datasets of words understood by each LLM by inputting every word along with its part of speech from the \u201cAll Balderdash\" dataset into each model five times, using a temperature value of 0.9. Each model is prompted to act as a universal dictionary and provide a definition of each word. Subsequently, we used Llama as a semantic equivalence judge to determine whether each definition was semantically equivalent to the word's actual definition. We explain this choice in Section 4.3. The prompts provided no context about the Balderdash game (all prompts are detailed in Appendix D). If the model affirmed the semantic equivalence of the majority (three or more out of five) of the definitions, the word is labeled as a \"known\" word for that model."}, {"title": "4.2.2 Basic Frequent English Words", "content": "We use the Oxford 3000 word list (Oxford University Press, 2024), containing the most frequent English words. Using the NLTK package (Bird and Loper, 2004), English stopwords are removed from this list, resulting in 2895 words. Then, the Merriam-Webster dictionary API (Merriam-Webster, 2024) is used to obtain the various definitions and part of speech tags of these words. Words that do not have any definitions in the Merriam-Webster dictionary are discarded, resulting in 2865 words. The gathered data is cleaned with regular expressions to remove special tokens as defined in the API's documentation. Given that the words"}, {"title": "4.3 Dasher (Judge)", "content": "The main responsibility of the Dasher is to act as a judge and examine participants' definitions. Following Zheng et al. (2024), where an LLM is used to evaluate open-domain question-answering, we use an LLM as the judge to determine whether each generated definition is semantically equivalent to the reference dictionary definition.\nWe created a dataset (\"Judge Evaluation Data\") to evaluate the best LLM for the Dasher role in the game. This dataset consists of 40 randomly selected words from the \"All Balderdash\" dataset. For each word, GPT was prompted once to provide an accurate definition and again to generate a deceiving definition within the context of the Balderdash game. A human annotator then labeled the GPT-generated definitions (including both correct and deceiving definitions) as \u201cTrue\u201d if they were equivalent to the dictionary definition and \"False\" otherwise. Each LLM was then prompted to do the same task and respond with either \u201cTrue\" or \"False.\" The specific prompts used are detailed in Appendix D.\nBased on the alignment of human labels and each LLM's labels, Llama was chosen as the judge of the game in all experiments. Surprisingly, GPT performed the worst. Further investigation revealed that the \"Judge Prompt\" described in Section 4.5 led GPT to become a very strict judge, resulting in generating \u201cFalse\u201d even for small differences in details. We acknowledge that LLMs might have a self-enhancement bias toward their own output or other machine-generated outputs (Chen et al., 2024; Zheng et al., 2024), resulting in a slightly unfair evaluation.\nIt is also worth mentioning that BERTScore (Song et al., 2021) is another method for calculating semantic distance used in machine translation. However, our experiments detailed in Appendix A demonstrate that it is not feasible to use BERTScore for the judge component."}, {"title": "4.4 History", "content": "To provide the players with a memory-based review of previous rounds' outcomes and a sense of their performance, we give each player a history of each round in the form of a CSV file."}, {"title": "4.5 Game Engine", "content": "We implemented a game engine capable of simulating Balderdash within a multi-agent environment with centralized communication. In this game engine, LLMs are given five categories of prompts (technical details of the game engine and prompts are available in Appendix C and Appendix D):\nGame Rules Prompt: Describes the game rules, scoring rules, and the player's objective, given as a \"system\u201d prompt. For models that do not support the \"system\" role, this prompt is placed at the beginning of the \"user\" prompt.\nHistory Prompt (Optional): Provides a review of a moving window of the previous rounds, given as a \"user\" prompt. This approach is to simulate how a human might recall and adapt their strategy over time. The history is available in two versions: 1. Full History includes detailed information for each round, namely round ID, player rank up to that round, score, word, reference definition, generated definition, semantic equivalence, correct guess indicator, deception ratio, and round winners' strategies. 2. Mini History includes a concise version with round ID, player rank up to that round, score, word, and generated definition.\nGenerate Definition Prompt: Asks the player to generate a definition for a given word based on the game rules, concatenated with the optional history prompt.\nVote on Definitions Prompt: Asks the player to choose the reference dictionary definition for a word among all given definitions during the voting phase, concatenated with the optional history prompt."}, {"title": "5 Evaluation", "content": "Each Balderdash game, denoted as $G_m$, consists of N rounds ($R_m$). In each $G_m$, a constant set of K players participate ($P = {P_1, ..., P_k}$). The set of all players using the lth LLM is denoted as LLM\u2081. Therefore, each player ($p_k$) is a member of one and only one LLM\u2081. $R_m$ contains information about all players participating in the nth round of $G_m$, including \"judge decision\u201d, \u201cllm knows one", "votes": "and \"scores", "votes\" is another mapping containing information on each $p_k$'s vote in the voting phase, either for another player ($p_k$) or for \u201c-1": "representing the reference dictionary definition. \"scores\" is a mapping between each $p_k$ and an integer value indicating $p_k$'s score in $R_m$."}, {"title": "5.1 Metrics", "content": "We define five metrics for each round ($R_m$): 1. True Definition Ratio (TDR), 2. LLM Knows Ratio (LKR), 3. Deception Ratio (DR), 4. Correct Guess Ratio (CGR), and 5. Average Score (AS). $TDR_M(LLM_l)$ represents the ratio of true definitions generated for the announced word in the mth game and the nth round for all players in LLM\u2081.\n$TDR_M(LLM_l) = \\frac{\\sum_{p_k \\in LLM_l} \\mathbb{1}_{R_m(judge\\ decision)[p_k]}}{|LLM_l|}$\nLKR measures the ratio of instances where the LLM aims to generate the true definition.\n$LKR_M(LLM_l) = \\frac{\\sum_{p_k \\in LLM_l} \\mathbb{1}_{R_m(llm \\ knows \\ one)[p_k]}}{|LLM_l|}$\nThe metrics DR and CGR are designed to evaluate the performance of each LLM in the voting phase. DR measures the success ratio of LLMs in deceiving other players.\n$DR_M(LLM_l) = \\frac{1}{|LLM_l|} \\sum_{p_k \\in LLM_l} \\frac{\\sum_{u \\in R_m(votes)} \\delta(v, p_k)}{|R_m(votes)| - 1}$\nCGR reflects the LLMs' ability to identify the reference dictionary definition amidst deceiving ones.\n$CGR_M(LLM_l) = \\frac{\\sum_{p_k \\in LLM_l} \\mathbb{1}(R_m(votes)[p_k], -1)}{|LLM_l|}$\nAS is the average score achieved by an LLM. This metric also represents a weighted summation of TDR, DR, and CGR, where the weights are determined by the game's scoring rules.\n$AS_M(LLM_l) = \\frac{\\sum_{p_k \\in LLM_l} R_m(scores)[p_k]}{|LLM_l|}$\nThe above metrics are used to assess the overall performance of LLMs in the LLM-MA Balderdash game. In cases where there is a dominant strategy that allows players to get the most points, such as generating the correct definition when the correct definition score is set to a high value, we define convergence to assess the LLM's strategy. The goal of convergence is to determine if the model"}, {"title": "6 Experiments & Results", "content": "To evaluate the LLMs' performance and strategy, we conduct three experiments. The first experiment provides a leaderboard of LLMs based on their proficiency in playing the original Balderdash game. The second experiment investigates whether LLMs learn from their history and converge to follow the most rewarding strategy. The final experiment targets LLMs' ability to reason over game rules and choose the best greedy choices."}, {"title": "6.1 Leaderboard Experiment", "content": "In this experiment, we aim to create a leaderboard for LLMs by having these models play Balderdash against each other. To keep the game fair, only models of comparable size (namely Llama, Phi, Gemma, and Mistral) are used. Using more advanced models would disrupt the game flow, as smaller models wouldn't be able to rise in the rankings and consequently learn from their history. Each game with four players representing four LLMs is run five times using five different subsets of words to ensure that the chosen set of words does not affect the results. This experiment is conducted with three types of history (none, mini, and full) and two datasets (\"Basic Frequent English Words\" and \"All Balderdash\u201d) to examine the models' performance on both frequent and infrequent English words.\nThe results for \u201cBasic Frequent English Words,\u201d indicate a considerable improvement for all models as the history becomes more informative. The only metric that decreases is CGR. As LKR approaches 1.0 for all models with increasing history, the ratio of rounds with more than one correct definition in the voting phase also increases. This could lead to confusion for all players and possibly result in a drop in CGR because the definitions in the voting phase are true definitions of the word but not the reference one used by the judge.\nThe results for \"All Balderdash\" are shown in . Contrary to the \"Basic Frequent English Words\" results, consistent improvement is not observed for all LLMs. A possible reason could be the infrequency of the words in this dataset. In almost all settings, Phi performs strongly in finding the correct definition during the voting phase,"}, {"title": "6.2 Convergence Experiment", "content": "Although the leaderboard provides some insight into LLMs' performance, evaluating their strategies and understanding their behavior remains challenging. Therefore, this experiment aims to evaluate LLMs' reasoning and strategy in an environment where a dominant method for maximizing scores exists based on the history of past rounds. The dataset used in this experiment is limited to \"Known Balderdash Words\" for each LLM, and the game is run with three players using the same LLM (including GPT). Considering that the players know the definitions of the announced words, we hypothesize that in each game, the LLMs' LKR will converge since generating the true definition is the most rewarding strategy. Similar to the first experiment, each game is run five times with five different subsets of the dataset. Only two types of history (mini and full) are used in this experiment.\ndepicts LKRn over rounds, showing that none of the models converge, contrary to our hypothesis. The plots show a reduction in fluctuations for the full history setting compared to the mini history, but still, there is no improvement or trend for any of the models over rounds. This phenomenon could be due to the infrequency of the words in the dataset or a weakness of these LLMS in finding or repeatedly using the best strategy."}, {"title": "6.3 Game Rules Experiment", "content": "The final experiment aims to assess LLMs' ability to understand and reason over the game rules without providing history. This experiment is conducted with one player, using the \u201cKnown Balderdash Words\" dataset for each LLM, and two distinct rule sets: 1. awarding fifty points for generating the true definition, and 2. awarding zero points points in each round."}, {"title": "7 Conclusion", "content": "Current LLM-MA game simulations overlook the assessment of creativity in LLMs. This study introduces a systematic framework through the Balderdash game to probe aspects of creativity, deception, and logical reasoning inherent in these models. Our initial assumption was that LLMs are familiar with most Balderdash words and can learn the patterns in machine-generated deceiving definitions, thereby enabling them to generate the correct definitions of words and choose the dictionary definition in the voting phase of Balderdash.\nContrary to our expectations, LLMs are not familiar with more than half of the Balderdash words and perform poorly during the voting phase. None of the models used in the experiments showed signs of correct reasoning based on game rules or strategy convergence derived from historical context. Interestingly, this phenomenon is more pronounced with Balderdash words (infrequent English words) compared to more frequent English words, suggesting that LLMs are more susceptible to failure in reasoning when encountering infrequent vocabulary.\nThe best judge among all LLMs we tested was Llama, which had the best alignment with human labels. Based on the leaderboard experiment, Phi performed strongly in finding the correct definition during the voting phase, suggesting its potential for detecting disinformation. Furthermore, Mistral showed the best overall performance in deceiving its opponents, likely due to its creativity in generating deceptive definitions."}, {"title": "Limitations", "content": "The judge in the LLM-MA Balderdash plays a crucial role in both running the game and evaluating its outcomes. Consequently, the accuracy of the judge is a critical factor in our work. In the current version of the game engine, an LLM serves as the judge. However, an alternative could involve replacing the LLM judge with a specialized model specifically trained to discriminate between true and deceiving definitions. This replacement would likely result in higher accuracy and a more reliable game simulation system.\nAdditionally, the possibility of self-enhancement bias should be considered when using an LLM as the judge. To evaluate this bias, we can assess each LLM as the judge on definitions generated not only by GPT (the model we're using) but also by all other LLMs employed in our work. By comparing error rates across different sets of generated definitions, we can gain insights into how biased these models are toward their own output."}]}