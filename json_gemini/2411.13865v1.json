{"title": "HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation in Recommender Systems", "authors": ["Qiyao Ma", "Menglin Yang", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Rex Ying"], "abstract": "Modern recommendation systems often create information cocoons, limiting users' exposure to diverse content. To enhance user experience, a crucial challenge is developing systems that can balance content exploration and exploitation, allowing users to adjust their recommendation preferences. Intuitively, this balance can be achieved through a tree-structured representation, where depth search facilitates exploitation and breadth search enables exploration. However, current works face two challenges to achieve this target: (1) Euclidean methods fail to fully capture hierarchical structures and lack flexibility in balancing exploration-exploitation, while (2) hyperbolic approaches, despite better hierarchical modeling, suffer from insufficient semantic alignment due to their reliance on Euclidean text encoders. To address these challenges, we propose HARec, a hyperbolic representation learning framework that jointly aligns user-item collaborative information with textual descriptions in hyperbolic space. Our framework introduces two key technique novelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables better hierarchical representation, and (2) a hyperbolic hierarchical tree structure that facilitates user-adjustable exploration-exploitation trade-offs. Extensive experiments demonstrate that HARec consistently outperforms both Euclidean and hyperbolic baselines, achieving up to 5.49% improvement in utility metrics and 11.39% increase in diversity metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "As online platforms grow exponentially, users are overwhelmed by an influx of information, resulting in information cocoons that restrict their exposure to diverse and novel content [4, 15].\nTraditional recommendation approaches have primarily relied on collaborative filtering and interaction data [10, 13], where users with similar historical preferences are expected to share future interests [6]. However, these methods face two critical challenges in improving user experience. First, they often fail to capture the rich semantic relationships between items and evolving user preferences. For example, a jazz enthusiast might wish to explore rock music, yet current models struggle to understand these semantic connections and adapt to such evolving preferences (as illustrated in Figure 1).\nSecond, existing approaches inadequately capture the underlying hierarchical structure inherent in user-item networks [26, 45], a critical factor for improving performance and personalization.\nThe emergence of large language models (LLMs) offers a solution to the limitations of semantic understanding. However, integrating LLM-derived semantic features with traditional collaborative signals remains challenging, especially when capturing the hierarchical structure of user preferences and item relationships.\nHyperbolic space has emerged as a promising solution to these issues. It is a powerful tool for modeling tree-like structures and power-law distributions [14, 24, 35, 36], where its exponential volume growth naturally aligns with hierarchical, scale-free structures [5, 9, 17, 37, 43]. In hyperbolic space, distances increase exponentially from the origin, allowing popular items or exploratory users to be positioned closer to the origin, while niche items or users with focused interests spread toward the boundaries. This spatial configuration enables hyperbolic graph neural network (GNN) models to perceive hierarchical relationships effectively, enhancing recommendation performance [24, 35, 36].\nHowever, current hyperbolic recommendation systems still face two key limitations. First, hyperbolic approaches often lack proper"}, {"title": "2 RELATED WORK", "content": "In this section, we review commonly used graph-based collaborative filtering approaches, large language models (LLMs) for recommendation, and hyperbolic representation learning methods."}, {"title": "3 PRELIMINARIES", "content": "Embeddings learned from the above graph collaborative filtering are by default in the Euclidean space, which is unable to capture tree-like structures. To address the problem, we introduce how to derive these embeddings in a hyperbolic space.\nRiemannian Manifold. A Riemannian manifold (M, g) is a smooth manifold that features an inner product $g_x: T_xM \\times T_xM \\rightarrow \\mathbb{R}$ at each point $x \\in M$. This inner product, known as the Riemannian metric, defines geometric characteristics of the space such as angles and curve lengths. The curvature of Riemannian manifolds results in different types of geometries: elliptic geometry with positive curvature, Euclidean geometry with zero curvature, and hyperbolic geometry with negative curvature. In this work, we focus on hyperbolic geometry, characterized by negative curvature. Hyperbolic space can be described by several equivalent models, each offering unique properties while being mathematically equivalent. The Lorentz model, also referred to as the hyperboloid model, is one of the commonly used representations of hyperbolic space.\nLorentz Model. An n-dimensional Lorentz manifold with negative curvature -1/\u043a (\u043a > 0) is described as the Riemannian manifold $(\\mathbb{H}, g_{\\mathcal{L}})$. Here, $\\mathbb{H}^n = {x \\in \\mathbb{R}^{n+1} : (x, x)_{\\mathcal{L}} = -\\kappa, x_0 > 0}$, $g_{\\mathcal{L}} = \\eta$ with $\\eta = I_n$ except $\\eta_{0,0} = -1$, and $(\\cdot, \\cdot)_{\\mathcal{L}}$ represents the Lorentzian inner product, which is defined as by\n$(x, y)_{\\mathcal{L}} := -x_0 y_0 + \\sum_{i=1}^n x_i y_i$. \t (1)\nA tangent space is an n-dimensional vector space that approximates Euclidean space to hyperbolic space. Specifically, given a point $x \\in \\mathbb{H}^n$, there exists a tangent space $T_x\\mathbb{H}^n$ that approximates $\\mathbb{H}^n$:\n$T_x\\mathbb{H}^n := {v \\in \\mathbb{R}^{n+1} : (v, x)_{\\mathcal{L}} = 0}$. \t (2)\nThe tangent space is defined to be orthogonal to x with respect to the Lorentzian inner product, therefore making the approximation accurate at the reference point.\nWe also need to define transitions between $\\mathbb{H}^n$ and $T_x\\mathbb{H}^n$, which give rise to the exponential and logarithmic maps. For $x \\in \\mathbb{H}^n$ and $v \\in T_x\\mathbb{H}^n$ with $v \\neq 0$ and $\\gamma \\neq x$, there exists a unique geodesic $\\gamma: [0, 1] \\rightarrow \\mathbb{H}^n$ such that $\\gamma(0) = x$ and $\\gamma'(0) = v$. The exponential map $\\exp_x: T_x\\mathbb{H}^n \\rightarrow \\mathbb{H}^n$ is given by $\\exp_x(v) = \\gamma(1)$:\n$\\exp_x(v) = \\cosh(\\frac{||v||_{\\mathcal{L}}}{\\sqrt{\\kappa}}) x + \\sqrt{\\kappa} \\sinh(\\frac{||v||_{\\mathcal{L}}}{\\sqrt{\\kappa}}) \\frac{v}{||v||_{\\mathcal{L}}}$,  (3)\nwhere $||v||_{\\mathcal{L}} = \\sqrt{(v, v)_{\\mathcal{L}}}$ denotes the Lorentzian norm of v. The logarithmic map $\\log_x$ is the inverse of the exponential map $\\exp_x$:\n$\\log_x(y) = \\frac{d(x, y)}{\\sqrt{\\kappa}} \\frac{y + (x, y)_{\\mathcal{L}} x}{||y + (x, y)_{\\mathcal{L}} x||}$.\t (4)\nwhere $d(x, y)$ represents the distance between x and y in $\\mathbb{H}^n$:\n$d(x, y) = \\sqrt{\\kappa} \\operatorname{arcosh} (-(x, y)_{\\mathcal{L}} / \\kappa)$.\t (5)\nWe choose $o := {\\sqrt{\\kappa}, 0, ..., 0} \\in \\mathbb{H}^n$ as the reference point for all the operations, making the transitions reproducible. In this work, we set $\\kappa = 1$, resulting in a curvature of -1. Therefore, in the following discussions, we would omit \u043a for simplicity."}, {"title": "4 METHODOLOGY", "content": "In this section, we present a comprehensive overview of our proposed model, HARec. Our approach not only aims to deliver accurate recommendations but also addresses users' growing demand for more diverse suggestions. Furthermore, by constructing a hierarchical tree from the learned embeddings, we introduce an additional mechanism that enables users to control the degree of exploration in the recommended items. The overall framework is in Figure 2."}, {"title": "4.1 Hyperbolic Graph Collaborative Filtering", "content": "Similar to Euclidean space, hyperbolic graph collaborative filtering captures high-order dependencies between users and items via a message aggregation mechanism. In our method, we use the Lorentz representation for both users and items.\nHyperbolic Embedding Initialization. As discussed in section 3, we fix the origin $o := {\\sqrt{\\kappa}, 0, ..., 0} \\in \\mathbb{H}^n$ and use it as the reference point. We adopt the Gaussian sampling method for the initialization of embeddings in Euclidean space, after that, by injecting them into the tangent space of the reference point, we derive the initial hyperbolic embeddings of both users and items:\n$\\mathbf{z}_u = (0, \\mathbf{e}_u), \\quad \\mathbf{h}_u = \\exp_o(\\mathbf{z}_u), \\qquad \\mathbf{z}_i = (0, \\mathbf{e}_i) \\quad \\mathbf{h}_i = \\exp_o(\\mathbf{z}_i)$\t (6)\nwhere $\\mathbf{e}_u, \\mathbf{e}_i$ are Gaussian distributed initialized Euclidean embeddings, and $\\mathbf{z}_u, \\mathbf{z}_i$ are corresponding embeddings in the tangent space of the origin point. $\\mathbf{h}_u, \\mathbf{h}_i$ therefore stands for the resulting initialized hyperbolic embeddings.\nHyperbolic Message Passing. Message passing between each node is realized through hyperbolic neighborhood aggregation, which is used to extract explicit user-item interaction. Given user u, item i, as well as their neighborhood $\\mathcal{N}_u, \\mathcal{N}_i$, their embeddings at layer l in the tangent space are updated as follows:\n$\\mathbf{z}_u^{l} = \\frac{1}{\\sqrt{|\\mathcal{N}_u|}} \\sum_{i \\in \\mathcal{N}_u} \\mathbf{z}_i^{l-1}, \\quad \\mathbf{z}_i^l = \\frac{1}{\\sqrt{|\\mathcal{N}_i|}} \\sum_{u \\in \\mathcal{N}_i} \\mathbf{z}_u^{l-1}$\t (7)\nwhere $\\mathcal{N}_u$ and $|\\mathcal{N}_i|$ represent the number of neighbors of u and i, respectively. The final tangent space embedding is derived by summing up embeddings at each layer:\n$\\mathbf{z}_u = \\sum_l \\mathbf{z}_u^l, \\quad \\mathbf{z}_i = \\sum_l \\mathbf{z}_i^l$\t (8)\nSince $\\mathbf{z}_u$ and $\\mathbf{z}_i$ are on the tangent space, we still need to project it into hyperbolic space to derive the hyperbolic embeddings:\n$\\mathbf{h}_u = \\exp_o(\\mathbf{z}_u), \\quad \\mathbf{h}_i = \\exp_o(\\mathbf{z}_i)$.\t (9)\nHyperbolic Prediction Function. Through hyperbolic messaging passing, we've captured high-order relational information into hyperbolic embeddings. Hyperbolic distance is now used to define a prediction function, which is used to predict the possibility that a user would interact with an item:\n$p(u, i) = \\frac{1}{d(\\mathbf{h}_u, \\mathbf{h}_i)}$\t (10)\nHyperbolic Margin Ranking Loss. Bayesian Personalized Ranking (BPR) loss has been widely applied in recommender systems [23], aiming to pull together user-item pairs with recorded interactions while pushing apart those without. In our model, we adopt the"}, {"title": "4.2 Semantic Representations Generation", "content": "In this subsection, we introduce a paradigm for distilling useful information from raw text descriptions of items and users into dense semantic embeddings, which will be used later in our alignment framework that derives the final embeddings, integrating both collaborative information and semantic insights. Inspired by prior research [19, 22] that leverages the power of large language models (LLMs) to extract useful user/item descriptions from raw, noisy textual data, we adopt a similar framework to generate user/item profiles with rich textual descriptions:\n$P_i = \\text{LLMs}(S_i, T_i)$ \t (13)\nwhere $S_i$ represents system prompts to the LLMs, $T_i$ denotes raw textual information, and $P_i$ is the resulting user/item profile, consisting of sentence-level descriptions.\nWhile we've derived textual information for users/items, it cannot be directly fused with the embeddings generated from hyperbolic collaborative filtering. To integrate both types of knowledge, we encode the textual information into embeddings using pre-trained text encoder models, such as BERT [7]:\n$\\mathbf{e}_i = \\text{Encoder}(P_i)$ \t (14)"}, {"title": "4.3 Hyperbolic Alignment", "content": "Given two sets of embeddings: hyperbolic embeddings with collaborative information and semantic embeddings with textual information, we aim to maximize the mutual information between them, creating unified embeddings that integrate both types of knowledge. To achieve this, we introduce a hyperbolic alignment loss, which mitigates the effects of noisy signals in representation learning and improves recommendations for tail items by aligning information within the same hyperbolic space. A theoretical proof of the advantages of alignment in hyperbolic space over Euclidean space is provided in the next subsection.\nBefore calculating the alignment loss, two steps are necessary: 1) the semantic embeddings must be adjusted to match the dimensionality of the collaborative embeddings, and 2) they should be transformed into a common representation space. To accomplish this, we first apply a multi-layer perceptron (MLP) to the semantic embeddings to ensure dimensional alignment. We then project the semantic embeddings into hyperbolic space so they can align with the hyperbolic collaborative embeddings:\n$\\mathbf{s}_i = \\exp_x(\\mathbf{e}_i)$\n$\\mathbf{s}'_i = \\text{MLP}(\\mathbf{s}_i)$ \t (15)"}, {"title": "4.4 Gradient Analysis of Semantic Alignment.", "content": "To further explore the impact of aligning semantic embeddings within the hyperbolic model, we analyze the gradient behavior with respect to nodes of varying norms in Proposition 4.1. This provides insight into the hierarchical preservation properties of the hyperbolic space, which are absent in Euclidean space.\nPROPOSITION 4.1. The gradient magnitude of semantic alignment in hyperbolic space $|\\nabla_{\\mathbf{x}} d_{\\mathcal{H}}| \\sim \\frac{|\\mathbf{y}|}{|\\mathbf{x}|(1-\\cos\\theta)}$ enables adaptive updates for hierarchical preservation, while Euclidean space maintains constant magnitude $|\\nabla_{\\mathbf{x}} d_{\\mathcal{E}}| = 1$, where the $d_{\\mathcal{H}}$ and $d_{\\mathcal{E}}$ represent the hyperbolic distance and Euclidean distance, respectively.\nHyperbolic Space. Suppose x is a hyperbolic embedding derived from 4.1, and y is the semantic embedding from 4.2. The hyperbolic distance is given by:\n$d_{\\mathcal{H}}(\\mathbf{x}, \\mathbf{y}) = \\text{arccosh}(z)$ \t (17)\nwhere $z = -(\\mathbf{x}, \\mathbf{y}) = x_0 y_0 - \\sum_{i=1}^n x_i y_i$. For large $|\\mathbf{x}|$ and $|\\mathbf{y}|$, given $x_0 = \\sqrt{1 + ||\\mathbf{x}||^2}$ and $y_0 = \\sqrt{1 + ||\\mathbf{y}||^2}$, we adopt the following approximations for further calculation:\n$x_0 \\approx ||\\mathbf{x}||, \\quad y_0 \\approx ||\\mathbf{y}||, \\quad \\sqrt{z^2 - 1} \\approx z$\t (18)\n$z = x_0 y_0 - \\sum_{i=1}^n x_i y_i \\approx ||\\mathbf{x}|| ||\\mathbf{y}|| (1 - \\cos \\theta)$ \t (19)\nLet $x_i = ||\\mathbf{x}|| \\hat{x}_i$ and $y_i = ||\\mathbf{y}|| \\hat{y}_i$ (where $\\hat{x}_i$ represents the direction of $x_i$). The partial derivative of z with respect to $x_i$ is:\n$\\frac{\\partial z}{\\partial x_i} = \\frac{x_i}{x_0} y_0 - y_i \\approx ||\\mathbf{y}|| (\\hat{x}_i - \\hat{y}_i)$ \t (20)\nThus, the gradient norm of $d_{\\mathcal{H}}$ for x can be approximated by:\n$||\\nabla_{\\mathbf{x}} d_{\\mathcal{H}}|| = \\frac{||\\nabla_{\\mathbf{x}} z||}{\\sqrt{z^2-1}} \\approx \\frac{||\\mathbf{y}|| ||\\hat{\\mathbf{x}} - \\hat{\\mathbf{y}}||}{| ||\\mathbf{x}|| ||\\mathbf{y}|| (1-\\cos\\theta) |} = \\frac{| ||\\hat{\\mathbf{x}} - \\hat{\\mathbf{y}}|| }{||\\mathbf{x}|| (1-\\cos\\theta)}$ \t (21)\n$\\approx \\frac{| ||\\hat{\\mathbf{x}} - \\hat{\\mathbf{y}}|| }{||\\mathbf{x}|| (1-\\cos\\theta)}$\nThis result indicates that nodes with large norms undergo smaller gradient updates, preserving local structures, while nodes with smaller norms adjust more, refining global hierarchical relationships and capturing hierarchical structure effectively.\nEuclidean Space. For comparison, we examine the gradient behavior in Euclidean space. The Euclidean distance is defined as:\n$f(\\mathbf{x}) = d_{\\mathcal{E}}(\\mathbf{x}, \\mathbf{y}) = ||\\mathbf{x} - \\mathbf{y}||$ \t (22)\nThe gradient of f (x) with respect to x is:\n$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\frac{\\mathbf{x}-\\mathbf{y}}{||\\mathbf{x}-\\mathbf{y}||} = \\frac{\\mathbf{x}-\\mathbf{y}}{d_{\\mathcal{E}}(\\mathbf{x}, \\mathbf{y})}$ \t (23)\nwhich has a magnitude of:\n$||\\nabla_{\\mathbf{x}} f(\\mathbf{x})|| = 1$\t (24)"}, {"title": "4.5 Hyperbolic Hierarchical Structure", "content": "In this subsection, we introduce a mechanism that allows users to control the level of exploration in their recommendations by constructing a hierarchy tree based on the learned hyperbolic embeddings, an example is shown in Figure 3. These embeddings integrate both collaborative information and language semantics, enabling more nuanced recommendations.\nHierarchy Tree. The primary motivation for constructing a hierarchy tree is to uncover hidden structures in the data that are not explicitly reflected in the dataset. For instance, consider the Amazon-CD dataset: a CD might be categorized under genres like \"folk music\", but it cannot be labeled as \"folk music preferred by teenagers\" or further refined to \"folk music preferred by teenagers who live in large cities\". However, in reality, such nuanced preferences exist within the collaborative information. When recommending based solely on past interactions, we may overlook these hidden layers of structure that are implicit in the dataset (e.g., \u201cmusic\u201d \u2192 \u201cfolk music\u201d \u2192 \u201cfolk music preferred by teenagers\u201d \u2192 \u201cfolk music preferred by teenagers who live in large cities\u201d).\nAs previously discussed, in hyperbolic space, the norm of the embeddings reflects the level of exploration, where depth search facilitates exploitation and breadth search enables exploration. Building on this concept, we use the learned embeddings to form the bottom layer of the hierarchy tree. We then apply K-Means clustering in hyperbolic space to derive higher-level cluster nodes, which act as pseudo clustering nodes within the hierarchy, representing group preferences. For example, if an item is classified as \"folk music preferred by teenagers who live in large cities\u201d, its parent cluster may represent \"folk music preferred by teenagers\", while its sibling clusters could include \u201cfolk music preferred by teenagers who live in small towns\u201d. This hierarchical clustering process continues iteratively until reaching the root, thereby creating a multi-level tree structure that captures different levels of exploration.\nExploration-Exploitation Balance. To allow users control over the balance of exploration and exploitation, we introduce two adjustable parameters: temperature t and hierarchy level l. In the standard recommendation approach, items are recommended based on similarity to the user's embedding, which may limit exploration. For users with focused preferences, this method may consistently recommend items that align too closely with their past choices, offering little opportunity to explore different categories."}, {"title": "5 EXPERIMENTS", "content": "To address this issue, we modify the recommendation process by replacing a portion of the recommended items with alternatives from other branches of the hierarchy tree. This modification gives users control over the balance between exploration and exploitation. Temperature $\\tau$ allows users to specify the proportion of original recommendations to replace, while level $l$ determines the extent of exploration within the hierarchy. For example, if a user selects $\\tau = 0.5$ and $l = 2$, half of the originally recommended items will be retained, while the other half will be replaced by items sampled from nodes two levels up in the hierarchy tree, with selections made from the connected bottom-layer nodes."}, {"title": "5.1 Experimental Settings", "content": "Datasets. To evaluate our proposed model, we utilize three widely-used public datasets: Amazon-books, which contains user purchase behaviors within the book category on Amazon; Yelp, which records customer ratings and reviews for restaurants on Yelp; and Google-reviews, which includes user reviews and business metadata from Google Maps. Detailed statistics for these datasets, including the distribution of head and tail items, are provided in Table 3. The high proportion of tail users (T80) across these datasets highlights the prevalence of long-tail distributions, supporting the assumption of a power-law distribution in real-world data.\nEvaluation Metrics. We assess our recommended items using both utility and diversity metrics, demonstrating that our model achieves precise recommendations for users' preference while enhancing diversity in the recommendation list.\nFor utility, we adopt two widely-used ranking metrics: Recall and NDCG, which measure model effectiveness. To evaluate diversity, we employ three metrics: Distance Diversity [40, 46], Shannon Entropy [20], and Expected Popularity Complement [27].\nDistance Diversity (Div) quantifies the variation among recommended items, encouraging diversity within the recommendation:\n$\\text{Div}(u) = \\frac{\\sum_{i \\in R_u} \\sum_{j \\in R_u, j \\neq i} d(i, j)}{\\text{Number of } (i, j) \\text{ pairs}}$\t (25)\nwhere $R_u$ is the recommendation list for user u, and d(i, j) represents the distance between items i and j. Number of (i, j) pairs is calculated as $\\binom{|R_u|}{2} = \\frac{|R_u|(|R_u|-1)}{2}$. For fair comparison, we compute distances within the same representation space across models.\nShannon Entropy (H) quantifies the unpredictability within the recommendations, with higher entropy signifying greater diversity:\n$H = -\\sum_{i \\in \\text{set}(\\cup_u R_u)} p(i) \\log p(i)$ \t (26)\nwhere $\\sum_{u} R_u$ denotes all recommended items across users (including duplicates), and $p(i) = \\text{count}(i) / \\sum_{i \\in \\text{set}(\\cup_u R_u)} \\text{count}(i)$ is the proportion of item i among all recommended items.\nExpected Popularity Complement (EPC) assesses the extent of bias towards popular items by promoting niche recommendations:\n$\\text{EPC} = 1 - \\frac{1}{|\\text{set}(N_u)|} \\sum_{i \\in \\text{set}(\\cup_u R_u)} \\frac{\\text{pop}(i)}{\\text{max}_i(\\text{pop}(i))}$ \t (27)\nwhere $N_u$ is the set of items user u has interacted with, and $\\text{pop}(i) = \\text{count}(i) / \\sum_{i \\in \\text{set}(\\cup_u N_u)} \\text{count}(i)$ denotes item i's popularity."}, {"title": "5.2 Performance Comparison", "content": "Overall Comparison. The performance comparison across all models is presented in Table 1. Our HARec consistently outperforms the baselines in both utility and diversity metrics, achieving up to 5.49% improvement in utility metrics and 11.39% increase in diversity metrics. Notably, most baselines struggle to achieve strong performance in both utility and diversity simultaneously. For instance, while HICF performs second-best in utility metrics on the Amazon-books dataset, outperforming other baselines by a significant margin (excluding our model), it lacks the same advantage in diversity metrics, where several baselines achieve better results. This highlights the superiority of our model, which achieves state-of-the-art performance in both utility and diversity: a feat not previously attained by any baseline.\nBaselines with Feature Enhancement. To ensure a fair comparison, we further enhance Euclidean baselines by integrating feature information. Specifically, we align semantic embeddings with the original Euclidean embeddings following the structure in Section 4.2, but in Euclidean space. The results are presented in Table 2, with enhanced baselines denoted by \"+\". The data shows that our HARec still surpasses all baselines in both utility and diversity. Interestingly, while the feature-enhanced versions of the Euclidean baselines generally improve in utility metrics, they do not exhibit a stable improvement in diversity metrics. This aligns with our theoretical analysis in Section 4.5, which demonstrates the advantages of our alignment model in the hyperbolic space."}, {"title": "5.3 Ablation Study", "content": "We perform an ablation study to examine the contributions of two key components in our model: hyperbolic margin ranking loss and semantic alignment loss. To maintain meaningful evaluation, we focus solely on utility metrics, as random recommendations may"}, {"title": "5.4 Hierarchical Structure Analysis", "content": "To validate the effectiveness of our hierarchical structure, which allows users to control their level of exploration within recommendations, we conducted a series of analyses. Our goal is to demonstrate how the hierarchical model provides a structured balance between exploration and exploitation, allowing users to adjust the generality of recommendations by themselves.\nFirst, we analyze the norms of node embeddings across hierarchical layers. As shown in Table 4, nodes in higher layers have consistently smaller norms. This observation supports our design intention: higher-layer nodes represent broader, generalized group preferences, aligning with findings in Section 4.5. This structure enables more abstract grouping as the layers ascend, thereby capturing progressively broader user interests.\nTo further examine the hierarchy's practical utility, we designed experiments that simulate varying user preferences in daily interactions. Using a temperature parameter $\\tau = 0.2$ to control recommendation randomness, we tested recommendation performance across different hierarchical levels, from layer 0 (original recommendation level) up to layer 3. The results, visualized in Figure 5, reveal a clear pattern: as the level $l$ increases, utility metrics tend to decrease, while diversity metrics increase.\nThis progression highlights the intended trade-off our model offers: recommendations from lower layers (e.g., layer 0) focus on utility and specific user interests, ideal for exploitation. In contrast, higher layers (e.g., layer 3) prioritize diversity by presenting broader, less tailored options, thus encouraging exploration. This systematic change across levels provides users with a flexible mechanism to adjust recommendation focus: from niche, highly relevant items to a more varied, exploratory set-based on their current preference for exploration or exploitation.\nTo underscore the effectiveness of semantic alignment, particularly for cold-start (tail) items, we conducted experiments measuring utility performance on both head and tail items. In our context, head items (H20) refer to the top 20% of items with the highest number of interactions in the training dataset, representing the most popular items. The remaining items are categorized as tail items (T80), which have limited interaction history, representing less frequently engaged (cold-start) items.\nFor simplicity, we compare our model with all hyperbolic baselines and a representative Euclidean baseline (LightGCN). The results, displayed in Table 5, reveal significant improvements, especially for tail items. Compared with hyperbolic baselines without semantic alignment, our model shows a notably larger performance boost on tail items than on head items. This finding indicates that semantic alignment particularly enhances the recommendation of cold-start items, effectively mitigating the cold-start problem for items with limited or no prior interactions."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce HARec, a novel hyperbolic graph-text alignment framework that effectively balances exploration and exploitation by integrating semantic and collaborative information, with theoretical analysis supporting the advantages of this approach within hyperbolic space. Additionally, we propose a unique mechanism that enables personalized control over the exploration-exploitation trade-off through a hierarchical tree structure, allowing users to adjust recommendation preferences dynamically. Extensive experiments demonstrate that HARec achieves state-of-the-art performance, with up to a 5.49% improvement in utility metrics and an 11.39% increase in diversity metrics, making it the first model to excel in both criteria simultaneously, a significant achievement that has yet to be attained by any previous models."}, {"title": "A APPENDIX", "content": "In the supplementary materials, we provide additional details on our datasets and present several case studies illustrating the performance of our decoder module."}, {"title": "A.1 Profile Decoder", "content": "To further evaluate the expressiveness of our learned hyperbolic embeddings, we introduce a profile decoder designed to reconstruct the original profile from these embeddings, thereby broadening their application beyond recommendation tasks. Our method begins by mapping the hyperbolic embeddings back into Euclidean space to facilitate alignment with traditional network layers. These Euclidean-mapped embeddings are then processed through a mixture of experts (MoE) module. The MoE serves to refine the embeddings, ensuring consistency in both dimensionality and semantic representation [19]. Once processed, these embeddings are fed into the language model, specifically after the positional layer, to incorporate them within a sequential architecture. To enhance training efficiency and minimize computational cost, we freeze the language model's parameters, making the MoE module the only trainable component in the architecture.\nDetails of the input prompt structure and processing flow are depicted in Figure 6. To facilitate seamless embedding integration, we introduce two additional tokens, <USER_EMBED> and <ITEM_EMBED>, to the language model's tokenizer. This addition enables these embeddings to be treated as single, coherent tokens, ensuring accurate tokenization and embedding interpretation. After passing through the positional layer, the placeholders are dynamically replaced by the adapted embeddings generated by the MoE, allowing the embeddings to retain context-specific information."}, {"title": "A.2 Decoder Quality", "content": "To assess the effectiveness of our decoder mechanism, we compare the embeddings from our model against those from Euclidean embeddings (LightGCN) and Hyperbolic embeddings (HICF) as inputs to the decoder. The results are shown in Table 6. We use BERTScore [41] to evaluate the semantic alignment of generated sentences with the original profile. Results indicate that our HARec outperforms both Euclidean and hyperbolic baselines, demonstrating that our model's embeddings support recommendation and retain user/item profile information. This finding underscores that our model captures collaborative information alongside recoverable language semantics: a capability not achieved by previous models."}, {"title": "A.2.1 Decoder Case Study.", "content": "To vividly illustrate the semantic capacity of our embeddings, we present two case studies demonstrating the model's ability to recover user and item profiles.\nOriginal user profile:\nBased on the user's interactions and reviews, this user is likely to enjoy unique dining experiences with a variety of options, trendy atmosphere with good drink and food selections, premium quality frozen yogurt with a wide variety of toppings, creative burgers and hot dogs, delicious Thai cuisine, and authentic and spicy Chinese cuisine served family-style."}]}