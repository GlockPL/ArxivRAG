{"title": "From Markov to Laplace: How Mamba In-Context Learns Markov Chains", "authors": ["Marco Bondaschi", "Nived Rajaraman", "Xiuying Wei", "Kannan Ramchandran", "Razvan Pascanu", "Caglar Gulcehre", "Michael Gastpar", "Ashok Vardhan Makkuva"], "abstract": "While transformer-based language models have driven the AI revolution thus far, their computational complexity has spurred growing interest in viable alternatives, such as structured state space sequence models (SSMs) and Selective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown remarkable inference speed ups over transformers while achieving comparable or superior performance on complex language modeling tasks. However, despite these architectural innovations and empirical successes, the fundamental learning capabilities of Mamba remain poorly understood. In this paper, we address this gap by studying in-context learning (ICL) on Markov chains and uncovering a surprising phenomenon: unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator, which is both Bayes and minimax optimal, for all Markovian orders. To explain this, we theoretically characterize the representation capacity of Mamba and reveal the fundamental role of convolution in enabling it to represent the optimal Laplacian smoothing. These theoretical insights align strongly with empirical results and, to the best of our knowledge, represent the first formal connection between Mamba and optimal statistical estimators. Finally, we outline promising research directions inspired by these findings.", "sections": [{"title": "Introduction", "content": "Transformers have been at the forefront of recent breakthroughs in language modeling, driving the AI revolution [VSP+17; RN18; DCLT18]. Despite their empirical success, transformers suffer from high computational complexity, such as quadratic scaling in sequence length during training and linear cache size at inference [GD23a]. To address these limitations, there is a growing interest in designing alternative efficient architectures among which structured state space models (SSMs) are the most prominent. In particular, Selective SSMs such as Mamba and Mamba-2, have achieved state-of-the-art results in various language modeling tasks, while greatly improving the inference throughput [COWSL25].\nMotivated by this success, there is tremendous interest in understanding the sequential modeling abilities of SSMs, especially that of Mamba. In particular, mirroring a theme that has been successful in unraveling fundamental mechanisms (e.g. induction heads) behind transformers [MBN+25; MBG+24; RBMRG24; NDL24; EEGMT24], a growing body of research explores Mamba through its in-context learning (ICL) capabilities [GSSBH24; HGR24; AWKA24]. While these works reveal interesting insights about Mamba's ICL abilities vis-a-vis transformers, they are largely empirical in nature, and we currently lack a fundamental understanding of Mamba and its underlying learning mechanisms. We are thus motivated to ask:\nCan we systematically characterize the ICL capabilities of Mamba?\nTo address this, in this paper we introduce a principled framework for theoretical and empirical analysis of Mamba's ICL capabilities via random Markov chains. Leveraging this framework, we uncover a surprising phenomenon: unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator, which is both Bayes and minimax optimal, for all Markov orders (Fig. 1). Towards explaining this, we theoretically characterize the representation capacity of Mamba and demonstrate that the convolution mechanism plays a fundamental role in realizing the Laplacian smoothing. Further we showcase that these theoretical insights align strongly with empirical results, both on Markovian and complex natural language data. To the best of our knowledge, this is the first result of its kind connecting Mamba and optimal statistical estimators.\nIn summary, we make the following contributions:\n\u2022 We provide a novel framework for a precise theoretical and empirical study of Mamba's ICL via random Markov chains (Sec. 2.1).\n\u2022 Capitalizing our framework, we uncover the surprising fact that the convolution mechanism plays a pivotal role in Mamba's learning abilities (Sec. 3).\n\u2022 Building upon these insights, we characterize the representational capacity of single-layer Mamba and demonstrate that it can represent the optimal Laplacian smoothing estimator for first-order processes (Sec. 4).\n\u2022 We demonstrate the generality of our findings on non-Markovian data and illustrate the fundamental role of convolution even on complex language-modeling tasks (Sec. 5)."}, {"title": "Related Work", "content": "State Space Models [GDERR20; GJG+21] have been recently introduced as an alternative recurrent architecture aimed at rivaling the well established transformer backbone [VSP+17] widely used across a range of domains, from language modeling to vision. The model was originally introduced as a discretization of a time-continuous linear dynamical system [GJG+21]. Recent works tried to re-frame the architecture from a linear recurrent perspective. For example [OSG+23] tried to understand what components of the architecture are necessary, highlighting the importance of linearity, as well as of the particular parametrization that allows control over the stability of the model.\nHowever, there are still many gaps in understanding this family of models [TLA+24], such as questions around expressivity [ODGPS23]. This is particularly important given the proliferation of Mamba-inspired architectures that has emerged since the introduction of the model. For example, Mamba-Spike [QL24] integrates a spiking front-end with the Mamba backbone, leveraging event-driven processing for efficient temporal data handling. The Mixture of Mamba approach combines Mamba's selective state spaces with the Transformer's Mixture of Experts (MoE) layer [CISPM24], potentially offering faster convergence and improved performance scaling. Vision Mamba (Vim) [ZLZ+24] adapts the architecture for visual tasks, employing bidirectional Mamba blocks to process high-resolution images efficiently. Besides specific adaptation of the model into new architectures, the core Mamba block also evolved, for example, by the introduction of gating in Mamba-2 [GD23b] and similar SSM architectures [DSF+24; BPS+24] which for example invalidates to a certain extent the convolutional view of the architecture.\nOur work squarely focuses on understanding the representation power of Mamba, and ICL, which remains an unanswered question. We believe such insights can help navigate the many flavors of SSM architectures. In this space, recent studies have shown that SSMs can perform gradient-based learning for in-context adaptation, similar to transformers [STM+24], where they show that a single SSM layer, augmented with local self-attention, is able to reproduce outputs of an implicit linear model after one step of gradient descent. While some work suggests that Mamba's ICL abilities rival state-of-the-art transformer models [GSSBH24], other research indicates that the pretrained Mamba models perform worse in in-context learning than comparable transformer models [HGR24; AWKA24]. SSMs have also demonstrated promising results in in-context reinforcement learning tasks, offering fast and performant solutions [LSG+24]. [JHLG24] has introduced a novel weight construction for SSMs, enabling them to predict the next state of any dynamical system in-context."}, {"title": "Problem setup", "content": "We formally define the problem setting and provide necessary background. We use the following notation: scalars are denoted by such italic lower case letters as x, y, Euclidean vectors by bold x, y, and matrices by upper case X, Y, etc. || . ||p denotes the lp-norm for p \u2208 [1,\u221e]. 1 refers to the all-one vector. For T\u2208 N, [T] \u2252 {1, ..., T}, and for a sequence (xt)t>1, define x(xk,...,xt) if k > 1 and (x1,...,xt) otherwise. For z \u2208 R, sigmoid(z) \u2252 1/(1 + e\u00afz), ReLU(z) \u2252 max(0, z), and softplus(z) \u2252 log(1+ez). Unif(S) denotes the uniform distribution over a set S and Dir(\u03b2) denotes the Dirichlet distribution with parameter \u03b2 > 0. DKL (P||Q) denotes the KL-divergence between distributions P and Q."}, {"title": "Input data: Random Markov chains", "content": "To investigate the ICL capabilities of Mamba, we let the input tokens to be stochastic and drawn from a random Markov chain of order k [EEGMT24]. That is, the token sequence x = (xt)t=1 \u2208 XT on the state space (vocabulary) X follows the transition dynamics:\n$\\mathbb{P} (x_{t+1} = \\cdot | x_t) = \\mathbb{P}(x_{t+1} = \\cdot | x_{t-k+1})$,\n(1)\nalmost surely for all t \u2208 [T], and the kth-order Markov kernels, $\\mathbb{P} (x_{t+1} = \\cdot | x_{t-k+1} = i_{t-k+1})$, are sampled independently for each tuple $(i_{t-k+1}, \\ldots, i_t)$ from the Dirichlet prior Dir(\u03b2\u00b71), with \u03b2 > 0. When \u03b2 = 1, this corresponds to the uniform distribution on the S-dimensional simplex \u0394, where size S = |X|.\nThe transition matrix $P = (P_{i_1 \\ldots i_k})_{i_1 \\ldots i_k \\in X^k, \\mathbb{P}} \\in [0,1]^S$, encapsulates the set of all Sk conditional probabilities of the chain, each row corresponding to one of them. While this transition matrix governs the generation of each token xt for t > k, the first k-tokens x1,...,xk are drawn i.i.d. from Unif(X). This constitutes the joint law of the random variables (P,x), termed random Markov distribution henceforth. More succinctly,\nData generation (Random Markov sequences).\n1. Draw P with each row sampled i.i.d. from Dir(\u03b2\u00b7 1).\n2. For t = 1, ..., k, sample xt ~ Unif(X).\n3. For t = k,...,T, sample xt+1 ~ Pxk+1\n4. Return the input x = (xt)t=1.\n5. Repeat the above steps to generate a batch {x(b)}b\u2208[B]\u00b7\nRelation to ICL. As a consequence of the generation process, no two sequences x share the same transition matrix P and hence every sequence follows a different Markov distribution. Owing to this fact, even when a model is trained on this random Markovian data, at inference, for every test sequence it has to estimate the next token in-context. Hence this data class serves as a good sandbox to gauge the ICL capabilities of Mamba, which was also used in a similar context for transformers [NDL24]. In this paper, we let the state space X = {0,1} for the ease of exposition and the order k to be any k > 1. Our results and insights can be readily extended to any finite vocabulary, as demonstrated for the natural language in Sec. 5.2."}, {"title": "Mamba architecture", "content": "Selective SSMs such as Mamba and Mamba-2 are a class of sequence-to-sequence models that are closely related to RNNs and classical state space models [GD23b]. A key feature underpinning these models is the selectivity mechanism, enabling them to selectively choose inputs at every timestep, as opposed to linear time-invariant (LTI) systems. While we believe our work captures the behavior of all selective SSMs, we will specifically focus on the state-of-the-art Mamba-2 model to simplify exposition. By slight abuse of terminology, henceforth we will also refer to this model simply as Mamba. Mathematically speaking, Mamba implements the sequence-to-sequence mapping Mamba : RdXT \u2192 RdXT, where given a sequence of input embeddings x = (xt)t=1 \u2208 RdxT of dimension d, it outputs the corresponding output embeddings 0 = (ot)t=1 \u2208 RdxT of the same dimension with o = Mamba(x). More precisely, fix t \u2208 [T]. Then the output ot at time t is computed as ot = Mamba(x) using the following recurrence equations [DG24]:\n$H_t = a_t H_{t-1} + x_t b \\in \\mathbb{R}^{ed \\times N}$,\n$Y_t = H_t c \\in \\mathbb{R}^{ed}$,\n$Z_t = Y_t \\text{ReLU}(W_z x_t) \\in \\mathbb{R}^{ed}$,\n$o_t = W_o z_t \\in \\mathbb{R}^{d}$,\n(Mamba)\nwhere the initial state Ho = 0, Wz \u2208 Red\u00d7d and Wo \u2208 Rd\u00d7ed are learnable parameters, and the input-selective\n$a_t \\triangleq \\exp(-a\\cdot\\Delta_t) \\in (0,1)$, with\n$\\Delta_t \\triangleq \\text{softplus}(w_\\Delta, x_t) + d) \\in \\mathbb{R}$,\n$x_t \\triangleq \\text{ReLU}(\\text{conv}_x (W_x x_{t-w+1})\\cdot \\Delta_t \\in \\mathbb{R}^{ed}$,\n$b_t \\triangleq \\text{ReLU}(\\text{conv}_B (W_B x_{t-w+1})) \\in \\mathbb{R}^{N}$,\n$c_t \\triangleq \\text{ReLU}(\\text{conv}_c(W_c x_{t-w+1})) \\in \\mathbb{R}^{N}$,\n(Input selectivity)\nwhere a > 0, w\u25b3 \u2208 Rd, d\u2208 R, Wx \u2208 Red\u00d7d, WB \u2208 RN\u00d7d and Wc \u2208 RN\u00d7d are all learnable parameters and conv(zt\u2212w+1) is a (learnable) time-wise convolution of window w \u2208 N with distinct kernels per dimension. Here e \u2208 N denotes the expansion factor for the features, typically 2. Let Mamba denote the set of all these parameters.\nIntuition behind Mamba. While the update equations in Mamba might seem complicated at a first glance, the underlying intuition is simple: given a sequence of input embeddings (xt), we first capture their local temporal information using three separate convolutions to compute xt, bt, and ct respectively (Input selectivity). Equipped with this local memory, we perform the classical linear state update to compute the current state Ht from the past Ht\u22121, weighed by an input-dependent decay factor at \u2208 (0,1), and (xt, bt). Subsequently, we compute the state projection yt, modulate it with an input-selective ReLU term to yield zt, and finally project it down to get the d-dimensional output embedding ot. Thus the output ot at time t is a function of the entire input sequence till then, x, yielding ot = Mamba(x).\nMamba-based language model. Mamba block is then incorporated into a full-fledged language model as follows: let x = (x1,x2,\u2026\u2026,xT) \u2208 XT be an input token-sequence over the alphabet X; here X = {0,1} as explained in Sec. 2.1. Then, at every t \u2208 [T], the output of the language model \u03b8 is given by the following sequence of equations [DG24]:\n$X_t = e_{x_t} = x_t e_1 + (1 - x_t) e_0 \\in \\mathbb{R}^d$,\n(Embedding)"}, {"title": "Learning task: next-token prediction", "content": "With the objective of auto-regressively estimating the next token, we train the model parameters 0 to minimize the cross-entropy loss between the next-token predicted probability f\u03b8(x1) and the corresponding ground-truth symbol xt+1 across all the positions t \u2208 [T]:\n$L(\\theta) = \\frac{1}{T} \\sum_{t \\in [T]} \\mathbb{E}_{P} \\mathbb{E}_{x_{t+1} \\sim P} [x_{t+1} \\cdot log f_{\\theta}^{(1)}(x_1) + (1 - x_{t+1}) \\cdot log f_{\\theta}^{(0)}(x_1)]$,\n(2)\nwhere f(x) \u2259 P\u03b8 (xt+1 = j | x1) for j \u2208 {0,1}, and the expectation is both over the transition kernels P and the Markov sequences x = (xt)t=1 sampled from P. In practice, it is replaced by empirical average across a finite set of batches, sampled according to the random Markov distribution in Sec. 2.1. For our experiments we use the AdamW optimizer [KB15]."}, {"title": "Optimal estimator: Laplacian smoothing", "content": "Given the Bayesian prediction loss in Eq. (2), it is natural to ask: what's the optimal \u0472 minimizing it? It follows from a classical result in statistics ([Ris84], \u00a7 A) that this minimum is achieved when the corresponding model prediction matches the (averaged) ground-truth predictive distribution,"}, {"title": "Does Mamba learn in-context estimators?", "content": "To investigate the ICL capabilities of Mamba, we consider the problem setup described above and train a single-layer Mamba using AdamW on the next-token prediction loss in Eq. (2) on random Markov chains. We repeat the same procedure for both single and two layer transformers and report the best performance for all these models (we refer to \u00a7 D for more experimental details). These experiments reveal interesting and rather surprising insights about Mamba:\n1. Mamba learns the optimal Laplacian smoothing estimator on the Markov prediction task, even with a single layer (Fig. 1a).\n2. Convolution mechanism plays a fundamental role in Mamba, aiding in its learning abilities (Fig. 3a)\nIn the sequel, we expand upon these observations in detail.\n1) Mamba learns the Laplacian smoothing. After training, we evaluate the Mamba and transformer models on the same test sequence fixed beforehand and compare their performance to that of the optimal Laplacian smoothing estimator. Specifically, we compare their next-token prediction probabilities with those of the add-\u03b2 estimator. Fig. 1 illustrates these results, which uncovers a surprising phenomenon: even a single-layer Mamba sharply matches the optimal estimator on the whole sequence. In fact, this approximation error is small even for various Markov orders. On the other hand, for transformers we observe that a two-layer model also matches the predictor, albeit less sharply and more variance, whereas a single layer transformers fails to solve the task. This is not fully surprising, given a recent theoretical result [SHT24] that while an induction head (realizing the counting estimator) can be efficiently represented by a two-layer transformer, its single-layer variant needs to be exponentially large.\n2) Convolution is the key. To decipher the key architectural component behind Mamba's success in Markov prediction task, we do an ablation study on its three main features: (i) convolution in Input selectivity, (ii) ReLU non-linearity in Input selectivity, and (iii) the gating mechanism in Mamba and MLP. Amongst them, surprisingly, convolution plays a fundamental role in the model's performance,"}, {"title": "Theoretical results", "content": "Motivated by Mamba's success in learning the optimal smoothing estimator, and convolution's pivotal role in it, here we study how it can represent Laplacian smoothing."}, {"title": "MambaZero: Simplified model", "content": "Building upon the insight that Mamba with just the convolution achieves the same performance as that of the full model (Fig. 3a), we consider its simplified version: MambaZero. MambaZero retains only the essential elements of the full model in Sec. 2.2: the Embedding layer, the convolution inside the Mamba block in Input selectivity, and the Linear layer. More formally, it's given by:\n$X_t = x_t e_1 + (1 - x_t) e_0 \\in \\mathbb{R}^d$,\n(Embedding)\n$u_t = x_t + \\text{MambaZero}(x_t) \\in \\mathbb{R}^{d}$,\n(MambaZero)\n$\\text{logit}_t = W_e u_t \\in \\mathbb{R}^{2}$,\n(Linear)\n$f_{\\theta}(x_1) \\triangleq P_{\\theta} (x_{t+1} = \\cdot | x_1) = (\\text{logit}_t/||\\text{logit}_t||_1) \\in [0,1]^2$,\n(Prediction)"}, {"title": "Main theorem for first-order Markov", "content": "Towards establishing how MambaZero can represent Laplacian smoothing, we start with the random first-order Markov sequences. Our main result is the following.\nTheorem 1 (MambaZero represents order-1 Laplacian smoothing). For the canonical MambaZero model with dimensions d = N = 2, e = 1, and convolution window w = 2, there is a choice of parameters such that the model prediction is arbitrarily close to the Laplacian estimator for random first-order Markov chains. More formally, for any \u03b2 > 0 and \u0454 \u2208 (0,1), there exists a set of parameters \u0472 such that, for all sequences (xt)t>1 and all t > 1,\n$D_{KL} (\\mathbb{P}^{(k)}(x_t) || P_{\\theta} (x_1)) \\le \\epsilon$.\nRemark. The KL divergence above is precisely the penalty paid in the cross-entropy loss at time t (Eq. (2)) when using the predictor P\u03b8 instead of the optimal P1). In other words, the result\n\u03b2\nimplies that the loss of MambaZero can be made arbitrarily close to that of the optimal.\nWe defer the full proof to \u00a7 B and outline the sketch below."}, {"title": "Proof sketch", "content": "Main idea. To build our intuition towards how MambaZero can realize the add-\u03b2 counting estimator for first-order Markov sequences, let's focus on the core MambaZero block. The key observation here is the following: if the state Ht-1 can capture all the transition counts i \u2192 j till xt-1, the new state Ht can be updated to account for the current transition xt-1 \u2192 xt on top of the existing counts, by a suitable choice of at, xt, and bt. Then the relevant count information corresponding to the current prefix xt could be read off from the state projection yt = Htct, and be modified to account for B-smoothing via the Linear and Prediction layers. Buttressing this idea are two key empirical facts, which in fact hold for any k \u2265 1, underpinning our construction:\n(i) State-to-state transition factor at \u2248 1 for all t > 1. We empirically observe that when MambaZero model is trained on random first-order Markov data, at convergence we have at \u2248 1 for all t\u2265 1 (Fig. 5). Since at modulates how much past information flows into the present, at = 1 serves as a natural choice when the states Ht carry count information till current tokens, which we would like to update without any diminishing factor. Note that this can be easily achieved by setting either a or At to be zero in Input selectivity, which we empirically observe as well.\n(ii) Convolution window w > k + 1. Recalling that k is the Markov order, we empirically observe that the window that w = k + 1 is sufficient for the full Mamba to learn the Laplacian smoothing on kth-order Markov chains. To understand why, note that in the MambaZero architecture above, apart from the MambaZero block, all remaining equations operate on the current token at time"}, {"title": "Higher order processes (k > 1)", "content": "While empirical evidence suggests that Thm. 1 would also hold for k > 1 (Figs. 1b, 3b), theoretically the proof becomes intractable due to the difficulty in tracking the correlations between the transition counts as k increases. Nonetheless, we believe that it is possible to extend the main proof ideas from the first-order case to here, though with a more involved construction, leading to the conjecture:\nConjecture 1. The canonical MambaZero model can implement a predictor that is arbitrarily close to the add-\u00df estimator, for the binary order-k Markov setting, with dimensions d = N = 2k+1, e = 1, and window w = k + 1.\nWe further strengthen this claim by the following matching lower bound, which shows that, with finite bit precision, any recurrent architecture such as Mamba cannot implement the Laplacian estimator with arbitrarily small error if the hidden dimension does not scale as \u03a9(2k).\nTheorem 2. Consider a recurrent model of the form\n$H_t = h(H_{t-1}, X_t)$,\n$Y_t = P_{\\theta} (\\cdot | x_1) = g(H_t)$,\nwith transformations (h, g), where Ht \u2208 Rd and the model has a bit precision of p. Suppose that the Markov kernel P is sampled from the Dirichlet prior with \u03b2 = 1, P ~ Dir(1.1). Suppose also that the recurrent architecture satisfies the following point-wise guarantee: for any sufficiently large t, almost surely over P and x1 ~ P,\n$|P_{\\theta}^{(k)}(x_1) - \\mathbb{P}^{(k)}(x_1)| < \\epsilon$.\n(4)\nThen, the recurrent architecture must satisfy\n$d \\cdot p \\geq 2^k (1 - 3 \\epsilon) \\log(1/\\epsilon)$.\nRemark. While we used time-invariant functions (h, g) in Thm. 2 for notational simplicity, the proof does not rely on this fact and holds for any general Ht = ht(Ht\u22121,xt) and yt = gt(Ht), which subsumes Mamba.\nWe provide the intuition behind the proof here. We defer the full proof and additional details to App. C.\nIntuition. The intuition behind this result is in the manner in which the recurrent architecture carries out computation: by proceeding sequentially and compressing the information from the sequence it has seen thus far at some time t into a small hidden vector, the model does not know what the next k tokens will be: the knowledge of this is vital to be able to compute the add-\u03b2 estimator at time t + k + 1 with a small memory footprint. Indeed, when the identity of the next k tokens changes, the output of the model at time t + k + 1 must look drastically different (as the add-\u03b2 estimator corresponds to approximately evaluating P(\u00b7|), which are unrelated distributions under different choices of it). There are ~ 22k possible values the set P = {P(\u00b7) : \u2208 {0,1}k}\ncan take. But when d and pare small, the output of the model just cannot take so many values: it\ncan realize at most 2dp possible sets. In other words, in order to succeed, the recurrent architecture\nis essentially forced to keep track of the number of occurrences of each i\u2208 {0,1}k in the sequence\nat each time t, which costs an exponential dependence on k in the hidden dimension/precision."}, {"title": "Beyond Markov", "content": "A key component of Mamba enabling selectivity is the state-transition factor at, that controls the flow of information from the past state Ht-1 to the current Ht: if at = 1, the past information is fully utilized in computing the current state, and hence the output, whereas at = 0 completely ignores the past. In the Markovian setting considered so far, the role of at has largely been dormant: at \u2248 1 for all t\u2265 1, as the optimal Laplacian predictor requires counts of all transitions, demanding the use of full past (Sec. 4.2). To better highlight this selectivity mechanism, we consider a non-Markovian process, where the role of at becomes fundamental. Specifically, we focus on the switching Markov process, where we add a switch token to the binary alphabet, i.e. we consider X = {0,1, S}. The key difference here compared to the random Markov generation in Sec. 2.1 is that until we hit switch token, the sequence follows the same binary Markov chain, but once the switch state is reached, the next symbols follow a newly sampled Markov chain, independent of the previous one. The switch tokens are sampled according to a parallel i.i.d. Bernoulli process with probability Pswitch (0.01 in our experiments). More formally, the process consists of the following steps:\n1. Initialize t = 1.\n2. Draw a binary Markov kernel P with each conditional distribution Pik sampled i.i.d. from Dir(\u03b2. 1).\n3. Let xt = S with probability Pswitch, or sample xt ~ Px-k+1 with probability 1 Pswitch (the first k samples after each switch token are sampled from Unif({0,1})).\n4. If xt = S, set t = t + 1 and go to step 2; if xt \u2260 S, set t = t + 1 and go to step 3.\nMamba learns the optimal estimator. With this data model, the optimal prediction strategy is to use the add-\u1e9e estimator in between two switch tokens, and reset the transition counts every time"}, {"title": "Switching Markov model", "content": "a switch occurs. Indeed, Fig. 4 illustrates that Mamba implements precisely this strategy, closely tracking the switching events via the transition factor at: it sets at to be zero whenever xt = S and to one otherwise. This enables the model to zero out the transition counts at every switch event, so that it can estimate the statistics of the new chain from scratch."}, {"title": "Natural language modeling", "content": "To test the generality of our finding that convolution plays key role on Markovian data (Fig. 3), we conduct experiments on the language modeling task using the WikiText-103 dataset with a sequence length of 256. We use a standard 2-layer Transformer consisting of attention and MLP modules with a model dimension of 256. To ensure a comparable parameter count, we stack the Mamba-2 cell across four layers, following the design in [DG24]. By adding or removing convolution in both these models, we obtain the results shown in Table 1. The results illustrate that while convolution significantly enhances the performance of Mamba-2, reducing perplexity from 30.68 to 27.55, the improvement for the Transformer is marginal. This highlights the fundamental role of convolution even on the complex language modeling tasks."}, {"title": "Conclusion", "content": "Structured state space sequence models (SSMs) and Selective SSMs such as Mamba have shown remarkable inference speed-ups over transformers while achieving comparable or superior performance on complex language modeling tasks. In this paper, we studied in-context learning (ICL) capabilities of Mamba on Markov chains and show that, unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator. To explain this, we theoretically characterized the representation capacity of Mamba, which revealed the fundamental role of convolution in enabling it. We provided empirical results that align with our theoretical insights. Extending our results to deeper Mamba models and understanding the role of depth are some interesting future directions."}, {"title": "Preliminaries on Laplacian smoothing", "content": "Laplacian smoothing is a mature and well understood topic. An account can be found, e.g., in [MF98; CL06", "BG25": ".", "xt)": "for every \u8bab \u2208 Xk. Since each conditional distribution is sampled from a Dirichlet distribution with parameter \u03b2, it is well known that the optimal predictor for such subsequences is the add-constant estimator, with constant equal to \u03b2. More specifically, if x\u2212k+1 = \u8bab, then the optimal estimation for xt is\n$\\mathbb{P}^{(k)}(x_{t+1} = j | x_t) = \\frac{n_j + \\beta}{n + 2\\beta}$,\n(5)\nwhere nj is the number of times token j appears in the subsequence $x_t|_{(x_{t-k+1} = i)}$, and n is the length of the subsequence.\nWe now provide a formal proof of this fact.\nTheorem 3. Consider the class of all k-th order Markov kernels $P = (\\mathbb{P}_{i_1 \\ldots i_k})_{i_1 \\ldots i_k \\in X^k}$, where each $\\mathbb{P}_{i_1 \\ldots i_k} = \\mathbb{P}(\\cdot | i)$ is a probability distribution on X = {0,1}. Let each $\\mathbb{P}_{i_1 \\ldots i_k}$ be sampled i.i.d. from Dir(\u03b2\u00b71), and let x ~ Unif(X) and xt+1/24 ~ Pik. Then, the predictor $f^{(i)}(x) = \\mathbb{P}(x_{t+1} = j | x_t)$, for j\u2208 {0,1}, that minimizes the loss\n$\\mathcal{L} = \\frac{1}{T} \\sum_{t \\in [T", "f^{(0)}(x_1)": "n(6)\nis the add-\u00df estimator in Eq. (5), i.e. the minimizer $f_\\theta^{(i)}(x) = \\mathbb{P}^{(k)}(x_{t+1} = j | x_t)$, for all t \u2265 k.\nProof. First note that\n$\\mathcal{L} = \\frac{1}{T} \\sum_{t} \\mathbb{E}_{P} \\mathbb{E}_{x_1} \\mathbb{E}_{x_{t+1}|x_1} [x_{t+1} \\cdot \\text{log }f^{(1)}(x_1) + (1 - x_{t+1}) \\cdot \\text{log }f^{(0)}(x_1)"}, {}, "f^{(0)}(x_1)"]}