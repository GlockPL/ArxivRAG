{"title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities", "authors": ["Enneng Yang", "Li Shen", "Guibing Guo", "Xingwei Wang", "Xiaochun Cao", "Jie Zhang", "Dacheng Tao"], "abstract": "Model merging, also known as model fusion, is an effective technique that merges the parameters of multiple separate models with different capabilities to build a universal model without needing access to the original training data or expensive computation. The concept most relevant to model merging is ensemble learning [33, 109, 142, 180], as both facilitate knowledge fusion and transfer. As shown in Figure 1, the main difference between them is that ensemble learning must save all individual models and fuse the predictions (or outputs) of multiple models during the inference phase, whereas model merging performs merging directly at the parameter level and only has one final model during inference. This gives model merging more attractive properties.\nAlthough model merging is a relatively young topic, it is evolving rapidly and has already found applications in several domains. For example, in foundation models, models fine-tuned by different downstream tasks are merged to enhance the capabilities of large language models, and image generative models with different styles are merged to create a new model with mixed-style capabilities. In particular, the number of pre-trained and fine-tuned checkpoints in the machine learning community has grown exponentially in recent years, including open-source repositories such as Huggingface [182], torchvision [111], and timm [181], making it easy for users to obtain well-trained expert models of varying abilities. These rich model repositories further promote the rapid development of model merging direction.\nAs model merging becomes increasingly popular in various areas of the machine learning community, it is crucial to have a comprehensive understanding of the advantages and limitations of existing model merging techniques and their applications across different domains. Although some efforts have been made by the community [48, 96, 157, 214], there are still large gaps to be filled. More specifically, MergeKit [48] and FusionBench [157] are technical reports in which only seven representative methods are discussed in MergeKit, and eight merging methods are discussed in FusionBench. Additionally, Zheng et al. [214] discuss the topic of \"learning from models\" and it only mentions model merging as a subsection (single page only) in the whole paper. The most related work to the \u201cmodel merging\" topic is [96], but in terms of application, it only discusses model merging in three scenarios: federated learning, fine-tuning, and distillation. It also ignores a lot of recently published articles due to the rapid evolution of the model merging direction. To address these gaps, this survey aims to elucidate the methods, theories, applications, and future trends in model merging direction, providing a comprehensive classification of relevant approaches. In particular, this paper enhances the comprehensive understanding of model merging by covering three main aspects:\nFirst, how are existing model merging methods classified? We first propose a new taxonomy in Figure 2 (upper part) that divides existing model merging methods into two phases (\u00a72): pre-merging and during-merging. (i) Pre-merging methods aim to create better conditions for merging. It is further divided into using linearized fine-tuning to achieve weight space and input space disentanglement, performing archi-", "sections": [{"title": "1 Introduction", "content": "Model merging, also known as model fusion, is an effective technique that merges the parameters of multiple separate models with different capabilities to build a universal model without needing access to the original training data or expensive computation. The concept most relevant to model merging is ensemble learning [33, 109, 142, 180], as both facilitate knowledge fusion and transfer. As shown in Figure 1, the main difference between them is that ensemble learning must save all individual models and fuse the predictions (or outputs) of multiple models during the inference phase, whereas model merging performs merging directly at the parameter level and only has one final model during inference. This gives model merging more attractive properties.\nAlthough model merging is a relatively young topic, it is evolving rapidly and has already found applications in several domains. For example, in foundation models, models fine-tuned by different down-stream tasks are merged to enhance the capabilities of large language models, and image generative models with different styles are merged to create a new model with mixed-style capabilities. In particular, the number of pre-trained and fine-tuned checkpoints in the machine learning community has grown exponentially in recent years, including open-source repositories such as Huggingface [182], torchvision [111], and timm [181], making it easy for users to obtain well-trained expert models of varying abilities. These rich model repositories further promote the rapid development of model merging direction.\nAs model merging becomes increasingly popular in various areas of the machine learning community, it is crucial to have a comprehensive understanding of the advantages and limitations of existing model merging techniques and their applications across different domains. Although some efforts have been made by the community [48, 96, 157, 214], there are still large gaps to be filled. More specifically, MergeKit [48] and FusionBench [157] are technical reports in which only seven representative methods are discussed in MergeKit, and eight merging methods are discussed in FusionBench. Additionally, Zheng et al. [214] discuss the topic of \"learning from models\" and it only mentions model merging as a subsection (single page only) in the whole paper. The most related work to the \u201cmodel merging\" topic is [96], but in terms of application, it only discusses model merging in three scenarios: federated learning, fine-tuning, and distillation. It also ignores a lot of recently published articles due to the rapid evolution of the model merging direction. To address these gaps, this survey aims to elucidate the methods, theories, applications, and future trends in model merging direction, providing a comprehensive classification of relevant approaches. In particular, this paper enhances the comprehensive understanding of model merging by covering three main aspects:\nFirst, how are existing model merging methods classified? We first propose a new taxonomy in Figure 2 (upper part) that divides existing model merging methods into two phases (\u00a72): pre-merging and during-merging. (i) Pre-merging methods aim to create better conditions for merging. It is further divided into using linearized fine-tuning to achieve weight space and input space disentanglement, performing archi-"}, {"title": "2 Advanced Model Merging Methods", "content": "In this section, we first introduce the notation and problem definition of model merging in \u00a72.1. We then elaborate on advanced model merging methods (Table 1 summarizes the primary purpose of each category of methods). Existing model merging techniques can be roughly divided into the following two categories: (i) Before Merging Methods in \u00a72.2: it provides better prior knowledge for model merging. (ii) During Merging Methods in \u00a72.3: it resolves task conflict/interference by various strategies, and then performs parameter merging operations. Finally, we conclude with theories or explanations for the effectiveness of model merging in \u00a72.4.\n\n2.1 Notation and Model Merging Problem Definition\nAssume there are $T$ models $(\\Phi_{\\Theta^{(1)}}, ..., \\Phi_{\\Theta^{(T)}})$ of the same architecture that need to be merged, and they train from scratch or fine-tune on the same pre-trained model $\\Phi_{\\Theta^{(0)}}$ respectively. The parameters (or weights) of the $t$-th model $\\Theta^{(t)}$ are represented as $\\Theta^{(t)} = {\\theta^{(t)}_{l} \\}_{l=1}^{L}$, where $l$ denotes the $l$-th layer of the model, and $L$ is the total number of layers.\nIn this survey, we focus on parameter-wise merging. In other words, the goal of model merging is to merge the parameters ${\\Theta^{(1)}, . . ., \\Theta^{(T)} }$, and finally obtain the new parameters $\\Theta^{(merge)} = merge(\\Theta^{(1)}, ..., \\Theta^{(T)})$. One straightforward solution for merging models is weighted averaging [146, 168], defined as $\\Theta^{(merge)} = \\sum_{t=1}^{T} \\lambda_t \\Theta^{(t)}$. However, the performance of this approach is often unacceptably poor or infeasible due to several possible factors: (i) The lack of suitable merging conditions, such as multiple models not being in the same basin or having inconsistent architectures. (ii) There are conflicts and interference among multiple models. We illustrate how advanced methods address these issues in \u00a72.2 and \u00a72.3, respectively.\n2.2 Pre-Merging Methods\nTo provide better preconditions for model merging, one class of work focuses on the fine-tuning step of independent models, such as fine-tuning the linearized model instead of the nonlinear model (in \u00a72.2.1). Additionally, when multiple model architectures that need to be merged are inconsistent, they must be pre-transformed to the same architecture (in \u00a72.2.2). Finally, another class of work attempts to align the weights/parameters before merging (in \u00a72.2.3)."}, {"title": "2.2.1 Linearization Fine-tuning", "content": "Ortiz-Jimenez et al. [123] reveal that one necessary condition for effective model merging is 'weight dis-entanglement'. This means that different directions of the weight space correspond to functional changes in disjoint regions of the input space. For example, if model 1 in weight space corresponds to a function change on $D_1$ in input space and model 2 in weight space corresponds to a function change on $D_2$ in input space, then the merged model will not interfere with each other in terms of function change. In other words, two models satisfying this weight disentanglement property can coexist in one model without affecting their respective performance, which is a very attractive property."}, {"title": "2.2.2 Architecture Transformation", "content": "In some cases, models that need to be merged may have different architectures and cannot be merged directly. To solve this problem, some studies [10, 120, 171, 172] propose to perform architecture transfor-mation before merging, that is, transform multiple models with different architectures into the same archi-tecture as shown in Figure 3 (a). For example, GAN Cocktail [10] attempts to merge multiple GAN models ${\\Theta^{(1)}, ..., \\Theta^{(T)} }$ with different architectures. It transforms all GAN models $\\Theta^{(t)} (t \\in {1,2,...,T} \\& t \\neq k)$ into a specified target model $\\Theta^{(k)}$, that is, $\\Theta^{(k)}$ is used as the initialization to learn the output of $\\Theta^{(t)}$, while adding implicit regularizations to ensure that $\\Theta^{(k)}$ does not forget knowledge of the task $k$. Consequently, the transformed GAN models have the same structure and shared knowledge, facilitating further model merging. Similarly, FuseChat [172] proposes to merge chat LLMs with diverse architectures and scales (e.g., NH2-Mixtral-8x7B [75], NH2-Solar-10.7B [84], OpenChat-3.5-7B [173] in their practical appli-cations). Specifically, FuseChat first uses knowledge distillation to transform all the architectures to match that of OpenChat-3.5-7B, and then performs the model merge operation. Unlike the above distillation-based approach, CLAFusion [121] adds layers/blocks (with weights set to the identity matrix) to the smaller model to align its architecture with that of the larger model. In summary, merging models with different architec-tures requires first transforming all models into a common architecture to merge later."}, {"title": "2.2.3 Weight Alignment", "content": "The linear mode connectivity (LMC) property of deep neural networks demonstrates that there is a con-nected path between multiple local minima of deep neural networks along which the loss remains nearly constant [37, 38, 39, 47, 162]. Numerous studies [38, 43, 117] have shown that two independent models, starting from the same pre-trained model and fine-tuned with different hyper-parameter configurations, typ-ically satisfy LMC. Further, Adilova et al. [3] and Zhou et al. [216] extended the study of LMC to the layer level. The LMC property implies that multiple local minima may be equivalent in the weight space, and different weight configurations of the same model may represent the same functionality. Inspired by this, many works proposed to permute the weights of one model (i.e., $\\Theta^{(1)} \\rightarrow \\Pi(\\Theta^{(1)})$) to align with the other model $\\Theta^{(2)}$ when merging/interpolating two separate models, as illustrated in Figure 3 (b). $\\Pi(\\cdot)$ denotes a permutation function, and researchers have dedicated efforts to studying effective and efficient permutation strategies for model alignment.\nOTFusion [148] and Imfeld et al. [66] adopt optimal transport to soft-align neurons across models. NeuronAlignment [162] introduces an inexpensive heuristic algorithm to approximate the optimal neuron alignment. CCAMerge [58] permutes by maximizing the correlation between linear combinations of neu-rons. Notably, Git re-basin [5] proposes three methods -activation matching, weight matching, and straight-through estimation- to align (or permute) the weights of models trained on different tasks. Based on the Git re-basin, Pe\u00f1a et al. [125] further incorporate a Sinkhorn-based projection to improve these alignment methods. In addition, MuDSC [189] proposes simultaneously performing model alignment in weight and ac-tivation spaces. Unlike heuristic alignment strategies, Deep-Align [119] proposes a learning-based approach to weight alignment, employing a novel learnable architecture that takes two sets of weights as input and outputs a permutation matrix for alignment.\nDespite the significant improvement of these alignment algorithms, Jordan et al. [80] argue that the suc-cess of these methods depends on the use of normalization layers (e.g., BatchNorm, LayerNorm, etc.) in the model; without these, the performance of the matching algorithms is greatly reduced. The authors call this the \"variance collapse\" problem and propose the REPAIR method to solve it. Additionally, Crisostomi et al. [27] noted that previous pairwise permutations do not guarantee cycle consistency, making the alignment fragile. They further proposed to globally optimize the permutations of all layers simultaneously at each step. Overall, aligned models experience much less interference or conflict during merging compared to directly merging unaligned models."}, {"title": "2.3 During Merging Methods", "content": "In this section, we provide a detailed discussion on how to merge a set of well-trained models. The existing methods can be roughly divided into five categories: basic merging methods (\u00a72.3.1), weighted-based merg-ing methods (\u00a72.3.2), subspace-based merging methods (\u00a72.3.3), routing-based merging methods (\u00a72.3.4), and post-calibration based methods (\u00a72.3.5)."}, {"title": "2.3.1 Basic Merging Methods", "content": "One of the most straightforward approaches to model merging is to directly weighted average the parameters of multiple models [146, 168], i.e., $\\Theta^{(merge)} = \\sum_{t=1}^{T} \\lambda_t \\Theta^{(t)}$. However, the performance of simple weight averaging is generally unsatisfactory. Recently, Task Arithmetic [65] introduced the concept of \u201ctask vector\" (in Figure 4(a)), which represents the model parameter $\\Theta^{(t)}$ fine-tuned on task $t$ subtract the pre-trained model parameter $\\Theta^{(0)}$, i.e., $T_t = \\Theta^{(t)} - \\Theta^{(0)}$. In other words, task vectors are thought to steer the behavior of a neural network meaningfully. For example, multitask learning (MTL) can be accomplished by adding task vectors, forgetting can be achieved by subtracting task vectors, and task analogies can be performed using analogous task vectors. Specifically, when we want the pretrained model to perform MTL, we can add multiple task vectors ${\\tau_1, . . ., \\tau_T }$ to the pretrained model, i.e., $\\Theta^{(merge)} = \\Theta^{(0)} + \\lambda \\cdot \\sum_{t=1}^{T} \\tau_t$ in Figure 4(b), where $\\lambda$ is a hyperparameter. Conversely, when we want the pretrained model to forget a function $t$, we can subtract the corresponding task vector from pretrained model as Figure 4(c), i.e., $\\Theta^{(merge)} = \\Theta^{(0)} - \\tau_t$. As"}, {"title": "2.3.2 Weighted-based Merging Methods", "content": "As we all know, different models (or task vectors) represent different functions, and intuitively, different functions have varying degrees of importance. Therefore, advanced weighted-based model merging methods design various clever rules to determine the merging coefficients, as shown in Figure 5(a). For instance, when merging two models $\\Theta^{(1)}$ and $\\Theta^{(2)}$ (or task vectors $\\tau_1$ and $\\tau_2$), the goal of the weighted merging method is to find the optimal coefficients $\\lambda_1$ and so that the merged model $\\Theta^{(merge)} = \\lambda \\Theta^{(1)} + \\Theta^{(2)}$ (or $\\Theta^{(merge)} = \\Theta^{(0)} + \\lambda_1 \\tau_1 + \\lambda_2 \\tau_2$) can retain the capabilities of the independent models as much as possible. However, when the number of models is large, it is impractical to use brute-force grid search to find the optimal merging coefficient because of the expensive search cost involved.\nTo determine the merging coefficient more effectively, Evolutionary-model-merge [6] and Checkpoint Merging [100] efficient searches for the merging coefficients using evolutionary algorithms and Bayesian optimization, respectively. AdaMerging [194] uses gradient descent optimization to learn the merging co-efficients by minimizing entropy as a surrogate loss in unlabeled test data. MetaGPT [215] casts the model merging problem as an MTL formalism, where the goal is to minimize the average loss of the merged model and the independent model. It employs local linearization of the model and the orthogonality of the task vectors to derive the optimal merging coefficient $\\lambda_t$ for each model $t$ as follows: $\\lambda_t = \\frac{||\\tau_t||^2}{\\sum_{t=1}^{T} ||\\tau_t||^2}$. SLERP [49] performs spherical interpolation of the parameters of the two models. The interpolated coeffi-cients of $\\tau_1$ and $\\tau_2$ are given by $\\lambda_1 = \\frac{sin((1-\\lambda)-\\rho)}{sin(\\rho)}$ and $\\lambda_2 = \\frac{sin(\\lambda \\rho)}{sin(\\rho)}$, respectively, where $\\rho = arccos \\frac{\\tau_1 \\cdot \\tau_2}{||\\tau_1|| \\cdot ||\\tau_2||}$ denotes the angle between the two task vectors, and $\\lambda$ represents the merging coefficient of the initial setting.\nThe above-sophisticated weighting methods operate at the model (or task) level. It is well known that each layer and even each neuron in a deep neural network model play a significantly different role, and some research has developed more fine-grained weighted merging strategies. For example, Layer-wise AdaMerg-ing [194] and aTLAS [206] adaptively learn different sets of merging coefficients for each layer or module of the model, respectively. RegMean [78] indicates that closed-form solutions (relying on the data statistics provided by the training set) exist for linear layers in model merging, while nonlinear layers can simply perform weight averaging. Other works utilize the Fisher information matrix [40] to assess the importance of parameters when merging. Fisher-Merging [113] performs model merging based on the importance of the parameters in each independent model, that is, $\\Theta^{(merge)} = \\frac{\\sum_{t=1}^{T} F^{(t)} \\Theta^{(t)}}{\\sum_{t=1}^{T} F^{(t)}}$, where $F^{(t)}$ is the diagonal of the Fisher information matrix with respect to task $t$. Fisher-nodes-merging [164] also combines a set of Transformer [169] models based on the Fisher information matrix. MaTS [155] developed a block"}, {"title": "2.3.3 Subspace-based Merging Methods", "content": "Another class of advanced methods transforms models into sparse subspaces for merging, thereby mitigating task interference. The over-parameterized nature of neural networks and the success of model pruning [22, 54] show that removing most of the parameters from the model barely affects its accuracy [190]. This insight opens up new opportunities for model merging, allowing us to remove insignificant neurons from a single model and merge multiple sparse models within the parameter subspace, as shown in Figure 5 (b).\nTIES-Merging [190] proposes to trim each individual model based on parameter magnitudes, retaining only the top 20% of parameters with the highest magnitudes. It further suggests eliminating parameter sign conflicts to reduce interference, and finally merging sparse models using Task Arithmetic [65]. Similarly, Drop And REscale (DARE) [200] also sparsifies by parameter magnitude, and highlights the importance of further performing rescaling on sparse models. In addition to removing the tail parameters with the smallest weight, the Model Breadcrumbs [30] highlight the importance of removing the parameters (outliers) with the largest weights to further reduce noise in model merging and enhance generalization to hyperparameters. TALL-masks [176] creates a mask matrix specific to each task based on a predefined threshold related to independent models. Unlike the standard practice of obtaining a single model through model merging, EMR-Merging [62] proposes maintaining a shared model among multiple tasks alongside a sparse task-specific model. In this approach, the value of the shared model at each index is the largest parameter value among all models. In contrast to the mask construction rules of the aforementioned heuristics, Concrete [156] frames mask construction and model merging as a learnable bi-level optimization problem. The outer-level optimizes the mask matrix, while the inner-level merges the model based on the mask matrix and optimizes it using the unlabeled test samples."}, {"title": "2.3.4 Routing-based Merging Methods", "content": "The basic, weighted-based, or subspace-based merging methods discussed in \u00a72.3.1, \u00a72.3.2 and \u00a72.3.3 are static merging methods. This means that the merged model remains the same for all samples or tasks. Given that there are differences between input samples/tasks, the model's ability may vary when processing different samples/tasks. As shown in Figure 5 (c), some works propose to dynamically merge models (or subsets of layers) based on the samples/tasks [93, 108, 116, 159] during the inference phase.\nFor a given input, SMEAR [116] first computes a weighted average of the parameters of each expert by using the distribution of router inputs to the expert modules. The advantage of this approach is that it has a similar computational cost to that of a single expert. Twin-Merging [108] also adaptively combines"}, {"title": "2.3.5 Post-calibration based Methods", "content": "Recently, Yang et al. [193] introduce a post-merging method to calibrate merged models. They observed that merged models (across multiple mainstream model merging methods) suffer from representation bias, meaning the representations extracted by the independent and merged models are very different, leading to performance degradation in the merged model. To alleviate this problem, they propose a module called 'representation surgery' to calibrate the representation bias. The core idea is to align the representation of the merged model after representation surgery with that of the independent model."}, {"title": "2.4 Theories and Analysis of Model Merging", "content": "In addition to designing various advanced methods in \u00a72.2 and \u00a72.3, the theoretical and effectiveness anal-ysis of model merging is also crucial. Currently, there is limited work on the theoretical analysis of model merging. Based on the source of the models to be merged, the existing theoretical analysis can be roughly divided into three categories: (i) model merging of different checkpoints in the same training trajectory, (ii) model merging of different models fine-tuned on the same dataset, and (iii) model merging of different models fine-tuned on different datasets or tasks.\nFirst, some analyses target model merging on the single-trajectory training, usually referring to stochastic weighted average (SWA) or exponential moving average (EMA). For example, Jain et al. [69] theoretically proved that the excess risk of the EMA is an upper bound of a bias term and a variance term in the con-text of least squares regression. The bias term depends on the initialization state of the parameters and decreases exponentially with the number of iterations once the model starts averaging. The variance term depends on the noise covariance inherent in the data, which decays at a faster rate when model averaging is used [8]. Similarly, Rame et al. [132] applies bias-variance decomposition to the domain generalization setting to explain why model averaging improves out-of-distribution performance. In addition, Hardt et al. [52] provide a stability bound for SWA under convex assumptions, while Wang et al. [177] further establish generalization bounds analysis in both convex and nonconvex cases.\nSecond, some studies explain the merging of multiple models with different hyperparameter fine-tuning for the same dataset in terms of connectivity and flatness of the loss landscape. Specifically, some works apply the theory of linear mode connectivity (LMC) [37, 47, 162] of neural networks to explain model merging. LMC reveals that neural network loss minima are not isolated points in the weight space. Recent studies [38, 43, 117, 217] have shown that two independent models, starting from the same pre-trained model and fine-tuned with different configurations, usually satisfy LMC. In other words, LMC is a general phenomenon that typically appears in fine-tuned models based on the \"pretraining-finetuning\" paradigm, which is the current standard in the machine learning community. Therefore, performing weight alignment according to LMC provides a robust validity guarantee for model merging [5, 80]. On the other hand, other studies explain model merging from the perspective of a flatter loss landscape [88], arguing that merging multiple weights fine-tuned under different optimization configurations with the same data usually converges to a flat local minimum [41], thus revealing why model merging has better generalization [15, 50, 67, 149, 207].\nFinally, an analysis by Ortiz-Jimenez et al. [123] is based on multiple models fine-tuned on different datasets, identifying weight disentanglement as a necessary precondition for effective model merging. More specifically, Ortiz-Jimenez et al. [123] provide theoretical and empirical analyses of the neural tangent kernel (NTK) and establish a compelling link between the task arithmetic [65] and the spectral properties of NTK."}, {"title": "3 Application of Model Merging in Foundation Models", "content": "The emergence of foundation models, including large language models (LLMs), multimodal large language models (MLLMs), and image generative models, is a significant indicator of technological progress in the field of artificial intelligence in recent years. However, despite their advancements, these large models still face several challenges, such as generating harmful content in LLMs, MLLMs struggling with fusing information from different modalities, and the difficulty of producing mixed-style images in image generation models. Recent studies suggest that model merging techniques offer a promising solution to these inherent challenges in foundational models. Table 2 first briefly summarizes the application of model merging in foundational models. Then, \u00a73.1, \u00a73.2 and \u00a73.3 provide a detailed discussion on how LLMs, MLLMs, and image generative models benefit from model merging, respectively."}, {"title": "3.1 Model Merging in Large Language Models (LLMs)", "content": "In recent years, large language models (LLMs), such as GPT-4 [2], Gemini [163], PaLM [23] and LLaMA [166], have made significant advancements and have been widely applied across various tasks. Despite their su-perhuman performance on most basic tasks, LLMs still face numerous challenges, including producing toxic content that violates laws or ethics, using unauthorized data during training, high training costs, and in-sufficient performance in specific domains. Model merging technology presents a promising opportunity to address these challenges."}, {"title": "3.1.1 Human Preference Alignment for LLMs", "content": "Humans often hold diverse opinions about aesthetics, politics, or fairness. When LLMs serve humans, differ-ent people have different expectations of the model, e.g., some expect LLMs to generate harmless responses, while others seek engaging and enjoyable interactions [134]. Consequently, the development of practical LLMs is generally divided into three stages, to generate responses that are more helpful, accurate, and safer [107]: Pre-training on a large amount of unsupervised data, supervised fine-tuning (SFT) on a small dataset with high-quality annotation, and interaction with humans to further optimize LLM alignment (e.g., direct preference optimization (DPO) [131] or reinforcement learning from human feedback (RLHF) [218]) with human preferences, rewards, or values.\nSome works propose to achieve better, safer, or faster alignment of human preferences by model merging. For example, ExPO [213] adds a task vector, constructed by a moderate model aligned using DPO or RLHF on a small amount of human preference data, to an unaligned SFT model. A more powerful aligned model can be directly obtained by setting a suitable merging coefficient. On the AlpacaEval 2.0 benchmark [97], fusing a model aligned on the 10%/20% preference data with an SFT model results in performance comparable to that of a model aligned on the full preference data. DogeRM [98] proposed merging the reward model with LLMs fine-tuned on different downstream domains to create domain-private reward models directly. Additionally, Lu et al. [107] propose an Online Merging Optimizer, that interpolates the gradient with the SFT model at each step of RLHF. This approach encourages RLHF to optimize toward reward maximization while preventing LLMs from forgetting general knowledge due to RLHF. Beyond preference alignment, sev-eral studies have examined the impact of model merging for secure alignment of LLMs [11, 51, 199]. For example, Hammoud et al. [51] find that merging two security-aligned models could compromise security. Thus, they proposed explicitly including secure alignment as an optimization objective when constructing synthetic data for model merging."}, {"title": "3.1.2 Detoxifcation of LLMs", "content": "LLMs have been widely noted for issues related to untruthfulness and toxicity in various applications [60], such as insults, threats, and profanity in responses to certain questions. To address the potential security risks in the application of LLMs, flexible techniques are needed to reduce the generation of toxic text, essentially detoxifying LLMs. A straightforward solution is to collect additional non-toxic data to fine-tune LLMs [83]; however, this approach requires significant computing resources and may interfere with the general capabil-ities of LLMs. Alternatively, directly reducing the probability of potentially toxic words during the decoding stage requires additional guidance information [87]. Recent studies have shown that reducing the toxic data generation of LLMs through model merging is a simple and effective scheme [60, 65, 210].\nTask Arithmetic [65] negates the task vectors of GPT-2 model [130] fine-tuned on toxic data (Civil Com-ments [13]) and shows that this operation effectively reduces the proportion of data classified as \"toxic\", with little change in the fluency of the language on the control task (WikiText-103). Additionally, some parameter-efficient models steer the toxic behavior of LLMs by manipulating a small number of parameters. PEM [210] negates LoRA [59] (and (IA)\u00b3 [102]) modules trained on poisoning data to maintain language proficiency while reducing toxicity of language model output. Ethos [46] and Ext-Sub [60] point out that while the task vector on toxic data is factually wrong, it also contains correct information about language modeling and logical narrative skills. Therefore, Ext-Sub decomposes the toxic task vector into two orthog-onal subspaces that represent general capability and destructive capability, respectively. Toxic knowledge is then eliminated by removing only the component representing the destructive ability from the LLM."}, {"title": "3.1.3 Knowledge Unlearning of LLMs", "content": "LLMs may inadvertently learn copyrighted material, raising significant legal and ethical concerns [1], and broader questions about responsible AI use [36]. In this context, the California Consumer Privacy Act [124] and the General Data Protection Regulations of the European Union [57] stipulate the right to data forget-ting. The foundational model's knowledge must be adapted to comply with these regulations. However, the cost of excluding copyrighted data for re-training from scratch is prohibitive. For instance, training a Llama-2-70B from scratch requires 1,720,320 GPU hours [167]. Traditional methods often use gradient ascent (GA) to achieve forgetting by fine-tuning the model using the GA algorithm on the specific data to be forgotten [165, 196]. Unfortunately, this approach typically catastrophically destroys other parts of the model's knowledge. That is, forgetting specific knowledge also erases other knowledge that should be retained. Recently, many studies based on model merging techniques have demonstrated the potential to forget LLM-specific knowledge without harming other knowledge [36, 60, 65].\nUnlike the GA-based approach, the model merging approach does not require additional data for other tasks to maintain old knowledge. To achieve forgetting, model merging typically incorporates a negatively fine-tuned model into the target model (i.e., the task-specific fine-tuned knowledge is subtracted from the target model). For example, Task Arithmetic [65] shows that negating task vectors degrade performance on specific tasks without substantial changes to the control tasks. Experiments demonstrate that model merging can forget the knowledge of the target task in a fine-tuned model without harming performance on control tasks. Similarly, Stable Sequential Unlearning (SSU) [36] extends this forgetting to the setting of sequential unlearning on LLMs, where different copyrighted content must be unlearned at different time"}, {"title": "3.1.4 Faster Training of LLMs", "content": "Training LLMs requires numerous iterations on massive data, making the training process extremely expen-sive. For example, training LLAMA2-70B with 2T tokens required 1,720,320 GPU hours [100", "81": "demonstrated that merging checkpoints during inter-mediate stages of model training speeds up the process. For example, training a ResNet50 model on the ImageNet dataset reduced the training time by 68 GPU hours, and training a RoBERTa-Base model on the WikiText-103 dataset saved 30 GPU hours. Sanyal et al. [143", "100": "comprehensively evaluates the effectiveness of model merging at different stages of the Baichuan2 [191", "21": "and ColD Fusion [35"}]}