{"title": "A Neural Transformer Framework for Simultaneous Tasks of Segmentation, Classification, and Caller Identification of Marmoset Vocalization", "authors": ["Bin Wu", "Sakriani Sakti", "Shinnosuke Takamichi", "Satoshi Nakamura"], "abstract": "Marmoset, a highly vocalized primate, has become a popular animal model for studying social-communicative behavior and its underlying mechanism. In the study of vocal communication, it is vital to know the caller identities, call contents, and vocal exchanges. Previous work of a CNN has achieved a joint model for call segmentation, classification, and caller identification for marmoset vocalizations. However, the CNN has limitations in modeling long-range acous-tic patterns; the Transformer architecture that has been shown to outperform CNNs, utilizes the self-attention mechanism that efficiently segregates information parallelly over long distances and captures the global structure of marmoset vocalization. We propose using the Transformer to jointly segment and classify the marmoset calls and identify the callers for each vocalization.", "sections": [{"title": "Introduction", "content": "The common marmoset (Callithrix Jacchus) is an animal model suitable for studying social vocal communication. First, the marmosets are non-human primates genetically and neurophysiologically close to human [5]. Second, in contrast to such primates as gorillas that primarily use gestures for communication, the marmosets are highly vocal and readily to respond to other marmosets, even non-related or non-pair-bonded ones, particularly when visually hindered such as in forests when vocal contact is crucial for survival [3]. Third, marmosets exhibit human-like conversational turn-taking, exchanging calls resembling coupled oscillators [12, 16]. Fourth, marmosets display prosocial behaviors: similar to human cooperative breeding, marmosets take care of offspring of nonpar-ents [1, 2], reflecting group social communications.\nResearchers have been using the marmoset as an animal model to study diseases and mechanisms related to vocal communication. Uesaka et al. [14] developed a marmoset model of autism disorder marked by the deficits in social communication and verbal interaction and by verbal preservation - by feeding pregnant mothers with valproic acid. By using the autism model, they aimed to study the developmental vocal characteristics of autism for early di-agnosis and investigate potential medicines to relieve the autism symptoms.\nThe ability of vocal communication is shaped by innate and empirical factors. Researchers manipulate autism-related genes of marmosets [6] to study innate gene-function relationships. Researchers manipulate environments including vi-sual or auditory sensory inputs [8] and parental interaction [11] to study the empirical modification of communicative behaviors.\nStudying turn-taking vocal communication between marmosets requires ex-tracting caller and callee information, call contents, and vocal exchanges from the recorded audios. Turesson et al. applied SVM and DNN for marmoset call classification on a small dataset of 321 marmoset calls [13]. Wisler et al. used SVM and decision tree on a larger dataset of 4 call types, each with 400 marmoset calls for classfication [17]. Uesaka et al. employed CNN to classify 3 call types (phee, twitter, and trill) to study the development and autism of the marmosets [14]. However, these studies were limited by either small datasets or focused on only a few call types.\nZhang et al. applied RNN and DNN for segmentation and classification of infant marmoset calls on a dataset that contains 10 call types, each with several thousand calls [18]. Their call types include phee, twitter, trill, trillphee, tsik, ek, pheecry, peep, and two infant-specific categories: ct-trill (twitter-connected-trill) and ct-phee (twitter-connected-phee). Sarkar et al. used the same dataset and implemented self-supervised learning on caller discrimination and classifi-cation [10]. These studies have two key limitations. First, both studies [18, 10] treat segmentation as a separate task and assumed known segmentation in-formation when classifying calls or callers. Second, their dataset recorded in-dividual marmosets in isolation, without communicative interaction with other marmosets, thus failing to capture the vocal characteristics of marmosets during social communication.\nOikarinen et al. used a dataset of recordings from paired of pairs marmosets housed together in single cages, enabling close-range vocal interaction [7, 9]. The dataset comprises 36 sessions totally 38 hours, with 8 call types: trill, phee, trillphee, twitter, chirp, tsik, ek, and chatter, along with a noise type that indi-cates the silences between the calls. Applying a CNN model on the dataset, they achieved simultaneous segmentation, classification, and caller identification [9].\nHowever, while previous works as [18] and [9] used an RNN or a CNN that maps acoustic segments to call labels, the Transformer structure has been proven to outperform RNN [15] and CNN [4] in both sequential language processing and"}, {"title": "Model", "content": "We use a two-stream Transformer model (Figure 1) on the dual-audio recordings with two simultaneously recorded channels that come from two interacting ani-mals. The model employs two stream transformer encoders that process sliding spectral segments from each channel to classify into a target label. The target labels indicate call types and caller identities (e.g., the 'tr2' label denotes a trill call from the second of two animals in interactions). The labels also indicate seg-mentation information: 'noise' denotes the non-call segment between calls. This approach allows our two-stream Transformer model to simultaneously perform three crucial tasks:\n\u2022 Segmentation: Identifying the start and end times of each vocalization.\n\u2022 Classification: Determining the type of call (e.g., trill, chirp).\n\u2022 Caller identification: Attributing each call to the correct animal.\nBy integrating these functions into a single model, we capture the complex dynamics of animal interactions through their vocalizations.\nFor our two-stream Transformer model, we utilize two Vision Transformer [15] modules; each (Figure 1) processes the high-resolution linear spectral image by dividing it into patches that form a sequence of patch tokens. These tokens undergo a linear transformation and are augmented with positional encoding and a learnable class token. The resulting sequence passes through a typical Transformer architecture that comprises alternated self-attention and feedfor-ward modules. The two-stream Transformer's vision transformer modules out-put two encoded class tokens, which are then linear projected and concatenated, passing through a shared linear layer for final class label prediction."}, {"title": "Evaluation", "content": "We use F-score and accuracy to evaluate classification, segmentation, and caller identification with the same evaluation methods used in [9].\nFor caller evaluation, we convert the predicted labels into two separate seg-ment files for a marmoset pair (e.g., 'tr2' labels are converted to 'tr' and added to the segment file of the second marmoset).We evaluate the two predicted seg-ment files against corresponding annotated ones for the marmoset pair housed together.\nFor segmentation evaluation, we process the annotated and predicted seg-ment files. First, we add 'noise' labels to fill in the intervals without any calls in the annotated segment files. Second, we reconstruct the interval for each call by merging predicted successive identical labels in the predicted segment files (e.g., the predicted 'noise, tr, tr, tr, noise' sequence indicates a three-time-unit trill call surrounded by noisy silences where a time-unit (50 millisecond) is the window shift of sliding spectral segment inputs for prediction)."}, {"title": "Result and discussion", "content": "We applied our system directly to each raw long-hour audio recording (approx-imately 2 to 3 hours) without extra processing. We compare our PyTorch-implemented two-stream CNN and transformer system with the best sytem of [9] (open-sourced) on the same dataset [7] of annotated dual audio recordings with the data division shown in the Figure 4.\nOur CNN system outperforms the best system from [9] in both F-score and accuracy. Furthermore, the Transformer system surpasses the CNN system across all F-score and accuracy-related metrics.\nThe Transformer's superior performance can be attributed to its ability to better capture overall patterns within each segment. However, the Vision Trans-former is constrained by the patch-based resolution due to its quadratic com-plexity in input token length, resulting in lower model resolution compared to the CNN model. In future work, we aim to improve our Transformer system for better resolution modeling.\nOur systems can be applied to raw, long-hour audio recordings to simulta-"}, {"title": "Conclusion", "content": "We implemented a two-stream Transformer system for dual-channel audio record-ings, enabling simultaneous classification, segmentation, and caller identifica-tion. This Transformer system outperforms previous CNN approaches. Our system can efficiently process long-hour or full-day marmoset recordings, pro-viding rich information about vocal communication between marmosets in spon-taneous interactions. This capability enhances the use of marmosets as an ani-mal model for studying language evolution, development, and dysfunction in a communication context."}]}