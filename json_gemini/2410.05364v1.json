{"title": "Diffusion Model Predictive Control", "authors": ["Guangyao Zhou", "Sivaramakrishnan Swaminathan", "Rajkumar Vasudeva Raju", "J. Swaroop Guntupalli", "Wolfgang Lehrach", "Joseph Ortiz", "Antoine Dedieu", "Miguel L\u00e1zaro-Gredilla", "Kevin Murphy"], "abstract": "We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach that learns a multi-step action proposal and a multi-step dynamics model, both using diffusion models, and combines them for use in online MPC. On the popular D4RL benchmark, we show performance that is significantly better than existing model-based offline planning methods using MPC and competitive with state-of-the-art (SOTA) model-based and model-free reinforcement learning methods. We additionally illustrate D-MPC's ability to optimize novel reward functions at run time and adapt to novel dynamics, and highlight its advantages compared to existing diffusion-based planning baselines.", "sections": [{"title": "1. Introduction", "content": "Model predictive control (MPC), also called receding horizon control, uses a dynamics model and an action selection mechanism (planner) to construct \"agents\" that can solve a wide variety of tasks by means of maximizing a known objective function (see e.g., [Sch+21] for a review of MPC). More precisely, we want to design an agent that maximizes an objective function $J(a_{t:t+F-1}, s_{t+1:t+F})$ over a planning horizon F from the current timestep t\n$a_{t:t+F-1} = \\underset{a_{t:t+F-1}}{\\text{arg max}} E_{p_d(s_{t+1:t+F}|s_t,a_{t:t+F-1}, h_t)} | J(a_{t:t+F-1}, s_{t+1:t+F}),$\nwhere $h_t = {s_{1:t-1}, a_{1:t-1}}$ is the history. MPC thus factorizes the problem that the agent needs to solve into two pieces: a modeling problem (representing the dynamics model $p_d(s_{t+1:t+F}|s_{1:t}, a_{1:t+F-1})$, which in this paper we learn from offline trajectory data) and a planning problem (finding the best sequence of actions for a given reward function). Once we have chosen the action sequence, we can execute the first action $a_t$ (or chunk of actions) and then replan, after observing the resulting next state, thus creating a closed loop policy.\nThe advantage of this MPC approach compared to standard policy learning methods is that we can easily adapt to novel reward functions at test time, simply by searching for state-action trajectories with high reward. This makes the approach more flexible than policy learning methods, which are designed to optimize a fixed reward function.\u00b9 In addition, learning a dynamics model usually requires less data than learning a policy, since the supervisory signal (next state prediction) is high dimensional, reducing the sample complexity. In addition, dynamics models can often be adapted more easily than policies to novel environments, as we show in our experiments.\nHowever, to make MPC effective in practice, we have to tackle two main problems. First, the dynamics model needs to be accurate to avoid the problem of compounding errors, where errors in next state prediction accumulate over time as the trajectory is rolled out [VHB15; Asa+19; Xia+19; LPC22]. Second, the planning algorithm needs to be powerful enough to select a good sequence of actions, avoiding the need to exhaustively search through a large space of possible actions."}, {"title": "2. Related work", "content": "Related work can be structured hierarchically as in Table 1. Model-based methods postulate a particular dynamics model, whereas model-free methods, whether more traditional \u2014behavioral cloning (BC), conservative Q learning (CQL) [Kum+20], implicit Q-learning (IQL) [KNL21], etc\u2014 or diffusion based  diffusion policy (DP) [Chi+23], diffusion BC (DBC) [Pea+23]\u2014 simply learn a policy. This can be done either by regressing directly from expert data or with some variant of Q-leaning. Model-based approaches can be further divided according to how they use the model: Dyna-style [Sut91] approaches use it to learn a policy, either online or offline, which they can use at runtime, whereas MPC-style approaches use the full model at runtime for planning, possibly with the guidance of a proposal distribution.2\nIt is possible to model the dynamics of the model and the proposal jointly using $p_j(s, a)$ or as"}, {"title": "3. Method", "content": "We will now describe our new D-MPC method. Our approach can be seen as a multi-step diffusion extension of the model-based offline planning (MBOP) paper [ADA21], with a few other modifications and simplifications."}, {"title": "3.1. Model predictive control", "content": "D-MPC first learns the dynamics model $p_{s|a}$, action proposal $\\pi$ and heuristic value function $J$ (see below), in an offline phase, as we discuss in Section 3.2, and then proceeds to alternate between taking an action in the environment with planning the next sequence of actions using a planner, as we discuss in Section 3.3. The overall MPC pseudocode is provided in Algorithm 1."}, {"title": "3.2. Model learning", "content": "We assume access to an offline dataset of trajectories, consisting of (state, action, reward) triples: $D = {\\{\\tau_1, ..., \\tau_M\\}}$, $\\tau_i = \\{s_{1:T_i}, a_{1:T_i}, r_{1:T_i}\\}$. We then use this to fit a diffusion-based dynamics model $p_d(s_{t+1:t+F}|s_t, h_t, a_{t:t+F-1})$ and another diffusion-based action proposal $p(a_{t:t+F-1}|s_t, h_t)$. To fit these models, we minimize the denoising score matching loss in the usual way. We include a detailed review of diffusion model training in Appendix A, and refer the readers to e.g. [Kar+22] for additional discussions.\nWe also define a function $J$ that approximates the reward-to-go given any proposed sequence of"}, {"title": "3.3. Planning", "content": "D-MPC is compatible with any planning algorithm. When the action space is discrete, we can solve this optimization problem using Monte Carlo Tree Search, as used in the MuZero algorithm [Sch+20]. Here we will only consider continuous action spaces.\nWe propose a simple algorithm which we call \"Sample, score and rank\u201d, depicted as Algorithm 2. In order to plan, given the current state $s_t$ and history $h_t$, we use our diffusion action proposal $p$ to sample $N$ action sequences, we predict the corresponding state sequences using $p_{s\\|a}$, we score"}, {"title": "3.4. Adaptation", "content": "As with all MPC approaches, our proposed D-MPC is more computationally expensive than methods that use a reactive policy without explicit planning. However, one of the main advantages of using planning-based methods in the offline setting is that they can easily be adapted to novel reward functions, which can be different from those optimized by the behavior policy that generated the offline data. In D-MPC, we can easily incorporate novel rewards by replacing $V_n$ in Alg. 2 by $V_n = \\kappa J(s_{1:F}, a_{n,1:F}) + \\tilde{\\kappa} \\tilde{J} (s_{1:F}, a_{n,1:F})$, where the novel objective $\\tilde{J}(s_{1:F}, a_{n,1:F}) = \\sum_{t=1}^{F} f_{novel} (s_t, a_{n,t})$, $f_{novel}$ is a novel reward function, and $\\kappa, \\tilde{\\kappa}$ are weights of the original and novel objectives, respectively. We demonstrate this approach in Section 4.2. Of course, if the new task is very different from anything the agent has seen before, then the action proposal may be low quality, and more search may be needed.\nIf the dynamics of the world changes, we can use supervised fine tuning of $p_{s\\|a}$ on a small amount of exploratory \"play\" from the new distribution, and then use MPC as before. We demonstrate this in Section 4.3."}, {"title": "4. Experiments", "content": "In this section, we conduct various experiments to evaluate the effectiveness of D-MPC. Specifically we seek to answer the following questions with our experiments:\n1. Does our proposed D-MPC improve performance over existing MPC approaches (which learn the model offline), and can it perform competitively with standard model-based and model-free offline RL methods?\n2. Can D-MPC optimize novel rewards and quickly adapt to new environment dynamics at run time?\n3. How do the different components of D-MPC contribute to its improved performance?\n4. Can we distill D-MPC into a fast reactive policy for high-frequency control?"}, {"title": "4.1. For fixed rewards, D-MPC is comparable to other offline RL methods", "content": "We evaluate the performance of our proposed D-MPC on various D4RL [Fu+20] tasks. Planning-based approaches are especially beneficial in cases where we do not have access to expert data. As a result, we focus our comparisons on cases where we learn with sub-optimal data. We experiment with locomotion tasks for Halfcheetah, Hopper and Walker2D for levels medium and medium-replay, Adroit tasks for pen, door and hammer with cloned data, and Franka Kitchen tasks with mixed and partial data.\nOur results are summarized in Table 2. We see that our method significantly beats MBOP, and a behavior cloning (BC) baseline. It also marginally beats Diffuser. We additionally compare to other popular model-free offline RL methods, like conservative Q-learning (CQL) [Kum+20] and implicit Q-learning (IQL) [KNL21], as well as model-based RL methods like MOREL [Kid+20], and"}, {"title": "4.2. Generalization to novel rewards", "content": "In Fig. 1, we demonstrate how novel rewards can be used to generate interesting agent behaviors. We first trained the dynamics, action proposal, and value models for D-MPC on the Walker2d medium- replay dataset. We then replaced the trained value model with a novel objective $V_n$ for scoring and ranking trajectories in our planning loop, using $f_{novel}(s_t, A_t) = 5 \\exp(-(h_t \u2013 h_{target})^2/2\\sigma^2)$, where $h_t$ is the dimension of the state $s_t$ that corresponds to the height of the agent, $h_{target}$ is the target height, $\\sigma^2 = 5 \u00d7 10^{-4}$, $\\kappa = 0$ and $\\tilde{\\kappa} = 1$ (so we only use the new $\\tilde{J}$ and ignore the original $J$). By using this simple method, we were able to make the agent lunge forward and keep its head down ($h_{target} = 0.9$), balance itself ($h_{target} = 1.2$), and repeatedly jump ($h_{target} = 1.4$)."}, {"title": "4.3. Adaptation to novel dynamics", "content": "In this section, we demonstrate our model's ability to adapt to novel dynamics at test time with limited experience. The need for such adaptions to novel dynamics is common in real world robotics applications where wear and tear or even imperfect calibrations can cause hardware defects and changed dynamics at test time. Because of our factorized formulation, where we separate dynamics"}, {"title": "4.4. Ablation studies", "content": "In this section, we conduct a series of detailed ablation studies to illustrate how different components in D-MPC contribute to its good performance. In particular, we investigate the effect of using diffusion for action proposals, and the impact of using single-step vs. multi-step models both for the action proposals and for the dynamics models. We evaluate all variants on D4RL locomotion tasks. See Table 3b for a high-level summary of the results, and Table 4 in the appendix for detailed performances of different D-MPC variants on individual D4RL domains and levels."}, {"title": "4.4.1. Diffusion action proposals improve performance and simplify the planning algorithm", "content": "Existing model-based offline planning methods, such as MBOP, typically use a single-step deterministic MLP policy for action proposals, an ensemble of single-step deterministic MLP models to emulate a stochastic dynamics, and rely on trajectory optimization methods for planning. This MBOP method achieves an average score of 33.13 on the locomotion tasks.\nWe can significantly improve on this baseline MBOP score, and simplify the algorithm, by (1) replacing their single-step MLP action proposal with a single-step diffusion proposal, and (2) replacing their TrajOpt planner with our simpler SSR planner. This improves performance to 52.93. Replacing their MLP dynamics with a single-step diffusion dynamics model further provides a minor improvement, to 53.32."}, {"title": "4.4.2. Multi-step diffusion action proposals contribute to improved performance", "content": "D-MPC uses multi-step diffusion action proposals. In this section, we illustrate how this further improves performance when compared with single-step diffusion action proposals.\nWe start with the same single-step MLP dynamics model as in Section 4.4.1. We then replace the single-step diffusion action proposal with a multi-step diffusion action proposal that jointly samples a chunk of actions. This improves average performance from 52.93 to 57.14. We then repeated this experiment on top of the single-step diffusion dynamics, and improved performance from 53.32 to 57.81."}, {"title": "4.4.3. Multi-step diffusion dynamics models contribute to improved performance", "content": "D-MPC uses a multi-step diffusion dynamics model. In this section we illustrate how this reduces compounding error in long-horizon dynamics prediction and contributes to improved performance.\nWe first measure the accuracy of long-horizon dynamics predictions of single-step diffusion, multi- step diffusion and auto-regressive transformer (ART) dynamics models (described in Appendix E.3),"}, {"title": "4.5. D-MPC can be distilled into a fast reactive policy for high-frequency control", "content": "Due to the use of diffusion models, D-MPC has slower runtime. In Appendix J, we include a detailed runtime comparison between D-MPC and other methods. However, if high-frequency control is important, we can distill the D-MPC planner into a fast task-specific MLP policy, similar to what is done in MoREL [Kid+20] or MOPO [Yu+20]. Concretely, we train an MLP policy on offline datasets, using the planned actions from pretrained D-MPC as supervision. We do this for the 6 D4RL locomotion domain and level combinations we use in our ablation studies, and compare performance with both the vanilla MLP policy and D-MPC. We train all models for 1M steps, and evaluate the last checkpoint for the distilled MLP policy.\nWe observe that the distilled MLP policy achieves an average normalized score of 65.08 across the 6 D4RL locomotion domain and level combinations, which is only slightly worse than D-MPC's average normalized score of 65.98, and greatly outperforms the vanilla MLP policy's average normalized score of 41.92. In addition, after distillation we just have an MLP policy, and it runs at the same speed as the vanilla MLP policy."}, {"title": "5. Conclusions", "content": "We proposed Diffusion Model Predictive Control (D-MPC), which leverages diffusion models to improve MPC by learning multi-step action proposals and multi-step dynamics from offline datasets. D-MPC reduces compounding errors with its multi-step formulation, achieves competitive performance on the D4RL benchmark, and can optimize novel rewards at run time and adapt to new dynamics. Detailed ablation studies illustrate the benefits of different D-MPC components.\nOne disadvantage of our method (shared by all MPC methods) is the need to replan at each step, which is much slower than using a reactive policy. This is particularly problematic when using diffusion models, which are especially slow to sample from. In the future, we would like to investigate the use of recently developed speedup methods from the diffusion literature, such as distillation (see e.g., [CKS23]).\nAnother limitation of the current D-MPC is we only explored setups where we directly have access to the low-dimensional states, such as proprioceptive sensors on a robot. In the future, we plan to extend this work to handle pixel observations, using representation learning methods that extract abstract latent representations, which can form the input to our dynamics models, similar to existing latent-space world modeling approaches such as the Dreamer line of work, but in an MPC context, rather than a Dyna context.\nLike all offline RL methods, D-MPC's performance is influenced by the distribution of behaviors in the training dataset. When offline datasets lack behaviors relevant to the target task, the generalization capabilities of any method are inherently constrained without additional data collection. While this does present a limitation for D-MPC, it is not unique to our approach but rather a fundamental challenge in offline RL. Within the scope of available data, D-MPC excels at optimizing and adapting to novel rewards and dynamics, which represents the realistic scenario for offline RL applications. Our approach's ability to effectively leverage the existing behavioral distribution is a significant strength. Future work could explore techniques to encourage broader exploration within the constraints of offline data, potentially expanding the applicability of D-MPC and similar methods to an even wider range of scenarios."}, {"title": "A. Algorithms for model learning", "content": "In Algorithm 3 and Algorithm 4, we present the multi-step and single-step training used in our experiments.\nDiffusion models are generative models that define a forward diffusion process and a reverse denoising process. The forward process gradually adds noise to the data, transforming it into a simple Gaussian distribution. The reverse process, which is learned, denoises the data step by step to recover the original data distribution.\nIn D-MPC, we model the action proposals and dynamics models as conditional diffusion models. Formally, let $x_0, y$ be the original data, where $x_0$ represents the data we want to model and $y$ represents the conditioning variable. Let $x_k$ be the data at step $k$ in the diffusion process. The forward process is defined as $q(x_k|x_{k-1}) = N(x_k; \\sqrt{\\alpha_k}x_{t-1}, (1 - \\alpha_k)I)$ where $\\alpha_k$ determines the variance schedule. The reverse process aims to approximate $p_\\theta(x_{k-1}|x_k) = N(x_{k-1}; \\mu_\\theta(x_k, k, y), \\Sigma_k)$ where $N(\\mu, \\Sigma)$ denotes a Gaussian distribution with mean $\\mu$ and covariance matrix $\\Sigma$. For suitably chosen $\\alpha_k$ and large enough $K$, $X_K \\sim N(0, I)$. Starting with Gaussian noise, we iteratively denoise to generate the samples.\nWe train diffusion models using the standard denoising score matching loss. Concretely, we start by randomly sampling unmodified original $x_0, y$ from the dataset. For each sample, we randomly select a step $k$ in the diffusion process, and sample the random noise $\\epsilon_k$ with the appropriate variance for the diffusion time step $k$. We train a noise prediction network $\\epsilon_\\theta$ by minimize the mean-squared-error loss $MSE(\\epsilon_k, \\epsilon_\\theta (x_0 + \\epsilon_k, k, y))$. $p_\\theta(X_{k-1}|x_k, y)$ can be calculated as a function of $\\epsilon_\\theta(x_k, k, y)$, which allows us to draw samples from the trained conditional diffusion model.\nIn the D-MPC case, for learning the action proposal $p$, the future actions $a_{t:t+F-1}$ are the clean data $x_0$, and the current state $s_t$ and history $h_t$ are the conditioning variable $y$. For learning the multi-step dynamics model $p_d$, the future states $s_{t+1:t+f}$ are the clean data $x_0$, and the current state $s_t$, the history $h_t$ and the future actions $a_{t:t+F-1}$ are the conditioning variable $y$. In both cases, the noise prediction network $\\epsilon_\\theta$ is a transformer. Details of the transformer are given in Appendix E.1."}, {"title": "B. The MBOP-TrajOpt Algorithm", "content": "In Algorithm 5, we include the complete MBOP-TrajOpt algorithm from [ADA21] adapted to our notations as reference."}, {"title": "C. Detailed ablation study results", "content": "In Table 4, we present detailed performances of the D-MPC variants studied in Section 4.4. See Table 3b for a a high-level summary."}, {"title": "D. Normalizing state coordinates", "content": "Following [Aja+23], we normalize the states that are input to our models by using the the empirical cumulative distribution function (CDF) to remap each coordinate to lie uniformly in the range [-1,1].\nGiven an offline dataset of trajectories, consisting of (state, action, reward) triples\n$D = {\\{\\tau_1, ..., \\tau_M\\}}$, $\\tau_i = \\{s_{1:T_i}, a_{1:T_i}, r_{1:T_i}\\}$"}, {"title": "E. Model architectures and training details", "content": "In this paper, we train 4 kinds of diffusion models: single-step diffusion action proposals, single-step diffusion dynamics models, multi-step diffusion action proposals, and multi-step diffusion dynamics models.\nWe implement all 4 models as conditional diffusion models. For single-step diffusion action propos- als, we use diffusion to model $p(a_t|s_t)$; for single-step diffusion dynamics models, we use diffusion to model $p(s_{t+1}|s_t, a_t)$; for multi-step diffusion action proposals, we use diffusion to model $p(a_{t:t+F-1}|s_t)$; and for multi-step diffusion dynamics models we use diffusion to model $p(s_{t+1:t+F}|s_t, a_{t:t+F-1})$.\nOur diffusion implementation uses DDIM [SME20] with cosine schedule [ND21]. We use trans- formers as our denoising network: for each conditional diffusion model, we embed the diffusion time using sinusoidal embeddings, project the time embeddings and each state and action (both for states/actions that are used as conditioning and for states/actions that are being modelled) to a shared token space with tokens of dimension 256, add Fourier positional embeddings with 16 Fourier bases to all tokens, and pass all the tokens through a number of transformer layers. We take the output tokens that correspond to the states/actions we are predicting, and project them back to the original state/action spaces."}, {"title": "F. Hyper-parameters for planning with SSR", "content": "For all of our experiments, we use a forecast horizon $F = 32$, number of samples $N = 64$, and a history length $H = 1$. A forecast horizon $F = 32$ already works well since our trained objective function $J$ predicts discounted future rewards."}, {"title": "G. Long-horizon dynamics prediction", "content": "Following [Sch+23], we measure prediction errors by the median Root Mean Square Deviation (RMSD) on the non-velocity coordinates, as depicted in Figure 2. While this metric allows us to directly analyze the effectiveness of the dynamics model, it is a somewhat crude metric of the correctness or usefulness of the prediction. For example, the following predictions would all produce errors\u2264 1.0 compared to the correct ones: treating each walker as a bundle of limbs with the same center-of-mass, predicting states that would trigger termination criteria (unreasonable joint angles), or predicting an upside down position for the HalfCheetah (a state from which it cannot recover). This metric is a more effective probe of dynamics model quality in the regime where the errors are smaller. We see in Figure 2 that the multi-step diffusion dynamics model in particular has low prediction errors even for long rollouts, indicating that it performs well in these situations."}, {"title": "H. Generalization to novel rewards", "content": "For the examples in Fig. 1, we first trained the dynamics, action proposal and value components of D-MPC on the Walker2d medium-replay dataset. The leftmost column shows the agent's height and rewards attained during an example episode generated by D-MPC.\nTo incorporate novel rewards, we replaced the the trained value model in the planning loop with a novel objective based solely on the height of the agent. For this objective, we used $f_{novel}(s_t, A_t) = 5 \\exp(-(h_t - h_{target})^2/2\\sigma^2)$, where $h_t$ is the dimension of the state $s_t$ that corresponds to the height of the agent, $h_{target}$ is the desired target height, $\\sigma^2 = 5 \u00d7 10^{-4}$, $\\kappa = 0$ and $\\tilde{\\kappa} = 1$. The scale factor of 5 in the reward function $f_{novel}$ was chosen to roughly match the maximum reward attainable in the environment."}, {"title": "I. Adaptation to novel dynamics", "content": "We used D-MPC and our implementation of the diffuser models trained on Walker2D medium dataset as our pre-trained models. To simulate defective hardware, we modified the action being executed in the environment. Specifically, we clip the action component corresponding to the torque applied on the right foot rotor to [\u22120.5, 0.5] vs the original [-1,1].\nTo collect the play data on this defective hardware, we run our D-MPC model trained on medium- replay dataset, and collect data for 100 episodes. It has a total of 30170 transitions (steps) and an average normalized episode reward of 23.14 (\u00b1 2.31). Note that the actions in this dataset correspond to the actions output by the model and not the clipped actions.\nFor fine-tuning, we load the pre-trained diffuser and D-MPC models and train them on this dataset with same training parameters as the original training. For D-MPC, we only train the dynamics model and freeze other components. For the diffuser, we fine-tune the full model, since it is a joint model. During online evaluation, we sampled 256 trajectories in both the models and picked the one with the best value to execute the action at each step. We report the maximum scores for both approaches."}, {"title": "J. Timing measurements", "content": "To illustrate the differences in run times between different methods, we list in Table 7 the measured wall clock planning time (along with standard errors) per execution step, on a single A100 GPU, for each algorithm, in three D4RL locomotion environments. The measurements illustrate that a simple MLP policy is the fastest, followed by an MBOP-like setup, followed by D-MPC, followed by MPC with an autoregressive transformer dynamics model. If tasks require faster control loops, D-MPC could be sped up in a few different ways such as amortizing the planning over a larger chunk of actions executed in the environment (since the planner naturally generates long-horizon plans), using accelerated diffusion sampling strategies [Lu+22; Shi+24], and distilling the diffusion models [Son+23; SD23; SH22; LGL22; Xie+24]. We leave this exploration for future work."}]}