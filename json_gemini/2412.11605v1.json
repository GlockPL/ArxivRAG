{"title": "SPAR: SELF-PLAY WITH TREE-SEARCH REFINEMENT TO IMPROVE INSTRUCTION-FOLLOWING IN LARGE LANGUAGE MODELS", "authors": ["Jiale Cheng", "Xiao Liu", "Cunxiang Wang", "Xiaotao Gu", "Yida Lu", "Dan Zhang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "abstract": "Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often op-timized by preference learning. However, existing methods often directly sample multiple independent responses from the model when creating preference pairs. Such practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), inter-fering with the goal of teaching models to recognize the key differences that lead to improved instruction following. In light of this, we introduce SPAR, a self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions. By playing against itself, an LLM employs a tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations. Our experiments show that a LLaMA3-8B model, trained over three iterations guided by SPAR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities. Furthermore, SPAR demonstrates promising scalability and transferability, greatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree search would impact model performance. Our code and data are publicly available at https://github.com/thu-coai/SPaR.", "sections": [{"title": "INTRODUCTION", "content": "To date, Large Language Models (LLMs) have achieved great success in a wide range of tasks. As LLMs are applied to various scenarios, their instruction-following capability becomes crucial, especially to follow instructions with multiple con-straints. The failure to accurately follow instructions can even lead to safety issues.\nSubtle nuances can determine the success of instruction-following tasks, making preference learning a well-suited solution. However, existing methods usually sample multiple independent responses from the target model, inadvertently introducing irrelevant variations to whether the instruction was successfully followed. As illustrated in Figure 1, given the instruction: 'Write a story and end it with The devil is in the details', sampling multiple independent responses from an LLM can result in responses as different as the story Little Red Riding Hood vs. Hansel and Gretel. This variation in the narrative content can interfere with the model's ability to learn how to realize the critical requirement\u2014the specified ending sentence-and ultimately mislead the comparison within the preference pair. Therefore, effective learning from preference pairs necessitates excluding these extraneous factors and focusing on the key differences that drive the success of instruction-following.\nIn this paper, we propose SPAR, a self-play method integrated with tree-search refinement to en-hance instruction-following capabilities of LLMs. The key lies in iteratively teaching LLMs to learn instruction-following from nuances by playing against itself with structured tree search. In each turn of self-play, an LLM takes two different roles: the actor and the refiner, which are both initialized from the same model. The actor executes complex instructions while the refiner critiques and refines the actor's responses. During the iteration, we first collect the actor's responses which fail to follow the instructions accurately, as judged by the refiner. Starting from those failed responses, we apply a tree-search algorithm for refinement, which ensures consistent improvements against previous turns and naturally creates valid comparison counterparts for model training.\nWe conduct experiments on several LLMs, LLaMA3 series, GLM-4-9B, and Mistral-7B-Instruct, over multiple iterations. Through ex-tensive experiments, we demonstrate significant improvements in the models' instruction-following capability, outperforming other self-improvement methods such as self-rewarding and meta-rewarding. Notably, after three iterations, SPAR improves LLaMA3-8B-Instruct over GPT-4-Turbo on the IFEval benchmark. Moreover, scaling test-time compute by integrating tree-search refinement during inference can further improve the quality of instruction following. Additionally, we find that with several iterations, the refiner's judgment and refinement capabilities can match or even exceed those of the distilled LLM, indicating great potential for continuous self-improvement without being limited by the initial bootstrapping data. Ablation studies demonstrate the importance of each component within our framework. Importantly, our method does not degrade performance on general benchmarks. In summary, our contributions are:\n\u2022 We reveal that preference pairs derived from independently sampled responses often contain interfering factors, hampering preference learning to improve instruction following. As a re-sult, a performing solution has to minimize such interference and highlight the key differences contributing to the success of instruction following.\n\u2022 We introduce SPAR, a novel self-play framework that enables continuous self-improvement in instruction-following tasks. Through three iterations, our method boosts LLaMA3-8B-Instruct to achieve GPT4-level performance and scales effectively to enhance LLaMA3-70B-Instruct.\n\u2022 We construct a high-quality dataset with 43K complex instruction-following prompts and an SFT dataset that can improve the instruction-following capabilities of LLMs."}, {"title": "METHOD", "content": "We introduce SPAR, an automated and scalable approach designed for self-improvement of instruction-following tasks through self-play. The core idea is to create paired responses with"}, {"title": "OVERALL FRAMEWORK", "content": "The overall framework of SPAR is illustrated in Figure 2. Briefly, our framework involves an actor model and a refiner model, which are both initialized from the same base model. The actor generates responses to given instructions while the refiner judges and refines these responses. This iterative self-play process, involving response generation, judgment, and refinement, fosters continuous self-improvement.\nFormally, in each iteration, given an instruction x from the prompt set, the actor generates a response y. The refiner identifies the responses that do not follow the instructions accurately, termed as negative responses. Our objective is to refine the negative response (represented as yo in Figure 2) into a correct response (represented as ys in the figure). These generated refinement pairs, e.g., (x, y8 > yo), are collected and used to optimize the actor via Direct Preference Optimization (DPO). Simultaneously, we apply Rejection-sampling Fine-Tuning (RFT) to improve the refiner. This process prepares both models for the next iteration of self-improvement.\nIn this iterative process, we face two major challenges: the scarcity of complex instruction-following data and the difficulty of achieving successful refinements. To address the lack of high-quality, multi-constraint instruction-following datasets, we generate complex instructions using a taxonomy-based approach and create corresponding SFT datasets to initialize the actor and refiner models (\u00a72.2). To ensure a high success rate in refining negative responses, we employ a tree search strategy that systematically explores refinement paths and facilitates subsequent training (\u00a72.3)."}, {"title": "DATA CONSTRUCTION", "content": ""}, {"title": "PROMPT CREATION", "content": "Given the scarcity of high-quality data for instruction-following tasks, especially those with multiple constraints, we start by creating a high-quality dataset of instruction-following prompts.\nSeed Prompts. To ensure the quality and diversity of our dataset, and to prevent issues like in-sufficient diversity or even model collapse, we use a seed set of prompts derived from the Infinity-Instruct dataset, which contains ten mil-lion high-quality conversations. After applying rule-based filtering based on length, keywords, and self-BLEU, we obtain approximately 50k seed prompts.\nTaxonomy-based Prompt Construction. Complex prompts constructed without human interven-tion tend to be poorly diversified, as the types of constraints added are often distributed unevenly. Therefore, we adopt a taxonomy-based mechanism to make constraint types com-"}, {"title": "\u0410\u0441TOR AND REFINER INITIALIZATION", "content": "The taxonomy-based prompt construction results in about 43k prompts. We utilize 8k prompts for actor initialization, another 5k for the refiner, and save 30k for further self-play training.\nActor Data Creation. To bootstrap the actor model with strong instruction-following capabilities, we first collect a strong LLM's responses to these complex prompts, thereby producing supervised fine-tuning (SFT) data (x, y) \u2208 DActor for the actor model, where x is the complex instruction and y is the strong LLM's response. Then, we fine-tune the base model to get an initial actor Mo.\nRefiner Data Creation. To bootstrap the refiner model with strong judgment and refinement ca-pability, we sample responses from the initial actor Mo. Then, we collect the judgments from a strong LLM to form a dataset, (x, y, j) \u2208 DJSFT. We collect responses that are judged not to ac-curately follow instructions and term them as negative responses. For these negative responses, we use the strong LLM to correct them with minimal revisions to avoid irrelevant variations. In this way, we get a refinement dataset, (x, Ynegative, \u0130, Yrefined) \u2208 DRSFT. The refiner is then trained with DRefiner = DJSFT U DRSFT to create the initial refiner Ro.\nTraining Strategy. For both actor and refiner models, we use standard supervised fine-tuning with the loss function:\n$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(r_i | q, r_{<i}),$\nwhere q denotes the input, r signifies the target response, and N represents the length of r. For actor training, we have input q = x and target r = y. When it comes to the refiner, we use input q = (x, y) and target r = j for DJSFT, and input q = (x, Ynegative, j) and target r = Yrefined for DRSFT."}, {"title": "TREE-SEARCH INTEGRATED SELF-PLAY TRAINING", "content": "After initializing the actor and refiner models, we embark on an iterative process for continuous self-improvement. In each iteration, we first collect the negative data, where the responses fail to accurately follow the instructions (\u00a72.3.1). Then, we utilize a tree-search algorithm to refine the negative responses (\u00a72.3.2) and form the training data for the next iteration of the actor (\u00a72.3.3) and refiner (\u00a72.3.4). This iterative self-play pipeline allows us to continuously improve both models."}, {"title": "NEGATIVE DATA COLLECTION", "content": "For each prompt x, we first sample K responses {Y1, Y2,\u2026\u2026\u2026,YK} from the actor model. This step ensures that there are enough negative responses to support subsequent learning. Then, for each prompt and response pair, we utilize the refiner to generate a judgment, which contains two parts: a label suggesting whether the response follows the instruction and an explanation about the assessment. To make this judgment more accurate, we incorporate the self-consistency mechanism, which is also applied in the subsequent refinement process. Specifically, we obtain multiple judgments from the refiner and determine the final label through majority voting, as detailed in Appendix D.4. After majority voting, we randomly select one judgment that matches the voted label to serve as the final judgment. This process allows us to identify challenging prompts that elicit responses that do not accurately follow the instructions, yielding tuples in the form of (x, Ynegative, j), where Ynegative is the incorrect response and j is its corresponding judgment."}, {"title": "TREE-SEARCH REFINEMENT", "content": "After collecting these negative instances, the core step is to refine the responses to form preference pairs. These self-refined pairs are crucial for highlighting the subtle differences that can determine the success of instruction-following tasks, thereby facilitating effective learning. Given that direct refinement often results in a low success rate, we employ a tree-search approach. We implement both breadth-first search (BFS) and depth-first search (DFS) strategies for this refinement. Detailed algorithms for these methods are provided in Appendix B.\nTo illustrate our process, we take BFS as an example and illustrate the procedure in Figure 2. Starting with an incorrect instruction-response pair and its judgment as the root node, we expand the search tree level-by-level until a correct response is found. At each intermediate node, we generate potential refinements for the current response and evaluate its correctness using the refiner. The number of generated refinements corresponds to the number of branches. Specifically, at a level of the tree, the refiner: 1). generates potential refinements for each node in the current level; 2). judges the correctness of these refinements. This creates a set of child nodes with new responses and their corresponding judgments. The search process continues until we obtain a tuple (x, Ynegative, Yrefined), where Yrefined is the newly refined, correct response. Importantly, SPAR combines the strengths of both tree-search and self-refinement, exploring multiple refinement paths while minimizing the interfering factors, producing effective preference learning data."}, {"title": "\u0410\u0441TOR TRAINING", "content": "To optimize the actor model, we leverage the refinement pairs for preference learning using DPO. At iteration t, we train the actor model Mt with refinement pairs (Ynegative, Yrefined), treating Ynegative as the rejected response (y\u0131) and Yrefined as the chosen response (yw). The training dataset is denoted as Dapo and the DPO loss is described as follows:\n$\\mathcal{L}_{DPO}(\\pi; \\pi_{ref}) = -E_{(x,y_w, y_l) \\sim D_{apo}} \\left[ \\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}\\right) \\right]$,\nwhere $\\pi_{\\theta}$ represents the actor model Mt, and the reference model tref initialized with Mt remains fixed during the training process. This results in a new actor model, Mt+1, for the next iteration."}, {"title": "REFINER TRAINING", "content": "Given that the input for the refiner is templated, we use RFT to obtain the new refiner Rt+1. The RFT training data consists of two components: the refinement data and the judgment data for improving the refiner's corresponding capabilities.\nRefinement Training Data. The refinement training data consists of tuples that capture the pro-cess of refining incorrect responses. For each incorrect response from the tree-search based refine-ment step, we collect tuples in the form of (x, Yp, \u0130p, Yrefined), where (x, yp, jp) represents the parent node of the final correct response in the refinement tree, and Yrefined is the correctly refined response.\nJudgment Training Data. The judgment training data is derived both from the negative data collection and nodes of the tree-search process. This dataset consists of tuples (x, yi, ji), where x is the prompt, yi is a response to x, and ji is the judgment consistent with majority voting.\nThen, we perform supervised fine-tuning using the constructed training data. For the refinement data Drefine we use the tuples (x, Yp, \u0130p, Yrefined) with input q = (x, Yp, jp) and target r = Yrefined. For the judgment data D judge, we use the tuples (x, yi, ji) with input q = (x, yi) and target r = ji. The supervised fine-tuning loss is given by Eq (1). By employing this self-play training process with the tree-search based self-refinement strategy, SPAR iteratively enhances both the actor and refiner models, aiming for continuous self-improvement in instruction-following tasks."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EXPERIMENT SETUP", "content": "Backbone Models. We have conducted experiments on several popular LLMs:\n\u2022 LLaMA3 Series are the best-performing models of their size, showcasing top-tier instruction-following capabilities among open-source LLMs.\n\u2022 GLM-4-9B-Chat excels in instruction-following tasks, offering competitive performance under 10B parameters.\n\u2022 Mistral-7B-Instruct is one of the most popular LLMs and has shown good performance across a wide range of tasks.\nSettings. In this work, we focus on enhancing the instruction-following abilities of LLMs in a self-play fashion. We utilize SFT to bootstrap models under 10B parameters as actor and refiner models. For the more advanced LLaMA3-70B-Instruct, we directly employ it in both roles. Following this, we perform a three-iteration self-play training using 10k prompts per iteration from our generated dataset. In each iteration, we apply DPO for the actor and RFT for the refiner. We refer to the trained LLaMA3-8B-Instruct as SPAR-8B, LLaMA3-70B-Instruct as SPAR-70B, GLM-4-9B-Chat as SPAR-9B, and Mistral-7B-Instruct as SPAR-7B. More implementation details are provided in Appendix C.\nBaselines. We compare our method with five popular self-improvement approaches, including:\n\u2022 AutoIF incorporates code feedback and online DPO training to improve instruction-following ability in both distillation and self-evolution settings.\n\u2022 SELF proposes leveraging language feedback to guide response generation in order to achieve iterative self-improvement.\n\u2022 Self-rewarding proposes to combine the reward model and policy model to enhance alignment capabilities simultaneously.\n\u2022 Meta-rewarding further introduces a meta-judge to address judgment capa-bility limitations, building on the self-rewarding framework.\n\u2022 Humpback proposes training an instruction generation model to synthesize high-quality data using web resources."}, {"title": "EVALUATION BENCHMARKS", "content": "As both the actor and refiner continually evolve within our framework, it's crucial to comprehen-sively evaluate both of their capabilities.\nActor's Instruction-following Capability. To assess the actor's ability to follow instructions, we rely on two widely-used benchmarks: IFEval and FollowBench. IFEval offers 541 verifiable instructions specifically designed for code-based evaluation. These instructions cover 25 verifiable types, including tasks like Keyword Frequency and Number of Words. FollowBench, on the other hand, encompasses five categories of more subjective constraints: Content, Situation, Style, Format, and Example. This dataset features 820 meticulously curated instructions across five difficulty levels and utilizes a hybrid assessment approach combining rule-based and LLM-as-judge evaluations.\nRefiner's Judgment and Refinement Capability. For assessing the refiner's judgment capability, we turn to LLMBar, a dataset designed to measure the assessment ability of LLMs in the context of instruction-following tasks. LLMBar includes 419 instruction-response pairs, categorized into two subsets: Natural and Adversarial. Originally, the task involves pair-wise comparisons to identify successful and failed responses. We adapted it to a point-wise judgment task, asking the model to determine whether each instruction-following task is successful.\nTo evaluate the refiner's capability in refinement, we split 200 samples from the DRSFT to create a test set, and we employ both GPT-40 and SPAR-8B-RFT-iter3, the refiner after three rounds of training, as judges to evaluate whether the refined responses are accurately following the instructions."}, {"title": "ACTOR EVALUATION RESULTS", "content": "SPAR significantly improves instruction-following ability. As illustrated in Table 1, the iter-atively trained LLMs demonstrate substantial improvements in both the IFEval and FollowBench"}, {"title": "TREE-SEARCH REFINEMENT", "content": "After collecting these negative instances, the core step is to refine the responses to form preference pairs. These self-refined pairs are crucial for highlighting the subtle differences that can determine the success of instruction-following tasks, thereby facilitating effective learning. Given that direct refinement often results in a low success rate, we employ a tree-search approach. We implement both breadth-first search (BFS) and depth-first search (DFS) strategies for this refinement. Detailed algorithms for these methods are provided in Appendix B.\nTo illustrate our process, we take BFS as an example and illustrate the procedure in Figure 2. Starting with an incorrect instruction-response pair and its judgment as the root node, we expand the search tree level-by-level until a correct response is found. At each intermediate node, we generate potential refinements for the current response and evaluate its correctness using the refiner. The number of generated refinements corresponds to the number of branches. Specifically, at a level of the tree, the refiner: 1). generates potential refinements for each node in the current level; 2). judges the correctness of these refinements. This creates a set of child nodes with new responses and their corresponding judgments. The search process continues until we obtain a tuple (x, Ynegative, Yrefined), where Yrefined is the newly refined, correct response. Importantly, SPAR combines the strengths of both tree-search and self-refinement, exploring multiple refinement paths while minimizing the interfering factors, producing effective preference learning data."}, {"title": "\u0410\u0441TOR TRAINING", "content": "To optimize the actor model, we leverage the refinement pairs for preference learning using DPO. At iteration t, we train the actor model Mt with refinement pairs (Ynegative, Yrefined), treating Ynegative as the rejected response (y\u0131) and Yrefined as the chosen response (yw). The training dataset is denoted as Dapo and the DPO loss is described as follows:\n$\\mathcal{L}_{DPO}(\\pi; \\pi_{ref}) = -E_{(x,y_w, y_l) \\sim D_{apo}} \\left[ \\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}\\right) \\right]$,\nwhere $\\pi_{\\theta}$ represents the actor model Mt, and the reference model tref initialized with Mt remains fixed during the training process. This results in a new actor model, Mt+1, for the next iteration."}, {"title": "REFINER TRAINING", "content": "Given that the input for the refiner is templated, we use RFT to obtain the new refiner Rt+1. The RFT training data consists of two components: the refinement data and the judgment data for improving the refiner's corresponding capabilities.\nRefinement Training Data. The refinement training data consists of tuples that capture the pro-cess of refining incorrect responses. For each incorrect response from the tree-search based refine-ment step, we collect tuples in the form of (x, Yp, \u0130p, Yrefined), where (x, yp, jp) represents the parent node of the final correct response in the refinement tree, and Yrefined is the correctly refined response.\nJudgment Training Data. The judgment training data is derived both from the negative data collection and nodes of the tree-search process. This dataset consists of tuples (x, yi, ji), where x is the prompt, yi is a response to x, and ji is the judgment consistent with majority voting.\nThen, we perform supervised fine-tuning using the constructed training data. For the refinement data Drefine we use the tuples (x, Yp, \u0130p, Yrefined) with input q = (x, Yp, jp) and target r = Yrefined. For the judgment data D judge, we use the tuples (x, yi, ji) with input q = (x, yi) and target r = ji. The supervised fine-tuning loss is given by Eq (1). By employing this self-play training process with the tree-search based self-refinement strategy, SPAR iteratively enhances both the actor and refiner models, aiming for continuous self-improvement in instruction-following tasks."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EXPERIMENT SETUP", "content": "Backbone Models. We have conducted experiments on several popular LLMs:\n\u2022 LLaMA3 Series are the best-performing models of their size, showcasing top-tier instruction-following capabilities among open-source LLMs.\n\u2022 GLM-4-9B-Chat excels in instruction-following tasks, offering competitive performance under 10B parameters.\n\u2022 Mistral-7B-Instruct is one of the most popular LLMs and has shown good performance across a wide range of tasks.\nSettings. In this work, we focus on enhancing the instruction-following abilities of LLMs in a self-play fashion. We utilize SFT to bootstrap models under 10B parameters as actor and refiner models. For the more advanced LLaMA3-70B-Instruct, we directly employ it in both roles. Following this, we perform a three-iteration self-play training using 10k prompts per iteration from our generated dataset. In each iteration, we apply DPO for the actor and RFT for the refiner. We refer to the trained LLaMA3-8B-Instruct as SPAR-8B, LLaMA3-70B-Instruct as SPAR-70B, GLM-4-9B-Chat as SPAR-9B, and Mistral-7B-Instruct as SPAR-7B. More implementation details are provided in Appendix C.\nBaselines. We compare our method with five popular self-improvement approaches, including:\n\u2022 AutoIF incorporates code feedback and online DPO training to improve instruction-following ability in both distillation and self-evolution settings.\n\u2022 SELF proposes leveraging language feedback to guide response generation in order to achieve iterative self-improvement.\n\u2022 Self-rewarding proposes to combine the reward model and policy model to enhance alignment capabilities simultaneously.\n\u2022 Meta-rewarding further introduces a meta-judge to address judgment capa-bility limitations, building on the self-rewarding framework.\n\u2022 Humpback proposes training an instruction generation model to synthesize high-quality data using web resources."}, {"title": "EVALUATION BENCHMARKS", "content": "As both the actor and refiner continually evolve within our framework, it's crucial to comprehen-sively evaluate both of their capabilities.\nActor's Instruction-following Capability. To assess the actor's ability to follow instructions, we rely on two widely-used benchmarks: IFEval and FollowBench. IFEval offers 541 verifiable instructions specifically designed for code-based evaluation. These instructions cover 25 verifiable types, including tasks like Keyword Frequency and Number of Words. FollowBench, on the other hand, encompasses five categories of more subjective constraints: Content, Situation, Style, Format, and Example. This dataset features 820 meticulously curated instructions across five difficulty levels and utilizes a hybrid assessment approach combining rule-based and LLM-as-judge evaluations.\nRefiner's Judgment and Refinement Capability. For assessing the refiner's judgment capability, we turn to LLMBar, a dataset designed to measure the assessment ability of LLMs in the context of instruction-following tasks. LLMBar includes 419 instruction-response pairs, categorized into two subsets: Natural and Adversarial. Originally, the task involves pair-wise comparisons to identify successful and failed responses. We adapted it to a point-wise judgment task, asking the model to determine whether each instruction-following task is successful.\nTo evaluate the refiner's capability in refinement, we split 200 samples from the DRSFT to create a test set, and we employ both GPT-40 and SPAR-8B-RFT-iter3, the refiner after three rounds of training, as judges to evaluate whether the refined responses are accurately following the instructions."}, {"title": "ACTOR EVALUATION RESULTS", "content": "SPAR significantly improves instruction-following ability. As illustrated in Table 1, the iter-atively trained LLMs demonstrate substantial improvements in both the IFEval and FollowBench"}, {"title": "Synthetic data experiment results", "content": "\u2022 Character Sequence Generation: The model needs to generate a specified number of given letters, with no restrictions on letter case, such as generating 12 letters a. For each prompt, we first construct a negative response in lowercase. In order to introduce disturbing factors, we have the correct response in uppercase for interfering pairs while maintaining refined pairs lowercase correctness.\n\u2022 Start/End Story Generation: The model is asked to generate a story that starts with sentence 1 and ends with sentence 2. The negative response lacks either sentence 1 or 2. Interfering pairs have a different story concatenated with these sentences; refined pairs keep the same story intact.\nFigure 4 shows that refinement pairs significantly outperform interfering pairs in both tasks, with larger and more effective improvements. Particularly in story generation, diverging stories results in worse accuracy than the original model. Moreover, in the character generation task, we can clearly observe that the interfering factor (uppercase ratio) is learned quickly. However, the task is not performed as well as the refinement setting, highlighting the necessity of focusing on key differences and excluding possible interfering factors.\nFurthermore, the ablation study on actor's performance in Table 4 further reveals a significant drop when refinement data is omitted. SPAR's superiority over self-rewarding and meta-rewarding meth-ods in Table 1 also underscores the importance of using refinement pairs to eliminate interfering factors. Additionally, the string-level similarity of refinement response pairs is 0.90, much higher than 0.85 of the independently sampled response pairs.\nEach element is crucial in SPAR. The primary elements of SPAR include the tree-search re-finement process and iterative training. We thus conduct ablation studies to assess the significance of these elements. For the tree-search process, as shown in Table 4, excluding tree search signifi-cantly reduces the actor's performance. This might be due to a lack of difficult samples that require more iterations to refine and a reduced number of preference pairs. Table 10 illustrates that tree search greatly outperforms greedy decoding in response refinement and surpasses other methods, such as best-of-N refinement or simple iterative refinement. Furthermore, tree search is essential for improving judgment capability, especially against adversarial inputs, as indicated in Table 5. Simi-"}, {"title": "JUDGMENT EVALUATION RESULTS.", "content": "As shown in Table 8, the judgment capability improves in each iteration and the accuracy outper-forms all baselines."}, {"title": "ABLATION STUDY ON JUDGMENT CAPABILITY.", "content": "In our experiments, we employ majority voting for iterative improvements for judgment capability. We show the results of the refiner SPAR-8B-SFT's sampling times and performance on LLMBar in Table 9. To balance the performance and computation time, we choose majority voting@5."}, {"title": "ABLATION STUDY ON REFINEMENT CAPABILITY.", "content": "Table 10 shows the results of different decoding strategies for the refinement task on SPAR-8B. For methods except greedy decoding, we use the same inference budget. We can see that the tree search algorithms largely outperform other methods, verifying the importance of incorporating tree search refinement."}, {"title": "INFERENCE-TIME SCALING COMPARISON", "content": "Figure 10 presents a comparison between SPAR and self-rewarding, focusing on their scalabil-ity with regard to inference times, measured by the number of response generations in our study. Our analysis includes both the LLaMA3-8B-Instruct and Mistral-7B-Instruct models. The results demonstrate that SPAR outperforms the self-rewarding method when additional computational re-sources are allocated for inference time, leading to enhanced performance."}, {"title": "RELATED WORK", "content": ""}, {"title": "INSTRUCTION FOLLOWING", "content": "Instruction-following is a fundamental capability of LLMs and is central to LLM alignment. Many studies have evaluated instruction-following capabilities from various perspectives. With the expanding application of LLMs, the tasks they are expected to perform become more intricate, often involving composite instructions with numerous constraints. Consequently, several benchmarks have been developed to test LLMs' abil-ity to follow these complex instructions. Additionally, multiple studies have focused on enhancing LLMs' instruction-following capabilities. One crucial aspect of the instruction-following task is that subtle differences in responses can significantly impact their correctness. Considering this, we introduce SPAR framework to construct pref-erence pairs that reduce extraneous elements to highlight these subtle variations for effective im-provements."}, {"title": "AUTONOMOUS LLM ALIGNMENT", "content": "Given the high cost of manually collecting alignment data, many studies focus on exploring au-tonomous LLM alignment methods. One common strategy involves using data distilled from advanced models to improve less powerful ones. Alternatively, as the LLMs become stronger, several studies investigate how to self-evolving LLMs' capabilities. Self-Instruct generates instructions by employing the model's in-context learning ability. Reinforced Self-Training samples data from an LLM policy and uti-lizes the dataset to enhance the policy through offline RL algorithms. Moreover, recent research has incorporated feedback from diverse sources. SELF trains LLMs to acquire meta-skills of self-feedback and self-refinement, enabling the models to self-evolve iteratively. AutoIF introduces the code execution feedback. Self-rewarding and Meta-rewarding leverage the LLM-as-judge ability to evaluate its own responses, thereby constructing preference pairs. However, these methods usually direct sample multiple in-dependent responses from the actor model, which is likely to introduce the interfering factors and thus affect the model's capture of the key differences. Thus, we propose a new framework that con-structs preference pairs by self-refining the model's responses, minimizing extraneous elements, and promoting more effective autonomous improvement."}, {"title": "CONCLUSION", "content": "In this study, we introduce a new self-play framework, SPAR, designed to improve the instruction-following capabilities of LLMs through training with refinement pairs. We reveal that, unlike tradi-tional approaches that rely on sampling multiple independent responses from the model to construct preference pairs, refining preference pairs to minimize extraneous factors and highlight key differ-ences lead to significant improvements in instruction-following tasks. Remarkably, the LLaMA3-8B-Instruct model, trained iteratively using our framework, outperforms GPT-4-Turbo on IFEval. With inference time compute scaling, its performance can be further improved. Moreover, the it-erative enhancement of instruction-following, judgment, and refinement abilities brought about by SPAR underscores a promising path to continuous self-improvement."}]}