{"title": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture", "authors": ["Colin Doyle"], "abstract": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt engineering and prompt architecture. Under this mental model, LLMs should be thought of as actors; prompts as scripts and cues; and LLM responses as performances. We apply this mental model to the task of improving LLM performance at playing Connections, a New York Times word puzzle game that prior research identified as a challenging benchmark for evaluating LLM reasoning. Our experiments with GPT-40 show that a \"Method Actors\" approach can significantly improve LLM performance over both a vanilla and \u201cChain of Thoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our dataset and a \"Chain of Thoughts\u201d approach solves 41% of puzzles, whereas our strongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's newest model designed specifically for complex reasoning tasks, o1-preview. When asked to solve a puzzle all at once, o1-preview solves 79% of Connections puzzles in our dataset, and when allowed to build puzzle solutions one guess at a time over multiple API calls, 01-preview solves 100% of the puzzles. Incorporating a \"Method Actor\" prompt architecture increases the percentage of puzzles that ol-preview solves perfectly from 76% to 87%.", "sections": [{"title": "1. Introduction", "content": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt engineering and prompt architecture. Under this mental model, LLMs should be thought of as actors; prompts as scripts and cues; and LLM responses as performances. Four principles for prompt writing and task decomposition follow from this mental model: 1) Prompt engineering is playwriting and directing. 2) Performance re-requires preparation. 3) Complex tasks should be decomposed to the point at which imitation and authenticity produce equivalent results. 4) Where imitation fails, compensate with methods that do not rely upon LLMs.\nWe apply this mental model to the task of improving LLM performance at playing Connections, a New York Times word puzzle game that prior research has identified as a useful benchmark for testing LLM complex reasoning performance. (Samadarshi et al., 2024; Todd et al., 2024). With these puzzles, a player is shown a four-by-four grid of 16 words and must identify four groups of four words that have a unique connection to one another. Each word appears in exactly one of the four groups, and each of the four groups is unique. For each game, the player is allowed to make up to three incorrect guesses and still solve the puzzle. Connections puzzles test a variety of reasoning skills for a player, and the open-ended and qualitative nature of Connections puzzles makes solving these puzzles a unique challenge for LLMs. The New York Times reports that, \"In Connections, there's no way to use math or even artificial intelligence to reliably solve the game.\"1\nOur experiments with GPT-40 show that a \u201cMethod Actors\u201d approach can significantly improve LLM performance over both a vanilla and \u201cChain of Thoughts\" approach. A vanilla approach solves 27% of the Connections puzzles in our dataset and solves 12% perfectly without making an incorrect guess. A \"Chain of Thoughts\u201d approach solves 41% of the puzzles and solves 20% perfectly. Our initial \u201cMethod Actor\" approach solves 78% of the puzzles and solves 41% perfectly. Our revised \u201cMethod Actor\" approach solves 86% of the puzzles and solves 50% perfectly.\nAfter these initial experiments were performed, OpenAI released o1-preview, an LLM model that specializes in performing complex reasoning tasks like Connections puzzles.2 Further experiments with o1-preview reveal 01-preview's superior baseline performance at Connections puzzles over GPT-40 and also reveal that using the \"Method Actor\" mental model can improve the rate at which o1-preview solves"}, {"title": "2. Related Work", "content": "2.1. Prompt Engineering and LLM System Architecture\nIn recent years, large language models have shown a remarkable improvement in performance on natural language processing tasks. (Chang et al., 2023). A wave of contemporary research focuses on improving large language models' performance at complex tasks through novel prompting techniques. These techniques can encompass both prompt engineering and prompt architecture. Prompt engineering refers to techniques for writing individual prompts to elicit LLM responses that exhibit stronger performance at natural language tasks. Prompt architecture refers to methods for structuring multiple prompts and responses to better accomplish complex tasks through a series of LLM calls. In contrast to conventional methods for improving LLM performance, prompting techniques do not require extensive retraining or fine-tuning, making these methods both cost-effective and widely accessible. (Vatsal & Dubey, 2024). Prompting techniques such as Chain-of-Thought, Tree-of-Thoughts, ReAct, and Self-consistency with CoT have shown to improve LLM performance at different complex reasoning tasks. (Wei et al., 2024; Yao et al., 2024a;b; Wang et al., 2024). Chain-of-Thought (CoT) is a prompt engineering technique for guiding an LLM to develop its reasoning step-by-step before reaching an ultimate conclusion. (Wei et al., 2024) Tree of Thoughts (ToT) builds upon CoT with the prompt architecture of a thought tree with multiple reasoning branches. (Yao et al., 2024a). But even state-of-the-art methods are limited. Their performance gains are limited to specific examples, and \u201cconcepts related to the LLM reasoning are not well-defined, hindering effective design of new more powerful schemes.\u201d (Besta et al., 2024). Unexpected and seemingly inexplicable results continue to crop up, such as research that found that injecting emotional stimuli into prompts such as, \u201cThis is very important to my career,\" can improve LLM reasoning performance. (Li et al., 2023). The goal of many of these techniques is to engineer a process by which an LLM mimics the steps of multi-step thinking and reasoning that humans perform. But \u201cwhile many schemes rely on the notion of the LLM thought, it is not clear how it relates to concepts such as a prompt.\u201d (Besta et al., 2024). Some research conceptualizes LLM responses not as \u201cthoughts\u201d but as imitations of the products of thought. (Banerjee et al., 2024; Ferrucci, 2010).\n2.2. Connections Puzzles as a Reasoning Benchmark\nThe advent of large language models has led many researchers to evaluate the models' performance playing and generating text-based reasoning games. (Gallotta et al., 2024). Prior work has evaluated LLM performance at NPR word puzzles; crossword puzzles; and the popular language-based board game, Codenames, among others. (Jaramillo et al., 2020; Zhao & Anderson, 2023; Yao et al., 2024a). Another line of research has evaluated LLM performance at generating text-based puzzles, including Connections puzzles. (Merino et al., 2024). Throughout the history of A.I., games have been used as benchmarks for model performance, including famous examples of chess, go, and Jeopardy. (Bory, 2019).\nConnections Puzzles have been proposed as a useful benchmark for testing LLM complex reasoning performance. (Samadarshi et al., 2024; Todd et al., 2024). Connections puzzles test a variety of reasoning skills for a player, including the ability to flexibly switch between methods of reasoning to solve a puzzle. The open-ended and qualitative nature of Connections puzzles makes solving these puzzles a unique challenge. Solving connections puzzles is a seemingly impossible task for LLMs, as even the New York Times reports that, \"In Connections, there's no way to use math or even artificial intelligence to reliably solve the game.\"\u00b3 At the same time, because Connections puzzles have clearly specified correct and incorrect answers, they provide objective criteria for evaluating an LLM system's"}, {"title": "3. Method Acting with Large Language Models", "content": "All the web's a stage,\nAnd all the models merely mimics;\nThey have their prompts and their tokens,\nAnd one model in its code plays many roles,\nIts scripts being endless acts.\nChat-GPT-40\n(response to a prompt asking it to rewrite Shakespeare lines to be about LLMs as method actors)\nWe introduce \"Method Actors\" as a mental model for LLMS that can guide prompt engineering and prompt architecture. Under this mental model, LLMs should be thought of as actors; prompts as scripts and cues; and LLM responses as performances. LLMs and actors have a lot in common. Both mimic the product of human thought and emotion. Success for both actors' and LLMs' performance is often measured by a performance's verisimilitude: how much the imitation of human thought and feeling seems authentic. (Chiang et al., 2024). Hallucinations are the sin qua non of both actors and LLMs. Actors' performances are faithful to the text of a script but not to external reality, just as LLMs' responses are faithful to the text of a prompt but not the external truth of the world. The analogy of LLMs as actors is more than cute it's useful. Imagining LLMs as actors performing a part can better align a user's expectations with LLMs' capabilities because prompting is more like giving a performer a cue than asking a robot mind for its thoughts. Four principles for prompt writing and task decomposition follow from this mental model:\nPrompt engineering is playwriting and directing. Prompts should set the scene like a playwright would: providing the LLM with a character, motivation, setting, and stage direction. Beyond just assigning the LLM a role, the scene should provide motivation and direction, setting up a story for the LLM to perform. Direct instructions for an LLM are like stage directions for an actor. They should focus on the form of the LLM's response, delineating the steps for an LLM to follow. Improvisation can be channeled effectively by scripting out the beats of the LLM's performance and the patterns of language to use.\nPerformance requires preparation. Under this mental model, LLM responses are not understood as thoughts but as performances. Just as the verisimilitude of an acting performance depends upon off-screen preparation the many smaller, unseen steps an actor takes to build up to the present mental state of the character the verisimilitude of an LLM's performance often requires similar background preparation. LLMs imitate the products of thinking, not thinking itself. For a complex performance, an LLM should be prompted to produce the products of any \"behind the scenes\" thinking required for a complex task. These intermediary performances can build incrementally to culminate in a complex final performance. To manage an LLM's context window, this often requires separate API calls that feed information forward to subsequent LLM calls.\nComplex tasks should be decomposed to the point at which imitation and authenticity produce equivalent results. When accurate results are required, a prompt architecture should be designed so that it relies not upon the truthfulness of an LLM's response but rather upon the authenticity of the LLM's performance. LLM responses are more reliable and accurate when an imitation of performing an action is equivalent to genuinely performing an action. The inverse is true. LLMs are less reliable and more prone to error when an imitation of a performing an action and genuinely performing an action produce divergent results. Therefore, a complex task should be decomposed into sub-tasks in which the imitation of performing the subtask and genuinely performing the subtask are equivalent.\nWhere imitation fails, compensate with methods that do not rely upon LLMs. Some complex tasks cannot be broken down into subtasks in which the imitation of"}, {"title": "4. Experimental Setup", "content": "4.1. Game Details\nConnections Puzzles are an interactive word puzzle game published on the New York Times website and mobile games app.4 The game debuted in June, 2023 and has quickly become one of the most popular and challenging puzzle games that the Times publishes. (Jennings, 2024; Schwedel, 2024). With these puzzles, a player is shown a four-by-four grid of 16 words and must identify four groups of four words that have a unique connection to one another. Each word appears in exactly one of the four groups, and each of the four groups is unique.\nThe rules for Connections are simple, but the puzzles are challenging. Players must use a variety of reasoning methods and draw upon different sources of knowledge. (Samadarshi et al., 2024). For each word, players must consider different possible meanings, relationships to words outside the puzzle, and unique linguistic attributes. The challenge of a Connections puzzle goes beyond brainstorming attributes for each word and lies in successfully grouping words together under a set of shared attributes.\nConnections puzzles are designed with a difficulty scale in mind. The Times labels each of the correct connections with a color yellow, green, blue, or purple representing the difficulty of identifying that connection. Each puzzle has one connection corresponding to each color. The connections for the easiest categories, yellow and green, are often words that are synonyms or words that belong\n4.2. Data\nConnections puzzles are released on a daily basis. This experiment used 100 recent New York Times Connections puzzles, 331 through 430, which were released between May 7, 2024 and August 13, 2024. The data for each puzzle, including the words and the solution, was acquired from a third-party website that collects past and present New York Times Connections puzzle information."}, {"title": "5. Experiment: GPT-40", "content": "The first experiment compares the performance of different approaches for solving Connections puzzles using OpenAI's GPT-40 model: Vanilla, Chain-of-Thought, Chain-of-Thought (Scripted), Actor, and Actor-2. The first three approaches vary only in the natural language content of the prompts. The two Actor approaches vary both in the natural language content of the prompts and in the prompt architecture.\nSome features are common to all five approaches. The LLM is asked to make and submit guesses one at a time. If the guess is a valid guess, the guess is submitted and checked"}, {"title": "5.1. Vanilla", "content": "The Vanilla approach sets a baseline for how GPT-40 performs at solving connections puzzles. Each guess consists of two separate API calls to the LLM. The first call prompts the LLM to guess one four-word group that represents part of the solution to the puzzle. The prompt is an edited version of the instructions on The New York Times Connections website that provides the LLM with more details about the puzzle than the website instructions provide. If the LLM cannot come up with a good guess, it is allowed to not submit a guess. The second call takes the LLM's response to the first prompt and asks the LLM to extract the guess information and format the guess in a way for subsequent steps to parse and submit."}, {"title": "5.2. Chain-of-Thought", "content": "The Chain-of-Thought approach is identical to the Vanilla approach except for changes to the \u201cmake a guess\u201d prompt. In this prompt, the LLM is assigned a role, \u201cYou are a professional puzzle solver.\" and at the end of the prompt the LLM is instructed, \"Let's think this through step-by-step.\" This now-ubiquitous closing instruction has been demonstrated many times over to improve LLM reasoning performance by having the LLM work through reasoning steps 'aloud' in the output of its responses. (Wei et al., 2024; Kojima et al., 2023)."}, {"title": "5.3. Chain-of-Thought (Scripted)", "content": "The Chain-of-Thought (Scripted) approach is identical to the Chain-of-Thought approach except for two changes to the \"make a guess\" prompt. First, the prompt includes a more expansive, carefully curated set of examples of puzzle solutions. Second, the prompt instructs the LLM to approach the puzzle solving task following a particular set of steps.\nThe prompt includes twenty-three examples of correct guesses to Connections puzzles. By comparison, the Vanilla and Chain-of-Thought approaches only had two examples of correct guesses (taken from the instructions on the New York Times website). The examples were chosen to represent each of the types of answers seen in Connections puzzles.8\nThe prompt includes instructions on a two-step process for the LLM to follow. The first step is to identify two words that have a connection with each other. The second step is to look through the remaining words and see if other words share the same connection. The LLM is also instructed to start over at the first step if it is unable to identify a group of four words that share a connection. These instructions were chosen to structure the LLM's thinking to follow a method commonly used by people solving Connections puzzles."}, {"title": "5.4. Actor", "content": "The Actor approach was designed according to the principles articulated in Section 3: prompt engineering is playwriting and directing; performance requires preparation; complex tasks should be decomposed to the point at which imitation and authenticity produce equivalent results; and where imitation fails, compensate with methods that do not rely upon LLMs.\nThe prompts for this approach include dramatic scene setting and role definition. The prompts inform the LLM that it is a professional puzzle solver who has been brought in by the FBI because terrorists have planted a bomb inside a children's hospital, and the only way to defuse the bomb is by solving this word puzzle correctly. An informal analysis comparing the LLM responses from prompts that either used or didn't use this scene-setting text revealed that including this scene-setting text resulted in the LLM using more of its output context window to keep thinking through possible solutions and less frequently concluding that it could not find an answer.\nFor prompt architecture, the task of solving Connections puzzles is divided into two stages: brainstorming and discernment. The brainstorming stage consists of five separate"}, {"title": "5.5. Actor-2", "content": "A second \"method acting\" approach, Actor-2, was conducted to test additional methods for working around LLM shortcomings within the Method-Actor approach. A qualitative analysis of the LLM responses at each stage in the Actor process revealed that the weak point in the process was the discernment stage. The LLM was able to successfully brainstorm many different possible guesses but struggled to distinguish between the good and bad guesses.\nUnder the principles spelled out in Section 3, options for revision include further decomposition or engineering around the LLM's shortcomings. Given that further decomposition did not seem viable, Actor-2 instead takes the LLM's weakness at discernment as a given and attempts to engineer the broader system to compensate for this weakness. Two changes are introduced. First, external validation criteria must be satisfied before a guess that the LLM has chosen to submit is actually submitted. Second, a validation process is introduced to filter out hallucinations.\nActor-2 keeps the brainstorm and discern processes the same as the Actor approach but revises the evaluate process to no longer rely exclusively upon the LLM's discernment over which potential guesses are strongest. This approach uses deterministic logic to help the system navigate around red herrings, which LLMs struggle to identify. After the LLM chooses to submit a guess, the guess is added to a \"final guesses\" list instead of submitting it right away. The system analyzes without an LLM's involvement whether any guesses in the \"final guesses\" list form unique pairs (i.e., there is no overlap of words between the two guesses). Once the \"final guesses\u201d list includes two guesses that are unique pairs, the guesses are submitted. In this way, the system deterministically reduces the frequency of submitting red herrings. Alternately, if the same guess appears in the \"final guesses\" list three times, the guess is submitted. One potential problem with the \"final guesses\" list is that it risks making the system much less efficient. The system may produce the same guess over and over again. To ensure that the LLM produces a diversity of guesses and explores the larger puzzle space of possibilities, words are sometimes removed from the possible words list when generating guesses. The removed words are words from guesses in the \"final guesses\" list and words from guesses that are waiting to be evaluated.\nOnce the system has submitted at least two incorrect guesses, the discern stage includes a validation process for filtering out hallucinations. If a proportion of hallucinated guesses can be filtered out from consideration, then the LLM will submit fewer incorrect guesses. To ferret out hallucinations, the LLM's prompts include \u201cmole\u201d words within the list of possible words that could comprise a guess for the puzzle."}, {"title": "6. Experiment: 01-preview", "content": "After the first experiment was performed, OpenAI released 01-preview, an LLM model that specializes in performing complex reasoning tasks like Connections puzzles. (OpenAI, 2024) A second experiment compares the performance of three different approaches use the ol-preview model: Oneshot-o1, Vanilla-o1, and Actor-o1."}, {"title": "6.1. Oneshot-01", "content": "With the Oneshot-01 approach, the LLM is prompted to solve the puzzle in its entirety within one response. With ol-preview, \"one-shot\" is a slight misnomer because although the LLM is given one prompt and returns one response ol-preview uses a backend, hidden reasoning process to \"think through\" its response before writing it. (OpenAI, 2024). OpenAI has disclosed few details about this process beyond saying that o1-preview follows a chain of thought that has been honed for reasoning tasks through reinforcement learning. (OpenAI, 2024). The process involves the creation of intermediary responses comprised of \"reasoning\" tokens that are kept hidden from the user. On the browser-based chat interface for o1-preview, the conversation history includes summaries of intermediary reasoning steps the model has taken to think through a response. At the time of running the experiment, these steps were not available via the API used for the experiment.\nThe prompt is an edited version of the instructions on The New York Times Connections website that provides the LLM with more details about the puzzle than the website instructions provide."}, {"title": "6.2. Vanilla-01", "content": "The Vanilla-01 approach is identical to the Vanilla approach from the GPT-40 experiment, except that it uses ol-preview as the model. It differs from the Oneshot-01 approach in that the LLM makes guesses one at a time rather than making all four guesses at once. The LLM receives feedback on whether prior guesses were correct or incorrect and can adjust its subsequent guesses accordingly. The process ends when the system has submitted either four correct or four incorrect guesses."}, {"title": "6.3. Actor-01", "content": "The Actor-o1 approach adapts the Actor-2 approach for the 01-preview model. Because ol already includes a backend reasoning process that was built to incorporate prompting techniques such as chain-of-thought, OpenAI discourages using these prompting techniques with o1-preview. Accordingly, the brainstorming and discern processes are simplified into one LLM call to select a guess to submit. Once five guesses are ready to submit, the LLM is asked to consider the potential guesses and select one to submit.11 That guess is added to the \u201cfinal guesses\u201d list. Given that o1-preview is more adept at generating correct guesses than GPT-40, the Actor-01 approach uses slightly different deterministic logic than Actor-2 for submitting guesses from the \"final guesses\" list. The system analyzes without an LLM's involvement whether the guesses on the \"final guesses\u201d list form unique pairs, triplets, or quadruplets. When the system has arrived at a unique quadruplet four guesses that share no words in common the system will submit that guess as a potential full solution to the problem. By waiting until the LLM has discerned four guesses without any overlap between them, red herrings are avoided because the four legitimate unique guesses cannot be made with a red herring among them. But the LLM system cannot be expected to always identify all four correct guesses within a reasonable timeframe. After the system has processed at least thirteen guesses, if the \u201cfinal guesses\" list includes three unique guesses, the system will submit those guesses. After more than fifteen guesses have been generated, the system will submit any two guesses on the \"final guesses\" list that don't overlap with one another. As with Actor-2, if the same guess appears in the \u201cfinal guesses\u201d list three times, the guess is submitted. Actor-o1 uses the same \"mole\" word"}, {"title": "7. Results", "content": "7.1. GPT-40\nOverall, chain-of-thought prompting improved GPT-40's performance at puzzle solving, and the method actor approaches improved performance even more. Table 2 shows each approach's performance. The \u201cpuzzles solved\u201d column captures how frequently each approach correctly solved the puzzle by submitting the four correct guesses before submitting four incorrect guesses and losing the game. The \"solved perfectly\" column captures how frequently each approach submitted four correct guesses without submitting any incorrect guesses."}, {"title": "7.2. 01-preview", "content": "As Table 3 illustrates, the 01-preview model represents a marked improvement over prior models at solving connections puzzles."}, {"title": "7.3. All Results", "content": "A comparison across all approaches reveals that shifting from GPT-40 to 01-preview and that incorporating a \"Method Actor\" approach to prompt engineering and architecture improves performance. A one-shot approach using 01-preview achieves comparable success rate to the Actor approach with GPT-40, although it does not perform as strongly the Actor-2 approach with GPT-40."}, {"title": "7.4. Success measured against puzzle difficulty", "content": "The success of each approach can also be measured against the difficulty of the puzzles in the dataset. The New York Times scores each puzzle's difficulty from 1 to 5. \u201cThe difficulty of each puzzle is determined by averaging the ratings provided by a panel of testers who are paid to solve each puzzle in advance to help us catch bugs, inconsistencies and other issues. A higher rating means the puzzle is more difficult.\u201d12 For this set of 100 puzzles, the difficulty ranged from 1.6 to 4.2. Figure 2 plots each approach's performance against these puzzles according to the puzzle difficulty, grouping difficulties in the ranges below 2.5 (28 puzzles), 2.5-3 (26 puzzles), 3-3.5 (33 puzzles), and above 3.5 (13 puzzles).\nThe general trend across all experimental approaches was one of higher performance for easier-rated puzzles and weaker performance for harder-rated puzzles. This tendency is most pronounced for Oneshot-01 and the weaker approaches (Vanilla, CoT, CoT-Scripted)."}, {"title": "8. Discussion", "content": "8.1. Comparison with prior work\nThis paper's results for Vanilla and Chain-of-Thought approaches are consistent with prior work testing similar approaches. Todd et al. found that Chain-of-Thought prompting with GPT-4-Turbo correctly solved 38.93% of connections puzzles and solved 23.46% perfectly. (Todd et al., 2024). In our experiments, Chain-of-Thought prompting with GPT-40 correctly solved 41% of connections puzzles and solved 20% perfectly. In Samardashi et al.'s experiment, LLMs were allowed one attempt to solve a connections puzzle without receiving feedback on any incorrect guesses, which aligns with our experiment's measurement of whether an LLM has solved a connections puzzle perfectly. (Samadarshi et al., 2024). Their vanilla approach with GPT-40 perfectly solved 5% of the puzzles. Our vanilla approach with GPT-40 perfectly solved 12% of the puzzles. The increase in performance for our vanilla approach may be due to differences in experimental design, including a validation step that prevented the submission of invalid answers and the decomposition of guess selection into multiple LLM calls. Other differences may be attributable to different connections puzzle datasets and variations in LLM performance, either due to inherent randomness or changes to the GPT-40 model over time.\n8.2. Comparison with humans\nBased upon prior evaluations of human performance at solving connections puzzles perfectly, the method-actor approaches with GPT-40 perform better than human novices and slightly worse than human experts. And each of the o1-preview approaches surpasses human expert performance at solving puzzles perfectly with Actor-o1 having the strongest performance. Samardashi et al. found that novice human players solved 18% of connections puzzles perfectly and that expert human players solved 60% of connections puzzles perfectly. (Samadarshi et al., 2024) In our experiments, the rate of solving connections puzzles perfectly was 41% for Actor, 50% for Actor-2, 72% for Oneshot-01, 76% for Vanilla-01, and 87% for Actor-o1.\nPuzzles that are easy for people tend to be easy for LLMs, and puzzles that are difficult for people tend to be difficult for LLMs. This was not a foregone conclusion, as one might expect people and LLMs to excel and fail at different kinds of puzzles. Figure 2 indicates that LLM performance tends to decrease as puzzle difficulty increases. Similarly, when an LLM only partially solved a connections puzzle, the correct guesses that it submitted tended to be the easier guesses within the puzzle. For each puzzle, the New York Times labels each of the correct guesses with a color yellow, green, blue, or purple representing the difficulty of identifying that connection from easiest (yellow) to most difficult (purple). Each puzzle has one correct guess in each color. As an example of LLM performance measured against guess difficulty, for the 21 puzzles that the Actor"}, {"title": "8.4. Actor-ol's One Incorrect Puzzle", "content": "Because the Actor-o1 approach submitted the correct solution to every puzzle except one, its performance on that puzzle deserves closer scrutiny. This puzzle was #410, rated at a medium difficulty of 2.6 out of 5 by the New York Times. Despite Actor-ol's strong performance in the overall experiment including perfectly solving 87% of puzzles, the highest among all approaches Actor-o1 was the only approach out of all seven approaches that failed to solve Puzzle #410. Actor-o1 was not even close to solving the puzzle, as it submitted only 1 correct guess and 4 incorrect guesses.\nWhat accounts for Actor-ol's failure? It may be due to chance. The behavior of language models is not deterministic. A limit of our experimental design is that each of the approaches was run only once for each puzzle due to cost constraints. Across multiple iterations, other approaches may have failed, and Actor-o1 may have succeeded. What appears as an aberration in our current results may have disappeared if multiple iterations were run.\nAs an informal test of this possibility, we ran each of the seven approaches three more times on puzzle #410. Random chance does not appear to be the culprit here. As Table 4 demonstrates, the Actor-o1 approach seems to have a peculiar difficulty with this puzzle. Over four iterations, only the Vanilla approach performed worse. Setting aside the Vanilla approach, Actor-o1 solved the puzzle less frequently and submitted more incorrect guesses than all other approaches. Notably, Oneshot-01 performed perfectly, solving the puzzle correctly four times without submitting a single incorrect guess."}, {"title": "9. Future Work", "content": "This paper demonstrates how using \"Method Actors\" as a mental model for LLMs can improve LLM performance with one particular complex reasoning task, but leaves open the opportunity to examine how this mental model affects LLM performance with different reasoning tasks or tasks distinct from reasoning, such as creative writing. There's an opportunity to draw upon the acting literature to test whether methods for improving actors' performances might also improve LLM performances. Just as method acting principles revolutionized acting in the mid-20th century to produce a new form of authentic onstage and onscreen performances, (Hirsch, 2014; Butler, 2022), similar principles may have the potential to produce stronger LLM performance today.\nIn these \"Method Actor\u201d approaches, LLMs performed a human reasoning task solving connections puzzles but not by taking the same steps that a human would. In contrast, the predominant approaches for prompt architecture for complex reasoning tend to guide an LLM to imitate the human process for reasoning through a problem. (Wei et al., 2024; Yao et al., 2024b;a). Implicitly or explicitly, these approaches treat LLM responses as equivalent to human thoughts and use these thoughts as building blocks for a structure that can tackle complex problems in a human-like way. In contrast, a method-actor approach treats LLM responses as equivalent to performances and uses these performances as building blocks for a structure that can tackle complex problems in way that deviates from how humans would solve these problems.\nAlthough designing prompt architecture to mimic human structures of cognition may expand the reasoning abilities of LLMs, the field should not be confined to this approach. At times, it can be a useful analogy to think of LLM responses as thoughts. But LLM responses are not precisely equivalent to thoughts, and the analogy may obscure other viable methods for prompt engineering and architecture. Novel structures, built from mental models of LLM responses as something other than thoughts, may achieve comparable or superior results."}, {"title": "10. Conclusion", "content": "We have introduced \"Method Actors\" as a mental model for guiding LLM prompt engineering and prompt architecture for complex reasoning. Our experiments with GPT-40 demonstrate that using the mental model for prompt writing and task decomposition can lead to significant improvements in performance at solving Connections puzzles. Further experiments with o1-preview reveal 01-preview's superior baseline performance at Connections puzzles over GPT-40 and also reveal that using the \"Method Actor\" mental model can improve the rate at which o1-preview solves puzzles perfectly. Future work can evaluate how this mental model affects LLM performance in other domains and how novel mental models can lead to unique and effective prompting methods."}, {"title": "11. Impact Statement", "content": "LLMs' ability to manipulate language and perform complex reasoning tasks are likely to have significant impacts on knowledge work and the economy at large. Only a handful of people in the world do Connections puzzles as part of their job that group may be limited to the professional puzzle testers that the New York Times pays to test its puzzles each day. So automating the task of solving Connections puzzles is not a direct threat to anyone's livelihood. But the prospect of automating complex reasoning tasks means that many parts of knowledge work jobs that were previously impervious to automation are now under threat. LLM systems may perform certain kinds of knowledge work more cheaply and efficiently than human labor, but the broader impact on public welfare is uncertain and is dependent on other political and economic factors. Wider adoption of the \"Method Actors\" approach to LLM system design may also have a negative environmental impact. Compared to simpler one-shot methods, more complex LLM systems require greater computational resources because the system design involves synthesizing and leveraging the results of many separate API calls to an LLM. At the same time, the use of many independent API calls creates a system with greater transparency and interpretability."}, {"title": "A. Example Prompts", "content": "A.1. Vanilla\nYour job is to solve a word puzzle that is just like a New York Times connections puzzle. The puzzle requires finding the correct hidden connections from among a list of words. From a list of words", "Examples": "nFISH: Bass", "Trout\nFIRE": "Ant, Drill, Island, Opal\nCategories will always be more specific than \u201c5-LETTER-WORDS,\u201d \u201cNAMES\u201d or \u201cVERBS.\u201d\nEach puzzle has exactly one solution. Watch out for words that seem to belong to multiple categories!\nYou don't need to solve the whole puzzle at once. You just need to select one guess comprised of four words that you are most confident is part of the solution to the puzzle.\nYou should reject any guess that has the same four words as a guess that we already know is incorrect.\n[[bad_guesses"}]}