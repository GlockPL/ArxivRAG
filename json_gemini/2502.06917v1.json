{"title": "KRUM FEDERATED CHAIN (KFC): USING BLOCKCHAIN TO DEFEND AGAINST ADVERSARIAL ATTACKS IN FEDERATED LEARNING", "authors": ["Mario Garc\u00eda-M\u00e1rquez", "Nuria Rodr\u00edguez-Barroso", "M.V. Luz\u00f3n", "Francisco Herrera"], "abstract": "Federated Learning presents a nascent approach to machine learning, enabling collaborative model training across decentralized devices while safeguarding data privacy. However, its distributed nature renders it susceptible to adversarial attacks. Integrating blockchain technology with Federated Learning offers a promising avenue to enhance security and integrity. In this paper, we tackle the potential of blockchain in defending Federated Learning against adversarial attacks. First, we test Proof of Federated Learning, a well known consensus mechanism designed ad-hoc to federated contexts, as a defense mechanism demonstrating its efficacy against Byzantine and backdoor attacks when at least one miner remains uncompromised. Second, we propose Krum Federated Chain, a novel defense strategy combining Krum and Proof of Federated Learning, valid to defend against any configuration of Byzantine or backdoor attacks, even when all miners are compromised. Our experiments conducted on image classification datasets validate the effectiveness of our proposed approaches.", "sections": [{"title": "Introduction", "content": "In the realm of artificial intelligence, Federated Learning (FL) [1] stands as a promising paradigm revolutionizing traditional machine learning approaches. Its distributed nature allows for training models collaboratively across decentralized devices while preserving data privacy. Furthermore, the integration of blockchain [2] technology into FL has emerged as a promising solution to enhance its security and integrity [3]. By leveraging the immutable and transparent nature of blockchain, FL systems can establish a tamper-resistant record of model updates and participant contributions. This not only enhances the trustworthiness [4, 5] of the collaborative learning process but also mitigates the risk of data manipulation and unauthorized access. Additionally, blockchain facilitates the establishment of smart contracts, enabling automatic verification and enforcement of agreements between participating nodes. Such advancements not only bolster the security of FL but also streamline its operation, paving the way for widespread adoption across diverse industries.\nDespite FL's significant benefits, such as data privacy by processing data locally on user devices, this same decentralized feature can also be a weakness [6]. In a typical adversarial attack, a malicious actor could manipulate the data or learning models at one of the system's nodes without other nodes being aware of the manipulation. Since the data isn't centrally collected, detecting these anomalies can be more challenging, making FL systems particularly vulnerable to such attacks [7]. Understanding and addressing these vulnerabilities are crucial to ensuring the security and robustness of FL systems.\nAs stated before, blockchain technology [8] has been extensively utilized in combination with FL, to enhance data integrity and transparency across distributed networks. However, its use has not been significantly explored as a defense against adversarial attacks in FL systems.\nWe hypothesize that the immutable and transparent nature of blockchain could potentially be harnessed to develop mechanisms that detect and prevent such attacks. By leveraging blockchain's decentralized ledger, it might be possible to create a more secure environment where alterations to data or models are immediately noticeable and traceable, thereby providing a robust layer of security against adversarial threats in FL applications. First, we test Proof of Federated Learning (PoFL) [9], a well known consensus mechanism designed ad-hoc to FL created with energy efficiency in mind, as a defense against Byzantine and backdoor attacks [10]. We find that PoFL is useful when there is at least one miner not being attacked, but the federated scheme remains vulnerable when all the miners are being attacked.\nSecond, the proposal to tackle the vulnerabilities of PoFL identified regarding the configuration of the attack, Krum Federated Chain (KFC). It is a defense based on the combination of the Krum aggregation operator [11] and blockchain with PoFL consensus mechanism, to defend FL against any configuration of Byzantine and backdoor attacks, including the configuration in which all the miners are being attacked.\nWe test our contribution using two different attacks, over three image classification datasets, namely: EMNIST [12], Fashion MNIST [13], and CIFAR-10 [14]. As adversarial attacks we consider:\n\u2022 Byzantine attacks [11, 15], which aim to degrade model performance by sending random updates. Specifically, we consider label-flipping Byzantine attacks [15] which involve randomly altering the labels of data points.\n\u2022 Backdoor attacks [10], which consists of injecting a secondary (or backdoor) task while maintaining the performance of the model in the original task. We focus on pattern-key backdoor attacks [10] which are based on injecting the secondary task using specific patterns out of the input distribution.\nRegarding the attack scenarios, we consider two:\n\u2022 Just one miner being attacked.\n\u2022 All the miners being attacked.\nAs baselines, we use other consensus mechanisms, such as: Proof of Work and Proof of Stake, raw FL without blockchain (namely, client-server) and furthermore, we evaluate prominent defense mechanisms commonly utilized in FL, namely: Krum [11] and Trimmed-mean [16].\nThe paper is organized as follows: in Section 2 we introduce some background concepts including FL in Section 2.1, adversarial attacks in FL in Section 2.3 and related works about blockchain applied to FL in 2.4. In Section 3 we explain the hypothesis of using PoFL as a defense mechanism. In Section 4 we detail the proposed defense mechanism KFC. Hereafter, we specify the experimental set up in which we test our contributions in Section 5 including: evaluation datasets (see Section 5.1), label-flipping Byzantine attacks (see Section 5.2), pattern-key backdoor attacks (see Section 5.3), attack scenarios (see Section 5.4), evaluation metrics (see Section 5.5), baselines (see Section 5.6) and implementations details (see Section 5.7). Finally, we test the hypothesis of PoFL in Section 6 and prove the proposal KFC in Section 7. The present study's limitations are discussed in detail in Section 8. The conclusions and future work are detailed in Section 9."}, {"title": "Background", "content": "In this section we provide the necessary concepts to follow the rest of the paper. In Section 2.1 we provide the basic notions about FL, Section 2.2 provides an introduction to the fundamental concepts of blockchain technology, in Section 2.3 we define the adversarial attacks in the context of FL subsequently delving into untargeted attacks and backdoor attacks in Sections 2.3.1 and 2.3.2 respectively, followed by an explanation of the model replacement technique in Section 2.3.3 and, finally, in Section 2.4 we relate the concepts of FL and blockchain according to recent related works."}, {"title": "Federated Learning", "content": "FL embodies a distributed machine learning paradigm aimed at constructing machine learning models without the direct exchange of training data among participating entities [17]. It operates within a network of data proprietors called clients, denoted by $\\{C_1,..., C_n\\}$ with their respective datasets $\\{D_1, . . ., D_n\\}$, engaging in two primary phases:\n1. Model training phase: During this phase, each client collaborates by sharing information without revealing their raw data, thereby collectively training a machine learning model. This model may be housed at a single client or distributed across multiple clients.\nFor this, each client, $C_i$, trains a local model, $L_i$, parametrized by a vector $V_i$, using its own data $D_i$. These local model parameters are then aggregated to form a global model G, parametrized by $V_G = \\Delta(V_1, ..., V_n)$, using a fixed federated aggregation operator, $\\Delta$. The global model parameters is distributed back to the clients, and the process is repeated iteratively until a predefined stopping criterion is met.\n2. Inference phase: Subsequently, clients work collaboratively to apply the jointly trained model, to process new data instances.\nBoth phases can operate synchronously or asynchronously, contingent upon factors such as data availability and the status of the trained model.\nIt's pivotal to note that while privacy preservation is central to this paradigm, another crucial aspect involves establishing a equitable mechanism for distributing the profits accrued from the collaboratively trained model."}, {"title": "Blockchain technology", "content": "A blockchain is a distributed ledger technology that stores a sequence of blocks containing a set of verifiable transactions. The blockchain is replicated and shared across multiple nodes called miners in a peer-to-peer network [19, 8], ensuring that every node maintains a copy of the blockchain. Formally, a blockchain can be defined as a finite sequence of data blocks $b_1, b_2, b_3, ..., b_n$, each possessing the following attributes:\n\u2022 Block ID: Denoted by $b_i.id$, it contains the hash value of some part of the block's content, generated through a predefined and publicly known cryptographic hash function $H$. A prevalent approach utilizes the block header to efficiently derive a unique identifier for the block. This identifier typically incorporates the Merkle root hash of the data contained within the block. Formally, this identifier can be represented as $b_i.id = H(b_i)$, where $H$ denotes a cryptographic hash function and $b_i$ signifies the i-th block. Notably, due to the readily computable nature of this identifier, it is often omitted from the stored block data.\n\u2022 Previous hash: Denoted by $b_i.prev$, it contains the identifier of the preceding block, $b_{i\u22121}$, to which $b_i$ is appended. This can be expressed as $b_i.prev = b_{i\u22121}.id."}, {"title": "Adversarial attacks in Federated Learning", "content": "Adversarial attacks pose a significant challenge to FL. The decentralized nature of FL can be exploited by malicious clients who may send poisoned updates to deliberately modify the global model. While adversarial attacks is a general problem in artificial intelligence [23], in FL the server's inability to directly inspect the client-side training data worsen this vulnerability. These attacks typically involve clients with white-box access to the aggregated model, their locally trained models, and their datasets. In many cases, attackers are assumed to possess even more information, including knowledge of the aggregation mechanism used by the server. These attacks can be categorized based on their objectives into targeted attacks, also known as backdoor attacks, and untargeted attacks.\nIn order to ensure the effectiveness of the attack and the no mitigation of the attack in the aggregation with benign clients' adversarial attacks are usually combined with model replacement techniques [10]."}, {"title": "Untargeted attacks in Federated Learning", "content": "The primary objective of untargeted attacks is to degrade the overall performance of the global model on its intended task. A particularly severe type of untargeted attack is known as a Byzantine attack, where malicious clients share randomly generated model updates or train on randomly modified data. These attacks can be detected by analyzing the performance of local model updates on the server. However, differentiating between Byzantine attacks and clients with very particular training data distributions can be challenging. [24]\nFormally, in the context of FL, the primary objective is to collaboratively train a global model $G$ by aggregating local models $L_i$ from multiple clients. This global model aims to approximate a target function $m : X \\rightarrow Y$, where $X$ represents the input space and $Y$ denotes the output space. An untargeted attack in FL seeks to maliciously perturb a set of the local models such that the resulting global model $\\hat{G}$ approximates instead a function $\\hat{m}$ characterized by $\\hat{m}(x) \\neq m(x)$ for all inputs $x \\in X$."}, {"title": "Backdoor attacks in Federated Learning", "content": "Backdoor attacks [10] consist of injecting a secondary or backdoor task while maintaining the performance in the original task. Their design has a wide range of options depending on the injected (backdoor) task [25]. In [24] a taxonomy for backdoor attacks is presented, categorizing them into two primary types: (1) input-instance-key, where the model target specific input instances, manipulating their labels to a predetermined target label different from the original, and (2) pattern-key, where the model associate a particular pattern within an input sample with a specific target label.\nFormally, the goal is to create a model $\\hat{G}$ that correctly approximates the desired function [26], $m$ while simultaneously learning a hidden function $m^* : X^* \\rightarrow Y$, where $X^*$ is the domain of inputs that triggers the backdoor. The trigger patterns can be part of the original input space $X$ in the case of input-instance-key strategies or a modified version of it in the case of pattern-key strategies. By learning both $m$ and $m^*$, the model $\\hat{G}$ will try to learn the function $m$ defined by Equation 1."}, {"title": "Model replacement technique for attacks in Federated Learning", "content": "The primary objective of this technique is to amplify the influence of the adversarial attack to avoid mitigation effects by other clients' updates. Let $G^t$ and $L_i^t$ be the global model and local model respectively, of the i-th client at a given round t, n the number of clients participating in the round and \u03b7 the server learning rate. Then, the update of the global model parameters in the round t is performed as in Eq. 2:\n$V_G^{t+1} = V_G^t + \\eta \\sum_{i=1}^n (V_i^t - V_G^t)$\nWe consider that, in the learning round t, only one adversarial client is selected which will try to replace the global model $G^t$ with his adversarial model $L_{adv}^t$, which is optimized for the goal of the attack. To do so, a new model is computed as follows\n$\\delta = \\beta (V_{adv}^t - V_G^t)$\nwhere $\\beta = \\frac{\\eta}{n}$ is the boost factor. Replacing Eq. 3 in Eq. 2 we deduce\n$V_G^{t+1} = V_G^{t-1} + \\frac{\\eta}{n}(V_{adv}^t - V_G^{t-1}) + \\frac{\\eta}{n} \\sum_{i=2}^n (V_i^t - V_G^{t-1})$\nNow, if we assume that the model converges, eventually we may also assume that $V_i^t - V_G^{t-1} \\approx 0$ for benign clients. Thus, we can approximate Eq. 4 by\n$V_G^{t+1} \\approx V_G^{t-1} + V_{adv}^t - V_G^{t-1} = V_{adv}^t$\nwhich replaces the global model with the model trained by the adversarial client. In the case where multiple adversarial clients participate, the boosting factor \u03b2 is divided by the number of attackers. In this paper, we consider \u03b7 = 1."}, {"title": "Related works on blockchain applied to federated learning", "content": "The characteristics of blockchain have derived into proposals about how to implement blockchain in a FL system [27, 28, 29]. This previous works tackle problems present in classical FL such as an environment with the server as a single point of failure or poor scalability in some scenarios [24]. As outlined in [29], a taxonomy based on network topology classifies blockchain-federated learning architectures into three primary categories: (1) decoupled, where nodes are restricted to operating exclusively within either the blockchain or the FL system, (2) coupled, all nodes participate in both the blockchain and FL systems, and (3) overlapped, which encompasses nodes that can operate within the blockchain, the FL system, or both. It is noteworthy to observe the predominance of decoupled architectures in the literature [30, 31, 32]. The nodes operating within the blockchain are called miners, meanwhile the ones operating within the FL system are called clients. Thus according to the architecture this terms may be (in the case of a coupled architecture) or not be (in the case of a decoupled or overlapped architecture) equivalent.\nA relevant number of this proposed architectures are found to use PoW as a consensus mechanism, which may be considered unfeasible due to the computational cost of a network using it [33]. Thus, is common to see consensus mechanisms with a higher energy efficiency such as PoS to be utilized instead, but they usually require the presence of a economic reward mechanism which may not always be assumed. This have led to the creation of consensus mechanisms specific to FL, where PoFL [9] is the most widely used.\nConsensus mechanism can be considered one of the most important parts of a blockchain architecture applied to FL since it usually determines the way training and aggregation is performed during the learning process of the federated model and thus, a significant part of the architecture of the network. However, there is little literature about the impact of this part of a blockchain network on adversarial attacks against the federated model."}, {"title": "POFL as a defense against adversarial attacks: A testing hypotesis", "content": "In this section we test the hypothesis that PoFL, the consensus mechanism, can be a defense against adversarial attacks in FL.\nInspired by the Proof-of-Useful Work concept [34], PoFL retains the core principle of PoW by requiring participants to solve computationally intensive problems to achieve consensus. However, unlike PoW, which utilizes computational power for tasks with no inherent value (e.g., finding a specific hash nonce in Bitcoin [8]) and thus being usually discarded for real applications, PoFL redirects these resources towards training a FL model empowering energy efficiency.\nIt is important to acknowledge that PoFL leverages a decoupled pooled-mining architecture. Within this structure, network clients are divided into distinct pools, each overseen by a designated miner. These pools operate independently, training their respective federated models without inter-pool communication. The pool that trains the model exhibiting the highest accuracy on a predetermined test dataset emerges victorious in the consensus round. Consequently, the winning model is integrated into the blockchain and broadcasted throughout the network to all clients. The process in each pool is showed in Figure 2:\n1. The miner broadcasts an initial model to the clients.\n2. Clients train the locally held copy of the model using their private data. This training step involves calculating the local updates based on the difference between the received model and the trained one.\n3. The clients send the model updates to the miner. The miner aggregates these updates to create a new, improved model. The miner evaluates the performance of the aggregated model against a predetermined accuracy goal. If the goal is achieved, consensus is reached for that round.\n4. Upon reaching consensus, the miner appends the improved model to the blockchain and broadcasts it to all miners and to its corresponding pool, and thus all participating clients, as shown in the step 4 of Figure 2. If consensus is not achieved (i.e., accuracy goal not met), the process restarts from step 2, with clients training until the desired accuracy is attained.\nFormally, we can model a blockchain network using PoFL as a set of pools $P_1, P_2, ..., P_n$, where each pool $P_i$ contains a miner $m_i$ and a set of clients $C_{i_1}, . . ., C_{i_j}$. In every learning round t, every pool $P_i$ will compute a federated model denoted by $L_i^t$ through a federated training process coordinated by the miner $m_i$. Then every miner $m_i$ computes the accuracy of its model $L_i^t$ on a determined dataset D, which we denote by $accuracy(L_i^t, D)$. Then, the miner $m_{i^*}$ wins consensus if\n$accuracy(L_{i^*}^t, D) > accuracy(L_i^t,D) \\forall i \\in \\{1,2,..., n\\}$.\nUpon reaching consensus, the model $L_{i^*}^t$ is broadcasted to all miners $m_j$ with $j \\neq i^*$ and check that the claimed accuracy is true. After a successful verification, the miner $m_{i^*}$ creates a new block $b_t$ which sets $G^{t+1} = L_{i^*}^t$ and appends it to the ledger. Finally, all the miners download the new version of the blockchain and broadcast the new global model $G^{t+1}$ to their clients, ensuring that all clients have access to the latest model."}, {"title": "Krum Federated Chain Defense Strategy", "content": "It is crucial to acknowledge that the potential resistance of PoFL against adversarial attacks hinges on the optimistic assumption that a pool lacking of adversarial clients consistently exists. This assumption can be considered unrealistic in real-world scenarios. Therefore, we propose the KFC defense strategy as a more robust security alternative to PoFL. While KFC maintains the same architectural foundation and energy efficiency principles as PoFL, it incorporates additional security mechanisms to mitigate adversarial attacks even in the presence of malicious actors within all the pools in the network.\nKFC leverages the Krum aggregation operator [11], to enhance its resilience against attacks on the federated model. This operator functions by sorting client updates based on the geometric distances between their respective model update distributions. Subsequently, it selects the update closest to the majority, effectively filtering out outliers. Adversarial clients attempting to manipulate the model are likely to generate updates that deviate significantly from the norm, making them susceptible to identification and exclusion by the Krum operator.\nFormally, we represent the update received from the i-th client as a vector $V_i$ inside a finite dimensional real vector space $R^d$. The Krum aggregation rule, denoted by $KR(V_1, . . ., V_n)$, is then defined as follows. For any two clients i and j, where $i \\neq j$, we use the notation $i \\rightarrow j$ to indicate that the vector $V_j$ belongs to the set of $n \u2212 f \u2212 2$ closest vectors to $V_i$ in terms of euclidean distance. Building upon this concept, a score can be assigned to each client:\n$s(i) = \\sum_{i \\rightarrow j} ||V_i \u2013 V_j||^2$.\nThat is, the sum of the square of distances between the $n \u2013 f \u2013 2$ closest vectors to $V_i$. Then, $KR(V_1, . . ., V_n)$ is set to be $V_{i^*}$ where $i^*$ is the index of the minimal score, that is, $s(i^*) \\leq s(i)$ for every i. Note that f is a parameter that we must take into account when using the Krum operator, in this paper we default it to be 1. This parameter serves as a threshold for the expected volume of adversarial clients. In practice, a conservative static value is typically employed.\nAs stated in Krum original paper [11], the Krum operator exhibits a time complexity of $O(n^2 \\cdot (d + log n))$ where d denotes the data dimensionality and n represents the number of clients. Notably, in the KFC framework, n specifically refers to the client count within an individual pool, rather than the entire network, thereby facilitating horizontal scalability of the system.\nKFC inherits the pooled-mining architecture from PoFL while incorporating the Krum aggregation operator. This design choice allows KFC to benefit from the inherent resistance to adversarial attacks offered by the underlying blockchain architecture and consensus mechanism. Moreover, the Krum operator reinforces KFC's security posture by filtering out outliers within each pool's client update distributions. This mechanism alleviates the need for the assumption of a pool entirely devoid of malicious actors. This combined approach leverages multiple defense mechanisms, resulting in a more robust solution for FL under adversarial conditions."}, {"title": "Experimental set up", "content": "The evaluation of the hypothesis of PoFL as defense mechanism and the proposed KFC is performed by means of the accuracy of the resulting FL model in three image classification datasets arranged for FL. This section specify: the datasets employed in Section 5.1, the Byzantine attack used in Section 5.2, the backdoor attack used in Section 5.3, the details about attacks scenarios in Section 5.4, the evaluation metrics employed in Section 5.5, the baselines which we compare with in Section 5.6 and, finally, the implementations details in Section 5.7."}, {"title": "Evaluation datasets", "content": "For the evaluation of both analyses we use classical image classification datasets. Since PoFL and KFC need a validation set for computing the accuracy of each model, we fix the 20% of the test data to validation data and the remaining 80% to the test data. The three datasets used in the evaluation are described as what follows:\n1. The EMNIST dataset, which contains a balanced subset of the digits dataset containing 28,000 samples of each digit. The dataset consists of 280,000 samples, which 240,000 of them are used for training and 40,000 as test samples (we use 8,000 as validation samples and the remaining 32,000 as test samples). We set the number of clients to 200 proportionally distributed among 3 miners.\n2. The Fashion MNIST [13] which contains a balanced subset of the 10 different classes containing 7,000 samples of each class. Thus, the dataset consists of 70,000 samples, which 60,000 are training samples and 10,000 test samples (we use 2,000 as validation samples and 8,000 as test samples). We set the number of clients to 200 proportionally distributed among 3 miners.\n3. The CIFAR-10 dataset which consists of 60,000 color images in 10 classes, with 6,000 images per class. 50,000 of the samples are used as training images and 10,000 as test images (we use 2,000 as validation samples and 8,000 as test samples), which correspond to 1,000 of each class. We set the number of clients to 100 (in order to have more data per client as the problem is more challenging) proportionally distributed among 2 miners."}, {"title": "Label-Flipping Byzantine Attacks", "content": "Regarding the untargeted attack, we implemented a Byzantine attack based on label-flipping. In this attack, the labels of randomly selected training data points were intentionally modified to a label different from the original. This deliberate mislabeling would induce adversarial clients to generate random model updates, ultimately leading to a degradation in the performance of the global model. The general procedure of this attack is illustrated in Figure 3."}, {"title": "Pattern-key Backdoor Attacks", "content": "Regarding the backdoor attack, we implement pattern-key backdoor attacks, in which all the clients know the complete pattern and use it in their training process. We set a target label and a pattern key. The attack consists in classifying any sample poisoned with the pattern-key as the target label. We employed a single static pattern for the attack, as this type of attack is generally more effective due to its collective nature targeting a common target [24]. In order to show that the behavior observed is agnostic of the pattern-key, we use two patterns for each dataset shown in Figure 4: (1) a single black pixel for EMNIST and Fashion MNIST and a 5x5 white square for CIFAR-10, and (2) a black cross of length 3 for EMNIST and Fashion MNIST and a white cross of length 5 for CIFAR-10."}, {"title": "Attack scenarios", "content": "With the aim of testing the proposals in different configurations we consider two distinct scenarios:\n1. Scenario A: There is only one adversarial client participating at a certain learning round t.\n2. Scenario B: The amount of attackers in a learning round is set to the number of miners in the network. In the case that the architecture being tested makes use of pooled-mining, that is, each miner get assigned a subset of the clients and make no use of any kind of resource from another miner's subset, we assure that each pool contains one adversarial client."}, {"title": "Evaluation metrics", "content": "Since the purpose of an untargeted attack is to degrade the performance of the model, we will consider the accuracy of the model in the corresponding test dataset of the model. On the other hand, when measuring a defense against a backdoor attack, we must consider two aspects. The performance of the resulting model in the original task and in the backdoor task. The aim is to reduce the effects of the attack as much as possible without compromising the performance of the model in the original task. For measuring this double goal we consider the following test datasets:\n1. Original task test. The original test of the dataset used for measuring the performance in terms of accuracy in the original task.\n2. Backdoor task test. This dataset represents the attack in order to measure the performance of the backdoor task. Consists of the test instances but poisoned using the pattern in order to measure the capability of generalization of the attack.\nSince the results may be highly heterogeneous in each learning round and in order to show robust results, we offer two distinct metrics for each dataset: (1) the accuracy in the last learning round of the model in the dataset, which will be denoted by accuracy and (2) the average of accuracy throughout the last ten learning rounds, which will be denoted by accuracy10. Also, Since results may differ from each miner in the case of PoFL and KFC, we show the metrics of the best miner in terms of performance in the original task, which we can measure with the validation dataset used by the PoFL consensus mechanism."}, {"title": "Baselines", "content": "We compare PoFL and KFC with the following FL architectures which represent the classical baselines and the most studied architectures for blockchain applied to FL, ecmland the most common aggregation mechanisms :\n1. Client-Server (C-S)[36]. It makes no use of blockchain. It is one of the most common FL settings where one central server acts as an aggregator and schedules the whole learning process.\n2. Proof Of Work (PoW)[29]. A coupled architecture using PoW, the most famous consensus mechanism where miners race each other in order to solve a computationally expensive puzzle. The winner will act as the aggregator for the given learning round.\n3. Proof Of Stake (PoS)[29]. A coupled architecture using PoS, a general purpose consensus mechanism proposed as an energy efficient alternative to PoW where the selected miner is chosen on a deterministic fashion based on his stake or wealth on the network.\n4. Krum[11]. It makes no use of blockchain. It employs of the Krum aggregation operator presented previously.\n5. Trimmed-mean[16]. It makes no use of blockchain. It utilizes the trimmed mean operator, a statistical method that excludes a specified percentage of the most extreme data points from the calculation of the mean. By default, 10% of the data points are removed from each end of the distribution.\nIn C-S, PoW, PoS and PoFL we used Federated Averaging (FedAvg) [36] as a federated aggregation operator which is often considered as the default aggregation operator for FL and the most studied in literature. It is noteworthy that both PoS and PoW consensus mechanisms operate within the same underlying blockchain architecture. To facilitate a comprehensive understanding of the results, we present combined data from both PoS and PoW implementations and we refer to this baseline as PoW/S."}, {"title": "Implementation details", "content": "As the main purpose of the work is to defend against adversarial attacks, we make use of standard image classification deep learning models. We deploy an image classification deep learning model in the FL setting. We use an standard CNN-based image classification model composed of two CNN layers followed by its corresponding max-pooling layers, a dense layer and the output layer for the EMNIST and Fashion MNIST datasets and a pre-trained model based on EfficientNet [37] for the CIFAR-10 dataset. We provide the code used to run all the experiments in order to ensure reproducibility\u00b9. The code is written using the FLEXible FL framework [38] and making use of his companion libraries flex-block and flex-clash for simulating the blockchain behavior and the attacks on the FL scheme respectively."}, {"title": "Analysis of PoFL as a defense mechanism", "content": "In this section we analyze the performance of PoFL as a defense mechanism in Scenario A (see Section 6.1) and Scenario B (see Section 6.2). We provide in this section the key information for understanding our findings. A more detailed visualization of our results can be found in Appendix A."}, {"title": "Scenario A: just one miner being attacked", "content": "In this section we analyze the performance of PoFL as a defense in Scenario A. The results shown in Figure 5 reveal a consistent trend throughout the experiments. We now delve into a detailed explanation of this observed pattern:\n1. In the context of the Byzantine attack, PoFL consistently demonstrated a high performance. Conversely, the PoW/S and C-S architectures exhibited fluctuating accuracy levels and generally lower values compared to those achieved by PoFL, confirming the success of the Byzantine attack. Finally, Krum exhibited resistance but a lower performance than PoFL, which is an expected result of this operator. The Trimmed-mean approach, though effective in two of the datasets, exhibited a significant drop in performance on CIFAR, indicating its limitations as a defense strategy in certain scenarios.\n2. For the backdoor attack, PoFL and Trimmed-mean demonstrates consistent superiority across original task test sets, achieving the highest accuracy throughout the training process. While the baseline approaches exhibit comparable accuracy levels, they consistently fall short of PoFL's performance."}, {"title": "Scenario B: all miners being attacked", "content": "In this section we analyze the performance of PoFL as a defense in Scenario B. The results shown in the Figure 6 highlight the main weaknesses of PoFL. A closer examination of these results reveals the following observations:\n1. In the context of the Byzantine attack, PoFL exhibited fluctuations during training similar to those observed in C-S or PoW/S during the previous scenario, accompanied by a decline in accuracy. This performance degradation indicates the susceptibility of the PoFL architecture to the Byzantine attack in this particular scenario. The Krum operator shows no decline indicating the success in supressing the attack.\n2. For the backdoor attack, in contrast to the previous scenario where PoFL emerged as a high performing architecture, here it exhibits a weak performance in the original task. PoFL shows inconsistency with fluctuations throughout the training process. The considered baseline approaches, however, maintain a similar behaviour compared to scenario A, unlike PoFL's significant decline.\n3. Moreover, in this scenario, PoFL exhibits a concerning vulnerability. PoFL achieves high performance on the backdoor tasks, indicating success for the attack. While its accuracy is lower than the baselines, which remains consistent with the previous scenario, it remains concerningly high (over 80% in average). Notably, even with fluctuations, PoFL exhibits a trend towards convergence on high accuracy for the backdoor task.\nIn conclusion, the experiments paint a concerning picture for PoFL's suitability in this scenario. As evidenced by the previous observations, PoFL exhibits a significant vulnerability to adversarial attacks. Not only does it achieve concerningly high accuracy on the backdoor tasks, exceeding 80% on average, but its performance on the original task under both attacks also suffers a substantial decline.\nThis vulnerability of PoFL motivates the design of KFC, as the combination of PoFL and a defense mechanism in each miner making it resilient to adversarial attacks even when all miners are being attacked. We go through this question in the following section."}, {"title": "Analysis of the performance of KFC Defense Strategy", "content": "In this section we analyze the performance of KFC as a defense against backdoor attacks in Scenario A (see Section 7.1) and Scenario B (see Section 7.2)."}, {"title": "Scenario A: just one miner being attacked", "content": "If we analyze the results shown in Figure 7 we conclude that indeed the results obtained by KFC in scenario A, where PoFL was already an effective defense, are equivalent. If we take a more detailed inspection, there is a slight accuracy decrease in the Byzantine attack and in the original task due to this mechanism of selecting only the best client in each miner given by Krum. Our analysis indicates that the KFC defense mechanism outperforms the Krum aggregation operator, upon which it is founded. These findings suggest that KFC is a viable and effective countermeasure against"}]}