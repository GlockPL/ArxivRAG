{"title": "VIMI: Grounding Video Generation through Multi-modal Instruction", "authors": ["Yuwei Fang", "Willi Menapace", "Aliaksandr Siarohin", "Tsai-Shien Chen", "Kuan-Chien Wang", "Ivan Skorokhodov", "Graham Neubig", "Sergey Tulyakov"], "abstract": "Existing text-to-video diffusion models rely\nsolely on text-only encoders for their pretrain-ing. This limitation stems from the absence of\nlarge-scale multimodal prompt video datasets,\nresulting in a lack of visual grounding and re-stricting their versatility and application in mul-timodal integration. To address this, we con-struct a large-scale multimodal prompt dataset\nby employing retrieval methods to pair in-context examples with the given text prompts\nand then utilize a two-stage training strategy\nto enable diverse video generation tasks within\nthe same model. In the first stage, we pro-pose a multimodal conditional video generation\nframework for pretraining on these augmented\ndatasets, establishing a foundational model for\ngrounded video generation. Secondly, we fine-tune the model from the first stage on three\nvideo generation tasks, incorporating multi-modal instructions. This process further refines\nthe model's ability to handle diverse inputs and\ntasks, ensuring seamless integration of multi-modal information. After this two-stage train-ing process, VIMI demonstrates multimodal\nunderstanding capabilities, producing contex-tually rich and personalized videos grounded\nin the provided inputs, as shown in Figure 1.\nCompared to previous visual grounded video\ngeneration methods, VIMI can synthesize con-sistent and temporally coherent videos with\nlarge motion while retaining the semantic con-trol. Lastly, VIMI also achieves state-of-the-art text-to-video generation results on UCF101\nbenchmark.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in video diffusion models\nhave led to significant successes across various\nvideo creation tasks (Singer et al., 2022; Villegas\net al., 2022; Zhang et al., 2023; Chai et al., 2023;\nChen et al., 2023; Ceylan et al., 2023; Geyer et al.,\n2023). These models have demonstrated impres-sive capabilities in generating high-quality videos\nfrom textual prompts (An et al., 2023; Blattmann\net al., 2023b; Ge et al., 2023; Guo et al., 2023b; He\net al., 2023; Ho et al., 2022a,b; Singer et al., 2022;\nWang et al., 2023; Zhou et al., 2023; Blattmann\net al., 2023a). However, the majority of these\nmodels rely solely on text-only encoders for their\ndiffusion-based pretraining. This limitation stems\nfrom the absence of large-scale multimodal prompt\ndatasets, which results in a lack of visual grounding\nduring the pretraining stage. Consequently, current\nmodels struggle to incorporate visual input effec-tively, restricting their versatility and application in\nscenarios that demand multi-modal integration.\nTo effectively incorporate visual input into pre-trained text-to-video models, standalone image\nencoders are often employed to process image\nprompts (Jiang et al., 2023b; Guo et al., 2023a; Ren\net al., 2024; He et al., 2024). The visual embed-dings generated by these encoders are then injected\ninto the diffusion model, enabling it to handle mul-timodal applications. However, this approach ne-cessitates customized model designs, leading to\nfragmented solutions that cannot support various\ntasks in a unified manner. As a result, the mod-els lack the flexibility and generalization needed to\nseamlessly integrate different modalities for diverse\nvideo generation tasks.\nRecently, generative pretrained multimodal lan-guage models have demonstrated robust multi-modal in-context learning capabilities, showcasing\ntheir ability to process and integrate various types\nof input data effectively (Team et al., 2023; Zhu\net al., 2023; Achiam et al., 2023; Liu et al., 2024).\nInspired by this success, we introduce a multi-modal instruction pretraining framework VIMI for\ngrounded video generation. This novel framework\naims to leverage the strengths of multimodal mod-els, enhancing the ability to generate videos that are\ncoherently grounded in both textual and visual in-puts. Specifically, the training of VIMI consists of\ntwo stages: (1) Retrieved Augmented Pretraining;"}, {"title": "2 Preliminary", "content": "We base our work on the diffusion framework pro-posed by Menapace et al. (2024), which adapts\nthe EDM (Karras et al. (2022a)) diffusion frame-work to high resolution video generation. In EDM,\nthe forward diffusion process is characterized\nby a variance-exploding mechanism $p(x_\u03c3|x) \\sim\\n\\mathcal{N}(x, \u03c3^2I)$, where noise \u03c3 is gradually added to\nthe data, causing the variance to increase over time,\nand x\u03c3 represents the data at the current noise\nlevel. The reverse process is modeled by learnable\ndenoiser function denoted as $D_\u03c3$, which is trained\nusing a denoising objective formulated as:\n$L(D_\u03c3) = \u0395_{\u03c3,\u03b1,\u03b5} [\u03bb(\u03c3) ||D_\u03c3(x_\u03c3) -x||^2]$, (1)\nwhere x is a data sample, \u03bb is a weighting function\nfor the loss and e is gaussian noise. Rather than\nlearning $D_\u03c3(x_\u03c3)$ directly, it is parametrized as:\n$D_\u03c3(x_\u03c3) = C_{out}(\u03c3)F_\u03b8(C_{in}(\u03c3)x_\u03c3) + C_{skip}(\u03c3)\u03b1\u03c3$, (2)\nwhere $F_\u03b8$ is a neural network. By appropriately\nchoosing scaling functions Cout, Cskip and Cin (see"}, {"title": "2.2 Multimodal Large Language Models", "content": "Building upon the success of Large Language Mod-els (LLMs), Multimodal Large Language Mod-els (MLLMs) (Liu et al., 2024; Zhu et al., 2023;\nTeam et al., 2023) integrate visual information\nfrom a pretrained vision encoder (Radford et al.,\n2021) with an advanced LLM (Touvron et al., 2023;\nJiang et al., 2023a). This integration is achieved\nby treating visual modalities as sequences of dis-crete tokens. In our work, we utilize MLLMs to\nprocess and interpret multimodal in-context input\ndata s = ($1,$2, ..., Sn), where si can be a sig-nal unit, such as an image. For the image unit si\nin the prompt, a pre-trained CLIP visual encoder\nViT-L/14, is used to provide the visual features\nvi = Visual-Encoder(si). The patch features vi\nbefore the last Transformer layer, combined with\nthe text tokens, are used for MLLM encoding, for-mulated as:\n$C = MLLM({81, 82, ..., Sn}|W(Vi))$, (3)\nwhere W projects vi to connect image features into\nthe word embeddings. This approach allows the\nMLLM to effectively interpret a combination of\ntextual and visual inputs, leveraging the strengths\nof both modalities to enhance the model's multi-modal understanding and generation capabilities."}, {"title": "3 Method", "content": "We aim to generalize the video generation pretrain-ing to the multimodal setting. Figure 2 shows the\noverview of our framework. Sec. 3.1 introduces\nhow we construct a large-scale multimodal input-video dataset by employing retrieval methods to\npair in-context examples with given text prompts.\nSec. 3.2 presents a multimodal conditional video\ngeneration framework for pretraining on these aug-mented datasets, establishing a foundational model\nfor grounded video generation. Sec. 3.3 introduces\nthe instruction finetuning stage on three video gen-eration tasks, incorporating multimodal instruc-tions."}, {"title": "3.1 Retrieval-Augmented Multi-modal Datasets", "content": "Retrieval-based methods collect relevant informa-tion to the input from an external multimodal mem-ory M. In our study, we use web-scale image-text pairs as our multi-modal memory for retrieval\nand build index into a list of key-value pairs, i.e.\nM = {(ki, vi)}. Then, given the input sequence s,\nthe retrieval engine & matches it with all keys and\nreturns the top K most similar keys to the query\ntogether with their values:\n${(ki1, Vi\u2081), ..., (kik, Vik)} = E(s|M)$ (4)\nIn this work, we build the retrieval engine based\non the widely used BM25 score (Sch\u00fctze et al.,\n2008). We choose BM25 over dense representa-tions due to the large scale of the retrieval datastore\nand its faster speed. In our work, we construct"}, {"title": "3.2 Retrieval-Augmented Video Pretraining", "content": "Given the retrieval-augmented multimodal input,\nwe first concatenate the text caption s with the re-trieved multiodal documents to form the new mul-timodal input. Then, we feed this combined in-put into the Multimodal Large Language Models\n(MLLMs) to generate the multimodal conditional\nembedding C:\n$C = MLLM(F({(ki1, Vi\u2081), ..., (kik, Vik)}, 8)$ (5)\nHere, F denotes concatenation and the embedding\nC encapsulates the rich contextual information"}, {"title": "3.3 Multimodal Instruction Tuning", "content": "After the first stage of retrieval-augmented pretrain-ing, VIMI can generate videos from prompts in-volving both text and images, leveraging the multi-modal understanding capabilities of the multimodal\nlanguage model. However, this initial stage primar-"}, {"title": "4 Experiments", "content": "In this section, we evaluate VIMI against baselines\nand ablate the model design components. Sec. 4.1\nintroduces our implementation details. Sec. 4.2\nshows our results in three different evaluation set-tings: (1) general text-to-video generation; (2)\nsubject-driven video generation; and (3) video pre-diction. Sec. 4.3 shows ablations of our framework."}, {"title": "4.1 Implementation Details", "content": "We use an internal licensed dataset of images and\nvideos, each paired with a corresponding text cap-"}, {"title": "4.2 Results", "content": "Zero-shot Text-to-Video Evaluation We gener-ate 10,000 videos (Wang et al., 2023; Blattmann\net al., 2023b) sampling classes with the same dis-tribution as the original UCF-101 dataset. We pro-duce a text prompt for each class label (Ge et al.,\n2023) and compute FVD (Unterthiner et al., 2018)\nand Inception Score (Salimans et al., 2016). Ta-ble 1 shows our competitive performance to previ-ous state-of-the-art text-to-video generators in both\nFVD and IS metrics. We achieve the best FVD\nscore of 193.7 which we attribute to our visual\ngrounding during pretraining."}, {"title": "4.3 Ablation Study", "content": "Effectiveness of retrieval-augmented pretrain-ing Figure 6a shows the evaluations of retrieval-augmented pretraining on our validation set for\nCLIP similarity and FID metrics. We denote VIMI\nwithout retrieval augmented pretraining as VIMI\n(w/o RAG). We use Snap Video (Menapace et al.,\n2024) with text encoders T5-11B as another base-line. The results indicate that using multimodal\nlarge language models as the encoding leads to\nunstable model training. Specifically, the FID re-sults converge slowly and do not decrease after\n125K pretraining steps. In contrast, with retrieval\naugmented pretraining, VIMI shows faster conver-gence and more stable training. After 200K pre-training steps, using a multimodal large language\nmodel as the encoder demonstrates performance"}, {"title": "5 Related Work", "content": "Video Generation Diffusion models are now the\nstandard methodology for both image (Ho et al.,\n2020; Nichol and Dhariwal, 2021; Rombach et al.,\n2022; Song et al., 2020) and video generation (An\net al., 2023; Blattmann et al., 2023b; Ge et al.,\n2023; Guo et al., 2023b; He et al., 2023; Ho et al.,\n2022a,b; Singer et al., 2022; Wang et al., 2023;\nZhou et al., 2023; Blattmann et al., 2023a). Early\nvideo diffusion models use the U-Net (Ronneberger\net al., 2015) for the video generation task. Ho\net al. (2022b) showed that jointly training on image\nand video data can improve text conditioned video\ngeneration greatly. Make-A-Video (Singer et al.,\n2022) proposed to build on text-to-image models\nwith novel and effective spatial-temporal modules.\nVideo LDM (Blattmann et al., 2023b) adopts a\nlatent diffusion paradigm where a pre-trained la-tent image generator and latent decoder are fine-tuned to generate temporally coherent videos. Most\nrecently, diffusion transformer (Peebles and Xie,\n2022) has been widely adopted for video genera-tion. Latte (Ma et al., 2024) proposes a latent diffu-sion transformer, which adopts a video Transformer\nas the backbone. W.A.L.T (Gupta et al., 2023) uses\na transformer-based method for latent video diffu-sion models and a window attention architecture\ntailored for joint spatial and spatiotemporal genera-tive modeling. Snap Video (Menapace et al., 2024)\nreplaced U-Nets with efficient transformer-based\nFITs (Chen and Li, 2023) and scaled to billions of\nparameters. However, these existing works are still\nlimited by the use of text encoders like T5 or the\nCLIP Text encoder, which lack visual grounding in\nthe pretraining phase. In our work, we propose to\nutilize multimodal large language models to encode\nmultimodal inputs for video generation, addressing\nthe limitations by integrating visual grounding into\nthe pretraining process.\nRetrieval Augmented Multimodal Pretraining\nRetrieval augmentation has shown significant\npromise, particularly in language models. Initial\nwork (Lewis et al., 2020; Guu et al., 2020) demon-strated how incorporating external knowledge into\na language model can enhance its performance."}, {"title": "6 Conclusion", "content": "In this work, we first construct a multimodal\nprompt dataset for video pretraining using retrieval\nmethods. We then propose a two-stage training\nstrategy to enable diverse video tasks within the\nsame model. For the first stage, we introduce a mul-timodal conditional video generation framework\nfor pretraining on these augmented datasets, estab-lishing a foundational model for grounded video\ngeneration. In the second stage, we fine-tune the\nmodel from the first stage on three video genera-tion tasks, incorporating multimodal instructions.\nOur experiments demonstrate the effectiveness of\nretrieval-augmented pretraining and the use of mul-timodal instruction tuning. We hope this approach\nopens up new opportunities for video pretraining,\nsuch as building large-scale multimodal datasets\nfor pretraining, utilizing stronger multimodal large\nlanguage models for encoding, and employing in-"}, {"title": "7 Limitations", "content": "Firstly, similar to subject-driven image generation\nmodels, our video generator sometimes struggles\nto produce accurate and faithful videos. To im-prove visual quality, future work will focus on uti-lizing stronger multimodal large language models,\ndiffusion tranformers and jointly fine-tuning these\nmodels. Secondly, due to memory and training con-straints, we only experimented with two context\nexamples and displayed at most two image enti-ties for multi-subject-driven generation. Extending\nthis work to support any-subject video generation\nwill be a goal for future research. Thirdly, our\ncurrent results are based on qualitative evaluation.\nDeveloping comprehensive evaluation methods for\ngrounded video generation, such as any-subject-driven video generation, will be crucial for building\na visually grounded video generator."}, {"title": "8 Ethical Considerations", "content": "Like all generative AI advancements, visually\ngrounded video generation models raise important\nethical considerations, such as the creation of mis-leading or false information and bias. Developers\nand researchers should consider safeguards to ad-dress these issues such as evaluating datasets, and\nadding watermarks or other identification mech-anisms. It is important to consider the societal\nimpacts and work towards solutions that balance\ninnovation with social responsibility."}, {"title": "A Training details", "content": "For pretraining, we can either start from scratch or\ninitialize the weights of the model from existing\ntext-to-video generators. In our work, we initialize\nthe FIT weights from (Menapace et al., 2024).\nWe keep its parameters frozen for 30,000 steps to\nstabilize the initial training phase, and then fine-tune the entire model for an additional 100,000\nsteps. In the second stage, we fine-tune the model\nstarting from the weights obtained in the first stage\nfor 30,000 steps. We use a learning rate of 5e-3, a\ncosine learning schedule, and a total batch size of\n256 videos and 256 images."}, {"title": "B Evaluation Protocol", "content": "We evaluate our method against baselines by fol-lowing the protocols in (Singer et al., 2022; Ge\net al., 2023; Wang et al., 2023; Blattmann et al.,\n2023b; Zhou et al., 2023; Luo et al., 2023) for\nzero-shot evaluation on the UCF-101 (Soomro\net al., 2012). We generate 16 frames videos in\n512 \u00d7 288px resolution at 24fps. To validate the ef-fectiveness of pretraining, ablations are performed\nin 64 \u00d7 36px resolution using the first-stage model\nonly, and compute FID (Heusel et al., 2017), FVD\n(Unterthiner et al., 2018) and CLIPSIM (Wu et al.,\n2021) metrics against the test set of our internal\ndataset on 50k generated videos."}, {"title": "C Inference", "content": "We produce video samples from gaussian noise and\nuser-provided conditioning information using the\ndeterministic sampler of (Karras et al., 2022b) and\ntwo-stage cascade. We use 256 sampling steps for\nthe first-stage and 40 for the second-stage model,\nand employ classifier free guidance (Ho and Sali-mans, 2022) to improve text-video alignment."}]}