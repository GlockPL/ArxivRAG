{"title": "VIMI: Grounding Video Generation through Multi-modal Instruction", "authors": ["Yuwei Fang", "Willi Menapace", "Aliaksandr Siarohin", "Tsai-Shien Chen", "Kuan-Chien Wang", "Ivan Skorokhodov", "Graham Neubig", "Sergey Tulyakov"], "abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for their pretraining. This limitation stems from the absence of large-scale multimodal prompt video datasets, resulting in a lack of visual grounding and restricting their versatility and application in multimodal integration. To address this, we construct a large-scale multimodal prompt dataset by employing retrieval methods to pair in-context examples with the given text prompts and then utilize a two-stage training strategy to enable diverse video generation tasks within the same model. In the first stage, we propose a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. Secondly, we fine-tune the model from the first stage on three video generation tasks, incorporating multimodal instructions. This process further refines the model's ability to handle diverse inputs and tasks, ensuring seamless integration of multimodal information. After this two-stage training process, VIMI demonstrates multimodal understanding capabilities, producing contextually rich and personalized videos grounded in the provided inputs, as shown in Figure 1. Compared to previous visual grounded video generation methods, VIMI can synthesize consistent and temporally coherent videos with large motion while retaining the semantic control. Lastly, VIMI also achieves state-of-the-art text-to-video generation results on UCF101 benchmark.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in video diffusion models have led to significant successes across various video creation tasks (Singer et al., 2022; Villegas et al., 2022; Zhang et al., 2023; Chai et al., 2023; Chen et al., 2023; Ceylan et al., 2023; Geyer et al., 2023). These models have demonstrated impressive capabilities in generating high-quality videos from textual prompts (An et al., 2023; Blattmann et al., 2023b; Ge et al., 2023; Guo et al., 2023b; He et al., 2023; Ho et al., 2022a,b; Singer et al., 2022; Wang et al., 2023; Zhou et al., 2023; Blattmann et al., 2023a). However, the majority of these models rely solely on text-only encoders for their diffusion-based pretraining. This limitation stems from the absence of large-scale multimodal prompt datasets, which results in a lack of visual grounding during the pretraining stage. Consequently, current models struggle to incorporate visual input effectively, restricting their versatility and application in scenarios that demand multi-modal integration.\nTo effectively incorporate visual input into pretrained text-to-video models, standalone image encoders are often employed to process image prompts (Jiang et al., 2023b; Guo et al., 2023a; Ren et al., 2024; He et al., 2024). The visual embeddings generated by these encoders are then injected into the diffusion model, enabling it to handle multimodal applications. However, this approach necessitates customized model designs, leading to fragmented solutions that cannot support various tasks in a unified manner. As a result, the models lack the flexibility and generalization needed to seamlessly integrate different modalities for diverse video generation tasks.\nRecently, generative pretrained multimodal language models have demonstrated robust multimodal in-context learning capabilities, showcasing their ability to process and integrate various types of input data effectively (Team et al., 2023; Zhu et al., 2023; Achiam et al., 2023; Liu et al., 2024). Inspired by this success, we introduce a multimodal instruction pretraining framework VIMI for grounded video generation. This novel framework aims to leverage the strengths of multimodal models, enhancing the ability to generate videos that are coherently grounded in both textual and visual inputs. Specifically, the training of VIMI consists of two stages: (1) Retrieved Augmented Pretraining;"}, {"title": "2 Preliminary", "content": "We base our work on the diffusion framework proposed by Menapace et al. (2024), which adapts the EDM (Karras et al. (2022a)) diffusion framework to high resolution video generation. In EDM, the forward diffusion process is characterized by a variance-exploding mechanism p(xo|x) ~ \u039d(x, \u03c32\u0399), where noise \u03c3 is gradually added to the data, causing the variance to increase over time, and xo represents the data at the current noise level. The reverse process is modeled by learnable denoiser function denoted as Do, which is trained using a denoising objective formulated as:\nL(Do) = \u0395\u03c3,\u03b1,\u03b5 [\u03bb(\u03c3) ||Do(xo) -x||2], (1)\nwhere x is a data sample, \u03bb is a weighting function for the loss and e is gaussian noise. Rather than learning Do(x) directly, it is parametrized as:\nDo(x) = Cout(\u03c3)Fe (Cin(\u03c3)xo) + Cskip(\u03c3)\u03b1\u03c3, (2)\nwhere Fe is a neural network. By appropriately choosing scaling functions Cout, Cskip and Cin (see"}, {"title": "2.2 Multimodal Large Language Models", "content": "Building upon the success of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) (Liu et al., 2024; Zhu et al., 2023; Team et al., 2023) integrate visual information from a pretrained vision encoder (Radford et al., 2021) with an advanced LLM (Touvron et al., 2023; Jiang et al., 2023a). This integration is achieved by treating visual modalities as sequences of discrete tokens. In our work, we utilize MLLMs to process and interpret multimodal in-context input data s = (s1,s2, ..., sn), where si can be a signal unit, such as an image. For the image unit si in the prompt, a pre-trained CLIP visual encoder ViT-L/14, is used to provide the visual features Vi = Visual-Encoder(si). The patch features vi before the last Transformer layer, combined with the text tokens, are used for MLLM encoding, formulated as:\nC = MLLM({s1, s2, ..., sn}|W(vi)), (3)\nwhere W projects vi to connect image features into the word embeddings. This approach allows the MLLM to effectively interpret a combination of textual and visual inputs, leveraging the strengths of both modalities to enhance the model's multimodal understanding and generation capabilities."}, {"title": "3 Method", "content": "We aim to generalize the video generation pretraining to the multimodal setting. Figure 2 shows the overview of our framework. Sec. 3.1 introduces how we construct a large-scale multimodal input-video dataset by employing retrieval methods to pair in-context examples with given text prompts. Sec. 3.2 presents a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. Sec. 3.3 introduces the instruction finetuning stage on three video generation tasks, incorporating multimodal instructions."}, {"title": "3.1 Retrieval-Augmented Multi-modal Datasets", "content": "Retrieval-based methods collect relevant information to the input from an external multimodal memory M. In our study, we use web-scale image-text pairs as our multi-modal memory for retrieval and build index into a list of key-value pairs, i.e. M = {(ki, vi)}. Then, given the input sequence s, the retrieval engine & matches it with all keys and returns the top K most similar keys to the query together with their values:\n{(ki1, Vi\u2081), ..., (kik, Vik)} = E(s|M) (4)\nIn this work, we build the retrieval engine based on the widely used BM25 score (Sch\u00fctze et al., 2008). We choose BM25 over dense representations due to the large scale of the retrieval datastore and its faster speed. In our work, we construct"}, {"title": "3.2 Retrieval-Augmented Video Pretraining", "content": "Given the retrieval-augmented multimodal input, we first concatenate the text caption s with the retrieved multiodal documents to form the new multimodal input. Then, we feed this combined input into the Multimodal Large Language Models (MLLMs) to generate the multimodal conditional embedding C:\nC = MLLM(F({(ki1, Vi\u2081), ..., (kik, Vik)}, s) (5)\nHere, F denotes concatenation and the embedding C encapsulates the rich contextual information from both the text and the retrieved multimodal data.\nFollowing (Menapace et al., 2024), we use FITs (Chen and Li, 2023) as the backbone to jointly model the spatial and temporal dimensions for high-quality video generation. However, here we only use the multimodal conditioning embedding C to control the generation process rather than the text embeddings from T5 text encoder. We concatenate additional tokens representing the diffusion timestep, framerate and original resolution of the current input, to support variable video framerates and large differences in resolution and aspect ratios in the training data. To generate high-resolution outputs, we pretrain a cascade model consisting of a first-stage model producing 36 \u00d7 64px videos and a second-stage upsampling model producing 288 \u00d7 512px videos."}, {"title": "3.3 Multimodal Instruction Tuning", "content": "After the first stage of retrieval-augmented pretraining, VIMI can generate videos from prompts involving both text and images, leveraging the multimodal understanding capabilities of the multimodal language model. However, this initial stage primar-"}, {"title": "4 Experiments", "content": "In this section, we evaluate VIMI against baselines and ablate the model design components. Sec. 4.1 introduces our implementation details. Sec. 4.2 shows our results in three different evaluation settings: (1) general text-to-video generation; (2) subject-driven video generation; and (3) video prediction. Sec. 4.3 shows ablations of our framework."}, {"title": "4.1 Implementation Details", "content": "We use an internal licensed dataset of images and videos, each paired with a corresponding text cap-"}, {"title": "4.2 Results", "content": "We generate 10,000 videos (Wang et al., 2023; Blattmann et al., 2023b) sampling classes with the same distribution as the original UCF-101 dataset. We produce a text prompt for each class label (Ge et al., 2023) and compute FVD (Unterthiner et al., 2018) and Inception Score (Salimans et al., 2016). Table 1 shows our competitive performance to previous state-of-the-art text-to-video generators in both FVD and IS metrics. We achieve the best FVD score of 193.7 which we attribute to our visual grounding during pretraining."}, {"title": "4.3 Ablation Study", "content": "Figure 6a shows the evaluations of retrieval-augmented pretraining on our validation set for CLIP similarity and FID metrics. We denote VIMI without retrieval augmented pretraining as VIMI (w/o RAG). We use Snap Video (Menapace et al., 2024) with text encoders T5-11B as another baseline. The results indicate that using multimodal large language models as the encoding leads to unstable model training. Specifically, the FID results converge slowly and do not decrease after 125K pretraining steps. In contrast, with retrieval augmented pretraining, VIMI shows faster convergence and more stable training. After 200K pretraining steps, using a multimodal large language model as the encoder demonstrates performance"}, {"title": "5 Related Work", "content": "Diffusion models are now the standard methodology for both image (Ho et al., 2020; Nichol and Dhariwal, 2021; Rombach et al., 2022; Song et al., 2020) and video generation (An et al., 2023; Blattmann et al., 2023b; Ge et al., 2023; Guo et al., 2023b; He et al., 2023; Ho et al., 2022a,b; Singer et al., 2022; Wang et al., 2023; Zhou et al., 2023; Blattmann et al., 2023a). Early video diffusion models use the U-Net (Ronneberger et al., 2015) for the video generation task. Ho et al. (2022b) showed that jointly training on image and video data can improve text conditioned video generation greatly. Make-A-Video (Singer et al., 2022) proposed to build on text-to-image models with novel and effective spatial-temporal modules.\nVideo LDM (Blattmann et al., 2023b) adopts a latent diffusion paradigm where a pre-trained latent image generator and latent decoder are fine-tuned to generate temporally coherent videos. Most recently, diffusion transformer (Peebles and Xie, 2022) has been widely adopted for video generation. Latte (Ma et al., 2024) proposes a latent diffusion transformer, which adopts a video Transformer as the backbone. W.A.L.T (Gupta et al., 2023) uses a transformer-based method for latent video diffusion models and a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Snap Video (Menapace et al., 2024) replaced U-Nets with efficient transformer-based FITs (Chen and Li, 2023) and scaled to billions of parameters. However, these existing works are still limited by the use of text encoders like T5 or the CLIP Text encoder, which lack visual grounding in the pretraining phase. In our work, we propose to utilize multimodal large language models to encode multimodal inputs for video generation, addressing the limitations by integrating visual grounding into the pretraining process.\nRetrieval Augmented Multimodal Pretraining\nRetrieval augmentation has shown significant promise, particularly in language models. Initial work (Lewis et al., 2020; Guu et al., 2020) demonstrated how incorporating external knowledge into a language model can enhance its performance."}, {"title": "6 Conclusion", "content": "In this work, we first construct a multimodal prompt dataset for video pretraining using retrieval methods. We then propose a two-stage training strategy to enable diverse video tasks within the same model. For the first stage, we introduce a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. In the second stage, we fine-tune the model from the first stage on three video generation tasks, incorporating multimodal instructions. Our experiments demonstrate the effectiveness of retrieval-augmented pretraining and the use of multimodal instruction tuning. We hope this approach opens up new opportunities for video pretraining, such as building large-scale multimodal datasets for pretraining, utilizing stronger multimodal large language models for encoding, and employing in-"}, {"title": "7 Limitations", "content": "Firstly, similar to subject-driven image generation models, our video generator sometimes struggles to produce accurate and faithful videos. To improve visual quality, future work will focus on utilizing stronger multimodal large language models, diffusion tranformers and jointly fine-tuning these models. Secondly, due to memory and training constraints, we only experimented with two context examples and displayed at most two image entities for multi-subject-driven generation. Extending this work to support any-subject video generation will be a goal for future research. Thirdly, our current results are based on qualitative evaluation. Developing comprehensive evaluation methods for grounded video generation, such as any-subject-driven video generation, will be crucial for building a visually grounded video generator."}, {"title": "8 Ethical Considerations", "content": "Like all generative AI advancements, visually grounded video generation models raise important ethical considerations, such as the creation of misleading or false information and bias. Developers and researchers should consider safeguards to address these issues such as evaluating datasets, and adding watermarks or other identification mechanisms. It is important to consider the societal impacts and work towards solutions that balance innovation with social responsibility."}, {"title": "A Training details", "content": "For pretraining, we can either start from scratch or initialize the weights of the model from existing text-to-video generators. In our work, we initialize the FIT weights from (Menapace et al., 2024). We keep its parameters frozen for 30,000 steps to stabilize the initial training phase, and then fine-tune the entire model for an additional 100,000 steps. In the second stage, we fine-tune the model starting from the weights obtained in the first stage for 30,000 steps. We use a learning rate of 5e-3, a cosine learning schedule, and a total batch size of 256 videos and 256 images."}, {"title": "B Evaluation Protocol", "content": "We evaluate our method against baselines by following the protocols in (Singer et al., 2022; Ge et al., 2023; Wang et al., 2023; Blattmann et al., 2023b; Zhou et al., 2023; Luo et al., 2023) for zero-shot evaluation on the UCF-101 (Soomro et al., 2012). We generate 16 frames videos in 512 \u00d7 288px resolution at 24fps. To validate the effectiveness of pretraining, ablations are performed in 64 \u00d7 36px resolution using the first-stage model only, and compute FID (Heusel et al., 2017), FVD (Unterthiner et al., 2018) and CLIPSIM (Wu et al., 2021) metrics against the test set of our internal dataset on 50k generated videos."}, {"title": "C Inference", "content": "We produce video samples from gaussian noise and user-provided conditioning information using the deterministic sampler of (Karras et al., 2022b) and two-stage cascade. We use 256 sampling steps for the first-stage and 40 for the second-stage model, and employ classifier free guidance (Ho and Salimans, 2022) to improve text-video alignment."}]}