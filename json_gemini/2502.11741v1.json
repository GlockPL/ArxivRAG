{"title": "SQL-01: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL", "authors": ["Shuai Lyu", "Haoran Luo", "Zhonghong Ou", "Yifan Zhu", "Xiaoran Shang", "Yang Qin", "Meina Song"], "abstract": "The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-01, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-01 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-01 improves execution accuracy by 10.8% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-01 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available.", "sections": [{"title": "1 Introduction", "content": "Text2SQL refers to the process of converting natural language questions into Structured Query Language (SQL), serving as an effective method for non-expert users to interact with databases using natural language. The development of this field can be categorized into three stages: first, encoding and decoding input sequences using pretrained models or abstract syntax trees (Wang et al., 2020); second, employing sequence-to-sequence methods (Xie et al., 2022); and more recently, large language models (LLMs) (Zhang et al., 2025) have been proven to be an effective solution for Text2SQL. However, accurately aligning natural language queries with the data in the database remains a significant challenge.\nRecent research on LLM-based Text2SQL has mainly focused on improving model performance through contextual learning prompt strategies and supervised fine-tuning with domain-specific data. The crucial methods in this field include Schema Linking, Self-correction, and Chain-of-Thought (CoT) (Tai et al., 2023), which aim to enhance the model's understanding of schemas, improve its reasoning ability, and assist it in generating more accurate SQL queries.\nHowever, there are still three main challenges: 1. These methods are often limited by the scale of the model. Smaller LLMs have limited ability to understand complex instructions, leading to poor generalization when handling complex tasks. 2. End-to-end generation methods suffer from the limitation of the generation space. Due to the lack of opportunities for gradual verification and flexible adjustment, the model finds it difficult to explore more potential paths during the generation process, limiting the diversity and accuracy of the output. 3. There are coherence issues in reasoning process during SQL generation. If an error occurs at any step, it often affects the correctness of subsequent steps, resulting in the final generated SQL query being unable to execute correctly.\nInspired by work on Process-supervised Reward Model (Luo et al., 2023), we propose SQL-01, a Self-Reward-based heuristic search method, as shown in Figure 1. First, we extensively mine the database schema, collecting table column fields, representative entities, and other information to construct a Schema-Aware dataset for fine-tuning large language models (LLMs). In addition, we introduce Monte Carlo Tree Search (MCTS) (\u015awiechowski et al., 2023) as an inference medium, leveraging process-level reasoning with Self-Reward to reduce logical errors in the LLM's generation process. By expanding the generation space while overcoming the consistency challenge in SQL generation, we significantly enhance the reasoning capabilities of LLMs.\nWe conduct experiments on the Bird and Spider datasets, as well as three variants of Spider. The experimental results show that SQL-01, when combined with common open-source models such as Llama 3 (Touvron et al., 2023) and Qwen 2.5 (Yang et al., 2024a), significantly outperforms most existing methods, even surpassing other GPT-4-based approaches. Moreover, we apply SQL-01 in the few-shot fine-tuning scenario, and the results indicate that when the sample size reaches 2000, nearly all performance metrics exceed those of models fine-tuned on the full dataset. Finally, we also discuss the transferability of SQL-01 and the contributions of its components. Our contributions can be summarized as follows:\n1.  We extract information from multiple perspectives of the database to build a Schema-Aware dataset, facilitating Progressive SQL Generation (PSG) for LLMs.\n2.  We propose SQL-01, a Self-Reward-based heuristic search method that significantly reduces coherence issues in the SQL generation process while expanding the SQL generation space.\n3.  We conduct a comprehensive evaluation and extensive experiments to fully validate the effectiveness and transferability of SQL-01."}, {"title": "2 Related Work", "content": "Recent significant progress in the Text2SQL task has primarily focused on LLMs, as their remarkable reasoning ability provides new directions and opportunities for the Text2SQL task. Currently, methods based on LLMs can generally be divided into two categories: Prompt Engineering and Agent-based interaction with LLMs.\nPrompt Engineering. In the early stages of LLMs, a direct and effective method to better exploit the potential of LLMs was to carefully design effective prompts to guide the models, which also applies to the Text2SQL task. Enhancing the reasoning ability of LLMs through Chain of Thought (Zhang et al., 2023) is a promising attempt. Several methods (Wang et al., 2024; Pourreza and Rafiei, 2023; Li et al., 2024a) utilize schema linking to combine natural language questions with database schema elements, achieving promising results. Among them, DAIL-SQL (Gao et al., 2024) systematically investigates prompt engineering for LLM-based text-to-SQL methods, including question representation, prompt components, example selection, and example organization.\nRecently, some works have shifted attention from prompt engineering (e.g., GPT-4 and other closed-source models) to fine-tuning LLMs. SENSE (Yang et al., 2024b) synthesizes strong data and performs Direct Preference Optimization (DPO) on weak data from a weak LLM, while ROUTE (Qin et al., 2024) proposes a multitask collaborative fine-tuning approach, reducing potential errors in SQL generation and achieving better results.\nAgent-based interaction with LLMs. The agent-based interactive methods (Chen et al., 2024b) guide LLMs to generate accurate SQL queries by designing feedback signals. Early works (Shi et al., 2022) focused on improving SQL based on execution results, by executing SQL queries and selecting the most accurate translation based on execution risks. Other works (Chen et al., 2024a; Guo et al., 2023) use LLMs to inspect results and correct discrepancies between the generated SQL and real SQL queries. MAC-SQL (Wang et al., 2024) introduces a multi-agent framework and other novel interactive methods (Xiong et al., 2024). However, most of these methods rely heavily on high-quality external feedback, which is often unavailable in practical applications, and they primarily depend on closed-source LLMs, overlooking the potential of open-source LLMs in reasoning."}, {"title": "3 Preliminaries", "content": "3.1 Problem Formulation\nGiven a Text2SQL dataset D = {(Di, Qi, Si)}=1, where each sample consists of an SQL database Di, a natural language question Qi, and the corresponding ground-truth SQL query Si, the goal of the Text2SQL task is to use a large language model to generate an SQL query Qi and ensure that the execution results match Si.\n3.2 Definition: Heuristic Dynamic Search with Self-Reward\nHeuristic Dynamic Search with Self-Reward primarily consists of a sequence of states O = {o0, o1, o2, ..., Ot\u22121}, and an action sequence A = {a1, a2,..., at} generated based on these states. Each time an action at is executed, the model will receive a corresponding reward Rt \u2208 R. Both rewards and actions are generated by the model \u03c0."}, {"title": "4 Methodology", "content": "In this section, we will introduce the three components of SQL-01: Schema-Aware Data Construction, Progressive SQL Generation, and Heuristic Dynamic Search with Self-Reward.\n4.1 Schema-Aware Data Construction\nSQL-01 needs to accurately understand the database structure and query conditions before performing heuristic dynamic search. Therefore, we design strategies such as extracting table field types and sample data entries to help the model better grasp the database schema, thereby optimizing the heuristic search process.\nTable Column Field Types. The data type of a column determines the values that can be stored in the field and how those values are processed. WSpec-"}, {"title": "4.2 Progressive SQL Generation", "content": "Progressive SQL Generation (PSG) is a variant of supervised fine-tuning (SFT), with the core idea of truncating a complete SQL query at specific keywords during training, where the model's task is to reconstruct the full query based on the prompt. We mainly focus on SQL queries in pre-trained large language models that either have prediction errors or complex syntactic structures. For instance, in the query 'SELECT name, age FROM employees WHERE Department = 'HR' AND salary > 50000', truncation occurs at keywords such as 'WHERE' or 'AND', rather than at arbitrary positions. If truncation happens at 'SELECT name, age FROM employees WHERE', the model is required to generate the full query from this fragment.\nThis incremental generation method leverages the continuous generation capability of LLMs, helping the model better understand query structure and syntax, reducing generation errors, especially when dealing with multiple joins or complex filtering conditions.\nBased on the previously discussed content, we have developed a basic fine-tuning dataset for LLMs, which mainly includes the content from Sections 4.1 and 4.2. We represent the constructed dataset as:\nDs = {op(Di, Qi), Si}1,\nwhere op denotes the prompt construction function we define, and Ns represents the total number of samples in the dataset."}, {"title": "4.3 Heuristic Dynamic Search with Self-Reward", "content": "The method proposed in this section integrates a reinforcement learning framework, Monte Carlo Tree Search, and Self-Reward evaluation to guide the model's decision-making process during SQL query generation. Based on the components of the algorithm, the method is primarily divided into: SQL Generation Planning, Self-Reward Evaluation, and Heuristic Dynamic Search.\n4.3.1 SQL Generation Planning\nWe define the SQL query generation task as a sequential decision-making task, where the model's objective is to choose the next SQL fragment (such as a table name, column name, or SQL keyword) based on the current context. This is treated as a policy generation problem, with the objective of teaching the model a strategy that maximizes the likelihood of generating correct SQL queries:\nat = \\underset{a'_t}{\\text{argmax }} \\pi(a'_t | O_{t-1}).\\tag{1}\nEquation (1) describes how the policy model \u03c0 selects the optimal action at (i.e., the SQL fragment generated at step t) based on the previous state Ot-1 (i.e., the SQL fragment generated at the previous step). Specifically, the model selects one of the possible SQL fragments $a'_t$ that maximizes the probability $\u03c0(\u03b1'_t | O_{t\u22121})$.\n4.3.2 Self-Reward Evaluation\nThe objective of this task is to evaluate the quality and validity of the generated SQL query fragments"}, {"title": "4.3.3 Heuristic Dynamic Search", "content": "Monte Carlo Tree Search (MCTS) is a powerful decision-making algorithm widely used in game theory (e.g., AlphaGo) and planning problems. As shown in Figure 2, we use MCTS as a heuristic search method to guide SQL query generation. It explores and generates SQL query sequences step by step, simulates the outcomes, and optimizes the search path based on Self-Reward guidance.\nSelection. The selection phase of MCTS begins at the root node and traverses through child nodes until reaching the leaf node. Each node represents a decision point in the SQL query generation process, where the model selects the next valid SQL token based on the Equation (1), gradually generating the query. At crucial syntactic and semantic decision points, the model uses heuristic truncation to expand partial queries. The UCT algorithm is then applied to guide node selection, balancing the exploration of unvisited query structures with the exploitation of high-reward paths:\n\\underset{n \\in N(o_{t-1})}{\\text{argmax}}\\left[Q(o_{t-1}+n)+w \\cdot \\sqrt{\\frac{\\ln N(o_{t-1})}{N(o_{t-1}+n)}}\\right]\\tag{3}\nwhere $N(.)$ represents the candidate expansion paths for a given state ot\u22121, Q(.) denotes the Q-value of the current state, reflecting the expected return for executing an action. N(.) indicates the visit count of agent states.\nExpansion. The selection process chooses the most relevant SQL query from the candidate extensions. When the maximum query depth L has not been reached, the model continues to expand the query by exploring the next possible SQL operation or clause:\nN(O_{t-1}) = \\bigcup_{n=1}^{(b) B} {\\overset{\\sim}{\\pi}(n_t|O_{t-1})_{Beam}},\n\\tag{4}\nwhere, {\\overset{\\sim}{\\pi}}(.)_{Beam} represents the Beam Search algorithm, and B is the beam width. Then, the model selects the most relevant SQL operation for expansion based on the semantic similarity with the previous query fragments:\nN(O_{t-1}) = \\bigcup_{i=1}^{(i) d} {\\underset{B_{Nt}}{\\text{argmax}}R_{\\alpha} \\left(\\left[n_t\\right]_{i=1}^{B} \\mid O_{t-1}\\right)}, \\tag{5}\nwhere $R_{\u03b1}$ represents the reward function that evaluates the quality of each candidate expansion and $d < B$. For example, if the current state is a partial query on the \"user\" table, the model might generate \"SELECT user.id\" or \"SELECT user.name\" and select candidates based on their semantic relevance to the input question.\nSimulation and Back-propagation. After expanding the nodes, the model assigns scores to all newly added child nodes, as indicated by Equation (2) and (6). According to Equation (7), the node with the highest score is selected for further simulation, continuing until the final state is reached, thus generating the complete SQL query generation trajectory.\nQ(\u03bf(\u03c0)) = \u03b4\u03c0 (\u03c0l | \u03bf\u2081) + (1 \u2212 \u03b4)R(S | Q), \\tag{6}\nwere d is a parameter between (0, 1), used to balance the process score and the overall score, and is often set to 0.5. The algorithm then performs backpropagation by updating the Q-values of all nodes along the trajectory from the leaf node to the root node.\nQ^{\\prime}(o_{n}^{(n)}) = \\underset{j=1}{\\overset{n}{\\text{max}}}\\left(\\frac{Q(o_{n}^{(j)})}{i+1}\\right) \\tag{7}\n'The parent node's Q-value is updated to the maximum average Q-value of its child nodes, and the visit count of each node along the path is incremented by 1 before the next simulation.\n4.3.4 Query Trajectory Optimization\nDuring generation, the model uses MCTS to perform N simulations, selecting the state with the highest Q-value in each simulation as the optimal trajectory. This method progressively optimizes query generation, yielding the optimal SQL query for question Q.\nS \u2190 \u00a7 = MCTS(Q, \u03c0), \\tag{8}\nwhere St is the optimal SQL generated for the query 2."}, {"title": "5 Experiments", "content": "5.1 Experimental Settings\nDatasets. All experiments are conducted on the Spider (Yu et al., 2018) and Bird (Li et al., 2023) datasets. The Spider dataset consists of 7,000 training Text-SQL pairs, 1,034 development pairs, and 2,147 test pairs, covering nearly 200 databases and 138 domains. Bird is a recently introduced benchmark that contains 9,428 training pairs, 1,534 development pairs, and 1,789 test pairs in total. Compared to Spider, Bird contains more complex databases, more challenging questions, and incorporates external knowledge, making it more difficult than Spider.\nBaselines. Similar to previous work (Qin et al., 2024; Yang et al., 2024b), our baseline methods can be divided into three categories: Prompting with Closed-Source LLMs, Prompting with Open-Source LLMs, and Fine-Tuning with Open-Source LLMs. The Prompting with Closed-Source LLMs mainly include DIN-SQL + GPT-4 (Pourreza and Rafiei, 2023), MAC-SQL + GPT-4 (Wang et al., 2024), DAIL-SQL + GPT-4 (Gao et al., 2023b), and MCS-SQL + GPT-4 (Lee et al., 2025). Fine-Tuning with Open-Source LLMs mainly include MAC-SQL and ROUTE-MCP (Qin et al., 2024). Finally, the fine-tuning methods using open-source LLMs involve models fine-tuned on the Spider"}, {"title": "5.2 Main Result", "content": "Results on Bird. As shown in Table 1, prompt-based open-source model methods achieve better results compared to pre-trained models. However, due to limitations in model size and training data, they still lag behind closed-source prompt-based models in performance. Among the finetuning approaches using open-source models, our method, when combined with open-source models like Llama3-8B and Qwen2.5-7B, shows a marked advantage. Specifically, SQL-01(Qwen2.5-7B) achieves a score of 66.7% on the Dev-EX metric, outperforming ROUTE + Qwen2.5-7B (55.9%). Furthermore, on the Dev-VES metric, SQL-01 even surpasses the performance of methods utilizing GPT-4, demonstrating strong performance in handling complex and challenging tasks.\nResults on Spider and its variants. In Table 1 and Table 2, SQL-01 performs better than ROUTE on average when using open-source models of the same size. It is worth noting that the performance on Spider is not as outstanding as on Bird, which can be attributed to the fact that our method relies on MCTS for heuristic search. MCTS is more effective for handling complex queries, which further supports the hypothesis that MCTS excels in dealing with intricate and computationally demanding tasks. In Table 2, even without the use of additional training data, SQL-01 still outperforms ROUTE and other baselines by approximately 1% to 5% on the Spider-derived dataset, which reveals its robustness."}, {"title": "5.3 Ablation Study", "content": "As shown in Table 3, the ablation study on Llama-3-8B-based SQL-01 investigates the impact of various components on the model's performance. The results indicate that each module plays a crucial role in the overall performance of the model. Specifically, removing the MCTS optimization leads to a significant drop in performance, particularly on the Bird-Dev dataset, where the score decreases substantially. Similarly, excluding PSG results in a slight decrease in performance, especially on the Spider-Dev EX and TS sets. The most significant performance decline occurs when the initial annotated supervised fine-tuning (SFT) is removed, with the scores dropping notably across both the Spider-Dev and Bird-Dev datasets."}, {"title": "5.4 Analysis of Transferability", "content": "We conduct portability experiments on several open-source models, such as CodeLlama and Deepseek-Coder. The experimental results are shown in Figure 3. Without the heuristic MCTS exploration, both CodeLlama and Deepseek-Coder suffer a performance loss, leading to suboptimal results. While the results of other experiments meet expectations, the curves in the figure reveal that with the help of MCTS, the performance improvement of CodeLlama and Deepseek-Coder on the Bird-Dev dataset is higher than that on the Spider-Dev dataset. This suggests that Heuristic Dynamic Search with Self-Reward is more effective in handling complex tasks or incomplete data scenarios."}, {"title": "5.5 Model Performance vs. Sample Size", "content": "we compare the performance of the SQL-01 model with the fully fine-tuned Llama-3-8B model on the Spider and Bird datasets, specifically on the Spider-Dev-EX, Spider-Dev-TS, and Bird-Dev-EX subsets. As shown in Figure 4, SQL-01 outperforms Llama-3-8B when the sample size is small (e.g., 2000 to 5000 samples), and its advantage becomes more pronounced on the Spider-Dev-EX and Bird-Dev datasets as the sample size increases. It is noteworthy that SQL-01 is particularly effective in few-shot learning tasks, where it makes better use of limited data. This highlights SQL-01's superiority in resource-constrained scenarios, where it can significantly enhance the model's reasoning capabilities, particularly in real-world applications with scarce or limited data."}, {"title": "6 Conclusion", "content": "We propose SQL-01, a Self-Reward-based heuristic search method that enhances the reasoning ability of large language models (LLMs) in Text-to-SQL tasks. By combining Monte Carlo Tree Search (MCTS) and Schema-Aware datasets, SQL-01 improves SQL generation accuracy. Experimental results show a 10.8% improvement in execution accuracy on the Bird dataset, outperforming GPT4-based methods. SQL-01 also excels in few-shot learning and cross-model transfer. Ablation studies validate the effectiveness of each component of SQL-01. In the future, we plan to explore reinforcement learning techniques for continual learning (DPO) and multi-agent systems."}, {"title": "Limitations", "content": "We analyze the reasoning complexity and processing efficiency of SQL-01 in Appendix. These analyses reveal that SQL-01 still has significant room for improvement."}, {"title": "Ethics Statement", "content": "This paper investigates Text-to-SQL using large language models and heuristic search methods. We use publicly available datasets and ensure that our research adheres to ethical guidelines. No sensitive or private data is involved, and our methods focus solely on improving SQL generation performance. Therefore, we believe our approach does not violate any ethical standards."}]}