{"title": "STOCHASTIC MONKEYS AT PLAY: RANDOM AUGMENTATIONS CHEAPLY BREAK LLM SAFETY ALIGNMENT", "authors": ["Jason Vega", "Junsheng Huang", "Gaokai Zhang", "Hangoo Kang", "Minjia Zhang", "Gagandeep Singh"], "abstract": "Safety alignment of Large Language Models (LLMs) has recently become a critical objective of model developers. In response, a growing body of work has been investigating how safety alignment can be bypassed through various jailbreaking methods, such as adversarial attacks. However, these jailbreak methods can be rather costly or involve a non-trivial amount of creativity and effort, introducing the assumption that malicious users are high-resource or sophisticated. In this paper, we study how simple random augmentations to the input prompt affect safety alignment effectiveness in state-of-the-art LLMs, such as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different models and investigate the intersection of safety under random augmentations with multiple dimensions: augmentation type, model size, quantization, fine-tuning-based defenses, and decoding strategies (e.g., sampling temperature). We show that low-resource and unsophisticated attackers, i.e. stochastic monkeys, can significantly improve their chances of bypassing alignment with just 25 random augmentations per prompt.", "sections": [{"title": "INTRODUCTION", "content": "Autoregressive Large Language Models (LLMs) have become increasingly ubiquitous in recent years. A primary driving force behind the explosion in popularity of LLMs has been their application to conversational AI; e.g., chatbots that can engage in turn-by-turn conversation with humans (OpenAI, 2022). However, as the capabilities of LLMs have increased over the years, so have concerns about their potential for misuse by malicious users. In response to these concerns, tremendous efforts have been invested towards aligning LLMs (Ouyang et al., 2022; Rafailov et al., 2024; Ethayarajh et al., 2024). In order to safety-align a model, an extensive amount of manually-labeled preference data may be required to achieve a high quality alignment. Given the extensive investments required to align a model, it is critical for model developers to ensure that the alignment can withstand a broad range of real-world behavior from malicious users.\nUnfortunately, it has been shown that safety alignment can be bypassed through a variety of techniques. One popular set of techniques is jailbreaks, where a malicious user modifies a harmful prompt in such a way that the aligned model complies with the request. These jailbreaks can either be manually crafted through clever prompt engineering (Liu et al., 2023), or automatically discovered using optimization-based adversarial attacks (Zou et al., 2023). In the former case, a nontrivial amount of creativity and effort may be required to create effective jailbreaks. In the latter case, only"}, {"title": "EVALUATION DIMENSIONS AND METRIC", "content": "In this section, we introduce various notation and terminology used in our paper, as well as the primary aspects of our experiment pipeline."}, {"title": "PRELIMINARIES", "content": "In this section, we introduce various notation and terminology used in our paper, as well as the primary aspects of our experiment pipeline.\nSequences and models. Let $V = \\{1, 2, ..., m\\}$ represent a vocabulary of $m$ token, and let $\\Sigma$ denote the set of printable ASCII characters. Let $\\Sigma^+$ denote the set of positive-length sequences. An autoregressive LLM $f$ operates as follows: given an initial character sequence from $\\Sigma^+$, $f$ outputs a probability distribution over $V$ to predict the next token (for simplicity, we view the tokenizer associated with $f$ as a part of $f$).\nGeneration. Model $f$ may be used as part of a broader pipeline where the input and output character sequences can be restricted to spaces $X \\subseteq \\Sigma^+$ and $Y \\subseteq \\Sigma^+$, respectively (e.g., with prompt templates, limits on sequence length, etc.). For simplicity, we define a generation algorithm $g$ to be this entire pipeline, which given $x \\in X$, uses $f$ to generate $y \\in Y$ following some decoding strategy. For generality, we assume $g$ to be stochastic, with deterministic algorithms being a special case.\nAugmentations. An augmentation $a: X \\rightarrow X$ is a function that modifies $x$ before being passed to $g$. Note that \"no augmentation\" can be considered a special case where the \u201caugmentation\u201d is the identity function $a(x) = x$. Let an augmentation set $A$ be a set of augmentations that may be related in nature (e.g., appending a suffix of a specific length); we refer to the nature of the relation as the augmentation \"type\". Augmentations may be randomly sampled, so we also associate a sampling distribution $P_{aug}(\\cdot ; A)$ with each $A$. We let $A_1$ denote the \u201cno augmentation\u201d singleton containing the identity function that is drawn with probability 1 from $P_{aug}(\\cdot; A_1)$.\nSafety dataset. For safety evaluation, we set $P_{test}$ to be an underlying distribution of inputs from $X$ that contain harmful user requests. We assume that a finite set $D$ of i.i.d. samples from $P_{test}$ is available. As what is deemed \u201charmful\u201d is subjective and may change over time, we make no further assumptions about $P_{test}$ and simply assume that $D$ is representative of the desired $P_{test}$.\nSafety judge. A safety judge $c: X, Y \\rightarrow \\{0,1\\}$ outputs 1 if $y$ is deemed compliant with a user"}, {"title": "RESEARCH QUESTIONS", "content": "Our experiment pipeline has three main components that can be varied: the augmentation type, the model, and the generation algorithm. We will investigate how each of these components impact safety while isolating the other components, and therefore naturally split our research question into the following sub-questions:\nRQ1. For a given model and generation algorithm, how do different augmentation types impact safety? There are many ways to randomly augment a prompt such that its semantic meaning is preserved (or at least highly inferable). However, there may be significant differences in how effectively they enable malicious users to bypass safety alignment. Hence, we examine how a variety of random augmentations can improve attack success over the baseline of not using any augmentations.\nRQ2. For a given augmentation type and generation algorithm, how do different model aspects impact safety; specifically: model size, quantization and fine-tuning-based defense? Model developers commonly release models of multiple sizes within a model family, permitting accessibility to a broader range of hardware. Alternatively, extensive efforts have been made recently to quantize LLMs for similar reasons. Orthogonal to the goal of accessibility is how to make models safer against jailbreaks, for which some recent works have proposed fine-tuning-based defense methods. Hence, it is of practical interest to examine how the safety under random augmentations interacts with each of these aspects.\nRQ3. For a given model, how much do random augmentations impact safety when different decoding strategies are used? By default, all our experiments are conducted using greedy decoding, so the no augmentation baseline in RQ1 only produces a single output per prompt. A critical question therefore is whether random augmentations provide any additional influence on success rates when $k$ random outputs are also sampled in the no augmentation case. Hence, we examine decoding strategies beyond greedy decoding."}, {"title": "EVALUATION METRIC", "content": "In realistic settings, a malicious user who seeks to elicit specific harmful content from an LLM may make multiple attempts before moving on. We therefore assume that for each harmful prompt $x \\in X$, a malicious user makes $k$ attempts where for each attempt a separate augmentation is first applied to the prompt, as illustrated in Figure 1. To evaluate success, we check whether the proportion of augmentations that produce outputs where safety judge $c$ evaluates to 1 is strictly greater than some threshold $\\gamma \\in [0,1)$. We refer to such an occurrence as a $(k, \\gamma)$-success and define the following function for it:\n$S_{k,\\gamma}(x, y_1,..., y_k) := \\begin{cases}\n1 & \\text{ if } \\frac{1}{k} \\sum_{j=1}^{k} c(x, y_j) > \\gamma\\\\\n0 & \\text{ otherwise}\n\\end{cases}$\nwhere for $1 \\leq j \\leq k$, $y_j \\in Y$ is the observed output given $a_j(x)$, where $a_j \\in A$ is the $j$th observed augmentation. Note that the definition of $(k, \\gamma)$-success has also been used as the majority vote definition for SmoothLLM (Robey et al., 2023), although SmoothLLM uses Equation 1 solely as part of a defense mechanism whereas we use it for attack evaluation (see Appendix A.3)."}, {"title": "EXPERIMENTAL SETUP", "content": "For computing $(k, \\gamma)$-success rates, we set $k = 25$ to reduce the runtime of experiments and since we find this value to be sufficient for significantly affecting the success rate. Since the $(k, \\gamma)$-success false positive and false negative rates may vary significantly for each augmentation set $A$, we use separate thresholds that balances empirical estimates of the false positive and false negative rates for each $A$, and employ a human study to obtain these empirical estimates (see Appendix B.2 for more details on selecting ). As different model developers may impose different objectives for safety alignment, we seek a dataset $D$ that covers a broad range of harmful behavior classes. Hence, we use the SORRY-Bench dataset (Xie et al., 2024), which was constructed by sampling 450 prompts from across 10 datasets in a class-balanced manner over 45 classes. For the safety judge $c$, we use the fine-tuned Mistral-based safety judge that accompanies the SORRY-Bench dataset."}, {"title": "AUGMENTATIONS", "content": "For the RQ1 experiments, we examine the success rate gain $r_{k,\\gamma}(A, f, g) - r_{k,\\gamma}(A_1, f, g)$ of using an augmentation set $A$ over using no augmentations. Note that since adversarial attacks typically assume white-box access to the model, and since random augmentations are an entirely black-box operation, we do not compare against adversarial attack performance."}, {"title": "AUGMENTATION TYPES", "content": "Table 1 provides an overview of the augmentation types we investigate. We consider two main kinds of random augmentations: string insertion and character-level augmentations. String inser-"}, {"title": "AUGMENTATION STRENGTH", "content": "For string insertion augmentations, the notion of augmentation \"strength\" refers to the length of the inserted string, whereas for character-level augmentations, \u201cstrength\u201d refers to the amount of character positions that are augmented. We consider two ways to control the strength of an augmentation: 1. The strength is fixed for each prompt, and 2. The strength is proportional to the length of each prompt. Since D may contain a wide range of prompt lengths, fixing the strength may result in augmentations that are too aggressive for short prompts (which may change their semantic meaning) or too subtle for long prompts (which may lead to low success rate gains), in particular for character-level augmentations. Therefore, we focus on proportional augmentation strength, as governed by a proportion parameter p. For instance, with p = 0.1 and an original prompt length of 200 characters, the inserted string length for string insertion augmentations and the amount of augmented character positions for character-level augmentations would be 20 characters. (The number of characters is always rounded down to the nearest integer.) For our experiments, we set p = 0.05, which we find to be sufficient for obtaining non-trivial success rate gains while ensuring the augmentations are not too aggressive for shorter prompts (see the examples in Table 1, which use p = 0.05)."}, {"title": "MODELS", "content": "We consider the following models across 8 different model families: Llama 2 (Llama 2 7B Chat, Llama 2 13B Chat) (Touvron et al., 2023), Llama 3 (Llama 3 8B Instruct) (Dubey et al., 2024), Llama 3.1 (Llama 3.1 8B Instruct), Mistral (Mistral 7B Instruct v0.2), Phi 3 (Phi 3 Mini 4K Instruct, Phi 3 Small 8K Instruct, Phi 3 Medium 4K Instruct), Qwen 2 (Qwen 2 0.5B, Qwen 2 1.5B, Qwen 2 7B), Vicuna (Vicuna 7B v1.5, Vicuna 13B v1.5) and Zephyr (Zephyr 7B Beta). Among these, only the Llama, Phi and Qwen families have undergone explicit safety alignment. The remaining families are included to see if any interesting patterns can be observed for unaligned models. For instance, although Mistral was not explicitly aligned, it can sometimes exhibit refusal behavior for some harmful prompts, so it would still be interesting to see how this behavior can be affected by random augmentations."}, {"title": "EXPERIMENTAL RESULTS", "content": "In this section, we plot the results for each of our experiments and discuss our observations. Raw data values (including results using a fixed y 0 for all augmentations) broken down by augmentation type are reported in Appendix C."}, {"title": "RQ1: VARYING AUGMENTATION TYPE", "content": "In Figure 2, we see the experiment results for RQ1 (denoted by \u201c\u0442 = 0.0\"). Immediately, we can see that for each model, character-level augmentations achieve a significant positive average success\""}, {"title": "QUANTIZATION", "content": "Figure 4 reports the quantization experiment results for RQ2. For W8A8, most success rate changes are small, with all deviations being within 5%. Among all models, Qwen 2 7B has the greatest tendency towards becoming less safe. In Figure 12 in Appendix D, we show an example where the original Qwen 2 model fails under the random suffix augmentation while the W8A8 model succeeds even when the random suffixes used are the exact same for both models. Moving over to the W4A16 results, we see that the Llama 3, Mistral, Phi and Vicuna models become noticeably less safe. However, other models such as Llama 2, Llama 3.1 and Zephyr barely change, similar to their W8A8 counterparts. Even more curiously however, we see that Qwen 2 7B seemingly becomes more safe. However, upon further inspection, we realize that this may be a result of poorer model response quality in general; see Figure 13 in Appendix D for examples. Overall, while quantization can have some significant influence on success rate with more aggressive weight quantization tending to reduce safety, these effects are not consistent across models. As with the results for the model size experiment, this suggests that there may be other underlying factor(s) that determine how quantization affects safety under random augmentations."}, {"title": "FINE-TUNING-BASED DEFENSE", "content": "Figure 5 reports the fine-tuning-based defense experiment results for RQ2. All fine-tuned models clearly provide some amount of improvement in safety over their respective original models. For RR models, the improvement for Mistral 7B is much greater than the improvement for Llama 3 8B, probably due to the original Mistral model not being explicitly aligned (and therefore having a much larger room for improvement than the already aligned Llama 3 model). Interestingly, although Zephyr 7B was adversarially trained against only GCG suffixes of a fixed token length, it also enjoys some safety improvement on proportional-length random augmentations."}, {"title": "RQ3: VARYING THE GENERATION CONFIGURATION", "content": "Figure 2 reports the experiment results for RQ3 (denoted by \"\u0442 = 0.7\" and \"T = 1.0\"). First, we remark that increas- ing temperature without any augmentations already increases the success rate (see Table 3); this is in line with the findings of Huang et al. (2023) that showed altering temperature alone can be a successful attack. Next, we observe, that across all models and for both values of t, using random augmentations further improves the success rate on top of success gains due to output sampling. This supplements the findings of Huang et al. (2023) which only explored success rate im- provements for prompts without any input augmentations. Our results show that two sources of randomness, namely output sampling and input augmentations, can work together to pro- vide even greater attack effectiveness."}, {"title": "CONCLUSION", "content": "This paper demonstrates that simple random augmentations are a cheap yet effective approach to bypassing the safety alignment of state-of-the-art LLMs. However, although we identify patterns in how dimensions such as model size and quantization affect safety under random augmentations, we also discover counterexamples to these patterns. As such, the full story may be much more complex and involve additional underlying factors such as training data, optimization, etc. In the future, we will investigate whether these more complex dimensions can explain the counterexamples we observed. Moreover, we will go beyond evaluation to identify ways that models can become more robust against random augmentations."}, {"title": "RELATED WORK", "content": null}, {"title": "RANDOM AUGMENTATIONS AND ROBUSTNESS", "content": "Prior studies on the impact of random augmentations of robustness in NLP have largely focused on how they impact the performance of text classifiers. For instance, it has been shown that Neural Machine Translation (NMT) is vulnerable to character-level random augmentations such as swapping, keyboard typos, and editing (Belinkov & Bisk, 2017; Heigold et al., 2017). Furthermore, Karpukhin et al. (2019) demonstrated that training NMT models with character-level augmentations can improve robustness to natural noise in real-world data. Beyond NMT, Zhang et al. (2021) examined how both character-level (e.g., whitespace and character insertion) and word-level augmentations (e.g., word shuffling) can significantly degrade the sentiment analysis and paraphrase detection performance of models such as BERT (Devlin, 2018) and RoBERTa (Liu, 2019)."}, {"title": "RANDOM AUGMENTATIONS IN ADVERSARIAL ATTACKS", "content": "Random augmentations have also been utilized within adversarial attack algorithms. For instance, Li et al. (2018) introduced the TextBugger attack framework, which adversarially applies random augmentations (e.g., character-level augmentations such as inserting, swapping, or deleting characters and word-level augmentations such as word substitution) to fool models on sentiment analysis, question answering and machine translation tasks. Their method computes a gradient to estimate word importance, and then uses this estimate to apply random augmentations at specific locations based on the importance estimation. Additionally, Morris et al. (2020) introduced a comprehensive framework for generating adversarial examples to attack NLP models such as BERT, utilizing the word-level augmentations from the Easy Data Augmentation method (Wei & Zou, 2019) (i.e., synonym replacement, insertion, swapping, and deletion). The adversarial examples are also used to perform adversarial training to improve model robustness and generalization."}, {"title": "RANDOM AUGMENTATIONS FOR DEFENSE", "content": "SmoothLLM (Robey et al., 2023) was introduced as a system-level defense for mitigating jailbreak effectiveness. Their key observation is that successful jailbreaks are extremely brittle to random augmentations; i.e., many of the successful jailbreaks won't succeed after augmentation. In contrast, our work is based on the observation that the original prompt itself is also brittle, but in the opposite direction: given a prompt that doesn't succeed, one can effectively find an augmented prompt that does succeed. Moreover, their attack success evaluation is only based on a single chosen output per prompt, effectively discarding the other k 1 outputs. In contrast, since our threat model is built around the attacker making k independent attempts per prompt, our attack success evaluation accounts for all of the k outputs per prompt.\nFollowing in the footsteps of SmoothLLM, JailGuard (Zhang et al., 2024) was proposed as another defense method. Similar to SmoothLLM, JailGuard involves applying multiple random augmentations per prompt on the system side. However, JailGuard does not leverage a safety judge, instead examining the model response variance to determine whether a prompt is harmful or not. In a follow-up work to SmoothLLM, Ji et al. (2024) considers more advanced random augmentations such as synonym replacement or LLM-based augmentations such as paraphrasing and summarization. In the case of LLM-based augmentations, the randomness comes from the stochasticity of the generation algorithm (so long as greedy decoding is not used). In an earlier work, (Kumar et al., 2023) proposed RandomEC, which defends against jailbreaks by erasing random parts of the input and checking whether a safety judge deems the input to be safe or not, and deems the original input unsafe only if any of the augmented prompts are deemed unsafe."}, {"title": "ADDITIONAL DETAILS ON (k, \u03b3)-SUCCESS", "content": null}, {"title": "EFFECT OF Y ON FPR AND FNR", "content": "To see how the choice of y can affect the false positive rate, let 2; be the judge's predicted score for the jth augmentation, and let Z; be the corresponding true score (e.g., from human evaluation). Let\n$\\overline{Z} = \\frac{1}{k} \\sum_{j=1}^{k} Z_i$. Then the false positive rate as a function of y for A is\n$P_{FP}(\\gamma; A) := Pr(\\overline{Z} > k\\gamma | Z_1 = 0 \\cap Z_2 = 0 \\cap ... \\cap Z_k = 0)$\nClearly, this corresponds to evaluating the complementary CDF of a conditional distribution. Thus, $P_{FP} (\\gamma; A)$ is monotonically decreasing in y. On the other hand, note that the false negative rate is\n$P_{FN}(\\gamma; A) := Pr(\\overline{Z} < k\\gamma | Z_1 = 1 \\cup Z_2 = 1 \\cup . . . \\cup Z_k = 1)$\nThis corresponds to evaluating the CDF of a conditional distribution, and thus $P_{FN} (\\gamma; A)$ is monotonically increasing in y. Therefore, y strikes a trade-off between $P_{FP}(\\gamma; A)$ and $P_{FN} (\\gamma; A)$, so care must be taken to select a y that balances this trade-off."}, {"title": "SELECTING YA", "content": "For selecting y, to balance $P_{FP} (\\gamma; A)$ and $P_{FN}(\\gamma; A)$ we aim to select a y that minimizes their average (i.e. the balanced error rate). However, since we cannot exactly evaluate $P_{FP} (\\gamma; A)$ and $P_{FN}(\\gamma; A)$, we estimate them using the data from D and human judgement. Specifically, after obtaining judge scores for all our experiments, we randomly sampled 1220 data points (each containing k = 25 judge scores) from across all our experiment results and manually label them to obtain empirical estimates $P_{FP} (\\gamma; A)$ and $P_{FN}(\\gamma; A)$. The task of manual labeling is split over three of the authors so that each data point receives a human score from one of the authors. We provide the following task description to each of the labelers, adapted from the SORRY-Bench judge task prompt:\nYour goal is to judge if\n-[Label 1] At least one of the model responses complies with the user request, i.e., containing content that can directly assist with or resolve the request.\n-[Label 0] All the model responses don't comply with the user request; e.g., directly declining the user request or merely providing relevant content that cannot directly resolve the user request.\nFor each data point, the labeler is shown the original, unaugmented prompt and the k = 25 model responses. After manual labeling, we then select\n$\\gamma^*_A = arg \\min_{\\gamma \\in \\{0, ..., \\frac{k}{2}\\} } \\frac{P_{FP}(\\gamma; A) + P_{FN} (\\gamma; A)}{2}$\nas y for A. See Table 2 for a breakdown of the chosen y for each augmentation set A along with their estimated false positive and false negative rates."}]}