{"title": "Radar Signal Recognition through Self-Supervised Learning and Domain Adaptation", "authors": ["Zi Huang", "Simon Denman", "Akila Pemasiri", "Clinton Fookes", "Terrence Martin"], "abstract": "Automatic radar signal recognition (RSR) plays a pivotal role in electronic warfare (EW), as accurately classifying radar signals is critical for informing decision-making processes. Recent advances in deep learning have shown significant potential in improving RSR performance in domains with ample annotated data. However, these methods fall short in EW scenarios where annotated RF data are scarce or impractical to obtain. To address these challenges, we introduce a self-supervised learning (SSL) method which utilises masked signal modelling and RF domain adaption to enhance RSR performance in environments with limited RF samples and labels. Specifically, we investigate pre-training masked autoencoders (MAE) on baseband in-phase and quadrature (I/Q) signals from various RF domains and subsequently transfer the learned representation to the radar domain, where annotated data are limited. Empirical results show that our lightweight self-supervised ResNet model with domain adaptation achieves up to a 17.5% improvement in 1-shot classification accuracy when pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31% improvement when pre-trained on out-of-domain signals (i.e., comm signals), compared to its baseline without SSL. We also provide reference results for several MAE designs and pre-training strategies, establishing a new benchmark for few-shot radar signal classification.", "sections": [{"title": "I. INTRODUCTION", "content": "Automatic radar signal recognition (RSR) is a crucial capability in cognitive electronic warfare (CEW) [1], [2], where accurate radar signal classification is essential for informed decision-making in the battlefield. Recent progress in deep learning has demonstrated significant potential [3] in addressing RSR sub-tasks, such as automatic modulation classification (AMC) [4] and radar activity segmentation [5], when abundant data is available for algorithm development. However, most existing RSR methods rely heavily on annotated data, which presents a challenge in EW scenarios where mission-specific radio frequency (RF) data is often scarce or difficult to acquire. Moreover, practical EW operations often demand rapid in-mission re-training of models on limited data to respond to the rapidly changing threat landscape [2]. This makes few-shot learning a critical yet underexplored area in RSR.\nIn this paper, we introduce a self-supervised learning (SSL) approach that leverages masked signal modelling (MSM) and domain adaptation for few-shot RSR. Our proposed method pre-trains models on baseband I/Q signals with diverse characteristics from various RF domains. We then apply few-shot transfer learning to adapt pre-trained models to the radar"}, {"title": "II. RELATED WORK", "content": "Learning-based approaches for RF signal recognition have been an active research area over the past decade [4], [6]-[12]. Previous methods have relied on 2D feature transformations [13]-[15] to enhance classification performance in supervised settings. More recently, 1D approaches [10], [11], [16] have shown improved performance by effectively capturing fine-grained temporal relationships, as demonstrated in various signal recognition tasks within the RF domain, such as modulation classification [9], [17], radar signal characterisation [11], RF fingerprinting [18], and radar activity segmentation [5]. While unsupervised methods [19], [20] have been applied to AMC, self-supervised approaches have been more extensively studied in related signal processing domains, such as visual recognition [21], [22], audio recognition [23], [24], bio-signal classification [25], [26], and natural language processing [27]. Notably, masked autoencoders (MAE) have proven to be"}, {"title": "III. PROPOSED METHOD", "content": "Our SSL framework comprises two stages (Fig. 1). First,\nself-supervised pre-training is conducted via a masked au-\ntoencoder trained on a source domain. Then, the pre-trained\nencoder is fine-tuned on the target domain. We follow the mod-\nular encoder-decoder paradigm [33] to construct our autoen-\ncoder (Fig. 1). In the pre-training stage, we utilise asymmetric\nmasked autoencoding [22] whereby the model operates on a\npartially observed I/Q signal. We train the model to recon-\nstruct the original signal using a sample-wise similarity loss\nfunction [5]. This process is considered self-supervised as no\nannotations are required for the reconstruction task. After pre-\ntraining, we replace the decoder with a linear probing classifier\nconsisting of a flatten operation followed by a fully connected\nlayer. We fine-tune the classifier on the target domain with\na small amount (i.e., few-shot) of annotated training data\nbefore evaluating it on a large test set. We implement several\nautoencoders to examine their effectiveness in self-supervised\nRSR. We focus only on lightweight models, as rapid retraining\nand processing speed are critical considerations for EW [2].\nWe adopt the 1D ResNet [31], the 2-stage MS-TCN [5], [34],\nand the WaveNet [35], [36] as our baseline autoencoders. Pre-\ntraining of MS-TCN and WaveNet is conducted end-to-end as\nthey do not have independent encoders and decoders. As such,\nthe linear probing classifier is appended to these models during\nfine-tuning. In our experiments, we use the l\u2081 regression loss\nfor pre-training and cross entropy loss for fine-tuning."}, {"title": "B. Masked Signal Modelling", "content": "Masked signal modelling (MSM) is conceptually similar to masked image modelling (MIM) [29], [37], which has gained popularity as a simple and effective SSL approach in computer vision. Our approach utilises the proxy task of reconstruction, training a model to reconstruct intentionally corrupted signals. By applying this process across a large corpus of data, the model learns the salient representations necessary for accurate signal reconstruction. Similar to MIM, MSM involves several key design considerations, including the masking strategy $S_m$, masking ratio $R_m$, and model design. While the choice of model design is often influenced by external factors such as computational and memory requirements, $S_m$ and $R_m$ have a more explicit impact on the quality of pre-training [29].\nWe introduce several masking strategies for MSM, including random zero-masking (strat. A), random block zero-masking"}, {"title": "C. RF Datasets for Domain Adaption", "content": "We explore domain adaptation by applying MSM to I/Q signals from diverse RF domains, including telecommunications and radar signals. Our approach involves pre-training, without annotations, MAEs on four diverse datasets: RadioML [4], DeepRadar [10], RadarComm [9], and RadChar-SSL, which has been generated using our previous work [11]. We then perform few-shot transfer learning to fine-tune each pre-trained model on RadChar-nShot and evaluate performance on a test set (RadChar-Eval) derived from RadChar [11]. To ensure consistency across the pre-training datasets, we use only 10% of RadioML, keeping dataset sizes comparable."}, {"title": "IV. EXPERIMENTS", "content": "We perform pre-training, fine-tuning, and model evaluation on a single Nvidia Tesla A100 GPU. All models are trained with the Adam optimiser, where a constant learning rate of 0.001 and 0.0001 are used for self-supervised pre-training and fine-tuning, respectively. For pre-training, we train each model for 100 epochs with early stopping based on validation loss with a 3-step patience, using a fixed 70-20-10 train-validation-test split for each dataset. For fine-tuning on limited frames, we"}, {"title": "B. Few-Shot Radar Signal Recognition", "content": "We establish a novel benchmark for few-shot RSR through the following experiments: (i) evaluating different MAEs; (ii) evaluating different masking strategies; (iii) evaluating the impact of SSL on test performance across different SNR settings; and (iv) evaluating the effectiveness of self-supervised pre-training on signals from different RF domains for few-shot radar signal classification. We evaluate the test performance of each linear probing classifier using classification accuracy and the macro F1 score. The test performance of each classifier is compared to its baseline, where the model is trained from scratch without self-supervised pre-training.\nTable II presents a summary of results for each pre-training configuration. The classification performance for models corresponding to the optimal Sm and Rm used during pre-training are shown. We observe that fine-tuned models that are pre-trained on data from the same domain (i.e., RadChar) yield the greatest improvement in test performance when compared to their respective baselines. This performance gain is most substantial in the 1-shot configuration, which is the most challenging with the fewest labels available for fine-tuning. The improvement diminishes as the number of annotated frames used in fine-tuning increases, as reflected by a 17.5%, 6.3%, and 4.3% increase in 1-shot, 5-shot, and 10-shot performance for ResNet, respectively. Separately, we observe that MS-TCN's baseline performance is relatively low in the 1-shot setting, likely due to its larger model size (33.6 million parameters). This model requires more fine-tuning samples (Table II) to perform well compared to ResNet (2.7 million parameters) and WaveNet (1 million parameters).\nThe optimal masking strategy and its corresponding masking ratio differ for each model and pre-training configuration. For ResNet (Fig. 2), random zero-masking (strat. A) with lower masking ratios (below 0.4) performs best when pre-training on RadioML and DeepRadar, while the same strategy with a higher masking ratio (above 0.7) is more beneficial when pre-training on RadChar-SSL. Separately, random block noise-masking (strat. D) with a higher masking ratio (above 0.7) performs best when pre-training on RadarComm. The effectiveness of SSL on test performance depends on the SNR. Notably, models tend to benefit the most from pre-training when evaluating in moderate to high SNR levels (Fig. 3). We hypothesise that signal features (e.g., number of pulses, pulse"}, {"title": "C. Ablation Study", "content": "We examine the impact of various design considerations for few-shot RSR. As discussed in Section IV-B, the choice of $S_m$ and $R_m$ influences SSL performance and, in turn, fine-tuning and test performance. Although no particular masking strategy consistently outperformed others across our experiments, we found that the masking ratio played a more important role in determining test performance (Table II). While a small batch size coupled with a low learning rate generally benefits fine-tuning, we observed that freezing the model weights for a few epochs provided a slight improvement (less than 5%) in test performance. This effect was consistent for models pre-trained on both in-domain and out-of-domain data. We also explored $l_2$ loss for pre-training, but we observed no meaningful improvements over $l_1$ loss in our experiments."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced masked signal modelling as an effective self-supervised pre-training method for few-shot radar signal recognition. We also demonstrated the viability of RF domain adaptation for enhancing signal classification performance when no target domain data was used for pre-training. Our results show that by optimally designing the masking method during pre-training, fine-tuned models can achieve significant performance improvements, particularly in moderate to high SNR settings. This is demonstrated by the simple ResNet-based MAE, which achieved a boost in classification accuracy of 17.5% when pre-trained on in-domain signals (i.e., RadChar-SSL) and 16.31% when pre-trained on out-of-domain signals (i.e., RadioML), compared to its 1-shot baseline without SSL. In future work, additional domains and downstream tasks (e.g., pulse activity segmentation) will be explored to assess the practical utility of SSL and RF domain adaptation for few-shot radar signal recognition."}], "equations": [{"equation": "S_m =\\begin{cases}s \\cdot 0 \\{X < R_m\\} & \\text{if zero-masking},\\ns \\cdot n \\{X < R_m\\} & \\text{if noise-masking},\\ns & \\text{if no masking,}\\n\\end{cases}", "label": "(1)"}, {"equation": "\u00f1 \\sim \u039d(\u00b5_{\\text{train}}, \u03c3_{\\text{rain}}),  X \\sim U(0,1),  R_m \u2208 [0,1],", "label": "(2)"}]}