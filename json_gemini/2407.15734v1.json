{"title": "TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON", "authors": ["John Chong Min Tan", "Prince Saroj", "Bharat Runwal", "Hardik Maheshwari", "Alankrit Chona", "Ambuj Kumar", "Brian Lim Yi Sheng", "Richard Cottrill", "Mehul Motani"], "abstract": "TaskGen is an open-sourced agentic framework which uses an Agent to solve an arbitrary task by breaking them down into subtasks. Each subtask is mapped to an Equipped Function or another Agent to execute. In order to reduce verbosity (and hence token usage), TaskGen uses StrictJSON that ensures JSON output from the Large Language Model (LLM), along with additional features such as type checking and iterative error correction. Key to the philosophy of TaskGen is the management of information/memory on a need-to-know basis. We empirically evaluate TaskGen on various environments such as 40x40 dynamic maze navigation with changing obstacle locations (100% solve rate), TextWorld escape room solving with dense rewards and detailed goals (96% solve rate), web browsing (69% of actions successful), solving the MATH dataset (71% solve rate over 100 Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset (F1 score of 47.03%).", "sections": [{"title": "1 Introduction", "content": "TaskGen (https://github.com/simbianai/taskgen) is an open-sourced agentic framework which breaks down a task into subtasks, each of which are mapped to an Equipped Function or another Agent to execute. The Agents and Equipped Functions operate independently, but share context on a need-to-know basis using Shared Memory (see Fig. 1).\nTaskGen is designed to be less verbose, and hence incurs lower processing latency and costs with potentially improved accuracy, than most existing agentic frameworks which output free text such as AutoGPT (Yang et al., 2023a), BabyAGI (Nakajima, 2023), MetaGPT (Hong et al., 2023), AutoGen (Wu et al., 2023), ChatDev (Qian et al., 2023), CrewAI (Moura, 2023), LangChain/LangGraph (LangGraph, 2024).\nOur Contributions. We propose a new open-sourced agentic framework named TaskGen:\n1.  TaskGen breaks a complex task down into bite-sized subtasks, each of which are mapped to an Equipped Function or Inner Agent to execute.\n2.  In contrast to free-form text output in agentic frameworks, TaskGen uses a concise JSON output for each part of the process. Specifically, it uses StrictJSON (Tan, 2023), which is an LLM output parser for JSON format with type checking, and helps ensure concise and extractable output which can be used for downstream tasks easily.\n3.  TaskGen has Shared Memory amongst various components on a need-to-know basis. This Shared Memory can come in the form of 1) Subtasks Completed, a list of past Equipped Functions inputs and outputs, or 2) Shared Variables, which stores important information that may also be of the form of long text or non-text modalities.\n4.  TaskGen utilises Global Context to inform the Agent of important information that may be dynamically changing. This allows the Agent to react to dynamic environments as the task progresses, or as the Agent switches tasks.\n5.  Lastly, as memory is key to learning and decision making, TaskGen implements memory of various abstraction spaces in the Agent's Memory Bank, which can be used to augment the prompt to the Agent via Retrieval Augmented Generation (RAG) (Lewis et al., 2020) based on semantic sim-"}, {"title": "2 Motivation", "content": "We strive to create an Agent that can solve arbitrary tasks in arbitrary environments. However, when solving an arbitrary task, we could potentially do many actions, and there are many potential outcomes possible, as shown in Fig. 2. This is intractable for any Agent to manage and we need to limit the scope of what the Agent can do for more robust Agents.\nHence, we should limit the scope of the Agent by giving it only relevant Equipped Functions. This will help filter the vast action space into something tractable. Moreover, based on the Equipped Functions provided, we can break down a potentially complicated task into bite-sized subtasks, each of which can be solved entirely by one Equipped Function. This is shown in Fig. 3.\nIn fact, for more complex tasks, we can even let another Agent be the Equipped Function. This Agent will henceforth be referred to as Inner Agent. This is similar to how a manager offloads tasks to each worker, each of whom have their own experiences and skills to do the task. By having intelligent Inner Agents as the Equipped Function, the top-level agent (Meta Agent) will have greater processing capability. This is shown in Fig. 4.\nInfusing Shared Awareness. Each Equipped Function or Inner Agent would now be able to perform a subset of the entire task independently. However, they will need some shared context, as 1) the outcome of the subtask may influence other subtasks down the line, or 2) they may need input from earlier subtasks in order to perform their sub-task. To solve this problem, we implement a Shared Memory amongst the Meta Agent, Equipped Function and Inner Agents. Notably, we have two types of Shared Memory, 1) Subtasks Completed and 2) Shared Variables. This is shown in Fig. 1."}, {"title": "3 TaskGen Overall Design Philosophy", "content": "TaskGen has three key design philosophies.\nFirstly, the output of each Agent or Equipped Function is made to be as concise as possible for minimal token use. This is done using StrictJSON. By ensuring a structured JSON output format with type checking, StrictJSON reduces verbosity typically associated with free-form text output in LLMs. This cuts down on latency and costs, and improves reliability of extracting output fields needed for downstream components. For a more in-depth run-through of StrictJSON, refer to Appendix A.\nSecondly, we map each subtask to exactly one Equipped Function or Inner Agent, so as to guarantee executability of the subtask. Unlike AutoGPT (Yang et al., 2023a), we ensure that there are no infinite loops when executing subtasks. This is done via the following design guidelines:\n1.  An Agent can only call an Equipped Function or Inner Agent that is not above it in the hierarchy.\n2.  Each Agent gets context relevant to its own processing abstraction space and are assigned Equipped Functions and Inner Agents suitable for that space."}, {"title": "4 The Core of TaskGen", "content": ""}, {"title": "4.1 Agent Definition", "content": "At the core of TaskGen is the definition of an Agent, which consists of the following components:\n1.  Agent Name: Name of the Agent\n2.  Agent Description: Description of the Agent\n3.  Equipped Functions: List of Equipped Functions and Inner Agents available to solve subtasks\n4.  Assigned Task: Agent's assigned task\n5.  Subtasks Completed: Python dictionary of past subtasks that Agent has done, which detail the Equipped Function's name and input parameters and their corresponding output\n6.  Shared Variables: Python dictionary containing variables that will be shared between Equipped Functions and Agents\n7.  Global Context: Additional context to the Agent that can reference persistent states, such as those in Shared Variables\n8.  Memory Bank: Python dictionary containing various abstraction spaces of memory that will be retrieved via top-k retrieval via similarity to Assigned Task"}, {"title": "4.2 Imbuing Agentic Capabilities with Equipped Functions", "content": "By default, an Agent comes pre-built with a use_llm function, which uses an LLM with the Agent Name and Agent Description as context to perform a task, and an end_task function to end the current task. Additionally, we can assign Equipped Functions or Inner Agents to the Agent to imbue it additional capabilities.\nEquipped Functions come in two forms:\n1.  Internal Functions use an LLM to do processing of input-output relations. They are useful for tasks that are difficult for traditional rule-based approaches to handle well, such as sentiment analysis and summarisation.\n2.  External Functions utilise any Python function to do processing to get output, which makes it very easy for TaskGen to utilise functions from other agentic frameworks such as LangChain or CrewAI. They are suitable for tasks that can be called via fixed functions, or APIs, which guarantee reliability while imbuing additional functions to the LLM. As an aside, if we need a hybrid approach of rule-based fixed processes with flexibility of LLMs, an LLM can also be called within the External Function."}, {"title": "4.3 Choosing the Next Subtask", "content": "The core ability of an Agent is the ability to choose the correct next subtask to fulfil the Assigned Task. This is a non-trivial problem as it requires understanding of the Assigned Task, Agent Name, Agent Description, Subtasks Completed, relevant Memory, Equipped Functions and Inner Agents in order to make an informed decision.\nIn order to increase robustness in choosing the right Equipped Function and corresponding input parameters, we split it up into two steps.\nStep 1: Decide on subtask and corresponding Equipped Function / Inner Agent. The first step simply takes the available information to the Agent and does a Chain-of-Thought (CoT) (Wei et al., 2022) prompting to elicit reasoning via thoughts, leading to more accurate selection of subtask and the corresponding Equipped Function / Inner Agent in the following format:\n1.  Observation: Reflect on what has been done in Subtasks Completed for Assigned Task\n2.  Thoughts: Brainstorm how to complete remainder of Assigned Task only given Observation\n3.  Current Subtask: What to do now in detail with all context provided that can be done by one Equipped Function for Assigned Task\n4.  Equipped Function Name: Name of Equipped Function to use for Current Subtask\nStep 2: Decide on input parameters to Equipped Function / Inner Agent. Instead of providing the entire list of Equipped Functions / Inner Agents as per Step 1, we only give this step information of the exact Equipped Function / Inner Agent we have decided in Step 1, so as to encourage greater output specificity. We then generate the input parameters of the Equipped Function / Inner Agent given the Current Subtask and Equipped Function details (Equipped Function Name, Equipped Function Description, Equipped Function Input Parameter Description and type), and uses StrictJSON to ensure that the input parameters meet the type that is stated for in the Equipped Function. This ensures robustness and reliability for the input parameters."}, {"title": "5 Using TaskGen", "content": "Using TaskGen is extremely simple and is designed for any new user to learn it within 5 minutes. The steps needed are detailed as follows:\n1.  Install TaskGen. \"pip install taskgen-ai\"\n2.  Define LLM. This takes in a user prompt and system prompt as Python strings, and returns a Python string for the LLM generated response \"def llm(user_prompt: str, system_prompt: str) -> str\"\n3.  Define Agent. Simply define an Agent class with the Agent Name, Agent Description \"agent = Agent(name, description, 11m = 11m)\"\n4.  Equip Functions. Equip the Agent with Equipped Functions or Inner Agents to broaden the Agent's capabilities. \"agent.assign_functions([fn_1, fn_2])\"\n5.  Run Agent. Run the Agent with a task \"agent.run(task)\"\n6.  Query Agent. Query the Agent about Subtasks Completed \"agent.reply_user(query)\"\nFor an in-depth tutorial on how to use TaskGen, refer to Appendix B."}, {"title": "6 Benefits of TaskGen", "content": "The key philosophy of TaskGen is to be concise. This helps greatly with the performance of the overall system, as numerous studies (Xiong et al., 2023; Ding et al., 2024) have shown that an increase in context length generally leads to poorer performance on tasks referencing the context."}, {"title": "6.1 JSON is more concise than free text", "content": "Given a similar input prompt, asking the LLM to output in a JSON format generally gives much less verbose output as compared to free text. An example can be seen from from Fig. 5 for a prompt about the meaning of life. This is likely because the pre-training data of JSON on the web is more concise without much explanation, and the value of the field is very correlated to the key of the field. This means that we can use a JSON format to constrain the generation of the LLM to give the desired fields which we are interested in."}, {"title": "6.2 StrictJSON is more concise than JSON", "content": "TaskGen steers clear away from the typical JSON schema approach to define functions, which are used in many agentic frameworks adopting Pydantic as the JSON parser. This is because the JSON schema format is extremely verbose, and TaskGen using the StrictJSON schema is able to express the entire JSON schema of a function with much fewer tokens. As can be seen in Fig. 6, in order to express two parameters, the StrictJSON Schema uses 58 tokens compared to JSON Schema of 110 tokens, or about 53% the amount of tokens. The token savings are significant, and would be even more so with a lot more parameters."}, {"title": "6.3 Modular and robust components", "content": "TaskGen utilises a modular approach, where for each part of the system, be it Equipped Function or Inner Agent, we give it only the required context to do the task. This results in shorter context for LLM prompts, leading to better performance.\nMoreover, as we move from one subtask to the next, we split the process into multiple smaller chunks as required. For instance, when deciding what to do for the next subtask, we choose the Equipped Function / Inner Agent as one chunk, and choose the input parameters as another chunk. This again helps with reducing context length and cognitive load on each part of the process, and we can error check better at each part of the process."}, {"title": "6.4 Shared Memory", "content": "One of the key design philosophy of TaskGen is to share information only on a need-to-know basis. To that end, we utilise Shared Memory (see Fig. 7) to share information between the Agent and Equipped Function / Inner Agents.\nThere are two kinds of Shared Memory:\n1.  Subtasks Completed. This is a Python dictionary which stores the outcome of each subtask. The dictionary key is the name of the Equipped Function / Inner Agent and its input parameters, the value is the function output. This past history of function inputs and outputs will be made known to all LLM-based components of the system to help with shared awareness. Do note that this differs from the traditional ReAct framework (Yao et al., 2022) in that we do not store the earlier Thoughts. We notice empirically that just having the Subtasks Completed in the form of function inputs and outputs is enough for the LLM to understand past history to make an informed decision, and at the same time results in reduced context length.\n2.  Shared Variables. This is a Python dictionary which stores Python variables. These Python variables will be made available to the Agent and all Equipped Functions / Inner Agents upon request. The exact names and values of these Shared Variables will not be in the prompt to LLM calls by default, meaning that this information will not increase context length unless explicitly referred to. As such, we are able to store lengthy text output as well as filenames for various other modalities for suitable pre-processing when needed later on. The Equipped Functions / Inner Agents are also allowed to modify these Shared Variables, and as such can directly update the Shared Memory whenever needed."}, {"title": "6.5 Global Context", "content": "Global Context augments the default LLM prompt for the Agent. We use Global Context to expose certain persistent variables, typically stored in Shared Variables, which we want to carry through the task / carry across tasks. This is very useful for letting the Agent know the current state in a dynamically changing environment. Global Context can also contain more specific instructions for the LLM beyond the defaults in TaskGen."}, {"title": "6.6 Memory Bank", "content": "The Memory Bank contains all the important information that an Agent might need to know for an arbitrary task. We posit that a generic problem solver will need to contain memory at multiple forms of abstraction. For instance, when given a piece of text, we can store the 1) summary of it, 2) extracted entities and relationships in a knowledge graph, 3) entire text. These information will be useful when we are doing 1) generic question and answer, 2) causal reasoning, 3) specific question and answer respectively. If we just store information at one form of abstraction only (e.g. summary), some tasks will be significantly harder or impossible (e.g. find out specific details in text).\nTask-Augmented Prompt. When given a task, we extract out the relevant memories using RAG or other semantic matching algorithms. This will be used to augment the LLM prompt when selecting the next subtask and using the use_11m function.\nEquipped Function Filtering by Task. Furthermore, when given a task, not all Equipped Functions/Inner Agents are relevant, so we can filter them by semantic similarity to the task. This will help improve LLM performance provided that the correct functions are kept."}, {"title": "6.7 Other Notable Features", "content": "Conversable Agent. TaskGen provides a wrapper for a two-person chat interface with the Agent, where the Agent can use its Equipped Functions to perform actions and then reply the User.\nCode Generator. TaskGen has an in-built code generator and code corrector, which can also be used to perform actions with Python code, similar to CodeAct. (Wang et al., 2024)\nAsynchronous Mode. TaskGen has asynchronous equivalents of strict_json, Function and Agent classes for faster asynchronous processing.\nCommunity Contributions. TaskGen has a community space where users can easily upload and download Agents (see Appendix C)."}, {"title": "7 Evaluation", "content": "We evaluate TaskGen on various environments to showcase its versatility: dynamic maze navigation (see Appendix D), escape room solving in TextWorld (see Appendix E), web browsing (see Appendix F), MATH dataset (see Appendix G), RAG-based Question Answering (QA) on NaturalQuestions dataset (see Appendix H)."}, {"title": "8 Results", "content": "Overall, TaskGen works well for generic environments. The summarised results for each environment are as follows:\n1.  Dynamic Maze Navigation. We implement a 40x40 maze with obstacles that change halfway during the Agent's learning, similar to Learning, Fast and Slow (Tan and Motani, 2023). TaskGen with Global Context and an external StrictJSON Planner manages to solve 100% of the episodes on the first try, even after environment changes.\n2.  Escape Room Solving in TextWorld. We used TaskGen as a generic interactive fiction player to solve TextWorld (C\u00f4t\u00e9 et al., 2019) challenges. Where dense rewards and detailed goals were provided, TaskGen achieved a 96% solve rate, outperforming a neural-network agent's (C\u00f4t\u00e9, 2024) solve rate of 88%. Where commands are not provided and needed to be derived by the agent, TaskGen achieved an 88% solve rate, outperforming the baseline LLM's 57%.\n3.  Web-Browsing Agents. We designed a series of tasks requiring agents to navigate and extract information from the web, simulating real-world scenarios where users need to find specific information across various websites. Tasks included searching for academic studies, gathering news headlines, summarising market trends, and exploring educational resources. The agent demonstrated varying levels of success across different tasks, with 69% of actions being completed successfully.\n4.  MATH Dataset. We randomly selected 20 problems from the test set of 5 categories (Algebra, Pre-Algebra, Intermediate Algebra, Number Theory, and Counting and Probability) of the MATH dataset (Hendrycks et al., 2021). Our experiments (see Appendix G) showed that the TaskGen Agent with Equipped Functions achieved an average accuracy of 71% on challenging Level-5 problems, compared to 44% accuracy for the Agent without these functions. This demonstrates that imbuing an Agent with code generation and debugging capabilities significantly improves problem-solving.\n5.  RAG-based QA on NaturalQuestions. On the Natural Questions dataset (Kwiatkowski et al., 2019), TaskGen with Equipped Functions for dynamic retrieval and answering (we term this Interactive Retrieval) outperformed the baseline LLM with RAG across all metrics (see Appendix H). Compared to the baseline LLM, Interactive Retrieval achieved an F1 Score of 47.03% (+5.49%), precision of 40.75% (+7.43%), and recall of 55.59% (+0.42%), demonstrating TaskGen's effectiveness in dynamically refining context for more accurate question answering."}, {"title": "9 Conclusion and Future Work", "content": "TaskGen is already used in production at Simbian AI, and we would like to share its benefits with others. TaskGen's approach of not using conversation, but instead focusing directly on solving the task is a marked improvement over most existing agentic frameworks. TaskGen will continue to be actively developed over the coming years. The future work includes: 1) better planning abilities using state-based graphs, parallel searching, 2) multiple memory abstraction spaces such as vector databases and knowledge graphs, 3) reflection as a way to consolidate experiences and use for future decision making, 4) extended multi-modal support and 5) multiple agents with different skills and biases collaborating with one another.\nTowards Hybrid Workflows. As demonstrated by systems like AGENTless (Xia et al., 2024), full end-to-end agentic workflows may not always provide the best performance, as we may want to fix parts of the processes without Agents if we already know what needs to be done. This mixture of fixed processes and flexible agentic process selection will form the core tenet of future agentic systems. While not featured in native TaskGen, such hybrid systems can be implemented by using StrictJSON or fixed rules for dynamic routing over TaskGen Agents. We will explore more of such approaches and incorporate key elements into TaskGen."}, {"title": "10 Build Together With Us", "content": "TaskGen is an actively developing framework, and we would love to seek your inputs / contributions / feedback. Build together with us via our GitHub (https://github.com/simbianai/taskgen), and join the discussion group at Discord (https://discord.com/invite/bzp87AHJy5)."}, {"title": "11 Experiment Details", "content": "For more information on the following experiments in the Appendix, do contact the following:\n1. Community Contributions (see Appendix C) Hardik (hardik121998@gmail.com)\n2. Dynamic Maze Navigation (see Appendix D) - John Tan Chong Min\n3. TextWorld (see Appendix E) Richard Cottrill\n4. Web-Browsing Agents (see Appendix F) - Brian Lim Yi Sheng\n5. MATH Dataset (see Appendix G) - Bharat Runwal (bharat.runwal@simbian.ai)\n6. NaturalQuestions QA (see Appendix H) - Prince Saroj\nLimitations\nThe experiments conducted in this paper are not extensive for all available LLMs. We mainly use OpenAI's \"gpt-4o\" and \"gpt-3.5-turbo\". That said, we have also empirically tested and verified, though not shown here, that TaskGen works with other LLMs such as OpenAI's \"gpt-40-mini\", Llama-3 8B and Claude-3 Haiku.\nAcknowledgements\nThe research is supported by Simbian AI, where it is used in the core products. This research is also supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-01-003[T]) and by A*STAR, CISCO Systems (USA) Pte. Ltd and National University of Singapore under its Cisco-NUS Accelerated Digital Economy Corporate Laboratory (Award I21001E0002)."}, {"title": "A StrictJSON Details", "content": "StrictJSON is a library created in order to parse LLM output into a structured JSON format, and is used for all LLM calls in TaskGen. This enables efficient extraction of LLM output based on the JSON keys and enables interfacing the LLM as part of a larger system, such as the agentic framework in TaskGen. Furthermore, StrictJSON comes in-built with rule-based type checking which increases output reliability. StrictJSON also has error checking capabilities, which uses the JSON parsing errors or type checking errors to feed into the LLM in an iterative feedback loop as an error message to regenerate the JSON again. This is similar to the error feedback mechanism in Voyager (Wang et al., 2023).\nComparison with json.loads(): Typically, in order to parse JSON string into a dictionary, the function json.loads() is called. This is not robust to variations of the JSON and can easily fail to parse incorrectly formatted JSON, especially when generating code. StrictJSON is more robust, as it adds a delimiter before and after the key which the regex uses to extract. This regex will still work even if the quotation marks are not closed properly or are missing within the string. See Section A.2 for more details.\nWhy not YAML? YAML could also potentially be the format for LLM outputs in order to reduce token counts. However, YAML formatting performance has been empirically tested to be poorer than JSON, at least on the GPT models. We posit that this is because current LLMs are extensively trained on web data, of which JSON is more prevalent than YAML since it is the earlier format to be used. This may change as more web data is of YAML format. For now, JSON format is used to get a reliable system working.\nThis appendix details how to use StrictJSON based on TaskGen v3.2.0."}, {"title": "A.1 Usage", "content": ""}, {"title": "A.2 How it works under the hood", "content": "StrictJSON creates a prompt to the LLM to output JSON in a specified format using delimiters to enclose the output keys, that is more reliable to extract with regex as compared to unmodified keys of JSON. This is because the unmodified keys are just words with quotation marks, like \"Sentiment\", which may appear in other parts of the JSON and confuse the regex extraction."}, {"title": "B TaskGen Details", "content": "This appendix details the various modules of TaskGen and how to use them based on TaskGen v3.2.0."}, {"title": "B.1 Initialising TaskGen", "content": ""}, {"title": "B.2 TaskGen Agent Overview", "content": ""}, {"title": "B.2.1 Initialising the Agent", "content": ""}, {"title": "B.2.2 Running the Agent", "content": ""}, {"title": "B.2.3 Querying the Agent", "content": ""}, {"title": "B.2.4 Asynchronous Agents", "content": "We can perform whatever we did for the Agent in asynchronous mode too. Such an asynchronous runtime has advantages in that we can run multiple Agents in a shorter time, as we can effectively let other Agents run in the downtime of one Agent.\nTaskGen has two main classes - Agent and Function. Their asynchronous equivalents are AsyncAgent and AsyncFunction. Furthermore, the asynchronous version of strict_json is strict_json_async."}, {"title": "B.3 Meta Agent", "content": "Sometimes, due to task complexity, we would like to assign our Agent another Agent as an Equipped Function. Henceforth, our main Agent will be termed the Meta Agent, and the Agent equipped to it be termed the Inner Agent."}, {"title": "B.3.1 Initialising the Meta Agent", "content": ""}, {"title": "B.3.2 Running the Meta Agent", "content": ""}, {"title": "B.3.3 Visualising the Meta Agent's Status", "content": ""}, {"title": "B.3.4 Querying the Meta Agent", "content": ""}, {"title": "B.4 Shared Variables", "content": ""}, {"title": "B.4.1 Initialising Shared Variables", "content": ""}, {"title": "B.4.2 Modifying Shared Variables at Runtime", "content": ""}, {"title": "B.5 Global Context", "content": ""}, {"title": "B.5.1 Initialising Global Context", "content": ""}, {"title": "B.5.2 Running Agent with Global Context", "content": ""}, {"title": "B.6 Memory", "content": ""}, {"title": "B.6.1 Initialising Function Memory", "content": ""}, {"title": "B.6.2 Using Function Memory", "content": ""}, {"title": "B.6.3 Storing Additional Task-based Memory in Memory Bank", "content": ""}, {"title": "B.7 Conversation Class - Beta Version", "content": "As many applications of LLM involve some form of chatbot or personal assistant, we have decided to create a wrapper class ConversableAgent that takes in an Agent and interfaces it with a conversational interface.\nIn addition to the shared variables in Agent, ConversableAgent adds on three more:\n1.  Persistent Memory. This stores memory which we want to persist over the entire conversation and it will be updated after each turn of the conversation.\n2.  Conversation. This stores the actual conversation itself.\n3.  Summary of Conversation. This stores the summary of the entire conversation, which will be used to provide a global context to the Agent.\nIn general, when given a task, the ConversableAgent firstly performs the actions needed to answer the User's query. The ConversableAgent would then use the summarised actions (if any), Global Context, Summary of Past Conversation, Past Conversation, Persistent Memory to reply the User. The ConversableAgent will also update the Summary of Conversation.\nAfter the reply to the User, ConversableAgent will append the User's message and the Agent's reply to Conversation, and update the Persistent Memory accordingly.\nOverall, the main goal is to imbue a conversation with persistent states such as Persistent Memory and Summary of Conversation, so as to be able to create more wholesome and natural conversation.\nInsights by Developer: Conversation is not the main means of solving the User's query, so as to make the task solving portion concise. The task is solved first, before the Agent is given the chance to reply the User. In earlier iterations of ConversableAgent, when we had given the LLM function directly to the Agent, it is quite likely that the Agent will use the LLM function to hallucinate an outcome for the task that has never happened. This is an interesting finding that the task executor and the response to User portion of ConversableAgent should be implemented separately to minimise hallucinations."}, {"title": "C Community Contributions to TaskGen", "content": "This section elucidates the methodology by which users of TaskGen can contribute to the library, thereby fostering the growth of the TaskGen community."}, {"title": "C.1 Motivation for Community Contribution", "content": "TaskGen, an open-source repository, actively encourages contributions from its user base to enhance the library's functionality and accessibility. As users of TaskGen, individuals are incentivised to develop sophisticated agents utilising the framework and subsequently contribute these agents for the benefit of the broader community. This approach aligns with the ethos of open-source development and aims to cultivate a collaborative ecosystem where users can build upon each other's contributions. The overarching vision is to establish a marketplace of powerful agents leveraging the TaskGen framework, ultimately increasing the repository's utility through reusability."}, {"title": "C.2 Key Features of the Contribution Process", "content": "To facilitate seamless community involvement, significant efforts have been invested in streamlining the contribution process. Notable features include:\n1.  Simplified Contribution: Users can contribute their agents through a single function invocation.\n2.  Minimal Prerequisites: The process requires only a configured GitHub profile, eliminating the need for local git setup or repository cloning.\n3.  Comprehensive Support: The contribution mechanism accommodates various configurations, including max_subtasks, summarise_subtasks_count, memory_bank, shared_variables, global context settings, sub_agents, and both internal and external functions.\n4.  Efficient Integration: Accepted contributions can be loaded as agents with a single line of Python code."}, {"title": "C.3 Technical Implementation", "content": "The contribution process involves the following steps:\n1.  Environment Configuration: Users must set the GITHUB_USERNAME and GITHUB_TOKEN environment variables.\n2.  Agent Contribution: Invocation of the contribute_agent function on the user's agent."}, {"title": "C.4 Examples", "content": "To illustrate the contribution and usage process, we provide the following examples:"}, {"title": "C.4.1 Contributing an Agent", "content": "The following code snippet demonstrates how to create and contribute an agent:"}, {"title": "C.4.2 Loading a Community Agent", "content": "To load a contributed agent, users can employ the following simple code:"}, {"title": "C.4.3 Generated Code", "content": "The contribution process generates a Python representation of the agent. Below is an example of the generated code:"}, {"title": "C.5 Future Work and Community Feedback", "content": "While efforts have been made to support diverse agent configurations, it is acknowledged that there may be limitations in the current contribution process. Users are encouraged to provide feedback by raising issues on the GitHub repository to continually improve this process."}, {"title": "D Dynamic Maze Navigation", "content": ""}, {"title": "D.1 Maze Navigation", "content": "We evaluate TaskGen with a StrictJSON Planner, Shared Variables and Global Context in a dynamic maze navigation environment. It manages to solve the hardest 40x40 dynamic grid world all the time, faring better than prior methods in Learning, Fast and Slow (Tan and Motani, 2023)."}, {"title": "D.1.1 Background", "content": ""}, {"title": "D.1.2 Experimental Setup", "content": "The environment used is a 2D grid world, where there are 40 by 40 squares. There are also some grid squares which are denoted as obstacles and are not traversable. The agent starts off at a grid square and is supposed to head towards the door (exit) position.\nThe obstacles change mid-way, and the start and end points vary randomly with each episode. This is a difficult environment to evaluate learning as it is continuously changing. See"}]}