{"title": "TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON", "authors": ["John Chong Min Tan", "Prince Saroj", "Bharat Runwal", "Hardik Maheshwari", "Alankrit Chona", "Ambuj Kumar", "Brian Lim Yi Sheng", "Richard Cottrill", "Mehul Motani"], "abstract": "TaskGen is an open-sourced agentic framework which uses an Agent to solve an arbitrary task by breaking them down into subtasks. Each subtask is mapped to an Equipped Function or another Agent to execute. In order to reduce verbosity (and hence token usage), TaskGen uses StrictJSON that ensures JSON output from the Large Language Model (LLM), along with additional features such as type checking and iterative error correction. Key to the philosophy of TaskGen is the management of information/memory on a need-to-know basis. We empirically evaluate TaskGen on various environments such as 40x40 dynamic maze navigation with changing obstacle locations (100% solve rate), TextWorld escape room solving with dense rewards and detailed goals (96% solve rate), web browsing (69% of actions successful), solving the MATH dataset (71% solve rate over 100 Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset (F1 score of 47.03%).", "sections": [{"title": "Introduction", "content": "TaskGen (https://github.com/simbianai/taskgen) is an open-sourced agentic framework which breaks down a task into subtasks, each of which are mapped to an Equipped Function or another Agent to execute. The Agents and Equipped Functions operate independently, but share context on a need-to-know basis using Shared Memory (see Fig. 1). TaskGen is designed to be less verbose, and hence incurs lower processing latency and costs with potentially improved accuracy, than most existing agentic frameworks which output free text such as AutoGPT (Yang et al., 2023a), BabyAGI (Nakajima, 2023), MetaGPT (Hong et al., 2023), AutoGen (Wu et al., 2023), ChatDev (Qian et al., 2023), CrewAI (Moura, 2023), LangChain/LangGraph (LangGraph, 2024).\nOur Contributions. We propose a new open-sourced agentic framework named TaskGen:\n1. TaskGen breaks a complex task down into bite-sized subtasks, each of which are mapped to an Equipped Function or Inner Agent to execute.\n2. In contrast to free-form text output in agentic frameworks, TaskGen uses a concise JSON output for each part of the process. Specifically, it uses StrictJSON (Tan, 2023), which is an LLM output parser for JSON format with type checking, and helps ensure concise and extractable output which can be used for downstream tasks easily.\n3. TaskGen has Shared Memory amongst various components on a need-to-know basis. This Shared Memory can come in the form of 1) Subtasks Completed, a list of past Equipped Functions inputs and outputs, or 2) Shared Variables, which stores important information that may also be of the form of long text or non-text modalities.\n4. TaskGen utilises Global Context to inform the Agent of important information that may be dynamically changing. This allows the Agent to react to dynamic environments as the task progresses, or as the Agent switches tasks.\n5. Lastly, as memory is key to learning and decision making, TaskGen implements memory of various abstraction spaces in the Agent's Memory Bank, which can be used to augment the prompt to the Agent via Retrieval Augmented Generation (RAG) (Lewis et al., 2020) based on semantic sim-"}, {"title": "Motivation", "content": "We strive to create an Agent that can solve arbitrary tasks in arbitrary environments. However, when solving an arbitrary task, we could potentially do many actions, and there are many potential outcomes possible, as shown in Fig. 2. This is intractable for any Agent to manage and we need to limit the scope of what the Agent can do for more robust Agents.\nHence, we should limit the scope of the Agent by giving it only relevant Equipped Functions. This will help filter the vast action space into something tractable. Moreover, based on the Equipped Functions provided, we can break down a potentially complicated task into bite-sized subtasks, each of which can be solved entirely by one Equipped Function. This is shown in Fig. 3.\nIn fact, for more complex tasks, we can even let another Agent be the Equipped Function. This Agent will henceforth be referred to as Inner Agent. This is similar to how a manager offloads tasks to each worker, each of whom have their own experiences and skills to do the task. By having intelligent Inner Agents as the Equipped Function, the top-level agent (Meta Agent) will have greater processing capability. This is shown in Fig. 4.\nInfusing Shared Awareness. Each Equipped Function or Inner Agent would now be able to perform a subset of the entire task independently. However, they will need some shared context, as 1) the outcome of the subtask may influence other subtasks down the line, or 2) they may need input from earlier subtasks in order to perform their subtask. To solve this problem, we implement a Shared Memory amongst the Meta Agent, Equipped Function and Inner Agents. Notably, we have two types of Shared Memory, 1) Subtasks Completed and 2) Shared Variables. This is shown in Fig. 1."}, {"title": "TaskGen Overall Design Philosophy", "content": "TaskGen has three key design philosophies.\nFirstly, the output of each Agent or Equipped Function is made to be as concise as possible for minimal token use. This is done using StrictJSON. By ensuring a structured JSON output format with type checking, StrictJSON reduces verbosity typically associated with free-form text output in LLMs. This cuts down on latency and costs, and improves reliability of extracting output fields needed for downstream components. For a more in-depth run-through of StrictJSON, refer to Appendix A.\nSecondly, we map each subtask to exactly one Equipped Function or Inner Agent, so as to guarantee executability of the subtask. Unlike AutoGPT (Yang et al., 2023a), we ensure that there are no infinite loops when executing subtasks. This is done via the following design guidelines:\n1. An Agent can only call an Equipped Function or Inner Agent that is not above it in the hierarchy.\n2. Each Agent gets context relevant to its own processing abstraction space and are assigned Equipped Functions and Inner Agents suitable for that space."}, {"title": "The Core of TaskGen", "content": "At the core of TaskGen is the definition of an Agent, which consists of the following components:\n1. Agent Name: Name of the Agent\n2. Agent Description: Description of the Agent\n3. Equipped Functions: List of Equipped Functions and Inner Agents available to solve subtasks\n4. Assigned Task: Agent's assigned task\n5. Subtasks Completed: Python dictionary of past subtasks that Agent has done, which detail the Equipped Function's name and input parameters and their corresponding output\n6. Shared Variables: Python dictionary containing variables that will be shared between Equipped Functions and Agents\n7. Global Context: Additional context to the Agent that can reference persistent states, such as those in Shared Variables\n8. Memory Bank: Python dictionary containing various abstraction spaces of memory that will be retrieved via top-k retrieval via similarity to Assigned Task"}, {"title": "Imbuing Agentic Capabilities with Equipped Functions", "content": "By default, an Agent comes pre-built with a use_llm function, which uses an LLM with the Agent Name and Agent Description as context to perform a task, and an end_task function to end the current task. Additionally, we can assign Equipped Functions or Inner Agents to the Agent to imbue it additional capabilities.\nEquipped Functions come in two forms:\n1. Internal Functions use an LLM to do processing of input-output relations. They are useful for tasks that are difficult for traditional rule-based approaches to handle well, such as sentiment analysis and summarisation.\n2. External Functions utilise any Python function to do processing to get output, which makes it very easy for TaskGen to utilise functions from other agentic frameworks such as LangChain or CrewAI. They are suitable for tasks that can be called via fixed functions, or APIs, which guarantee reliability while imbuing additional functions to the LLM. As an aside, if we need a hybrid approach of rule-based fixed processes with flexibility of LLMs, an LLM can also be called within the External Function."}, {"title": "Choosing the Next Subtask", "content": "The core ability of an Agent is the ability to choose the correct next subtask to fulfil the Assigned Task. This is a non-trivial problem as it requires understanding of the Assigned Task, Agent Name, Agent Description, Subtasks Completed, relevant Memory, Equipped Functions and Inner Agents in order to make an informed decision.\nIn order to increase robustness in choosing the right Equipped Function and corresponding input parameters, we split it up into two steps.\nStep 1: Decide on subtask and corresponding Equipped Function / Inner Agent. The first step simply takes the available information to the Agent and does a Chain-of-Thought (CoT) (Wei et al., 2022) prompting to elicit reasoning via thoughts, leading to more accurate selection of subtask and the corresponding Equipped Function / Inner Agent in the following format:\n1. Observation: Reflect on what has been done in Subtasks Completed for Assigned Task\n2. Thoughts: Brainstorm how to complete remainder of Assigned Task only given Observation\n3. Current Subtask: What to do now in detail with all context provided that can be done by one Equipped Function for Assigned Task\n4. Equipped Function Name: Name of Equipped Function to use for Current Subtask\nStep 2: Decide on input parameters to Equipped Function / Inner Agent. Instead of providing the entire list of Equipped Functions / Inner Agents as per Step 1, we only give this step information of the exact Equipped Function / Inner Agent we have decided in Step 1, so as to encourage greater output specificity. We then generate the input parameters of the Equipped Function / Inner Agent given the Current Subtask and Equipped Function details (Equipped Function Name, Equipped Function Description, Equipped Function Input Parameter Description and type), and uses StrictJSON to ensure that the input parameters meet the type that is stated for in the Equipped Function. This ensures robustness and reliability for the input parameters."}, {"title": "Using TaskGen", "content": "Using TaskGen is extremely simple and is designed for any new user to learn it within 5 minutes. The steps needed are detailed as follows:\n1. Install TaskGen. \"pip install taskgen-ai\"\n2. Define LLM. This takes in a user prompt and system prompt as Python strings, and returns a Python string for the LLM generated response \"def llm(user_prompt: str, system_prompt: str) -> str\"\n3. Define Agent. Simply define an Agent class with the Agent Name, Agent Description \"agent = Agent(name, description, llm = llm)\"\n4. Equip Functions. Equip the Agent with Equipped Functions or Inner Agents to broaden the Agent's capabilities. \"agent.assign_functions([fn_1, fn_2])\"\n5. Run Agent. Run the Agent with a task \"agent.run(task)\"\n6. Query Agent. Query the Agent about Subtasks Completed \"agent.reply_user(query)\"\nFor an in-depth tutorial on how to use TaskGen, refer to Appendix B."}, {"title": "Benefits of TaskGen", "content": "The key philosophy of TaskGen is to be concise. This helps greatly with the performance of the overall system, as numerous studies (Xiong et al., 2023; Ding et al., 2024) have shown that an increase in context length generally leads to poorer performance on tasks referencing the context."}, {"title": "JSON is more concise than free text", "content": "Given a similar input prompt, asking the LLM to output in a JSON format generally gives much less verbose output as compared to free text. An example can be seen from from Fig. 5 for a prompt about the meaning of life. This is likely because the pre-training data of JSON on the web is more concise without much explanation, and the value of the field is very correlated to the key of the field. This means that we can use a JSON format to constrain the generation of the LLM to give the desired fields which we are interested in."}, {"title": "StrictJSON is more concise than JSON", "content": "TaskGen steers clear away from the typical JSON schema approach to define functions, which are used in many agentic frameworks adopting Pydantic as the JSON parser. This is because the JSON schema format is extremely verbose, and TaskGen using the StrictJSON schema is able to express the entire JSON schema of a function with much fewer tokens. As can be seen in Fig. 6, in order to express two parameters, the StrictJSON Schema uses 58 tokens compared to JSON Schema of 110 tokens, or about 53% the amount of tokens. The token savings are significant, and would be even more so with a lot more parameters."}, {"title": "Modular and robust components", "content": "TaskGen utilises a modular approach, where for each part of the system, be it Equipped Function or Inner Agent, we give it only the required context to do the task. This results in shorter context for LLM prompts, leading to better performance. Moreover, as we move from one subtask to the next, we split the process into multiple smaller chunks as required. For instance, when deciding what to do for the next subtask, we choose the Equipped Function / Inner Agent as one chunk, and choose the input parameters as another chunk. This again helps with reducing context length and cognitive load on each part of the process, and we can error check better at each part of the process."}, {"title": "Shared Memory", "content": "One of the key design philosophy of TaskGen is to share information only on a need-to-know basis. To that end, we utilise Shared Memory (see Fig. 7) to share information between the Agent and Equipped Function / Inner Agents.\nThere are two kinds of Shared Memory:\n1. Subtasks Completed. This is a Python dictionary which stores the outcome of each subtask. The dictionary key is the name of the Equipped Function / Inner Agent and its input parameters, the value is the function output. This past history of function inputs and outputs will be made known to all LLM-based components of the system to help with shared awareness. Do note that this differs from the traditional ReAct framework (Yao et al., 2022) in that we do not store the earlier Thoughts. We notice empirically that just having the Subtasks Completed in the form of function inputs and outputs is enough for the LLM to understand past history to make an informed decision, and at the same time results in reduced context length.\n2. Shared Variables. This is a Python dictionary which stores Python variables. These Python variables will be made available to the Agent and all Equipped Functions / Inner Agents upon request. The exact names and values of these Shared Variables will not be in the prompt to LLM calls by default, meaning that this information will not increase context length unless explicitly referred to. As such, we are able to store lengthy text output as well as filenames for various other modalities for suitable pre-processing when needed later on. The Equipped Functions / Inner Agents are also allowed to modify these Shared Variables, and as such can directly update the Shared Memory whenever needed."}, {"title": "Global Context", "content": "Global Context augments the default LLM prompt for the Agent. We use Global Context to expose certain persistent variables, typically stored in Shared Variables, which we want to carry through the task / carry across tasks. This is very useful for letting the Agent know the current state in a dynamically changing environment. Global Context can also contain more specific instructions for the LLM beyond the defaults in TaskGen."}, {"title": "Memory Bank", "content": "The Memory Bank contains all the important information that an Agent might need to know for an arbitrary task. We posit that a generic problem solver will need to contain memory at multiple forms of abstraction. For instance, when given a piece of text, we can store the 1) summary of it, 2) extracted entities and relationships in a knowledge graph, 3) entire text. These information will be useful when we are doing 1) generic question and answer, 2) causal reasoning, 3) specific question and answer respectively. If we just store information at one form of abstraction only (e.g. summary), some tasks will be significantly harder or impossible (e.g. find out specific details in text).\nTask-Augmented Prompt. When given a task, we extract out the relevant memories using RAG or other semantic matching algorithms. This will be used to augment the LLM prompt when selecting the next subtask and using the use_llm function.\nEquipped Function Filtering by Task. Furthermore, when given a task, not all Equipped Functions/Inner Agents are relevant, so we can filter them by semantic similarity to the task. This will help improve LLM performance provided that the correct functions are kept."}, {"title": "Other Notable Features", "content": "Conversable Agent. TaskGen provides a wrapper for a two-person chat interface with the Agent, where the Agent can use its Equipped Functions to perform actions and then reply the User.\nCode Generator. TaskGen has an in-built code generator and code corrector, which can also be used to perform actions with Python code, similar to CodeAct. (Wang et al., 2024)\nAsynchronous Mode. TaskGen has asynchronous equivalents of strict_json, Function and Agent classes for faster asynchronous processing.\nCommunity Contributions. TaskGen has a community space where users can easily upload and download Agents (see Appendix C)."}, {"title": "Evaluation", "content": "We evaluate TaskGen on various environments to showcase its versatility: dynamic maze navigation (see Appendix D), escape room solving in TextWorld (see Appendix E), web browsing (see Appendix F), MATH dataset (see Appendix G), RAG-based Question Answering (QA) on NaturalQuestions dataset (see Appendix H)."}, {"title": "Results", "content": "Overall, TaskGen works well for generic environments. The summarised results for each environment are as follows:\n1. Dynamic Maze Navigation. We implement a 40x40 maze with obstacles that change halfway during the Agent's learning, similar to Learning, Fast and Slow (Tan and Motani, 2023). TaskGen with Global Context and an external StrictJSON Planner manages to solve 100% of the episodes on the first try, even after environment changes.\n2. Escape Room Solving in TextWorld. We used TaskGen as a generic interactive fiction player to solve TextWorld (C\u00f4t\u00e9 et al., 2019) challenges. Where dense rewards and detailed goals were provided, TaskGen achieved a 96% solve rate, outperforming a neural-network agent's (C\u00f4t\u00e9, 2024) solve rate of 88%. Where commands are not provided and needed to be derived by the agent, TaskGen achieved an 88% solve rate, outperforming the baseline LLM's 57%.\n3. Web-Browsing Agents. We designed a series of tasks requiring agents to navigate and extract information from the web, simulating real-world scenarios where users need to find specific information across various websites. Tasks included searching for academic studies, gathering news headlines, summarising market trends, and exploring educational resources. The agent demonstrated varying levels of success across different tasks, with 69% of actions being completed successfully.\n4. MATH Dataset. We randomly selected 20 problems from the test set of 5 categories (Algebra, Pre-Algebra, Intermediate Algebra, Number Theory, and Counting and Probability) of the MATH dataset (Hendrycks et al., 2021). Our experiments (see Appendix G) showed that the TaskGen Agent with Equipped Functions achieved an average accuracy of 71% on challenging Level-5 problems, compared to 44% accuracy for the Agent without these functions. This demonstrates that imbuing an Agent with code generation and debugging capabilities significantly improves problem-solving.\n5. RAG-based QA on NaturalQuestions. On the Natural Questions dataset (Kwiatkowski et al., 2019), TaskGen with Equipped Functions for dynamic retrieval and answering (we term this Interactive Retrieval) outperformed the baseline LLM with RAG across all metrics (see Appendix H). Compared to the baseline LLM, Interactive Retrieval achieved an F1 Score of 47.03% (+5.49%), precision of 40.75% (+7.43%), and recall of 55.59% (+0.42%), demonstrating TaskGen's effectiveness in dynamically refining context for more accurate question answering."}, {"title": "Conclusion and Future Work", "content": "TaskGen is already used in production at Simbian AI, and we would like to share its benefits with others. TaskGen's approach of not using conversation, but instead focusing directly on solving the task is a marked improvement over most existing agentic frameworks. TaskGen will continue to be actively developed over the coming years. The future work includes: 1) better planning abilities using state-based graphs, parallel searching, 2) multiple memory abstraction spaces such as vector databases and knowledge graphs, 3) reflection as a way to consolidate experiences and use for future decision making, 4) extended multi-modal support and 5) multiple agents with different skills and biases collaborating with one another.\nTowards Hybrid Workflows. As demonstrated by systems like AGENTless (Xia et al., 2024), full end-to-end agentic workflows may not always provide the best performance, as we may want to fix parts of the processes without Agents if we already know what needs to be done. This mixture of fixed processes and flexible agentic process selection will form the core tenet of future agentic systems. While not featured in native TaskGen, such hybrid systems can be implemented by using StrictJSON or fixed rules for dynamic routing over TaskGen Agents. We will explore more of such approaches and incorporate key elements into TaskGen."}, {"title": "Build Together With Us", "content": "TaskGen is an actively developing framework, and we would love to seek your inputs / contributions / feedback. Build together with us via our GitHub (https://github.com/simbianai/taskgen), and join the discussion group at Discord (https://discord.com/invite/bzp87AHJy5)."}, {"title": "Experiment Details", "content": "For more information on the following experiments in the Appendix, do contact the following:\n1. Community Contributions (see Appendix C) - Hardik (hardik121998@gmail.com)\n2. Dynamic Maze Navigation (see Appendix D) - John Tan Chong Min\n3. TextWorld (see Appendix E) Richard Cottrill\n4. Web-Browsing Agents (see Appendix F) - Brian Lim Yi Sheng\n5. MATH Dataset (see Appendix G) - Bharat Runwal (bharat.runwal@simbian.ai)\n6. NaturalQuestions QA (see Appendix H) - Prince Saroj"}, {"title": "Limitations", "content": "The experiments conducted in this paper are not extensive for all available LLMs. We mainly use OpenAI's \"gpt-4o\" and \"gpt-3.5-turbo\". That said, we have also empirically tested and verified, though not shown here, that TaskGen works with other LLMs such as OpenAI's \"gpt-40-mini\", Llama-3 8B and Claude-3 Haiku."}, {"title": "Acknowledgements", "content": "The research is supported by Simbian AI, where it is used in the core products. This research is also supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-01-003[T]) and by A*STAR, CISCO Systems (USA) Pte. Ltd and National University of Singapore under its Cisco-NUS Accelerated Digital Economy Corporate Laboratory (Award I21001E0002)."}, {"title": "StrictJSON Details", "content": "StrictJSON is a library created in order to parse LLM output into a structured JSON format, and is used for all LLM calls in TaskGen. This enables efficient extraction of LLM output based on the JSON keys and enables interfacing the LLM as part of a larger system, such as the agentic framework in TaskGen. Furthermore, StrictJSON comes in-built with rule-based type checking which increases output reliability. StrictJSON also has error checking capabilities, which uses the JSON parsing errors or type checking errors to feed into the LLM in an iterative feedback loop as an error message to regenerate the JSON again. This is similar to the error feedback mechanism in Voyager (Wang et al., 2023).\nComparison with json.loads(): Typically, in order to parse JSON string into a dictionary, the function json.loads() is called. This is not robust to variations of the JSON and can easily fail to parse incorrectly formatted JSON, especially when generating code. StrictJSON is more robust, as it adds a delimiter before and after the key which the regex uses to extract. This regex will still work even if the quotation marks are not closed properly or are missing within the string. See Section A.2 for more details.\nWhy not YAML? YAML could also potentially be the format for LLM outputs in order to reduce token counts. However, YAML formatting performance has been empirically tested to be poorer than JSON, at least on the GPT models. We posit that this is because current LLMs are extensively trained on web data, of which JSON is more prevalent than YAML since it is the earlier format to be used. This may change as more web data is of YAML format. For now, JSON format is used to get a reliable system working.\nThis appendix details how to use StrictJSON based on TaskGen v3.2.0."}, {"title": "Usage", "content": "Example LLM Definition\nTo use StrictJSON, we firstly need to have an LLM available in order to generate the JSON from the text input given. Fig. A1 illustrates an example LLM function (named llm) that can be interfaced with StrictJSON. It takes as input the system prompt, which is the overall system message for the LLM, as well as the user prompt, which is what the user typically enters into the LLM for a response. This returns an LLM model response in the form of a string, which is the output of this LLM function. By exposing the entire LLM function to the user, StrictJSON is extremely versatile and can operate with both API-based LLM models and local models."}, {"title": "Example Usage", "content": "In order to use StrictJSON to process the LLM's output, we simply use the strict_json function. We give it the system prompt, user prompt, and the output format in a dictionary format with keys being the field name and values being the description of the field. For instance, Fig. A2 illustrates how to use StrictJSON to classify a sentence in the user prompt. As can be seen, StrictJSON processes the type of sentiment, an array of adjectives in the sentence, and the number of words all in the same function call."}, {"title": "Advanced Usage of StrictJSON for code", "content": "StrictJSON is also able to process code reliably, as shown in Fig. A3."}, {"title": "Type Checking in StrictJSON", "content": "StrictJSON also supports type checking of the following types: int, float, str, dict, list, array, code, bool, Dict[], List[], Array[], Enum[]. If there is a [], you can nest datatypes within it such as List[int] for a list of integers. Only Dict[] cannot be nested, and Dict[dictionary_keys] is used instead to enforce the presence of the dictionary_keys within the dictionary. Fig. A4 illustrates how to use StrictJSON with type checking. This can ensure greater output specificity and greater reliability for downstream tasks."}, {"title": "How it works under the hood", "content": "StrictJSON creates a prompt to the LLM to output JSON in a specified format using delimiters to enclose the output keys, that is more reliable to extract with regex as compared to unmodified keys of JSON. This is because the unmodified keys are just words with quotation marks, like \"Sentiment\", which may appear in other parts of the JSON and confuse the regex extraction.\nFig. A5 demonstrates how to visualise the actual LLM system and user prompt using verbose = True as a parameter to strict_json. We can see that we get the LLM to enclose keys with delimiters (default '###'), and enclose the JSON values with <>, which the LLM will be instructed to update.\nThe regex that is used to parse the LLM output can be seen in Fig. A6. By extracting keys of the form '###{key}###' or \"###{key}###\", we can extract and parse the JSON even when there are mismatched quotation marks, unclosed brackets, and many other issues that will cause json.loads() to fail."}, {"title": "TaskGen Details", "content": "This appendix details the various modules of TaskGen and how to use them based on TaskGen v3.2.0."}, {"title": "Initialising TaskGen", "content": "Fig. B1 shows how to initialise TaskGen. Here, we use \"gpt-4o\", but TaskGen can also work with \"gpt-3.5-turbo\" or equivalent LLM models at the cost of lower performance.\nThere three steps are:\n1. Install TaskGen\n2. Import required functions and setup relevant API keys for your LLM\n3. Define your own LLM, which takes in a system prompt and user prompt and outputs the response string from the LLM"}, {"title": "TaskGen Agent Overview", "content": "Fig. B2 shows how to initialise the Agent.\nWe firstly define the functions for the Agent.\nThis can be of the form of an Internal Functions using Function class, which takes in the function description and output format of the function. We denote the variables in function description via <> enclosing the variable name. The output format is in the style of StrictJSON's output format. The Internal Function uses LLM to process the function, leading to very flexible functions that rule-based solutions may not allow for.\nFunctions can also be of the form of an External Function, which is very flexible as it is just a Python function. We simply define the function with typing for inputs and outputs, and with a docstring that contains the input parameter names. If any of the typing or docstring is missing, we will omit them from the function description, but the External Function can still work. External Functions allow for both rule-based rigidity and LLM-based flexibility, as an LLM call can be made inside the External Function as well.\nAfter defining our Functions, we define our Agent by calling Agent(name, description, llm). Thereafter, we proceed to assign our functions via assign_functions.\nTo see how the functions look like, we can also use print_functions to visualise it. Notice that the functions just consists of Name, Description, Input and Output fields, which is much shorter than the JSON schema or Pydantic way of defining a function."}, {"title": "Running the Agent", "content": "Fig. B3 shows how to assign a task and run the Agent by simply calling run(task). Notice how we can visualise the output via Observation, Thoughts, Action (Subtask) in the traditional ReAct framework. The difference between TaskGen and the original ReAct framework is that the observation here is actually the observation of the Subtasks Completed instead of the Observation of the function's output. By structuring Observation this way, this helps to provide a summary of what has been done so far, which aids in decision making.\nWe also do not store these Observation and Thoughts as they are just used in decision making at that point of time, but not needed in the longer term. The entire history of what has been done is stored in Subtasks Completed, which can be visualised via status() or via the subtasks_completed variable of the agent. Notice also that calling status() also gives us the Agent's details, such as Agent Name, Agent Description, Equipped Functions, Shared Variable Names, Assigned Task, Subtasks Completed, and whether the task is completed. We can call status() anytime to check on how the Agent is performing."}, {"title": "Querying the Agent", "content": "Fig. B4 shows how we can reply the user by simply calling reply_user() to get the Agent to reply based on what has been done in Subtasks Completed. If reply_user() is called without any query parameter, it will reply based on the assigned task. If there is a query parameter given, then it will reply based on the query.\nThis functions as a simple question answer bot, from which we can ask multiple questions about what the Agent has done so far and reply the user."}, {"title": "Asynchronous Agents", "content": "We can perform whatever we did for the Agent in asynchronous mode too. Such an asynchronous runtime has advantages in that we can run multiple Agents in a shorter time, as we can effectively let other Agents run in the downtime of one Agent.\nTaskGen has two main classes - Agent and Function. Their asynchronous equivalents are AsyncAgent and AsyncFunction. Furthermore, the asynchronous version of strict_json is strict_json_async.\nFig. B5 shows how to initialise the asynchronous LLM. Simply define a function that takes in a system prompt and user prompt, and outputs the response string of the LLM operating in asynchronous mode."}, {"title": "Meta Agent", "content": "Sometimes, due to task complexity, we would like to assign our Agent another Agent as an Equipped Function. Henceforth, our main Agent will be termed the Meta Agent, and the Agent equipped to it be termed the Inner Agent."}, {"title": "Initialising the Meta Agent", "content": "Fig. B8 show how to initialise the Meta Agent. It is generally the same process as initialising functions to the Agent, except that this function is of class \"Agent\". Note that we can specify how each Inner Agent should behave, including the max_subtasks it should run for and what LLM it should use.\nThe Inner Agents will have full access to the Subtasks Completed and Shared Variables of the Meta Agent, and all the Equipped Functions of the Inner Agents will have access to these as well. This helps ensure that the context of the Meta Agent is fed downwards to the Inner Agents, and the Inner Agents can also change the Shared Memory of the Meta Agent."}, {"title": "Running the Meta Agent", "content": "Figs. B9"}, {"title": "TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON", "authors": ["John Chong Min Tan", "Prince Saroj", "Bharat Runwal", "Hardik Maheshwari", "Alankrit Chona", "Ambuj Kumar", "Brian Lim Yi Sheng", "Richard Cottrill", "Mehul Motani"], "abstract": "TaskGen is an open-sourced agentic framework which uses an Agent to solve an arbitrary task by breaking them down into subtasks. Each subtask is mapped to an Equipped Function or another Agent to execute. In order to reduce verbosity (and hence token usage), TaskGen uses StrictJSON that ensures JSON output from the Large Language Model (LLM), along with additional features such as type checking and iterative error correction. Key to the philosophy of TaskGen is the management of information/memory on a need-to-know basis. We empirically evaluate TaskGen on various environments such as 40x40 dynamic maze navigation with changing obstacle locations (100% solve rate), TextWorld escape room solving with dense rewards and detailed goals (96% solve rate), web browsing (69% of actions successful), solving the MATH dataset (71% solve rate over 100 Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset (F1 score of 47.03%).", "sections": [{"title": "Introduction", "content": "TaskGen (https://github.com/simbianai/taskgen) is an open-sourced agentic framework which breaks down a task into subtasks, each of which are mapped to an Equipped Function or another Agent to execute. The Agents and Equipped Functions operate independently, but share context on a need-to-know basis using Shared Memory (see Fig. 1). TaskGen is designed to be less verbose, and hence incurs lower processing latency and costs with potentially improved accuracy, than most existing agentic frameworks which output free text such as AutoGPT (Yang et al., 2023a), BabyAGI (Nakajima, 2023), MetaGPT (Hong et al., 2023), AutoGen (Wu et al., 2023), ChatDev (Qian et al., 2023), CrewAI (Moura, 2023), LangChain/LangGraph (LangGraph, 2024).\nOur Contributions. We propose a new open-sourced agentic framework named TaskGen:\n1. TaskGen breaks a complex task down into bite-sized subtasks, each of which are mapped to an Equipped Function or Inner Agent to execute.\n2. In contrast to free-form text output in agentic frameworks, TaskGen uses a concise JSON output for each part of the process. Specifically, it uses StrictJSON (Tan, 2023), which is an LLM output parser for JSON format with type checking, and helps ensure concise and extractable output which can be used for downstream tasks easily.\n3. TaskGen has Shared Memory amongst various components on a need-to-know basis. This Shared Memory can come in the form of 1) Subtasks Completed, a list of past Equipped Functions inputs and outputs, or 2) Shared Variables, which stores important information that may also be of the form of long text or non-text modalities.\n4. TaskGen utilises Global Context to inform the Agent of important information that may be dynamically changing. This allows the Agent to react to dynamic environments as the task progresses, or as the Agent switches tasks.\n5. Lastly, as memory is key to learning and decision making, TaskGen implements memory of various abstraction spaces in the Agent's Memory Bank, which can be used to augment the prompt to the Agent via Retrieval Augmented Generation (RAG) (Lewis et al., 2020) based on semantic similarity"}, {"title": "Motivation", "content": "We strive to create an Agent that can solve arbitrary tasks in arbitrary environments. However, when solving an arbitrary task, we could potentially do many actions, and there are many potential outcomes possible, as shown in Fig. 2. This is intractable for any Agent to manage and we need to limit the scope of what the Agent can do for more robust Agents.\nHence, we should limit the scope of the Agent by giving it only relevant Equipped Functions. This will help filter the vast action space into something tractable. Moreover, based on the Equipped Functions provided, we can break down a potentially complicated task into bite-sized subtasks, each of which can be solved entirely by one Equipped Function. This is shown in Fig. 3.\nIn fact, for more complex tasks, we can even let another Agent be the Equipped Function. This Agent will henceforth be referred to as Inner Agent. This is similar to how a manager offloads tasks to each worker, each of whom have their own experiences and skills to do the task. By having intelligent Inner Agents as the Equipped Function, the top-level agent (Meta Agent) will have greater processing capability. This is shown in Fig. 4.\nInfusing Shared Awareness. Each Equipped Function or Inner Agent would now be able to perform a subset of the entire task independently. However, they will need some shared context, as 1) the outcome of the subtask may influence other subtasks down the line, or 2) they may need input from earlier subtasks in order to perform their subtask. To solve this problem, we implement a Shared Memory amongst the Meta Agent, Equipped Function and Inner Agents. Notably, we have two types of Shared Memory, 1) Subtasks Completed and 2) Shared Variables. This is shown in Fig. 1."}, {"title": "TaskGen Overall Design Philosophy", "content": "TaskGen has three key design philosophies.\nFirstly, the output of each Agent or Equipped Function is made to be as concise as possible for minimal token use. This is done using StrictJSON. By ensuring a structured JSON output format with type checking, StrictJSON reduces verbosity typically associated with free-form text output in LLMs. This cuts down on latency and costs, and improves reliability of extracting output fields needed for downstream components. For a more in-depth run-through of StrictJSON, refer to Appendix A.\nSecondly, we map each subtask to exactly one Equipped Function or Inner Agent, so as to guarantee executability of the subtask. Unlike AutoGPT (Yang et al., 2023a), we ensure that there are no infinite loops when executing subtasks. This is done via the following design guidelines:\n1. An Agent can only call an Equipped Function or Inner Agent that is not above it in the hierarchy.\n2. Each Agent gets context relevant to its own processing abstraction space and are assigned Equipped Functions and Inner Agents suitable for that space."}, {"title": "The Core of TaskGen", "content": "At the core of TaskGen is the definition of an Agent, which consists of the following components:\n1. Agent Name: Name of the Agent\n2. Agent Description: Description of the Agent\n3. Equipped Functions: List of Equipped Functions and Inner Agents available to solve subtasks\n4. Assigned Task: Agent's assigned task\n5. Subtasks Completed: Python dictionary of past subtasks that Agent has done, which detail the Equipped Function's name and input parameters and their corresponding output\n6. Shared Variables: Python dictionary containing variables that will be shared between Equipped Functions and Agents\n7. Global Context: Additional context to the Agent that can reference persistent states, such as those in Shared Variables\n8. Memory Bank: Python dictionary containing various abstraction spaces of memory that will be retrieved via top-k retrieval via similarity to Assigned Task"}, {"title": "Imbuing Agentic Capabilities with Equipped Functions", "content": "By default, an Agent comes pre-built with a use_llm function, which uses an LLM with the Agent Name and Agent Description as context to perform a task, and an end_task function to end the current task. Additionally, we can assign Equipped Functions or Inner Agents to the Agent to imbue it additional capabilities.\nEquipped Functions come in two forms:\n1. Internal Functions use an LLM to do processing of input-output relations. They are useful for tasks that are difficult for traditional rule-based approaches to handle well, such as sentiment analysis and summarisation.\n2. External Functions utilise any Python function to do processing to get output, which makes it very easy for TaskGen to utilise functions from other agentic frameworks such as LangChain or CrewAI. They are suitable for tasks that can be called via fixed functions, or APIs, which guarantee reliability while imbuing additional functions to the LLM. As an aside, if we need a hybrid approach of rule-based fixed processes with flexibility of LLMs, an LLM can also be called within the External Function."}, {"title": "Choosing the Next Subtask", "content": "The core ability of an Agent is the ability to choose the correct next subtask to fulfil the Assigned Task. This is a non-trivial problem as it requires understanding of the Assigned Task, Agent Name, Agent Description, Subtasks Completed, relevant Memory, Equipped Functions and Inner Agents in order to make an informed decision.\nIn order to increase robustness in choosing the right Equipped Function and corresponding input parameters, we split it up into two steps.\nStep 1: Decide on subtask and corresponding Equipped Function / Inner Agent. The first step simply takes the available information to the Agent and does a Chain-of-Thought (CoT) (Wei et al., 2022) prompting to elicit reasoning via thoughts, leading to more accurate selection of subtask and the corresponding Equipped Function / Inner Agent in the following format:\n1. Observation: Reflect on what has been done in Subtasks Completed for Assigned Task\n2. Thoughts: Brainstorm how to complete remainder of Assigned Task only given Observation\n3. Current Subtask: What to do now in detail with all context provided that can be done by one Equipped Function for Assigned Task\n4. Equipped Function Name: Name of Equipped Function to use for Current Subtask\nStep 2: Decide on input parameters to Equipped Function / Inner Agent. Instead of providing the entire list of Equipped Functions / Inner Agents as per Step 1, we only give this step information of the exact Equipped Function / Inner Agent we have decided in Step 1, so as to encourage greater output specificity. We then generate the input parameters of the Equipped Function / Inner Agent given the Current Subtask and Equipped Function details (Equipped Function Name, Equipped Function Description, Equipped Function Input Parameter Description and type), and uses StrictJSON to ensure that the input parameters meet the type that is stated for in the Equipped Function. This ensures robustness and reliability for the input parameters."}, {"title": "Using TaskGen", "content": "Using TaskGen is extremely simple and is designed for any new user to learn it within 5 minutes. The steps needed are detailed as follows:\n1. Install TaskGen. \"pip install taskgen-ai\"\n2. Define LLM. This takes in a user prompt and system prompt as Python strings, and returns a Python string for the LLM generated response \"def llm(user_prompt: str, system_prompt: str) -> str\"\n3. Define Agent. Simply define an Agent class with the Agent Name, Agent Description \"agent = Agent(name, description, llm = llm)\"\n4. Equip Functions. Equip the Agent with Equipped Functions or Inner Agents to broaden the Agent's capabilities. \"agent.assign_functions([fn_1, fn_2])\"\n5. Run Agent. Run the Agent with a task \"agent.run(task)\"\n6. Query Agent. Query the Agent about Subtasks Completed \"agent.reply_user(query)\"\nFor an in-depth tutorial on how to use TaskGen, refer to Appendix B."}, {"title": "Benefits of TaskGen", "content": "The key philosophy of TaskGen is to be concise. This helps greatly with the performance of the overall system, as numerous studies (Xiong et al., 2023; Ding et al., 2024) have shown that an increase in context length generally leads to poorer performance on tasks referencing the context."}, {"title": "JSON is more concise than free text", "content": "Given a similar input prompt, asking the LLM to output in a JSON format generally gives much less verbose output as compared to free text. An example can be seen from from Fig. 5 for a prompt about the meaning of life. This is likely because the pre-training data of JSON on the web is more concise without much explanation, and the value of the field is very correlated to the key of the field. This means that we can use a JSON format to constrain the generation of the LLM to give the desired fields which we are interested in."}, {"title": "StrictJSON is more concise than JSON", "content": "TaskGen steers clear away from the typical JSON schema approach to define functions, which are used in many agentic frameworks adopting Pydantic as the JSON parser. This is because the JSON schema format is extremely verbose, and TaskGen using the StrictJSON schema is able to express the entire JSON schema of a function with much fewer tokens. As can be seen in Fig. 6, in order to express two parameters, the StrictJSON Schema uses 58 tokens compared to JSON Schema of 110 tokens, or about 53% the amount of tokens. The token savings are significant, and would be even more so with a lot more parameters."}, {"title": "Modular and robust components", "content": "TaskGen utilises a modular approach, where for each part of the system, be it Equipped Function or Inner Agent, we give it only the required context to do the task. This results in shorter context for LLM prompts, leading to better performance. Moreover, as we move from one subtask to the next, we split the process into multiple smaller chunks as required. For instance, when deciding what to do for the next subtask, we choose the Equipped Function / Inner Agent as one chunk, and choose the input parameters as another chunk. This again helps with reducing context length and cognitive load on each part of the process, and we can error check better at each part of the process."}, {"title": "Shared Memory", "content": "One of the key design philosophy of TaskGen is to share information only on a need-to-know basis. To that end, we utilise Shared Memory (see Fig. 7) to share information between the Agent and Equipped Function / Inner Agents.\nThere are two kinds of Shared Memory:\n1. Subtasks Completed. This is a Python dictionary which stores the outcome of each subtask. The dictionary key is the name of the Equipped Function / Inner Agent and its input parameters, the value is the function output. This past history of function inputs and outputs will be made known to all LLM-based components of the system to help with shared awareness. Do note that this differs from the traditional ReAct framework (Yao et al., 2022) in that we do not store the earlier Thoughts. We notice empirically that just having the Subtasks Completed in the form of function inputs and outputs is enough for the LLM to understand past history to make an informed decision, and at the same time results in reduced context length.\n2. Shared Variables. This is a Python dictionary which stores Python variables. These Python variables will be made available to the Agent and all Equipped Functions / Inner Agents upon request. The exact names and values of these Shared Variables will not be in the prompt to LLM calls by default, meaning that this information will not increase context length unless explicitly referred to. As such, we are able to store lengthy text output as well as filenames for various other modalities for suitable pre-processing when needed later on. The Equipped Functions / Inner Agents are also allowed to modify these Shared Variables, and as such can directly update the Shared Memory whenever needed."}, {"title": "Global Context", "content": "Global Context augments the default LLM prompt for the Agent. We use Global Context to expose certain persistent variables, typically stored in Shared Variables, which we want to carry through the task / carry across tasks. This is very useful for letting the Agent know the current state in a dynamically changing environment. Global Context can also contain more specific instructions for the LLM beyond the defaults in TaskGen."}, {"title": "Memory Bank", "content": "The Memory Bank contains all the important information that an Agent might need to know for an arbitrary task. We posit that a generic problem solver will need to contain memory at multiple forms of abstraction. For instance, when given a piece of text, we can store the 1) summary of it, 2) extracted entities and relationships in a knowledge graph, 3) entire text. These information will be useful when we are doing 1) generic question and answer, 2) causal reasoning, 3) specific question and answer respectively. If we just store information at one form of abstraction only (e.g. summary), some tasks will be significantly harder or impossible (e.g. find out specific details in text).\nTask-Augmented Prompt. When given a task, we extract out the relevant memories using RAG or other semantic matching algorithms. This will be used to augment the LLM prompt when selecting the next subtask and using the use_llm function.\nEquipped Function Filtering by Task. Furthermore, when given a task, not all Equipped Functions/Inner Agents are relevant, so we can filter them by semantic similarity to the task. This will help improve LLM performance provided that the correct functions are kept."}, {"title": "Other Notable Features", "content": "Conversable Agent. TaskGen provides a wrapper for a two-person chat interface with the Agent, where the Agent can use its Equipped Functions to perform actions and then reply the User.\nCode Generator. TaskGen has an in-built code generator and code corrector, which can also be used to perform actions with Python code, similar to CodeAct. (Wang et al., 2024)\nAsynchronous Mode. TaskGen has asynchronous equivalents of strict_json, Function and Agent classes for faster asynchronous processing.\nCommunity Contributions. TaskGen has a community space where users can easily upload and download Agents (see Appendix C)."}, {"title": "Evaluation", "content": "We evaluate TaskGen on various environments to showcase its versatility: dynamic maze navigation (see Appendix D), escape room solving in TextWorld (see Appendix E), web browsing (see Appendix F), MATH dataset (see Appendix G), RAG-based Question Answering (QA) on NaturalQuestions dataset (see Appendix H)."}, {"title": "Results", "content": "Overall, TaskGen works well for generic environments. The summarised results for each environment are as follows:\n1. Dynamic Maze Navigation. We implement a 40x40 maze with obstacles that change halfway during the Agent's learning, similar to Learning, Fast and Slow (Tan and Motani, 2023). TaskGen with Global Context and an external StrictJSON Planner manages to solve 100% of the episodes on the first try, even after environment changes.\n2. Escape Room Solving in TextWorld. We used TaskGen as a generic interactive fiction player to solve TextWorld (C\u00f4t\u00e9 et al., 2019) challenges. Where dense rewards and detailed goals were provided, TaskGen achieved a 96% solve rate, outperforming a neural-network agent's (C\u00f4t\u00e9, 2024) solve rate of 88%. Where commands are not provided and needed to be derived by the agent, TaskGen achieved an 88% solve rate, outperforming the baseline LLM's 57%.\n3. Web-Browsing Agents. We designed a series of tasks requiring agents to navigate and extract information from the web, simulating real-world scenarios where users need to find specific information across various websites. Tasks included searching for academic studies, gathering news headlines, summarising market trends, and exploring educational resources. The agent demonstrated varying levels of success across different tasks, with 69% of actions being completed successfully.\n4. MATH Dataset. We randomly selected 20 problems from the test set of 5 categories (Algebra, Pre-Algebra, Intermediate Algebra, Number Theory, and Counting and Probability) of the MATH dataset (Hendrycks et al., 2021). Our experiments (see Appendix G) showed that the TaskGen Agent with Equipped Functions achieved an average accuracy of 71% on challenging Level-5 problems, compared to 44% accuracy for the Agent without these functions. This demonstrates that imbuing an Agent with code generation and debugging capabilities significantly improves problem-solving.\n5. RAG-based QA on NaturalQuestions. On the Natural Questions dataset (Kwiatkowski et al., 2019), TaskGen with Equipped Functions for dynamic retrieval and answering (we term this Interactive Retrieval) outperformed the baseline LLM with RAG across all metrics (see Appendix H). Compared to the baseline LLM, Interactive Retrieval achieved an F1 Score of 47.03% (+5.49%), precision of 40.75% (+7.43%), and recall of 55.59% (+0.42%), demonstrating TaskGen's effectiveness in dynamically refining context for more accurate question answering."}, {"title": "Conclusion and Future Work", "content": "TaskGen is already used in production at Simbian AI, and we would like to share its benefits with others. TaskGen's approach of not using conversation, but instead focusing directly on solving the task is a marked improvement over most existing agentic frameworks. TaskGen will continue to be actively developed over the coming years. The future work includes: 1) better planning abilities using state-based graphs, parallel searching, 2) multiple memory abstraction spaces such as vector databases and knowledge graphs, 3) reflection as a way to consolidate experiences and use for future decision making, 4) extended multi-modal support and 5) multiple agents with different skills and biases collaborating with one another.\nTowards Hybrid Workflows. As demonstrated by systems like AGENTless (Xia et al., 2024), full end-to-end agentic workflows may not always provide the best performance, as we may want to fix parts of the processes without Agents if we already know what needs to be done. This mixture of fixed processes and flexible agentic process selection will form the core tenet of future agentic systems. While not featured in native TaskGen, such hybrid systems can be implemented by using StrictJSON or fixed rules for dynamic routing over TaskGen Agents. We will explore more of such approaches and incorporate key elements into TaskGen."}, {"title": "Build Together With Us", "content": "TaskGen is an actively developing framework, and we would love to seek your inputs / contributions / feedback. Build together with us via our GitHub (https://github.com/simbianai/taskgen), and join the discussion group at Discord (https://discord.com/invite/bzp87AHJy5)."}, {"title": "Experiment Details", "content": "For more information on the following experiments in the Appendix, do contact the following:\n1. Community Contributions (see Appendix C) - Hardik (hardik121998@gmail.com)\n2. Dynamic Maze Navigation (see Appendix D) - John Tan Chong Min\n3. TextWorld (see Appendix E) Richard Cottrill\n4. Web-Browsing Agents (see Appendix F) - Brian Lim Yi Sheng\n5. MATH Dataset (see Appendix G) - Bharat Runwal (bharat.runwal@simbian.ai)\n6. NaturalQuestions QA (see Appendix H) - Prince Saroj"}, {"title": "Limitations", "content": "The experiments conducted in this paper are not extensive for all available LLMs. We mainly use OpenAI's \"gpt-4o\" and \"gpt-3.5-turbo\". That said, we have also empirically tested and verified, though not shown here, that TaskGen works with other LLMs such as OpenAI's \"gpt-40-mini\", Llama-3 8B and Claude-3 Haiku."}, {"title": "Acknowledgements", "content": "The research is supported by Simbian AI, where it is used in the core products. This research is also supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-01-003[T]) and by A*STAR, CISCO Systems (USA) Pte. Ltd and National University of Singapore under its Cisco-NUS Accelerated Digital Economy Corporate Laboratory (Award I21001E0002)."}, {"title": "StrictJSON Details", "content": "StrictJSON is a library created in order to parse LLM output into a structured JSON format, and is used for all LLM calls in TaskGen. This enables efficient extraction of LLM output based on the JSON keys and enables interfacing the LLM as part of a larger system, such as the agentic framework in TaskGen. Furthermore, StrictJSON comes in-built with rule-based type checking which increases output reliability. StrictJSON also has error checking capabilities, which uses the JSON parsing errors or type checking errors to feed into the LLM in an iterative feedback loop as an error message to regenerate the JSON again. This is similar to the error feedback mechanism in Voyager (Wang et al., 2023).\nComparison with json.loads(): Typically, in order to parse JSON string into a dictionary, the function json.loads() is called. This is not robust to variations of the JSON and can easily fail to parse incorrectly formatted JSON, especially when generating code. StrictJSON is more robust, as it adds a delimiter before and after the key which the regex uses to extract. This regex will still work even if the quotation marks are not closed properly or are missing within the string. See Section A.2 for more details.\nWhy not YAML? YAML could also potentially be the format for LLM outputs in order to reduce token counts. However, YAML formatting performance has been empirically tested to be poorer than JSON, at least on the GPT models. We posit that this is because current LLMs are extensively trained on web data, of which JSON is more prevalent than YAML since it is the earlier format to be used. This may change as more web data is of YAML format. For now, JSON format is used to get a reliable system working.\nThis appendix details how to use StrictJSON based on TaskGen v3.2.0."}, {"title": "Usage", "content": "Example LLM Definition\nTo use StrictJSON, we firstly need to have an LLM available in order to generate the JSON from the text input given. Fig. A1 illustrates an example LLM function (named llm) that can be interfaced with StrictJSON. It takes as input the system prompt, which is the overall system message for the LLM, as well as the user prompt, which is what the user typically enters into the LLM for a response. This returns an LLM model response in the form of a string, which is the output of this LLM function. By exposing the entire LLM function to the user, StrictJSON is extremely versatile and can operate with both API-based LLM models and local models."}, {"title": "Example Usage", "content": "In order to use StrictJSON to process the LLM's output, we simply use the strict_json function. We give it the system prompt, user prompt, and the output format in a dictionary format with keys being the field name and values being the description of the field. For instance, Fig. A2 illustrates how to use StrictJSON to classify a sentence in the user prompt. As can be seen, StrictJSON processes the type of sentiment, an array of adjectives in the sentence, and the number of words all in the same function call."}, {"title": "Advanced Usage of StrictJSON for code", "content": "StrictJSON is also able to process code reliably, as shown in Fig. A3."}, {"title": "Type Checking in StrictJSON", "content": "StrictJSON also supports type checking of the following types: int, float, str, dict, list, array, code, bool, Dict[], List[], Array[], Enum[]. If there is a [], you can nest datatypes within it such as List[int] for a list of integers. Only Dict[] cannot be nested, and Dict[dictionary_keys] is used instead to enforce the presence of the dictionary_keys within the dictionary. Fig. A4 illustrates how to use StrictJSON with type checking. This can ensure greater output specificity and greater reliability for downstream tasks."}, {"title": "How it works under the hood", "content": "StrictJSON creates a prompt to the LLM to output JSON in a specified format using delimiters to enclose the output keys, that is more reliable to extract with regex as compared to unmodified keys of JSON. This is because the unmodified keys are just words with quotation marks, like \"Sentiment\", which may appear in other parts of the JSON and confuse the regex extraction.\nFig. A5 demonstrates how to visualise the actual LLM system and user prompt using verbose = True as a parameter to strict_json. We can see that we get the LLM to enclose keys with delimiters (default '###'), and enclose the JSON values with <>, which the LLM will be instructed to update.\nThe regex that is used to parse the LLM output can be seen in Fig. A6. By extracting keys of the form '###{key}###' or \"###{key}###\", we can extract and parse the JSON even when there are mismatched quotation marks, unclosed brackets, and many other issues that will cause json.loads() to fail."}, {"title": "TaskGen Details", "content": "This appendix details the various modules of TaskGen and how to use them based on TaskGen v3.2.0."}, {"title": "Initialising TaskGen", "content": "Fig. B1 shows how to initialise TaskGen. Here, we use \"gpt-4o\", but TaskGen can also work with \"gpt-3.5-turbo\" or equivalent LLM models at the cost of lower performance.\nThere three steps are:\n1. Install TaskGen\n2. Import required functions and setup relevant API keys for your LLM\n3. Define your own LLM, which takes in a system prompt and user prompt and outputs the response string from the LLM"}, {"title": "TaskGen Agent Overview", "content": "Fig. B2 shows how to initialise the Agent.\nWe firstly define the functions for the Agent.\nThis can be of the form of an Internal Functions using Function class, which takes in the function description and output format of the function. We denote the variables in function description via <> enclosing the variable name. The output format is in the style of StrictJSON's output format. The Internal Function uses LLM to process the function, leading to very flexible functions that rule-based solutions may not allow for.\nFunctions can also be of the form of an External Function, which is very flexible as it is just a Python function. We simply define the function with typing for inputs and outputs, and with a docstring that contains the input parameter names. If any of the typing or docstring is missing, we will omit them from the function description, but the External Function can still work. External Functions allow for both rule-based rigidity and LLM-based flexibility, as an LLM call can be made inside the External Function as well.\nAfter defining our Functions, we define our Agent by calling Agent(name, description, llm). Thereafter, we proceed to assign our functions via assign_functions.\nTo see how the functions look like, we can also use print_functions to visualise it. Notice that the functions just consists of Name, Description, Input and Output fields, which is much shorter than the JSON schema or Pydantic way of defining a function."}, {"title": "Running the Agent", "content": "Fig. B3 shows how to assign a task and run the Agent by simply calling run(task). Notice how we can visualise the output via Observation, Thoughts, Action (Subtask) in the traditional ReAct framework. The difference between TaskGen and the original ReAct framework is that the observation here is actually the observation of the Subtasks Completed instead of the Observation of the function's output. By structuring Observation this way, this helps to provide a summary of what has been done so far, which aids in decision making.\nWe also do not store these Observation and Thoughts as they are just used in decision making at that point of time, but not needed in the longer term. The entire history of what has been done is stored in Subtasks Completed, which can be visualised via status() or via the subtasks_completed variable of the agent. Notice also that calling status() also gives us the Agent's details, such as Agent Name, Agent Description, Equipped Functions, Shared Variable Names, Assigned Task, Subtasks Completed, and whether the task is completed. We can call status() anytime to check on how the Agent is performing."}, {"title": "Querying the Agent", "content": "Fig. B4 shows how we can reply the user by simply calling reply_user() to get the Agent to reply based on what has been done in Subtasks Completed. If reply_user() is called without any query parameter, it will reply based on the assigned task. If there is a query parameter given, then it will reply based on the query.\nThis functions as a simple question answer bot, from which we can ask multiple questions about what the Agent has done so far and reply the user."}, {"title": "Asynchronous Agents", "content": "We can perform whatever we did for the Agent in asynchronous mode too. Such an asynchronous runtime has advantages in that we can run multiple Agents in a shorter time, as we can effectively let other Agents run in the downtime of one Agent.\nTaskGen has two main classes - Agent and Function. Their asynchronous equivalents are AsyncAgent and AsyncFunction. Furthermore, the asynchronous version of strict_json is strict_json_async.\nFig. B5 shows how to initialise the asynchronous LLM. Simply define a function that takes in a system prompt and user prompt, and outputs the response string of the LLM operating in asynchronous mode."}, {"title": "Meta Agent", "content": "Sometimes, due to task complexity, we would like to assign our Agent another Agent as an Equipped Function. Henceforth, our main Agent will be termed the Meta Agent, and the Agent equipped to it be termed the Inner Agent."}, {"title": "Initialising the Meta Agent", "content": "Fig. B8 show how to initialise the Meta Agent. It is generally the same process as initialising functions to the Agent, except that this function is of class \"Agent\". Note that we can specify how each Inner Agent should behave, including the max_subtasks it should run for and what LLM it should use.\nThe Inner Agents will have full access to the Subtasks Completed and Shared Variables of the Meta Agent, and all the Equipped Functions of the Inner Agents will have access to these as well. This helps ensure that the context of the Meta Agent is fed downwards to the Inner Agents, and the Inner Agents can also change the Shared Memory of the Meta Agent."}, {"title": "Running the Meta Agent", "content": "Figs. B9, B10 and B11 show the process of running the Meta Agent by simply calling run() and showcase the responses of the respective Creative Writer, Chef, Economist Inner Agents.\nNotice that if we call the Inner Agent as the function, we will generally repeat the Observation, Thoughts, Action (Subtask Identified) loop at the Inner Agent level. This kind of recursiveness helps to make the implementation of the Inner Agent easy, and we can stack as many Inner Agents as we would like to scale up the system.\nWe give the Inner Agent the full awareness of the Meta Agent's Assigned Task, Subtasks Completed and Shared Variables. When the Inner Agent ends the subtask, it does not give all the information back to the Meta Agent, but instead call reply_user() to consolidate important information to put into Subtasks Completed (reply text shown in magenta). This helps to minimise the information stored in Shared Memory, which helps to reduce the overall context length, as many details done by the Inner Agent do not need to be known by the Meta Agent.\nThe Agents should generally be given context and Equipped Functions appropriate for their level of processing. In practice, such a hierarchical structure of Agents help with decomposing a complex problem into bite-sized bits, with the Agents at the higher levels focusing on the broader picture, while the Agents at the lower levels will do more of the specific details needed. This structure can be used to do most tasks that have such a hierarchical"}]}]}