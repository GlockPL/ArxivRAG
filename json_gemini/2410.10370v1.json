{"title": "INNOVATIVE THINKING, INFINITE HUMOR: HUMOR\nRESEARCH OF LARGE LANGUAGE MODELS THROUGH\nSTRUCTURED THOUGHT LEAPS", "authors": ["Han Wang", "Yilin Zhao", "Dian Li", "Xiaohan Wang", "Gang Liu", "Xuguang Lan", "Hui Wang"], "abstract": "Humor is a culturally nuanced aspect of human language that presents challenges\nfor understanding and generation, requiring participants to possess good creativ-\nity and strong associative thinking. Similar to reasoning tasks like solving math\nproblems, humor generation requires continuous reflection and revision to foster\ncreative thinking, rather than relying on a sudden flash of inspiration like Cre-\native Leap-of-Thought (CLoT) paradigm. Although GPT-01 can realize the ability\nof remote association generation, this paradigm fails to generate humor content.\nTherefore, in this paper, we propose a systematic way of thinking about generat-\ning humor and based on it, we built Creative Leap of Structured Thought (CLOST)\nframe. First, a reward model is necessary achieve the purpose of being able to cor-\nrect errors, since there is currently no expert model of humor and a usable rule to\ndetermine whether a piece of content is humorous. Judgement-oriented instruc-\ntions are designed to improve the capability of a model, and we also propose\nan open-domain instruction evolutionary method to fully unleash the potential.\nThen, through reinforcement learning, the model learns to hone its rationales of\nthe thought chain and refine the strategies it uses. Thus, it learns to recognize and\ncorrect its mistakes, and finally generate the most humorous and creative answer.\nThese findings deepen our understanding of the creative capabilities of LLMs and\nprovide ways to enhance LLMs' creative abilities for cross-domain innovative ap-\nplications.", "sections": [{"title": "1 INTRODUCTION", "content": "Currently, humor is becoming increasingly prevalent because it not only alleviates stress and en-\nhances interpersonal relationships but also brings a sense of ease and optimism in complex envi-\nronments. The content of humor usually involves providing a creative answer or explanation to a\nspecific question or phenomenon. However, even for humans, generating novel ideas is very chal-\nlenging, primarily due to difficulties in creative association, making leaps in thinking, understanding\npunchlines, and determining whether they are humorous.\nThe Creative Leap of Thought (CLoT) paradigm, as proposed by Zhong et al. (2024), extends the\nLeap-of-Thought (LoT) paradigm (Bai et al., 2022; Kim et al., 2023) to enhance the model's judge-\nment ability and the connections between concepts by incorporating a seemingly unrelated concept\ninto the input. This approach aims to boosting the creative thinking and the ability to make leaps\nin thought in Large Language Models (LLMs). However, similar to how a human may think for a\nlong time before responding to a difficult question and methods of reasoning tasks like solving math\nproblems (Lightman et al., 2023), a flash of inspiration for humor generation like Creative Leap-of-"}, {"title": "2 METHOD", "content": "2.1 PROBLEM FORMULATION\nIn general, the knowledge graph G is defined as a set of triples G = {(e, r, e') | e, e' \u2208 E, r \u2208 R},\nwhere & is the set of entities and R is the set of relations. Each triple represents a relation r from\nthe head entity e to the tail entity e' (Sun et al., 2023a; Yang et al., 2023). In the special application\nof humor generation, we consider a knowledge graph composed of question-related entities Eq and\nanswer-related entities Ea, where the intersection Ez = EqNEa is regarded as confounding variables\n(we refer to it here as confounding entities). Assuming the existence of causal relationships Ez \u2192 Eq\nand Ez \u2192 Ea, we use the intervention formula P(Ea | do(Eq)) = \u2211ezeEz P(Ea | Eq, Ez =\nez).P(Ez = ez) to show how to use confounding entities Ez to get an answer. Our goal is to utilize\nthis causal relationship to generate creative and logical answers, while noting that the confounding\nentities Ez are interconnected with Eq and Ea through the relations Rqa such as example in Figure\n12 in Appendix A.2.\nThe overall training framework is illustrated in Figure 4. In the first stage, we train a model with\njudgement and divergent thinking abilities using human-designed and automatically evolving in-\nstructions. In the second stage, we employ this model as a humor judgement expert as well as an\nexpert teaching rationales to generate online preference data for DPO training.\n2.2 ASSOCIATIVE AUTOMATIC INSTRUCTION EXPANSION\nIn this section, we mainly introduce the design of the discrimination template and how to design\nopen-domain instructions to unleash the potential of LLMs. (Luo et al., 2023; Zhu et al., 2023; Xu\net al., 2023)."}, {"title": "Associative Automatic Instruction Evolution.", "content": "Given a dataset D = {(q,a)}=0, where qf is a question, af is a funny answer, and I\n= {io, yo, eo}, where io is the instruction, yo is the reply based on io and eo is the initial rule to\nmake instructions complicated. As shown in Figure 3, this framework involves three agents, each\nplaying the roles of rewriter, imaginator, and analyst. Here's how the process works:\n1.Rewriter Agent: Given a conversation (qk, ak) and an instruction io, such as \"What is the punchline\nof this response?\", the rewriter agent transforms it into a more complex version i\u2081 based on an initial\nrule eo (see Appendix A.1). And then the agent provides a reply y1 based on the complex version of\nthe instruction.\n2.Imaginator Agent: The imaginator agent then takes the complex instruction i\u2081 and the reply y\u2081 to\nimagine a new conversation (q1, \u03b11).\n3.Analyst Agent: Finally, both the original conversation (q, a) and the imagined conversation\n(q, a) are sent to the analyst agent. The analyst evaluates \"whether the two conversations express\nthe same viewpoint or punchline?\". If they do, the interaction between the agents ends.\nIf not, the analyst applies a second criterion: \"Is the instruction i\u2081 more complex than the ini-\ntial instruction io?\" If the answer is \"yes,\" the original conversation is replaced with the imag-\nined conversation and sent back to the rewriter, allowing the three agents to continue their in-\nteraction based on the imagined conversation. If the answer is \"no,\" the analyst agent formu-\nlates a more complex rule e\u2081 and sends it to the rewriter. This process continues until the\nimagined and conversations in last round express different viewpoint or punchline, or until a\nmaximum number of communication rounds me is reached. Finally, the system outputs D'\n= {(qk, ak, io, yo, ..., qmk, amk, imk, Ymk)}mk\u00d7N, where mk is the maximum number of communi-\ncation rounds between the three agents.\nThis iterative process ensures that instructions become increasingly complex, as shown in Ap-\npendix A.1, and that the model is trained on a diverse set of scenarios. This enhances its ability\nto handle a wide range of conversational topics and complexities. Additionally, the process covers\nconversational content across different knowledge domains, thereby expanding the entities within\nthe large language model's knowledge graph. Finally, we use these data to train a LoRA model\nnamed J."}, {"title": "2.3 GUIDED EXPLORATIVE SELF-IMPROVEMENT TUNING", "content": "In the previous section, we developed model J, which possesses judgement and divergent capabil-\nities. To enhance the graph's accuracy and interpretability, we incorporate additional causal prin-\nciples through online Direct Preference Optimization (DPO) (Rafailov et al., 2024). By increasing\nthe number of entities and explicitly modeling the causal relationships (Rcause\u2286 Rqa), the model\ngains a deeper understanding of how changes in one entity influence others."}, {"title": "3 EXPERIMENTS", "content": "In this section, we first construct a humor-\nrelated dataset in both Chinese and English, or-\nganize it into an appropriate format, and eval-\nuate the performance of our method by com-\nparing it with multiple large language mod-\nels (LLMs) through various validation experi-\nments."}, {"title": "3.1 DATASET AND TASK", "content": "Datasets. Currently, there is limited data on humor-focused question-answer pairs. To enrich the\nEnglish dataset, we collected various humor-related datasets, including short jokes and headline\nwordplays. We compiled English humor data from three sources\u2014Oogiri-GO, SemEval 2020, and\nSemEval 2021-and organized them into the required format as described below. For Chinese data,\ndue to the scarcity of relevant datasets beyond Oogiri-GO, we supplemented the training and testing\nsets with internally sourced data."}, {"title": "3.1.1 IMPLEMENTATION DETAILS", "content": "Our model is fine-tuned based on QWEN1.5-32B-Chat Bai et al. (2023) with fine-tuning method\nLORA Hu et al. (2021) on 8 A100 GPUs. For the first stage, we train the model on the 95% of\ndataset mentioned above for 6 epochs with AdamW optimizer and the learning rate of 3e - 4. In the\nsecond stage, 5% of the dataset are used to train GESIT for 3 epochs with AdamW optimizer and\nthe learning rate of 2e - 4. The models are tested in the tasks introduced in previous part."}, {"title": "3.2 RESULTS ANALYSIS", "content": "Evaluation by Choice Questions in English Tasks. The top-1 accuracy of completing each se-\nlection task and show the performance of several models in Table 1. Overall, compared with open-\nsource language models including LLAMA3 and QWEN, the state-of-the-art closed-source large\nlanguage models show impressive zero-shot performance on humor discrimination and rank tasks.\nBy training on the English task instruction data designed by us, our model CLOST has a significant\nimprovement compared with other models (such as LLAMA3-70B and GPT-40) (with an average\naccuracy in diverse English benchmark increase of 4.55% and 5.91%).\nEvaluation by Choice Questions in Chinese Tasks.\nWe also evaluate the accuracy rate (acc%) of completing each selection task in Chinese and show\nthe performance of several models in Table 2. Overall, compared with open-source language mod-"}, {"title": "3.3 RELATED WORKS", "content": "Large Language Models (LLMs) and Their Creativity. Recently, language models (Bai et al.,\n2023; Wang et al., 2023; Liu et al., 2024; Chen et al., 2023) have garnered widespread attention\ndue to their impressive reasoning capabilities (Wang et al., 2023; Saparov & He, 2022; Zeng et al.,\n2022; Driess et al., 2023; Dong et al., 2023; Ye et al., 2023; Liang et al., 2024). Additionally, an\nincreasing number of studies are focusing on exploring the creativity of LLMs (Ling et al., 2023;\nSummers-Stay et al., 2023; Sun et al., 2023b; Bhavya et al., 2023), with applications in fields such\nas scientific discovery (Park et al., 2023; Kang et al., 2022; Hope et al., 2022; Liang et al., 2022;\nHuang et al., 2023) and creative writing (Swanson et al., 2021; Chakrabarty et al., 2022; Wu et al.,\n2022; Mirowski et al., 2023; Dang et al., 2023).\nComputational humor is a branch of computational linguistics and artificial intelligence that uti-\nlizes computers to study humor (Binsted et al., 2006). It encompasses various tasks, including hu-\nmor discrimination (Shahaf et al., 2015; Tanaka et al., 2022; Xu et al., 2022; Chen & Zhang, 2022;\nKumar et al., 2022; Wu et al., 2021; Ofer & Shahaf, 2022; Xie et al., 2023; Meaney et al., 2021;\nHossain et al., 2020a), humor interpretation (Hwang & Shwartz, 2023; Evans et al., 2019; V\u00e1squez\n& Aslan, 2021), and humor generation (Amin & Burghardt, 2020; Zhang et al., 2020; Hossain et al.,\n2020b; Valitutti et al., 2013; Chaudhary et al., 2021). With the advancements in generative LLMs,\nhumor generation has become a hot focus. However, humor generation still faces challenges such\nas insufficient punchlines (Popova & Dadic, 2023).\nChain-of-thought prompting methods provide models with a \"chain of thought\" (Kojima et al.,\n2022; Wei et al., 2022; Zhang et al., 2022; Yao et al., 2024; Long, 2023), which are reasoning\nexamples (Wei et al., 2022), or a simple prompt like \"let's think step by step\" (Kojima et al., 2022),\nto encourage LLMs to engage in reasoning rather than just providing direct answers (Huang &\nChang, 2022).\nInstruction tuning has emerged as a key strategy for unlocking the potential of large language\nmodels (LLMs). By curating high-quality datasets, we can more effectively align these models with\ndesired behaviors (Zhou et al., 2024). However, the challenge of expanding high-quality instruction\ndata remains a central research focus. Some researchers prioritize human annotation to create in-\nstruction data, as seen in projects like OpenAssistant (K\u00f6pf et al., 2024). (Zeng et al., 2024; Xu et al.,\n2023) explore more efficient methods such as iterative approach for optimizing instruction-following\ndata, generating more complex and diverse datasets, to surpass the quality limits of existing datasets\nin math and code problems. Since the content of the conversation varies widely, it is difficult to\napply uniform rules to complicate the explanation. Therefore, the above approach may not work.\nPreference Optimization. Aligning instruction-tuned language models with preference data has\nbeen shown to improve performance in both proprietary (Ouyang et al., 2022; Touvron et al., 2023)\nand open-source models. To facilitate further research, several preference datasets have been re-\nleased recently (Bai et al., 2022; Cui et al., 2023). Algorithmically, simpler variants of standard\nReinforcement Learning from Human Feedback (RLHF) have been proposed, such as reward-free\nmethods that directly optimize using initial preference data (Rafailov et al., 2024; Azar et al., 2024;\nEthayarajh et al., 2024) and iterative updates of preference data with gold annotators (Guo et al.,\n2024; Touvron et al., 2023). Alternative annotation strategies include using learned reward models\n(Zhao et al., 2023; Liu et al., 2023; Calandriello et al., 2024) or leveraging the policy model itself for\npreference labeling (Lee et al., 2024; Yuan et al., 2024). (Ahmadian et al., 2024) studied more stable\nreinforcement learning algorithms. However, these methods either ignore the distribution shift of\npolicy models and its impact on reward model performance or rely entirely on expensive gold anno-\ntations during training. Our work bridges this gap by introducing a cost-effective method to prevent\nreward model degradation during policy training."}, {"title": "3.4 CONCLUSION", "content": "In this paper, we propose the Creative Leap of Structured Thought (CLOST) method to enhance the\ngeneration capabilities of large language models (LLMs). CLOST begins with transforming humor\ndatasets into instruction-tuning data to train LLMs, thereby improving their LoT and judgement abil-\nities. Subsequently, CLoST employs Guided Explorative Self-Improvement, which enables LLMs to\ngenerate more creative structured thought data through an understanding of rationales and to select\nhigh-quality data for self-refinement training. Experimental results demonstrate the effectiveness\nand generalization ability of CLOST across various tasks, including witty response generation and\nhumor discrimination."}, {"title": "A APPENDIX", "content": "A.1 DETAILED IN AAIE\nTo illustrate the challenges encountered during the data evolution process, we designed an initial\ninstruction (Figure 9 Turn 0) for empirical analysis. This instruction was subsequently evolved us-\ning the initial evolution method (Figure 7), resulting in the evolved instructions shown in Figure 9\nTurn n. Our analysis identified several shortcomings of the initial evolution method. Specifically, it\ndoes not fully account for the inherent complexity of the evolved instructions, leading to incomplete\nanswers that lack necessary background information and fail to consider factors such as social cul-\nture and psychological theory. Consequently, the problem comprehension in the answers remains\nsuperficial, hindering genuine divergent thinking.\nTo address these issues, we developed an improved evolution method through Analytic Agent anal-\nysis, as illustrated in Figure 8. In this figure, the blue elements represent the evolution method after\nthe first correction, and the red elements represent the method after the second correction. This en-\nhanced approach ensures more comprehensive and contextually aware responses, thereby fostering\ndeeper understanding and more effective divergent thinking."}, {"title": "A.2 TEMPLATE IN GESIT", "content": "Inspired by (Vashishtha et al., 2023), to guide experts to output causality, we designed a template as\nshown in Figure 10. The output of the expert is stricted as Figure 11."}, {"title": "A.3 GENERATION SHOWCASE", "content": "We found some posts online and used the Algorithm 2 to output the most funny response. The\nshowcases are shown in Figure 14"}]}