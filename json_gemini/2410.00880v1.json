{"title": "GEMS: Generative Expert Metric System through Iterative Prompt Priming", "authors": ["TI-CHUNG CHENG", "CARMEN BADEA", "CHRISTIAN BIRD", "THOMAS ZIMMERMANN", "ROBERT DELINE", "NICOLE FORSGREN", "DENAE FORD"], "abstract": "Across domains, metrics and measurements are fundamental to identifying challenges, informing decisions, and resolving conflicts.\nDespite the abundance of data available in this information age, not only can it be challenging for a single expert to work across\nmulti-disciplinary data [22], but non-experts can also find it unintuitive to create effective measures or transform theories into\ncontext-specific metrics that are chosen appropriately [11]. This technical report addresses this challenge by examining software\ncommunities within large software corporations, where different measures are used as proxies to locate counterparts within the\norganization to transfer tacit knowledge. We propose a prompt-engineering framework inspired by neural activities, demonstrating that\ngenerative models can extract and summarize theories and perform basic reasoning, thereby transforming concepts into context-aware\nmetrics to support software communities given software repository data. While this research zoomed in on software communities, we\nbelieve the framework's applicability extends across various fields, showcasing expert-theory-inspired metrics that aid in triaging\ncomplex challenges.", "sections": [{"title": "1 INTRODUCTION", "content": "Across the industry, many engineering teams express a desire to understand and improve their software practices [27].\nHowever, consistently and accurately measuring software engineering has remained elusive [8]. Measuring engineering's\nprocesses, artifacts, collaborations, and even impact can be challenging. Yet attempts to create measures of software\nengineering have always been around: most computer science students have seen the picture of Margaret Hamilton\nnext to a stack of paper that stands taller than her, containing the printed code she wrote for the Apollo program. While\nthese stacks of paper do not represent the impact of her work, they are some symbols of her work (the authors maintain\nthat lines of code, or reams of code printouts, are not good metrics for measuring coding output).\nMany engineering teams and organizations struggle to understand and improve their software practices. When faced\nwith questions about impact or collaboration, what data should be used? When faced with questions about software,\nmany engineers resort to data that is easily available and instrumented in their systems, for example, pull requests and\ncommits (e.g., [4]). While these data can be useful in many cases, they are not always suitable proxies. For example, the\nnumber of pull requests can miss the complexity or difficulty of the work done; in another example, the best solution\nmay actually delete code, which would reflect \"negative\" productivity if simply counting lines of code. In our experience,\nwe have seen practitioners make these mistakes quite often, reaching for available data rather than taking a thoughtful\napproach to operationalizing the concept they are trying to measure and test. This is understandable! They are trained"}, {"title": "2 PROBLEM DEFINITION", "content": "We begin by splitting the research question: \"How can we leverage Large Foundation Models (LFMs) to match\nteams within software corporations to improve developer team performance?\" into three key components: (a)\nenhancing developer team performance, (b) team matching, and (c) the application of LFMs.\nTo address the improvement of developer team performance, we referenced prior literature covering decades of\nresearch which highlights challenges such as individual information needs during software engineering tasks [18],\ncommunication issues within and between software teams, and the complexities of navigating software systems [24].\nFurthermore, efforts to define holistic measures for assessing developer productivity are noted [8]. Despite the valuable\ndirections provided by these studies, the development of tailored solutions for specific team challenges remains\nprohibitively expensive. We define a high-level software engineering challenge as C, which may be so ambiguous due\nto the broad nature of the challenge that an individual would not be able to come up with solutions without in-depth\nresearch into its specifics. We introduce a goal metric (Mg) as a proxy for optimizing the resolution or mitigation of C.\nFor the purpose of our system prototype, we limit C to a single Mg, while acknowledging that addressing C fully may\nrequire improving multiple goal metrics.\nUnderstanding that numerous factors might influence Mg, we identify a set of supporting metrics (Ms) believed to\nimpact Mg indirectly or directly. It's crucial to recognize Mg and Ms not as definitive metrics, but as proxies for better\nmeasurement and quantification of the targeted metrics.\nIn our efforts to improve developer team productivity, we adopt the communities of practice model advocated by Lave\n[20], which fosters regular interaction among individuals with a common interest or concern. This approach proves\nbeneficial, particularly when addressing challenges that stem from institutional or tacit knowledge, which are not\neasily conveyed through documentation, or widely understood by general experts. We propose not just a match but a\npartnership between two teams: Tx, requiring support to address C, and Ty, a team that is deemed capable of providing\nsupport and improvement of Mg for team Tx.\nFurthermore, we aim to harness LFMs for synthesizing and utilizing a broad spectrum of text-based information to\ndevelop metrics covering Mg and the set of Ms that helps identify Tx and Ty. This text-based information includes but\nis not limited to: code artifacts, system documentation, markdown, git commits, and measures of interactivity. With\nthe advancements in LFMs [6], we leverage their contextual understanding, built-in logical reasoning capabilities, and\nability to develop code. The LFM-based prototype system we developed is denoted as Generative Expert Metric System\n(GEMS)."}, {"title": "3 APPROACH AND SYSTEM DESIGN", "content": "In this technical report, we developed a Generative Expert Metric System (GEMS) that accomplishes the user-specified\ntask or goal specified in Section 2. It can successfully match two teams Tx and Ty that the system identifies, where Tx is\nidentified as the team that most needs improvement with respect to the goal, and Ty is the best fit for supporting Tx\ntoward achieving the goal. We detail how GEMS works, starting with an overview and followed by implementation\ndetails."}, {"title": "3.1 System Overview", "content": "To construct this system, we implemented multiple LFM-powered agents, orchestrated using an existing orchestration\nframework called AutoGen [30]. Additionally, we used Guidance [2], the OpenAI API, and a MySQL database in the\nimplementation. The database stores the data used to test our prototype, representing common information available\nto developer teams. We used software artifacts from open-source software (OSS) repositories including source code,\ncommit details, and discussion threads. Each agent is designed and prompt-engineered to complete specific operations;\nsee system overview in Figure 1 below. GEMS consists of the following components:\nLead Orchestrator Acts as the primary interface and memory store, coordinating between the user, other agents, and\nthe database to produce a final recommendation.\nProgrammer Crafts database queries to retrieve relevant information. Given a task, it designs proxies or refines a\nsearch based on its knowledge of the database.\nExpert Leverages domain-specific knowledge of a human expert's research outcomes to make logical suggestions for\nGEMS. A breakdown of the expert agent is available in Figure 2\nJudge Employs well-established decision-making algorithms to finalize the team or community match, ensuring the\nrecommendation aligns with the user intent specified.\nThe scenario we used to drive the GEMS implementation is one where the user specifies the high-level goal G, e.g.\nimprove developer on-boarding process, and expects team Tx to be identified, followed by team Ty that is the best-suited\nteam or community match to drive Tx's improvement with regard to goal G. Once the user submits the high-level goal\nto the GEMS, the Lead Orchestrator proceeds with the following two tasks:\n(1) First, the Lead Orchestrator attempts to define a main goal metric (naively, i.e., without external information)\nthat will serve as a proxy for the specified high-level goal and will be used to identify the target team (Tx) that is\nthe worst performer with regard to the specified goal. The Lead Orchestrator presents this metric as a function\nname and a description of what it should measure.\nThis information is then passed to a Programmer agent that will either select from a predefined set of functions\navailable in GEMS that matches closest to the Lead Orchestrator's goal or construct a new function to imple-\nment this metric. The Programmer agent will return the resulting function to the Lead Orchestrator and the\nsubsequently computed metric values for all teams within the database. Intuitively, this process is similar to a\nmanager in a corporation asking each of their direct reports to provide a specific metric score for each team\nunder their management to make an initial evaluation."}, {"title": "3.2 Implementation Details", "content": "In this subsection, we break down intuitions and provide implementation details for used across multiple expert agents\nand judges."}, {"title": "3.2.1 Expert Agent and Iterative Prompt Priming", "content": "Expert agents in GEMS have two tasks to perform:\n(1) Design and define several metrics to measure and reflect on the given goal;\n(2) Comprehend the results of these metrics to generate an interpretation of the aggregation of these results.\nTo achieve Task 1, we developed and implemented a prompt engineering technique that we called iterative prompt\npriming for expert agents. This process is inspired by [21], where knowledge is provided before answering a question\nto elicit more accurate responses. In the original method, the model is prompted to generate related pieces of knowledge\nbefore using this generated knowledge to answer the given question. This approach could not complete our goal,\ngiven the dimension and the complexity of our problem. Thus, we scaffold this methodology iteratively for expert\nagents to 'prime' the language model to respond with more accurate and usable metrics. Our approach guides the\nknowledge-generation process with meta-prompting techniques that do not necessarily relate to the original question,\nonly within context. We also integrated databases where expert agents can consume direct information. This iterative\nprompt priming process is guided through conversations, denoted as the first circle in Fig 2. Since it is a conversation,\nthe entire text from the previous stage is appended to the latter stage and passed to the language model. We detail them\nas three stages below:\nStage 1. The goal of stage 1 is to extract specific knowledge about the expert(s), absent information about the goal\nitself. The prompt requests detailed information regarding the experts' publications and insights into the field, together\nwith a biography of the expert. This prompt-generated content refers to the expert's bibliography and describes the\ncontributions made by the expert. One can easily imagine using a knowledge base or search tool to introduce specific\ncontent in this step that the LFM may not have access to, for example when including internal experts and their unique\nand specific expertise."}, {"title": "4 METHODS", "content": "We created 10 different goals and generated metrics using GEMS and the OpenAI GPT-4 API without any prompt\nengineering. We conducted a qualitative comparative study to evaluate the metrics generated by GEMS.\nGoals. We used ChatGPT-4 to generate five common challenges software development teams face. These topics\nwere reviewed by co-authors on this paper. Given that this GEMS implementation was designed to support software\nengineering teams, we rewrote these challenges into goals that a software engineering manager might want to achieve.\nUnderstanding that not all goals could be easily expressed [13], we created abstract goals that transformed detailed\ndescriptions into less specific requirements a software engineering manager might set out to achieve. We referred to\nthose as complex goals vs. abstract goals. Here are two sets of examples:\n\u2022 Challenge: Ensure Performance and Scalability\nComplex Goal: Design and implement a scalable architecture that supports dynamic scaling and optimizes\nperformance under varying load conditions.\nAbstract Goal: Build systems that grow with our success, handling more users effortlessly.\n\u2022 Challenge: Enhance Cross-Team Coordination\nComplex Goal: Establish a cross-functional liaison role responsible for coordinating activities and depen-\ndencies between teams, ensuring seamless integration and timely project progression.\nAbstract Goal: Create seamless collaboration across all teams to work as one unified force.\nEvaluation Process. As GEMS allows the user to specify a series of parameters, we first describe the parameters used\nfor this evaluation. We asked GEMS to generate metrics to achieve a certain goal by using the following parameters:\n(1) the number of perspectives or fields of study to consider when coming up with experts (i.e., 4)\n(2) the number of experts to be selected for the panel of experts (i.e., 3)\n(3) the expert selection criteria for the expert panel (i.e.,\"\"panel must contain 2 experts from the same field and 1\nfrom a different field\")\n(4) the number of metrics each expert will generate for use in the team matching process (i.e., 3).\nMore concretely, each time GEMS executed a task prompt based on the settings described above, the system generated\n4 fields of study relevant to the specified task, then generated 3 experts for each field, as shown in Figure 1. For example\nfor a field of study like development operations, 3 real-world experts are generated: one known for a book about IT\ntransformation, another for pioneering continuous delivery, and the third for impactful research on DevOps practices.\nA total of 12 experts are generated, out of which 3 are selected for the expert panel that will then generate 9 metrics\ntotal to be used for team ranking and final team matching. These ranking results are then aggregated to provide a final\nteam match outcome.\nFor comparison, we used OpenAI GPT-4 API to pass the same goals without specific prompt engineering. We call this\nthe Vanilla approach, following works like Liu et al. [21]. LangChain [1] was used as a templating engine to generate\nthe 9 different metrics for the Vanilla approach."}, {"title": "5 RESULTS", "content": "In this section, we highlight the differences observed in the metrics generated by both systems-GEMS vs. Vanilla. To\nillustrate key differences between systems, we summarize our findings and observations for each goal. Each of the 9\ngoals is denoted by a number, followed by ABS for abstract goal, or CPX for complex goal. We represent metrics generated\nby GEMS with G, and Vanilla with V. We reformatted the defined function names to camelCase for readability, and\nshared the descriptions generated as is for authenticity. Finally, we highlight observations from comparing the two\nsystems with theoretical justifications for these differences."}, {"title": "5.1 GEMS Generates Complex Metrics Built Through Compositions", "content": "Metrics generated by GEMS often display greater complexity. Complexity here does not mean that the metric is more\ndifficult to generate; instead, it indicates a composite function of direct metrics. In other words, it tries to compress\ndifferent perspectives of measures into one measure. After matching the closest metrics from both systems, we show a\nfew examples to highlight the difference. Table 1 presents three specific metric pairs across three different goals.\nObservations: In the examples shown in Table 1, GEMS generated more comprehensive and detailed metrics compared\nto Vanilla. The GEMS metrics are more complex, as these metrics require mathematical operations or aggregation\nthrough composition. For example, userEngagement is a composition of various forms of user interaction over time\nusing different actions. In contrast, the Vanilla metric counts only the number of contributors without considering their\nlevel of activity or engagement. The GEMS metrics rateOfChangeAdoption and trackAdaptivePlanning demonstrate\nthe multiple dimensions that GEMS elicits compared to the Vanilla approach."}, {"title": "5.2 GEMS Generates Metrics With Specificity Grounded In Theories", "content": "Metrics generated by GEMS often display specificity grounded in theoretical frameworks. Here, specificity does not\nindicate that the metric is more limited in scope; instead, it highlights a clear and focused measure grounded in\nestablished theories due to iterative prompt priming. Let us examine two examples of specificity in Table 2."}, {"title": "5.3 GEMS Generates Metrics Harder to Operationalize and With More Assumptions", "content": "GEMS metrics offer valuable, holistic insights, but also bring challenges. First, the metric complexity and specificity can\nmake them harder to operationalize and implement automatically. Second, some metrics come with specific assumptions\nthat are not apparent from their descriptions. We illustrate these differences by comparing Vanilla and GEMS metrics in\nTable 3.\nObservations: In Table 3, GEMS generated two metrics that reflect concepts stemming from existing literature which\nVanilla did not generate. However, both metrics cannot be derived solely based on code repository information or readily\navailable data. While the descriptions pointed to possible proxies that might be able to estimate these values, GEMS\nmade assumptions that information such as the implementation cost of a technology is available to the system, or to\nthe individuals operating the tool. This makes these metrics difficult to scale and make use of. That being said, these\nmetrics are the first steps for teams to locate reasonable measures as proxies that reflect them."}, {"title": "5.4 GEMS Generates More Diverse Metrics", "content": "In spite of the occasional metric that is hard to interpret and operationalize, the metrics generated by GEMS exhibit\ngreater diversity. The Vanilla system tends to repeat or use similar metrics, despite the wide range of goals specified.\nFor example, across the 178 metrics generated by the Vanilla system for 10 cases, the top 5 frequently identified metrics\nare: issueResolutionTime appearing 14 times, commitFrequency appearing 13 times, testCoverage appearing 12\ntimes, codeChurn appearing 11 times, contributorsCount appearing 11 times. These 5 metrics account for 34% of all\n178 metrics."}, {"title": "5.5 Detailed Case Analysis", "content": "In the previous subsection, we discussed characteristics observed across all the metrics generated for the 10 cases by\nselecting pairs of metrics between the two systems. In this subsection, we focus on two specific goals and describe these\ndifferences holistically. We selected Goal 05, relating to the challenge of \"Prevent Scope Creep\" that many software\ndeveloper teams face. The two derived goals are listed as:\n\u2022 Goal 05 ABS: Keep projects focused and on-track, delivering what we promised on time.\n\u2022 Goal 05 CPX: Implement a rigorous project management protocol that requires detailed documentation and\napproval for all changes to project scope or objectives.\nWe select Goal 05 for the detailed case analysis because the programmer agent in GEMS was able to automatically\ngenerate the code implementation (using AutoGen) for all the metrics proposed by GEMS for Goal 05 ABS. Thus, for\nthis goal we show a fully functional, end-to-end process of metrics being generated, implemented, and executed to\nfind a team match that achieves the given goal. The other goals that did not have code implementations for all metrics\nsuccessfully auto-generated were not marked as failures since automatic code generation is not the primary goal of this\npaper. We limit the scope of this project to metric generation because code generation itself is a separate, growing field\nof study [3, 15, 30]. This example provides a baseline benchmark to compare against for future implementations, as\nmore advanced self-coding systems can only enhance GEMS performance. Table 4 and Table 5 list results for Goal 05\nABS and Goal 05 CPX, respectively. We placed a  for the metrics where code implementation was auto-generated by\nAutoGen, and then executed by GEMS by using software communities data from the database. For example, for the first\nmetric in Table 5a, AutoGen generated Python and SQL code that implements CalculateIncrementalDeliveryRate\nby executing SQL code querying the database for the number of closed issues in 2022.\nWhen comparing metrics generated by GEMS and the Vanilla approach, as shown in Table 4, we see that they echo\nthe characteristics discussed in the previous subsection. When we consider the 9 metrics GEMS generated, we see that\nsome metrics can be similar in terms of the attributes they consider. For instance, GEMS generated MeanTimeToResolve\nand ComputeCommunicationEfficiency. The former calculates the average time it takes for an issue to be resolved.\nThe latter considers the average time an issue receives an interaction. As such, both metrics implicitly prioritize and\nweight specific measures more, in this case, time and issues, by creating variations within all 9 metrics that make use of\nthese same measures.\nOther times, composite metrics that GEMS generated contain measures that other metrics cover. In this example,\ncalculateSprintCompletionRate partially covers the ratio of open to closed issues which taskCompletionRate\naims to measure. This overlap became an implicit weighting of specific measures when the final judge considered all 9\nmetrics.\nIn contrast with GEMS metrics, metrics generated by the Vanilla approach are broad and encompass a wide variety\nof items that do not necessarily address the given goal. The metrics typically represent general software engineering\nconcepts without being tailored specifically to the given task.\nNext, we examine how metrics change when goals become more specific. Shifting from abstract to concrete, we\ncompare the metrics listed in Table 5. First, we notice the sensitivity in metrics generated by GEMS in this case. Adding\nconcepts such as \u201cproject management protocol\u201d and \u201cdetailed documentation\" to the goal specification led to the increase\nin metrics that are less technical, such as ProcessAdherence Score and calculateRiskIdentificationAndPlan, as\nwell as specific measures such as documentationComplianceScore. While GEMS did not automatically generate the\ncorresponding code for these metrics, implementations for them could be crafted with human intervention, or using"}, {"title": "6 DISCUSSION", "content": "GEMS acts as a copilot in constructing proxies tailored to the specified goal, and using the data at hand. As noted in\nthe Results section, the system sometimes generates metrics that are overly narrow, or overfits its conclusions. As\nsuch GEMS plays the role of the real-world experts who bring theoretical insights to the given problem, and local\nindividuals familiar with the context of the problem would guide the experts to apply these insights effectively.\nThe literature on copilot roles and expert-novice collaboration supports this. Constructing knowledge requires\nactive participation, developed and used through engagement with the activity, context, and culture [20]. GEMS lowers\nthe barriers by surfacing expert knowledge directly to individuals without actual experts present. In the example,\nthe metric calculateEconomicImpact shown in Table 3 encouraged using the ROI of previous technology usages as\na proxy for evaluating technologies out of thousands, if not millions, of possible metrics. As models improve with\nadvancements in code generation and logical reasoning, they will be able to offer better, more specific recommendations.\nHowever, individuals familiar with the problem context will still need to guide metrics, positioning GEMS as a Copilot\nrather than a people replacement."}, {"title": "6.1 GEMS as a Copilot", "content": "GEMS acts as a copilot in constructing proxies tailored to the specified goal, and using the data at hand. As noted in\nthe Results section, the system sometimes generates metrics that are overly narrow, or overfits its conclusions. As\nsuch GEMS plays the role of the real-world experts who bring theoretical insights to the given problem, and local\nindividuals familiar with the context of the problem would guide the experts to apply these insights effectively.\nThe literature on copilot roles and expert-novice collaboration supports this. Constructing knowledge requires\nactive participation, developed and used through engagement with the activity, context, and culture [20]. GEMS lowers\nthe barriers by surfacing expert knowledge directly to individuals without actual experts present. In the example,\nthe metric calculateEconomicImpact shown in Table 3 encouraged using the ROI of previous technology usages as\na proxy for evaluating technologies out of thousands, if not millions, of possible metrics. As models improve with\nadvancements in code generation and logical reasoning, they will be able to offer better, more specific recommendations.\nHowever, individuals familiar with the problem context will still need to guide metrics, positioning GEMS as a Copilot\nrather than a people replacement."}, {"title": "6.2 Mapping to Expert Decisions", "content": "One of the strengths of the metrics generated by GEMS is its composition and specificity; akin to the work of real-world\nexperts. For instance, experts often already had a sense of the specific relationships, concepts, and thresholds among a\nselection of measurements. GEMS takes a similar approach where measurements then form metrics when a scenario is\ngiven.\nHerbert Simon's work on intuition and judgment posits that intuition and sound judgments are outcomes of extensive\npractice and experience, which he denotes as \"frozen analysis\" [29]. These internalized analyses allow experts to respond\nrapidly to familiar patterns when given a context. In many cases, experts rely on experience-based intuition and pattern\nrecognition in dynamic environments [17], thereby reducing errors [12]. These findings support the use of the iterative\nprompt priming technique that GEMS employs.\nBy making pre-processed information (in this case an expert's prior works) available to the Large Foundation Model\n(LFM), the model does not have to regenerate or digest information from scratch, allowing the model to generate more\naccurate and expert-like decisions. Hence, we observe the different metrics generated by the Vanilla model and the\nGEMS system.\nRecall the metrics grounded in existing theories listed in Sec. 5.2. If GEMS was able to map critical works in the field\nof experts who have unique, significant experiences to specific metrics, it is reasonable to project its capability when"}, {"title": "6.3 Blueprinting Metrics to Solve Difficult Problems using LFMs", "content": "GEMS, in its role as a copilot, demonstrates its ability to forge expert knowledge into context-specific solutions,\neffectively crafting blueprints for solving difficult problems by leveraging the capabilities of Large Foundation Models\n(LFMs).\nPrior research highlights two primary challenges when it comes to crafting metrics. The first challenge lies in\ninitiating metrics that can effectively measure and resolve issues [11, 22]. Once this challenge is overcome, teams\nmay still misuse inappropriate metrics for their goals. For example, in software engineering, developers frequently\nequate productivity with lines of code, a simplistic measure that fails to capture the true complexity of team output.\nExperts have corrected such measurements by developing metrics that better reflect a team's collective contributions,\nencompassing multiple dimensions of developer work [8]. In these situations, teams typically require expert guidance\nto navigate and select metrics suitable for comparison.\nGEMS positions itself as a blueprinting tool, guiding the creation of a more comprehensive set of metrics. These\nblueprints are designed to stimulate informed discourse among teams, leading to better comparisons and decisions\nwithout an immediate need for physical experts, particularly when teams face complex and challenging problems.\nThe iterative prompt priming technique with the expert panel selection design translates complex problems into\nmultiple perspectives. Thus, even if GEMS introduces complex and less operationalized metrics, it prevents novices\nfrom selecting more accessible but possibly poor metrics. Instead, it seeds a conversation that overcomes this hurdle,\nallowing local expertise and discussion to facilitate more situated metrics based on GEMS's initial recommendations.\nThis design also allows CMS to generate metrics covering both breadth and depth. The case study in the results section\n(Sec. 5.5) demonstrates the breadth of metrics generated, showcasing the system's ability to identify experts from\ndiverse domains. While priming in behavioral economics can introduce biases, intentionally priming models are used\nhere to reduce overly narrow metrics and solutions."}, {"title": "6.4 Limitations and Future Work", "content": "While GEMS metrics show that such systems can generate better metrics given a goal and a dataset, we do not know\nhow individual users would use such a system. Our report did not evaluate how decision-makers use team pairings\ngenerated by GEMS. The system still relies on users providing authentic and trustworthy information. Correctness\nand transparency in software version control are crucial for accurate pairing suggestions. Decision-makers must take\nresponsibility for final decisions and effectively use systems like GEMS. Thus, future work should experiment with\nmanagers and decision-makers to understand how they evaluate the metrics generated and make use of systems\nlike GEMS."}, {"title": "7 CONCLUSION", "content": "Identifying suitable metrics to address challenging goals in improving software engineering practices is inherently\ndifficult. Traditionally, individuals have often relied on suboptimal metrics that are more readily available. While prior"}]}