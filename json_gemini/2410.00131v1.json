{"title": "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "authors": ["Ji Liu", "Jiaxiang Ren", "Ruoming Jin", "Zijie Zhang", "Yang Zhou", "Patrick Valduriez", "Dejing Dou"], "abstract": "As a promising paradigm to collaboratively train models with decentralized data, Federated Learning (FL) can be exploited to fine-tune Large Language Models (LLMs). While LLMS correspond to huge size, the scale of the training data significantly increases, which leads to tremendous amounts of computation and communication costs. The training data is generally non-Independent and Identically Distributed (non-IID), which requires adaptive data processing within each device. Although Low-Rank Adaptation (LoRA) can significantly reduce the scale of parameters to update in the fine-tuning process, it still takes unaffordable time to transfer the low-rank parameters of all the layers in LLMs. In this paper, we propose a Fisher Information-based Efficient Curriculum Federated Learning framework (FibecFed) with two novel methods, i.e., adaptive federated curriculum learning and efficient sparse parameter update. First, we propose a fisher information-based method to adaptively sample data within each device to improve the effectiveness of the FL fine-tuning process. Second, we dynamically select the proper layers for global aggregation and sparse parameters for local update with LORA so as to improve the efficiency of the FL fine-tuning process. Extensive experimental results based on 10 datasets demonstrate that FibecFed yields excellent performance (up to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61% faster) compared with 17 baseline approaches).", "sections": [{"title": "1 Introduction", "content": "As a promising paradigm to collaboratively train models with decentralized data, Federated Learning (FL) can be exploited to fine-tune Large Language Models (LLMs) without aggregating the raw data from a large number of devices (Fan et al., 2023; Kuang et al., 2023; Che et al., 2023a; Liu et al., 2024b; Che et al., 2023b; Liu et al., 2022b,a; Zhou et al., 2022). A number of stringent legal regulations (Official Journal of the European Union, 2016; Californians for Consumer Privacy, 2020) have been set up in order to protect the security and the privacy of personal data, which hinders the aggregation of the decentralized raw data. FL typically utilizes a parameter server (Li et al., 2014; Liu et al., 2024c, 2023a,b) to aggregate the distributed model updates in devices, which only transfers the parameters or the gradients of the updated models in replace of the raw personal data. By leveraging the distributed raw data of end users, Large Language Models (LLMs) can be trained on devices with excellent performance (Zhao et al., 2024). While ChatGPT (OpenAI, 2022) has achieved remarkable progress, LLMs (Touvron et al., 2023; Jiang et al., 2023; Du et al., 2022; Zeng et al., 2023; Zhang et al., 2024) have attracted extensive attention. The size of LLMs ranges from several million parameters, e.g., RoBERTALARGE (Liu et al., 2020), to several hundreds billion parameters (Wang et al., 2019). While the large scale brings strong capability in various Natural Language Processing (NLP) tasks (Zhao et al., 2023b), the pre-training and the fine-tuning process of LLMs significant communication and computation costs (et al., 2023).\nTwo types of parameter-efficient approaches exist for reducing the number of parameters within the fine-tuning process of LLMs. Prompt tuning (Liu et al., 2021; Lester et al., 2021; Liu et al., 2022d) can dynamically adjust the prompts to fine-tune LLMs with only a few trainable parameters, which may introduce performance degradation. Although Low-Rank Adaptation (LoRA) (Hu et al., 2021) can significantly reduce the scale of parameters to update in the fine-tuning process of LLMs so as to enable the training of LLMs on edge devices (Xu et al., 2024), it still takes unaffordable time to update the low-rank parameters of all the layers in"}, {"title": "LLMs when dealing with decentralized data.", "content": "While being an effective method to improve the efficiency and effectiveness of training process, curriculum learning (Bengio et al., 2009) is exploited to train large-scale models (Li et al., 2022a). Inspired by the learning strategy of starting small (Elman, 1993), the curriculum training process starts with easier data and then gradually increase the difficulty. Instead of randomly sampling the batch from training dataset, curriculum learning allows the model to gradually learn from easy samples to hard samples during the training or the fine-tuning process. Existing approaches generally measure the complexity of samples based on heuristic methods (Li et al., 2024) or a simple mode-based method (Xu et al., 2022), both of which cannot provide an accurate estimation of difficulty of data samples and cannot be directly applied in FL. In the context of FL, the data is generally non-Independent and Identically Distributed (non-IID), which requires an adaptive difficulty evaluation approach for diverse devices (Vahidian et al., 2023).\nModel compression methods, e.g., pruning (Wu et al., 2021; Liu et al., 2024d; Zhang et al., 2022) or sparse training (Bibikar et al., 2022), are exploited in FL to reduce computation and communication costs. Sparse training can achieve personalization so as to further improve the performance of FL (Liu et al., 2023c; Dai et al., 2022). However, the pruning or sparse training incurs severe accuracy degradation due to lossy strategies or simple component (neuron) selection mechanisms. In addition, the model can be split into two parts, i.e., the server part and the device part, in order to achieve both the generalization and personalization capability (Han et al., 2023).\nFisher information can be exploited to accelerate the training process of LLMs (Ollivier, 2015; Martens and Grosse, 2015; Osawa et al., 2023). Fisher information is defined as the amount of information carried by a random variable corresponding to some unknown parameters (Duy et al., 2022). As a measure of the local curvature (Martens, 2020), Fisher Information Matrix (FIM) defines the Riemannian metric of the parameter space (Karakida et al., 2019), which can indicate the difficulty of data samples and the importance of each component of the network along with the generalization performance (Jastrzebski et al., 2021).\nIn this paper, we propose FibecFed, i.e., a Fisher Information-based Efficient Curriculum Federated Learning framework. FibecFed is composed of two"}, {"title": "novel methods, i.e., adaptive federated curriculum learning and efficient sparse parameter update. To\nthe best of our knowledge, we are among the first\nto exploit the fisher information to perform cur-\nriculum learning and sparse training at the same\ntime within FL settings. We summarize out major\ncontributions as follows:", "content": "\u2022 We propose an adaptive federated curriculum\nlearning method to sample easy data samples\nfirst and to gradually improve the difficulty of\nsamples so as to improve the effectiveness of\nthe FL fine-tuning process. We exploit a fisher\ninformation-based method to measure the dif-\nficulty of training data within each device.\n\u2022 We propose an efficient sparse parameter up-\ndate method to select proper layers for global\naggregation and to adaptively update sparse\nparameters to achieve excellent efficiency and\neffectiveness. We utilize fisher information\nto evaluate the importance of diverse compo-\nnents of LLMs and propose a lossless method\nfor global aggregation and local update.\n\u2022 We conduct extensive experimentation to val-\nidate our approach using 10 datasets. The\nexperimental results reveal that FibecFed sig-\nnificantly outperforms 17 baseline approaches\nin terms of accuracy (up to 45.35% higher)\nand fine-tuning speed (up to 98.61% faster)."}, {"title": "The rest of the paper is organized as follows.\nThe related work is presented in Section 2. We\nformulate the problem to address in Section 3. We\npresent the architecture of FibecFed and propose\nthe adaptive federated curriculum learning and the\nefficient sparse parameter update method in Sec-\ntion 4. We demonstrate the experimental results in\nSection 5. Finally, Section 6 concludes.", "content": "2 Related Word & Preliminaries\nInspired by the learning strategy of starting small (Elman, 1993), curriculum learning (Bengio et al., 2009) is exploited in large-scale model training (Li et al., 2022a). Existing works measure the complexity of samples based on static characteristics of data samples, e.g., sequence length (Li et al., 2024; Platanios et al., 2019). Although a simple global mode-based method is proposed to predict the performance improvement based on several training states (Xu et al., 2022), it still cannot provide an accurate estimation of difficulty of data samples due"}, {"title": "to non-IID data in FL settings. Direct evaluation\nbased on the inference loss of models (Vahidian\net al., 2023) cannot well explore the impact on the\ngeneralization of the training process. The atten-\ntion scores can analyze the dependency among di-\nverse layers, but varies significantly between heads\n(Vig and Belinkov, 2019), which cannot be directly\nutilized in FL settings. While a sharpness-aware\nminimization method (Foret et al., 2021) can help\nminimize loss value and loss sharpness to improve\nmodel generalization, it does not consider federated\nfine-tuning settings of LLMs.", "content": "FIM can be exploited to enable the second-order\noptimization so as to improve the training process\n(Osawa et al., 2023; Jin et al., 2022) and to compute\na global posterior for federated learning (Jhunjhun-\nwala et al., 2024). In addition, continual learning\ncan be used to improve the performance of trained\nmodels while addressing the forgetting problem\n(Wu et al., 2022a). Different from (Osawa et al.,\n2023; Wu et al., 2022a; Jhunjhunwala et al., 2024),\nwe exploit the sum of diagonal of FIM to evalu-\nate the difficulty of samples within the efficient\ncurriculum learning method and to calculate the\nimportance score of each layer and neuron within\nthe LLM.\nModel compression methods (Wu et al., 2021;\nBibikar et al., 2022) are exploited in FL to re-\nduce both computation and communication costs.\nAlthough pruning methods can reduce the size\nof large models (Wang et al., 2020; Ma et al.,\n2023; Xia et al., 2023), it is complicated to\nchoose a proper pruning rate and may incur in-\nferior performance in terms of accuracy (Wu et al.,\n2021). Sparse training can achieve personalization\n(Bibikar et al., 2022; Liu et al., 2023c; Setayesh\net al., 2022; Dai et al., 2022) while addressing the\nclient shift problem brought by the non-IID data\n(Setayesh et al., 2022; Karimireddy et al., 2020).\nHowever, the existing sparse training methods may\nincur severe accuracy degradation with poor gen-\neralization capacity due to simple component se-\nlection mechanisms. The model can be split into a\nserver part and a device part to achieve both gener-\nalization and personalization capability (Han et al.,\n2023), which still incurs severe computation and\ncommunication costs in the FL settings of LLMs.\nPlease note our approach is orthogonal with model\ncompression methods.\nFor NLP tasks, prompt tuning (Liu et al., 2021;\nLester et al., 2021; Liu et al., 2022d) can fine-tune\nLLMs with only a few parameters. With prompt"}, {"title": "tuning, an extra network is exploited to generate\nproper prompts or prefix, which is concatenated\nwith the input to guide LLMs to generate proper\nanswers. Furthermore, LoRA updates trainable\nrank decomposition matrices while freezing the\nparameters of the original network, which can sig-\nnificantly reduce the scale of parameters to update\n(Hu et al., 2021). However, both the prompt tuning\nand LoRA still incur significant communication\ncosts due to the update for all the layers.", "content": "3 Problem Formulation\nIn this paper, we delve into the problem of how to\nefficiently fine-tune a large language model within\na FL setting. The FL setting is composed of a\nparameter server and K devices. We assume that\nthe data samples are distributed among the devices,\neach of which contains a dataset $D_k = \\{s_i, m_i\\}_{i=1}^{n_k}$ with $s_i, m_i$, and $n_k$ referring to a data sample, the\ncorresponding label, and the cardinality of $D_k$. We\ndenote the cardinality of the whole dataset D =\n{$D_1, D_2, ..., D_k\\}$ by N.\nWe consider a LLM M of L layers, each layer\ncontains a full parameter matrix W. We exploit\nthe LORA method to reduce the parameters to up-\ndate in this paper (Hu et al., 2021), and denote the\nLORA parameters of the the LLM M by P with P\nrepresenting the set of LoRA parameters in Layer l\nof M. We denote the updated LoRA parameters on\nDevice k by $P_k$. Then, we formulate the problem\nto address in this paper as how to efficiently update\nP so as to minimize the global loss:\n$\\min_{P} F(M, P) \\overset{\\triangle}{=} \\frac{1}{K} \\sum_{k=1, P_k\\in P}^{K} F_k(M, P_k)$                                                (1)\nwhere $F(M, P)$ is the global loss,\n$F_k(M,P_k) \\overset{\\triangle}{=} \\sum_{\\{s_i,m_i\\} \\in D_k} f(M, P_k, s_i, m_i)$ represents the local loss function on Device k with\n$f(M, P_k, s_i, m_i)$ calculating the local loss on\nDevice k."}, {"title": "4 Efficient Curriculum Federated\nLearning", "content": "In this section, we first explain the system model.\nThen, we propose the adaptive federated curricu-\nlum learning method. Afterward, we further detail\nthe efficient sparse tuning method."}, {"title": "4.1 System Model", "content": "The system model of FibecFed is shown in Figure\n1. We assume that the LLM (M) is deployed on\neach device. The parameters of M stays frozen\nwhile we update the LoRA parameters (Hu et al.,\n2021). As shown on the top left of Figure 1, on\nDevice k, the parameters ($W_0^l$) at each layer (l) is\ndecomposed into two matrices (LoRA), i.e., A and\n$B_0^l$, which can be updated during the fine-tuning\nprocess. Then, the hidden values (h) generated\nat Layer l with the input x is calculated based on\nFormula 2.\n$h = W_0^l x + B_0^l A_0^l x$.                                                        (2)\nThe fine-tuning process is composed of two\nphases, i.e., initialization and tuning. Within the\ninitialization phase, we evaluate the difficulty score\nfor each batch of data samples (see Formulas 3-5\nin Section 4.2 and Lines 1-4 in Algorithm 1) and\nthe importance score for each layer (see Formulas\n6-11 and details in Section 4.3.1) based on fisher\ninformation on each device (Step 1). Then, the\nimportance score of each layer is transferred to the\nserver (Step 2), which aggregates the scores and\nselects proper layers as the Global Aggregation\nLayers (GAL) (see details in Section 4.3.1, Line 7\nin Algorithm 1, Step 3). Afterward, the GAL are\nboradcasted to all the devices (Step 4). Finally,\nthe parameters, which are not in the GAL, are lo-\ncally evaluated on each device so as to generate\nlocal update part of parameters and the local static"}, {"title": "parameters to be frozen (see details in Section 4.3.2,\nLines 8-10 in Algorithm 1, Step 5). During the\ntuning phase, only the parameters in GAL and the\nlocal update part of parameters are updated while\nthe local static parameters are kept frozen. The\ntuning phase consists of multiple rounds, each of\nwhich consists of five steps. First, the server ran-\ndomly selects K devices and broadcasts the global\nparameters in GAL on the server to the selected\ndevices (Step \u2465, Line 12 in Algorithm 1). Then,\nthe parameters in GAL and the local update part\nof parameters are updated based on our proposed\ncurriculum FL (see details in Appendix) on each\nselected device (Step 7, Lines 13-17 in Algorithm\n1). Afterward, the updated parameters in GAL are\nuploaded to the server (Step 8). Finally, the server\naggregate and update the global parameters in GAL\n(Step, Line 19 in Algorithm 1).", "content": "The tuning phase is composed of multiple\nrounds, each of which consists of five steps. First,\nthe server randomly selects K devices and broad-\ncasts the global parameters in GAL on the server to\nthe selected devices (Step 6). Then, the parame-\nters in GAL and the local update part of parameters\nare updated based on our proposed curriculum FL\n(see details in Section 4.2) on each selected device\n(Step \u2466). Afterward, the updated parameters in\nGAL are uploaded to the server (Step 8). Finally,\nthe server aggregate and update the global parame-\nters in GAL (Step 9)."}, {"title": "4.2 Fisher information-based Curriculum\nFederated Learning", "content": "Inspired by the starting small strategy (Elman,\n1993), we propose a fisher information-based cur-\nriculum FL method to enable efficient federated\nfine-tuning. As the FIM can help indicate the\namount of information carried by each data sample\nto generate the response (Ly et al., 2017), we pro-\npose utilizing the FIM to measure the difficulty of\ndata samples. The FIM is defined in Formula 3:\n$F_i \\approx E_{s_i} [(\\nabla \\log p_k(s_i)) (\\nabla \\log p_k(s_i))^T]$.                                                        (3)\nwhere $F_i$ represents the FIM corresponding to\ndata sample $s_i$, $p_k(s_i)$ represents the probability\ndensity function of the inference with the LLM\nM, the LoRA parameters $P_k$, and data sample $s_i$,\n$\\nabla \\log p_k(s_i)$ denotes the first-order derivative of\nthe LORA parameters, calculated by the gradient\nof the loss respect to $P_k$, T refers to the transpose\nof a matrix. Practically, the expected FIM can be\napproximated by empirical FIM (Kunstner et al.,\n2019) as defined in Formula 4:\n$F_i \\approx \\frac{1}{N} [\\sum_{i=1}^{N} (\\nabla \\log p_k(s_i)) (\\nabla \\log p_k(s_i))^T]$,                                                        (4)\nHowever, calculating the FIM is computational\nexpensive as the multiplication of $\\nabla \\log p_k(s_i)$ is\nboth time and memory consumption when the size\nof the derivative matrix, i.e., $|\\nabla \\log p_k(s_i)|$, is sub-\nstantial. Inspired by (Pascanu and Bengio, 2013),\nwe calculate the diagonal of FIM to approximate\nthe FIM as shown in Formula 5.\n$F_i = I_{\\nabla \\log p_k(s_i)} F_i$                                                     (5)\nwhere $I_{\\nabla \\log p_k(s_i)}$ is the identity matrix with the\nsame size of the derivative matrix. Then, we cal-\nculate the sum of the trace of $F_i$ (Jastrzebski et al.,\n2021) as the score of the data sample. Finally, we\ncan calculate the difficulty score of a batch of data\nsamples (see details in Appendix).\nIn order to improve the training efficiency, we\npropose a curriculum data selection strategy. We\ntake the simplest $B_t$ data samples for the local\nupdate on Device k in Round t. B. becomes bigger\nalong with the epoch number within the training\nprocess (see details in Apendix)."}, {"title": "4.3 Efficient Sparse Parameter Update", "content": "In this section, we propose an efficient sparse pa-\nrameter method composed of a global aggregation"}, {"title": "layer selection method and a local update parame-\nter selection method.", "content": "4.3.1 Global Aggregation Layer Selection\nIn order to reduce communication costs, we only\ntransfer the LoRA parameters in important layers\n(GAL) between the server and devices for global\naggregation. In this section, we propose a global\naggregation layer selection method with a novel\nlayer importance score calculation technique and a\nglobal aggregation layer selection technique based\non the importance score.\nWhile important layers generally capture distin-\nguishable features of data (Mellor et al., 2021),\nwe select the layers that are sensitive to the input\ndata samples. When a layer exhibit less resilience\nagainst the noise on the data, it corresponds to\nhigher sensitivity, and thus is more important. We\ncalculate the output difference of a certain layer\nwith two similar input data samples to indicate its\nresilience, which represents the importance score.\nIn order to get two similar input data samples,\nwe add noise to an original sample. Within a prede-\nfined noise budget, we calculate the noise that max-\nimizes the loss, so as to well evaluate the sensitivity\nof the layer. Then, the noise ($\\epsilon_i$) corresponding to\n$s_i$ is calculate based on Formula 6:\n$\\epsilon_i = argmax_{\\|\\epsilon_i\\|_p<\\gamma} \\frac{f(M, P_k, S_i + \\epsilon_i, m_i)}{L_k(S_i+\\epsilon_i)} - \\frac{f(M, P_k, s_i, m_i)}{L_k (S_i)} ,$                                                        (6)\nwhere $L_k$ is the local loss, $\\| \\cdot \\|_p$ represents the $l_p$-\nnorm of the noise, and $\\gamma$ refers to the noise budget.\nWe decompose $L_k(s_i + \\epsilon_i) - L_k(s_i)$ via the first-\norder Talyor extension as defined in Formula 7:\n$L_k(S_i + \\epsilon_i) - L_k(S_i)$\n$\\approx L_k(S_i) + \\epsilon \\nabla P_kL_k(S_i) - L_k(S_i)$,                                                        (7)\n$=\\epsilon \\nabla P_kL_k(S_i)$.\nThen, we can solve the approximation by the solu-\ntion to a classic dual problem (Foret et al., 2021)\nas defined in Formula 8.\n$\\epsilon_i = \\gamma \\frac{sign(\\nabla P_kL_k(P_k))|\\nabla P_kL_k(P_k)|^{q-1}}{\\|\\nabla P_kL_k(P_k)\\|_p^{q-1}}$.                        (8)\nwhere $|\\cdot|^{q-1}$ denotes the absolute value and power\nin terms of each element, q is a factor that satisfies\n$\\frac{1}{p} + \\frac{1}{q}=1$. Afterward, we take $\\epsilon_i'$ as the noise $\\epsilon_i$\nto calculate the sensitivity."}, {"title": "As Frobenius norm can characterize features in\nthe latent space (Chen et al., 2021), we exploit a rel-\native difference of the Frobenius norm to measure\nthe sensitivity of each layer. The relative difference\ncan avoid the bias brought by the absolute values.\nThe relative difference of of Frobenius norm is\ndefined in Formula 9.", "content": "$F'(s_i) = \\frac{\\|h'(s_i + \\epsilon)\\|_F - \\|h'(s_i)\\|_F}{\\|h^l(s_i)\\|_F} ,$                                                                                       (9)\nwhere $F^l(s_i)$ is the relative difference of the Frobe-\nnius norm, $h^l (s_i)$ represents the output embeddings\nat Layer l of the LLM M for data sample $s_i$, $\\|\\cdot \\|_F$\nrefers to the Frobenius norm. Then, the importance\nscore of Layer l on Device k is calculated based on\nits local data Dk as defined in Formula 10.\n$I_k^l = \\frac{1}{n_k} \\sum_{s_i \\in D_k} F'(S_i),$                                                                                                       (10)\nwhere $I_k^l$ represents the importance score of Layer\nl on Device k. Afterward, the global importance\nscore $I^l$ is calculated based on Formula 11.\n$I^l = \\frac{1}{K} \\sum_{k=1} n_kI_k$                                                                                                         (11)\nWe propose a lossless method to select proper\nimportant layers as GAL. On each device k, the\nLoRA parameters are initialized as $P_k$. After T\nrounds of fine-tuning, the LoRA parameters are\ndenoted by $P_k^T$. We construct a base function\nas $A_k=P_k^T - P_k$. We calculate the Hessian ma-\ntrix of the local loss function with its eigenvalues\nsorted in ascending order (${\\{\\lambda^k_1, \\lambda^k_2, ...,  \\lambda^k_i, ..., \\lambda^k_{R_k} \\}} \\in \\Re^{R_k}$\nwith r representing the index of an eigenvalue and\n$R_k$ indicating the rank of the Hessian matrix). We\ncalculate the Lipschitz constant ($L_k$) of a base func-\ntion $H_k(P)$\\Delta_k - \\nabla L'_k(\\Delta_k + P_k)$ with $\\nabla L(\u00b7)$\nbeing the gradient of the local loss function and\n$H_k$ referring to the Hessian matrix of the local loss\nfunction. Inspired by (Zhang et al., 2021), we find\nthe first $r_k$ that satisfies $\\lambda^k_{r_k+1} - \\lambda^k_{r_k} > 4L_k$ to\nachieve lossless performance. Then, we calculate\nthe expected number of layers in GAL on Device\nk as $N_k = \\frac{r_k}{R_k}(1-\\mu)L$ with L being the number\nof layers in M. Then, we calculate the number of\nlayers in GAL as $N^* = \\frac{\\mu}{\\sum n_k} L$ with \u03bc is a\nhyper-parameter to adjust the ratio between global\nand local number. Finally, we select $N^*$ layers\nwith the highest importance scores."}, {"title": "4.3.2 Local Update Parameter Selection", "content": "In order to reduce computation costs, we only up-\ndate important LoRA parameters within the lo-\ncal update while freeze the remaining parameters.\nApart from the parameters in GAL, we dynamically\nselect an important part of parameters in other lay-\ners to update. In this section, we propose a novel\nfisher information-based local update parameter\nselection method with momentum.\nWhile the LORA parameters may significantly\nvary during the fine-tuning process, we calculate\nthe FIM with momentum within first T' epochs by\n$F_{t}^{k} = \\eta * F_{t-1}^{k} +(1-\\eta)F^k$, where \u03b7 represents the\ncoefficient that controls the step size of the moving\naverage, $F^k_t$ refers to the FIM on Device k at Round\nt, $F^k$ is the empirical average diagonal approxima-\ntion of the FIM, i.e., $F^k = \\frac{1}{S}\\sum_{s_i\\in D} F_i$ with $F_i$\ncalculated based on Formula 5, while $F^k$ is directly\ncalculated without moment. Finally, we get a FIM\n$F^{k,l}_{T'}$ for each layer l outside of GAL. Inspired by\n(Diao et al., 2023), we exploit a neuron-wise aggre-\ngation of the FIM to indicate the importance score\nof Neuron \u03bc in Layer l as defined in Formula 12.\n$ I_{\\mu}^{k,l} =  \\sum_{\\nu=0}^{\\|W_{\\mu}: -1}\\|} [F_{T'}^{k} [\\mu * [W_{\\mu}] + v] ,$                                                        (12)\nWhere $|W_{\\mu}:|  denotes the number of elements in\nthe \u03bcth row of the full weight matrix $W_i$ in M,\n$F[V]$ represents the vth diagonal element in $F^k$.\nAfterward, we exploit the lossless method to cal-\nculate the proper local update parameter ratio as\n$P_{k,1} =  \\frac{r_{k,1}}{R_{k,1}}$(see details in Section 4.3.1, with\n$r_{k,l}$ and $R_{k,l}$ representing the corresponding $r_k$ and\n$R_k$ in Layer l. Finally, we take the most important\n$P_{k,l}$ neurons in terms of the importance score fas\nthe local update parameters to be updated with the\nparameters in GAL and freeze the other parameters\nwithin the local update."}, {"title": "4.4 FibecFed Algorithm", "content": "The FibecFed algorithm is shown in Algorithm 1.\nWithin the initialization phase (Lines 1 - 10), the\ndifficulty scores of each batch are calculated based\non Formula 7 (Lines 2 - 4), the batches of data sam-\nples are sorted in ascending order in terms of the\ndifficulty scores for the curriculum data selection\nstrategy (Line 5), the GAL are calculated in Line 7\n(see details in Section 4.3.1 ), and the local update\nparameters are computed in Line 9 (see details in\nSection 4.3.2). Then, within the fine-tuning phase"}, {"title": "is performed in Lines 11 - 19. A set of devices K is\nrandomly selected (Line 12). Then, on each device,\nlocal update is carried out in Lines 13 - 17. First,\nthe data samples are selected based on the curricu-\nlum data selection strategy in Line 14. Second, the\nLORA parameters are updated with the global pa-\nrameters in PSL transferred from the server in Line\n15 (see details in Section 4.1). Third, the LORA\nparameters are updated based on the local training\nwith the selected $B_t$ data samples in Line 16 (see\ndetails in Section 4.3.2). Finally, the parameters in\nthe global aggregation layers are aggregated using\nthe FedAvg algorithm (McMahan et al., 2017) on\nthe server (Line 18).", "content": "5 Experiments\nIn this section, we demonstrate extensive experi-\nmentation with 17 baseline approaches and 10 NLP"}, {"title": "tasks to reveal the advantages of FibecFed.", "content": "5.1 Experimental Setup\nWe take an FL environment composed of 100 de-\nvices and a parameter server in the experimentation.\nWe randomly sample 10 devices in each epoch. We\nutilize 10 commonly-used NLP tasks in the ex-\nperimentation, i.e., QNLI (Rajpurkar et al., 2016),\nSST-2 (Socher et al., 2013), COLA (Warstadt et al.,\n2019), MRPC (Dolan and Brockett, 2005), RTE\n(Giampiccolo et al., 2007), BoolQ (Clark et al.,\n2019), MPQA (Wiebe et al., 2005), Subj (Pang and\nLee, 2004), TREC (Voorhees and Tice, 2000), and\nMR (Pang and Lee, 2005). The input data of the\ntasks are non-IID among the 100 devices. We com-\npare FibecFed with 17 baseline approaches, i.e., a\nparameter efficient fine-tuning-based approaches\n(Adapter (Houlsby et al., 2019)), 6 prompt-based\ntuning methods (FedPrompt (Zhao et al., 2023a),\nP-tuning v2 (Liu et al., 2022d), IDPG (Wu et al.,\n2022b), ATTEMPT (Asai et"}]}