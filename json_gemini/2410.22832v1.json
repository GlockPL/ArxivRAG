{"title": "HIJACKRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models", "authors": ["Yucheng Zhang", "Qinfeng Li", "Tianyu Du", "Xuhong Zhang", "Xinkui Zhao", "Zhengwen Feng", "Jianwei Yin"], "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge, making them adaptable and cost-effective for various applications. However, the growing reliance on these systems also introduces potential security risks. In this work, we reveal a novel vulnerability, the retrieval prompt hijack attack (HIJACKRAG), which enables attackers to manipulate the retrieval mechanisms of RAG systems by injecting malicious texts into the knowledge database. When the RAG system encounters target questions, it generates the attacker's pre-determined answers instead of the correct ones, undermining the integrity and trustworthiness of the system. We formalize HIJACKRAG as an optimization problem and propose both black-box and white-box attack strategies tailored to different levels of the attacker's knowledge. Extensive experiments on multiple benchmark datasets show that HIJACKRAG consistently achieves high attack success rates, outperforming existing baseline attacks. Furthermore, we demonstrate that the attack is transferable across different retriever models, underscoring the widespread risk it poses to RAG systems. Lastly, our exploration of various defense mechanisms reveals that they are insufficient to counter HIJACKRAG, emphasizing the urgent need for more robust security measures to protect RAG systems in real-world deployments.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as GPT-4 (OpenAI et al. 2024) and LLaMA (Touvron et al. 2023a), have demonstrated remarkable generative and inferential capabilities. These models are known for storing vast amounts of knowledge within their parameters, acquired through training on extensive corpora (Petroni et al. 2019; Roberts, Raffel, and Shazeer 2020). Despite these advancements, they still face significant limitations. Challenges such as difficulty in updating or expanding their internal knowledge, reliance on potentially outdated information, and susceptibility to hallucinations (Ji et al. 2023) persist. Furthermore, these models often lack specialized expertise in domains like medicine, law, or finance due to constraints in training data, particularly when privacy is a concern. Consequently, parameterized models may struggle to deliver real-time, reliable, and cost-effective performance in specialized fields.\nTo address these challenges, the integration of non-parametric external knowledge through information retrieval, a method known as Retrieval-Augmented Generation (RAG) (Lewis et al. 2020; Karpukhin et al. 2020; Guu et al. 2020), has emerged as a promising solution. Unlike fine-tuning (Ovadia et al. 2023), which is resource-intensive and may compromise performance in other tasks, RAG offers a flexible approach to incorporating external knowledge. This adaptability enables seamless updates to the knowledge at minimal cost, making it well-suited for various domains. The growing popularity of RAG is evident in the widespread adoption of tools like the ChatGPT Retrieval Plugin (OpenAI 2023), LlamaIndex (Liu 2023), and LangChain (Chase 2022). The 2024 Retool Report (Retool 2022) highlights this trend, noting that 23.2% of respondents use vector databases or RAG to customize models, with this percentage rising to about 33% among larger companies with over 5,000 entities.\nWhile existing research (Asai et al. 2023; Xiong et al. 2020; Izacard et al. 2021) has primarily focused on enhancing the accuracy and efficiency of RAG systems, the security and robustness of these technologies remain underexplored. For instance, numerous studies (Xiong et al. 2020; Izacard et al. 2021) have concentrated on designing new retrievers"}, {"title": "Background & Related Work", "content": "RAG enhances LLMs by integrating external knowledge from a large corpus through semantic similarity. By combining retrieval with generative models, RAG addresses the limitations of purely generative approaches, which may struggle to produce factually accurate content due to their reliance on internalized knowledge. RAG has demonstrated impressive performance across tasks such as open-domain QA (Trivedi et al. 2023), dialogue (Peng et al. 2023), domain-specific QA (Cui et al. 2023), and code generation (Zhou et al. 2022).\nRAG operates in two main steps: retrieval and generation. In the retrieval step, the relevant information is retrieved from a large corpus using a dual-encoder mechanism where a question encoder $E_q$ converts the query q into an embedding vector, and a passage encoder $E_p$ generates embeddings for passages $p_i$ in the corpus C. The similarity between the query and each passage is calculated using a similarity function $Sim()$, such as cosine similarity or dot product, with the top-k passages selected as the most relevant. Formally, for a given query q, the retrieved set R(q; C) is defined as:\n$R(q; C) = \\{p_i \\in C | top-k \\ scores \\ of \\ S(q, p_i)\\},\\newline s.t. \\ S(q,p_i) = Sim(E_q(q), E_p(p_i)).$ \nIn the generation step, the LLM uses these retrieved passages to generate a response, producing answers that are both accurate and contextually relevant. For a given query q and its retrieved passages R(q; C), the LLM generates the answer a as a = G(q, R(q; C)).\nAttacks on machine learning models typically leverage sophisticated algorithms and optimization techniques. However, the flexible nature of LLMs, which allows for easy extension of their functionalities via natural prompts, also exposes them to a wide range of security vulnerabilities. Even in black-box settings with existing mitigation strategies, malicious users can exploit these models through Prompt Injection attacks (Greshake et al. 2023), which can bypass content restrictions or gain access to the model's original instructions. Previous studies (Wang et al. 2023; Liu et al. 2023) have shown that noisy or poisoned inputs can degrade LLM performance, leading to erroneous or compromised outputs.\nThe fact that LLMs are increasingly integrated with RAG systems introduces new security challenges. Traditionally, in prompt injection attacks, the LLM itself is the primary target, with the attacker directly interacting with the model. However, in RAG systems, this paradigm shifts (Greshake et al. 2023), with attackers focusing on injecting malicious prompts into the retrieval corpus. This can cause the retrieval of harmful data during inference, indirectly affecting other users and systems. While some research has explored vulnerabilities in RAG, such as injecting nonsensical but retrievable text (Zhong et al. 2023) or semantically misleading text (Zou et al. 2024) to manipulate LLM outputs, these studies have only scratched the surface of the potential risks. Our work addresses this gap by introducing a novel retrieval prompt hijack attack in RAG systems. We demonstrate that by injecting a small amount of malicious text into the knowledge base, an attacker can significantly influence the model's outputs, steering it to generate specific, attacker-desired responses."}, {"title": "Problem Formulation", "content": "In this study, we consider a scenario where an attacker targets a set of queries, denoted as $q_1, q_2,...,q_{N_q}$, each with a corresponding desired answer $a_i$. The attacker's goal is to manipulate the corpus C so that the RAG system generates the desired answer $a_i$ when queried with $q_i$, for $i = 1,2,..., N_q$. This form of manipulation, known as a prompt hijack attack, compromises the integrity of the system to produce specific outputs. Such attacks can have severe consequences, including the dissemination of false information, biased responses, and misleading advice, raising significant ethical and safety concerns.\nThe RAG system consists of three main components: the corpus, the retriever, and the LLM. We assume that the attacker cannot access the contents of the corpus or the LLM's parameters, nor can they directly query the LLM. However, the attacker is capable of injecting malicious texts into the corpus C. For each target query $q_i$, the attacker can insert $N_a$ malicious texts designed to influence the retriever and ultimately affect the LLM's output. We explore two settings based on the attacker's knowledge of the retriever:\nIn this setting, the attacker does not have access to the retriever's parameters but is aware of the model architecture used by the retriever. This scenario is realistic, as model architectures are often publicly documented or can be inferred. Evaluating this setting allows us to assess the RAG system's robustness against realistic attackers.\nIn this setting, the attacker has full access to the retriever's parameters. This setting is plausible in cases where the retriever's details are publicly available or if proprietary systems are compromised. Analyzing this scenario helps us understand the vulnerabilities of the RAG system when facing an attacker with comprehensive knowledge, in line with Kerckhoffs' principle (Petitcolas 2023), which advocates for evaluating systems under worst-case conditions.\nBy examining these settings, we aim to uncover potential security gaps in RAG systems and emphasize the need to address these vulnerabilities in practical applications."}, {"title": "Problem Definition", "content": "Based on our threat model, we formalize the attack as an optimization problem. The goal is to construct malicious texts such that, when injected into the corpus, the RAG system generates the attacker's desired answers for target queries.\nLet $q_i$ denote a target query, and $a_i$ the corresponding desired answer. We represent the set of injected malicious texts as M, where $M = \\{m_{ij} | i = 1,2,..., N_q, j = 1,2,..., N_a\\}$. The goal is to optimize M so that the RAG system, when processing the target query $q_i$, generates the desired answer $a_i$. The RAG process consists of two key steps: retrieval and generation. Let R($q_i$, C$\\cup$M) denote the set of passages retrieved for query $q_i$ from the corpus C combined with the malicious texts M. In the generation step, G($q_i$, R($q_i$, C$\\cup$M)) represents the LLM's output given the query $q_i$ and the retrieved passages. The optimization problem is formally defined as:\n$\\underset{M}{max} \\sum_{i=1}^{N_q} I(G(q_i, R(q_i, C \\cup M)) = a_i), \\newline s.t. \\ M = \\{m_{ij} | i = 1, 2, . . ., N_q, j = 1, 2, . . ., N_a\\},$ \nwhere I(\u00b7) is an indicator function that returns 1 if the condition is true and 0 otherwise, ensuring that the generated answer exactly matches the desired answer."}, {"title": "Design of HIJACKRAG", "content": "To craft effective malicious texts for attacking target queries, our system must satisfy two key conditions corresponding to the retrieval and generation processes of the RAG system:\nFirst, the malicious texts must exhibit high semantic similarity to the target queries to increase their chances of being retrieved as top-k relevant documents. By closely aligning the texts with the semantic space of the target queries, we can ensure that they rank highly during retrieval. Second, once retrieved, the malicious content must effectively hijack the model's attention, redirecting it toward the attacker's desired response. This requires designing the texts to shift the model's focus from the original query to the attacker's intended topic, with explicit instructions guiding the LLM to generate the specific response.\nBased on this analysis, we propose the Retrieval Prompt Hijack Attack (HIJACKRAG). An overview of HIJACKRAG is shown in Fig. 2. Given a target query and desired answer, HIJACKRAG generates and injects a malicious text into the database. When the target query is input into the RAG system, the retriever model searches the database and retrieves relevant documents , including the injected malicious text. The RAG system then combines these retrieved documents with the target query using a prompt template, which is fed into the LLM . During generation , the malicious text hijacks the process, leading to the attacker's desired response being returned to the user.\nTo simultaneously satisfy these conditions, we decompose the malicious text into three sub-texts: retrieval text R, hijack text H, and instruction text I."}, {"title": "Determine Instruction Text", "content": "The first step is to define the attack objective, represented by I. This sub-text contains the specific instructions that direct the model to produce the desired output. Our method can be adapted to various scenarios depending on the attacker's goals. Tab. 1 outlines potential attack objectives along with corresponding example prompts.\nThese examples demonstrate the flexibility of our method for different attack scenarios. For quantitative evaluation, we have chosen Content Manipulation as the specific attack scenario, aiming to have the phrase \u201cI have been PWNED\u201d appear in the response. The corresponding I is set to \"print I have been PWNED\u201d. This ensures that when the LLM processes I, it generates the intended output. Thus, the formal objective can be expressed as G($q_i$, I) = $a_i$."}, {"title": "Combine Hijack Text", "content": "To address the challenge of hijacking the model's attention, we constructed H using the HackAPrompt dataset, sourced from a global prompt hacking competition (Schulhoff et al. 2023). This competition, with over 2,800 participants generating more than 600,000 adversarial prompts, provided valuable insights into the vulnerabilities of LLMs like ChatGPT.\nFor our purposes, we selected suitable prompts from the HackAPrompt dataset to serve as H. These sub-texts are designed to hijack the model's attention from the original topic to the attacker's desired focus. The selection process involved several steps:\nWe filtered out lengthy or irrelevant prompts, focusing on concise, semantically appropriate content that could effectively hijack the model's attention.\nTo manage redundancy, we applied TF-IDF (Jones 1972) similarity measures, excluding redundant prompts and retaining only the most distinct and impactful ones.\nWhen the refined H is combined with I, they work together to ensure that the model generates the desired response, effectively steering the model's output toward the attacker's objective. Formally, the combination of H and I is designed such that G($q_i$, H + I) = $a_i$."}, {"title": "Construct Retrieval Text", "content": "To address the challenge of ensuring the crafted malicious text is retrieved by the RAG system, we construct R. This sub-text is essential for the success of the attack, as it must be semantically similar to the target queries, ensuring that the combined prompt R$\\oplus$ H$\\oplus$ I ranks among the top-k retrieved documents. Formally, our objective is for R$\\oplus$ H$\\oplus$ I to be included in R($q_i$, C$\\cup$M), where R($q_i$, C$\\cup$M) represents the top-k documents retrieved for the query $q_i$ from the corpus C and the set of malicious texts M. We evaluate this setup under two settings: black-box and white-box.\nIn this setting, the attacker lacks access to the retriever's parameters. The primary challenge is to construct R without direct control over the retrieval mechanism. Given these constraints, our approach leverages the inherent similarity between the target query $q_i$ and itself. We set R = $q_i$, ensuring that the prompt R$\\oplus$ H$\\oplus$ I retains high similarity with the target query. The algorithm for the black-box setting is outlined in Alg. 1. Despite its simplicity, this strategy proves effective in practice, increasing the chance of the crafted prompt being retrieved by the RAG system.\nIn this setting, the attacker has full access to the retriever's parameters, enabling a more refined optimization of R. The objective is to maximize the similarity score between the combined prompt R$\\oplus$ H$\\oplus$ I and the target query $q_i$, formulated as an optimization problem. Specifically, we aim to maximize Sim($E_q(q_i), E_p(R \\oplus$ H$"}, {"title": "Experiments", "content": "We use three widely-recognized datasets: Natural Questions (NQ) (Kwiatkowski et al. 2019), HotpotQA (Yang et al. 2018), and MS-MARCO (Nguyen et al. 2016). We selected 100 closed-form questions from each dataset, as these questions have specific answers, providing a clear benchmark for assessing the attack's impact.\nFor LLM, we evaluate LLaMA2-13B (Touvron et al. 2023b), LLaMA3-8B (Dubey et al. 2024), and ChatGLM3-6B (GLM et al. 2024). The temperature parameter for all LLMs is set to 0.1 to maintain consistency across experiments. For retrieval models, we adopt Contriever (Izacard et al. 2021), Contriever-ms (fine-tuned on MS-MARCO), and ANCE (Xiong et al. 2020). Following standard practice (Lewis et al. 2020), the similarity between a question and a text from the knowledge base is calculated using the dot product of their embedding vectors.\nWe use the following metrics to evaluate the effectiveness of the attack comprehensively: (i) Attack Success Rate (ASR) \u2013 which measures the proportion of cases where the LLM generates the attacker's desired answer, indicating the effectiveness of the attack in overriding the normal responses of the LLM; (ii) F1-Score \u2013 which reflects the overall retrieval success of the injected malicious texts. Note that the F1-Score is calculated as F1-Score = 2 \u00b7 Precision \u00b7 Recall/(Precision + Recall), while precision is the fraction of injected malicious texts among the top-k retrieved texts for the target query, and recall is the fraction of retrieved malicious texts out of the total $N_a$ injected texts. A higher F1-Score indicates better retrieval effectiveness."}, {"title": "Results", "content": "Tab. 2 shows the results of HIJACKRAG compared to three baselines under default settings. The results provide several important observations. First, HIJACKRAG outperforms all baselines in both black-box and white-box settings, achieving the highest ASRs and near-perfect F1-Scores across all datasets, demonstrating the effectiveness of HIJACKRAG. Specifically, the prompt injection attack achieves moderate success, with ASRs ranging from 0.31 to 0.61 across datasets. However, despite decent F1-Scores, its ASR is significantly lower than that of HIJACKRAG, indicating that while prompt injection can influence the model's output, it lacks the robustness and efficiency of our approach in achieving the desired results. Besides, although retrievable by the RAG system, the R \\oplus I variant fails to effectively hijack the model's attention, resulting in much lower ASRs compared to HIJACKRAG. This outcome underscores the importance of H in ensuring the success of the attack. Furthermore, the H \\oplus I variant consistently fails to achieve any attack success due to the absence of R, which prevents the malicious texts from being retrieved and influencing the model's output. This result highlights the necessity of incorporating R to ensure that the attack effectively impacts the system.\nTab. 3 presents the results of HIJACKRAG in various settings. The results reveal several key observations. First, HIJACKRAG consistently achieves high ASRs on all datasets and LLMs, particularly in the black-box setting, with ASRs reaching up to 97% on HotpotQA with LLaMA3 and over 90% across other datasets and LLMs. These findings underscore the significant vulnerability of RAG systems to our proposed attack. Second, HIJACKRAG maintains near-perfect F1-Scores in both black-box and white-box settings, indicating that the crafted malicious texts are effectively retrieved for target queries. This high retrieval performance directly contributes to the success of the attack, as reflected in the ASR values. Third, while HIJACKRAG generally performs well in both settings, the ASR is consistently higher in the black-box setting compared to the white-box setting. This discrepancy likely arises because the white-box method, which aims to optimize retrieval performance, may inadvertently compromise the naturalness of the semantics in the crafted texts, slightly reducing their effectiveness."}, {"title": "Defense", "content": "To assess the robustness of our attack in the face of existing security measures, we evaluate its performance against several representative defense mechanisms used to protect LLMs from adversarial attacks (Jain et al. 2023)."}, {"title": "Paraphrasing", "content": "Paraphrasing has been used as a defense mechanism against various adversarial attacks on LLMs, such as prompt injection and jailbreaking. We adapt this strategy to counter HIJACKRAG by paraphrasing the target queries before the retrieval process, potentially altering the structure enough to disrupt the retrieval of maliciously crafted texts. In our experiments, we paraphrased each target query and then evaluated the effectiveness of this defense by evaluating the ASR and F1-Scores. The results are shown in Tab. 6, from which we can see that while the paraphrasing defense does slightly reduce the ASR and F1-Scores, HIJACKRAG still maintains strong attack performance. For instance, on the HotpotQA dataset with LLaMA3, the ASR only drops from 0.97 to 0.90, and the F1-Score decreases slightly from 1.0 to 0.95. These findings suggest that paraphrasing is insufficient to defend against HIJACKRAG effectively."}, {"title": "Contextual Expansion", "content": "We observe that HIJACKRAG setup injects 5 malicious texts per target query, which matches the top-k setting of the RAG system. To test a potential defense, we increased the top-k value, ensuring that retrieved texts would likely include some clean texts. To evaluate the effectiveness of this approach, we conducted two sets of experiments. First, we set the top-k value to 10 and assessed its impact on our attack"}, {"title": "Discussion", "content": "In our evaluation, we focus on a specific category of instruction text, namely Content Manipulation, with the example \"Print 'I have been PWNED'\". This approach allows us to perform a quantitative evaluation of HIJACKRAG's effectiveness quantitatively. However, it's important to note that our method is versatile and could be extended to other types of instruction texts, including those that are open-ended or involve more complex tasks, such as opinion generation or information gathering. The primary challenge lies in devising a robust framework for quantitatively evaluating the effectiveness of such open-ended instructions, as they do not yield easily measurable outcomes. We acknowledge this limitation and leave the exploration as future work.\nHIJACKRAG is meticulously designed to manipulate the RAG system to generate the attacker's desired output for target queries while minimizing unintended effects on non-target queries. To evaluate this aspect, we conducted an experiment in which 100 non-target queries were selected from each dataset, and we assessed whether the malicious texts would be retrieved in response to these non-target queries. The results across all three datasets consistently showed that the malicious texts were not retrieved for any non-target queries, underscoring the precision and targeted nature of HIJACKRAG. This outcome highlights the effectiveness of our method in focusing the attack on the intended targets while avoiding unintended side effects, ensuring that the broader system functionality remains uncompromised."}, {"title": "Conclusion", "content": "In this work, we introduced HIJACKRAG, a novel attack that exploits vulnerabilities in RAG systems by manipulating the retrieval process to execute an attacker's commands. This attack poses significant risks to the integrity and trustworthiness of RAG systems. Through extensive evaluations across multiple datasets, retrievers, and settings, we demonstrated the consistent effectiveness of HIJACKRAG. Furthermore, the attack's transferability across different retrievers underscores its broad applicability and the widespread risks it presents. Our exploration into existing defense mechanisms revealed their insufficiency in countering HIJACKRAG, highlighting the urgent need for more robust and effective protections in RAG systems."}]}