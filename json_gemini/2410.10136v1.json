{"title": "Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations", "authors": ["Garima Agrawal", "Sashank Gummuluri", "Cosimo Spera"], "abstract": "In customer contact centers, human agents often struggle with long average handling times (AHT) due to the need to manually interpret queries and retrieve relevant knowledge base (KB) articles. While retrieval augmented generation (RAG) systems using large language models (LLMs) have been widely adopted in industry to assist with such tasks, RAG faces challenges in real-time conversations, such as inaccurate query formulation and redundant retrieval of frequently asked questions (FAQs). To address these limitations, we propose a decision support system that can look beyond RAG by first identifying customer questions in real time. If the query matches an FAQ, the system retrieves the answer directly from the FAQ database; otherwise, it generates answers via RAG. Our approach reduces reliance on manual queries, providing responses to agents within 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva CQ, this system improves efficiency, reduces AHT, and lowers operational costs. We also introduce an automated LLM-agentic workflow to identify FAQS from historical transcripts when no predefined FAQs exist.", "sections": [{"title": "Introduction", "content": "In today's fast-paced customer contact centers, the human agents must address customer inquiries efficiently while maintaining high service quality. One of the key metrics for measuring the effectiveness of these interactions is average handling time (AHT) (Brantevica, 2018). AHT directly impacts both operational costs and customer satisfaction; the longer it takes to resolve a query, the higher the chances of customer dissatisfaction. Traditionally, agents manually interpret queries and search through vast knowledge bases (KB) for relevant articles during the call, which leads to prolonged resolution times. To streamline this process, large language models (LLMs) have been increasingly integrated into customer service workflows, with retrieval-augmented generation (RAG) emerging as a popular solution (Veturi et al., 2024; Agrawal et al., 2024a). RAG systems combine the power of LLMs with retrieval mechanisms that pull relevant documents from knowledge bases to assist agents during conversations. Despite its wide adoption, RAG faces several limitations in real-time environments (Salemi and Zamani, 2024; Agrawal et al., 2024b).\nBefore addressing the challenges we faced and how we overcame them, we will first provide a brief overview of the Minerva CQ (MinervaCQ, 2021) human-agent assist AI solution and the specific use cases that highlighted these limitations.\nMinerva CQ serves as an AI copilot, supporting customer service and sales agents by offering real-time assistance during live conversations. This is an example of collaborative intelligence (CQ) or collaborative AI, which combines human intelligence and artificial intelligence to achieve better outcomes than either could alone. In the workplace, CQ allows employees to team up with AI during conversations to resolve complex issues, improve customer experiences, and focus on more meaningful work. At Minerva CQ, as call transcripts are displayed in real time, the AI drives the interaction toward efficient resolutions using adaptive workflows, dialogue suggestions, behavioral cues, and knowledge surfacing.\nIn this setting, we implemented state-of-the-art RAG-based solutions to retrieve relevant knowledge articles for human experts, commonly referred to as agents in the customer service industry. However, agents frequently reported that the retrieved documents were not sufficiently useful or relevant to the context of the query. Upon investigation, we found that agents were inputting only keywords to search for KB articles, causing the RAG system to retrieve inaccurate or irrelevant documents due to a lack of proper context. This increased the time"}, {"title": "Solution Design", "content": "Problem Formulation: Customer service agents spend valuable time manually searching large KB systems to address customer queries, increasing AHT and reducing efficiency. The challenge is to automate this process by providing timely and accurate suggestions from either a predefined FAQ database or dynamically generating relevant questions based on the conversation context. The key questions are:\n\u2022 How can we efficiently identify customer intent and provide relevant FAQs or KB articles in real-time without disrupting the conversation?\n\u2022 How can the system distinguish between questions that can be answered from the FAQ database and those requiring RAG model retrieval?\n\u2022 How can we ensure the system avoids redundant or already-answered questions while providing contextually appropriate and timely responses?\nThe objective is to minimize manual effort, reduce AHT, and improve customer satisfaction by providing real-time, context-aware question suggestions and answers using a dual-threaded system integrating FAQ retrieval and RAG-based generation."}, {"title": "FAQ Model", "content": "We refer to our solution as the FAQ Model, which assists agents by offering real-time question suggestions, reducing the time spent searching KB articles. It maintains a database of FAQs learned from historical transcripts, with corresponding answers retrieved through the RAG model. During a live call, the model identifies relevant questions by analyzing the most recent conversation turns, ignoring questions already answered by the agent. The model performs two parallel operations: Match and Generate.\nThe Match operation analyzes the intent of the current conversation and retrieves the top three most relevant questions from the FAQ database, while the Generate operation dynamically creates the top three questions based on the conversation's"}, {"title": "Views in the FAQ Model:", "content": "The FAQ model provides the following four views:\nAgent View: During the call, the agent receives six question suggestions (matched and generated) from the FAQ model.\n\u2022 The model is designed to ignore already-answered questions and only present those that require reference to KB articles.\n\u2022 The agent selects a question, and the model retrieves the corresponding answer.\n\u2022 The agent can tag a generated question as FAQ then the question and retrieved answer is stored in the FAQ database.\nSupervisor View: The system gives full control to the agent supervisors, allowing them to view and manage the list of FAQs and their answers, with the ability to add, remove, edit or update them as needed.\n\u2022 If a question lacks an answer, it can be manually added or retrieved via the RAG model, with the answer stored in the FAQ database.\nModel View: The FAQ model is invoked manually by the agent or triggered automatically during conversation.\n\u2022 Both Match and Generate threads run in parallel, suggesting six questions based on the conversation context.\n\u2022 Based on agent's selection, answers are retrieved from the FAQ database or the RAG model.\nData View: All FAQ questions and answers are stored in a persistent vector database for future retrieval and can be edited or updated by the supervisor. The database also allows new FAQs to be stored at runtime if tagged as such by the agent."}, {"title": "Key Advantages of the FAQ Model", "content": "The FAQ Model offers the following benefits:\n\u2022 Agent Experience: Automatically suggests relevant questions to agents based on the customer query's intent and the conversation context, improving workflow efficiency.\n\u2022 Agent Time and Effort: Eliminates the need for agents to manually write queries or input keywords, and provides immediate answers to frequently asked questions, saving both time and effort.\n\u2022 Accuracy: FAQs can be validated and edited by supervisors or agents through the supervisor portal, ensuring responses are up-to-date and accurate. For questions not in the FAQ database, the system sends a complete query to the RAG model, leading to more precise and context-aware answers.\n\u2022 Cost Efficiency: If an answer is already in the FAQ database, the system bypasses the RAG model, reducing unnecessary API calls and associated LLM costs."}, {"title": "Implementation", "content": "In this section, we outline the implementation approach. We begin by describing the automated workflow developed for generating FAQs. Next, we explain the real-time implementation of the Match and Generate models. Finally, we discuss the various approaches considered for simulating these processes and evaluating response latency."}, {"title": "LLM-Agentic Workflow to Generate FAQ:", "content": "In cases where FAQs are unavailable or insufficient, it is necessary to build a comprehensive list of frequently asked questions. The FAQ model relies on this list to match and determine whether a customer query corresponds to an existing FAQ during real-time conversations, thereby minimizing the need to call the RAG model.\nTo automatically generate FAQs from provided transcripts, we employed a set of LLM agents. Historical call transcripts, consisting of turn-wise interactions between human agents and customers, were used as the source data for this process. The size of the transcripts varies depending on the availability of call logs, with a minimum of 500 calls being sufficient. For our experiments, we used a large dataset containing approximately 30,000 calls.\nThe first LLM agent is responsible for analyzing the transcripts to understand the customer's intent during each call and generating a list of questions asked by the customers across all calls in the source transcript. The second LLM agent acts as a critic, reviewing the output from the first agent and discarding irrelevant questions related to greetings, agent information, personal details (e.g., email address, authentication, ticket number, phone/email"}, {"title": "Match and Generate Model", "content": "Now that the FAQ database is established, we move on to the implementation of the Match and Generate model. As discussed in the previous section, the goal is to assist human agents during customer calls by identifying the intent of the customer's query and presenting a list of probable questions. This allows the agent to select the most relevant one, after which the system provides the corresponding answer, speeding up query resolution.\nWe achieve this through two methods: Match and Generate. The model either finds a match in the FAQ database and fetches the corresponding answer, or, if no match is found, it generates relevant questions and retrieves the answer using the existing RAG pipeline.\nThe key challenge lies in identifying the optimal moment during the conversation when the agent requires assistance to resolve the query in real time. At that point, the model should be triggered to present relevant questions.\nThere are several methods to invoke the model. The simplest approach is to allow the agent to request help manually, but this undermines the goal of providing real-time assistance. Another option is for the model to automatically suggest questions whenever it detects a need in the conversation. However, invoking the model after every conversational turn is computationally expensive.\nTo address this, we implemented a fixed rolling window approach, where the model is invoked at regular intervals. Additionally, a manual option allows the agent to request assistance through a button click, providing both proactive and on-demand support."}, {"title": "Implementation Approaches", "content": "We explored various approaches to implement the Match and Generate model at runtime. Multiple"}, {"title": "Results and Observations", "content": "Due to client data privacy restrictions, we are unable to present specific example conversations in this paper. However, we began by developing a proof of concept using historical transcripts from one of our clients. After observing positive outcomes, the FAQ model was fully deployed within the Minerva CQ solution.\nThe following key observations were made regarding the model's performance and results:\n\u2022 Improved RAG Response Quality: When well-structured queries were sent to the RAG model, rather than simple keywords, the quality of the answers retrieved from the KB articles improved significantly. These answers were far more contextually relevant and were verified by client domain experts and human agents.\n\u2022 Effective FAQ Generation from Transcripts: For the automated approach to generating FAQs from historical transcripts, we manually analyzed and verified the quality of the LLM agents' cached output at each stage by sampling random entries. The final list of FAQs was then validated by agent supervisors and solution experts.\n\u2022 Relevance of Matched and Generated Questions: The questions matched and generated by the two parallel LLM threads were rigorously tested during simulations. We iteratively refined the prompts to enhance the relevance and accuracy of the responses, making adjustments until the desired output was consistently achieved.\nEvaluation Approach: To fully evaluate the performance of the deployed model, we propose the following evaluation parameters:\n\u2022 Agent Interaction with the Model: Measure the number of times agents select an FAQ question versus a generated question, compared to instances where they manually type their own query. This will provide insights"}, {"title": "Related Work", "content": "Recent research on real-time assistance systems highlights the transformative impact of AI on the division of labor between humans and machines, particularly in the service sector. Studies such as (Link et al., 2020) explore both the challenges and opportunities presented by AI-driven support systems, which aim to enhance productivity and reduce employee workload by shifting decision-making to advanced, real-time AI technologies. In particular, the growing use of machine learning-powered chatbots and virtual assistants has proven effective in automating routine customer support tasks, providing efficient solutions for common inquiries (KATRAGADDA, 2023).\nBuilding on these advancements, optimizing Natural Language Processing (NLP) and LLMs has become essential for further improving customer service efficiency and enabling hyper-personalization. By integrating NLP and LLMs, businesses can streamline operations through various applications such as chatbots, virtual assistants, sentiment analysis, and text summarization\u2014ultimately boosting productivity and improving the overall customer experience (Kolasani, 2023).\nFurther advancements in RAG systems, such as RAG-based question answering for customer service (Veturi et al., 2024) and the RAGADA architecture (Pitk\u00e4ranta and Pitk\u00e4ranta, 2024), showcase the potential of RAG systems to enhance decision-making in corporate environments. These systems bridge human and AI-driven decision-making, leveraging LLMs to deliver more accurate and context-aware responses.\nThe evaluation of RAG-based chatbots for customer support (Analytics and Sukhwal, 2024) highlights specific challenges in ensuring the accuracy and relevance of system responses. The study emphasizes the critical role of the retriever module, where factors such as keyword generation, prompting strategies, and other parameters significantly impact performance. Despite these advances, there is still a need for further research to address the practical limitations of traditional RAG systems in customer service settings. Two key issues stand out: First, during real-time interactions, human agents often struggle to formulate well-structured queries for the RAG system, reducing its effectiveness. Second, the frequent submission of redundant queries by multiple agents leads to inefficiencies. This underscores the need for a more robust solution, such as the FAQ model proposed in this paper, which can better handle recurring questions and streamline the query process."}, {"title": "Conclusion", "content": "In this paper, we present a novel decision support system that advances beyond traditional RAG approaches by optimizing its use in customer contact centers. Our system addresses key challenges, such as inaccurate query formulation and the redundant retrieval of frequently asked questions (FAQs), by integrating real-time question identification with RAG to improve response accuracy, reduce average handling times (AHT), and lower operational costs. Deployed within Minerva CQ's AI-powered human-agent assist solution, the system demonstrates significant potential to enhance both cost efficiency and operational effectiveness over the long term. Additionally, we introduce an automated LLM-agentic workflow for identifying FAQs from transcripts in scenarios where a pre-existing database is now available. Future research will focus on further advancing beyond RAG, specifically on improving the system's ability to retain conversation history and manage more complex contexts. We also aim to explore the scalability of the system across various industries, driving innovation in real-time question identification and answer generation."}]}