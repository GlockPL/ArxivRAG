{"title": "Fooling LLM graders into giving better grades\nthrough neural activity guided adversarial prompting", "authors": ["Atsushi Yamamura", "Surya Ganguli"], "abstract": "The deployment of artificial intelligence (AI)\nin critical decision-making and evaluation pro-\ncesses raises concerns about inherent biases that\nmalicious actors could exploit to distort decision\noutcomes. We propose a systematic method to\nreveal such biases in AI evaluation systems and\napply it to automated essay grading as an exam-\nple. Our approach first identifies hidden neural\nactivity patterns that predict distorted decision\noutcomes and then optimizes an adversarial in-\nput suffix to amplify such patterns. We demon-\nstrate that this combination can effectively fool\nlarge language model (LLM) graders into assign-\ning much higher grades than humans would. We\nfurther show that this white-box attack transfers\nto black-box attacks on other models, including\ncommercial closed-source models like Gemini.\nThey further reveal the existence of a \"magic\nword\" that plays a pivotal role in the efficacy of\nthe attack. We trace the origin of this magic word\nbias to the structure of commonly-used chat tem-\nplates for supervised fine-tuning of LLMs and\nshow that a minor change in the template can\ndrastically reduce the bias. This work not only\nuncovers vulnerabilities in current LLMs but also\nproposes a systematic method to identify and re-\nmove hidden biases, contributing to the goal of\nensuring Al safety and security.", "sections": [{"title": "1. Introduction", "content": "Human decision-making and evaluation processes, such as\nvoting and academic peer-reviews, are inherently subject\nto biases (Lee et al., 2013; Cortes & Lawrence, 2021). In\nthis context, we define bias in any decision-making system,\nwhether human or artificial, as a sensitivity of the system\nto irrelevant information which could yield inconsistent or\nunreasonable decision outcomes. One notable example in\na human context is the bandwagon effect: in elections,\nfor example, voters can be influenced by the popularity of\nchoices made by others (Kiss & Simonovits, 2014). Sim-\nlarly, in collective evaluation processes, when evaluators\nare exposed to prior ratings, their judgments often converge\ntoward those previous ratings (Botelho, 2024). Another hu-\nman example of more direct relevance to this paper is the\nnonsense math effect (Eriksson, 2012). In this experiment,\nparticipants were asked to score abstracts from academic\npapers. For some participants, the abstracts include an ir-\nrelevant suffix not related to the abstract: 'A mathematical\nmodel $\\textit{TPP = T0 \u2013 fT0d} \u2013 fTpdf)$ is developed to de-\nscribe sequential effects. 'Remarkably, the addition of this\nirrelevant suffix increased the scores assigned by individu-\nals without a mathematical or engineering background.\nIn such research, inputs for participants are typically hand-\ncrafted by researchers, and the resulting outputs are ana-\nlyzed to determine whether the input triggers biases in the\nparticipants. The effectiveness of these experiments often\nhinges on the researchers' ability to build hypotheses and\nto design appropriate inputs, which is largely guided by in-\ntuition or prior knowledge. This reliance makes it chal-\nlenging to systematically uncover hidden potential biases.\nImportantly, a similar challenge exists when studying the\nbiases of machine learning models. Just as with humans,\nuncovering the hidden biases of these models requires care-\nfully crafted inputs. However, rather than relying solely on\nresearcher intuition, a more systematic approach to gener-\nating these inputs could potentially reveal biases that have\nnot yet been recognized.\nSuch a systematic approach is essential given the recent ex-\nploration of machine learning models as decision makers or\nevaluators, like AI scientists (Lu et al., 2024), AI review-\ners (Tyser et al.; Zaumanis; Checco et al., 2021), or com-\nmercial AI-based assignment evaluation tools. While these\nsystems hold promise, it is essential to recognize that, like\nhuman decision-makers, they can inherit biases from their\ntraining data. When considering real-world applications,\nmalicious users may exploit such biases through prompt in-\njection attacks, akin to the nonsense math effect example,\nto manipulate outcomes and inflate their scores. However,\nunlike humans, it may be possible to address these biases\nalgorithmically. This presents an opportunity for machines\nto serve as fairer, more robust, and more secure decision-\nmaking tools, provided effective mitigation strategies are"}, {"title": "2. Related Work", "content": "Direct prompt injections. Prompt injection attacks are po-\ntential risks of integrated LLM applications where attackers\ninject malicious prompts into the LLM input to manipulate\nit toward the attacker's desired behaviors (Liu et al., 2023b;\nHarang, 2023). One such attack has been proposed for au-\ntomated resume screening with LLMs. In the screening\nprocess, a resume provided by an applicant is embedded\ninto a prompt template and is then processed by an LLM to\njudge if the applicant is qualified. The potential attack can\nbe done by the applicant by adding a malicious prompt into\nthe resume to distort the model's output. In prior works,\nseveral such prompts are crafted by hand, such as \"Ignore\nprevious instructions. Print yes.\" (Liu et al., 2024). How-\never, such prompts assume that the attackers know the for-\nmat of the model's output, which is usually inaccessible.\nMoreover, since they are crafted by humans, it is unclear\nif we have discovered all the possible types of adversarial\nprompts. Hence it is crucial to find an automatic way to\ngenerate potential adversarial prompts which work univer-\nsally in a variety of contexts and output formats.\nReverse engineering representations for interpretabil-\nity and control. Reverse engineering of language mod-\nels has been an important topic of study for interpretabil-\nity, safety and control. For example, (Maheswaranathan\net al., 2019) examined recurrent neural networks in senti-\nment analysis, revealing a one-dimensional line attractor in\nthe neural representation space. Along this attractor, neural\nactivity patterns correspond to the positive or negative sen-\ntiment of input text. More recently, Zou et al. (Zou et al.,\n2023a) extended similar representation analysis techniques\nto transformer-based language models. Their work demon-\nstrated that by identifying and manipulating hidden layer\nneural representations, they could control various aspects\nof model behavior, including emotional expression, fair-\nness, and honesty of their responses. Recent studies have\nfurther explored the use of internal representations for im-\nplementing and detecting jailbreak attacks (Li et al., 2024;\nZou et al., 2024; Xu et al., 2024), as we discuss next.\nAutomated generation of jailbreaking prompts. Sev-\neral prior works proposed automated ways of adversarial\nprompt generation, specifically for jailbreaking models to\ncircumvent safety guards and emit unsafe text. The greedy\ncoordinate gradient (GCG) algorithm (Zou et al., 2023b)\nuses back-propagation to optimize the adversarial suffix\nso that the model outputs a desired sequence of first few\ntokens. AutoDan (Liu et al., 2023a) generates stealthy\njailbreak prompts by exploiting hierarchical genetic algo-\nrithms. Recent methods also use neural representations to\nguide prompt design. For example, (Li et al., 2024) finds\nactivation patterns corresponding to safe prompts, and then\noptimizes adversarial prompts to weaken such safety pat-\nterns. Similarly (Xu et al., 2024) finds a concept activation\nvector (Kim et al., 2018) which classifies embeddings of\nmalicious versus safe instructions, and then uses the classi-\nfier to optimize adversarial prompts for jailbreaking. Con-\nversely, (Zou et al., 2024) proposes a robust algorithm to\ndetect jail-breaking by directly operating on internal rep-\nresentations. The focus of these works are on jailbreaking,\nbut on finding hidden biases in the evaluation processes that\ncould distort decisions."}, {"title": "3. Method", "content": "We investigate adversarial attacks on LLM essay grading\nsystems, utilizing \"The Hewlett Foundation: Automated\nEssay Scoring\" dataset from Kaggle (Hamner et al., 2012).\nThis dataset comprises eight essay problem sets, each\ncontaining approximately 2000 high-school student es-\nsays, along with corresponding problem statements, rubric\nguidelines, and scores graded by human experts. This\ndataset is particularly suitable for our research objectives\nfor several reasons: (1) The provided rubric guidelines en-\nable LLMs to understand grading criteria. (2) Human-\ngraded scores serve as ground truth for ensuring LLM grad-\ning quality. (3) The text-only format of student essays elim-\ninates the need for multi-modal models, reducing unneces-\nsary complexity. (4) The relatively concise nature of prob-\nlem statements, rubric guidelines, and essays ensures that\ninput and output sequences fit within context windows of\nrecent open-source models."}, {"title": "3.1. Prompt template for essay grading", "content": "We develop multiple prompt templates for language mod-\nels to grade student essays. While detailed templates are\nprovided in Appendix A, we outline their general struc-\nture here. As illustrated in Figure 2 (a), the model input\ncomprises three components: grading instructions, a stu-\ndent essay, and a brief restatement of the grading task. The\ninstructions include: (1) a short declaration of the LLM's\nrole (2) dataset-provided rubric guideline with score ranges\n(3) required output format specifications (4) problem state-\nment (5) An example essay for each possible score. The\nlanguage models' outputs are expected to adhere to the\nformat given in the input and include an analysis of the\ngiven essay and a score within a given score range. We"}, {"title": "3.2. Identifying the neural representation of scores:\nLLMs have scores in mind well before they speak", "content": "Our adversarial prompt generation process consists of two\nsteps. First, we first identify an activation pattern in a hid-\nden layer that is predictive of the LLM assigning the high-\nest possible score. This activation pattern can be thought\nof as representing a cognitive state of the model, associated\nwith a high evaluation of the given essay. Second, we opti-\nmize adversarial suffixes attached to the end of the essay to\namplify the identified activation pattern.\nFor each essay problem set, we construct prompts for the\nlanguage model using approximately 1500 different essays\nwith each prompt template. In the residual stream of ev-\nery layer and token position, we record the pairs of acti-\nvation patterns and the corresponding LLM-graded score.\nWe employ an open-source LLM, specifically LLama3.1-\n8B-instruct (Dubey et al., 2024), which is one of the most\nrecent publicly available models. This model follows in-\nstructions smoothly, with relatively low inference cost. For\neach essay, we obtain 8 output samples that provide an\nempirical distribution of the scores. We checked that the\nmodel assigns a score within the given score range; cases\nwhere the model does not are discarded. After obtaining\nthe (x,p) pairs, where x represents the activation pattern\nand p denotes the empirical score distribution, we train a\nlinear readout that maps the activation pattern at each layer\nto the logits of each possible score, via\n$f(x; W, b) = Wx + b,$\nwhere $x \\in R^N$ is the activation pattern with the embed-\nding dimension N, $W \\in R^{S\\times N}$ is the weight matrix of\nthe linear readout with the number of possible scores S,\nand $b \\in R^S$ is a bias vector. $f_i(x; W, b)$ represents the\npredicted logit associated to the i-th score. These read-\nout weight matrix and bias vector are trained to minimize\nKullback-Leibler divergence between the predicted score\ndistribution and the empirical one:\n$L(W, b) = \\sum_{(x,p)}KL(\\text{softmax}[f(x; W, b)]; p) + \\lambda ||W||_2$.\nHere the L2-regularization is introduced as the second term\nsince the number of data points (i.e., number of essays)\nis smaller than the embedding dimension N = 4096. In\nour experiment, we use $\\lambda = 2 \\times 10^{-5}$ determined through\ncross-validation using 30% of the training data as a valida-\ntion set. Note that this readout is individually trained for\neach layer and each token position.\nAs is shown in Figure 2 (a), we focus on the two specific\ntoken positions: the end of student essay and the end of\nthe input to the language model. We train a readout for\neach layer, and found that the readouts from the middle\nlayers have equivalent or lower KL loss compared to those\nfrom the activation patterns in later layers, and hence we\nhere focus on 16th layer out of 32 layers in Llama3.1-8B-\ninstruct model. The scatter plots in Figure 2 (b) and (c)\nshow comparisons between the scores graded by the lan-\nguage model and the scores predicted by the linear read-\nouts. Our score comparisons are on four (two times two)\ndifferent setups, with two different essay problem sets and\ntwo different prompt templates. Each scatter plot in the fig-\nure corresponds to each different setup, and each point in\nthe plot corresponds to a single held-out student essay. The\nscore range for each essay problem is shown in Table 2 in\nthe appendix.\nOur analysis reveals a strong correlation coefficient (r =\n0.8~0.9) between the scores read out from the LLM's\nhidden representations and the final scores at the output of\nthe LLM, suggesting that the model forms an implicit eval-\nuation immediately upon processing the student essay, well\nbefore generating an explicit score. This early formation\nof internal evaluation metrics proves crucial for adversarial\nprompt optimization techniques that rely on backpropaga-\ntion, such as Greedy Coordinate Gradient algorithm (Zou\net al., 2023b), which is discussed in the following section.\nComparing the linear readouts at the two different token\npositions, we observe that the readouts from the end of in-\nput tokens demonstrate slightly higher correlation with the\nground truth scores. Based on this finding, we concentrate\nour subsequent analysis on the linear readout specifically at\nthe final token position in the input.\nSince we are interested in cognitive states of the model cor-\nresponding to high-quality essays, we extract the readout\nvectors corresponding to the highest score, i.e., the vector\n$\\{W_{ij}\\}_{j\\in [N]}$ where i is the row corresponding to the high-\nest score. Such a vector can be obtained for each essay\nproblem set and prompt template. Figure 3 compares these\nvectors in terms of cosine similarity. Despite the vector di-\nmension being large (N = 4096), different readout vectors\noverlap highly, suggesting the existence of a cognitive state"}, {"title": "3.3. Adversarial Suffix Optimization", "content": "The goal of our prompt injection attack is to amplify\nthe projection of neural activity onto the average readout\nweight vector shown in Figure 3. Such amplified patterns\nshould correspond to an LLM cognitive state associated\nwith high-quality essays. The optimization of adversarial\nsuffixes involves two steps. First, we employ GCG the al-\ngorithm proposed by (Zou et al., 2023b). In the second\nstep, we further refine the obtained suffixes by eliminating\nredundant tokens.\nThe GCG optimization process resembles the asyn-\nchronous update of Hopfield neural networks (Hopfield,\n1982). Starting with an initial (random) token sequence, we\niteratively update the sequence, in a token-by-token man-\nner, to minimize a specified loss function. At each iteration\nwe: (1) randomly choose a token position to update; (2)\ncompute the gradient of the loss function in the space of\nthe one-hot token representation by backpropagation; (3)\nreplace the current token with a new token selected ran-\ndomly from the top K 256 candidates which lower the\nloss the most based on the computed gradient. This single-\ntoken update is performed multiple times independently in\neach step and then the best token sequence with the lowest\nloss is chosen as the next updated token sequence. Note\nthat we restrict token updates to only lie within the set of\ntokens with purely ASCII characters.\nIn the original set up of the GCG algorithm, they employ\na loss function designed to control the initial tokens of\nmodel's output for specific jailbreaking purposes. How-\never, this approach is ineffective in our case because fix-\ning the initial output tokens can prevent the models from\nfollowing various types of output formats, and the attack-\ners need to know the specified format in advance. Hence,\ninstead of controlling output tokens, we manipulate inter-\nnal activation patterns, or cognitive states, to obtain a more\ngeneral and robust method for controlling the model's be-\nhavior. Accordingly, we define our loss function as the\nnegated inner product between the model's activation pat-\ntern and the target activation vector to achieve amplifica-\ntion of the cognitive states associated with high scores. We\niterate the GCG update 1000 times to obtain the best ad-\nversarial suffice with the lowest loss. Since the algorithm\ntends to be trapped by local minima in the loss landscape,\nwe also repeat this optimization process 10 times using dif-\nferent random seeds. We initially fix each suffice to be 20\ntokens long.\nWhile GCG is one of the most effective algorithms for\nprompt optimization, we observe that the optimized strings\noften contain redundant tokens. These tokens either do\nnot contribute to or occasionally diminish the performance\nof adversarial suffixes, suggesting they can be removed\nto simplify the suffix. We hypothesize that these redun-\ndant tokens emerge because the number of tokens remains\nconstant during GCG's optimization process, leaving the\nalgorithm without a mechanism to eliminate unnecessary\nelements. To address this, we conduct a token removal\nprocess after GCG optimization. For each 20 token suf-\nfix generated by GCG, we systematically evaluate the im-\npact of removing individual tokens by measuring the av-\nerage normalized score gain with the same set of essay\nproblems and prompt templates used in the GCG optimiza-\ntion. We define the normalized score gain on a single es-\nsay as the score of the essay with the token suffix minus\nthe score of the original essay without it, divided by\noverall score range. This evaluation was conducted us-\ning Llama3.1-8B-Instruct-Turbo\u00b9 . The average normalized\nscore gains are calculated by averaging results across 100\nessays for each problem-template combination and then av-"}, {"title": "4. Analysis of the optimized suffixes", "content": "In this section, we discuss the effectiveness of the obtained\nadversarial suffixes and a hidden bias of language models."}, {"title": "4.1. Performance of the adversarial suffixes", "content": "We first measure the effectiveness of the three adversarial\nsuffixes obtained from attacking Llama3.1-8B-Instruct. To\ntest the generalization capability of the suffixes, we evalu-\nate their ability to improve essay scores on essay problem\nsets and prompt templates which are not used to optimize\nour adversarial suffixes. In Figure 4, the score gain with\nthe three adversarial prompts in Table 1 are shown. Each\npoint in the scatter plot corresponds to a single student es-\nsay. The adversarial suffixes are clearly effective in im-\nproving scores on these held-out essay problem sets and\nprompt templates.\nWe next investigate whether the ability of these adversarial\nsuffixes to improve scores transfers to different language\nmodels. We obtain score gains with adversarial suffix #1\napplied to various publicly available supervised fine-tuned\nlanguage models. 3 We show our transfer results on 4 dif-\nferent models in Figure 5, and 3 more models in Figure 10\nin the Appendix. While the effectiveness of the adversar-\nial suffix varies across models, it does successfully trans-\nfer to many tested models with different essay problems"}, {"title": "4.2. A hidden bias: the nonsense \"user\" effect", "content": "Our ability to algorithmically find adversarial suffixes that\ndistort output decisions (in this case essay grades) can in\ngeneral reveal hidden biases of LLMs, whereby irrelevant\ninput features can distort decision outputs. In this subsec-\ntion, we discuss just such a hidden bias we found, which\nwe call the nonsense \"user\" effect, inspired by the term\n\"nonsense math effect\" (Eriksson, 2012) discussed in the\nintroduction. This bias can be easily noticed by observ-\ning Table 1. Interestingly, all of these adversarial suffixes\ncontain the word \u201cuser\u201d near the beginning, indicating the\npossible importance of this magical word in distorting the\nmodels' evaluation. We perform a quantitative investiga-\ntion of the contribution of this word by conducting an ab-\nlation study: we remove a single token at a time from each\nof the three adversarial suffixes and then evaluate how the\naverage score gain changes after the removal. If a given\ntoken in a given suffix plays an important functional role in\ncontributing to the score gain, then the removal of the token\nshould significantly reduce the score gain.\nFor each adversarial suffix with a single token removed,\nwe compute the average normalized score gain across 200\ndifferent student essays of essay problem #3 and #4, and\nacross prompt template #3 and #4. In Figure 6, we plot\nthese average normalized score gains after single token re-\nmovals (blue bars) and compare them to the original score\ngain before token removal (dashed black line).\nNote that this study is done with Google's gemini-1.5-flash\nmodel, which is different from the model used for optimiz-"}, {"title": "4.3. Debugging the nonsense \"user\" effect", "content": "Why do these LLMs exhibit a nonsense \"user\" effect? In\nthis section, we analyze the cause of this bias and propose\na straightforward solution.\nWe hypothesize that this bias stems from the chat tem-"}, {"title": "5. Conclusion", "content": "We have presented a novel and systematic approach to un-\ncover hidden biases in LLM evaluation systems, and illus-\ntrated its application to automated essay grading. Identify-\ning such biases is crucial to ensure the security and fairness\nof these systems, as they are susceptible to prompt injection\nattacks by malicious actors seeking to exploit these biases\nto manipulate model decisions. We develop a systematic\nmethod for generating adversarial suffixes that successfully\ninflate grading scores, and generalization across different\nessay problems, prompt templates, and even language mod-\nels. Our analysis reveals a significant bias associated with a\nword \"user\" in the adversarial suffixes, which proves essen-\ntial for their effectiveness. Finally, we show that a simple\nmodification of chat templates commonly used for super-\nvised fine-tuning can drastically reduce the vulnerability to\nsuch attacks. This work highlights the importance of proac-\ntively identifying and mitigating hidden biases in language\nmodels to ensure their robustness, fairness, and reliability\nin real-world applications."}, {"title": "Limitations and Future Directions", "content": "Our current optimized adversarial suffixes primarily exploit\na vulnerability related to a word \"user\". Future research\nshould focus on discovering other types of biases inherent\nin language models. One avenue is to investigate the behav-\nior of optimized adversarial suffixes in models fine-tuned\nwith revised chat templates.\nAn very interesting future research direction is to fine-tune\nmodels on datasets of human evaluations such as data from\nOpenReview. Such models should inherit the the biases\nof human evaluators, and our scheme might systematically\nidentify such hidden human biases."}]}