{"title": "REDIFINE: REUSABLE DIFFUSION FINETUNING FOR MITIGATING DEGRADATION IN THE CHAIN OF DIFFUSION", "authors": ["Youngseok Yoon", "Dainong Hu", "Iain Weissburg", "Yao Qin", "Haewon Jeong"], "abstract": "Diffusion models have achieved tremendous improvements in generative modeling for images, enabling high-quality generation that is indistinguishable by humans from real images. The qualities of images have reached a threshold at which we can reuse synthetic images for training machine learning models again. This attracts the area as it can relieve the high cost of data collection and fundamentally solve many problems in data-limited areas. In this paper, we focus on a practical scenario in which pretrained text-to-image diffusion models are iteratively finetuned using a set of synthetic images, which we call the Chain of Diffusion. Finetuned models generate images that are used for the next iteration of finetuning. We first demonstrate how these iterative processes result in severe degradation in image qualities. Thorough investigations reveal the most impactful factor for the degradation, and we propose finetuning and generation strategies that can effectively resolve the degradation. Our method, Reusable Diffusion Finetuning (ReDiFine), combines condition drop finetuning and CFG scheduling to maintain the qualities of generated images throughout iterations. ReDiFine works effectively for multiple datasets and models without further hyperparameter search, making synthetic images reusable to finetune future generative models.", "sections": [{"title": "Introduction", "content": "Can state-of-the-art AI models learn from their own outputs and improve themselves? As generative AI (e.g., GPT [1],\nDiffusion [2, 3]) can now churn out uncountable synthetic texts and images, this question piqued curiosity from many\nresearchers in the past couple of years [4-18]. While some show positive results of self-improving [4, 15], most\nreport an undesirable \"model collapse\" [5-14, 16\u201318]\u2014a phenomenon where a model's performance degrades when\nit goes through multiple cycles of training with the self-generated data. When large language models (LLMs) are\ntrained with their own outputs, it begins to produce low-quality text that has a lot of repetitions [19], and its linguistic\ndiversity declines rapidly [8, 16]; image models also show quality degradation [7, 10] and loss of diversity [12,20]. This\nphenomenon has even been compared to mad cow disease, a fatal neurodegenerative disease caused by feeding cattle\nwith the carcasses of other cattle [12].\nOur work examines model collapse in a common use case of generative AI: customizing pretrained text-to-image\ngenerative models through finetuning. While previous works [5, 12, 14, 19] suggest including more real images in the\ntraining set to avoid model collapse, this is challenging for several reasons. First, the number of artworks generated by a\nsingle artist is often limited. Even highly productive artists create at most a few thousands of pieces during their lifetime\n(e.g., Monet produced 1,983 paintings over 61 years). Secondly, during image collection, it is easy to be deceived by\nrealistic AI-generated images and accidentally include them in the training set. Therefore, we need to understand mode\ncollapse behavior when the proportion of real data is limited and seek solutions beyond simply increasing the number of"}, {"title": "Related Work", "content": "The self-consuming training loop and the associated phenomenon known as \u201cmodel collapse\u201d have become significant\nareas of study in the past two years [5-18]. Model collapse, defined as \u201ca degenerative process affecting generations of\nlearned generative models, where generated data end up polluting the training set of the next generation of models\u201d"}, {"title": "Model Collapse in Self-Consuming Chain of Diffusion Finetuning", "content": ""}, {"title": "Problem setting & Experimental Setup", "content": "Chain of Diffusion We begin with formally defining the self-consuming chain of diffusion finetuning. Given a\npretrained generative model Mo, an original training image set Do = {xo,i|i \u2208 [0, N \u2013 1]}, and a prompt set P =\n{yili \u2208 [0, N-1]}, where N is the number of total images in the dataset, each image xo,i is paired with a corresponding\ntext prompt yi. Mk+1 is a model finetuned from Mo using the generated image set Dk = {xk,ii \u2208 [0, N \u2013 1]} and the\nprompt set P, which simulates a common adaptation case of a pretrained generative model with a personally collected\ndataset. Then, Mk+1 generates a set of images Dk+1 for the next iteration using the prompt set P:\nMk+1 = Finetune(Mo, Dk, P),\n(1)\nDk+1 = Generate(Mk+1, P).\n(2)\nDuring the Chain of Diffusion, Mo and P are fixed across all the iterations. To maintain the same size of training\ndataset, we generate one image per text prompt for all iterations. The overall pipeline of the Chain of Diffusion is\nshown in Figure 1(a).\nModel and datasets. We use Stable Diffusion v1.5 [3] as the pretrained model Mo and apply LoRA [21] to finetune\nthe Stable Diffusion at each iteration. We build our implementation on [26] and perform experiments on four datasets:"}, {"title": "ReDiFine", "content": "In the previous section, we showed that choosing a good CFG scale can considerably improve reusability in the Chain\nof Diffusion. However, such optimal CFG scales vary from dataset to dataset. To mitigate model collapse solely"}, {"title": "Condition drop finetuning.", "content": "We propose condition drop finetuning, which randomly drops the text condition during finetuning to update both\nconditional and unconditional scores. Even though condition drop was suggested in the original CFG paper [32],\nit is not commonly done during finetuning since the first-iteration image qualities are outstanding without updating\nunconditional scores. However, the small difference between conditional and unconditional scores can accumulate over\niterations and create a large gap as the iteration progresses in the Chain of Diffusion if we do not explicitly finetune the\nunconditional scores as shown in Figure 4(b). Inspired by this observation, we drop text conditions with probability\n0.2 during finetuning. We illustrate the effect of condition drop with CFG 7.5 in Figure 4(b). Note that the norm of\nDiff does not grow as fast even with a high CFG of 7.5. While this prevents a potential cause of the high-frequency\ndegradation, the Diff value still spikes up in iteration 6. We conjecture that this is because the D5 already contains\nsevere artifacts that conditional scores have to grow substantially to fit the data."}, {"title": "CFG Scheduling.", "content": "Based on the analysis showing that a high CFG scale amplifies the discrepancy between the condition and uncondition\nscores during iterative fine-tuning (see Figure 4(b)), we propose gradually reducing the CFG scale during diffusion\nsteps. This CFG scheduling mechanism mitigates the negative impacts of overemphasizing the conditional score in later\nstages while preserving the initial conditional information in the early stages. Specifically, we exponentially decrease\nthe CFG scale s during T diffusion steps as\ns = s_0 \\times e^{-axt/T}\n(6)\nwhere T is 30 and so is 7.5. \u03b1 denotes the degree of decrease and we use 2 throughout this paper. Comparison to\ndifferent \u03b1 and linear decreasing can be found in Appendix E.\nOur method gradually decreases the CFG scale during diffusion steps, leveraging the benefits of high CFG scale in\nthe early stage and low CFG scale in the later stage. This adaptive approach aligns with observations from [34] that\ndifferent steps contribute uniquely to the generation process. In addition, our CFG scheduling takes advantage of using\na small CFG scale without the need for extensive hyperparameter searches for each dataset, providing a flexible and\nrobust choice for the Chain of Diffusion."}, {"title": "Reusable Image Generation", "content": "In this section, we present generated images and qualitative metrics for ReDiFine. We use the CFG scale so 7.5 for all\ndatasets and experiments except for the ablation study. Note that ReDiFine can be further improved with hyperparameter\ntuning, e.g., CFG scale, condition drop probability, and CFG scheduling function. However, our focus in this work is to\ndemonstrate that ReDiFine can robustly stabilize iterative finetuning regardless of datasets and hyperparameters. This\nrobustness provides a great advantage when applying our ReDiFine to finetune Stable Diffusion and generate images\nusing any personally collected image sets."}, {"title": "Conclusion and Discussion", "content": "Motivated by the prominent usage of diffusion models by artists and the trend of the Internet being filled with synthetic\ndata, we investigate how image quality degrades as we feed diffusion-generated synthetic images to finetune another\ndiffusion model. In our self-consuming chain of diffusion finetuning setting, we observe image degradation in all four\ndatasets we tested. We reveal that CFG plays a crucial role in the trade-off between image reusability and its perceptual\nquality in the first iteration. We propose a novel idea of generating reusable iamges and propose the ReDiFine strategy\nthat combines two small techniques (condition drop finetuning and CFG scheduling) that can be easily implemented in\nthe diffusion finetuning pipeline with no requirement for additional parameter tuning.\nWe started this paper with a question: can current AI models learn from their own output and improve themselves?\nThis is not only an imminent question as synthetic data rapidly outnumbers human-generated data [37,38], but also a\nlong-term existential question as it relates to reaching AI singularity\u2014the point where AI learns from its own output to\ncontinuously enhance its intelligence and far surpasses human intelligence. Our paper shows a glimpse that widely-used"}]}