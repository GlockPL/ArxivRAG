{"title": "REDIFINE: REUSABLE DIFFUSION FINETUNING FOR MITIGATING DEGRADATION IN THE CHAIN OF DIFFUSION", "authors": ["Youngseok Yoon", "Dainong Hu", "Iain Weissburg", "Yao Qin", "Haewon Jeong"], "abstract": "Diffusion models have achieved tremendous improvements in generative modeling for images, enabling high-quality generation that is indistinguishable by humans from real images. The qualities of images have reached a threshold at which we can reuse synthetic images for training machine learning models again. This attracts the area as it can relieve the high cost of data collection and fundamentally solve many problems in data-limited areas. In this paper, we focus on a practical scenario in which pretrained text-to-image diffusion models are iteratively finetuned using a set of synthetic images, which we call the Chain of Diffusion. Finetuned models generate images that are used for the next iteration of finetuning. We first demonstrate how these iterative processes result in severe degradation in image qualities. Thorough investigations reveal the most impactful factor for the degradation, and we propose finetuning and generation strategies that can effectively resolve the degradation. Our method, Reusable Diffusion Finetuning (ReDiFine), combines condition drop finetuning and CFG scheduling to maintain the qualities of generated images throughout iterations. ReDiFine works effectively for multiple datasets and models without further hyperparameter search, making synthetic images reusable to finetune future generative models.", "sections": [{"title": "1 Introduction", "content": "Can state-of-the-art AI models learn from their own outputs and improve themselves? As generative AI (e.g., GPT [1], Diffusion [2, 3]) can now churn out uncountable synthetic texts and images, this question piqued curiosity from many researchers in the past couple of years [4\u201318]. While some show positive results of self-improving [4, 15], most report an undesirable \"model collapse\" [5\u201314, 16\u201318]\u2014a phenomenon where a model's performance degrades when it goes through multiple cycles of training with the self-generated data. When large language models (LLMs) are trained with their own outputs, it begins to produce low-quality text that has a lot of repetitions [19], and its linguistic diversity declines rapidly [8, 16]; image models also show quality degradation [7, 10] and loss of diversity [12, 20]. This phenomenon has even been compared to mad cow disease, a fatal neurodegenerative disease caused by feeding cattle with the carcasses of other cattle [12].\nOur work examines model collapse in a common use case of generative AI: customizing pretrained text-to-image generative models through finetuning. While previous works [5, 12, 14, 19] suggest including more real images in the training set to avoid model collapse, this is challenging for several reasons. First, the number of artworks generated by a single artist is often limited. Even highly productive artists create at most a few thousands of pieces during their lifetime (e.g., Monet produced 1,983 paintings over 61 years). Secondly, during image collection, it is easy to be deceived by realistic AI-generated images and accidentally include them in the training set. Therefore, we need to understand mode collapse behavior when the proportion of real data is limited and seek solutions beyond simply increasing the number of real images. In this paper, we propose a novel concept of generating \u201creusable\u201d images, which refers to their potential for finetuning without causing catastrophic model collapse.\nOur goal in this paper is two-fold: (1) to conduct an in-depth empirical analysis of model collapse behavior in the self-consuming chain of diffusion finetuning (referred to as Chain of Diffusion)\u2014a setting already widely used by artists\u2014and to examine which factors in finetuning and sampling affect the reusability of diffusion-generated images; (2) to develop a strategy that can improve the reusability of synthetic images. Our contributions toward these goals are summarized below:\n\u2022 We consider a setting widely used for style transfer in diffusion models: finetuning Stable Diffusion [3] with low-rank adaptation (LoRA) finetuning [21] using a moderately-sized training set(~1000 images). We examine the evolution of images in the Chain of Diffusion across four datasets: two digital art datasets (Pokemon [22] and Kumapi [23]) and two natural image datasets (Butterfly [24] and CelebA-1k [25]). Through extensive analysis, we show that model collapse universally occurs in all datasets regardless of training set size, real image mixing (up to 90%), and a wide range of scenarios with different configurations (see Figure 1(b) and Table 1).\n\u2022 We discover that among all factors in the Chain of Diffusion, only one substantially impacts the speed of model collapse: classifier-free guidance (CFG) scale. With low CFG, we observe low-frequency degradation where images gradually become unrecognizably blurry. With high CFG scales, high-frequency degradation occurs, where some features of the images become amplified beyond the normal range with disturbing color saturation. There exists a medium CFG range that can significantly slow down model collapse (see Figure 2(b)). To the best of our knowledge, we are the first to demonstrate such a role of CFG in the Chain of Diffusion.\n\u2022 Further, we reveal that the default CFG scale 7.5 widely used for Stable Diffusion performs very poorly in terms of reusability metric (defined in Eqn (4)) despite its excellent image quality in the first iteration (measured by FID). This highlights the trade-off between first-iteration image quality and future reusability (see Figure 2(a)).\n\u2022 We develop a new finetuning strategy, Reusable Diffusion Finetuning (ReDiFine), to mitigate degradation inspired by our analysis of how CFG modulates the model collapse pattern in latent diffusion models. We combine condition drop (for finetuning) and CFG scheduling (for generation) in ReDiFine. While an optimal CFG scale can sufficiently slow down model collapse, it is impractical to sweep through CFG scales over multiple iterations for each dataset to find an optimal trade-off. With our proposed ReDiFine approach, we show that even the default CFG scale (CFG=7.5) can generate reusable images across all four datasets without the need for parameter tuning (see Figure 5 and 6)."}, {"title": "2 Related Work", "content": "The self-consuming training loop and the associated phenomenon known as \u201cmodel collapse\u201d have become significant areas of study in the past two years [5\u201318]. Model collapse, defined as \u201ca degenerative process affecting generations of learned generative models, where generated data end up polluting the training set of the next generation of models\u201d in [11], has been observed in both language and image generative models. Empirical studies on LLMs [8, 15, 16] reveal that linguistic diversity collapses, especially in high-entropy tasks [8], although this can be mitigated with data accumulation [15]. In image generation, several works [5, 7, 10, 12, 18, 20] note image degradation when diffusion models are recursively trained with self-generated data. The authors of [5] and [12] provide theoretical insights into diversity reduction, demonstrating that the covariance in a toy Gaussian model converges to zero as iterations increase in the self-consuming training loop. Among theoretical studies on this topic [13, 14, 17, 19], [17] is particularly relevant. They use dynamical system tools to show that generative distributions either collapse to a small set of outputs or become uniform over a large set, depending on the temperature setting. This finding is analogous to our observations with modulating CFG scales. Connecting our empirical analysis on finetuning Stable Diffusion with their theoretical framework could be an interesting future direction.\nRegarding mitigation strategies, existing works echo the importance of incorporating a large proportion of real data throughout the training loop [5, 12, 14] or accumulating more data [15]. The only mitigation strategy beyond altering the training data composition is proposed by [6], who suggests a self-correcting self-consuming loop using an expert model to correct synthetic outputs. While this approach is demonstrated in human motion generation with a physics simulator, having an expert model may not always be feasible for finetuning customized image generative models. In our work, we explore an alternative solution through reusable image generation. By making small improvements in finetuning and sampling steps, we can substantially enhance the reusability of images, allowing them to be used for many more iterations before the model starts to deteriorate. In a tangentially related work [9], the authors discuss bias amplification in self-consuming chains, noting minimal amplification over iterations."}, {"title": "3 Model Collapse in Self-Consuming Chain of Diffusion Finetuning", "content": "We begin with formally defining the self-consuming chain of diffusion finetuning. Given a pretrained generative model Mo, an original training image set Do = {xo,i|i \u2208 [0, N \u2013 1]}, and a prompt set P = {yili \u2208 [0, N-1]}, where N is the number of total images in the dataset, each image xo,i is paired with a corresponding text prompt yi. Mk+1 is a model finetuned from Mo using the generated image set Dk = {xk,ii \u2208 [0, N \u2013 1]} and the prompt set P, which simulates a common adaptation case of a pretrained generative model with a personally collected dataset. Then, Mk+1 generates a set of images Dk+1 for the next iteration using the prompt set P:\nMk+1 = Finetune(Mo, Dk, P), (1)\nDk+1 = Generate(Mk+1, P). (2)\nDuring the Chain of Diffusion, Mo and P are fixed across all the iterations. To maintain the same size of training dataset, we generate one image per text prompt for all iterations. The overall pipeline of the Chain of Diffusion is shown in Figure 1(a).\nWe use Stable Diffusion v1.5 [3] as the pretrained model Mo and apply LoRA [21] to finetune the Stable Diffusion at each iteration. We build our implementation on [26] and perform experiments on four datasets:\nObservation 1: Model collapse is universal in the Chain of Diffusion. We observe significant image degradation in all four datasets (see Figure 1(b)) in the Chain of Diffusion. The quality of generated images begins to clearly deteriorate in the third iteration and it drops even more rapidly once the visible degradation emerges, reaching an unrecognizable level in two or three additional iterations. We investigated a variety of different scenarios (summarized in Table 1) to see if this degradation is an anomaly of specific hyperparameter settings or if it is a ubiquitous phenomenon. We tested various dataset sizes for Do and Dk (Appendix C.1), increasing the size of synthetic datasets Dk by generating more than one image per prompt (Appendix C.2), mixing real images from Do to Dk (Appendix C.3), changing the descriptiveness of prompts (Appendix C.4), freezing U-Net or text encoder during finetuning (Appendix C.5), and various other hyperparameters (# sampling steps, # epochs, learning rate, and CLIP skip) (Appendix C.6, C.7, C.8, and C.9). We also tested adding a small Gaussian noise in each image in the original set Do to see if having small random perturbations can improve reusability (Appendix C.10), and investigated if the degradation happens for larger Stable Diffusion (Appendix C.11). In all settings we tested, image degradation was universally present. We plot the results for Pokemon dataset on Figure 2(a) where y-axis is reusability and x-axis is the FID1. While adding noise and mixing 90% real images show the best reusability (the lower, the better), they still exhibit significant degradation (FID score has been doubled in 6 iterations). More visual inspections and metric plots can be found in Appendix C.\nObservation 2: CFG is the only factor that impacts the model collapse. Throughout all our experiments, classifier-free guidance (CFG) had the biggest impact in the speed of model collapse. CFG scale was first introduced in [32] to modulate the strength between unconditional and conditional scores at each diffusion step as follows:\nTotal Score = Unconditional Score + CFG (Conditional Score \u2013 Unconditional Score). (5)\nHigh CFG means that we emphasize the conditional score for the given prompt more, which pushes the generation to align better with the prompt and often leads to higher-fidelity images. On the other hand, lower CFG places less weight on the conditional score and provides more diversity in generated images. For those familiar with temperature sampling [33], CFG plays a similar role as temperature, which adjusts the trade-off between fidelity and diversity.\nObservation 3: High CFG scales cause high-frequency degradation and low CFG scales cause low-frequency degradation. CFG scale does not only affect the speed of model collapse, but also the pattern of model collapse. As shown in Figure 2(b), CFG 1.0 makes the images progressively more blurry in the Chain of Diffusion, eventually collapsing to images without any structure, which we refer to low-frequency degradation. On the other hand, for CFG 7.5 how images degrade looks completely different: some features start to be emphasized excessively, repetitive patterns begin to appear, and the overall color distribution becomes saturated. The t-SNE plot in Figure 3(a) clearly demonstrates that the distribution shift over iterations follows distinct paths for high, low, and medium CFG scales. These patterns were consistent in all four datasets and detailed results can be found in Appendix B.\nWe further analyze this phenomenon by examining the latent space. First, we show a histogram of the final latent vectors before decoding in Figure 4(a). For a CFG of 1.0, the latent distribution quickly evolves into a Gaussian-like shape, with its variance decreasing over iterations. This behavior aligns with previous work [5, 12, 19], which theoretically predicted that the self-consuming loop would crop out the tails of the distribution, reducing output diversity until it collapses to a single mode. We believe that these narrowing distributions in the latent space lead to blurrier and more homogeneous outputs in the image space. For a CFG of 7.5, the evolution of the latent distribution is completely opposite, creating longer tails and approaching a more uniform distribution over space. For a CFG scale of 2.5, which has the best reusability among the three, the latent distribution is better preserved over iterations.\nWe then plot how the average norm of Diff (= Cond Score - Uncond Score) evolves over diffusion steps in different iterations (Figure 4(b)). In the first iteration, CFG 1.0 has the highest Diff value, followed by CFG 2.5 and CFG 7.5. It can be understood as the models' adaptive behavior to preserve the values added to the latent vectors, Diff multiplied by CFG scale, at each step. This trend changes in later iterations. The Diff value for CFG 7.5 continues to grow with each iteration, and by iteration 6, we observe high Diff values during the entire diffusion steps, creating a significant gap compared to CFG 2.5 and CFG 1.0. We conjecture that this accumulation is the cause of high-frequency degradation of images for CFG 7.5. In contrast, the Diff value for CFG 1.0 remains relatively stable or even decreases over iterations.\nImplications of our observations. Through extensive investigations, we gained a comprehensive understanding of CFG scales' effects on the Chain of Diffusion. Even though a high CFG scale of 7.5 is a common choice to generate visually appealing images, it significantly sacrifices reusability to achieve slightly better FID. Moreover, it is noteworthy that CLIP score increases for the early three iterations and drops rapidly in the latter when the CFG scale is 7.5 (Figure 3(b)). This shows that Stable Diffusion tends to generate images favored by CLIP encoder, which can contain artifacts easily amplified by further finetuning. Sampling for maximizing the perceptual quality was coined as 'sampling bias' in [12] and they provide FID plots with varying levels of sampling bias by changing the CFG scale. While they reported a monotonic decrease in reusability as CFG increased from 1.0 to 2.0, we show that the holistic picture is not entirely monotonic when we look at a wider range of CFG scales from 1.0 to 10.0. It shows an intriguing trade-off between perceptual quality and reusability. This suggests that if developers of diffusion models are mindful of the reusability metric, can improve future generations of images substantially by carefully choosing CFG."}, {"title": "4 ReDiFine", "content": "In the previous section, we showed that choosing a good CFG scale can considerably improve reusability in the Chain of Diffusion. However, such optimal CFG scales vary from dataset to dataset. To mitigate model collapse solely with CFG tuning, we have to do multiple rounds of finetuning and sampling to evaluate the reusability-FID trade-off. This is too computationally burdensome to be a practical solution. In this section, we propose ReDiFine, Reusable Diffusion Finetuning, which can mitigate model collapse without any parameter tuning. ReDiFine combines two ideas-condition drop finetuning and CFG scheduling\u2014and effectively improves the reusability-FID trade-off in all four datasets (comparable to optimal CFG scales).\nWe propose condition drop finetuning, which randomly drops the text condition during finetuning to update both conditional and unconditional scores. Even though condition drop was suggested in the original CFG paper [32], it is not commonly done during finetuning since the first-iteration image qualities are outstanding without updating unconditional scores. However, the small difference between conditional and unconditional scores can accumulate over iterations and create a large gap as the iteration progresses in the Chain of Diffusion if we do not explicitly finetune the unconditional scores as shown in Figure 4(b). Inspired by this observation, we drop text conditions with probability 0.2 during finetuning. We illustrate the effect of condition drop with CFG 7.5 in Figure 4(b). Note that the norm of Diff does not grow as fast even with a high CFG of 7.5. While this prevents a potential cause of the high-frequency degradation, the Diff value still spikes up in iteration 6. We conjecture that this is because the D5 already contains severe artifacts that conditional scores have to grow substantially to fit the data.\nBased on the analysis showing that a high CFG scale amplifies the discrepancy between the condition and uncondition scores during iterative fine-tuning (see Figure 4(b)), we propose gradually reducing the CFG scale during diffusion steps. This CFG scheduling mechanism mitigates the negative impacts of overemphasizing the conditional score in later stages while preserving the initial conditional information in the early stages. Specifically, we exponentially decrease the CFG scale s during T diffusion steps as\ns = s0 \u00d7 e\u2212\u03b1\u00d7t/T (6)\nwhere T is 30 and so is 7.5. \u03b1 denotes the degree of decrease and we use 2 throughout this paper. Comparison to different \u03b1 and linear decreasing can be found in Appendix E.\nOur method gradually decreases the CFG scale during diffusion steps, leveraging the benefits of high CFG scale in the early stage and low CFG scale in the later stage. This adaptive approach aligns with observations from [34] that different steps contribute uniquely to the generation process. In addition, our CFG scheduling takes advantage of using a small CFG scale without the need for extensive hyperparameter searches for each dataset, providing a flexible and robust choice for the Chain of Diffusion."}, {"title": "5 Reusable Image Generation", "content": "In this section, we present generated images and qualitative metrics for ReDiFine. We use the CFG scale so 7.5 for all datasets and experiments except for the ablation study. Note that ReDiFine can be further improved with hyperparameter tuning, e.g., CFG scale, condition drop probability, and CFG scheduling function. However, our focus in this work is to demonstrate that ReDiFine can robustly stabilize iterative finetuning regardless of datasets and hyperparameters. This robustness provides a great advantage when applying our ReDiFine to finetune Stable Diffusion and generate images using any personally collected image sets."}, {"title": "6 Conclusion and Discussion", "content": "Motivated by the prominent usage of diffusion models by artists and the trend of the Internet being filled with synthetic data, we investigate how image quality degrades as we feed diffusion-generated synthetic images to finetune another diffusion model. In our self-consuming chain of diffusion finetuning setting, we observe image degradation in all four datasets we tested. We reveal that CFG plays a crucial role in the trade-off between image reusability and its perceptual quality in the first iteration. We propose a novel idea of generating reusable iamges and propose the ReDiFine strategy that combines two small techniques (condition drop finetuning and CFG scheduling) that can be easily implemented in the diffusion finetuning pipeline with no requirement for additional parameter tuning.\nWe started this paper with a question: can current AI models learn from their own output and improve themselves? This is not only an imminent question as synthetic data rapidly outnumbers human-generated data [37, 38], but also a long-term existential question as it relates to reaching AI singularity\u2014the point where AI learns from its own output to continuously enhance its intelligence and far surpasses human intelligence. Our paper shows a glimpse that widely-used text-to-image models are not ready to improve from their own creation quite yet. However, we believe that there are many open questions on how we develop AI in a world dominated by AI data. We present one angle by emphasizing the importance of generating reusable data for future AI developers. Another pathway could be developing a new learning algorithm that can discern real and synthetic data and take a different training technique for each."}]}