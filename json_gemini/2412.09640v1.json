{"title": "Blockchain Data Analysis in the Era of Large-Language Models", "authors": ["Kentaroh Toyoda", "Xiao Wang", "Mingzhe Li", "Bo Gao", "Yuan Wang", "Qingsong Wei"], "abstract": "Blockchain data analysis is essential for deriving insights, tracking transactions, identifying patterns, and ensuring the integrity and security of decentralized networks. It plays a key role in various areas, such as fraud detection, regulatory compliance, smart contract auditing, and decentralized finance (DeFi) risk management. However, existing blockchain data analysis tools face challenges, including data scarcity, the lack of generalizability, and the lack of reasoning capability.\nWe believe large language models (LLMs) can mitigate these challenges; however, we have not seen papers discussing LLM integration in blockchain data analysis in a comprehensive and systematic way. This paper systematically explores potential techniques and design patterns in LLM-integrated blockchain data analysis. We also outline prospective research opportunities and challenges, emphasizing the need for further exploration in this promising field. This paper aims to benefit a diverse audience spanning academia, industry, and policy-making, offering valuable insights into the integration of LLMs in blockchain data analysis.", "sections": [{"title": "INTRODUCTION", "content": "Blockchain data analysis involves the examination of data recorded on and off the blockchain networks to derive insights, identify patterns, and monitor activities. The need for robust blockchain data analysis emerged alongside the rise of cryptocurrencies like Bitcoin. Early use cases focused on tracing illicit transactions and addressing concerns about the misuse of pseudonymous networks (e.g., SilkRoad [1]). To-day, blockchain data analysis spans diverse domains, from auditing smart contracts and detecting network anomalies to predicting market trends and assessing the impact of governance proposals.\nWe have seen a variety of methods that analyze on-chain and off-chain data using statistics, machine learning, graph analysis, and natural language processing (NLP). While traditional analytical methods have provided valuable in-sights, they often face challenges such as limitations due to the scarcity of ground truth, the lack of generalizability, and the lack of explainability.\nWe believe that large language models (LLMs) will overcome these challenges and have enormous potential as a core technology for a wide range of blockchain data analysis tasks. In particular, LLMs could move the needle in blockchain data analysis from the following perspectives.\n1) Pre-trained Knowledge to Address Data Scarcity:\nA major challenge in blockchain data analysis is the scarcity of ground truth data or labeled datasets re-quired for effective machine learning models. LLMs, trained on vast amounts of diverse data, bring ex-tensive pre-trained knowledge that enables them to infer insights even in the absence of domain-specific datasets. This capability allows LLMs to act as robust tools for blockchain analytics, especially in scenarios where labeled data is limited or unavail-able.\n2) Generalizability Across Multiple Blockchains:\nBlockchains vary widely in their underlying pro-tocols and data structures (e.g., UTXOs versus ac-count models). Traditional analytical tools often re-quire customization to be compatible with specific blockchain platforms. LLMs, however, exhibit high generalizability, enabling them to adapt to mul-tiple blockchains without requiring extensive re-engineering. This makes LLMs particularly suited for environments where interoperability and scala-bility across heterogeneous blockchain networks are critical.\n3) Explainability for Insightful Decision-Making:\nblockchain data analysis often generates complex insights that need to be interpreted by developers, auditors, and regulators. Explainability, one of the core strengths of LLMs, allows these models to provide understandable reasoning behind the out-puts they generate. This capability not only builds trust in the insights derived from LLMs but also facilitates better decision-making by enabling users to understand the rationale behind key findings. In contexts such as fraud detection, compliance, and governance, explainability is essential for actionable insights and regulatory alignment.\nLLMs have shown promising results in specific areas of blockchain data analysis, such as smart contract auditing, where they help detect vulnerabilities and optimize code. While such individual use cases exist, there is a lack of a uni-fied framework to understand how LLMs can be effectively deployed across the diverse analytical tasks that blockchain ecosystems demand."}, {"title": "2 BLOCKCHAIN DATA ANALYSIS", "content": "blockchain data analysis refers to the process of examining data recorded on blockchain networks and/or data avail-able outside the blockchains to achieve downstream tasks, such as fraud detection, regulatory compliance, risk assess-ment in decentralized finance (DeFi), and smart contract auditing. Effective blockchain data analysis enables stake-holders, including developers, auditors, regulators, and fi-nancial institutions, to make data-driven decisions while safeguarding the network against malicious actors.\nDue to its demand, a variety of blockchain analyses have been conducted. Though we will not fully cover the state-of-the-art as we already see many survey papers in this domain (e.g., [2], [3], [4], [5]), here we summarize available data, tasks and use cases, and underlying techniques to achieve them."}, {"title": "2.1 Data", "content": "Effective blockchain data analysis relies on a diverse range of data sources that provide critical insights into blockchain networks and applications. Table 1 lists a comprehensive list of data for blockchain data analysis. It can be broadly categorized into on-chain data and off-chain data."}, {"title": "2.1.1 On-chain Data", "content": "On-chain data refers to information that can be fetched directly from blockchain networks. Key types of on-chain data include transactions, blocks, smart contract bytecodes, tokens, and network-side information. The on-chain data are typically captured via RPC (remote procedure call) to blockchain nodes or APIs to blockchain explorers or RPC providers, such as Infura, Alchemy, Quicknode, and Block-chain ETL."}, {"title": "2.1.2 Off-chain Data", "content": "Off-chain data refers to data that exists outside of the blockchain but is relevant for understanding and analyzing blockchain networks, such as smart contract source code, market data, social media conversations, and legal and reg-ulation information. This data provides additional context and is often used to enrich on-chain data analysis. The off-chain data are mostly available via the APIs of social media, forums, and websites or scraping content from Tor (the onion router)."}, {"title": "2.2 Tasks and Use Cases", "content": "This section summarizes a comprehensive list of key tasks that can be achieved by combining on-chain and off-chain data with underlying analytical techniques."}, {"title": "2.2.1 Fraud Detection", "content": "Fraud detection is a cornerstone of blockchain analyt-ics, aimed at identifying and mitigating malicious activ-ities on the blockchains [6]. Fraudulent behaviors, such as phishing schemes (e.g., [7], [8]), Ponzi scams (e.g., [9], [10]), and money laundering (e.g., [11], [12]), exploit the pseudonymity and global nature of blockchain networks to evade detection. By leveraging techniques such as graph analysis, machine learning, and anomaly detection, fraud detection systems can uncover suspicious patterns in trans-action flows, address interactions, and trading volumes. Additionally, integrating off-chain data, such as compliance reports and social media signals, provides a comprehensive view of fraudulent behaviors."}, {"title": "2.2.2 Smart Contract Analysis", "content": "One of the most critical tasks in smart contract analysis is the detection of vulnerabilities that could compromise security and lead to significant financial or reputational losses. Depending on the use cases, the data to be analyzed are raw source code or compiled one (bytecode) and ABI. Techniques in NLP, graph mining, and software engineering are often used for smart contract analysis (e.g., [13], [14], [15]).\nFraudulent contracts, such as Ponzi schemes and \u201chon-eypot\" scams, represent a significant risk in the blockchains (e.g., [16]). These contracts are deliberately designed to deceive users, often by promising unsustainable returns or locking funds in ways that prevent withdrawals.\nAuditing smart contract logic ensures that the imple-mented functionality aligns with the project's stated goals and requirements [17]. This process involves reviews of the contract's source code to verify that it behaves as intended under all possible conditions. For example, an auditing process may verify that a DeFi lending protocol correctly calculates interest rates, maintains collateralization thresh-olds, and prevents unauthorized fund withdrawals."}, {"title": "2.2.3 Market Analysis and Prediction", "content": "Market analysis and prediction are useful for understanding and forecasting trends in blockchain ecosystems, enabling investors and stakeholders to make informed decisions in volatile and dynamic markets. By leveraging historical on-chain data, such as transaction volumes and token flows, predictive models can anticipate price movements, identify trading opportunities, and assess market dynamics (e.g., [18], [19]). Additionally, off-chain data sources, including social media sentiment, news articles, and public forums, are incorporated using NLP techniques to evaluate com-munity sentiment and its impact on market behavior (e.g., [20], [21]). In DeFi, monitoring metrics such as total value locked (TVL), liquidity pool behavior, and staking activity, provides insights into the health and risks of financial protocols. Similarly, in the NFT space, tracking ownership trends, transaction activity, and market liquidity reveals user engagement and the evolving value of digital assets (e.g., [22], [23]."}, {"title": "2.2.4 Network, Governance, and Compliance Monitoring", "content": "Network monitoring focuses on evaluating critical metrics such as node distribution, block propagation times, and transaction throughput to detect anomalies or vulnerabili-ties, such as network congestion or potential attacks (e.g., [24], [25]). Governance monitoring assesses participation in decision-making processes, including on-chain voting and proposal outcomes, to ensure alignment with community objectives and detect potential centralization risks in decen-tralized governance systems [26]. Meanwhile, compliance monitoring involves tracking adherence to legal and regu-latory frameworks, such as anti-money laundering (AML) and know-your-customer (KYC) requirements, by identify-ing suspicious transactions, sanction violations, or patterns indicative of money laundering."}, {"title": "2.2.5 Privacy Analysis", "content": "Privacy analysis involves analyzing transactions conducted using privacy-focused cryptocurrencies such as Monero and Zcash (e.g., [27], [28], [29]). These coins employ advanced cryptographic techniques like ring signatures, stealth ad-dresses, and zk-SNARKs to obfuscate transaction details, making them challenging to trace. Privacy analysis in this context focuses on identifying patterns in the use of such technologies, assessing their effectiveness, and ensuring compliance with regulations without compromising user anonymity. On the other hand, privacy analysis also in-volves detecting de-anonymization techniques or patterns that adversaries may use to infer identities or transac-tional relationships within blockchain networks. Techniques such as transaction graph analysis, address clustering, and metadata correlation can reveal vulnerabilities in seemingly private systems."}, {"title": "2.3 Analytical Tools and Techniques", "content": "The following are the basic underlying techniques to achieve the above tasks with given data."}, {"title": "2.3.1 Address Clustering", "content": "Address clustering is a technical process in blockchain data analysis that aims to group blockchain addresses based on shared behavioral characteristics or transactional relation-ships, enabling the identification of entities and activity patterns. This process relies heavily on heuristic and algo-rithmic techniques to infer connections between addresses that are not explicitly linked due to the pseudonymous nature of blockchains (e.g., [30], [31], [32]). For instance, many transactions on blockchains like Bitcoin produce a \"change\" address to return unspent outputs to the sender. Identifying such change addresses by analyzing output patterns and reusing addresses enables the clustering of multiple addresses likely controlled by the same entity."}, {"title": "2.3.2 Supervised Machine Learning", "content": "Supervised learning, mainly classification and regression, is used for prediction tasks where labeled data is available for training. Some examples include binary classification (e.g., fraudulent vs. non-fraudulent transactions), address classification into categories (e.g., exchange, Ponzi scheme, mixer), and price prediction. The underlying algorithms such as SVM (support vector machine) [33], RF (Random Forests) [34], XGBoost [35], and neural networks [36], are often used (e.g., [37], [38])."}, {"title": "2.3.3 Unsupervised and Semi-supervised Learning", "content": "Unsupervised and semi-supervised learning, such as time-series analysis, clustering, and anomaly detection, are em-ployed when labeled data is unavailable, or only one class is available, helping to discover patterns or anomalies in blockchain data (e.g., [39]). They could be statistical ap-proaches like z-scores (e.g., [40]) and proximity-based ap-proaches (e.g., [41]), and ML-based approaches like Isolation Forest [42] and One-Class (OC) SVM [43].\nOn-chain data often includes time-stamped events, such as transactions and block creation times. Time-series analy-sis models temporal patterns to detect trends and anomalies. For instance, they could be used to identify change points in account behavior and model price movements or trans-action volumes."}, {"title": "2.3.4 Graph Mining and Network Analysis", "content": "On-chain data can be represented as a graph, where nodes correspond to entities (e.g., wallet addresses) and edges represent relationships (e.g., transactions) [44]. Graph min-ing techniques are used to analyze the structure, patterns, and anomalies in these networks (e.g., [45]). When treating a blockchain network as a graph, we could apply tech-niques like community detection, centrality measurement, and shortest path analysis to detect high-activity addresses and the relationships between target entities (e.g., exchanges or coordinated fraud groups). We see trends in applying graph embedding techniques that convert graphs into low-dimensional representations for use in machine learning tasks (e.g., scammer address detection) [5]."}, {"title": "2.3.5 Natural Language Processing (NLP)", "content": "Although primarily used for analyzing off-chain data, NLP plays a growing role in understanding smart contracts and blockchain-related discussions and documents. Sentiment analysis and Named Entity Recognition (NER) analyze public sentiment on social media or forums and extract mentions of projects, tokens, or addresses. Formal verifica-tion is a rigorous mathematical method used to prove the correctness and security of smart contracts or blockchain protocols by ensuring they meet predefined specifications and are free from vulnerabilities or logical errors.\nFor instance, these techniques are used for detecting Ponzi schemes operating on smart contracts (e.g., [16]), correlating sentiment trends with market behavior (e.g., [20], [21]), identifying scams or phishing attempts in an-nouncements [46], and identifying vulnerabilities in smart contracts (e.g., [47], [48])."}, {"title": "2.3.6 Visualization", "content": "Data visualization is a useful tool in blockchain data analy-sis, transforming complex and voluminous blockchain data into intuitive visual representations that facilitate under-standing of blockchain networks [4]. Graph visualizations, for instance, effectively illustrate transaction flows and address clusters, revealing patterns such as fund move-ment between entities or coordinated behaviors in scams. Heatmaps highlight temporal trends, such as periods of heightened activity in token trading or network congestion during peak usage. Dashboards aggregate and display real-time metrics, including transaction throughput, gas fees, and validator performance, enabling stakeholders to mon-itor network health and activity (e.g., [49], [50], [51]). Ad-ditionally, geospatial mapping can reveal the geographic distribution of blockchain nodes, providing insights into network decentralization and resilience."}, {"title": "2.4 Challenges", "content": "We want to emphasize that the existing blockchain data analysis papers more or less face the following challenges:\n1) Pseudonymity: Blockchain's pseudonymity, com-bined with obfuscation tools such as mixers and pri-vacy coins, complicates the identification of fraudu-lent or illegal activities. Conventional methods often struggle to link suspicious addresses or transactions with real-world identities.\n2) Lack of Labeled Datasets and Ground Truth: The scarcity of labeled datasets limits the development of machine learning models for blockchain data analysis, making it difficult to detect new types of fraud or risks effectively.\n3) Protocol Dependencies: Blockchain networks op-erate independently with unique protocols and data structures (e.g., Bitcoin's UTXO model versus Ethereum's account-based model). This fragmenta-tion makes it difficult to conduct cross-chain analy-ses and extract meaningful insights consistently.\n4) Scalability Issues: As the size of blockchain net-works increases, the volume of transactional data grows exponentially. Analyzing large datasets in real time poses computational challenges, especially when multiple chains are involved.\n5) Interpretability of Insights: Blockchain analytics tools often generate complex insights that require expertise to interpret. This creates a barrier for non-experts, such as regulators or general users, to en-gage with the network meaningfully."}, {"title": "3 MOTIVATIONS OF OUR STUDY", "content": "As illustrated in Figure 2, we foresee that LLMs will offer promising solutions to overcome some of these challenges through their advanced capabilities:\n1) Pre-trained Knowledge for Data Scarcity: LLMs are trained on vast datasets across multiple do-mains, allowing them to generate meaningful in-sights even in the absence of blockchain-specific labeled datasets. This capability addresses the chal-lenge of limited ground truth by leveraging knowl-edge from related contexts.\n2) Generalizability Across Blockchains: LLMs can understand and process information from diverse blockchain protocols, enabling cross-chain analysis without the need for extensive re-engineering. Their adaptability makes them ideal for environments where multiple blockchains coexist with different architectures and consensus mechanisms.\n3) Explainability for Decision-Making: LLMs excel at providing human-readable explanations for com-plex insights, facilitating better decision-making. This feature is critical for building trust in block-chain systems, as it allows auditors, regulators, and developers to understand the reasoning behind de-tected patterns or recommendations.\nWe argue that a thorough discussion of available data and design patterns for LLM-integrated analysis, along with an exploration of how such combinations achieve down-stream tasks, is currentlywmissing. While there are related survey papers on LLM integration in fields such as time-series analysis [52] and graph analysis [5], [53], [54], to the best of our knowledge, a comprehensive and systematic paper on the integration of LLMs in blockchain data analysis has yet to be published.\nThis paper seeks to fill this gap by providing a compre-hensive framework for understanding and leveraging how LLMs could benefit downstream tasks in blockchain data analysis\u00b9. By systematically exploring the diverse types of data available for blockchain data analysis and the architec-tural approaches for integrating LLMs, we aim to highlight how these powerful models can enhance analytical capabili-ties, improve data-driven decision-making, and address key challenges in areas such as fraud detection, smart contract security, compliance monitoring, and market prediction."}, {"title": "4 LLM-INTEGRATED BLOCKCHAIN DATA ANALYSIS", "content": "LLMs are advanced machine learning models designed to understand, generate, and manipulate natural language. They are typically transformer-based pre-trained models containing billions of parameters [58]. Trained on massive datasets across diverse domains, LLMs such as GPT-4 have become powerful tools for various applications, including text generation, summarization, and reasoning.\nThe performance of LLMs is heavily influenced by the way input prompts are structured. Techniques such as prompt engineering, in-context learning, RAG, and reason-ing frameworks help guide the model in producing accurate and relevant outputs. Also, design patterns, i.e., how LLMs should be integrated into blockchain data analysis, have not yet been discussed. This section covers major techniques in prompt engineering and our suggested systematic classifi-cation of the design patterns in LLMs-integrated blockchain data analysis."}, {"title": "4.1 Prompt Engineering", "content": "Prompt engineering is a critical aspect of LLM applications, as it directly influences the model's ability to understand tasks, generate accurate outputs, and adapt to diverse use cases by carefully crafting inputs that guide its reasoning and behavior."}, {"title": "4.1.1 Basics of Prompting", "content": "We start with the basics of prompting. First, we need to identify the objectives with analytical tasks and desired outcomes, for example, address classification and price pre-diction with reasoning and fraud reporting generation. We can then design a template that provides clear and concise instructions for the LLM based on the defined objectives.\n\u2022 Instruction: A clear statement of the task or question.\n\u2022 Context: Supporting information or retrieved data (e.g., past knowledge about fraud).\n\u2022 Input Data: The raw data to be analyzed (e.g., the list of transactions).\n\u2022 Formatting Guide: Optional guidelines specifying the expected format of the output (e.g., numeri-cal scores, textual explanations, or predefined cate-gories).\nThe following shows the basic structure of a prompt tem-plate. For ease of understanding, we will take an example of a simple fraud detection use case from now on. The template should include placeholders for dynamic data and specify the context of the task in code (e.g., in Python). For instance:\nAnalyze the following transaction history and identify risk level from 0 to 1 scale (0: no risk, 1: highest risk):\n\u2190\n\u2190\nTransaction Data: {transaction_data}\nRisk level:\nTransaction history captured from a blockchain, e.g., a list of JSON-formatted transactions, is embedded in {transaction_data} to complete a prompt. We could manually craft templates or use an LLM to generate ones. This process often requires iteratively refining and experi-menting with prompts to optimize the quality of the gener-ated responses."}, {"title": "4.1.2 In-Context Learning", "content": "We could extend the above template to accommodate more contextual information. In-context learning, or few-shot learning, involves providing an LLM with a carefully de-signed prompt containing examples (demonstrations) of the task at hand, followed by a query. The model uses the examples in the context to infer the desired task and gen-erate the correct response. Here is a template example with two demonstrations. An LLM analyzes the given transaction data in the query and outputs its decision and explanation based on the two demonstrations above.\n### Instruction ###\nYou are an expert in blockchain fraud detection. Analyze the given transaction data and determine whether it is likely fraudulent. Provide an explanation for your decision.\n\u2190\n\u2190\n\u2190\n### Examples ###\nTransaction Data:"}, {"title": "4.1.3 Retrieval-Augmented Generation (RAG)", "content": "In the in-context learning example, we assumed we already had contextual information related to the question. How-ever, such insights are often stored in a large knowledge database, and it is infeasible to embed every single insight in the database. The basic idea of RAG is to retrieve the most relevant contexts from such a database using similarity search and append them in a prompt. To illustrate, consider a fraud detection task where external metadata is retrieved and incorporated:\nYou are an expert in blockchain analysis. Using the provided external metadata and transaction history, identify whether the transaction is likely fraudulent.\n\u2190\n\u2190\n\u2190\n\u2190\nExternal Metadata: {retrieved_data}\nTransaction History: {transaction_data}\nAnswer with \"Fraudulent\" or \"Non-Fraudulent\" and provide reasoning.\n\u2190\n\u2190\n\u2190\nAssuming we have domain knowledge about scam-mers (e.g., scammers' transaction patterns), we search for such information in a database and append it in {retrieved_data}."}, {"title": "4.1.4 Reasoning Frameworks", "content": "Some analytical use cases require intricate reasoning. By reasoning step-by-step, the LLM was found to reduce errors and handle complexity more effectively (e.g., [59], [60]). Reasoning frameworks, such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT), guide LLMs to generate intermediate steps or explore decision paths rather than directly jumping to an answer.\nCoT involves a linear progression of thoughts (sequential reasoning), while ToT organizes reasoning in a tree-like structure to explore multiple options at each step (hierarchi-cal reasoning), and GoT leverages graph-like relationships to handle dependencies or multiple paths simultaneously (interconnected reasoning).\nDue to space limitations, we put the examples of CoT, ToT, and GoT in the fraud detection scenarios in Ap-pendix A."}, {"title": "4.1.5 Compress Embedded Data", "content": "Data compression techniques aim to reduce the token foot-print of large inputs, such as transaction histories or long smart contract logs, enabling efficient processing within LLM token limits. The following are some techniques for compression.\n\u2022 Feature Extraction and Summarization: Extract dis-tinguished features from transactions, such as fre-quency of transactions, transaction volume per ad-dress, and time intervals, and provide them in a prompt rather than raw transactions.\n\u2022 Sampling: For historical data, retain only the most recent or relevant portions of the input that align with the tasks.\n\u2022 Format Conversion: Reduce the context size by con-verting the format (e.g., from JSON to plain text)."}, {"title": "4.2 Design Patterns", "content": "Integrating LLMs into blockchain data analysis introduces solid design patterns that cater to diverse analytical needs. Following the classification in [52], these patterns can be broadly classified into two categories: LLM-as-enhancers, where LLMs augment specific tasks while the final predic-tion or decision is made by other components, and LLM-as-predictors, where LLMs directly output the final prediction."}, {"title": "4.3 Use Cases", "content": "Here, we discuss how they would improve the operations from a use-case point of view. We try to be aligned with the use cases in Section 2."}, {"title": "4.3.1 Fraud Detection", "content": "LLMs will demonstrate significant potential in enhancing fraud detection capabilities on blockchain networks and applications. One prominent application is the develop-ment of sophisticated account representation models. Hu et al.'s BERT4ETH [66] is a pre-trained transformer model specifically designed for Ethereum fraud detection tasks. This model utilizes masked address prediction as a pre-training task to capture the co-occurrence relationships be-tween transactions, enabling it to generate more expressive and context-aware account representations compared to traditional graph-based methods. BERT4ETH demonstrates superior performance in detecting phishing accounts and linked accounts that are being de-anonymized. For instance, in phishing account detection, BERT4ETH achieved an F1 score improvement of 21.61 absolute percentage points over the best-performing graph neural network model. This sig-nificant enhancement in detection accuracy showcases the potential of LLMs in addressing the challenges posed by sophisticated fraud schemes on blockchain platforms.\nAnother application is the ZipZap framework proposed by Hu et al. [70]. ZipZap addresses the computational chal-lenges associated with training LLMs in large-scale block-chain datasets. By incorporating frequency-aware compres-sion techniques and an asymmetric training paradigm, ZipZap achieves both parameter efficiency and computa-tional efficiency in LM training for blockchain applications. ZipZap's frequency-aware compression technique allows for a remarkable 92.5% reduction in model parameters with only a marginal performance loss. This compression is achieved by correlating the embedding dimension of an address with its occurrence frequency in the dataset, effectively addressing the power-law distribution of address frequencies in blockchain transactions. Such efficiency im-provements are crucial for the deployment of LLM-based fraud detection systems on a scale, particularly given the ever-growing volume of blockchain data.\nThe application of LLMs in blockchain fraud detection offers several key advantages. LLMs can leverage their pre-trained knowledge to generate meaningful insights even when blockchain-specific labeled datasets are limited. This capability is particularly valuable in the rapidly evolv-ing blockchain ecosystem, where new fraud patterns may emerge faster than labeled data can be collected. Although the discussed models focus on Ethereum, the underlying principles of LLM-based fraud detection can be adapted to other blockchain platforms. This generalizability allows for cross-chain analysis without extensive re-engineering, making LLMs a versatile tool for fraud detection across diverse blockchain ecosystems."}, {"title": "4.3.2 Smart Contract Analysis", "content": "Smart contract analysis is one of the intuitive use cases that LLMs can contribute to. This domain is rather advanced in terms of LLM integration [71]. LLMs have been extensively studied to identify and repair vulnerabilities within smart contracts. Liu et al. introduced FELLMVP, an ensemble LLM framework to classify vulnerabilities in smart contracts [67]. This approach combines multiple LLM agents, each spe-cialized in distinct areas of security auditing, including contract code analysis, vulnerability identification, and se-curity summary. By leveraging the collective capabilities of these specialized agents, FELLMVP demonstrates superior performance in detecting a wide range of vulnerabilities, including complex logic vulnerabilities that traditional tools often overlook. Sun et al. developed two innovative tools: ACFIX and GPTScan [72]. ACFIX utilizes GPT-4 to repair access control vulnerabilities in smart contracts. By min-ing common role-based access control (RBAC) practices and guiding the LLM with contextual information, ACFIX achieves a high success rate in repairing vulnerabilities, significantly outperforming baseline models. GPTScan inte-grates GPT with static analysis to detect logic vulnerabilities in smart contracts. This approach showcases the ability of LLMs to address the challenge of data scarcity by utilizing pre-trained knowledge to generate meaningful insights even with limited blockchain-specific labeled datasets.\nLLMs have also been employed to improve formal ver-ification processes for smart contracts. Liu et al. proposed PropertyGPT, a novel system that leverages LLMs for the automated generation of smart contract properties [63]. This method enables the generation of diverse types of properties, including invariants, pre-/post-conditions, and rules, significantly enhancing the effectiveness of formal verification. PropertyGPT addresses several challenges in property generation, ensuring that the generated properties are compilable, appropriate, and runtime-verifiable. It uses compilation and static analysis feedback as an external oracle to guide LLMs in iteratively revising the generated properties. This approach not only improves the quality of generated properties but also demonstrates the potential of LLMs to transfer knowledge from existing human-written properties to new, unknown smart contract codes.\nSligpt is a methodology that integrates GPT-40 with the static analysis tool Slither to perform data dependency analyses on Solidity smart contracts [64]. This approach not only improves the accuracy of code analysis but also enables users to query and analyze smart contracts using natural language, significantly enhancing the user experience and accessibility of smart contract analysis tools."}, {"title": "4.3.3 Market Analysis and Prediction", "content": "LLMs have shown significant potential to analyze the sen-timent of social networks and news sources to predict cryptocurrency price movements. Using their natural lan-guage understanding capabilities, these models could pro-cess large amounts of textual data to gauge market sen-timent and its potential impact on cryptocurrency values. Roumeliotis et al. utilized GPT-4, BERT, and FinBERT mod-els to perform sentiment analysis on cryptocurrency news articles for predicting Ethereum price trends [73]. This ap-proach not only captures the sentiment of the news but also identifies correlations between different cryptocurrencies, providing valuable information for investment decisions. By elucidating the reasoning behind sentiment assessments and price predictions, these models can enhance transparency and trust in cryptocurrency markets, aiding investors and regulators in decision-making processes.\nLLMs can analyze complex relationships and correla-tions between multiple cryptocurrencies. Singh and Bhat developed a Transformer-based neural network model to predict Ethereum prices [74]. The model incorporated not only Ethereum's own price and volume data but also data from other highly correlated cryptocurrencies such as Polka-dot and Cardano. This approach leverages the interdepen-dencies between different assets in the cryptocurrency mar-ket, aiming to improve prediction accuracy. This approach showcases the generalizability of LLMs across different blockchain ecosystems, allowing for comprehensive market analysis without the need for extensive reengineering for each cryptocurrency. The ability to process and correlate data from multiple sources demonstrates the potential of LLMs to provide a holistic view of the cryptocurrency market.\nLLMs can enhance the analysis of on-chain transaction data. Li et al.'s LLM-driven trading agent, CryptoTrade, integrates both on-chain and off-chain data to optimize cryptocurrency trading decisions [69]. By analyzing trans-action statistics, market data, and news summaries, the model can make more informed trading decisions. This application highlights the potential of LLMs to address data scarcity issues in blockchain analytics. By leveraging pre-trained knowledge from diverse domains, LLMs can generate meaningful insights even when blockchain-specific labeled datasets are limited."}, {"title": "4.3.4 Network, Governance, and Compliance Monitoring", "content": "LLMs will be a powerful tool for anomaly detection in blockchain networks, offering several advantages over tra-ditional methods. One notable application in this domain is BlockFound, a customized foundation model for anomaly detection in blockchain transactions [61]. BlockFound mod-els the unique multi-modal data structure of blockchain transactions, which typically contain blockchain-specific to-kens, texts, and numbers. The model employs a modular-ized tokenizer to handle these diverse inputs, effectively balancing information across different modalities. Block-Found's approach addresses key challenges in blockchain anomaly detection by utilizing pre-trained knowledge to generate meaningful insights even with limited blockchain-specific labeled datasets. This capability is crucial in the rapidly evolving blockchain landscape, where new types of anomalies may emerge faster than labeled data can be collected. The model also demonstrates effectiveness on both Ethereum and Solana networks, showcasing its ability to adapt to different blockchain architectures without exten-sive reengineering. This cross-chain applicability is particu-larly valuable in the increasingly interconnected blockchain ecosystem.\nThough not using a proprietary LLM, another notable approach in this field is BlockGPT [65], which uses a GPT-style model to detect anomalous blockchain transactions. BlockGPT generates tracing representations of blockchain activity and trains an LLM from scratch to act as a real-time blockchain anomaly detection. This method offers an unrestricted search space and does not rely on predefined rules or patterns, enabling it to detect a wider range of anomalies.\nThese LLM-based methods offer several advantages over traditional anomaly detection techniques. They can capture complex patterns and contextual information in transaction data, allowing for more nuanced anomaly detection com-pared to rule-based systems. By learning from vast amounts of data, LLMs can potentially identify novel types of anoma-lies that might be missed by static, rule-based systems."}, {"title": "4.3.5 Privacy Analysis", "content": "To the best of our knowledge, we could not find any papers that use LLMs for transaction privacy analysis. However, we believe it could still benefit from LLMs. By integrat-ing on-chain and off-chain data, LLMs may help identify anomalous behaviors in privacy coins like Monero or Zcash. Furthermore, LLMs' inherent capabilities of finding patterns could be useful for detecting de-anonymization techniques, such as new heuristics of address clustering."}, {"title": "5 CHALLENGES AND FUTURE RESEARCH DIRECTIONS", "content": "To fully realize the potential of Large Language Models (LLMs) in blockchain data analysis, we argue that future research must address six critical areas, namely (1) latency, (2) reliability, (3) cost, (4) scalability, (5) generalizability, and (6) autonomy. We believe that research in these areas will pave the way for a new generation of LLM-powered tools, transforming the landscape of blockchain analytics with greater efficiency, reliability, and innovation."}, {"title": "5.1 Latency", "content": "First, latency remains a key challenge, as the responsiveness of LLMs must be optimized for real-time blockchain appli-cations such as fraud detection, compliance monitoring, and DeFi trading. Hence, it is crucial to design latency-aware methods. For instance, one potential idea is a hybrid LLM design that offloads time-consuming tasks, such as complex reasoning, keeps the insights in a local database, and uses them with a local LLM to obtain the final results. Here it is important to carefully design systems as"}]}