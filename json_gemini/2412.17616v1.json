{"title": "Facial Expression Analysis and Its Potentials in IoT Systems: A Contemporary Survey", "authors": ["ZIXUAN SHANGGUAN", "YANJIE DONG", "SONG GUO", "VICTOR C. M. LEUNG", "M. JAMAL DEEN", "XIPING HU"], "abstract": "Facial expressions convey human emotions and can be categorized into macro-expressions (MaEs) and micro-expressions (MiEs) based on duration and intensity. While MaEs are voluntary and easily recognized, MiEs are involuntary, rapid, and can reveal concealed emotions. The integration of facial expression analysis with Internet-of-Thing (IoT) systems has significant potential across diverse scenarios. IoT-enhanced MaE analysis enables real-time monitoring of patient emotions, facilitating improved mental health care in smart healthcare. Similarly, IoT-based MiE detection enhances surveillance accuracy and threat detection in smart security. This work aims at providing a comprehensive overview of research progress in facial expression analysis and explores its integration with IoT systems. We discuss the distinctions between our work and existing surveys, elaborate on advancements in MaE and MiE techniques across various learning paradigms, and examine their potential applications in IoT. We highlight challenges and future directions for the convergence of facial expression-based technologies and IoT systems, aiming to foster innovation in this domain. By presenting recent developments and practical applications, this study offers a systematic understanding of how facial expression analysis can enhance IoT systems in healthcare, security, and beyond.", "sections": [{"title": "1 Introduction", "content": "Since its inception in 1999, Internet-of-Things (IoT) technology has seamlessly integrated into the fabric of modern society, driving key innovations in both civilian and industrial sectors. The widespread adoption of IoT technology generates skyrocketing amounts of daily data that were traditionally processed in clusters of cloud servers. Due to the ever-increasing privacy concerns, the information process is shifted from the cloud to the network edge. The low access latency and high data security of edge computing enable new application domains, such as emotion detection, traffic surveillance, and healthcare [38, 39]."}, {"title": "2 Recent Surveys on Facial Expression Analysis", "content": "Facial expression analysis is a critical tool for understanding human emotions and intentions. The research on facial expression analysis started in the 1970s [41, 42]. Over the years of development, facial expression analysis has penetrated into our human daily lives. More specifically, facial expression analysis can be categorized into MaE analysis and MiE analysis. Therefore, we are motivated to review recent research contributions on both MaE and MiE analysis to better understand the current research stage."}, {"title": "2.1 Recent Surveys on MaE Analysis", "content": "MaE analysis is a burgeoning field at the intersection of computer vision and human emotion recognition and can offer transformative potential in healthcare, security, and human-computer interaction. Pioneering research has laid the groundwork for recognizing subtle expressions. For example, Pantic et al. [176] introduced three key problems in automatic MaE analysis: (1) detection of an image segment as a face; (2) extraction of facial expression information; and (3) classification of the expression. Yet, significant challenges remain in achieving robust, scalable, and generalizable solutions to MaE Analysis. Leveraging the power of deep learning, researchers endeavor to address these challenges, advancing the field through innovative algorithms and practical IoT applications. In [110], Li et al. provided a comprehensive review of MaE recognition based on deep learning. More specifically, the available datasets for MaE analysis and the principles of data selection and evaluation were explored [110]. Later, Canal et al. [13] conducted an exhaustive systematic literature review encompassing 94 leading methods from 51 related articles. Their review meticulously delineates the computational workflow for MaE recognition, such as, preprocessing, feature extraction, and classification algorithms. Through a rigorous statistical analysis, Canal et al. [13] revealed that traditional machine learning methods suffer from limited generalization capabilities though the high precision in classification can be achieved. With the powerful feature capabilities, deep learning techniques thrive in scenarios involving larger and more diverse datasets. Karnati et al. [93] introduced the cutting-edge models that are tailored for deep MaE recognition across diverse input modalities. The comprehensive evaluations provide compelling evidence of enhanced performance and generalization potential of deep models that set new benchmarks for future research [93].\nOver the last decades, research on MaE analysis has undergone remarkable evolution that was transitioned from a formidable challenge to a structured and highly efficient process. Once fraught with complexities, the MaE analysis workflow now adheres to a robust standard pipeline that encompasses preprocessing, feature extraction, and classification. The integration of deep learning into this pipeline has been a transformative milestone, propelling algorithmic performance to exceed 97% accuracy under controlled laboratory conditions-an unprecedented achievement. Despite these advancements, comprehensive overviews remain scarce to bridge the gap between foundational research and practical applications. This limitation hampers the widespread adoption of MaE technologies in real-world (a.k.a., wild) scenarios, especially in areas like IoT integration and multimodal recognition. By bridging the gap between foundational research and wild application, we first introduce deep MaE recognition algorithms in Section 4 and the MaE applications based on IoT in Section 6.1. Our analysis not only consolidates existing knowledge but also sets the stage for pioneering applications, ensuring our research addresses the pressing needs of this rapidly advancing field."}, {"title": "2.2 Recent Surveys on MIE Analysis", "content": "As MiE analysis continues to gain significant attention across academia and industry, numerous surveys have systematically reviewed advancements in MiE analysis from diverse perspectives. For example, Ben et al. [10] conducted the first comprehensive survey that offers a systematic examination of MiE analysis within a unified evaluation framework. A neuropsychological perspective was provided on the distinction between MiEs and MaEs, i.e., the greater inhibitory effect of MiE on facial expressions in [10]. Additionally, several representative MiE datasets were also introduced and could be categorized into four distinct types: (1) instructing participants to perform specific MiEs [169, 189]; (2) constructing high-stakes scenarios [229]; (3) eliciting emotions through video stimuli while maintaining neutral expressions [33, 115, 173, 245, 247]; and (4) capturing real high-stakes situations [84]. Notably, the micro-and-macro in expression warehouse (MMEW) dataset that contains high-resolution samples with a balanced distribution of MaE and MiE was proposed in [10]. Xie et al. [238] further advanced the understanding of MiE recognition by delving into critical areas, such as, macro-to-micro adaptation, recognition based on apex frames, and analysis leveraging facial action units (AUs). Their detailed exploration offers a more nuanced perspective, complementing prior works and addressing emerging challenges in the field. Li et al. [121] presented a pioneering survey on MiE analysis through the lens of deep learning techniques. They systematically reviewed existing deep learning approaches by analyzing datasets, delineating the stepwise pipelines for MiE recognition, and conducting performance comparisons with state-of-the-art methods. Their survey introduced a novel taxonomy for deep learning-based MiE recognition that can classify input data into static, dynamic, and hybrid categories. Furthermore, Li et al. [121] meticulously examined the architectural components of deep learning models, including network blocks, structural designs, training strategies, and loss functions. Their findings emphasized that leveraging multiple spatiotemporal features as input yields superior performance in MiE recognition. Zhao et al. [273] provided a holistic overview of MiE research from foundational psychological studies and early computer vision efforts to advanced computational MiE analysis. Zhao et al. [273] highlighted key research directions that include MiE AU detection and MiE generation, and explored practical applications (e.g., covert emotion recognition and professional training). Additionally, Zhao et al. [273] identified pressing challenges in wild MiE applications, such as, data privacy, data protection, fairness, diversity, and the regulated use of MiE technologies.\nPrevious surveys on MiE analysis have comprehensively examined the field from various perspectives, e.g., MiE analysis, dataset creation, and psychological foundations of MiE development. However, critical aspects such as advanced MiE analysis and practical MiE applications remain underexplored. To address the aforementioned gaps, our work delves into state-of-the-art MiE analysis in Section 5. Additionally, we explore the transformative potential of MiE applications in the IoT system in Section 6.2. We expect to provide a holistic perspective that can bridge foundational research with cutting-edge advancements and wild applicability."}, {"title": "3 Datasets for Facial Expression Analysis", "content": "Facial expression analysis relies on high-quality datasets to drive advancements in model development and evaluation. In this section, we will provide a holistic introduction to the current datasets used for MaE and MiE analysis."}, {"title": "3.1 MaE Datasets", "content": "The recent MaE datasets including Japanese female facial expression (JAFFE) [138], Karolinska directed emotional faces (KDEF) [59], MMI [161, 206], Binghamton university 3D facial expression (BU-3DFE) [253], the Toronto face database (TFD) [198], The Extended Cohn-Kanade Dataset (CK+) [137], Radboud faces database (RaFD) [100], MUG [3], Multi-PIE [63], static facial expressions in the wild (SFEW 2.0) [37], Oulu-CASIA [272], facial expression recognition 2013 (FER-2013) [61], Indian spontaneous expression database (ISED) [68], EmotioNet [46], wild affective faces database (RAF-DB) [111], AffectNet [148], the acted facial expressions in the wild (AFEW 7.0) [36], expression in-the-wild (ExpW) [271], Aff-Wild2 [99], context-aware emotion recognition (CAER) [102], dynamic facial expression in the wild (DFEW) [89], FERV39k [228], Padova emotional dataset of facial expressions (PEDFE) [146], CalD3r + MenD3s [204]. The basic expressions of most MaE datasets can be divided into seven categories, i.e., anger, neutral, disgust, fear, happiness, sadness, and surprise. Although being considered as a basic emotion of the face, contempt expression is not common in most MaE datasets. Few datasets (e.g., CK+ and RaFD) have collected the contempt expression; therefore, the number of samples on contempt expression is limited. In the CK+ dataset, the number of contempt expression samples accounts for 5% of total samples. In addition to the seven basic expressions in MaE datasets, several datasets (e.g., AffectNet and AFF-wild2) also used the continuous-valued valence and arousal to describe the intensity of expression. Moreover, only the ExpW dataset considered the compound expressions. In the ExpW dataset, 23 basic or compound emotion categories were used for emotion classification.\nThe early datasets were usually collected from the laboratory and required a certain number of subjects to join. The collection of these datasets required a lot of labor, which may include subjects participating in the collection, guides of collection experiments, and professional annotators. In addition to the annotation of expression categories, several datasets (e.g., CK+, MMI, and Oulu-CASIA) required additional information in terms of facial action coding system (FACS), AU, and index. Typically, CK+ and Oulu-CASIA have utilized the annotation information of the index to label their sequences with the onset and peak of expressions. MMI and CK+ have labeled the FACS and AU to provide more additional information. In addition to the labor-consuming collection of MaE datasets, EmotioNet has utilized the automatic annotation algorithm to label the data collected from the internet. In this dataset, a total of 9500,000 images were annotated with AUs, AU intensities, and emotion categories.\nThe early MaE datasets (e.g., JAFFE) were utilized to obtain the posed images and sequences. These datasets contained the posed data for the front view. To meet the requirement of practical MaE in wild conditions, many datasets (e.g., Multi-PIE, RaFD) provided the MaE data with various environmental conditions including multiple head poses, occlusions, and illuminations. Typically, the Multi-PIE contained MaE images under 19 illumination and 15 viewpoint conditions in four sessions. In their process of dataset collection, each subject was recorded between -90\u00b0 to 90\u00b0 with an interval of 15\u00b0. MaE in RaFD was recorded at the same moment through five different camera angles and shown with three different gaze directions. In addition, the MaE datasets collected from the internet (e.g., AffectNet and ExpW) contained an amount of MaE data with more wild scenarios. These datasets can benefit the robustness and generalization of further MaE research."}, {"title": "3.2 MIE Datasets", "content": "In recent years, multiple datasets have been built for further development of MiE analysis. Initially, researchers collected the MiE databases (e.g., Polikvsky's database [169] and USF-HD [189]) by asking the participants to pose or mimic facial expressions. These posed datasets supported the preliminary studies of MiE, but were not used for current analysis due to the properties of private datasets and not capturing the nature of MiE. Later, more spontaneous datasets, including the spontaneous micro-expression corpus (SMIC) [115], Chinese Academy of Sciences micro-expression (CASME) [247], Chinese Academy of Sciences micro-expression II (CASME II) [245], Chinese Academy of Sciences macro-expressions and micro-expressions (CAS(ME)2) [173], spontaneous actions and micro-movements (SAMM) [33], micro-expression videos in the wild (MEVIEW) [84], CAS(ME)3 [104], MMEW [10], and 4D spontaneous micro expression database (4DME) [112], were utilized in MiE research.\nDue to the subtle and fleeting nature of MiE, the annotation of MiE datasets is a challenging and demanding task that requires a significant amount of time and labor. Coders who label MiE need FACS training and half an hour to detect suitable clips [160]. The labeling of MiE includes the AU and emotion classification. By incorporating the AU into certain emotion taxonomy, the technique of MiE synthesis can be used to generate new samples. The classification of MiE in the dataset is different, which results in inconsistent annotations between various datasets. For example, the classification of MiE in SMIC is positive, negative, and surprising. The classification of MiE in other datasets, such as CASME, MEVIEW, and SAMM, can be divided into more than six categories.\nTo induce spontaneous MiE, many studies have used a collection paradigm that requires participants to maintain a poker face while watching intensely emotional video clips. MiEs are revealed and recorded using a high-speed camera. This effective and simplified method was applied to five datasets: SMIC, CASME, CASME II, SAMM, and CAS(ME)2. However, this paradigm was criticized for its laboratory setting, which lacks practical or realistic situations. Consequently, studies expanded to more realistic settings. In real-life scenarios, such as high-stakes poker games or TV interviews, participants often conceal their true emotions, leading to MiE occurrences. For example, MEVIEW constructed an in-the-wild MiE dataset from real poker games and TV interviews. Despite its benefits for real-scene analysis, MEVIEW has limited data samples and frequent face pose changes, resulting in side views and occlusions. Subsequently, CAS(ME)\u00b3 adopted a trade-off method using mock crime paradigms to collect MiE samples in practical scenes with controllable factors.\nEarly spontaneous MiE datasets collected the 2D facial video, which had the advantages of convenient collection and control. With the concern about adequate and accurate MiE analysis, more modalities of the dataset were emphasized. Except for using 2D facial video, the dataset of SMIC implemented the 2D near infrared videos to alleviate the influence of illumination. More recently, the depth information has proven its effectiveness from face recognition [145] to expression recognition [28]. A standard RGB image can be constructed into a 3D model of the human face with depth information, which can enhance the robust perception of expression features and cognitive behavior. Therefore, the modal of depth information was applied in the datasets of CAS(ME)3 and 4DME. Besides, CAS(ME)\u00b3 enriched the data information with physiological signals and voice signals. Also, 4DME developed more facial multi-modal videos, consisting of reconstructed dynamic 3D facial meshes, grayscale 2D frontal facial videos, Kinect-color videos, and Kinect-depth videos. \nIn addition, some datasets considered the composition of MiE and other expressions for the real-scene situation analysis. For example, MMEW, CAS(ME)2, CAS(ME)\u00b3, and 4DME contained the MiE and MaE, which can be employed for detecting MiE from complete videos and further analysis about the evolution of different expressions. Due to the short length of video data, the analysis of MiE was constrained. To collect long videos, several datasets, including CASME, CASME II, and SMIC, were expanded to incorporate frames that do not exhibit MiE before and after the annotated MiE samples. Long MiE videos can benefit more sharper MiE algorithms in more complex situations, such as head movement and verbal behavior that occurred in common scenarios."}, {"title": "4 Deep MaE Recognition", "content": "The ever-increasing data volume has motivated various data-driven methods to handle the technical challenges in computer vision, such as, emotion recognition [110], event detection [78] and action detection [205]. As an important research direction in computer vision, deep learning-based MaE recognition (a.k.a. deep MaE recognition) has seen its significant advancements in both practical applications and research experiments. More specifically, deep MaE recognition has progressed from analyzing multiple static images to handling complex dynamic image sequences. Hereinafter of this section, we provide an in-depth exploration of deep learning-based MaE recognition methods that are tailored for static image analysis (i.e., deep static MaE recognition) and for dynamic sequential image analysis (i.e., deep dynamic MaE recognition). By examining the state-of-the-art methodologies, we aim to highlight the strengths and advancements in MaE recognition and to emphasize their potential to address increasingly complex wild scenarios."}, {"title": "4.1 Deep Static MaE Recognition", "content": "As shown in Table 3, MaE recognition from multiple static images has been proposed due to the efficiency of data processing and the potential for more detailed analysis."}, {"title": "4.1.1 Ensemble learning", "content": "Ensemble learning in deep static MaE recognition can exploit complementary information from multiple feature representations of a single emotion image. More specifically, ensemble learning can be used at different stages of the deep static MaE recognition, such as, data preprocessing [96], input enhancement [71, 147, 182, 183, 208], network generation [97, 171], feature extraction [209, 240], and emotion classification [79, 147]. More specifically, deformation and normalization have been used to preprocess the data before training the deep models for MaE recognition [96]. When applied at the stage of input enhancement, ensemble learning can improve the MaE recognition by integrating multiple types of textural features, such as, local binary patterns (LBPs) [71, 183], facial landmark point [147], covariance feature [182], gradient images [71], and gravitational force [208]. At the stage of network generation, deep models with various kernel shapes and parameter initializations are ensembled to improve performance [97, 171]. For example, Saurav et al. [181] proposed an integrated convolutional neural network (CNN) architecture that leveraged two deep models with distinct kernel shapes. The aggregation of features extracted from deep models represents another prevalent research direction for constructing model ensembles in deep static MaE recognition. Several studies [209, 240] have explored integrating different regions (e.g., eyes, mouth, nose, and the entire image) of the face for model ensembling. Through the different ensemble strategies [209, 240], an integrated feature that combines information from all facial regions can be generated for MaE recognition. In addition to the common integration rules (e.g., majority voting, simple average, and weighted average), the weighted average for decision ensemble was also investigated in [79, 147]. Despite the aforementioned merits, ensemble learning increases the training cost of the model and lacks transparency and interoperability."}, {"title": "4.1.2 Transfer learning", "content": "Transfer learning in deep static MaE recognition enables deep models to be fine-tuned on MaE datasets after being pre-trained on large-scale, high-quality MaE datasets. The utilization of transfer learning can alleviate the overfitting issue due to the limited training data for static MaE recognition. By pre-training the model with additional data from other relative tasks, deep static MaE recognition methods can obtain more generic knowledge. For example, Ng et al. [150] used ImageNet [34] dataset to obtain the pre-trained models that are fine-tuned by the other related MaE datasets. The training efficiency can be improved by only fine-tuning the dense layers of deep models; therefore, the training computing expenditure is reduced in [4]. Instead of using general-purpose datasets, MaE-specific datasets can be used for deep model pre-training [8, 151]. For instance, Ngo et al. [151] employed the face identification dataset (e.g., VGGFace2 [14]) for model transfer. To further mitigate the overfitting in deep static MaE recognition, Atabansi et al. [8] used another MaE recognition dataset with high resolution and huge quantity for model pre-training."}, {"title": "4.1.3 Multi-task learning", "content": "Multi-task learning allows a deep model to extract macro-expression features from static facial images by using other facial behavior analysis tasks as auxiliary tasks in the deep static MaE recognition. For example, recent deep static MaE recognition methods have integrated facial landmark localization [17] and AU detection [170] to extract more robust MaE features [17, 135, 170, 258]. Moreover, Pons and Masip found that leveraging shared features across AU and MaE can enhance the performance of recognition [170]. To further improve the synergy, Li et al. [113] introduced an alignment loss to constrain the feature distribution between the AU detection and MaE recognition tasks. Furthermore, the multi-task learning can be used to fuse the global and local facial expressions by automatically assigning weights based on the importance of global and local facial information [237, 256]. Besides, facial expression synthesis [276], head pose estimation [21], body gesture [262], and gender learning [159] had been demonstrated as promising collaborative auxiliary tasks for deep static MaE recognition and could significantly improve performance. Therefore, Chen et al. [21] proposed a dual-attention based multi-task learning framework that contains a separate channel attention mechanism to calculate task-specific attention weights and an orthogonal channel attention loss to optimize the selection of feature channels for each auxiliary task. Since human emotion is conveyed equally via the body and the face in most cases, Zaghbani et al. [262] combined the upper body gesture action detection task for the deep static MaE recognition. Pan et al. [159] incorporated gender learning as an auxiliary task to factor in the effects of gender on the deep static MaE recognition since the characteristics of the same facial expressions from males, females, and infants can be significantly different. Instead of using a single auxiliary task, several studies proposed various multiple tasks for deep static MaE recognition to achieve more synergy. For instance, Qin et al. [172] developed an algorithm that simultaneously performs face recognition, MaE recognition, age estimation, and face attribute prediction. Similarly, Foggia et al. [51] proposed a comprehensive multi-task framework to integrate gender, age, ethnicity, and MaE recognition using facial images."}, {"title": "4.1.4 Attention-based learning", "content": "Attention-based learning is used to enrich the spatial information in deep static MaE recognition. The attention modules can focus on both global and local regions and generate more comprehensive static facial expression features that typically include local information, global information, and the corresponding dependency. Drawing inspiration from the human ability to locate salient objects in complex visual environments, attention mechanisms have emerged as a powerful tool in deep static MaE recognition [86]. By prioritizing critical facial regions, attention modules can significantly enhance the extraction and integration of expression features in order to achieve a more accurate and robust recognition model. For example, Fernandez et al. [141] developed an attention module to enhance the extraction of expression features by assigning higher weights to relevant regions. Li et al. [105] integrated LBP and deep features with an attention mechanism to enhance the performance of deep static MaE recognition. In wild scenarios, pose variations and occlusions present significant challenges for deep static MaE recognition [122, 215]. To address the aforementioned issues, various attention modules have been proposed to enhance model robustness and performance. For example, Li et al. [122] introduced an attention module designed to identify local facial regions associated with MaEs while simultaneously incorporating complementary global facial information. This dual-focus approach enables more robust MaE recognition under occlusion conditions by balancing localized details with a holistic view of the face. Wang et al. [215] proposed a region attention module that is tailored to address occlusions and pose variations. The region attention module in [215] highlights significant facial regions by leveraging attention weights and combines local region features to generate a compact and fixed-length feature representation for final classification. Motivated by the idea of integrating global and local features, Zhao et al. [280] developed a global multi-scale and local attention module. Zhao et al. [280] confirmed that the proposed attention module can ensure a comprehensive representation of facial expressions by capturing global features through multiple receptive fields while guiding the model to focus on salient local features. Motivated by facial attributes and human perception mechanisms, Liu et al. [130] introduced an adaptive multilayer perceptual attention module. By effectively capturing critical information from global, local, and salient facial regions, the proposed module in [130] significantly improves the robustness of deep static MaE recognition.\nFirst proposed in [40], vision transformers (ViTs) have brought remarkable advancements in the domain of deep static MaE recognition, primarily due to their ability to capture long-range dependencies across image patches [7, 281]. By leveraging the self-attention module inherent in transformers, Aouayeb et al. [7] pioneered the integration of ViTs with a squeeze-and-excitation block that can extract expression-related features. Xue et al. [244] exploited the ViT architecture to explore relationships among various facial regions for a more holistic understanding of static MaEs. To address the occlusion issue, Liu et al. [128] introduced a patch attention module that assigns attention weights to local facial regions. The integration of the patch attention module with a ViT can effectively capture both local and global dependencies. Building upon the advancements of ViTs, Zheng et al. [281] developed a dual-stream model that leverages a cross-fusion mechanism to combine facial landmark features with image-based features. Furthermore, the incorporation of additional modalities has proven instrumental in augmenting transformer-based deep static MaE recognition. For example, gradient images [144] and gray-level co-occurrence matrices [196] have been used as complementary inputs to enrich the feature space. These modalities not only enhance the discriminative power of the models but also provide deeper insights into the complex interplay of facial features. The optimization of transformer architectures also plays a critical role in advancing deep static MaE recognition. In [49], Feng et al. demonstrated that adjusting the parameter structures of ViTs through optimization algorithms has led to significant performance improvements."}, {"title": "4.1.5 Self-supervised learning", "content": "Self-supervised learning can enhance deep static MaE recognition models by broadening their ability to understand and analyze MaEs from a spatial perspective. More specifically, self-supervised learning is utilized to extract meaningful features from unlabeled data that come from unannotated MaEs [6, 24, 48, 117, 214], data captured from different perspectives [177, 178, 263], and multi-modal data sources [15, 66, 191].\nWhen applying for unlabeled MaE datasets, Li et al. [117] proposed a self-supervised learning method to facilitate compound MaE recognition with multiple labels. To avoid the expenditure of manual annotation, Wang et al. [214] introduced an automatic occluded MaE recognition method that can effectively use large volumes of unlabeled data. Chen et al. [24] combined self-supervised learning with few-shot strategies to train deep models for static MaE recognition with a limited amount of labeled data. Fang et al. [48] introduced an innovative approach by leveraging contrastive clustering in self-supervised models. The proposed method in [48] can strategically refine pseudo labels within face recognition datasets and thereby unlock the potential to enhance deep static MaE recognition. An et al. [6] proposed a self-supervised static MaE recognition that learns multi-level facial features without requiring labeled data.\nThe self-supervised learning can be employed for deep static MaE recognition tasks using non-frontal data samples. For example, Roy et al. [177] proposed a contrastive learning method for multi-view MaEs to address the viewpoint sensitivity and limited quantities of labeled data. By aligning features of the same expression from different perspectives, the proposed contrastive learning method can generate view-invariant embedding features for multi-view static MaE recognition. Later, Roy et al. [178] introduced an improved version of the proposed contrastive learning method in [177]. After obtaining effective view-invariant features, the proposed approach in [178] can incorporate supervised contrastive loss and Barlow Twins loss [263] to further differentiate MaE features with minimized redundancy.\nThe self-supervised learning had also been applied to multi-modal MaE recognition to address the challenges of integrating data from various modalities without explicit annotation. Siriwardhana et al. [191] utilized pre-trained self-supervised models to extract text, speech, and vision features to improve deep static MaE recognition tasks. Moreover, multi-modal self-supervised learning frameworks had been explored to extract and fuse multi-modal features to achieve promising results in deep static MaE recognition [15, 66]."}, {"title": "4.1.6 Potential limitations of different methods on deep static MaE recognition", "content": "While various methods in deep static MaE recognition have significantly improved model performance and broadened their applicability, they still present certain limitations. Ensemble learning, which combines multiple models to boost performance, often increases computational costs and reduces transparency and interoperability due to added complexity. Transfer learning, which relies on pre-trained models, can be less effective if there is a significant domain gap between the source and target datasets. Moreover, improper fine-tuning can reduce the model's ability to adapt to new datasets. Multi-task learning enhances model performance by jointly learning MaE recognition alongside related tasks. However, the choice of auxiliary tasks is critical since irrelevant or conflicting tasks may hinder learning. Attention mechanisms, while powerful, can focus excessively on certain regions, potentially ignoring other important information, especially in cases of facial occlusions or pose variations. Self-supervised learning is very dependent on the quality of input data or the limited labeled data. Most current large-scale MaE datasets are collected from different viewing angles, lighting conditions, and occlusions in network environments that cannot be controlled. Therefore, if the unlabeled data contains much noise or atypical samples, then the model may learn incorrect feature representations, which impairs its final recognition performance."}, {"title": "4.2 Deep Dynamic MaE Recognition", "content": "While MaE recognition can be achieved using static images and isolated frames, it greatly benefits from analyzing consecutive dynamic frames over time. In this section, we provide a comprehensive review of recent deep dynamic MaE recognition methods, summarized in Table 4."}, {"title": "4.2.1 Ensemble learning", "content": "Ensemble learning can enhance the accuracy and robustness of dynamic MaE recognition by frame aggregation that can combine temporal dynamic features across multiple frames and frame-level emotion classification results In deep dynamic MaE recognition, the frame aggregation can be implemented at decision-level [91, 92] and feature-level [132, 134, 152, 157].\nThe decision-level frame aggregation integrates the classification results of individual frames in the form of class probability vectors. For instance, Kahou et al. [92] explored the decision-level frame aggregation by using averaging and expansion for deep dynamic MaE recognition. Later, Kahou et al. [91] used expansion or contraction methods to aggregate single-frame probabilities into fixed-length video descriptors. Since the number of frames may vary, statistical characteristics (e.g., mean, variance, minimum, and maximum) can be utilized to summarize the frame-level outputs. However, relying solely on per-frame decisions may overlook the temporal dependencies between consecutive frames, which may impact the performance of deep dynamic MaE recognition.\nThe feature-level frame aggregation focuses on integrating frame-level features for final prediction. Nguyen et al. [152] concatenated frame features and fed them into a 3D CNN model for MaE recognition. By leveraging the attention mechanism, Liu et al. [132] introduced a frame aggregation method to integrate expression-related features. Specifically, they segmented videos into short clips and employed an attention-based feature extractor to capture salient features from these segments. Then, an emotional intensity activation network was designed to locate salient clips for generating robust features. Building on this, Liu et al. [134] proposed a transformer-based frame aggregation method for dynamic MaE recognition. To effectively integrate interrelationships among multi-cue features, Pan et al. [157] developed a hybrid fusion method for video-based MaE recognition. The proposed ensemble method in [157] combines the strengths of different types of features (e.g., appearance, geometry, and high-level semantic knowledge) and thereby improves the overall performance and robustness of dynamic MaE recognition systems."}, {"title": "4.2.2 Explicit spatio-temporal learning", "content": "Explicit spatio-temporal learning focuses on constructing deep models that are designed to extract spatio-temporal information from the dynamic MaE datasets. The explicit spatio-temporal learning methods can incorporate the temporal information into the encoded features by using a sequence of frames within a sliding window. The recurrent neural network (RNN) [43", "75": "have been used in dynamic MaE recognition due to the ability to process sequential data [2, 5, 103, 165, 203, 219, 261, 265, 287", "265": "employed an RNN to articulate morphological changes and dynamic evolution of MaEs by exploiting key facial regions based on facial landmarks. Zhi et al. [282", "103": "introduced attention-guided convolutional LSTM to capture dynamic spatio-temporal information. By leveraging the depth and thermal sequences as guidance priors, the proposed model can guide the model to focus on discriminative visual regions. An LSTM autoencoder was used to learn temporal dynamics for dynamic MaE recognition [203", "87": "have also been adopted to extract spatio-temporal features directly for dynamic MaE recognition [12, 35, 72", "72": "combined 3D Inception-ResNets [199", "35": "proposed a 3D CNN framework consisting of a stem layer, 3D Inception-ResNets structure, and gated recurrent unit (GRU) layer to"}]}