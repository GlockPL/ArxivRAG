{"title": "Phikon-v2\nA large and public feature extractor for biomarker\nprediction", "authors": ["Alexandre Filiot", "Paul Jacob", "Alice Mac Kain", "Charlie Saillard"], "abstract": "Gathering histopathology slides from over 100 publicly available cohorts, we\ncompile a diverse dataset of 460 million pathology tiles covering more than 30\ncancer sites. Using this dataset, we train a large self-supervised vision transformer\nusing DINOv2 [1] and publicly release one iteration of this model for further\nexperimentation, coined Phikon-v2. While trained on publicly available histology\nslides, Phikon-v2 surpasses our previously released model (Phikon) and performs\non par with other histopathology foundation models (FM) trained on proprietary\ndata. Our benchmarks include eight slide-level tasks with results reported on\nexternal validation cohorts avoiding any data contamination between pre-training\nand evaluation datasets. Our downstream training procedure follows a simple\nyet robust ensembling strategy yielding a +1.75 AUC increase across tasks and\nmodels compared to one-shot retraining (p<0.001). We compare Phikon (ViT-B)\nand Phikon-v2 (ViT-L) against 14 different histology feature extractors, making\nour evaluation the most comprehensive to date. Our result support evidences that\nDINOv2 handles joint model and data scaling better than iBOT. Also, we show\nthat recent scaling efforts are overall beneficial to downstream performance in\nthe context of biomarker prediction with GigaPath [2] and H-Optimus-0 [3] (two\nViT-g with 1.1B parameters each) standing out. However, the statistical margins\nbetween the latest top-performing FMs remain mostly non-significant; some even\nunderperform on specific indications or tasks such as MSI prediction - deposed\nby a 13x smaller model developed internally. While latest foundation models\nmay exhibit limitations for clinical deployment, they nonetheless offer excellent\ngrounds for the development of more specialized and cost-efficient histology\nencoders fueling AI-guided diagnostic tools.", "sections": [{"title": "1 Introduction", "content": "Histopathology is fundamental to disease diagnosis, treatment planning, and medical research. Tradi-\ntionally, pathologists manually analyze histology slides under microscopes to identify abnormalities,\ntissue patterns, and disease markers. Over the past years, the advent of digital pathology and the\ngrowing availability of Whole Slide Images (WSIs) has facilitated the emergence of computational\npathology (CPath), offering great potential for enhancing disease classification, treatment planning\nand drug discovery through advanced computational methods [4, 5].\nEarly CPath approaches often relied on transfer learning from ImageNet [6] due to the scarcity\nof large-scale annotated datasets in digital pathology. Recent advances in self-supervised learning\n(SSL) have significantly improved representation learning for histopathology, eliminating the need"}, {"title": "2 Related Work", "content": "Self-supervised learning has allowed researchers to leverage massive amounts of WSIs available\nthrough large-scale multicentric datasets such as TCGA\u00b9, CPTAC [14], GTEx\u00b2 or PAIP\u00b3. Early studies\nfocused on applying SSL methods to pre-train convolutional neural networks (CNN, [15]) or ViT [16]\narchitectures on such cohorts to later improve downstream applications. These studies mostly involve\npatch-level training strategies with various pretext tasks starting by contrastive learning methods\n[17, 18, 19, 20, 21] such as SimCLR [22] or MoCoV2 [23], and extending to non-contrastive ones\nover time [24, 25] (for instance, DINO [26] or MAE [27]). All of these methods have highlighted the\nbenefits of in-domain SSL pre-training and the use of ViTs over CNNs for a large panel of pre-training\nmethods [28].\nOver the past years, representation learning in CPath has witnessed significant advances in terms\nof data and model scaling, in combination with more sophisticated, histology-tailored frameworks.\n[29] pre-train CTransPath (a Swin-T architecture of 28M parameters [30]) on 14.3M patches from\nTCGA and PAIP, using semantically-relevant contrastive learning, an extension of MoCo v3 [31].\n[32] introduces HIPT (10M parameters), a hierarchical image pyramid vision transformer pre-trained\nwith DINO on 104M patch-level and 400 thousand region-level images. [28] publicly released five\nSSL models (up to 23M parameters) pre-trained on 32.6M TCGA tiles using histology-specific\naugmentations.\nRecent studies have notably marked a turning point for representation learning with the publication\nof first-ever foundation models in CPath. [33] introduces REMEDIS, a 230M parameter ResNet [34]\npre-trained with SimCLR on 50M pathology tiles from TCGA. [13] investigates the scaling properties\nof iBOT [12] and release Phikon, a ViT-Base model pre-trained on 43M histology tiles from TCGA.\nMore recently, DINOv2 [1] method has been extensively used to develop the latest FMs for histology\ndata. [10] proposed and publicly released UNI, a ViT-Large pre-trained on a curated pre-training\ndataset of over 100M tissue patches from 100 thousand diagnostic H&E WSIs from GTEx collection\nand proprietary hospital data. [8] introduces Virchow, a ViT-Huge (632M parameters) pre-trained\non 1.5M H&E slides collected from the Memorial Sloan Kettering Cancer Center. [35] pre-train a\nViT-S architecture using DINO on the largest pre-training histology dataset made of over 3 billion\nimages extracted from 420 thousand WSIs collected at Mount Sinai Health System. [9] introduces\nRudolfV, a ViT-L model pre-trained on a curated and diverse dataset of 133 thousand slides (1.2\nbillion tiles) covering different preparation protocols. Lastly, [36] pre-trained five models from ViT-S\nto ViT-L using either DINO or DINOv2 methods on 29 thousand WSI (Fresh-Frozen and FFPE) from\nTCGA. The authors evaluate their model using eva, an open-source framework for evaluating FMs\non clinically relevant downstream tasks.\nOver the past months, model and pre-training data scaling has reached unprecedented heights starting\nwith Prov-GigaPath [37], a 1.3 billion parameters model (ViT-g) pre-trained with DINOv2 on 1.3\nbillion tiles, followed by slide-level pretraining. [3] released H-Optimus-0, a ViT-g pre-trained on"}, {"title": "3 Materials & Methods", "content": "more than 500,000 WSIs. Lastly, [11] built upon Virchow to propose Virchow2 and Virchow2G, a\nViT-H and ViT-G pre-trained using multi-level resolutions on 3.1M WSIs, respectively.\nWhile the current trend of scaling FMs in CPath is a noteworthy step forward, larger models do not\nsystematically guarantee better and more robust representations of histology tissues, especially in the\ncontext of biomarker prediction [38] which is critical for the clinical deployment of AI solutions [39].\n[35] experimented downstream performance saturation for ViTs pre-trained with more than 325M\nhistology tiles. This observation is consistent with the work of [40] showing no benefit from training\na ViT-B on an uncurated dataset of 600M tiles instead of a ViT-S pre-trained on a curated subset. In\nthe same vein, the results from [41] question the necessity for scaling by showing competitive results\non WSI retrieval using only a 9M parameters transformer pre-trained on 6M histology tiles. Lastly,\nexperiments from [36] highlighted no significant benefits of ViT pre-training with DINOv2 over\nDINO through tile-level evaluation, assuming that the higher complexity of representations learned\nby DINOv2 can be detrimental to solve simple downstream tasks. Through extensive benchmarks,\n[38] suggests that smaller models perform on par with much larger models ones on most biomarker\nprediction tasks and are only marginally worse in others.\nBy comparing Phikon-v2 (ViT-L) to Phikon (ViT-B) and 14 other histology encoders publicly\navailable, we first aim at verifying the potential benefits of DINOv2 over iBOT when scaling together\nthe pre-training dataset size (10x) and model size (4x), then potential performance trends that may\narise from FM scaling.", "3.1 Training dataset": "PANCAN-XL Our pre-training set is composed of 132 datasets of digital pathology WSIs publicly\navailable along with 4 internal datasets, covering more than 30 cancer sites and normal tissues for a\ntotal of 58,359 WSIs. PANCAN-XL is highly heterogeneous with tissue samples (resections and\nbiopsies) collected from various sources, stainings and more than 50 different scanners. The main data\nsources are CPTAC (6,193 slides) and TCGA (29,502 slides) for malignant tissue, and GTEx (13,302\nslides) for normal tissue (23% of PANCAN-XL). Fresh-Frozen (FF) WSIs accounts for 31% of the\npre-training dataset (17,962 WSIs). Figure 1 displays the number of WSIs per site and indication\nfor our pre-training dataset. Of interest, 34% of PANCAN-XL WSIs come from three indications\nonly (Breast, Lung, Colorectal), while the majority (approx. 55%) comes from six different sites\n(Breast, Lung, Colorectal, Brain, Kidney, Uterus). The exhaustive list of the datasets comprised in\nPANCAN-XL is available in Extended Table 6.\nPreprocessing Prior to histology tiles extraction, an in-house bi-directional U-Net [42] is used to\nsegment tissue on the input WSIs and discard background and artifacts at 2.5x magnification. Using\nthis tissue mask, we then retrieve 224 x 224 histology tiles at magnification 20x (0.5 micrometers per\npixel) with a minimal tissue matter proportion of 60%. At the end of the process, PANCAN-XL is\ncomposed of 456,060,584 tiles at magnification 20x extracted from the initial 58,359 WSIs."}, {"title": "3.2 Model and pre-training setup", "content": "Following current trends in scaling both the dataset and model sizes, we used a vision transformer\nlarge (ViT-L) initialized with random weights and trained it on the PANCAN-XL 20x magnification\ntiles using the DINOv2 self-supervised training method.\nExtended Table 5 provides the exhaustive list of hyper-parameters used for pre-training our ViT-L\nwith DINOv2. Most hyper-parameters are taken from the original implementation\u2074 with the following\nadjustment based on iterative findings: our model is trained for 250,000 iterations with a base learning\nrate of 4e-3 followed by standard cosine schedule. The number of warmup iterations was set to\n25,000 and 75,000 for teacher temperature. No registers were used.\nConsidering that the total number of iterations was set to 250,000 with a batch size of 4,096, this\nmeans that roughly 1 billion images were seen during training, which approximately corresponds"}, {"title": "3.3 Evaluation setup", "content": "We evaluate and compare different feature extractors on WSI-level weakly-supervised tasks. Those\nfollow a conventional two-step multiple instance learning (MIL) approach. First for each WSI, Nt\nnon-overlapping histology tiles of size 224x224 are extracted at 20x magnification (0.5 micrometers\nper pixel). These images are passed into a frozen feature extractor yielding a matrix of size (Nt, d), d\nbeing the output features dimension. Finally, a permutation-invariant pooling operator is optimized\nto aggregate patch-level features into a single slide-level prediction. For all tasks, the same exact\nNt = 5,000 tiles were randomly sampled from each WSI across feature extractors (padding is applied\nfor slides with less than Nt tiles with organic tissue). Note that we set Nt = 400 for ISUP grading\ntask due to memory constraint. Also, we chose the Attention-Based Multiple Instance Learning\n(ABMIL) algorithm [43] to perform feature matrices aggregation, which serves as a strong baseline\namong MIL algorithms [44]. We use the original one-layer gated architecture with input embeddings\nmapped to an embedding dimension of 128 and weights attention matrix of dimension (128, 128),\nwithout dropout and with sigmoid activation. This makes a total of 164, 482 learnable parameters.\nABMIL was trained using the Adam optimizer [45] with parameters (0.9, 0.999), a batch-size of 16\nand a constant learning rate of 1e-3. Binary cross-entropy (resp. cross-entropy) was used for binary\nclassification tasks (resp. multi-class classification).\nOur training and evaluation protocol is performed as follows:\n1. For each task, we create 5 case and label-stratified splits on the training dataset that are kept\nconstant across all feature extractors. Then, we perform a 5-fold cross validation by keeping\none split apart each time for validation. For each training split, we train an ensemble of\n5 models (which differ only from initialization) for 100 epochs (except for the DINOv2\nViT-L ImageNet, for which we use 300 epochs to take into account domain shift). A single"}, {"title": "3.4 Evaluation tasks", "content": "We derive 8 WSI-level tasks coming from 5 different cancer sites. All tasks consider WSIs at the 20x\nmagnification for feature extraction, except for RCC (Kidney) (5x). Note that there is an overlap\nbetween TCGA downstream training cohorts and our pre-training dataset; however, we make sure to\navoid any data leakage between PANCAN-XL and downstream external validation cohorts for which\nwe report classification metrics, as this data leakage tends to overestimate generalization error [10].\nMETASTASIS (Breast Lymph Nodes): The breast metastasis detection task from the 2016 Cancer\nMetastases in Lymph Nodes Challenge (CAMELYON16 [47]) involves 399 H&E FFPE histopathol-\nogy whole slide images (WSIs) of sentinel lymph nodes from Radboud University Medical Center\nand University Medical Center Utrecht. For our training and evaluation process, we utilized the\nofficial train-test splits. Train and test sets contain 269 (159 normal, 110 metastasis) and 130 (80\nnormal, 50 metastasis) WSIs, respectively.\nMSI (Colorectal): MSI mutation impact the deoxyribonucleic acid damage repair (DDR) process\nin tumors. Early recognition of this biomarker may benefit the patients through specific therapies.\nThis is of particular interest in colorectal cancer [48] where MSI mutation can account for up to\n20%. We aim at predicting high vs low instability (MSI-H vs MSS/MSI-L). Training sets: We use\nTCGA-COAD and TCGA-READ as training cohorts, accounting for 611 H&E FFPE WSIs and 595\npatients (511 MSS/MSI-L, 84 MSI-H). Evaluation sets: We use two different external validation\ncohorts. First one is Paip. The PAIP consortium provides 2,547 H&E FFPE WSI collected from three\nKorean centers (Seoul National University Hospital, Seoul National University Bundang Hospital\nand SMG-SNU Boramae Medical Center), covering six cancer types. We retrieved 47 patients (47\nWSIs, 35 MSS/MSI-L and 12 MSI-H) from PAIP with colorectal tumors and available MSS/MSI-L\nlabels, provided by the Pathology AI Platform. Second cohort is Cy1, which is a private collection\nof 698 H&E and H&E&S biopsies from 698 patients (450 MSS/MSI-L, 248 MSI-H) digitized in\nFrance. For all cohorts, MSI labels were determined using 4-plex IHC and MSI-PCR when available.\nHER2 (Breast): HER2 (encoded by ERBB2 gene) is a protein present in the membranes of cells\nand controlling their growth. HER2 is amplified and/or overexpressed in approximately 15-20% of\nbreast cancers. The overexpression and/or amplification of HER2 has been associated with aggressive\nclinical behavior but with a high probability of response to HER2 targeted therapy during and/or after\nchemotherapy, resulting in a significant improvement in disease-free and overall survival. Training\nsets: We use TCGA-BRCA as training cohort using reliable labels from [49]. This corresponds to 801\nH&E FFPE WSIs (680 HER2-negative, 121 HER2-positive) collected from 752 patients. Evaluation"}, {"title": "4 Results", "content": "Table 2 shows the results of the slide-level evaluation described in section 3.3. We compare Phikon-v2\nagainst our previously released feature extractor (Phikon) and 14 other extractors from the literature\n(13 in-domain and one out-of-domain baseline). To identify general performance trends, it is necessary\nto aggregate performance across multiple tasks. A common practice consists in reporting mean or\nmedian performance over downstream tasks (see Table 2 last column), yet statistical comparisons\noffer different insights on models general behaviours and pairwise rankings. For this reason, we also\nhighlight Fisher-combined corrected p-values in Table 3, which indicates whether a given model i\nat ith row shows superior performance overall with respect to another model j at jth column from a\nstatistical standpoint. This evaluation procedure less penalizes excellent models under-performing\non a very limited number of tasks (e.g., H-Optimus-0 on IDH1, or UNI on MSI-Cy1), hence giving\nmore reliable models rankings per se. We believe 3 should guide the way models are evaluated and\ncompared in future benchmarks.", "4.1 ViT-L+ histology encoders stand out": "Tables 2 and 3 consistently put GigaPath to the forefront. The later ranks first on 7 out of 10\ndownstream external cohorts. Similarly, H-Optimus-0 ranks first on 4 out of 7, explaining the absence\nof statistical difference between Bioptimus model and GigaPath in Table 3. Phikon-v2 and UNI\nachieve excellent performance overall, respectively ranking first on 2 out of 7 tasks, and ranking\nsecond on 3 out of 7 tasks. Those two models share key components by design: they both leverage a\nViT-L architecture pre-trained using DINOv2 on a dataset with same order of magnitude (~50k slides"}, {"title": "4.2 Don't set aside specialized models", "content": "Table 4 compares iBOT (B/16) Coad (Ours) against top foundation models on MSI prediction\ntask in colorectal cancer. The former is a ViT-B pre-trained on 4 million tiles from TCGA-COAD\ncohort for 165k iterations using iBOT [13]. This model, 13x smaller and pre-trained on 350x less\nhistology images than Virchow2, ranks first on our benchmark made of 3 external validation cohorts\n(2 private, 1 public being Paip). Obviously, these results suggest that scaling per se is not as of\nnow, the systematic solution for solving biomarker prediction tasks. This is absolutely critical for the\nvalidation, clinical deployment and adoption of diagnostic and pre-screening tools, may it be working\non MSI prediction [48] or any other biomarker [65]. Note that pre-training with DINOv2 slightly\ndecreased performance compared to iBOT, which may suggest the DINOv2 superiority over iBOT\n(both methods beings conceptually very similar) is not straightforward for lighter models."}, {"title": "4.3 Ensembling yields consistent improvements over one-shot retraining", "content": "Lastly as shown on Figure 2, our ensembling strategy designed for slide-level downstream tasks\nyields strong improvements over one-shot retraining (see Section 3.3 for details), a very popular\nyet sub-optimal practice. Indeed, ensembling yields a statistically significant AUC increase over"}, {"title": "5 Conclusion", "content": "Phikon-v2 significantly improves upon Phikon by employing a new self-supervised approach, a\nwider architecture, and a larger collection of public datasets for pre-training. It demonstrates\ncompetitive performance compared to current state-of-the-art methods across a range of 8 slide-level\nbiomarker prediction tasks covering 5 external datasets unseen during pre-training. As with Phikon,\nwe are publicly releasing Phikon-v2 under a non-commercial license, enabling widespread access,\ncollaboration, and further research and development in the field.\nIn light of recent literature, our scaling experiments with DINOv2 have been successful and marked\nan improvement over iBOT which was used to pre-train our previous foundation model. Nonetheless,\nwe did not assess the scalability of iBOT-only on PANCAN-XL, which is led to further research.\nOther Masked Image Modelling methods may also bring improvements over DINOv2 or iBOT [66].\nOur evaluation of a large and exhaustive panel of feature extractors proposed by the community has\nalso confirmed the undeniable benefit of in-domain self-supervised pre-training over out-of-domain\npre-training, even for models that are 30 times smaller [41].\nWhile GigaPath, H-Optimus-0, Phikon-v2, UNI and Virchow2 all reinforce the benefits of FM\nscaling to improve downstream performance, these models remain general-purpose by design in\nterms of downstream tasks and target organs. Consequently, with the extensive range of models\navailable in the community, it is likely that some may be better suited to specific tasks and organs, a\ntrend already observed in other benchmarks [38] and highlighted in this work specifically for MSI\nprediction. Ensembling at the feature extractor is an existing avenue [68] yet comes with a significant\ncomputational bottleneck. We believe that, similar to the specialization seen in large language models,\nan important focus should be placed on organ- and task-specific fine-tuning [69] or distillation [70] of\nfoundational models for clinical deployment [71]. This focus is crucial as scaling experiments reach\nan inflexion point where significant improvements become challenging without access to massive\ndatasets and substantial computing power."}]}