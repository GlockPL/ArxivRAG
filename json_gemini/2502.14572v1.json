{"title": "FACTOR GRAPH-BASED INTERPRETABLE NEURAL NETWORKS", "authors": ["Yicong Li", "Kuanjiu Zhou", "Shuo Yu", "Qiang Zhang", "Renqiang Luo", "Xiaodong Li", "Feng Xia"], "abstract": "Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations. To address this challenge, we propose AGAIN, a fActor Graph-based Interpretable neural Network, which is capable of generating comprehensible explanations under unknown perturbations. Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference. Specifically, we construct the factor graph to express logical rules between explanations and categories. By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic. Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations. Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph. Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Comprehensibility of neural network explanations depends on their consistency with human insights and real-world logic. Comprehensible explanations promote better understanding of decisions and establish trust in the deployment of neural networks in high-stake scenarios, such as healthcare and finance (Virgolin & Fracaros, 2023; Fokkema et al., 2023; Luo et al., 2024a;b). However, as shown in Figure 1, interpretable neural networks are vulnerable to malicious perturbations which are infused into inputs, misguiding the model to generate incomprehensible explanations (Tan & Tian, 2023; Baniecki & Biecek, 2024). Such explanations may cause users to make wrong judgments, resulting in security concerns in high-stake domains. For example, a doctor prescribing medication based on a medically illogical explanation (i.e. incomprehensible explanation) of the pathological prediction may lead to misdiagnosis. Therefore, it is crucial to ensure that interpretable neural networks generate comprehensible explanations under perturbations.\nSeveral existing efforts have been devoted to investigating comprehensible explanations (Kamath et al., 2024; Sarkar et al., 2021; Chen et al., 2019). Many of them craft adversarial samples by adding perturbations to the dataset beforehand and retrain the model with extra regularization terms. Regularization terms constrain model to generate explanations that are similar to the explanation labels of the adversarial samples. Empirical results show that retrained models are able to learn the adversarial sample data distribution and reduce the probability of being misled by the predetermined perturbation.\nHowever, the above solutions assume perturbations are known to the model, which leads to their failure to generate comprehensible explanations under unknown perturbations (Novakovsky et al., 2023). The reasons are as follows: 1) it is impossible to craft adversarial samples for all unknown perturbation types (G\u00fcrel et al., 2021); 2) even if the adversarial samples are available, retraining is effective for only a limited number of perturbation types simultaneously (Tan & Tian, 2023). Thus, despite recent progress on comprehensible interpretability, it is still challenging to provide comprehensible explanations under unknown perturbations. Considering this, we seek to solve this problem from a different perspective - instead of optimizing the training strategy, we innovate the inference process. Our goal is to design an interpretable neural network capable of rectifying incomprehensible explanations under unknown perturbations during inference. We draw inspiration from knowledge integration with factor graphs. Unknown perturbations cause the explanatory semantics to violate the exogenous knowledge in the factor graph (Tu et al., 2023; Xia et al., 2021). Factor graph reasoning enables us to identify and rectify explanatory logical errors without learning perturbations.\nWe propose AGAIN (fActor GrAph-based Interpretable neural Network), which generates comprehensible concept-level explanations based on the factor graph under unknown perturbations (Tiddi & Schlobach, 2022). AGAIN consists of three modules, including factor graph construction, explanatory logic errors identification, and explanation rectification. In the first module, semantic concepts, label catergories, and logical rules between them are encoded as two kinds of nodes (i.e., variable and factor) in the factor graph, while their correlations are encoded as the edges. Based on the constructed factor graph, the logic relations among concepts and categories are explicitly represented. In the second module, AGAIN generates the concept-level explanations and predictive categories and then imports them into the factor graph to identify erroneous concept activations through logical reasoning. In the third module, we propose an interactive intervention switch strategy for concept activations to correct logical errors in explanations. The explanations that are further regenerated align with external knowledge. The regenerated explanations are used to predict categories. Extensive experiments are conducted on three datasets including CUB, MIMIC-III EWS, and Synthetic-MNIST. Experimental results demonstrate concept-level explanations generated by the proposed AGAIN under unknown perturbations have better comprehensibility compared to baselines such as ICBM, PCBM, free CBM, and ProbCBM.\nOur contributions can be summarized as follows: 1) against unknown perturbations: we present an innovative interpretable neural network based on factor graph. It integrates real-world logical knowledge to generate comprehensible explanations under unknown perturbations; 2) forward feedback: we design logic error identification and rectification methods based on the factor graph. Our method is able to rectify logic violating explanations during inference without learning perturbations, unlike previous methods; 3) theoretical foundation of factor graph: we prove that the comprehensibility of explanations is positively correlated with the involvement of factor graph; 4) superior performance: we conduct extensive experiments on three datasets to demonstrate that AGAIN can generate more comprehensible explanations than existing methods under unknown perturbations."}, {"title": "2 RELATED WORK", "content": "Comprehensible Explanation under Perturbation. Studies of comprehensible explanations under perturbations can be divided into two categories: attacks on comprehensibility and defenses of comprehensibility. Studies of attacks on comprehensibility aim to design perturbations that misguide the model to generate incomprehensible explanations. Some methods modify salient mappings with perturbations that make the explanation incomprehensible to users (Ghorbani et al., 2019; Dombrowski et al., 2022). Furthermore, there are several efforts that propose additional types of perturbations (Rahmati et al., 2020; Carmichael & Scheirer, 2023; Huai et al., 2022). They demonstrate that many types of perturbations can undermine the comprehensibility of explanations. In contrast,"}, {"title": "3 NOTATIONS AND PRELIMINARIES", "content": "Interpretable Neural Network. Interpretable neural networks are defined as neural networks that automatically generate explanations for decisions (Esterhuizen et al., 2022; Rieger et al., 2020; Peng et al., 2024). For more comprehensible explanations, we utilize a concept bottleneck model to generate concept-level explanations, which utilize various semantic concepts to explain the predictions Koh et al. (2020b); Huang et al. (2024). Specifically, let x denote an input sample, the concept bottleneck model predicts the category y and outputs a boolean vector $c \\in {0,1}^M$ of M concepts. Let $c \\in c$ denote a concept. Let c = 1 indicate that concept c is present in x and influences the model decision. c is the concept-level explanation of the model prediction.\nFactor Graph. Factor graph serves as a probabilistic graphical model to depict relationships among events (Yu et al., 2023; Bravo-Hermsdorff et al., 2023). As shown in Figure 2, within a factor graph, two node types exist: 1) variables, which delineate events; 2) factors, which articulate the relationships between events. Formally, a factor graph G = (V, F) contains the set of variables V and the set of factors F. We denote the set of edges as E. For any $v_i \\in V$ and $f_i \\in F$, we let $(v_i, f_i) \\in E$ denote an edge of G. Let $N(f_i) = {v_i \\in V | (v_i, f_i) \\in E}$ denote the set of neighbors of factor $f_i$ in G. We let variables correspond to concept and category labels. We let factors correspond to logical rules. This enables G to encode logical rules between concepts and categories.\nKnown and Unknown Perturbation. Formally, let $ \\delta $ denote perturbations uniformly. The designer of the model crafts adversarial samples against one perturbation $ \\delta_k$ to obtain a retrained model $\\hat{h}$ that minimizes $|\\hat{h} (x; \\theta) - h (x + \\delta_k; \\theta)||$. $\\theta$ is the model parameter. For model h, $ \\delta_k$ denotes one known perturbation, and any $ \\delta_u \\in {\\delta | \\delta \\neq \\delta_k }$ denotes one unknown perturbation.\nAdversarial Attacks against Concept-level Explanations. Unlike standard adversarial attacks, adversarial attacks against explanations do not compromise task predictions. For concept-level in-"}, {"title": "4 THE DESIGN OF AGAIN", "content": "AGAIN consists of three modules: 1) first, we encode logic rules of the real-world as a factor graph (Section 4.1); 2) then, we generate the initial concept-level explanation through the concept bottleneck. The factor graph reasoning is utilized to identify whether the explanation of concept bottleneck violates the external logic, and thus to detect whether the perturbation exists (Section 4.2); 3) finally, an interactive intervention strategy is designed to rectify the explanation and input it to the category predictor (Section 4.3). The overall architecture of AGAIN is shown in Figure 3."}, {"title": "4.1 FACTOR GRAPH CONSTRUCTION", "content": "To construct a factor graph, we first define the logic rule set $R = \\{ r_i \\}_{i=1}^N$, which contains two types: 1) concept-concept rule: all predicates consist of concepts. Such rules are used to constrain potential relationships among various concepts. For instance, there is a rule of coexistence or exclusion between concepts $c_i$ and $c_j$, which can be formalized in logical notation as: $c_i \\Leftrightarrow c_j$ or $c_i \\oplus c_j$. 2) Category-concept rule: all predicates are defined by concepts and categories. Such rules are used to constrain potential correlations between concepts and categories. For instance, the coexistence or exclusion rule that exists between concept $c_i$ and category label $y_j$ can be formalized as: $C_i \\Leftrightarrow y_j$ or $C_i \\oplus y_j$.\nThen, we encode the above logic rules into a factor graph G. As shown in Figure 4, we illustrate the construction of the factor graph. Specifically, there are two types of variables $V = V^C \\cup V^Y$, where $V^C$ and $V^Y$ denote concept and category variable set, respectively, and are linked by F. In this way, each factor $f_i \\in F$ corresponds to the i-th logic rule $r_i$. Each factor is defined as a potential function that performs logical operations based on different rules, which can be categorized into coexistence and exclusion operations. Moreover, we define a potential function $ \\Psi_i$ for each factor $f_i$, which outputs a boolean value for each $N(f_i)$. If $N(f_i)$ makes $r_i$ true, $ \\Psi_i (N(f_i)) = 1$, otherwise $ \\Psi_i (N(f_i)) = 0$."}, {"title": "4.2 EXPLANATORY LOGIC ERRORS IDENTIFICATION", "content": "AGAIN generates an initial concept-level explanation and identifies logical errors in the initial explanation. Specifically, we employ a concept bottleneck structure, a popular concept-level interpretable module, to capture the semantic information from instances, which can learn the mapping between semantic information and concepts (Koh et al., 2020a). The concept bottleneck contains a concept predictor $h_c : R^D \\rightarrow R^M$ and a category predictor $h_y : R^M \\rightarrow R$. The instance x is first mapped to the concept space by $h_c$ and obtains the corresponding concept activation vector, i.e., $ \\hat{c} = h_c(x)$, where $ \\hat{c} \\in [0, 1]^M$, and then the conceptual activation vector is fed into $h_y$ to yield the final predicted category, i.e., $ \\hat{y} = h_y(\\hat{c})$, where $ \\hat{y} \\in \\{ 0, 1 \\}$. $ \\hat{c}$ is defined as the initial explanation.\nNext, G takes $ \\hat{c}$ and $ \\hat{y}$ as inputs to assign $V_c$ and $V_y$, respectively. If concept $ \\hat{c} > 0.5$, $ \\hat{c} \\in c$, we set variable $v_c = 1$, $v_c \\in V^C$, otherwise $v_c = 0$. For the category variables, we set $v_{\\hat{y}} = 1$, $V_{\\hat{y}} \\in V^Y$, and $V_Y \\backslash v_{\\hat{y}} = {0}^{K-1}$.\nSubsequently, we evaluate the likelihood of the variable assignment under rule constraints through logical reasoning. Firstly, after each variable (concept and category) in G is assigned a value, boolean values are output from potential functions of all factors. These boolean values indicate whether the assignments of concepts and categories satisfy the logical rules represented by potential functions. Therefore, the weighted sum of all potential functions quantifies the extent to which concept assignments satisfy the logic rules in G.\nThen, we seek to obtain the likelihood of the current concept assignments occurring, conditional on the known categories and logic rules. We quantify this likelihood by computing a conditional probability using the weighted sum of potential functions. We consider all possible concept assignments and compute the expectation of current concept assignments. This expected value is considered as the conditional probability, which is then used to detect whether concept activations are perturbed. For illustrative purposes, we provide an example. Suppose there are concepts A and B. The current concept assignment is {1,0} denoting A = 1 (active) and B = 0 (inactive). We iterate through all four possible assignments: {1,0}, {0, 1}, {1, 1}, {0,0}. We compute the weighted sum of the potential functions for each of the four cases and compute the expectation of the potential function for {1,0}. This expectation is the conditional probability that concept assignment {1,0} occurs conditionally on the known categories and logic rules. Formally, we denote this conditional probability as $P(V^C |V^Y)$: \n$P(V^C |V^Y) = \\frac{ exp(\\sum_{i \\in N} w_i \\Psi_i )}{ \\sum_{V \\in \\Phi} exp(\\sum_{i \\in N} w_i \\Psi_i ) } $          (1)\nwhere $ \\Phi $ represents all cases of concept assignments, and V represents a case in $ \\Phi $. This implies that the denominator of Eq. (1) is the normalized constant term. We use $P(V^C |V^Y)$ to evaluate the comprehensibility of explanation $ \\hat{c}$. Higher $P(V^C |V^Y)$ indicates that $ \\hat{c}$ is more comprehensible, and vice versa. In theory, we consider that a comprehensible $ \\hat{c}$ should satisfy each $r_i \\in R$, ensuring that $P(V^C |V^Y)$ attains an upper bound denoted as $vP (V^C |V^Y)$: \n$vP (V^C |V^Y) = \\frac{1}{a}  max_{V \\in N} (exp(\\sum_{i \\in N} w_i \\Psi_i )) $                (2)\nwhere a denotes the denominator of Eq. (1). However, in practice, even concept explanations generated in a benign environment (without perturbations) rarely satisfy all the rules. Overly strict logical constraints may instead cause G to lose its ability to recognize perturbations. Therefore, we allow a comprehensible explanation to violate some low-weight rules. Naturally, we establish a relaxed identification condition for distinguishing explanations corrupted by perturbations from comprehensible explanations:\n$P (V^C |V^Y) > \\delta * vP (V^C |V^Y),  $            (3)"}, {"title": "4.3 EXPLANATION RECTIFICATION", "content": "Once explanations with logical errors are identified, AGAIN rectifies the explanation and put it as an input to the category predictor for the final prediction. For this objective, we propose an interactive intervention switch strategy aimed at enhancing the conditional probability of the G. The proposed strategy intervenes on the values of $V_C$ and interactively observing the potential function difference. In this paper, we assume that $ \\hat{y}$ are unaffected under perturbations, thus we do not intervene in $V_Y$. The pseudocode of the interactive intervention switch is listed in Appendix A.\nOur intervention strategy can be divided into three steps. First, we traverse all factors with $ \\Psi_i = 1$. For factor $f_i \\in F$, we modify the boolean value of its concept variables, considering the modification as a single intervention operation. Given that $f_i$ may be connected to multiple concept variables, there exist numerous intervention cases. For instance, consider $f_i$ containing concept variables $v_i$ and $v_j$. There are three possible intervention cases: intervene only $v_i$, intervene only $v_j$, and intervene both $v_i$ and $v_j$. We define the full set of possible intervention cases for $f_i$ as $T_i$. For each case $t_i \\in T_i$, we compute the potential function difference $s_i$, which represents the change in the potential function after executing $t_i$. Note that $t_i$ does not only change $f_i$, but also changes the 1-hop neighbor factors of $N(f_i)$. Thus, we define $s_i$ as follows:\n$s_i = \\sum_{j \\in F_i } w_j (\\Psi_j^{t_i} - \\Psi_j),  $                        (4)\nwhere $F_i = \\{ f_j | N(f_i) \\in N(f_j) \\} \\cup \\{f_i \\}$. $ \\Psi_j^{t_i}$ denotes the $ \\Psi_j$ value after $t_i$ intervention. Subsequently, after traversing through all possible interventions in $T_i$, we identify the intervention with the highest $s_i$ as a candidate intervention. We generate the candidate intervention for each factor with $ \\Psi_i = 1$. We aggregate all the candidate interventions into a final intervention, denoted as $t^*$. We execute $t^*$ on $V_c$. From the set of intervened $V_c$ and $t^*$, we generate a binary concept intervention vector $z \\in \\{ -1, 1 \\}^M$ and a binary mask vector $m_t \\in \\{ 0, 1\\}^M$. z denotes the concept activation status, where -1 indicates activated, and 1 indicates inactivated. $m_t$ denotes whether the concept is intervened or not, where 1 indicates intervened, and 0 indicates not intervened.\nFinally, we employ z and $m_t$ to rectify the initial explanation $ \\hat{c}$. We utilize $m_t$ to aggregate $ \\hat{c}$ and z for a rectified concept activation vector $ \\hat{c}_{re}$: \n$ \\hat{c}_{re} = z \\odot m_t + \\hat{c} \\odot m_{\\neg},             $          (5)\nwhere $m_{\\neg}$ is obtained by flipping the bits of $m_t$. $ \\odot $ denotes dot product operation. The purpose of employing the intervention mask is to facilitate $ \\hat{c}_{re}$ to retain activations in $ \\hat{c}$ that are not intervened."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETTINGS\nDatasets and Baselines. We evaluated AGAIN on two real-world datasets, CUB, MIMIC-III EWS, and one synthetic dataset, Synthetic-MNIST. We choose two categories of methods. (1) Concept-level methods: CBM (Koh et al., 2020a), Hard AR (Havasi et al., 2022), ICBM (Chauhan et al., 2023), PCBM (Yuksekgonul et al., 2023), ProbCBM (Kim et al., 2023), Label-free CBM (Oikarinen et al., 2023), ProtoCBM (Huang et al., 2024), and ECBMs (Xu et al., 2024). (2) Knowledge integration methods: DeepProblog Manhaeve et al. (2018), MBM Patel et al. (2022), C-HMCNN Giunchiglia & Lukasiewicz (2020), LEN Ciravegna et al. (2023), DKAA Melacci et al. (2021), and MORDAA Yin et al. (2021). In addition, we compare AGAIN with the retrained versions of these baselines that employ state-of-the-art adversarial training strategy. More details on the datasets and baselines are provided in Appendix C.1 and C.2. The experimental results on the synthetic dataset are presented in Appendix D.4."}, {"title": "5.2 EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS", "content": "Identifying Perturbations. We apply adversarial perturbations acquired during black-box training to randomly perturb multiple instances in the test set. Known and unknown perturbations are denoted by $ \\delta_k$ and $ \\delta_u$, respectively, with $ \\epsilon $ representing the perturbation magnitude. We evaluate the ability of AGAIN to recognize logical errors of explanations by reporting SR and IR values under different perturbation magnitudes in Table 1. The results demonstrate that AGAIN achieves remarkable IR and SR values under both $ \\delta_k$ and $ \\delta_u$. Specifically, AGAIN attains nearly 100% IR across all perturbation magnitudes. With SR results averaging up to 98%, we also validate that factor graph G can effectively identify explanations from benign instances and permit them to directly predict categories without logical reasoning. Furthermore, it is also worth noting that as the perturbation magnitude increases, the IR value also gets larger. This observation is attributed to the larger perturbation magnitude causing a more pronounced logical violation in the generated explanations. The G more readily identifies these violations."}, {"title": "Comprehensibility of Explanations.", "content": "To investigate the comprehensibility of the explanations generated by AGAIN, we perform extensive experiments on both datasets for evaluating the LSM of the explanations, and the comparison are reported in Table 2. The baselines subjected to the retraining are identified by the \"-AT\" suffix. The results reveal that the comprehensibility of the explanations generated by AGAIN outperforms all concept-level methods, including the \"-AT\" versions of these baselines, under different perturbation magnitudes. Particularly, previous interpretable models fail to generate logically complete explanations with LSMs lower than 48 under unknown perturbations of magnitude 32, but explanations from AGAIN can reach as high as 92.30. Moreover, we demonstrate that AGAIN is hardly affected by the perturbation magnitude compared to the baseline methods. This effect is attributed to the corrective capability provided by G for any level of logic violation. For kowledge integration methods, since DeepProblog, MBM, and C-HMCNN are unable to generate concepts, we splice their knowledge integration modules onto the CBM. The results show that the LSM of AGAIN is optimal. In contrast, deepProblog can only constrain category predictions, not concept predictions, which results in low LSM under perturbation. The knowledge introduced by methods MBM and C-HMCNN can constrain concepts, but they only use logical rules between concepts and concepts, making their performance inferior to AGAIN. Meanwhile, since DKAA and MORDAA have Multi-label predictors, we directly use Multi-label predictors to predict concepts. LEN can only constrain category predictions. DKAA and MORDAA detect adversarial perturbations in the samples using external knowledge, but they cannot correct the wrong concepts triggered by these perturbations."}, {"title": "Validity of the Factor Graph.", "content": "As the theoretical analysis in Appendix B demonstrates, G improves the comprehensibility of explanations. We experimentally validate this claim and further demonstrate that increasing the number of factors in G enhances the predictive accuracy of concepts. Specifically, we employ subgraph G' extracted from the original G for reasoning and analyze the impact on prediction accuracy by increasing the ratio of G' to G. In Figure 5, we depict the changes in P-ACC and E-ACC across four perturbation magnitudes on both datasets. It is evident that both P-ACC and E-ACC exhibit substantial improvement as the number of factors in G' increases. This observation indicates that G contributes in generating explanations with similarity to the ground truth explanations for improving the predictive accuracy. Moreover, as the number of factors in G' exceeds that of G (ratio > 1.0), E-ACC begin to converge. This also validates the setting for the number of factors in the original G is reasonable. In addition, we report the P-ACC and E-ACC comparison results of AGAIN on the CUB dataset (see Table 3 and Table 4). The comparison results of P-ACC and E-ACC on other datasets are provided in Appendix D.5. The results indicate that AGAIN is optimal for E-ACC on all three datasets. Furthermore, since perturbations do not impact the final predictions, the P-ACC remains consistent across different levels of perturbation. The P-ACC of AGAIN are comparable to the other baselines because the factor graph does not improve the task predictive accuracy. We present a comparison of E-ACC and P-ACC between the CBM and AGAIN on the two real-world datasets in Figure 6. The results show that AGAIN achieves higher"}, {"title": "Rectification of Interactive Intervention Switch.", "content": "In Figure 7 (a) and (b), we illustrate several instances of two real-world datasets along with rectified explanation segments with a dimension of 5 and show the utilized rules. The interactive intervention switch effectively rectifies the logical error of the explanation based on the predefined rules, thereby enhancing the overall logical coherence of the explanation.\nAblation Study. We conducted ablation studies to examine the effectiveness for each rule type in G. The larger number of rules in CUB compared to MIMIC-III EWS contributes to a more significant ablation effect, so we executed ablation studies on the CUB data. We investigated the performance of all factor graph variants by reporting the LSM results in Table 5. Fy and Fc denote the set of factors encoding category-concept rules and the set of factors encoding concept-concept rules, respectively.\nAccording to Table 5, we can draw the following conclusions. First, the variant without G (i.e., $F = \\{ \\emptyset \\}$ ) yielded the lowest LSM values, affirming the essential role of G in the model. Second, G encoding both concept-concept and category-concept rules (i.e., $F = F_Y \\cup F_C$) achieved the best performance. This factor graph (constructed in this paper) demonstrates an average improvement of 6.86 over other variants of G that encode only the concept-concept or category-concept rules."}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "In this paper, we explore the comprehensibility of explanations under unknown perturbations and propose AGAIN, an factor graph-based interpretable neural network. Inspired by the knowledge integration of factor graphs, AGAIN obtains comprehensible explanations by encoding prior logical rules as the factor graph and utilizing factor graph reasoning to identify and rectify logical error in explanations. It addresses the inherent limitations of current adversarial training-based interpretable models by guiding explanation generation during inference. Furthermore, we provide a theoretical analysis to demonstrate that factor graphs significantly contribute to obtaining comprehensible explanations. We present an initial attempt to generate comprehensible explanations under unknown perturbations from the inference perspective. AGAIN provides an effective solution for the defense of interpretable neural networks against various perturbations and meanwhile saves the high cost of retraining. It takes a significant step towards resolving the crisis of trust between humans and interpretable models. In addition, we note two limitations of AGAIN: 1) the validity of AGAIN relies on the correct prediction categories. Wrong categories imported into the factor graph cause explanations to be wrongly rectified; 2) when domain knowledge changes, the factor graph needs to be reconstructed, which implies AGAIN lacks generalizability. We leave these for future work."}, {"title": "A ALGORITHMS", "content": "The overall algorithm for the interactive intervention switch is summarized in Algorithm 1."}, {"title": "B THEORETICAL ANALYSIS OF AGAIN", "content": "In this section, we present theoretical proofs to ensure that the G in AGAIN contributes to generating comprehensible explanations under unknown perturbations. Previous theoretical analyses on the validity of interpretable models have emphasized that comprehensible explanations tend to have a high similarity to explanatory labels (Li et al., 2020; Karpatne et al., 2017; Von Rueden et al., 2021). Therefore, we hope to guarantee that AGAIN can generate comprehensible explanations by proving that G contributes to explanations under unknown perturbations in approximation to the explanatory labels. Specifically, our theoretical proof is divided into two parts. In the first part, we establish correlations between the conditional probabilities of the factors and the lower bounds on the predictive accuracy of concepts. In the second part, we show that this lower bound increases according to the larger size of G."}, {"title": "B.1 THE CONCEPT PREDICTIVE ACCURACY LOWER BOUND FOR AGAIN", "content": "Under the adversarial distribution, given an perturbation $ \\delta $ and the concept explanation label c, the accuracy of the explanation is denoted as $A_h$:\n$A_h := \\sum_{CEC} P_\\delta (\\hat{c} = c).   $                                                                                                                              (6)\nTo simplify the writing, we use $h_G(\\hat{c}, \\hat{y})$ to denote the process of identifying (Section 4.2) and rectifying (Section 4.3) in AGAIN. We extend this definition to assess the accuracy of explanations generated by AGAIN:\n$A^{AGAIN} := \\sum_{CEC} P_\\delta (h_G(\\hat{c}, \\hat{y}) = c), $                                                                                                 (7)\nwhere $h_G(\\cdot)$ denotes the output of the factor graph reasoning with respect to the m-th concept.\nLemma 1. Given a factor graph G, the following equation is valid:\n$A^{AGAIN} = \\prod_{CEC} P_\\delta (\\Delta_N (v_{\\hat{c}}) > 0 |c),          $                                                                                                       (8)\nwhere\n$ \\Delta_N (v_{\\hat{c}}) = \\sum_{i\\in N(v_{\\hat{c}})| } (2w_i\\Psi_i - w_i).    $                                                                                                                              (9)"}, {"title": "B.2 LOWER BOUND OF ACCURACY VERSUS NUMBER OF FACTORS IN THE FACTOR GRAPH", "content": "After analyzing the lower bound of concept accuracy, we aim to demonstrate a positive correlation between this lower bound and the number of factors in the factor graph. In simpler terms, an increase in the number of factors is directly proportional to an enhancement in predictive accuracy. This implies that G contribute to the predictive performance of the model. To facilitate the analysis, we introduce a lemma below (G\u00fcrel et al., 2021). This lemma illustrates an unequal relationship of the accuracy lower bound under specific factor graph characterizations.\nLemma 4. Suppose that the upper and lower bounds of each factor graph characteristic are identical, then there exists\n$\\prod_\\{ CEC } (1 \u2013 E_{i \\in N } exp(  -\\frac{2(L_{\\Delta N(v_{\\hat{c}}), c})^2}{\\sum_{i \\in N} (log (\\frac{U^T (1-L^F)}{L^F (1-U^T)} ) }^2 )))  \\geq  \\prod_\\{ CEC } (1 \u2013 exp(-2N (\\Theta^T \u2013 \\Theta^F))), $                  (18)\nwhere\n$ \\Theta^T := U^T = L^T    \\wedge  \\Theta^F := U^F = L^F.  $                                                                                                         (19)\nFurthermore, we present a theorem that highlights a significant correlation between the lower bound of concept accuracy and the number of factors in G."}, {"title": "B.3 DISCUSSION", "content": "Our theoretical proof validates that G has the potential to enhance prediction accuracy, provided that the rules embedded in the factor graphs restrict concept activation in alignment with prior logic, satisfying the qualifying sufficient condition outlined in Theorem 2 (i.e., $ \\Theta^T > \\Theta^F $). Intuitively, as the size of N increases, the lower bound on $A^{AGAIN}$ grows exponentially, leading to a substantial improvement in $A^{AGAIN}$. We observe that Theorem 2 holds depending on the condition set by Lemma 4 (i.e., $U^T = L^T $ and $U^F = L^F $). Indeed, $U^T = L^T $ and $U^F = L^F $ are inherently met when the rules governing G adhere to prior logic. More specifically, if a factor satisfies $ \\Theta^T > \\Theta^F $ and its neighboring variables adopt the same value as ground-truth labels, then its potential function value must be e. In this case, the conditional probability of the potential function is fixed."}, {"title": "C EXPERIMENTAL DETAILS", "content": "C.1 DATASETS AND DATA PREPROCESSING\nAll data processing and experiments are executed on a server with two Xeon-E5 processors, two RTX4000 GPUs and 64G memory. We construct logical rule sets on the three datasets respectively. The detailed information of data processing on both datasets is summarized as follows.\nSynthetic-MNIST. The Synthetic-MNIST dataset is a composite dataset derived from the original MNIST dataset. Each category of the MNIST handwritten digits is treated as a concept, and four digits from different categories are concatenated to form a Synthetic-MNIST sample. Consequently, each Synthetic-MNIST sample contains 4 concept labels and a synthetic category label. The Synthetic-MNIST comprised 79,261 samples from 12 synthetic categories. The mapping of each synthetic category to concepts is shown in Table 9. According to Table 9, we construct the first-order logical rule set for Synthetic-MNIST. The samples, whose category label is 0, are taken as examples. We construct category-concept rules: $c_0 \\Rightarrow y_0$, $c_2 \\Rightarrow y_0$, $c_4 \\Rightarrow y_0$, $c_6 \\Rightarrow y_0$; concept-concept rules: $c_0 \\Leftrightarrow c_1$, $c_2 \\Leftrightarrow c_3$, $c_4 \\Leftrightarrow c_5$, $c_4 \\Leftrightarrow c_6$, $c_7 \\Leftrightarrow c_8$, $c_9 \\Leftrightarrow c_5$. Note that the logical rules used on the"}, {"title": "IMPLEMENTATION DETAILS OF ADVERSARIAL ATTACKS", "content": "In this section, we provide implementation details of adversarial attacks against concept-level explanations. Specifically, adversarial attacks against concept-level explanations can be categorized into three types: erasure attacks, introduction attacks, and confounding attacks."}]}