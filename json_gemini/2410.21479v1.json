{"title": "TRANSFORMLLM: ADAPTING LARGE LANGUAGE MODELS VIA\nLLM-TRANSFORMED READING COMPREHENSION TEXT", "authors": ["Iftach Arbel", "Yehonathan Refael", "Ofir Lindenbaum"], "abstract": "Large Language Models (LLMs) have shown promise in highly-specialized domains, however chal-\nlenges are still present in aspects of accuracy and costs. These limitations restrict the usage of ex-\nisting models in domain-specific tasks. While fine-tuning pre-trained models have shown promising\nresults, this process can be computationally expensive and require massive datasets of the special-\nized application in hand. In this work, we bridge that gap. We have developed Phi-2-Legal and\nMistral-Legal-7B, which are language models specifically designed for legal applications. These\nmodels are based on Phi-2 and Mistral-7B-v0.1, and have gone through continued pre-training with\nover 500 million tokens of legal texts. Our innovative approach significantly improves capabilities\nin legal tasks by using Large Language Models (LLMs) to convert raw training data into reading\ncomprehension text. Our legal LLMs have demonstrated superior performance in legal benchmarks,\neven outperforming models trained on much larger datasets with more resources. This work em-\nphasizes the effectiveness of continued pre-training on domain-specific texts, while using affordable\nLLMs for data conversion, which gives these models domain expertise while retaining general lan-\nguage understanding capabilities. While this work uses the legal domain as a test case, our method\ncan be scaled and applied to any pre-training dataset, resulting in significant improvements across\ndifferent tasks. These findings underscore the potential of domain-adaptive pre-training and reading\ncomprehension for the development of highly effective domain-specific language models.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLM) domain-adaptive pre-training, also known as continued pre-training on domain-\nspecific corpora [12], is a technique that has been proven effective in adapting large language models (LLMs) to\nspecific domains [35, 5]. This approach allows LLMs to leverage their general language understanding capabilities\nwhile incorporating domain-specific knowledge, which can benefit downstream domain-specific tasks at reduced costs\n[22, 26, 27].\nIn this process, the LLM is further pre-trained using raw data from the specific domain, such as biomedicine, finance,\nor law. This helps the LLM gain domain knowledge, which is demonstrated by its improved performance in fine-tuning\nand knowledge probing evaluations within those domains [20, 1, 2]. However, a notable drawback is that continued\npre-training on raw domain corpora can lead to a significant drop in the LLM's prompting performance, potentially due\nto the specialized nature of the domain-specific data [11]. Despite this trade-off, domain-adaptive pre-training remains\na promising approach for adapting LLMs to specific domains, capitalizing on their general language understanding\ncapabilities while tailoring them to domain-specific tasks and knowledge. Ongoing research efforts aim to mitigate the\npotential negative impacts on prompting performance while maximizing the benefits of domain-specific knowledge\nacquisition [10, 28].\nThe notion of reading comprehension was suggested in [6], where instead of continuing to train a large language model\non domain-specific raw data, the raw texts be converted into reading comprehension materials. In this approach, each\ntext is followed by related tasks, transitioning the model from a \"reading\" phase to a \"comprehension\" phase. These"}, {"title": "2 Using LLMs to Transform Raw Text", "content": "Building upon the foundation of AdaptLLM [6], which converts raw legal text into reading comprehension tasks, we\ndraw from the concept of human learning through reading comprehension. This approach, where practice after reading\nimproves the ability to answer questions based on acquired knowledge, inspired our work. Rather than continuing\nto train large language models on raw domain-specific corpora, AdaptLLM proposes converting the raw text into\nstructured reading comprehension tasks, with each passage followed by questions.\nWhile AdaptLLM leverages a set of rules and heuristics to perform this transformation, its reliance on such methods\nposes limitations, especially in the resulting data qualityquality of the resulting data. These challenges highlight a\ncritical need for more sophisticated text transformation techniques [21, 29]. Our solution addresses this by leveraging\nlarge language models (LLMs) to generate high-quality training data. With the decreasing costs of LLM inference,\nwe can move beyond structured heuristics, using LLMs to efficiently create comprehensive reading comprehension\ndatasetscreate comprehensive reading comprehension datasets efficiently.\nTo improve text quality, we designed a prompt database that guides the model's capabilities. LLMs were tasked with\ngenerating answers and additional questions, additional questions, and transforming the raw legal texts based on tai-\nlored prompts. Through further refinement and post-processing, we developed a superior legal reading comprehension\ndataset, offering enhanced performance for domain adaptation.\nWe primarily used open-source models ranging from 7B to 70B for data transformation. These models were selected\nbased on factors like cost and operational efficiency. Upon reviewing the outputs of these open-source models com-\nparedin comparison to more advanced proprietary models like those from OpenAI and proprietary Mistral models, we\nobserved no significant differences in quality for our transformation task. However, to ensure a diverse data distribu-\ntion and to benefit from knowledge distillation of the most powerful models, we also transformed a portion of the data\nusing state-of-the-art proprietary (closed-source) models.\nSome transformations were also applied on the general, non-legal data to generate Chain-of-Thought (CoT) data and\nimprove the reasoning capabilities of the model, which we find crucial in the legal domain. For the same reason, we\nincorporated math and coding data in the training set, striving to boost logical and inference capabilities."}, {"title": "3 Data Collection and Processing", "content": "Our data collection focused primarily on English-language legal texts, drawing heavily from the United States, which\nfollows the common law system. We also included materials from Canada and the United Kingdom, both of which also\nadhere to common law principles. This emphasis on jurisdictions with common law traditions ensures that our dataset\naligns closely with the legal usage specific to the United States, which is the primary focus of our model. Through\nmeticulous curation and rigorous cleaning procedures, we compiled a comprehensive corpus tailored to capture the\nintricacies of legal language within the United-States Federal-jurisdiction.\nThroughout the development of the model and the data collection process, our goal was not to expose the model\nto all existing legal data. Instead, we focused on providing the model with a strong foundation of legal knowledge,\nbackground, understanding, and reasoning abilities. Our aim is for the model to be able to handle various legal tasks,\nincluding document drafting, reviews, and answering questions, by equipping it with these tools. However, if you ask\nthe model about specific data such as cases or laws, it may provide inaccurate information. In such cases, Retrieval-"}, {"title": "4 Model Architecture and Training", "content": "We have trained two versions of the legal model: Phi-2-Legal and Mistral-Legal-7B. As suggested by their names, these\nmodels are based on Phi-2 [16] and Mistral-7B [17]. We selected these models because they demonstrate cutting-edge\nperformance, are available for commercial use, and are well-supported by inference libraries (vLLM [19], etc.) and\nfor server-less deployment (Fireworks, Together, etc.)."}, {"title": "4.1 Training considerations", "content": "To save on resources and consider the very limited availability of GPUs, we opt to train the models using LoRA [15],\navoiding full parameter update. Lora is a parameter-efficient fine-tuning (PEFT) technique that has beenParameter-\nEfficient Fine-Tuning (PEFT) technique, proven to match the results of full-parameter updates while requiring signif-\nicantly fewer training resources (Note that any state-of-the-art variants of LoRA [25, 4, 32, 34] may be used as an"}, {"title": "5 Evaluation", "content": "We evaluate the models on MMLU (legal subsets) and LexGLUE datasets. We aim for a simple and accessible\nevaluation scheme that is easy to understand and measures model accuracy. MMLU is typically evaluated using the\nLog probabilities of tokens, as in [9]. However, this type of model evaluation has two main drawbacks: (1) Attaining\nraw requires setting up servers with GPUs and server-less inference providers,, and server-less inference providers\nas these are limited in the number of Log probabilities they output. (2) Measuring against Log probabilities may\nencounter issues due to tokenization mismatches. LexGLUE typicallynormally evaluates classification models rather\nthan generative ones. Therefore, we adapt benchmark prompts for instruct-type models, detailing the various options\nand asking for the most suitable option to be selected. This means that models may be evaluated quickly and affordably\nusing inference frameworks such as vLLM, or server-less inference providers. We also utilize recent advancements in\ndecoding techniques, allowingadvancements in decoding techniques, which us to define a closed listallows to define\na closed-list of possible options. The result is a transparent and simple evaluation scheme suitable for, suitable to be\nused with chat-aligned models.\nMMLU is a straightforward multiple-question benchmark. LexGLUE, on the other hand, has subsets that are simple\nmultiple-question, while others have 8-100 label options. In LexGLUE, we only use the subsets that are suitable for\nuse with generative models. For that, the EUR-LEX subset was not used as it only has numerical labels, not verbal,\nmeaningful ones, while the SCOTUS subset was avoided as many of its instances are longer than a 4K token window;\ntherefore, it, therefore is has very few usable data instances. Lastly, we did not use the ECtHR subsets, as they refer to\nproceedings brought to the European Court of Human Rights (ECtHR), and therefore rely on the European Convention\non Human Rights,relies on European Convention on Human Rights which is a codified document more typical of civil\nlaw systems [30].\nOur legal models were benchmarked compared to their underlying base models, Phi-2 and Mistral-7B, to measure the\nimprovement achieved by continued pre-training. The Mistral-7B is also compared to the legal variant of AdaptLLM\nmode, which also uses continued pre-training using reading comprehension text. Additionally, we compare it to Saul-\n7B [7], another recent legal model that uses at least x30 more training data and full-parameter update (compared to\nour LoRA training). We are unawarenot aware of legal models smaller than 7B parameters; therefore,, therefore the\nPhi-2 models are the only ones in this category. These benchmark results are presented in Table 2.\nBoth classes of models show considerable improvement over their base models. Mistral-Legal-7B performs better\nin all subsets thancompared to AdaptLLM, highlightingwhich highlights the benefit of transforming raw data using\nLLMs, compared to the heuristic and regex rules. It also performs better than Saul-7B in five out of six subsets. We\nobserved the most significant performance gains in the LexGLUE subsets. We suspect this is because LexGLUE\nis a more niche benchmark, receiving less attention from model developers. In contrast, the MMLU benchmark is\nhighly popular, and the original models were already extensively optimized for it, making further improvements more\nchallenging. Nevertheless, our method still enhancedmanaged to enhance results, with Phi-2-Legal outperforming the\noriginal Mistral-7B in all but one of the benchmark subsets."}, {"title": "6 Conclusion", "content": "In this work, we presented a framework for domain-specific adaptation of LLMs using continued pre-training. By\ntraining models in the legal domain, we have shown that it is possible to obtain high-performing models with relatively\nlow resources. To the best of our knowledge, this is the first time this technique has been used.\nFuture research could employ Reinforcement Learning from Human Feedback (RLHF) to enhance the model's align-\nment with human preferences. This would lead to improved generation capabilities and more refined outputs, advanc-\ning the applicability and efficacy of the model in diverse applications."}, {"title": "Limitations", "content": "The models were evaluated using multiple-question benchmarks, which serve as proxies for their legal capabilities.\nHowever, a dedicated framework for evaluating its text generation capabilities, particularly in specific applications\nsuch as contracts and reviews, is necessary to obtain a comprehensive assessment. The models are not intended or\nable to provide factual information, they may generate information that is false or misleading, and reflect social and\ncultural biases from their training data, both the original pre-training data as well as our continued pre-training data."}, {"title": "A Ethics statements", "content": "As with all large language models, there is an inherent risk of the model producing biased or toxic responses, which\nremains a significant concern and requires ongoing attention. Given its application in the legal domain, the language\nmodel's biases could have more severe implications than those in general usage scenarios. Moreover, mistakes and\nhallucinations produced by the model can result in costly errors to the end user."}, {"title": "B Training Samples Example", "content": "In Table 3, we provide examples of our text transformations, including prompts and responses by the LLM."}]}