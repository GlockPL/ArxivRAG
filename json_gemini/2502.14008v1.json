{"title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures", "authors": ["Jiayu Qin", "Jianchao Tan", "Kefeng Zhang", "Xunliang Cai", "Wei Wang"], "abstract": "The remarkable performance of large language models (LLMs) in various language tasks has attracted considerable attention. However, the ever-increasing size of these models presents growing challenges for deployment and inference. Structured pruning, an effective model compression technique, is gaining increasing attention due to its ability to enhance inference efficiency. Nevertheless, most previous optimization-based structured pruning methods sacrifice the uniform structure across layers for greater flexibility to maintain performance. The heterogeneous structure hinders the effective utilization of off-the-shelf inference acceleration techniques and impedes efficient configuration for continued training. To address this issue, we propose a novel masking learning paradigm based on minimax optimization to obtain the uniform pruned structure by optimizing the masks under sparsity regularization. Extensive experimental results demonstrate that our method can maintain high performance while ensuring the uniformity of the pruned model structure, thereby outperforming existing SOTA methods.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), such as OpenAI's GPT series (Achiam et al., 2023) and Meta's LLAMA (Touvron et al., 2023a,b), have made substantial advancements in the domain of Natural Language Processing (NLP). These models exhibit robust capabilities in language understanding and generation, facilitated by extensive pre-training and fine-tuning. However, as the size of these models continues to expand, their computational and storage demands increase sharply, presenting significant challenges for practical applications. Model compression, a vital approach to reducing memory footprint and computational load during model deployment, offers unique benefits across various domains. Techniques such as pruning (Frantar and Alistarh, 2023; Ma et al., 2023; Sun et al., 2023), quantization (Frantar et al., 2023; Xiao et al., 2023; Lin et al., 2024), knowledge distillation (Gu et al., 2024; Agarwal et al., 2023), and low-rank factorization (Yuan et al., 2023; Wang et al., 2024) can significantly decrease the number of model parameters and computational complexity, thereby enabling large-scale language models to function efficiently in resource-constrained environments.\nThe pruning technique reduces the size and computational complexity of the models by eliminating redundant parameters, which can generally be categorized into unstructured pruning (Frantar and Alistarh, 2023; Sun et al., 2023; Dong et al., 2024), semi-structured pruning (Mishra et al., 2021), and structured pruning (Ma et al., 2023; Xia et al., 2023; An et al., 2023). Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning. Currently, most pruning techniques employ metric-based methods, which determine the modules to be pruned by introducing specific pruning metrics. These metrics are usually designed heuristically and often perform poorly at high pruning rates. Moreover, a single metric cannot fully capture the importance of model weights, making it difficult to identify superior local optimal solutions. In contrast, optimization-based pruning methods determine which weights to prune by learning a pruning mask, thereby avoiding the performance degradation associated with manually designed metrics. This paper primarily focuses on optimization-based pruning methods.\nGiven the large scale of Large Language Models (LLMs), existing optimization-based pruning methods employ structured pruning, wherein a single mask prunes entire modules of the model. Methods such as CoFi (Xia et al., 2022), Compresso (Guo et al., 2023) and NutePrune (Li et al., 2024) follow the \\(L_0\\) regularization (Louizos et al., 2018) training paradigm during the training of pruning masks, learning masks by setting a total sparsity without additional constraints. This approach results in a lack of uniformity between layers during training, causing each layer to have a different number of attention heads and FFN intermediate dimensions, as illustrated in Figure 1, which leads to suboptimal inference speed. Moreover, this irregular structure necessitates adaptations during model deployment. Additionally, to achieve higher performance post-compression, existing model compression techniques typically involve continued training and fine-tuning after compression. However, the irregular structure hinders models from fully utilizing existing model parallelism techniques, resulting in diminished performance for the same continued training cost(Xia et al., 2023).\nTo address these issues, this paper proposes a method called MaskPrune for jointly training pruning masks and target structures across various dimensions. This approach optimizes the target dimension parameters simultaneously during training to maintain uniformity of dimensions across the layers of the pruned model while achieving the preset model sparsity. The key idea is to frame the sparsity constraint of model pruning as a minimax problem. Since the introduced sparsity loss is non-differentiable, it cannot be directly optimized using gradient descent. By employing proximal operators and straight-through estimators to optimize masks and target dimensions respectively, the pruning optimization problem is effectively solved. The contributions of this paper can be summarized as follows:\n\u2022 We propose a mask training method based on minimax optimization, enabling end-to-end optimization of mask values during the pruning and automatically maintaining the layer-wise uniform structure throughout training.\n\u2022 Mask parameters are optimized by proximal operators, maintaining the original model's capabilities to the greatest extent while adhering to target sparsity constraints and minimizing performance degradation during pruning.\n\u2022 Extensive experiments were conducted across various sparsity levels on models from the LLaMA family, demonstrating the effectiveness of our method by maintaining high performance on diverse tasks while preserving the model's uniform structure."}, {"title": "Related Work", "content": "Importance metric-based Methods SparseGPT (Frantar and Alistarh, 2023) evaluates the importance of weights using second-order Hessian information and compensates for other weights during the pruning process, thereby achieving unstructured pruning. Wanda (Sun et al., 2023) simplifies this approach by relying solely on the magnitude of the weights and the activation values on a calibration set to determine the importance of the weight, accelerating the pruning process. Additionally, its methods can be extended to semi-structured pruning. Pruner-Zero (Dong et al., 2024) employs genetic programming to efficiently search for optimal symbolic pruning metrics, avoiding heuristic weight importance searches. LLM-Pruner (Ma et al., 2023) was the first to utilize structured pruning methods to compress large language models (LLMs), assessing the importance of weight groups through approximate first-order Hessian information. Bonsai (Dery et al., 2024) samples the correlation between sub-modules and model performance, using linear regression to determine the importance of weight groups. LoRAPrune (Zhang et al., 2023) estimates the original gradients of weights through the gradients of LoRA matrices, thereby reducing memory consumption during backpropagation.\nOptimization-based Methods However, metric-based methods like LLM-Pruner (Ma et al., 2023) often fail to fully capture the importance of weights, leading to suboptimal generalization performance. To address this, many optimization-based methods have focused on learning masks for pruned weights. \\(L_0\\) regularization (Louizos et al., 2018) offers a general paradigm for mask learning, enabling the optimization of non-differentiable masks. CoFi (Xia et al., 2022) integrates a hierarchical distillation loss into the training loss function, while SheardLlama (Xia et al., 2023) specifies target structures to achieve a unified model architecture and employs dynamic batch loading to enhance generalization performance. Compresso (Guo et al., 2023) introduces specific prompts during training and incorporates LoRA modules into the optimization process, combining fine-tuning with mask training. NutePrune (Li et al., 2024) leverages progressive distillation to enhance the transfer of knowledge from teacher to student models."}, {"title": "Methodology", "content": "In this chapter, we explain how MaskPrune employs an optimization-based approach to generate structured pruning masks while maintaining consistency across inter-layer structures throughout the process. Specifically, Section 3.1 defines the optimization problem and Section 3.2 introduces the methods to solve this problem. Figure 2 illustrates the overall framework of our proposed method."}, {"title": "Problem Definition", "content": "For the transformer models such as Llama families, the Multi-Head Attention (MHA) layer and the Feed-Forward Network (FFN) comprise the primary components. During the pruning, the main structural elements, specifically, the attention heads and the intermediate dimensions of the FFN, are typically processed. To facilitate pruning, the corresponding masks, \\(m_{head}\\) and \\(m_{inter}\\) are introduced at their respective positions within the model. Taking the Llama architecture as an example:\n\\(MHA'(X) = \\sum_{j=1}^{N_h} m_{head}^{(l, j)} \\cdot Attn^{(l, j)}(X)\\)\nwhere l denotes the l-th layer and \\(N_h\\) denotes the number of attention heads in the Multi-Head Attention layer.\n\\(FFN'(X) = m_{inter} \\cdot (W_{gate}^{(l)}(X) \\cdot W_{up}^{(l)}(X)) \\cdot W_{down}^{(l)}(X)\\)\nHere, \\(m_{head}\\) and \\(m_{inter}\\) are restricted within the range [0, 1]. When a mask value is 0, the corresponding module is pruned. The actual sparsity of the model is calculated as follows:\n\\(\\frac{1}{M} + \\frac{1}{M} \\cdot \\sum_{l=1}^{L} \\sum_{j=1}^{N_h} 4 \\cdot d_h \\cdot d_{hidden} \\cdot \\mathbb{I}(m_{head}^{(l,j)} = 0) + \\frac{1}{M} \\cdot \\sum_{l=1}^{L} \\sum_{j=1}^{df} 3 \\cdot d_{hidden} \\cdot \\mathbb{I}(m_{inter}^{(l,j)} = 0)\\)\nHere, M denotes the original size of the model, L is the total number of layers in the model, \\(d_{hidden}\\) represents the hidden dimension, and \\(d_f\\) and \\(d_h\\) signify the head dimension and the intermediate dimension within the FFN, respectively. The indicator functions are used to identify the pruned components of the model, these components constitute the pruned model. The objective of our method is to implement structured pruning using a regularized approach. Therefore, the optimization target encompasses not only the pruning mask m but also the sparsity parameters s = {\\(s_{head}\\), \\(s_{inter}\\)} across various dimensions, where \\(s_{head}\\) and \\(s_{inter}\\) represent the sparsity ratios of the MHA layers and FFN layers, respectively. For clarity, s below denotes"}, {"title": "Parameter Update Strategy", "content": "We iteratively solve the aforementioned optimization problem. Following the approach of (Tono et al., 2017; Chen et al., 2023; Yu et al., 2022), we first update the mask to optimize the training loss. Subsequently, we update s, which balances the sparsity loss and resource loss, ultimately achieving a convergence state. Finally, we update the variables y and z to increase the penalties associated with the sparsity and resource losses, thereby promoting the convergence of s. The specific update strategy is as follows:\nIn this optimization step, we fix the model parameters, the sparsity variable s, and the Lagrange multipliers y and z while updating m. Following the methodology of (Yang et al., 2019), we minimize the loss proxy of m in the original loss function L(m) at \\(m^t\\):\n\\(L(m^t) + (\\nabla L(m^t), m \u2013 m^t) + \\frac{1}{2\\eta_1} ||m - m^t||^2\\)\nwhere \\(\\eta_1\\) is the learning rate for m. Therefore, the original problem can be simplified to the following proximal optimization update:\n\\(arg \\min_m \\frac{1}{2} ||m - m^t||^2 + \\eta_1\\gamma ||m||_{s_t, 2}\\)\nwhere \\(m = m^t - \\eta_1 \\nabla L(m^t)\\). We set \\(S(\\gamma^t, s^t, m) = \\gamma^t ||m||_{s^t, 2}\\) and the solution to the proximal operator \\(Prox_{\\eta_1 S(\\gamma^t, s^t, m)}(m)\\) can be defined as follows:\n\\(m_i^* = \\begin{cases} \\frac{m_i}{1 + 2\\eta_1 \\gamma} & \\text{if } m_i \\geq m_{\\text{least}-[s]} \\\\ 1 \\cdot m_i & \\text{otherwise} \\end{cases}\\)\nHere, \\(m_i\\) represents the i-th element of the mask and least-j denotes the index of the element in m with the j-th smallest norm.\nUnlike previous optimization-based methods, we do not use reparameterization to force m to polarize to 0 and 1. Instead, we adopt a uniformly distributed normal mask, allowing m to update freely within the range of 0 to 1. This approach continually decays m during the proximal optimization update to achieve pruning. Meanwhile, \\(\\eta_1\\) as the decay rate, can be freely adjusted as a hyperparameter and does not need to match the learning rate of the mask during actual optimization. In this process, the mask itself acts as a scaling factor for the"}, {"title": "Optimization with LoRA and Distillation", "content": "LORA (Hu et al., 2021) has been widely demonstrated to be efficient in fine-tuning LLMs. To effectively update weights during the optimization of the mask and achieve enhanced performance, similar to Compresso (Guo et al., 2023) and NutePrune (Li et al., 2024), we introduce the LoRA module during optimization:\n\\(W' = W + \\Delta W = W + B A\\)\nwhere \\(B \\in \\mathbb{R}^{d \\times r}\\), \\(A \\in \\mathbb{R}^{r \\times k}\\) and \\(r << \\min(d, k)\\). During the training process, the model's original weights W are frozen, and only the parameters in the low-rank matrices A and B are trained.\nRegarding loss functions, we introduce a distillation loss similar to those in CoFi (Xia et al., 2022) and NutePrune (Li et al., 2024). Specifically, \\(L_{KL}\\) denotes the Kullback-Leibler (KL) divergence between the probability distributions \\(p_t\\) and \\(p_s\\) of the output from the teacher model before pruning and the student model after pruning, respectively. Additionally, \\(L_{layer}\\) represents the sum of mean squared errors (MSE) of the hidden representations \\(h_s^l\\) and \\(h_t^l\\) cross the intermediate layers of the teacher and student models:\n\\(L_{KL} = D_{KL} (p_s || p_t)\\)\n\\(L_{layer} = \\sum_{l=1}^{L} MSE(h_s^l, h_t^l)\\)\nThe coefficient of the two losses is controlled by the hyperparameter a, and the final loss is formulated as:\n\\(L_{distill} = L_{KL} + a * L_{layer}\\)"}, {"title": "Experiments", "content": "To validate the effectiveness and generalizability of our method, we conducted experiments on several models, including the Llama-1 (Touvron et al., 2023a) and Llama-2 (Touvron et al., 2023b) families, encompassing 7B and 13B configurations.\nWe sampled 20,000 data instances, each consisting of 512 tokens, from the C4 dataset (Raffel et al., 2020) to serve as training data for mask optimization using the AdamW optimizer. The learning rate was set to le-2 for the mask parameters and le-3 for the LoRA parameters, with a batch size of 16. The pruning process"}, {"title": "Main Results", "content": "Zero-Shot Tasks For the Llama-7B baseline model, we applied pruning to generate models with three sparsity levels: 50%, 25%, and 20%. As shown in Table 1, our method outperforms Compresso (Guo et al., 2023), which employs uneven inter-layer sparsity, in terms of both perplexity and common sense reasoning tasks, demonstrating superior performance under challenging conditions. Compared to the layer-uniform version of NutePrune (Li et al., 2024), our method maintains higher model capabilities at each sparsity level, showing improvements of 0.58%, 0.38%, and 0.51%, respectively. These results indicate that our method can consistently identify appropriate modules for pruning during optimization and effectively scale the remaining modules to preserve model performance.\nOur method also demonstrates superior capabilities for larger models, such as Llama-13B and Llama-2-13B. Specifically, for Llama-13B and Llama-2-13B at a 20% sparsity level, our method maintains 94% and 95% of the original model's zero-shot task capabilities, with no significant decrease in perplexity.\nAs illustrated in Figure 3, our method continuously regularizes the sparsity across the model's layers to maintain uniformity throughout the training process, ultimately achieving a completely uniform structure. In contrast, Compresso and NutePrune permit unrestricted learning"}, {"title": "Analysis", "content": "We explored the impact of different mask types on our method by selecting three representative mask distributions: (1) a standard uniform distribution mask, which can be freely optimized within the range of [0,1]; (2) a standard \\(L_0\\) regularization mask (Louizos et al., 2018), which drives the mask values to 0 and 1 through reparameterization; and (3) a differentiable polarizing"}, {"title": "Conclusion", "content": "This paper introduces a structured pruning method for Large Language Models (LLMs) by formulating model pruning as a minimax optimization problem. During optimization, the pruning mask and the model's target dimensions are trained simultaneously, resulting in a pruned model with uniform inter-layer structures. We evaluated models with varying sparsity levels and sizes across multiple benchmarks, and the results consistently demonstrate the superiority of our method. Our approach also investigates different strategies for performing optimization-based structured pruning on LLMs, providing new insights into the compression of these models."}, {"title": "Limitations", "content": "The efficacy and effectiveness of mask learning-based pruning is highly dependent on the quality of the training or calibration data. Determining whether the chosen data is optimal and developing various strategies for selecting superior datasets remain significant challenges that warrant further investigation."}, {"title": "Mask Distribution", "content": "To investigate the effects of mask values across different layers, we analyzed spatial patterns through heatmap visualizations. Figure 7 reveals distinct layer-wise patterns in the multi-head attention pruning masks. The first half of the model's layers show mask values predominantly clustered around 0.8, suggesting these layers employ parameter scaling to compensate for pruning-induced performance degradation. Conversely, the latter layers maintain values near 1, indicating lower pruning sensitivity where direct pruning minimally impacts model performance. Similar layer-specific patterns emerge in the FFN layers, as shown in Figure 8, where intermediate masks demonstrate varying distribution trends across layers, with some maintaining unpruned masks significantly below 1 while others approach unity."}, {"title": "Algorithm", "content": "The pseudocode of the algorithm used in this paper is shown in Algorithm 1.\nAlgorithm 1: Gradient-based algorithm.\nInput: Pruned Size Mprune, learning rates\n\\(\\eta_1\\), \\(\\eta_2\\), \\(\\eta_3\\), \\(\\eta_4\\), number of total iterations \\(\\tau\\).\nResult: mask m*\n1 Initialize t = 1;\n2 Initialize \\(m^1\\) = 1;\n3 while t < \\(\\tau\\) do\n4\n\\(m^{t+1} = Prox_{\\eta_1 S(\\gamma^t, s^t, m)}(m^t -\\eta_1\\nabla_m L(m^t));\\)\n5\n\\(s^{t+1} = s^t -\\eta_2 (\\nabla_s S(\\gamma, s^t, m^{t+1}) + \\nabla_s(z*(M(s^t) \u2013 M_{prune})));\\)\n6\n\\(\\gamma^{t+1} = \\gamma^t + \\eta_3||m^{t+1}||_{s^{t+1},2};\\)\n7\n\\(z^{t+1} = max(0, z^t + \\eta_4(M(s^{t+1}) \u2013 M_{prune}));\\)\n8 end\n9 m* = \\(m^\\tau\\)."}]}