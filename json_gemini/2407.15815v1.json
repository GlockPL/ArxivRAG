{"title": "Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning", "authors": ["Zhecheng Yuan", "Tianming Wei", "Shuiqi Cheng", "Gu Zhang", "Yuanpei Chen", "Huazhe Xu"], "abstract": "Can we endow visuomotor robots with generalization capabilities to operate in diverse open-world scenarios? In this paper, we propose Maniwhere, a generalizable framework tailored for visual reinforcement learning, enabling the trained robot policies to generalize across a combination of multiple visual disturbance types. Specifically, we introduce a multi-view representation learning approach fused with Spatial Transformer Network (STN) module to capture shared semantic information and correspondences among different viewpoints. In addition, we employ a curriculum-based randomization and augmentation approach to stabilize the RL training process and strengthen the visual generalization ability. To exhibit the effectiveness of Maniwhere, we meticulously design 8 tasks encompassing articulate objects, bi-manual, and dexterous hand manipulation tasks, demonstrating Maniwhere's strong visual generalization and sim2real transfer abilities across 3 hardware platforms. Our experiments show that Maniwhere significantly outperforms existing state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Visuomotor control tasks present roboticists with a vexing issue the hardware setup can severely influence the performance of the robot policies. A prime example arises from the issue of immovable cameras - envision a carefully calibrated visual sensor, painstakingly positioned to enable seamless real-world deployment, only to have it disturbed by a lab mate. This single, seemingly innocuous incident can grind progress to a halt, forcing tedious recalibration or the collection of new demonstration data. Furthermore, changes in the background or the presence of extraneous objects within the captured views may undermine the effectiveness of a trained policy. Such obstacles have long plagued the field of robotics, representing critical barriers to realizing the full potential of advanced visuomotor systems.\nAcknowledging these obstacles, when attempting to achieve sim2real visual policy transfer, it is common to instantiate a digital twin that closely resembles the actual real-world environment [1, 2, 3, 4, 5, 6, 7, 8]. Otherwise, the significant discrepancy between the digital twin and the real setting would render the trained models wholly ineffective. Hence, robots that adeptly handle in-the-wild scenarios should possess generalizability against various visual changes such as camera views, visual appearances, lighting conditions, etc.\nWhile prior works have sought to tackle the challenges against visual scene variations [9, 10, 11, 12, 13, 14, 15], these studies primarily focus on resolving a single aspect and are unable to handle multiple visual generalization types simultaneously. Meanwhile, it is non-trivial to incorporate various inductive biases into the training process. Naively applying domain randomization or data augmentation methods can destabilize the entire RL training, ultimately leading to divergence for the learned policy [4, 9, 12, 16]. More importantly, the generalization abilities of these methods have yet to be thoroughly evaluated on real robots.\nIn this paper, we are dedicated to enabling robots to acquire strong visual generalization ability so that they can step out of simulations and apply their learned skills to complex real-world scenarios without camera calibration. We introduce Maniwhere: A Visual Generalizable Framework for Reinforcement Learning. As shown in Figure 1, Maniwhere employs a multi-view representation objective to capture implicitly shared semantic information and correspondences across different viewpoints. In addition, we fuse the STN module [17] within the visual encoder to further enhance the robot's robustness to view changes. Subsequently, to achieve sim2real transfer, we utilize a curriculum-based domain randomization approach to stabilize RL training and prevent divergence. The resulting trained policy can be transferred to real-world environments in a zero-shot manner.\nTo conduct the evaluation, we develop 3 types of robotic arms and 2 types of robotic hands to design a total of 8 diverse tasks, alongside 3 corresponding hardware setups to validate the efficacy of our algorithm. Our comprehensive experiments demonstrate that, in both simulation and real-world scenarios, Maniwhere significantly outperforms existing state-of-the-art baselines by a large margin."}, {"title": "2 Method", "content": "In this section, we present Maniwhere, a generalizable framework for visual reinforcement learning. We propose a multi-view representation learning objective aimed at empowering the training agent with the ability to extract invariant features and generalize across different viewpoints. To further augment the model's spatial awareness, we incorporate an STN module into the visual encoder by actively spatially transforming feature maps. Additionally, we employ a curriculum of domain randomization to stabilize reinforcement learning (RL) training and facilitate sim2real. Next, having established the blueprint for Maniwhere, we proceed to elaborate it with details."}, {"title": "2.1 Multi-View Representation Objective", "content": "To endow the agents' ability to adapt to different viewpoints, we propose a multi-view representation learning objective to achieve this property. At each timestep t, the simulation returns the RGBD"}, {"title": "2.2 Curriculum Domain Randomization", "content": "Due to the high sensitivity of RL training towards different types of randomization, introducing additional noise can potentially lead to divergence in the entire training process. However, domain randomization and augmentation are indispensable for the sim2real transferability. Therefore, we propose a curriculum randomization approach in which the magnitude of randomization parameters is incrementally increased as training progresses. We employ an exponential scheduler to adjust the incremental change of the parameters. Additionally, we also establish a curriculum for the objective of stabilizing Q-value training [9]:\n$Q_{e}\\left(f_{\\theta}\\left(a u g\\left(o_{t}\\right)\\right), a_{t}\\right)=\\left[r_{t}+\\gamma \\max _{\\theta^{\\prime}} Q_{t g t}\\left(f_{\\theta}\\left(o_{t+1}\\right), a^{\\prime}\\right)\\right]^{2}$ (4)\nwhere $aug$ is the augmentation method for the image observations, $Q_{t g t}$ is the target Q network. The augmented data incorporates increasing amounts of noise along with the training procedure. Here we choose SRM [11] with random_overlay [22], a frequency-based data augmentation as our augmentation method."}, {"title": "2.3 Inserting the STN Module", "content": "Spatial Transformer Network (STN [17]) enables the spatial transformation of data within the network, empowering the agent with enhanced abilities to perceive spatial information. Furthermore, to expand the model's capability for transformations beyond the 2D plane, we modify the affine transformations in the original STN to perspective transformations:\n$\\begin{array}{l}\\left(\\begin{array}{l}x_{i}^{\\prime} \\\\ y_{i}^{\\prime} \\\\ 1\\end{array}\\right)=\\left(\\begin{array}{lll}\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\ \\theta_{21} & \\theta_{22} & \\theta_{23} \\\\ \\theta_{31} & \\theta_{32} & \\theta_{33}\\end{array}\\right)\\left(\\begin{array}{c}x_{i} \\\\ y_{i} \\\\ 1\\end{array}\\right)\\end{array}$ (5)\nwhere $\\theta_{ij}$ is the learnable transformation parameters, $\\left(x_{i}^{\\prime}, y_{i}^{\\prime}\\right)$ denotes the target coordinates on the output feature map's regular grid while the $\\left(x_{i}, y_{i}\\right)$ is the counterpart from the source image. Additionally, we leverage the first two layers of ResNet18 [23] as the backbone of visual encoder [13] and integrate the STN within it."}, {"title": "3 Experiments", "content": "In this section, we conduct numerous experiments in both simulated and real-world settings to showcase the effectiveness of Maniwhere in terms of generalizing to diverse visual scenarios with a combination of visual disturbance types."}, {"title": "3.1 Experiment Setup", "content": "Tasks: We have developed 8 tasks based on MuJoCo engine [24] with joint position control, including a variety of embodiments and objects such as single arm, bi-manual arms, dexterous hands, and articulated objects. We also establish the real-world counterparts for these tasks. In both simulation and real-world experiments, the observations are 128 \u00d7 128 RGB-D images with 3 frame stacks.\nSim2real: First, we train the agents in each simulated environment, where images from two different cameras will be observed: one offering a fixed viewpoint and the other moving throughout the given randomized range. Then, Maniwhere will integrate the knowledge from both viewpoints into the visual encoder via the approach mentioned in Section 2. Once finishing training in simulation, the"}, {"title": "3.2 Baselines", "content": "We compare Maniwhere with the following visual RL leading algorithms: SRM [11]: implement a frequency-based augmentation method to achieve better generalization ability for visual appearances;\nPIE-G [13]: apply a pre-trained visual encoder to enhance agent's generalization ability; SGQN [14]: SGQN leverages saliency maps to enhance the agent's attention on task-relevant information, and as suggested by Yuan et al. [22], it reveals better visual generalization capability across different camera views.\nMoVie [27]: utilizes domain adaptation to refine visual representations at new viewpoint through the dynamics model. MV-MWM [5]: applies MAE [28] to distill multi-view information into the visual encoder. It is worth noting that, unlike MV-MWM, Maniwhere does not require additional expert demonstrations, nor does it necessitate the acquisition of new data to adapt to environments as Movie does. Maniwhere can seamlessly transition to the real world in a zero-shot manner. We evaluate each algorithm over 5 seeds."}, {"title": "3.3 Simulation Results", "content": "Generalize to different viewpoints. In this section, we evaluate Maniwhere and the baseline methods across 8 challenging tasks. For each evaluation, 50 episodes from different viewpoints are tested. As shown in Table 1, compared to the existing baselines, Maniwhere achieves superior performance across all tasks with a large margin. The experiments indicate that the previous visual generalization algorithms struggle to manage visual changes in camera views. Regarding MoVie, while it adapts to the specific viewpoint change through domain adaptation, our setting involves different viewpoints among episodes. We find that MoVie cannot generalize to the visual scenarios where the viewpoint continuously changes. Hence, we argue that single-view image inputs are insufficient for fully perceiving spatial information. As for MV-MWM, it also utilizes multi-view images to enable the"}, {"title": "Generalize to different visual appearances.", "content": "In addition to changes in camera views, we further alter visual appearances by perturbing the colors of the table and background. As shown in Figure 3, despite the introduction of these visual appearance variations, Maniwhere maintains comparable performance levels with previous results, while MV-MWM suffers a substantial decline in performance drop. The underlying reason is that Maniwhere is compatible with various types of generalization, and our proposed objectives can effectively stabilize the impact of noise introduced by data augmentation and domain randomization."}, {"title": "Generalize to different embodiments.", "content": "Then, we seek to validate the agent's generalization capability across different embodiments by replacing the UR5e robot arm with a Franka arm. As shown in Table 2, we surprisingly find that our trained model can directly perform zero-shot transfer to a different embodiment while maintaining the camera-view generalization ability. The qualitative analysis can be found in Appendix C.2."}, {"title": "3.5 Ablations", "content": "To investigate the necessity of each component in Maniwhere, we ablate two main design choices in Maniwhere, including the multi-view contrastive representation learning objective and STN module. Our ablations are conducted on two lifting tasks and one pickplace task. As shown in Table 4, we observe that the multi-view objective contributed significantly to the improvement; without it, the model would be deprived of its ability to generalize across different camera views. Meanwhile, the integration of the STN enhances the model's capacity to understand and adapt to spatial view changes. Furthermore, we adopt TCN loss [32], which also applies multi-view contrastive learning, to replace our multi-view objective. The results reveal that there remains a significant generalization performance gap compared with Maniwhere, highlighting the advantages of our approach."}, {"title": "3.6 Qualitative Analysis", "content": "To delve deeper into the reasons behind Maniwhere's superior performance, we examine it from the aspects of visual representations and Q-value functions of RL."}, {"title": "Q-value distribution.", "content": "Conceptually, if an RL agent can produce the Q-value distribution from noisy visual inputs that closely approximates that obtained from the original images, the trained agent can be regarded as a more robust and generalizable learner. [12, 9] We visualize the representation of the penultimate layer of the critic using t-SNE to examine how the Q-distribution differs under various viewpoints with our proposed multi-view representation learning method. As shown in Figure 6, our method maintains a distribution similar to that of the original fixed viewpoint, whereas relying solely on the objective in Eq 4 fails to adapt to different camera views. Consequently,"}, {"title": "Trajectory embedding.", "content": "For the visual representation side, we visualize the feature maps of images rendered from different viewpoints along the same execution trajectory, and then apply t-SNE to embed the feature maps. Figure 7 shows that Maniwhere is capable of mapping the images from different viewpoints into the similar regions as well as maintain consistency throughout the entire execution trajectory."}, {"title": "3.7 Imitation Learning", "content": "Beyond visual RL, we also conduct experiments in Imitation Learning (IL) to verify the effectiveness of Maniwhere. The Pickplace task with dex-hand is selected for evaluation. In this setting, we utilize the RL trained policy as the expert to collect 100 demonstrations, and apply Diffusion Policy [29] with RGBD input as the training algorithm. Consistent with RL, we use the same visual encoder and proposed multi-view representation learning objective for training. As shown in Table 5, Maniwhere demonstrates robust generalization capability as well."}, {"title": "4 Related Work", "content": "Generalization in visual RL. In recent years, multiple works have resorted to addressing the critical issue of generalization [6, 33, 34, 35, 36, 37, 38, 39, 40, 41]. Based on the strong data augmentation, several studies integrate advanced methods such as pre-trained visual encoders [13], saliency maps [14], and normalization techniques [39] to enhance the visual generalization capabilities of agents. Despite these advancements, current methods primarily address only variations in visual appearances and fall short when confronted with other types of visual changes. Another line of works devote to solving the camera view changes. For instance, MoVie [27] utilizes an inverse dynamics model to facilitate the model adapt to a novel view pattern. However, it is limited to a singular type of view and cannot accommodate multiple different view patterns. Meanwhile, MV-MWM [5] leverages model-based RL to train a multi-view masked encoder. However, its dependency on demonstrations for task completion remains a significant limitation. Moreover, these two approaches are unable to adapt to the changes of visual appearances. On the contrary, Maniwhere offers a versatile visual RL approach that is compatible with multiple visual generalization types and does not require any demonstrations.\nRepresentation learning for visuomotor control. Representation learning plays a critical role in visuomotor control tasks [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]. Recent works [13, 55, 56,"}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we present Maniwhere, a visual generalizable framework for reinforcement learning. Maniwhere leverages multi-view representation learning to acquire the view consistency information, and utilize curriculum randomization and augmentation approach to train generalizable visual RL agents. Our experiments demonstrate that Maniwhere can adapt to diverse visual scenarios and achieve sim2real transfer in a zero-shot manner.\nThe major limitation of Maniwhere is that performing long-horizon complex tasks remains challenging for visual RL. In the future, we will continually explore the potential of Maniwhere in tackling more difficult and long-horizon mobile manipulation tasks."}, {"title": "A Task Description", "content": "Lift Cube: This task involves a UR5 arm equipped with a Robotiq gripper. A red cube is placed on the table. The agents arerequired to grasp the cube and lift it off the table. A reward greater than 250 is considered a success. We lock 3 out of the 6 DoFs of the UR5 arm to restrict unnecessary movements and reduce the action space, facilitating more efficient RL learning.\nPull Drawer: This task contains a UR5 arm equipped with a Robotiq gripper. A drawer is placed on the table. The agents need to approach the handle and pull the drawer open. A reward greater than 230 is considered a success. We lock 3 out of the 6 DoFs of the UR5 arm.\nPick Cube To Bowl: Except for the red cube, we additionally place a bowl on the table. The agent needs to lift the cube and place it into the bowl. A reward greater than 230 is considered a success. We lock 3 out of the 6 DoFs of the UR5 arm.\nButton with Dex: This task involves a Franka arm equipped with an Allegro Hand. The agent is required to press the button to receive the reward. A reward greater than 250 is considered a success. We lock 3 out of the 7 DoFs of the Franka arm and the DoFs of Allegro Hand.\nClose-Laptop Dex: This task is equipped with a Leap Hand, an XArm, and a Ranger Mini 2 base from AgileX. The agent requires to close the laptop on the table. We lock the DoFs of Leap hand and 4 DoFs of Franka Arm. When the joint of the laptop is smaller than 1.7 rad, we consider it a success.\nLiftCube Dex: This task involves a Franka arm equipped with an Allegro Hand. The agent is required to grasp the cube and lift it off the table. A reward greater than 50 is considered a success. We lock 3 out of the 7 DoFs of the Franka arm and use 4 DoFs of Allegro Hand (The rest of the DoFs will be set to a default value to keep a proper gesture).\nPickPlace Dex: This task involves a Franka arm equipped with an Allegro Hand. The agent is required to grasp the cube and lift it off the table and place it to the box. A reward greater than 50 is considered a success. We lock 3 out of the 7 DoFs of the Franka arm and use 4 DoFs of Allegro Hand (The rest of DoFs will be set to a default value to keep a proper gesture). Additionally, we use the moving average technique to smooth the motion.\nHandover Dex: We utilize two Franka arms, one equipped with a gripper and the other with an Allegro hand. This task requires cooperation between the two arms; the gripper must grasp a spatula and pass it to the hand. Success is determined if the distance between the hand and the object is less than 0.03 meters."}, {"title": "B Implementation Details", "content": null}, {"title": "B.1 Environment Randomization Parameters", "content": null}, {"title": "B.2 Curriculum Randomization", "content": "For each task, a threshold of 2e5 steps is established as the initial frame for domain randomization. The randomization parameters will vary exponentially within the ranges specified in Table 6 starting from the 2e5-step mark (the Close Laptop task beginning at 7e4 step). Concurrently, the stabilizing objective described in Eq 4 will process augmented images from the fixed view prior to this threshold, and will incorporate augmented images from the moving view thereafter."}, {"title": "B.3 Hyper-Parameters", "content": "We list the training hyper-parameters used in Maniwhere in Table 7."}, {"title": "C Additional Results", "content": null}, {"title": "C.1 Real-world Experiments", "content": "Real-world setup. Due to the limitation that a single PC cannot control two Franka arms simulta- neously, we developed a control logic framework using zmq to coordinate three PCs. In this setup, one PC is regarded as the client, while the other two serve as servers. The client PC receives visual input and performs network inference, subsequently transmitting the inferred actions via socket connections to the two server PCs. The server PCs are responsible for controlling the Franka arms and executing the received actions. This process is iterative, with the servers sending new visual input back to the client for continuous processing. Given that MV-MWM has a large model size and requires substantial memory for loading, we deployed it on a desktop equipped with an RTX 3090 GPU. In contrast, the deployment of Maniwhere demands significantly less hardware, allowing it to perform inference even on CPU desktops. Regarding the camera setup, we establish the evaluation"}, {"title": "C.3 View Generalization", "content": "We further investigate how Maniwhere's performance varies across different camera view ranges. We divide the randomized camera view range into three parts, within each of which the camera's pitch and field of view are randomly altered as well. The value for each range is calculated as the average of both the left and right sides. Due to the excessive angular range in handover task potentially obscuring the other arm, we confined the range for this task to 0-30 degrees. Table 8 illustrates that, although Maniwhere's performance exhibits a slight decline as the angle increases, it still retains the capability to handle these scenarios effectively."}, {"title": "C.4 Depth information helps sim2real transfer", "content": "To ensure the depth images closely resemble real-world conditions, we first pre-process the depth image. We introduce Gaussian noise N(0,0.01) and depth-dependent noise N(0, depth_scale), where the depth_scale equals np.abs (depth_image) * 0.05. Then, we apply Gaussian Blur to smooth the noise. Additionally, the depth values are clipped to within 2 meters and normalized to the range [0, 255]. During sim2real, we find that depth image can largely help to alleviate the"}, {"title": "C.5 MV-MWM with data augmentation", "content": "We also apply the data augmentation method on MV-MWM. As shown in Table 9, MV-MWM suffers a significant performance drop while facing data augmentation. These results are consis- tent with the recent works [9, 12]. Naively applying data augmentation can cause instability and large variance during training. In turn, the results also prove that simultaneously handling multiple types of generalization is non-trivial and highlights the superiority of Maniwhere."}, {"title": "C.6 Regarding target object color", "content": "tractors, it fails the task when the color of the target object is changed. Figure 13 exhibits that during executing a trajectory, the agent focuses more attention on the target object while ignoring task-irrelevant information, making it more sensitive to changes in the color of the target object. We use the Grad-CAM [60] and the output of value function to visualize the agent's attention."}]}