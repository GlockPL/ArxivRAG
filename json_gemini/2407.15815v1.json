{"title": "Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning", "authors": ["Zhecheng Yuan", "Tianming Wei", "Shuiqi Cheng", "Gu Zhang", "Yuanpei Chen", "Huazhe Xu"], "abstract": "Can we endow visuomotor robots with generalization capabilities to operate in diverse open-world scenarios? In this paper, we propose Maniwhere, a generalizable framework tailored for visual reinforcement learning, enabling the trained robot policies to generalize across a combination of multiple visual disturbance types. Specifically, we introduce a multi-view representation learning approach fused with Spatial Transformer Network (STN) module to capture shared semantic information and correspondences among different viewpoints. In addition, we employ a curriculum-based randomization and augmentation approach to stabilize the RL training process and strengthen the visual generalization ability. To exhibit the effectiveness of Maniwhere, we meticulously design 8 tasks encompassing articulate objects, bi-manual, and dexterous hand manipulation tasks, demonstrating Maniwhere's strong visual generalization and sim2real transfer abilities across 3 hardware platforms. Our experiments show that Maniwhere significantly outperforms existing state-of-the-art methods. Videos are provided at https://gemcollector.github.io/maniwhere/.", "sections": [{"title": "Introduction", "content": "Visuomotor control tasks present roboticists with a vexing issue the hardware setup can severely influence the performance of the robot policies. A prime example arises from the issue of immovable cameras - envision a carefully calibrated visual sensor, painstakingly positioned to enable seamless real-world deployment, only to have it disturbed by a lab mate. This single, seemingly innocuous incident can grind progress to a halt, forcing tedious recalibration or the collection of new demonstration data. Furthermore, changes in the background or the presence of extraneous objects within the captured views may undermine the effectiveness of a trained policy. Such obstacles have long plagued the field of robotics, representing critical barriers to realizing the full potential of advanced visuomotor systems.\nAcknowledging these obstacles, when attempting to achieve sim2real visual policy transfer, it is common to instantiate a digital twin that closely resembles the actual real-world environment [1, 2, 3, 4, 5, 6, 7, 8]. Otherwise, the significant discrepancy between the digital twin and the real setting would render the trained models wholly ineffective. Hence, robots that adeptly handle in-the-wild scenarios should possess generalizability against various visual changes such as camera views, visual appearances, lighting conditions, etc.\nWhile prior works have sought to tackle the challenges against visual scene variations [9, 10, 11, 12, 13, 14, 15], these studies primarily focus on resolving a single aspect and are unable to handle multiple visual generalization types simultaneously. Meanwhile, it is non-trivial to incorporate various inductive biases into the training process. Naively applying domain randomization or data augmentation methods can destabilize the entire RL training, ultimately leading to divergence for the learned policy [4, 9, 12, 16]. More importantly, the generalization abilities of these methods have yet to be thoroughly evaluated on real robots.\nIn this paper, we are dedicated to enabling robots to acquire strong visual generalization ability so that they can step out of simulations and apply their learned skills to complex real-world scenarios without camera calibration. We introduce Maniwhere: A Visual Generalizable Framework for Reinforcement Learning. As shown in Figure 1, Maniwhere employs a multi-view representation objective to capture implicitly shared semantic information and correspondences across different viewpoints. In addition, we fuse the STN module [17] within the visual encoder to further enhance the robot's robustness to view changes. Subsequently, to achieve sim2real transfer, we utilize a curriculum-based domain randomization approach to stabilize RL training and prevent divergence. The resulting trained policy can be transferred to real-world environments in a zero-shot manner.\nTo conduct the evaluation, we develop 3 types of robotic arms and 2 types of robotic hands to design a total of 8 diverse tasks, alongside 3 corresponding hardware setups to validate the efficacy of our algorithm. Our comprehensive experiments demonstrate that, in both simulation and real-world scenarios, Maniwhere significantly outperforms existing state-of-the-art baselines by a large margin."}, {"title": "Method", "content": "In this section, we present Maniwhere, a generalizable framework for visual reinforcement learning. We propose a multi-view representation learning objective aimed at empowering the training agent with the ability to extract invariant features and generalize across different viewpoints. To further augment the model's spatial awareness, we incorporate an STN module into the visual encoder by actively spatially transforming feature maps. Additionally, we employ a curriculum of domain randomization to stabilize reinforcement learning (RL) training and facilitate sim2real. Next, having established the blueprint for Maniwhere, we proceed to elaborate it with details."}, {"title": "Multi-View Representation Objective", "content": "To endow the agents' ability to adapt to different viewpoints, we propose a multi-view representation learning objective to achieve this property. At each timestep t, the simulation returns the RGBD\n$J_{con} (\\theta) = -log \\frac{exp(f_\\theta(o_{fixed})^T. f_\\theta(o_{move^+})/\\tau)}{exp(f_\\theta(o_{fixed})^T. f_\\theta(o_{move^+})/\\tau) + \\sum_{move^-} exp(f_\\theta(o_{fixed})^T. f_\\theta(o_{move^-})/\\tau)}$\n$J_{feat}(\\theta) = \\sum_{(o_{fixed}, o_{move}) \\in B} ||F(o_{fixed}) \u2013 F(o_{move}) ||_2$\n$L_{Maniwhere}(\\theta) = I_{con}(\\theta) + \\lambda J_{feat} (\\theta)$"}, {"title": "Curriculum Domain Randomization", "content": "Due to the high sensitivity of RL training towards different types of randomization, introducing additional noise can potentially lead to divergence in the entire training process. However, domain randomization and augmentation are indispensable for the sim2real transferability. Therefore, we propose a curriculum randomization approach in which the magnitude of randomization parameters is incrementally increased as training progresses. We employ an exponential scheduler to adjust the incremental change of the parameters. Additionally, we also establish a curriculum for the objective of stabilizing Q-value training [9]:\n$Q_{\\theta} (f_{\\theta}(aug (o_t)), a_t) = [r_t + \\gamma \\max_{a'} Q_{\\theta_{tgt}}(f_{\\theta}(o_{t+1}), a')]^2$"}, {"title": "Inserting the STN Module", "content": "Spatial Transformer Network (STN [17]) enables the spatial transformation of data within the network, empowering the agent with enhanced abilities to perceive spatial information. Furthermore, to expand the model's capability for transformations beyond the 2D plane, we modify the affine transformations in the original STN to perspective transformations:\n$\\begin{pmatrix}\nx'\\\\y'\\\\1\n\\end{pmatrix} = \\begin{pmatrix}\n\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n\\theta_{21} & \\theta_{22} & \\theta_{23} \\\\\n\\theta_{31} & \\theta_{32} & \\theta_{33}\n\\end{pmatrix} \\begin{pmatrix}\nx_i\\\\y_i\\\\1\n\\end{pmatrix}$"}, {"title": "Experiments", "content": "In this section, we conduct numerous experiments in both simulated and real-world settings to showcase the effectiveness of Maniwhere in terms of generalizing to diverse visual scenarios with a combination of visual disturbance types."}, {"title": "Experiment Setup", "content": "Tasks: We have developed 8 tasks based on MuJoCo engine [24] with joint position control, including a variety of embodiments and objects such as single arm, bi-manual arms, dexterous hands, and articulated objects. We also establish the real-world counterparts for these tasks. In both simulation and real-world experiments, the observations are 128 \u00d7 128 RGB-D images with 3 frame stacks.\nSim2real: First, we train the agents in each simulated environment, where images from two different cameras will be observed: one offering a fixed viewpoint and the other moving throughout the given randomized range. Then, Maniwhere will integrate the knowledge from both viewpoints into the visual encoder via the approach mentioned in Section 2. Once finishing training in simulation, the trained model will be directly transferred to the real world in a zero-shot manner. It should be noted that during both simulation and real-world evaluation, the trained agents receive images solely from a single camera for inference. The visual scenes will be modified from various aspects, including appearance, camera view, lighting conditions, and even cross embodiments at evaluation time."}, {"title": "Baselines", "content": "We compare Maniwhere with the following visual RL leading algorithms: SRM [11]: implement a frequency-based augmentation method to achieve better generalization ability for visual appearances; PIE-G [13]: apply a pre-trained visual encoder to enhance agent's generalization ability; SGQN [14]: SGQN leverages saliency maps to enhance the agent's attention on task-relevant information, and as suggested by Yuan et al. [22], it reveals better visual generalization capability across different camera views. MoVie [27]: utilizes domain adaptation to refine visual representations at new viewpoint through the dynamics model. MV-MWM [5]: applies MAE [28] to distill multi-view information into the visual encoder. It is worth noting that, unlike MV-MWM, Maniwhere does not require additional expert demonstrations, nor does it necessitate the acquisition of new data to adapt to environments as Movie does. Maniwhere can seamlessly transition to the real world in a zero-shot manner. We evaluate each algorithm over 5 seeds."}, {"title": "Simulation Results", "content": "Generalize to different viewpoints. In this section, we evaluate Maniwhere and the baseline methods across 8 challenging tasks. For each evaluation, 50 episodes from different viewpoints are tested. As shown in Table 1, compared to the existing baselines, Maniwhere achieves superior performance across all tasks with a large margin. The experiments indicate that the previous visual generalization algorithms struggle to manage visual changes in camera views. Regarding MoVie, while it adapts to the specific viewpoint change through domain adaptation, our setting involves different viewpoints among episodes. We find that MoVie cannot generalize to the visual scenarios where the viewpoint continuously changes. Hence, we argue that single-view image inputs are insufficient for fully perceiving spatial information. As for MV-MWM, it also utilizes multi-view images to enable the model to learn view-invariant features. Nevertheless, the experiment results exhibit that Maniwhere owns stronger multi-view generalization abilities than MV-MWM with a +68.6% boost on average."}, {"title": "Generalize to different visual appearances", "content": "In addition to changes in camera views, we further alter visual appearances by perturbing the colors of the table and background. As shown in Figure 3, despite the introduction of these visual appearance variations, Maniwhere maintains comparable performance levels with previous results, while MV-MWM suffers a substantial decline in performance drop. The underlying reason is that Maniwhere is compatible with various types of generalization, and our proposed objectives can effectively stabilize the impact of noise introduced by data augmentation and domain randomization."}, {"title": "Generalize to different embodiments", "content": "Then, we seek to validate the agent's generalization capability across different embodiments by replacing the UR5e robot arm with a Franka arm. As shown in Table 2, we surprisingly find that our trained model can directly perform zero-shot transfer to a different embodiment while maintaining the camera-view generalization ability. The qualitative analysis can be found in Appendix C.2."}, {"title": "Real-World Experiments", "content": "Regarding real-world experiments, as shown in Figure 4, we deploy our models trained in simulation on real-world scenarios across 3 hardware setups in a zero-shot manner. For gripper-based tasks, we implement multiprocessing alongside a shared memory queue to synchronize the execution of network inference and the controller [29], thereby ensuring a smooth movement process. As for dexterous-hand tasks, we introduce a moving average factor to reduce the jittering motions during execution [30, 31]. We select 5 challenging tasks in simulation to verify the effectiveness of agent's sim2real tranferability. For each task, we choose 5 different viewpoints that cover the workspace, and the visual appearances of the scenario will be altered under each viewpoint as well. Each algorithm is evaluated 5 trials under every visual condition. In each trial, yaw and pitch angles of the camera will be randomized. Figure 5 exhibits snapshots of real-world settings. As shown in Table 3, consistent with the simulation results, Maniwhere outperforms MV-MWM on all tasks. The experiment indicates that Maniwhere not only narrows the sim2real gap but also enables the trained robots to achieve the real-world generalization ability. More details and results can be found in Appendix and webpage."}, {"title": "STN features", "content": "Figure 3 (b) illustrates that when facing real-world images, the STN layer assists the agent in transforming inputs from different viewpoints to closely resemble the fixed view used during training, thus facilitating the camera-view generalization and acquiring view-invariant representations."}, {"title": "Ablations", "content": "To investigate the necessity of each component in Maniwhere, we ablate two main design choices in Maniwhere, including the multi-view contrastive representation learning objective and STN module. Our ablations are conducted on two lifting tasks and one pickplace task. As shown in Table 4, we observe that the multi-view objective contributed significantly to the improvement; without it, the model would be deprived of its ability to generalize across different camera views. Meanwhile, the integration of the STN enhances the model's capacity to understand and adapt to spatial view changes. Furthermore, we adopt TCN loss [32], which also applies multi-view contrastive learning, to replace our multi-view objective. The results reveal that there remains a significant generalization performance gap compared with Maniwhere, highlighting the advantages of our approach."}, {"title": "Qualitative Analysis", "content": "To delve deeper into the reasons behind Maniwhere's superior performance, we examine it from the aspects of visual representations and Q-value functions of RL."}, {"title": "Q-value distribution", "content": "Conceptually, if an RL agent can produce the Q-value distribution from noisy visual inputs that closely approximates that obtained from the original images, the trained agent can be regarded as a more robust and generalizable learner. [12, 9] We visualize the representation of the penultimate layer of the critic using t-SNE to examine how the Q-distribution differs under various viewpoints with our proposed multi-view representation learning method. As shown in Figure 6, our method maintains a distribution similar to that of the original fixed viewpoint, whereas relying solely on the objective in Eq 4 fails to adapt to different camera views. Consequently,\nManiwhere not only closes the distance between visual embeddings to obtain more robust visual representations, but also narrows the gap between Q-distributions, further stabilizing training and enhancing agent's visual generalization ability."}, {"title": "Trajectory embedding", "content": "For the visual representation side, we visualize the feature maps of images rendered from different viewpoints along the same execution trajectory, and then apply t-SNE to embed the feature maps. Figure 7 shows that Maniwhere is capable of mapping the images from different viewpoints into the similar regions as well as maintain consistency throughout the entire execution trajectory."}, {"title": "Imitation Learning", "content": "Beyond visual RL, we also conduct experiments in Imitation Learning (IL) to verify the effectiveness of Maniwhere. The Pickplace task with dex-hand is selected for evaluation. In this setting, we utilize the RL trained policy as the expert to collect 100 demonstrations, and apply Diffusion Policy [29] with RGBD input as the training algorithm. Consistent with RL, we use the same visual encoder and proposed multi-view representation learning objective for training. As shown in Table 5, Maniwhere demonstrates robust generalization capability as well."}, {"title": "Related Work", "content": "Generalization in visual RL. In recent years, multiple works have resorted to addressing the critical issue of generalization [6, 33, 34, 35, 36, 37, 38, 39, 40, 41]. Based on the strong data augmentation, several studies integrate advanced methods such as pre-trained visual encoders [13], saliency maps [14], and normalization techniques [39] to enhance the visual generalization capabilities of agents. Despite these advancements, current methods primarily address only variations in visual appearances and fall short when confronted with other types of visual changes. Another line of works devote to solving the camera view changes. For instance, MoVie [27] utilizes an inverse dynamics model to facilitate the model adapt to a novel view pattern. However, it is limited to a singular type of view and cannot accommodate multiple different view patterns. Meanwhile, MV-MWM [5] leverages model-based RL to train a multi-view masked encoder. However, its dependency on demonstrations for task completion remains a significant limitation. Moreover, these two approaches are unable to adapt to the changes of visual appearances. On the contrary, Maniwhere offers a versatile visual RL approach that is compatible with multiple visual generalization types and does not require any demonstrations.\nRepresentation learning for visuomotor control. Representation learning plays a critical role in visuomotor control tasks [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]. Recent works [13, 55, 56, 57] have verified that leveraging the pre-trained visual encoders via representation learning approaches can facilitate the execution of numerous downstream control tasks. Furthermore, SODA [10] utilizes a BYOL-style [58] objective to decouple augmentation from policy learning. RL3D [3] pretrains a deep voxel-based 3D autoencoder and continually finetunes the representation with in-domain data. H-index [59] applies the keypoint detection and pose estimation method to derive a customized representation for the hand. In contrast to these works, Maniwhere not only strives to obtain generalizable visual representations but also seeks to enable these representations to bridge the sim2real gap."}, {"title": "Conclusion and Limitations", "content": "In this paper, we present Maniwhere, a visual generalizable framework for reinforcement learning. Maniwhere leverages multi-view representation learning to acquire the view consistency information, and utilize curriculum randomization and augmentation approach to train generalizable visual RL agents. Our experiments demonstrate that Maniwhere can adapt to diverse visual scenarios and achieve sim2real transfer in a zero-shot manner.\nThe major limitation of Maniwhere is that performing long-horizon complex tasks remains challenging for visual RL. In the future, we will continually explore the potential of Maniwhere in tackling more difficult and long-horizon mobile manipulation tasks."}]}