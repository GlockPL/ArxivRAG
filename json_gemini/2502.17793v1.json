{"title": "SYNTHIA: Novel Concept Design with Affordance Composition", "authors": ["Xiaomeng Jin", "Hyeonjeong Ha", "Jeonghwan Kim", "Jiateng Liu", "Zhenhailong Wang", "Khanh Duy Nguyen", "Ansel Blume", "Nanyun Peng", "Kai-wei Chang", "Heng Ji"], "abstract": "Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence- the integration of multiple affordances into a single coherent concept-remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.", "sections": [{"title": "1 Introduction", "content": "Imagine a coffee machine with wheels that brews a morning coffee and delivers it to your bed every morning. This example illustrates a novel concept that is atypical and dissimilar to everyday concepts we regularly encounter in our lives. Novel concept synthesis requires an effective fusion of disparate concepts (e.g., coffee machine, trolley), akin to how humans blend ideas across cognitive domains to generate creative innovations (Fauconnier and Turner, 2002; Han et al., 2018).\nExisting studies on conceptual design using T2I models have enabled rapid ideation of novel visual concepts (Cai et al., 2023; Ma et al., 2023; Wang et al., 2024; Lin et al., 2025) by identifying user challenges such as interpreting abstract concepts in language to help visualize a novel design concept (Lin et al., 2025), or using large language models (LLMs) to bootstrap initial ideation in texts (Cai et al., 2023; Zhu and Luo, 2023). However, they often naively feed LLM-generated textual prompts into T2I models, relying on simple key phrases or semantic variations of concept description (Cai et al., 2023; Wang et al., 2024). While existing works show that T2I models can generate images that seem to correctly reflect complex human-formulated textual descriptions (e.g., \"beautiful rendering of neon lights in futuristic cyberpunk city\"), they do not focus on whether a model can synthesize a novel concept when given a set of affordances (e.g., brew, deliver) as input, while ensuring these affordances are preserved.\nAn important aspect lacking in existing approaches to concept synthesis is their focus on pixel-based control, overlooking the structural and functional roles embedded in design. Many real-world concepts are naturally \u201cdecomposable\u201d into parts, where each part signals a specific functionality. To address this, we propose SYNTHIA, a framework for Concept Synthesis with Affordance composition that generates functionally coherent and visually novel concepts given a set of desired affordances. Unlike prior works relying on complex descriptive text to generate stylistic variations or aesthetic features (Richardson et al., 2024; Vinker et al., 2023), SYNTHIA leverages affordances- defined as \"the functionality offered by an object or its parts\"-as a structural guide for novel concept synthesis. By aligning textual descriptions with affordances as control signals, our models implicitly learn to \"decompose and reassemble\u201d functional parts, ensuring that, for instance, a hybrid of a coffee machine and a trolley not only appears novel but also retains its brewing and mobility functions, achieving functional coherence.\nTo facilitate structured affordance composition, we construct a hierarchical concept ontology that decomposes visual concepts (e.g., Furniture-Sofa) into their constituent parts (e.g., leg, cushion) and associated affordances (e.g., support, rest). It provides a structured representation of concept-affordance associations, serving as the foundation for generating functionally meaningful designs. Inspired by the theory of combinational creativity in humans (Han et al., 2018), which suggests novel concepts emerge from disparate ideas, we propose an affordance sampling mechanism that strategically selects affordances associated with sufficiently different concepts using our novel similarity-based metric (\u00a73.1). This ensures that generated designs integrate novel functionalities, avoiding trivial combinations, whereas random sampling yields similar affordances (e.g., cook, heat) that result in redundant outputs (Fig la).\nWe also introduce a new curriculum learning scheme that fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. Our curriculum gradually increases the affordance distance, allowing models to first learn basic concept-affordance associations from close affordance pairs before tackling complex affordance compositions that integrate multiple affordances into a single, coherent form. To further ensure novelty, we employ contrastive objectives to push learned representations away from existing concepts in our ontology. This addresses a critical limitation of existing T2I models, which struggle to generate coherent multi-functional concepts (Fig. 1b). Without structured affordance composition, models tend to default to familiar objects-e.g., when prompted with drive and vacuum affordances, Stable Diffusion models simply generate a car with missing vacuum functions (Fig. 1b), rather than blending both affordances into an integrated design. Importantly, unlike existing AI-driven design frameworks that rely on detailed LLM-generated descriptions, SYNTHIA enables direct affordance-based prompting, e.g., \u201ca new design that has functions of {desired affordances}.\u201d."}, {"title": "2 Related Work", "content": "Text-to-Image Models. The advancement of text-to-image (T2I) models has enabled high-quality image synthesis from textual descriptions (Sohn et al., 2023; Xue et al., 2024; Shi et al., 2024; Chen et al., 2024). Especially, the invention of diffusion-based models, such as DALL-E (Ramesh et al., 2021a) and Stable Diffusion (Rombach et al., 2022), significantly increases the performance of the T2I generation by utilizing a transformer-based architecture, where the image embeddings and text encodings are aligned in the shared representation space. For instance, Bao et al. (2024) propose a compositional fine-tuning method for T2I Diffusion Models that focuses on two novel objectives and performs fine-tuning on critical parameters. However, these models still struggle to understand practical functionalities and integrate multiple components into coherent novel concepts. This highlights the need for a new framework to enhance the compositional reasoning ability of T2I models, which our work aims to address.\nNovel Concept Generation. The great power of T2I models provides a potential boost to content creation (Ko et al., 2023; Rangwani et al., 2024; Sankar and Sen, 2024; Rahman et al., 2024; Tang et al., 2024). Novel concept generation aims to produce visual outputs that extend the existing concepts by specifying the requirements as input to the T2I models. For instance, Concept Weaver (Kwon et al., 2024) first generates a template image based on a text prompt, then refines it using a concept fusion strategy. ConceptLab (Richardson et al., 2024) utilizes Diffusion Prior models and formulates the generation process as an optimization process over the output space of the diffusion prior. Yet, they focus on concept-level generation and ignore the relationships between concepts and their parts. By prioritizing aesthetics, they limit real-world practicality. Our work bridges this gap by designing an affordance-driven framework for novel concept synthesis, ensuring the fusion of desired functions to output novel but practical concepts."}, {"title": "3 SYNTHIA: Novel Concept Design with Affordance Composition", "content": "Our ultimate goal is to utilize T2I models in designing novel concepts that are both visually novel and functionally coherent. Specifically, we take the desired affordances as text inputs, the T2I models should generate an image that depicts the novel concept design. To achieve this, we (1) construct a training recipe that explicitly embeds hierarchical relations on visual concepts, parts, and corresponding affordances, and (2) fine-tune T2I models with curriculum-based optimization."}, {"title": "3.1 Affordance Composition Curriculum", "content": "The primary challenge in the novel concept generation of existing T2I models is the lack of structured functional grounding. These models often struggle to design visually novel yet functionally coherent concepts while maintaining intended functionalities. For example, when combining affordances like Brew and Cut, they may prioritize aesthetics over functionality, omitting parts or objects relevant to Brew (Fig 1b). To address this, we construct a structured training recipe in two key steps: (1) building a hierarchical concept ontology, and (2) designing an affordance sampling strategy for curriculum-based training. This improves the model's composition ability by learning the connection between concepts and affordances.\nHierarchical Concept Ontology. To provide a structured basis for novel concept synthesis with functional coherence, we define a hierarchical concept ontology that decomposes visual concepts into constituent parts and their affordances, capturing concept-affordance associations (Fig 5). This ontology allows T2I models to retrieve relevant parts based on affordances, enabling generation to be well-grounded on the functionality of concepts rather than superficial visual feature combinations. Formally, we define the ontology as a four-level hierarchy $O = (S, C, P, A)$. Superordinate $S$ denotes the highest-level categories, such as furniture, followed by Concept $C$, which is the set of visual concepts. Each $c \\in C$ belongs to a superordinate category $s \\in S$, e.g., $S_{table}$ = furniture, and decomposes into its parts $P$ that serve specific functions in an object design, e.g., $P_{table}$ = {leg, drawer}. The Affordance $A$ describes functionalities of concepts and parts. Both a concept $c \\in C$ and its part $p \\in P$ are linked to affordances set $A_c$ = {$a_1$,\u2026,$a_n$} $ \\in A$, e.g, $A_{table}$ = {write,organize}, and $A_p$ = {$P_1$,..., $P_n$}, e.g., $A_{leg}$ = {support}. Our ontology spans 30 superordinates, 590 concepts, 1172 parts, and 686 affordances, explicitly providing a structured representation of how affordance is associated with fine-grained parts for functionally grounded novel concept synthesis.\nAffordance Sampling. Given our ontology, we can utilize it to create fine-tuning data to improve the functional coherence of the generated novel concept by T2I models. A naive approach to obtaining training data would be to exhaustively pair all possible affordances. However, this would yield 235K affordance pairs, which is computationally expensive. Moreover, random combination risks generating redundant concepts (e.g., Heat and Cook examples in Fig 1a) or functionally incoherent objects. To achieve sufficiently different affordance pairs that enable novel concept synthesis while still being functionally integrable, we introduce a distance-based affordance sampling strategy that selects meaningful, disparate affordance pairs based on ontology-derived distances.\nWe define a concept distance $D_C(c_i,c_j)$ between two concepts $c_i$, $c_j \\in C$ by incorporating functional relatedness at the affordance level and semantic similarity at the concept level. We compute functional relatedness using Jaccard similarity $J(X, Y) = \\frac{|X \\cap Y|}{|X \\cup Y|}$ between their affordance sets while quantifying the semantic similarity Sim via measuring embedding similarity using the BERT (Devlin et al., 2019) model as follows:"}, {"title": null, "content": "$D_C(c_i, c_j) = \\alpha * {J(A_{ci}, A_{cj}) + J(A_{p_{ci}}, A_{p_{cj}})}, +\n\\beta * Sim(BERT(c_i), BERT(c_j)),$"}, {"title": null, "content": "where $ \\alpha $, $ \\beta $ are adjustable hyperparameters that balance between functional relatedness based on affordances, and semantic relevance of concepts, respectively. Since we prioritize affordance-level similarity over concept-level similarity during training, we set $ \\alpha $ = 0.7 and $ \\beta $ = 0.3. Two semantically similar concepts sharing more affordances have closer distances, such as sofa and chair, while those that have different affordances and semantic differences, such as car and vacuum cleaner have more distant distances.\nWe further obtain affordance distance $D_A(a_i, a_j)$ between two affordances $a_i$, $a_j \\in A$ by averaging pairwise concept distances $D_C(\u00b7,\u00b7)$ between associated concepts:"}, {"title": null, "content": "$D_A(a_i, a_j) = \\frac{1}{|C_{a_i}| * |C_{a_j}|} \\sum_{c_p\\in C_{a_i}} \\sum_{c_q\\in C_{a_j}} D_C(c_p, c_q),$"}, {"title": null, "content": "where $C_{a_i}$ and $C_{a_j}$ are the sets of concepts associated with affordances $a_i$ and $a_j$, respectively. The resulting $D_A(\u00b7,\u00b7)$ is distributed from 0.1 to 1.0.\nBased on our distance metric, close affordance pairs associated with similar concepts, e.g., {sit,rest} from {sofa, chair}, support learning basic affordance-concept associations, which can be easily merged into existing concepts. In contrast, distant affordance pairs derived from sufficiently distant concepts, e.g., {drive, vacuum} from {car, vacuum cleaner}, enforce greater functional coherence by requiring meaningful part-affordance integration, which is more complex than a trivial combination.\nCurriculum Construction. In novel concept generation, existing T2I models struggle with (1) concept-affordance associations and (2) the composition of functionally coherent affordances into a single concept. To address these challenges with limited data, we propose a three-stage curriculum that progressively increases affordance pair distances. In the earliest stage, we utilize close affordance pairs to reinforce fundamental knowledge of the concept-affordance associations. The second stage employs the affordance pairs from the mid-range distances to encourage the model to learn the fine-grained compositional structures while maintaining prior knowledge. In the last stage, we only introduce distant affordance pairs to challenge the model to synthesize novel, functionally coherent concepts by applying the previously learned basics on the fine-grained parts and affordance relations.\nWe sample 600 affordance pairs uniformly across the full distance spectrum and categorize them into three groups. For training images used as pseudo novel concepts, we generate 10 images per pair using DALL-E (Ramesh et al., 2021b) with GPT-4 (OpenAI, 2024) generated captions that describe different novel designs integrating the specified affordances. We then filter images using CLIP similarity scores and manually select the top three. This curriculum-based training enables T2I models to learn basic concept-affordance associations while fusing affordances into coherent and functionally meaningful designs. Thus, the T2I models can successfully produce novel concepts that are visually distinctive and functionally coherent."}, {"title": "3.2 Contrastive Fine-tuning with Curriculum Learning", "content": "The goal of fine-tuning T2I models is to enable them to fuse multiple affordances into a single, functionally coherent concept while ensuring visual novelty. With our curriculum, we propose a curriculum learning strategy to fine-tune the diffusion-based T2I models. From a data-driven perspective, training with affordance pairs and DALL-E-generated pseudo-novel concepts helps the model design novel concepts given specified affordances.\nTo further enhance visual novelty, we incorporate contrastive learning objectives, ensuring that generated images not only reflect desired affordances but also differ from existing concepts associated with them. Specifically, we define two sets of constraints derived from our ontology to guide the model: (1) Positive Constraints specify the target affordances that must be included in the novel concepts, shaping their functional structure; (2) Negative Constraints consist of all existing concepts from our ontology that already have the target affordances in the positive constraints. These act as references to avoid. By adhering to these constraints, the model generates concepts that successfully integrate the specified affordances while maintaining a high degree of novelty.\nTraining Objectives. The training objective of fine-tuning is formulated using a triplet loss, which can balance two components to achieve the desired outcomes. The first component aims to minimize the similarity loss between the generated image and the pseudo-novel image created during curriculum construction, ensuring visual novelty. To reduce the overfitting problem, we also sample multiple pseudo-novel images that describe different concepts. Given the set of affordances $A_{pos} = {a_1,\u2026, a_n}$ in the positive constraints, together with a sampled image $I^+$ from the pseudo-novel images from DALL-E, the positive loss is defined as follows:"}, {"title": null, "content": "$L_{pos}(\\theta_t) = ||I^+ - \\hat{I_i}||^2 + E_{\\epsilon, t} [||\\epsilon \u2013 \\epsilon_{\\theta_t}(t)||^2],$where $ \\theta_t $ is T2I model parameters, $ \\hat{I_i} $ denotes the generated image, $ \\epsilon $ is Gaussian random noise. We employ noise prediction loss, where the model takes the latent embedding of $ I^+ $ as input and predicts the noise as $ \\epsilon_{\\theta_t}(t) $, preventing catastrophic forgetting of learned training distribution.\nThe second component of the triplet loss maximizes the similarity loss between the generated image and a randomly sampled existing concept image $I^-$ that contains partial affordances from the positive constraints as follows:"}, {"title": null, "content": "$L_{neg}(\\theta_t) = ||I^- - \\hat{I_i}||^2$In this way, the model learns to avoid generating existing concept images and increase its novelty. Our overall triplet loss is defined as follows:"}, {"title": null, "content": "$L(\\theta_t) = L_{pos}(\\theta_t) - \\gamma * L_{neg}(\\theta_t),$where $ \\gamma $ is an adjustable hyperparameter. By balancing two losses, our framework ensures that the generated images align with the desired affordances while remaining distinct from existing concepts."}, {"title": "3.3 Novel Concept Generation during Inference", "content": "After fine-tuning the diffusion-based T2I models, our approach requires only the desired affordances as positive constraints during inference time, eliminating the need for manually collecting existing concepts as negative constraints. This efficiency gain stems from incorporating both positive and negative constraints-derived from our hierarchical concept ontology-into the training objective. By embedding these constraints during training, the model learns concept-affordance associations and improves its ability to compose parts associated with desired affordances into a novel design. Therefore, the model can produce novel, structured designs without redundant textual prompting."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. To train T2I models with our approach, we construct a dataset from two types of resources (more details in Appendix D.1): (1) Existing Concept Images: For each existing concept in our ontology, we collect 60 images from external platforms including Google Images and iStock. To ensure that images are object-centric and aligned with the concept, we filter out low-quality images using CLIP model (Radford et al., 2021). (2) Generated Novel Concept Images: With our affordance sampling, we uniformly sample 600 affordance pairs among 235K possible pairs for fine-tuning. For the test dataset, we select 500 affordance pairs among the ones not used for fine-tuning.\nFoundation Models We adopt Kandinsky3.0 (Vladimir et al., 2024) as the T2I backbone model; it generates images based on a given text prompt, with an optional negative text prompt to refine outputs. During fine-tuning, we incorporate the desired affordances as positive inputs, while using the existing concepts from the ontology as negative constraints. During the inference, we provide only the text prompts with desired affordances, \u201ca new design that has functions of {desired affordances}.\u201d.\nBaselines Methods We compare our proposed method against three baseline methods, which are Stable Diffusion (Esser et al., 2024), Kandinsky (Arkhipkin et al., 2023), and ConceptLab (Richardson et al., 2024). While Stable Diffusion and Kandinsky are general T2I models, ConceptLab optimizes generation over diffusion before creative concept design. For a fair comparison, we fine-tune ConceptLab using the same training data as our method. In contrast, our framework directly fine-tuned the diffusion model, integrating the hierarchical visual ontology to enforce the design of a single, coherent concept that achieves multiple affordances.\n4.2 Evaluation Metrics\nAutomatic Evaluation. To automatically evaluate the performance of our proposed method, we design four novel metrics to assess the quality of the generated data:\n\u2022 Faithfulness: This metric evaluates how well the generated object aligns with instructions, focusing on its intended affordances and whether the image effectively conveys the object's purpose.\n\u2022 Novelty assesses the originality and creativity of the generated design, emphasizing uniqueness and unconventional concepts that surprise or intrigue users.\n\u2022 Practicality evaluates the real-world applicability of the design. It examines usability, alignment with human preferences, and feasibility for production.\n\u2022 Coherence evaluates whether the generated image is object-centric, depicting a single clear and functional object without unintended elements. It examines whether multiple affordances are fused into a unified concept rather than shown as separate objects.\nFor all four metrics, we use absolute scores ranging from 1 to 5, with higher scores indicating better quality. However, since the scores for these metrics may be influenced by subjective interpretation, we also include a relative evaluation. Specifically, we present each generated image with its corresponding DALL-E generated image, and ask the automatic evaluator to compare and determine which is superior or if they are equally strong. This relative comparison ensures a more fair evaluation and reduces potential biases. We use GPT-4O (OpenAI, 2024) as the evaluation model to assess the metrics.\nHuman Evaluation. To assess the quality of the generated concepts beyond automated evaluations, we conduct a human evaluation with 36 non-design expert annotators. Recruited from the university across diverse majors, they are provided with a detailed rubric using the same metrics and a 1-5 scale as the automated evaluations."}, {"title": "4.3 Results and Analysis", "content": "Automatic Evaluation In Table 1, we compare SYNTHIA against three existing T2I models using our evaluation metrics. For a fair comparison, we randomly sample 500 test pairs that are not used for training. From the results in Table 1, we observe that the Stable Diffusion model always maintains a high practicality but lower novelty. This aligns with our observation (Fig 1) that it tends to generate existing concepts rather than novel concepts. In addition, when it cannot generate an existing concept that satisfies all affordances in the text prompt, the Stable Diffusion model will generate multiple objects in an image without any fusion. This is also reflected in the low coherence scores for both concept and affordance levels. For the other baseline methods, Kandinsky-3 and ConceptLab show an increase in terms of novelty and coherence. However, they suffer from a reduction in practicality. Compared to all the models, our method SYNTHIA achieves the best faithfulness, novelty, and coherence scores while maintaining high performance in practicality. These results reflect that finetuning with the curriculum strategy can successfully follow the text instruction, fuse various affordances, and generate novel concepts.\nHuman Evaluation To assess the consistency of human evaluations, we computed inter-annotator agreement (IAA) between two independent raters, where ratings were considered in agreement if their absolute difference was \u2264 1. IAA across all images was 67.5% with Cohen's Kappa of 22.3%, where Novelty achieved the highest agreement at 70.9%, followed by Faithfulness (68.5%), Practicality (66.4%), and Coherence (64.1%). Additionally, the agreement between aggregated human ratings and automatic evaluations reached 91.25%, indicating that human annotators achieve a reasonable level of consistency while automatic evaluations closely align with human judgments. The human evaluation results consistently demonstrate that our model generates functionally coherent and visually novel concepts with outperforming scores on faithfulness, novelty, practicality, and coherence."}, {"title": "4.4 Ablation Studies", "content": "The Size of Fine-tuning Training Data In our experiment, We fine-tune the diffusion model using 600 affordance pairs as the training data. To investigate the impact of the training data size, we compare performance across different scales: training with 200, 400, 600, and 800 affordance pairs and using automatic evaluation. As shown in Figure 6, we find that the performance improves with larger datasets, and reaches the optimal point at 600. Across all four training sizes, our model consistently outperforms baseline methods.\nNumber of Positive Affordances To evaluate the impact of the number of positive affordances in the input prompt, we also conduct experiments using 3 and 4 positive affordances and compare the performance of all methods. As shown in Table 3 and 4, while a slight performance drop occurs across all models as the number of affordances increases, our method consistently maintains high novelty and coherence, showing the generalization ability of SYNTHIA to integrate multiple affordances.\nEffectiveness of Affordance Sampling To examine the impact of affordance pair distance on novelty, we select 100 pairs with the lowest and 100 pairs with the highest distance scores from the test set. The automatic novelty scores for each group, shown in Table 5, demonstrate that all three three baseline methods achieve relatively low novelty scores for close affordance pairs, which indicates a tendency to generate existing concepts rather than novel designs. In contrast, our method always exhibits high novelty across various distances and outperforms the baseline models.\nEffectiveness of Curriculum Learning In our framework, we incorporate a curriculum learning (CL) strategy by gradually increasing the difficulty of the training during the fine-tuning process. To examine the importance of this component, we also compare the performance with and without curriculum learning (Fig 4). Specifically, we train the diffusion model by randomly shuffling the training data and computing the absolute automatic evaluation results. As shown in Table 6, without curriculum learning, we observe a performance drop and results demonstrate the effectiveness of the CL."}, {"title": "5 Conclusions and Future Work", "content": "Text-to-image models have shown great potential in concept generation. In this work, we introduce a framework for novel concept design, which integrates concept ontology construction, data generation, and a T2I model contrastive training pipeline with curriculum learning technique. In addition, we propose a four-dimensional metric that evaluates the quality of generated concept images. Experimental results across three strong T2I models from both automatic and human evaluations demonstrate that our method significantly outperforms the competing baseline methods. Ablation studies also highlight the importance of our affordance sampling and curriculum learning techniques."}, {"title": "Limitations", "content": "Our work tackles an important yet underexplored problem of retaining functional coherence in AI for design using T2I models. While our model, in comparison to other state-of-the-art models, is able to generate more coherent and faithful images provided a set of affordances, e.g., brew, cut as in Fig. 1a, our work inherently relies on the human intuition to evaluate the novelty of the generated concepts. Although we try to alleviate the human bias and lack of coverage using LLM-as-a-judge for automatic evaluation, the question may persist. Moreover, although our concept ontology covers many different concept categories, it does not cover every plausible concept category in the real-world. It would be interesting to see follow-up works explore the direction of constructing a more diverse, richer concept ontology, which in turn would contribute to the generation of more novel concept designs."}, {"title": "Ethical Consideration", "content": "We acknowledge that our work is aligned with the ACL Code of the Ethics \u00b9 and will not raise ethical concerns. We do not use sensitive datasets/models that may cause any potential issues/risks."}]}