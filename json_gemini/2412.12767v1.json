{"title": "A Survey of Calibration Process for Black-Box LLMs", "authors": ["Liangru Xie", "Hui Liu", "Jingying Zeng", "Xianfeng Tang", "Yan Han", "Chen Luo", "Jing Huang", "Zhen Li", "Suhang Wang", "Qi He"], "abstract": "Large Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge. While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters. Black-Box LLMS, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints. Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs. We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration. Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps. Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated exceptional generalization abilities and contextual understanding across various application scenarios (Chang et al., 2024), particularly in open-domain question-answering tasks (Srivastava and Memon, 2024; Wang et al., 2023). However, when faced with ambiguous prompts or insufficient domain knowledge (Guu et al., 2020; Feng et al., 2024; Guo et al., 2017; Ji et al., 2023), LLMs often produce hallucinations (Kry\u015bci\u0144ski et al., 2019; Liu et al., 2024a; Manakul et al., 2023b; Perkovi\u0107 et al., 2024) or exhibit overconfidence in their responses (Zhao et al., 2021; Yang et al., 2024). While traditional solutions such as Reinforcement Learning with Human Feedback (RLHF) (Wang et al., 2024a) and parameter-level interventions (e.g., model weight modification, adapter tuning) have been proposed for white-box LLMs where model checkpoints and architectures are accessible (like Llama (Touvron et al., 2023), ChatGLM (GLM et al., 2024), and Vicuna (Chiang et al., 2023)), these parameter-dependent approaches prove ineffective for black-box LLMs such as GPT (Achiam et al., 2023), Claude (Caruccio et al., 2024), and Gemini (Team et al., 2023), which only allow input-output interactions through API calls. Given these limitations and the high computational costs of traditional methods, Confidence Estimation and Calibration Methods have emerged as promising alternatives (Ni et al., 2024; Mielke et al., 2022), offering cost-effective solutions applicable to both white-box and black-box models while maintaining scalability to large-scale deployments.\nIn recent years, Confidence Estimation and Calibration have frequently been discussed together, as the estimation of confidence is often influenced by the uncertainty in the model or data, and calibration methods help the model recognize its own knowledge limitations (Jiang et al., 2021; Srivastava et al., 2023). Calibration allows LLMs to adjust their confidence to more accurately reflect the quality of their outputs (Kuhn et al., 2023; Duan et al., 2023). For example, in the context of diagnosing rare diseases, LLMs may estimate a 95% confidence score to an incorrect response, while the accurate confidence, due to a lack of domain knowledge, should be closer to 40%. Calibration can identify this discrepancy and adjust the model's confidence to more accurately reflect the quality of the response, preventing overconfidence in generated responses (Ren et al., 2023; Geng et al., 2024; Tian et al., 2023). This process of Confidence Estimation, and Calibration to achieve well-calibrated confidence is referred to as the Calibration Process in this survey.\nA growing number of surveys have reviewed factors influencing LLMs Calibration Process. For instance, some studies have summarized how model parameters, training stages, or training data influence confidence and calibration (Zhu et al., 2023), while others investigate which confidence representations are more beneficial for calibration during the RLHF phase (Tian et al., 2023). Additionally, many studies have reviewed various confidence estimation and calibration methods on Language Models(LMs) and LLMs (Geng et al., 2024). However, existing surveys predominantly focus on white-box LLMs, where Calibration Process methods typically encompass logit-based calibration (Bi et al., 2024; Huang et al., 2023; Kuhn et al., 2023), additional parameter layers (Liu et al., 2024b), instruction tuning (Kapoor et al., 2024), and reinforcement learning techniques (Band et al., 2024).\nIn contrast to white-box LLMs, research on the Calibration Process for black-box LLMs lacks systematic analysis despite active development. black-box LLMs have recently garnered significant attention due to their exceptional performance across various real-world applications, especially as benchmark judge models for evaluating LLMs output quality and overall performance (Zheng et al., 2023). This role has been widely adopted by small and medium-sized enterprises for product development (Lin et al., 2023; Li et al., 2024b; Chen and Mueller, 2024). Given black-box LLMs' function as critical judge models, enhancing their sensitivity to uncertainty, improving output reliability, and fostering user trust are essential. The Calibration Process has demonstrated substantial potential in meeting these requirements (Wagner et al., 2024), further driving interest in black-box LLMs Calibration Process research (Si et al., 2022a; Ye and Durrett, 2022; Lin et al., 2023; Li et al., 2024b; Chen and Mueller, 2024).\nDespite this growing interest, no comprehensive study, to the best of our knowledge, has been dedicated specifically to the Calibration Process for black-box LLMs. This survey aims to bridge this gap by providing a systematic review and analysis of current research progress in this field. Our investigation reveals that existing Calibration Process methods for black-box LLMs can be categorized into two main approaches: the introduction of third-party proxy models (Ulmer et al., 2024; Shen et al., 2024) that partially transform black-box models into gray-box models, and post-processing techniques that operate solely on input-output data. The complexity of calibrating black-box LLMs is further exacerbated by the inherent challenge of inaccessible internal logic information (Guidotti et al., 2018; Jiang et al., 2018), highlighting the critical importance of this survey in advancing the field of Calibration Process.\nImportantly, since methods for black-box settings are often applicable to white-box as well,"}, {"title": "2 Calibration Process Methods of black-box LLMS", "content": "As shown in Figure 1, the Calibration Process is a cyclical and consists of two main steps: Confidence Estimation and Calibration, where the Calibration step is further divided into calibration and measurement phases. This process aims to obtain well-calibrated confidence values that accurately reflect the quality of the response and align with correctness. This Calibration Process can be represented by Formula 1:\n$V(Correct | Confidence \\upsilon) = \\upsilon$ (1)\nIn the Formula 1, $V$ represents the response correctness value, while $\\upsilon$ represents the confidence value. It is worth noting that while white-box and black-box LLMs share the same Calibration Process steps, many methods suitable for white-box models cannot be directly applied to black-box settings due to their inherent closed nature. Therefore, this section focuses on Confidence Estimation and Calibration methods applicable to black-box LLMs, with comprehensive coverage of measurement methods in the Calibration portion."}, {"title": "2.1 Confidence Estimation", "content": "For black-box LLMs, Confidence Estimation depends exclusively on input-output information, leading researchers to develop methods that extract reference information from model outputs through designed interactions and multiple queries. While some studies term this process \"Confidence Elicitation\" (Xiong et al., 2023; Huang et al., 2024b; Shrivastava et al., 2023), we maintain the term \"Confidence Estimation\".\nTwo main approaches exist: Consistency and Self-Reflection, both post-processing techniques that enable near-well-calibrated estimation without accessing model parameters. These methods can be employed individually or integrated into hybrid approaches, sometimes leveraging third-party proxy models. Furthermore, beyond the methods discussed in the following sections, additional latest methods are summarized in Table 1."}, {"title": "2.1.1 Consistency Methods", "content": "Consistency methods enable effective estimation of black-box LLMs by capturing the relationships and variations among model responses. These methods fall into two categories: similarity-based and entropy-based approaches. While similarity measurements directly yield confidence scores, entropy calculations indicate uncertainty levels that inversely reflect confidence, where confidence measures the stability of model responses and uncertainty captures their potential variability.\nIn similarity-based methods, for instance, self-CheckGPT (Manakul et al., 2023b) uses metrics like BERTScore (Zhang et al., 2019), MQAG (Manakul et al., 2023a), n-gram, NLI (He et al., 2021), or GPT-3 (text-davinci-003) prompts (Ye et al., 2023) to evaluate the similarity between response samples of the same query and thus derive the confidence values. Wightman et al. (2023) proposed a framework to assess the similarity between responses generated by different prompts containing the same question, introducing a prompt generation mechanism that calculates confidence values based on the frequency of responses across multiple prompts. They demonstrated that more prompts improve confidence calibration. Pedapati et al. (2024) introduced six methods for expanding output variable space in black-box models, namely Stochastic Decoding, Split Response Consistency, Paraphrasing, Sentence Permutation, Entity Frequency Amplification, and Stop-word Removal. The lexical and semantic similarity features between responses generated by each method are then extracted and input into logistic regression models to map these features to reliable confidence values.\nIn entropy-based methods, Lin et al. (2023) introduced a framework for uncertainty quantification using methods such as semantic set quantity, graph Laplacian eigenvalues, and degree matrix to measure uncertainty among response samples. Nikitin et al. (2024) proposed a more comprehensive approach, first calculating semantic similarity between response samples using a PSD semantic kernel, constructing a semantic graph with response samples as nodes and similarities as edges, and finally quantifying uncertainty using von-Neumann entropy. Tonolini et al. (2024) propose a Bayesian Prompt Ensembles method that assigns weights to semantically equivalent prompts based on their inherent uncertainty, integrating them to compute a lower bound on model error and produces near-well-calibrated uncertainty estimates."}, {"title": "2.1.2 Self-Reflections Methods", "content": "In black-box LLMs, self-reflection uses prompts to explore model knowledge boundaries and evaluate their own responses for reliable Confidence Estimation. Beyond this, self-reflection leverages LLMs' capability for multi-dimensional text evaluation, finding wide application in tasks like Reasoning (Yan et al., 2024), Translating (Wang et al., 2024b) and Refinement (Madaan et al., 2024). While self-reflection can incorporate external feedback from diverse sources to evaluate (Yan et al., 2024)\u2014such as critical models (Peng et al., 2023a), knowledge bases (Yao et al., 2022), or iterative self-reflection methods (Shinn et al., 2024)-in black-box LLMs settings, it primarily relies on the model's inherent self-reflective capabilities.\nFundamentally, self-reflection is a verbalization-based method (Lin et al., 2022; Tian et al., 2023). For example, Lin et al. (2022) designed prompts with a suffix \"Confidence (0-1):\" to allow the LLM to output its confidence in the original response. In practice, self-reflection prompts enable LLMs to reflect from multiple perspectives and generate confidence closer to well-calibrated values. The core of these methods lies in designing effective reflective questions and constructing feedback mechanisms. For instance, Peng et al. (2023b) asked models to provide confidence values on a 0 to 100 scale, but experiments showed this often led to overestimated confidence. To address this, Chen and Mueller (2024) proposed reflective questions in the form of multiple-choice items, averaged the scores, normalized them, and subsequently mapped them to corresponding confidence intervals.\nIn recent methods, self-reflection not only provides confidence values but also enhances response quality. For instance, Li et al. (2024b) proposed a T\u00b3 prompting framework that first creates prompts to reflect on each candidate responses and generate argumentative explanations, then uses Top-K Verbalized Confidence (Lin et al., 2022; Tian et al., 2023) to estimate the K most probable responses and their probabilities, finally averaging the confidence scores from different explanations to determine the optimal response and its confidence. Zhang et al. (2024b) proposed a framework where LLMs generate multiple diverse solving perspectives for a problem, then contrast and self-reflect on them to identify discrepancies. Based on this reflection, LLMs correct errors and produce the optimal response along with a confidence value. Fang et al. (2024) proposed a reflective framework, CFMAD, comprising two steps: abduction generation and counterfactual debate. LLMs first assume each candidate answer is correct and generate supporting reasons. Subsequently, LLMs engage in counter-factual debates as critics and defenders, with judge LLMs evaluating the proceedings to determine the most reliable answer through confidence scoring."}, {"title": "2.1.3 Hybrid Methods", "content": "Although both consistency and self-reflection methods can be used to obtain near-well-calibrated confidence without accessing internal parameter states, making them well-suited for black-box LLMs and widely used for reducing hallucinations, they remain post-processing techniques. As such, they cannot alter the internal parameters and states of the model and are subject to the inherent biases within the LLMs, unable to correct major logical errors. To achieve better results, recent methods combine different strategies with consistency and self-reflection techniques. For example, Chen and Mueller (2024) proposed a dual-branch model that combines consistency and self-reflection methods. They generate multiple response samples, calculate similarity values, and design self-reflection questions to compute reflection values. These values are then normalized and combined to produce the final confidence values.\nMoreover, introducing third-party models to transform black-box model into gray-box model is another common approach in Confidence Estimation for black-box LLMs. However, studies have found that directly using a white-box model, like LlaMA (Touvron et al., 2023), to output token-level probabilities for the black-box model's response results in poor calibration due to generation differences (Manakul et al., 2023b). To address this issue, Shrivastava et al. (2023) first demonstrated that confidence values generated solely by proxy models are not always accurate. They then used confidence probabilities derived from self-consistency and verbalization methods to cross-validate with the confidence values produced by third-party proxy models, resulting in more reliable confidence estimates. Ulmer et al. (2024) further attempted to train a white-box model to adapt to the generation patterns of black-box LLMs by learning the mapping between responses' confidence and correctness, thus obtaining well-calibrated confidence values.\nAdditionally, many studies have introduced multiple LLMs to cross-validate and optimize responses across LLMs, ultimately combining various explanations to generate the final confidence score. For instance, Zhang et al. (2023) utilized multiple LLMs with cross-model consistency, where responses from other LLMs for the same query are used as references to calculate the confidence value for the target black-box LLMs. Feng"}, {"title": "2.2 Calibration", "content": "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced. This section discusses the subsequent steps required to achieve well-calibration following Confidence Estimation.\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function $f(c)$ to map the model's confidence $C_1, C_2,..., C_n$ to correctness. $f(c)$ need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n$f(c_1) \\leq f(c_2) \\leq \\leq f(c_n) \\text{ for } C_1 \\leq C_2 \\leq \\ldots \\leq C_n$ (2)\nThough flexible, Isotonic Regression is less effective in classification tasks as it doesn't directly modify the decision boundary.\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. (2018) introduced a multi-calibration method, ensuring not only overall alignment but also calibration across multiple subgroups $S_i$ through iterative alignment. The overall goal can be expressed as Formula 3.\n$\\min \\sum_{i=1}^{N} \\sum_{S_j \\subset N} (E[p_{con}(S_j)] \u2013 E[p_{corr}(S_j)])^2$ (3)\nWhile this method promotes fairness, it is computationally expensive and depends heavily on subgrouping strategies, usually relying on known features. Detommaso et al. (2024) extended this to LLMs by clustering prompts in embedding space to form semi-structured groups, then applying the Improved Grouped Histogram Binning algorithm for independent group-level calibration, ensuring well-calibrated confidence both within each group and overall. This approach is better suited to generative tasks in LLMs, where decision boundaries are inherently less defined.\nIn addition to post-processing calibration methods, some approaches introduce proxy models to transform black-box LLMs into gray-box LLMs to facilitate calibration. For example, Bayesian calibration Network (DeJong et al., 1996) is a classic method that builds a probabilistic model to represent the dependency between confidence $P_{con}$ and correctness $P_{corr}$. The Bayesian Network is trained on small batches of data to estimate the conditional probability distribution $P(p_{corr} | P_{con})$, which is then used to infer calibrated confidence for new samples. Similarly, K\u00fcppers et al. (2021) applied a Bayesian framework with Stochastic Variational Inference to model epistemic uncertainty in calibration. By optimizing the Evidence Lower Bound, they generated calibrated confidence through a posterior distribution $q(\\theta)$ of validation samples as follows Formula 4.\n$P(p_{corr} | p_{con}) = \\frac{E_{q(\\theta)}[log P(p_{con} | p_{corr}, \\theta)]P(p_{corr})}{P(p_{con})}$ (4)\nFurthermore, Ulmer et al. (2024) proposed achieving calibration for each subgroup by training an auxiliary model to minimize the mean squared error between confidence and correctness. This method uses sentence embeddings (such as Sentence-BERT (Reimers, 2019)) to cluster similar questions and uses the accuracy within each cluster as correctness labels. Ye and Durrett (2021) developed a calibration model using random forests to dynamically adjust confidence, and employed Local Interpretable Model-agnostic Explanations and SHapley Additive exPlanations to generate local explanations of model predictions, extracting"}, {"title": "2.2.1 Calibration Methods", "content": "Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced. This section discusses the subsequent steps required to achieve well-calibration following Confidence Estimation.\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function $f(c)$ to map the model's confidence $C_1, C_2,..., C_n$ to correctness. $f(c)$ need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n$f(c_1) \\leq f(c_2) \\leq \\leq f(c_n) \\text{ for } C_1 \\leq C_2 \\leq \\ldots \\leq C_n$ (2)\nThough flexible, Isotonic Regression is less effective in classification tasks as it doesn't directly modify the decision boundary.\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. (2018) introduced a multi-calibration method, ensuring not only overall alignment but also calibration across multiple subgroups $S_i$ through iterative alignment. The overall goal can be expressed as Formula 3.\n$\\min \\sum_{i=1}^{N} \\sum_{S_j \\subset N} (E[p_{con}(S_j)] \u2013 E[p_{corr}(S_j)])^2$ (3)\nWhile this method promotes fairness, it is computationally expensive and depends heavily on subgrouping strategies, usually relying on known features. Detommaso et al. (2024) extended this to LLMs by clustering prompts in embedding space to form semi-structured groups, then applying the Improved Grouped Histogram Binning algorithm for independent group-level calibration, ensuring well-calibrated confidence both within each group and overall. This approach is better suited to generative tasks in LLMs, where decision boundaries are inherently less defined.\nIn addition to post-processing calibration methods, some approaches introduce proxy models to transform black-box LLMs into gray-box LLMs to facilitate calibration. For example, Bayesian calibration Network (DeJong et al., 1996) is a classic method that builds a probabilistic model to represent the dependency between confidence $P_{con}$ and correctness $P_{corr}$. The Bayesian Network is trained on small batches of data to estimate the conditional probability distribution $P(p_{corr} | P_{con})$, which is then used to infer calibrated confidence for new samples. Similarly, K\u00fcppers et al. (2021) applied a Bayesian framework with Stochastic Variational Inference to model epistemic uncertainty in calibration. By optimizing the Evidence Lower Bound, they generated calibrated confidence through a posterior distribution $q(\\theta)$ of validation samples as follows Formula 4.\n$P(p_{corr} | p_{con}) = \\frac{E_{q(\\theta)}[log P(p_{con} | p_{corr}, \\theta)]P(p_{corr})}{P(p_{con})}$ (4)\nFurthermore, Ulmer et al. (2024) proposed achieving calibration for each subgroup by training an auxiliary model to minimize the mean squared error between confidence and correctness. This method uses sentence embeddings (such as Sentence-BERT (Reimers, 2019)) to cluster similar questions and uses the accuracy within each cluster as correctness labels. Ye and Durrett (2021) developed a calibration model using random forests to dynamically adjust confidence, and employed Local Interpretable Model-agnostic Explanations and SHapley Additive exPlanations to generate local explanations of model predictions, extracting"}, {"title": "2.2.2 Measurement Methods", "content": "In Calibration, measurement methods are needed to assess the distance between the confidence and correctness, ensuring that the confidence is well-calibrated. These methods are generally applicable, unaffected by the model's nature\u2014whether white-box or black-box\u2014or the specific task characteristics. Calibration measurement Methods are typically categorized into two types: Error-Based and Correlation-Based calculation methods. When using Error-Based methods, correctness $y$ and confidence $p$ must be in the same measurement space to directly evaluate the model's calibration ability for specific samples, with a primary focus on detecting overconfidence. Common methods include the ECE series (Naeini et al., 2015; B\u0142asiok and Nakkiran, 2023; Si et al., 2022b; Naeini et al., 2015) and Brier Score (Brier, 1950). The general formula is typically expressed as Formula 5:\n$Cal\\_Error = \\sum_{i=1}^{N} w_i\\cdot f (P_i, Y_i)$ (5)\nIn Formula 5, $i$ represents a sample from the dataset of size $N$, and $w$ is the weight assigned to the $i$-th sample. When using Correlation-Based methods, correctness $Y$ and confidence $P$ do not need to follow the same scoring scale, typically evaluating overall calibration performance. For example, in (Zhang et al., 2024a), calibration error is calculated by comparing the ranking of test samples based on confidence with the ranking based on FActScore (Min et al., 2023a) (i.e. correctness). This type of method is also known as \"Relative Confidence\" (Geng et al., 2024), Common methods include AUROC (Boyd et al., 2013) and AUARC (Nadeem et al., 2009). The general formula is typically expressed as Formula 6:\n$Cal\\_Error = \\int g(P,Y;\\theta) \\mathrm{d}D(P,Y)$ (6)\nIn Formula 6, $D$ is the joint distribution of $P$ and $Y$ for dataset. $g$ is the measurement function defined according to the specific calibration method, and controlled by the parameter $\\theta$. $\\theta$ defines the specific measurement rules of the method, such as ranking information, linear correlation, threshold, etc. The integral symbol $\\int$ represents a holistic measurement across all combinations of $(P,Y)$, providing a comprehensive evaluation of the calibration error over the dataset.\nIt is worth noting that in generation tasks and uncertainty estimation tasks, Correlation-Based calculations are often used, as they help assess calibration error in complex scenarios like long-form text generation and accommodate multiple correctness scoring standards (e.g., Factuality and Coherence metrics in Summarization tasks (Spiess et al., 2024b)). In classification tasks, Error-Based calculations are more popular as they help measure calibration error for each class and identify potential knowledge gaps. We summarize various calibration measurement Methods in Table 2, with further details available in the Appendix A.1."}, {"title": "3 Applications", "content": "The Calibration Process enhances both the reliability of LLMs in practical applications and the level of user trust. Based on different objectives, the major applications are categorized into two aspects: Risk Assessment and Mitigation focusing on objective safety goals, and Human-LLM Trust Enhancement and Optimization addressing subjective human perception goals.\nRisk Assessment and Mitigation. Black-box LLMs face challenges in high-risk domains like medical diagnosis and autonomous driving due to potential hallucinations and overconfidence (Liu et al., 2023; Xiao et al., 2022). The Calibration Process helps mitigate these risks, enabling practical deployments. For instance, Qin et al. (2024) proposed 'Atypical Presentations Recalibration' in healthcare, while Savage et al. (2024) demonstrated improved medical diagnosis through sample consistency calibration. In production environments, Zellinger and Thomson (2024) proposed HCMA, a framework for risk management based on uncertainty evaluations, while Shen and Kejriwal (2023) introduced the calibration method DwD to address compound decision risks.\nHuman-LLM Trust Enhancement and Optimization. In human-machine collaboration, user trust is critical and depends on LLM response quality (Do et al., 2024). Beyond factuality calibration (Zhao et al., 2024b), research focuses on aligning with user preferences. Steyvers et al. (2024) studied LLMs' confidence calibration based on human perception, while Tsai et al. (2024) developed preference-aligned decision-making processes for black-box LLMs. Do et al. (2024) enhanced user-LLM trust through calibrated Factuality Scores and Source Attributions, while Spiess et al. (2024a) demonstrated that calibration improves efficiency and accuracy in code generation tasks, a typical user-intensive application."}, {"title": "4 Future Direction", "content": "Developing Comprehensive Calibration Benchmarks. In the Calibration Process, the definition of correctness varies widely across different tasks, directly affecting the calibration objectives. For instance, some tasks focus on evaluating factual accuracy, while others prioritize dimensions like logical coherence, claim consistency, or human satisfaction. Moreover, in complex generative tasks, more granular and multi-dimensional calibration methods are often required to assess multiple aspects of the output. Therefore, developing a multi-dimensional, multi-stage benchmark that accommodates the complexity of diverse tasks is critical to improving model calibration performance.\nBias Detection and Mitigation for black-box LLMs. Bias detection and mitigation remain critical challenges in black-box LLMs. The inaccessibility of model internals makes it difficult to identify and address bias, imposing inherent limitations on both confidence estimation and calibration. This often leads to disproportionately high or low calibration errors for specific groups or tasks, undermining the model's reliability and fairness. Traditional methods have limited effectiveness in addressing bias within black-box settings, underscoring the necessity for tailored solutions. Addressing these challenges requires a focus on improving output transparency, integrating external calibration mechanisms, and leveraging user feedback for dynamic confidence adjustment. These strategies not only tackle the unique obstacles of black-box settings but also enhance the calibration performance and practical utility of LLMs, enabling more reliable and interpretable applications across diverse real-world scenarios.\nCalibration for Long-form Text. Calibrating long-form text poses significant challenges for both white-box and black-box LLMs. As the demand for LLMs grows, long texts in QA and other generative tasks often encompass multiple claims and facts, making calibration increasingly complex. Assessing whether confidence scores are well-calibrated becomes particularly difficult when only portions of the text are correct, and quality evaluation is further complicated by its inherently subjective nature. Addressing these challenges requires effective methods that incorporate human perception as a metric for correctness. Additionally, it is crucial to develop approaches tailored to black-box settings, where model parameters remain inaccessible."}, {"title": "5 Conclusion", "content": "In this survey, we comprehensively review the current research on calibration in black-box LLMs. Since the internal states of black-box LLMs are inaccessible, the available calibration methods are limited. However, due to the exceptional performance and wide application of black-box LLMs, calibration methods for these models have recently garnered significant attention. We begin by discussing how calibration can help mitigate Hallucinations and Overconfidence problems in black-box LLMs. Then, we systematically define the Calibration Process as consisting of two components: Confidence Estimation and Calibration, with the latter involving methods to measure calibration error. In the following sections, we delve into the challenges each component faces in black-box LLMs and introduce the latest research methods. Finally, we explore the applications where the Calibration Process can be beneficial in black-box LLMs and propose directions for future research. To the best of our knowledge, although numerous studies have explored calibration in black-box LLMs, no comprehensive survey has been conducted to summarize this body of work, highlighting the pioneering nature of our contribution."}, {"title": "Limitations", "content": "This survey has the following key limitations:\nScope Limitation. While our survey provides a detailed examination of Calibration Process methods applicable to black-box LLMs, it does not extensively cover approaches for white-box LLMs. As mentioned previously, the majority of existing calibration research focuses on white-box LLMs, meaning our survey may not encompass some well-known and widely recognized methods in the field. Nevertheless, conducting a survey specifically for black-box LLMs is still significant, as black-box methods are often applicable to white-box settings and may offer new perspectives for white-box LLMs Calibration Process research.\nCoverage Limitations We have made a concerted effort to comprehensively collect and summarize Calibration Process methods for black-box LLMs. However, as the Calibration Process often appears in many studies as an auxiliary optimization or evaluation tool rather than a central focus, some important works on calibration processes might have been overlooked."}, {"title": "Ethics Statement", "content": "This study involves reviewing published academic literature on Calibration Process for black-box LLMs. As it does not involve human subjects, experimental procedures, or sensitive datasets, there are no ethical concerns or risks. The research follows ethical guidelines for academic integrity, and the researchers declare no conflicts of interest. All data collection adheres to standard ethical principles for systematic literature reviews."}, {"title": "A Appendix", "content": "This section provides a comprehensive explanation of the Error-Based Methods and Correlation-Based Methods within calibration measurement Methods. Additionally, these methods are summarized in Table 2."}, {"title": "A.1.1 Error-Based Methods", "content": "In Error-Based calibration measurement Methods, the ECE (Expected Calibration Error) (Naeini et al., 2015) series of methods is most widely adopted. In the ECE (Naeini et al., 2015), the weight coefficient $w_i$ in the general error calculation Formula 5 can be defined based on the number of test samples or confidence interval weights, typically expressed as $\\frac{B_b}{N}$, where $B_b$ represents the $b$-th confidence interval, and $B_b$ denotes the number of samples within that interval. The error term $f(p_i, Y_i)$ quantifies the deviation between predicted confidence probabilities and actual correctness probabilities, typically expressed as $\\frac{1}{B_l}\\sum_{i \\in B_l} p_i - \\frac{1}{B_l}\\sum_{i \\in B_l} Y_i$. Furthermore, both Brier Score (Brier, 1950) and Negative Log-Likelihood (NLL) (Gneiting and Raftery, 2007) are commonly employed. In the Brier Score, all"}]}