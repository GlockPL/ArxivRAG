{"title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks", "authors": ["Zihan Wang", "Gim Hee Lee"], "abstract": "We introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks. Our g3D-LF processes posed RGB-D images from agents to encode feature fields for: 1) Novel view representation predictions from any position in the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying targets using multi-granularity language within the above-mentioned representations. Our representation can be generalized to unseen environments, enabling real-time construction and dynamic updates. By volume rendering latent features along sampled rays and integrating semantic and spatial relationships through multiscale encoders, our g3D-LF produces representations at different scales and perspectives, aligned with multi-granularity language, via multi-level contrastive learning. Furthermore, we prepare a large-scale 3D-language dataset to align the representations of the feature fields with language. Extensive experiments on Vision-and-Language Navigation under both Panorama and Monocular settings, Zero-shot Object Navigation, and Situated Question Answering tasks highlight the significant advantages and effectiveness of our g3D-LF for embodied tasks.", "sections": [{"title": "1. Introduction", "content": "Embodied agents seek to understand 3D environments, enabling interaction with environments and human by performing tasks such as Question Answering [4, 37, 40], Navigation [3, 6, 27, 28, 39, 62], etc. To this end, various 3D scene representation models tailored for embodied tasks have been proposed, including point cloud-based models [11, 22, 73], 3D occupancy [34], hybrid voxel [14], and feature fields [44, 49, 57, 64].\nFor multimodal embodied tasks in large-scale scenes, 3D representation models typically need: 1) generalization to unseen scenes, 2) construct and update representations in real time, and 3) open-vocabulary semantic space. The generalizable 3D feature fields provides the above advantages and has been widely explored across various embodied tasks. Unlike point cloud-based models that depend on complete and low-noise point clouds which are less robust, the implicit representations of the feature fields are derived from the 2D foundation model, preserving semantic expressiveness even with few-shot observations from 3D scenes. As shown in Figure 1, the feature fields model uses RGB-D images as input to encode and update implicit scene representations, which are then used to predict novel view, panorama and BEV map representations associated with language through volume rendering. These predicted representations can assist embodied tasks such as navigation planning [44, 57, 58], etc. However, several significant drawbacks remain in these feature fields models: 1) The supervision for the predicted representations comes from 2D foundation models, e.g., CLIP [45] and DINOv2 [42] greatly limits the understanding for 3D spatial relationships; 2) These models are trained without language supervision, resulting in a substantial gap with language semantics; 3) The large-scale representations, e.g., panorama and BEV map from feature fields is particularly challenging for long text understanding. These issues severely limit the potential of the feature fields model on language-guided embodied tasks.\nTo circumvent the above-mentioned issues, we introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks. We first curate and consolidate a large amount of 3D-language data from previous works [7, 23, 66] to train our g3D-LF model. These data include 5K indoor scenes and almost 1M language descriptions of multiple granularities. The text annotations include object categories, object characteristics, object relationships, and the spatial layout of the entire scene, which are employed to supervise multiscale encoders of the g3D-LF model. We then design our g3D-LF model to learn generalizable 3D-language feature fields. To this end, we employ multi-level contrastive learning for multi-scale encoders to align predicted representations and language across different scales. For the regional representation within the novel view, a contrastive loss is calculated across 1,883 indoor object categories. For the predicted novel view representation, both the CLIP visual representations and language are employed for contrastive training to balance generalization ability and language alignment. For large-scale panorama and BEV representations, we propose the fine-grained contrastive learning based on the affinity matrix to achieve long text understanding.\nThe pre-trained g3D-LF model is subsequently evaluated on various embodied tasks, including vision-and-language navigation (monocular setting [58] and panorama setting [57]), zero-shot object navigation [62], and situated question answering [37], gains significant performance improvements. In this work, our main contributions include:\n\u2022 We organize a large-scale 3D-language dataset to train the feature fields model.\n\u2022 This work proposes the Generalizable 3D-Language Feature Fields (g3D-LF) with a multi-level contrastive learning framework to align the multi-scale representations of feature fields with multi-granularity language.\n\u2022 Our proposed g3D-LF model improves multiple baseline methods to state-of-the-art performance across various embodied tasks, thus validating the potential of our generalizable feature fields for Embodied AI."}, {"title": "2. Related Work", "content": "Generalizable 3D Feature Fields. The neural radiance field (NeRF) [41] has gained significant popularity in various AI tasks, which predicts the RGB image from an arbitrary viewpoint in a 3D scene. Furthermore, some works leverage NeRF-based methods to predict novel view representations instead of RGB values, enabling 3D semantic segmentation [51] and 3D language grounding [24]. However, these methods with implicit MLP networks can only synthesize novel view representations in seen scenes, which makes it difficult to generalize to unseen large-scale scenes and adapt to many embodied AI tasks (e.g., navigation). To this end, some works [44, 50, 57] attempt to encode 2D visual observations into 3D representations (called Generalizable 3D Feature Fields) via the depth map. Through volume rendering [41], these models decode novel view representations from the feature fields and align them with open-world features (e.g., CLIP embeddings [45]). The 3D feature fields can generalize to unseen scenes, enabling real-time construction and dynamic updates. However, the drawback of these models lies in the fact that the supervision of their predicted representations comes from 2D visual models, which limits their performance in language-guided embodied tasks. Our work offers a feasible approach to training the 3D feature fields model with large-scale 3D-language data.\nVision-and-Language Navigation. Vision-and-Language Navigation (VLN) [3, 9, 19, 27, 43, 54, 69] requires the agent understand complex natural language instructions and navigate to the described destination using low-level actions, e.g., turn left 15 degrees, turn right 15 degrees, or move forward 0.25 meters. To address inefficiencies and poor performance in atomic action prediction, some works [20, 26, 58] develop waypoint predictors to generate several candidate waypoints around the agent. The navigation policy model can then select the optimal waypoint as the next sub-goal and execute atomic actions to move, greatly enhancing planning efficiency. In this context, how to represent waypoints and carry out planning have become critical. Some works use a topological map [2, 10] or BEV map [1, 32, 56] to represent semantic relationships between waypoints, while some [57, 58] explore feature fields to predict waypoint representations of novel views and improve navigation planning. Our g3D-LF model further improves the performance of methods using feature fields.\nZero-shot Object Navigation. In object-goal navigation [6, 47, 68], an agent is tasked with locating a specified object within indoor environments. Typically, reinforcement learning [72] is used to train a policy network that predicts actions, while object detection [35, 52] or segmentation models [18, 25, 65] help identify the object. However, these navigation models are often limited to specific objects, making open-vocabulary navigation challenging and hindering generalization in real-world applications [17]. To address this issue, zero-shot navigation methods have emerged [15, 39, 62, 71], leveraging Vision-and-Language Models (VLMs) [30, 31, 45] to identify potential directions or areas containing the target, followed by using the pre-trained pointgoal navigation models [59] to search the potential areas. Considering that general 2D VLMs are not fully suited for indoor 3D environments and to the best of our knowledge, we are the first to attempt using the indoor 3D feature fields model for zero-shot object navigation.\nSituated Question Answering. The Embodied Question Answering tasks [4, 13, 40] require the agent to observe the 3D environment and answer questions from humans. Furthermore, Situated Question Answering [37] requires advanced 3D spatial understanding of the agent to answer the question and to interpret and locate the position and orientation of the textual description. Compared to previous works [14, 22, 23] using point clouds, we only use RGB-D images to encode feature fields and leverage their multi-scale representations for localization and question answering."}, {"title": "3. Our Method", "content": "3.1. 3D-Language Data\nWe prepare a large-scale 3D-language dataset to align the representations of the feature fields with language. Our dataset includes about 5K 3D indoor scenes, mainly sourced from the single-room scans ScanNet [12], multi-room house scans of the Habitat-Matterport 3D dataset (HM3D) [46, 60], and the photo-realistic multi-room scenes of Structured3D [70]. The total number of language annotations is close to one million, which are mainly sourced from the SceneVerse dataset [23]. SceneVerse uses 3D scene graphs and large language models (LLMs) to automate high-quality object-level and scene-level descriptions. The annotations also includes the large set of human-annotated object referrals [7].\nWe organize the dataset as follows to streamline feature fields training: 1) For each 3D scene, the agent can observe numerous RGB-D images and its corresponding poses as inputs. 2) An instance-level point clouds mark each instance in the scene with an instance ID which can be used to retrieve associated language descriptions from the database. It is thus easy to get instances that are near any given point in the 3D scene and obtain their language descriptions. This enables the training code to efficiently obtain language annotations for specific regions within a novel view or a BEV map.\n3.2. 3D-Language Feature Fields\nFeature Fields Encoding. As shown in Figure 2, our g3D-LF model follows HNR [57] to take a posed RGB image as input and uses the CLIP image encoder to extract fine-grained visual features ${g_{t,i}}_{i=1}^{n} \\in \\mathbb{R}^{768}$. $g_{t,i}$ denotes the i-th feature patch of the CLIP feature map extracted from t-th frame observed by the agent. We then map $g_{t,i}$ to the corresponding 3D world coordinates ${P_{t,i}}_{i=1}^{n}$ using the depth map and camera parameters.\nFor each feature $g_{t,i}$, the observed horizontal orientation $O_{t,i}$ and the regional size $s_{t,j}$ are also calculated and stored to enhance the spatial representation. The set of feature points $M$ can therefore be dynamically updated as:\n$M_t = M_{t-1} \\cup \\{[g_{t,i}, P_{t,i}, O_{t,i}, s_{t,i}]_{i=1}^{n}\\}$.\nRay-View-Panorama Encoding. The MLPview network aggregates nearby features within feature fields $M$ and encode their spatial information [57] (i.e., relative positions and relative directions) to predict semantic representations $r \\in \\mathbb{R}^{768}$ and volume density $\\sigma \\in \\mathbb{R}^{1}$ at any point from any direction in the continuous fields.\nFor each novel view, our g3D-LF model generates a feature map $R \\in \\mathbb{R}^{12 \\times 12 \\times 768}$ by predicting subregion features through volume rendering within feature fields. The model samples N points along the ray from the camera position to each subregion center to search for the k-nearest features and predicting volume density on and latent representation $r_n$, which then are composited into a subregion feature:\n$R_{(u,v)} = \\sum_{n=1}^{N} \\tau_n (1 - exp(-\\sigma_n \\Delta_n))r_n$,\nwhere\n$\\tau_n = exp(-\\sum_{i=1}^{n-1} \\sigma_i \\Delta_i)$.\nHere, $\\tau_n$ represents volume transmittance and $\\Delta_n$ is the distance between sampled points. $R_{(u,v)}$ denotes the regional feature at the u-th row and v-th column of the novel view feature map R. We integrate context of the surrounding by feeding the feature map R together with a learnable view token $V \\in \\mathbb{R}^{768}$ into the transformer-based view encoder to obtain the encoded R' and novel view representation V' that represent the entire novel view. Furthermore, to reason relationships across multiple views within a panorama, our g3D-LF model predicts 12 novel views ${V_i}_{i=1}^{12}$ around the viewpoint at 30-degree intervals and combines them into a transformer-based panorama encoder to obtain ${V_i}_{i=1}^{12}$.\nRay-BEV Encoding. The novel view and panorama representations are insufficient for larger-scale scene understanding. To circumvent this problem, we propose to construct BEV map representation via our g3D-LF as shown in Figure 2. Unlike novel view prediction where rays are emitted from the viewpoint along the viewing cone, the rendering rays for the BEV map are rendered vertically from top to bottom. The starting point of the rendered ray is set slightly below the ceiling to avoid being blocked.\nSpecifically, the MLPBEV network is used to aggregate the nearest feature points to the sampled point and predict its semantic representation $\\hat{r}_n$ and volume density $\\hat{\\sigma}_n$ in the continuous field. Subsequently, the ray representation $\\hat{R}_{(h,w)} \\in \\mathbb{R}^{768}$ can be obtained using the similar volume rendering method of Equation 2, where (h, w) denotes the h-th row and w-th column of the BEV map $\\hat{R} \\in \\mathbb{R}^{168 \\times 168 \\times 768}$. To cover the large scene, the BEV map R encompasses a"}, {"title": "3.3. Multi-level Contrastive Learning", "content": "Balanced Object-level Alignment. We apply contrastive supervision using an object vocabulary $O \\in \\mathbb{R}^{1883 \\times 768}$ that spans 1,883 indoor object categories for supervision of the MLPview and MLP BEV networks to predict latent features in feature fields. For ray representations R obtained via volume rendering, the cosine similarities ${CosSim(R, O_i)}_{i=1}^{1883}$ are computed with each vocabulary embedding. The training objective is to maximize and minimize similarity for the correct and other object category, respectively, i.e.:\n$L_{object} = CrossEntropy({CosSim(R, O_i)/\\tau}_{i=1}^{1883}, O_{gt})$,\nwhere $O_{gt}$ denotes the ground-truth category and $\\tau$ is the temperature coefficient for contrastive learning. Similarly, the object alignment loss for the ray representations R of the BEV map denoted as Lobject can also be calculated.\nWe notice the network struggles to recognize smaller objects such as the lamp due to the dominance of some objects (e.g., floor and walls) leading to long-tailed distribution in the indoor scenes. To address this issue, we implement a balanced loss that emphasizes harder-to-recognize objects. Specifically, the weight of loss for the rays of top 10% cross entropy are significantly increased using a scaling factor \u03b1 for ray representations within the novel view or BEV map. In short, rays with higher cross entropy indicate harder-to-recognize objects and therefore have a higher loss weight.\nFine-grained Contrastive for Long Text. To enable our g3D-LF model to understand object relationships and spatial layouts, we propose a fine-grained contrastive learning method for long text alignment. As shown in Figure 2, our g3D-LF aligns the BEV features in a window (e.g., 5 \u00d7 5) with the long text features to enhance the representation of the BEV map for spatial semantics. Specifically, centered on an instance, the BEV features ${R_i}_{i=1}^{25}$ within the window are associated with L word features ${W_l}_{l=1}^{L}$ from the CLIP text encoder through an affinity matrix A:\n$A_{(l,i)} = CosSim(R_i, W_l)/\\tau$.\nThe highest L similarity scores (equal to the number of words) are extracted from the affinity matrix A, and their average is used as the fine-grained similarity score between the BEV window and the long text features:\n$FineSim({R_i}_{i=1}^{25}, {W_l}_{l=1}^{L}) = Avg(Topk(A, L))$.\nDenoting the BEV features within the i-th window as $B_i$ and the j-th text features as $T_l$, the fine-grained contrastive learning loss can be calculated as:\n$L_{long-text} = \\frac{1}{J} \\sum_{j=1}^{J} CrossEntropy({FineSim(B_i, T_l)}_{i=1}^{25}, j) + \\frac{1}{L} \\sum_{l=1}^{L} CrossEntropy({FineSim(T_l, B_i)}_{i=1}^{25}, l)$.\nSimilarly, our g3D-LF model performs fine-grained contrastive learning between encoded panoramic representations ${V_i}_{i=1}^{12}$ and long-text features ${W_l}_{l=1}^{L}$ to compute the fine-grained contrastive loss $L_{long_text}$.\nCLIP Knowledge Distillation. Since the 3D-language data is orders of magnitude smaller than image-language data (millions vs. billions [45]), our g3D-LF model still distills visual features from CLIP model [45] to ensure robust generalization. Specifically, our g3D-LF uses CLIP features extracted from the ground-truth novel view or corresponding region image for contrastive supervision on the predicted new view representation V', the panorama representation V, and the BEV map representation $R_i'$, i.e.:\n$L_{view_clip} = \\frac{1}{I} \\sum_{i=1}^{I} CrossEntropy({CosSim(V', V_j^{gt})/\\tau}_{j=1}^{J}, i)$,\nwhere $V_j^{gt}$ denotes the ground truth CLIP feature for j-th novel view representation V. Similarly, the contrastive loss $L_{pano_clip}$ for the panoramic representation and $L_{bev_clip}$ for the BEV map can also be computed."}, {"title": "3.4. Embodied Tasks", "content": "To verify the effectiveness of our g3D-LF model for embodied tasks, we integrate the predicted representations from our model into existing baseline methods and evaluates performance on Vision-and-Language Navigation, Zero-shot Object Navigation, and Situated Question Answering tasks.\nVision-and-Language Navigation. We evaluate the g3D-LF model on VLN tasks with two settings. The first setting is with the monocular camera, which only allows the agent to observe the forward-facing view. As shown in Figure 3, the VLN-3DFF [58] is a monocular VLN model that predicts candidate waypoints around the agent using a semantic map, and predicts each candidate's representation with generalizable feature fields [57] and then selects the optimal waypoint to move through a cross-modal graph encoder [2, 10]. Based on this baseline method, we incorporate novel view representations from our g3D-LF model and input the BEV map into the cross-modal graph encoder following GridMM [56] to enhance spatial layout understanding. The second setting is with the panorama camera, in which the agent can observe 12 RGB-D view images within the panorama. Following HNR [57], a waypoint predictor [20] is used to predict candidate waypoints, and our g3D-LF model generates panorama representations of these waypoints for navigation planning.\nZero-shot Object Navigation. As shown in Figure 4, unlike the baseline method VLFM [62] that uses the 2D foundation model BLIP-2 [30] to calculate the similarity between the target object and visual observations to construct the value map, we use our g3D-LF to predict the value of potential regions. Although the monocular agent can only observe the forward view, our g3D-LF predicts 12 novel view feature maps surrounding the agent within panorama based on historical observations, and calculates max similarity in feature map with the target object. The text features of the target object are also used to calculate the similarity with each region representation on the BEV map to obtain a larger-scale value map. Combining these two value maps, the navigation agent prioritizes traveling to the candidate waypoint with the highest similarity score.\nSituated Question Answering. A three-stage framework is shown in Figure 5, where we use our g3D-LF to train three transformer-based decoders for position, orientation and answer predictions. First, the Localization Decoder predicts the heatmap for location of the textual description based on the BEV map. Our g3D-LF model generates the panorama representations around the predicted location, which are then processed by the Orientation Decoder to predict the orientation. Finally, the textual description, question, BEV map, and panorama representations are fed into the Answer Decoder to generate the final answer."}, {"title": "4. Experiments", "content": "4.1. Experiment Setup and Metrics\ng3D-LF Pre-training. We pre-train our g3D-LF model shown in Figure 2 on 5K 3D scenes. During training, 30 frames are uniformly sampled from the RGB-D video of each scene in the ScanNet [12] dataset to construct the feature fields, with an additional frame randomly selected as the novel view for prediction. The g3D-LF then predicts the panorama representation and BEV map centered on the camera of this novel view. For each ray in the novel view or BEV map, the corresponding instance ID can be searched by calculating the nearest instance point to the rendered surface within the annotated instance point cloud. The language annotations of the novel view, panorama, and BEV map can thus be obtained by retrieving language annotations with their instance IDs from the database for training. Due to the limited number of images per scene (fewer than 20), we use all available images from the Structured3D [70] dataset for training. We follow HNR [57] for the HM3D [46, 60] dataset using the Habitat simulator [48] to randomly sample navigation trajectories and the observed RGB-D images to predict the novel views and panoramas around candidate waypoints, and construct the BEV map centered on the agent. The multi-level contrastive losses described in Section 3.3 are utilized to optimize the g3D-LF model.\nFinally, we combine scenes from all datasets and pretrain our g3D-LF model for 50K episodes (about 10 days) on two RTX 6000 Ada GPUs. To ensure fair comparisons on downstream tasks, all training data only includes the train split, the val and test splits are removed.\nVision-and-Language Navigation. We evaluate the VLN model on the VLN-CE dataset [27] in both monocular [58] and panorama [57] settings. R2R-CE is collected based on the Matterport3D [5] scenes with the Habitat simulator [48]. The R2R-CE dataset includes 5,611 trajectories divided into train, validation seen, validation unseen, and test unseen splits. Each trajectory has three English instructions with an average path length of 9.89 meters and an average instruction length of 32 words. Several standard metrics [3] are used to evaluate VLN performance: Navigation Error (NE), Success Rate (SR), SR given the Oracle stop policy (OSR), Success Rate weighted by normalized inverse Path Length (SPL).\nZero-shot Object Navigation. For object navigation, we evaluate our approach using the Habitat simulator [48] on the validation splits of two different datasets HM3D [46] and MP3D [5]. The HM3D validation split contains 2,000 episodes across 20 scenes and 6 object categories. The MP3D validation split contains 2,195 episodes across 11 scenes and 21 object categories. The main metrics [3] include Success Rate (SR) and Success Rate weighted by normalized inverse Path Length (SPL).\nSituated Question Answering. Following ScanNet [12], the SQA3D dataset comprises 20.4k descriptions and 33.4k diverse questions, which is splited into train, val, and test sets. The main metric is the Exact Match (EM@1) of the answer. Additionally, for localization evaluation, Acc@0.5m and Acc@1.0m metric means the prediction is counted as correct when the predicted position is within 0.5 meter and 1.0 meter range to the ground truth position. The Acc@15\u00b0 and Acc@30\u00b0 metric means the prediction is counted as correct when the prediction orientation is within 15\u00b0 and 30\u00b0 range to the ground truth orientation."}, {"title": "4.2. Comparison with SOTA Methods", "content": "As shown in Table 1 and Table 2, we evaluate the VLN performance of our g3D-LF model on the R2R-CE dataset in both monocular and panorama settings, respectively. Table 1 shows that our g3D-LF significantly outperforms previous monocular VLN methods on the Success Rate (SR) metric, even compared to LLM-based methods such as NaVid [67] and InstructNav [36]. Compared to the panorama setting, monocular VLN has the advantage of being compatible with a broader range of real-world monocular robots. Our g3D-LF model overcomes the limitations of monocular cameras, enhancing the multi-view and BEV perception capabilities of the agent for monocular VLN.\nWe follow HNR [57] to perform lookahead exploration through predicted candidate waypoint representations for the panorama setting in Table 2. Although the results show minor performance gains and the advanatges are not as pronounced as its monocular counterpart in Table 1, our g3D-LF model still achieves SOTA performance on the SPL metric and demonstrated competitive results on the SR metric.\nIn Table 3 for the Zero-shot Object Navigation, our g3D-LF achieves SOTA performance in the SPL metric and achieves competitive results in the SR metric. Notably, our g3D-LF is the only method that queries targets using feature fields instead of VLM. Replacement of BLIP-2 [30] in VLFM [62] with g3D-LF improves the navigation success rate (SR) by nearly 3%. Although the MP3D benchmark includes some targets outside the g3D-LF object vocabulary, our model still performs well, demonstrating strong generalization. Compared to methods using LLM: InstructNav [36] and SG-Nav [61], our g3D-LF also offers significant advantages in response time and computational cost.\nIn Table 4 for the Situated Question Answering task, our g3D-LF achieves good localization performance in metrics of Acc@0.5m, Acc@1m, Acc@15\u00b0 and Acc@30\u00b0. Although our performance on the answering accuracy (EM@1) is significantly lower than that of LLM-based methods: LEO [22] and Scene-LLM [14], it is worth noting that our g3D-LF only uses images as input without low-noise 3D point clouds. This actually offers a significant advantage in agent-centered embodied tasks since it is more adaptable to unseen dynamic real-world environments, where the low-noise point clouds are difficult to collect."}, {"title": "4.3. Ablation Study", "content": "Perfromance impact of g3D-LF on embodied tasks. In row 1 of Table 5, the performance of monocular VLN and object navigation drops significantly without representations from g3D-LF. In this setting, the VLN model only uses the CLIP features from the forward-facing view with features of all other directions set to zero. The object navigation model uses BLIP-2 [30] instead of g3D-LF to construct the value map. Examining rows 2 and 3 shows that removing either the novel view or the BEV map reduces the performance of both two tasks, highlighting the role of each g3D-LF module.\nNovel views are crucial for monocular VLN. As shown in row 1 and row 2 of Table 5, the novel view representations significantly boost VLN performance by overcoming the narrow perception of the monocular camera [58], enabling the monocular agent to have panoramic perception capabilities. To some extent, this confirms that novel view prediction is a very important and valuable capability for monocular agents. Based on this capability, the g3D-LF model predicts the novel view representations of candidate waypoints around the agent to construct the topological map for better navigation planning.\nObject navigation requires balancing local and global targets. As shown in row 3 of Table 5, we observe that relying solely on BEV representation significantly reduces object navigation performance. This decline occurs because the global value map from the BEV map fails to select optimal nearby waypoints if the target is far from these waypoints. In this case, a local value map constructed from novel views is also essential to identify the optimal short-term goal, i.e., nearby waypoints around the agent.\nPre-training is essential for generalizable feature fields model. Table 6 analyzes the impact of multi-level contrastive pre-training on downstream embodied tasks. As shown in row 1 of Table 6, the performance on VLN and object navigation drops significantly when the model is optimized solely by the navigation loss [2] without pre-training.\nBoth CLIP distillation and language supervision are indispensable. For row 3 of Table 6 without supervision from the CLIP visual features, the VLN performance lags behind the model distilled by CLIP. This suggests that millions of language annotations are still far from sufficient for g3D-LF pre-training, and distilling representations from 2D foundation models to enhance semantic generalization remains necessary. However, in Table 6, we can also see that language supervision significantly improves g3D-LF performance on embodied tasks, the model performs poorly in row 2 when using only CLIP distillation.\nLong-tail distribution limits object-level semantic learning. As shown in row 4 of Table 6, the performance of object navigation decreases drastically without the balanced loss mentioned in Section 3.3. The long-tail distribution of object categories in indoor environments leads models to overlook of rare or small objects such as towels and cups, significantly limiting the ability of our g3D-LF model to query target objects. Fortunately, row 6 of Table 6 shows that the balanced object alignment works well by balancing the weight for loss of hard-to-recognize objects.\nFine-grained contrastive benefits long text understanding. In the row 5 of Table 6, we use the [SEP] feature (single vector) from the CLIP text encoder to supervise panorama and BEV representations. However, compared to the fine-grained contrastive learning in row 6, compressing long text into a coarse vector significantly limits g3D-LF's performance on long-text understanding tasks such as VLN. As shown in Figure 2, fine-grained contrastive learning between long texts and windows within the BEV map helps g3D-LF understand spatial layouts, overcoming the limitations of semantic representation in large-scale scenes.\ng3D-LF enables real-time inference. As shown in Table 7, we calculate the inference time of our g3D-LF model on the val unseen split of the R2R-CE dataset in the VLN task. Our g3D-LF achieves novel view volume rendering at 73.6 FPS, which slightly drops to 71.1 FPS when rays are further encoded by the View Encoder. For a panorama containing 12 views, the inference speed is 5.9 FPS. Due to the large rendered range, our g3D-LF renders BEV maps at 6.3 FPS, which drops slightly to 6.1 FPS with the BEV Map Encoder.\nOur g3D-LF model adopts the same sparse sampling strategy as in HNR [57], where the MLP network is only used to render sampled regions containing feature points nearby, while skipping empty regions. This reduces rendering time by over 10 times, enabling real-time embodied tasks."}, {"title": "5. Conclusion", "content": "In this work, we propose Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language data for embodied tasks. We organize the first large-scale 3D-language dataset for feature fields training, demonstrating the feasibility of using generalizable feature fields for large-scale scene understanding, i.e., panorama and BEV. Our proposed g3D-LF leverages multi-level contrastive learning strategies such as balanced object semantic alignment, fine-grained text alignment, and CLIP knowledge distillation to optimize generalized feature fields. More importantly, the value of g3D-LF has been widely evaluated in multiple embodied tasks. We believe that our g3D-LF can provide sufficient inspiration for subsequent research on feature fields and embodied AI.\nLimitations and future works. Our g3D-LF still has some limitations with significant potential for future research: 1) g3D-LF cannot be adapted to dynamic environments, where objects or people are moving in real time. This requires better update strategies for implicit representations. 2) g3D-LF has not been evaluated on dynamic tasks such as object manipulation. 3) The scale and quality of 3D-language data used for training g3D-LF remain limited, which essentially restricts the ability of generalizable feature field models. 4) The 3D feature fields combined with LLM can enable better text generation. These may become the guiding directions for the next phase of generalizable feature fields."}, {"title": "A. More Details of the g3D-LF Model", "content": "Model structure. Figure 6 illustrates the structure of main modules in the g3D-LF model. Compared to HNR [57", "57": "if no nearby feature points are found within a sampled point's search radius, the latent feature and volume density are set to zero. The rendered ray is uniformly sampled from 0 to 10 meters, and the number of sampled points is set as 501. After volume rendering, the number of rays within a novel view is set as 1"}]}