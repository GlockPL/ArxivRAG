[{"title": "Graph Neural Network Causal Explanation\nvia Neural Causal Models", "authors": ["Arman Behnam", "Binghui Wang"], "abstract": "Graph neural network (GNN) explainers identify the impor-\ntant subgraph that ensures the prediction for a given graph. Until now,\nalmost all GNN explainers are based on association, which is prone to\nspurious correlations. We propose CXGNN, a GNN causal explainer via\ncausal inference. Our explainer is based on the observation that a graph\noften consists of a causal underlying subgraph. CXGNN includes three\nmain steps: 1) It builds causal structure and the corresponding structural\ncausal model (SCM) for a graph, which enables the cause-effect calcula-\ntion among nodes. 2) Directly calculating the cause-effect in real-world\ngraphs is computationally challenging. It is then enlightened by the re-\ncent neural causal model (NCM), a special type of SCM that is trainable,\nand design customized NCMs for GNNs. By training these GNN NCMS,\nthe cause-effect can be easily calculated. 3) It uncovers the subgraph that\ncausally explains the GNN predictions via the optimized GNN-NCMs.\nEvaluation results on multiple synthetic and real-world graphs validate\nthat CXGNN significantly outperforms existing GNN explainers in exact\ngroundtruth explanation identification\u00b9.", "sections": [{"title": "1 Introduction", "content": "Graph is a pervasive data type that represents complex relationships among\nentities. Graph Neural Networks (GNNs) [6, 13, 15, 44], a mainstream learning\nparadigm for processing graph data, take a graph as input and learn to model\nthe relation between nodes in the graph. GNNs have demonstrated state-of-the-\nart performance across various graph-related tasks such as node classification,\nlink prediction, and graph classification, to name a few [42].\nExplainable GNN provides a human-understandable way of the prediction\noutputted by a GNN. Given a graph and a label (correctly predicted by a GNN\nmodel), a GNN explainer aims to determine the important subgraph (called\nexplanatory subgraph) that is able to predict the label. Various GNN explanation\nmethods [3,5,8,9,14,18,21,26\u201328,30,33,39,40,46-50] have been proposed, wherein\nalmost all of them are based on associating the prediction with a subgraph that"}, {"title": "2 Related Work", "content": "Association-based explainable GNN: Almost all existing GNN explainers\nare based on association. These methods can be roughly classified into five types.\n(i) Decomposition-based methods [8, 27] consider the prediction of a GNN model\nas a score and decompose it backward layer-by-layer until it reaches the input.\nThe score of different parts of the input can be used to explain its importance\nto the prediction. (ii) Gradient-based methods [3, 27] take the gradient of the\nprediction with respect to the input to show the sensitivity of a prediction to\nthe input. The sensitivity can be used to explain the input for that prediction.\n(iii) Surrogate-based methods [5, 14, 26, 33, 50) replace the GNN model with a\nsimple and interpretable surrogate one. (iv) Generation-based methods [18, 30,\n40,47] use generative models or graph generators to generate explanations. (v)\nPerturbation-based methods [9, 21, 28, 39, 46, 48, 49] aim to find the important\nsubgraphs as explanations by perturbing the input. State-of-the-art explainers\nfrom (iii)-(iv) show better performance than those from (i) and (ii).\nCausality-inspired explainable GNN: Recent GNN explainers [7,19,20, 32,\n38,41] are motivated by causality. These methods are based on a common obser-\nvation that a graph consists of the causal subgraph and its non-causal counter-\npart. For instance, OrphicX [20] uses information-theoretic measures of causal\ninfluence [2], and proposes to identify the (non)causal factors in the embedding\nspace via information flow maximization. CAL [32] introduces edge and node\nattention modules to estimate the causal and non-causal graph features.\nCXGNN vs. causality-inspired explainers. The key difference lies in CXGNN\nfocuses on identifying the causal explanatory subgraph by directly quantifying the\ncause-and-effect relations among nodes/edges in the graph. Instead, causality-\ninspired explainers are inspired by causality concepts to infer the explanatory\nsubgraph, but they inherently do not provide causal explanations."}, {"title": "3 Preliminaries", "content": "In this section, we provide the necessary background on GNNs and causality to\nunderstand this work. For brevity, we will consider GNNs for graph classification.\nNotations: We denote a graph as G = (V, E), where V and E are the node set\nand edge set, respectively. v \u2208 V(G) represents a node and eu,v \u2208 E(G) is an\nedge between u and v. Each graph G is associated with a label yg \u2208 y, with y\nthe label domain. An uppercase letter X and the corresponding lowercase one x\nindicate a random variable and its value, respectively; bold X and x denote a set\nof random variables and its corresponding values, respectively. We use Dom(X)\nto denote the domain of X, and P(X) as a probability distribution over X."}, {"title": "4 GNN Causal Explanation via NCMs", "content": "In this section, we propose our GNN causal explainer, CXGNN, for explaining\ngraph classification. Our explainer also utilizes the common observation that a\ngraph consists of a causal subgraph and a non-causal counterpart [7,19,20,32,38,\n41]. The overview of CXGNN is shown in Figure 10 in Appendix and all proofs\nare deferred to Appendix C."}, {"title": "4.1 Overview", "content": "Given a graph G = (V,E) and a ground truth or predicted label by a GNN\nmodel, our causal explainer bases on causal learning and identifies the causal\nexplanatory subgraph (denoted as F) that intrinsically yields the label.\nOur CXGNN consists of three key steps: 1) define the causal structure G\nfor the graph G and the respective SCM M(G) (we call GNN-SCM) to enable\ncausal effect calculation via interventions; 2) However, directly calculating the\ncausal effect in real graphs is computationally challenging. We then construct\nand train a family of parameterized GNN neural causal model M(G, 0)) (we call\nGNN-NCM), a special type of GNN-SCM that is trainable. 3) We uncover the\ncausal explanatory subgraph (denoted as \u0393) based on the trained GNN-NCM\nthat best yields the graph label. Next, we will illustrate step-by-step in detail."}, {"title": "4.2 Causal Structure and Induced SCM on a Graph", "content": "In the context of causality, the problem of GNN explanation can be solved by\ncause and effect identification among nodes and their connections in a graph. In-\nterventions enable us to interpret the causal relation between nodes. To perform\ninterventions on a graph, one often needs to first define the causal structure for\nthis graph, which involves the observable and latent variables.\nObservable and latent variables in a graph: Given a G = (\u03bd,\u03b5). For\neach node v \u2208 V, there are both known and unknown effects from other nodes\nand edges on v, which we call observable variables (denoted as V\u2082) and latent\nvariables (denoted as U\u2082), respectively. With it, we define a congruent causal\nstructure for enabling the graphs to admit SCMs."}, {"title": "Definition 2 (Causal structure of a graph).", "content": "Consider a graph G = (V, \u03b5),\nwe define the causal structure G of G as a subgraph that centers on a reference\nnode v and accepts the SCM structure:\nG(G) = {V\u2081 = {Yv} \u222a {Yvz : V\u00ec \u2208 N\u2264k(v)}, Uv = {Uv\u2081 : V\u00ec \u2208 N<k(v)} \u222a {Uv,v; : Cu,v; \u2208 E}},\nwhere v is to be learnt (see Section 4.4), Yv\u2081 is the node vi 's label, N<k(v) means\nnodes within the k-hop neighbors of v, Uv; is vi's latent variable, called node\neffect; and Uv,vi the edge ev,vi latent variable, called edge effect. In practice, we\ncan specify Uv; and Uv,vi as random variable, e.g., from a Gaussian distribution.\nWith a causal structure for a graph, we can build the corresponding SCM in\nthe following theorem:"}, {"title": "Theorem 1 (GNN-SCM).", "content": "For a GNN operating on a graph G, there exists\nan SCM M(G) w.r.t. the causal structure G of the graph G.\nAppendix A shows an example on how to compute the causal effects on a\ntoy graph via a SCM truth table."}, {"title": "4.3 GNN Neural Causal Model", "content": "In reality, it is computationally challenging to build a truth table for variables\nin GNN-SCM and perform do-calculus computation due to the large number of\nnodes/edges in real-world graphs. Such a challenge impedes the calculation of\ncausal effects. To address it, we are motivated by estimating the causal effect via\nNCM (see Section 3). Specifically, Definition 6 in Appendix shows: to ensure the\nequivalence between NCM and SCM, NCM is required to be G-constrained. How-\never, the general G-constrained NCM cannot be directly applied in our setting.\nTo this end, we first define a customized G-constrained GNN-NCM as below:"}, {"title": "Definition 3 (G-Constrained GNN-NCM (constructive)).", "content": "Let GNN-SCM\nM(G,0) be induced from the causal structure G(G) on a graph G. Then GNN-\nNCM M(G,0) will be constructed based on the causal structure G(G).\nThis construction ensures that any inferences made by MNCM(G, 0) respect\nthe causal dependencies as captured by G(G). Note that M(G, 0) represents\na family of GNN-NCMs since the parameters of the neural networks are not\nspecified by the construction. Next, we propose a construction of a G-constrained\nGNN-NCM, following Definition 3."}, {"title": "GNN Neural Causal Model Construction", "content": "One should consider the sound\nand complete structure of GNN-NCMs that are consistent with Definition 2.\nHere, we define the general GNN-NCM structure as shown in below Equation 3,\nwhich is an instantiation of Theorem 2.\n\\begin{aligned}\n&\\mathcal{M}(\\mathcal{G},\\theta) =\\\\\n&\\left\\{\\begin{array}{ll}\nV := \\mathcal{V}(\\mathcal{G}) & \\\\\n\\widehat{U} := \\{\\widehat{U}_{v_{i}}, v_{i} \\in \\mathcal{V}(\\mathcal{G}\\} & \\\\\nP(\\widehat{U}) := {\\widehat{U}_{v_{i}} \\sim \\text{Unif}(0,1)\\} \\cup {\\tau_{k,v_{i}} \\sim N(0,1) : k \\in {0,1\\}} & \\\\\nf_{v_{i}}(\\widehat{u}_{v_{i}}, \\widehat{u}_{v_{i}, v_{j}}) & :=\\arg \\max _{k \\in {0,1\\}} \\begin{cases}\\text{log} \\sigma(f_{v_{i}}(\\widehat{u}_{v_{i}}, \\widehat{u}_{v_{i}, v_{j}}; \\theta_{v_{i}})) & \\text{if} k = 1 \\\\\\\\(log(1 - \\sigma(f f_{v_{i}}(\\widehat{u}_{v_{i}}, \\widehat{u}_{v_{j}}; \\theta_{v_{i}})))) & \\text{if} k = 0,\\end{cases} \\\\\nF := {f_{v_{i}}(\\widehat{u}_{v_{i}}, \\widehat{u}_{v_{i}, v_{j}})\\} & \\\\\n\\end{array}\\right.\n\\end{aligned}"}, {"title": "Algorithm 1 GNN Neural Causal Model Training", "content": "Input: The causal structure G (including a reference node v, its within k-hop neighbors\nN<k(v), and set of latent variables U\u2082), node label yv\nOutput: An optimized GNN-NCM M(G, 0*) for the causal structure G centered at v\n1: Build the GNN-NCM M(G, 0) based on G and Eqn. 3\n2: for each node vi \u2208 N\u2264k(v) do\n3: Calculate PM(9,0) (yv | do(vi)) via Eqn. 4\n4: end for\n5: Calculate PM(9,0) (yz) via Eqn. 5\n6: Calculate the loss L(M(G, 0); v) via Eqn. 6\n7: Minimize the loss to reach the GNN-NCM M(G,0*)"}, {"title": "Theorem 2 (GNN-NCM).", "content": "Given causal structure G of a graph G and the\nunderlying GNN-SCM M(G), there exists a G-constrained GNN-NCM M(9,0)\nthat enables any inferences consistent with M(G).\nIn Equation 3, V are the nodes in the causal structure G(G); each Tv is a\nstandard Gaussian random variable; each f fu, is a feed-forward neural network\non vi parameterized by \u03b8\u03c5; (note one requirement of f fv, is it could approximate\nany continuous function), and o is sigmoid activation function. The parameters\n{0} are not yet specified and must be learned through training the NCM.\nTraining Neural Networks for GNN-NCMs We now compute the causal\neffects on a target node v. Based on Definition 1 and the constructed GNN-\nNCM M(9,0) in Equation 3, the causal effect on v of an intervention do(vi)\n(vi \u2208 N\u2081(v)) is pM(G,9) (yv|do(vi)). This do-calculus then can be calculated as\nthe expected value of nodes and edges affects values for v shown below:\n\\begin{aligned}\np^{M(\\mathcal{G},\\theta)}\\left(y_{v} \\mid do(v_{i})\\right)=\\mathbb{E}_{p(u_{v})}\\left[\\prod_{V_{(u_{j}, u_{i})} \\in E} f u_{i}(u_{v_{j}}, u_{v_{j}, v_{i}})\\right]\n\\\\=\\frac{1}{|N_{\\leq k}(v)|}\\sum_{\\left(v_{i}, v_{j}\\right) \\in E(\\mathcal{G})} \\prod_{v_{i} \\in N_{\\leq k}(v)} f u_{i}(U_{v_{j}}, u_{v_{j}, v_{i}}).\n\\end{aligned}\nThen one can calculate the probability of the target node label yu as the expected\nvalue of all the effects from the neighbor nodes on v:\n\\begin{aligned}\np^{M(\\mathcal{G},\\theta)}(y_{v}) = \\mathbb{E}_{p(v)}[f_{v}] \\sim \\frac{1}{|\\mathcal{V}|} \\sum_{Y \\in \\mathcal{V}} \\sum_{v \\in N_{1}(v)} p^{M(\\mathcal{G},\\theta)}(y_{v} = y \\mid do(v_{i}))\n\\end{aligned}\nThe true GNN-SCM induces a causal structure that encodes constraints over\nthe interventional distributions. We now first investigate the feasibility of causal\ninferences in the class of G-constrained GNN-NCMs. These models approximate\nthe likelihood of the observed data based on the graph's latent variables. The\ncross-entropy loss measures the discrepancy between the target node's label pre-\ndiction and its true label. Inspired by [43], we define the GNN-NCM loss as:\nL(M(G, 0); v) = \u2013 \u03a3 yv log(pM(9,0) (Yv))\nY\u03c5 \u0395\u03bd"}, {"title": "Algorithm 2 CXGNN: GNN Causal Explainer", "content": "Input: Graph G with label, and expressivity threshold \u03b4\nOutput: Explanatory subgraph \u0393\n1: for each node v \u2208 V(G) do\n2: Build G based on the reference node v\n3: Train the GNN-NCM M(Gu, 0) via Alg. 1 and calculate the node expressivity\nexp (M(Gu, 0))\n4: end for\n5: Find v* = argmax\u2208v(G) exp\u2082 (M(Gv, 0));\n6: Return the explanatory subgraph I induced by Gu*\nTo train neural networks for GNN-NCMs, one should generate samples from\nthe GNN-SCM. If provided, it is the specific realization of the interventions.\nSpecifically, GNN-NCMs are trained on node effects \u00fbv, and edge effects \u016bvi,vj\non the target node, as shown in Equation 3, and should specify fui (\u016bvi, \u016bvi,vz).\nThen a model, denoted as 0*, is achieved by minimizing the GNN-NCM loss:\n\\theta^{*} \\in \\arg \\min L(\\mathcal{M}(\\mathcal{G}, \\theta); v)\nDetails of training GNN-NCMs are shown in Algorithm 1. Basically, this algo-\nrithm takes the causal structure G with respect to a reference node v as input\nand returns an optimized GNN-NCM model M(G,0*)."}, {"title": "4.4 Realizing GNN Causal Explanation", "content": "The remaining question is: how to find the causal explanatory subgraph I from\na graph G to causally explain GNN predictions? The answer is using the trained\nGNN-NCMS M(G, 0*). Before that, the first step is to clarify a node's role in\nGNN-NCMs for explanation."}, {"title": "Theorem 3 (Node explainability).", "content": "Let a prediction for a graph G be ex-\nplained. A node v \u2208 G is causally explainable, if pM(G(G),0) (y) can be computed.\nThe G-constrained GNN-NCM is trained on interventions and can interpret\nthe GNN predictions. Moreover, the information extracted from interventions\ncan be used for interpreting nodes. Specifically, we define expressivity to measure\nthe information for an explainable node."}, {"title": "Theorem 4 (Explainable node expressivity).", "content": "An explainable node v has\nexpressivity defined as exp (M(G, 0)) = \u03a3\u03c8\u03c5 \u03a8\u03c5\u03c1M(G,9) (\u03c8\u03c5).\nIn other words, the node expressivity reflects how well the node is in the\ncausal explanatory subgraph. Now we are ready to realize GNN causal explana-\ntion based on learned GNN-NCMs. Given a graph G, we start from a random\nnode v, and build the causal structure G centered on v. By Algorithm 1, we can\nreach an optimized GNN-NCM M(G, 0*) and obtain the v's expressivity.\nWe repeat this process for all nodes in the graph G and find the node v* with\nthe associated M(G, 0*) yielding the highest expressivity exp\u201e* (M(G, 0*)). The\nunderlying subgraph of the causal structure centered by v* is then treated as\nthe causal explanatory subgraph \u0393. Algorithm 2 describes the learning process."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets: Following prior works [19, 46], we use six synthetic datasets, and\ntwo real-world datasets with groundtruth explanation for evaluation. Dataset\nstatistics are shown in Table 1.\nSynthetic graphs: 1) BA+House: This graph stems from a base random\nBarab\u00e1si-Albert (BA) graph attached with a 5-node \u201chouse\"-structured mo-\ntif as the groundtruth explanation; 2) BA+Grid: This graph contains a\nbase random BA graph and is attached with a 9-node \"grid\" motif as the\ngroundtruth explanation; 3) BA+Cycle: A 6-node \"cycle\" motif is ap-\npended to randomly chosen nodes from the base BA graph. The \"cycle\" motif\nis the groundtruth explanation; 4) Tree+House: The core of this graph is\na balanced binary tree. The 5-node \"house\" motif, as the groundtruth ex-\nplanation, is attached to random nodes from the base tree. 5) Tree+Grid:\nSimilarly, binary tree a the core graph and a 9-node \"grid\" motif as the\ngroundtruth explanation is attached; 6) Tree+Cycle: A 6-node \"cycle\"\nmotif, the groundtruth explanation, is appended to nodes from the binary\ntree. The label of the synthetic graph is decided by the label of nodes in\nthe groundtruth explanation. Following existing works [19, 46], a node v's\nlabel y is set to be 1 if v is in the groundtruth, and 0 otherwise. Hence, in\nthese graphs, the base graph acts as the non-causal subgraph that can cause\nthe spurious correlation, while the attached motif can be seen as the causal\nsubgraph, as it does not change across graphs and decides the graph label.\nReal-world graphs: We use two representative real-world graph datasets with\ngroundtruth [1]. 1) Benzene: it includes 12,000 molecular graphs extracted\nfrom the ZINC15 [31] database and the task is to identify whether a given\nmolecule graph has a benzene ring or not. The groundtruth explanations\nare the nodes (atoms) forming the benzene ring. 2) Fluoride carbonyl:\nThis dataset contains 8,671 molecular graphs with two classes: a positive\nclass means a molecule graph contains a fluoride (F-) and a carbonyl (C=0)\nfunctional group. The groundtruth explanation consists of combinations of\nfluoride atoms and carbonyl functional groups within a given molecule."}, {"title": "Models and parameter setting:", "content": "In CXGNN, we use a feedforward neural\nnetwork to parameterize GNN-NCM. The neural network consists of an input\nlayer, two fully connected hidden layers, and an output layer. ReLU activation\nfunctions is used in all hidden layers, while a softmax activation function is ap-\nplied to the output layer. The input to the network is the target node v's node\neffects and edge effects (see Equation 2), whose values are sampled from a stan-\ndard Gaussian distribution, and the output is the predicted causal effect on v.\nThe detailed hyperparameters are shown in Appendix D.1. The hyperparameters\nin the compared GNN explainers are optimized based on their source code.\nBaseline GNN explainers: We compare CXGNN with both association-based\nand causality-inspired GNN explainers. We choose 4 representative ones: gradient-\nbased Guidedbp [12], perturbation-based GNNExplainer [46], surrogate-based\nPGMExplainer [33], and causality-inspired GEM [19], RCExplainer [38], and\nOrphicX [20]. We use the public source code of these explainers for comparison.\nThe causality-inspired explainers are inspired by causality concepts to infer the\nexplanatory subgraph, but they inherently do not provide causal explanations.\nEvaluation metrics: Given a set of testing graphs G. For each test graph\nGEG, we let its groundtruth explanatory subgraph be IG and the estimated\nexplanatory subgraph by a GNN explainer be F. We use two common metrics,\ni.e., graph explanation accuracy and explanation recall from the literature [1]. In\naddition, to justify the superiority of our causal explainer, we introduce a third\nmetric groundtruth match accuracy, which is the most challenging one."}, {"title": "Graph explanation accuracy:", "content": "For a graph G, the graph explanation accu-\nracy is defined as the fraction of nodes in the estimated explanatory subgraph\nI that are contained in the groundtruth FG, i.e., |V(\u0413) \u2229 V(\u0413G)|||V(\u0413G)|.\nWe then report the average accuracy across all testing graphs."}, {"title": "Graph explanation recall:", "content": "Different GNN explainers output the estimated\nexplanatory subgraph with different node sizes. When two explainers output\nthe same number of nodes in IG, the one with a smaller node size should be\ntreated as having a better quality. To account for this, we use the explanation\nrecall metric that is defined as |V(\u0413) \u2229V(IG)|/|V(\u0393)| for a given graph G.\nWe then report the average recall across all testing graphs."}, {"title": "Groundtruth match accuracy:", "content": "For a testing graph G, we count a 1 if the\nestimated and groundtruth IG exactly match, i.e., \u0393G = \u0393, and 0 other-\nwise. In other words, the groundtruth match accuracy of all testing graphs\nGis defined as \u2211G\u2208G1[\u0393G = \u0393]/|G|, where 1[\u00b7] is an indicator function."}, {"title": "5.2 Results on Synthetic Datasets", "content": "Comparison results: Table 2 shows the results of all the compared GNN ex-\nplainers on the 6 synthetic datasets with 500 testing graphs and 3 metrics. We\nhave several observations. In terms of explanation accuracy, CXGNN performs\ncomparable or slightly worse than causality-inspired methods. This is because,\nto ensure high accuracy, the estimated explanatory subgraph of these methods\nshould have a large size. This can be reflected by the explanation recall, where"}, {"title": "5.3 Results on Real-World Datasets", "content": "Comparison results: Table 3 shows the results of all the compared explainers\non the real-world datasets and three metrics. We have similar observations as\nthose in Table 2. Especially, no existing explainers can even find one exactly\nmatched groundtruth. Particularly, the explanation subgraphs produced by the\ntwo causality-inspired baselines can cover the majority or almost all groundtruth\nin synthetic datasets (hence high accuracy), and the sizes of the explanation sub-\ngraphs are slightly larger than those of the groundtruth (hence relatively large"}, {"title": "6 Conclusion", "content": "GNN explanation, i.e., identifying the informative subgraph that ensures a GNN\nmakes a particular prediction for a graph, is an important research problem.\nThough various GNN explainers have been proposed, they are shown to be prone\nto spurious correlations. We propose a causal GNN explainer based on the fact\nthat a graph often consists of a causal subgraph and fulfills the goal via causal\ninference. We then propose to train GNN neural causal models to uncover the\ncausal explanatory subgraph. In future work, we will study the robustness of our\nCXGNN under the adversarial graph perturbation attacks [17, 22, 34-37, 45]."}, {"title": "A AGNN-SCM Example", "content": "The causal structure of a graph is a subgraph centering on a reference node and\naccepts the SCM structure via Definition 2. The goal of causality in a graph\nis to identify the subgraph with the maximum explainable node expressivity as\nTheorem 4 that causally explains GNN predictions. Below, we use a toy example\ngraph to show how our explainer captures the causality in this graph.\nWe use a toy example to demonstrate SCMs and\nthe intervention process in GNNs. Figure 7 shows a\ngraph G that contains four nodes A, B, C, and D, and\nthree edges A-B, A-C', and B-D. In GNNs, these edges\ncontain messages passing between two nodes. For ex-\nample, the message between two nodes A and B in\nthe 1-th layer of the GNN is MA,B = MSG(h\u00b9, h\u00b9\nCA,B). If there exists an edge, it means that there is an\ninteraction between nodes that has a specific value in\neach layer l. If there is no edge, two nodes don't share\na message. If we consider node A as the reference node v, the nodes B, and Care\nin the 1-hop neighbors N\u22641(v), and node D is in the 2-hop neighbors N\u22642(v).\nThis GNN-SCM induces its causal structure G from Graph G, as discussed in\nDefinition 2."}, {"title": "A.1 GNN-SCM construction", "content": "Following [23], we build a GNN-SCM M(G) that learns from the causal structure\nG. The endogenous variables are node labels {yv; v \u2208 A, B, C, D}. The exogenous\nvariables in G are reference node A's states: UA\u2081, UA\u2082 (in this example we con-\nsider binary states A1 and A2), edges effects on reference node A: UA,B, UA,C,\nand neighbor nodes' effects on reference node A: \u0438\u0432,uc, up. All of these la-\ntent variables are assumed to accept the same probability P(U) as a probability\nfunction defined over the domain of U since we don't want to input new specific\ninformation. F is a set of functions based on the observable and latent variables\ndiscussed above. One should consider that uv; and uv; are not independent.\nAs discussed in Theorem 1, we construct the GNN-SCM M(G) based on\ngraph G as M(G) =: (U, V, F, P(U)), where:"}, {"title": "Graph Neural Network Causal Explanation", "content": "\\begin{aligned}\n&\\mathcal{M}(\\mathcal{G}) =\\\\\n&\\left\\{\\begin{array}{ll}\nU := & {\\begin{array}{l}U_{A_{1}}, U_{A_{2}} \\\\\nU_{A, B}, U_{A, C} \\\\\nU_{B}, U_{C}, U_{D}\\end{array}} \\\\\nV := & {A, B, C, D} \\\\\nF:= & {\\begin{array}{l}f_{A}(B, C, D, U_{A_{1}}, U_{A_{2}}) = f_{A}(B, U_{A_{1}}, U_{A_{2}}) \\wedge f_{A}(C, U_{A_{1}}, U_{A_{2}}) \\wedge f_{A}(D) \\\\\nf_{A}(B, U_{A_{1}}, U_{A_{2}}) = (((-\\mathbb{B} + U_{A_{1}}) \\vee U_{A,B}) \\oplus U_{A_{2}}) \\\\\nf_{A}(C, U_{A_{1}}, U_{A_{2}}) = (((-\\mathbb{C} \\oplus U_{A_{1}}) \\vee U_{A,C}) \\oplus U_{A_{2}}) \\\\\nf_{A}(D) = \\mathbb{D} \\\\\nf_{B}(u_{B}, u_{A,B}) = -u_{B} \\wedge -u_{A,B} \\\\\nf_{C}(u_{C}, u_{A,C}) = -u_{C} \\wedge -u_{A,C} \\\\\nf_{D}(U_{D}) = -U_{D}\\end{array}} \\\\\nP(U) & : = {\\begin{array}{l}P(U_{A_{1}}) = P(A_{2}) = P(U_{A,B}) = P(U_{A,C}) = \\\\\nP(U_{A,D}) = P(u_{B}) = P(u_{C}) = P(U_{D}) = 1/8\\end{array}}\n\\end{array}\\right.\n\\end{aligned}"}, {"title": "A.2 SCM tables", "content": "By interpreting the variables and their values from the logic tree, the truth\ntable will be in four different states. If none of the 1-hop neighborhood nodes'\nobservable variable affects reference node A, if one of them (node B or C) has an\neffect, or otherwise both of them affect the reference node. Probabilities in P(U)\nare labeled from po to p63 for convenience, which are 7 binary variables(layers\nin the logic tree).\nIn all provided table rows, Up = 0 and D = 1, meaning D is not the cause\nand the latent variable of it is 1. However, there is no edge between nodes A and\nD, we need to mention this in our calculations. For simplicity, we just showed\nthe cases that up = 0, but there are the same truth tables with up = 1 and\nD = 0 by probabilities p64 to P127. Given the probabilities from the truth tables,\nwe can define them as follows:\nP(U) := Unif(0, 1) \u2192 po = p1 = p2 = ... = p63 = P127 = 1/128"}, {"title": "A.3 GNN-SCM results", "content": "The capability of the tables shows that our specified GNN-SCM M(G) can\ncalculate all queries from each PCH layer [25]. In continue, we will calculate\nan example for each layer:\nAn association layer query such as P(A = 1|C = 1) which is the probability\nof observable variable A to be 1 given observable variable C to be 1, can be\ncomputed as:"}, {"title": "Graph Neural Network Causal Explanation", "content": "P(A = 1|C = 1) =\n=\n=\n+\n+\nP(A = 1, C = 1)\nP(C = 1)\np1 + p2 + p4 + p5 + p17 + p18+ p20 + p21\np0+ p1 + p2 + p3 + p4+p5+p6+ p7+ p16 + p17+ p18+ p19 + p20 + p21 + p22 + p23\np24 + p25 + p28 + p29 + p36 + p37 + p40 + p41 + p44 + p45 + p52 + p53 + p56 + p57 + p60+ p61\np8+ p9 + ... + p14 + p15 + p24 + p25 + p26 + p27 + p28 + p29 + p30 + p31 + p32 + ... + p63\n0.25 +0.33 = 0.58"}, {"title": "A.4 Example's GNN-NCM", "content": "With respect to literature, an NCM is as expressive as an SCM, and all NCMs are\nSCMs. The causal diagram constraints are the bias between SCMs and NCMs.\nIn our specified GNN-SCM M(G), and GNN-NCM M(G, 0)), all causal informa-\ntion(observable, and latent variables) comes from the causal structure defined in\nDefinition. 2. The respective GNN-NCM M(G,0) is constructed as a proxy of\nthe exact GNN-SCM M(G). Based on Equation. 3, reference node A is chosen\nas the target node for causal structure G. This GNN-NCM M(G, 0) is an induc-\ntive bias type of the GNN-SCM as M(G, 0)=:(\u00db, V, F, P(\u00db)). The construction\nof the corresponding GNN-NCM that induces the same distributions for our\nexample dataset is as follows:\n\\begin{array}{l}\n\\widehat{U} :=\\{\\widehat{U}\\}, D_{\\widehat{U}}=[0,1] \\\\\nV :={A, B, C, D} \\\\\n\\mathcal{M}(\\mathcal{G}, \\theta)) & \\begin{array}{l}\nF_{A}(\\widehat{U}) =? \\\\\nB(A, \\widehat{U}) =? \\\\\nf_{C}(A, \\widehat{U}) =? \\\\\nf_{D}(A, \\widehat{U}) =? \\\\\nP(\\widehat{U}) :=?\n\\end{array}\n\\end{array}\nWe know the observable variables V values, but there is no clue about the\nexact values of latent variables U, so we have to estimate them by functions F\nbased on the information in the given causal structure G. First, we have to build\nthe causal structure G given example graph G.\nG(G) = {V\u2081 = {A, B, C, D}},\nUv = {Uv; : {UB, UC, UD, UA1, UA2 } \u222a {Uv,v\u2081 : {UA, B, UA,C}}"}, {"title": "Arman Behnam and Binghui Wang", "content": "\\begin{array}{l}\n\\widehat{U} :=\\{U_{v} :{\\widehat{U}_{B} = 0, \\widehat{U}_{C} = 0, \\widehat{U}_{D} = 1, \\widehat{U}_{A_{1}} = 0, \\widehat{U}_{A_{2}} = 1\\}\\\\\n{U}_{v, v_{i}} : {\\widehat{U}_{A,B} = 1, \\widehat{U}_{A,C} = 1\\}\\\\\nV :={A = (B \\wedge C), B=1, C=1, D=0} \\\\\n\\mathcal{M}(\\mathcal{G}, \\theta)) & \\begin{array}{l}\n\\text{f}_{A}(U) =  f_{A}(\\widehat{u}_{A_{1}} = 0, \\widehat{u}_{A_{2}} = 1, \\widehat{u}_{A,B} = 1, \\widehat{u}_{A,C} = 1) \\\\\n\\text{f}_{B}(A, U) =  f_{B}(\\widehat{u}_{B} = 0, \\widehat{u}_{A,B} = 1) \\\\\n\\text{f}_{C}(A, U) =  f_{C}(\\widehat{u}_{C} = 0, \\widehat{u}_{A,C} = 1) \\\\\n\\text{f}_{D}(A, U) =  f_{D}(\\widehat{u}_{D} = 1)\n\\end{array} \\\\\nP(\\widehat{U}) := {\\begin{array}{l}P(\\widehat{u}_{A_{1}}) = P(\\widehat{u}_{A_{2}}) = P(\\widehat{u}_{A,B}) = P(\\widehat{u}_{A,C}) \\\\\n= P(\\widehat{u}_{A,D}) = P(\\widehat{u}_{B}) = P(\\widehat{u}_{C}) = P(\\widehat{u}_{D})\\end{array} }= 1/8\n\\end{array}"}, {"title": "BMore Background on Causality", "content": "According to the literature, causality interprets the information by the Pearl\nCausal Hierarchy (PCH) layers [25]."}, {"title": "Definition 4 (PCH layers).", "content": "The PCH layers Li for i \u2208 1,2,3 are: L\u2081 asso-\nciation layer, L2 intervention layer, and L3 counterfactual layer."}, {"title": "Definition 5 (G-Consistency).", "content": "Let G be the causal structure induced by SCM\nM*. For any SCM M, we say M is G-consistent w.r.t M* if M imposes the\nsame constraints over the interventional distributions as the true M*."}, {"title": "Definition 6 (G-Constrained NCM).", "content": "Let G be the causal structure induced\nby SCM M*. We can construct NCM M as follows: 1) Choose \u016a s.t. \u00dac \u2208 \u00db,\nwhere any pair (Vi, Vj) \u2208 C is connected with a bidirected arrow in G and is\nmaximal; 2) For each Vi \u2208 V, choose Pa(Vi) \u2286 V s.t. for every V; \u2208 V,\nV; \u2208 Pa(Vi) iff there is a directed arrow from V; to Vi in G. Any NCM in this\nfamily is said to be G-constrained."}, {"title": "Theorem 5.", "content": "Any G-constrained NCM M(0) is G-consistent."}, {"title": "C Proofs", "content": "In this section, we provide proofs of the theorems in the main body of the paper."}, {"title": "C.1 Proof of Theorem 1", "content": "Theorem 1 (GNN-SCM). For a GNN operating on a graph G", "as": "nGNN(v) =\n\\begin{array"}, {}]}, {}, {}, "as", {"embed": {"h}\u222a{hu": "u \u2208 N<k(v)"}, "message": {}}]