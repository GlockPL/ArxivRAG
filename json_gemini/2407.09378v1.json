{"title": "Graph Neural Network Causal Explanation via Neural Causal Models", "authors": ["Arman Behnam", "Binghui Wang"], "abstract": "Graph neural network (GNN) explainers identify the important subgraph that ensures the prediction for a given graph. Until now, almost all GNN explainers are based on association, which is prone to spurious correlations. We propose CXGNN, a GNN causal explainer via causal inference. Our explainer is based on the observation that a graph often consists of a causal underlying subgraph. CXGNN includes three main steps: 1) It builds causal structure and the corresponding structural causal model (SCM) for a graph, which enables the cause-effect calculation among nodes. 2) Directly calculating the cause-effect in real-world graphs is computationally challenging. It is then enlightened by the recent neural causal model (NCM), a special type of SCM that is trainable, and design customized NCMs for GNNs. By training these GNN NCMS, the cause-effect can be easily calculated. 3) It uncovers the subgraph that causally explains the GNN predictions via the optimized GNN-NCMs. Evaluation results on multiple synthetic and real-world graphs validate that CXGNN significantly outperforms existing GNN explainers in exact groundtruth explanation identification.", "sections": [{"title": "1 Introduction", "content": "Graph is a pervasive data type that represents complex relationships among entities. Graph Neural Networks (GNNs) [6, 13, 15, 44], a mainstream learning paradigm for processing graph data, take a graph as input and learn to model the relation between nodes in the graph. GNNs have demonstrated state-of-the-art performance across various graph-related tasks such as node classification, link prediction, and graph classification, to name a few [42].\nExplainable GNN provides a human-understandable way of the prediction outputted by a GNN. Given a graph and a label (correctly predicted by a GNN model), a GNN explainer aims to determine the important subgraph (called explanatory subgraph) that is able to predict the label. Various GNN explanation methods [3,5,8,9,14,18,21,26\u201328,30,33,39,40,46-50] have been proposed, wherein almost all of them are based on associating the prediction with a subgraph that"}, {"title": "2 Related Work", "content": "Association-based explainable GNN: Almost all existing GNN explainers are based on association. These methods can be roughly classified into five types. (i) Decomposition-based methods [8, 27] consider the prediction of a GNN model as a score and decompose it backward layer-by-layer until it reaches the input. The score of different parts of the input can be used to explain its importance to the prediction. (ii) Gradient-based methods [3, 27] take the gradient of the prediction with respect to the input to show the sensitivity of a prediction to the input. The sensitivity can be used to explain the input for that prediction. (iii) Surrogate-based methods [5, 14, 26, 33, 50) replace the GNN model with a simple and interpretable surrogate one. (iv) Generation-based methods [18, 30, 40,47] use generative models or graph generators to generate explanations. (v) Perturbation-based methods [9, 21, 28, 39, 46, 48, 49] aim to find the important subgraphs as explanations by perturbing the input. State-of-the-art explainers from (iii)-(iv) show better performance than those from (i) and (ii).\nCausality-inspired explainable GNN: Recent GNN explainers [7,19,20, 32, 38,41] are motivated by causality. These methods are based on a common obser-vation that a graph consists of the causal subgraph and its non-causal counterpart. For instance, OrphicX [20] uses information-theoretic measures of causal influence [2], and proposes to identify the (non)causal factors in the embedding space via information flow maximization. CAL [32] introduces edge and node attention modules to estimate the causal and non-causal graph features.\nCXGNN vs. causality-inspired explainers. The key difference lies in CXGNN focuses on identifying the causal explanatory subgraph by directly quantifying the cause-and-effect relations among nodes/edges in the graph. Instead, causality-inspired explainers are inspired by causality concepts to infer the explanatory subgraph, but they inherently do not provide causal explanations."}, {"title": "3 Preliminaries", "content": "In this section, we provide the necessary background on GNNs and causality to understand this work. For brevity, we will consider GNNs for graph classification.\nNotations: We denote a graph as G = (V, E), where V and E are the node set and edge set, respectively. v \u2208 V(G) represents a node and \\( e_{u,v} \\) \u2208 E(G) is an edge between u and v. Each graph G is associated with a label \\( y_G \\) \u2208 \\( \\mathcal{Y} \\), with \\( \\mathcal{Y} \\) the label domain. An uppercase letter X and the corresponding lowercase one x indicate a random variable and its value, respectively; bold X and x denote a set of random variables and its corresponding values, respectively. We use Dom(X) to denote the domain of X, and P(X) as a probability distribution over X."}, {"title": "4 GNN Causal Explanation via NCMs", "content": "In this section, we propose our GNN causal explainer, CXGNN, for explaining graph classification. Our explainer also utilizes the common observation that a graph consists of a causal subgraph and a non-causal counterpart [7,19,20,32,38, 41]. The overview of CXGNN is shown in Figure 10 in Appendix and all proofs are deferred to Appendix C."}, {"title": "4.1 Overview", "content": "Given a graph G = (V,E) and a ground truth or predicted label by a GNN model, our causal explainer bases on causal learning and identifies the causal explanatory subgraph (denoted as F) that intrinsically yields the label.\nOur CXGNN consists of three key steps: 1) define the causal structure G for the graph G and the respective SCM M(G) (we call GNN-SCM) to enable causal effect calculation via interventions; 2) However, directly calculating the causal effect in real graphs is computationally challenging. We then construct and train a family of parameterized GNN neural causal model M(G, \\( \\theta \\))) (we call GNN-NCM), a special type of GNN-SCM that is trainable. 3) We uncover the causal explanatory subgraph (denoted as \\( \\Gamma \\)) based on the trained GNN-NCM that best yields the graph label. Next, we will illustrate step-by-step in detail."}, {"title": "4.2 Causal Structure and Induced SCM on a Graph", "content": "In the context of causality, the problem of GNN explanation can be solved by cause and effect identification among nodes and their connections in a graph. Interventions enable us to interpret the causal relation between nodes. To perform interventions on a graph, one often needs to first define the causal structure for this graph, which involves the observable and latent variables.\nObservable and latent variables in a graph: Given a G = (\\( \\nu \\),\u03b5). For each node v \u2208 V, there are both known and unknown effects from other nodes and edges on v, which we call observable variables (denoted as \\( V_v \\)) and latent variables (denoted as \\( U_v \\)), respectively. With it, we define a congruent causal structure for enabling the graphs to admit SCMs."}, {"title": "Definition 2 (Causal structure of a graph)", "content": "Consider a graph G = (V, \u03b5), we define the causal structure G of G as a subgraph that centers on a reference node v and accepts the SCM structure:\n\n\nG(G) = {\\( V_v \\) = {\\( Y_v \\)} \u222a {\\( Y_{v_i} \\) : \\( V_i \\) \u2208 \\( N_{\\leq k}(v) \\)}, \\( U_v \\) = {\\( U_{v_i} \\) : \\( V_i \\) \u2208 \\( N_{<k}(v) \\)} \u222a {\\( U_{v,v_i} \\) : \\( e_{u,v_i} \\) \u2208 E}}, \n\nwhere v is to be learnt (see Section 4.4), \\( Y_{v_i} \\) is the node \\( v_i \\) 's label, \\( N_{<k}(v) \\) means nodes within the k-hop neighbors of v, \\( U_{v_i} \\) is \\( v_i \\)'s latent variable, called node effect; and \\( U_{v,v_i} \\) the edge \\( e_{v,v_i} \\) latent variable, called edge effect. In practice, we can specify \\( U_{v_i} \\) and \\( U_{v,v_i} \\) as random variable, e.g., from a Gaussian distribution.\nWith a causal structure for a graph, we can build the corresponding SCM in the following theorem:"}, {"title": "Theorem 1 (GNN-SCM)", "content": "For a GNN operating on a graph G, there exists an SCM M(G) w.r.t. the causal structure G of the graph G."}, {"title": "4.3 GNN Neural Causal Model", "content": "In reality, it is computationally challenging to build a truth table for variables in GNN-SCM and perform do-calculus computation due to the large number of nodes/edges in real-world graphs. Such a challenge impedes the calculation of causal effects. To address it, we are motivated by estimating the causal effect via NCM (see Section 3). Specifically, Definition 6 in Appendix shows: to ensure the equivalence between NCM and SCM, NCM is required to be G-constrained. However, the general G-constrained NCM cannot be directly applied in our setting. To this end, we first define a customized G-constrained GNN-NCM as below:"}, {"title": "Definition 3 (G-Constrained GNN-NCM (constructive))", "content": "Let GNN-SCM M(G,0) be induced from the causal structure G(G) on a graph G. Then GNN-NCM M(G,0) will be constructed based on the causal structure G(G).\nThis construction ensures that any inferences made by \\( \\mathcal{M}_{NCM}(G, \\theta) \\) respect the causal dependencies as captured by G(G). Note that \\( \\mathcal{M}(G, \\theta) \\) represents a family of GNN-NCMs since the parameters \\( \\theta \\) of the neural networks are not specified by the construction. Next, we propose a construction of a G-constrained GNN-NCM, following Definition 3.\nGNN Neural Causal Model Construction One should consider the sound and complete structure of GNN-NCMs that are consistent with Definition 2. Here, we define the general GNN-NCM structure as shown in below Equation 3, which is an instantiation of Theorem 2.\n\n\\begin{aligned}\n&\\mathcal{M}(\\mathcal{G},\\theta)=\\left\\{\\begin{array}{l}\nV:=\\mathcal{V}(G) \\\\\n\\hat{\\mathcal{U}}:=\\{\\hat{\\mathcal{U}}\\_{v\\_{i}}, \\forall v\\_{i} \\in \\mathcal{V}(G\\}, \\quad \\mathcal{P}(\\hat{\\mathcal{U}}):=\\{\\hat{\\mathcal{U}}\\_{v\\_{i}} \\sim \\text { Unif }(0,1)\\} \\cup\\left\\{\\mathcal{T}\\_{k, v\\_{i}} \\sim \\mathcal{N}(0,1): k \\in\\{0,1\\}\\right\\} \\\\\nf\\_{v\\_{i}}\\left(\\bar{u}\\_{v\\_{i}}, \\bar{u}\\_{v\\_{i}, v\\_{j}}\\right):=\\arg \\max \\_{k \\in\\{0,1\\}} \\begin{cases}\\text { Teu }+\\log \\sigma\\left(f f v\\_{i}\\left(\\bar{u}\\_{v\\_{i}}, \\bar{u}\\_{v\\_{i}, v\\_{j}} ; v\\_{i}\\right)\\right) & \\text { if } k=1 \\\\\n\\log \\left(1-\\sigma\\left(f f v\\_{i}\\left(\\bar{u}\\_{v\\_{i}}, \\bar{u}\\_{v\\_{i}, v\\_{j}} ; \\theta v\\_{i}\\right)\\right)\\right) & \\text { if } k=0,\\end{cases} \\\\\n\\mathcal{F}:=\\left\\{f v\\_{i}\\left(\\bar{u}\\_{v\\_{i}}, \\bar{u}\\_{v\\_{i}, v\\_{j}}\\right)\\right\\}\n\\end{array}\\right."}, {"title": "Algorithm 1 GNN Neural Causal Model Training", "content": "Input: The causal structure G (including a reference node v, its within k-hop neighbors\n\\( N_{\\leq k}(v) \\), and set of latent variables \\( U_v \\)), node label \\( y_v \\)\nOutput: An optimized GNN-NCM \\( \\mathcal{M}(\\mathcal{G}, \\theta^*) \\) for the causal structure G centered at v\n1: Build the GNN-NCM \\( \\mathcal{M}(\\mathcal{G}, \\theta) \\) based on G and Eqn. 3\n2: for each node \\( v_i \\) \u2208 \\( N_{\\leq k}(v) \\) do\n3: Calculate \\( p_{\\mathcal{M}(\\mathcal{G}, \\theta)}(y_v | do(v_i)) \\) via Eqn. 4\n4: end for\n5: Calculate \\( p_{\\mathcal{M}(\\mathcal{G}, \\theta)}(y_v) \\) via Eqn. 5\n6: Calculate the loss \\( \\mathcal{L}(\\mathcal{M}(\\mathcal{G}, \\theta); v) \\) via Eqn. 6\n7: Minimize the loss to reach the GNN-NCM \\( \\mathcal{M}(\\mathcal{G}, \\theta^*) \\)"}, {"title": "Theorem 2 (GNN-NCM)", "content": "Given causal structure G of a graph G and the underlying GNN-SCM M(G), there exists a G-constrained GNN-NCM \\( \\mathcal{M}(\\mathcal{G}, \\theta) \\) that enables any inferences consistent with M(G).\nIn Equation 3, V are the nodes in the causal structure G(G); each \\( T_v \\) is a standard Gaussian random variable; each \\( f_{fv_i} \\) is a feed-forward neural network on \\( v_i \\) parameterized by \\( \\theta_v \\); (note one requirement of \\( f_{fv_i} \\) is it could approximate any continuous function), and \\( \\sigma \\) is sigmoid activation function. The parameters {\\( \\theta_v \\)} are not yet specified and must be learned through training the NCM.\nTraining Neural Networks for GNN-NCMs We now compute the causal effects on a target node v. Based on Definition 1 and the constructed GNN-NCM \\( \\mathcal{M}(\\mathcal{G}, \\theta) \\) in Equation 3, the causal effect on v of an intervention do(\\( v_i \\)) (\\( v_i \\) \u2208 \\( N_1(v) \\)) is \\( p_{\\mathcal{M}(\\mathcal{G}, \\theta)}(y_v|do(v_i)) \\). This do-calculus then can be calculated as the expected value of nodes and edges affects values for v shown below:\n\n\n\\begin{gathered}\np^{\\mathcal{M}(\\mathcal{G}, \\theta)}\\left(y\\_{v} \\mid d o\\left(v\\_{i}\\right)\\right)=\\mathrm{E}\\_{p\\left(u\\_{v}\\right)}\\left[\\prod\\_{(v, v\\_{j}) \\in \\mathrm{E}(\\mathcal{G})} f u\\_{v\\left(u\\_{v\\_{j}}, u\\_{v, v\\_{j}}\\right)}\\right] \\sim \\nonumber \\\\\n\\frac{1}{\\left|N\\_{\\leq k}(v)\\right|} \\sum\\_{v\\_{i} \\in N\\_{\\leq k}(v)} \\prod\\_{\\left(v, v\\_{j}\\right) \\in E(\\mathcal{G})} f u\\_{v\\_{j}}\\left(u\\_{v\\_{j}}, u\\_{v, v\\_{j}}\\right).\n\\end{gathered}\n\n\nThen one can calculate the probability of the target node label \\( y_v \\) as the expected value of all the effects from the neighbor nodes on v:\n\n\np^{\\mathcal{M}(\\mathcal{G}, \\theta)}\\left(y\\_{v}\\right)=\\mathbb{E}\\_{p(v)}\\left[f\\_{v}\\right] \\sim \\sum\\_{y \\in \\mathcal{V}} \\sum\\_{v\\_{i} \\in N\\_{1(v)}} \\frac{1}{\\mathbb{N} o} \\sum\\_{i \\in \\nu} p^{\\mathcal{M}(\\mathcal{G}, \\theta)}\\left(y\\_{v}=y \\mid d o\\left(v\\_{i}\\right)\\right) \n\n\nThe true GNN-SCM induces a causal structure that encodes constraints over the interventional distributions. We now first investigate the feasibility of causal inferences in the class of G-constrained GNN-NCMs. These models approximate the likelihood of the observed data based on the graph's latent variables. The cross-entropy loss measures the discrepancy between the target node's label pre-diction and its true label. Inspired by [43], we define the GNN-NCM loss as:\n\n\\mathcal{L}(\\mathcal{M}(\\mathcal{G}, \\theta) ; v)=-\\sum\\_{Y\\_{v} \\in V} y\\_{v} \\log \\left(p^{\\mathcal{M}(\\mathcal{G}, \\theta)}\\left(Y\\_{v}\\right)\\right)"}, {"title": "Algorithm 2 CXGNN: GNN Causal Explainer", "content": "Input: Graph G with label, and expressivity threshold \u03b4\nOutput: Explanatory subgraph \u0393\n1: for each node v \u2208 V(G) do\n2: Build G based on the reference node v\n3: Train the GNN-NCM \\( \\mathcal{M}(\\mathcal{G}_v, \\theta) \\) via Alg. 1 and calculate the node expressivity\n\\( \\exp\\_v (\\mathcal{M}(\\mathcal{G}\\_v, \\theta)) \\)\n4: end for\n5: Find \\( v^* \\) = \\( \\operatorname{argmax}\\_{v \\in V(G)} \\exp\\_v (\\mathcal{M}(\\mathcal{G}\\_v, \\theta)) \\);\n6: Return the explanatory subgraph \u0393 induced by \\( \\mathcal{G}\\_{v^*}\\)"}, {"title": "4.4 Realizing GNN Causal Explanation", "content": "The remaining question is: how to find the causal explanatory subgraph \\( \\Gamma \\) from a graph G to causally explain GNN predictions? The answer is using the trained GNN-NCMS \\( \\mathcal{M}(\\mathcal{G}, \\theta^*) \\). Before that, the first step is to clarify a node's role in GNN-NCMs for explanation."}, {"title": "Theorem 3 (Node explainability)", "content": "Let a prediction for a graph G be ex-plained. A node v \u2208 G is causally explainable, if \\( p^{\\mathcal{M}(\\mathcal{G}(\\mathcal{G}), \\theta)}(y) \\) can be computed.\nThe G-constrained GNN-NCM is trained on interventions and can interpret the GNN predictions. Moreover, the information extracted from interventions can be used for interpreting nodes. Specifically, we define expressivity to measure the information for an explainable node."}, {"title": "Theorem 4 (Explainable node expressivity)", "content": "An explainable node v has expressivity defined as \\( \\exp \\_{v}(\\mathcal{M}(\\mathcal{G}, \\theta))=\\sum\\_{\\psi\\_{v}} \\psi\\_{v} p^{\\mathcal{M}(\\mathcal{G}, \\theta)}\\left(\\psi\\_{v}\\right) \\).\nIn other words, the node expressivity reflects how well the node is in the causal explanatory subgraph. Now we are ready to realize GNN causal explana-tion based on learned GNN-NCMs. Given a graph G, we start from a random node v, and build the causal structure G centered on v. By Algorithm 1, we can reach an optimized GNN-NCM \\( \\mathcal{M}(\\mathcal{G}, \\theta^*) \\) and obtain the v's expressivity.\nWe repeat this process for all nodes in the graph G and find the node \\( v^* \\) with the associated \\( \\mathcal{M}(\\mathcal{G}, \\theta^*) \\) yielding the highest expressivity \\( \\exp\\_{v^*}(\\mathcal{M}(\\mathcal{G}, \\theta^*)) \\). The underlying subgraph of the causal structure centered by \\( v^* \\) is then treated as the causal explanatory subgraph \u0393. Algorithm 2 describes the learning process."}, {"title": "5 Experiments", "content": "Datasets: Following prior works [19, 46], we use six synthetic datasets, and two real-world datasets with groundtruth explanation for evaluation. Dataset statistics are shown in Table 1.\nSynthetic graphs: 1) BA+House: This graph stems from a base random Barab\u00e1si-Albert (BA) graph attached with a 5-node \u201chouse\"-structured motif as the groundtruth explanation; 2) BA+Grid: This graph contains a base random BA graph and is attached with a 9-node \"grid\" motif as the groundtruth explanation; 3) BA+Cycle: A 6-node \"cycle\" motif is ap-pended to randomly chosen nodes from the base BA graph. The \"cycle\" motif is the groundtruth explanation; 4) Tree+House: The core of this graph is a balanced binary tree. The 5-node \"house\" motif, as the groundtruth ex-planation, is attached to random nodes from the base tree. 5) Tree+Grid: Similarly, binary tree a the core graph and a 9-node \"grid\" motif as the groundtruth explanation is attached; 6) Tree+Cycle: A 6-node \"cycle\" motif, the groundtruth explanation, is appended to nodes from the binary tree. The label of the synthetic graph is decided by the label of nodes in the groundtruth explanation. Following existing works [19, 46], a node v's label y is set to be 1 if v is in the groundtruth, and 0 otherwise. Hence, in these graphs, the base graph acts as the non-causal subgraph that can cause the spurious correlation, while the attached motif can be seen as the causal subgraph, as it does not change across graphs and decides the graph label.\nReal-world graphs: We use two representative real-world graph datasets with groundtruth [1]. 1) Benzene: it includes 12,000 molecular graphs extracted from the ZINC15 [31] database and the task is to identify whether a given molecule graph has a benzene ring or not. The groundtruth explanations are the nodes (atoms) forming the benzene ring. 2) Fluoride carbonyl: This dataset contains 8,671 molecular graphs with two classes: a positive class means a molecule graph contains a fluoride (F-) and a carbonyl (C=O) functional group. The groundtruth explanation consists of combinations of fluoride atoms and carbonyl functional groups within a given molecule."}, {"title": "5.2 Results on Synthetic Datasets", "content": "Comparison results: Table 2 shows the results of all the compared GNN ex-plainers on the 6 synthetic datasets with 500 testing graphs and 3 metrics. We have several observations. In terms of explanation accuracy, CXGNN performs comparable or slightly worse than causality-inspired methods. This is because, to ensure high accuracy, the estimated explanatory subgraph of these methods should have a large size. This can be reflected by the explanation recall, where"}, {"title": "5.3 Results on Real-World Datasets", "content": "Comparison results: Table 3 shows the results of all the compared explainers on the real-world datasets and three metrics. We have similar observations as those in Table 2. Especially, no existing explainers can even find one exactly matched groundtruth. Particularly, the explanation subgraphs produced by the two causality-inspired baselines can cover the majority or almost all groundtruth in synthetic datasets (hence high accuracy), and the sizes of the explanation sub-graphs are slightly larger than those of the groundtruth (hence relatively large"}, {"title": "6 Conclusion", "content": "GNN explanation, i.e., identifying the informative subgraph that ensures a GNN makes a particular prediction for a graph, is an important research problem. Though various GNN explainers have been proposed, they are shown to be prone to spurious correlations. We propose a causal GNN explainer based on the fact that a graph often consists of a causal subgraph and fulfills the goal via causal inference. We then propose to train GNN neural causal models to uncover the causal explanatory subgraph. In future work, we will study the robustness of our CXGNN under the adversarial graph perturbation attacks [17, 22, 34-37, 45]."}, {"title": "B More Background on Causality", "content": "According to the literature, causality interprets the information by the Pearl Causal Hierarchy (PCH) layers [25]."}, {"title": "Definition 4 (PCH layers)", "content": "The PCH layers \\( L_i \\) for i \u2208 1,2,3 are: \\( L_1 \\) asso-ciation layer, \\( L_2 \\) intervention layer, and \\( L_3 \\) counterfactual layer."}, {"title": "Definition 5 (G-Consistency)", "content": "Let G be the causal structure induced by SCM \\( M^* \\). For any SCM M, we say M is G-consistent w.r.t \\( M^* \\) if M imposes the same constraints over the interventional distributions as the true \\( M^* \\)."}, {"title": "Definition 6 (G-Constrained NCM)", "content": "Let G be the causal structure induced by SCM \\( M^* \\). We can construct NCM M as follows: 1) Choose \\( \\bar{U} \\) s.t. \\( \\hat{U}_c \\) \u2208 \\( \\bar{U} \\), where any pair \\( (V_i, V_j) \\) \u2208 C is connected with a bidirected arrow in G and is maximal; 2) For each \\( V_i \\) \u2208 V, choose Pa(\\( V_i \\)) \u2286 V s.t. for every \\( V_j \\) \u2208 V, \\( V_j \\) \u2208 Pa(\\( V_i \\)) iff there is a directed arrow from \\( V_j \\) to \\( V_i \\) in G. Any NCM in this family is said to be G-constrained."}, {"title": "Theorem 5", "content": "Any G-constrained NCM M(\\( \\theta \\)) is G-consistent."}, {"title": "C Proofs", "content": "In this section, we provide proofs of the theorems in the main body of the paper."}, {"title": "C.1 Proof of Theorem 1", "content": "For a GNN operating on a graph G, there exists an SCM M(G) w.r.t. the causal structure G of the graph G.\nProof. A GNN is a neural network operating on a graph G = (V,E) including set of nodes V and set of edges E. Recall the GNN background in Section 3, the GNN learning mechanism for a node v in the l-th layer can be summarized as:\n\n\nGNN(v) = \\begin{cases}\n\\text{node embeddings } h^{l-1} \\text{ from previous layer } l-1, \\text{ for } u \\in {v} \\cup N(v) \\\\\n\\text{message } m\\_{u v}=\\operatorname{MSG}\\left(h\\_{u}^{l-1}, h\\_{v}^{l-1}, e\\_{u, v}\\right) \\text{ for current layer } l \\\\\n\\text{aggregated message } h^{l}=\\operatorname{AGG}\\left(m\\_{u, v} \\mid u \\in N(v)\\right) \\text{ for current layer } l\n\\end{cases}\n\n\nwhere the above process is iteratively performed k times for a k-layer GNN. In doing so, each node v will leverage the information from all its within k neighborhoods. We denote the k-layer GNN learning for v as:\nGNN(G) = {node embed: {\\( h^l_v \\)}\u222a{\\( h^l_u \\) : u \u2208 N\\_{\\<k}(v)}, message: {\\( m\\_{u,v}, u \\) \u2208 N\\_{\\<k}(v)}}, where \\( h^l_v \\) is v' node feature and we omit the dependence on node v for notation simplicity."}, {"title": "C.2 Proof of Theorem 2", "content": "Given causal structure G of a graph G and the underlying GNN-SCM M(G), there exists a G-constrained GNN-NCM M(\\( \\theta \\),0) that enables any inferences consistent with M(G).\nProof. From the literature, we know there exists a SCM M that includes exact values of observable and latent variables through studying the causes and effects within the SCM structure. First, we show a lemma that demonstrates the in-heritance of neural causal models (NCMs) (see its definition in Section B) from SCMs, which are built upon Definition 5, Definition 6 and Theorem 5.\nLemma 1 ( [43]). All NCMs M(\\( \\theta \\)) (parameterized by \\( \\theta \\)) are SCMs (i.e., M(\\( \\theta \\)) < M). Further, any G-constrained M(\\( \\theta \\)) (see Definition 6) has the same empirical observations as the SCM M, which means G-constrained NCMs can be used for generating any distribution associated with the SCMs.\nBy Lemma 1, we know a G-constrained NCM M(\\( \\theta \\)) inherits all properties of the respective SCM M and ensures causal inferences via G-constrained NCM. In our context, we need to build the corresponding G-constrained GNN-NCM M(G,\\( \\theta \\)) for the GNN-SCM defined in Equation 17. With it, we ensure all G-constrained GNN-NCMs M(G,\\( \\theta \\)) are GNN-SCMs (M(G,\\( \\theta \\))"}, {"title": "C.3 Proof of Theorem 3", "content": "Let a prediction for a graph G be ex-plained. A node v \u2208 G is causally explainable, if \\( p^{\\mathcal{M}(\\mathcal{G}(\\mathcal{G}), \\theta)}(y) \\) can be computed.\nProof. In a graph classification task, GNN predicts a graph label yg for a graph G with label yg. The GNN explanation measures how accurately did the GNN classify the graph by finding the groundtruth explanation \\( \\Gamma_g \\) in the graph G. In other words, the graph explanation demands the nodes in \\( \\Gamma \\) should be as accu-rate as possible. That is, if \\( y_v = \\hat{y}_v; \\forall v \\) \u2208 \\( \\Gamma\\_G \\), then GNN explanation explained G's prediction accurately.\nBased on Theorem 2, G-constrained GNN-NCM M(G, \\( \\theta \\)) induces causal struc-ture G based on the reference node v \u2208 V(G). So, the trained G-constrained GNN-NCM estimates node effect \\( U_{v_i} \\), and edge effect \\( U_{v,v_i} \\) defined in Equa-tion 2. Note that all the effects are respective to the reference node v, and if v changes, the causal structure G will be also changed, and as a result G-constrained GNN-NCM will be completely different.\nAccording to Equation. 5, a G-constrained GNN-NCM M(G, \\( \\theta \\)) calculates \\( p^{\\mathcal{M}(\\mathcal{G}, \\theta)}(y_v) \\) as the expected value of all the causal effects from the neighbor nodes \\( v_i \\), i.e. do(\\( v_i \\)), on the reference node v:\n\n\\forall v \\in V(G), (\\exists v\\_i \\in N\\_{\\leq k}(v)) \\Rightarrow\\left(p^{\\mathcal{M}(\\mathcal{G}(\\mathcal{G}), \\theta)}\\left(y\\_{v}\\right) \\geq 0\\right) \n\nAs the expected value was calculated for v, we can explain v's node label based on the outcome of \\( p^{\\mathcal{M}(\\mathcal{G}(\\mathcal{G}), \\theta)}(y_v) \\)."}, {"title": "C.4 Proof of Theorem 4", "content": "An explainable node v has expressivity defined as \\( \\exp \\_{L2}(\\mathcal{M}(\\mathcal{G}, \\theta))=\\sum\\_{\\psi\\_{v}} \\psi\\_{v} p^{\\mathcal{M}(\\mathcal{G}, \\theta)}\\left(\\psi\\_{v}\\right) \\)."}, {"title": "D Experiments", "content": "CXGNN: The hyperparameter settings were determined through a systematic search"}]}