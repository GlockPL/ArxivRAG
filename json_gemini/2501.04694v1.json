{"title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation", "authors": ["Jie Wu", "Xiao Liu", "Yaoxiang Wang", "Haoling Li", "Xin Zhang", "Wenxiang Hu", "Zhongxin Guo", "Yangyu Huang", "Ying Xin", "Yujiu Yang", "Jinsong Su", "Qi Chen", "Scarlett Li"], "abstract": "Effective instruction tuning is indispensable for optimizing code LLMs, aligning model behavior with user expectations and enhancing model performance in real-world applications. However, most existing methods focus on code snippets, which are limited to specific functionalities and rigid structures, restricting the complexity and diversity of the synthesized data. To address these limitations, we introduce a novel feature tree-based synthesis framework inspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic structure of code, our framework models semantic relationships between code elements, enabling the generation of more nuanced and diverse data. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features. This process enables the identification of more complex patterns and relationships within the code. By sampling subtrees with controlled depth and breadth, our framework allows precise adjustments to the complexity of the generated code, supporting a wide range of tasks from simple function-level operations to intricate multi-file scenarios. We fine-tuned widely-used base models to create the EpiCoder series, achieving state-of-the-art performance at both the function and file levels across multiple benchmarks. Notably, empirical evidence indicates that our approach shows significant potential in synthesizing highly complex repository-level code data. Further analysis elucidates the merits of this approach by rigorously assessing data complexity and diversity through software engineering principles and LLM-as-a-judge method.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [OpenAI, 2023, Zhang et al., 2022] have demonstrated significant potential in the field of code understanding and generation, particularly through pre-training on large-scale code data [Zhu et al., 2024]. However, the latent code knowledge in these models is often underutilized without fine-tuning on high-quality instruction data. Instruction tuning [Wei et al., 2021] has emerged as a critical step in unlocking the full capabilities of code LLMs, enabling a more precise alignment with user intent and improving the usability of the model in real-world scenarios.\nDespite recent advances, most existing methods for synthesizing high-quality instruction data still rely on code snippets as seed data Chaudhary [2023], Wei et al. [2024a], Yu et al. [2023a]. While code snippets demonstrate specific functionalities, they fail to capture the full range of program- ming constructs, patterns, and interactions that are common in real-world programming scenarios. Additionally, due to the inherent rigidity of code snippets, it is difficult to rearrange or recombine them to generate new and diverse combinations. These limitations restrict the overall complexity and diversity of the synthesized data, highlighting the critical need for more structured representations as seed data to overcome these constraints and address real-world programming challenges.\nInspired by Abstract Syntax Tree (AST), we propose a feature tree-based code data synthesis framework that revolves around hierarchical code features derived from high-level abstractions such as variable types and control flow. While AST captures the syntactic structure of code, our approach extends this idea by organizing features into a tree structure that captures the semantic relationships between code elements.\nSpecifically, we extract features from the seed data and use hierarchical clustering to generate a tree structure demonstration, starting from a feature set and proceeding bottom-up. This demonstration serves as a guideline for the LLM to directly extract tree structures from raw code data. To ensure comprehensive coverage of real-world scenarios, we enhance the diversity of features by iteratively expanding the tree structure both in breadth and in depth. Compared to methods that evolve based on individual code snippets or single instructions Luo et al. [2023], our approach is more efficient and achieves broader coverage, as the tree structure provides clear and organized directions for evolution. The resulting feature tree is a large and hierarchical structure, from which we can sample subtrees to generate code data. Our feature tree-based synthesis method offers two advantages: (1) Scalable Complexity: Our framework allows for controlling the complexity of synthesized data by adjusting the depth and breadth of the subtrees, enabling us to create code at different levels of complexity, ranging from simple function-level tasks to comprehensive multiple file solutions. (2) Targeted Learning: By adjusting the probability of sampling features, we can prioritize specific knowledge areas that are underrepresented, ensuring a more balanced and focused learning process for the LLMs.\nWe conduct extensive experiments to validate our feature tree-based code data synthesis framework, training Qwen2.5-Coder-7B-Base and DeepSeek-Coder-6.7B-Base to derive the EpiCoder series model. Our EpiCoder-Qwen-7B trained on 380k frequency-level and 53k file-level data, respectively, achieves state-of-the-art (SOTA) performance among models of comparable size on average across five function-level code generation benchmarks. Furthermore, on file-level benchmark XFileDep, LLM trained with multi-file data exhibits superior performance, underscoring its capability to address programming problems of varying complexity. Surprisingly, our approach also shows remarkable potential in synthesizing highly complex repository-level data, as evidenced by Figure 5. Definitions from software engineering principles and the LLM-as-a-judge methodology were employed to evaluate the complexity of the data. In addition, we constructed feature trees to assess data diversity to highlight the advantages of our approach.\nIn summary, our work makes the following key contributions:\n\u2022 We propose a feature tree-based code synthesis framework that enables controllable synthesis of diverse and complex instruction data, ranging from function-level to file-level tasks.\n\u2022 We synthesize 433k instruction data and train EpiCoder. Our EpiCoder-Qwen-7B achieves state-of-the-art performance among comparably sized models in multiple function-level and file-level benchmarks, demonstrating its capability to tackle programming problems of varying complexity.\n\u2022 By conducting further analysis, we showcase the advantages of our data in terms of com- plexity and diversity, as well as its potential for synthesizing large-scale repositories."}, {"title": "2 Method", "content": "In this section, we present our feature tree-based code generation framework, which consists of three key steps: (1) Feature Tree Extraction (Section 2.1), where we construct the tree demonstration and extract feature trees from seed data; (2) Feature Tree Evolution (Section 2.2), where we iteratively expand the feature tree to enhance diversity and coverage; and (3) Feature Tree-Based Code Generation (Section 2.3), where we use the evolved feature tree to generate diverse code instruction data with varying complexity. An overview of the framework is illustrated in Figure 2.\n2.1 Feature Tree Extraction\nInspired by Abstract Syntax Trees (AST), which represent the syntactic relationships in code, we use the LLM to extract a hierarchical representation that organizes key elements of code into a tree structure to capture more fundamental semantic relationships.\nRaw Code Collection To ensure data diversity and comprehensiveness, we obtain seed data from The Stack v2 [Lozhkov et al., 2024], a large-scale dataset widely used for pre-training code LLMs. Following [Yu et al., 2023a], we apply the KCenterGreedy algorithm [Sener and Savarese, 2018] to select a core set of diverse samples based on the code embeddings encoded by roberta-large-v1 [Liu et al., 2019].\nTree Demonstration Construction To extract features from the seed data, we leverage a powerful large language model (LLM), specifically GPT-40. Since the initial prompt provided to the LLM can significantly impact its response, we propose an iterative method to optimize the demonstration"}, {"title": "2.2 Feature Tree Evolution", "content": "To overcome the limitations in both diversity and quantity of features in the seed data, we expand the feature tree through an evolutionary process. Compared to evolving directly from the seed code or instructions, this approach ensures broader feature coverage and improves efficiency by leveraging the tree's structured representation, which provides clear and systematic directions for evolution. As illustrated in Figure 2(b), at each iteration, a subtree is sampled from the full feature tree. This subtree is then evolved by the LLM along two dimensions, depth and breadth, by adding finer-grained child nodes to existing nodes as well as sibling nodes at the same hierarchical level. These newly evolved subtrees are then merged back into the overall structure, significantly enriching the feature space and facilitating the generation of more diverse and complex code. An example of the evolution of a single subtree is provided in Appendix A.2.\nOne key challenge in feature evolution is to estimate the frequency of newly generated features. Unlike the feature extraction stage, where frequencies are calculated directly, the frequency of an evolved feature is estimated as the average frequency of its siblings, as illustrated in Algorithm 1. This approach reasonably estimates the frequency of new features according to the existing feature distribution, ensuring that evolved features integrate seamlessly into the broader feature tree."}, {"title": "2.3 Feature Tree-Based Code Generation", "content": "Traditional code generation methods that rely solely on code snippets often produce outputs that closely resemble the original seed data, limiting diversity and complexity. We mitigate this issue by generating code based on feature trees and the process is outlined as follows.\nFeature Distribution Adjustment The previously recorded feature frequencies partially reflect the distribution of natural data, helping to simulate real-world scenarios. However, some high-frequency but easy features, such as config and initialize, do not require strong focus during instruction tuning. To address this issue, we adjust the probability distribution of a node's child features as follows:\n$P' = \\frac{exp(log p_i/t)}{\\Sigma_{j \\in C} exp(log p_j/t)}$  (1)\nwhere $p_i$ represents the normalized original frequency of the child feature $i$ and $p'_i$ is the adjusted probability. As detailed in Algorithm 2, the summation is over all child features $j$ in the set $C$, which denotes the set of child nodes for the current parent node. A higher temperature value $t$ leads to a smoother distribution, allowing less dominant features a higher probability of being selected. To further enhance the diversity of the generated data, we employed multiple temperature values during the data synthesis process for a broader range of feature distributions.\nTask Generation To generate diverse coding tasks, we sample a subtree of candidate features from the feature tree according to the adjusted probability distribution. The sampling process is guided by a predefined shape of the subtree to sample, where we recursively apply Algorithm 2 to get the subtree. The LLM then uses this subtree to sequentially generate a scenario and corresponding task. By adjusting the depth and breadth of the subtree to sample, we can flexibly create tasks with varying complexity, providing a broad spectrum of coding challenges.\nCode Generation Based on the generated task, the LLM proceeds to generate an analysis and the corresponding code. The solution code can range from a single function to a comprehensive multi-file project, depending on the task's complexity. By supporting multi-file solutions, this approach enables the generation of code that reflects a realistic project structure and effectively captures cross-file information for problems requiring interactions across different components. As illustrated in Figure 3, different files implement distinct functionalities and collectively form an integrated system through their interdependencies.\nTo improve the quality of the generated data, the solution code is accompanied by relevant test files, which are also generated by the LLM. These tests are executed in an isolated environment, allowing us to identify and filter out incorrect solutions. Through an iterative debugging process, information"}, {"title": "3 Evaluation", "content": "In this section, we introduce the details of the synthetic data (3.1) and evaluate the model's perfor- mance in code generation at different levels. Specifically, in Section 3.2, we evaluate the model's ability using five function-level code generation benchmarks. In Section 3.3, we employ our meticu- lously crafted benchmark to evaluate the model's file-level code generation capabilities.\n3.1 Implementation Details\nWe utilized our pipeline to extract 5k features from the core set of 150k Python language files in The Stack V2. These features were then expanded through evolutionary methods to 140k features. To validate the complexity of the synthetic data, we synthesized 380k function-level data samples and 53k file-level data samples, respectively. We chose DEEPSEEK-CODER-BASE-6.7B [Guo et al., 2024] and QWEN2.5-CODER-7B [Hui et al., 2024] as the base LLMs. To ensure a fair comparison with other baselines, we incorporated the EVOL-CODEALPACA-V1 (applied the same filtering criteria as described in [Yu et al., 2023a]) dataset into the training of only the DEEPSEEK-CODER-BASE-6.7B model. We obtained the EpiCoder-DS-6.7B and EpiCoder-Qwen-7B models after training. For models trained solely on file-level data, we employed additional notation. We evaluated the models on benchmarks corresponding to their respective training levels.\n3.2 Function-level Generation\nMany previous code LLMs have exhibited overfitting to specific benchmarks after fine-tuning, which constrains their ability to generalize to other benchmarks. To prevent this, we employed five benchmarks for evaluation, ensuring they are broad, comprehensive, reliable, and decontaminated.\nHumanEval and MBPP HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021] are popular benchmarks for assessing code generation capabilities. Considering the limited test cases in these benchmarks, we followed previous work [Wei et al., 2024a, Zheng et al., 2024a] and utilized the EvalPlus [Liu et al., 2023] framework to evaluate the model's robustness across a broader range of test cases. To be fair in comparison, we used version 0.2.0 of MBPP+ provided by EvalPlus, which removes some broken tasks (399 \u2192 378 tasks)."}, {"title": "4 Analysis", "content": "4.1 Complexity Evaluation\nWe emphasize that our ability to generate more complex code data stems from its hierarchical feature tree structure. By flexibly adjusting the depth and width of the feature tree, we can dynamically control the complexity of the synthetic data, ensuring adaptability to diverse requirements. To evaluate code complexity, we employ two approaches: 1) using various software engineering principles to assess complexity; and 2) leveraging an external LLM judge to evaluate from multiple perspectives.\n4.1.1 Evaluation from the Software Engineering Perspective\nTo achieve a precise and robust evaluation of code complexity, we first adopt a software engineering perspective to compare the complexity of our generated code (at both the function and file levels) with existing code datasets. This comparison is based on three well-established software engineering metrics: Halstead Complexity Halstead [1977], Strictness Complexity Ray et al. [2016], and Cyclo- matic Complexity McCabe [1976]. Halstead Complexity measures a program's logical complexity by evaluating operands and operators; Strictness Complexity assesses the strictness of execution paths in the main function; and Cyclomatic Complexity quantifies code control flow complexity by analyzing its branching structure.\nTable 5 presents the overall Halstead complexity comparison, with detailed results in Appendix C.2.1. Our function-level data exceeds the runner-up by 2.55 and 20.99 for unique operators and operands, respectively. For total operators and operands, our function-level results are significantly higher, reaching 56.98 and 100.36, roughly double the current baseline. Additionally, the file-level data shows even greater Halstead complexity, far surpassing the function-level results."}, {"title": "4.1.2 Evaluating Code Complexity using LLMs", "content": "We adopt the LLM-as-a-judge methodology for complexity evaluation, comparing our samples with the existing codebase using GPT-40. Specifically, we evaluate code complexity across four dimensions: Error Handling, Modularity, Data Structure Complexity, and Third-Party Dependencies. Detailed evaluation criteria and prompts are provided in the Appendix C.2.2."}, {"title": "4.2 Diversity Evaluation", "content": "Code generation based on feature trees not only results in greater code complexity but also introduces a richer diversity. To assess the feature diversity, we sample 1k instances from ours, CodeFeedback, and other datasets for comparison. Features are extracted from each sample using GPT-40 (with prompt in Appendix C.3.1), and the number of unique features in each dataset is counted.\nTable 8 shows that our data outperforms other datasets in feature diversity, with an average of 8.53 unique features for function-level data and 8.95 for file-level data. Our function-level data significantly improves in areas like data processing (2,533 features), error handling (357 features), dependency relations (305 features), and user interaction (363 features), all 2-3 times higher than in existing codebases. While leading in most categories, our data also remains competitive in aspects like functionality and algorithms. Additionally, our data also surpasses existing codebases in total feature count, as detailed in Table 14 of Appendix C.3.2."}, {"title": "4.3 Data Leakage", "content": "In this section, we investigate potential data leakage issues to ensure that our synthetic data are free from such risks. Specifically, we use the jinaai/jina-embeddings-v3 embedding model to generate embeddings for the output segments of all training data, including our synthetic data and other training datasets used for comparison. For the HumanEval, MBPP, and BigCodeBench benchmarks, we encode their test datasets and compute the cosine similarity between each test instance and all training samples. For each test instance in the benchmarks, we identify the training-test data pair with the highest similarity score and plot the distribution of these scores in Figure 6. Furthermore, through a case-based analysis of similarity scores, we define a threshold for potential leakage (Similarity Score=0.9), with detailed explanations provided in Appendix C.1. Despite the large scale of our dataset, which puts it at a disadvantage when identifying the most similar sample for each test instance, Figure 6 demonstrates that our 380k synthetic function-level data show minimal evidence of data leakage, even for the HumanEval benchmark, where the risk of leakage is most pronounced. Further analysis of similarity scores across other benchmarks supports the conclusion that our synthetic data are not strongly similar to the benchmark. This also confirms that"}, {"title": "4.4 Scaling of Code Instruction Data", "content": "Although both the fields of mathematics and code are characterized by rigorous logic and precision, they exhibit different phenomena in terms of the quantity of instruction data. Motivated by previous analyses of instruction data scaling laws in the mathematical domain, we design experiments to understand the scaling laws in the code domain. We randomly sample 380k data points and set a series of data quantities for our experiments. The results on the HumanEval, MBPP, and BigCodeBench benchmarks are depicted in Figure 7. It is evident that with the increase in data volume, the performance improves significantly. Moreover, even with the data size reaching 380k, there is still an upward trend, demonstrating that our dataset possesses sufficient diversity to effectively combat overfitting."}, {"title": "5 Related Work", "content": "5.1 Code LLMs\nFollowing the success of general LLMs such as GPT-3 [Brown, 2020] and PaLM [Chowdhery et al., 2023] in both academia and industry, models like CodeX [Chen et al., 2021] have catalyzed a new surge of research in code intelligence. The applications of code intelligence have expanded beyond mere code-related tasks to encompass broader real-world scenarios Zhu et al. [2024]. Currently, code LLMs are typically developed through continual pre-training [Roziere et al., 2023] and supervised fine-tuning [Yu et al., 2023a] based on general LLMs, leveraging either unannotated code corpora or high-quality labeled datasets. These methodologies conserve resources compared to training LLMs from scratch and enhances performance in downstream tasks. Given that general LLMs have already extensively utilized real-world data during their pre-training phases, the construction of data for post-training stages remains a critical issue that requires urgent attention [Ding et al., 2024].\n5.2 Data Synthesis for Code\nMeticulously crafting [Zhou et al., 2024] diverse and complex instruction-solution examples is costly, particularly in the coding domain, which necessitates specialized expertise. Current research indicates that using LLMs to generate synthetic instruction pairs is an effective strategy to address the scarcity of instructions data for both coding and general tasks [Wang et al., 2023, Yu et al., 2023b]. WizardCoder [Luo et al., 2023] employs the Evol-Instruct method to synthesize more complex and"}, {"title": "6 Conclusion", "content": "In this work, we introduce a feature tree-based synthesis framework for generating diverse and complex code instruction data. Inspired by Abstract Syntax Trees (AST), our approach constructs hierarchical feature trees to capture semantic relationships within code, enabling scalable synthesis of instruction data with controllable complexity. By evolving the feature tree and sampling features, we enhance both the diversity and complexity of the generated data, offering significant advantages for instruction tuning of code LLMs. The experimental results demonstrate that our synthesized data excels in both diversity and complexity, and the trained EpiCoder achieves outstanding performance in tasks of varying complexity, from function-level to file-level benchmarks. Moreover, our approach shows strong potential for scaling to repository-level code synthesis and advancing the usability of code LLMs in increasingly complex programming environments."}, {"title": "A Appendix of Method", "content": "A.1 Prompts of Feature Tree Extraction\nHere are our draft prompts for pre-extraction and the refined prompts for feature tree extraction. For brevity, only a portion is shown here. The complete prompts can be found in our released code.\nDraft Prompts for Pre-extraction\nExtract high-level information from a code snippet using keywords separated by \"##\". For example:\n1. Function Description: Describe the main functionality of the code. Use keywords such as list sorting ## input parsing ## data storage ## image processing.\n2. Algorithm Details: Describe the specific algorithm used and its characteristics. Use keywords such as dynamic programming ## greedy algorithm ## divide and conquer ## backtracking ## graph traversal.\n3.\nPlease use this code as input and extract as much of the specified information as possible based on the content of the code.\nInput: {source_code}\nOutput: <your answer>\nPart of Refined Prompts for Feature Tree Extraction\nExtract features from the provided code snippets, following the requirements for each category below, formatted in JSON structure.\nCategories to extract:\n1. Programming Language: Note the specific programming language used. Example: [\"Python\", \"Java\"].\n2. Workflow: Outline the main steps and operations the code performs. Example: [\"data loading\", \"preprocessing\", \"model training\", \"evaluation\", \"results saving\"].\n3. Implementation Style: What programming paradigm the code follows. Example: [\"procedural\", \"object-oriented\", \"functional\"].\n4. Functionality: Explain the function of the code. Example: [\"data processing\", \"user interaction\", \"system control\"].\n5. Resource Usage: Analyze how the code utilizes system resources. Example: [\"CPU Cycles\", \"GPU ComputeOperations\", \"Network Bandwidth\"].\n6. Data Processing: Describe how the data is processed. This category can include the following subcategories:\n\u2022 6.1 Data Preparation: Steps taken to prepare data for further processing. Example: [\"validate units\", \"strip whitespace\"].\n\u2022 6.2 Data Retrieval: Methods for obtaining data. Example: [\"fetching records\", \"retrieve top-level items\"].\n\u2022 6.3 Data Transformation: Describe data transformation operations. Example: [\"convert to numpy array\", \"jsonschema\"].\n\u2022 Other relevant subcategories...\n7. Computation Operation: What computation operations are used. This category can include the following subcategories:\n\u2022 7.1 Mathematical Operation: Specify mathematical computations, such as calculations in- volving statistics or algebra. Example: [\"standard deviation calculation\", \"compute power flow\"].\n\u2022 7.2 Algorithmic Operation: Identify algorithm-based operations, such as those in optimiza- tion or data sorting. Example: [\"simulated annealing\", \"Best-Fit Decreasing\"].\n\u2022 7.3 Statistical Operation: Note operations involving statistical analysis. Example: [\"calculate min and max\", \"calculate percentage positions\"].\n\u2022 Other relevant subcategories...\n8. More content is omitted here: The demonstration tree for extracting additional categories has been truncated for brevity. For the full list of categories and detailed instructions, please refer to our code.\nInput: {source_code}\nOutput: <your answer>"}, {"title": "A.2 Case of Feature Evolution", "content": "Prompts for Feature Evolution\nFeature Tree Evolution Task: You are provided with a feature tree represented as a nested JSON structure. Each node in this tree represents a feature or a sub-feature of a software system, with the leaves being the most specific features. Your task is to expand this feature tree both in depth and breadth. Depth expansion means adding more specific sub-features to existing leaves. Breadth expansion means adding more sibling features at the current levels.\nHere are some explanations of the features: {explanations}\nThe input feature tree will be provided in JSON format, and your output should be a JSON structure that represents the expanded feature tree.\nOutput Format:\nExpanded Feature Tree: Provide the expanded feature tree as a JSON structure.\n{example}\nConstraints:\n1. For breadth expansion, add at least 2 new sibling features to each existing node.\n2. For deep expansion, add new sub-features to any leaf node that could have a more fine-grained feature.\n3. Focus on generating new and innovative features that are not present in the provided examples. Please follow the above constraints and expand the feature tree accordingly.\nInput: {feature_tree}\nOutput: <begin>expanded feature tree"}, {"title": "A.3 Case of Task Generation", "content": "To ensure that the language model (LLM) does not consistently default to familiar or common content, we introduced a strategy to guide the selection of features. From the sampled optional features, certain features are designated as mandatory, and the LLM is directed to incorporate them into the scenario and task. Below is an example of how this approach is applied.\nPrompts for Task Generation\nYou are provided with a set of features/keywords, and a specific programming language. Your task is to use these inputs to conceive a specific real-world application scenario that effectively integrates some of these features. Then, based on the scenario, formulate a task or problem that needs to be addressed with code.\nProcedures:\n1. Receive Inputs: These can range from technical specifics like data processing to broader concepts like system interaction.\n2. Select Features: Choose a combination of features from the provided set that can be realisti- cally integrated into a cohesive scenario.\n3. Conceive a Scenario: Think of a practical application where the selected features play a critical role.\n4. Formulate a Task Description: Based on the scenario, formulate a task that needs to be addressed with code. The task should have a certain level of difficulty and test the programmer's coding skills through logical reasoning. Specific details, such as numerical values or environmental conditions, should be included to create a well-defined setting for the task, ensuring the programmer doesn't need to guess any missing information. The task description should not include any code or detailed guidance on code implementation.\n5. Generate an Instruction: Generate a high-level instruction one or two sentence that describes the overall goal or problem to be solved, without diving into the specific implementation details.\nEnclose the selected features with <f> and </f>. Enclose the scenario with <s> and </s>. Enclose the task with <t> and </t>. Enclose the instruction with <i> and </i>.\nInputs:\n\u2022 Optional Features: {optional_features}\n\u2022 Mandatory Features: {mandatory_features}\nOutput:\n\u2022 Features: <f>your answer</f>\n\u2022 Scenario: <s>your answer</s>\n\u2022 Task Description: <t>your answer</t>\n\u2022 Instruction: <i>your answer</i>"}, {"title": "A.4 Case of Code Generation", "content": "To demonstrate the process of generating code based on the previously outlined task, we present the prompts used, the generated code, its test cases, and the debugging process leading to the final implementation.\nA.4.1 Prompt for Code Generation\nThe prompt provided to the model specifies a detailed structure for code generation, including the expected outputs and format."}, {"title": "A.4.2 Initial Code Implementation", "content": "A.4.3 Debugging and Optimization\nInitial execution of the tests revealed several issues:\n\u2022 The IP validation function incorrectly accepted some invalid IP addresses, such as 192.168.1.999.\n\u2022 Edge cases in operational status validation were not handled properly.\nGiven the error message and the original code, the LLM gives a corrected implementation."}, {"title": "B Appendix of Evaluation", "content": "B.1 Cross-File Dependency Benchmark\nThe Cross-File Dependency Benchmark (XFileDep) is a specialized standard benchmark designed to evaluate the performance of code generation models in handling cross-file dependencies. In many real-world programming scenarios, there exists a complex web of dependencies between different code files. XFileDep provides a comprehensive framework that tests a model's ability to generate missing files by considering multiple interdependent code files as context. This benchmark not only measures the model's capability to generate individual isolated files but also assesses its proficiency in understanding and managing dependencies between files. As illustrated in Figure 9, the process of constructing the benchmark consists of six distinct steps.\nStep 1: Data Sample Selection. From the initial cross-file dataset of 35,000 data samples constructed using our pipeline of synthetic data based on features, we filtered out cross-file data samples with fewer than 5 files (excluding any test files), resulting in 3,435 samples. Figure 10 displays the distribution of file counts and the average file length for each data sample.\nStep 2: Dependency File Selection. We utilized Abstract Syntax Trees (AST) to conduct dependency analysis and structural identification of the Python code files. AST allows parsing of the syntactic structure of Python files, enabling extraction of module import dependencies, function definitions, and class definitions. By parsing all code files within the data sample and identifying the collaboration between classes and functions, we documented the details of defined functions and classes along with the information on imported modules. With these capabilities, we were able to traverse the entire data sample, analyze the dependency relationships between files, discern key files, and select a representative candidate file (to serve as the target file for the code generation task) that has a substantial amount of cross-file dependencies. This approach allows us to generate a structured data analysis report. The systematic nature of this analysis allows us to efficiently handle large cross-file data, providing clear dependency graphs and detailed information on code structure. We also filtered out data samples that lacked rich cross-file dependencies, retaining 2,934 samples that met the criteria."}, {"title": "Step 3: Filtering.", "content": "We analyzed the runtime environment, required libraries, and external files (such as .npy, .xlsx, .json, .log) for each data sample. Based on this analysis, data samples that could not provide the necessary files or dependencies were filtered out. We also excluded data samples that had long runtimes or required the generation of specific output files, which made obtaining test results difficult. In addition, to increase the overall difficulty of the task and ensure that cross-file code generation operates at an appropriate file-level length, we filtered out candidate files whose length was less than 300 characters. Finally, we obtained a total of 2,231 data samples.\nStep 4: Test Case Augmentation. To enhance the test coverage of the code within each data sample, we utilized GPT-40 to generate additional test cases. This process ensured that the core functional methods were robustly and comprehensively tested. In Table 9, we have compiled the statistical data before and after the augmentation of the test cases. We conducted a runtime check on the data after augmenting the test cases and obtained 611 samples that successfully passed the test cases. The prompt for augmenting the test case with GPT-40 is illustrated as follows:"}, {"title": "Step 5: Iterative Test Code Refinement.", "content": "For data samples that fail the test cases, the code content, test cases, and error information are extracted. Based on these detailed input descriptions, we utilize GPT-40 for checking and modification, and subsequently re-run the refined test code for validation. We performed a single iteration of modification on the test code, resulting in 394 successful test cases"}, {"title": "Step 6: Unsafe Filtering.", "content": "To ensure the validity of our test cases, we constructed a unit test environment based on the dependency requirements specified in each Python file. We then executed all test cases and filtered out any samples that failed the tests or presented unsafe operations, such as kill, terminate, rmtree, rmdir, and rm. This approach ensures that our canonical solution is absolutely correct. Finally, we retained 930 samples of cross-file data.\nStep 7: Annotation. We selected target files with extensive cross-file dependencies (either frequently invoked by other files or frequently invoking other files). Using GPT-40, we annotated all classes and methods in these files with detailed documentation, emphasizing their purpose and functionality. The annotation process did not alter the original code, and the execution of the annotated files verified the correctness of the ground truth. The prompt for annotating the target file with GPT-40 is illustrated as follows:"}, {"title": "Step 8: Benchmark Construction.", "content": "To maintain a high level of difficulty in the benchmark con- struction, we extracted all code blocks from the functions and classes within the target files, leaving only the import statements, FunctionDef, ClassDef, and the corresponding docstrings. The instruction set provided the names and contents of all other files in the cross-file data sample as context and included the target file's name and skeleton structure for the completion task."}, {"title": "B.2 Case of File-Level Code Generation", "content": "This section provides comprehensive details on file-level code generation. The directory structure of the example and the detailed contents of each file are as follows:"}, {"title": "C Appendix of Analysis", "content": "C.1 Leakage Threshold Setting"}, {"title": "C.2 Complexity Evaluation", "content": "C.2.1 Detailed Metrics from the Software Engineering Perspective.\nWe supplement Halstead-derived metrics in Table 11, which are based on unique operators (n\u2081), unique operands (n2), total operators (N1), and total operands (N2). Among these recognized metrics, our data consistently shows significant performance gains compared to the current codebase. For instance, we achieve notable complexity advantages in program volume and program difficulty. The programming effort and estimated programming time further confirm that our data requires more time and effort to achieve. While the increased complexity may suggest a higher potential for bugs, we address this issue by incorporating test cases during the data systhesis.\nIn Table 12, we break down cyclomatic complexity to observe scores for specific operations, such as while, for, and boolean operations. Table 12 shows that our gains in Cyclomatic complexity are mainly due to the higher occurrence of if, for, except, and return statements. This suggests that our program handles more loops and incorporates a broader range of exception handling scenarios.\nWe also break down the code strictness complexity scores. Table 13 shows that our data achieves a significantly improvement in Doc Strings, indicating a more comprehensive and rigorous considera- tion to code documentation. Additionally, we demonstrate clear advantages in exception handling, return value validation, and type hints, suggesting that our data is more standardized and stringent."}, {}]}