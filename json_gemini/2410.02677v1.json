{"title": "CULTURALBENCH: A ROBUST, DIVERSE AND CHALLENGING BENCHMARK ON MEASURING THE (LACK OF) CULTURAL KNOWLEDGE OF LLMS", "authors": ["Yu Ying Chiu", "Liwei Jiang", "Bill Yuchen Lin", "Chan Young Park", "Shuyue Stella Li", "Sahithya Ravi", "Mehar Bhatia", "Maria Antoniak", "Yulia Tsvetkov", "Vered Shwartz", "Yejin Choi"], "abstract": "To make large language models (LLMs) more helpful across diverse cultures, it is essential to have effective cultural knowledge benchmarks to measure and track our progress. Effective benchmarks need to be robust, diverse, and challenging. We introduce CULTURALBENCH: a set of 1,227 human-written and human-verified questions for effectively assessing LLMs' cultural knowledge, covering 45 global regions including the underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions - each verified by five independent annotators - span 17 diverse topics ranging from food preferences to greeting etiquettes. We evaluate models on two setups: CULTURALBENCH-Easy and CULTURALBENCH-Hard which share the same questions but asked differently. We find that LLMs are sensitive to such difference in setups (e.g., GPT-40 with 27.3% difference). Compared to human performance (92.6% accuracy), CULTURALBENCH-Hard is more challenging for frontier LLMs with the best performing model (GPT-40) at only 61.5% and the worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to converge to a single answer. Our results also indicate that OpenAI GPT-40 substantially outperform other proprietary and open source models in questions related to all but one region (Oceania). Nonetheless, all models consistently underperform on questions related to South America and the Middle East.", "sections": [{"title": "1 INTRODUCTION", "content": "Uneven cultural representation has been a notorious recurrent limitation of LLMs (Santy et al., 2023; Cao et al., 2023; Arora et al., 2023). Yet, establishing a quality benchmark to effectively gauge LLMs' nuanced multicultural knowledge remains a formidable challenge (Hershcovich et al., 2022). Effective benchmarks need to be robust, diverse, and challenging. We believe the previous and existing cultural benchmarks may not be satisfactory to be effective. The concrete consequence is that no recent major LLM releases have included cultural evaluation performance in their technical reports (OpenAI et al., 2023; Dubey et al., 2024; Anthropic, 2024). Conventional human-written benchmarks are static and often fail to keep pace with the evolving capabilities of LLMs (Yang et al., 2023). Alternatively, existing auto-generated benchmarks cannot reflect the real struggles of models and the true concerns of users on multicultural knowledge. They often rely on web resources e.g., Wikipedia (Naous et al., 2023; Fung et al., 2024), and LLMs' responses on established human surveys e.g., World Value Survey (Durmus et al., 2023b; Li et al., 2024). Those benchmarks could be less effective since the scraped web sources have been used directly on training and the surveys have limited cultural concepts. The latest synthetic data benchmark approach (Rao et al., 2024; Fung"}, {"title": "2 RELATED WORK", "content": "Multicultural knowledge evaluation of LLMs have been widely investigated through building ex- tensive knowledge bases (Shi et al., 2024; Keleg & Magdy, 2023); using socio-cultural surveys like World Value Survey (Durmus et al., 2023a; Tao et al., 2023; Ramezani & Xu, 2023); and generating more training data (Li et al., 2024). Here, we select four representative benchmarks with compara-"}, {"title": "3 DATA COLLECTION PIPELINE", "content": "Our data collection pipeline consists of three steps, as illustrated in Fig. 2: (1) Data collection via AI-assisted red-teaming (2) Human quality check on full data (3) Filtering with majority vote. Such a multi-step process enables us to collect robust data for CULTURALBENCH."}, {"title": "3.1 STEP 1: DATA COLLECTION VIA INTERACTIVE AI-ASSISTED RED TEAMING", "content": "Question Formulation. Human annotators are instructed to brainstorm culturally relevant scenarios based on their personal experiences of their cultures (e.g., Singaporeans use tissue packet to reserve seats). A step-by-step guideline with detailed examples is provided to inspire them, as shown in Appendix H.\nThe AI helper bot then transforms the scenario into a structured question with four options, which the annotators can review and edit afterward.\nQuestion Verification & Revision. Human annotators can use the formulated question as basis to challenge the AI verifier in our interactive platform. The platform provide further assistance in revising the questions to make it more challenging by offering various revision strategies along with drafted examples (e.g., \"Negate the Question\"), as shown in Appendix H.\nInternal Filtering. After collecting over 2,600 questions, the researchers carefully reviewed and removed those that are not relevant to any countries/regions (e.g., Bangladesh, Peru), resulting in a filtered set of 2,486 cultural questions."}, {"title": "3.2 STEP 2: HUMAN QUALITY CHECK", "content": "Recruitment Criteria. We collected questions at the country/regional level, pairing each question with a specific region. To ensure culturally attuned and thorough verification, we recruited five annotators for each region through the Prolific platform 1. We set two main criteria to ensure that the recruited annotators have a deep understanding of the culture of the targeted country or region -(1) Nationality (2) Primary residence before age 18. For certain cultures (e.g. the United States, the United Kingdom), when the platform allowed more detailed selections and the collected question targeted specific groups in the country/region, we added detailed criteria such as ethnicity (e.g. African American, Native American), and place of residence (e.g., Wales).\nMultiple Selection Settings. To better reflect the true representation of each cultural question, we allow annotators to select multiple answers on our questions with four options. As a result, some questions may have more than one majority-vote answer. This approach also helps test models' mode-seeking behavior, examining whether they rely solely on cultural stereotypes (i.e., modes) without considering broader cultural diversity."}, {"title": "3.3 STEP 3: FILTERING BY MAJORITY VOTE & CONSTRUCTING BENCHMARKS", "content": "Majority Vote Criteria. To build a robust benchmark that captures the accurate representation on cultural knowledge, we set the majority-vote threshold to be >= 4 out of 5 annotators. During human validation, we first filtered out questions without majority consensus, resulting in a final set of 1,227 questions. Subsequently, we further processed the remaining questions. To construct our CULTURALBENCH in two setups (CULTURALBENCH-Easy: Multiple-choice, CULTURALBENCH-Hard: True/False), we processed the questions differently depending on the numbers of majority votes they contain.\n(1) Single-Mode Questions (Only one majority vote). For CULTURALBENCH-Easy, we directly keep the original question with four options. The gold label is the option with a majority vote (i.e., A, B, C or D). For CULTURALBENCH-Hard, we transform the question with four options into four binary questions. For instance, the question drafted (e.g., \"What do Singaporeans ...? A. Tissue D. ...\") will form binary questions (e.g. \"Is this answer true or false for this question? Answer True or False only. Question: What do Singaporeans...? Answer: Tissue.\")."}, {"title": "4 CULTURALBENCH DESCRIPTION AND DISCOVERED TOPICS", "content": "Our benchmarks cover a wide range of global regions, spanning 45 countries and regions, includ- ing underrepresented regions such as Bangladesh, Zimbabwe, and Peru. A detailed breakdown of regional distribution can be found in Appendix B while example questions by topic are available in Appendix C.\nCULTURALBENCH-Easy. It contains 1,227 multi-choice questions, each with four options. For instance, a question of \u201cWhat do Singaporeans usually use to reserve seats?\u201d with options of \u201cA. Tissue ... D. Book\u201d as shown in Fig. 1. The gold label is the correct option (A, B, C or D). For multi- mode questions (i.e., questions with more than one answer), we added an instruction of \u201cSelecting the option with all applicable statements\u201d to ensure that models consider all possible answers for fair evaluation. For instance, a question of \u201cWhat do Singaporeans...? Selecting the option \nstatements. i) Tissue iv) Books\u201d with options of \u201cA. (i) ... D. (i), (ii), (iv)\u201d. The questions contain 17.2 words on average (\u03c3 = 12.06). Options at various positions have similar number of whitespace-separated words on average, specifically option A with 5.48 words (\u03c3 = 4.24), option B with 5.44 words (\u03c3 = 4.27), option C with 5.57 words (\u03c3 = 4.24), and option D with 5.57 words (\u03c3 = 4.24).\nCULTURALBENCH-Hard. In this dataset, each question is transformed into four binary true/false questions, requiring models to evaluate each option separately. For example, the earlier multiple- choice example in CULTURALBENCH-Easy will transform into four binary questions such as \"Is this answer true or false for this question? Question: What do Singaporeans usually use to reserve seats? Answer: Tissue.\", as shown in Fig. 2. The gold label in this case is either True or False. This"}, {"title": "4.2 DIVERSE TOPICS DISCOVERED ACROSS CULTURES", "content": "Most existing cultural benchmarks have predefined topics to collect data on, typically on universal topics such as dining (Adilazuarda et al., 2024). However, this approach can overlook cultural el- ements unique to specific regions. To capture a broader spectrum of cultural topics, we adopted a discovery-based approach by encouraging human annotators to brainstorm cultural concepts from their personal experiences. The detailed instruction for annotators can be found in Appendix H. CULTURALBENCH spans a diverse range of cultural elements with 17 topics under three categories (Daily life, Social Etiquette, and Wider Society), as shown in Fig. 3. Daily life relates to the every- day experiences of people e.g., Workplace. Social Etiquette means the acceptable norms in society e.g., Greeting. Wider Society included special elements for broader spectrum of cultural topics e.g., Celebrations. We classified questions into topics by prompting GPT-40-mini. The classification prompt and the topic detailed definitions are in Appendix C.\nTo collect diverse data for each culture, we allow each annotator to create at most 3-7 questions, depending on the availability of annotators for each region. Notably, in curating CULTURALBENCH, we observed that people from different regions focused on distinct topics. For instance, annotators from Italy and Mexico provided more questions related to Food, with 15 out of 35 questions and 13 out of 49 questions respectively. In contrast, participants from South Africa and India focused more on Religion, contributing 19 out of 58 questions and 14 out of 46 questions respectively. Our discovery-based approach allow us to capture diverse cultural elements from people in different regions without being limited by a predefined set of topics."}, {"title": "5 EXPERIMENTS: EVALUATION OF LLMS ON CULTURALBENCH", "content": "We evaluate 30 current LLMs in a zero-shot setting on CULTURALBENCH in two setups: (1) CUL- TURALBENCH-Easy: Multiple choice; (2) CULTURALBENCH-Hard: True/False. We prompted the models to ensure they follow the output format to allow fair comparison. The detailed prompt is in Appendix D. To avoid exposing the correct answers to models for fair comparison, our annotation platform, which involves using OpenAI APIs did not allow the collected data to be used for further training.\nCULTURALBENCH-Easy. We evaluate model performance by measuring accuracy, specifically whether the model correctly identifies the label for each multiple-choice question. A random base- line can achieve 25%.\nCULTURALBENCH-Hard. We evaluate model performance based on the proportion of tasks in which the model can get all four options predicted correctly. For each task, an LLM has to make four binary judgements (True/False) from the transformation of four options in each multiple choice question. To demonstrate robust cultural knowledge, we believe the LLM has to accurately which option(s) are False as well as which option(s) are True. A random baseline can achieve 0.54 = 6.25%."}, {"title": "5.1 COMPARING LLMS ON TWO BENCHMARKS ACROSS MODEL FAMILY AND SIZE", "content": "We show the performance of 18 models across model families and sizes on CULTURALBENCH-Hard in Fig. 4. The corresponding Fig for CULTURALBENCH-Easy is in Appendix A.\nModels performance on CULTURALBENCH-Easy. The best-performing model, GPT-4o, achieves 88.8% accuracy, slightly lagging behind human performance at 92.4%, as illustrated in Appendix A. Nonetheless, this benchmark remains an effective tool for assessing model capabilities, with the lowest score of 61.7% (Claude 3-Haiku) clearly highlighting the wide range of model performance.\nModels performance on CULTURALBENCH-Hard. As shown in Fig. 4, this benchmark is signif- icantly more challenging for current LLMs, with accuracy ranging from 21.4% for Llama3-8b to 61.5% for GPT-4o. These scores are considerably lower compared to the human baseline of 92.6%, highlighting the difficulty of the task even for the most advanced models."}, {"title": "5.2 INVESTIGATING EFFECTS OF QUESTION TYPE AND TIME VERSION OF MODELS", "content": "LLMs show distinct gaps between question types, unlike humans. We evaluate the performance based on question types \u2013 (1) Single-mode (N=1141) and (2) Multi-mode (N=86). The first type refers to the questions with only one correct, majority-voted answer while the second type includes questions with multiple correct answers, as explained in Section 3.3. In Fig. 5 (Left), the average across all models shown in Fig. 4 is 43.4% on Single-mode questions and 25.2% on Multi-mode"}, {"title": "5.3 STUDYING DIFFERENT PROVIDERS' LLMS ON QUESTIONS FROM DIFFERENT REGIONS", "content": "We include detailed performance of models across different family and sizes (shown in Fig 4) to understand how well different models performance in questions relating to different geographic regions at a continent/sub-continent level.\nOverall, models perform better in questions relating to North America, South Asia, and West/South Europe. From Fig. 4, it is evident that models achieve higher performance averages in regions like North America (58.0%), South Asia (52.3%), West Europe (47.1%) and South Europe (45.4%). We hypothesize that the higher performance in these regions can be attributed to several factors including their representation on web-data used for model training (Longpre et al., 2023) and the proportion of annotators recruited from these regions by LLM providers to curate post-training alignment data. For instance, many annotators are known to be recruited from India as they have good English ability and costs substantially less than their counterpart in the US (Lohchab & Roy, 2024).\nModels score lower in questions relating to South America, East Europe, and the Middle East. Models exhibit lower performances on average in regions like South America (38.2%), East Europe (37.6%), and Middle East/West Asia (33.6%), compared to neighbouring regions such as North America (58.0%) and West Europe (47.1%). These disparities suggest insufficient representation of cultural knowledge from these regions in the training data.\nGPT-4o leads in most regions, followed by Llama-3.1 405b and Claude-3.5 Sonnet. GPT-4o consistently ranks highest across most regions among all tested models. Llama-3.1 405b shows strength in regions where cultural knowledge is traditionally less represented, such as South America and East Europe, while Claude-3.5 Sonnet performs particularly well in other regions e.g., Oceania, West Europe, Africa, and Southeast Asia."}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "While CULTURALBENCH has several advantages over existing cultural benchmarks, we would also like to clarify some of its current limitations as well as ways to address them in the future.\nMultilingual vs. Multicultural. We develop an English-only benchmark as the initial step in evaluating models' cultural knowledge. This approach facilitates fair comparisons of cultural under- standing across different regions. For instance, in underrepresented regions such as Bangladesh, the availability of training data in local languages is often limited. As a result, models lacking sufficient exposure to these languages may struggle to comprehend questions phrased in them (Yong et al., 2023). By employing an English-only benchmark, we can assess models' cultural knowledge re- garding these underrepresented areas without considering their (lack of) proficiency in low-resource languages. Additionally, prior research on multilingual models' emotional understanding (Havaldar et al., 2023) and reasoning skills (Liu et al., 2023) indicates that a model's multilingual capabili- ties may not necessarily correlate with its multicultural competencies. Notably, our discovery-based benchmark includes language elements on some questions, particularly in the Language and Com- munication topic with 6.2%. For example, we included questions like: \u201cWhat do Singaporeans usually say at the end? A. lah ...\". As we await advances in developing stronger multilingual abili- ties in models for low-resource languages, our goal is to establish a robust, diverse, and challenging benchmark to track our progress toward addressing the uneven representation of cultural knowledge.\nSmall sample of human verifiers on subjective cultural knowledge. Due to the limitations of crowd-sourcing platforms like Prolific, the number of available annotators from underrepresented regions, such as Bangladesh, is quite small (fewer than 30 active human annotators). As a result, we were able to recruit only five annotators for consistency verification. To enhance the robustness of our dataset, we allow human verifiers to select multiple labels for each question, ensuring that all possible answers are captured. Additionally, we establish a strict majority-vote threshold (majority votes >= 4 out of 5). During the annotation process, we also provide two extra options: \u201cI don't have knowledge\u201d and \u201cThis question is unanswerable\u201d \u2013 to enable annotators to indicate when they cannot provide a response.\nFurther fine-grained culture classification. We noticed that the country/region classification adopted by our CULTURALBENCH may not capture the cultural diversity within each region. How- ever, the data annotation platform we accessed does not have a further fine-grain classification when recruiting human annotators for most of the regions except for the United States and the United Kingdom. To capture the diversity on these two countries, we revisited the data that have been fil- tered by having not enough majority votes and with mostly responses of \u201cI don't have knowledge\u201d. For example, questions asking for the Welsh custom in the United Kingdom may not be answerable for people living in England. Then, we conducted a second round of human quality check by as- signing those questions for the specific groups of human annotators (e.g., people living in Wales in the United Kingdom), as explained in Section 3.2. We hope to see more data annotation tools for different local cultures to facilitate more fine-grained cultural data collection.\nStrong instruction prompts and strict evaluation criteria on models' outputs. We evaluated models on zero-shot setting with the prompts. However, for some models such as Claude-3 Haiku, they need more instructions to have the right formatting. Therefore, we have added one-line instruc- tion for all models to ensure they outputting the answer in the correct format on our evaluation, as described in Appendix D. However, with the strong instruction prompt, sometimes they still refuse to answer the questions e.g., \"This question ...\" rather than outputting the four options (i.e., A, B, C, or D) on CULTURALBENCH-Easy or the binary labels (i.e., True/False) on CULTURALBENCH-Hard. To ensure the fair evaluation, we set the output token to be 2. The model is treated as answering correctly when its output contains the correct labels only."}, {"title": "7 CONCLUSION", "content": "We present CULTURALBENCH in two setups: CULTURALBENCH-Easy and CULTURALBENCH- Hard. By establishing a robust, diverse, and challenging benchmark to track our progress in cultural knowledge, we hope it can motivate LLM providers to develop models that can be helpful to users across more geographical regions."}, {"title": "ETHICS STATEMENT", "content": "Our data collection has been reviewed by university's IRB board to ensure it has no harm on human annotators. We pay annotators according to our vendor (Prolific)'s guidance, which is higher than the local wage requirement. Our annotation guidance has specifically asked annotators to not include their personal identifiable information when giving their responses. Before human verification, our internal team has reviewed the collected data to ensure there is no harmful or unsafe context such as sexual or violence content."}]}