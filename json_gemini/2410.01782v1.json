{"title": "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "authors": ["Shayekh Bin Islam", "Md Asib Rahman", "K S M Tozammel Hossain", "Enamul Hoque", "Shafiq Joty", "Md Rizwan Parvez"], "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, OPEN-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. OPEN-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, OPEN-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based OPEN-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Models (LLMs) has significantly improved various NLP tasks (Beeching et al., 2023). However, these models often suffer from factual inaccuracies (Min et al., 2023a; Mallen et al., 2022). Retrieval-Augmented Generation (RAG) has emerged as a promising approach to integrate LLMs with external knowledge, thereby improving generation accuracy (Asai et al., 2023; Lewis et al., 2020). Despite this, existing RAG methods demonstrate limited reasoning capabilities, particularly when employing open-source LLMs and addressing high-complexity queries such as multi-hop retrieval augmented tasks (Jeong et al., 2024b; Zhang et al., 2024b). Thus, building an effective RAG model using open-source LLMs remains an open challenge. To address this gap, we present OPEN-RAG, a novel framework aimed at improving reasoning capabilities in RAG with open-source LLMs.\nReasoning over retrieved documents is particularly difficult. In general, retrievers are imperfect and can return noisy passages (Shi et al., 2023). The generated outputs can also be inconsistent with retrieved passages (Gao et al., 2023a) or can even override the LLM's accurate parametric knowledge (Parvez, 2024). Approaches like re-ranking or filtering retrieved documents (Xu et al., 2023; Nogueira and Cho, 2020; Wang et al., 2018) and active retrieval methods (i.e., retrieve only when needed) (Mallen et al., 2023; Jiang et al., 2023a; Trivedi et al., 2023a) have shown promising success in tackling these, but they require substantial human annotations, can filter out useful information, often perform sequential and repetitive calls (hence slow), and can still suffer from distracting content, even in relevant passages (Wang et al., 2023).\nTo address and control these behaviors such as retrieval frequency of the RAG model and guide the generation to be contextually consistent, Self-RAG and its variants (Asai et al., 2024; Yan et al., 2024; Jeong et al., 2024a) adopt a self-reflection-based method. During training, these models learn to generate both task output and intermittent special reflection or critic tokens (e.g., is_supported, is_relevant, etc.), leveraging knowledge distillation from proprietary models like GPT-4. At inference, these generated tokens determine the usability of each candidate output. While these methods enable the model to effectively rank candidate outputs from different retrievals and partially improve"}, {"title": "2 OPEN-RAG: Enhanced Retrieval-Augmented Reasoning", "content": "OPEN-RAG transforms an arbitrary dense LLM into a parameter-efficient sparse MoE model capable not only of self-reflection but also of handling complex reasoning tasks.\nAdditionally, we devise an adaptive hybrid retrieval schema to balance the retrieval frequency and speed trade-off. Below we first present the overview of OPEN-RAG and then discuss the training, including dataset and fine-tuning, and hybrid adaptive inference."}, {"title": "2.1 Overview", "content": "We define OPEN-RAG LLM as a model MG that, given an input query q, generates an output sequence of m tokens 0 = [01, 02, ..., 0m]. To control model behavior and generate more context-supported responses, we adopt the reflection-based generation from Self-RAG (Asai et al., 2024) and augment output vocabularies with four types of special reflection tokens: Retrieval, Relevance, Grounding and Utility. During training, given q, the model learns to first generate the Retrieval tokens ([RT]/[NoRT]) that indicate whether retrieval is necessary to answer q. q. During inference, we employ a hybrid adaptive retrieval schema, leveraging both the Retrieval tokens and model confidence.\nIf no retrieval is needed, MG generates the response using only the parametric knowledge of the LLM (i.e., return o as Ypred). If retrieval is needed, for both single- or multiple-hop from an external knowledge source D = {di}=1, we use a user-defined frozen retriever R to retrieve the top-k documents S = {st}t=1, where each st consists of {r;}j=1 with r; e D and NH denoting the hop size. For each retrieved content st, MG generates a Relevance token, the output response yt, a Grounding token, and a Utility token. The Relevance tokens ([Relevant/Irrelevant]) indicate if st is relevant to q, the Grounding tokens ([Fully Supported/Partially Supported/No Support]) indicate if yt is supported by st, and the Utility tokens ([U:1]-[U:5]) define how useful Yt is to q. We process each st in parallel and generate the final answer Ypred by ranking them (i.e., all yt) based on the weighted sum of the normalized confidence of the corresponding predicted Relevance, Grounding, and Utility tokens (see Figure 1)."}, {"title": "2.2 OPEN-RAG Training", "content": "Here, we discuss our training data collection (Sec 2.2.1) and parameter-efficient MoE fine-tuning (Sec 2.2.2)."}, {"title": "2.2.1 Data Collection", "content": "To empower OPEN-RAG to tackle retrieval-free queries, as well as single- and multi-hop queries that require retrieval, we build our training data using various types of tasks and datasets. Given an input-output data pair (q, y) in an original dataset, we augment the data with reflection tokens (Sec. 2.1) leveraging ground truth annotation or critic LLM C to create supervised data. If the corresponding Retrieval token added by C is [RT], we further augment the data and create three different new instances accordingly as follows. First, we use R to retrieve the top-k documents S. For each retrieved document st, C evaluates whether st is relevant or not and returns the Relevance token. To address both single- and multi-hop queries, we"}, {"title": "2.2.2 Parameter-Efficient MoE Finetuning", "content": "RAG tasks are inherently complex, composed of various components such as queries with single (single-hop) or multiple (multi-hop) passages. The ability to leverage different parts of the model selectively based on such complexities can facilitate more adaptive and fine-grained reasoning capabilities over versatile input contexts. Therefore, instead of traditional dense models that treat all parts uniformly, we propose to transform MG into a MoE architecture on the fly, which learns to selectively activate the most suitable experts dynamically for each query with versatile complexity (e.g., single/multi-hop). This selective activation is learned (fine-tuned) using our tailored training data, ensuring that the model learns to differentiate between useful and misleading information.\nAs open-source models are often used in low-compute settings, OPEN-RAG employs sparse upcycling (Komatsuzaki et al., 2022; Wu et al., 2024) to transform MG into a parameter-efficient sparse MoE. This approach adds only a few million adapter parameters, preserving the same order of active parameters as in the original LLM. The sparse MOE OPEN-RAG model augments the FFN layer of the dense backbone LLM with a parameter-efficient MoE transformer block consisting of a set NE of expert layers E = {e}e along with an efficient routing mechanism as in Figure 3. Each expert layer comprises a replicated original shared FFN layer weight, adapted by an adapter module Ae with parameters \u03b8e. To ensure parameter efficiency, in each expert, we keep the FFN layer frozen and train the adapter module Ae only. In this way, we are only required to store one FFN replica keeping the model size unchanged except for the increase in the parameters in the adapter and the router modules. The rest of the layers, such as Norm and Attention, are copied from the dense model.\nFor a given input x, the router module R activates Top-k experts out of NE experts based on the normalized output Xin of the attention layer. Given W|.| denotes the weight of the corresponding expert module, we define the router module as follows:\nR(xin) = Softmax(Top-k(Wr.xin)) (1)\nWe formulate the adapter Ae as:\nAe(x) = (xWdown)Wup + x. (2)\nThe efficiency of OPEN-RAG model results from the setup that |0e| = |Wdown | + |WP| << |40| where we keep o from the dense LLM frozen during fine-tuning. Finally, we express the output y of a parameter-efficient expert module as:\ny = \\sum_{e=1}^{NE} R(x)_e A_e(E_e(x)). (3)\nIn our implementation, we use NE = 8 and k = 2 if not otherwise specified. In other words, only 2 of the 8 experts are active during training and inference. We train OPEN-RAG with QLoRA (Dettmers et al., 2023) adapters during fine-tuning which has a load-balancing objective along with the standard conditional language modeling objective. To mitigate the approximation error in the expert adapters, we use the adapters with a dimension of 512 by default."}, {"title": "2.3 Hybrid Approach for Adaptive Retrieval", "content": "Since LLMs possess different parametric knowledge, instead of using other LLMs, we propose a hybrid adaptive retrieval method with two threshold alternatives based on model confidence to determine retrieval on-demand and balance performance speed. We take motivation from both control token-based (Asai et al., 2024; Lu et al., 2022) and confidence-based (Liu et al., 2023; Jiang et al., 2023a) inference methods.\nDuring training, MG learns to generate Retrieval reflection tokens ([RT] and [NoRT]). At inference, we measure the confidence of the output sequence o conditioned on an enforced no retrieval setting by adding [NoRT] to the input, such that q = q [NoRT]. We design two different confidence scores f|.|: (i) fminp, the minimum value of the probabilities of the individual tokens, and (ii) fmeanp, the geometric mean of the probabilities of the individual tokens in the generated sequence.\nf_{minp}(o|\\hat{q}) = min_{i=1}^m p(o_i|\\hat{q},o_{<i}) (4)\nf_{meanp}(o|\\hat{q}) = (\\prod_{i=1}^m p(o_i|\\hat{q},o_{<i}))^{\\frac{1}{m}} (5)\nWe control retrieval frequency with a tunable threshold \u03b3, where retrieval occurs if f|.| < \u03b3."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Tasks and Datasets", "content": "Single-hop short-form tasks include PopQA (Mallen et al., 2022), TriviaQA-unfiltered (Joshi et al., 2017), and PubHealth (Zhang et al., 2023). These datasets involve answering factual questions and verifying public health facts, using retrieved contexts provided by Self-RAG. We use the accuracy metric for evaluation.\nSingle-hop long-form generation tasks cover biography generation (Bio) (Min et al., 2023b) and the long-form QA benchmark ALCE-ASQA (Gao et al., 2023b; Stelmakh et al., 2022). Biographies are evaluated with FactScore (Min et al., 2023b), while ALCE-ASQA uses official metrics for correctness (str-em) and fluency based on MAUVE (Pillutla et al., 2021).\nMulti-hop reasoning tasks include HotpotQA (distractor dev split) (Yang et al., 2018a), MuSique-Ans (Trivedi et al., 2022), and 2WikiMultihopQA (Ho et al., 2020) which require systems to answer complex multi-hop questions. We use official EM and F1 metrics for evaluation."}, {"title": "3.2 Experimental settings", "content": "Training Data and Settings. In our data curation process, as detailed in Section 2.2.1, we compile a diverse set of instruction-following input-output pairs encompassing retrieval-free, single-hop, and multi-hop datasets requiring retrieval. For no-retrieval and single-hop datasets, we utilize 150K instruction-output pairs curated by Self-RAG. For the multi-hop dataset, we randomly sample 16K two-hop instances from the HotpotQA (Yang et al., 2018b) Distractor train split, each with 10 passages annotated with the ground truth Relevance tokens. Using our data collection method from Section 2.2.1, we generate 28K new multi-hop training instances. All other reflection tokens are labeled by the Llama27B (Touvron et al., 2023) critic LLM in Self-RAG, which is distilled from GPT-4. Additional information regarding training is provided in Appendix Section A. Following previous works and for a fair comparison, we use the Llama27B (Touvron et al., 2023) as the base RAG model MG. OPEN-RAG is transformed into a MoE model with NF = 8 and k = 2, incorporating adapters with a dimension of 512, totaling an additional (8\u00d7135M) adapter model parameters. Moreover, we train a larger version of OPEN-RAG based on Llama213B with additional (8\u00d7213M) parameters to demonstrate the scalability of our framework. By OPEN-RAG model, we indicate OPEN-RAG7B+8\u00d7135M if not explicitly mentioned.\nInference Data and Settings. We assign the default weight of 1.0, 1.0, and 0.5 to Relevance, Grounding, and Utility tokens respectively. Following Self-RAG, we compare the model performances with always retrieval and vary the retrieval frequency as discussed in Sec 2.3 only to demonstrate optimum thresholding and performance-speed trade-offs. In multi-hop evaluations, from the corresponding retrieval candidate passages, we use Beam Retriever (Zhang et al., 2024a) to retrieve Top-3 multi-hop contexts, each with the mentioned NH number of passages. For single-hop tasks, we use Self-RAG's setup (See Appendix B)."}, {"title": "3.3 Baselines", "content": "Baselines without retrievals. We compare ours with several strong, publicly available pre-trained LLMs, including Llama2-7B,13B (Touvron et al., 2023), SAIL-7B (Luo et al., 2023) as well as instruction-tuned models, Alpaca-7B,13B (Dubois et al., 2023). Additionally, we consider models"}, {"title": "4 Results and Analysis", "content": "Here, we (i) evaluate the RAG models (ii) demonstrate the effectiveness of our adaptive retrieval in balancing the performance-speed (iii) present ablation studies and further analysis."}, {"title": "4.1 Main Results", "content": "Comparison against baselines without retrieval. Table 1 (top and middle blocks) shows the performance of open-source baselines without retrieval. OPEN-RAG demonstrates substantial performance"}, {"title": "4.2 Performance-Speed by Adaptive Retrieval", "content": "As discussed in Sec 2.3, given the query, adaptive retrieval method provides a probability/confidence score from the model. By thresholding on that score, we can control the retrieval frequency and balance the performance-speed trade-off and this can also guide to determine when retrieval is needed. A better scoring method should achieve higher accuracy at any retrieval frequency. In order to demonstrate our hybrid adaptive retrieval scoring over the existing reflection token probability-based method fret in Self-RAG, in Figure 4, we plot"}, {"title": "4.3 Ablation Studies", "content": "Robustness to Different Retrieval (CRAG) Methods. CRAG (Yan et al., 2024) proposes a corrective RAG method where, if corpus (e.g., Wikipedia) retrievals are detected as low-quality, a web search is performed to obtain new retrievals. These new retrievals are then fed into the system. The Self-CRAG method combines both reflection-based models and CRAG-based datasets (Self-RAG + CRAG dataset). We evaluate OPEN-RAG and OPEN-CRAG (OPEN-RAG + CRAG datasets) on the benchmarks (PopQA, PubHealth, and Bio) using CRAG, Self-RAG (Asai et al., 2024), and Self-CRAG as baselines, as illustrated in Figure 5. OPEN-CRAG outperforms all baselines across all tasks. Specifically, OPEN-RAG achieves 2%, 4% higher accuracy than Self-CRAG in (Bio, PopQA) and PubHealth respectively. This demonstrates OPEN-RAG's robustness to retrieval quality and its potential for improvement with high-quality contexts.\nRouting Analysis of OPEN-RAG. We perform routing analysis for PopQA, PubHealth, HotpotQA, and 2WikiMultihopQA tasks to demonstrate Top-2 expert activation in different layers during retrieval-free generation by OPEN-RAG as illustrated in Figure 6. We observe, that E7 is a general expert that is highly activated in the first (Layer 1), middle (Layer 16), and final (Layer 32) layers for all datasets. Whereas E2 is activated in the first layer while E6 is activated mostly in the final layer. In the middle layer, we also observe a higher activation of E5 and a lower activation of E7 in the PopQA and PubHealth datasets (single-hop), but the opposite in the case of multi-hop datasets \u2013 showing that the experts implicitly learn to identify query complexity and play important roles across layers for different kinds of task complexities.\nSparse Upcycling Hyperparameters. We experiment with different hyper-parameters of OPEN-RAG as shown in Table 2. We observe that increasing the number of experts Ne slightly improves the performance in MuSiQue, and performance improvement in training longer (epoch 1 vs 2). Increasing the number of active experts k from 2 to 4 causes performance degradation showing the necessity of less active experts.\nImpact of Modules. It is important to understand how much gain is coming from our contrastive learning and how much from the architectural transformation. In Figure 7 with reference to Self-RAG, we plot OPEN-RAG performances with both dense and MoE architecture. OPEN-RAG-Dense outperforms Self-RAG-7B by 1.8% in PopQA, 1.6% in PubHealth, 4.2% in ASQA (MAUVE), 17.9% in MuSiQue (EM) and 21.7% in HotpotQA (EM). Moreover, OPEN-RAG-MoE improves over OPEN-RAG-Dense by 1.6% in PopQA, 2.2% in PubHealth, 5.2% in ASQA (MAUVE), 1.6% in MuSiQue (EM) and 1.4% in HotpotQA (EM) both components enhances the model significantly while contrastive learning as highest."}, {"title": "5 Related work", "content": "Complex factual reasoning requires contextualizing information from multiple documents (Trivedi et al., 2022; Yang et al., 2018b). Prior works (Khattab et al., 2022; Press et al., 2023; Pereira et al., 2023; Khot et al., 2023) proposed decomposing multi-hop queries into single-hop queries, then repeatedly using LLMs and Retrievers. In addition, Jiang et al. (2023b) retrieved new documents if the tokens within generated sentences have low confidence. However, the performance improvement of these approaches often comes at the cost of resource-intensive techniques such as interleave Chain-of-Thought (Yao et al., 2023; Trivedi et al., 2023b; Zhang et al., 2024b) or Tree-of-Thought (Chan et al., 2024) reasoning with document retrieval; and requiring external models (Jeong et al., 2024b). In this work, we train a single MoE model capable of answering complex questions in one iteration with a minimal increase in model complexity."}, {"title": "6 Conclusion", "content": "To enhance reasoning capabilities in RAG models with open-source LLMs, we develop OPEN-RAG featuring a PEFT MoE architecture, contrastive learning, and adaptive retrieval. OPEN-RAG shows significant performance improvements in complex reasoning tasks, outperforming SoTA methods. However, there is still a gap in tasks like long-form generation compared to proprietary models, which we aim to address in future."}, {"title": "7 Limitations", "content": "OPEN-RAG has a higher memory footprint due to an increase in total parameters (7.81B) in comparison to Llama27\u00df family baselines (6.74B). But our OPEN-RAG outperforms open LLMs with total parameters ranging from 7B to 65B, rivaling proprietary models such as ChatGPT, Perplexity.ai, and Command R+ in various downstream tasks. Thus, OPEN-RAG eventually reduces the compute and memory cost with 7.01B active parameters during inference in comparison to its performance. Additionally, as our framework is general, future direction can be building stronger sparse-upcycled LLMs based on recent models such as Llama38B and Mistral7B utilizing OPEN-RAG multi-hop training dataset. Although our approach is theoretically applicable to any domain, future work can explore developing high-performance domain-specific RAG based on our OPEN-RAG."}, {"title": "A Training Details", "content": "We train both MoE and Dense models with LoRA rank 64, LORA a 16, and LoRA dropout 0.1. We optimize the models with the AdamW optimizer with a linear learning rate scheduler and a weight decay of 0.0. Both models have a context length of 4096 for facilitating long-context multi-hop QAs. Other training hyper-parameters are mentioned in Table 3."}, {"title": "A.1 Dataset Details", "content": "The complete breakdown of OPEN-RAG training dataset is displayed in Table 4. Algorithm 1 shows the process of the multi-hop training data preparation."}, {"title": "B Inference Details", "content": ""}, {"title": "B.1 Inference Hyper-parameters", "content": "The weights of the Relevance, Grounding and Utility tokens types are 1.0, 1.0, and 0.5 respectively during inference of OPEN-RAG and Self-RAG. During long-form generation, we use the maximum depth of search of 7 and the size of the beam of 2 following Self-RAG. To evaluate the performance in the retrieval setting, we report the performance in the always retrieval setup in Table 1. Next, we employ greedy decoding for OPEN-RAG and Self-RAG; and top-p (nucleus) sampling for open baseline models with temperature 0.8 and p = 0.95.\nWe discuss the different soft retrieval constraints in Section 2.3 and Section 4.2. Moreover, we identify a bug in the implementation of soft-constraint for adaptive retrieval in Self-RAG where the implementation utilizes the log-probability of the Retrieval token instead of the probability."}, {"title": "B.2 Instruction Format", "content": "We utilize standard prompt without any complex prompting, such as Chain-of-Thoughts (CoT). For single-hop tasks, we follow the instruction format in Self-RAG, whereas the instruction format for multi-hop question answering is shown in Table 5."}]}