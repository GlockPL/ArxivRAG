{"title": "CRRG-CLIP: Automatic Generation of Chest Radiology Reports and Classification of Chest Radiographs", "authors": ["Jianfei Xu", "Thanet Markchom", "Huizhi Liang"], "abstract": "The complexity of stacked imaging and the massive number of radiographs make writing radiology reports complex and inefficient. Even highly experienced radiologists struggle to maintain accuracy and consistency in interpreting radiographs under prolonged high-intensity work. To address these issues, this work proposes the CRRG-CLIP Model (Chest Radiology Report Generation and Radiograph Classification Model), an end-to-end model for automated report generation and radiograph classification. The model consists of two modules: the radiology report generation module and the radiograph classification module. The generation module uses Faster R-CNN to identify anatomical regions in radiographs, a binary classifier to select key regions, and GPT-2 to generate semantically coherent reports. The classification module uses the unsupervised Contrastive Language-Image Pre-training (CLIP) model, addressing the challenges of high-cost labelled datasets and insufficient features. The results show that the generation module performs comparably to high-performance baseline models on BLEU, METEOR, and ROUGE-L metrics, and outperformed the GPT-40 model on BLEU-2, BLEU-3, BLEU-4, and ROUGE-L metrics. The classification module significantly surpasses the state-of-the-art model in AUC and Accuracy. This demonstrates that the proposed model achieves high accuracy, readability, and fluency in report generation, while multi-modal contrastive training with unlabelled radiograph-report pairs enhances classification performance.", "sections": [{"title": "1 Introduction", "content": "Chest radiographs are commonly utilized in clinical disease screening and diagnosis because of their advantages of fast imaging and high definition [10]. However, due to the complex imaging characteristics and high-frequency use of chest radiographs, it is challenging for expert radiologists to accurately and consistently process and interpret the vast amount of complex information [29]. Studies show that 20%-50% of nodule diagnoses are missed or misdiagnosed on chest"}, {"title": "2 Related Work", "content": "Object Detection Ren et al. [28] introduced the Faster R-CNN framework to accelerate object detection, which integrates the Region Proposal Network (RPN) with Fast R-CNN [7]. Kisilev et al. [16] applied the Faster R-CNN framework to lesion region recognition in radiographs and proposed a method for semantic description of lesion regions. Ma et al. [19] used enhanced Faster R-CNN to identify spinal cord lesion regions. The VGG used for feature extraction was replaced with ResNet-50, which enhanced the traditional Faster R-CNN model. Inspired by Ma et al. [19], this work will use the enhanced Faster R-CNN framework as the object detector.\nReport Generation The research on radiology report generation focuses on the encoder-decoder framework, showing the trend from the CNN-RNN neural network model to the Transformer [30]. Ni et al. [23] solved the issue of poor fluency caused by the repetition of words and sentences, and a cross-modal retrieval method was introduced into the hierarchical CNN-RNN model. Xiong et al. [40] introduced a hierarchical Transformer model designed for generating radiology reports. Ziegelmayer et al. [47] assessed multi-modal GPT-4 to generate radiological reports. Despite the advancement, current report generation approaches often struggle with the inability to understand the context in radiographs, leading to low personalization in reports [17]. The transparency of the generation process is low, and the poor interpretability of conclusions creates a trust gap between radiologists and the model [39]. Inspired by [2,47], this work uses GPT-2 to generate radiology reports.\nRadiograph Classification Supervised radiograph classification models heavily rely on high-cost datasets with accurate labels [41], resulting in failure to obtain high benefits. The unsupervised radiograph classification model can learn rich features from enormous low-cost unlabelled radiographs, which is widely sought after by scholars [38]. However, the unsupervised model has poor transferability, and the model cannot recognize new categories. Mikolov et al. [20] proposed a self-supervised model, and Zhang et al. [46] applied it in the field of radiographs and achieved good results. Chen et al. [4] found that a self-supervised model utilizing contrastive learning can improve the classification performance of the model on chest radiographs.\nSelf-supervised learning has also been leveraged for multi-modal learning. In radiography classification, images provide detailed visual features of the structure and shape of tissues and lesions, while reports contain helpful conclusions, symptoms and contexts. Radford et al. [27] introduced Contrastive Language-Image Pre-Training (CLIP), which demonstrated outstanding performance across over"}, {"title": "3 Chest Radiology Report Generation and Radiograph Classification Model", "content": "The proposed Chest Radiology Report Generation and Radiograph Classification Model (CRRG-CLIP) (Figure 1) consists of two parts: the radiology report generation (RRG) module and the radiograph classification (R-CLIP) module. RRG module (Figure 2) consists of an object detection submodule, a region selection submodule, and a generation submodule. The R-CLIP module (Figure 3) consists of a CLIP backbone, which includes both an image encoder and a text encoder, along with a downstream linear classifier submodule."}, {"title": "3.1 Radiology Report Generation (RRG) Module", "content": "Object Detection Submodule The Faster R-CNN model [28] is adopted for object detection since it can accurately detect 29 anatomical regions [31] and get the boundary coordinates for each region [22]. When a radiograph is fed into the module, the image features are initially extracted using a ResNet-50 [8] that has been pre-trained on ImageNet [6], and a feature map is generated. Then, on the one hand, a window is sliding over the feature map via the RPN [28], and RPN generates border coordinate predictions and target scores for multiple candidate"}, {"title": "3.2 Radiograph Classification (R-CLIP) Module", "content": "CLIP Backbone Submodule In this submodule, the image encoder uses RestNet-50 to extract image features. The text encoder uses the fine-tuning BioClinicalBERT"}, {"title": "3.3 Training Procedure", "content": "The training process for the model is split into two phases. The initial phase focuses on training the radiology report generation module, while the subsequent phase involves training the radiograph classification module. During the training of the radiology report generation module, the object detection submodule is first trained, so that the model can identify 29 key anatomical regions in the radiograph. The region selection submodule is then trained so that the model can determine the most valuable bounding box for generating the report. Finally, the generation submodule is trained to generate reports according to the image features in the bounding box. During the training of the radiograph classification module, firstly, the CLIP backbone submodule is trained, so that the image and text encoders can extract features respectively, and understand the relationship between radiographs and reports. The final step involved training a downstream classifier submodule that can classify radiographs."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Datasets", "content": "To conduct experiments, this work uses four datasets:\nMIMIC-CXR Database [13] contains data from 227,835 chest radiology reports in TXT format with radiographs in DICOM format. Each report includes various sections. This study only focuses on the 'FINDINGS' section, representing the radiologists' diagnostic results. This helps avoid the impact of low-quality and incoherent erased patient information in other sections due to privacy protection [34]. In our work, the reports were extracted and used to train the multimodal classification model, while the DICOM-format radiographs, which are large and challenging to process, were not utilized.\nMIMIC-CXR-JPG Database [12] is derived from the database of the MIMIC-CXR. The DICOM format image file was converted to JPG format and the unstructured report was converted to structured disease labels. In our work, JPG format radiographs were used as the image data source.\nChest ImaGenome Dataset [37] is also derived from the MIMIC-CXR dataset and annotated in more detail. Each radiograph contains bounding boxes labelled as normal or abnormal tissues. Radiologists described the prominent features of each bounding box with sentences, indicating possible disease names. The final diagnosis report for each radiograph was generated from these sentences. In our work, local region coordinates in the dataset were employed to train the object detection submodule, the correspondence between local regions and diagnostic sentences was used to train the region selection submodule, and high-value local regions were employed as input for the report generation submodule to produce diagnostic sentences.\nRSNA Pneumonia Dataset [14] contains chest radiographs in DICOM format, labels for pneumonia, and other metadata. In the work, this dataset was used to train and evaluate the downstream linear classification submodule.\nTo form a complete dataset for training and evaluating the proposed model, the reports from the MIMIC-CXR Database, the radiographs from the MIMIC-CXR-JPG Database, and the scene graph JSON files from the Chest ImaGenome Dataset were matched using the ID fields subject_id, study_id, and image_id for each radiology report."}, {"title": "4.2 Preprocessing", "content": "The radiology report generation module uses the dataset partitioning rules of the Chest ImaGenome Dataset. The CLIP backbone submodule in the radiograph classification module also uses the same partitioning. The downstream classifier submodule of the radiograph classification module uses data from the RSNA Pneumonia Dataset, partitioned into training, validation, and test sets in a 7:1.5:1.5 ratio. Due to GPU limitations, experiments were conducted on a reduced dataset of 10,000 sampled images (3.70% of the dataset) and their associated reports, maintaining the same partition ratio.\nThe images from the MIMIC-CXR dataset were resized to 512 pixels on the long side, with black padding added to the short side to reach 512 pixels. Random colour dithering was applied to the H channel, Gaussian noise was added, random translation and rotation were performed, and a normalization operation was carried out. For the RSNA Pneumonia dataset, each image was randomly cropped from the centre to 224 pixels, and the brightness (\u00b110%), contrast (\u00b120%), saturation (\u00b120%) and hue (\u00b110%) were randomly changed.\nThe 'FINDINGS' section of the report was extracted with newline symbols removed. Back Translation [34] was applied for data augmentation using Helsinki-NLP's Marian Machine Translation model, translating English to Italian and back to generate semantically similar but differently expressed texts."}, {"title": "4.3 Baselines", "content": "Evaluation was performed on the RRG model for a radiology report generation task and the R-CLIP model for a radiograph classification task. Each model was compared with various baseline models, including high-performance and commercial models, as well as its variations. The details of the compared models for each task are as follows:\nRadiology Report Generation\n\u2022 S&T [33]: a high-performance model that uses CNN and LSTM to construct a neural and probabilistic framework for caption generation.\n\u2022 ADAATT [18]: a high-performance model extensively used in the literature. This model utilizes adaptive attention, allowing the model to decide where to focus on image features during training.\n\u2022 GPT-40 [24]: the advanced GPT-40 from OpenAI. This model is considered as a commercial model used in numerous real-world applications.\n\u2022 RRG-base: the proposed RRG model using only 1.35% of the complete dataset, serving as a baseline. In this model, the imbalance between regions with and without text bounding boxes was ignored, as well as the text generation length limit during report generation.\n\u2022 RRG-opt: the optimized RRG model using 3.70% of the complete dataset. The average number of tokens per report was used as the maximum text generation length. Weights were assigned to regions with and without text bounding boxes, ensuring the model fairly considers both types of regions. The hyperparameter settings for both RRG-base and RRG-opt can be found in Appendix A (Tables 3 and 4).\nRadiograph Classification\n\u2022 ConVIRT [45]: a state-of-the-art model using . For this model's image encoder, both default random initialization and weights pre-trained on ImageNet [6] were used as the initial weights.\n\u2022 R-CLIP-base: a baseline version of the proposed R-CLIP model. This model was trained using image-text pairs consisting of the Radiologist's reports and chest radiographs from the dataset.\n\u2022 R-CLIP-opt: an optimized version of the proposed R-CLIP model. It was trained on image-text pairs, which included radiographs from the dataset and corresponding reports produced by the RRG module. The detailed hyperparameters and training parameters are provided in Appendix A (Tables 5 and 6).\nAll models were trained on Google Colab T4 GPU."}, {"title": "4.4 Evaluation Metrics", "content": "The generated reports were evaluated by BLEU-1 (for the consistency and accuracy), BLEU-2, BLEU-3, and BLEU-4 (for the readability and fluency [26]), METEOR (for consistent meaning with the ground truth), ROUGE-L (for the semantic consistency [3]), CIDEr (for overlap degree of generated text and reference text), and TF-IDF [9] (for the similarity between texts [32]). Meanwhile, Accuracy and AUC were used to evaluate the classification model."}, {"title": "5 Results", "content": null}, {"title": "5.1 Report Generation Results", "content": "The proposed report generation approach shows significantly improved performance after optimization compared to the baseline model. The optimized report generation module performs similarly to the S&T and ADAATT models and surpasses the GPT-40 in terms of fluency and readability, with the autogenerated reports closely resembling those of radiologists. Please refer to Figure 4 for an example of the result and Table 1 for detailed experimental results.\nComparison with RRG-base The baseline model RRG-base was trained using a 1.35% dataset. By fine-tuning the baseline model and increasing the training data to 3.7% of the dataset, the performance of the model was improved on all seven metrics. From the experimental results in Table 1, the BLEU-1 score increased by 7.59%, BLEU-2 score by 7.53%, BLEU-3 score by 6.93%, BLEU-4 score by 8.33%, \u039c\u0395\u03a4\u0395\u039fR score by 4.81%, ROUGE-L score by 0.84%, CIDEr score by 19.58%, with an average improvement of 7.95%.\nComparison with High-Performance Models Limited by GPU hardware equipment, the amount of data used in this experiment is small, only 3.7% of the complete data set, but the performance of the trained model is close to that of previous high-performance models, S&T and ADAATT, trained with the full dataset (Table 1), which proves that the model has strong performance under small samples, and the model can achieve better results if the experimental conditions are sufficient."}, {"title": "5.2 Radiograph Classification Results", "content": "From Table 1, the performance of R-CLIP-base using radiologists' reports and R-CLIP-opt using reports generated by the generative module is similar. It shows that the proposed report generation approach produces reports comparable to those written by humans. Additionally, the proposed classification approach outperformed ConVIRT. This indicates its superior performance compared to a state-of-the-art high-performance model."}, {"title": "6 Conclusions", "content": "In this work, the CRRG-CLIP model, composed of a radiology report generation module based on local features and a radiograph classification module based on multimodal features, is proposed. The radiology report generation module simulates the process of radiologists' interpretation of radiographs, which implements automatic, efficient and accurate segmentation of anatomical regions, extraction of valuable local tissue features, and generation of smooth and professional reports. It enhances the detailed description of the report by focusing on local anatomical features and improves the accuracy of diagnosis. It also solves the problems of low interpretability, poor readability and low fluency of previous models, and provides support for model tuning and abductive analysis of report content. The radiograph classification module enhances the type and quantity of features obtained, improving downstream classification performance by learning the consistency and differentiation of image-text features. The experimental results show that the reports generated by the radiology report generation module and the reports written by radiologists can achieve similar results in classification tasks, which proves that the radiology report generation module can assist radiologists in report writing. In addition, experiments also show that the proposed model can achieve strong performance on small datasets, which is close to the performance of previous high-performance models and exceeds the commercial model. Future work will focus on optimizing model performance through advanced architectures, experimenting with the full dataset, and incorporating human evaluation of generated reports."}, {"title": "A Appendix: Parameter Settings", "content": "Table 3, 4 show the hyperparameter settings and training parameter settings of the report generation module.\nTable 5, 6 show the hyperparameter settings and training parameter settings of the radiograph classification module."}]}