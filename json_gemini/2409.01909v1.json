{"title": "LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models", "authors": ["Lipeng Ma", "Weidong Yang", "Sihang Jiang", "Ben Fei", "Mingjie Zhou", "Shuhao Li", "Bo Xu", "Yanghua Xiao"], "abstract": "Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for an-alyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, mak-ing them more practical. However, these smaller PLMs face chal-lenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs. Our source code and detailed experimental data are available at https://github.com/LeaperOvO/LUK.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing complexity and scale of IT systems, the maintenance and operation of large-scale systems become more challenging. Logs record the valuable runtime status, they play a crucial role in troubleshooting and enable engineers to monitor system health effectively. However, the increasing volume of logs has made manual analysis a harder task [1]. Consequently, numerous automated log analysis methods utilizing machine learning (ML) or deep learning (DL) models have been proposed to automatically analyze logs, encompassing various tasks such as log parsing [2]-[5], anomaly detection [6]\u2013[12], root cause analysis [13]\u2013[15], failure prediction [16]\u2013[18], etc. In particular, with the recent success of pre-trained language models (PLMs) in natural language processing (NLP), especially the advent of large language models (LLMs) represented by ChatGPT and GPT-4 [19], language models (LMs) have garnered significant attention in log understanding and these LM-based approaches [20]\u2013[30] achieve tremendous achievements in automated log analysis due to their outstanding performance.\nAs language models increase in scale and gain enhanced capabilities, two mainstream paradigms have emerged for utilizing LMs in log understanding. The first one is based on PLMs, which follows the pre-train & fine-tune paradigm, such as BERT [31], fine-tuning a PLM with task-specific data and enabling it to specialize in the given task. Considering the need for fine-tuning with limited computational resources and cost savings, LMs in this paradigm are typically smaller in size, such as BERT with 110M parameters. The second one is based on LLMs\u00b9, which follows the In-Context Learning (ICL) paradigm [32], [33], learning from a few examples in the context without updating the parameters. Despite the powerful performance of LLMs, they are sensitive to the context, influencing the model's behavior significantly [34], [35]. In addition, given that modern software systems can produce several petabytes per day [36], [37], directly employing LLMs for log analysis is impractical due to the significant overhead of querying LLMs, such as inference time and network latency [29], [38]. Hence, we argue that the smaller PLM is more practical in log analysis scenarios [23]\u2013[25].\nUnfortunately, the smaller PLM encounters a bottleneck in log understanding due to the inadequacy of expert knowledge. Logs inherently employ concise and highly specialized ter-minology, the smaller PLM struggles to understand valuable information from logs like experts [22]. This issue can be attributed to two main factors: lack of rich sources for knowl-\n\u00b9 We use LLM to specifically refer to the large language models over 10B parameters that utilize the ICL paradigm without fine-tuning the parameters."}, {"title": "II. RELATED WORK AND MOTIVATION", "content": "edge acquisition (e.g. almost unable to find relevant documents for logs coming from Loghub [39]) and small-scale models struggle to capture knowledge [40], [41]. Fortunately, LLMs with extensive knowledge can understand logs comprehen-sively, attributing to training on massive textual data related to code [42] and logging [43], [44]. For example, as depicted in Fig. 1, ChatGPT can better assist in understanding the Mac log. Consequently, LLMs such as Chatgpt can compensate for the lack of knowledge of the smaller PLM.\nTo more effectively harness the knowledge embedded within LLMs for log understanding, we argue that expert knowledge can be elicited from LLMs and subsequently utilized to empower the smaller PLM, making the smaller model under-stand logs like experts. However, there are two main chal-lenges in enhancing log understanding with expert knowledge from LLMs. (1) Effective Knowledge Acquisition: Although LLMs have shown remarkable capabilities in various tasks, LLMs still have difficulty in acquiring complete and accurate knowledge due to hallucination [45], [46]. Therefore, design-ing effective strategies to make LLM generate reasonable ex-pert knowledge is a challenge. (2) Knowledge Enhancement: Since logs and expert knowledge obtained from LLM are heterogeneous data, the smaller PLM cannot directly utilize the knowledge to empower its capabilities. Incorporating external knowledge and perceiving useful information from them to improve log understanding is another challenge.\nTo overcome the challenges mentioned above, we propose a novel knowledge enhancement framework to empower log understanding on a smaller PLM, called LUK. Rather than utilizing LLMs to solve specific tasks directly, LUK first acquires expert knowledge from LLMs, then enhances the log pre-training with the corresponding expert knowledge, and finally the knowledge-enhanced PLM for logs can be fine-tuned to solve downstream log analysis tasks.\nSpecifically, to solve the first issue, we design a multi-expert collaboration framework to acquire domain knowledge from LLMs. Motivated by the waterfall model [47] in software en-gineering, we build a professional team consisting of Director, Executor, and Evaluator, and define the identity and responsi-bility of the roles through prompt, enabling LLMs to think and handle tasks just like role play. Then the team cooperates and interacts to construct expert knowledge. To solve the second issue, we propose two novel pre-training tasks with knowledge enhancement to incorporate and perceive knowledge into the pre-trained model: word-level token prediction and sentence-level semantic alignment.\nTo evaluate the effectiveness of LUK, we conduct experi-ments on the software system and network device logs, includ-ing six log analysis downstream tasks. The experiment results show that LUK can harness knowledge from LLMs more effectively to improve log understanding and achieve state-of-the-art results on different log analysis tasks. In addition, LUK exhibits remarkable generalization and robustness in log analysis, with notable advantages in low-resource scenarios. Our main contributions can be summarized as follows:"}, {"title": "A. Pre-training on Language Models", "content": "Pre-training is the key to the success of the PLMs and LLMs, where a language model is first trained on the extensive corpus with effective pre-training tasks to capture general knowledge and then be adapted to solve different downstream tasks [48], [49]. The emergence of BERT [31] heralds the success of the pre-train & fine-tune paradigm in natural language. Such language models improve the model's ability on a specific task by fine-tuning using task-specific objective functions after pre-training. As the corpus and parameter scale grows, the capability of the language model is enhanced, and In-Context Learning paradigm [32] is proposed due to the ex-pensive cost of fine-tuning whole models. Many studies [50]\u2013[52] have demonstrated that pre-training on domain corpus and fine-tuning with supervised data can improve performance on a specific task, even outperforming LLMs [53], [54]. In the field of log analysis, many studies including pre-training on log corpus [20], [21] and fine-tuning on log analysis tasks [24], [25] have demonstrated the effectiveness of the pre-trained model for log analysis.\nHowever, training solely on general or log corpus struggles to further improve log understanding due to the lack of expert knowledge. KnowLog [22] firstly proposes to enhance log un-derstanding by integrating domain knowledge during the pre-training phase. Nevertheless, the usage scenarios of KnowLog are limited as it heavily relies on documentation created by human experts to acquire knowledge. In contrast, this work addresses more generalized log analysis scenarios where logs are available without accompanying expert knowledge."}, {"title": "B. Large Language Model", "content": "As a significant advancement in artificial intelligence, large language models (LLMs) with training on diverse corpus can generate human-like text and answer questions with high accuracy [55]. In particular, the recent emergence of ChatGPT and GPT-4, which hold huge model scales and align with human feedback, brings a new opportunity to aid software engineering tasks [23], [56]\u2013[58]. LLMs can learn from the prompt context without training the model, which is called In-context Learning (ICL) [59]. In addition, scaling up the model size of LLMs has been demonstrated to enhance its capacity for knowledge encoding and reasoning significantly [40], [60]. Due to the outstanding abilities of LLMs, many recent studies [26]\u2013[30] utilize the ICL paradigm of LLMs for log analysis without fine-tuning the model and make tremendous achievements in log analysis tasks.\nHowever, the computational costs associated with making predictions using LLMs remain challenging in real-world application scenarios. For example, the GPT-3 model with 175 billion parameters requires 326GB of GPU memory to deploy [61]. In addition, the results generated by LLM may be unreliable due to the inherent hallucination problem. Rather than directly utilizing LLMs for specific log analysis tasks, we argue that the smaller PLM can make predictions better in log analysis tasks given sufficient expert knowledge. In this paper, we guide LLMs as domain experts with prompts and explore how to effectively acquire and utilize expert knowledge from LLMs to empower log understanding."}, {"title": "III. METHOD", "content": ""}, {"title": "A. Overview", "content": "Fig. 2 shows the conceptual overview of LUK, which consists of three phases: knowledge acquisition, knowledge-enhanced pre-training, and fine-tuning with downstream tasks. Specifically, in the first stage, we collect various logs as input, and then we design a multi-expert collaboration framework based on LLMs to acquire expert knowledge of logs. In the second stage, we take the raw logs and the corresponding expert knowledge as input. To effectively leverage expert knowledge to empower log understanding on a smaller model, we enhance the log pre-training with knowledge based on BERT-base and propose two pre-training tasks on word level and sentence level. Finally, a specific PLM for logs is obtained, which can be fine-tuned on downstream tasks to solve log analysis tasks. In the following sections, we describe the details of LUK."}, {"title": "B. Multi-Expert Collaboration Framework", "content": "Hallucination is an inherent flaw of LLMs, which may lead to incomplete or incorrect results and challenge constructing expert knowledge. Motivated by cognitive synergy [62] and teamwork theory [63], humans can leverage the power of collaboration and interaction to solve complex problems. As shown in Fig. 3, we design a multi-expert collaboration (MEC) framework, where we assign LLMs to different roles, and these roles collaborate to generate relevant expert knowledge.\n1) Role Design: Based on the classical waterfall model [47] in software engineering, we design a similar waterfall model to analyze logs consisting of three stages: analysis, execution, and evaluation. Thus, we build a professional team based on LLMs and define clear goals for each role, comprising a Director, Executor and Evaluator. Specifically, we pre-define a role card for each role in the prompt to meet specific goals, which contains: character identity, task objective, requirement and query. By role-playing, we can effectively contextualize the usage of LLMs in log analysis, allowing us to tap into their expertise. These three different roles are assigned the following tasks:\n\u2022\nDirector. The goal of the Director is to develop a high-level framework and focus on guiding the Executor in understanding logs by outlining the key points of the log.\n\u2022\nExecutor. As the central role of this team, the Executor receives key points or feedback from the Director or Evaluator. Thus, the Executor undertakes two tasks: 1) Generate the detailed content, adhering to the key points provided by the Director. 2) Revise or polish the content, considering the feedback provided by the Evaluator.\n\u2022\nEvaluator. The Evaluator evaluates whether the content generated by the Executor satisfies the requirements. We define three evaluation requirements for the Evaluator: 1) Completeness, avoiding content missing. 2) Consistency, avoiding deviation from key points. 3) Conciseness, avoiding irrelevant content.\n2) Collaboration: After assigning roles to LLMs, different roles start working to collaborate and interact according to the requirements in the prompt. First, the Director analyzes the input log and plans the key points for understanding the logs. Subsequently, the director provides these insights to the executor. And then, the Executor generates detailed content based on key points. The detailed content will be given to the Evaluator for evaluation. Finally, the Evaluator evaluates the output of the Executor. The detailed content will be directly output as expert knowledge if it meets the evaluation requirements. Otherwise, the feedback will be given to the Executor, who will refine or improve the content according to the feedback. The pseudocode of the multi-expert collaboration framework is outlined in Algorithm 1.\nThis collaboration framework offers two key advantages. Firstly, the introduction of multiple roles enables a multi-perspective analysis of the input log, thereby mitigating the one-sidedness and bias inherent in a single model's response. Secondly, the feedback mechanism enables the correction of model errors, thereby enhancing the accuracy of the acquired knowledge."}, {"title": "C. Pre-Training with Knowledge Enhancement", "content": "The overall pre-training framework is shown in Fig. 4, the input of the pre-training framework is a batch of pairs consisting of logs and the corresponding expert knowledge, then these pairs are fed into two encoders to obtain log representations and knowledge representations through two encoders respectively. We use BERT based on the Transformer [64] encoder structure as the backbone, which benefits from its relatively small number of parameters (110M) and excellent natural language understanding abilities. Finally, we introduce two novel pre-training tasks: (1) token prediction and (2) semantic alignment, to incorporate background knowledge for improving log understanding.\n1) Word-Level Token Prediction: Logs contain many do-main terminologies, to sufficiently understand these domain terminologies, we propose a token prediction pre-training task (TP). Unlike existing word-level tasks of traditional PLMS only utilizing local context to predict tokens, our proposed word-level task requires models to aggregate context and knowledge for predicting tokens, leading to a knowledgeable pre-trained model.\nSpecifically, given a log $l$ and background knowledge $k$ generated from the LLM, we get the corresponding input token sequence $l_s = \\{l_0, l_1,..., l_n\\}$, $k_s = \\{k_0, k_1,..., k_m\\}$ after tokenization. The token prediction task randomly masks a certain percentage (15% in our experiments) of the log tokens $l_i$ with a special [MASK] token, and then tries to recover them by perceiving external knowledge. To perceive knowledge, we design a knowledge perception module (KPM). The process of this module can be described in three steps: Firstly, log encoder and knowledge encoder encode $l_s$ and $k_s$, respectively, and get the corresponding token rep-resentations $\\textbf{l} = \\{l_0, l_1,..., l_n\\}$ and $\\textbf{k} = \\{k_0, k_1,..., k_m\\}$. Secondly, since not all tokens in knowledge contribute equally to the masked token prediction and to measure the importance of each token in knowledge for the token semantic, KPM calculates the semantic similarity between masked token $l_i$ and knowledge $\\textbf{k}$, each token in knowledge is assigned a weight to represent its importance:\n$Q = W_Q l_i, K = W_K \\textbf{k}, V = W_V \\textbf{k}$,\n$\\alpha = \\frac{\\text{softmax}(QK^T)}{\\sqrt{d_k}}, \\textbf{k}' = \\alpha V,$\nwhere $W_Q$, $W_K$ and $W_V$ are learnable parameter matrices, $d_k$ is the dimension of representation and $\\alpha$ refers to the attention distribution. Thirdly, we concatenate the vector $l_i$ of the masked token with $\\textbf{k}'$ and use $[l_i; \\textbf{k}']$ to predict the original token:\n$\\hat{y}_i = \\text{softmax}(W_f [l_i; \\textbf{k}'])$,\nwhere $W_f$ is the weight parameter. At last, the TP objective is to predict the original tokens which are masked out, formulated as follows:\n$L_{TP}(\\theta) = - \\text{log}p(l_i|l_s \\setminus \\{l_i\\})$,\nwhere $l_s \\setminus \\{l_i\\}$ denotes the log tokens sequence $l_s$ with token $l_i$ being masked."}, {"title": "D. Fine-Tuning with Downstream Tasks", "content": "After pre-training, we fine-tune the log encoder on different downstream tasks. We group all downstream tasks into two categories based on the input type: log-single tasks and log-pair tasks. We use the final hidden state of the first token (the [CLS] token) as the sentence representation. For log-single tasks, we input the output of the language model $\\textbf{l}_{CLS}$ into a multi-layer perception network function $f_H$ and obtain the prediction as $f_H(\\textbf{l}_{CLS})$. For log-pair tasks, we follow sentence-bert [65] and use the language model to encode two separate sentences $u, v$, then input $\\textbf{u}_{CLS}, \\textbf{v}_{CLS}$ and element-wise difference $|\\textbf{u}_{CLS} - \\textbf{v}_{CLS}|$ into a multi-layer perception network function $f_H$ and obtain the prediction as $f_H([\textbf{u}_{CLS};\\textbf{v}_{CLS}; |\\textbf{u}_{CLS} - \\textbf{v}_{CLS}|])$."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Data preparation", "content": "In this paper, we collect logs from software systems and network devices to construct expert knowledge and the log pre-trained model. Specifically, we collect software system logs from Loghub [39], including operating system logs composed of Windows and BGL, as well as distributed system logs composed of HDFS and OpenStack. Given that these datasets contain a significant proportion of duplicated logs, we employ widely used Drain [66] to parse these logs and achieve log templates. Subsequently, we only sample one instance log as input that corresponds to each unique template. In addition, we also collect network device logs from the public documentation of two vendors, Cisco\u00b2 and Huawei\u00b3, including three devices: Switches, Routers, and WLAN. The detailed statistics are shown in Table I."}, {"title": "B. Parameters Setting", "content": "As for expert knowledge acquisition, we explore different large language models for experiments, including gpt-3.5-turbo (ChatGPT) and Llama-2-13b-chat-hf [67]. To increase the stability of LLM's output, we set the tem-perature of LLMs to 0.\nFor pre-training, we use bert-base-uncased with 110M parameters in our experiments. During pre-training, we set the batch size as 32, epochs as 50 and the maximum length of the input text as 512. Moreover, the optimizer we adopt is Adam with a learning rate of 5e-5, a weight decay of 0.01, learning rate warmup for 2,000 steps and linear decay of the learning rate after.\nFor fine-tuning, we adopt the cross-entropy loss as the loss function in downstream tasks and we set epoch to 20 and 10 on log-single and log-pair tasks, respectively. We conduct all the experiments on 4 NVIDIA RTX 3090 GPUs with PyTorch 1.10.1."}, {"title": "C. Downstream tasks", "content": "To explore the performance of LUK on different log analysis domains, we conduct experiments on different downstream tasks, including software system and network device logs.\nFirstly, following existing studies in software system logs [39], [68], we evaluate LUK on two widely researched log analysis tasks to validate the representativeness of our ap-proach: anomaly detection and failure identification.\nSecondly, we conduct experiments on network device logs to investigate the model's generalization ability. This domain poses additional challenges due to the presence of various vendors and highly specialized logs. Due to the lack of public tasks on network device logs, we construct four different downstream tasks referring to [22]. These tasks are commonly encountered by engineers and require substantial domain knowledge for effective solutions.\nMoreover, apart from logs involved in pre-training, to verify the generalization capability of LUK, we also collect data beyond the pre-training data for evaluation. In Table II - III we provide statistics for different tasks of their datasets. Next, we give an introduction to each task and its evaluation metrics."}, {"title": "1) Downstream Tasks of Software System Logs:", "content": "\u2022\nAnomaly Detection (AD). Anomaly detection is a widely researched log analysis task to predict whether anomalies exist within a short period of log messages, where the input is a log sequence and the output is True or False. Following Biglog [21], we concatenate each log in the sequence and then input them to the encoder to obtain the representation of the sequence.\nDataset and Metric. Following previous anomaly detec-tion studies [12], [69], we collect datasets from Loghub [39]. To measure the effectiveness of different models in anomaly detection, we report Precision, Recall, and F1 on the True (anomaly) class as evaluation metrics.\n\u2022\nFailure Identification (FI). Failure identification aims to further discern the type of failure present in the anomaly log. Given the log messages, the model is required to determine what error emerges.\nDataset and Metric. This dataset comes from [18], which is an OpenStack dataset including 396 failure tests and 16 kinds of API errors, such as \"network delete error\", \"openstack network create error\u201d. Usually, engineers are interested in whether top-K recommended results contain the correct error, hence, we report the Recall@K rate as the evaluation metric."}, {"title": "2) Downstream Tasks of Network Device Logs:", "content": "To intu-itively understand these tasks, we give examples of each task in Table IV - V.\n\u2022\nModule Classification (MC). MC is a log-single task aiming at identifying which module the log originates from, which input is a log with the masked module name and the output is the corresponding module name.\nDataset and Metric. We collect the log from Table III and replace the module name with [MASK] as input, the module name in the log as ground truth. Obviously, this is a multi-classification task and the model needs to understand the contextual information of the logs to accurately identify their source. As an unbalanced multi-class classification task and considering the importance of different classes, we report Accuracy and Weighted F1 as evaluation metrics.\n\u2022\nFault Phenomenon Identification (FPI). FPI is also a log-single task to identify the fault category to which the log belongs. Different from the previous failure identifi-cation task, FPI is a multi-label classification task due to a log may appear in more than one fault category.\nDataset and Metric. We collect 602 Huawei switches logs covering 43 fault categories from real-world as the dataset, these logs are annotated by experts. Unlike the multi-class classification task, we report Average Accu-racy of all samples [70] as the evaluation metric, where the accuracy for each sample is the number of correctly predicted labels.\n\u2022\nLog and Description Semantic Matching (LDSM). LDSM is a log-pair task aimed at determining whether the semantics of a given log align with the corresponding natural language description, where the input is a log and description pair, and the output is True or False.\nDataset and Metric. We collect descriptions of logs from the documentation, then we build (log, description) pair as ground truth and randomly select one other description for each log as a negative sample. This task requires the model to accurately understand the semantics of the logs and descriptions. As a binary classification task, both positive and negative cases require attention, we report the Accuracy and Weighed Fl as evaluation metrics.\n\u2022\nLog and Possible Cause Ranking (LPCR). LPCR is a log-pair ranking task to find the most probable answer from a list of possible causes for a given log, where the input is a log as a query and an answer candidate set and the output is the ranking result.\nDataset and Metric. We collect logs and the correspond-ing possible causes from the Huawei public documen-tation and build (log, possible cause) pairs as ground truth. Then we randomly select 15 possible causes of other logs the ground truth as a candidate set. This task necessitates that the model understands the background information of the log to accurately identify its potential causes. As a typical ranking task, following [49], we report Precision@K and Mean Reciprocal Rank (MRR) as evaluation metrics, where MRR is a statistic measure for evaluating search algorithms."}, {"title": "D. Baselines", "content": "We categorize baselines for log understanding into three groups according to technology type: traditional deep-learning methods, pre-trained language models and large language models. For each type, we choose two open-source and supe-rior performance methods as baselines. In addition, to ensure a fair comparison, apart from LLMs, we re-produce all baselines from their repositories, and the parameters of baseline models are according to their original settings. Considering that ICL suffers from unstable results due to example selection [33], [34] and well-designed in-context is not the focus of this paper, to compare with the stable result, we utilize the zero-shot capability of LLMs for log analysis.\n\u2022\nB\u0130LSTM [71]. BiLSTM is a deep model in log analy-sis, which converts each log message into a vector by word embedding model and then the vector input to an attention-based BiLSTM model."}, {"title": "E. Evaluation", "content": "We evaluate LUK by answering the following research questions (RQs):\n\u2022\nRQ1: How effective is LUK compared with the cur-rent mainstream methods on downstream tasks?"}, {"title": "RQ2: How effective is LUK in generalization ability?", "content": "As the system evolves, many previously unseen logs will be collected [71]. In this RQ, to verify the generalization of LUK, we conduct experiments on downstream tasks beyond the pre-training logs.\nThe experimental results are shown in Table IX, it can be found that LUK still has superior performance on the pre-training unseen logs, especially on the task of Log and Possible Cause Ranking, LUK improves 7.5% on Precision@1 com-pared to Biglog. From the observations, it can be inferred that the expert knowledge generated by LLMs plays a crucial role in enabling the smaller PLM to acquire a deep understanding of logging principles and mechanisms. This capability ensures that the PLM will not be overfitted on specific logs during training. In contrast, other fine-tuned models fail to capture essential information when confronted with new logs, thus hindering their ability to analyze logs effectively.\nWe can conclude that LUK demonstrates superior general-ization capabilities in understanding previously unseen logs. Expert knowledge from LLMs enables the smaller PLM to effectively capture and utilize critical information from logs, thereby preventing overfitting and enhancing its performance in log analysis tasks."}, {"title": "RQ3: How effective is LUK on unstable log data?", "content": "In real-world systems logs are unstable, meaning that new but similar logs often appear, this is caused by the fact that devel-opers may frequently modify the logging statements in source code. According to the investigation [73], around 20% - 45% of logging statements may change throughout the lifetime. In this RQ, to evaluate the effectiveness of LUK on unstable logs, we conduct experiments on two log analysis tasks with unstable logs. Following [71], we create two synthetic datasets to reflect the unstable characteristics of real-world logs, which are based on the BGL dataset of Anomaly Detection and Huawei Switches dataset of Log and Description Semantic Matching. Specifically, we simulate unstable logs by randomly inserting or removing a few random words in the original log with different injection ratios.\nThe experimental results are shown in Fig. 5, it can be seen that LUK performs much better than other baselines. With the increasing injection ratio of unstable logs, the performance of all methods has declined in different degrees. However, LUK declines relatively smoothly and still maintains high performance even under a high injection ratio. Specifically, as the injection ratio increased from 20% to 40%, LUK's F1 and Accuracy decreased by merely 1% and 0.54% on the anomaly detection and LDSM tasks, respectively. It confirms that LUK is robust enough to the unstable logs. The reason is that incorporating expert knowledge from LLMs into the smaller model assists in noise filtering, thereby enhancing the accuracy of log analysis for the smaller pre-trained model. Compared with CNN and BiLSTM, traditional methods perform the worst on unstable logs, which suggests that the limited semantic understanding of traditional methods hinders their ability to analyze logs more robustly. Compared with BERT and Biglog, although pre-training on log corpus can further improve log understanding, models are still limited to understanding logs with professional knowledge.\nConsequently, we can conclude that LUK demonstrates superior remarkable robustness when handling unstable logs. It can be inferred that relevant expert knowledge from LLMS can teach the smaller PLM to grasp the essence of under-standing logs, demonstrating increased resilience to noise and instability."}, {"title": "RQ4: How effective is LUK with limited labeled logs?", "content": "In real-world scenarios, acquiring a considerable quantity of annotated samples is difficult [12], [23], thereby presenting a significant obstacle to the efficacy of automated log analysis models. To assess the effectiveness of LUK in low-resource tasks, characterized by limited annotations, we conduct experiments on Anomaly Detection and Fault Phenomenon Identi-fication with different ratios of training datasets.\nThe experimental results are shown in Fig. 6, we find that with the reduction of training samples, the performance of various models exhibits a decline, with BERT and Biglog demonstrating a particularly significant downward trend. On the other hand, LUK based on MEC achieves optimal results by fine-tuning the models with different proportions of anno-tated samples. Specifically, on the AD task, compared to the full data, LUK drops only 2.56% in the F1-value with 1% of the training data. And on the FPI task, LUK's Accuracy drops by only 14.4% with 30% of the training data, while BERT and Biglog drop by 31.45% and 31.25%, respectively. This demonstrates that LUK gains expert knowledge from LLM, which helps to compensate for the shortcomings of the smaller pre-trained model when less annotated data is avail-able. Compared with BERT and Biglog, this illustrates that models with limited knowledge struggle to gain an advantage in low-resource scenarios, whereas leveraging knowledge from LLMs can help reduce the reliance on extensive annotation. Compared with utilizing COT to acquire knowledge, on the FPI task with 30% of the training data, the Accuracy of MEC-based LUK is 12.25% higher than COT. This suggests that the MEC framework is more effective, which can be inferred that more rational and accurate knowledge enables the model to gain an advantage in low-resource scenarios.\nIt is worth noting that the FPI task is a real scenario dataset, which cannot be analyzed directly with LLMs considering pri-vacy issues. By using log templates to obtain expert knowledge from LLMs, a task-specific model is constructed based on LUK, which proves the effectiveness of LUK and improves the efficiency of task analysis.\nIn conclusion, LUK achieves outstanding performance in low-resource scenarios. The incorporation of expert knowl-edge acquired from LLMs compensates for the limitations of smaller PLMs when data availability is limited. By rationally utilizing this knowledge, we can build efficient and accurate models even with limited resources."}, {"title": "RQ5: How efficient is the knowledge of LUK from LLMs v.s. retrieved from the documentation?", "content": "Notably, KnowLog [22] is the first proposal to utilize knowl-edge to enhance log pre-training, and its knowledge relies on documentation created by human experts. However, most logs do not have readily available knowledge, KnowLog cannot work in such cases, and we do not consider it as the primary baseline. To further verify the effectiveness of the expert knowledge obtained from the LLM, we compare it with the documentation. Referring to KnowLog, we collect the descriptions of logs from the Huawei and Cisco public documentation as background knowledge to enhance the log pre-training in the same way. To ensure fairness, logs utilized for pre-training are identical to LUK.\nThe results are shown in Table X, which indicates the knowledge acquired by ChatGPT is comparable to the knowl-edge in Huawei documentation but better than the content in Cisco documentation. It is worth noting that the description of logs in Huawei documentation is more detailed, while the description in Cisco documentation is relatively brief. This means that the knowledge generated by LLM can be equivalent to that of high-quality documentation, but better than low-quality documentation.\nSince most logs lack documentation, it is difficult to obtain expert knowledge of logs (e.g., almost unable to find relevant documents for logs coming from Loghub [39]). It should be emphasized that our method without relying on documentation achieves comparable or even better results than documentation. Hence, this work reduces human experts' dependence on expensive knowledge construction and is more practical than collecting knowledge from documentation."}, {"title": "RQ6: How efficient is LUK in inference compared with LLMs?", "content": "Efficiency plays a crucial role in log analysis for practical applications, considering the vast volumes of logs [23"}]}