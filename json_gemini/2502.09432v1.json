{"title": "Dual Formulation for Non-Rectangular Lp Robust Markov\nDecision Processes", "authors": ["Navdeep Kumar", "Adarsh Gupta", "Maxence Mohamed Elfatihi", "Giorgia Ramponi", "Kfir Yehuda Levy", "Shie Mannor"], "abstract": "We study robust Markov decision processes (RMDPs) with non-rectangular uncertainty\nsets, which capture interdependencies across states unlike traditional rectangular models.\nWhile non-rectangular robust policy evaluation is generally NP-hard, even in approximation,\nwe identify a powerful class of Lp-bounded uncertainty sets that avoid these complexity\nbarriers due to their structural simplicity. We further show that this class can be decomposed\ninto infinitely many sa-rectangular Lp-bounded sets and leverage its structural properties to\nderive a novel dual formulation for Lp RMDPs. This formulation provides key insights into\nthe adversary's strategy and enables the development of the first robust policy evaluation\nalgorithms for non-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising foundation for future\ninvestigation into non-rectangular robust MDPs.", "sections": [{"title": "Introduction", "content": "Robust Markov Decision Processes (MDPs) provide a framework for developing solutions that are\nmore resilient to uncertain environmental parameters compared to standard MDPs [1, 2, 3, 4, 5].\nThis approach is particularly critical in high-stakes domains, such as robotics, finance, healthcare,\nand autonomous driving, where catastrophic failures can have severe consequences. The study of\nrobust MDPs is further motivated by their potential to offer superior generalization capabilities\nover non-robust methods [6, 7, 8].\nRobust solutions are highly desirable, but obtaining them is a challenging task. In particular,\nrobust policy evaluation has been shown to be strongly NP-hard [9] for general convex uncertainty\nsets. Consequently, much of the existing work makes rectangularity assumptions, with the most\ncommon being s-rectangular uncertainty sets and its special case sa-rectangular uncertainty sets\n[10, 11, 9, 12, 13, 4, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22]."}, {"title": "Related Work", "content": "To the best of our knowledge, there exists no work on non-rectangular robust MDPs with kernel\nuncertainty. This work is the first to propose an efficient method for robust policy evaluation for\na very useful class of uncertainty sets, otherwise thought to be NP-Hard [9].\nRectangular Robust MDPs. In literature, sa-rectangular uncertainty is a very old\nassumption [5, 4]. [9] introduced s-rectangular uncertainty sets and proved its tractability, in\naddition to the intractability of the general non-rectangular uncertainty sets.\nThe most advantageous aspect of the s-rectangularity, is the existence of contractive robust\nBellman operators. This gave rise to many robust value based methods [14, 17]. Further, for\nmany specific uncertainty sets, robust Bellman operators are equivalent to regularized non-robust\noperators, making the robust value iteration as efficient as non-robust MDPs [18, 15, 19].\nThere exists many policy gradient based methods for robust MDPs, relying upon contractive\nrobust Bellman operators for the robust policy evaluation [16, 20].\nFurther, [21, 22] trie to tweak the process, and directly get samples from the adversarial\nmodel via pessimistic sampling.\nThere exist other notions of rectangularity such as k-rectangularity [10] and r-rectangularity\n[11] which are sparsely studied. However, [26] shows, the theses uncertainty sets are either\nequivalent to s-rectangularity or non-tractable.\nNon-Rectangular Reward Robust MDPs. Policy evaluation for robust MDPs with\nnon-rectangular uncertainty set is proven to be a Strongly-NP-Hard problem [9], in general. For\na very specific case, where uncertainty is limited only to reward uncertainty bounded with Lp\nnorm, [23] proposed robust policy evaluation via frequency (occupation measure) regularization,\nand derived the policy gradient for policy improvement.\nConvergence Rate of Robust Policy Gradient. The robust policy gradient method\nhas been shown to converge with iteration complexity of O(e\u00af4) for general robust MDPs [17].\nHowever, it requires oracle access to robust policy evaluation (i.e., the computation of the worst\nkernel), which can be computationally expensive [17]."}, {"title": "Preliminary", "content": "A Markov Decision Process (MDP) can be described as a tuple (S, A, P, R,\u03b3, \u03bc), where S is\nthe state space, A is the action space, P is a transition kernel mapping S \u00d7 A to As, R is a\nreward function mapping S \u00d7 A to R, \u00b5 is an initial distribution over states in S, and y is a\ndiscount factor in [0,1) [25, 27]. A policy \u03c0: S \u2192 \u25b3\u2081 is a decision rule that maps state space\nto a probability distribution over action space. Let II = (A4) denote set of all possible policies.\nFurther, \u03c0(\u03b1|s), P(s'|s, a) denotes the probability of taking action a in state s by policy \u03c0, and\nthe probability of transition to state s' from state s under action a respectively. In addition, we\ndenote P\u2122(s'|s) = \u03a3\u03b1\u03c0(a|s)P(s'|s, a) and R", "R": "dp) where v := D\u2122 R", "\u03bc\u300cD": "s occupation measure and D\u2122 = (I \u2212 \u04afP\u3160)\u2212\u00af\u00b9 is occupancy matrix [25].\nAs a shorthand, we denote dp(s,a) = dp(s)n(a|s) and the usage shall be clear from the context.\nA robust Markov Decision Process (MDP) is a tuple (S, A, R,U,\u03b3, \u03bc) which generalizes"}, {"title": "Method", "content": "In this section, we derive the dual formulation for non-rectangular robust Markov decision\nprocesses (RMDPs) with uncertainty sets bounded by Lp balls. This dual perspective not only\nintroduces several new research questions but also provides critical insights into the underlying\nproblem. Further, we develop a method to compute the worst kernel, thereby enabling robust\npolicy evaluation."}, {"title": "Dual Formulation of Robust MDPS", "content": "In this section, we derive, for the first time, a dual formulation for robust MDPs. While it\nis more complex than the dual formulation for non-robust MDPs and applies specifically to\nLp-bounded uncertainty sets, it lays the groundwork for all the subsequent results in the paper.\nFrom [20], we know sa-rectangular worst-case kernel Puasa (6) PbkT is a rank-one\nperturbation of the nominal kernel, where k \u2208 K := {k| | ||k||p \u2264 1,1Tk = 0}. Hence, it is\nenough for the adversary to focus on the rank-one perturbations, allowing us to rewrite (2) as\nJup =minminJ minmin\u03bc D-bk R,\nb\u2208B KEK\nb\u2208B KEK\nwhere the last equality comes from Jp = \u00b5T DR. Further, as shown in Lemma 4.4 of [20],\napplying the Sherman-Morrison formula [34] (see Proposition D.1), the robust return can be\nexpressed as:\nJup =minD\"R- Y\u00b5\u00b2D\u00bbb\u00b7R],\nb\u2208B,KEK \u03b3\u03bc D b\nkT D\u2122 R\n1+ykTb\nwhere b := \u03a3\u03b1\u03c0(als)bsa. The following result presents a more compact and interpretable form\nof this expression.\nLemma 3.3. The robust return can be expressed as:\nJu=J-y_max <k><d", "D": "represents the value function with uncertainty radius b as the reward vector.\nFor the first time, the above result expresses the robust return in terms of the nominal\nreturn J\" and a penalty term involving only nominal values (d\", v = v\", and vf). Notably, the\ndenominator term 1 + y(k, v) is strictly positive (see appendix for details).\nIn the subsequent subsections, we delve deeper into evaluating this penalty term and analyzing\nthe nature of the optimal (k, b) for a given policy \u03c0, revealing the adversary.\""}, {"title": "Robust Policy Evaluation", "content": "In this section, we propose an algorithm for robust policy evaluation and establish its performance\nguarantees. The following result states that the robust return can be computed using the function:\nF(X) = max ||Eb||q,\nbEB\nwhere Ex := \u03b3(I-5) [DR-u D \u2013 AD]HR, and HR : R = R consists of easily computable\nquantities."}, {"title": "Revealing the Adversary", "content": "Non-rectangular robust MDPs have been sparsely studied in the literature, and the nature of the\nadversary remains unexplored. The following result provides the first insights into the adversary's"}, {"title": "Policy Improvement", "content": "Once a worst kernel for policy is obtained using Algorithm 1, we can compute the policy gradient\nto update the policy. Alternatively, we can use the policy gradient theorem derived in the result\nbelow.\nLemma 5.1. [Policy Gradient] Given the worst transition kernel Pup = P \u2013 bk, the gradient\nis given as\nR\n\u2207 \u03c0Jup = d\" \u00b0 QR - UR k TvRUT Q\n1+yk v\nJ(k D\u2122) d\u2122 + \u00a52 J (k Tv) (kT Dx) Q\nQ + \u03b3\n(1 + \u03b3k\u2122v)2\nwhere (u o v)(s) := u(s)v(s).\""}, {"title": "Experiments", "content": "In this section, we evaluate the performance of Algorithm 1 for robust policy evaluation, focusing\non the case p = 2. The algorithm requires computing F(X) at each iteration, which involves\nsolving the constrained matrix norm problem maxx\u2208B ||Ax||2. This can be efficiently handled\nusing our spectral Algorithm 2. Figures 4 and 5 compare four methods for computing the robust\nreturn, specifically the penalty term J\u2122 \u2013 J\u016b\u2082:\n\u2022SLSQP from scipy [35]: This is A semi-brute force approach that uses Lemma 3.3.\nIt computes the penalty term via scipy.minimize to directly optimize over (b, k). This\nis equivalent to optimize over only rank-one perturbation of nominal kernel as a(b, k)\ncorresponds to selecting a rank-one perturbation of the nominal kernel P = P \u2212 bkT. Note\nthat this method is local, hence can sometime be stuck in very bad local solution.\n\u2022Binary search Algorithm 1 : Uses the binary search Algorithm 1, and spectral Algorithm\n2 for computing F(X) in each iteration.\n\u2022Random Rank-One Kernel Sampling: A semi-brute force approach that uses Lemma\n3.3 to sample random pairs (b, k) \u2208 B, K, empirically maximizing the penalty term. Since\nchoosing (b, k) corresponds to selecting a rank-one perturbation of the nominal kernel\nP = P \u2013 bk, the method is named accordingly.\n\u2022Random Kernel Sampling: A brute-force approach that samples random kernels\ndirectly from the uncertainty set U2, computing the empirical minimum as an estimate of\nthe robust return."}, {"title": "Discussion", "content": "We studied robust Markov decision processes (RMDPs) with non-rectangular Lp-bounded uncer-\ntainty sets, balancing expressiveness and tractability. We showed that these uncertainty sets can\nbe decomposed into infinitely many sa-rectangular sets, reducing robust policy evaluation to a\nmin-max fractional optimization problem (dual form). This novel dual formulation provides key\ninsights into the adversary and leads to the development of the first robust policy evaluation\nalgorithms. Experiments demonstrate the effectiveness of our approach, significantly outperform-\ning brute-force methods. These findings further pave the way for scalable and efficient robust\nreinforcement learning algorithms.\nOur results naturally extend to uncertainty sets that can be expressed as a\nfinite union of Lp balls. Furthermore, any uncertainty set can be approximated using a finite\nnumber of Lp balls, with smaller balls providing a better approximation. However, the number\nof balls required for an accurate approximation may grow prohibitively large. While this work is\nlimited to Lp norms, it may be possible to generalize the approach to other types of uncertainty\nsets. A key challenge in such an extension would be identifying the structure of the worst-case\nkernel and developing the corresponding matrix inversion techniques.\nAnother promising direction is to design policy improvement methodologies compatible with\ndeep neural networks."}]}