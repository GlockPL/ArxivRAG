{"title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning", "authors": ["Zhirui Deng", "Zhicheng Dou", "Yutao Zhu", "Ji-Rong Wen", "Ruibin Xiong", "Mang Wang", "Weipeng Chen"], "abstract": "The outstanding capabilities of large language models (LLMs) render them a crucial component in various autonomous agent systems. While traditional methods depend on the inherent knowledge of LLMs without fine-tuning, more recent approaches have shifted toward the reinforcement learning strategy to further enhance agents' ability to solve complex interactive tasks with environments and tools. However, previous approaches are constrained by the sparse reward issue, where existing datasets solely provide a final scalar reward for each multi-step reasoning chain, potentially leading to ineffectiveness and inefficiency in policy learning. In this paper, we introduce StepAgent, which utilizes step-wise reward to optimize the agent's reinforcement learning process. Inheriting the spirit of novice-to-expert theory, we first compare the actions of the expert and the agent to automatically generate intermediate rewards for fine-grained optimization. Additionally, we propose implicit-reward and inverse reinforcement learning techniques to facilitate agent reflection and policy adjustment. Further theoretical analysis demonstrates that the action distribution of the agent can converge toward the expert action distribution over multiple training cycles. Experimental results across various datasets indicate that StepAgent outperforms existing baseline methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have begun a revolutionary era in artificial general intelligence (AGI), due to their remarkable capabilities in handling complex interactive tasks with environments and tools [42, 46]. The tasks involve multiple areas including web browsing [12], web shopping [48], house holding [35], and complex question answering [15, 40, 47]. Although these models (e.g., ChatGPT [24] and GPT-4 [25]) are endowed with extensive knowledge during pre-training on a large-scale corpus, they demonstrate a tendency to generate hallucinated content [21, 54]. To tackle this issue and further align with human preferences, researchers have introduced training LLM agents with reinforcement learning (RL) to enhance their ability for complicated task planning and resolving.\nInitial efforts in developing LLM agents [26, 28, 38, 55] concentrated on maximizing the token-level generation probability of the expert actions, denoted in Figure 1(a). These methods, while straightforward and reward-free, fall short when confronted with the training data shortage situation and struggle to generalize beyond the training data distribution. Recognizing these constraints, researchers [2, 11, 36, 37, 57] have shifted towards leveraging manually annotated preferences or the final environment feedback as additional reward signals and conducting reinforcement learning training on the basis of the supervised fine-tuning (SFT) model. Nevertheless, these methods are restricted by the sparsity and delay of the reward signals. As shown in Figure 1(b), existing reward signals are represented as a single scalar reward for each generated observation-action trajectory. Such sparse feedback renders it challenging for the model to discern the quality of each action, particularly for tasks with long reasoning chain. Consequently, the model struggles to precisely refine low-quality actions, resulting in low learning efficiency. The delayed reward feedback prevents the model from making timely corrections, potentially leading to sub-optimal responses.\nProcess-supervised reinforcement learning [27, 41] presents a promising solution to these challenges by providing supervision at each intermediate reasoning step. Throughout the process of agent reasoning, the reward of each intermediate step can assist in identifying underperformed policies timely, allowing for agent"}, {"title": "2 Related Work", "content": "Recently, the outstanding capabilities of large language models (LLMs) have led researchers to explore adopting these models as agent core controllers and constructing artificial intelligence (AI) agents. The development of existing agent systems can be roughly divided into two primary categories: prompt-based methods and fine-tuning-based methods.\nPrompt-based Methods. Prompt-based methods [22, 30] focused on carefully designing the prompt and directly utilizing closed-source large language models, such as ChatGPT [24] or GPT-4 [25], for task planning and reasoning. Chain-of-Thought (CoT) prompting [45] was the fundamental of most prompt-based methods which introduced intermediate reasoning steps in demonstrations to enhance the capacity to do sophisticated reasoning. Inherit the spirit of CoT prompting, ReAct [50] devised a think-and-act format prompt to inspire LLMs to generate both reasoning traces and task-specific actions in an interleaved manner. ToT [49] further generalized to tree-structure ensuring to explore various reasoning paths and make global decisions by looking ahead or backtracking when necessary. Driven by human revision behavior, SELF-REFINE [20] utilized a single LLM as the generator, refiner, and feedback provider. In addition, Reflexion [34] leveraged linguistic feedback maintained in a memory buffer to reinforce agents and induce better decision-making.\nFine-tuning-based Methods. Although prompt-based methods could achieve promising performances without training, they heavily rely on well-designed prompts and advanced closed-source models (e.g., ChatGPT and GPT-4) leading to high usage costs. To address these challenges, recent studies [8, 10, 51, 53] constructed expert trajectory data with teacher agents (e.g., GPT-4 or humans)"}, {"title": "2.2 Reinforcement Learning for LLMs", "content": "With the development of the LLMs, reinforcement learning (RL) [11, 57] plays a vital role in improving the capabilities of LLMs. Actor-Critic [17] was the basis of many advanced RL algorithms which leveraged the actor policy network to interact with the environment and perform policy updates under the guidance of the critic value function. Based on the actor-critic algorithm, Trust Region Policy Optimization (TRPO) [32] introduced trust region to ensure monotonic performance of policy learning while Proximal Policy Optimization (PPO) [33] further proposed penalty and clip strategies to simplify the algorithm implementation. To solve the problem of instability during Reinforcement Learning from Human Feedback (RLHF) training, Direct Preference Optimization (DPO) [29] adopted a simple classification loss to fine-tuning LLMs and achieve higher efficiency and better performances. Since the reward signal is uncertain or sparse in real-world scenarios, researchers proposed behavior cloning (BC) [38] to imitate the behaviors of experts. Furthermore, Generative Adversarial Imitation Learning (GAIL) [14] devised an iterative reward function learning strategy forcing the agent to fit the expert data distribution.\nThe reward function in previous agent approaches was either manually annotated [2, 11, 37] or limited to the final reward feedback from the environment [35, 43, 48]. In this work, we propose a step-wise reinforcement learning method and automatically generate rewards for each step."}, {"title": "3 Preliminaries", "content": "In this section, we first formulate the agent task and then review supervised fine-tuning for LLMs, a crucial step before reinforcement learning that prepares the model for specific tasks."}, {"title": "3.1 Problem Formulation", "content": "The process of an agent interacting with the environment for task solving can be formalized as a partially observable Markov decision process (POMDP) with the state set \\( S \\), action set \\( A \\), observation set \\( O \\), transition function \\( F : S\\times A \\rightarrow S \\), and reward function \\( R: S \\times A \\rightarrow [0, 1] \\). Initially, the environment provides a general task instruction \\( Prompt_{sys} \\) as the system prompt, along with the agent's initial observation \\( o_1 \\in O \\) as the specific task input, and the agent needs to interact with the environment multiple times for completing the task and generating responses.\nSpecifically, at the time step \\( t \\), the large language model agent parameterized by \\( \\theta \\) receives an observation \\( o_t \\in O \\) from the environment and decides to take an action \\( a_t \\in A \\) according to the"}, {"title": "3.2 Supervised Fine-tuning", "content": "Supervised fine-tuning (SFT) entails leveraging relatively smaller labeled expert data to better adapt the pre-trained LLMs to specific domains or downstream tasks [26, 55], providing a solid foundation for creating a powerful agent.\nGiven an expert interaction trajectory \\( t_e = (o_1, a_1,..., o_n, a_n) \\) in the expert trajectory set \\( T \\), we leverage the auto-regressive loss to fine-tune the initial LLM and obtain the base agent \\( \\pi_{\\theta} \\) as follows:\n\\[L_{SFT} = -E_{t_e\\sim T}[\\pi_{\\theta}(a_n|o_1)].\\]\nFollowing Equation (1), \\( \\pi_{\\rho}(a_n| o_1) = \\Pi_{t=1}^n \\pi_{\\theta}(a_t| s_t) \\), where \\( s_t = (o_1, a_1, ..., o_t) \\). We first concatenate the instruction prompt, actions and observations in trajectory \\( t_e \\) as a token sequence \\( w = (w_1, ..., w_L) \\) with length \\( L \\). Then, the probability \\( \\pi_{\\theta}(a_n |o_1) \\) in Equation (2) can be formulated as follows:\n\\[\\pi_{\\rho}(a_n o_1) = - \\sum_{k}log \\pi_{\\rho}(w_k|w_{<k}) \\times1_{w_k\\in A}, \\]\nwhere \\( w_{<k} \\) indicates tokens before the k-th token and \\( 1_{w_k \\in A} \\) is an indicator function indicating whether \\( w_k \\) is a token of actions generated by the agent. We mask the observation tokens and compute the probability solely for the action tokens."}, {"title": "4 From Novice to Expert", "content": "Large language model (LLM) agents have demonstrated superior capabilities in tackling complex interactive tasks, by leveraging reinforcement learning strategy to align the agent policy with human preferences. However, existing research on LLM agents [9, 36] encounter significant challenges stemming from reward signal sparsity and the complexities associated with reasoning process. To address these limitations, in this section, we introduce a step-wise reinforcement learning framework to optimize the agent policy without manually annotating the procedural rewards. Our approach is inspired by the principles of Benner's novice to expert [3, 4], facilitating progressively self-iterative experience acquisition. By constantly monitoring the expert's behaviors and practice spontaneously, the LLM agent can accumulate experience and eventually advance from novice to expert proficiency.\nThe overall framework of StepAgent is depicted in Figure 2. StepAgent comprises two major phases: (1) Inspection and (2) Reflection. The details of the two stages are introduced in the following sections."}, {"title": "4.1 Inspection: Recognizing Capability Gaps", "content": "Inspection, in accordance with Benner's novice to expert theory [3, 4], involves the novice initially observing expert behaviors and attempting to replicate these behaviors independently under the same circumstance. This comparative practice aims to recognize the capability gap between the novice and the expert, thereby facilitating subsequently novice policy improvements. Previous methods for constructing LLM agents [9, 36] focus on observing and imitating the complete behavior trajectory of the expert with the final environmental reward feedback for optimization. However, due to the complexity of the agent tasks, LLM agents need to constantly interact with the environment and engage in trial-and-error to arrive at the ultimate reasoning outcome. The inherent multi-step reasoning characteristics of agent tasks bring dual challenges of efficiency and effectiveness for the novice's self-attempts of the complete trajectory. First, emulating the full trajectory of the expert and acquiring the final environmental feedback require the agent to constantly interact with the environment. This interaction is sequential and cannot be parallelized, resulting in the significant consumption of computational time and resources. Besides, the necessity for the novice to comprehend every expert action simultaneously can lead to information overload. This overload complicates the novice to digest and master the specifics of each behavior, often resulting in inefficient learning processes. Consequently, novices may require additional training data or iterations to fully grasp the insights derived from the expert's experiences.\nTo address these limitations, it is essential for the novice to attentively observe and imitate the expert's actions step-by-step. This enables the novice to identify shortcomings in their behaviors and facilitate the mastery of critical skills. Specifically, considering an expert trajectory \\( t_e = (o_1, a_1,..., o_n, a_n) \\) with n-steps, we segment this trajectory after each action, treating each action as a short-term learning objective for the novice:\n\\[(\u00d4_1, \u00e2_1,\u2026\u2026\u2026, \u00d4_i, \u00e2_i) \u2208 T_{sample}, i = 1,2,..., n.\\]\nWhen the novice establishes learning targets, it triggers the practice stage in the expert-novice learning process. This spontaneous exercise is geared towards identifying the behavioral discrepancies between the novice agent and the expert, allowing for the accumulation of experience and the gradual development of the novice's behavioral patterns through repeated practice. Central to"}, {"title": "4.2 Reflection: Strategizing Policy Refinement", "content": "In novice-to-expert theory, progression toward expert-level performance requires novices to reflect on their interaction trajectories. This introspection is intended to summarize and internalize experiences, ultimately leading to the development of individualized behavior patterns and policies. Therefore, in this section, we leverage interactions constructed in Section 4.1 and devise two distinct reflection strategies, including implicit-reward reinforcement learning and inverse reinforcement learning.\nWe begin by directly comparing the actions of the expert and the novice agent without introducing explicit reward estimation. Given a trajectory pair \\( (t_{sample}, t_e) \\) where \\( t_{sample} = (6_1, \u00e2_1,\u2026\u2026, \u00f4i, \u00e2_i) \\) is the expert trajectory while \\( t_e = (\u00f4_1, \u00e2_1,\u2026\u2026, \u00f4i, a_i) \\) is the corresponding agent trajectory. Inheriting the spirit of previous works [9, 36], we utilize the direct preference optimization loss [29], defined as follows:\n\\[L_{implicit} (\\pi_\\theta, \\pi_e) = -E[log \u03c3(\u03b2log \\frac{\\pi_\\rho(\u00ee_i)}{\\pi_e(\u00ee_i)} - Blog \\frac{\\pi_\\rho(\u03b1|\u015d_i)}{\\pi_e(\u03b1|\u015d_i)} -)],\\]\nwhere \\( \\pi_e \\) is the current agent policy needed to be optimized, \\( \\pi_e \\) is the expert policy, \\( t_{ref} \\) is the reference model initialized with the agent policy and \\( \u03b2 \\) is a hyper-parameter.\nConsidering the lack of reward signals for each reasoning step in existing datasets, we introduce an inverse reinforcement learning (IRL) method [1, 14, 23, 31].\nThis method first infers the step-wise reward function based on the expert's and agent's behaviors and then leverages the reward function to fine-grained optimizes the agent policy.\nWe first define the occupancy measure \\( \\rho_{\\pi} \\) for a policy \\( \\pi \\), indicating the normalized distribution of state-action pairs when the"}, {"title": "4.2.2 Inverse Reinforcement Learning", "content": "Given a trajectory pair \\( (t_{sample}, t_e) \\) where \\( t_{sample} = (6_1, \u00e2_1,\u2026\u2026, \u00f4i, \u00e2_i) \\) is the expert\n4.2.1 Implicit-Reward Reinforcement Learning."}, {"title": "5 Theoretical Analysis", "content": "In this section, we provide a theoretical analysis to prove that the distribution of actions generated by the agent can converge toward the expert action distribution over multiple training cycles.\nSince our policy update method employs gradient descent, under Assumption 1, the policy \u03c0\u03b8 will converge to a local minimum as the iterations increase. The following analyses are conducted under Assumption 1.\nPROPOSITION 5.1. The occupancy measure pre for the agent policy can converge to closely approximate the expert's occupancy measure Pre, after several iterations."}, {"title": "6 Experimental Settings", "content": "To thoroughly evaluate the ability of our proposed model StepAgent, we utilize representative tasks from three aspects, including web tasks, agent tasks, and multi-hop question-answering tasks. The statistics of these datasets are delineated in Table 1.\nWeb tasks consist of WebShop [48] for online shopping and Mind2Web [12] for complex tasks on various websites. Rewards in the two datasets are dense variable and range from 0 to 1.\nAgent tasks contain Science World [43] for science experiments, and ALFWorld [35] for embodied housework. The former contains continuous final rewards from zero to one while the latter has binary rewards demonstrating the completion of the task. For both datasets, we treat the in-distribution test sets as the validation set and the out-of-distribution unseen variations which aim to assess the generalization capabilities of agents as the test set.\nMulti-hop question-answering tasks include HotpotQA [47], 2WikiMultihopQA [15], and MuSiQue [40]. For each dataset, we leverage their associated Wikipedia articles contexts as our retrieval corpus to conduct multi-step reasoning. Considering the restrictions of experimental costs, following previous approaches [50, 56], we utilize a subset of the entire dataset, selecting 5,000 samples for training from the training set and 500 samples each for the validation and test sets from the development set."}, {"title": "6.2 Backbone Models and Baselines", "content": "We verify the effectiveness and robustness of our StepAgent on two widely-used open-source models: Mistral-7B (Mistral-7B-Instruct-v0.1) and Llama-3-8B (Meta-Llama-3-8B-Instruct)."}, {"title": "6.3 Evaluation Metrics", "content": "To align with previous methods [9, 36], we report the average results of the test set. For WebShop and Science World, we employ the final reward automatically assessed by the environment as the evaluation metric while for ALFWorld, we utilize the success rate for judgement. In terms of Mind2Web, we report macro element accuracy. Additionally, for the three multi-hop question-answering tasks, we leverage Exact Match (EM) for evaluation."}, {"title": "6.4 Implementation Details", "content": "Consistent with existing works [19, 36], we employ ReAct-form [50] to generate the interaction trajectory, which additionally generates Chain-of-Thought (CoT) rationales [45] before each action. For each task, a one-shot in-context example is employed in the instruction prompt. The details of prompts are described in Appendix A. For the three multi-hop question answering tasks, due to the lack of intermediate reasoning steps in the datasets, we employ GPT-4 [25] as the expert to generate trajectories and select trajectories with the exact match score equalling one as the expert trajectories. We leverage greedy generation for our method and all baseline approaches. In the SFT stage, we set the learning rate as 1e-5 and the batch size as 64. we choose the cosine scheduler with a 0.03 warm up. We train the model for four epochs on all datasets. For the reflection stage, the learning rate is 5e-7 and the batch size is 16. The training epoch is set as one. We leverage the AdamW optimizer in both stages. All experiments are carried out on 8 NVIDIA A100 80G GPUs."}, {"title": "7 Results and Analysis", "content": "The overall performance of our proposed methods StepAgent and all baselines are shown in Table 2. We can observe that:\n(1) Both variants of StepAgent consistently outperform all baseline methods across three distinct task categories by a significant margin. In comparison with ETO and SPIN, which introduce the entire trajectory for training, StepAgent achieves a significant edge with improvements of the results over all tasks. This performance demonstrates the effectiveness of utilizing the step-wise reward signals to emulate the expert policy. Even without human-annotated step-wise preference data, StepAgent still can gradually align with"}, {"title": "7.2 Ablation Studies on Reward Type", "content": "In this section, we conduct ablation studies to analyze the influence of different reward types on our StepAgent model. We investigate our model StepAgent with three variants [27, 41]: (1) Step-wise reward, which constructs step-wise reward by observing and imitating the expert behaviors and optimizes agent strategy with step-wise rewards, as introduced in Section 4. (2) Final reward, which utilizes the final environmental feedback as the reward for optimization. (3) We also explore the combination of the two reward types to evaluate their impact on the performance of StepAgent. Experiments are conducted based on Llama38B and inverse RL and we can obtain similar conclusions with other settings.\nFrom the results in Table 3, we can observe that optimizing the reinforcement learning process solely with the final environmental feedback as rewards results in performance degradation on all"}, {"title": "7.3 Performance with Different Model Size", "content": "To further illustrate the robustness of StepAgent, we conduct experiments with different backbone model parameter sizes. We utilize Mistral7\u00df and Mistral13B for this analysis. The results are depicted in Figure 3. \"-Implicit\" indicates StepAgent with implicit-reward reinforcement learning strategy while \"-Inverse\" represents inverse reinforcement learning method. We abbreviate Science World and 2WikiMultihopQA as Sci-World and 2WikiQA for limited space.\nFirst, we can observe that StepAgent demonstrates consistent and robust efficacy across models with different parameter scales. This performance stability highlights our model's adaptability to different configurations, ensuring that its reliability in achieving effective results regardless of the parameter scale employed. Second, compared with Mistral7B, Mistral13B achieves superior performances. This indicates the importance of the backbone model's capability, as it significantly influences the effectiveness of post-imitated learning. The enhanced capacity of Mistral13B allows for more effective learning and adaptation, contributing to improved performances."}, {"title": "7.4 Exploration of Parameters Settings", "content": "In StepAgent, two important hyper-parameters will impact the experimental performance - the number of training iterations in Algorithm 1 and the practice number of the agent during the inspection stage of each iteration. In this section, we conduct experiments to investigate their influences. We randomly selected two representative datasets WebShop and HotpotQA for this experiment and we can draw similar conclusions on other datasets."}, {"title": "8 Conclusion and Future Work", "content": "Reinforcement learning has become an effective approach for aligning agent behaviors with human preferences. However, existing reinforcement learning methods primarily adopt the final environmental feedback to optimize the agent strategy. In this paper, inspired by Benner's novice-to-expert theory, we proposed StepAgent,"}, {"title": "A.1 WebShop", "content": "Task Instruction for WebShop\nYou are web shopping. I will give you instructions about what to do. You have to follow the instructions. Every round I will give you an observation and a list of available actions, you have to respond an action based on the state and instruction. You can use search action if search is available. You can click one of the buttons in clickables. An action should be of the following structure:\nsearch[keywords]\nclick[value]\nIf the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be a value in the list of available actions. Remember that your keywords in search should be carefully designed.\nYour response should use the following format:\nThought: I think ...\nAction: search[something]"}, {"title": "A.2 Mind2Web", "content": "Task Instruction for Mind2Web\nYou are a helpful assistant that is great at website design, navigation, and executing tasks for the user.\nUser: \"<html> <div> <div>  Home Page  book a reservation. toggle open>  Book a reservation   book a reservation. toggle open>  </div> <div>   Dine in   Pickup   Delivery   Events   Wineries   All   Exploring now  </div> </div> </div> </html>\" Based on the HTML webpage above, try to complete the following task:\nTask: Check for pickup restaurant available in Boston, NY on March 18, 5pm with just one guest\nPrevious actions: None"}, {"title": "A.3 Science World", "content": "Task Instruction for Science World\nYou are a helpful assistant to do some scientific experiment in an environment. In the environment, there are several rooms: kitchen, foundry, workshop, bathroom, outside, living room, bedroom, greenhouse, art studio, hallway You should explore the environment and find the items you need to complete the experiment. You can teleport to any room in one step. All containers in the environment have already been opened, you can directly get items from the containers. The available actions are:\nopen OBJ: open a container\nclose OBJ: close a container\nactivate OBJ: activate a device\ndeactivate OBJ: deactivate a device\nconnect OBJ to OBJ: connect electrical components\ndisconnect OBJ: disconnect electrical components\nuse OBJ [on OBJ]: use a device/item\nlook around: describe the current room\nexamine OBJ: describe an object in detail\nlook at OBJ: describe a container's contents\nread OBJ: read a note or book\nmove OBJ to OBJ: move an object to a container\npick up OBJ: move an object to the inventory\npour OBJ into OBJ: pour a liquid into a container\nmix OBJ: chemically mix a container\nteleport to LOC: teleport to a specific room\nfocus on OBJ: signal intent on a task object\nwait: task no action for 10 steps\nwait1: task no action for a step\nYour response should use the following format:"}, {"title": "A.4 ALFWorld", "content": "Task Instruction for ALFWorld\nInteract with a household to solve a task. Imagine you are an intelligent agent in a household environment and your target is to perform actions to complete the task goal. At the beginning of your interactions, you will be given the detailed description of the current environment and your goal to accomplish. For each of your turn, you will be given the observation of the last turn. You should first think about the current condition and plan for your future actions, and then output your action in this turn.\nThe available actions are:\n1. go to recep\u0440\n2. task obj from recep\n3. put obj in/on recep\n4. open recep\n5. close recep\n6. toggle obj recep\n7. clean obj with recep\n8. heat obj with recep\n9. cool obj with recep\nwhere obj and recep correspond to objects and receptacles. After your each turn, the environment will give you immediate feedback based on which you plan your next few steps. if the envrionment output \"Nothing happened\", that means the previous action is invalid and you should try more options.\nYour response should use the following format:\nThought: \nAction:"}, {"title": "A.5 HotpotQA, 2WikimultihopQA and Musique", "content": "Task Instruction for Multihop-QA Datasets\nYou are an expert in this field. Please answer the question as simply and concisely as possible. Every round I will give you an observation, you have to respond with interleaving Thought and Action steps. Thought can reason about the current situation, and Action can be two types:\n(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n(2) Finish[answer], which returns the answer and finishes the task.\nYour response should use the following format:\nThought: I think ...\nAction:"}]}