{"title": "Suspected Undeclared Use of Artificial Intelligence in the Academic Literature", "authors": ["Alex Glynn, MA"], "abstract": "Since generative artificial intelligence (AI) tools such as OpenAI's Chat- GPT became widely available, researchers have used them in the writing process. The consensus of the academic publishing community is that such usage must be declared in the published article. Academ-AI documents ex- amples of suspected undeclared AI usage in the academic literature, dis- cernible primarily due to the appearance in research papers of idiosyn- cratic verbiage characteristic of large language model (LLM)-based chat- bots. This analysis of the first 500 examples collected reveals that the prob- lem is widespread, penetrating the journals and conference proceedings of highly respected publishers. Undeclared AI seems to appear in journals with higher citation metrics and higher article processing charges (APCs), precisely those outlets that should theoretically have the resources and ex- pertise to avoid such oversights. An extremely small minority of cases are corrected post publication, and the corrections are often insufficient to rec- tify the problem. The 500 examples analyzed here likely represent a small fraction of the undeclared AI present in the academic literature, much of which may be undetectable. Publishers must enforce their policies against undeclared Al usage in cases that are detectable; this is the best defense currently available to the academic publishing community against the pro- liferation of undisclosed AI.", "sections": [{"title": "Background", "content": "Shortly after the public release of OpenAI's ChatGPT in November 2022, researchers began incorporating its use into their workflows. The use of ChatGPT or similar tools based on large language models (LLMs) to gener- ate text for inclusion in research articles was immedi- ately controversial. A consensus rapidly emerged in the academic publishing community on two points:\n1. No artificial intelligence (AI) system may be listed as an author.\n2. Any author who uses AI in the writing process must declare that they have done so in the pub- lished article.\nThese principles have declared by a variety of academic publishing organizations, including the Committee on Publication Ethics (COPE), the Council of Science Ed- itors (CSE), the International Committee of Medical Journal Editors (ICMJE), the World Association of Medi- cal Editors (WAME), the Institute of Electrical and Elec- tronics Engineers (IEEE), the Institute of Physics (IOP), the Society of Photo-Optical Instrumentation Engineers (SPIE), Elsevier, Frontiers, the Multidisciplinary Digital Publishing Institute (MDPI), the Public Library of Sci- ence (PLoS), Sage, Science, Springer, Taylor & Francis, and Wiley.\nThe justification for point 1 is that LLMs cannot be held accountable for their statements, but human au- thors can. Authorship implies the assumption of re- sponsibility for the contents of a manuscript; since an LLM cannot assume this responsibility, it cannot be an author. Responsibility instead lies with the author who chooses to use an LLM to generate text and incorporate that text into their own work. It is also the author's re- sponsibility to validate such text:\nResearchers who use these NLP [natural language processing] systems to write text for their manuscripts must therefore check the text for factual and citation accuracy; bias; mathematical, logical, and common- sense reasoning; relevance; and originality.\n(Hosseini et al., 2023)\nPoint 2 ensures that the implementation of point 1 does not compromise transparency. LLMs should not be listed as authors, but it is still important for readers to know that they are reading AI-generated text:\nBecause NLP systems may be used in ways that may not be obvious to the reader, re- searchers should disclose their use of such systems and indicate which parts of the text were written or co-written by an NLP sys- tem.\n(Hosseini et al., 2023)\nThese are more than academic principles, beyond re- sponsibility or transparency for their own sake. LLMs have a well-documented tendency to \"make up in- formation\", write \u201cplausible-sounding but incorrect or nonsensical answers\u201d (OpenAI, 2022), or \"confidently state information that isn't correct\" (Weston & Shuster, 2021). This phenomenon is known as \u201cconfabulation,\" \"hallucination,\" or \"fabrication\" and is one of twelve risks novel to or exacerbated by the use of generative AI identified by the United States National Institute of Standards and Technology (2024, NIST). Examples of the real-world impact of confabulation include several legal cases, in which attorneys cited non-existent case law to support their arguments or may have done so.\nAs the NIST emphasizes, confabulation is a conse- quence of training generative AI models to optimize the next word prediction objective. LLMs such as the gener- ated pretrained transformer (GPT) architecture of Chat- GPT (OpenAI) are programmed to predict the next to- ken (part of a word) in a text string. This is, in the words of OpenAI researchers, \u201conly a proxy for what these models want to do\u201d (Ouyang et al., 2022). They are not sentient; they do not \"know\u201d information or \"understand\" the instructions they are given; such ver- biage is inappropriately anthropomorphic, although its use is difficult to avoid entirely. As long as we use LLMs trained to optimize the next word prediction objective, confabulation will occur. It is imperative, therefore, that authors scrutinize the outputs of AI systems to ensure that they are accurate before including them in research manuscripts. Readers must be informed whenever AI is used to generate text so that they may apply their own scrutiny.\nThese principles, while generally agreed upon by the academic publishing community, have not consistently been upheld. Many journals and other academic pub- lications, including those with explicit policies against undeclared generative AI use, have published articles containing what appears to be AI-generated text, which can be identified by the occurrence of certain character- istic features (see \u00a7 3.4), or other AI-generated media. I maintain a repository of examples known as Academ-AI (https://www.academ-ai.info/). The purpose of this ar- ticle is to report the development of this repository and characterize the first 500 examples collected. I will also discuss how journals represented in the repository dif- fer from their peers in terms of publication costs and citation metrics."}, {"title": "Methods", "content": "2.1 Data collection\nPhrases characteristic of AI-generated text were used to query Google Scholar. Examples include \"as an AI language model,\u201d \u201ccertainly, here are,\u201d and \"don't have access to real-time.\" The Retraction Watch list of pa- pers with evidence of ChatGPT writing was also used to supplement these searches (Retraction Watch, 2024). Each result was manually examined and included or ex- cluded according to the following criteria.\n2.2 Inclusion criteria\n1. Publication as an academic journal article or con- ference paper.\n2. Appearance of:\n(a) Phrasing characteristic of AI-generated text or\n(b) Statement from the publication or authors that AI was used without declaration.\n2.3 Exclusion criteria\n1. Suspicious phrasing justified by context on man- ual examination.\n2. Any medium other than journal articles and con- ference papers, such as books, preprints, and tech- nical reports.\nExclusion criterion 3 was a time-saving measure since AI-generated text would be unlikely to appear in the academic literature more than ten months prior to the public release of ChatGPT (November 2022). How- ever, since unscrupulous publishers could falsify meta- data, dates of publication were relied upon only when confirmed by a reputable independent indexing service (criterion 3b).\n2.4 Data extraction\nFor each article or paper included, sections of the text containing suspect phrasing were extracted and stored in Markdown files. Citations were collected us- ing Zotero version 6.0.37 (Takats et al., 2024). Meta- data not captured automatically by Zotero were entered manually. The ISSN Portal and the websites of indi- vidual publications were used to supplement metadata collection (International Standard Serial Number Inter- national Centre, 2024). International Standard Serial Numbers (ISSNs) of journals were validated using the ISSN Portal. Those that yielded no results were tested using the ISSN checksum (Abdalla et al., 2015). Biblio- graphic data were exported to YAML for analysis using Better BibTeX version 6.7.210 (Heyns, 2024).\n2.5 Third-party data\nInformation on article processing charges (APCs) was drawn from the website of each journal. APC data were also drawn from the Directory of Open Access Journals (2024) to facilitate comparison between DOAJ- indexed journals that were and were not represented in the Academ-AI dataset; where journal websites con- flicted with DOAJ-reported APCs, the journal website figure was used, and the discrepancy was reported to the DOAJ. Foreign exchange rates were drawn from the"}, {"title": "Analysis of text", "content": "A natural language processing analysis was conducted using quanteda version 4.0.2 (Benoit et al., 2018). Text excerpts from the Academ-AI database were tokenized. A dictionary of keywords associated with each of eight textual features of interest was compiled and used to construct a document feature matrix . In the case of referral to other sources, the feature was ex- pressed using so many different phrasings that a sin- gle set of keywords with sufficient sensitivity and speci- ficity became inefficient. Instead, an excessively sensi- tive set was used, and results were manually screened. With the document feature matrix constructed, docu- ments (articles and papers) were dichotomized as hav- ing or lacking each textual feature."}, {"title": "Statistical methods", "content": "Documents were categorized by type (journal article or conference paper), alleged year of publication, and pub- lisher. Journal articles were also categorized accord- ing to whether they appeared in a journal with APCs (yes, no, or unknown). APCs were converted to US dol- lars and summarized, overall and by publisher. To give appropriate weight to containers (journals and confer- ences) that published multiple documents (articles and papers) in the dataset, documents, rather than contain- ers, were treated as single observations even where re- porting and comparing container-level variables, such as publisher or APC.\nThe number of DOAJ-indexed journals that had APCs, stratified by whether or not a given journal was represented in the Academ-AI dataset, was calculated. The same two groups, excluding journals that did not charge APCs, were compared in terms of APC value in US dollars. Additionally, the citation metrics of SJR- indexed journals represented in the Academ-AI dataset were compared with those of all other journals in the SJR database. For the comparisons with the DOAJ and SJR databases, containers (journals) were treated as sin- gle observations.\nCategorical variables were reported as frequencies and percentages and compared using x2 tests. Contin- uous variables were reported as medians and interquar- tile ranges (IQR) and compared using Wilcoxon rank- sum tests. P-values <0.05 were considered statistically significant.\nTokenization, currency conversion, and analysis were performed using R version 4.4.0 (Chambers et al., 2024)."}, {"title": "Results", "content": "Although further examples have since been collected, the dataset used for the present investigation comprised excerpts from 500 published documents: 449 journal ar- ticles and 51 conference papers. Years of publication according to the publishers are depicted in Figure 1; 93.2% had publication dates in 2022 or later, leaving 6.8% ostensibly published long before the public release of ChatGPT (November 2022).\nArticles were published in 345 different journals, the overwhelming majority of which (86%) published only one of the articles  The journals most highly"}, {"title": "Article processing charges", "content": "At least 295 articles (65.7%) were published in a journal with some form of publication charge; 21 of these were in journals that mentioned having such a charge on their websites but did not state a specific figure. Among the remaining 274, the median charge was US $150 (IQR 50-1,295). Charges stratified by publisher are depicted in Figure 4. Wiley had the highest median APC; major publishers had much higher APCs than other publishers (median [IQR] $3,039 [2,191\u20133,530] vs. $80 [34\u2013300]; P < 0.001)."}, {"title": "DOAJ", "content": "Thirty-five articles (7.8%) were published in 33 journals indexed in the DOAJ. Of these, 28 journals (84.8%) had publication charges, a significantly greater propor- tion than among the remainder of the journals in the database (34.3%; P < 0.001). The APCs of journals rep- resented in the Academ-AI dataset (median $1,982; IQR 194-2,994) were much higher than the rest of the DOAJ (median $842; IQR 100\u20132,070; P = 0.039); see Figure 5."}, {"title": "Citation metrics", "content": "Seventy journals in the SJR database published 82 ar- ticles (18.3%) in the Academ-AI dataset. Compared to other journals in the database, journals represented on Academ-AI had significantly higher median SJR scores, h-indices, publication outputs, citations, and citation rates; see Table 3 and Figure 6."}, {"title": "Textual features", "content": "3.4.1 First-person singular\nUse of the first-person singular is typical of LLM chat- bots but not of multi-author journal articles; it therefore functions as a tell-tale sign of AI involvement. The ma- jority of excerpts in the Academ-AI dataset (55.2%) fea- tured use of the first-person singular; see Figure 7. Even in the excerpts from single-author articles, the context made clear on manual examination that the first-person singular pronoun referred to the chatbot, not to the au- thor of the article.\n3.4.2 Model update\nLLM chatbots are trained to warn users of their own limitations, such as the knowledge cutoff in their train- ing data. Lack of data after this cutoff date can affect the accuracy of a chatbot's outputs (OpenAI, 2024); hence, chatbots frequently inform users of their cutoff date or", "4": null}, {"title": "Corrections", "content": "Fifteen articles with suspected AI-generated content in as many journals have been corrected post publication, including six full retractions , five corrections , and five \"stealth\u201d corrections: that is, articles corrected without a formal statement. In total, therefore, 3.0% of ex- amples in the dataset were corrected, with 2.2% being formally corrected and 1.0% stealth-corrected. Formal corrections occurred a median 104 days (IQR 48-151) after publication; see Figure 8. Journals were faster to issue full retractions (median 50 [IQR 38\u201392] days) than corrections (median 120 [IQR 104\u2013182] days), but this difference was not statistically significant (P = 0.17).\nOf the 11 formal corrections, four identified Chat- GPT as the AI tool used ; one claimed that only Grammarly was used ."}, {"title": "Summary of findings", "content": "Hundreds of examples of undeclared AI-generated con- tent appear to be present in the academic literature; ap- proximately one in five of these evaded detection by a major publisher with an explicit policy against such content. Journals represented in the dataset more com- monly have APCs, and their APCs and citation scores are higher than those of their peers in the DOAJ and SJR database, respectively; in other words, the journals committing editorial oversights concerning Al are para- doxically paid more for editorial processing and achieve greater academic impact. An extremely small propor- tion of cases are corrected following discovery, and still fewer are corrected formally. Four out of five formal cor- rigenda failed to revise the original article sufficiently to comply with editorial policy."}, {"title": "Discussion", "content": "It seems that in a remarkable number of cases, editorial and peer review have failed to uphold publishing stan- dards with regard to artificial intelligence. Part of the problem may be a lack of awareness of the idiosyncratic phrasing used by LLMs. While some features, such as explicit self-identification \u201cas an AI language model\u201d are difficult to miss, it is unsurprising that others, such as inappropriate use of conversational phrasing or the first-person singular, do not immediately lead review- ers to think of AI specifically. Phrasing such as \"as of my last knowledge update\" should, however, alert edi- tors and reviewers that something is amiss, and if errors such as these endure the scrutiny of multiple reviewers and editors, what else are they overlooking?\nThe Shoukat et al. (2024) case is particularly illus- trative of this problem. The appearance of the phrase \"regenerate response\" in the reference list of an article does not in itself diminish the scientific quality of that article any more than a slight misspelling or a poorly formatted paragraph break. However, as an indicator of AI involvement, the error led to the discovery that al-"}, {"title": "Dark AI", "content": "In the examples described here, the AI-generated text is sufficiently incongruous with the remainder of the arti- cle as to be obvious, but LLM outputs vary widely, and identifying them amid human-authored text is not al- ways straightforward or even possible. Neither humans, nor machines, nor a combination of the two can reliably distinguish AI-generated content from human-authored content (Campbell & Jovanovi\u0107, 2023). Future AI de- tection systems may be more advanced, but so will fu- ture generative AI systems, meaning that detection will likely continue to present a problem.\nThe examples on Academ-AI were predominantly identifiable due to characteristic turns of phrase, which are only reproduced in a manuscript when authors copy text directly from an AI system with little to no scrutiny. Light proofreading would be sufficient to remove such clear Al fingerprints, though not necessarily sufficient to identify deeper AI-induced problems in the content, such as confabulated information. In other words, once characteristic AI phraseology becomes widely known or AI detectors become more reliable, unscrupulous au- thors may be incentivized to expend moderate addi- tional effort disguising AI-generated text rather than significant additional effort thoroughly validating it; this would mask the symptoms while failing to treat the underlying disease.\ngesting to authors that declaration of AI usage is unnec- essary. By contrast, a precedent of enforcing these poli- cies incentivizes declaration among careless and even unscrupulous authors on the off-chance that the use of Al is detectable in their case.\nI have previously drawn a comparison to conflict of interest (Glynn, 2024b). Briefly, scientific research has successfully transitioned in recent decades from a prohibitive model, in which competing interests are disqualifying, to a transparent model, in which com- peting interests are disclosed and managed (Institute of Medicine (US) Committee on Potential Conflicts of Interest in Patient Outcomes Research Teams, 1991, pp. 61-63). With disclosure becoming commonplace and failure to disclose meeting severe consequences, re- searchers have every incentive to disclose their conflicts rather than concealing them. We can use the same in- centives to encourage transparency with regard to AI usage, but this depends on journals enforcing their own policies."}, {"title": "Conclusion", "content": "Given the risks posed by the inclusion of AI-generated text in research literature, publishers and journals must establish a precedent of enforcing their policies against undeclared Al usage in the writing process. When pol- icy violations that escape the notice of editors and peer reviewers are identified post publication, formal correc- tions must be made."}, {"title": "Declarations", "content": "Funding\nThe author received no specific funding for this work.\nConflict of interest\nThe author declares no conflict of interest.\nData availability\nGlynn, A. (2024a). Academ-AI Analysis. OSF. https:// doi.org/10.17605/OSF.IO/S4YGV"}]}