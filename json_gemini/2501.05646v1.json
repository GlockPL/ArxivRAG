{"title": "Efficient Representations for High-Cardinality Categorical Variables in Machine Learning", "authors": ["Zixuan Liang"], "abstract": "High-cardinality categorical variables pose significant challenges in machine learning, particularly in terms of computational efficiency and model interpretability. Traditional one-hot encoding often results in high-dimensional sparse feature spaces, increasing the risk of overfitting and reducing scalability. This paper introduces novel encoding techniques, including means encoding, low-rank encoding, and multinomial logistic regression encoding, to address these challenges. These methods leverage sufficient representations to generate compact and informative embeddings of categorical data. We conduct rigorous theoretical analyses and empirical validations on diverse datasets, demonstrating significant improvements in model performance and computational efficiency compared to baseline methods. The proposed techniques are particularly effective in domains requiring scalable solutions for large datasets, paving the way for more robust and efficient applications in machine learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Many regression problems often arise when analyzing data collected across several groups, each of which can contribute uniquely to the statistical relevance of the model. For instance, in a healthcare context, if I want to model health outcomes using patient data from various hospitals, I need to account for hospital-specific effects that other covariates might not capture. This type of group effect is observed in numerous contexts, such as examining students from different schools, voters living in separate zip codes, employees at various firms, and many more scenarios.\nA widely employed method for addressing such issues is fixed effect modeling. To illustrate, let's assume that I have n samples {Xi, Gi, Yi} for i = 1,...,n, where X\u2081 \u2208 RP is a set of covariates pertaining to individual subjects, Gi\u2208 G is a categorical variable denoting group membership, and Y\u00bf \u2208R is the outcome of interest. My aim is to estimate:\n$\\mu(x, g) = E[Y; | X\u2081 = x, G\u2081 = g].$\n(1)\nThe conventional fixed effects approach models this expectation as:\n$\\mu(x,g) = a_g + x\\beta,$\n(2)\nwhere the parameters \u03b2 and ag are estimated through ordinary least squares regression. Extensions of this basic approach can involve applying non-linear transformations to x, considering interactions between the group membership variable and other covariates, or adding regularization to improve stability and predictive power [1], [2], [3].\nHowever, in practice, I often observe that fixed effects modeling faces limitations when the underlying signal is complex and non-linear or when the number of groups, |G|, is large.\nThe rigidity of the model (2) can hinder its ability to capture complex signals, and the high number of ag parameters can lead to challenges with statistical inference [4]. In essence, the model might become too cumbersome to yield stable results while also lacking the flexibility to capture nuanced patterns in the data.\nTo address these challenges, the focus of this paper is to propose a more efficient way to represent group membership. Specifically, I seek to develop a mapping \u03c8 that transforms group membership Gi into a k-dimensional space without compromising predictive capacity:\n$\\psi: G \\rightarrow R^k, \\mu(x,g) = f(x,\\psi(g)),$\n(3)\nwhere k is relatively small (i.e., k < |G|) and the function f(, ) remains easy to learn. With such a mapping, the problem in (1) reduces to a standard regression problem with (p+ k)-dimensional real-valued features (Xi, \u03c8(Gi)), allowing me to apply off-the-shelf statistical learning tools.\nTo create an effective representation of group membership Gi, I must make assumptions about the link between Gi and the outcome Y\u2081. The central premise, referred to as the sufficient latent state assumption, is illustrated in Figure 1. This assumption posits that while Gi does not directly impact Yi, it may relate to latent variables Li that exert a direct influence on Yi.\nAs an example, in examining patients across multiple hos-pitals, I assume that hospitals themselves do not directly influence health outcomes Y. Rather, they serve as predictors due to associations with latent causal variables such as disease"}, {"title": "II. LITERATURE REVIEW", "content": "The effective transformation of high-cardinality categorical variables into numerical representations is a critical area in machine learning and data science. One-hot encoding, the most commonly used method, often suffers from high-dimensionality issues, particularly when the number of unique categories is large. Feature hashing, proposed by Weinberger et al., reduces dimensionality but may introduce information loss due to hash collisions. Dimensionality reduction tech-niques, such as PCA and autoencoders, offer alternatives for encoding, but their interpretability and scalability are limited in practical applications. Recent advancements in embedding-based methods, popularized by applications in natural lan-guage processing, have shown promise for dense and mean-ingful representations of categorical variables. Mikolov et al.'s word2vec and subsequent embedding techniques have inspired extensions into domains like recommender systems. However, these methods require large amounts of training data to achieve optimal performance.\nMore recently, methods such as target encoding and leave-one-out encoding have emerged to address overfitting con-cerns in tree-based models. Similarly, low-rank approxima-tions and sparse encodings, as explored by Menon et al., have demonstrated their ability to capture latent group struc-tures effectively. Despite these advancements, gaps remain in achieving a balance between interpretability, scalability, and computational efficiency, particularly for high-cardinality settings. This paper builds upon these foundations, introducing novel encoding strategies that address the trade-offs inherent in existing methods while ensuring practical applicability across diverse domains."}, {"title": "III. RELATED WORK", "content": "In data science and applied statistics, the transformation of high-cardinality categorical data into meaningful numerical representations is critical for many applications. Categories in large datasets often represent grouping factors such as regions, customer segments, or other classifications. These groupings allow for more nuanced analysis by distinguishing between distinct observational units. However, high-cardinality variables, where categories number in the thousands or more, pose significant challenges for traditional statistical methods, particularly in fields such as econometrics and machine learning.\nOne of the most common challenges arises in panel data studies, where observations are repeated over time. In such settings, the application of group-level effects can reduce model complexity without losing valuable group-specific in-formation. A typical approach is to cluster data into groups and apply a single fixed effect for each cluster, rather than individual fixed effects for each observation. This strategy helps simplify the model while retaining essential between-group variation. The clustering methods often combine itera-tive techniques, such as k-means, with regression models to fine-tune group-level effects. A notable technique proposed by [5] uses this method, alternating between clustering and estimation of cluster-specific fixed effects to streamline the analysis of complex panel datasets."}, {"title": "A. Alternative Clustering and Estimation Approaches", "content": "The method outlined by [5] focuses on efficiently grouping time series data, where the objective is to minimize the computational burden of applying fixed effects individually for each time series. This is achieved through a novel clus-tering algorithm that incorporates both the data structure and temporal relationships. While there are some similarities to my approach, I depart from this framework by not relying on the assumption that the latent states of observations can be consistently estimated. In contrast, the work of [6] seeks to mitigate model misspecification by employing a propensity score weighting technique that adjusts for group-level differ-ences. Their method balances the flexibility of the model with necessary adjustments for group-specific variations, offering an improved way to estimate treatment effects while dealing with complex data structures."}, {"title": "B. Encoding High-Cardinality Categorical Data in Machine Learning", "content": "In machine learning, encoding categorical variables effec-tively is crucial for building models that can generalize well to unseen data. A popular technique is one-hot encoding, where each unique category is transformed into a binary vector. However, this approach becomes inefficient when the number of categories is large, resulting in sparse matrices where most entries are zeros. This sparsity can negatively impact the performance of machine learning algorithms, particularly when the number of categories M grows large, making the model less effective due to high-dimensionality. Furthermore, sparse vectors often cause problems for algorithms that rely on variance or distributional properties of features, as the majority of the data remains uninformative.\nTo address these challenges, several techniques have been developed to reduce the dimensionality of categorical data while preserving its meaningful structure. One approach is feature hashing, which uses hash functions to map categories to a smaller number of feature bins. This reduces the di-mensionality of the data but at the cost of losing exact category information due to potential hash collisions. Another"}, {"title": "C. Random Projections and Embeddings in Data Encoding", "content": "An alternative to one-hot encoding is random projection, which projects categorical variables into lower-dimensional spaces using random matrices. This method has been applied to high-dimensional categorical data to reduce the feature space and maintain essential relationships between categories. By transforming categorical variables into a lower-dimensional space, random projections make it easier for models to learn from the data while mitigating the curse of dimensionality. Additionally, embeddings have become increasingly popular in fields like natural language processing and recommender systems, where categorical variables are transformed into dense vectors that capture underlying semantic relationships. These embeddings provide a more compact and efficient rep-resentation of categorical data, enabling better generalization in machine learning models."}, {"title": "D. Challenges with High-Dimensional Encoding", "content": "Despite the availability of various encoding techniques, one of the most persistent issues with categorical data is the high dimensionality that results from techniques like one-hot encoding. For variables with a large number of unique levels, this can lead to prohibitively large feature sets, especially in applications with numerous categorical variables. Further-more, models built on high-dimensional data may suffer from overfitting, as they tend to learn patterns that are specific to the training data rather than generalizable to new data. This is particularly evident in decision tree-based methods, which can easily overfit when faced with many levels in categorical variables.\nAnother challenge lies in the use of sparsity-driven methods like lasso regularization [7] or decision tree-based models such as random forests [8]. These methods often assign zero weight to less informative features, leading to the exclusion of rare categories that may carry important information. In the case of one-hot encoding, rare categories often do not contribute significantly to the model due to their sparse representation, which can result in the model overlooking valuable group-level effects."}, {"title": "E. Factorial Splits and Overfitting in Decision Trees", "content": "A notable strategy for handling categorical variables in decision trees is factorial splitting. This technique allows deci-sion trees to explore interactions between different levels of a categorical variable, effectively dividing the feature space into subgroups based on these interactions. The factorial approach is flexible, as it can generate a large number of potential splits based on the categorical levels. However, the sheer number of potential splits grows exponentially with the number of categories, making it prone to overfitting, especially when the number of categories is large. The challenge here is to balance model complexity with predictive accuracy, ensuring that the model does not become overly tailored to the training data at the expense of its generalization capability."}, {"title": "IV. REPRESENTING GROUPS WITH SUFFICIENT LATENT STATE", "content": "The sufficient latent state assumption outlined earlier and depicted in causal graph 1 suggests that the distribution of the outcome variable Y relies on the observable factor Gi solely through a hidden latent variable Li. To put it differently, knowing the value of Gi offers no additional insight into the outcome if Li is already known. For example, consider a patient's health condition (Li \u2208 {good, poor}) that might influence which hospital they are admitted to (Gi), their exhibited symptoms (Xi), and ultimately their health outcomes (Y). Once I account for their underlying health status, the hospital itself provides no further information about these other variables. On the contrary, knowing which hospital a patient attends may offer useful hints about their health status.\nThe following lemma outlines how the information con-tained within the categorical variable Gi affects the model. Specifically, the conditional expectation of the outcome is influenced by the conditional probabilities of the latent state given the observed category. This characterization is essential for developing future representation methods.\nproving that they serve as sufficient representations because they can be expressed as invertible transformations of \u03c8(g).\nLemma 1. Let Li be a latent variable that is discrete and has k distinct possible values. Under the assumption that the conditional dependencies between observed variables Xi and Li are captured through a latent state model, I have the following:\n$\\mu(x, g) = \\frac{\\sum_{l=1}^k E [Y_i | X_i = x, L_i = l] P [X_i = x | L_i = l] \\psi_l(g)}{\\sum P [X_i = x | L_i = l] \\psi_r(g)}$\nThis lemma formalizes the representation of a categorical group as a set of latent states. Specifically, the model assumes that the membership of an individual in a latent category can be described by a set of probabilities, P(Li = l | Gi = g), where Gi = g corresponds to the group assignment. If there are k possible latent categories, each group is represented by a k-dimensional vector of probabilities, capturing the likelihood of membership in each of the k states. This representation enables me to express group membership in terms of these probabilities without losing any information.\nA crucial insight is that the function I aim to predict, \u03bc(\u00b7), can be learned by any universally consistent machine learning method when I provide it with data of the form ((Xi, \u03c8(Gi)), Yi). Here, \u03c8(Gi) denotes the group-specific probabilities (the likelihoods of latent state Li), and Y represents the target variable. Universal consistency means that given sufficient data, methods such as decision trees, nearest"}, {"title": "V. ENCODING STRATEGIES FOR CATEGORICAL VARIABLES", "content": "In this section, I introduce novel approaches for trans-forming categorical variables into numerical representations suitable for machine learning models. Instead of relying on traditional methods like one-hot encoding, my methods focus on capturing the underlying structure of categorical data by encoding it into multiple columns. These transformations pre-serve the information inherent in the categorical feature while also improving computational efficiency. I also discuss how these methods relate to the structural principles outlined in the preceding section. For a more detailed review of alternative encoding strategies, refer to section VI-F in the Appendix."}, {"title": "A. Average Encoding", "content": "One of the key methods I propose is the average encoding technique. In this approach, categorical variables Hj are substituted with the mean values of the continuous features Xj, conditioned on each category. This allows the encoding to incorporate the statistical properties of the continuous features in a way that is both interpretable and computationally efficient. Figure 2 provides a clear visualization of how this encoding works.\nThe main benefit of this encoding method lies in its simplicity and ease of interpretation. By replacing categorical values\nwith the mean of continuous variables within each category, the approach avoids the curse of dimensionality that often accompanies high-cardinality categorical variables. In settings where the number of features p is smaller than the number of categories (p < |H|), average encoding provides a significant reduction in dimensionality compared to traditional encoding techniques like one-hot encoding, making it more efficient for both computation and model interpretation.\nMoreover, average encoding captures the latent group struc-ture inherent in the categorical variable. This can be particu-larly advantageous when there is a strong correlation between the categories and the continuous features. For instance, in the case of customer segmentation, the average encoding of features like age and income can reveal important patterns about different customer groups. Figure 3 further illustrates how group-wise averages of the continuous features (X1, X2) can often provide insights into the dominant latent group associated with each category.\nA key advantage of the average encoding method is its flexibility across different types of models. Unlike one-hot encoding, which increases dimensionality and can slow down the training of models like decision trees and linear regres-sion, average encoding works well with a wide range of algorithms. Whether used in linear models, decision trees, or more advanced techniques like gradient boosting, the method adapts seamlessly while preserving the essential information contained within the categorical variable.\nThe following lemma formalizes the conditions under which the average encoding provides a sufficient representation. The lemma establishes the relationship between the categorical variable and the encoded representation in terms of conditional expectations. All proofs for this section are provided in the appendix.\nLemma 2. Under the assumptions outlined in Lemma 1, suppose that the matrix B defined by (B)tj := E[Xit | Li = 1] is invertible. Then, the p-dimensional vectors \u0237(h) := E[Xi |\nH; = h] serve as sufficient representations for each category, as formalized in (3):\n1) Extension to High-Cardinality Categories: While the average encoding method works well when the number of categories is manageable, it may face challenges with high-"}, {"title": "B. Low-rank Encodings", "content": "Low-rank encoding methods are powerful tools for sum-marizing categorical variables, especially when the number of features, p, is significantly smaller than the number of categories, M. This imbalance can lead to overfitting when encoding categorical variables with high-dimensional repre-sentations, and low-rank encoding helps mitigate this issue by capturing the essential variance in the data. These tech-niques transform the categorical data into lower-dimensional representations, thus simplifying the data and improving model performance. Below, I introduce two distinct methods based on matrix factorization.\nThe first approach involves decomposing the group-wise mean matrix \u03a9, where each entry (\u03a9)jg = E[Xij | G = g] represents the mean of the continuous feature Xij conditioned on the group or category g. This factorization reduces the dimensionality of the representation while preserving the key patterns and correlations between categories. Figure 4 illustrates this encoding method.\nThis method applies Singular Value Decomposition (SVD) to the p\u00d7 M matrix \u03a9, resulting in \u03a9 \u2248 UDVT. The first k columns of the matrix U (the left singular vectors) are used to represent the gth category, where k is the number of components selected for the encoding. Cross-validation is often used to determine the optimal value of k, balancing model performance with complexity. An example of this encoding process is shown in Figure 4.\nIn addition to SVD, I also explore Sparse Principal Compo-nent Analysis (SPCA) [9], which applies an elastic-net penalty to the principal component coefficients, making the resulting components sparse. This sparsity helps in identifying a reduced set of components that still capture the majority of the variance in the data. Sparse PCA has the advantage of enhancing interpretability, as it leads to more compact representations of the data, which are particularly useful for tree-based models like random forests [8] and XGBoost [10]. The regularization parameter A in SPCA is tuned using cross-validation to obtain the optimal sparse components.\nThese encoding methods, by reducing dimensionality, help avoid the curse of dimensionality and reduce computational complexity while still capturing the key structure of the data. Moreover, they prevent overfitting, which can arise when using high-dimensional categorical encodings in machine learning models.\nThe following lemma demonstrates that the first k com-ponents, whether derived from SVD or SPCA, are sufficient representations for the categorical groups, assuming certain conditions on the matrix A, which is defined as (A)tj := E[Xit | Li = g]. Under these conditions, the k-dimensional vector u(g) := Ug,1:k is sufficient for each category g.\nLemma 3. Given that the matrix A is left-invertible, the k-dimensional vectors u(g) := Ug,1:k, derived from the first k columns of the left-singular matrix U, can be used as adequate representations for each category, as formalized in (3)."}, {"title": "Algorithm 1 Category Average Encoding Procedure", "content": "1: procedure CALCULATECATEGORYAVERAGES(X, H)\n\u25b7 Initialize a matrix to store category-wise averages\n2:\n\u00c2 \u2190 zeros(p, M)\n3:\nfor h in 1 to M do\n4:\n\u0394:,h\u2190 \u2211H-1(h)]:Hi=h Xi\n\u25b7 Compute the average of continuous features for each category h\n5: return \u00c2\n6:\n7: procedure APPLYCATEGORYAVERAGES(X, H)\n\u00c2 \u2190 CalculateCategoryAverages(X, H)\nT\u2190 zeros(n, p)\n8:\n9:\n10:\nfor i in 1 to n do\n11:\nTi: \u2190 \u2206:,Hi\n\u25b7 Assign category averages to each row in T based on membership Hi\n12: return T"}, {"title": "C. Multinomial Logistic Regression Encoding", "content": "Our final encoding approach leverages the conditional prob-ability of category membership modeled by multinomial logis-tic regression, parameterized by a set of coefficients {\u03b2g}g\u2208G:\n$P(G_i|X_i) = \\Lambda_{\\beta}(G_i = g|X_i) = \\frac{exp(X_i^T\\beta_g)}{\\sum_{g'} exp(X_i^T\\beta_{g'})}$\n(4)\nHere, the p-dimensional vector of coefficients \u03b2g for each category g serves as its representation. This approach is motivated by how the predictive model u(x,g) can be re-expressed to depend solely on the conditional probability\n$P(G_i = g|X_i = x)$, and under the multinomial logistic regression formulation, this naturally simplifies to dependency on the Bg parameters.\nLemma 4. Assuming the conditions in Lemma 1 hold, and further that the matrix A in (3) is left-invertible with P [Gi = g|Xi] distributed as a multinomial logit with coef-ficients {Bg}geg, including an intercept, then \u03b2g \u2208 RP is a sufficient representation as per (3):\n$\\mu(x, g) = \\frac{\\sum_{l=1}^k E [Y_i | X_i = x, L_i = l] P [X_i = x | L_i = l] (A^+f(\\theta_g))_1}{\\sum_{l=1}^k P [X_i = x | L_i = l] (A^+f(\\theta_g))_1}$\nwhere $f(\\theta_g):=\\frac{E_{X_i}[X_i\\Lambda_{\\theta_g}(g|X_i)]}{E_{X_i}[\\Lambda_{\\theta_g}(g|X_i)]}$"}, {"title": "Algorithm 4 Support Vector Machine (SVM) Encoding", "content": "1: procedure SVM(X, G)\n2:\nw, b\u2190 arg minw,b \u2211i L(f(Xi; w, b), Gi)\n\u25b7 Fit support vector machine model\n3:\nS\u2190 Onxp\n4:\nfor i = 1 to n do\n\u25b7 Assign coefficients based on category\n5:\nSi, \u2190 wGi\n6:\nreturn S"}, {"title": "VI. EMPIRICAL EVALUATION", "content": "This section explores the effectiveness of various encoding techniques in machine learning models, comparing them against traditional methods, such as one-hot encoding. I apply these techniques across both simulated and real-world datasets to assess their impact on model performance. Specifically, I evaluate two widely used algorithms: random forests and gradient-boosted trees (XGBoost)."}, {"title": "A. Simulated Experiments", "content": "We conduct a set of simulations designed to test the ro-bustness of the encoding methods under different settings. My experiments involve generating data with a mix of categorical and continuous features, and the evaluation is based on how well the encodings enable models to predict outcomes.\na) Generating Synthetic Data: For the first experiment, I simulate data with categorical variables where the observed groups are partially influenced by latent variables. This allows me to observe how encoding methods handle varying levels of complexity in feature relationships.\nLet me define a latent group L\u2081 as a categorical variable that governs the true underlying distribution of each data point. Each latent group corresponds to a distinct set of observed group categories, which are dependent on Li but with some noise. The assignment of Gi to these observed groups is governed by the distribution parameters influenced by the latent variable Li. Mathematically, I model the relationship as:\n$P(G_i = g | L_i) =\\begin{cases} p_{L_i}, & \\text{if } g \\in G_{L_i} \\\\ 1-p_{L_i}, & \\text{otherwise,} \\end{cases}$\nwhere pL\u2081 represents the assignment probability of an observed group being linked to the latent group.\nb) Covariates and Feature Generation: The covariates Xi are generated based on a mixture of discrete and continuous random variables. For each latent group, the associated feature vectors are normally distributed with group-specific means and covariances. To introduce non-linearity, certain covariates are manipulated by randomizing their values across groups, which helps evaluate the encoding methods' ability to capture complex relationships.\nc) Outcome Models: To evaluate the effectiveness of encoding methods, I define three progressively complex outcome models:\nLinear Model: In this model, I assume a simple linear relationship between the outcome variable Yi and the covariates. The model includes a shared slope for all groups, but each latent group has its own intercept:\n$Y_i = a_l + X_i^T\\beta + \\epsilon_i$\nwhere al represents group-specific intercepts, and \u03b2 is a shared slope vector. The residual \u03f5i is Gaussian noise.\nGroup-Specific Linear Model: In this more complex model, I introduce group-specific coefficients \u03b2e for each latent group, allowing each group to have its own set of slopes while the intercept remains the same across all groups:\n$Y_i = a + X_i^T\\beta_e + \\epsilon_i$\nHere, \u03b2e is specific to each latent group, allowing for more granular interactions between features and out-comes.\nPiecewise Linear Model: This model divides each fea-ture into two parts based on its median value, with a"}, {"title": "B. Real-World Data Analysis", "content": "In addition to the synthetic experiments, I also evaluate the performance of different encoding methods on several real-world datasets. These datasets contain a mix of numerical and categorical features, and the goal is to assess how well different encoding techniques support models in learning from complex, real-world data distributions.\na) Dataset 1: Customer Purchase Data: We analyze customer purchase behavior using categorical data, such as customer demographics, product preferences, and purchase history. The target variable is binary, indicating whether a customer will purchase a particular product.\nb) Dataset 2: Housing Market Dataset: In this dataset, the goal is to predict house prices based on various factors such as location, square footage, and age of the house. The categorical features in this dataset include neighborhood classifications and house style types, which require encoding before model training."}, {"title": "C. Evaluation of Encoding Methods", "content": "For each simulation, I trained the models using various encoding techniques discussed in Section V, then evaluated the models using mean squared error (MSE) on their predictions. Each setup was run for 200 different random seeds to ensure robustness of the results.\nThe simulation results are summarized in 6. My find-ings suggest that methods which estimate latent group struc-tures consistently outperform approaches that add additional columns for each group. Specifically, the sparse low-rank encoding method performs well for smaller numbers of la-tent groups, while multinomial and means encoding tend to perform better as the number of latent groups increases. The multinomial approach performs particularly well when the number of samples is large, and group sizes are sufficiently high.\nFor methods not exploiting low-rank structures, I observe that the primary benefit in regression forests and xgboost comes from reducing the dimensionality. Permutation-based methods and multiple permutations outperform one-hot en-coding, though they still lag behind latent group estimation techniques.\nPerformance improvements over one-hot encoding are mod-est (1-10%) for datasets with 2 latent groups but can reach up to 27-33% for 10 latent groups. Intuitively, the complexity of the underlying relationships in models like regression forests and xgboost increases with the number of latent groups, leading to higher performance improvements in more complex"}, {"title": "D. Empirical Applications", "content": "We evaluate the proposed methods using publicly available datasets from Kaggle. To ensure that the training and testing data remain consistent, I implement 4-fold stratified cross-validation. This method helps alleviate issues arising when categorical variables in the test set may not appear in the training set. To validate the outcomes, I also perform a paired t-test to confirm or challenge the results obtained from the cross-validation process.\na) Chicago Educational Performance: The dataset an-alyzed here was compiled by Hemani through a series of surveys conducted by Alif Ailaan, a nonprofit organization dedicated to enhancing education in Chicago. The surveys were designed to provide a neutral comparison of educational systems across various cities and provinces, promoting com-petition among local governments to spur educational reforms.\nThis dataset consists of n = 580 observations (after remov-ing rows with missing values, reducing it to 504 observations) across |G| = 127 cities from 2013 to 2016. The dataset contains 20 additional covariates, with further preprocessing and cleaning steps available on GitHub.\nb) Ames Housing: The Ames Housing dataset [12] was developed as a more intricate alternative to the Boston Housing dataset [13]. De Cock created it for a regression course project, offering a rich set of features for students to demonstrate their proficiency in regression analysis.\nThis dataset includes n = 2,930 home sales from Ames, Iowa, recorded between 2006 and 2010. It contains 80 covari-ates, and the categorical feature \"neighborhood\" has |G| = 25 distinct categories."}, {"title": "E. Empirical Results", "content": "The application of various encoding strategies to regression forests has demonstrated improvements over the traditional one-hot encoding method. However, when applied to XG-Boost, these alternative encodings appear to have a minimal effect. Notably, in the case of regression forests, the Ames dataset presents an intriguing anomaly, where the number of covariates p greatly exceeds the number of distinct groups |G|. This imbalance leads to a significant increase in dimensionality when using certain encoding methods. Specifically, techniques like Means and MNL add up to 80 dimensions, while one-hot encoding only adds 25. Despite this, methods that focus on low-rank representations and sparse low-rank encodings seem to hold more promise, yielding potentially more robust per-formance in this context. In contrast, XGBoost's performance generally did not show a marked improvement over one-hot encoding, with the Ames dataset standing out as the exception, where a slight gain was observed.\nAcross the datasets, low-rank encoding techniques seem to provide the most consistent results for regression forests. This could be due to dimensionality reduction, which offers two key benefits. First, these methods capture the low-rank relationships between the covariates, which more accurately reflect the underlying structure of the data. Second, when the categorical variable provides limited information, low-rank"}, {"title": "Algorithm 2 Principal Component Analysis (PCA) Encoding Method", "content": "1: procedure PCAENCODING(X, G, k)\n2:\n\u0108 \u2190 GROUPCOVARIANCE(X, G)\n\u25b7 Compute the covariance matrix for each group\n3:\nP, A\u2190 EigenDecomposition(C)\n\u25b7 Perform eigenvalue decomposition on the covariance matrix\n4:\nS\u2190 Onxk\n5:\nfor i in 1:n do\n6:\nSi, \u2190 PG,1:k\n\u25b7 Assign the top k eigenvectors corresponding to the largest eigenvalues\n7:\nreturn S\nSimilarly, the NMF method decomposes the input matrix into two non-negative matrices, allowing me to extract latent features that sum up to the original data matrix. NMF has the advantage of being able to handle sparse data efficiently, as it ensures that the factorized components are non-negative, which aligns well with the nature of many real-world datasets."}, {"title": "Algorithm 3 Non-Negative Matrix Factorization (NMF) Encoding Method", "content": "1: procedure NMFENCODING(X, G, k)\n2:\nW, H \u2190 NMF(X, k)\n\u25b7 Factor the matrix X using Non-Negative Matrix Factorization\n3:\nS\u2190 Onxk\n4:\nfor i in 1:n do\n\u25b7 Populate with the factorized components for each sample\n5:\nSi, WG,1:k\n6:\nreturn S"}, {"title": "F. Supplementary Encoding Techniques", "content": "For a more comprehensive analysis, refer to [15]. It is worth noting that many of the methods discussed here are essentially different linear transformations of one another, and in theory, they should yield the same results. However, as demonstrated in Sections VI-C and VI-D, real-world outcomes can differ considerably.\na) One-hot or Indicator: This is the most frequently utilized categorical encoding, which serves as my reference method to compare with all alternative techniques. It divides the categorical variable into k-1 columns, where k represents the count of distinct elements within the categorical levels of the column. Each column is binary, with values of 1 or 0 depending on the presence of the corresponding category in the original column. [16, sec 2.3.2]\nb) Deviation: This method is similar to one-hot encod-ing, except that the row representing the kth unique category, which serves as the baseline level, is assigned all values of -1. As a result, the categorical levels are compared against the overall average of all levels, rather than the mean of a particular level in relation to the baseline.\nc) Difference: This method contrasts each level with the average of the preceding levels.\nd) Helmert: This method compares the levels of a se-lected categorical variable with the mean of subsequent levels that have been uniquely observed up to that point.\ne) Cumulative Effect: The columns are encoded to capture a progressive comparison between each subsequent level and the previous ones.\nf) Permutation: This technique assigns a distinct integer value to each category. Even though the categories may not"}, {"title": "VII. CONCLUSION", "content": "This paper presents a set of novel encoding techniques for high-cardinality categorical variables, addressing the limita-tions of traditional methods like one-hot encoding. Through theoretical analysis and extensive empirical validation, we demonstrate that means encoding, low-rank encoding, and multinomial logistic regression encoding significantly enhance model performance and computational efficiency. The results highlight the potential of these techniques to streamline feature engineering processes in machine learning while maintaining predictive accuracy.\nOur findings have practical implications across domains such as healthcare, finance, and cybersecurity, where high-cardinality categorical variables are prevalent. Future research could explore hybrid approaches combining these methods with advanced machine learning models or extending their ap-plicability to real-time data processing scenarios. The proposed techniques contribute to a growing body of research aimed at overcoming the challenges of categorical data representation, offering a scalable and interpretable solution for modern machine learning applications."}]}