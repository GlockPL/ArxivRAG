{"title": "Polish Medical Exams:\nA new dataset for cross-lingual medical knowledge transfer assessment", "authors": ["\u0141ukasz Grzybowski", "Jakub Pokrywka", "Michal Ciesi\u00f3\u0142ka", "Jeremi I. Kaczmarek", "Marek Kubis"], "abstract": "Large Language Models (LLMs) have demonstrated significant potential in handling specialized tasks, including medical problem-solving. However, most studies predominantly focus on English-language contexts. This study introduces a novel benchmark dataset based on Polish medical licensing and specialization exams (LEK, LDEK, PES) taken by medical doctor candidates and practicing doctors pursuing specialization. The dataset was web-scraped from publicly available resources provided by the Medical Examination Center and the Chief Medical Chamber. It comprises over 24,000 exam questions, including a subset of parallel Polish-English corpora, where the English portion was professionally translated by the examination center for foreign candidates. By creating a structured benchmark from these existing exam questions, we systematically evaluate state-of-the-art LLMs, including general-purpose, domain-specific, and Polish-specific models, and compare their performance against human medical students. Our analysis reveals that while models like GPT-40 achieve near-human performance, significant challenges persist in cross-lingual translation and domain-specific understanding. These findings underscore disparities in model performance across languages and medical specialties, highlighting the limitations and ethical considerations of deploying LLMs in clinical practice.", "sections": [{"title": "Introduction", "content": "The potential of Artificial Intelligence, particularly Large Language Models, is vast, yet these technologies come with considerable risks. One of the most pressing concerns is the phenomenon of \u201challucinations\", where LLMs generate responses that appear accurate but are incorrect or misleading. This issue is particularly critical in fields like medicine, where errors can have serious, sometimes life-threatening, consequences. Thus, rigorous evaluation of LLM performance is imperative before their integration into clinical practice (Minaee et al., 2024).\nLLM performance varies widely due to differences in training methods, datasets, and objectives. These factors ultimately dictate how well an LLM can execute specific tasks. The quality and diversity of training datasets are especially crucial for LLM performance in specialized domains, such as medicine (Minaee et al., 2024). Models trained on comprehensive, domain-specific datasets tend to outperform those trained on general-purpose data, although this view has been questioned by many.\nLanguage also plays a significant role in the performance of LLMs. Many of the most widely studied models are trained using multilingual datasets that predominantly contain English texts. Consequently, LLMs often exhibit better performance with English-language inputs, while struggling with non-English content (Minaee et al., 2024). Moreover, LLMs trained solely on non-English texts may miss crucial knowledge available only in English.\nModern medicine is fundamentally evidence-based, and one might presume that the correct management of a certain medical problem should be nearly universal worldwide. However, in practice, the situation is considerably more complex. Clinical practices are influenced by a complex interplay of factors, including epidemiological trends, economic resources, healthcare infrastructure, technology availability, cultural norms, legal and regulatory frameworks, and even environmental conditions (Minaee et al., 2024).\nEpidemiology shapes medical education, with curricula typically emphasizing conditions prevalent in the local population. Medical students are trained to manage health problems that are common in their geographical region. For example, in areas with high tuberculosis prevalence, medical education places significant focus on its diagnosis and management. Conversely, regions facing a higher prevalence of non-communicable diseases tend to emphasize training in chronic, lifestyle conditions. This localized focus ensures healthcare professionals are adequately prepared for the most relevant health issues within their communities. However, it can also mean that language-specific training data, when applied to other populations, may under-represent certain conditions.\nFurthermore, physicians often rely on the local prevalence and incidence of diseases to guide their diagnostic decisions. For instance, when the clinical presentation is non-specific, in regions where infectious diseases are common, healthcare professionals are more likely to suspect an infectious cause, whereas in developed regions, noninfectious etiologies may be considered earlier.\nTreatment choices are similarly influenced by local health data, for example when choosing antibiotics without specific microbiological test results (an approach called empiric antibiotic therapy). Doctors often decide on antibiotic combinations based on how resistant local bacteria are to these drugs. For example, the treatment to eliminate Helicobacter pylori, a bacteria which can cause stomach ulcers, differs depending on regional resistance to common antibiotics like clarithromycin and metronidazole. In areas where clarithromycin is less effective, doctors may use a more complex combination of drugs to increase success rates (Yang et al., 2014). These differences show why treatment plans need to be adapted to local conditions for the best patient outcomes.\nMoreover, the availability of diagnostic methods and medications varies across different regions of the world, leading to differences in clinical practice standards. For instance, if a blood test result justifies initiating a specific treatment, in regions with widespread access to laboratory testing, the standard approach would typically involve performing the test first and basing the treatment decision on its result. Conversely, in countries with limited access to laboratory diagnostics, the standard practice might involve initiating treatment without prior testing and discontinuing it if no improvement is observed. Clinical decision-making often hinges on weighing potential benefits against risks. In the example above, initiating therapy without laboratory confirmation may not necessarily be considered a mistake, especially if the medication is safe and testing is significantly limited.\nSocietal norms and legal frameworks significantly influence medical practice, sometimes diverging from current medical evidence. In regions with restrictive legislation, access to procedures such as abortion or gender-affirming surgeries may be heavily restricted. Conversely, certain procedures like elective cosmetic surgeries are widely practiced in some countries. As a result, the corpora used to train LLMs can embed cultural and legal influences, potentially skewing the model's recommendations towards decisions aligned with the dominant perspectives in the data. Additionally, medical examinations can also be biased, as these norms and laws can influence the framing of questions and the designation of correct answers.\nLLMs trained predominantly on English-language data are more likely to align with the disease prevalence and clinical guidelines typical of English-speaking countries. As a result, their diagnostic and therapeutic recommendations may be biased towards practices common in these regions. On the other hand, when faced with the same clinical scenario presented in different languages, an LLM may yield dissimilar responses, reflecting the variety of healthcare practices across multiple countries that appear in the training data. This phenomenon becomes especially pertinent when evaluating LLM performance on non-English medical tests, such as those conducted in Poland, where the disease prevalence and medical guidelines may diverge from those in English-speaking countries.\nTo study LLM performance in the medical setting, we propose a new benchmark that consists of publicly available exam questions from medical and dental licensing examinations, as well as specialist-level exams conducted in Poland. The dataset comprises over 24,000 questions, primarily in Polish. A subset of questions from the licensing examinations also includes English-language counterparts, facilitating comparative analysis.\nWe use the proposed benchmark to study the behavior of LLMs aiming at the following research questions:\n\u2022 How does the performance of LLMs on Polish medical examinations differ across various models and various exam types?\n\u2022 How does the performance of evaluated LLMs compare to that of human doctors and medical students?"}, {"title": "Related work", "content": "LLMs are poised to transform various aspects of medicine by supporting medical professionals and enhancing research and education (Abd-Alrazaq et al., 2023; Clusmann et al., 2023). They can assist with literature summarization, data extraction, manuscript drafting, patient-clinical trial matching, and the creation of educational content (Clusmann et al., 2023; Harrer, 2023; Yang et al., 2023). By facilitating the conversion of unstructured to structured data, they streamline communication, translating complex medical information and summarizing patient records to simplify documentation (Clusmann et al., 2023). Applications in medical report generation and preauthorization letters can notably reduce the administrative burden on clinicians, allowing more focus on patient care (Harrer, 2023). This enhanced efficiency benefits healthcare systems not only economically but also supports personalized, patient-centered care through improved clinician-patient interactions (Clusmann et al., 2023; Nazi and Peng, 2024). Additionally, LLMs may aid diagnostics and management planning by analyzing large volumes of medical data and monitoring patient parameters (Nazi and Peng, 2024).\nThe integration of LLMs in healthcare necessitates rigorous evaluation to ensure reliability and safety, given the complexity and high stakes of medical decisions. Robust evaluation is crucial for assessing the performance of LLMs, identifying weaknesses, and mitigating biases to maintain patient safety and promote equity (Karabacak and Margetis, 2023; Li et al., 2023). Further, evaluation should ensure that LLMs genuinely enhance clinical care and effectively support healthcare professionals (Karabacak and Margetis, 2023).\nEvaluating LLMs for medical use requires a tailored approach that goes beyond standard metrics, incorporating the specific demands of healthcare. Evaluation efforts should consider both technical performance, such as accuracy, reasoning abilities, and factual reliability using benchmark datasets (e.g., medical licensing exams), and real-world utility, including clinical impact studies and workflow integration (Karabacak and Margetis, 2023; Chang et al., 2024; Nazi and Peng, 2024).\nMedical datasets are created using various approaches. The MedQA dataset comprises United States Medical Licensing Examination (USMLE)-style questions (Jin et al., 2020), while JAMA Clinical Challenge is based on the JAMA Network Clinical Challenge archive (Chen et al., 2024). Med-bullets uses simulated clinical questions sourced from social media posts (Chen et al., 2024), and PubMedQA utilizes questions and contexts derived from PubMed articles (Jin et al., 2019).\nMost of the current datasets focus on English, which reflects both the dominance of English in medical research and the initial English-centric development of LLMs. However, there is growing recognition of the need for multilingual and non-English datasets to ensure the broader applicability of medical LLMs. MedQA is notable for its multilingual approach, incorporating questions from medical board exams in English, Simplified Chinese, and Traditional Chinese (Jin et al., 2020). Additionally, there are datasets built around medical examinations in specific languages, including Swedish MedQA-SWE (Hertzberg and Lokrantz, 2024), Chinese CMExam (Liu et al., 2024), Japanese IGAKU QA (Kasai et al., 2023), and Polish.\nFor Polish, Lekarski Egzamin Ko\u0144cowy (LEK, Eng. Medical Final Examination) was used as a benchmark (Roso\u0142 et al., 2023; Bean et al., 2024; Suwa\u0142a et al., 2023). LEK is available in both Polish and English, allowing researchers to evaluate the influence of language on LLM performance. To date, analyses have primarily focused on GPT models, though several other LLMs, including LLaMa and Med42, have also been evaluated (Bean et al., 2024).\nRegarding the Pa\u0144stwowy Egzamin Specjalizacyjny (PES, Eng. State Specialisation Examination), a few studies have assessed GPT's performance in specialized field exams (Suwa\u0142a et al., 2023; Kufel et al., 2023; Wojcik et al., 2023). Pokrywka et al. (2024) provided a comprehensive evaluation of GPT-3.5 and GPT-4 on the PES, uti-"}, {"title": "Polish Medical Exams", "content": "In Poland, five types of exams for physicians and dentists are conducted: LEK (Lekarski Egzamin Ko\u0144cowy, Eng. Medical Final Examination), LDEK (Lekarsko-Dentystyczny Egzamin Ko\u0144cowy, Eng. Dental Final Examination), LEW (Lekarski Egzamin Weryfikacyjny, Eng. Medical Verification Examination), LDEW (Lekarsko-Dentystyczny Egzamin Weryfikacyjny, Eng. Dental Verification Examination), and PES (Pa\u0144stwowy Egzamin Specjalizacyjny, Eng. National Specialization Examination, Board Certification Exam). LEW and LDEW are for graduates of medical or dental studies carried outside of the European Union. Passing these exams is necessary for them to legally practice in Poland. However, these LEW and LDEW are taken by a relatively small number of candidates, and access to previous exam questions is limited. Therefore, they are not included in our work.\nMedical studies in Poland last 6 years, while dentistry takes 5 years. Final-year students and graduates can take their respective final exams\u2014LEK for medicine and LDEK for dentistry. Passing the final examination and completing a postgraduate internship are required to obtain a medical license.\nBoth LEK and LDEK are four-hour exams conducted twice a year. Each exam consists of 200 multiple-choice questions with five possible answers, of which only one is correct. The questions cover a wide range of medical or dental disciplines. To pass, a candidate must correctly answer at least 56% of the questions. Physicians and dentists can retake these exams multiple times, even after passing, if they are dissatisfied with their score. A controversial rule has been introduced in 2022, stipulating that 70% of the exam questions come from a publicly available database, which includes 2,870 questions for LEK and 3,198 for LDEK. After these changes, the average exam scores and the percentage of passing candidates increased significantly."}, {"title": "Dataset overview", "content": "The dataset comprises medical exams from the Medical Examination Center (CEM) and the Supreme Medical Chamber (NIL), covering LEK, LDEK, and PES exams from 2008\u20132024. The exams were sourced as HTML quizzes and PDF files, with missing data from 2016\u20132020 (LEK/LDEK) and 2018-2022 (PES) partially filled using archives published on the NIL website. The exams were categorized by specialization, with questions and answers stored separately. Automated tools were used to scrap and process data, balancing parallelization with server constraints. Preprocessing ensured the dataset's suitability for text-only AI benchmarks by removing irrelevant files, questions containing images, and content misaligned with current medical knowledge. We refer to these as \"invalidated questions\" throughout the text. Detailed descriptions of data sources, acquisition methods, and quality considerations are provided in Appendix B.\nFinally, we created five sub-datasets: LEK, LDEK, PES, LEK en (LEK translated into English), and LDEK en (LDEK translated into English). Not all of them were released in the same edition, particularly the Polish and English counterparts. Therefore, the results presented in Section 5 should not be used to directly compare LLM performance on Polish exams with their English translations. To address this, we focused on the overlapping years and reported these results in Section 7. For PES dataset, we collected a total of 180,712 questions. For our analysis, we selected only the latest exam from each specialty and based our analysis on these. Detailed dataset statistics are provided in Table 3. In summary, our analysis encompasses over 24,000 questions. For LLM inference, we utilized the Huggingface Transformers library (Wolf, 2019) and the OpenAI API."}, {"title": "Performance of LLMs on exams", "content": "We categorized the models under study into the following groups: medical LLMs (models fine-tuned on English medical data), general-purpose multilingual LLMs, Polish-specific models, and models with restricted APIs.\nMedical Models: BioMistral-7B (Labrak et al., 2024), Meditron-3 (8B and 70B versions) (OpenMeditron, 2024), JSL-MedLlama-3-8B-v2.0 (johnsnowlabs, 2024).\nGeneral-Purpose Multilingual Models: Qwen2.5 Instruct (7B and 72B versions) (Team, 2024), Llama-3.1 Instruct (8B and 70B versions), Llama-3.2-3B (Dubey et al., 2024), mistralai/Mistral-Small-Instruct-2409, and Mistral-Large-Instruct-2407 (Jiang et al., 2023).\nPolish-Specific Model: Bielik-11B-v2.2 Instruct (Ociepa et al., 2024).\nRestricted API Models: GPT-40-mini and GPT-4-0 (Achiam et al., 2023).\nLLMs were evaluated by directly prompting them to answer exam questions. Each prompt included a brief introduction stating that the task was an exam for medical professionals consisting of single-choice questions. No additional examples or explanations were provided in the prompt; specifically, few-shot prompting was not employed. We believe this approach is appropriate for evaluating the models in a setting closely resembling the actual human exam environment.\nWe report the models' results as the percentage of correct answers in Table 4 and the number of exams passed in Table 5. Our findings are as follows: GPT-40 is the best performing model overall. Particularly in the PES category, it outperforms the second-best model. GPT-40 is capable of passing all evaluated exams except for six PES exams. However, GPT-40-mini performs significantly worse than GPT-40 and is also inferior to general-purpose open models. Among the non-restricted API models, Meta-Llama-3.1-70B-Instruct is the best performer. Generally, general-purpose models outperform medical-specific models, possibly because the latter were fine-tuned on English medical data. The Polish-specific general-purpose model, Bielik-11B-v2.2-Instruct, performs worse than the top multilingual general-purpose models such as Meta-Llama-3.1-70B-Instruct, Qwen2.5-72B-Instruct, and"}, {"title": "Specialty performance on PES exams", "content": "Among the 72 unique PES specialties, certain areas of medicine consistently challenge the majority of tested models, while others frequently rank among the highest-scoring categories based on model accuracy. By identifying the top five highest and lowest-scored categories, we gain insights into specific domains where models excel or struggle, highlighting their potential limitations in these fields. Their detailed results are presented in Appendix A.\nThe general field of medicine where LLMs struggle the most is dentistry, specifically in orthodontics, which appeared ten times in the top five lowest scores across 17 models, followed by conservative dentistry with endodontics and pediatric dentistry. These results suggest that certain nuances in dental specialties may not yet be fully captured by modern LLMs, leading to difficulties in understanding this broad field.\nThe most frequently occurring specialty among the highest-scoring categories was laboratory diagnostics, which appeared twelve times. This observation may indicate that diagnostics tasks align well with the pattern recognition and data interpretation capabilities of LLMs. Additionally, other specialties with high scores, such as public health and pulmonary diseases reflect the vast quantity and accessibility of data in those fields. The COVID-19 pandemic could have largely increased the resource pool regarding pulmonary and respiratory conditions."}, {"title": "Cross-lingual knowledge transfer", "content": "To compare the performance of various LLMs on Polish and English versions of the same datasets, we restricted the LEK and LDEK datasets to identical subsets (LEK exams in English are exact translations of the Polish exams). The analysis results, similar to the previous one, are presented in Tables 6 and 7. As shown, all medical models, except for OpenMeditron/Meditron3-70B, perform better on the English versions of the datasets. This may be due to these models being fine-tuned on English medical corpora. General-purpose multilingual models perform better on the English versions of the exams as well. This result is anticipated since these models are trained on corpora containing significantly more English than Polish. While these models are proficient in Polish, their performance on the tests remains lower in Polish than in English. The difference can be considerable; for example, meta-llama-Meta-Llama-3.1-8B-Instruct passed only one LEK exam in Polish but passed all 13 when translated into English. However, as model quality improves, the performance gap between languages narrows. For instance, with meta-llama-Meta-Llama-3.1-8B-Instruct, the accuracy difference between Polish LEK (51.25%) and English LEK (64.69%) is 13.44 percentage points (or a 26% relative change). In contrast, with meta-llama-Meta-Llama-3.1-70B-Instruct, the difference is only 1.66 percentage points (80.94% for Polish LEK vs. 82.60% for English LEK, or a 2% relative change).\nFor GPT-40-mini, which generally performs well, the results in English are only slightly better than in Polish. Interestingly, for GPT-40, performance is actually higher on the Polish version. The only Polish LLM, Bielik, performs better on Polish LEK and slightly better on Polish LDEK, likely due to its fine-tuning from the multilingual model Mistral-7B-v0.2 specifically for Polish. Overall, our observations suggest that language transfer is more effective as the model's general performance improves."}, {"title": "Comparison against human results", "content": "For each group of models described in Section 5, the top-performing model was selected to compare its predictions against the exam results of human students from the last four LEK and LDEK sessions: Spring 2024, Autumn 2023, Spring 2023, and Autumn 2022. That is 977 LEK and 984 LDEK questions. The selected models are as follows: Meditron3-70B for medical-specific models, Meta-Llama-3.1-70B-Instruct for general-purpose multilingual models, Bielik-11B-v2.2-Instruct for Polish-specific models, and gpt-4o-2024-08-06 for restricted API models.\nWhile all selected models passed the chosen LEK exams, only Meta-Llama-3.1-70B-Instruct and gpt-4o-2024-08-06 scored within the range defined by an average number of points \u00b1 standard deviation achieved by humans. Assuming a normal distribution of exam results, it could be concluded that these models performed as a typical medical student. Notably, for the spring 2024 LEK exam, Meditron3-70B also achieved an average-level result, while gpt-40-2024-08-06 exceeded the average student score. These findings are presented in Table 9.\nFor the LDEK exams, all models performed noticeably worse. Assuming a normal distribution of exam results, only gpt-40-2024-08-06 maintained a performance level comparable to that of an average medical student, consistent with its LEK exam results. In contrast, Meditron3-70B and Bielik-11B-v2.2-Instruct performed poorly, failing all exams, while Meta-Llama-3.1-70B-Instruct scored below the average but managed to pass each exam. These outcomes are summarized in Table 10.\nThe same models were used to compare their performance with that of human students on the PES exams. This analysis was conducted on a dataset created from the intersection of human results and LLM test results, encompassing 9,965 medical questions across 68 specializations from 12 exam sessions: Spring 2024, Autumn 2023, Spring 2023, Autumn 2020, Autumn 2019, Spring 2018, Spring 2017, Autumn 2016, Spring 2016, Autumn 2015, Spring 2012, and Autumn 2008. The number of specializations is smaller than in the previous analysis due to inconsistencies in the specialization names between shared exams and published human results. The best-performing model was gpt-4o-2024-08-06, which achieved results in 60% of cases better than half of the student population or within the top 25% of scores. Notably, this model outperformed all students in a thoracic surgery exam. However, it is important to note that the student population for this particular exam was relatively small, consisting of only six participants. However, it is worth noting that even the best model achieved results worse than half of the student population in over 30% of specializations. For the Audiology & phoniatrics specialization, the model underperformed compared to all students. However, the student population for that particular case was relatively small, consisting of only nine participants. The second-best model, Meta-Llama-3.1-70B-Instruct, delivered significantly worse performance compared to the best model. Only 10% of its results across specializations were above the population median, while in over 30% of medical specializations, its performance was above the 25th percentile. The remaining models, Meditron3-70B and Bielik-11B-v2.2-Instruct, performed extremely poorly, with most of their results falling below the 25th percentile or even below the lowest scores of the entire student population. The students' results are illustrated in the box plot diagrams presented for each medical specialization in Appendix C. In these plots, the whiskers represent the minimum and maximum student scores rather than the inter-quartile range. The red horizontal line indicates the exam passing threshold, while the other colored horizontal lines represent the scores achieved by the tested LLM models. An aggregated exam results are provided in Table 8, where the scores of each model X are categorized into the following ranges:\n\u2022 X < p\u2081: Represents the range where model X achieves scores lower than all students, indicating that the model underperforms compared to humans.\n\u2022 \u03a7\u2208 [po, P25): Results achieved by model X fall within the range of the lowest 25% of student scores.\n\u2022 X\u2208 [P25, P50): Results achieved by model X are between the 25th and 50th percentiles of student scores, indicating performance worse than half of the student population but better than the first quartile.\n\u2022 \u03a7\u2208 [P50, P75): Results achieved by model X fall in the range of scores better than half of the students but below the top 25%.\n\u2022 X \u2208 [p75, P100): Results achieved by model X are within the top 25% of student scores.\n\u2022 X \u2265 P100: Results where model X achieved or exceeded the best human score."}, {"title": "Limitations", "content": "While LLMs have demonstrated impressive performance on Polish medical multiple-choice exams, this achievement represents only a narrow facet of medical expertise. Becoming a licensed physician in Poland requires extensive training, rigorous coursework, and hands-on experience with practical medical procedures\u2014far beyond what written exams can assess. Clinical practice necessitates analyzing diverse information and solving complex problems with multiple possible solutions. Physicians must determine what data is needed, obtain it through patient interviews, physical examinations, diagnostic tests, and consultations\u2014all heavily reliant on direct human interaction that AI models cannot replicate. Moreover, the exams are multiple-choice, and real-world work is not narrowed to a few possible options. Therefore, despite strong exam results, LLMs cannot currently substitute the comprehensive qualifications and essential human interactions integral to effective medical care. However, this work shows that LLMs may be useful tools for medical practitioners. (Ullah et al., 2024; Park et al., 2024; Clark and Bailey, 2024; Liu et al., 2023; Lee et al., 2023).\nDue to regional access restrictions, we were unable to evaluate PaLM 2 (Anil et al., 2023) and certain Llama 3.2 models. Additionally, highly resource-intensive models such as Meta-Llama-3.1-405B-Instruct or some other restricted access LLMs, such as Gemini (Gemini et al., 2023) were not evaluated."}, {"title": "Ethics Statement", "content": "This research involves the collection and analysis of publicly available medical examination data from the Medical Examination Center and the Supreme Medical Chamber. All data used are openly accessible and do not contain any personally identifiable information or confidential content. We have ensured compliance with the terms of use specified by the data providers and have respected all applicable laws and regulations regarding data privacy and intellectual property.\nThe primary objective of this study is to assess the performance of LLMs on Polish medical exams and to introduce a new dataset for cross-lingual knowledge transfer assessment. While LLMs have demonstrated strong performance on multiple-choice medical exams, we acknowledge that these models cannot replace the comprehensive qualifications, clinical experience, and human interactions essential to effective medical practice. Our findings should not be interpreted as an endorsement for using LLMs as substitutes for licensed medical professionals.\nWe recognize the potential risks associated with deploying LLMs in healthcare settings, including the dissemination of inaccurate or misleading information. To mitigate these risks, we emphasize that any application of LLMs in medical contexts should involve oversight by qualified healthcare practitioners and adhere strictly to ethical guidelines and regulatory standards."}, {"title": "Data preparation", "content": "Data sources\nMedical exams in Poland are conducted biannually, in spring and autumn. Past exam content and corresponding answers are available on the Medical Examination Center (Centrum Egzamin\u00f3w Medycznych, CEM) website, either as quizzes or PDF files. The site archives the following exams in the Polish language:\n\u2022 LEK exams from autumn 2008 to autumn 2012 are provided as PDF files,\n\u2022 LEK exams from spring 2013 to autumn 2015, and from spring 2021 to autumn 2024 are available as quizzes,\n\u2022 LDEK exams from autumn 2008 to autumn 2012 are available as PDF files,\n\u2022 LDEK exams from spring 2013 to autumn 2015, and from spring 2021 to autumn 2024 are provided as quizzes,\n\u2022 PES exams from spring 2003 to autumn 2017, and from spring 2023 to spring 2024 are available as quizzes.\nLEK and LDEK exams published as quizzes are also available in English. The missing LEK and LDEK exams from spring 2016 to autumn 2020 have not been found. The missing PES exams from spring 2018 to autumn 2022 have been published as PDF files on the Supreme Medical Chamber (Naczelna Izba Lekarska, NIL) website."}, {"title": "Data acquisition and processing", "content": "The missing PES exams were published on the Supreme Medical Chamber platform across two distinct pages, with separate archives for the periods 2018-2020 and 2021-2022. Each medical specialization's exams were compressed into a zip file and provided as individual download links. To streamline the downloading process, a JavaScript script was executed via Chrome's Developer Tools, iterating through the links and simulating clicks for automatic downloads. The exams were then categorized by specialization, with each folder containing two types of PDF files: questions and the corresponding correct answers.\nCustom Python scraping scripts were developed to automate the downloading of quizzes from the Medical Examination Center platform. Separate scripts were created for LEK/LDEK exams, PES exams, and exam statistics. Due to the server's slow response time, the entire process took several days, even with parallelized data download. When too many concurrent threads were used, the server became overwhelmed, resulting in timeouts."}, {"title": "Data quality", "content": "Data is stored in two formats: PDF and HTML, both of which are inconsistent and present several challenges. Since the goal of creating this dataset is to establish a Polish medical benchmark for Large Language Models, questions containing images were excluded. Additionally, some questions were disqualified by their authors due to errors or inconsistencies with current medical knowledge."}, {"title": "HTML format", "content": "HTML format is relatively straightforward to process, as specific HTML tags can be used to extract information such as questions and correct answers. However, some questions contain images that are essential for context, which poses a challenge for Al models designed to process text. Since the final dataset is intended for text-based AI models, questions containing images were excluded using specific tags. Additionally, the quiz interface allows anonymous users to leave comments on individual questions. These comments could potentially highlight areas where the content's alignment with current knowledge has been questioned. However, many of the comments appeared unprofessional and seemed not to be moderated by the platform administrators. As a result, the presence of comments was not considered a valid indicator for filtering questions, and all of them were kept in the final dataset.\nMoreover, the raw dataset contains empty questions. The platform uses two static drop-down lists to browse questions based on exam date and medical specialization, even when no corresponding exam or question is available in the database. According to the platform's messages, missing data occurs either due to the absence of questions in the database or because exams were not conducted during a specific time. This design leads to a collection of HTML files with no meaningful content. Since the user interface does not manage these cases, it was necessary to filter out and remove such files from the dataset after downloading."}, {"title": "PDF files", "content": "Processing PDF files is more challenging compared to HTML due to the need to handle content sequentially, line by line, while applying multiple conditions to accurately extract medical exam questions. Additionally, the structure of questions is inconsistent across points, pages, and files. The question content or answer options may be presented in various formats, such as horizontal lists, vertical lists, two separate lists of options, or a table where points must be matched across columns. This inconsistency complicates the extraction process and poses difficulties for data processing."}]}