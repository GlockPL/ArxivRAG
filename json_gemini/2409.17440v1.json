{"title": "A TIME SERIES IS WORTH FIVE EXPERTS: HETEROGE- NEOUS MIXTURE OF EXPERTS FOR TRAFFIC FLOW PRE- DICTION", "authors": ["Guangyu Wang", "Yujie Chen", "Ming Gao", "Zhiqiao Wu", "Jiafu Tang", "Jiabi Zhao"], "abstract": "Accurate traffic prediction faces significant challenges, necessitating a deep understand- ing of both temporal and spatial cues and their complex interactions across multiple vari- ables. Recent advancements in traffic prediction systems are primarily due to the devel- opment of complex sequence-centric models. However, existing approaches often embed multiple variables and spatial relationships at each time step, which may hinder effec- tive variable-centric learning, ultimately leading to performance degradation in traditional traffic prediction tasks. To overcome these limitations, we introduce variable-centric and prior knowledge-centric modeling techniques. Specifically, we propose a Heterogeneous Mixture of Experts (TITAN) model for traffic flow prediction. TITAN initially consists of three experts focused on sequence-centric modeling. Then, designed a low-rank adap- tive method, TITAN simultaneously enables variable-centric modeling. Furthermore, we supervise the gating process using a prior knowledge-centric modeling strategy to ensure accurate routing. Experiments on two public traffic network datasets, METR-LA and PEMS-BAY, demonstrate that TITAN effectively captures variable-centric dependencies while ensuring accurate routing. Consequently, it achieves improvements in all evaluation metrics, ranging from approximately 4.37% to 11.53%, compared to previous state-of-the- art (SOTA) models. The code is open at https://github.com/sqlcow/TITAN.", "sections": [{"title": "1 INTRODUCTION", "content": "Traffic prediction involves forecasting future traffic conditions based on historical data collected from sen- sors(Jiang et al., 2021), a task that has garnered significant attention in recent years(Jin et al., 2024). Highly accurate traffic predictions can provide valuable guidance to decision-makers, enhance safety and conve- nience for citizens, and reduce environmental impact(Jin et al., 2022; Cai et al., 2020). Moreover, with the rapid advancement of artificial intelligence, autonomous driving technologies will also benefit from precise and timely traffic flow forecasts(Guo & Jia, 2022).\nTraffic data is predominantly spatio-temporal(Yuan & Li, 2021), indicating that it is intrinsically linked to the spatial location of sensors while also exhibiting temporal variations, thereby demonstrating considerable spatio-temporal heterogeneity. This characteristic makes certain methods that excel in conventional time se- ries forecasting, such as Support Vector Regression (SVR)(Awad et al., 2015), Random Forest (RF)(Rigatti, 2017), and Gradient Boosting Decision Trees (GBDT)(Ke et al., 2017), less effective in spatio-temporal prediction tasks. In recent years, the latest advancements in spatio-temporal forecasting have witnessed the rise of Graph Neural Networks (GNNs) as powerful tools for modeling non-Euclidean spaces(Wu et al.,"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 TRAFFIC FLOW PREDICTION", "content": "Traffic Flow Prediction tasks exhibit significant spatio-temporal heterogeneity and complex variable interac- tion patterns. Traditional machine learning approaches like Support Vector Regression (SVR)(Awad et al., 2015), Random Forest (RF)(Rigatti, 2017), and Gradient Boosting Decision Trees (GBDT)(Ke et al., 2017), which rely heavily on feature engineering, struggle to capture these intricate interactions. Early spatio- temporal prediction models primarily focused on incorporating spatial information into models through graph structures that could effectively handle non-Euclidean spaces. For example, in 2018, DCRNN(Li et al., 2018) was introduced, injecting graph convolutions into recurrent units, while Yu et al. (2018)com- bined graph convolutions with Convolutional Neural Networks (CNN) to model spatial and temporal fea- tures, achieving better performance than traditional methods like ARIMA(Shumway et al., 2017). Although these approaches were effective, they depended significantly on predefined graphs based on Euclidean dis- tance and heuristic rules (such as Tobler's First Law of Geography(Miller, 2004)), overlooking the dynamic nature of traffic (e.g., peak times and accidents). Later work, such as GraphWaveNet(Wu et al., 2019), addressed this limitation by constructing learnable adjacency matrices using node embeddings for spatial modeling. Despite achieving improved results, its ability to capture anomalies remained limited. More recent models like MegaCRN(Jiang et al., 2023b) improved the model's adaptability to anomalies by inte- grating meta-graph learners supported by meta-node libraries into GCRN encoder-decoder structures. While these models enhanced robustness, they were constrained by independent modeling techniques. This lim- itation has led to growing interest in spatio-temporal prediction models based on MoE structures(Jawahar et al., 2022; Zhou et al.). For instance, TESTAM(Lee & Ko, 2024) integrated three experts with different spatial capturing modules to improve spatio-temporal forecasting performance. However, these studies re- main sequence-centric, limiting their ability to capture inter-variable relationships effectively. In this paper, we aim to address this challenge by jointly modeling both sequence-centric and variable-centric dependen- cies, allowing for adaptive consideration of both temporal and cross-variable interactions. This approach provides a more comprehensive view of the data and enhances the ability to model complex spatio-temporal dynamics."}, {"title": "2.2 MIXTURE OF EXPERTS", "content": "The Mixture of Experts (MoE) model, originally proposed by Jacobs et al. (1991), allows individual experts to independently learn from subsets of the dataset before being integrated into a unified system. Build- ing on this concept, Shazeer et al. (2017a) introduced the sparse gated Mixture of Experts (SMoE), which utilizes a gating network for expert selection and implements a top-K routing strategy, selecting a fixed number of experts for each input. Lepikhin et al. (2020) advanced this with Gshard, and Wang et al. (2024) further demonstrated that not all experts contribute equally within MoE models, discarding less important experts to maintain optimal performance. Despite these advancements, MoE models face challenges in spatio-temporal tasks. The early training phase often leads to suboptimal routing, especially when encoun- tering unpredictable events. In such cases, MoE struggles to query and retrieve the appropriate information from memory, resulting in ineffective routing decisions. While SMoE introduces inductive bias through fine-grained, location-dependent routing, it primarily focuses on avoiding incorrect routing and neglects to optimize for the best paths. Similarly, TESEAM(Lee & Ko, 2024) improves routing by utilizing two loss functions one to avoid poor paths and another to optimize the best paths for expert specialization but still fails to address the fundamental issue in spatio-temporal predictions. In tasks with high spatio-temporal heterogeneity, such as traffic flow prediction, the MoE model's reliance on independent expert structures"}, {"title": "3 METHODS", "content": null}, {"title": "3.1 PROBLEM DEFINITION", "content": "Traffic flow prediction is a spatio-temporal multivariate time series forecasting problem. Given historical observations $X = {Xt \u2208 R^{N\u00d7F}}$, where N is the number of spatial vertices and F is the number of raw input features (e.g., speed, flow), each time step t is represented by a spatio-temporal graph $G_t = (V, E_t, A_t)$. The task is to predict T future graph signals based on T' historical signals. The model learns a mapping function $f(\u00b7) : R^{T'\u00d7N\u00d7C} \u2192 R^{T\u00d7N\u00d7C'}$, where C represents features derived from processing the original F."}, {"title": "3.2 MODEL ARCHITECTURE", "content": "Although sequence-centric models have shown success in spatio-temporal forecasting, they often struggle to capture complex variable interactions and are challenging to train, leading to reduced accuracy. Ad- ditionally, in MoE models, improper memory initialization during pre-training can result in suboptimal routing.TITAN overcomes these limitations by incorporating variable-centric and prior knowledge-centric approaches alongside the traditional sequence-centric method. As shown in Figure 1, TITAN integrates these five experts: 1) three sequence-centric experts (handling sequence-center dependencies in 3.2.1), 2) a variable-centric expert (focusing on variable-center interactions in 3.2.2), and 3) a prior knowledge ex- pert (guiding early-stage routing in 3.2.3). The experts, except for the prior knowledge expert, are based"}, {"title": "3.2.1 SEQUENCE-CENTRIC MODELING", "content": "Sequence-centric modeling is a classical approach for spatio-temporal tasks. Building on the backbone of the transformer architecture and incorporating the MOE structure, we have designed three distinct modeling approaches: (1) Temporal Attention expert, (2) Spatio-Temporal Attention expert, and (3) Memory Attention expert. To facilitate explanation, we define the classical Multi-Head Self-Attention (MSA) computation process (Vaswani et al., 2017) as follows: Given the input $X_{data} \u2208 R^{N\u00d7F}$, the attention result is computed as $X_{out}$ = MSA($X_{data}$, $X_{data}$, $X_{data}$). The formula for MSA is as follows:\n$Q = XinWQ, K = XinWK, V = XinWv,$\n$MSA(Xin, Xin, Xin) = softmax (\\frac{Xin WQ(XinWK)}{\\sqrt{d}}) XinWv,$\nwhere $W_Q$, $W_K$, $W_v \u2208 R^{F\u00d7d}$ are learnable weight matrices, and d is the dimension of each attention head. At the same time, during the training process, we define that all experts accept the same input $X_{data}$\nTemporal Attention expert is designed to capture dependencies between time steps, and therefore, it does not involve spatial modeling. However, urban traffic flow is significantly influenced by people's travel patterns and lifestyles, exhibiting clear periodicity (Jiang et al., 2024), such as during morning and evening rush hours. To enhance the ability of temporal attention to capture these periodic patterns, we introduce additional periodic embeddings. Let w(t) be a function that transforms time t into a week index (ranging from 1 to 7). The periodic time embedding $X_w \u2208 R^{T\u00d7d}$ is obtained by concatenating the embeddings of all T time slices. Finally, the periodic embedding output is calculated by simply adding the data embedding and the periodic embedding:\n$X_{emb} = X_{data} + X_w$ and $X_{ta}, H_{ta}$ = MSA($X_{emb}$, $X_{emb}$, $X_{emb}$)\nBy incorporating these periodic embeddings, the temporal attention mechanism is better equipped to capture the cyclical patterns present in traffic data.\nSpatio-Temporal Attention expert aims to enhance temporal predictions by leveraging the similarity be- tween nodes. Unlike Temporal Attention, it does not use additional periodic embeddings. Instead, it directly applies a linear transformation to map the input $X_{data}$ into a higher-dimensional space. After embedding, the transformed data $X_{emb} \u2208 R^{T\u00d7N\u00d7C}$ represents the information for T historical time steps, where N is the number of spatial nodes and C is the feature dimension. The process first applies a MSA layer across the N nodes, followed by another MSA layer across the T time steps to capture both spatial and temporal dependencies.\n$X_{sta}^{(1)} = MSA(X_{emb}.transpose(1,0, 2))$\n$X_{sta}, H_{sta} = MSA(X_{sta}^{(1)}).transpose(1, 0, 2))$\n$X_{sta}$ is the output of Spatio-Temporal Attention expert.\nMemory attention expert is inspired by memory-augmented graph learning (Hong et al.; Lee et al., 2022) and leverages the unique advantages of the memory module in the MOE model. The memory module M consists of a set of vectors with the shape $R^{N\u00d7m}$, where m represents the memory size. A hypernetwork (Ha et al., 2016) is employed to generate node embeddings conditioned on M. Once the new node em- beddings are generated, Memory Attention Expert first applies a Graph Convolutional Network (GCN) for spatial feature aggregation, followed by a MSA layer for final prediction. Since the memory module M is"}, {"title": "3.2.2 VARIABLE-CENTERED", "content": "In sequence-centric modeling approaches, each time step embedding integrates multiple variables represent- ing latent delayed events or different physical measurements. However, this may fail to learn variable-centric representations and could lead to meaningless attention maps. Additionally, all sequence-centric models share the same inductive bias, which limits the performance of the MOE model. To address these issues, we propose a variable-centric modeling approach. Specifically, time points in each time series are embed- ded into variable-specific tokens, and the attention mechanism leverages these tokens to capture correlations between different variables. Simultaneously, a feed-forward network is applied to each variable token to learn its nonlinear representation. Unlike sequence-centric models, which generate hidden states based on time steps, the variable-centric model does not share the same hidden state structure, making it difficult to be controlled by the MOE routing mechanism. To resolve this issue, we inject trainable rank decomposition matrices into each layer of the variable-centric model to generate hidden states similar to those in sequence- centric models, thereby reducing inductive bias. For input data $X_{data} \u2208 R^{T\u00d7N\u00d7F}$ over T historical time steps, we first map the time steps to a higher-dimensional space through a linear embedding. Specifically, the time steps T are transposed and embedded into the high-dimensional space:\n$X_{emb} = Embedding(X_{data}.transpose(1,2,0))$\nHere, Embedding is a linear mapping that projects the time steps T into a higher-dimensional space. The ex- pert's output is the result of passing this embedding through the MSA layer, and trainable low-rank matrices are injected between the MSA layers to generate the expert's hidden states\n$X_{enc} = MSA(X_{emb}, X_{emb}, X_{emb}); H_{hidden} = LowRank(X_{enc})$\nBy introducing these trainable low-rank matrices(See appendix A.1 for the detailed introduction), the variable-centric model is able to generate hidden states similar to those of the sequence-centric model, which helps reduce inductive bias and improves the performance of the model within the MOE framework."}, {"title": "3.2.3 ANNEALING ROUTING", "content": "In regression problems, traditional MoE models tend to make nearly static routing decisions after initializa- tion because the gating network is not effectively guided by gradients from the regression task, as highlighted by Dryden & Hoefler (2022). In this case, the gating network leads to a \"mismatch,\u201d resulting in routing in- formation that lacks richness and dynamics. Therefore, in time series tasks, the goal of the MoE architecture is typically to learn a direct relationship between input signals and output representations (Lee & Ko, 2024).\nFor the input $X_{input}$, which includes the input for multiple experts, the memory query process is defined as follows, and the hidden states output by all the experts are further used to calculate \u00d4:\n$O = softmax(\\frac{X_{out} M^{T}}{\\sqrt{d}}) M; \u00d4 = softmax(\\frac{H_{expert} H_{expert}}{\\sqrt{d}}) H_{expert}$"}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct experiments on two benchmark datasets: METR-LA and PEMS-BAY. METR-LA and PEMS-BAY contain four months of speed data recorded by 207 sensors on Los Angeles freeways and 325 sensors in the Bay Area, respectively(Li & Shahabi, 2018). Before training TITAN, we have performed z-score normalization. In the case of METR-LA and PEMS-BAY, we use 70% of the data for training, 10% for validation, and 20% for evaluation."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "For all two datasets, we use Xavier initialization to initialize the parameters and embeddings. Hyperpa- rameters are obtained via a limited grid search on the validation set with hidden size = {16, 32, 48, 64, 80}, memory size = {16, 32, 48, 64, 80}, and layers = {1, 2, 3, 4, 5}. Figure 2 illustrates the results of the hyperparameter search and the corresponding model parameter counts. Different colored lines represent various error metrics, while the bar chart shows the model's parameter count under different hyperparam- eters. We use the Adam optimizer with \u03b2\u2081 = 0.9, \u03b22 = 0.98, and \u20ac = 10-9. The learning rate is set to"}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "The experimental results are shown in Table 4. TITAN outperforms all other models, demonstrating an average improvement of approximately 9% across all forecasting horizons compared to the best baseline. It is noteworthy that we compare our reported results from respective papers with those obtained by replicating the official code provided by the authors. Sequence-centric modeling approaches, including both static graph models (DCRNN, RGDAN, MTGNN, CCRNN) and dynamic graph models (GMAN, AdpSTGCN), exhibit competitive performance in capturing spatiotemporal dependencies. However, STD-MAE achieves superior performance by reconstructing time series in both sequential and variational dimensions to capture complex spatiotemporal relationships. TESTAM, employing a MOE structure to simultaneously capture multiple spatial relationships, shows suboptimal performance. However, it is still subject to the inherent bias shared among homogeneous experts, resulting in performance inferior to TITAN. In contrast, our model TITAN demonstrates superiority over all other models, including those with learnable matrices. The results underscore the critical importance of correctly handling sudden events in traffic flow prediction. A more detailed efficiency analysis is provided in Appendix A.2, and the robustness for each indicator is visualized in Appendix A.5."}, {"title": "4.3 ABLATION STUDY", "content": "The ablation study has two goals: to evaluate the actual improvements achieved by each method and to test two hypotheses: (1) For heterogeneous MOE models, organizing the model using a low-rank adaptive approach is beneficial; (2) Organizing the experts through supervised routing is effective. To achieve these goals, we designed a set of TITAN variants as described below:\nw/o Semantics: This variant removes the heterogeneous semantics expert. In this case, TITAN only retains a sequence-centric modeling approach, which can be seen as similar to the TESTAM structure.\nEnsemble: The final output is not computed using MoE but as a weighted sum of the outputs of each expert via a gating network. This setup can use all heterogeneous experts but cannot use the a priori expert because there is no routing process.\nw/o priori expert: This variant directly removes the priori expert, and the routing process is only supervised by the classification loss.\nReplaced priori expert: This variant changes the organization of the priori expert by adding a temporal pre- diction module. In this structure, the priori expert can be seen as a sequence-centric modeling approach. The"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a heterogeneous mixture of expert model (TITAN) that can simultaneously perform sequence-centric, variable-centric, and prior knowledge-centric modeling, incorporating cosine annealing of prior knowledge during the training process. TITAN achieves state-of-the-art performance on two real- world datasets, delivering an average improvement of approximately 9% compared to the previous best baseline. Through ablation experiments, we demonstrated the effectiveness of each module within TITAN. Additionally, we conducted parameter and efficiency analyses to further assess TITAN's performance. For future work, we aim to introduce more intelligent algorithms, such as heuristic methods, to optimize the routing process in TITAN and extend its application to multivariate time series forecasting tasks."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 DETAILED INTRODUCTION TO LOW-RANK MATRIX ADAPTATION", "content": "The pseudocode above outlines the process of Low-Rank Adaptation in the Semantic Expert of the MoE model. The low-rank matrices WA and WB act as a bridge between the hidden layers and the final output of the Semantic Expert. In Step 4, a low-rank projection is applied to the embeddings using WA, followed by dimensional recovery using WB. The model iteratively updates both the main model parameters and the low-rank matrices WA and WB, ensuring that the low-rank matrices adapt to the data alongside the model's learning process."}, {"title": "A.2 EFFICIENCY ANALYSIS", "content": "To analyze the computational cost, we use seven baseline models:\nSTGCN (Yu et al., 2018), a lightweight approach that predicts future traffic conditions by utilizing GCN and CNN;\nDCRNN (Li et al., 2018), a well-established traffic forecasting model that integrates graph convolution into recurrent units, depending on a predefined graph structure;"}, {"title": "A.4 SOME MEANINGFUL OBSERVATIONS", "content": "In this section, we provide a deeper analysis of the expert selection behavior as presented in Table 5, which illustrates how the number of times each expert is selected changes with the prediction horizon (i.e., 3-step, 6-step, and 12-step predictions) on the PEMS-BAY dataset.\nThe Temporal Attention expertis rarely selected for short-term predictions (3-step horizon), with only 13 selections, suggesting that short-term forecasts do not heavily depend on temporal dynamics alone. How- ever, its selection increases moderately to 283 at the 6-step horizon, indicating growing importance as the prediction length increases. By the 12-step horizon, its selection count dramatically rises to 11,469,918, highlighting the critical role of temporal dependencies in long-term predictions. This indicates that the Tem- poral Attention expert becomes indispensable for longer-term forecasting, likely due to the increasing need to capture sequential patterns over time. The Semantics-Centered expert is most frequently selected during mid-term predictions (6-step horizon) and is utilized across all time steps, demonstrating the effectiveness of our low-rank adaptive method in designing this expert. The consistent selection of the Semantics expert across different horizons shows its robust contribution to the model's overall predictive capabilities."}, {"title": "A.5 FURTHER VISUAL CONCLUSIONS", "content": "In this section, we provide a more detailed explanation of the results discussed in the main text, with ad- ditional insights into TITAN's performance across various datasets and time steps. Specifically, we have generated box plots illustrating the error distributions for each evaluation metric, segmented by dataset and prediction time step, as shown in Figure 3."}, {"title": "A.6 REPRODUCIBILITY STATEMENT", "content": "We perform experiments using two extensive real-world datasets powered by (Li et al., 2018):\n\u2022 METR-LA: This dataset provides traffic information gathered from loop detectors installed on high- ways in Los Angeles County(Jagadish et al., 2014). For our experiments, we use data from 207 selected sensors and collect traffic data over a 4-month period from March 1st, 2012, to June 30th, 2012. The total number of traffic data points observed amounts to 6,519,002.\n\u2022 PEMS-BAY: This traffic dataset is provided by California Transportation Agencies (CalTrans) via the Performance Measurement System (PeMS). We gather data from 325 sensors located in the Bay Area over a 6-month period, spanning from January 1st, 2017, to May 31st, 2017. The total number of observed traffic data points is 16,937,179.\nThe distribution of sensors for both datasets is shown in Figure 4.\nAll model parameters and the selection process have been described in detail in the paper. We trained the models using 8 NVIDIA GPUs with a fixed random seed to ensure reproducibility. The weight files generated during training will be made publicly available along with the code after the paper is officially accepted, allowing researchers to access and verify the results."}]}