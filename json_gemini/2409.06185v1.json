{"title": "Can Large Language Models Unlock Novel Scientific Research Ideas?", "authors": ["Sandeep Kumar", "Tirthankar Ghosal", "Vinayak Goyal", "Asif Ekbal"], "abstract": "\"An idea is nothing more nor less than a new combination of old elements\" (Young, 2019). The widespread adoption of Large Language Models (LLMs) and publicly available Chat- GPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study ex- plores the capability of LLMs in generating novel research ideas based on information from research papers. We conduct a thorough ex- amination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). We found that the future re- search ideas generated by Claude-2 and GPT- 4 are more aligned with the author's perspec- tive than GPT-3.5 and Gemini. We also found that Claude-2 generates more diverse future re- search ideas than GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evalua- tion of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available\u00b9.", "sections": [{"title": "1 Introduction", "content": "An idea can be defined as a thought or suggestion aimed at solving a problem or considering a possi- bility. This concept is central to fields ranging from philosophy to science and economics. According to (Plato et al., 2000), ideas are archetypal forms that represent the most accurate reality. In the context of scientific research, (Kuhn and Hawkins, 1963) in \"The Structure of Scientific Revolutions\" describes an idea as a realization or hypothesis that can chal- lenge and shift paradigms within a scientific com- munity. Therefore, an idea can be understood as a cognitive construct that arises from the human mind's ability to process information, reflect, and imagine, serving as a cornerstone for creativity, problem-solving, and innovation. Idea generation can be generally understood as a state of focused internally-directed attention involving controlled semantic retrieval (Benedek et al., 2014).\nAs technology improves, new capabilities emerge. Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine (Zhao et al., 2023). Technological advancements serve two key functions in innovation. Firstly, they influence the goals of generating and selecting ideas. Secondly, they impact the methodology of how ideas are gen- erated and chosen (Kornish and Hutchison-Krupat, 2017). LLMs have exhibited unparalleled mastery of natural language processing (NLP). Since, these have become increasingly powerful, researchers have begun to investigate their reasoning ability in problem-solving tasks (Yao et al., 2022; Brahman et al., 2023). The concept of an idea is essentially a new combination of old elements. LLMs have access to a broad spectrum of knowledge, due to their extensive training on vast amounts of text data. However, understanding how information extracted from a research paper can give rise to new ideas, which have not yet been explored much. This leads us to ponder:"}, {"title": "2 Related Work", "content": "Recently, LLMs have shown emergent abilities to perform tasks they were not explicitly trained for (Wei et al., 2022; Bubeck et al., 2023). This in- cludes common sense question answering, code generation, and cross-domain problem solving, en- riching their utility across unforeseen domains (Chen et al., 2021; Sarsa et al., 2022). Their capa- bility extends to advanced scientific domains such as computer science, physics, medicine, and math- ematics (Romera-Paredes et al., 2023; Huang et al., 2023). Technology Semantic Network (TechNet) was proposed to stimulate idea generation in en- gineering design (Sarica et al., 2021). There have been a few works in the discovery of new proteins to accelerate scientific discovery. The prior work reported in (Spangler et al., 2014) involves utilizing published studies to find new protein kinases that phosphorylate the tumor suppressor protein p53.\nA hypothesis is a hunch, assumption, suspicion, assertion or an idea about a phenomenon, relation- ship or situation, the reality or truth of which you do not know (Kumar, 1996). There have been some works on hypothesis generation. Initial stud- ies on automated hypothesis generation begin by constructing a corpus of distinct concepts. Sub- sequently, they explore the relationships between these concepts using machine learning techniques, such as analyzing the similarities among vectors representing different words (or concepts) (Tshi- toyan et al., 2019), or applying link prediction meth- ods over a graph (where concepts are nodes) (Nad- karni et al., 2021). Recently (Qi et al., 2023) used LLMs and extensive pre-existing knowledge of var- ious scientific fields for hypothesis generation. Pa- perRobot (Wang et al., 2019) predicts related enti- ties for an input title and writes key elements of a new paper, including the abstract, conclusion, and future work, and predicts a new title.\nXu et al. (2023) developed a framework that leverages the concept co-occurrence graphs and a masked language model to explore and verbal- ize academic ideas. Their method involves con- structing evolving concept graphs across various disciplines and utilizing temporal link prediction to identify potential interdisciplinary connections. The framework also incorporates pre-trained lan- guage models to articulate these connections in a coherent academic context."}, {"title": "3 Dataset", "content": "Our dataset creation involves three steps: (1) Dataset Collection, (2) FRI Identification and re- moval, and (3) FRI generation."}, {"title": "3.1 Dataset Collection", "content": "We construct a corpus D from S2ORC collected 100 papers from the domains of Computer Science, Economics, Physics, Chemistry, Medical from (Lo et al., 2020). To ensure the quality and relevance of the data and to utilize the future research ideas mentioned in a paper, the selected papers must meet the following requirements: (1) the paper must contain the full content, and (2) the paper must include a section on future work."}, {"title": "3.2 FRI Identification and Removal", "content": "We first identify and remove any potential research ideas mentioned in the paper. By doing this, we ensure that the LLMs have no prior access to these ideas, which could otherwise affect the objectivity of the analysis."}, {"title": "3.2.1 Annotation Guidelines", "content": "Inspired by Hao et al. (2020), we define a future re- search idea as a discussion that the authors believe they will conduct in the future or believe needs to be investigated in future research. We discuss more details about the annotation guidelines in Appendix A."}, {"title": "3.2.2 Annotator Training", "content": "Given the complexity of the papers and their fre- quent use of technical terminology, we hired two doctoral students, each boasting over four years of experience in scientific research publishing. To facilitate their training, an expert with more than ten years of experience in scientific publishing an- notated 20 random papers from each domain, ad- hering to our guidelines. After this initial round of annotation, we reviewed and corrected any misin- terpretations with the annotators, further refining their training and enhancing the clarity of our an- notation guidelines. To assess the effectiveness of the initial training, we compiled another 20 pa- pers from each domain. From the second round onwards, the annotators demonstrated improved proficiency, accurately identifying at least 95% of the future research ideas on average.\nWe discuss more details about the annotation process and annotator's pay in Appendix A."}, {"title": "3.2.3 Future Work Removal", "content": "We observed two types of future research ideas (FRIs) 2 (Direct FRI and Mixed FRI). We discuss them in details in Appendix \u041d.\nAP-FRI Corpus: We removed the sentence from the paper's input text if it pertains to Direct FRI. However, in the case of Mixed FRI, we did not entirely remove the sentences; instead, we elimi- nated only parts of sentences or markers indicating future research ideas. We added the removed fu- ture ideas to a corpus, which we refer to as the AP-FRI (Author Perspective Future Research Idea Corpus). This corpus contains the future research ideas proposed by the authors of the paper. Also, before adding to the AP-FRI corpus, we merged"}, {"title": "3.3 FRI Generation using LLM", "content": "We investigate various prompts and utilize the fol- lowing prompts to generate FRIs for papers.\nSystem: You are a research scientist.\nUser: Imagine you are a research scientist. After reading the following paper, brain- storm to generate potential future research ideas:\n[paper text]\nPotential future research ideas from the pa- per in bullet points are:\nHere, '[paper text]' contains the full content of the paper after removal of future work sections."}, {"title": "3.4 Data Statistics", "content": "Figure 3 provides a domain-wise distribution of the average word count in academic papers, exclud- ing discussions on future work (FWK). It can be observed that the length of papers across all fields falls within a range of 7,000 to 8,000 words. Addi- tionally, we calculated the average word count of extracted future work within each domain, provid- ing comparative insights into how different fields prioritize discussions of future research directions. Figure 4 compares the average word count of future work text across six distinct scholarly domains. We observed that the literature in Computer Science notably prioritizes extensive discourse on future research, with an average word count significantly higher than that of other disciplines. In contrast, the literature in Chemistry demonstrates a more concise approach to discussions of future research, as evidenced by its lower average word count."}, {"title": "4 Experiments", "content": "4.1 Challenges\nTo accurately assess the novelty, relevance, and ap- plicability of ideas generated by LLMs, evaluators must possess a high level of expertise in the specific domain and a deep understanding of the research topic to fully grasp the context. Additionally, they need knowledge of related literature to evaluate the ideas' future potential and the broader implications of their implementation."}, {"title": "4.2 Idea Alignment Score (IAScore)", "content": "With the above challenges, the evaluation of ideas generated by LLMs is a challenging process that demands a high number of domain-specific experts. We, therefore, proposed an Idea Alignment Score (IAScore), which reflects how well the generated ideas align with those proposed by the author. The underlying idea for this score is that authors of ac- cepted papers can be regarded as experts in their respective subjects. The reason being that they possess thorough background knowledge and have conducted deep analyses of the research topic be- fore getting the paper accepted. Consequently, they are well-acquainted with the pertinent challenges which also may have been discussed by expert re- viewers. Therefore, we propose that future ideas mentioned by the authors in the paper could be utilized as good quality of potential FRIs.\nThe IAScore quantifies the alignment of newly generated ideas with author's perspectives within a specific domain, and is computed via a two-step process, detailed in Equations 1 and 2.\nInitially, we compute the average alignment score AvgScore; for each paper's ideas. The IdeaMatcher model measures the alignment be-"}, {"title": "4.2.1 IdeaMatcher", "content": "To select an effective IdeaMatcher, we create a small annotated corpus. Our dataset was divided using the standard 30:70 ratio for validation and test sets, respectively. Since our study involves comparing two ideas using a pre-trained model, we did not require a separate training set. We first manually searched for matching pairs of ideas from generated ideas and AP-FRI of the paper. After obtaining 61 matching pairs, we searched for non- matching pairs of ideas, which is straightforward as only one generated idea will match or would not match with another one from AP-FRI while others would not match, so we picked an equal number of non-matching pairs. Then, we experimented with the idea-matching task by considering it similar to the Natural Language Inference (NLI) task. In particular, we considered the generated FRIs to be hypotheses and their corresponding AP-FRIs of the paper to be premises. If the idea matches, the hypothesis should be entailed by the premise. In particular, we used a pre-trained ROBERTa MNLI model (Liu et al., 2019) for this task. We found that this technique produces many false negative cases, resulting in an accuracy of 65.5%.\nWe also evaluated the idea-matching capability of BERTScore (Zhang* et al., 2020), as it utilizes BERT embeddings for comparison. We discuss the details in Appendix F. We found that BERTScore performed better than the entailment technique, re- sulting in an accuracy of 75.4%. We also tried GPT by prompting it with various questions and found that it resulted in 91.8% accuracy when prompted with a specific question prompt below:-\nPrompt: Your task is to examine whether a particular idea is incorporated within a set of ideas and to what degree.\nCollection of ideas: {API-FRIs}\nSingle idea: {A generated Idea}\nIs the single idea contained within the col- lection of ideas?\nIf yes, quantify its degree of presence or relevance of the single idea in the collection of ideas on a scale from 0 to 1.\nWe found that GPT performs better than the ex- isting NLI (Natural Language Inference) and simi- larity measure such as BERTScore. Therefore, we chose GPT for this task\u00b3."}, {"title": "4.3 Idea Distinctness Index", "content": "Distinct-N (Li et al., 2015), is a metric that mea- sures the diversity of a sentence. It focuses on the number of distinct n-grams of a sentence, and thus penalizes sentences with a lot of repeated words. However, comparing two ideas need semantic com- parisons rather than just syntactic differences. So, we introduce a method to semantically evaluate the distinctness of the generated ideas. This method in particular leverages semantic embedding to cap- ture the essence of each idea and computes their distinctness based on semantic similarity measures.\nGiven a set of generated ideas $I = {id_1, id_2,..., id_n}$, representing individual ideas, we first encode each idea into a high-dimensional"}, {"title": "5 Results and Discussion", "content": "5.1 Alignment Results\nFigure 5 provides a comparative overview of the IAScore for four language models Claude-2, Gemini-1.0, GPT-3, and GPT-4 across five aca- demic domains: Chemistry, Computer Science, Economics, Medical, and Physics.\nIn the Chemistry and Economics domains, Claude has the highest IAScore, indicating strong alignment with the authors' future research ideas. Claude and GPT-4 have almost similar values for the Computer, Medical, and Physics domains (with GPT-4 slightly higher). GPT-3 and Gemini have lower scores than both GPT-4 and Claude in ev- ery domain. GPT-3 has almost the same score as Gemini in the Chemistry and Economics do- mains. However, it scores higher than Gemini in the Computer, Medical, and Physics domains. The results underscore the advancements in language model capabilities, with each model showcasing domain-specific strengths in idea generation. This alignment of LLMs shows that LLMs are able to generate relevant and novel ideas to some extent. We also studied the effect of length of future work on IAScore (See Appendix D). We also conducted a human analysis to understand the quality of re- search ideas generated when the IAScore is low (see Appendix G)."}, {"title": "5.2 Distinctness Results", "content": "We show the comparative evaluation of idea dis- tinctness scores in Figure 6. The line graph depicts the variation of distinctness between the generated ideas and the human-written ideas (AP-FRIs). GPT- 3 shows the least distinctness among the generated ideas, except in the Computer domain, where it is slightly more distinct than Gemini. As shown in the graph, the distinctness of Gemini is also quite low; however, it is slightly better than GPT-3, except in the Computer domain.\nThe generated ideas of GPT-4 are more distinct than those of Gemini and GPT-3 (except for eco- nomics, whereas the distinctness of GPT-4 is the same as Gemini). However, it is lower than both Claude and Human. The Idea Distinctness Index of the generated ideas from Claude are almost"}, {"title": "5.3 Human Evaluation Results", "content": "We conducted a human evaluation on 460 generated ideas for 46 papers in the computer science domain. To validate the quality of human annotation, we measure the inter-annotator agreement ratio where 20% of the generated ideas are evaluated by two different authors of the same paper. We measured Cohen's kappa coefficient (Cohen, 1960), which was 0.83, thereby confirming the high quality of the annotations of generated research ideas.\nNovelty: Figure 7 displays the results of the human evaluation. We observed that Claude gener- ates 14.78% of non-novel and 16.52% generic FRIs, 41.73% moderately novel, 20.86% very novel, and 16.52% extremely novel FRIs. GPT generates 7.83% not-novel, 13.91% generic, 42.61% mod- erately novel, 28.70% very novel, and 6.96% ex- tremely novel ideas. Claude generates more non- novel and generic ideas than GPT-4, while GPT- 4 produces more very novel ideas and nearly the same number of excellent ideas. This demonstrates that although LLMs also generate generic or al- ready explored ideas, they are capable of producing novel ideas that have either not been explored or have been minimally explored.\nRelevance and Feasibility: After human evalu- ation, we found that that 76.67% of the ideas gener- ated by Claude and 93.34% by GPT-4 are relevant. Furthermore, 83.34% of Claude's generated ideas and 96.64% of GPT-4's ideas were judged to be practically feasible and factually correct. These re- sults highlight that Claude and GPT-4 can generate relevant and feasible research ideas. However, the reason Claude generates more impractical and irrel- evant research ideas may be that Claude attempts to generate more distinct research ideas than GPT-4, as we evaluated and discussed in Section 5.2."}, {"title": "5.4 Open-ended generation:", "content": "We tested whether LLMs could retain open-ended generation capabilities by providing only a title and abstract as input. Our findings showed that, overall, LLMs can still generate open-ended content due to their past knowledge. However, they may not pro- duce many high-quality ideas, as they lack access to recent publications and methodological insights relevant to the current paper. We discuss this in detail in Appendix C."}, {"title": "5.5 Adding additional background knowledge", "content": "We designed our framework based on the Retrieval- Augmented Generation (RAG) model (Lewis et al., 2020) to integrate background knowledge into LLMs, as illustrated in Figure 9. We collected the titles and abstracts of around 1.9 lakh computer science research papers. Using BERT embeddings, we created vector representations of these titles and stored them in a vector database. From there, we retrieved the 20 papers most similar to our target paper's title. We extracted contributions from these papers' abstracts to gather relevant data and then generated ideas by prompting GPT-4 with the tar- get paper and the retrieved background knowledge. We found that adding background knowledge re- duced the generation of generic or non-novel ideas and improved relevance and factual accuracy. How- ever, further research is needed to boost the novelty of generated ideas. We discuss this in detail in Appendix E."}, {"title": "6 Conclusion and Future Work", "content": "In conclusion, we present the first attempt to evaluate the potential of LLMs in generating fu- ture research ideas across five domains: Com- puter Science, Economics, Chemistry, Physics, and Medicine. Our results and analysis show that LLMs possess domain-specific strengths in idea genera- tion. Furthermore, the results from the Idea Dis- tinctness Index indicate that LLMs, such as Claude and GPT-4, generate distinct research ideas than Gemini and GPT 3.5. GPT-4 and Claude aligns bet- ter with authors written future research ideas than Gemini and GPT-4. The alignment of LLMs with the authors of generated ideas, and our human eval- uations on relevance, novelty, and feasibility, reveal that although LLMs often produce non-novel and generic ideas, they have the potential to generate relevant and novel and diverse ideas to a significant extent. We hope that the findings and experiments of this work will unlock the potential of LLMs in idea generation and will foster new advancements in automated scientific innovation.\nIn future work, we plan to investigate more effec- tive way of integrating knowledge from multiple papers to enhance the novelty of ideas generated and prevent the generation of generic and existing ideas."}, {"title": "7 Limitations", "content": "7.1 Limitations of Data Collection\nWe extracted papers using the Semantic Scholar Academic Graph API from January 2023 to Febru- ary 2024. The number of papers available is limited by the scope of our data extraction from the Seman- tic Scholar Academic Graph. We excluded papers that are not in English, as well as those whose ab- stracts could not be correctly parsed from the PDFs. Not all of these papers include sections on future work; therefore, we annotated only those that con- tained sections outlining future research directions. So due to such limitations, we collected 100 papers from each domain for analysis."}, {"title": "7.2 Memorization", "content": "(Carlini et al., 2022) highlight that LLMs are prone to memorizing portions of their training data, a sig- nificant concern in the evaluation of contemporary LLMs. Despite this, the data used for pre-training and post-training includes \"a small amount\" of more recent data. Therefore, we gathered recent papers from 2023 and 2024. By focusing our eval- uation on papers published in these years, the like- lihood of test papers appearing in the pre-training corpora for the models is substantially reduced. In addition, we conducted a manual review of these papers to assess memorization. This involved ask- ing various questions related to the papers, such as their titles, publishing venues, author names, etc., to see if the models could supply the missing infor- mation. Our findings showed no evidence of such memorization occurring. A similar approach is also followed by (Wang et al., 2023) (discussed in Sec- tion 6.4) and even they did not find any evidence of this occurring."}, {"title": "Ethics Statement", "content": "We have utilized the open source dataset for our work. Our aim for this work is to assess the poten- tial of language models in generating ideas. Our Institutional Review Board (IRB) evaluated and ap- proved this study. We do not encourage the use of LLMs to generate AI generated research papers (by generating new ideas) or misuse it for harmful idea generation. LLMs can process and synthesize vast amount of literature faster than humans, potentially identifying new patterns or gaps in research that might not be obvious, thus accelerating scientific discovery. However, since LLMs can generate con- tent that may be similar to existing materials, this raises concerns about intellectual property rights and the originality of ideas. LLMs utilized for generating ideas might be misapplied to produce harmful materials such as plans for schemes for designs for destructive devices, explosive devices, ideas for spamming. Notably, it is a common chal- lenge among existing LLMs with strong creative and reasoning abilities. So, we emphasize the re- sponsible use of LLMs for idea generation and the need to broadly improve the safety of LLMs."}, {"title": "Frequently Asked Questions (FAQs)", "content": "\u2022 How does our work differ from Scimon?\n\u21d2 Our paper is fundamentally different from the Scimon paper. We would like to highlight a few major differences. While the focus of Scimon is on developing a framework that generates novel scientific ideas, we clarify that our focus is not on generating ideas but on evaluating the capability of LLMs to generate future research ideas/works. We proposed the novel Idea Alignment Score (IAScore) and the Idea Distinctness Index. Unlike Scimon, we approached authors who are knowledgeable about their paper topics and the broader literature (see Section 4.4). Scimon used only GPT for comparison, while we used GPT-4, GPT-3.5, Claude, and Gemini models. Unlike Scimon, we provide the full paper as input. Scimon used the proposed idea written in the abstract as the target, while we used the future work section written in the full paper as our target. Additionally, they utilized a classifier for this purpose, whereas we employed human evaluators, resulting in fewer chances of error and better evaluation results. Our findings are completely different from those of Scimon. We created a novel annotated dataset for these experiments. While Scimon only experimented with computer science papers from the ACL Anthology, we expanded our experiments to five different domains. Scimon generated ideas guided by seed terms to generate specific ideas. Nonetheless, our goal here is to assess the inherent ability of LLMs to generate future work independently. Introducing external aids or additional context would shift the focus from evaluating the LLM's standalone capabilities to assessing its performance under enhanced conditions. Such an approach would not align with our objective, which is to understand and measure the raw, unaided generative power of LLMs.\n\u2022 Does incorporating extra contextual information alongside individual papers prove counter- productive?\n\u21d2 A paper encompasses not only its contributions, findings, and methodology, but also includes the related work and introduction sections, which contain significant background information. It is likely that the major recent related papers pertinent to the current work have already been mentioned. Additionally, LLMs possess general knowledge about the many older papers and the paper itself contains some of the most important related papers. However, we also conducted an experiment to understand the effect of adding additional information (using the RAG framework). We discuss the results and details in Appendix E of the paper. Overall, we observed that incorporating additional background knowledge can somewhat help prevent the generation of non-novel or generic ideas. However, further research is needed to enhance the ability of LLMs to generate more novel ideas."}, {"title": "A Dataset Annotation", "content": "A.1 Dataset Annotation Guidelines\nRecognizing future research idea in a paper in- volves analyzing the portion of text containing di- rections for future research. The following steps can be followed:\nStep 1: Begin by reading the Title and Abstract of the paper to gain an understanding of its subject matter. It is important to read these sections multi- ple times to grasp the paper's main points, such as its motivation, contributions, and other relevant as- pects. If necessary, refer to the paper itself or read related material to enhance your understanding.\nStep 2: Identify Key Sections for Analysis Focus primarily on the Discussion and Conclusion sec- tions of the paper, as these areas often contain ex- plicit mentions of future research directions. Scan the Methodology section as well, as sometimes sug- gestions for improving future studies or addressing current study limitations are mentioned here.\nStep 3: Distinguish Future Research Ideas from General Statements: Differentiate explicit future re- search suggestions from general discussion. Future research directions usually involve specific recom- mendations, plans, or identified gaps that require further exploration. These are often phrased using terms like \"future studies should,\" \"further research is needed,\" or \"additional work will.\" Avoid con- fusing these with broader statements of potential relevance or applicability, which do not provide direct guidance on future work.\nWe offer multiple examples of papers with its future research ideas to assist and direct the anno- tators. We found a few text which looks like future work but is on contrary the motivation of the work. As an example, consider the following: \"The goal of this work was to direct attention to emerging and novel research involving \"magnetogel nanohybrid materials\" that might be relevant in future applica- tions for the treatment of wastewater, as well as in other fields.\nThe second example is: \"Our data could be use- ful for designing high-quality trials in the future to define the exact role of hemoadsorption in ARDS.\". Here, how novel research involving magnetogel nanohybrid material will help in future application is written.\nAlso another example is: \"The goal of this work was to direct attention to emerging and novel re- search involving magnetogel nanohybrid materials that might be relevant in future applications for the treatment of wastewater, as well as in other fields.\" This is the application in future, and not the future work.\nStep 4: Separate Future Research from Limi- tations: Carefully examine any limitations men- tioned in the paper to determine if they are explic- itly linked to future research. Only consider a limi- tation as future work if the authors clearly indicate a direct intention to address it in subsequent stud- ies. This helps avoid assuming that all limitations naturally lead to future research directions.\nThere is also very thin line between limitation and future work, where a limitation can or cannot be a future work. There were few cases where limitations were mentioned \"One limitation of this paper is the absence of a coordinated attention structure to capture cross-channel information.\". As limitations can or cannot be a future work, we only take those limitations which is explicitly men- tioned by the author to be a future work. Hence, we only considered the explicit mention of the future work by the author in their paper."}, {"title": "A.1.1 Annotator Training", "content": "Given the complexity of the papers and their fre- quent use of technical terminology, we hired two doctoral students, each boasting over four years of experience in scientific research publishing. To facilitate their training, an expert with more than ten years of experience in scientific publishing an- notated 20 random papers from each domain, ad- hering to our guidelines. After this initial round of annotation, we reviewed and corrected any misin- terpretations with the annotators, further refining their training and enhancing the clarity of our an- notation guidelines. To assess the effectiveness of the initial training, we compiled another 20 pa- pers from each domain. From the second round onwards, the annotators demonstrated improved proficiency, accurately identifying at least 95% of the future research ideas on average."}, {"title": "A.1.2 Annotation Process", "content": "We regularly monitored the annotated data, plac- ing emphasis on identifying and rectifying incon- sistencies and cases of confusion. We also im- plemented an iterative feedback system that con- tinuously aimed to refine and improve the anno- tation process. In cases of conflict or confusion, we removed those papers as we wanted only good quality dataset. Following the annotation phase, we obtained an average inter-annotator agreement"}, {"title": "A.1.3 Annotator's Pay", "content": "We compensated each annotator according to the standard PhD salaries in India, based on the hours they worked. The appointment and salaries adhere to our university's established practices. Payment was made per paper since the time required to read and extract future research ideas from each paper varies, depending on its complexity, technical ter- minology, and the annotator's familiarity with the subject. Thus, paying based on time spent could have potentially compromised the quality of the annotations. To maintain accuracy and prevent fa- tigue, we imposed a daily limit of six hours for annotators."}, {"title": "B Human Annotation", "content": "We prepared a Google Form for each paper and provided the links to the annotators. We also spec- ified instructions for them at the beginning of the form. We have added an example of the form for a paper in Figure 10, Figure 11, and Figure 12.\nHere is the little modified from for human evalu- ation that generates only top 5 research ideas:-\nSystem: You are a research scientist.\nUser: Imagine you are a research scientist. After reading the following paper, brain- storm to generate potential top 5 future re- search ideas:\n[paper text]\nPotential top 5 future research ideas from the paper in bullet points are:\nHere, '[paper text]' contains the full content of the paper after removal of future work sections."}, {"title": "C Effect of giving only Title and Abstract as Input", "content": "We found a few cases where we provided only an title and abstract as input to see if LLMs can still re- tain open-ended generation capabilities. We discov- ered few cases where GPT-4 still generated novel ideas, such as for a paper (Kumar et al., 2023b) it generated: \"Incorporate explainable AI methods to provide transparency into how the AI model makes its predictions, thereby making the outcomes more interpretable and acceptable to human editors.\". This kind of analysis has not been done yet and could be helpful. After providing full paper con- tent to the model we found that same idea was again generated.\nThere were also cases where GPT-4 generated a novel idea of solving the problem using transform- ers for a task (The task was mostly solved using techniques like RNN), which had not been done before. However, after providing the full paper con- tent, the model understood that this transformer has already been implemented for this task, so further suggested to add more contextual information to it to boost the result (limited information was given as input to the paper). Overall, we found that LLMs can still retain open-ended generation because it has past knowledge. But it may not generate many good ideas since it doesn't have access to recently published papers or other methodological findings related to the current paper."}, {"title": "D Effect of Length of Idea on IAScore", "content": "In our analysis, we explore the relationship be- tween the length of ideas and their corresponding Impact Assessment Score (IAScore), specifically focusing on computer science papers and outputs generated by GPT-4. This relationship is visually represented in the bar chart found in Appendix Fig- ure 8. The data reveal that shorter ideas, typically under 20 words, tend to receive lower IAScores. This could be attributed to their lack of detailed information, which might be essential for a compre- hensive understanding and assessment. Conversely, we observe that ideas spanning 40-60 words also tend to score lower. This may result from their ver- bosity; excessive information can dilute the core message, making it challenging to discern the main points. Interestingly, ideas with a moderate length, ranging from 20 to 40 words, achieve the high- est IAScores. This length seems optimal as it al- lows for sufficient detail without overwhelming the reader, striking a balance that facilitates clearer understanding."}, {"title": "E Effect of Adding Additional Background Knowledge", "content": "We designed our framework based on the Retrieval- Augmented Generation (RAG) model (Lewis et al., 2020) to integrate background knowledge into LLMs, as illustrated in Figure 9."}, {"title": "F BERTScore Implementation Details", "content": "The motivation to use BERT embeddings is that the generated and the original ideas often do not use the same words, so we need to understand the contextual meanings of the ideas in order to compare them. We used the default setting of the BERTScore metric, which employs a 24-layer ROBERTa-large model and utilizes the 17th layer for embedding. We determined the threshold us- ing the validation set. If the similarity exceeds that threshold"}]}