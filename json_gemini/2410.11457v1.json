{"title": "LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under Low-Resource Scenarios", "authors": ["Wuzhenghong Wen", "Yongpan Zhang", "Su Pan", "Yuwei Sun", "Pengwei Lu", "Cheng Ding"], "abstract": "Large language models revolutionize Text2SQL through supervised fine-tuning, yet a crucial limitation is overlooked: the complexity of databases leads to an increased context length, consequently resulting in higher GPU memory demands for model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL comprises two supervised fine-tuning models: the schema_link model and the SQL_generation model, with the schema_link model serving as the focal point for streamlining the overall process. During the fine-tuning of the schema_link model, LR-SQL breaks down the complete database into flexible combinations of tables with adjustable quantities, enabling the model to learn the relationships within the entire database from these dispersed slices. Furthermore, to enhance the model's ability to perceive the relationships among various discrete slices during inference, LR-SQL trains the model's Chain-of-Thought capability for this task. Experimental results demonstrate that LR-SQL can reduce the total GPU memory usage by 40% compared to existing fine-tuning methods, while only losing 2% of table prediction accuracy in schema_link task. For the overall Text2SQL task, the Execution Accuracy decrease by 0.6%.Our project is now available on https://github.com/hongWin/LR-SQL", "sections": [{"title": "I. INTRODUCTION", "content": "The success of commercial large models in the market, notably GPT-4 [1] and PaLM2 [2], has markedly ac- celerated the growth of the entire Large Language Model industry. This achievement has further catalyzed the advent of a multitude of open-source large models, including GLM- 4 [3], Qwen2 [4], and DeepSeek [5]. Due to industry re- quirements for privacy and low operational costs, utilizing instruction-tuned fine-tuning technology to transform open- source large language models into experts in their respective fields represents a new technological revolution in the field of AI applications. However, the issue of computational resources constitutes being the primary threshold for large language models to become domain-specific experts. Consequently, ex- isting fine-tuning methods generally adopt Parameter-Efficient Fine-Tuning (PEFT) techniques. These techniques introduce partially trainable modules into each layer of the transformer structure of the large language model and only train these parameters during fine-tuning. This approach significantly reduces the number of parameters involved in training and decreases GPU memory usage. By utilizing PEFT technol- ogy, open-source language models can be trained under rel- atively modest resource constraints to serve as secure and efficient personal database assistants, with the core technology underpinning these assistants being Text2SQL technology. This technology is effective in addressing the demands for repetitive and relatively complex structured query language (SQL) within practical production environments. [6] indicates that medium-sized models, after undergoing supervised fine- tuning, exhibit superior performance compared to existing commercial large models in Text2SQL tasks, highlighting their strong application prospects.\nPEFT reduces the memory footprint for supervised fine- tuning of Text2SQL at the parameter level, but the length of the context input to the model is also one of the important factors affecting the memory usage during fine-tuning. it is important to note that GPU memory usage during fine-tuning is influenced by factors such as training batchsize which refers to the number of samples used in one training iteration of the model and the total number of tokens utilized for training. In the context of Text2SQL tasks, the structure and quantity of tables within the database play a pivotal role in determining the overall number of tokens required for the task. As the number of tables and columns increases, the model needs higher number of tokens to encode these features, ultimately resulting in escalated GPU memory requirements during the supervised fine-tuning process.In practical applica- tion scenarios, large databases frequently encompass hundreds of tables. When employing the methods outlined in [7], [8], [9] to directly establish relationships among query requirements, query predictions, and the comprehensive database informa- tion, a substantial number of tokens are necessitated for encoding per instance during the supervised fine-tuning phase. Consequently, given the constraints of limited GPU memory resources, it becomes infeasible to carry out supervised fine- tuning on the mode.\nThe currently supervised fine-tuning method improved based on PEFT and designed to further reduce memory usage during fine-tuning, cannot be adapted and applied to large language models of all architectures.Current advancements in PEFT, such as QLORA [10], are capable of reducing the mem- ory requirements for fine-tuning by decreasing the precision of model parameters. Long LoRA [11], on the other hand, can both extend the context window of LLAMA2 [12] and further decrease the memory footprint of supervised fine-tuning at the token level. However, both of these methods require the design of specific implementation codes tailored to large language models with different architectures, necessitating a deep un- derstanding of the model's underlying mechanics. Therefore, in practical applications, these techniques cannot be rapidly generalized to large models with diverse architectures.To our"}, {"title": "II. REFERENCE WORK", "content": "Text2SQL can be classified as a pipeline method that consists of two primary components: schema linking and SQL generation. The schema linking component is dedicated to establishing relationships between questions, databases, and target tables or columns, whereas the SQL generation component is responsible for generating SQL queries that are concerning to the given question, based on the provided question and relevant tables. Existing research on Text2SQL can be classified into three main categories:\nThis method designs an appropriate encoder to capture the relationship between questions and SQL syntactic structures, tailored to the grammatical characteristics of SQL statements,"}, {"title": "III. METHOD OVERVIEW", "content": "The goal of Text2SQL is to establish a critical path from query questions to structured query language statements, and can be defined as:\n$F (Q, S) \\rightarrow A$                                                                                                 (1)\nwhere is the set of query questions, S is the description of the database, and A is the SQL query statement predicted by the model. Furthermore $S\\in {T1, T2,...,Tn,...,TN}$"}, {"title": "IV. SUPERVISED FINE-TUNING METHODS FOR SCHEMA_LINK TASK UNDER LOW-RESOURCE CONDITIONS", "content": "Both the schema_link task and the SQL Generation task incorporate all table descriptions S. In existing supervised fine- tuning methodologies, several factors influence GPU memory usage during the fine-tuning process, including the model's parameter count, the batchsize utilized for training, the extent of model quantization, and the total number of tokens in each training data instance. Notably, in Text2SQL tasks, the comprehensive table description S of the database serves as the primary determinant of the total number of tokens in each training data instance.\nIn current supervised fine-tuning practices, S is positioned after #instruction#in Figure 1, following which the entire supervised fine-tuning template is encoded to construct the"}, {"title": "V. EXPERIMENTS", "content": "Introduced in 2018, Spider [14] is a premier Text2SQL dataset, featuring 164 databases and 9693 natural language It classifies SQL queries into four difficulty levels (easy, medium, hard, extra hard) based on components and condi- tions, and innovatively introduces Exact Matching and Execu- tion Accuracy as evaluation metrics for Text2SQL tasks."}, {"title": "VI. CONCLUSION", "content": "We propose LR-SQL, which redesigns the objectives of supervised fine-tuning for the Schema_link task by breaking down the database into slices for learning knowledge under limited GPU memory. The model can effectively learns the reasoning patterns of Chain-of-Thought and enhances the model's ability to perceive the entire database from partial slices during the inference phase. Ultimately, our method maintains performance close to the baseline while effec- tively reducing GPU memory consumption during fine-tuning. Furthermore, our approach can collaborate with QLORA to further decrease GPU memory usage during supervised fine-tuning.\nFuture work In our research, to effectively carry out the supervised fine-tuning of Text2SQL tasks under GPU memory limitations, we decompose the extensive database tables into several slices, facilitating batch-based prediction. While this strategy adeptly allows the model to retain a comprehen- sive understanding of the associations between queries and database tables, it does result in an increased number of inference steps during the model's operation. Nevertheless, our experimental findings demonstrate that the resulting inference latency stays within tolerable limits when juxtaposed with contemporary approaches. Hence, in our next step, we plan to investigate methods to reduce the overall time required for the LR-SQL inference process, while maintaining its robust performance."}]}