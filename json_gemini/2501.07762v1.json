{"title": "PSReg: Prior-guided Sparse Mixture of Experts for Point Cloud Registration", "authors": ["Xiaoshui Huang", "Zhou Huang", "Yifan Zuo", "Yongshun Gong", "Chengdong Zhang", "Deyang Liu", "Yuming Fang"], "abstract": "The discriminative feature is crucial for point cloud registration. Recent methods improve the feature discriminative by distinguishing between non-overlapping and overlapping region points. However, they still face challenges in distinguishing the ambiguous structures in the overlapping regions. Therefore, the ambiguous features they extracted resulted in a significant number of outlier matches from overlapping regions. To solve this problem, we propose a prior-guided SMoE-based registration method to improve the feature distinctiveness by dispatching the potential correspondences to the same experts. Specifically, we propose a prior-guided SMoE module by fusing prior overlap and potential correspondence embeddings for routing, assigning tokens to the most suitable experts for processing. In addition, we propose a registration framework by a specific combination of Transformer layer and prior-guided SMoE module. The proposed method not only pays attention to the importance of locating the overlapping areas of point clouds, but also commits to finding more accurate correspondences in overlapping areas. Our extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art registration recall (95.7%/79.3%) on the 3DMatch/3DLoMatch benchmark. Moreover, we also test the performance on ModelNet40 and demonstrate excellent performance.", "sections": [{"title": "Introduction", "content": "Point cloud registration is fundamental in computer vision and widely applies in domains such as 3D reconstruction (Gro\u00df, O\u0161ep, and Leibe 2019; Izadi et al. 2011), autonomous driving (Nagy and Benedek 2018), and augmented reality (Huang et al. 2016). The objective is to estimate the optimal rigid transformation between two point clouds, thereby establishing their correspondences within the same coordinate system. Currently, learning-based methods are leading the task of point cloud registration. However, successfully registering them remains challenging when the point clouds only partially overlap.\nTo address the aforementioned issues, we believe that applying sparse mixture of experts (SMoE) in Transformer is a feasible solution. The routing network of SMoE can cluster input tokens into different groups and then distribute these groups to different experts for processing. If the routing network can distinguish between overlapping and non-overlapping regions of tokens, and further capture the correspondence between tokens of two point clouds, then the multi-expert networks through SMoE can extract more discriminative features. To the best of our knowledge, we are pioneering the exploration of SMoE for addressing the point cloud registration task.\nHowever, SMoE's simple application does not fulfill our expectations. Due to insufficient interaction between the features of the source and target point clouds, and more importantly, the lack of relevant guidance, it's challenging for SMoE's routing network to differentiate tokens from overlapping and non-overlapping regions (Fig. 1 (a)). Inspired by PEAL (Yu et al. 2023), we propose Prior-guided routing Sparse Mixture of Experts (PSMOE), a novel module to address the aforementioned challenge. Specifically, we first calculate superpoint correspondences using transformations estimated by a SOTA pretrained model. Then, we design a Prior superpoint Correspondences Encoding (PCE) module to obtain prior embeddings. PCE module encodes matched superpoints with discrete ordered numbers, while non-matched superpoints are assigned the same number, all the numbers assigned to superpoints are encoded using an embedding layer. Finally, the routing network groups the original tokens based on prior embeddings and sends them to selected experts to extract distinctive features. Unlike PEAL, our PCE module not only encodes prior overlapping information but also further captures the potential correspondence of matching points. These prior correspondences guide the distribution of potential matching tokens to be processed by the same expert as much as possible, which facilitates feature alignment. Therefore, our method effectively locates overlapping areas and enhances matching accuracy within these regions (Fig. 1 (c)).\nWe design a coarse-to-fine point cloud registration framework based on the PSMOE module, named Prior-guided SMoE-based Registration (PSReg), to estimate the final transformation matrix. Additionally, we conduct extensive experiments on indoor benchmarks and synthetic datasets, demonstrating the superiority of our approach over previous methods. A comprehensive series of ablation studies validate the effectiveness of each module within our proposed methodology. Our contributions can be summarized as follows:\n\u2022 We analyze the reasons for the poor performance of directly applying vanilla SMoE in point cloud registration, and propose a prior-guided SMoE (PSMoE) module to further improve the feature distinctiveness within the overlapping regions. To the best of our knowledge, we are the pioneers in exploring the SMoE in the point cloud registration task.\n\u2022 We propose a registration method, PSReg, by combining both the Transformer and our PSMOE module.\n\u2022 We conducted extensive experiments to verify the effectiveness of our methodology and explored the feasibility of applying SMoE to the point cloud registration task."}, {"title": "Related Work", "content": "The overall idea of non-Transformer-based point cloud registration methods is typically using networks specifically designed for 3D data to extract features in a single stage, without further enhancing high-level features. These features are then utilized to establish correspondences and indirectly solve for the transformation matrix, or the transformation matrix is solved directly from the features. Based on the different types of input data, we can further broadly classify the existing methods into two categories. Firstly, methods based on structured data (Zeng et al. 2017; Elbaz, Avraham, and Fischer 2017; Choy, Park, and Koltun 2019; Ao et al. 2021), which typically involve projecting the point cloud into 2D images or constructing 3D volume, and directly applying 2D CNN or 3D CNN. Secondly, methods based on raw point clouds (Aoki et al. 2019; Du et al. 2019; Huang, Mei, and Zhang 2020; Bai et al. 2020; Xu et al. 2021; Huang et al. 2022b), which directly use the point cloud data as input. PointNet (Qi et al. 2017a) directly uses MLPs to extract point-wise features and addresses permutation invariance through max pooling. PointNet++ (Qi et al. 2017b) further introduces sampling and grouping layers to capture multi-scale contextual information. KPConv (Thomas et al. 2019) uses kernel points that carry convolution weights to simulate the kernel pixels in 2D convolution, thereby defining the convolution operation on raw point clouds. The above three backbone networks, which directly process raw point cloud data, are widely used in point cloud registration. However, the above methods face issues with information loss and challenges in capturing global contextual information.\nTransformer-based registration methods typically involve a two-stage feature extraction process. Initially, high-level features are extracted by the existing 3D backbone, followed by further feature enhancement through Transformer. DCP (Wang and Solomon 2019) is the pioneer in utilizing Transformer to enhance feature extraction in point cloud registration, whereby unaligned point clouds are fed into a feature embedding module, followed by context aggregation executed by a Transformer encoder. REGTR (Yew and Lee 2022) replaces explicit feature matching and outlier filtering, typically performed by RANSAC, with an end-to-end Transformer framework to directly discover point cloud correspondences. Taking into consideration that the vanilla Transformer does not explicitly encode the geometric structure of point clouds, GeoTransformer (Qin et al. 2022) is engineered with a geometric self-attention module that expressly captures the internal geometric structures of point clouds. PEAL (Yu et al. 2023) introduces an overlapping prior, utilizing one-way attention to relieving feature ambiguity. However, these methods are prone to failure when dealing with scenes containing a large number of ambiguous structures."}, {"title": "Sparse Mixture of Experts (SMOE)", "content": "SMOE is an efficient deep learning model architecture that utilizes a multi-expert system to handle complex tasks. In this structure, the network is composed of multiple experts (multiple FFNs) and a gating mechanism. The gating mechanism is responsible for deciding which experts to activate based on the input data, allowing the network to dynamically adjust its behavior for specific tasks, thereby enhancing processing speed and efficiency. The efficacy of SMoE has been extensively demonstrated across various tasks in NLP (Shazeer et al. 2017; Lepikhin et al. 2020; Zhou et al. 2022; Fedus, Zoph, and Shazeer 2022; Jiang et al. 2024) and CV (Gross, Ranzato, and Szlam 2017; Abbas and Andreopoulos 2020; Pavlitskaya et al. 2020; Riquelme et al. 2021).\nHowever, research on SMoE in 3D vision remains unexplored. In fact, the SMoE is naturally suited for the task of partially overlapping point cloud registration. Partial overlapping point cloud registration focuses more on the overlapping regions of the point clouds. The gating mechanism of SMOE can sparsely activate a subset of experts to specifically handle data from these overlapping regions, which can enhance the uniqueness of the features. The aforementioned sparse activation method is theoretically feasible, but it is difficult to implement in practical applications. Therefore, to achieve controllable sparse activation, we propose a novel prior-guided routing mechanism that leverages SMOE for more effective discriminative feature extraction."}, {"title": "Method", "content": "In this section, we will introduce the preliminaries of point cloud registration and sparse mixture of experts (SMOE).\nGiven a pair of partially overlapping point clouds, namely the source point cloud $P = \\{p_i \\in \\mathbb{R}^3|i = 1,..., N\\}$ and the target point cloud $Q = \\{q_j \\in \\mathbb{R}^3|j = 1, ..., M\\}$, where $N$ and $M$ are the number of points in point clouds $P$ and $Q$. The goal of point cloud registration is to predict the rotation matrix $R \\in SO(3)$ and translation vector $t \\in \\mathbb{R}^3$ to align $P$ and $Q$ under the same coordinate system. Our approach needs to be combined with Transformer, while Transformer-based registration methods typically employ an encoder (existing 3D feature extraction networks) for downsampling the raw point clouds and extracting relevant neighborhood features before applying the Transformer. We use $\\hat{P} = \\{p_i \\in \\mathbb{R}^3 i = 1,..., N'\\}$ and $\\hat{Q} = \\{\\hat{q}_j \\in \\mathbb{R}^3|j = 1, ..., M'\\}$ to represent downsampled points (superpoints), and the corresponding superpoint features are represented by $\\hat{F}_P \\in \\mathbb{R}^{N'\\times d}$ and $\\hat{F}_Q \\in \\mathbb{R}^{M' \\times d}$.\nThe vanilla SMoE (Riquelme et al. 2021) is a neural network layer comprising self-attention mechanisms and multiple feed-forward expert networks. Given an input point cloud token $x$, it is sent to the routing network $G(\\cdot)$, which delegates the token to several highly relevant experts for processing. Typically, the routing network is a fully connected layer, represented as follows:\n$G = top-k(softmax(W_gx))$, (1)\nWhere $W_g$ represents the trainable parameters in the router neural network and $top-k(\\cdot)$ select the $k$ experts with the largest gating values. In our approach, $k = 1$. The final output of the SMoE layer will be the summary of the weighted active expert features:\n$y = \\sum G_i\\cdot E_i(x)$, (2)\nwhere $E_i$ represents the neural network of expert $i$, and $G_i$ represents the corresponding gate value.\nThe multi-expert network of SMOE can focus on different features or patterns, which should help enhance the uniqueness of superpoint features in different regions. To analyze the ability of SMoE in the point cloud registration, we directly use vanilla SMoE on the selected baseline model (Yu et al. 2023). Specifically, we replace the FFN layer in the self-attention block of the baseline with the SMoE layer. This SMoE with the tokens choice routing policy, following the default configurations in (Fedus, Zoph, and Shazeer 2022). To intuitively observe the routing results of vanilla SMoE, we visualized the distribution of tokens in the last SMoE block. As illustrated in Fig. 2 (b), the tokens do not exhibit a clear clustering phenomenon, indicating that the vanilla SMoE does not handle the expert subsets for tokens in overlapping and non-overlapping regions separately.\nAnalysis: The primary reason for the failure of vanilla SMOE in locating overlapping regions is that the routing network is typically just a fully connected layer. Without explicit guiding signals, its expressive capability struggles to capture the overlapping information between point clouds.\nTo solve the above-mentioned problem and further alleviate matching ambiguities in overlapping areas, we propose Prior-guided Sparse MoE (PSMOE), as shown in Fig. 3, which offers explicit prior guidance. Fig. 2 (c) shows the token distribution in our method. Due to the introduction of prior information, the routing network can distinguish between overlapping and non-overlapping patches. Therefore, providing clear guidance signals can assist the routing network in better identifying which experts are more suitable for specific inputs. Next, we will sequentially introduce our approach from three aspects: prior superpoint correspondence prediction, prior superpoint correspondence encoding, and prior-guided routing.\nSuperpoint correspondences not only provide overlapping information but also directly associate matching superpoints in two point clouds. These explicit signals can effectively guide the routing network as priors to activate proper expert extract discriminative point-wise features. However, it is a chicken-and-egg problem between superpoint correspondences and discriminative point-wise feature extraction. To solve this problem, inspired by the approach of (Yu et al. 2023), we calculate the prior superpoint correspondences by using a pretrained SOTA registration model. Specifically, we employ GeoTransformer to predict the prior transformation matrix $T_p = \\{R_p, t_p\\}$, and use $T_p$ to compute the overlap ratio matrix $\\hat{O} \\in \\mathbb{R}^{N'\\times M'}$ between the superpoints (patches) of $\\hat{P}$ and $\\hat{Q}$. Then, we select entries in $\\hat{O}$ with an overlap ratio greater than $\\tau_o$ to serve as prior superpoint correspondences $C = (P', Q')$, and the corresponding prior overlap ratios are represented as $O$.\nTo aid the routing network in discerning the precise locations of tokens within overlapping regions and to effectively capture the token correspondences in these putative overlap areas, our aim is to partition all tokens into $L$ (the number of experts) clusters. These clusters can be broadly classified into two categories: the non-anchor cluster (a singular cluster where the majority of tokens reside in non-overlapping zones) and anchor clusters (comprising $L - 1$ clusters primarily housing tokens within overlapping regions). Ideally, each sub-cluster within the anchor clusters should encompass tokens with potential matches between the two sets of data points.\nConsequently, we have devised a PCE module to explicitly encode prior superpoint correspondences, as illustrated in Fig. 4. Specifically, we first encode the prior superpoint correspondences $C$ using a discrete and ordered sequence $Z_A$, with $\\bar{e} = |Z_A| = |C|$. We then employ the same numerical value $Z_{NA}$ to encode unmatched superpoints in $\\hat{P}$ and $\\hat{Q}$. Finally, we obtain a set of natural numbers, denoted as $\\mathbb{N} = \\{i \\in \\mathbb{N} | i = 0, ..., \\bar{e}\\}$: \n$\\mathbb{N} = Concat[Z_{NA}, Z_A]$, (3)\nConsidering the continuous nature of the sinusoidal embedding function (Vaswani et al. 2017), we utilize this function and MLP to form an embedding layer to compute the prior embeddings $F_\\mathbb{N} \\in \\mathbb{R}^{(\\bar{e}+1)\\times d}$ for $\\mathbb{N}$:\n$\\Gamma_{\\mathbb{N}, 2k} = sin(\\frac{i}{10000^{2k/d}})$\n$\\Gamma_{\\mathbb{N}, 2k+1} = cos(\\frac{i}{10000^{2k/d}})$\n$F_\\mathbb{N} = MLP(\\Gamma_{\\mathbb{N}})$,(4)\nBased on $C$, we select the corresponding superpoints from $\\hat{P}$ to form a set of anchor superpoints $A \\subset \\hat{P}$. However, superpoints in $A$ may correspond to multiple superpoints in $\\hat{Q}$ that meet the criteria (overlap ratio greater than $\\tau_o$), which are denoted as $A'$. Therefore, to ensure the uniqueness of the prior embedding corresponding to $A'$, we perform a weighted sum based on the overlap ratio. For each superpoint in $A$, we search for the entries corresponding to this superpoint from $\\hat{P}'$ in $C$ and the corresponding index is represented as $X_i$. Ultimately, the prior embeddings $\\bar{F}^P \\in \\mathbb{R}^{|\\hat{P}|\\times d}$ corresponding to the superpoints in $\\hat{P}$ is denoted as:\n$\\bar{F}_i^P =\\begin{cases} softmax(O(X_i)) \\cdot F_{\\mathbb{N}}(X_i), & X_i > 1, \\\\ F_{\\mathbb{N}}(X_i), & X_i = 1, \\\\ F_{\\mathbb{N}}^{Z_{NA}}, & otherwise.\\end{cases}$(5)\nWhere $F_{\\mathbb{N}}^{Z_A}$ and $F_{\\mathbb{N}}^{Z_{NA}}$ respectively represent the embeddings corresponding to $Z_A$ and $Z_{NA}$. The prior embeddings $\\bar{F}^Q \\in \\mathbb{R}^{|\\hat{Q}|\\times d}$ corresponding to $\\hat{Q}$ are obtained in the same manner.\nIn Fig. 3, it is illustrated that to ensure prior embeddings effectively guide the routing process and simplify model design, we adopt a straightforward yet efficient approach. This involves directly integrating them with prior information before distributing the input tokens through the routing network. More precisely, We sum the prior embeddings encoded by the PCE module with the original tokens, and giving them equal weight. Subsequently, we conduct routing using these composite tokens enriched with prior information, as detailed below:\n$G^{\\hat{P}} = top-k(softmax(W_g(\\hat{X}^P + \\bar{F}^P)))$. (6)\nWhere $\\hat{X}^P$ denote the tokens in $\\hat{P}$, and $\\bar{F}^P$ represent their corresponding prior embeddings. Under the guidance of prior information, distribute the original tokens to the appropriate experts for processing. We obtain the final output of the PSMOE block for tokens in $\\hat{P}$ according to Equation 2, i.e., $G^{\\hat{P}}_i \\cdot E_i(\\hat{x}^p)$. Tokens in $\\hat{Q}$ are processed using the same method.\nTo solve the point cloud registration task, we propose a Prior-guided SMoE-based Registration algorithm (PSReg). Our approach builds upon (Yu et al. 2023), and the pipeline is illustrated in Fig. 5. The framework adopts a coarse-to-fine paradigm. Initially, we utilize the KPConv-FPN backbone (Lin et al. 2017; Thomas et al. 2019) network to downsample the original point clouds, obtaining superpoints and their corresponding features. Discriminative superpoint features are crucial for high-quality point cloud registration, as the accuracy of correspondences in the fine stage depends on superpoint correspondences. Therefore, our method primarily focuses on feature extraction in the coarse stage.\nWe integrate our PSMOE with Transformer, leveraging PSMOE's multi-expert network to extract highly discriminative superpoint features for precise superpoint correspondence identification. Subsequently, the point matching module deduces exact point correspondences, followed by the application of local-to-global registration (LGR) (Qin et al. 2022) for the final transformation estimation.\n$\\mathcal{L} = \\mathcal{L}_c + \\mathcal{L}_f + \\mathcal{L}_g$.(7)\nThe coarse correspondence loss $\\mathcal{L}_c$ uses the overlap-aware circle loss (Qin et al. 2022), which focuses more on the positive samples with high overlap. The fine correspondence loss $\\mathcal{L}_f$ uses the negative log-likelihood loss. To encourage a balanced load across experts, we add a load balancing loss $\\mathcal{L}_g$, which is the same as in (Fedus, Zoph, and Shazeer 2022)."}, {"title": "Experiments", "content": "We validate the efficacy of our approach on real-world datasets such as 3DMatch/3DLoMatch and synthetic datasets like ModelNet/ModelLoNet. Furthermore, we have performed comprehensive ablation experiments.\nThe 3DMatch (Zeng et al. 2017) comprises data collected from 62 indoor scenes, with 46 scenes designated for training, 8 for validation, and 8 for testing. We utilize data processed by (Huang et al. 2021). The 3DMatch benchmark includes pairs of point clouds with an overlap rate of more than 30%, while the 3DLoMatch benchmark only includes pairs of scans with an overlap rate of 10%-30%.\nWe follow (Huang et al. 2021), using the following three metrics to evaluate the performance of registration: Registration Recall (RR), Feature Matching Recall (FMR) and Inlier Ratio (IR).\nAs for IR, our method significantly outperforms other baseline methods, demonstrating that under the strategy of prior-guided routing, the multi-expert network of PSMOE significantly enhances the distinguishability of features, thereby obtaining reliable correspondences. For FMR, our method does not perform as well as some baseline methods on 3DLoMatch. We speculate that this is because our method is more sensitive to the quality of the priors. It can significantly improve the IR in most point cloud pairs, but in some point cloud pairs where the priors cannot provide enough correct superpoint correspondences, our method would reduce their IR, resulting in a slightly poorer FMR.\nModelNet40 (Wu et al. 2015) consists of CAD models of 12,311 objects from 40 different categories. We follow the data settings in (Yew and Lee 2020; Huang et al. 2021), where the point clouds are sampled randomly from mesh faces of the CAD models, cropped and subsampled. A total of 5,112 samples are used for training, 1,202 samples for validation, and 1,266 samples for testing. Following to (Huang et al. 2021), we evaluated two partially overlapping settings: the average overlap rate for ModelNet is 73.5%, and for ModelLoNet, it is 53.6%. We trained only on ModelNet and directly generalized to ModelLoNet.\nFollowing (Yew and Lee 2020; Huang et al. 2021), we report the Relative Rotation Error (RRE) that evaluates the error between estimated and ground truth rotation matrices, and Relative Translation Error (RTE) that measures the error between estimated and ground truth translation vectors, as well as the Chamfer distance (CD) between the registered scans.\nWe compare the proposed PSReg with five SOTA baselines, including two end-to-end methods (Wang and Solomon 2019; Yew and Lee 2020), one correspondence-based method (Huang et al. 2021), and two coarse-to-fine registration approaches (Qin et al. 2022; Yu et al. 2023). For the coarse-to-fine methods, we report the metrics based on both the LGR estimator and the RANSAC estimator, with the number of RANSAC iterations set to 50K. As mentioned in (Huang et al. 2021), many end-to-end methods are specifically tuned for ModelNet40, and RPM-Net (Yew and Lee 2020) also utilizes surface normal information. Although our method's RRE and CD on ModelNet are slightly lower than RPM-Net (Yew and Lee 2020), it surpasses other baseline methods on the more challenging ModelLoNet in terms of the RRE metric, even without using RANSAC."}, {"title": "Qualitative Results", "content": "Qualitative comparison with other SOTA methods is shown in Fig. 6. It is worth noting that in the 3DLoMatch example, the point cloud pair only has partially overlapping walls. In such scenarios with a large number of similar structures, our method successfully achieves registration, whereas PEAL fails. From the quantitative and qualitative results, it can be seen that our proposed PSReg significantly enhances the distinguishability of features, which helps infer reliable superpoint correspondences in low-overlap point cloud registration, thereby achieving precise alignment."}, {"title": "Ablation Studies", "content": "In this section, following (Qin et al. 2022), we conduct extensive ablation experiments based on the LGR estimator. We report the FMR and IR of all dense point correspondences, as well as RR.\nTo evaluate the impact of the selection of prior superpoint correspondences, we tested various overlap thresholds, as shown in Tab. 3. The experimental results indicate that more prior superpoint correspondences are more conducive to registration. Hence, we ultimately set $\\tau_o = 0$.\nIn Tab. 4, we compare different configurations of PSReg, including: (a) the baseline (Yu et al. 2023) we chose does not integrate SMOE, (b) using the vanilla SMOE (Fedus, Zoph, and Shazeer 2022) on the baseline. It can be observed from Tab. 4 that the integration of SMoE offers negligible improvement in registration performance. In addition, we also compared different encoding methods in the PCE module. According to the prior superpoint correspondences, we classify superpoints into matched and unmatched categories, encoding them with 0 and 1 respectively, which is binary coding. The ordered coding is the method shown in Fig. 4. Tab. 4 presents the results of different encoding methods: (c) the PCE module uses binary coding, (d) the PCE module uses ordered coding. Both encoding methods effectively improve registration performance, particularly with the ordered coding method significantly enhancing the IR.\nFig. 7 visualizes the correspondence of superpoints, it can be clearly observed that the ordered coding method achieves more accurate correspondences in overlapping areas compared to the binary coding method and the baseline. This is because the ordered coding method can distribute potentially matching tokens to the same expert as much as possible, which helps to infer more accurate correspondences."}, {"title": "Conclusion", "content": "In this paper, we propose the prior-guided SMOE (PSMOE) to differentiate between the ambiguous structures in the overlapping regions. Based on the PSMOE, we propose the first SMoE-based registration framework. Through extensive experiments, our method demonstrates a significant enhancement in feature discriminability, leading to a notable improvement in inlier ratio. Our method explores a new direction by utilizing a multi-expert neural network to enhance feature discriminability for point cloud registration."}]}