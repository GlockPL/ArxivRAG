{"title": "Synthesis of Reward Machines\nfor Multi-Agent Equilibrium Design\n(Full Version)", "authors": ["Muhammad Najib", "Giuseppe Perelli"], "abstract": "Mechanism design is a well-established game-theoretic paradigm for\ndesigning games to achieve desired outcomes. This paper addresses a\nclosely related but distinct concept, equilibrium design. Unlike mecha-\nnism design, the designer's authority in equilibrium design is more con-\nstrained; she can only modify the incentive structures in a given game\nto achieve certain outcomes without the ability to create the game from\nscratch. We study the problem of equilibrium design using dynamic incen-\ntive structures, known as reward machines. We use weighted concurrent\ngame structures for the game model, with goals (for the players and the\ndesigner) defined as mean-payoff objectives. We show how reward ma-\nchines can be used to represent dynamic incentives that allocate rewards\nin a manner that optimises the designer's goal. We also introduce the main\ndecision problem within our framework, the payoff improvement problem.\nThis problem essentially asks whether there exists a dynamic incentive\n(represented by some reward machine) that can improve the designer's\npayoff by more than a given threshold value. We present two variants of\nthe problem: strong and weak. We demonstrate that both can be solved\nin polynomial time using a Turing machine equipped with an NP oracle.\nFurthermore, we also establish that these variants are either NP-hard or\ncoNP-hard. Finally, we show how to synthesise the corresponding reward\nmachine if it exists.", "sections": [{"title": "Introduction", "content": "Over the past decade, Nash equilibrium (NE) and other game-theoretic concepts\nhave been extensively used to analyse concurrent and multi-agent systems (see"}, {"title": "Preliminaries", "content": "In this section we introduce the basic notions that will be used throughout\nthe paper. We start with the definition of mean-payoff value and multi-player\nmean-payoff games.\nMean-Payoff For an infinite sequencer \u2208 R, let mp(r) be the mean-payoff\nvalue of r, that is, mp(r) = lim infn\u2192\u221e avgn (r) where, for n\u2208N\\{0}, we define\navgn(r) = = = rj, with r; the (j+1)th element of r.\nMulti-Player Mean-Payoff Game A multi-player mean-payoff game is a\ntuple G = (N, Ac, St, Sin, (di)i\u2208n, tr, (Wi)i\u2208N, Wg) where\n\u2022 N = {1, ..., n}, Ac, and St are finite non-empty sets of players, actions,\nand states, respectively;\n\u2022 Sin \u2208 St is the initial state;\n\u2022 di: St\u2192 2Ac \\ {\u2205} is a protocol function for player i returning possible\naction at a given state;\n\u2022 tr: St \u00d7 Ac \u2192 St is a transition function mapping each pair consisting\nof a states \u2208 St and an action profile a = (a1,..., an) \u2208 Ac = Ac"}, {"content": "one for each player, to a successor state we write a\u00bf for a{i} and a_i\nfor an\\{i}. For two decisions a and a', we write (ac, ac) to denote the\ndecision where the actions for players in CCN are taken from a and the\nactions for players in N\\Care taken from a';\n\u2022 wi: St\u2192 Z is player i's weight function mapping, for every player i and\nevery state of the game into an integer number; and\n\u2022 wg: St\u2192 Z is a global weight function mapping every state of the game\ninto an integer number.\nWe define the minimum and maximum weights appearing in G as follows.\nDefinition 1. For a given game G and its set of states St, define MinW =\nmin{w(s) | s \u2208 St} and MaxW = max{w;(s) | s \u2208 St}.\nA path is an infinite sequence \u03c0 S0, S1, S2, ... \u2208 St such that for each\nk\u2208 N, there is an action profile (in k-th step) ak\u2208 Hiendi(sk), such that\nSk+1 = tr(sk,ak). We write \u03c0<k to denote the prefix of up to and including\nsk. Similarly, \u03c0>k denotes the suffix of starting from sk. Let Pathsg(s) be\nthe set of all possible paths in G starting from s."}, {"title": "", "content": "A strategy for agent i is a Mealy machine \u03c3\u2081 = (Ti, ti, St, Yi, pi), where Ti is a\nfinite and non-empty set of internal states, t is the initial state, Yi : St\u00d7Ti \u2192 Ti\nis a deterministic internal transition function, and pi: St \u00d7 Ti \u2192 Aci an action\nfunction. We say that a strategy \u03c3\u03b5 is valid with respect to G if and only if\npi(s,tj) \u2208 di(s). From now on, we restrict our attention to valid strategies, and,\nunless otherwise stated, refer to them simply as strategies. We denote Stri(G)\nthe set of valid strategies for player i in G. Moreover, for a given strategy \u03c3\u03c4\nand a finite sequence \u2208 St*, by \u03c3\u03b5(\u03c0) \u2208 Ac we denote the action prescribed\nby the action function pi of \u03c3\u03b5 after the sequence \u00fb has been fed to the internal\ntransition function Yi. Note that the model of strategies implies that strategies\nhave perfect information and finite memory, although we impose no bounds on\nmemory size.\nA strategy profile \u1edf = (51,...,on) is a vector of strategies, one for each\nplayer. As with actions, i denotes the strategy assigned to player i in profile\n\u1edf. Moreover, by (\u2642\u00df,\u2642c) we denote the combination of profiles where players\nin disjoint B and C are assigned their corresponding strategies in \u1edf and \u1edf',\nrespectively. We denote Stra(G) the set of strategy profiles for the set A of\nagents. We also use Str(G) = Strn(G) to denote the strategy profiles for all\nthe agents in the game. Whenever the game is clear from the context, we also\nsimply use Str. Once a state s and profile \u2642 are fixed, the game has an outcome,\na path in G, denoted by \u03c0(\u2642, s). Because strategies are deterministic, \u03c0(\u2642, s) is\nthe unique path induced by \u1edf, that is, the sequence so, S1, S2, .. such that\n\u2022 Sk+1 = tr(sk, (P1(sk, t\u2081),..., pn(sk,th))), and\n\u2022 th+1 = vi (s,t), for all k \u2265 0.\nFor a subset of agents CCN and strategies c, we say that a path is\ncompatible with oc if, for every k \u2208 N, there exists an action profile at with\na = \u03c3\u03af(\u03c0<k) for each i \u2208 C, such that sk+1 = tr(sk, ak). Intuitively, \u03c0is\ncompatible with oc if it can be generated when the agents in C play according\nto their respective strategies. We denote by outg(s, \u2642c) the set of paths starting\nfrom s and compatible with c. Observe that Pathsg(s) can also be written as\noutg (s, \u00d8).\nGiven a game G and a strategy profile \u2642, a path \u03c0(\u2642) induces, for each player\ni, an infinite sequence of integers wi(\u03c0(\u2642)) = Wi(Sin)Wi(S1)\u2026\u2026. Similarly, \u03c0(\u2642)\nalso induces such a sequence of integers for the global weight function wg(\u00b7).\nThe payoff of player i in game G is pay(\u2642) = mp(w\u2081(\u03c0(\u2642))), and the global\npayoff of G is pay(\u2642) = mp(wg(\u03c0(\u2642))). Whenever the game is clear from the\ncontext, we simply use pay\u00bf(\u2642) and pay,(\u2642), respectively.\nNash Equilibrium Using payoff functions, we can define the game-theoretic\nconcept of Nash equilibrium [22]. For a multi-player game G, a strategy profile\n\u1edf is a Nash equilibrium of G if, for every player i and strategy of for player i,\nwe have\npay(\u2642) \u2265 pay;((\u2642\u2212i, \u03c3\u03ad)) ."}, {"title": "", "content": "We also say that \u1ee1 is a j-fixed Nash Equilibrium [19] if pay\u2081(\u2642) \u2265 pay\u00bf((\u2642\u2212i, \u03c3'))\nfor every player i \u2260 j different from the fixed j.\nLet NE(G) and NE;(G) be the set of Nash Equilibria and j-fixed Nash Equi-\nlibria of G. We define bestNE(G) = SUP&\u2208NE(G) {pay,(\u2642)} as the best global\npayoff over the set of possible outcomes sustained by a Nash Equilibrium in\nthe game. Equivalently, we define worstNE(G) = infz\u2208NE(G) {pay,(\u2642)} as the\nworst global payoff over the set of possible outcomes sustained by a Nash Equi-\nlibrium in the game. In the case of NE(G) is empty, in order to make the\nvalues of bestNE(G) and worstNE(G) well defined, we assume that bestNE(G) =\nworstNE(G) = MinW."}, {"title": "Reward Machines for Equilibrium Design", "content": "In this section, we introduce a type of finite state machine, called a reward\nmachine (RM). A RM takes a path was input, and outputs a sequence of\nvectors 20, 21 \u00b7\u00b7\u00b7 \u2208 (Nn)w that corresponds to the reward granted to the players\nat each step of the path. Formally, a RM is defined as a Mealy machine:\nDefinition 2 (Reward Machine). A RM is a Mealy machine M = (QM, QM, SM, TM),\nwhere QM is a finite (non-empty) set of states, qM the initial state, SM :\nQM \u00d7 St\u2192 QM a deterministic transition function, and \u2533M : QM \u00d7 St \u2192 Nn\na reward function where r\u00bfM(q) = \u0442M(q)(i) is the reward in the form of a natural\nnumber k \u2208 N imposed on player i if the play visits (s,q) \u2208 St \u00d7 QM. Some-\ntimes, when it is clear from the context, the elements of the RM are denoted\nwithout superscripts.\nReward Machine implementation. For a given game G = (N, Ac, St, sin, (di)i\u2208n, tr, (wi)i\u2208N, Wg),\nthe implementation of MonG is the game\nG \u2020 M = (N, Ac, St \u00d7 Q, (sin, q0), (dM)i\u2208N, trM, (wM)i\u2208N, WM),\nwhere: (i) dM(s, q) = di(s), for each agent i \u2208 N; (ii) trM((s, q), \u1ea3) = (tr(s, \u0101), d(s, q));\n(iii) w\u00bfM(s, q) = wi(s) + Ti(s, q); (iv) w/M(s, q) = wg(s) \u2013 ||T(s, q)|| 1.\nFor a given natural number \u03b2\u2208 N, a \u03b2-RM, denoted M\u00df, is RM such that\n||T(s,q)|| \u2264 \u03b2 for each (s,q) \u2208 St \u00d7 Q. In this paper, we consider a budget B\nbeing fixed and restrict our attention only to B-RMs.\nDefinition 3 (Global payoff improvement problems). For a given game G, a\nbudget B, and a threshold \u2206. The global payoff weak improvement problem\nconsists in deciding whether there exists a \u1e9e-RM M such that:\nbestNE(G\u2020 M) \u2013 bestNE(G) \u0394.\nThe global payoff strong improvement problem consists in deciding whether\nthere exists a B-RM M such that:"}, {"title": "Reward Engineering", "content": "In this and the next section, we show how to solve global payoff improvement\nby constructing an auxiliary game that allows to look at the problem as an\nequilibrium verification per se. More specifically, such construction regards\nreward machines as the strategies of a designated agent in the game, whose\nweight function corresponds to the global weights of the original game updated\nwith the rewards spent on the others at each iteration. First we provide the\ndefinition of such auxiliary game, which is inspired from the constructions given\nin [24, 1].\nDefinition 4. Given a game G and a budget \u03b2\u2208 N, we define its auxiliary\ngame G' = (N', Ac', St', s\u00edn, (di)i\u2208n', tr', (w')i\u2208n'), where (i) N\u2032 = {0} UN, Ac' =\nAcU\u1e9er, St' = St \u00d7 \u1e9en, sin = (sin, 0); (ii) tr'((s, \u0e1a\u0e35), (\u00e3, v')) = (tr(s, a), v');\n(iii) d(s,v) = di(s),i \u2208 N; (iv) do(s,\u0e1b\u0e35) = {\u028a : ||\u028a|| \u2264 \u1e9e}; (v) w(s,\u0e1b\u0e35) =\nwi(s) + vi; (vi) wo = wg ||||.\nIntuitively, we are adding agent 0 to the original game G, whose actions are\nn-dimensional vectors representing the possible rewards assigned to every other\nagent. All the other components of the auxiliary game are defined accordingly.\nThe protocol function remains the same for every original agent, whereas the\none for agent 0 prescribes that the amount of reward distributed to the agents\nat each iteration does not exceeds the budget B. The set of states is augmented"}, {"title": "", "content": "to record the amount of reward received by each agent, which is then reflected\nin the corresponding weight function w. Finally, the global weight function\nis updated by subtracting the amount of reward established by agent 0 in the\ncurrent iteration.\nIn the next two constructions, we show how to transform a B-RM for G into\na strategy for agent 0 and viceversa.\nConstruction 1 (RM to Strategy). Given a RM M = (\u0430\u043c, \u0434\u043c, \u0431\u043c, \u0442\u043c) of\nG\u2020M, we define the strategy of player 0 in G'as \u03c3\u03bc = (To, to, St', Yo, Po) where\n\u0422\u043e = Q\u043c, t0 = 9M, and the internal transition and action functions defined as\n\u2022 yo((s, v), t) = \u0431\u043c(s, t)\n\u2022 \u03c1\u03bf((s, v), t) = \u0442\u043c(s,t)\nfor every (s, \u0e1a\u0e35) \u2208 St' and t \u2208 \u03a4\u03bf.\nIntuitively, the strategy \u03c3m uses the same internal states of the RM M,\nwhile the transition and action functions of om are defined by modifying those\nof M to match with the types required to be considered a strategy for 0 in G'.\nSuch construction can be reverted by carefully modifying the types, in order\nto move from a strategy of agent 0 in G' to a RM for G, as it is shown in the\nfollowing.\nConstruction 2 (Strategy to RM). Given a strategy \u03c3\u03bf = (To, to, St', yo, po)\nin G', we define the RM for Gas \u039c\u03c3\u03bf = (QM60, Mo, Mo, Moo) where\nQMoo = T \u00d7 \u03b2n, IM, = (t0, 0), and the transition and reward functions defined\nas\n\u2022 \u03b4. (s, (t, v)) = (yo((s, v), t), po((s, v), t))\n\u2022 TMoo (s, (t, v)) = po((s, v), t)\nfor every s \u2208 St and (t, v) \u2208 QMoo\u00b7\nWe write \u03c0\u03b9st to denote the sequence in St obtained from by projecting\nthe component in St and \u03c4(\u03c0) the sequence in (Zn) obtained from w\u2081M(\u03c0),..., wM(\u03c0).\nIn the following Lemma, we prove that the constructions presented above\ncorrectly translate RMs into strategies and viceversa, meaning that they make\na connection between paths of G\u2020 M and outcome of G' when agent 0 uses the\ncorresponding strategy and viceversa.\nLemma 1. For a given G\u2020M and its associated auxiliary game G' the following\nhold:\n(1) for every path \u03c0\u2208 Pathsg\u2020\u043c((Sin, q\u00ba)), there is a path \u03c0' = (\u03c0\u2020St, \u03c4(\u03c0)) \u2208\nouts, ((sin, 0), \u03c3\u03bc), and w\u00bfM(\u03c0) = w'(\u03c0') for all i \u2208 N and w^^(\u03c0) =\nw\u03cc(\u03c0');"}, {"title": "", "content": "(2) for every path \u03c0' \u2208 outg, ((sin, 0), \u03c3\u03bf), there is a path \u03c0 = (\u03c0'st, \u03b4\u03bc\u03bf, (\u03c0')) \u0395\n\u039c\u03c3\u03bf (\u03c0) = w' (\u03c0') for all i \u2208 N and w\nPathsg+Moo ((Sin, q\u00ba)), and w\nwo(\u03c0').\nProof. We prove the first item only, as the other has a similar proof.\n\u039c\u03c3\u03bf (\u03c0)\n=\nObserve that the path \u03c0' is uniquely identified from \u03c0. Moreover, from\nthe definitions of Moo and wo, it immediately follows that w/M(\u03c0) = w\u02b9(\u03c0').\nTherefore, we only need to prove that \u03c0' belongs to outg, ((sin, 0), \u03c3\u03bc). We do\nit by induction on k \u2208 N by showing that every prefix \u03c0'\u3111k of \u03c0' can be extended\ncompatibly with \u03c3\u03bc.\n<k\nFor the base case k = 0, we have that '<0 = (0,0) which is trivially\nextendable to any path in out\u00e7,((Sin, 0), om).\nFor the induction case k > 0, assume that \u03c0\u3111k is extendable to a path\nin outg, ((sin, 0), \u03c3\u03bc). Then, consider \u3160<k and a the joint action such that\n\u03c0k+1 = tr(\u03c0k,a), which exists since \u03c0\u2208 Pathsg\u2020M((Sin, q\u00ba)). Clearly, it holds\nthat \u03c0'>k+1 = tr' (\u03c0'>k, (\u03c3\u03bc(\u03c0'>k,\u0101))), which makes \u03c0'<k+1 extendable compati-\nbly with \u03c3\u03bc.\nSimilarly to the correspondence between RMs and agent O's strategies, a\nconnection between strategies of any other agent i in G\u2020 M and G' exists. In\nother words, once a RM M and its corresponding strategy \u03c3\u03bc are fixed, every\nstrategy \u03c3\u03b5 for agent i in G\u2020 M can be translated into a strategy \u03c3\u03af.\n=\nConstruction 3 (G\u2020 M to G'). For a game G\u2020 M and a strategy \u03c3i\n(Ti, t, Vi, Pi) in it, we define a strategy \u00f4\u2081 = (Ti, ti, \u017fi, \u017fi) in the correspond-\ning game G' as follows:\n\u2022 \u00ce\u00bf = T \u00d7 QM, and t = (t, q^);\n\u2022 \u00cei : (T \u00d7 QM) \u00d7 (St \u00d7 \u1e9en) \u2192 (T \u00d7 QM) such that \u0177i((t,q), (s, 0)) =\n(i(t, (s, q)), \u0431\u043c(s, q));\n\u2022 pi: (T \u00d7 QM) \u00d7 (St \u00d7 \u1e9en) \u2192 (T \u00d7 QM) such that pi((t,q), (s, \u0e1b\u0e35)) =\npi(t, (s, q)).\nBy \u03b8\u04ab\u2020\u043c(\u03c3\u03b9) = \u00f4\u2081 we denote the strategy for player i in G' obtained from \u03c3i\nby applying the construction above.\nOn the other hand, once a strategy \u03c3\u03bf for agent 0 in G' and the corresponding\nRM Moo are fixed, the translation from strategies for agent i in G' to strategies\nin G\u2020 Moo is possible.\nConstruction 4 (G' to G\u2020M). For a game G' and a strategy \u00f4\u2081 = (Ti, t\u00ec, \u0108i, Pi),\nwe define a strategy oi = (Ti,t, Vi, pi), in the corresponding game G\u2020 M_as\nfollows:\n\u2022 \u00ce\u00bf = T \u00d7 QM, and t = (t, q);"}, {"title": "", "content": "\u2022 \u00cei : (T \u00d7 QM) \u00d7 (St \u00d7 \u1e9en) \u2192 (T \u00d7 QM) such that \u011di((t,q), (s, \u0e1b\u0e35)) =\n(yi(t, (s, q)), \u0431\u043c(s,q));\n\u2022 pi: (T \u00d7 QM) \u00d7 (St \u00d7 \u1e9en) \u2192 (T \u00d7 QM) such that pi((t,q), (s, \u0e1b\u0e35)) =\nPi(t, (s, q)).\nBy \u03b8\u03c2, (\u03b4\u03b9) = \u03c3\u00a1 we denote the strategy for player i in G \u2020 M obtained from\n\u00f4i by applying the construction above.\nThe following two lemma shows that the connection among strategies in\nbetween the games also preserve the payoff of agents.\nLemma 2. For a given game G, RM M, and strategy profile \u1edf \u2208 Str(G \u2020 M),\nit holds that\npay+M (\u2642) = pay' (\u043e\u043c, \u04e9\u04ab\u2020\u043c(\u2642))\nProof. Observe that the path \u03c0 = \u03c0(\u2642, (sin, q\u00ba)) belongs to the set Paths\u00e7\u2020M((sin, q\u00ba)).\nMoreover, by Construction 3, the path \u03c0' = \u03c0((\u03c3m, lg\u2020m(\u2642)), (sin, q\u00ba)) is ex-\nactly the one such that w/M(\u03c0) = w\u00f3(\u03c0') as proved in the Item 1 of Lemma 1.\nThis straightforwardly shows that pay\u2020M(\u2642) = pay\" (\u043e\u043c, \u04e8\u04ab\u2020\u043c(\u2642)).\nLemma 3. For a given game G, a strategy \u03c3\u03bf \u2208 Stro(G'), and strategy profile\n\u1edf \u2208 StrG\u2020M, it holds that\n,,\npay' (\u03c3\u03bf, \u2642) = pay\nSketch. The proof is similar to the one of Lemma 2, with the use of Construc-\ntion 4 and Item 2 of Lemma 1.\n\u039c\u03c3\u03bf (\u03b8\u03c2, (\u03c3))\nBy having the same set of payoffs, it simply follows from Lemma 1, Lemma 2,\nand Lemma 3, that the games G\u2020 M and G', where agent 0 is bound to the use\nof om share the same set of Nash Equilibria.\nTheorem 4. For a given game G and a budget \u1e9e, the two following hold:\n1. For every \u1e9e-RM M and strategy profile \u1edf in G \u2020 M, it holds that\n\u1edf \u2208 NE(G \u2020 M) iff (om), \u2642) \u2208 NE\uff61(G').\n2. For every strategy profile (\u03c3\u03bf,\u2642) in G', it holds that\n(\u03c3\u03bf, \u2642) \u2208 \u039d\u0395\u03bf(G') iff 0\u00e7\u2020m\uff61\uff61(\u2642) \u2208 NE(G \u2020 M\uff61\uff61)\""}, {"title": "Solving Improvement Problems", "content": "In this section, we present a technique for solving the weak and strong improve-\nment problems. We also demonstrate how to synthesise the RM, if it exists.\nWith the definition of the improvement problems, it makes sense to start with\nthe problems of computing worstNE and bestNE. To this end, we introduce\nNE threshold problem [28] that we will use as a subroutine in our algorithms.\nThis problem asks whether there exists a NE in G, such that the payoffs for the\nplayers fall between two vectors 7 and \u1ef9.\nDefinition 5 (NE Threshold Problem). Given a game G and vector x, \u1ef9 \u2208\n(QU{\u00b1\u221e})"}, {"content": "decide whether there is \u1edf \u2208 NE(G) with xi \u2264 pay\u00bf(\u2642) \u2264 yi for\nevery i \u2208 N.\nWhen the players have pure strategies, the NE threshold problem can be\nsolved in NP [28].\nWe begin with the following observation. For a given game G, it holds\nthat MinW worstNE(G) MaxW and MinW bestNE(G) MaxW.\nMoreover, it also holds that for a given G', we have MinWg' worstNE(G')\nMaxWgand MinW' bestNE(G') MaxWg'. As such, by using binary\nsearch and the NE threshold problem subroutine, we can compute the values of\nworstNE and bestNE for G and G'.\nAs we previously discussed, the optimal values of worstNE(G) and worstNE(G\u2020\nM) may not be achievable with finite-state strategies and RMs. To see this,\nconsider again Example 1. Suppose that we have a RM M' shown in Figure 3,\nwhere TM' (95, m) = 1 and r\u2081M' (q,s) = 0 for all (q, s) \u2260 (95,m). Intuitively,\nplayer 1 is only given a reward of 1 after it finishes two cycles of deliveries.\nClearly the set of NE still corresponds to the same sequence p(s,l,m). How-\never, since now the designer only needs to pay 1 unit for every two cycles, we\nhave worstNE(G\u2020 M') =, which is strictly greater than worstNE(G \u2020 M) = 3\nobtained by the RM in Figure 2. In fact, we can increase the number of cycles\nneeded to be done before giving 1 unit of reward by adding more states in the\nRM, thus obtaining strictly greater worstNE value. Since the size of RM is not\nbounded, we can do this indefinitely. A similar argument can also be given\nfor the optimal value of worstNE(G), the complete explanation can be found in\nAppendix A.1. Observe that by multiplying pay, with -1, we can also use the\nexample above to analogously reason about bestNE.\nThe above arguments shows that the binary search for computing the values\nof worstNE and bestNE may not terminate. To ensure termination, we compute"}, {"title": "", "content": "Algorithm 1 Computing \u025b-worstNE\ninput: G, \u03b5\n1: a1 \u2190 MinW; a2 \u2190 MaxW\n2: while a2 \u2013 a1 \u2265 \u03b5 do\na'\u2190 a1+a2;\nif \u2203\u1ee1 \u2208 NE(G), a1 \u2264 pay,(\u2642) \u2264 a' then\na2 a'\nelse\na1 a'\nend if\nend while\n10: return a2\napproximate values instead.\nDefinition 6. Given \u025b > 0, an approximate value of worstNE (resp. bestNE)\nis a value a such that a - \u025b < o, where o is the optimal value of worstNE (resp.\nbestNE). We refer to such an approximate value as \u025b-worstNE (resp. \u03b5-bestNE).\n5:\n6:\n7:\n8:\n9:\nWe provide Algorithm 1 for computing \u025b-worstNE given Gand \u025b encoded\nin binary. The check in Line 4 corresponds to the NE threshold problem from\nDefinition 5. Notice that the threshold vectors 7, \u0177 are not explicitly given, as\nwe are not interested in these values. Thus, we fix xi = MinW, yi = MaxW\nfor each i \u2208 N, i.e., they can be of any possible values. On the other hand, we\nare interested in payg, which in fact does not correspond to the payoff of any\nplayer. However, we can easily modify the underlying procedure for solving the\nproblem in [28] to handle this. Specifically, by [28, Lemmas 14 and 15], we can\nspecify an additional linear equation corresponding to the value of pay, being\nin between a\u2081 and a', thus yielding a procedure that is also in NP. Algorithm 1\ncan also be used to compute \u025b-worstNE(G') with the following adaptation: Line\n4 is slightly modified into \u2203 \u2208 NE0(G), a1 \u2264 payo(\u2642) \u2264 a', that is, the NE\nset corresponds to the 0-fixed NE. Just as with pay,, payo is not the payoff of\nany player in N. Therefore, we modify the underlying procedure for the NE\nthreshold problem using the same approach as the above.\nTo compute \u025b-bestNE, we can employ a similar technique. We make the\nfollowing modification to Algorithm 1: in each iteration, instead of checking the\nleft-half part, we check the right-half part (i.e., instead of minimising, we are\nmaximising). This is done in Lines 4-8 of the algorithm by checking whether\n\u2203 \u2208 NE(G), a' \u2264 pay,(\u2642) \u2264 a2. If the check returns true, we set a\u2081 \u2190 a',\notherwise a2 \u2190 a'. Again, as with worstNE, we slightly modify Line 4 in order\nto compute bestNE(G').\nTheorem 5. Given a game G (resp. G') and \u025b > 0, the problems of comput-\ning e-bestNE(G) and \u025b-worstNE(G) (resp. \u025b-bestNE(G') and \u03b5\u025b-worstNE(G')) are\nFPNP-complete."}, {"title": "", "content": "Proof. The upper bounds follows from Algorithm 1. The while loop runs in\npolynomial number of steps (i.e., logarithmic in |G|\u00b7), and in each step calls\na NP oracle. Observe that \u025b can be arbitrarily small (i.e., arbitrary precision).\nFor the lower bound we reduce from TSP COST which is FPNP-hard [23]. Given\na TSP COST instance (G, c), G = (V, E) is a graph, c : E \u2192 Z is a cost function,\nwe construct a game G such that the \u025b-worstNE(G) corresponds to the value of\noptimum tour\u00b3. Let G be such a game where\n\u2022 N = V,\n\u2022 St = {(e, v) : e \u2208 \u0395 \u2227 v = trg(e)} \u222a {(*, sink)},\n\u2022 s\u00ba can be chosen arbitrarily from St \\ {(*, sink)},\n\u2022 for each state (e, v) \u2208 St and each player i \u2208 N\ndi((e, v)) = {out(v)} \u222a {*} if i = v\ndi((e, v)) = {0, *}, otherwise;\n\u2022 for each state (e, v) \u2208 St and action profile Ac\ntr((e, v), Ac) = (a, trg(a)) if v \u2260 sink and Vi \u2208 N, a\u017c \u2260 *;\ntr((e, v), Ac) = (*, sink), otherwise;\n\u2022 for each state (e, v) \u2208 St and player i \u2208 N\nwi((e, v)) = |V|, if v = i and v \u2260 sink,\nwi((e, v)) = 0, if v \u2260 i and v \u2260 sink,\nwi((e, v)) = 1, if v = sink;\n\u2022 for each state (e, v) \u2208 St\nwg((e, v)) = max{c(e') : e' \u2208 E} \u00b7 |V|, if v = sink\nwg((e, v)) = c(e) \u00b7 |V|, otherwise;\nwhere,, sink are fresh symbols. We also set \u025b = 1. The construction is\ncomplete and polynomial to the size of (G, c).\nWe argue that [E-worstNE(G)\u300d is exactly the value of optimal valid tour.\nFirst, observe that for any \u2642\u2208 NE(G), it holds that either (1) \u03c0(\u2642) visits every\nv \u2208 V (i.e., visits every city), thus a valid tour, or (2) \u03c0(\u2642) enters (*, sink) and\nstays there forever. Case (1) holds because if \u03c0(\u2642) does not visit v \u2208 V, then\npay (\u2642) = 0 thus player v will deviate to (*, sink) and obtain better payoff. In\nfact, \u1edf visits each city exactly once, because otherwise, there is a player who\ngets payoff strictly less than 1, and deviates to (*, sink). Case (2) is trivially\ntrue; however, assuming that the costs are not uniform (otherwise TSP COST\nbecomes trivial), it cannot be a solution to \u025b-worstNE. Let o be the optimal"}, {"title": "", "content": "tour cost, and suppose for a contradiction that [\u025b-worstNE(G)\u300d < 0. Let \u1edf be a\ncorresponding strategy profile. By the construction of G, this means that \u1edf does\nnot visit some cities or visits some cities more than once. However, by (1) above,\n\u1edf cannot be in NE(G)\u2014a contradiction. We can argue in a similar manner for\n[\u03b5-worstNE(G)] > 0; it is not possible because either the corresponding strategy\ndoes not form a valid tour (and by (1) above, it is not a NE), or it is not the\noptimal solution to \u025b-worstNE; again a contradiction. Finally, since \u025b-worstNE\napproaches worstNE from the right, we have [\u03b5-worstNE(G)] = 0.\nFor bestNE, we can use the same construction but with the following modi-\nfication to wg:\n\u2022 wg((e, v)) = \u2212(max{c(e') : e' \u2208 E} \u00b7 |V|), if v = sink\n\u2022 wg((e, v)) = \u2212(c(e) \u00b7 |V|), otherwise;\nand use similar argument as the above.\nApproximate improvement problems We define the approximate improve-\nment problems as follows.\nDefinition 7 (\u025b-improvement problem). Given a game G, a budget \u00df, a thresh-\nold \u2206, and \u025b. The \u0393\u03b5-improvement problem, with \u0393 \u2208 {strong, weak}, decides\nwhether there exists a B-RM M such that:\n\u03b5-\u0393\u039d\u0395(G \u2020 \u039c) \u2013 \u03b5-\u0393\u039d\u0395(G) > \u0394.\nHaving the procedures for computing \u03b5-worstNE and \u03b5-bestNE for both G and\nG', we can then directly solve the \u025b-improvement problem with the following\nprocedure.\n1. Build the auxiliary game G';\n2. Compute \u03b5-\u0393\u039d\u0395(G) and \u03b5-\u0393\u039d\u0395(G');\n3. If \u03b5-\u0393\u039d\u0395(G') \u2013 \u03b5-\u0393\u039d\u0395(G) > \u2206, then return \"yes\"; otherwise return \u201cno\u201d.\nTheorem 6. Strong and weak \u025b-improvement problems are \u2206.\nProof. The upper bounds follow from the procedure described above. Steps 1\nand 3 can be done in polynomial time, Step 2 only needs two calls to an FPNP\noracle. Thus we have a decision procedure that runs in PNP = \u2206.\nTheorem 7. Strong and weak \u025b-improvement problems are NP-hard and coNP-\nhard, respectively.\nProof. To show that strong \u025b-improvement problem is NP-hard, we reduce from\nHAMILTONIAN PATH problem: given a directed graph G = (V, E), is there a path\nthat visits each vertex exactly once; this problem is NP-hard [23]. We build a\ngame G and fix \u03b2, \u0394 and \u025b such that the strong \u025b-improvement problem returns\nyes if and only if HAMILTONIAN PATH returns yes. Given a HAMILTONIAN PATH\ninstance G = (V, E), we construct a game G as follows."}, {"title": "", "content": "\u2022 N = VU {n + 1, n + 2}, where V = {1, ..., n},\n\u2022 St"}]}