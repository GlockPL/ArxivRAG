{"title": "Assessing Personalized AI Mentoring with Large Language Models in the Computing Field", "authors": ["Xiao Luo", "Sean O'Connell", "Mithun, Shamima"], "abstract": "This paper provides an in-depth evaluation of three state-of-the-art Large Language Models (LLMs) for personalized career mentoring in the computing field, using three distinct student profiles that consider gender, race, and professional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2 using a zero-shot learning approach without human intervention. A quantitative evaluation was conducted through a custom natural language processing analytics pipeline to highlight the uniqueness of the responses and to identify words reflecting each student's profile, including race, gender, or professional level. The analysis of frequently used words in the responses indicates that GPT-4 offers more personalized mentoring compared to the other two LLMs. Additionally, a qualitative evaluation was performed to see if human experts reached similar conclusions. The analysis of survey responses shows that GPT-4 outperformed the other two LLMs in delivering more accurate and useful mentoring while addressing specific challenges with encouragement languages. Our work establishes a foundation for developing personalized mentoring tools based on LLMs, incorporating human mentors in the process to deliver a more impactful and tailored mentoring experience.", "sections": [{"title": "I. INTRODUCTION", "content": "AI mentoring is increasingly recognized as a critical tool in education, offering personalized guidance and support to students in ways that traditional mentoring may not always provide. The importance of AI-driven mentoring lies in its ability to scale mentoring efforts, making high-quality support accessible to a larger number of students, particularly in fields like computing where individual mentorship demand often exceed supply [28]. AI mentors can offer personalized feed-back, identify student strengths and weaknesses, and adapt to individual learning styles, fostering a more tailored educational experience [19]. Additionally, AI mentors are available around the clock, providing assistance whenever students need it, thus enhancing the learning process [22]. Recent trends show a growing integration of AI in mentoring systems, with advance-ments in natural language processing and machine learning enabling more nuanced and context-aware interactions [14], [15]. As these technologies evolve, AI mentoring is expected to become more sophisticated, offering increasingly effective support for students' academic and personal development.\nThe trend of AI mentoring using Large Language Models (LLMs) is rapidly gaining momentum, driven by the ability of these models to understand and generate human-like text across a wide range of contexts. LLMs, such as OpenAI GPT, Meta LLaMA, and Google Gemini, etc., have shown remarkable capability in natural language understanding, mak-ing them ideal candidates for mentoring and educational applications [7]. These models can engage in meaningful dialogue, provide personalized feedback, and support students in navigating complex subject matter, particularly in domains like computing where mentorship is crucial [34]. The ability of LLMs to process and analyze vast amounts of textual data allows them to tailor responses based on individual student needs and background, fostering a more personalized learning experience through textual data [13]. Moreover, the integration of LLMs in educational tools is becoming more prevalent, as they are increasingly being used to develop intelligent tutoring systems that can simulate one-on-one mentoring sessions in various domains [1]. As LLMs continue to evolve, their role in Al mentoring is expected to expand, offering even more sophisticated and contextually aware support to students across various disciplines.\nIn this research, we evaluated current state-of-the-art Large Language Models (LLMs) about their effectiveness in per-sonalized mentoring for students in computing domain. We focused evaluating the ability of LLMs on providing person-alized mentoring experiences for career planning in the com-puting fields while considering mentees' social backgrounds and proficiency levels in computing. The research questions guiding our investigation are following:\n\u2022\tRQ1: Considering the low enrollment of underrepresented students in computing programs [23], it's important to recognize that students from diverse social backgrounds may have varying needs. Our first research question is: If LLMs are utilized for personalized mentoring, do they take into account the student's social background (e.g., race, ethnicity)?\n\u2022\tRQ2: Considering that computing students have varying levels of experience and education in college, their needs may differ. Our second research question is: If LLMs are employed for personalized mentoring, do they take"}, {"title": "II. RELATED WORK", "content": "A. LLMs in Education\nGPT-based models have been actively explored in vari-ous educational fields, including academic writing [10], [39], teaching introductory programming [9], [26], and mathematics to impart fundamental skills [30], [33]. Recently, there has been a growing application of AI tools in computer science and information technology education [32], [37]. These AI-based tools are being utilized to teach introductory pro-gramming courses, offering the capability to automatically generate code, identify errors, and provide suggestions to help students produce precise and efficient code [17], [18], [21], [35], [36]. Despite the potential of LLM-based models, research on the effectiveness of tools like ChatGPT is limited. Okonkwo and Ade-Ibijola [25] highlighted the transformative nature of AI tools in traditional teaching methods, offering personalized learning and automated tasks. AI-based chatbots were specifically mentioned for their role in problem-solving and personalized guidance. However, Kosar et al. [21] pointed out challenges with AI chatbots, such as fostering laziness and hindering critical thinking. Ismail and Ade-Ibijola [20] suggested designing AI chatbots that prioritize emotional and personalized engagement to help students with programming difficulties, emphasizing continuous practice over memoriza-tion to reinforce a deeper understanding of programming principles. In conclusion, the literature consistently indicates the significant potential of AI tools in computing education.\nB. LLMs for Mentoring\nSeveral review articles [6], [27] have examined the poten-tial and challenges of AI-enhanced personalized mentoring. Bagai and Mane [6] emphasize the promising possibilities and challenges, indicating that AI-enabled mentorship could improve career advancement, skill development, and mentee satisfaction. However, concerns such as security, algorithmic bias, and ethical considerations persist. The article also covers the essential characteristics and technological foundations for effective AI mentoring platforms, concluding that a fully realized AI-driven mentorship platform is still under devel-opment. Cronj\u00e9's 2023 study [11] utilized ChatGPT to assist fourth-year IT students with research proposals. Key findings emphasized the quality of feedback, the importance of well-designed prompts, and the necessity of student reflection. The study highlighted that well-designed prompts and reflection are vital for effective AI interaction. Akiba and Fraboni's 2023 study [3] investigates the potential of AI-powered tools like ChatGPT to improve the accessibility, efficiency, and effectiveness of academic advising. Similar to our research, the authors compiled a list of frequently asked questions from current and prospective students in a teacher education bachelor's degree program in the United States, selecting seven for this study. These questions were input into the ChatGPT to evaluate the quality and delivery of the generated answers. Sanya-Isijola and Leung [29] found ChatGPT to be valuable for learning, collaboration, exam preparation, and keeping updated. Other studies [24], [31] reported positive outcomes for AI-enhanced mentors and stressed the necessity of human involvement to enhance AI performance.\nTo the best of our knowledge, no existing work uses both NLP techniques and human evaluation to compare the perfor-mance of multiple LLMs for personalized mentoring towards career planning in computing field. Our study is the first to evaluate personalized mentoring while taking into account the mentees' social backgrounds and experience levels."}, {"title": "III. METHODOLOGY", "content": "In this research, we utilized a proposed personalized AI mentoring framework, as illustrated in Figure 1. The frame-work takes various questions from students and generates responses. Three LLMs were incorporated into this framework for evaluation purposes. We then evaluated these LLMs using a developed NLP pipeline, beginning with an analysis of the similarities between the responses to each question, consid-ering different student social and educational backgrounds. Following this, we focused on identifying the unique topics within the answers for each individual question. Finally, we conducted an independent qualitative analysis of the responses generated by each LLM for each question.\nA. Student Profile and Mentoring Questions\nTo comprehensively assess the capability of LLMs for personalized AI mentoring, we created three distinct student profiles and prepared 15 mentoring questions."}, {"title": "B. AI Mentoring using LLMS", "content": "In this research, we systematically evaluated these three LLMs towards personalized AI mentoring through conducting the zero-shot experiments using the Application Programming Interfaces (APIs) of GPT-40 [2], PaLM 2 [4], and LLaMA 3 [12]. The API allows users to provide instructions via two role variables. Our prompt is structured in the following order:\n\u2022\tSystem: defines task instructions for LLMs in the desired role. We use the system variable to provide task instruc-tions so that the model acts as the role of an Al mentor and provide suggestions to students who asked question with social and educational background information.\n\u2022\tUser: seek mentor suggestion by inputting question and social and educational background.\nFollowing the above definitions, a user message is designed to contain a description of student's background information and the question relevant to computing career planning. Figure 2 illustrates the design of the prompt. The prompt guides the LLMs to consider the student's background information when providing mentoring suggestions."}, {"title": "C. NLP Analysis Pipeline", "content": "The NLP analysis pipeline designed in this research is to objectively evaluate whether and how the LLMs consider each student's social and educational background when answering the questions.\nThe pipeline first analyzes the similarity between the an-swers for each question using each LLM. The hypothesis is that the percentage of sentences in the answers with high semantic similarity (>= 0) need to be low to have distinct answers. For example, if the answers of the same question from two students with different background have more 90% of the sentences with high semantic similarity, they are treated as the same answer, which means the AI mentoring does not provider personalized answers. To calculate the semantic similarity, we applied sentence transformer to convert candi-dates and ground truth into embeddings, then, applied cosine similarity to measure the similarity shown as Equation 1. Here, a\u00b7b represents the dot product of embeddings of a and b, and ||a|| and ||b|| represent the magnitudes (or Euclidean norms) of embeddings of a and b, respectively.\n$$c(a, b) = \\frac{a \\cdot b}{||a|| ||b||}$$\nWe computed the percentage of sentences in the responses that had a high semantic similarity, using a threshold of 0.7. This threshold indicates that if two sentences have a cosine similarity greater than 0.7, they are considered to have similar or identical semantic meanings and are counted as the same sentence.\nAfter conducting a similarity analysis, we extracted topics from each response and identified both shared and unique topics among the answers. We then explored whether the unique aspects were influenced by technical skills, social skills, or cultural or community perspectives.\nD. Design of Human Evaluation\nOther than the objective analysis using the NLP analysis pipeline, we developed a protocol for human evaluation. Specifically, we designed a survey with thirteen questions (5-point Likert scale with 1 being completely disagreed and 5 being completely agreed) based on the existing evaluation metrics used to assess the responses of a generative AI [3], [16] as well as other criteria used to evaluate human mentors' effectiveness [8], [38] that considering whether it is supporting mentees academically and emotionally to reach to their career goals. The designed survey also measured whether the responses considered the unique social and educational backgrounds of the students who asked the mentoring ques-tions."}, {"title": "IV. EXPERIMENTAL SETTINGS AND EVALUATION RESULTS", "content": "A. Result of NLP Analysis\nTable IV show the results of semantic similarity analysis using the method described in Section III.C using three LLMs, respectively. The values in Table III display the percentage of sentences in a response that share the same semantic meaning with sentences from other responses, yet originate from different student profiles. Based on the overall average of the semantic similarity analysis, we found that the answer to the male white freshman student has less overlapped content with other two students. After investigating the answers, we found this is primarily because that this student is an undecided major students. So that the answer to the mentoring questions are less specific, but more in general with regards to how to explore various subjects or opportunities to discover personal interests. Whereas the answer to the male African American junior student and the female, Hispanic freshman student have more overlapped content. Since they both are in the computer science major, the answers often include the same content that provides to computer science majors, such as a set of specific certifications that students can acquire for career planning etc. Upon comparing responses to questions that require more personalized content, we found that GPT-40 and Palm 2 tend to produce answers with higher average similarity scores to generic questions. This trend is not observed with LLaMA 3. For instance, for the question, 'How many years of ed-ucation/training should I expect to get a computing job?', LLaMA 3 provides significantly varied answers based on three different profiles, while GPT-40 and Palm 2 offer re-sponses with slightly higher similarity. Further investigation revealed that LLaMA 3 personalizes its answers according to the professional level of the students. If a student has chosen computer science as their major, LLaMA 3 tailors its response based on whether the student is a junior or freshman, indicating the number of additional years needed to secure a job. For students who have not decided on a major, LLaMA 3 provides detailed information on the range of options, from an associate degree to a PhD, including the corresponding number of years required for each.\nComparing the answers from the three LLMs for each student profile, we found that they also include some shared content that is generic to the questions. For example, in response to the question, 'What I want to know about a computing career is whether many jobs are available and how much positions in this field pay,' all three LLMs reference labor statistics data to provide detailed information about the average salaries for various computing jobs.\nTo summarize the unique aspects of the answers to the questions based on each profile, we generated word cloud to highlight the most unique aspects in the answers as accumu-lative frequency of the words in the answers of all questions. Figure 4 to 6 show the word clouds generated for profiles M-AA-J-CS, M-W-F-U, and F-H-F-CS, respectively. Based on the word clouds we can tell that for profile M-AA-J-CS, all three LLMs mentioned 'African American' to some extend, whereas LLaMA 3 emphasized more and also mentioned male. Since this is a junior CS major student, many answers men-tioned 'professional network', 'professional profile', \u2018industry experience' for 'job' and 'career'. Different from the answers for profile 1, the answers for profile 2 who is an undecided major freshman student, the answers from all LLMs emphasize on working on 'project' to know 'computing field' and gain 'skills'. None of these answers specifically mentioned about how to handle diversity challenges etc. For profile 3 who is a Hispanic female CS major freshman, both GPT-40 and LLaMA 3 mentioned more about 'Hispanic' 'woman' in the answers of the questions, which mean these two LLMs try to personalize the answers towards the profile of the mentee, whereas Palm 2 personalized less in the content of the answers. However, the answers for profile 3 shared some keywords with the answers for profile 1, such as 'professional', 'career', 'experience' since they both are CS majors, the answers for them often contain 'professional certification', 'professional skills', 'career advancement', 'career services', 'career path', 'personal experience', etc., in different answers.\nB. Result of Human Evaluation\nFigure 3 presents the results of the human evaluation based on a survey using the evaluation questions listed in Table III. GPT-40 received the highest human evaluation scores for most of the survey questions, while Palm 2 scored the lowest. For three questions 'Does the answer recommend competency developments?', 'Does the answer promote professional devel-opment and networking?', and 'Is the answer respectful?' both evaluators gave the highest scores across all LLMs. This indicates that all three LLMs effectively identified information relevant to competency development in computing domains, promoted professional networking to enhance soft skills, and provided answers in a respectful manner. However, the human evaluation revealed that all three LLMs performed poorly in helping students set appropriate goals. Upon reviewing all the questions and answers, we found that this may be due to the absence of questions specifically addressing personal or career goals. While some answers mentioned general pathways to achieve career goals, they lacked specificity to the students' profiles. Therefore, the lower scores may not accurately reflect the actual capabilities of the LLMs. Our research primarily focuses on the question-answering aspects of AI mentoring. However, further investigation is needed into interactive AI mentoring in a chatbot format."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "Our research evaluated the ability of LLMs to provide personalized mentoring by considering students' social back-grounds and skill levels. The findings indicate that while LLMs can account for personal backgrounds to some extent, human involvement is essential to deliver truly personalized mentoring experiences. We also suggest that fine-tuning LLMs with educational data could enhance their performance. How-ever, ethical concerns, particularly regarding privacy, bias, and transparency, must be addressed for real-world implementation of such systems.\nFuture work involves developing a human-in-the-loop men-toring system using LLMs, incorporating a chatbot-like mech-anism to gain a deeper understanding of individual needs and provide tailored mentoring experiences. This approach can also help mitigate biases and unfairness that may arise from the LLMs' training data."}]}