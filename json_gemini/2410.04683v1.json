{"title": "Towards Measuring Goal-Directedness in AI Systems", "authors": ["Dylan Xu", "Juan-Pablo Rivera"], "abstract": "Recent advances in deep learning have brought attention to the possibility of creating advanced, general AI systems that outperform humans across many tasks. However, if these systems pursue unintended goals, there could be catastrophic consequences. A key prerequisite for AI systems pursuing unintended goals is whether they will behave in a coherent and goal-directed manner in the first place, optimizing for some unknown goal; there exists significant research trying to evaluate systems for said behaviors. However, the most rigorous definitions of goal-directedness we currently have are difficult to compute in real-world settings. Drawing upon this previous literature, we explore policy goal-directedness within reinforcement learning (RL) environments. In our findings, we propose a different family of definitions of the goal-directedness of a policy that analyze whether it is well-modeled as near-optimal for many (sparse) reward functions. We operationalize this preliminary definition of goal-directedness and test it in toy Markov decision process (MDP) environments. Furthermore, we explore how goal-directedness could be measured in frontier large-language models (LLMs). Our contribution is a definition of goal-directedness that is simpler and more easily computable in order to approach the question of whether AI systems could pursue dangerous goals. We recommend further exploration of measuring coherence and goal-directedness, based on our findings.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of deep learning has seen remarkable advances in capabilities and generality from training large neural networks on large corpuses of data. For example, transformers (Vaswani et al. [2023]) have been successful in NLP (Brown et al. [2020]), image generation (Ramesh et al. [2021]), and protein structure prediction (Jumper et al. [2021]). Deep learning has also found success in games like Go (Silver et al. [2016]) and Starcraft 2 (Vinyals et al. [2019]). These successes have raised the possibility, now being pursued by multiple companies (OpenAI, Deepmind), of advanced artificial general intelligences (AGIs) that achieve better than human performance in a wide variety of tasks.\nHowever, there are concerns that AGIs could generalize poorly to unintended behavior in unforeseen environments. In particular, if an AGI is well-modeled as goal-directed, then it could pursue a misaligned goal that could lead to unwanted behavior such as power-seeking (Shah et al. [2022], Langosco et al. [2023]). A particularly dangerous hypothetical failure mode arising from goal misgeneralization is if the AGI learns to scheme against or deceive its training process to protect its misaligned goal from being changed (Hubinger et al. [2019], Carlsmith [2023]). A significant proportion of current AI safety work focuses on detecting and analyzing (e.g. via analogous case studies of model organisms like Hubinger et al. [2023, 2019]) scheming-type behavior in AI systems."}, {"title": "2 Background and Related Work", "content": "One paper related to our methods is Orseau et al. [2018] (cited by Shah et al. [2022]), which uses a variant of Bayesian inverse reinforcement learning (Ng and Russell [2000], Choi and Kim [2015]) to calculate the posterior probability that a system is well described as an \"agent\" versus a \"device\". This definition is explicitly based on the intentional stance (Dennett [2009]), which roughly claims that there is no observer-independent truth as to whether a system is or is not a goal-directed agent; we can only describe how well-modeled a system is as an agent by noticing behavioral patterns corresponding to things we might call \"beliefs\" or \"desires\". However, Orseau et al. [2018] only test their method in small gridworlds, while we attempt experiments in more complex environments. More mathematical equivalences are provided in section A.7.\nA related concept is that goal-directed models should be able to adapt and generalize to out-of-distribution settings well (Kenton et al. [2023], Turner and Tadepalli [2022]). Similar to these definitions, our method focuses on the mechanistic internals of a policy, however our definition also relies on the intentional stance. Previous literature often models the preferences of a possible agent with a utility or reward function, which is justified theoretically through coherence theorems in decision theory that equate axioms about preferences to expected utility maximization (von Neumann et al. [1944], Savage [1954])."}, {"title": "3 Methods and Key Definitions", "content": "We now present our mathematical model for measuring the goal-directedness of a policy in an environment. Inspired by the intentional framework (Dennett [2009]), goal-directed AIs are AIs that are \"well-described\" as having a goal, which we can formalize as a reward function in the context of sequential decision-making (Orseau et al. [2018]).\nGiven some environment with states and transitions, goal-directed policies should be good at many reward functions $R(s, a, s')$, indicating their ability to generalize across states in the environment and across tasks. (This is a common thread in other goal-directedness definitions, such as Orseau et al. [2018] and Kenton et al. [2023].) Such a goal-directed policy would need to adapt to new circumstances and likely internally need to select actions based on their consequences, which are core features of goal-directedness. Our definition is similar to (a simplified version of) Kosoy [2023] and Orseau et al. [2018]; we compare methods in further detail in section A.7. We give a full mathematical definition of our model in deterministic Markov decision processes (MDP, Puterman [1994]) without preset reward functions in section A.3. Briefly speaking, in an MDP our methodology is to generate different sets of policies in increasing order of goal-directedness:"}, {"title": "4 Experiments and Results", "content": "We now present three experiments of training a goal-directedness classifier under three stages of complexity: hand-picked MDP environments, a simple RL setting, and fine-tuned LLMs. We describe the first and third such settings here, and the second setting in section A.11. At each stage, we gradually make more assumptions and loosen more restrictions on our mathematical model as described in section A.2, which makes computation easier at the risk of increasing the number of confounding variables. Throughout the rest of the paper, we use $\\frac{P(USS)}{P(URS)}$ as a shorthand for the goal-directedness definition $G(\\pi_0) := \\frac{P(\\pi=\\pi_0|URS)}{P(\\pi=\\pi_0|UPS)}$, and similarly for $\\frac{P(USS)}{P(UPS)}$ and $\\frac{P(URS)}{P(UPS)}$.\nEstablishing ground truth for goal-directedness is challenging due to varying definitions. In this section, we provide provisional metrics and assess the classifier's performance through its loss and generalization across datasets. We find in our tests that these classifiers properly measure a consistent property that is goal-directedness and do not overfit to confounding variables, especially when we approximate our classifier training process from our toy MDP definition. When the classifiers successfully generalize and identify goal-directed behaviors, it offers preliminary evidence that our approximation of the ideal classifier training process effectively captures goal-directedness. Future work would verify this method on more datasets, architectures, and training regimes."}, {"title": "4.1 Markov Decision Process experiments", "content": "We now train a classifier of our goal-directedness metric under randomly generated MDPs with certain structural properties. Specifically, consider a deterministic MDP, such that each transition $T(s, a, s')$ has either probability 0 or 1, with guaranteed self-loops (i.e. for any s, there exists an action a such that $T(s, a, s) = 1$). As a case study, let $|S| = 10$, $|A| = 4$, and $\\gamma = 0.9$.\nWe use the Python MDP toolbox (MDP) to generate $10^4$ different MDPs and pick a $k \\sim U[1, |T|]$, then k rewards using URS. We then solve half of the MDPs to get half of our optimal policies, and randomize the other half, while labeling which were solved for and which were randomized. Then by default $P(USS|\\pi = \\pi_0) = P(UPS|\\pi = \\pi_0) = 0.5$. We use two classifier structures: a 3-layer, 64-width sequential neural network, and binary logistic regression. We then input certain features that intuitively seem relevant to the classifier, as defined in more detail in section A.9."}, {"title": "4.2 LLM experiments", "content": "To investigate whether the mathematical definitions of goal-directedness extend to large neural networks, we conducted experiments using the open-source LLaMA-2-7B model developed by Meta (Touvron et al. [2023b]). These experiments were designed to evaluate how well the concepts of sparse and dense reward functions, as described in the appendix (see Section A.2), generalize when applied to LLMs. These loss functions are designed to be analogous to sparse versus dense reward functions without strictly adhering to the specific structure of the MDP framework. This corresponds to the distinction found in our $\\frac{P(USS)}{P(URS)}$ metric. The exact definitions of these loss functions can be found in Section A.3.4.\nWe evaluated the performance of classifiers trained to predict our approximate $\\frac{P(URS)}{P(USS)}$, or activations from models trained on dense versus sparse loss functions, on two distinct datasets, the GSM8K (Cobbe et al. [2021]) and Orca (Mitra et al. [2024]) datasets, when applied to re-fine-tuned LLaMA-7B models (Touvron et al. [2023a]). To fine-tune these models, we use the LoRA method (Hu et al. [2021]) on all LLMs. If our $\\frac{P(USS)}{P(URS)}$ metric captures a real phenomenon and is not noise, then the classifier should be tractable to train and generalize to classify sparse loss function-trained models on other datasets.\nWe find that, although there is some noise in generalization, our goal-directedness classifier is able to separate sparse from dense loss function-trained models as having a higher probability of being sparsely trained across our tests, with a clear difference between (from high to low) the sparse 1-token,"}, {"title": "5 Discussion", "content": "In this work, we provide an intuitive definition of goal-directedness that is more tractable to compute than existing definitions, and a method to create a goal-directedness classifier. We first show that within an MDP setting, our definition is measurable and correlates with features associated with power-seeking. Features like a deterministic policy not taking self-loops and maximizing the number of out-arrows visited at each state tend to occur in optimal, power-seeking policies as shown by Turner et al. [2023a], and also occur in goal-directed policies according to our methods. We then adapt our methods to RL environments and again provide evidence that our definition is measurable in OpenAI Gym games (Brockman et al. [2016]).\nWe also provide a demonstration of our method on LLMs, where we conducted experiments to measure the classifier's ability to identify the LoRA activations from different levels of sparse and dense loss functions, as defined in Section A.3.4. Our preliminary results indicate that it is possible to achieve an accuracy greater than 70 percent for the dense loss function. As shown in Figure 2, when a model is fine-tuned on the loss signal from a specific dataset (GSM8K Cobbe et al. [2021] and the Orca Math dataset Mitra et al. [2024]), and a classifier is trained on a different dataset, the classifier's ability to accurately detect signals within the activations of the sparse or dense loss function increases with the training steps.\nWe propose that goal-directedness is a requirement for advanced AIs pursuing unintended goals, which could lead to catasrophic risk. In this paper, we reviewed the literature for definitions of goal-directedness and created our own preliminary definition that fits the intentional stance and is agnostic about the agent's internal structure, while also being computationally efficient. We experiment on building classifiers based on our definition in increasingly complex environments and find preliminary evidence that these classifiers are tractable and properly generalize. We strongly recommend future research in this direction, and we provide more details in our recommendations in the appendix A.14."}, {"title": "A Appendix", "content": "In this section, we provide detailed mathematical proofs, additional experimental details, and speculative statements that may be useful.\nIn this paper, we discuss goal-directedness to better understand and prevent worst-case risks from advanced AI systems. Understanding whether Als are goal-directed is crucial to evaluating models and possibly preventing misalignment risks like goal misgeneralization. This work could also elucidate which misalignment threat models depending on goal-directedness are more or less likely. While this work could hypothetically enable future AI developers to build more goal-directed models, increasing worst-case risks, we believe the benefits of understanding and measuring goal-directedness outweigh the possible downsides."}, {"title": "A.2 Implications on experimental setup", "content": "We use the model as described above for our MDP experiments in Section 4.1, sampling optimal policies for each strategy. However, in order to apply our model to more complicated settings listed in section 4.2, we adapt our model by changing some of the sampling methods. For instance, instead of using e-greedy policies, we \"sample\" policies trained on their respective reward functions for near-optimal policy sampling. We also lean on a more general notion of what it means to have \"sparse\" or \"dense\" reward in section 4.2. As such, we consider this model to give a general guideline for how to measure goal-directedness, rather than a strict formula. The key elements are optimality and generality, as measured by comparing URS to UPS, and sparsity of the reward function, as measured by comparing USS to URS."}, {"title": "A.3 Our mathematical model in more detail", "content": "For our mathematical definition, we work within the MDP (Markov Decision Process) framework (Puterman [1994]) with a set of states S, a set of possible actions A that can be selected in each state, and a transition function T(s, a) that returns a probability distribution over all states $s' \\in S$ (such that $T(s, a, s') \\in \\mathbb{R}$), but without a predefined reward function. Then, we can define a distribution from which we sample a reward function $R \\sim D$, and since R and the MDP are invariant across time-steps, we can define a (deterministic) policy $\\pi \\in [1,|A|]^{|S|}$ as a tuple of actions, one action to take for each state.\nIn this section, we will talk about deterministic MDPs and policies (as an example of environments and AIs, respectively). Whenever we talk about sampling from the space of policies, we will assume that this just samples uniformly from all combinations of discrete actions; we will call this uniform policy sampling (UPS). We sample rewards from some prior distribution $\\mathcal{D}$ (r; since optimal policies are invariant under scaling reward functions, we let $\\mathcal{D}_1 = U[-1,1]$ as an example. Thus let $\\mathcal{D}_{U, =} D_{U[-1,1]}-IID$ be the distribution of reward functions where each reward of each transition $R(s, a, s')$ is drawn uniformly and independently from U[-1,1]. We call this Uniform Reward Sampling (URS). Even under URS, some policies will be more coherent than others, because they will be optimal for more reward functions."}, {"title": "A.3.1 Estimation classifier", "content": "In order to estimate $P(\\pi = \\pi_0|URS)$, we first estimate the reverse. Specifically, consider a setting where we first flip a coin, then sample \u3160 using URS if it is heads, and UPS if it is tails. In this setting, we can train a binary classifier $P(URS|\\pi = \\pi_0)$ by generating two sets of URS and UPS policies and using these sets as training data. But by Bayes' theorem:\n$P(URS|\\pi = \\pi_0) = \\frac{P(\\pi = \\pi_0|URS)P(URS)}{P(\\pi = \\pi_0)} = \\frac{0.5P(\\pi = \\pi_0|URS)}{0.5P(\\pi = \\pi_0|URS) + 0.5P(\\pi = \\pi_0|UPS)} $\nRearranging gives:\n(2)\n$P(URS|\\pi = \\pi_0) = \\frac{P(\\pi = \\pi_0|URS)}{P(\\pi = \\pi_0|URS) + P(\\pi = \\pi_0|UPS)}$\n(3)\n$P(\\pi = \\pi_0|URS) = P(URS|\\pi = \\pi_0)P(\\pi = \\pi_0|URS) + P(URS|\\pi = \\pi_0)P(\\pi = \\pi_0|UPS)$ (4)\nAnd so: $P(\\pi = \\pi_0|URS)(1 - P(URS|\\pi = \\pi_0)) = P(URS|\\pi = \\pi_0)P(\\pi = \\pi_0|UPS)$\n$G(\\pi_0) = \\frac{P(\\pi=\\pi_0|URS)}{P(\\pi=\\pi_0|UPS)} = \\frac{P(URS|\\pi=\\pi_0)}{1-P(URS|\\pi=\\pi_0)}$\nTherefore,\nIn theory, the classifier learns to distinguish between more goal-directed and less goal-directed policies, leading to a goal-directedness classifier. Note that the correct classification of a policy may depend on the graph structure of the underlying MDP, in a way which is hard to capture with standard classifiers. Classifier architectures could include a graph neural network (Zhou et al. [2021]), a simple linear/logistic classifier using hand-crafted features, and/or with interpretability tools like linear probes in the case of an underlying neural network as the policy network."}, {"title": "A.3.2 Sparsity as simplicity", "content": "Intuitively speaking, we would still like to differentiate policies that are optimal for \"simple\" reward functions, to avoid convoluted, degenerate reward functions explaining a policy's behavior. In the context of MDPs, if we only need to specify rewards for a few states or transitions, that is much simpler than specifying rewards for every state or transition. Thus our version of \"simplicity\" in our model is the sparsity of the reward function.\nSpecifically, consider a new distribution over rewards: Uniform Sparsity Sampling (USS). As a simplification of some simplicity prior $s_r$, for an MDP with N state-action transitions, we set some value k, either via $k \\sim U[1, N]$ for section 4.1 or presetting $k = 0.01 \\cdot N$ for section 4.2. We then sample random rewards for k transitions (selected uniformly without replacement), and finally sample a policy which is optimal for that reward. All the equations work the same, meaning that:\n$G(\\pi_0) = \\frac{P(\\pi = \\pi_0|USS)}{P(\\pi = \\pi_0|UPS)} = \\frac{P(USS|\\pi = \\pi_0)}{1 - P(USS|\\pi = \\pi_0)}$ (5)\nWe can also define goal-directedness in this setting as $\\frac{P(\\pi=\\pi_0|URS)}{P(\\pi=\\pi_0|USS)}$, which intuitively represents how sparse the reward functions that a policy is optimal for is given that it is already \u201ccoherent\" to some degree. (This is also our latter definition of goal-directedness divided by our former definition.) Doing the same calculation as in Section A.3.1 also gives us $G(\\pi_0) = \\frac{P(USS|\\pi = \\pi_0)}{1 - P(USS|\\pi = \\pi_0)}$, except that the choice is between USS and URS instead of UPS. Another way of generating sparse policies is by sampling rewards from a high-variance distribution, and possibly discarding the ones which are below a given threshold.\nUnder our setup, $G(\\pi_0)$ ranges from 0 when $P(URS|\\pi = \\pi_0) = 0$ to $+\u221e$ when $P(URS|\\pi = \\pi_0) = 1$; the prior, not knowing anything specific about $\\pi_0$, is $P(URS|\\pi = \\pi_0) = 0.5$, implying $G(\\pi_0) = 1$. Policies that are optimal, or almost optimal, for a broader class of reward functions will have higher $P(\\pi = \\pi_0|URS)$ and thus higher goal-directedness."}, {"title": "A.3.3 Suboptimality", "content": "The current method only counts a policy if it is exactly optimal for a given reward function. But real-world agents will never be actually optimal for any non-trivial reward function. So if a policy is almost optimal for many reward functions, that should still count towards its goal-directedness.\nWe can therefore add another step. Instead of only sampling from optimal policies for a given reward function, we could first sample a value $e \\in (0, 1)$, then sample a policy which has expected reward within e of the expected reward of the optimal policy (e.g. by early stopping). Note that this can be combined with different possibilities for how to do simplicity-weighted reward sampling.\nTo recap, the methodology in this toy setting is:\nSample a value k which determines the simplicity of the reward function, or what percent of R(s) or $R(s, a, s') \u2260 0$, or otherwise have some simplicity prior \u015ar over the distribution of reward functions in the environment.\nChoose k N states or transitions to give reward to (where N is the total number of states or transitions).\nSample a reward function from $U[-1,1]^{IID}$, or generally from \u015ar, over these states/transitions.\nSample a value e controlling optimality.\nSample an e-greedy optimal policy.\nFor optimal policies, UPS consists of step 5 (for a zero reward function), URS consists of steps 3 and 5, and USS consists of steps 1-3 and 5. For suboptimality, add step 4."}, {"title": "A.3.4 Dense and Sparse Loss Function", "content": "The dense loss function used in our model is based on the Cross-Entropy Loss, which is commonly employed for classification tasks. The function is defined as follows:\n$\\begin{aligned}L_{\\text {dense }}(y, \\hat{y})=\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i, c} \\log (\\hat{y}_{i, c}),\\end{aligned}$\nwhere y represents the ground truth labels, \u0177 represents the predicted logits from the model, N is the number of samples, and C is the number of classes. In this context, $y_{i, c}$ is a binary indicator (0 or 1) if class label c is the correct classification for sample i, and $\\hat{y}_{i, c}$ is the predicted probability (logit) of sample i being in class c.\nThe sparse loss function is designed to focus on a subset of logits and labels, specifically a random subset of n tokens. This can be useful in scenarios where the model needs to pay particular attention to recent outputs. The function is defined as follows:\n$L_{\\text {sparse }}(y, \\hat{y}, n)=L_{\\mathrm{CE}}(\\hat{y}\\_{n}, y\\_{n}),$"}, {"title": "A.4 Speculation", "content": "In this section, we discuss speculative impacts of our work on AI alignment and AI safety.\nOne of the most difficult parts of AI alignment is that, if you already have a misaligned optimizer trying to deceive a training process (Hubinger et al. [2019], Carlsmith [2023]), it is very difficult to detect or align it since it is actively scheming against your techniques. A strong optimizer of a misaligned utility function would want to protect itself from being modified or being interpretable, possibly leading to steganography (de Witt et al. [2023]) to prevent interpretable natural language outputs, or phenomena like reward hacking (Ngo et al. [2022]). By contrast, if you are able to build a highly capable AI that is not goal-directed, perhaps using a dense reward signal, then you avoid these failure modes entirely.\nEven if this AI is not competitive with agentic AIs, it may make the alignment problem substantially easier. For instance, we can conceptualize the pre-training process of a transformer as providing a very dense reward signal: instead of simply providing a scalar update value, pre-training specifically updates a transformer towards certain completions, making it suitable for the bulk of compute used in the training of current LLMs. If this is true, then we can train LLMs to understand human values and ontology in the sense that it can follow directions and do what a user intends from natural language instructions alone, without it being goal-directed. Then, if we want to agentize the LLM through, for instance, RL, we do not need a reward signal that robustly describes human preferences in all situations; we only need to provide good enough reward or prompting for it to do what we want, since it already understands what we want it to do. This substantially simplifies the alignment problem (Barnett [2023])."}, {"title": "A.5 Further background", "content": "Another relevant definition has been written by Kosoy [2023] for the \"agency\u201d of a policy in her learning-theoretic agenda. Kosoy's definition generalizes our uniform reward prior to some Solomonoff prior (Solomonoff [1964]), only relying on the utility of a policy with respect to some utility function and simplicity priors ($\\mu$ and \u03be(which can be generalized to any prior over the space of policies and reals respectively). Rather than using sparsity over the environment, Kosoy weighs by the Kolmogorov complexity of the utility function (Kolmogorov [1998]). However, computing this definition requires taking the maximum of a function over all utility functions U and universal Turing machines M respectively. Meanwhile, Wentworth [2024] considers a policy coherent in the long-term if it does not contradict itself, i.e. if there exists a value function that fits the policy and satisfies the Bellman equations with zero payoff.\nOther definitions of goal-directed behavior place more emphasis on whether a model has agentic structure, such as a \"goal-slot\" for which a model optimizes directly (Hubinger et al. [2019]). Related to this is the idea that goals should be \"concepts\" in an agents' world-model (Adam Shimi [2021]), or that agents should share human-like agentic characteristics like self-awareness or planning (Ngo [2020], section 3.2). However, Ngo (Section 3.1) also discusses the problems with finding an explicit representation of a goal or a search algorithm in Hubinger et al. [2019], and in practice more"}, {"title": "A.6 Better baselines", "content": "One problem we might face in following the above strategy: what if it is computationally too easy to distinguish policies sampled via UPS from policies sampled via USS? If so, binary classifier values of $G(\\pi_0)$ might cluster near 0 or near 1, leading to numerical problems. In other words, for highly coherent policies, UPS is a very poor baseline to compare USS against. So what if we used a series of baselines for training classifiers instead? For example, we could calculate goal-directedness as:\n$G(\\pi_0) = \\frac{P(\\pi = \\pi_0|USS)}{P(\\pi = \\pi_0|UPS)}$ (6)\nThis would be useful given the assumption that URS is a good baseline for USS, and UPS is a good baseline for URS. We might also be interested in other sampling strategies which are, intuitively speaking, \"somewhere between\u201d USS and UPS. One possibility is uniform value sampling (UVS). By UVS I mean the following procedure:\nSample a random value function by assigning every state a value from U(-1,1).\nSample a random reward function which is consistent with that value function. (Note that a) there is some state-action reward function consistent with any value function; and b) for any given value function, most state-action reward functions are not consistent with it.)\nSample an optimal policy for that reward function.\nOne of the benefits of using UVS as an intermediate baseline is that knowing the value function makes it very easy to translate a reward function to an optimal policy. Another possible intermediate baseline is uniform trajectory sampling-sampling a given trajectory (or set of trajectories), then sampling a reward function consistent with that trajectory being optimal, then sampling an optimal policy for that reward function."}, {"title": "A.7 Mathematical equivalences", "content": "We compare our method to Orseau et al. [2018] via the following transformation. Suppose instead of uniform, IID reward functions (in our terminology) or utility functions (in their terminology), we weigh all utility functions u by some $w_u$. We can define $w_u := \\frac{1}{|U|}$ where $|U|$ is the number of utility functions if finite, or we can attach a speed prior by defining $w_u = 2^{-\\lambda l(u)}$ for some constant A where l(u) is the description length of the utility function according to some Turing-complete reference machine.\nThey then generalize from deterministic to probabilistic policies by defining $\\pi_{u, \\epsilon(Y,U)} \\epsilon, (y0|x0). They then obtain a mixture of probabilities via a weighted sum over all goals g:\n$M_{g}(y7 | x \\in 4 \\cdot x 5. Meanwhile, we calculate $M_{a}(y7 | x_0}$"}, {"title": "A.8 Miscellaneous theoretical arguments", "content": "One particular objection that some may have about our definition is that, even if coherent policies meaningfully tend to maximize reward functions, those reward functions may in practice be \"low-impact\", and thus not matter for AI risk. One example is the concept of a \"myopic\" AI, which is only goal-directed within a small time-frame, and hence cannot affect the world in ways we would consider dangerous. We give preliminary empirical evidence that coherent policies tend to pursue long-term reward (at least with a high enough discount rate, e.g. 0.9). We can also provide a loose argument that myopic policies will tend to have low goal-directedness.\nSuppose you have a policy \u3160 that is myopic at a state s. Then we can model the policy as taking the action a with the highest expected next-step reward $E_{s'\\in s}[R(s, a, s')]$, which given that the MDP is deterministic, equals some $R(s, a)$. If this policy is optimal for this reward function, then $R(s, a)$ will be very high, and there will be many policies that are also myopic in taking action a at state s, and are also optimal for R at s. But then $P(\\pi = \\pi_0|URS)$ will be low, as \u03c0 is only one of many policies taking the same action at s. Therefore, its goal-directedness will also be low; this argument works similarly for $P(\\pi = \\pi_0|USS)$.\n1-\u03b3\nOur methodology was also substantially inspired by Turner et al. [2023a], which studies the properties of optimal policies under MDPs. They find that certain properties and symmetries of an MDP lead to power-seeking behavior by optimal policies. Specifically, for any state s, discount rate \u03b3, and distribution of reward functions Dbound with some bounding conditions, then POWER is defined as\n$POWER_{D_{bound}}(s,\\gamma)=\\frac{1}{1-\\gamma}\\mathbb{E}_{R \\sim D\\_{\\text{bound}}}[V_{\\pi}\\^\\*(s,\\gamma)-R(s)]$"}, {"title": "A.9 MDP experiment terminology", "content": "(P). One \"brute force\" method is by joining the (tuple) optimal policy \u03c0\u03bf, flattened transition function, and discount rate into a 1-dimensional vector. This in theory contains all the infor-mation about the MDP and \u03c0\u03bf that we can provide, but in practice needs more processingbefore it can be classified. (Again, a more principled approach would likely involve somekind of graph neural network.)\n(LL). Given that \u03c0 is deterministic in a deterministic MDP, let $\u03c0(st) = st+1$ for all t\u2265 0.Then eventually $st1 = st2$ for some t1 and t2, at which point the policy will be in a \"loop\".Thus another possible set of features is, for every state so, measuring how long it takesfor the optimal policy \u03c0\u03bf to reach a loop when starting from so, and how long the loopitself is. We can think of optimal policies as implementing an explore and exploit dynamic:navigating to a particularly high-reward area of the MDP, and then looping through that areato maximize reward indefinitely. Intuitively, a policy that takes longer to reach a stable loopcan access more of the MDP and can thus reach higher-reward areas, while a policy thattakes a bigger loop can incorporate more reward into the loop."}, {"title": "A.10 Miscellaneous MDP results", "content": "We performed additional tests on goal-directedness. When we try to build a classifier for the $\\frac{P(USS)}{P(URS)}$ definition of goal-directedness, we find that our current classifier architectures and features are insufficient.\nLess structured MDPs, such as MDPs where the transition probability distribution for eachT(s, a) (for any state s and action a) were IID randomized via Dirichlet distribution, tendedto be harder to build a classifier for. Indeed, when we sampled from this set of MDPs,randomized the reward function $10^4$ times, and then calculated the optimal policy via valueor policy iteration for each reward function, we found that the resulting distribution ofoptimal policies was roughly uniform (the mode policy occurred 1-3 times), and did notbecome less uniform with increased sparsity. This would make it harder to distinguisthopt"}, {"title": "A.11 RL results", "content": "We now present experiments in a more complex environment where it is intractable for optimal policies to be found via value or policy iteration. We approximate sampling near-optimal policies via USS or URS by training policies on their respective reward functions. We then train a classifier using these policies 6 and run the classifier on test cases to test if it is measuring goal-directedness properly. The purpose of these experiments is to see whether our definition of goal-directedness can be measured cheaply and effectively in complex environments.\nIn the Taxi-v3 environment in OpenAI's Gym (Brockman et al. [2016", "1989": "and 50 trained similarly with reward functions from URS. We used state-based rewards for URS and USS, and a sparsity value of \u20ac = 0.99. We generated a policy greedy in value for each Q-table, then labeled each policy with which method it was generated by"}]}