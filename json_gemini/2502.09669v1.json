{"title": "Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation", "authors": ["Maizhe Yang", "Kaiyuan Tang", "Chaoli Wang"], "abstract": "Implicit neural representation (INR) has emerged as a promising solution for encoding volumetric data, offering continuous representations and seamless compatibility with the volume rendering pipeline. However, optimizing an INR network from randomly initialized parameters for each new volume is computationally inefficient, especially for large-scale time-varying or ensemble volumetric datasets where volumes share similar structural patterns but require independent training. To close this gap, we propose Meta-INR, a pretraining strategy adapted from meta-learning algorithms to learn initial INR parameters from partial observation of a volumetric dataset. Compared to training an INR from scratch, the learned initial parameters provide a strong prior that enhances INR generalizability, allowing significantly faster convergence with just a few gradient updates when adapting to a new volume and better interpretability when analyzing the parameters of the adapted INRs. We demonstrate that Meta-INR can effectively extract high-quality generalizable features that help encode unseen similar volume data across diverse datasets. Furthermore, we highlight its utility in tasks such as simulation parameter analysis and representative timestep selection. The code is available at https://github.com/spacefarers/MetaINR.", "sections": [{"title": "1 INTRODUCTION", "content": "Volume data representation is a long-standing theme for scientific computing and visualization. Recently, researchers have explored using implicit neural representation (INR) [17] for volume data representation [10, 5, 21, 20]. Using an architecture of multilayer perception (MLP), INR maps spatial coordinates to corresponding voxel values for volume fitting and stores the parameters of the MLP to represent the underlying volume. In this context, INR offers three advantages. First, the model size of a fully connected network is usually much smaller than the original volume, implying that INR can achieve highly compressive results. Second, INR can be naturally embedded into the volume rendering pipeline. A renderer can directly access the voxel values along a ray by inferring the trained network, eliminating the need to decode the entire volume beforehand. Third, once trained, an INR can achieve arbitrary-scale interpolation without accessing the original data, significantly enhancing the convenience of post-hoc analysis.\nDespite its effectiveness, the current INR network parameters optimized on one volume are not generalizable enough to be adapted to other unseen volumes. When users represent new simulation volume data using INR, the parameters of the trained INR cannot be directly reused to accelerate training on other volumes, which leads to inefficient encoding for time-varying or ensemble datasets. Inspired by meta-learning in neural networks [7], we propose Meta-INR, a pretraining strategy for INR that only uses partial observation of a volumetric dataset but enables rapid adaptation to unseen volumes with similar structures in a few gradient steps."}, {"title": "2 RELATED WORK", "content": "INR for scientific visualization. Using deep learning techniques for data representation [24] has been extensively studied recently. One solution uses INR, which inputs spatial coordinates and outputs corresponding voxel values to achieve the generation and reduction of scientific data. Typically, INR leverages the MLP architecture to represent volumetric data. For example, Lu et al. [10] compressed a single scalar field by optimizing an INR with weight quantization. Han and Wang [5] proposed CoordNet, a coordinate-based network to tackle diverse data and visualization generation tasks. Tang and Wang [21] presented STSR-INR, leveraging an INR to generate simultaneous spatiotemporal super-resolution for time-varying multivariate volumetric data. Han et al. [6] proposed KD-INR to handle large-scale time-varying volumetric data compression when volumes are only sequentially accessible during training. Tang and Wang [20] designed ECNR to achieve efficient time-varying data compression by combining INR with the Laplacian pyramid for multiscale fitting. Li and Shen [8] leveraged Gaussian distribution to model the probability distribution of an INR network to achieve efficient isosurface extraction.\nBesides the vanilla MLP architecture, multiple works integrate grid parameters into INR to achieve efficient encoding and rendering. Weiss et al. [25] implemented fV-SRN, achieving significant rendering speed gain over [10] using a volumetric latent grid. Wurster et al. [27] proposed APMGSRN, which uses multiple spatially adaptive feature grids to represent a large volume. Xiong et al. [28] designed MDSRN to simultaneously reconstruct the data and assess the reconstruction quality in one INR network. Tang and Wang [22] developed StyleRF-VolVis, leveraging a grid-based encoding INR to represent a volume rendering scene that supports various editings. Yao et al. [29] proposed ViSNeRF, utilizing a multidimensional INR representation for visualization synthesis of dynamic scenes, including changes of transfer functions, isovalues, timesteps, or simulation parameters. Gu et al. [4] and Lu et al. [9] presented NeRVI and FCNR, respectively, utilizing INRs for the effective compression of a large collection of visualization images.\nUnlike existing works that mainly focus on network architecture design, this paper aims to develop a pretraining strategy for optimizing the initial parameters of an INR network to enhance the representation generalizability."}, {"title": "Meta-learning", "content": "Meta-learning is a deep learning technique primarily aiming for few-shot learning [18]. Numerous recent studies have explored using it to optimize the initialization of neural networks, enabling them to adapt to new tasks with just a few steps of gradient descent. For instance, Sitzmann et al. [16] leveraged meta-learners to generalize INR across shapes. Tancik et al. [19] employed meta-learning to initialize INR network parameters according to the underlying class of represented signals. Emilien et al. [2] proposed COIN++, a neural compression framework that supports encoding various data modalities with a meta-learned base network. Similar to these works, we develop Meta-INR based on existing meta-learning algorithms [3, 11]. However, we focus on applying meta-learning in INR for volume data representation, setting it apart from existing studies."}, {"title": "3 META-INR", "content": "Meta-INR is a pretraining approach designed to optimize the initial parameters of an INR network for efficient finetuning on unseen volumes. The training pipeline of Meta-INR consists of two sequential stages: meta-pretraining and volume-specific finetuning.\n\u2022 The meta-pretraining stage optimizes a meta-model on a sparse subsampled volumetric dataset, utilizing less than 1% of the original data, to learn the initial parameters of an INR network that supports rapid adaptation to other unseen volumes within the dataset.\n\u2022 The volume-specific finetuning stage finetunes the initial parameters of the meta-model on a specific volume, resulting in a volume-specific adapted INR for high-fidelity volume reconstruction.\nThe adapted INR shares the same network architecture, \u03a6: R3 \u2192 R as the meta-model, which maps a 3D coordinate to the corresponding voxel value. Let \u0398m denote the parameters of the meta-model. For a volumetric dataset D = {d1,d2,...,dT} that contains T timesteps or ensembles, one volume di consists of a set of coordinate-value pairs (Ci, Vi), where C = {(x1, y1, z1), (x2, y1, z1),... } is a set of spatial coordinates and V = {V1, V2,...} is the corresponding voxel values at these positions. After meta-pretraining, we finetune \u0398m on each set of coordinate-value pairs in D independently, resulting in a series of volume-specific adapted INRs with parameters {\u03981, \u03982,..., \u0398T} that can represent each volume within the temporal or ensemble sequence."}, {"title": "3.1 Meta-Pretraining", "content": "The meta-pretraining stage optimizes the parameters of \u0398m, serving as the initial parameters in the volume-specific finetuning stage. Meta-pretraining is data efficient, requiring only a small portion of the data to derive sufficiently generalizable \u0398m. In this paper, we achieve this by spatiotemporally subsampling the original dataset D. Specifically, we subsample D using an interval of \u03bbs on each spatial dimension and \u03bbt on the temporal dimension, resulting in a downsampled dataset D.\nThe spatial subsampling interval \u03bbs = 4 and temporal subsampling interval \u03bbt = 2 are chosen empirically, balancing pretraining efficiency and quality of the prior. This configuration retains structural patterns for most datasets without significant accuracy loss. It further implies that only (43 \u00d7 2) \u00d7 100% \u2248 0.78% original data samples are used for pretraining.\nThen, we consider optimizing \u0398m as a meta-learning problem and propose to leverage a MAML-like algorithm [3]. In particular, we randomly initialize the meta-model parameters and iteratively update \u0398m through a nested inner and outer loop. In the inner loop, for each subsampled volumetric dataset di in D, we finetune a cloned set of parameters \u0398' using K gradient steps on randomly sampled batches of coordinate-value pairs (Ci, Vi). The inner-loop loss is computed as the mean squared error (MSE) between the predicted and ground-truth (GT) voxel values. After processing all volumes in D, the outer loop accumulates the gradients from the inner loop and updates \u0398m using the average gradient across the volumes. The optimization continues until \u0398m converges."}, {"title": "3.2 Volume-Specific Finetuning", "content": "Once the meta-pretraining stage finishes, we utilize \u0398m as initial parameters to finetune each adapted INR on a specific volume in D. The finetuning process mirrors the inner-loop adaptation during meta-pretraining, where the pre-trained initial parameters \u0398m are taken and updated in K gradient steps. The only difference is that we utilize all available data points in this stage instead of a subsampled version of the volumetric dataset. This ensures that the fine-tuned parameters follow a more accurate gradient descent direction from \u0398m, leading to improved reconstruction accuracy. After the volume-specific finetuning stage, we can obtain a series of volume-wise adapted INRs, each representing a specific volume within the target time-varying or ensemble sequence."}, {"title": "4 RESULTS AND DISCUSSION", "content": "4.1 Datasets, Training, Baselines, and Metrics\nDatasets and network training. We utilize the time-varying datasets, including the half-cylinder, ionization, Tangaroa, and vortex datasets, to evaluate the reconstruction accuracy of the Meta-INR compared with other baseline methods. The time-varying earthquake dataset is leveraged to showcase the interpretability of the parameters of adapted INRs under t-SNE projection. We demonstrate the ability of Meta-INR on simulation parameter analysis using the ensemble Nyx dataset with three simulation parameters: ho, \u03a9mM0, and \u03a9bB0. Meta-INR uses a seven-layer SIREN model [17] as its network backbone, with a hidden layer dimension of 256. It is trained across all experiments with 500 outer steps and sets the number of inner-loop steps K = 16. We set both inner and outer loop learning rates \u03b1 and \u03b2 to 0.0001 during meta-pretraining and 0.00001 during volume-specific finetuning. Both stages optimize the parameters with a batch size of 50,000 coordinate-value pairs for each iteration.\nBaselines. We compare Meta-INR with two baseline strategies:\n\u2022 SIREN [17] is the backbone network architecture of Meta-INR. Here, SIREN as a baseline method means training each INR network independently for each volume from scratch.\n\u2022 Pretrained SIREN is a vanilla pretraining baseline method that optimizes the initial parameters on the same subsampled dataset without leveraging meta-learning techniques (i.e., no inner-loop updating). After pretraining, we finetune the learned initial parameters using the same number of adaptation steps as the Meta-INR for fair comparisons."}, {"title": "Evaluation metrics.", "content": "We use three metrics to evaluate the reconstruction accuracy of Meta-INR and baseline methods. We utilize peak signal-to-noise ratio (PSNR) to measure the volume reconstruction accuracy, and learned perceptual image patch similarity (LPIPS) to evaluate rendered image qualities for volumes reconstructed by different methods. Chamfer distance (CD) is used to assess similarity at the surface level by calculating the average distance of isosurfaces."}, {"title": "4.2 Time-Varying Data Representation", "content": "Time-varying data representation directly evaluates the effectiveness of Meta-INR in speeding up model training and improving reconstruction fidelity. We consider all methods on four datasets with various dimensions.\nQuantitative comparison. In Meta-INR performs the best across the three quality metrics for all datasets. Although SIREN achieves decent quality, sometimes comparable to Meta-INR, the need to retrain a model from scratch for each volume implies that a significant encoding time is required for the model to converge. For Meta-INR, despite the model taking the majority of total time for meta-pretraining, only a few adaptation steps are needed for encoding, saving considerable time compared to SIREN. In particular, the encoding time of adapted INRs is 5.87\u00d7 faster on average across all datasets than SIREN, which trains each INR from scratch. Pretrained SIREN performs the worst in terms of the three quality metrics as it fails to capture the intrinsic patterns from partial observation of volumetric datasets and struggles to converge efficiently during encoding. Through diverse evaluation across multiple datasets varying in size and complexity, we found Meta-INR to be highly adaptable. Meta-INR's generalizability stems from its meta-learning framework, which uses inner loop adaptation during meta-pretraining that mimics the finetuning process, forcing parameters into a region where a small number of gradient steps can minimize task-specific loss. This design ensures the prior captures shared structural patterns while remaining sensitive to volume-specific variations.\nQualitative comparison. pixel-wise difference image is also provided on the bottom left to show the differences between each method and the GT. Pretrained SIREN performs the worst among all three methods, showing significant deviations from the GT. In many cases, it demonstrates block-like artifacts. Both SIREN and Meta-INR demonstrate excellent visual fidelity for the reconstructed volumes. Although they perform similar results in simple datasets such as vortex, Meta-INR can achieve significantly higher accuracy for complex ones with many details, such as Tangaroa. This is because such details are already captured in the meta-model, which serves as the prior for quick adaptation. Then, during volume-specific finetuning, only slight parameter adaptation is needed."}, {"title": "4.3 Representative Timestep Selection", "content": "We showcase the interpretability of Meta-INR by analyzing its ability on the representative timestep selection task of the earthquake dataset. When we use t-SNE [23] to project the learned parameters of each SIREN at each timestep to a 2D space and connect the points in the order of timesteps, the resulting projection view is meaningless and offers no interpretability. This is because of significantly increased noise and randomness in training, which leads to models finding drastically different local minima. In contrast, Meta-INR's volume-specific finetuning starts with a common prior, eliminating most noise and focusing on each volume's differences. In particular, the connected points of SIREN across different timesteps lack continuity, resulting in numerous sharp turnings. As SIREN encodes volumes at each timestep independently from scratch, their parameter representations fail to capture a smooth transition of the dataset along the time dimension. Unlike SIREN, the connected points are meaningful and smooth when leveraging t-SNE to project the parameters of adapted INRs finetuned on each timestep. We select representative timesteps following [13] and observe reasonable results. Moreover, we can see that the connected points of Meta-INR are smoother than the results obtained by [13] using an autoencoder. This difference arises from the need to downsample the data before training the autoencoder. Unlike Meta-INR, the network architecture of the autoencoder is constrained by volume dimensions and cannot directly process high-resolution volumetric data. Therefore, the connected points for the projections of the autoencoder are less smooth due to the noise introduced from downsampling."}, {"title": "4.4 Simulation Parameter Analysis", "content": "When analyzing ensemble datasets, it is often insightful to see how changes in each parameter affect the simulation. Meta-INR can aid in this process by effectively visualizing the relative differences between each volume via t-SNE projection. We apply Meta-INR to the Nyx dataset with three simulation parameters, ho, \u03a9mM0, and \u03a9bB0. We perform meta-pretraining on the subsampled Nyx dataset, which is downsampled along the spatial and ensemble dimensions, and then conduct volume-specific finetuning to fit all volumes corresponding to different simulation parameters. We visualize the pattern in model parameters using t-SNE and mark t-SNE projections sharing the same parameter values. we highlight all projections with ho equal to 550,000 and 700,000, respectively. We can see that ho = 550,000 corresponds to projections centralized on the left side, and the projections of ho = 700,000 are centralized at the bottom of the plot. Similarly, in  and , which mark projections with \u03a9mM0 equal 120,000 and 155,000, respectively, we observe that \u03a9mM0 = 120,000 corresponds to projections located at the outer region of the plot. On the other hand, \u03a9mM0 = 155,000 corresponds to projections close to the inner region. These results show that the parameters of adapted INRs assimilate information about the simulation parameters during the encoding process."}, {"title": "5 CONCLUSIONS AND FUTURE WORK", "content": "We have presented Meta-INR, a pretraining method designed to optimize a meta-model that can adapt to unseen volume data efficiently. The generalizability of the meta-model allows for fast convergence during volume-specific finetuning while retaining the model interpretability within its parameters. The evaluation of various volumetric data representation tasks demonstrates better quantitative and qualitative performance of the meta-pretraining than other training strategies.\nFor future work, we would like to explore continual learning to improve the capabilities of the meta-model for handling more complex time-varying volume data with significantly larger timesteps and variations. Moreover, we want to investigate meta-pretraining on grid-based INRs such as fV-SRN [25] or APMGSRN [27]. Finally, we plan to incorporate transfer learning techniques to learn potentially more challenging variations across variables for multivariate datasets, allowing a meta-model trained on one variable sequence to be effectively used in finetuning another."}]}