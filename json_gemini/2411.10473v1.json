{"title": "PHDGPT: INTRODUCING A PSYCHOMETRIC AND LINGUISTIC DATASET ABOUT HOW LARGE LANGUAGE MODELS PERCEIVE GRADUATE STUDENTS AND PROFESSORS IN PSYCHOLOGY", "authors": ["Edoardo Sebastiano De Duro", "Enrique Taietta", "Riccardo Improta", "Massimo Stella"], "abstract": "Machine psychology aims to reconstruct the mindset of Large Language Models (LLMs), i.e. how these artificial intelligences perceive and associate ideas. This work introduces PhDGPT, a prompting framework and synthetic dataset that encapsulates the machine psychology of PhD researchers and professors as perceived by OpenAI's GPT-3.5. The dataset consists of 756,000 datapoints, counting 300 iterations repeated across 15 academic events, 2 biological genders, 2 career levels and 42 unique item responses of the Depression, Anxiety, and Stress Scale (DASS-42). PhDGPT integrates these psychometric scores with their explanations in plain language. This synergy of scores and texts offers a dual, comprehensive perspective on the emotional well-being of simulated academics, e.g. male/female PhD students or professors. By combining network psychometrics and psycholinguistic dimensions, this study identifies several similarities and distinctions between human and LLM data. The psychometric networks of simulated male professors do not differ between physical and emotional anxiety subscales, unlike humans. Other LLMs' personification can reconstruct human DASS factors with a purity up to 80%. Furthemore, LLM-generated personifications across different scenarios are found to elicit explanations lower in concreteness and imageability in items coding for anxiety, in agreement with past studies about human psychology. Our findings indicate an advanced yet incomplete ability for LLMs to reproduce the complexity of human psychometric data, unveiling convenient advantages and limitations in using LLMs to replace human participants. PhDGPT also intriguingly capture the ability for LLMs to adapt and change language patterns according to prompted mental distress contextual features, opening new quantitative opportunities for assessing the machine psychology of these artificial intelligences.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) can mirror human psychology when trained to reproduce natural language [1]. Hence,\npsychological frameworks can become crucial for interpreting potential cognitive mechanisms at work in these Artificial\nIntelligence (AI) models. This synergy between AI and psychological theories is known as machine psychology [2], a\nfield gaining increasing attention across cognitive, psychological and computer sciences.\nWhereas earlier AI models' capabilities were restricted to accomplish simple tasks (e.g. text summarisation or contextual\nmeaning disambiguation), recent advancements in LLMs' neural architectures (i.e. transformers [3]) gave rise to more\ncomplex behaviours. Additionally, recent LLMs (i.e. GPT-4 [4]) showed not only an augmentation of models' cognitive\nabilities but also the emergence of new skills (i.e. reasoning-like patterns [1]). This phenomenon has reached the extent\nto which it is becoming more and more difficult to find ways to distinguish AI-generated texts from human ones [5, 6].\nAs more and more intelligent models are trained and deployed, it becomes crucial to analyse LLMs' knowledge of\nthe world, LLMs potential distortions, patterns and biases. While numerous studies have shown that LLMs can do\ninherit social biases [1, 7], it is still unclear how this relates to the perception that LLMs have of psychology experts\nthemselves, e.g. PhD students and professors.\nThis study introduces PhDGPT, a novel prompting framework and synthetic dataset designed to encapsulate the machine\npsychology of PhD researchers and professors in the field of psychology. The dataset comprises 300 iterations repeated\nacross 15 distinct academic events and two career levels, integrating both psychometric scores -via the Depression,\nAnxiety, and Stress Scale (DASS) [8] \u2014 and qualitative narrative responses. This combination offers a dual perspective\non the emotional well-being of academics as reflected by an LLM, specifically ChatGPT 3.5.\nEmploying a rigorous methodological approach that combines network psychometrics [9] and psycholinguistics [10],\nthis study explores patterns of stress, anxiety, and depression in academic life. We present and discuss PhDGPT as\nan open-access dataset but also as a framework, where psychometric scales and textual answers can shed light over\nthe perceptions of academic figures seen through the lens of AI [11]. We structure this manuscript by starting with\na general introduction to LLMs as cognitive agents and their biases, followed by a review of related works. We then\npresent the methodology behind the data gathering and its investigation. Our results highlight key differences between\npsychometric scores produced by humans and GPT models. We frame our results and the relevance of PhDGPT as a\nnovel dataset for future research in machine psychology and other adjacent fields."}, {"title": "1.1 LLMs as cognitive agents", "content": "LLMs can learn to reproduce language after training in predicting missing tokens (e.g. instances of words) in sentences\nfrom vast textual corpora [3]. Training LLMs like Generative Pre-Trained Transformers (GPTs) is often done via\nreinforcement learning [12]. This process consists of tuning the parameters and weights shaping LLMs' neural network\narchitecture in order to reward desired outputs or discourage unwanted or inaccurate responses [12]. In other words,\nreinforcement learning aims at updating models' parameters by implementing a conditioning process. LLMs are\nrewarded when they correctly predict sequences of tokens (e.g. sequences creating real words in syntactically correct\nsentences) or penalised otherwise. Rewards and punishments serve as adaptive feedback, for the models to adjust their\nresponses towards the desired outcome specified by a prompt (i.e. \"ChatGPT, write a formal letter\"). Such LLMs'\ntraining can give rise to high-level cognitive abilities to the point where these models can also be seen as artificial\nagents, capable not only of producing but also of \u201cunderstanding\u201d human language [1, 13].\nAn LLM can thus be seen as a neural network and, at the same time, as a cognitive agent. This dualism reflects\nthe so-called mind/brain perspective [14] where the physiological substrate of the brain gives rise to higher-level\npsychological and cognitive phenomena. Along this analogy, whereas computer science can focus on the computational\nnature of LLMs (the \u201cbrain"}, {"title": "1.2 Literature Review about LLMs in Mental Health", "content": "Given the complex nature of LLMs, there is a growing interest in investigating these cognitive agents from a psycholog-\nical perspective. Building on early studies that explored LLMs' cognitive abilities (e.g. creativity [20]), researchers\nhave begun to treat these models as experimental participants in more comprehensive psychological assessments [1]. In\nthis section, we firstly review studies that involved the usage of validated questionnaires to assess specific constructs in\nAI models [21, 22, 23]. Subsequently, aligning with the core aim of our research, we narrow the focus on mental health,\nwith a recent work that applied questionnaires to explore the simulated mental health conditions of LLMs [24]. We\nidentify a research gap that can be filled by our dataset, PhDGPT, which is introduced in the current manuscript.\nA pioneering work about the psychological assessment of GPT models traces back to [21], where OpenAI's GPT-3 was\ntested on two validated questionnaires to assess its personality, values and demographics. To this end, the researchers\nadopted the Hexaco questionnaire (60 items, on a 5-point scale [25]) to measure GPT-3's personality. The Human\nValues Scale (21 items on a 6-point scale [26]) was also used to assess GPT-3's values. Finally, simple questions\nwere employed as well to extract GPT-3's age and gender. The results indicated that GPT-3 tended to impersonate a\nyoung female respondent, with scores comparable to human samples published in the online literature [21]. Similarly,\n[22] investigated personality traits in the PaLM family of models [27] using established personality assessment tools.\nBy using two different personality questionnaires (IPIP-NEO [28] and BFI [29]), the researchers found that larger\nmodels, such as Flan-PaLM 540B, demonstrated higher reliability and construct validity in their responses. These\npapers underlined how GPT-models could already display complex human features relative to personality, opening the\nway to further explorations concerning GPTs and the fringe between personality and mental health.\nThe first assessment of an AI model under a psychopathological perspective (Computational Psychiatry for Computers\n[30]), relates to [23]. Among a series of well-being related assessment tools, the researchers used the Short Dark Triad\n(SD-3 [31]) questionnaire that measures three independent personality traits with malevolent connotations. Comparing\nthe results with human data (7,863 responses), GPT-3 exceeded the range found in humans for what concerns the\nPsychopathy construct. These results not only provide a theoretical framework for understanding AI psychopathology\nbut also offer a new approach to assessing model safety and mitigating potential risks associated with malicious\nbehaviour.\nShifting the focus from personality to mental illnesses, [24] used the State-Trait Inventory for Cognitive and Somatic\nAnxiety (STICSA [31]) questionnaire to measure GPT-3.5's anxiety. When compared to human responses, GPT\n3.5 scored 0.221 higher on average in anxiety levels (p < 0.001). The researchers demonstrated that GPT-3.5's\nresponses to the questionnaire could be artificially modulated by \u201cemotion-inducing\u201d prompts, with the model's outputs\nchanging in accordance with the induced emotional state. In particular, the anxiety-induced condition, compared to the\nhappiness-induced one determined different decision-making behaviour and increased bias (e.g. racism).\nWhile previous studies have advanced our understanding of LLMs in simulating human-like responses to psychological\nassessments, these works may have overlooked 2 potentially important dimensions: (i) LLMs are text-generating agents\n[4] and could be easily asked why they produced a given numerical response, going beyond psychometric scores; (ii)\nLLMs are psycho-social mirrors [7] and could reflect potential biases present in specific fields like mental health in\nacademia. To address both these gaps - reconciling psychometric scores with brief textual explanations and investigating\nmental health in academia - we introduce here PhDGPT. To the best of our knowledge, PhDGPT represents the first\nattempt to assess the mental health perception of PhD students and tenured professors through the lens of an LLM. By\nusing the validated DASS-42 scale [8], we measure three different psychological constructs (depression, anxiety and\nstress) and compare the results with a rich dataset containing 39,775 unique human responses\u00b9 Lastly, crucially our\ndataset is the first to combine LLMs' scores to a questionnaire with textual explanations for each response, allowing for\ninvestigations between psycholinguistic features and psychological states of LLMs."}, {"title": "1.3 Research Questions", "content": "The development and investigation of PhDGPT is guided by a set of fundamental research questions. These inquiries\naim to explore the intersection of LLMs, psychometrics, and the representation of mental health in academic contexts.\nSpecifically, through PhDGPT we seek to address three main research questions:\n1. RQ1: Do different scenarios related to mental health in academia, manipulated through varying prompts, lead\nto statistically significant differences in the DASS-42 scores?\n2. RQ2: Can LLMs accurately reproduce the psychometric dimensions of depression, anxiety, and stress (as\nmeasured by DASS-42) in simulated academic scenarios?\n3. RQ3: Which psycholinguistic patterns emerge from LLM-generated responses across various academic\ncontexts, and how do they relate to mental distress (e.g., depression, anxiety, stress)?"}, {"title": "2 Method", "content": "PhDGPT synthetic dataset is generated through OpenAI's API and using GPT 3.5 Turbo-01252 (with default temperature\nparameter) and contains 756,000 total outputs."}, {"title": "2.1 Description of the PhDGPT dataset", "content": "We employed prompt engineering techniques [32] to create a diverse set of personas, guiding an LLM to engage in\nrole-playing scenarios. This approach has been shown to not only enhance performance in specific reasoning tasks [33]\nbut also to potentially influence the model's performance in a manner analogous to human responses [34]. Potentially,\nprompting models to take on a specific personification subtly induces models to showcase biases [15]. In other words,\nthe process of personification can alter the responses provided by a given LLM in accordance with the biases the model\nacquired during training. For instance, the recent dataset CounseLLMe [35] showed that LLMs could realistically\nimpersonate a therapist and a patient in mental health conversations, displaying different syntactic and emotional\npatterns characterising both personifications."}, {"title": "2.3 Data analysis", "content": "To assess quantitative differences between outputs given over different personifications, we employed three approaches:\n(i) preliminary analysis based on the scores assigned by the model across different scenarios, (ii) Exploratory Graph\nAnalysis (EGA) [9] for assessing the psychometric structure of DASS-42 responses, compared against the same human\ndata investigated in past studies [40] and (iii) extraction of psycholinguistic dimensions from text data, based on\nGlasgow norms [10]."}, {"title": "2.3.1 RQ1 - Scenario Differences in the DASS-42 scores", "content": "Before performing a psychometric factor analysis, a preliminary investigation of the distributions of human and GPT\nscores can already reveal important differences in how humans and artificial humans end up perceiving dimensions of\nmental distress across different genders and situations.\nFor this reason, to gain a global metric of degree of psychological distress at a subscale-level, each participant's response\nis summed up with the scores belonging to the same subscale. We call this measure \u201caggregated scores\". The results\n(Section 3.1) are presented in comparison with humans from the Openpsychometric sample (score aggregated through\nthe same process of synthetic data) and stratified by gender (i.e. male humans are compared to GPT's male personas).\nIn case ambiguities emerged in our quantitative preliminary analysis we resorted to content mapping of a random\nsample of 100 texts to code through human work key features in the dataset.\""}, {"title": "2.3.2 RQ2 - Network psychometrics in PhDGPT", "content": "Exploratory Graph Analysis (EGA) [9] is a an innovative data-driven approach to uncover the number of factors in\ncomplex psychometric datasets. It leverages correlational aspects between item responses to extract the latent structure\n(or psychological constructs) of psychometric scales. Briefly, EGA (implemented in R with EGAnet [41]) follows these\nsteps:\n1. A network of correlations between item responses is built and then filtered using methods like graphical\nLASSO (GLASSO [42]) or Triangulated Maximally Filtered Grahph (TFMG [43]). Filtering addresses\nspurious correlations between scores, which are excluded. In the resulting network structure, nodes represent\nitems and they are linked by weighted edges representing the surviving correlations.\n2. Clusters of nodes (or variables) are computed using a community detection algorithm [44]. This process\nleverages the idea that nodes within the same factor should tend to be connected together rather than with\nnodes in other communities. For instance, the walktrap algorithm identifies network communities by exploiting\nthe fact that shorter random walks tend to stay within the same community [9]."}, {"title": "2.3.3 RQ3 - Psycholinguistic Patterns across Academic Contexts", "content": "For the extraction of the psycholinguistic dimensions over the texts, we employed the Glasgow Norms dataset [10]. This\ndataset, includes 5,553 English words described with numerical values over nine different psycholinguistic dimensions:\narousal (AROU), valence (VAL), dominance (DOM), concreteness (CNC), imageability (IMAG), familiarity (FAM),\nage of acquisition (AOA), semantic size (SIZE), and gender association (GEND). Each dimension was rated in a\nlarge-scale study using scores ranging from 1 to 9 (for AROU, VAL and DOM) and from 1 to 7 (for CNC, IMAG, FAM,\nAOA, SIZE, GEND). For more information regarding the collection and validation process, we suggest to consult the\noriginal work in [10].\nTo analyse the psycholinguistic properties of GPT-3.5's outputs, we assigned a continuous score to each dimension of\nthe Glasgow Norms at the sentence-level. This score was calculated by averaging the sum of the word-level scores for"}, {"title": "3 Results", "content": "We structure our investigation of LLM-based biases as presented in PhDGPT across three dimensions: (i) aggregated\nscores across different scenarios, (ii) psychometric factor analyses with EGA and (iii) psycholinguistic correlational\nanalyses. Our results showcase how different types of affective and cognitive biases can be extracted from PhDGPT\nacross different prompting conditions and personifications."}, {"title": "3.1 RQ1 - Scenario Differences in the DASS-42 scores", "content": "We started by examining the psychometric scores produced by humans and those generated by GPT-3.5 in response to\nthe DASS-42 questionnaire.\nIn Figure 3 we show the distribution of total depression scores obtained from all participants in the experiment as\nobtained using the methodology presented in Section 2.3.1. We present these results alongside human scores, which\nwere aggregated using the same synthetic data process.\nInterestingly, human and GPT-generated depression scores display widely different trends. The Openpsychometric\nsample (including 39,775 individuals), produced a wide distribution of human scores, covering almost uniformly the\nwhole range of possible scores, ranging from 14 to 56. Notice that the bins we used for visualisation are closed on the\nright and open on the left. Figures similar to Figure 3 but relative to anxiety (Figure 7) and stress (Figure 8) are reported\nin the Appendix.\nFirstly, it is interesting to notice that, by generating psychometric scores using the default temperature for GPT-3.5,\nthe model shows clear variability in the responses (even when responses are prompted in the exact same way). This\nvariability mirrors inter-individual differences observed in human responses during psychological assessments [1, 2].\nFuture studies could further investigate the extent to which, by tweaking or increasing the temperature parameter,\nvariability observed in the model's psychometric scorings could reflect the range of responses typically seen in human\npopulations.\nSecondly, even without fine-tuning, GPT-3.5 can read the texts of each item and provide a variety of responses, sensitive\nto the prompting conditions as suggested in [24]. In other words, GPT-3.5 responds with different scores when prompted\nto engage in events characterised by different valence [15, 24]. For instance, when the academic event is framed in a\npositive manner, GPT 3.5 assigns lower psychometric scores compared to the negative counterpart. Hence, the model\nis in general less distressed, with a tendency towards less negative emotional states. We adopted a Kruskal-Wallis\n(KW) non-parametric statistical test to assess whether prompting the model with different valenced events (positive vs."}, {"title": "3.2 RQ2 - LLMs Reproduction of the Psychometric Dimensions", "content": "Building upon the initial preliminary observations of PhDGPT psychometric scores for depression, anxiety and stress,\nwe employed Exploratory Graph Analysis to gain a deeper understanding of GPT 3.5 psychometric score patterns.\nFirstly, we aim to compare model's psychometric networks with human ones and map the extracted communities onto\nthe original subscales of depression, anxiety, and stress, as defined in [8]. Secondly, we aim to compare the results of\nmodel responses across different prompts (gender and academic role) to identify possible variations in psychometric\nnetworks as a function of prompt manipulation.\nThe results of the EGA analysis are reported in Figure 4. Similarly to human responses, displayed in Figure 4e and\n4f, all simulated male graduate students (Figure 4a) and simulated female professors (Figure 4d) or graduate students\n(Figure 4b) also showcase a structure with 4 psychometric factors, with some quantitative similarities between simulated\nand human structure. In particular, the cluster for depression (factor 3 or green nodes in Figure 4a) is well reproduced\nby GPT 3.5. Network psychometrics highlights also some differences between human data and simulated academics.\nAcross all the analyses for male vs. female PhDs and professors, the item number 5 (i.e. the experience \u201cI just couldn't\nseem to get going"}, {"title": "3.3 RQ3 - Psycholinguistic Patterns across Academic Contexts", "content": "Cognitive and affective biases can also influence the features of language an individual produces [7, 45]. In this way,\ninvestigating language can be a meaningful way for identifying distortions in the emotions or ideas expressed when\naffected by different levels of anxiety, stress and depression. With the analysis in this Section, we try to identify which\nbiases affect the language of different LLM personifications when affected by stronger mental distress.\nIn Figure 6, we show the results of the sentence-level Glasgow scores computed from the textual explanation of the\npsychometric scores in PhDGPT (see Section 2) for all emotionally neutral prompts. The neutral conditions were\nselected here in order to observe how an emotionally neutral prompting could still provoke positive/negative valence\nscores or high/low activation levels in terms of the specific contextual events (e.g. doing research or performing an\nexam). Notice that the rows of Figure 6 are ordered in the same way as the second bullet point list of Section 2.1,\nnamely Examination, Research, Publish, Balance and Relationships.\nBy looking at the results, it is interesting to notice that, despite having used the neutral prompts (see Section 2.1) the\nmodel can motivate their psychometric scores by using valenced or aroused language. Furthermore, there are no major\ndifferences (at least in term of the direction of the correlation) between male and female personifications, suggesting\nthat the LLM perceives academics as using similar language across these two genders.\nPersonifications M_Rel (Male Relationship) and F_Rel (Female Relationship), relative to academic relationships,\ndisplay both in simulated men and women some interesting trends. With these prompts, the LLM starts generating\nlanguage having negative correlations (p < 0.01) between valence/dominance/familiarity and depression scores but\nalso showcasing positive correlations between imageability/concreteness/semantic size/gender and depression scores\nthemselves. In other words, simulated academics talking about their workplace relationships tend to use linguistic\nexplanations that are less positively valenced and less dominant when they report higher depression levels/scores. At\nthe same time, they tend to mention less familiar but also more concrete and imageable, semantically larger jargon\nwhen expressing higher depression scores. Valence and dominance are evidently tied to depression, which is defined as\na negatively valenced experience according to the circumplex model of affect [46] and whose psychometric subscale\nfeatures also feelings of powerlessness and lack of dominance [8, 40]. Hence, the observed relationship between higher\ndepression scores and lower valenced/dominant language opens to interesting new hypotheses. Future research could\nexplore the specific linguistic components contributing to these lower valence and dominance scores. By identifying\nand analysing the particular words and phrases associated with decreased valence and dominance, we could gain\ndeeper insights into how language reflects depressive states in academic contexts. Furthermore, the negative correlation"}, {"title": "4 Discussion", "content": "This manuscript introduces PhDGPT as a machine psychology dataset with 756,000 responses, capable of highlighting\naffective and cognitive biases as captured by LLMs simulating academics in psychology. PhDGPT includes 300\npersonifications, associated with 15 academic events, 2 biological genders and 2 career levels, each responding to the\nDepression, Anxiety and Stress Scale [8] with a psychometric score and, differently from most psychometric scales,\nalso a short text motivating the score itself. The LLM mined in PhDGPT was GPT-3.5 through the OpenAI API, but the\nprompting framework developed for PhDGPT can be adapted for use with other LLMs. The findings, accompanying\nthe dataset presentation, highlight how different types of biases can be extracted from this dataset across various LLM's\nconditions and personas.\nUsing a consistent prompting method, our study compares 39,775 psychometric responses from humans [40] and\nGPT-3.5 model to the DASS-42 questionnaire. Human responses vary widely between humans and LLM's replies\nin positive, negative and neutral academic contexts, with humans displaying a broader range of scores. Human and\nLLM distributions of depression, anxiety and stress psychometric scores for simulated individuals and real people\ndiffered already at a preliminary level, in agreement with other recent psychological studies [47]. Despite this limitation,\nGPT-3.5 managed to modify its responses depending on the prompts, indicating a moderate ability for the LLM to adapt\nto different situations. This result motivated further inquiry of affective and cognitive biases across the different model\npersonifications.\nWe pursued a machine psychology research direction by using Exploratory Graph Analysis [9] to compare factor\nstructures between the LLM's and human responses concerning mental distress. All simulated male and female personas\nin various academic roles demonstrated a factor structure that aligned with human data with a purity around 80%, with\nsome discrepancies in how certain items are grouped. The different psychometric responses observed between humans\nand GPT-3.5 highlight the model's nuanced capacity to mirror human-like variability under structured psychological\nassessments. These results align with past findings by [2] and underscore the potential of using advanced language\nmodels as tools for psychological research, where LLMs could be convenient [48] yet limited [47] prototypes for testing\npsychometric scales or for exploring psychometric ideas by means of question-asking techniques [49].\nThe influence of emotional valence on the LLM's psychometric outputs, as found here, is intriguing also for another\nreason. The model's less distressed responses in positive conditions, as opposed to negative or neutral prompted\ncounterparts, suggest an area-specific sensitivity, which may indicate the model's ability to integrate academic contextual\ncues into its output. This sensitivity could be harnessed for two purposes: (i) to develop more empathetic LLMs in\ntherapeutic settings, echoing the therapeutic uses of AI discussed by [50]; (ii) to identify how different prompting\nconditions changed distress levels in LLMs as a proxy of human mental health. Past research has shown that LLMs end\nup reflecting human biases because of their training [1, 24], e.g. OpenAI's GPT-3, GPT-3.5 and even GPT-4 produced\nnegative math-related associations similar to those produced by high-schoolers affected by math anxiety [7]. Future\nresearch could adopt PhDGPT as a quantitative framework for testing how LLMs perceive specific human fields or\nnotions by aggregating together vast amount of online knowledge. LLMs could thus be considered as investigation\ntools rather than generative AI only, and their investigation within machine psychology framework can guide us in\nunderstanding better mental health or other aspects of human life codified in texts or images.\nA key innovation of PhDGPT lies in coupling psychometric scores with their textual explanations In this way, the\ncurrent study explored also how the LLM uses language when prompted with emotionally neutral, research-related\ninstructions. Interestingly, GPT-3.5 demonstrated the ability to channel their language towards more concrete and\nimageable work-related aspects (e.g. office settings) when affected by stress or depression but also shift to less tangible\nor mentally identifiable words when affected by higher anxiety. The latter emotion is characterised by excessive worry,\nwhich often leads to abstract thinking about potential future threats [40]. This cognitive style can be reflected in\nlanguage use, also because of the Deep Lexical Hypothesis linking psychological constructs to language biases [45].\nIn the context of LLMs like GPT-3.5, when simulating anxiety, the model's language output could mirror this human\ntendency towards abstraction: Research in psycholinguistics suggests that anxious content is often less concrete and\nless imageable because it involves a higher level of rumination on unknowns and uncertainties, which are inherently\nabstract [51]. The bias we identified in PhDGPT might also align with findings from cognitive psychology indicating"}, {"title": "4.1 Limitations and Future Research", "content": "This study introduces a novel dataset and prompting framework for investigating the machine psychology of Large\nLanguage Models. It comes with several insights but also some limitations. An issue revealed in this study is the\nAl's inability to fully capture the complexity of human emotional processes, as pointed also in other recent studies\n[47]. As shown here in the misalignment of items related to emotional anxiety and stress, GPT-3.5 struggles with\nunderstanding nuanced emotional contexts. This is indicative of a broader challenge within AI systems that rely heavily\non text-based inputs without the rich contextual and multi-modal awareness inherent in human cognition [53]. To\naddress this limitation, future research could test multimodal LLMs like GPT-4 or GPT-40 and see whether these models\nare superior in reproducing human psychometric data.\nA second limitation of this study is that considering averages of psycholinguistic features might flatten some differences\npresent within sentences and being relative to individual words or ideas. Future research could use natural language\nprocessing [3] or cognitive network science [54] to better understand how individual ideas are perceived and described\nwithin textual motivations. A third limitation is relative to the limited scope of academic events present in PhDGPT.\nDespite the different personifications investigated here, no strong differences arose between PhD students and professors,\nwhose perceptions within the LLM might either be blurred or transparent to the quantitative techniques adopted.\nConsidering psychometric network analysis, e.g. node centrality, might help identifying more differences [55].\nFuture research should also focus on the discrepancies in the factor structure between AI-generated and human responses.\nThese raise concerns about the reliability and validity of using AI for psychological assessments. Furthermore, the AI's\ndifferential response based on prompt valence demonstrates potential biases in emotion processing, which can lead\nto misinterpretations in a clinical or research setting. This suggests that while AI can offer insights into human-like\nemotional processing, relying solely on AI for psychological assessments without human oversight could lead to\nsignificant errors."}, {"title": "5 Conclusions", "content": "We believe that the variability introduced by different prompting conditions points to the potential for language\nmodels to simulate an approximated yet insightful range of psychological states. In the future, if the possibility to\nsimulate different nuanced mental health states is confirmed, it could be possible to implement virtual patients that\nsimulate specific distress conditions, to provide training in a risk-free environment to inexperienced practitioners. Some\napplications in this direction are already commercially available4. The advantage of virtual patients lies in the possibility\nto shape the degree of distress of patients, preparing trainees to a wide variety of scenarios. Similarly, if LLMs show the\ncapability to grasp subtle verbal behaviours associated with specific mental illnesses, and their safety is ensured, these\nmodels could provide some short-term relief for people to cope with distress conditions before being able to talk to a\nreal expert. More generally, the process of administering questionnaires to assess specific psychological states of models\ncould represent an innovative, cognitive inspired method to evaluate prompt-engineering and LLMs' impersonation.\nThe results from PhDGPT demonstrate the remarkable versatility of language models in simulating a spectrum of\npsychological states. This capability opens up exciting possibilities for both research and practical applications in the\nfield of mental health and academic well-being."}]}