{"title": "PLANNING ANYTHING WITH RIGOR: GENERAL-PURPOSE ZERO-SHOT PLANNING WITH LLM-BASED FORMALIZED PROGRAMMING", "authors": ["Yilun Hao", "Yang Zhang", "Chuchu Fan"], "abstract": "While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such as multi-constraint or long-horizon tasks. On the other hand, many frameworks aiming to solve complex planning problems often rely on task-specific preparatory efforts, such as task-specific in-context examples and pre-defined critics/verifiers, which limits their cross-task generalization capability. In this paper, we tackle these challenges by observing that the core of many planning problems lies in optimization problems: searching for the optimal solution (best plan) with goals subject to constraints (preconditions and effects of decisions). With LLMs' commonsense, reasoning, and programming capabilities, this opens up the possibilities of a universal LLM-based approach to planning problems. Inspired by this observation, we propose LLMFP, a general-purpose framework that leverages LLMs to capture key information from planning problems and formally formulate and solve them as optimization problems from scratch, with no task-specific examples needed. We apply LLMFP to 9 planning problems, ranging from multi-constraint decision making to multi-step planning problems, and demonstrate that LLMFP achieves on average 83.7% and 86.8% optimal rate across 9 tasks for GPT-40 and Claude 3.5 Sonnet, significantly outperforming the best baseline (direct planning with OpenAI ol-preview) with 37.6% and 40.7% improvements. We also validate components of LLMFP with ablation experiments and analyzed the underlying success and failure reasons.", "sections": [{"title": "1 INTRODUCTION", "content": "Making complex plans subject to multiple constraints is a time- and labor-intensive process, but is critical in many aspects of our lives such as work arrangement, business management, logistics, and robotics. In the past, people used domain-specific tools and languages to make specific plans in their areas, which often required a steep learning curve and were hard to adapt to other domains. When large language models (LLMs) emerge with their versatile capabilities such as language understanding, reasoning, and tool-using, using LLMs for planning has gained significant traction.\nFor such planning systems to be deployed in complex, real-world applications, two desirable proper-ties need to be satisfied: 1). Zero-shot flexibility: Unlike many experimental settings where planning tasks usually come with labeled datasets, it is very challenging to request such datasets from users in many realistic settings. Ideally, a flexible planning system should be able to conduct planning with only a task description provided by users, and nothing else. 2). High performance on complex tasks: Realistic planning problems usually require multi-step, long-horizon solutions, with many explicit and implicit constraints.\nHowever, there is a trade-off between flexibility and task complexity, and thus existing LLM-based planning systems are typically unable to achieve both simultaneously. On one hand, planning systems capable of performing zero-shot planning, utilizing the abundant knowledge and generalization"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 LLMS FOR PLANNING", "content": "The remarkable capabilities of LLMs in reasoning (Wei et al., 2022; Kojima et al., 2022; Yao et al., 2022; 2024) and tool-use (Qin et al., 2023; Schick et al., 2024) brings up interests of utilizing LLMs to solve planning problems. Based on LLMs commonsense and zero-shot generalization capabil-ity, many works capable of performing zero-shot planning are proposed (Huang et al., 2022a; Ahn et al., 2022; Huang et al., 2022b). However, their planning scenarios are limited to simple single-objective tasks such as household cleaning and they often require step-by-step interactive planning with grounding. To improve LLMs reasoning and planning capabilities for more complex problems, chain-of-thought (CoT) prompting appends reasoning steps before the answer to encourage LLMs to perform step-by-step reasoning (Wei et al., 2022). Recent works also propose to aid the LLM plan-ning processes with external tools (Liu et al., 2023; Guan et al., 2023; Chen et al., 2023; Li et al., 2023; Gundawar et al., 2024; Hao et al., 2024; Chen et al., 2024). For example, (Liu et al., 2023; Xie et al., 2023; Gundawar et al., 2024) leverages LLMs as translators to convert problems into fixed formats and inputting them to external planners, (Li et al., 2023) prompts LLM to add short codes to existing optimization codes of certain domain to account for follow-up what-if questions, and (Gundawar et al., 2024; Chen et al., 2024) empowers LLMs to iteratively refine plans or prompts based on feedback from external task-specific critics/verifiers/humans. However, to achieve strong performance, these methods are often based on extensive task-specific pre-defined efforts. For exam-ple, CoT depends on task-specific examples to achieve strong performance, domain files written in the planning domain definition language (PDDL) (Aeronautiques et al., 1998; Haslum et al., 2019) are required for (Liu et al., 2023), codes that encode domains into a mixed-integer linear program (MILP) are necessary for (Li et al., 2023), and external constraint critics are needed for (Gundawar et al., 2024). These requirements limit the generalization capability of these works to new domains."}, {"title": "2.2 LLM + SOLVER", "content": "As existing LLMs still do not have the capability to perform long-horizon reasoning for complex tasks (Achiam et al., 2023; Valmeekam et al., 2022; 2023; Kambhampati et al.), many works propose to take advantages of both LLMs and external planners or solvers by combining them for reasoning or planning. (Wu et al., 2022; He-Yueya et al., 2023; Pan et al., 2023; Ye et al., 2024) combines LLM with symbolic solvers to solve logical reasoning problems. LLMFP differs from them in that we aim to solve complex planning problems, which could include implicit constraints not clearly described in the task description and could be sequentially long-horizon tasks with defined actions. Instead, most logical reasoning problems are single-step satisfiction problems and clearly specify all constraints in question. In addition, LLMFP proposes a general approach, which does not require task-specific examples or task-specific efforts. (Li et al., 2023) teaches LLMs to add constraints to existing MILP code snippets of planning problems. (Li et al., 2024) asks the developer to express planning problems into automaton and guide the LLMs for planning based on it. (Liu et al., 2023; Guan et al., 2023; Zhou et al., 2024; Xie et al., 2023) leverages PDDL planner to aid the planning processes. Except for the natural language task description, they require human efforts to design solver-related specifications and task-specific examples, which is not needed for LLMFP."}, {"title": "3 LLMFP", "content": "LLMFP aims to deliver a plan that can resolve generic planning problems. For example, consider a coffee supply chain problem, where a coffee company sources beans from three suppliers with fixed capacity, roasts them at two facilities into either dark or light coffee and ships the roasted coffee to three retail locations to fulfill their demands. Then a planning problem involves accomplishing the task at the cheapest cost.\nTo achieve this, LLMFP takes the following inputs from users, as shown in Figure 1 (top panels).\n\u2022 Natural Language Task Description. A natural language description that details the problem settings and the planning objective, such as the above description of the coffee problem.\n\u2022 Background Information & API. A list of background information about the tasks as well as information on APIs that the planner can use. An example of the background information for the coffee task is the variables containing specific numbers of supplier capacities, cafe demands, and costs for shipping and roasting.\n\u2022 User Query. The question that either describes the detailed initial and/or goal states or adds/mod-ifies existing requirements of the tasks. In the coffee planning task, one example query is 'What would be the outcome if cafel experienced a 23% rise in demand'."}, {"title": "3.1 OVERVIEW", "content": "Emulating how humans construct and write codes to solve optimization problems, LLMFP solves the planning problems via the following steps, as shown in Fig. 1. DEFINER: LLMFP first prompts an LLM to define the problem by analyzing and proposing the optimization goal, decision variables, and constraints of the optimization problem (Sec. 3.2). FORMULATOR: LLMFP asks LLM to think about the necessary variables and steps to build when programming, and formulate a representation to summarize all key information of these variables (Sec. 3.3). CODE GEN-ERATOR: Given this representation, LLMs generate codes that initialize all necessary variables, assert constraints, and add goals (Sec. 3.4). RESULT FORMATTER: After LLMFP executes the generated codes, it prompts LLMs to convert the execution result into a fixed format and provide a short assessment of the execution results (Sec. 3.5). SELF ASSESSMENT AND MODIFICA-TION: LLMFP assesses each step based on the execution result, and modifies the first incorrect step (Sec. 3.6). The generated plan is delivered when it passes self-assessments of all steps."}, {"title": "3.2 DEFINER", "content": "The first step of building an optimization problem is to identify the goal, decision variables, and con-straints of the problem. This is accomplished by prompting the LLM to express in a natural language format (See Figure 1 for an example). The prompt includes all the user-supplied task informa-tion; a description of what goal, decision variables, and constraints mean; and an instruction to output the aforementioned information. The detailed prompt is listed in Appendix A.7.2.\nWhile generating the goals and decision variables are straightforward, generating the constraints is challenging, because certain constraints are not explicitly stated and can only be inferred by commonsense reasoning. We refer to these as the implicit constraints.\nFor example, in the coffee supply chain task example, the implicit constraints include 'the roasted coffee in each roastery does not exceed the beans it receives', 'the shipped coffee from each roastery does not exceed the coffee it roasts', and importantly but easily overlooked, \u2018all numbers of shipped and roasted beans and coffee need to be non-negative integer\u2019.\nTo facilitate uncovering the implicit constraints, we include in the prompts (under the description of constraints) a three-step instruction to derive the constraints: Identify all decision variables in this task, for each pair of decision variables, consider relations (explicit, implicit, underlying assumption, unmentioned commonsense) between them to make sure all variables are connected, and provide a constraint reasoning first before answering. This effectively helps LLMs to better identify implicit constraints for multi-constraint planning problems. Since for multi-step planning problems the task description needs to explicitly define the preconditions and effects of each action, there will be no implicit constraint so this step is omitted."}, {"title": "3.3 FORMULATOR", "content": "Although from the last step, we have defined the optimization goal, decision variables, and con-straints, more efforts are needed to integrate these into executable and correct code snippets. For example, the decision variables may be continuous, binary, or integer; they may need to be arranged into data structures such as lists and dictionaries; they may have fixed values or belong to flexible"}, {"title": "3.4 CODE GENERATOR", "content": "With the representation generated from the FORMULATOR, now we have all the information needed to build an optimization problem with codes. In the CODE GENERATOR's prompt, we explain the meanings of different stages and fields in the JSON representation and ask LLMs to follow Python and Z3 SMT syntax (De Moura & Bj\u00f8rner, 2008). By including user-provided task information and results from DEFINER and FORMULATOR, with no examples, LLM could reliably generate reasonable, executable, and correct Python codes. Then, LLMFP executes the codes and returns to re-generation if there are runtime errors. We set our maximum re-generation times to be 5."}, {"title": "3.5 RESULT FORMATTER", "content": "Since the variable names are decided by LLMs and have chances to be very different across queries, after code generation, we use a RESULT FORMATTER to ask LLM to convert the execution result to a fixed output format. For example, the output for the coffee task would be a JSON with the following fields:  number of coffee beans shipped from each supplier to each roastery, the number of light and dark coffee roasted in each roastery, and the number of light and dark coffee shipped from each roastery to each cafe. After filling in this result, we prompt the LLM to provide a brief evaluation of the result based on whether the result achieves the goal, satisfies constraints, and makes sense in common sense. Taking commonsense into consideration is important because sometimes if a necessary constraint is missing from the DEFINER step, it could result in unreasonable execution result thus could be unrealistic in commonsense. For example, for the coffee task, if the DEFINER does not include the non-negative constraint, to minimize the cost, the solver could propose negative units of shipped coffee. Detecting these unrealistic plans is helpful in the SELF ASSESS & MODIFICATION step."}, {"title": "3.6 SELF ASSESS & MODIFICATION", "content": "After we have the execution result and evaluation, LLMFP perform self-assessment to reason about the correctness and provide a rating for the DEFINER, FORMULATOR, and CODE GENERATOR. If the assessment marks all three steps to be correct, this plan will be delivered as the final output of our framework. Otherwise, the assessment will reason about how to modify this step, and provide a modification by itself. This modification will replace the output of the incorrect step and LLMFP will loop back to continue the next steps from there again. That is, if the SELF ASSESSOR thinks the output of the FORMULATOR is incorrect, it will generate a JSON representation by itself, and the framework will use this modified representation to enter CODE GENERATOR again. We set the maximum number of loops to be 5."}, {"title": "3.7 CHOICE OF SOLVER", "content": "As a framework that proposes to formulate and solve planning problems as optimization problems, LLMFP could be adapted to use any planner or solver by modifying the requirements in prompt to follow the syntax of new solvers. In this work, we compare the SMT solver with popular PDDL and MILP solvers and choose SMT solver with following reasons: SMT allows explicit goal and constraint assertion from scratch, which could be used to solve both single-step multi-constraint problems and multi-step problems, while PDDL solvers require a PDDL domain file and a PDDL problem file with strictly fixed formats, which limits its capability to solve non-PDDL problems. SMT is complete and sound, that is, it guarantees to find the optimal plan, while PDDL planners are not guaranteed to be complete. In addition, for all optimization solvers like SMT and MILP, the processes of building optimization problems for different optimization solvers or planners are the same: defining the goal, constraints, and decision variables, and writing codes to encode relation-ships between decision variables. Thus, utilizing any optimization planner has a similar process. We show how easily our framework could adapt to use MILP by including prompt differences and output examples in Appendix A.7.3. We selected SMT over MILP because the SMT Z3 solver is a publicly available package and is more accessible to all users than the Gurobi MILP solver, which requires licenses and limits the number of devices per license."}, {"title": "4 EXPERIMENTAL RESULTS", "content": ""}, {"title": "4.1 DOMAINS", "content": "We test on 9 planning problems, which includes 5 multi-constraint decision making tasks, Coffee, Workforce, Facility, Task Allocation, and Warehouse, and 4 multi-step tasks, Blocksworld, Mys-tery Blocksworld, Movie, and Gripper (Li et al., 2023; Valmeekam et al., 2024; Stein & Koller, 2023). Detailed task descriptions are included in Appendix A.1. The queries are either what-if ques-tions that change/add constraints to the existing scenarios or different task initial and goal conditions. Task inputs including example queries are given in Appendix A.5."}, {"title": "4.2 LLMFP PERFORMANCE", "content": "We evaluate LLMFP on 9 tasks with two models: GPT-40 (gpt) and Claude 3.5 Sonnet (cla) with temperature 0. Each task comes with a natural language task description, background information on the scenarios or info collection API, and natural language queries. For single-step decision-making problems, expected output formats are also provided. With no task-specific example, LLMFP takes these inputs and delivers plans as outputs. We use optimal rate as the evaluation metric, that is, whether the plans are optimal under the current task scenario and query.\nBaselines We compare LLMFP against 1) Direct: LLM direct plan generation, 2) CoT: chain-of-thought prompting (Wei et al., 2022) by asking LLMs to reason before generating the final answer, and 3) Code: prompts LLM to generate Python codes to solve the problem, allowing the use of any package or solver. For all baselines, we use both GPT-40 and Claude 3.5 Sonnet and also include a direct plan generation baseline with OpenAI o1-preview (o1p). All baselines are zero-shot with no task-specific examples. All baselines have the same input information as LLMFP, including task description, task background information or info collection API, and query. Please refer to Sec. A.7 for prompts of baselines.\nResults and Analysis We include the optimal rate comparison of LLMFP and baselines on 5 multi-constraint problems and 4 multi-step problems in Table 1 and 2. There are three key takeaways:\nFirst, LLMFP achieves strong performance across all 9 tasks, significantly outperforming all base-lines. For GPT-40, LLMFP achieves an average of 83.7% optimal rate across 9 tasks (79.1% for 5 multi-constraint problems and 87.5% for 4 multi-step problems). For Claude 3.5 Sonnet, LLMFP achieve an 86.8% optimal rate across 9 tasks (80.7% for 5 multi-constraint problems and 91.8% for 4 multi-step problems). For 5 multi-constraint problems, LLMFP GPT-40 and LLMFP Claude 3.5 Sonnet outperform best baselines CodeGPT-40 and CodeClaude 3.5 Sonnet by a large mar-gin of 48.0% and 36.5%. For 4 multi-step problems, LLMFP GPT-40 and LLMFP Claude 3.5 Sonnet outperform Directo1-preview and CoTClaude 3.5 Sonnet by an average of 19.4% and 45.9%. This high-lights both the effectiveness and the generalization capability of LLMFP."}, {"title": "4.3 EFFECTIVENESS OF LLMFP COMPONENTS", "content": "We then validate each component of LLMFP with ablation experiments on 9 tasks. We examine the effectiveness of DEFINER, FORMULATOR, and SELF ASSESS & MODIFICATION by removing these components from our framework one at a time and comparing with LLMFP. We do not re-move CODE GENERATOR and RESULT FORMATTER because they are the necessary components of LLMFP to deliver outputs. We use GPT-40 as the LLMs and optimal rate as the evaluation metric.\nResults and Analysis We include the optimal rate performance comparison of LLMFP and baselines on 9 problems in Table 3. From Table 3 there are two key takeaways:\nFirst, removing any of the 3 components from LLMFP negatively affects the performance. For multi-constraint problems, removing DEFINER, FORMULATOR, and SELF ASSESS & MODIFICA-TION lowers the optimal rate by 15.4%, 22.2%, and 21.9%. For multi-step problems, removing FORMULATOR and SELF ASSESS & MODIFICATION reduces the optimal rate by 87.4% and 12.4%.\nSecond, for different problems, the most effective components are different. Coffee degrades the most for No DEFINER; Warehouse and all multi-step problems drop the most for No FORMULATOR; and Workforce decreases the most for No SELF ASSESS & MODIFICATION. This again validates the diversity of the 9 problems and how they require different efforts to be successfully solved. Thus, LLMFP is an overall framework that could aid the process of planning from all aspects.\nNote that Warehouse with No DEFINER is the only entry that reaches a higher optimal rate than LLMFP. We notice that the main failure case for LLMFP in Warehouse is: LLM overwrites the pro-vided API get_distance and provides 1 as the output during Code Generation. For Warehouse with No DEFINER, this happens less frequently. This kind of issue could be alleviated by adding non-task-specific modifications to our framework, such as emphasizing direct use of the provided API in prompt or implementing a checker that checks for overwriting of provided APIs.\nTo summarize, all three components in LLMFP are effective and could account for diverse problems by providing comprehensive aids to solve planning problems."}, {"title": "4.4 LLMFP WITH TASK-SPECIFIC EXAMPLE", "content": "Although from Sec. 4.2 and Sec. 4.3 we prove that LLMFP is capable of achieving strong perfor-mance on a wide range of problems with no task-specific example, we test LLMFP by only replacing the two examples in FORMULATOR to one task-specific example on Coffee task to see how much the task-specific example could further improve LLMFP. Queries of Coffee tasks are what-if ques-tions and are categorized into 7 sets. Each set is a type of question. For example, the type of Set 1 is \"demand-increase\u201d, that is, every query in Set 1 asks about what if the demand in some cafes"}, {"title": "5 CONCLUSION", "content": "To account for the challenge of the trade-off between flexibility and task complexity for existing LLM planning works, we observe that the core of many planning problems lies in optimization problems and propose a universal approach for LLMs to solve planning problems. We propose LLMFP, a general-purpose LLM-based planning framework that captures key information from planning problems and formally formulates and solves them as optimization problems, with no task-specific examples needed. We test LLMFP on 9 diverse planning tasks with two LLMs to prove LLMFP's capability of achieving strong performance over fundamentally very different tasks and show the effectiveness of components in our framework.\nLimitations One limitation of LLMFP is that it needs clear and detailed task descriptions and queries. It is hard for LLMFP to define the problems' goals and constraints if the task description is ambiguous or missing some important information. Another limitation is that since LLMFP encodes the planning problems into optimization problems and solves them with optimization solvers, the"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DOMAINS", "content": "We test on 9 planning problems, including 5 multi-constraint decision making tasks and 4 multi-step tasks (Li et al., 2023; Valmeekam et al., 2024; Stein & Koller, 2023):\n\u2022 Coffee Coffee company sources beans from three suppliers with fixed capacity, roasts them at two facilities into dark or light coffee, and ships the roasted coffee to three retail locations. The company aims to minimize the total shipping and roasting cost while fulfilling the demand at each retail location. There are 266 different queries of 7 types in the dataset.\n\u2022 Workforce Assign workers to shifts; each worker may or may not be available on a particular day. The goal is to minimize the total payments to workers while fulfilling the shift requirements for two weeks. There are 231 different queries of 5 types in the dataset.\n\u2022 Facility A company currently ships its product from 5 plants to 4 warehouses. It is considering closing some plants to reduce costs. The goal is to decide which plant(s) to close to minimize transportation and fixed costs. There are 165 different queries of 4 types in the dataset.\n\u2022 Task Allocation Given tasks and three robots skilled in different tasks, the goal is to assign tasks to robots to minimize finish time. The finish time counts when the last robot stops working. There are 50 different queries describing the number of different tasks. This task and its data and queries are created by us.\n\u2022 Warehouse The robots need to finish tasks by visiting stations that are capable of accomplishing corresponding tasks. The goal is to find the list of stations while minimizing the total distance traveled. There are 50 different queries that include the random-length list of tasks to finish. This task and its data and queries are created by us.\n\u2022 Blocksworld The robot has four actions: pickup, putdown, stack, and unstack. The goal is to stack the blocks in the scene from their initial setup to a specific order with minimum steps. There are 602 different queries that describe blocks' initial conditions and goal states.\n\u2022 Mystery Blocksworld An obfuscated version of Blocksworld. The action and predicate names are replaced with names that logically make no sense. There are 602 different queries that describe objects' initial conditions and goal states.\n\u2022 Movie The goal is to get the required snacks, watch the movie, and recover the movie and counter to the original state with minimum steps. There are 21 different queries that describe objects' initial conditions and goal states.\n\u2022 Gripper There are robots and balls in different rooms. Each robot, with two grippers, can pick, drop, and move balls between rooms. The goal is to place balls in specific rooms with minimum steps. There are 25 different queries describing objects' initial conditions and goal states.\nThe queries for Coffee, Workforce, and Facility are what-if questions that change or add constraints to the existing scenarios. The queries of the rest tasks are different task initial and goal conditions. Task inputs including example queries are given in Appendix A.5."}, {"title": "A.2 LLMFP PERFORMANCE OVER ITERATIONS", "content": "Fig. 3 shows the performance of LLMFP over 5 iterations. The key observation is: number of it-erations of Self Assess & Midification stage enables LLMFP to further improve the optimal rates, although we can observe that LLMFP does not need extensive iterations to achieve an overall satis-fying performance."}, {"title": "A.3 LLMFP TIME AND COST STATISTICS AND ANALYSIS", "content": "Table 5, 6, and 7 shows the time and cost statistics of LLMFP for GPT-40 on 9 tasks.\nWe could observe that for both LLM querying time and solver running time, all stages of LLMFP requires reasonable runtime. The longest runtime is prompting FORMULATOR for Movie, which takes 31.9 seconds because it contains 9 actions thus requiring longer representation formulation.\nFrom cost statistics we could observe that the average cost per query for all 9 tasks is around 0.1 dollar, indicating LLMFP is not costly."}, {"title": "A.4 LLMFP FAILURE CASE ANALYSIS", "content": "Here we analyze the major failure cases for all 9 tasks."}, {"title": "A.4.1 COFFEE", "content": "There are two major failure cases for Coffee tasks:\nFirst, some queries are not clearly presented, indicating ambiguous information. Queries of Coffee tasks are what-if questions and are categorized into 7 sets. Each set is a type of question. We notice that the type \"supply-roastery\u201d asks queries like \u201cWhat led to the decision to use supplier3 for the roasting facility at roasteryl?\u201d. To answer this question, it is both plausible to test \u201cusing supplier3\" or to test \u201cnot using supplier3\" to see the performance. However, the ground truth answer for this type of questions is to \u201cnot using supplier 3\". As confusing queries even for human, they are hard for LLMs to understand. Thus, for these queries, LLMFP sometimes generate codes with opposite meanings as what is expected.\nSecond, sometimes LLMFP DEFINER fails to consider all implicit constraints. The most easily neglectable implicit constraints are 1) the beans roasted in each roastery do not exceed the beans it receives, and 2) the beans ship from each roastery do not exceed the coffee it roasts. When any of the two constraints are missing, to minimize the cost, the model will automatically set the shipped beans or roasted coffee to be 0, assuming the company delivers coffee without sourcing beans or roasting coffee."}, {"title": "A.4.2 WORKFORCE", "content": "There are two major failure cases for Coffee tasks:\nFirst, sometimes LLMFP fails to understand the queries. Some of the queries asks questions like 'Can Gu transition from Sun14 to Sun7 for work purposes?'. The meaning is to force Gu to work on Sun7 and take rest on Sun14. However, sometimes LLMFP builds variables to test both taking and not taking this transition, and returns solutions with less costs.\nSecond, sometimes when the solution space is large, it is hard to find the optimal solution within maximum runtime set for solver. We set the maximum solver runtime to be 15 minutes, which is exceeded when solving some hard queries."}, {"title": "A.4.3 FACILITY", "content": "Similarly as the first failure case of Coffee, some queries are not clearly presented. The queries are like \"What justifies the opening of plant 0?\u201d, which is confusing even for humans. Both opening plant 0 and closing plant 0 to report the costs make sense to answer this query. However, the ground truth meaning of this query is to close plant 0."}, {"title": "A.4.4 TASK ALLOCATION", "content": "LLMFP only fails one query in Task Allocation. The reason is the FORMULATOR generates wrong values for robot finish time."}, {"title": "A.4.5 WAREHOUSE", "content": "The major failure case for Warehouse is CODE GENERATOR overwrites the provided API get_distance and provide 1 as the output during Code Generation. Thus, the distance between each station is mistakenly set to be 1."}, {"title": "A.4.6 BLOCKSWORLD", "content": "One major failure case for Blocksworld is CODE GENERATOR fails to initialize the states of predi-cates correctly and thoroughly. Since the query will only meantion the predicates that are true, for example, block 1 is on block 2, but when initializing, LLMFP needs to initialize both mentioned states but also unmentioned states that are false. For example, block 2 is not on block 1. However, CODE GENERATOR sometimes fails to consider all unmentioned states."}, {"title": "A.4.7 MYSTERY BLOCKSWORLD", "content": "Similarly as Blocksworld, Mystery Blocksworld has same failure case. For Mystery Blocksworld, since the predicate and action names are not meaningful, more this kind of errors are made by GPT-40. However, Claude seems to have better reasoning capability to support it from making more these errors."}, {"title": "A.4.8 MOVIE", "content": "There is no failure case for Movie."}, {"title": "A.4.9 GRIPPER", "content": "The major failure case for Gripper is when the solver fails to find the solution because there are some code generation errors, the SELF ASSESS & MODIFICATION sometimes would think it is because the timestep is not enough, thus adding another loop within the original loop. However, this would result in the program to execute forever."}, {"title": "A.5 INPUTS ON 9 TASKS", "content": "We include the inputs, which includes task description, background information or API, and example queries, for all 9 tasks in Fig. 4 - Fig. 12:"}, {"title": "A.6 EXAMPLE OUTPUTS ON COFFEE TASKS", "content": "In this section, we include the outputs of all models for the query \"What is the potential impact of a 29% increase in demand at cafe cafe2?\". The optimal solution has total cost to be 2612."}, {"title": "A.6.1 BASELINES", "content": "We include the outputs of four baselines for Coffee task in Fig. 13 - Fig. 15, and analyze the failure reason in the figure descriptions:"}, {"title": "A.6.2 LLMFP", "content": "We include the outputs of all stages of LLMFP for Coffee task in Fig. 16 - Fig. 20:"}, {"title": "A.7 PROMPTS", "content": ""}, {"title": "A.7.1 BASELINE PROMPT", "content": ""}, {"title": "A.7.2 LLMFP PROMPT", "content": "We use general templates for all tasks. The full prompts for all tasks will be released soon. Here we show the templates we have for GPT-40. Since Claude naturally considers more constraints and is more strict in assessing, we edit the prompts a little to account for the different traits of Claude, and prompts are also included in the codes.\nWe include the prompt template we use for GPT-40 as below:"}]}