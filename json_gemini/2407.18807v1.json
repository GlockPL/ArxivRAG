{"title": "Robust Learning in Bayesian Parallel Branching Graph Neural Networks: The Narrow Width Limit", "authors": ["Zechen Zhang", "Haim Sompolinsky"], "abstract": "The infinite width limit of random neural networks is known to result in Neural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterized by task-independent kernels. It is widely accepted that larger network widths contribute to improved generalization (Park et al. [2019]). However, this work challenges this notion by investigating the narrow width limit of the Bayesian Parallel Branching Graph Neural Network (BPB-GNN), an architecture that resembles residual networks. We demonstrate that when the width of a BPB-GNN is significantly smaller compared to the number of training examples, each branch exhibits more robust learning due to a symmetry breaking of branches in kernel renormalization. Surprisingly, the performance of a BPB-GNN in the narrow width limit is generally superior or comparable to that achieved in the wide width limit in bias-limited scenarios. Furthermore, the readout norms of each branch in the narrow width limit are mostly independent of the architectural hyperparameters but generally reflective of the nature of the data. Our results characterize a newly defined narrow-width regime for parallel branching networks in general.", "sections": [{"title": "1 Introduction", "content": "The study of neural network architectures has seen substantial growth, particularly in understanding how network width impacts learning and generalization. It is generally believed that wider networks generally perform better (Allen-Zhu et al. [2019], Jacot et al. [2018], Gao et al. [2024]). However, this work challenges the prevailing assumption by exploring the narrow width limit of Bayesian Parallel Branching Graph Neural Networks (BPB-GNNs), an architecture inspired by residual GCN networks (Chen et al. [2020a, 2022]). We show theoretically and empirically that narrow-width networks can perform better than their wider counterparts due to a symmetry-breaking effect in kernel renormalization, in bias-limited scenarios. This paper presents a detailed theoretical analysis of BPB-GNNs in the narrow-width regime, highlighting realistic conditions under which these networks demonstrate robust learning and comparable generalization.\nContributions :\n1. We introduce a novel yet simple GCN architecture with parallel independent branches, and derive the exact generalization error for node regression in the statistical limit as the sample"}, {"title": "2 Related works", "content": "Infinitely wide neural networks: Our work follows a long tradition of mathematical analysis of infinitely-wide neural networks (Jacot et al. [2018], Lee et al. [2018], Bahri et al. [2024]), resulting in NTK or NNGP kernels. Recently, such analysis has been extended to structured neural networks including GNNs (Du et al. [2019], Walker and Glocker [2019]). However, they do not provide analysis of feature learning where the kernel is dependent on the tasks.\nKernel renormalization and feature learning: There has been progress on understanding simple MLPs in the feature-learning regime as the kernel shape changes with the task or time (Li and Sompolinsky [2021], Atanasov et al. [2021], Avidan et al. [2023], Wang and Jacot [2023]). We develop such understanding in Graph-based networks.\nTheoretical analysis of GCN: There is a long line of works that analyze theoretically the expressiveness (Xu et al. [2018], Geerts and Reutter [2022]) and generalization performance (Tang and Liu [2023], Garg et al. [2020], Aminian et al. [2024]) of GNN. However, it is challenging to calculate the generalization error dependence on tasks. To our knowledge, our work is first to provide a tight bound of the generalization error for GNN with residual-like structures. The closest architecture to our linear BPB-GCN is the linearly decoupled GCN proposed by Cong et al. [2021]; however, the readout vector in their case has weight-sharing, which will not result in kernel renormalization for different branches."}, {"title": "3 BPB-GNN", "content": "We are motivated to study the parallel branching networks as they resemble residual blocks in commonly used architectures and tractable to study analytically with our Bayesian framework. Given graph G = (A, X), where A is the adjacency matrix and X the node feature matrix, the final readout for node u is a scalar $f_r(G; \\Theta)$ which depends on the graph and network parameters \u0398."}, {"title": "3.1 Parallel branching GNN architecture", "content": "The parallel branching GNN is an ensemble of GNN branches, where each branch operates independently with no weight sharing. In this work, we focus on the simple setup of branches made of linear GCN with one hidden layer, but with different number of convolutions $A^l$ on the input node features (Figure 1a). In this way, the parallel branching GNN is analogous to GCN with residual connections, for which the final node readout can also be thought of as an ensemble of convolution layers (Veit et al. [2016]). Concretely, the overall readout $f_r(G; \\Theta)$ for node u is a sum of the branch readouts\n$f^r(G; \\Theta) = \\sum_{l=0}^{L-1} f^l(G; \\Theta_l = \\{W_l^{(1)}, a_l^{(1)}\\}),$ \t\t(1)\nwhere\n$f_l^{(1)}(G, \\Theta_l) = \\frac{1}{\\sqrt{L}} \\sum_{i=1}^{N} \\sum_{j=1}^{N_0} \\frac{1}{\\sqrt{N}} a_{l,i}^{(1)} W_{l,ij}^{(1)} \\sum_{\\nu=1}^{n} (A^l)_{\\mu\\nu} x_{\\nu},$ \t\t(2)\nIn this paper, the convolution operation is normalized as $\\mathbf{A} = D^{-1/2}(\\hat{A}+I)D^{-1/2}$, where $\\hat{A}$ is the original Adjacency matrix and D is the degree matrix. We also use feature standardization after convolution for each branch to normalize the input."}, {"title": "3.2 Bayesian node regression", "content": "We consider a Bayesian semi-supervised node regression problem, for which the posterior probability for the weight parameters is given by\n$P(\\Theta) = \\frac{e^{-E(\\Theta;G,Y)/T}}{Z} = \\frac{1}{Z} exp(-\\frac{1}{T} (\\sum_{\\mu=1}^{P} \\frac{1}{2\\sigma^2} (f(\\mathbf{G}, \\Theta) - y_{\\mu})^2 - \\frac{1}{2\\sigma^2} \\Theta^T\\Theta)),$  \t\t(4)\nwhere the first term in the exponent corresponds to the likelihood term induced by learning P node labels $y_\\mu$ with squared loss and the second term corresponds to the Gaussian prior with variance $\\sigma^2$. $Z = \\int e^{-E(\\Theta)/T} d\\Theta$ is the normalization constant. This Bayesian setup is well motivated, as the Langevin dynamics trained with energy potential E and temperature T that results in this equilibrium posterior distribution shares a lot in common with the Gradient Descent (Avidan et al. [2023], Naveh et al. [2021]) and Stochastic GD optimizers (Mignacco and Urbani [2022], Wu et al. [2020]). In fact, Li and Sompolinsky [2021] shows empirically that the Bayesian equilibrium is a statistical distribution of the usual gradient descent with early stopping optimization with random initializations at 0 temperature in DNNs, and the $L_2$ regularization strength $\\sigma^2$ corresponds to the Gaussian initialization variance.\nWe are interested in understanding the weight and predictor statistics of each branch and how they contribute to the overall generalization performance of the network. In the following theoretical derivations, our working regime is in the overparametrizing high dimensional limit (Li and Sompolinsky [2021], Montanari and Subag [2023], Bordelon and Pehlevan [2022], Howard et al. [2024]): $P, N, N_0 \\rightarrow \\infty, \\frac{P}{N} = a$ finite, and the capacity $a_0 = \\frac{L N_0}{N} < 1$. As we will show later, this limit is practically true even with P, N not so large (our smallest N is 4). We will also use near 0 temperature in which case the training error will be near 0 and the prior $L_2$ regularization has an inductive bias on the solution space that will influence the generalization properties."}, {"title": "3.3 Kernel renormalization and order parameters", "content": "The normalization factor, or the partition function, $Z = \\int e^{-E(\\Theta)/T} d\\Theta$ carries all the information for calculating the predictor statistics and the generalization dependence on network hyperparameters N, L, $\\sigma^2$. Using Eq. 2,4 we can integrate out the readout weights $a_{r's}$ first, resulting in\n$Z = \\int dW e^{-H(W)},$\t\t(5)"}, {"title": "3.4 Predictor statistics and generalization", "content": "Under the theoretical framework, we obtain analytically (Appendix A.2) the mean prediction and variance for a single test node v as\n$\\langle y^r(G) \\rangle = \\frac{1}{L} \\sum_l k_{lv} (K + TI)^{-1}Y$\t\t(13)\n$\\langle \\delta y_v^r(G)^2 \\rangle = K_{vv} - \\frac{1}{L} \\sum_{l,l'} k_l^T (K + TI)^{-1} k_{l'v},$\t\t(14)"}, {"title": "4 The narrow width limit", "content": "As we discussed briefly in 3.3, the kernel becomes highly renormalized at narrow width. In fact, in the other extreme scenario when N/P \u2192 0, the energetic term in the Hamiltonian completely dominates, and we would expect that the generalization performance saturates as the order parameters in the energetic terms become independent of width N. Therefore, just as infinitely wide networks correspond to the GP limit, we propose that there exists a narrow width limit when the network width is extremely small compared to the number of training samples."}, {"title": "4.1 Robust learning of branches: the equipartition conjecture", "content": "What happens in the narrow width limit? In the following, we demonstrate that each branch will learn robustly at narrow width.\nConsider a student-teacher network setup, where the teacher network is given by\n$f^*(G; \\Theta^*) = \\frac{1}{L} \\sum_l f_l^*(G; W_l^*) = \\frac{1}{L} \\sum_l \\sum_i a_{l,i}^* h_i(G, W_l^*).$ \t\t (19)\n$W_l \\sim N(0, \\sigma_l^2)$ and $a_l \\sim N(0, \\beta_l^2)$, where $\\beta_l^2$ is the assigned variance of the readout weight for the teacher branch l. Now we feed the data generated from this distribution Y to the student network with the same number of branches, with student variance $\\sigma^2$.\nConsider $(YY^T)_{\\mu,\\nu} = \\sum_{i,j,l_1,l_2} \\frac{N L_0}{N L^2} a_{l_1,i}^* a_{l_2,j}^* h_i(G) h_j(G);$ we conjecture that this quantity concentrates at its expectation value $E_{a^*,w^*}(YY^T) = \\sum_l \\frac{\\beta_l^2}{L} K_l$. Given this assumption, we can calculate the order parameters in the saddle point equation 40 (A.1) at 0 temperature. Ignoring the entropic term at narrow width, $r_l$ (Appendix A) becomes\n$r_l - Y^T K^{-1} \\frac{K_l}{L} K^{-1} Y \\approx T K^{-1} \\frac{K_l}{L} K^{-1} \\sum_l \\beta_l^2 K_l = Tr(K^{-1} \\frac{K_l}{L}) \\sum_l \\beta_l^2 \\frac{K_l}{L}$  \t\t (20)\nOne solution that satisfies the saddle point equations is\n$K_l \\sigma^2 = \\beta_l^2 \\sigma_l^2$.\t\t(21)\nWe call this the equipartition conjecture, as the mean readout squared and the variance (A.3) has to exactly balance each other. Furthermore, by Eq.12, the student readout norm at each branch on"}, {"title": "4.2 Student-Teacher experiment on robust branch learning", "content": "We demonstrate this robust learning phenomenon and provide a first evidence of the equipartition conjecture with the student-teacher experiment setup introduced in the previous section. We use the contextual stochastic block model (CSBM) (Deshpande et al. [2018]) as the shared node input features, where the adjacency matrix is given by a stochastic block model with two blocks, and the node feature is generated with latent vectors corresponding to the two blocks (See B.1 for details)."}, {"title": "5 BPB-GNN on Cora", "content": "We also conduct experiments on the benchmark dataset Cora with binary node regression, and observe a similar narrow-to-wide width transition for the bias term. We analyze the generalization performance for both network width N and the number of branches used L. As shown in Figure 5, the bias increases with network width, transitioning to the GP regime. Generally, more branches improve performance.\nConvergence of Branch Importance at Narrow Width An interesting aspect of the BPB-GNN network is that the branch norms seem to converge regardless of the hyperparameters \u03c3 and L, reflecting the natural branch importance of the dataset and task.\nAs shown in Figure 6, the BPB-GNN with L = 6 branches robustly learns the readout norms at narrow width, verifying the equipartition conjecture. The last branch of the BPB-GNN network has a larger contribution, reflecting the presence of higher-order convolutions in the Cora dataset. From a kernel perspective, increasing branches better distinguish the nodes, as shown in Figure 7. This could explain the selective turn-off of intermediate branches and the increased contribution of the last branch."}, {"title": "6 Discussion", "content": "The findings presented in this paper reveal that BPB-GNNs exhibit unique characteristics in the narrow width limit. Unlike the infinite-width limit where neural networks behave as Gaussian Process (GP) with task-independent kernels, narrow-width BPB-GNNs undergo significant kernel renormalization. This renormalization leads to symmetry breaking among the branches, resulting in more robust and differentiated learning. Our experiments demonstrate that narrow-width BPB-GNNs can retain and, in some cases, improve generalization performance compared to their wider counterparts,"}, {"title": "A Details on Theory of BPB-GNN", "content": "A.1 Kernel renormalization\nFollowing similar derivations as the original kernel renormalization work Li and Sompolinsky [2021], we will integrate out the weights in the partition function $Z = \\int d\\Theta exp(-E(\\Theta)/T)$, from the readout layer weights ar's to the hidden layer weights W\u2081's and arrive at an effective Hamiltonian shown in the main text.\nFirst, we linearize the energy in terms of ar's by introducing the auxiliary variables t\u2122, \u03bc = 1, ..., \u03a1.\n$Z = \\int d\\Theta \\prod_{\\mu=1}^{P} \\int dt^{\\mu} exp [\\frac{1}{T} (\\frac{1}{2\\sigma^2} \\Theta^T\\Theta - (\\sum_{\\mu=1}^{P} \\frac{i t^{\\mu}}{T} (\\frac{1}{\\sqrt{NL}} \\sum_{l=0}^{L-1} (a_l^{(1)} h_l(G))^{\\mu} - Y^{\\mu}) - \\frac{(t^{\\mu})^2}{T}),$ \t\t(24)\nNow we can integrate out ar's as they are linearized and the partition function becomes\n$Z = \\int dW e^{-H(W)},$  \t\t (25)\nwith effective Hamiltonian\n$H(W) = \\frac{1}{2\\sigma^2} \\sum_{l=0}^{L-1} Tr W_l W_l^T + \\frac{1}{2} Y^T (K(W) + TI)^{-1} Y + \\frac{1}{2} log det(K(W) + TI),$ \t\t(26)\nwhere\n$K(W) = \\frac{1}{L} \\sum_{l=0}^{L-1} \\frac{\\sigma^2}{N} (H_l(W_l) H_l(W_l)^T)|_P$ \t\t(27)\nis the P \u00d7 P kernel matrix dependent on the observed P nodes with node features H\u2081 = AXW\u2081 and denote p as the matrix restricting to the elements generated by the training nodes.\nNow we perform the integration on Wi's, and get a Fourier representation of Z with hi,ui as auxiliary variables after inserting t:\n$Z = \\prod_{l=0}^{L} \\int dh_l du_l dt exp \\{ (i t Y - \\frac{N}{2} \\sum_l log(1+ h_l) + \\sum_{l=0}^{L} \\frac{N}{2\\sigma^2} u_l w_l h_l - \\frac{i t^2}{2} (\\sum_l u_l K_l + TI))\\}$\n$=\\prod_{l=0}^{L} \\int dh_l du_l \\exp (- \\frac{N}{2} \\sum_l log(1+ h_l) + \\sum_{l=0}^{L} \\frac{N}{2\\sigma^2} u_l w_l h_l - \\frac{i}{2} Y^T (\\sum_l u_l K_l + TI)^{-1} Y),$\t\t(28)\nwhere\n$K_l = \\frac{\\sigma^2}{N_0} [AX X^T A^T]|_P$\t\t(29)\nis the input kernel for branch l. Now as N \u2192 \u221e and a = $\\frac{P}{N}$ fixed, we can perform the saddle point approximation and get the saddle points for $h_l$ as\n$\\frac{1}{1 + h_l} \\frac{u_l}{\\sigma^2}$\t\t(30)\nPlugging this back to the equation, we get\n$Z = \\prod_{l} \\int du_l e^{-H_{eff}},$\t\t(31)\nwith the effective Hamiltonian\n$H_{eff} = S + E,$\t\t(32)\nwhere we call S the entropic term\n$S = - \\frac{N}{2} \\sum_l log u_l + \\sum_l \\frac{u_l}{2 \\sigma^2},$ \t\t (33)\nand E is the energetic term\n$\\frac{b}{2} Y^T (\\sum_l u_l K_l + TI)^{-1}Y + \\frac{1}{2} log det(\\sum_l u_l K_l + TI)$  \t\t (34)"}, {"title": "A.2 Predictor statistics and generalization", "content": "We can get the predictor statistics of each branch readout $y^l(G)$ on a new test node v by considering the generating function:\n$Z(\\eta_1,...,\\eta_L) = \\int D\\Theta exp\\{\\frac{1}{2T} \\sum_{\\mu} (f^r(G; \\Theta) - y_{\\mu})^2 + \\frac{1}{2T} \\sum_l \\frac{i\\eta_l}{\\sqrt{NL}} \\sum_{\\mu} (y_l(G; W_l) - \\langle y_l(G; W_l) \\rangle  - \\frac{T}{2\\sigma^2} \\Theta^T\\Theta \\},$  \t\t(41)\nTherefore, by taking the derivative with respect to each \u03b7\u03b9, we arrive at the statistics for y\u2081(x) as:\n$\\langle y_l^r(G) \\rangle = \\delta_{\\eta_l} log Z |_{\\eta=0}$\t\t(42)\n$\\langle \\delta y_v^r(G) \\rangle = \\delta^2_{\\eta_l} log Z |_{\\eta=0}$\t\t(43)\nAfter integrating out the weights \u0398 layer by layer, we have:\n$Z(\\eta_1,..., \\eta_L) = \\int d u_l exp\\{\\sum_l (\\frac{1}{2} log u_l + \\frac{N}{2\\sigma^2 u_l})\n+ \\frac{i}{T} (iY+ \\frac{1}{2T} \\sum_l \\eta_l k_l )^T (\\sum_l u_l K_l+TI)^{-1}(iY+ \\frac{1}{2T} \\sum_l \\eta_l k_l)\n- \\frac{1}{2} log det(\\sum_l \\frac{1}{2} u_l K_l + TI) - \\frac{1}{T} \\sum_l \\frac{1}{2} \\eta_l K_{lvv}  \\}.$ \t\t (44)\nHere\n$k_l^v = \\frac{\\sigma^2}{N_0} [AXX^T A^T]|_{(P,v)}$ \t\t (45)\nis the P \u00d7 1 column kernel matrix for test node v and all training nodes, and\n$K_{lv}^v = \\frac{\\sigma^2}{N_0} [AXX^T A^T]|_{(v,v)}$ \t\t(46)"}, {"title": "A.3 Statistics of branch readout norms", "content": "From the partition function Eq.24, we can relate the mean of readout weights ar to the auxiliary variable t by\n$\\langle a_l \\rangle_W = -i\\frac{\\sigma^2}{\\sqrt{NL}} \\frac{\\partial \\Tau}{\\partial T} (t) = \\frac{\\sigma^2}{\\sqrt{NL}} \\Phi_l^T (K + TI)^{-1} Y,$\t\t (51)\nwhere \u03a6\u03b9 is the node feature matrix for the hidden layer nodes. We have\n$\\langle a_l \\rangle_W \\langle a_l \\rangle_W = \\sigma^2Y^T (K + TI)^{-1} \\frac{K_l}{L} (K + TI)^{-1}Y = r_l \\sigma^2$\t\t(52)\nWe can calculate the second-order statistics of ar: the variance is\n$\\langle \\delta a_l^2 \\delta a_l^2 \\rangle = \\frac{\\sigma^2}{NL} Tr(\\frac{1}{1 + \\frac{2\\delta\\Phi_i \\Phi_i^T}{1} }^2 )^{-1} = \\sigma^2(N - Tr(K + TI)^{-1} \\frac{K_l}{L} ) = \\sigma^2(N - Tr_l)$ \t\t(53)\nTherefore,\n$\\langle a_l^2 \\rangle = \\langle \\delta a_l^2 \\delta a_l^2 \\rangle + \\langle \\delta a_l \\rangle \\langle \\delta a_l \\rangle = N\\sigma^2 + \\sigma^2r_l + 1 - \\sigma^2Tr_l = Nu_l$\t\t(54)\nTherefore, we have proved the main text claim that the order parameter ui's are really the mean squared readout norms of the branches."}, {"title": "B Experimental details", "content": "B.1 Student-teacher CSBM\nFor the student-teacher task, we use the contextual stochastic block model introduced by Deshpande et al. [2018]. The adjacency matrix is given by\n$A_{ij} = \\{\n\\begin{array}{ll}\n1 \\text{ with probability } p = c_{in}/n, \\text{ if } i, j \\leq n/2 \\\\\n1 \\text{ with probability } p = c_{in}/n, \\text{ if } i, j \\geq n/2 \\\\\n1 \\text{ with probability } q = c_{out}/n, \\text{ otherwise }\n\\end{array}$\t\t(55)\nwhere\n$c_{in,out} = d \\pm \\sqrt{d\\lambda}$\t\t(56)\nd is the average degree and \u03bb the homophily factor.\nThe feature vector z\u03bc for a particular node \u03bc is given by\n$z_{\\mu} = \\frac{i}{\\sqrt{N_0}} (\\mu \\pi + \\xi_{\\mu})$\t\t(57)\nwhere\n$\\pi_\\mu \\sim N(0, I_{N_0}), \\xi_{\\mu} \\sim N(0, I_{N_0})$ \t\t(58)\nIn the experiment, we use $N_0$ = 950,d = 20,\u03bb = 4 and \u03bc = 4. The teacher network parameters are variance \u03c3\u03b5 = 1, width $N_1$ = 1024, branch norms variance \u03b2 = 0.4, \u03b2\u2081 = 2 for individual element of the readout vector al. Temperature T = 0.0005\u03c32 for each \u03c3 value."}, {"title": "B.2 Cora", "content": "For the Cora dataset, we use a random split of the data into 21% as training set and 79% as test set. We also group the classes (1, 2, 4) into one group and the rest for the other group for binary node regression. We use temperature T = 0.01 for both theory and sampling as the sampling becomes more difficult for smaller temperature. This explains the discrepancy of the GP limit bias for different \u03c3 values."}, {"title": "B.3 Hamiltonian Monte Carlo", "content": "The sampling in the paper are all done with Hamiltonian Monte Carlo simulations, which has fast convergence to the posterior distribution compared to Langevin dynamics. Due to memory constraint, we only sampled up to N = 1024 hidden layer width for the student-teacher CSBM experiment and N = 64 for the Cora experiment. Since we mainly aim to demonstrate the narrow width effect in this paper, this suffices the purpose."}]}