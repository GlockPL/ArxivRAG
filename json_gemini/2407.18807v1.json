{"title": "Robust Learning in Bayesian Parallel Branching Graph Neural Networks: The Narrow Width Limit", "authors": ["Zechen Zhang", "Haim Sompolinsky"], "abstract": "The infinite width limit of random neural networks is known to result in Neural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterized by task-independent kernels. It is widely accepted that larger network widths contribute to improved generalization (Park et al. [2019]). However, this work challenges this notion by investigating the narrow width limit of the Bayesian Parallel Branching Graph Neural Network (BPB-GNN), an architecture that resembles residual networks. We demonstrate that when the width of a BPB-GNN is significantly smaller compared to the number of training examples, each branch exhibits more robust learning due to a symmetry breaking of branches in kernel renormalization. Surprisingly, the performance of a BPB-GNN in the narrow width limit is generally superior or comparable to that achieved in the wide width limit in bias-limited scenarios. Furthermore, the readout norms of each branch in the narrow width limit are mostly independent of the architectural hyperparameters but generally reflective of the nature of the data. Our results characterize a newly defined narrow-width regime for parallel branching networks in general.", "sections": [{"title": "Introduction", "content": "The study of neural network architectures has seen substantial growth, particularly in understanding how network width impacts learning and generalization. It is generally believed that wider networks generally perform better (Allen-Zhu et al. [2019], Jacot et al. [2018], Gao et al. [2024]). However, this work challenges the prevailing assumption by exploring the narrow width limit of Bayesian Parallel Branching Graph Neural Networks (BPB-GNNs), an architecture inspired by residual GCN networks (Chen et al. [2020a, 2022]). We show theoretically and empirically that narrow-width networks can perform better than their wider counterparts due to a symmetry-breaking effect in kernel renormalization, in bias-limited scenarios. This paper presents a detailed theoretical analysis of BPB-GNNs in the narrow-width regime, highlighting realistic conditions under which these networks demonstrate robust learning and comparable generalization.\nContributions :\n1. We introduce a novel yet simple GCN architecture with parallel independent branches, and derive the exact generalization error for node regression in the statistical limit as the sample"}, {"title": "Related works", "content": "Infinitely wide neural networks: Our work follows a long tradition of mathematical analysis of infinitely-wide neural networks (Jacot et al. [2018], Lee et al. [2018], Bahri et al. [2024]), resulting in NTK or NNGP kernels. Recently, such analysis has been extended to structured neural networks including GNNs (Du et al. [2019], Walker and Glocker [2019]). However, they do not provide analysis of feature learning where the kernel is dependent on the tasks.\nKernel renormalization and feature learning: There has been progress on understanding simple MLPs in the feature-learning regime as the kernel shape changes with the task or time (Li and Sompolinsky [2021], Atanasov et al. [2021], Avidan et al. [2023], Wang and Jacot [2023]). We develop such understanding in Graph-based networks.\nTheoretical analysis of GCN: There is a long line of works that analyze theoretically the expressiveness (Xu et al. [2018], Geerts and Reutter [2022]) and generalization performance (Tang and Liu [2023], Garg et al. [2020], Aminian et al. [2024]) of GNN. However, it is challenging to calculate the generalization error dependence on tasks. To our knowledge, our work is first to provide a tight bound of the generalization error for GNN with residual-like structures. The closest architecture to our linear BPB-GCN is the linearly decoupled GCN proposed by Cong et al. [2021]; however, the readout vector in their case has weight-sharing, which will not result in kernel renormalization for different branches."}, {"title": "BPB-GNN", "content": "We are motivated to study the parallel branching networks as they resemble residual blocks in commonly used architectures and tractable to study analytically with our Bayesian framework. Given graph $G = (A, X)$, where $A$ is the adjacency matrix and $X$ the node feature matrix, the final readout for node $u$ is a scalar $f_r(G; \\Theta)$ which depends on the graph and network parameters $\\Theta$."}, {"title": "Parallel branching GNN architecture", "content": "The parallel branching GNN is an ensemble of GNN branches, where each branch operates independently with no weight sharing. In this work, we focus on the simple setup of branches made of linear GCN with one hidden layer, but with different number of convolutions $A^l$ on the input node features (Figure 1a). In this way, the parallel branching GNN is analogous to GCN with residual connections, for which the final node readout can also be thought of as an ensemble of convolution layers (Veit et al. [2016]). Concretely, the overall readout $f_r(G; \\Theta)$ for node $u$ is a sum of the branch readouts\n$$f^r(G; \\Theta) = \\sum_{l=0}^{L-1} f^l(G; \\Theta_l = \\{W_l^{(1)}, \\alpha_l^{(1)}\\}),$$$$f^l(G, \\Theta_l) = \\frac{1}{\\sqrt{L}} \\frac{1}{\\sqrt{N}} \\sum_{i=1}^N \\sum_{j=1}^{N_0} W_{ij}^{(1)} \\alpha_l^{(1)} \\sum_{\\nu=1}^{l} (A^\\nu)_{u, i}x_i$$"}, {"title": "Bayesian node regression", "content": "We consider a Bayesian semi-supervised node regression problem, for which the posterior probability for the weight parameters is given by\n$$P(\\Theta) = \\frac{1}{Z}e^{-E(\\Theta; G, Y)/T} = \\frac{1}{Z} exp(-\\frac{1}{T} \\sum_{\\mu=1}^{P} (f(G, \\Theta) - y_\\mu)^2 - \\frac{1}{2\\sigma^2} \\Theta^T\\Theta),$$where the first term in the exponent corresponds to the likelihood term induced by learning $P$ node labels $y_\\mu$ with squared loss and the second term corresponds to the Gaussian prior with variance $\\sigma^2$. $Z = \\int e^{-E(\\Theta)/T} d\\Theta$ is the normalization constant. This Bayesian setup is well motivated, as the Langevin dynamics trained with energy potential $E$ and temperature $T$ that results in this equilibrium posterior distribution shares a lot in common with the Gradient Descent (Avidan et al. [2023], Naveh et al. [2021]) and Stochastic GD optimizers (Mignacco and Urbani [2022], Wu et al. [2020]). In fact, Li and Sompolinsky [2021] shows empirically that the Bayesian equilibrium is a statistical distribution of the usual gradient descent with early stopping optimization with random initializations at 0 temperature in DNNs, and the $L2$ regularization strength $\\sigma^2$ corresponds to the Gaussian initialization variance.\nWe are interested in understanding the weight and predictor statistics of each branch and how they contribute to the overall generalization performance of the network. In the following theoretical derivations, our working regime is in the overparametrizing high dimensional limit (Li and Sompolinsky [2021], Montanari and Subag [2023], Bordelon and Pehlevan [2022], Howard et al. [2024]): $P, N, N_0 \\rightarrow \\infty, \\frac{P}{N} = a$ finite, and the capacity $a_0 = \\frac{L N_0}{N} < 1$. As we will show later, this limit is practically true even with $P, N$ not so large (our smallest $N$ is 4). We will also use near 0 temperature in which case the training error will be near 0 and the prior $L2$ regularization has an inductive bias on the solution space that will influence the generalization properties."}, {"title": "Kernel renormalization and order parameters", "content": "The normalization factor, or the partition function, $Z = \\int e^{-E(\\Theta)/T} d\\Theta$ carries all the information for calculating the predictor statistics and the generalization dependence on network hyperparameters $N, L, \\sigma^2$. Using Eq. 2,4 we can integrate out the readout weights $\\alpha_l$'s first, resulting in\n$$Z = \\int dW e^{-H(W)},$$"}, {"title": "Predictor statistics and generalization", "content": "Under the theoretical framework, we obtain analytically (Appendix A.2) the mean prediction and variance for a single test node $v$ as\n$$\\langle y^r(G) \\rangle = \\sum_l \\frac{1}{L} k_{l,v}^T (K + TI)^{-1} Y$$$$(\\delta_{yy}(G)^2) = K_{vv} - \\sum_{l,l'} \\frac{1}{L} k_{l,v}^T (K + TI)^{-1} \\frac{1}{L} k_{l',v}$$"}, {"title": "The narrow width limit", "content": "As we discussed briefly in 3.3, the kernel becomes highly renormalized at narrow width. In fact, in the other extreme scenario when $N/P \\rightarrow 0$, the energetic term in the Hamiltonian completely dominates, and we would expect that the generalization performance saturates as the order parameters in the energetic terms become independent of width $N$. Therefore, just as infinitely wide networks correspond to the GP limit, we propose that there exists a narrow width limit when the network width is extremely small compared to the number of training samples."}, {"title": "Robust learning of branches: the equipartition conjecture", "content": "What happens in the narrow width limit? In the following, we demonstrate that each branch will learn robustly at narrow width.\nConsider a student-teacher network setup, where the teacher network is given by\n$$f^*(G; \\Theta^*) = \\sum_l f_l^* (G; W_l^*) = \\sum_l \\frac{1}{\\sqrt{N}} \\sum_i \\beta_l^* h_l^*(G; W_l^*).$$$W_l^* \\sim N(0, \\sigma_1^2)$ and $\\alpha_l^* \\sim N(0, \\beta_l^2)$, where $\\beta_l^2$ is the assigned variance of the readout weight for the teacher branch $l$. Now we feed the data generated from this distribution $Y$ to the student network with the same number of branches, with student variance $\\sigma^2$.\nConsider $(YY^T)_{\\mu,\\nu} = \\sum_{i,j,l1,l2} \\frac{1}{N L^2} \\alpha_{l1}^* \\alpha_{l2}^* h_{l1}^* (G) h_{l2}^* (G)$; we conjecture that this quantity concentrates at its expectation value $E_{\\alpha^*, W^*} (YY^T) = \\sum_l \\frac{\\beta_l^2}{L} K_l$. Given this assumption, we can calculate the order parameters in the saddle point equation 40 (A.1) at 0 temperature. Ignoring the entropic term at narrow width, $r_l$ (Appendix A) becomes\n$$r_l - Y^T K^{-1} \\frac{\\frac{\\beta_l^2}{L} K}{K} K^{-1} Y \\approx T K^{-1} \\frac{\\frac{\\beta_l^2}{L} K}{K} K^{-1} - Tr[(K-\\sum \\frac{\\beta_l^2}{L}) K^{-1}] \\approx (T).$$One solution that satisfies the saddle point equations is\n$$\\alpha_l \\sigma^2 = \\frac{\\beta_l^2 \\sigma_1^2}{L}.$$We call this the equipartition conjecture, as the mean readout squared and the variance (A.3) has to exactly balance each other. Furthermore, by Eq.12, the student readout norm at each branch on"}, {"title": "Student-Teacher experiment on robust branch learning", "content": "We demonstrate this robust learning phenomenon and provide a first evidence of the equipartition conjecture with the student-teacher experiment setup introduced in the previous section. We use the contextual stochastic block model (CSBM) (Deshpande et al. [2018]) as the shared node input features, where the adjacency matrix is given by a stochastic block model with two blocks, and the node feature is generated with latent vectors corresponding to the two blocks (See B.1 for details)."}, {"title": "Convergence of Branch Importance at Narrow Width", "content": "An interesting aspect of the BPB-GNN network is that the branch norms seem to converge regardless of the hyperparameters $\\sigma$ and $L$, reflecting the natural branch importance of the dataset and task.\nAs shown in Figure 6, the BPB-GNN with $L = 6$ branches robustly learns the readout norms at narrow width, verifying the equipartition conjecture. The last branch of the BPB-GNN network has a larger contribution, reflecting the presence of higher-order convolutions in the Cora dataset. From a kernel perspective, increasing branches better distinguish the nodes, as shown in Figure 7. This could explain the selective turn-off of intermediate branches and the increased contribution of the last branch."}, {"title": "Discussion", "content": "The findings presented in this paper reveal that BPB-GNNs exhibit unique characteristics in the narrow width limit. Unlike the infinite-width limit where neural networks behave as Gaussian Process (GP) with task-independent kernels, narrow-width BPB-GNNs undergo significant kernel renormalization. This renormalization leads to symmetry breaking among the branches, resulting in more robust and differentiated learning. Our experiments demonstrate that narrow-width BPB-GNNs can retain and, in some cases, improve generalization performance compared to their wider counterparts,\nLimitations: We demonstrate that the bias term robustly decrease at narrow width; however, the variance term sometimes dominates and it is not clear BPB-GNN generalization is provably smaller at narrow width. Furthermore, our results rely on the Bayesian network setting, which might not directly transfer to understanding generalization of GNN trained with SGD, for which the loss landscape may be different."}, {"title": "Conclusion", "content": "In conclusion, this paper introduces and investigates the concept of narrow width limits in Bayesian Parallel Branching Graph Neural Networks. Contrary to the common belief that wider networks inherently generalize better, our results indicate that BPB-GNNs with significantly narrower widths can achieve better or competitive performance. This is attributed to effective symmetry breaking and kernel renormalization in the narrow-width limit, which lead to robust learning. Our theoretical analysis, supported by empirical evidence, establishes a new understanding of how network architecture influences learning outcomes. This work provides a novel perspective on neural network design and suggests further research into optimizing network structures for specific learning tasks, especially in scenarios where model simplicity and computational efficiency are crucial."}, {"title": "Details on Theory of BPB-GNN", "content": "Following similar derivations as the original kernel renormalization work Li and Sompolinsky [2021], we will integrate out the weights in the partition function $Z = \\int d\\Theta exp(-E(\\Theta)/T)$, from the"}]}