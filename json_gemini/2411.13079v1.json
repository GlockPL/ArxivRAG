{"title": "Neural Internal Model Control: Learning a Robust Control Policy via Predictive Error Feedback", "authors": ["Feng Gao", "Chao Yu", "Yu Wang", "Yi Wu"], "abstract": "Accurate motion control in the face of disturbances within complex environments remains a major challenge in robotics. Classical model-based approaches often struggle with nonlinearities and unstructured disturbances, while reinforcement learning (RL)-based methods can be fragile when encountering unseen scenarios. In this paper, we propose a novel framework, Neural Internal Model Control (Neural-IMC), which integrates model-based control with RL-based control to enhance robustness. Our framework streamlines the predictive model by applying Newton-Euler equations for rigid-body dynamics, eliminating the need to capture complex high-dimensional nonlinearities. This internal model combines model-free RL algorithms with predictive error feedback. Such a design enables a closed-loop control structure to enhance the robustness and generalizability of the control system. We demonstrate the effectiveness of our framework on both quadrotors and quadrupedal robots, achieving superior performance compared to state-of-the-art methods. Furthermore, real-world deployment on a quadrotor with rope-suspended payloads highlights the framework's robustness in sim-to-real transfer. Our code is released at https://github.com/thu-uav/NeuralIMC.", "sections": [{"title": "I. INTRODUCTION", "content": "Achieving stable and precise motion in complex, disturbed environments is essential for robots, such as quadrotors maintaining stable flight in wind fields and legged robots carrying loads over diverse terrains. This challenge, known as the robust control or adaptive control problem, has been extensively studied [1]. Classical adaptive control methods adjust control parameters in real-time to compensate for dynamic environmental changes, often using mathematical models of the system and disturbances [2], [3], [4], [5], while robust control methods utilize more informative signals to enhance robustness, like Internal Model Control [6]. Despite their guaranteed stability, these methods rely on accurately modeling system dynamics, which is challenging for non-linear systems with unstructured disturbances. In contrast, reinforcement learning (RL) methods have shown remarkable success in continuous control across various robot morphologies, including skill learning for armed robots [7], [8], high-speed quadrotor racing [9], [10], and adaptive locomotion for legged robots [11], [12], [13], [14], [15]. Large-scale training with randomized dynamics and environmental parameters enables RL-based control methods to generalize"}, {"title": "II. RELATED WORK", "content": "Classical adaptive controllers require estimating system parameters in a closed-loop manner to adjust control actions in real-time. L\u2081 adaptive control has been widely used to estimate force disturbances in quadrotors [22], [23]. Neural-Fly [18] incorporated pre-trained representations to enhance rapid online adaptation through deep learning, achieving stable flight in previously unseen wind conditions. Internal Model Control (IMC) [6], [3] is a general structure for robust control that uses a predefined model to predict system responses and estimate disturbances. However, applying IMC to more complex and nonlinear systems is challenging due to the difficulty of crafting reliable models for such systems. Long et al. [13] further proposed Hybrid Internal Model, in which they leverage a learning-based internal model with unsupervised contrastive learning. The learned model serves as a predictive encoder for estimating the system responses in the latent space. Despite their advantages, these methods either rely on explicit disturbance estimation or are heavily influenced by prediction accuracy, which can significantly affect performance if compromised."}, {"title": "B. Learning-based adaptive control", "content": "With the rapid advancement of highly parallel simulators [17], [24], [25], reinforcement learning (RL) methods are increasingly demonstrating powerful capabilities to achieve robust control, including quadrotors [26], [20] and legged robots [11], [12], [13], [14], [15]. Unlike classical model-based control, these learning-based methods often use model-free RL algorithms like PPO [27] to train neural controllers in large-scale randomized environments. Koryakovskii et al. [28] proposed to incorporate an RL-baesd policy to generate residual actions to compensate nonlinear model predictive control in model-mismatch cases. Rapid Motor Adaptation (RMA) [12] is a pure RL-based methods, which predicts environmental parameters in the latent space using state-action histories through a teacher-student training procedure. This approach has been extended to quadrotors [26], bipedal robots [29], and humanoid robots [15]. A follow-up study found that a dual-history architecture, leveraging both short-term and long-term input/output history, achieves superior performance with a straightforward one-stage training process [14]. Conversely, DATT [20] incorporates an L1 adaptation law [21] to estimate forces applied to quadrotors directly, which are then used as policy inputs. Despite its effectiveness with precise parameter tuning, this method is sensitive to handcrafted settings and challenging to generalize to complex systems."}, {"title": "III. METHODS", "content": "In this section, we present a general framework that bridges model-based and RL-based control through a simplified predictive model, named NeuralIMC. We start by introducing the basics of Internal Model Control (IMC) and our extension to integrate it with RL-based neural control (Sec. III-A). By leveraging the model-free RL algorithm PPO [27], we reduce the complexity and accuracy requirements typically associated with classical model-based control. Next, we detail how rigid transformation is used to predict the robot's body state, establishing a general approach to estimate disturbances and incorporate them into the control policy (Sec. III-B). Finally, we explain the application of this framework to two specific robot morphologies: quadrotors and quadrupedal robots (Sec. III-C)."}, {"title": "A. Internal Model Control", "content": "Internal Model Control (IMC) is a powerful structure for robust control, offering significant advantages over classical"}, {"title": "B. Design of a Simplified Predictive Model", "content": "The predictive model is crucial in our framework, providing the baseline needed to detect and compensate for system discrepancies and disturbances. Designing an effective predictive model involves two main challenges: accurately estimating the system response and ensuring the model's predictions closely match actual system behavior. Accurate estimation is essential for the feedback loop to reflect the true robot state, while alignment between predictions and real responses is critical for reliable performance under varying conditions.\nTo streamline state estimation across diverse robots, we model only the rigid body state, excluding actuator- or limb-specific states. For robots equipped with monocular or binocular cameras and built-in IMUs, numerous visual(-inertial) odometry methods provide robust state estimations [32], [33], [34], [35], which can be used as the actual system response within the IMC structure. By treating the robot as a rigid body and using Euclidean transformation to estimate the next state, we significantly reduce the modeling complexity, making it more practical and efficient.\nHere, we first present a general approach for formulating the predictive model and calculating the error, followed by specific adaptations for different robot morphologies in the next section. We assume that the model inputs include velocity or acceleration commands, and define the current body state as $x_k = [p_k, q_k, v_k, \\omega_k] \\in \\mathbb{R}^{13}$, where $p_k$ is the position, $q_k$ is the quaternion (orientation), $v_k$ is the velocity, and $\\omega_k$ is the angular velocity. The time interval between two steps is denoted as $\\Delta t$.\nThe next position and velocity of the body can be computed via Newton's law [36]:\n$p_{k+1} = p_k + v_k\\Delta t + \\frac{1}{2}a_k \\Delta t^2$,\n$v_{k+1} = v_k + a_k \\Delta t$.\nFor rotation, we adopt a discrete update rule while handling singularities. First, we compute the change in angle $\\Delta\\theta$ with the angular velocity $\\omega_k$ and angular acceleration $\\dot{\\omega_k}$:\n$\\Delta \\theta = \\omega_k \\Delta t + \\frac{1}{2}\\dot{\\omega_k} \\Delta t^2$.\nNext, we follow [37] to apply the exponential map to project the rotation from $\\mathbb{R}^3$ to $SO(3)$, obtaining the corresponding quaternion $q_{\\Delta\\theta}$:\n$q_{\\Delta\\theta} = \\begin{cases}  \\begin{bmatrix} \\cos(\\frac{\\|\\Delta\\theta\\|}{2}) \\\\\\ \\frac{\\Delta\\theta}{\\|\\Delta\\theta\\|} \\sin(\\frac{\\|\\Delta\\theta\\|}{2}) \\end{bmatrix}, & \\|\\Delta\\theta\\| < \\epsilon_{machine} \\\\ \\\\ [1, 0, 0, 0], & otherwise  \\end{cases}$"}, {"title": "C. Specifications for Common Robots", "content": "In this section, we further explain how to derive velocity or acceleration commands for various robots, including quadrotors and quadrupedal robots. We demonstrate how these commands align with existing control methods, ensuring seamless integration into the policy learning process.\nTraditional quadrotor controllers often use multi-layered cascaded structures [38], offering flexible options for integrating neural controllers. Recent research shows that using collective thrust and bodyrate (CTBR) as the action space for neural controllers provides significant advantages [39], [20], [9]. \u0421\u0422BR-based quadrotor controllers exhibit strong resilience to dynamics mismatches, effective domain transfer, and high agility and performance. This action space design is inherently compatible with our framework. We can directly use the bodyrate command from the policy to update the rotation, while the linear acceleration is computed using the collective thrust to translate the robot. Consequently, the predictive error, calculated from the previous policy output and the latest successive body states, can be seamlessly integrated into the current policy input without further modifications to the learning algorithm.\nUnlike quadrotors, legged robots, particularly quadrupedal and bipedal robots, have garnered significant attention for RL-based controllers due to their higher control dimensions and the complexities of modeling interactions with diverse terrains [8], [11], [12], [14], [19]. Typically, a neural controller for legged robots generates target actuator positions to achieve the desired body velocity, including forward velocity ($v_x$), lateral velocity ($v_y$), and yaw rate ($\\omega$). To integrate our predictive model into the control loop, we use the reference velocity command as the model input instead of the high-dimensional actuator motions. By differentiating two consecutive reference velocity commands, we obtain the linear acceleration in x and y axes, and the angular acceleration for the yaw angle. In this way, we can"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "To evaluate our framework's performance and versatility, we conduct experiments on quadrotors and quadrupedal robots, comparing against state-of-the-art methods in each domain. Then, we perform ablation studies on the feedback structure and noise tolerance of the simplified predictive model. Finally, we present sim-to-real deployment results on a quadrotor with two rope-suspended payloads, showcasing its effectiveness in enhancing system robustness."}, {"title": "A. Experiment Settings", "content": "For quadrotors, controllers are trained using a customized simulator based on the dynamics shown in [40], where the collective thrust and bodyrate commands are followed by a PD controller. The fourth-order Runge-Kutta method [41] updates quadrotor states at each simulation step with a update frequency at 200 Hz, and the controller's execution frequency is 50 Hz. Following Huang et al. [20], we train controllers on two types of reference trajectories, including smooth chained polynomial trajectories and infeasible zigzag trajectories. External disturbances are applied as randomized force perturbations within [-3.3 m/s\u00b2, 3.3 m/s\u00b2], representing Brownian motion variations as in [20]. Dynamical parameters are randomized at the start of each episode for training, and unseen parameters are used for evaluation to test robustness. The detailed randomization range is shown in Tab. I. We train and evaluate each method using five random seeds, reporting the average performance across 1,024 parallel trials. For evaluation, we test controllers trained with smooth trajectories on two types of randomly generated smooth paths: circle and chained polynomial, while controllers trained on infeasible paths are tested on 5-point star and zigzag trajectories.\nFollowing Long et al. [13], we train controllers for the Unitree Aliengo using Isaac Gym [17] with 4096 parallel environments and a 100-step rollout. The training curriculum includes progressively challenging terrains and reference commands. Controllers are"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel framework, Neural Internal Model Control (NeuralIMC), that combines model-based control with RL-based neural control. By focusing on body dynamics and adopting the white-box rigid-body transformation, we bypass high-dimensional nonlinearity and employ a simplified explicit predictive model to form a closed-loop feedback structure. Integrating model-free RL algorithms allows the neural control policy to react according to the predictive error feedback. Our method demonstrates superior performance on quadrotors and quadrupedal robots, surpassing state-of-the-art control methods in both domains. Real-world deployment on a quadrotor with heavy payloads highlights the robustness of our approach in handling disturbances and adapting to varying dynamics. This work underscores the potential of our combined framework for enhanced robustness and adaptability in robotic systems and suggests a direction for more interpretable learning-based control systems in future research.\nWhile extensive simulation experiments highlight the advantages of the proposed neural control framework, real-world evaluation remains limited. We plan to test our framework on a wider range of robots, including quadrotors of varying sizes and legged robots on complex terrains. Additionally, our method can be further strengthened by incorporating more effective disturbance training [45]."}, {"title": "APPENDIX", "content": "In this section, we describe the integration of the simplified predictive model discussed in Sec. III-B into the neural control pipeline of two widely used robotic systems: quadrotors and quadrupedal robots. We specifically detail the process of obtaining the necessary model inputs to update the predicted body states. For clarity, we omit the subscript k; all states-position x, quaternion q, velocity v, and angular velocity \u03c9 are from the previous time step, i.e., k = t - 1. Our goal is to predict the expected body state at the current time step t."}, {"title": "A. Quadrotors", "content": "For quadrotors, as demonstrated in Sec. III-C, we define the action space of the policy using mass-normalized collective thrust and body rates (CTBR). This is followed by a PD controller to manage the quadrotor's flight dynamics.\nWe denote the collective thrust as a and the body rates as \u03c9b, both expressed in the quadrotor's body frame. The acceleration in the world frame can then be computed as follows:\na = Tq(\\tau_a) + g\nwhere Tq represents the rotation of the z-axis acceleration from the body frame to the world frame, and g = [0,0, -9.81]T is the free-fall acceleration. Then, we can apply Eqs. (1) and (2) to obtain the predicted position x and velocity v.\nFor the rotation, we set \\dot{\\omega} to zero and directly replace \u03c9 with \u03c9b. Then, we can apply Eqs. (3) to (6) to obtain the predicted quaternion q and angular velocity \u03c9."}, {"title": "B. Quadrupedal Robots", "content": "For quadrupedal robots, we utilize reference velocity commands as the model inputs rather than the policy outputs. Given that the provided velocity commands do not cover all degrees of freedom, we set the unspecified parts to zero. We then use the command values as target quantities, calculating the required acceleration by taking the difference between the target and the actual state of the robot, in order to update the state.\nGiven the reference forward velocity vB, lateral velocity vB, and yaw rate \u03a9B, all defined in the body frame, we can compute the desired acceleration as follows:\na = \\begin{bmatrix} (v_x^B - v_x^B)/\\Delta t \\\\ (v_y^B - v_y^B)/\\Delta t \\\\ 0 \\end{bmatrix}\nIn this context, vB and vB denote the forward and lateral velocities of the body from the previous step, respectively, transformed into the body frame. The world-frame acceleration can then be obtained by applying the transformation Tq:\na = Tq(aB)\nSimilarly, the angular acceleration can be computed as:\n\\dot{\\omega} = \\begin{bmatrix} 0 \\\\ 0 \\\\ (\\Omega_B - \\Omega_B)/\\Delta t  \\end{bmatrix}\nSubsequently, the computed acceleration a and angular acceleration \u03c9 can be substituted into the relevant equations (Eqs. (1) to (6)) to predict the next body state x."}, {"title": "C. Quadrotors", "content": "We employ a customized quadrotor simulator to conduct our experiments. For detailed formulations on quadrotor dynamics, we refer readers to [40]. To ensure a broad spectrum of domain randomization, we compute the inertia parameters based on the randomized size and mass of the quadrotor. This approach prevents the generation of unrealistic configurations by avoiding the independent randomization of these physical parameters.\nWe denote the arm length of the quadrotor as L. The width and height of the quadrotor are given by \u03b1wL and \u03b1hL, respectively, where \u03b1w and \u03b1h represent the width-to-length and height-to-length ratios. Suppose the mass of the quadrotor is M, then the inertia matrix is computed as:\nI = \\begin{bmatrix} \\frac{M}{12} (L^2(+ \\alpha_w^2)) & 0 & 0 \\\\ 0 &  \\frac{M}{12} L^2(\\alpha_w^2 +  \\alpha_h^2) & 0 \\\\ 0 & 0 &  \\frac{M}{12} L^2(2\\alpha_w^2)  \\end{bmatrix}"}]}