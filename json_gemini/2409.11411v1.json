{"title": "AIVRIL: AI-DRIVEN RTL GENERATION\nWITH VERIFICATION IN-THE-LOOP", "authors": ["Mubashir ul Islam", "Humza Sami", "Pierre-Emmanuel Gaillardon", "Valerio Tenace"], "abstract": "Large Language Models (LLMs) are computational models capable of performing complex natural\nlanguage processing tasks. Leveraging these capabilities, LLMs hold the potential to transform the\nentire hardware design stack, with predictions suggesting that front-end and back-end tasks could\nbe fully automated in the near future. Currently, LLMs show great promise in streamlining Register\nTransfer Level (RTL) generation, enhancing efficiency, and accelerating innovation. However, their\nprobabilistic nature makes them prone to inaccuracies a significant drawback in RTL design, where\nreliability and precision are essential.\n\nTo address these challenges, this paper introduces AIVRIL, an advanced framework designed to\nenhance the accuracy and reliability of RTL-aware LLMs. AIVRIL employs a multi-agent, LLM-\nagnostic system for automatic syntax correction and functional verification, significantly reduc-\ning-and in many cases, completely eliminating-instances of erroneous code generation. Experi-\nmental results conducted on the VerilogEval-Human dataset show that our framework improves code\nquality by nearly 2\u00d7 when compared to previous works, while achieving an 88.46% success rate\nin meeting verification objectives. This represents a critical step toward automating and optimizing\nhardware design workflows, offering a more dependable methodology for AI-driven RTL design.", "sections": [{"title": "1 Introduction", "content": "In the swiftly advancing domain of Artificial Intelligence (AI), Large Language Models (LLMs) have risen as revolu-\ntionary tools, possessing the power to transform a multitude of sectors significantly. The specialized sphere of hardware\ndesign is no exception as these models have become increasingly influential, particularly due to their proficiency in\nautomating the generation of Register Transfer Level (RTL) code which could lead to a significant stride towards the\nautomation and optimization of hardware design workflows [1,2].\n\nDespite their capabilities, LLMs are not without limitations. Being probabilistic by nature, they are prone to encountering\nsyntax and logical discrepancies, thus mirroring the challenges ubiquitous in traditional programming settings. This\nissue, however, highlights a critical gap: although LLMs can generate code swiftly and efficiently, the accuracy and\nfunctionality of the produced code necessitate thorough verification and refinement. As a byproduct, the benefit of\nstreamlined code generation might be offset by a potentially unsustainable increase in the burden associated with the\nverification process.\n\nAlthough still in its infancy, the field of Generative AI (GenAI) for RTL design has already captured significant interest.\nEarly research focused primarily on the pure code generation capabilities of LLMs [3]. More recent studies have begun\nto incorporate specific safeguards designed to detect and automatically correct syntax errors [2, 4, 5], significantly\nreducing inaccuracies. Yet, as of now, no existing works have fully integrated robust verification mechanisms."}, {"title": "2 Background & Related Work", "content": "Recent advancements in decision-making processes for multi-agent systems have significantly influenced perspective\nhardware design workflows through the application of GenAI. This section provides an overview of the integration of\nverbal reasoning and action planning in autonomous systems and examines the growing role of GenAI in RTL design,\nhighlighting both recent progress and ongoing challenges."}, {"title": "2.1 Decision-Making in Multi-Agent Systems", "content": "Recent work has been focusing on the integration of verbal reasoning and interactive decision-making within autonomous\nsystems, where LLMs have demonstrated to exhibit advanced capabilities in processing multiple reasoning steps to\nextract answers from various tasks, e.g., arithmetic, commonsense, and symbolic reasoning, thus showing the LLMs'\nadeptness in navigating intricate reasoning pathways [7]. However, the fact that models tend to rely on their own\ninternal representations, without any external real-world grounding, proved to be a strong limitative factor for reasoning,\nresulting in increased inaccuracies or hallucination rates as they progress through the chain-of-thought sequence. Other\nstudies have explored LLM agents while executing tasks within interactive environments and then make language-based\npredictions to formulate action plans [8]. In this context, ReAct [9] stands out as a pivotal example of this new paradigm.\nThrough reasoning traces, ReAct aids the model in inducing, monitoring, and updating action plans while managing\nexceptions. Concurrently, actions serve as a medium for the model to interact with and assimilate additional information\nfrom external resources, such as knowledge bases, environmental data, or other agents, enhancing its decision-making\nabilities and contextual awareness. As detailed in the next section, most recent contributions in GenAI for RTL design\nsignificantly converge on this paradigm, directly or indirectly, yielding considerable enhancements in the quality of\noutcomes."}, {"title": "2.2 Advancements and Challenges of GenAI for RTL Design", "content": "Chip-Chat [10] marked a pioneering effort in emulating a conventional hardware workflow, from design to tapeout,\nutilizing \"generalist\u201d LLMs such as ChatGPT throughout the process. Following the introduction of other advanced,\nopen-source LLMs like Llama, CodeGen, and many others [11], the realm of RTL code generation has seen a significant\nboost. Since generating RTL is akin to producing any other programming language, the primary focus has rapidly shifted\non enhancing the reliability of these innovative LLM-driven systems. With solutions ranging from domain-adapted\nLLMs [2] and data augmentation techniques [5], to approaches that integrate an existing knowledge base [4] through\nthe Retrieval-Augmented Generation (RAG) mechanism [12], the current trend leans heavily towards ReAct-based\nsolutions. However, a common limitation among these works is their lack of proper built-in EDA functional verification\nmechanisms, which negatively affects perceived user feedback. Additionally, many of these solutions rely on fine-tuned\nmodels, restricting their adaptability across different settings. This work aims to enhance the reliability of LLM-"}, {"title": "3 The AIVRIL Framework", "content": "In this section, we introduce the two core components of the AIVRIL framework, as shown in Figure 1: AutoReview\nand AutoDV (Automatic Design Verification). AutoReview is a design-focused loop that enforces syntax correctness in\ngenerated RTL, while AutoDV is a verification loop targeting functional correctness. Both components coordinate auto-\nmated multi-agent procedures that enhance output quality through systematic iterations and collaborative interactions\nbetween agents."}, {"title": "3.1 AutoReview", "content": "AutoReview is responsible for enforcing syntax checks and providing automatic corrections for RTL code generated\nby LLMs. The process begins with the Code Agent, which receives an initial user prompt. These prompts can vary\nsignificantly in detail and complexity, and are categorized into three primary scenarios:\n\n Case I: Detailed Prompts - In this scenario, the user provides a comprehensive description of the desired\ndesign, including specific functionalities and constraints. The Code Agent uses this information to generate\ncandidate RTL implementations and corresponding testbench code. This approach minimizes the need for\nfurther interaction, assuming the initial input is complete and precise.\n\n Case II: Vague Prompts - When user prompts are broad or lack specifications, the Code Agent engages\ninteractively with the user, asking clarifying questions to gather necessary details. This adaptive querying\nhelps refine the prompt to a level where an actionable RTL design can be generated. In practice, this behavior\nis realized through ad hoc system prompts that dynamically adapt the conversation, guiding further inputs to\nensure the generated RTL code aligns with the user's implicit requirements.\n\n Case III: Task-Based Prompts For prompts that provide a partial or complete RTL description with a\nrequest for compilation or verification, the Code Agent utilizes the provided data to generate a testbench and"}, {"title": "3.2 AutoDV", "content": "AutoDV encapsulates the AutoReview process, therefore concepts introduced in Section 3.1 still hold in this scenario.\nThe verification workflow begins with a syntactically correct RTL description, which is subjected to simulation and\ncoverage analysis. The output from these simulations, including coverage reports and any failed assertions, is then\nanalyzed by the Review Agent.\n\nIn this phase, the Review Agent identifies discrepancies, such as failed assertions or coverage gaps, and formulates\ncomprehensive review prompts that address both functional and code coverage anomalies. These prompts are fed back\nto the Code Agent, guiding subsequent iterations of the RTL code. The loop continues until a predefined coverage\nthreshold is achieved, typically set to 90% or higher to ensure comprehensive verification.\n\nThe AutoDV process is tailored to handle various verification challenges, including nuanced aspects of RTL behavior\nthat may not be explicitly stated in the initial design specifications. By iterating between code generation and verification,\nAIVRIL ensures that the RTL code not only compiles correctly but it also mitigates functional discrepancies."}, {"title": "3.3 Integration with EDA Tools and Third-Party LLMs", "content": "AIVRIL is designed with versatility at its core, being both tool-agnostic and LLM-agnostic. Although this paper focuses\non describing specific tools for RTL compilation and coverage analysis, the framework is flexible enough to integrate a\nbroad range of EDA tools, whether open-source or commercial. This allows users to select tools that align with their\nexisting workflows and project requirements without compromising functionality.\n\nIn the same vein, AIVRIL is not dependent on any specific LLM. It can work with various LLMs capable of processing\nRTL code generation and verification, allowing users to select the model that best fits their requirements.\n\nThis dual agnosticism enables AIVRIL to be seamlessly incorporated into diverse hardware design environments with\nminimal adaptation, providing a robust and flexible solution that enhances RTL generation and verification across\ndifferent workflows."}, {"title": "4 Experimental Results", "content": "In this section, we present the experimental evaluation of the proposed AIVRIL framework. Our objective is to\nrigorously assess the performance and robustness of our tool across a diverse set of benchmarks, ensuring a fair and\ncomprehensive analysis of its capabilities. To achieve this, we selected key evaluation metrics that highlight the\nadvantages of our approach in realistic and unbiased scenarios. Specifically, we focused on metrics that capture both\nsyntax and functional correctness, as well as success rates in meeting verification objectives, providing a holistic view\nof AIVRIL's effectiveness in design and verification tasks."}, {"title": "4.1 Methodology", "content": "We employed all the 156 benchmarks from the VerilogEval-Human dataset [13] in our experiments, thus encompassing\na wide range of design complexities. For the AutoReview syntax correction loop, we assess performance using the\nunbiased pass@k estimator (with k = 1) as described in [14], noting that we differentiate between pass@k_{syntax},\nwhich solely indicates the success rate of designs passing all syntax checks, and pass@k_{functional}, which reflects the\nsuccess rate of designs that are not only syntactically correct but also functionally accurate. For AutoDV, we evaluate\nits robustness by measuring again pass@k_{functional} and its success rate in achieving a predetermined total coverage\nthreshold, set at >90% for this experiment. This success rate refers to the percentage of benchmarks that met this\ncoverage target, ensuring that at least 90% of the design's functionality was tested and validated. It is worth noting\nthat in both scenarios, pass@k_{functional} was obtained by executing the testbenches provided by the benchmark suite,\nensuring comprehensive validation of the entire approach. For both syntax checks and functional simulation stages, we\nrelied on Icarus Verilog [15], whereas Covered [16] was used for coverage analyses."}, {"title": "4.2 Results & Discussion", "content": "Figure 2 compares the performance of the proposed AIVRIL framework when using Claude 3.5 Sonnet, GPT4-0,\nand, Llama3 70B, focusing on the impact of syntax and functional errors reduction through the AutoReview process.\nFigure 2-a shows the total number of syntax errors for each model under baseline conditions and after applying\nAutoReview. The results indicate that AutoReview significantly reduces syntax errors across all models. Both GPT4-0\nand Claude 3.5 Sonnet exhibit complete elimination of syntax errors, dropping from 389 and 139 errors, respectively, to\nzero after AutoReview. In contrast, Llama3 70B shows a more modest reduction, with syntax errors decreasing from\n124 at baseline to a total of 9 after AutoReview.\n\nThese results are further highlighted in Figure 2-b, where pass@k_{syntax} (red area) and pass@k_{functional} (blue area)\nare depicted for each configuration. It is noteworthy how the proposed framework not only enhances syntax correctness\nbut also significantly boosts functional correctness across all benchmarks. The most substantial gains are seen with\nGTP4-0, where pass@k_{functional} with AutoReview improves over the baseline by 21.2%. Similarly, Claude 3.5 Sonnet\nand Llama3 70B exhibit improvements of approximately 11% and 12%, respectively.\n\nFigure 3 illustrates the performance of the AutoDV loop. In particular, for each evaluated LLM, we present the\npass@k_{functional} for the baseline (green bar), the pass@k_{functional} after applying AutoDV (orange bar), and the\nsuccess rate in meeting the total coverage target percentage (yellow bar). The plot reveals that AutoDV further enhances\nthe performance of the proposed AIVRIL framework, resulting in a 23.2% increase in pass@k_{functional} compared to\nthe baseline for GPT4-0. Consistent with previous findings, Claude 3.5 Sonnet and Llama3 70B show more modest\ngains of 16% and 16.5%, respectively. In terms of meeting the predefined coverage threshold, LLama3 70B recorded\nthe lowest success rate at 78.85%, while the other two models exceeded a success rate of 87%."}, {"title": "4.3 Comparison with State-of-the-Art Approaches", "content": "As highlighted in Section 2.2, recent frameworks and fine-tuned LLMs have been proposed to improve RTL code quality\nin the context of GenAI solutions. RTLFixer [4] introduced a ReAct and RAG-based approach that leverages error\nlogs to iteratively correct errors, with a claimed pass@1_{functional} of 36.8% in the best case. CodeV [6] is a collection\nof open-source instruction-tuned Verilog-driven LLMs. This solution employs fine-tuned versions of CodeLlama 7B"}, {"title": "5 Conclusions", "content": "The results presented in this paper demonstrate the effectiveness of the proposed AIVRIL framework in enhancing\nthe accuracy and reliability of GenAI for RTL. By integrating automated review and verification loops-AutoReview\nand AutoDV-the framework significantly reduces syntax errors and improves functional correctness across various\nbenchmarks. The observed improvements, particularly with GPT4-0, underscore the potential of AIVRIL to address\ncommon pitfalls in LLM-generated code, advancing the state-of-the-art in automated hardware design.\n\nThe complete elimination of syntax errors in GPT4-o and Claude 3.5 Sonnet, coupled with notable gains in\npass@k_{functional}, highlights the robustness of the AutoReview process. Similarly, the AutoDV loop further re-\nfines the verification process, achieving significant performance boosts and high success rates in meeting coverage\ntargets. These results validate the framework's capability to seamlessly integrate syntax correction and verification,\nenhancing the overall design quality without the need for extensive fine-tuning of the adopted LLMs.\n\nFurthermore, the orthogonality of AIVRIL to different models and environments, as demonstrated by the diverse\nperformance gains observed with Claude 3.5 Sonnet and Llama3 70B, positions the framework as a versatile and\neffective GenEDA tool. Future work will focus on further expanding the framework's verification capabilities and\nexploring its integration with other LLMs and EDA tools to foster more comprehensive and efficient hardware design\nconsolidation.\n\nIn summary, AIVRIL represents a significant advancement in the field of GenAI for RTL design, offering a reliable,\ntransparent, and efficient approach to overcoming the inherent challenges of LLM-based design automation. Its\nmulti-agent system provides a new benchmark for integrating generative AI with verification mechanisms, paving the\nway for future innovations in automated hardware design."}]}