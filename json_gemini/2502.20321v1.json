{"title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding", "authors": ["Chuofan Ma", "Yi Jiang", "Junfeng Wu", "Jihan Yang", "Xin Yu", "Zehuan Yuan", "Bingyue Peng", "Xiaojuan Qi"], "abstract": "The representation disparity between visual generation and understanding imposes a critical gap in integrating these capabilities into a single framework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer that encodes fine-grained details for generation while also capturing high-level semantics for understanding. Despite recent studies have shown that these objectives could induce loss conflicts in training, we reveal that the underlying bottleneck stems from limited representational capacity of discrete tokens. We address this by introducing multi-codebook quantization, which divides vector quantization with several independent sub-codebooks to expand the latent feature space, while avoiding training instability caused by overlarge codebooks. Our method significantly raises the upper limit of unified discrete tokenizers to match or even surpass domain-specific continuous tokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus 0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on ImageNet.", "sections": [{"title": "1. Introduction", "content": "Autoregressive modeling has recently extended its predominance in natural language processing to visual domains. This is characterized by the rapid growth of autoregressive models in image and video generation (Kondratyuk et al., 2023; Tian et al., 2024; Li et al., 2024c), as well as the increasing prevalence of Multimodal Large Language Models (MLLMs) for diverse visual understanding tasks (Liu et al., 2024a; Tong et al., 2024a; Wu et al., 2024c). Given these advancements in individual fields, there is a growing interest in developing unified MLLMs by integrating visual generation and understanding within a single autoregressive framework (Liu et al., 2024b; Team, 2024; Wang et al., 2024).\nHowever, a critical challenge persists in unification: the disparity in visual tokenization for understanding and generation. For instance, the CLIP (Radford et al., 2021) tokenizer, widely used in multimodal understanding, does not naturally fit into generative modeling, which requires precise encoding of fine-grained details. Conversely, the VQVAE (Van Den Oord et al., 2017) tokenizer, which is tailored for autoregressive generation, potentially falls short of capturing high-level semantics crucial for visual understanding (Xie et al., 2024). To address this issue, recent studies attempt to use separate tokenizers for each task (Wu et al., 2024a), yet this increases model complexity without fundamentally bridging the gap in representation. These limitations underscore the need for a unified visual tokenizer to serve both generation and understanding objectives.\nRecently, VILA-U (Wu et al., 2024d) has proposed a promising paradigm for unified tokenizers, which integrates CLIP supervision into VQVAE training to complement VQ tokens with text alignment and rich semantics, as illustrated in Figure 1(a). Despite this innovation, it has been observed that the unified tokenizer struggles to converge in training and tends to underperform domain-specific tokenizers. This is commonly attributed to loss conflicts arising from divergent optimization goals (Wu et al., 2024d;a). However, recent studies on visual generation suggest that these objectives might not inherently conflict, as evidenced by the alignment behavior in generative and discriminative representation learning (Yu et al., 2024). This contrast raises an important question: Do reconstruction and contrastive losses truly conflict in unified tokenizer training, or might there be an underlying bottleneck that remains unidentified?"}, {"title": "2. Related Work", "content": "Image Tokenization for Generation. In the domain of visual generation, image tokenization plays an important role in encoding raw pixels into compact latent features for generative modeling (Van Den Oord et al., 2017; Rombach et al., 2022a). Particularly, among a variety of tokenizers, the vector-quantized tokenizer (Van Den Oord et al., 2017) is favored for its discrete latent space and compatibility with autoregressive or masked generative models (Tian et al., 2024; Sun et al., 2024a; Chang et al., 2022; Yu et al., 2023a). The pioneering work VQVAE (Van Den Oord et al., 2017) initially introduced the concept of discretizing continuous tokens by mapping them to the nearest neighbors in a learnable codebook. Built on this, VQGAN (Esser et al., 2021) incorporated perceptual loss (Zhang et al., 2018) and discriminator loss (Karras et al., 2019) to significantly improve the reconstruction quality. Subsequently, ViT-VQGAN (Yu et al., 2021) advanced the framework with the transformer architecture. In recent literature, considerable efforts have been devoted to developing better quantization methods such as residual quantization (Lee et al., 2022) and lookup-free quantization (Yu et al., 2023b), which also constitute a focal point of this paper.\nImage Tokenization for Understanding. The unprecedented success of large language models (LLMs) (Wei et al., 2022; Achiam et al., 2023) has catalyzed the development of multimodal large language models (MLLMs) (Liu et al., 2024a; Lin et al., 2024; McKinzie et al., 2024). As a critical component of MLLMs, the selection of an effective vision tokenizer has been the subject of extensive study (Wang et al., 2023; Tong et al., 2024a). A common choice of the vision tokenizer is the pretrained CLIP model (Radford et al., 2021), which undergoes alignment with language during its pretraining phase. While self-supervised learning models, such as DINOv2 (Oquab et al., 2023), are shown to be advantageous at region-level tasks (Ma et al., 2025). Cambrian-1 (Tong et al., 2024a) further demonstrates that MLLMs can benefit from hybrid representations from a mixture of vision encoders. Nonetheless, these tokenizers predominantly encode images into continuous tokens, presenting a challenge for uniformly modeling both vision and text tokens. To address this disparity, several works have explored discretizing CLIP tokens (Ge et al., 2023) or employing VQVAE encoders (Liu et al., 2024b; Xie et al., 2024). However, these approaches have been observed to substantially impair understanding performance of MLLM.\nUnified Vision-Language Models. The rise of MLLMs is not limited to the realm of visual understanding. Recent advancements have witnessed an increasing focus on unifying visual generation and understanding within one MLLM (Dong et al., 2023; Wu et al., 2023; Team, 2024; Zhou et al., 2024; Xie et al., 2024; Tong et al., 2024b). Specifically,"}, {"title": "3. Method", "content": "In this section, we introduce UniTok, a unified visual tokenizer well-suited for both generation and understanding tasks. We start with a unified training recipe that integrates reconstruction (VQVAE) and contrastive (CLIP) supervisions (Section 3.1). However, we find that simply combining both training objectives leads to severe performance degradation, which can be mainly attributed to limited representational capacity of discrete tokens (Section 3.2). To this end, we propose multi-codebook quantization and attention factorization to enhance the latent feature space and derive unified visual representations (Section 3.3). An overview of the framework is presented in Figure 2.\nVisual generation and understanding typically impose distinct demands on the visual tokenizer. For instance, generation emphasizes lossless compression for accurate reconstruction, whereas understanding prioritizes semantically meaningful and discriminative features. To accommodate both requirements, we jointly train the tokenizer with (i) a VQVAE-based reconstruction loss to preserve low-level information, and (ii) an image-text contrastive loss that enhances high-level semantics of the features.\nTo be specific, the VQVAE-based loss term $\\mathcal{L}_{recon}$ consists of a pixel-level reconstruction loss $\\mathcal{L}_{R}$, a perceptual loss $\\mathcal{L}_{p}$ based on the LPIPS metric (Zhang et al., 2018), a discriminator loss $\\mathcal{L}_{G}$ to enhance reconstruction fidelity (Karras et al., 2019), and a vector quantization loss $\\mathcal{L}_{vq}$ to minimize distance between the encoder output and its nearest code entry. It is denoted as:\n$\\mathcal{L}_{recon} = \\mathcal{L}_{R} + \\lambda_{vq}\\mathcal{L}_{vq} + \\lambda_{p}\\mathcal{L}_{p} + \\lambda_{G}\\mathcal{L}_{G}$, (1)\nwhere $\\lambda$ is the weight factor for the corresponding loss term. The image-text contrastive loss term $\\mathcal{L}_{contra}$ is basically the same as in CLIP (Radford et al., 2021). Therefore, the final loss term of UniTok can be written as:\n$\\mathcal{L} = \\mathcal{L}_{recon} + \\lambda_{contra} \\mathcal{L}_{contra}$. (2)\nWe simply choose $\\lambda_{contra} = 1$ in this paper."}, {"title": "3.2. Quantization Bottleneck", "content": "Despite being augmented with CLIP supervision, we find that the unified tokenizer still exhibits unsatisfactory performance in visual understanding tasks, significantly lagging behind the commonly used CLIP tokenizer. To figure out the underlying causes of this underperformance, we break down the key components involved in training a unified tokenizer, as illustrated in Figure 3. Starting with the CLIP baseline, we provide a step-by-step walk-through and ablation of all changes in the following paragraphs.\nFactorization. Modern VQ-tokenizers typically project continuous tokens to a lower-dimensional latent space for code index lookup (e.g. from 768-d to 16-d), known as token factorization (Yu et al., 2021). This increases the relative density of codes by compressing the latent code space, thereby reducing quantization error. To evaluate the impact of factorization in CLIP training, we add two linear projection layers on top of the CLIP vision encoder (right before average pooling), which transforms tokens from 768-d to 16-d and then back to 768-d. Notably, vector quantization and reconstruction supervision are not included at this stage. Surprisingly, it turns out that this channel compression operation significantly compromises the expressiveness of tokens, leading to severe performance degradation in downstream VQA tasks.\nDiscretization. Based on the implementation described above, we further introduce vector quantization to CLIP training, which maps factorized tokens to their nearest code entries. Compared to language tokenizers with vocabularies exceeding 200k entries, the vocabulary size of modern VQ-tokenizers is markedly smaller (i.e., typically ranging from 4k to 16k). Mapping continuous tokens to such a small codebook results in considerable information loss. This is validated in our experiment, which demonstrates that discretizing the factorized tokens with a 16k codebook causes an average accuracy drop of 2.1 in VQA tasks.\nReconstruction Supervision. Finally, we integrate reconstruction losses into the training process to build a unified tokenizer, as outlined in Section 3.1. Previous literature suggests that loss conflict between VQVAE and CLIP is a major cause of performance degradation in joint training (Wu et al., 2024d). We observe a similar phenomenon where joint training results in sub-optimal ImageNet zero-shot classification accuracy and reconstruction FID compared to specialized training. However, surprisingly, we find that this degradation has negligible impacts on downstream understanding performance. Moreover, the degradation in classification accuracy and reconstruction FID diminishes after we improve the quantization methods (detailed in the next section). Based on these observations, we speculate that the perceived loss conflict is only a superficial issue, and the primary cause of the underperformance lies in the limited representational capacity of discrete tokens."}, {"title": "3.3. UniTok", "content": "A straightforward solution to breaking the quantization bottleneck could be increasing the codebook size and the latent code dimension. However, current studies on VQVAE tokenizers suggest that there is diminishing gain in scaling and the performance saturates after the codebook size reaches 16k (Yu et al., 2023b; Sun et al., 2024a). Continuing expansion results in a substantial portion of codes being rarely used or becoming 'dead' during training, which negatively impacts downstream task performance (Yu et al., 2021). To address this issue, we propose multi-codebook quantization and attention factorization in the following paragraphs.\nMulti-codebook quantization (MCQ) discretizes the latent tokens with a set of independent codebooks. Specifically, the latent vector $f \\in \\mathbb{R}^{d}$ is first evenly split into n chunks $\\{f_{1}, f_{2}, ..., f_{n}\\}$, where $f_{i} \\in \\mathbb{R}^{\\frac{d}{n}}$. The subsequent quantization process is denoted as:\n$f = Concat\\left(Q(Z_{1}, f_{1}), Q (Z_{2}, f_{2}), ..., Q (Z_{n}, f_{n})\\right)$ (3)\nwhere f is the discretized latent vector, Q is the code index lookup operation, and $Z_{i}$ is i-th sub-codebook. Compared to conventional quantization methods, the proposed MCQ effectively scales up the vocabulary size. For instance, by increasing the number of sub-codebooks from 1 to 4, and suppose each sub-codebook contains 16k code entries, the theoretical vocabulary size exponentially increases from $2^{14}$ to $2^{56}$ (i.e., there are up to $2^{14 \\times 4}$ possible combinations of codes for each token). As the size of each individual codebook remains constant, it circumvents the optimization problem associated with large codebooks. Besides, the dimensionality of the latent codes also scales proportionally with the number of codebooks (i.e., increasing from 16-d to 64-d in this case), which further enhances the representational capacity of discrete representations.\nAttention factorization. Existing VQ methods usually employ linear or convolutional projection layers for token factorization. But as shown in Section 3.2, this over-simplified design fails to preserve rich semantics in original tokens, leading to degraded understanding performance. To alleviate this problem, we suggest adapting the multi-head attention modules for factorization, as illustrated in Figure 4. Despite its simplicity, we find this design effectively strengthens the representational power of factorized tokens. Notably, to ensure compatibility with autoregressive generation, the factorization blocks are configured with causal attention."}, {"title": "3.4. Unified MLLM", "content": "We proceed to develop a unified multimodal model with UniTok. Particularly, we leverage the unified framework introduced in Liquid (Wu et al., 2024b), which models (discrete-valued) vision and language sequences with a universal next-token prediction loss. But instead of learning the visual codebook from scratch, we reuse code embeddings of UniTok by projecting them to the MLLM token space with an MLP projector. Notably, despite UniTok encodes an image into $H \\times W \\times K$ codes (where K represents the number of sub-codebooks), we simplify this for MLLM input by merging every K consecutive codes into a single visual token. Similarly, when it comes to visual token prediction, we make each token autoregressively predict the next K codes, using a depth transformer head as implemented in RQ-Transformer (Lee et al., 2022) and VILA-U (Wu et al., 2024d). This design maintains efficiency for visual generation in the context of multi-codebooks."}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nTokenizer Setup. Leading VQVAE tokenizers predominantly adopt the CNN architecture, while ViT is preferred in CLIP training for its scalability. To take advantage of both, we choose a hybrid architecture, ViTamin-L/16 (Chen et al., 2024), to instantiate UniTok. We configure UniTok with eight sub-codebooks, each containing 4,096 code entries and a latent dimension set to 8-d (the global latent dimension is thus 64-d). The discriminator is initialized with pretrained DINOv2-S (Oquab et al., 2023). We train the tokenizer for one epoch on the public dataset DataComp-1B (Gadre et al., 2024) consisting of 1.28B image-text pairs, with all images resized to 256 \u00d7 256 resolution and a global batch size of 16k. The learning rate is set to 1e-3 for the tokenizer and 2e-4 for the discriminator. Besides, we prepare two settings for evaluation: one with pretrained CLIP weight initialization and one with random initialization (the default setting).\nMLLM Setup. We instantiate a unified MLLM described in Section 3.4 with the Llama-2-7B base model (Touvron et al., 2023). Following Liquid, we first pretrain the model on a mix of multimodal data, which is composed of 10M language data from DCLM (Li et al., 2024b), 30M internal MidJourney-style synthetic data, and 30M re-captioned image-text pairs from COYO (Minwoo et al., 2022) and Laion (Schuhmann et al., 2022). Subsequently, we finetune the model on 1.5M text-to-image data and 1.5M multimodal instruction tuning data introduced in Mini-Gemini (Li et al., 2024d). Specifically, the learning rate is set to 5e-5 in the pretraining stage and 2e-5 in the finetuning stage. For visual understanding evaluation, we report results on standard VQA benchmarks including VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), TextVQA (Singh et al., 2019), POPE (Li et al., 2023), MME (Yin et al., 2023), and MM-Vet (Yu et al., 2023d). For visual generation evaluation, we report results on GenAI-Bench (Lin et al., 2025) and MJHQ-30K (Li et al., 2024a)."}, {"title": "4.2. Tokenizer Comparison", "content": "Table 1. Comparison with modern tokenizers on ImageNet reconstruction FID and zero-shot classification accuracy. The rFID is measured at 256 \u00d7 256 resolution with 16\u00d7 downsample ratio. \u2020 indicates the model uses pretrained CLIP weights for initialization.\n4.3. Visual Understanding Performance\nWe evaluate the understanding performance of UniTok on diverse VQA benchmarks in Table 2. Our unified MLLM showcases clear advantages when compared to other unified models that also utilize a discrete visual tokenizer. Specifically, UniTok significantly outperforms the Chameleon model, which relies on a traditional VQVAE tokenizer, by 7.2% higher accuracy on VQAv2. Additionally, it surpasses VILA-U, another model with a unified tokenizer, by 3.3% in accuracy on the TextVQA benchmark and by a notable margin of 112 points on the MME-Perception scores. Furthermore, we can see that UniTok largely narrows the performance gap with MLLMs that incorporate continuous visual tokenizers. These strong results confirm the candidacy of UniTok as a unified visual tokenizer for multimodal models.\n4.4. Visual Generation Performance\nQuantitative Results. Table 3 presents the text-to-image generation performance of our unified MLLM on GenAI-Bench (advanced prompts)\u00b9, a challenging benchmark that measures the alignment of generated images with text\n4.5. Ablation Studies\nImpact of Supervision Types. To ablate the impact of contrastive and reconstruction losses in UniTok training, we conduct experiments on tokenizers trained with different supervision types, as shown in Table 5. It is worth noting that all the tokenizers are vector-quantized even though some do not have reconstruction supervision. First, we show that reconstruction-oriented tokenizer significantly lags behind tokenizers with contrastive supervision in visual understanding performance. This observation evidences the limitations of traditional VQVAE. Second, we demonstrate that reconstruction and contrastive training objectives do not inherently conflict, or can be addressed by enhancing discrete feature space. With multi-codebook quantization, the jointly trained tokenizer not only exhibits understanding performance on par with the tokenizer trained solely with contrastive loss, but also slightly improves generation performance over the reconstruction-oriented tokenizer.\nNumber of Sub-Codebooks. To gain deeper insights into multi-codebook quantization, we evaluate how tokenizer performance changes with the number of sub-codebooks. Specifically, for rFID evaluation, we train the tokenizer solely with reconstruction loss on OpenImages (Kuznetsova et al., 2020), and evaluated it on ImageNet (256 \u00d7 256) validation set. While for ImageNet zero-shot accuracy evaluation, the tokenizer is trained on DataComp-1B 128m subset using only contrastive loss. As shown in Table 6, given a constant global codebook size, increasing the number of sub-codebooks consistently improves reconstruction FID and classification accuracy. This indicates that multi-codebook quantization generally benefits vector-quantized models, independent of the training objectives.\nCLIP Weight Initialization. In Table 7, we ablate the impact of CLIP weight initialization on visual understanding performance. Specifically, we adopt the classic LLaVA framework for evaluation, replacing the original CLIP tokenizer with UniTok while keeping all other the training settings unchanged. One tokenizer is initialized with the pretrained ViTamin-L-256 (Chen et al., 2024) weights, while the other is randomly initialized. To our surprise, UniTok that is trained from scratch surpasses the one initialized with pretrained CLIP weights, despite the latter actually achieves better zero-shot classification accuracy. This suggests downstream VQA performance may not be highly correlated with ImageNet classification accuracy. More importantly, it also implies that CLIP weight initialization may serve as a negative prior for unified tokenizers, as the unified visual feature space could drastically differ from CLIP feature space."}, {"title": "5. Conclusion", "content": "This paper studies unified visual tokenization for generation and understanding, which serves as the cornerstone of unified multimodal large language models. We investigate the training paradigm of unified tokenizers and identify that the current challenge in unification mainly arises from the limited representational power of discrete tokens. To address this limitation, we introduce multi-codebook quantization and attention-based factorization to build a unified tokenizer called UniTok. We show that UniTok achieves comparable or even superior performance than domain-specific tokenizers, and excels in downstream visual generation and understanding tasks. The ablation study further reveals that discriminative and generative representation learning does not inherently conflict. We hope our findings could inspire future research in this domain.\nHowever, due to limited computational resources, UniTok is only trained for one epoch, which is not sufficient for CLIP-based semantic representation learning. We believe extending the training schedule could further benefit the tokenizer, especially in understanding performance."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}