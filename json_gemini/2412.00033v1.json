{"title": "Can an AI Agent Safely Run a Government?\nExistence of Probably Approximately Aligned Policies", "authors": ["Fr\u00e9d\u00e9ric Berdoz", "Roger Wattenhofer"], "abstract": "While autonomous agents often surpass humans in their ability to handle vast and\ncomplex data, their potential misalignment (i.e., lack of transparency regarding\ntheir true objective) has thus far hindered their use in critical applications such as\nsocial decision processes. More importantly, existing alignment methods provide\nno formal guarantees on the safety of such models. Drawing from utility and social\nchoice theory, we provide a novel quantitative definition of alignment in the context\nof social decision-making. Building on this definition, we introduce probably\napproximately aligned (i.e., near-optimal) policies, and we derive a sufficient condi-\ntion for their existence. Lastly, recognizing the practical difficulty of satisfying this\ncondition, we introduce the relaxed concept of safe (i.e., nondestructive) policies,\nand we propose a simple yet robust method to safeguard the black-box policy of\nany autonomous agent, ensuring all its actions are verifiably safe for the society.", "sections": [{"title": "1 Introduction", "content": "The deployment of AI systems in critical applications, such as social decision-making, is often stalled\nby the following two shortcomings: 1) They are brittle and usually provide no guarantees on their\nexpected performance when deployed in the real world [8], and 2) there is no formal guarantee that\nthe objective they have been trained against, typically a scalar quantity such as a loss or a reward,\nfaithfully represents human interest at large [41]. Addressing these limitations is commonly referred\nto as Al alignment, an umbrella term including a wide array of methods supposed to make AI systems\nof different modalities behave as intended [17, 24].\nYet, to our knowledge, every metric for alignment is a posteriori, i.e., a system is deemed aligned as\nlong as it does not display misaligned behavior (e.g., through red teaming [14]). This stems from\nthe fact that most of these methods focus on aligning generative models of complex modalities (text,\nimages, video, audio, etc.) where the input and output domains are particularly vast, and where no\nsingle metric can perfectly represent the intended behavior.\nIn the context of critical (e.g., social) decision processes, where an autonomous agent must repeatedly\ntake actions in a complex environment with many stakeholders, a posteriori alignment is not sufficient.\nIndeed, for the same reasons that a society would not trust a human policymaker with hidden motives\nand unknown track record, it would also distrust an autonomous policymaker whose objective is not,\na priori, perfectly clear and verifiably aligned, as the cost of a single bad action (due to their known\nbrittleness) could easily outweigh the benefits of leveraging such systems. This issue is accentuated\nby the fact that, unlike humans, holding a deceptive AI agent accountable remains a challenge [25].\nConversely, recent breakthroughs in AI have significantly increased its potential for beneficial use\nin these critical settings. For example, tax rates and public spending are typically set periodically\nby a parliament. However, this small group of representatives is inevitably overwhelmed by the\nvast amount of complex economic data as well as the pleas of millions of individuals. Due to this"}, {"title": "2 Background", "content": "We are interested in building autonomous agents whose objective is to maximizes social satisfaction\nby taking actions that alter the state of the society. In this section, we define what is meant by social\nsatisfaction (often called social welfare), and we provide the conditions under which it is quantifiable."}, {"title": "2.1 Utility and social choice theory", "content": "It is commonly assumed that the agency of an individual is governed by its internal binary preference\nrelationship over the set of outcomes S. When presented with two choices s and s', the individual\nwill introspect its satisfaction (welfare) levels and either strictly prefer one outcome (s \u4eba s' or s' \u4eba s)\nor be indifferent to both (s ~ s'). We are interested in quantitatively measuring these welfare levels.\nDebreu's representation theorem [10] states that if this preference relationship is complete, continuous,\nand transitive on the topological space S, then there exists infinitely many continuous, real-valued\nfunctions $u : S \\rightarrow \\mathbb{R}$ (called utility functions) such that $u(s) \\leq u(s') \\Leftrightarrow s < s'$, $\\forall s, s' \\in S$ (note\nthat any strictly monotonically increasing function can transform a valid utility function into another).\nWhile these findings establish the existence of these utility functions, which are proxies for the\nintrinsic welfare levels of individuals, they do not provide insights into their measurability and\ninterpersonal comparability. It is these two properties, however, that eventually determine what\nmeasure of social welfare can be derived. In a nutshell, measurability and comparability impose how\nmuch information can be extracted from the values $|u_i(s) \u2013 u_i(s')|$ and $u_i(s) \u2013 u_j(s)$, respectively,\nfor any $i \\neq j$ and $s \\neq s'$. We detail the various measurability and comparability levels in Appendix\nA.1.1, and we refer to these levels as the informational basis of utilities. Apart from that, we fix\n$0 < U_{min} \\leq U_i(s) \\leq U_{max} < \\infty$ for any s and i (we will allow $U_{min} = 0$ in specific cases). That\nis, we assume that individuals cannot be infinitely satisfied or dissatisfied, and that they must scale\ntheir utilities when reporting (which does not imply measurability or comparability!). Lastly, we\ndefine $\\Delta U \\triangleq U_{max} - U_{min}$ and $\\mathbb{U} \\equiv \\{u : S \\rightarrow [U_{min}, U_{max}]\\}$."}, {"title": "2.1.1 Utility theory", "content": "Let $I$ be a society composed of $N$ members, each with its preference relationship and a corre-\nsponding utility function $u_i \\in \\mathbb{U}$ over state space $S$, $i \\in I$. Let $\\mathcal{R}_S$ be the set of complete orderings\non $S$ and $u \\in \\mathbb{U}^N$ be a vector gathering the utility functions of all individuals. A social welfare\nfunctional f (SWFL) is a mapping $D_f \\rightarrow \\mathcal{R}_S$ with $D_f \\subseteq \\mathbb{U}^N$. In other words, it is an aggregator of\nindividuals' utilities, indirectly preferences. A long line of work [9, 32, 29] has attempted to define\nwhich conditions this SWFL should satisfy (sometimes called axioms of cardinal welfarism, see\nAppendix A.1.2 for an extensive list of these properties and their respective implications). For the\nremainder of this work, we will follow the common assumption that any reasonable SWFL should\nsatisfy the following: universality (U), informational basis invariance (XI), independence of irrelevant\nalternatives (IIA), weak Pareto criterion (WP) and anonymity (A).\nAn important result [29] states that, for any informational basis (X) listed in Appendix A.1.1 and\nany SWFL $f$ satisfying (XI), (U), (IIA) and (WP), there exists a social welfare function (SWF)\n$W: \\mathbb{R}^N\\rightarrow\\mathbb{R}$ such that, if $W(u(s)) > W(u(s'))$, then s ranks strictly higher than s' in f(u). This\nis important as it states that the best social state must maximize a certain function W, which can\ntherefore be used as a measure of social satisfaction. In other words, the non-welfare characteristics\n(i.e., any information influencing f(u) beside u, such as the judgement of an AI agent) are of\nsecondary importance, as they can only break ties between s and s' such that $W(u(s)) = W(u(s'))$\nand cannot be detrimental to the society. Although we do not require it in this work, maximization of\nW can be made sufficient if one imposes Welfarism (W), e.g., by replacing (WP) with Weak Pareto\nIndifference (WPI) or more drastically by imposing Continuity (C) (see Appendix A.1.2 for more\ndetails). We are left with the following question: Given a SWFL satisfying (XI), (U), (IIA), (WP)\nand (A), what is the form of the corresponding SWF? It turns out that the choice is relatively limited\nand depends mostly on the informational basis invariance (XI). It has been shown, with additional\nsmall technical assumptions [7], that the power mean defined in Eq. (1) covers all possible SFWL.\nSee Appendix A.1.3 for a detailed mapping between informational bases, SWFLs and parameter q."}, {"title": "2.1.2 Social choice theory", "content": "$W_q(u(s); \\mathbb{I}) =\\begin{cases}\nmin_{i \\in \\mathbb{I}} u_i(s) & q = -\\infty\\\\\n\\left(\\frac{1}{\\mid \\mathbb{I} \\mid} \\sum_{i \\in \\mathbb{I}} u_i(s)^q\\right)^{\\frac{1}{q}} & q \\in \\mathbb{R}\\^*\\\\\n\\sqrt[\\mid\\mathbb{I} \\mid]{\\prod_{i \\in \\mathbb{I}} u_i(s)} & q = 0 \\\\\nmax_{i \\in \\mathbb{I}} u_i(s) & q = +\\infty\n\\end{cases}$"}, {"title": "2.1.3 Future discounted social welfare", "content": "At deployment, a safe autonomous agent must provide assurances that its future actions will continue\nto serve the best interests of society. This becomes ill-defined if these interests evolve with time. To\naddress this, we assume that both $I$ and $u$ are constant, i.e., $u_i(s; t) = u_i(s;t') = u_i(s)$ for all s, i\nand discrete times $t \\neq t'$. In addition, we also assume that the meaning of these utilities does not\nchange with time, that is, if $u_i(s; t) \\geq u_i(s'; t')$ for states s, s' and times $t \\neq t'$, then i's welfare is at\nleast as high in state s at time t than in state s' at time t' (or vice versa for $\\leq$). Finally, we assume\nthat the SWFL f is such that its corresponding SWF remains the same. In other words, only the\nmethod to break ties between states can evolve through time. This makes it possible to predict, at\ntime t, what will be the satisfaction levels at time $t' > t$ in any given state. However, to model the\nfact that humans prefer immediate reward, we discount the utility of the state at time t' with a factor\n$\\gamma^{(t'-t)}$ when comparing it with the utility of the state at time t, where $\\gamma \\in [0, 1[$ is a discount factor.\nFrom these assumptions, it becomes possible, at time $t = 0$, to quantify the cumulative social welfare\nof any future state trajectory $s_1s_2s_3s_4...$ by computing the quantity $\\sum_{t=0}^{\\infty} \\gamma^t W_q(u(s_{t+1}))$, which we\nwill refer to as the future discounted social welfare of that trajectory (see Section 2.2.2). Using this\nquantity, we formally define alignment as follows:\nAn autonomous agent is aligned if and only if it always takes actions that maximize the expected\nfuture discounted social welfare."}, {"title": "2.2 Social dynamics", "content": "The expectation in the above definition accounts for the inherent randomness of most natural systems.\nIn this section, we model the social dynamics as a particular type of Markov Decision Process (MDP),\nwhere the probability of transitioning to any state depends solely on the current state and next action."}, {"title": "2.2.1 Markov decision process", "content": "Let $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, p, r, \\gamma)$ be an infinite horizon, $\\gamma$-discounted, discrete time MDP where $\\mathcal{S}$ is the state\nspace (discrete or continuous), $\\mathcal{A}$ is the action space (discrete or continuous), $p : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{P}(\\mathcal{S})$\nis the transition dynamics of the environment (with $\\mathcal{P}(\\mathcal{S})$ the set of probability distributions over\n$\\mathcal{S}$), $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [R_{min}, R_{max}]$ is the reward of the environment, and $\\gamma \\in [0, 1[$ is a discount factor\n(favoring immediate over distant rewards). Given $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$, $p(s'|s, a)$ is the probability of\ntransitioning to state s' after taking action a in state s, and r(s, a) is the expected immediate reward\nafter taking that action. In MDPs, actions are chosen according to a policy $\\pi : \\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$, with $\\pi(a|s)$\nthe probability of taking action a in state s. Given an initial state $s_0$, the tuple $(\\mathcal{M}, \\pi, s_0)$ fully defines\na distribution $p_\\tau$ over trajectories $\\tau = s_0a_0s_1a_1s_2a_2\\ldots$, where $a_t \\sim \\pi(\\cdot|s_t)$ and $s_{t+1} \\sim p(\\cdot|s_t, a_t)$.\nIf the environment dynamics or the policy are deterministic, we will use the slight abuse of notation\n$s_{t+1} = p(s_t, a_t)$ and $a_t = \\pi(s_t)$, respectively. The efficacy of a given policy $\\pi$ is measured by the\nstate and state-action value functions, defined respectively as follows:\n$V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim p_{\\tau}(\\cdot|\\pi,s_0=s)} \\left[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right], \\qquad Q^{\\pi}(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s'\\sim p(\\cdot|a,s)}[V^{\\pi}(s')].$\nFor a given state s and action a, the optimal state and action-state value functions are defined by\n$V^*(s) = \\sup_\\pi V^{\\pi}(s)$ and $Q^*(s, a) = \\sup_\\pi Q^{\\pi}(s, a)$. Given $\\epsilon \\geq 0$, a policy is called $\\epsilon$-optimal\n(or optimal) if it satisfies $V^{\\pi}(s) > V^*(s) - \\epsilon$ (respectively $V^{\\pi}(s) = V^*(s)$) for all s. Given small\ntechnical assumptions, it can be shown that there always exists an optimal policy [37, 12]."}, {"title": "2.2.2 Social Markov decision process", "content": "Definition (Social Markov Decision Process). Let I be a society with utility profile $u \\in \\mathbb{U}^N$ and\nsocial welfare function $W_q$. In addition, let S and A be the corresponding state and action spaces, p\nthe social environment dynamics and $\\gamma$ a discount factor. The MDP $(S, A, p, r_{\\mathbb{I}}, \\gamma)$ with reward $r_{\\mathbb{I}}$\ndefined by\n$r_{\\mathbb{I}}(s, a) = \\mathbb{E}_{s'\\sim p(\\cdot|s,a)} [W_q(u(s'); \\mathbb{I})]$\nis a Social Markov Decision Process (SMDP), denoted $\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$.\nIn this setting, S contains all the possible states of the N individuals, as well as all those of the\nenvironment in which they evolve, and A contains all the actions that are delegated to an autonomous\nagent. Expanding on this definition, we can formally define the alignment metric proposed above:\nDefinition (Expected Future Discounted Social Welfare). Let $\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$ be a SMDP.\nThe expected future discounted social welfare of a policy $\\pi$ in state s is defined as\n$V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim p_{\\tau}(\\cdot|\\pi,s_0=s)} \\left[\\sum_{t=0}^{\\infty} \\gamma^t W_q(u(s_{t+1}); \\mathbb{I})\\right]$\nand takes value between $W_{min} = W_q(u(s); \\mathbb{I}) = \\frac{W_{max} - W_{min}}{1-\\gamma}$.\nAs shown with the next lemma, the expected future discounted social welfare of a SMDP is equivalent\nto the state value function of the corresponding MDP. This equivalence makes it a natural metric for\nalignment, as it enables the use of a wide array of known results on MDPs.\nLemma 1. For any SMDP $\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$, the expected future discounted social welfare\nof a policy $\\pi$ is the state value function of $\\pi$ in the MDP $\\mathcal{M} = (S, A, p, r_{\\mathbb{I}}, \\gamma)$, with $r_{\\mathbb{I}}$ set in Eq. (2).\nThe proof follows directly from the tower property of conditional expectations (see Appendix A.2.1)."}, {"title": "2.2.3 Approximate rewards", "content": "If p is unknown, the true reward of the SMDP in Eq. (2) can only be estimated a posteriori, i.e.,\nafter taking action a in state s multiple times and observing $W_q(u(s'))$. This would require a long\nexploration phase if S is large, which can be costly and even impossible in critical decisions processes.\nInstead, one must usually plan using an approximate dynamics model $\\hat{p} \\in \\mathcal{P}(S)$ to anticipate the\neffect of an action. Moreover, even if p is known, computing $W_q(u(s'))$ exactly for a given $s' \\sim p$\nwould require full knowledge of u(s'), which is only possible by obtaining feedback from the entire\nsociety about s' without making additional assumptions on u. For these reasons, we consider a more\nrealistic scenario: Given a set of assessors $\\mathbb{I}_n \\subset \\mathbb{I}$ of size $n \\leq N$ and an approximate dynamics\nmodel $\\hat{p}$, the true reward can be approximated by asking the assessors about their utilities on the\nanticipated future societal states:\n$\\hat{r}_{(\\mathbb{I}_n)}(s, a; K) = \\mathbb{E}_{s' \\sim \\hat{p}(\\cdot|s,a)} [W_q(u(s'); \\mathbb{I}_n)] \\approx \\frac{1}{K} \\sum_{k=1}^K W_q(u(s_k'); \\mathbb{I}_n)$"}, {"content": "where $\\mathbb{E}_{s'\\hat{p}}$ is a \u201cMonte Carlo\u201d estimation of $\\mathbb{E}_{s'\\hat{p}}$, with $K \\in \\mathbb{N}$ samples independently drawn\nfrom $\\hat{p}(s, a)$, denoted $s_1', s_2',..., s_K'$. The core of our analysis is to understand how K, n and the\ninaccuracies of $\\hat{p}$ affect the validity of alignment guarantees."}, {"title": "3 Results", "content": "Having formally derived a quantitative measure of alignment $W^{\\pi}$ in the context of social decision\nprocesses, we are now prepared to introduce and prove the existence of verifiably aligned policies:\nDefinition (Probably Approximately Aligned Policy). Given $0 < \\delta < 1$, $\\epsilon > 0$ and a SMDP\n$\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$, a policy $\\pi$ is $\\delta$-$\\epsilon$-PAA (Probably Approximately Aligned) if, for any\ngiven $s \\in S$, the following inequality holds with probability at least $1 - \\delta$:\n$W^{\\pi}(s) > \\max_{\\pi'} W^{\\pi'}(s) - \\epsilon$"}, {"title": "3.1 Existence of aligned policies", "content": "Definition (Approximately Aligned Policy). Given $\\epsilon > 0$, a policy $\\pi$ is $\\epsilon$-AA (Approximately\nAligned) if and only if it is 0-$\\epsilon$-PAA.\nWe state below one of our main contribution, i.e., the existence of computable PAA and AA policies,\nwhich follows directly from Theorem 3 ($\\delta > 0$) and Corollary 4 ($\\delta = 0$) in the next section.\nTheorem 2 (Existence of PAA and AA policies). Given a SMDP $\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$ with\n$q \\in \\mathbb{R}$ and any tolerances $\\epsilon > 0$ and $0 < \\delta < 1$, if there exists an approximate world model $\\hat{p}$ such\nthat\n$\\sup_{(s,a)\\in S \\times A} D_{KL}(p(\\cdot|s, a)|\\hat{p}(\\cdot|s, a)) < \\frac{\\epsilon^2(1-\\gamma)}{8\\Delta W^2}$"}, {"content": "then there exists a computable $\\delta$-$\\epsilon$-PAA policy. Consequently, there also exists a computable $\\epsilon$-AA\npolicy."}, {"title": "3.2 Near optimal planning", "content": "We prove Theorem 2 by providing a planning policy $\\Pi_{PAA}$ and by proving it satisfies Eq. (4) under\nthe given assumptions. To this end, we present a modified version of the sparse sampling algorithm\n[19] (which originally assumes that p and r are known, which is not the case here). Given some\nparameters K, C and n, we define the recursive functions:\n$Q^h(s, a; K, C, \\mathbb{I}_n) =\\begin{cases}\n0 & h=0\\\\\n\\hat{r}_{(\\mathbb{I}_n)}(s, a; K) + \\gamma\\mathbb{E}_{s'\\sim \\hat{p}(\\cdot|s,a)} [V^{h-1}(s'; K, C, \\mathbb{I}_n)] & h \\in \\mathbb{N}\\^*\\\\\n\\end{cases}$"}, {"content": "$V^h(s; K, C, \\mathbb{I}_n) = \\max_{a\\in A} Q^h(s, a; K, C, \\mathbb{I}_n)$.\nIntuitively, $Q^h$ and $V^h$ are recursive approximations of $Q^*$ and $V^*$, K and C controls the accuracy of\nthe empirical expectation operators in $Q$, h controls how far one looks into the future, and n controls\nthe accuracy of the social welfare function estimates. The proposed PAA policy is simply the greedy\npolicy acting on the state-action value estimates, i.e.,\n$\\pi_{PAA}(s) = \\argmax_{a\\in A} Q^H(s, a; K, C, \\mathbb{I}_n)$.\nIt is deterministic in the sense that, for a given $Q^H$, it outputs a single action. However, $Q^H$ is\nnon-deterministic since $\\mathbb{I}_n$ and s' are sampled randomly. The next results clarifies under which\nconditions $\\Pi_{PAA}$ is indeed $\\delta$-$\\epsilon$-PAA (Theorem 3) or $\\epsilon$-AA (Corollary 4).\nTheorem 3 ($\\Pi_{PAA}$ is $\\delta$-$\\epsilon$-PAA). Let $\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$ be a SMDP with $q \\in \\mathbb{R}$ and $\\hat{p}$\nan approximate dynamics model such that $\\hat{d} \\triangleq \\sup_{(s,a)} D_{KL}(p(\\cdot|s, a)|\\hat{p}(\\cdot|s, a)) < \\frac{\\epsilon^2(1-\\gamma)^6}{8\\Delta U^2}$ for\nany desired tolerances $\\epsilon > 0$ and $0 < \\delta < 1$. For any $k \\geq \\log(\\frac{1}{\\sqrt{\\delta}})$, define\n$\\beta \\triangleq \\frac{(1-\\gamma)^2\\epsilon}{8\\sqrt{\\Delta U}} \\sqrt{2 \\sqrt{d}}$ and let $\\Pi_{PAA}$ be the policy defined in Eq. (7) with param-\neters\n$\\bullet H > \\max \\left\\{1, \\log\\left(\\frac{1}{\\beta}\\right)\\right\\}$,\n$\\bullet K > \\frac{\\Delta U^2}{\\beta^2} \\left( (H-1) \\ln \\left( \\frac{\\pi}{\\gamma} \\sqrt{24k(H-1) \\mid A\\mid^2} \\right) + \\ln \\left(\\frac{1}{\\delta} \\right) \\right)$,\n$\\bullet C \\geq \\sqrt{\\frac{2\\Delta U^2}{K}}$,\n$\\bullet n > N \\left(1 + \\frac{\\Delta U^2}{K} \\frac{N}{A \\Gamma(\\beta, U_{min}, U_{max}, q)}\\right)^{-1}$,\nand where $\\Gamma (\\beta, U_{min}, U_{max}, q)$ is a function defined in Eq. (9). Then $\\pi_{PAA}$ is a $\\delta$-$\\epsilon$-PAA policy.\nCorollary 4. Let $\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$ be a SMDP with $q \\in \\mathbb{R}$ and $\\hat{p}$ an approximate dynamics\nmodel such that $\\hat{d} \\triangleq \\sup_{(s,a)} D_{KL}(p(\\cdot|s, a)|\\hat{p}(\\cdot|s, a)) < \\frac{\\epsilon^2(1-\\gamma)}{8\\Delta U^2}$ for any desired tolerance $\\epsilon > 0$"}, {"title": "3.3 Safe policies", "content": "Although Theorem 2 may initially inspire optimism regarding the title of the paper, the policy $\\Pi_{PAA}$\nproposed in Eq. (7) is expensive for small $\\epsilon$, both in terms of sample complexity and in terms of\nrequired accuracy of the world model. A more efficient PAA policy derived in future work might\npartially solve the sample complexity issue, but the challenge of building predictive models of high\naccuracy remains untouched. In most realistic settings, $D_{KL}(P||\\hat{P})$ is imposed by the state-of-the-art\nknowledge upon which $\\hat{p}$ is built, which implicitly restricts the achievable tolerance $\\delta$. Therefore, it\nseems unlikely that such policies could be used as a primary tool for social decisions, as their sole\nobjective would be to maximize a dubious approximation of social welfare. On the other hand, even\nfor large $\\epsilon$, we will show that we can use our PAA policy to adapt any black-box policy $\\pi$ (e.g., a\npolicy built on top of a LLM) into a safe policy, which we formally define as follows:\nDefinition (Safe Policy). Given $w \\in [W_{min}, W_{max}]$ and $0 < \\delta < 1$, a policy $\\pi$ is $\\delta$-$w$-safe if, for\nany current state s, the inequality $\\mathbb{E}_{s'\\sim p(\\cdot|s,a)} [\\sup_{\\pi'} W^{\\pi'}(s')] > w$ holds with probability at least\n$1 - \\delta$ for any action a such that $\\pi(a|s) > 0$.\nIntuitively, a safe policy ensures (with high probability and in expectation over the environment\ndynamics) that the society is not led in a destructive state, that is, a state which might generate high\nimmediate satisfaction but where no policy can generate an expected future discounted social welfare\nof at least w. This is considerably weaker than the PAA requirements, as we are no longer concerned\nabout social welfare optimality. The ability to adapt any black-box policy into a safe policy would\nallow to leverage their strengths while fully removing their brittleness (by bounding the probability\nof a destructive decision by any desired value $\\delta > 0$). To this end, we use another type of policy:\nDefinition (Restricted Version of a Black-Box Policy). Let $\\pi : S \\rightarrow A$ be any policy and $A(s) \\subseteq A$\nbe restricted subsets of actions for all states s, with $\\Pi(s) = \\sum_{a\\in A(s)} \\pi(a|s)$. The restricted version\nof $\\pi$ is defined as\n$\\hat{\\pi}(a|s) =\\begin{cases}\n0 & a \\in A \\backslash \\bar{A}(s) \\text{ or } \\Pi(s) = 0,\\\\\n\\frac{\\pi(a|s)}{1-\\Pi(s)} & a \\in \\bar{A}(s) \\text{ and } 0 < \\Pi(s) < 1,\\\\\n\\pi(a|s) & a \\in \\bar{A}(s) \\text{ and } \\Pi(s) = 1,\\\\\n\\end{cases}$"}, {"content": "This is similar to action masking presented in [20]. It might happen that $\\hat{\\pi}(a|s) = 0$ for all actions\na, in which case it stops operating. However, if this happens, we have the guarantee that, with high\nprobability, the society is currently not in a destructive state. The challenge lies in finding what are\nthe subset of safe actions for every s. Our proposed method to safeguard any policy is the following:\nTheorem 8 (Safeguarding a Black-Box Policy). Given a SMDP $\\mathcal{M}_{\\mathbb{I}} = (S, A, p, W_q, u, \\gamma)$ with\n$q \\in \\mathbb{R}$, a predictive model $\\hat{p}$ and desired tolerances $w \\in [W_{min}, W_{max}]$ and $0 < \\delta < 1$,\ndefine $\\mathcal{Q}_w(s,a) \\triangleq Q^H(s,a; K,C, \\mathbb{I}_n)$ with $Q^H$ given in Eq. (6) and any $H,K,C,n \\geq 1$.\nFor any policy $\\pi$, let $\\pi_{safe}$ be the restricted version of $\\pi$ obtained with the restricted subsets\n$A_{safe}(s) = \\{a \\in A : \\mathcal{Q}_w(s, a) \\geq \\gamma w + U_{max} + \\alpha\\}$, where\n$\\alpha \\triangleq \\frac{2\\Delta U}{(1-\\gamma)^2} \\sqrt{\\hat{d}'} + \\sqrt{\\frac{\\ln\\left(\\frac{12(C \\mid A \\mid)^{H-1}}{\\delta}\\right)}{1-\\gamma}} + \\left(\\sqrt{\\frac{N-n}{n \\mathbb{N} \\Gamma_{max}}} + \\sqrt{\\frac{\\Delta U^2}{2 K}} \\right) + \\frac{2C\\left(1-\\sqrt{\\frac{1-\\gamma}{2}}\\right)^2}{(1-\\gamma)^2} + \\frac{\\gamma^H \\Delta U}{1-\\gamma}$"}, {"content": "and with the shortened notation $\\Gamma_{max} \\triangleq \\Gamma(U_{max}, U_{min}, U_{max}, q)$, $\\hat{d}' \\triangleq \\min\\left\\{\\frac{1}{2},1 - \\sqrt{1 - e^{-\\hat{d}}}\\right\\}$ and\n$\\hat{d} \\triangleq \\sup_{(s,a)} D_{KL}(p(\\cdot|s, a)|\\hat{p}(\\cdot|s, a))$. Then $\\pi_{safe}$ is $\\hat{\\delta}$-w-safe."}, {"title": "4 Related work", "content": "MDP for social choice MDPs have already been used in the context of social decision processes.\nFor instance", "28": "and more recently [21", "ours": "In their work, the state space S is the set of preference profiles $\\mathbb{U}^N$, and p dictate how\nthese preferences evolve based on the outcome selected by the social choice functional (the policy).\nAnother relevant line of study is that of preference-based reinforcement learning (PbRL) [40", "30": ".", "1": ".", "20": "for comprehensive surveys on the topic.\nHowever, RL relies on exploration, which is not allowed in our setting. On the other hand, existing\nplanning methods (where exploration is not needed if a world model $\\hat{p}$ is available) do not relate the\naccuracy of $\\hat{p}$ to the validity of the desired safety guarantees, as they mostly assume that p is known.\nAlignment The goal of alignment can be entirely different based on the context [13"}]}