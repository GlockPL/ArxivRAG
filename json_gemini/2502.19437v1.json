{"title": "Evolutionary Algorithms Approach For Search Based On Semantic Document Similarity.", "authors": ["Chandrashekar Muniyappa", "Eujin Kim"], "abstract": "Advancements in cloud computing and distributed computing have fostered research activities in Computer science. As a result, researchers have made significant progress in Neural Networks, Evolutionary Computing Algorithms like Genetic, and Differential evolution algorithms. These algorithms are used to develop clustering, recommendation, and question-and-answering systems using various text representation and similarity mea-surement techniques. In this research paper, Universal Sentence Encoder (USE) is used to capture the semantic similarity of text; And the transfer learning technique is used to apply Genetic Algorithm (GA) and Differential Evolution (DE) algorithms to search and retrieve relevant top N documents based on user query. The proposed approach is applied to the Stanford Question and Answer (SQUAD) Dataset to identify a user query. Finally, through experiments, we prove that text documents can be efficiently represented as sentence embedding vectors using USE to capture the semantic similarity, and by comparing the results of the Manhattan Distance, GA, and DE algorithms we prove that the evolutionary algorithms are good at finding the top N results than the traditional ranking approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Evolutionary computing refers to applying optimization techniques using computer programs known as evolutionary algorithms that are similar to the biological evolution process. They are good at searching large solution spaces in a dynamically changing environment which is often difficult to code using deterministic rules, like determining the shortest path for robot navigation, simulating biological experiments, and many other optimization problems. There are many evolutionary algorithms, Genetic and Differential evolutionary algorithms are the most widely used algorithms. This research paper will show how they are applied to sentence embeddings generated by Universal Sentence Encoder to retrieve answers for the given user question. An overview of the genetic and differential evolution algorithms is shown in \"Algo. 1\u201d and \"Algo. 2\" respectively. Traditionally, the ranking approach is used to score the answers for the given question based on the similarity of the text and pick the top answers. However, research has shown that this approach may not always yield the best possible solution, especially when we want to retrieve the Top N results. Even though, the first one or two answers match the question the rest of the answers may not be accurate. To overcome this problem we will apply evolutionary algorithms with advanced text embedding techniques to improve the overall quality of the Top N search results.\nThe rest of the paper is organized into the following sec-tions, the background is presented in section II, the approach in section III, the experiments and results in section IV, and finally, the conclusion and future work are presented in section V."}, {"title": "II. BACKGROUND", "content": "Measuring text similarity is an important and challenging task in Natural Language Processing(NLP) that can be applied to retrieve documents matching users' requests. Jiapeng wang et al [1] in their study, described different types of text similarity measurements and the best text representation technique to be used. The study found that the string-based similarity measurement has the lowest performance as it can capture only lexical similarity and not semantic similarity, vector-based techniques make use of statistical methods to measure the semantic similarity between texts, and graph representation is used to identify the meaning of the same word in different contexts. However, the study concludes that the performance is relative to specific tasks and there is a need for the development of advanced algorithms. Jiyeon Kim et al [29] utilized semantic similarity to determine the similarity between music and movie two different content types to recommend relevant, but diverse content based on users' preferences. In addition, D. Meenakshi et al [30] developed the shared input LSTM (Long Short Term Memory) neural network to determine if a given set of documents are duplicates based on their semantic similarity, Manhattan distance was used to measure the similarity between documents. The work done by researchers in [29,30] shows how important it is to consider semantic similarity over string-based similarity. In this research, we will represent text as embedding vectors generated by the Universal Sentence Encoders (USE) [2]. USE is a deep neural network (DNN) model that is pre-trained on millions of documents, it can be integrated with other machine learning models using the transfer learning approach [5]. Therefore, researchers can readily use the model to encode the text into vectors without the need for any training data which is usually a challenging step in NLP tasks. Besides, the USE is based on an advanced general-purpose encoding scheme that effectively captures the semantic similarity of texts of variable lengths. The results presented in USE [2] clearly show improved performance when compared to other techniques. S. Velampalli et al [3] applied USE on Twitter comments to build a sentiment classification model using the transfer learning technique. In the research, they show how transfer learning can be effectively used to integrate different Machine Learning models to increase the speed of development and also achieve overall high accuracy.\nRanking documents to retrieve relevant documents based on user search is an old problem [7] many researchers have studied this area for a long time and have devised many algo-rithms and metrics to evaluate the performance of the ranking system. Ranking documents for search results is a challenging task, as the data may keep changing, and measuring the similarity based on the content is difficult. Besides, it is often challenging to factor in all these changes as part of the ranking fit function. Considering all these challenges, D. Bollegala et al [6] invented a novel RankDE algorithm based on the Differential Evolution algorithm to retrieve the most relevant documents based on user search. Similarly, Urszula Boryczka et al [8], applied the Differential Evolution (DE) algorithm [9] to create a personalized list of recommended items. As part of this research, they used Singular Value Decomposition (SVD) [10], the matrix factorization technique to generate the real numbers based on the users-to-items rating relationship for each user and represented them as the chromosomes to build a recommendation system. They make use of Mean Average Precision (MAP) shown in equation (2), as the fitness function, to measure the quality of the recommended items.\n$P@n = \\frac{\\text{Number of relevant items in top n results}}{n}$ (1)\nAverage precision (AP) averages the $P@n$ for different n values:\n$AP = \\frac{\\sum_{n=1}^{N} (P@n \\times rel(n))}{\\text{Number of relevant items for this query}}$ (2)\nWhere\n\u2022 rel(n) is a binary function assigning the value 1 if the nth document is relevant to the query (otherwise 0) and N is the number of items obtained. Mean Average Precision (MAP) averages the AP values for all U users in the system\nP. Sihombing et al [4] applied the Genetic algorithm to search and retrieve the documents based on users' inputs. In this study, keywords (String-based similarity measurement) in the document were converted into a binary sequence to represent the chromosomes, and Dice distance measurement was used to measure the fitness score. B. Alhijawi et al [11], applied the Genetic algorithm to build a collaborative recom-mendation system. They represented the similarity between users and items as vectors of randomly generated floating point values and the fitness was measured using the Mean Absolute Error (MAE) between the predicted and known user ratings. The lesser the value of MAE better is the results.\nAlan Diaz-Manriquez et al [12] used the graph-based rep-resentation to cluster the documents based on ACM taxonomy using the Genetic algorithm. Even though the clustering was based on taxonomy, the keywords in the documents were used to represent the chromosomes, and similarity was measured using the Floyd-Warshall algorithm [13]. To measure the cluster purity Davies-Bouldin index (DB Index) [14] was used, which measures the radius of clusters, the smaller the value better is the performance. To represent the chromosomes, keywords in the documents were assigned unique numbers to generate the floating-point vectors. In order to identify the context of words, a graph was used where each node represents"}, {"title": "III. APPROACH", "content": "Based on the literature survey, one can observe that re-searchers used different types of text representation techniques like binary, vector embeddings, and graph embeddings to represent chromosomes and applied variations of Genetic Algorithm (GA) and Differential Evolution (DE) algorithms to measure text similarity. However to generate the vector embeddings, words were assigned unique integers or random floating point values. The main problem with these approaches is different words with the same meaning will get different values without taking synonym property into consideration, as a result, semantic meaning will not be captured despite using the adaptive evolutionary algorithms. To overcome this prob-lem, we will use embeddings generated by Universal Sentence Encoder (USE) which will capture the semantic similarity between the words and generate fixed-length embeddings. As shown in Figure 1, text can be directly fed into the USE model without applying any NLP cleaning steps [18].\nThe model will take a list of sentences, and paragraphs of variable length and generate fixed-length embeddings of length 512 for each element in the input list. The model uses a general-purpose encoding scheme mainly designed for transfer learning purposes. For this research, we are using the \"Stanford Question Answering Dataset\u201d [19] which contains more than 100 thousand questions and answers. We will encode all the questions using USE and use it as the population for GA and DE algorithms.\nChromosomes\nChromosomes play a pivotal role in evolutionary algorithms. There are different ways of representing the chromosomes like integer, binary, and floating point values. However, research has proved that floating point representation has the best performance [20]. Therefore, word embeddings generated as"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The GA and DE algorithms were applied to the SQUAD dataset [19] independently as described in \"Algo. 3\" with different configurations. Finally, the optimal performance was observed with the configurations described in their respective sections. Besides, the Manhattan distance was applied to measure the similarity without GA, and DE algorithms, and the results of all three algorithms are shown in Figures [3-5]. The equation (3) was used as the objective function for GA and DE algorithms. As one can see, the results of Manhattan distance, GA, and DE algorithms shown in Figure 3, Figure 4, and Figure 5 respectively are similar, with accurate results at the top and inaccurate matches at the bottom of the list. This approach works for the Top 1 or 2 answers, but not for the top 10 answers. To solve this problem, instead of considering only the optimal answer returned by the objective function. Let us also consider the top 2 suboptimal answers of the GA and DE algorithms from various generations.\nWhen we observe Figure 4, Figure 6, and Figure 7 optimal result, and suboptimal results 1 and 2 of the GA algorithm respectively. We can see that, the optimal result in Figure 4 has an exact match at the top and noise at the bottom of the list. However, the suboptimal results in Figure 6 and Figure 7 have more relevant answers all the way to the bottom of the list, but the most accurate answers at the top of the list are dropped. On the contrary, when we compare the DE algorithm results in Figure 5, Figure 8, and Figure 9, we can observe that the exact match answer at the top of the list is retained in both optimal and suboptimal results. In addition, noise at the bottom of the list is also reduced. This clearly demonstrates the power of the evolutionary algorithms over the traditional ranking approach to fetch relevant Top N documents. Besides, we can also observe that many documents with semantic meaning are part of the result, demonstrating the power of sentence embedding and transfer learning techniques. In addition, when we apply human annotations 1 when an answer is relevant"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this research paper, we have applied the GA and DE evolutionary algorithms on sentence embeddings generated by USE, using the transfer learning technique to search and re-trieve the documents based on semantic similarity. By applying the suggested approach to the \"SQuAD\" question and answer dataset, we show how text can be efficiently represented as sentence embeddings vectors to capture the semantic similarity using USE. In addition, by comparing the results of Manhattan distance, GA, and DE algorithms we prove that evolutionary algorithms are good at retrieving the Top N search results than the traditional ranking approach. However, due to the nature of the document search and retrieval problem, it is difficult to design a single objective function to capture all the optimal results. Therefore, suboptimal results have to be considered and post-processing has to be done to build the final resultset. As part of future work, we can consider multi-objective evolutionary algorithms that can effectively capture and return the results without the need to examine and post-process the suboptimal results."}]}