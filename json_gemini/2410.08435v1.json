{"title": "Symbolic Music Generation with Fine-grained\nInteractive Textural Guidance", "authors": ["Tingyu Zhu", "Haoyu Liu", "Zhimin Jiang", "Zeyu Zheng"], "abstract": "The problem of symbolic music generation presents unique challenges due to the\ncombination of limited data availability and the need for high precision in note\npitch. To overcome these difficulties, we introduce Fine-grained Textural Guid-\nance (FTG) within diffusion models to correct errors in the learned distributions.\nBy incorporating FTG, the diffusion models improve the accuracy of music gener-\nation, which makes them well-suited for advanced tasks such as progressive music\ngeneration, improvisation and interactive music creation. We derive theoretical\ncharacterizations for both the challenges in symbolic music generation and the ef-\nfect of the FTG approach. We provide numerical experiments and a demo page\nfor interactive music generation with user input to showcase the effectiveness of\nour approach.", "sections": [{"title": "1 Introduction", "content": "Symbolic music generation is a subfield of music generation that focuses on creating music in sym-\nbolic form, typically represented as sequences of discrete events such as notes, pitches, rhythms,\nand durations. These representations are analogous to traditional sheet music or MIDI files, where\nthe structure of the music is defined by explicit musical symbols rather than audio waveforms. Sym-\nbolic music generation has a wide range of applications, including automatic composition, music\naccompaniment, improvisation, and arrangement. It can also play a significant role in interactive\nmusic systems, where a model can respond to user inputs or generate improvisational passages in\nreal-time. A lot of progress has been made in the field of deep symbolic music generation in recent\nyears; see Huang et al. (2018), Min et al. (2023), von R\u00fctte et al. (2023), Wang et al. (2024) and\nHuang et al. (2024).\nDespite recent progress, some unique challenges of symbolic music generation remain unresolved.\nA key obstacle is the scarcity of high-quality training data. While large audio datasets are readily\navailable, symbolic music data is more limited, often due to copyright constraints. Additionally,\nunlike image generation, where the inaccuracy of a single pixel may not significantly affect overall\nquality, symbolic music generation demands high precision, especially in terms of pitch. In many\ntonal contexts, a single incorrect note can be glaringly obvious, even to less-trained ears.\nAs a partial motivation, we empirically observe the occurrence of \u201cwrong notes\" in existing state-\nof-the-art symbolic music generation models. We provide theoretical explanations for why these\nmodels may fail to avoid such errors. Apart from that, we find that many models encounter chal-\nlenges in generating well-regularized accompaniment. While human-composed accompaniment of-\nten exhibits consistent patterns across bars and phrases, the generated symbolic accompaniment\ntends to vary significantly. These observations and theoretical discoveries highlight the need to ap-"}, {"title": "1.1 Related work", "content": "Symbolic music generation. Symbolic music generation literature can be classified based on the\nchoice of data representation, among which the MIDI token-based representation adopts a sequential\ndiscrete data structure, and is often combined with sequential generative models such as Transform-\ners and LSTMs. Examples of works using MIDI token-based data representation include Huang\net al. (2018), Huang & Yang (2020), Ren et al. (2020), Choi et al. (2020), Hsiao et al. (2021), Lv\net al. (2023) and von R\u00fctte et al. (2023). While the MIDI token-based representation enables gen-\nerative flexibility, it also introduces the challenge of simultaneously learning multiple dimensions\nthat exhibit significant heterogeneity, such as the \"pitch\" dimension compared to the \"duration\" di-\nmension. An alternative data representation used in music processing is the piano roll-based format.\nMany recent works adopt this data representation; see Min et al. (2023), Zhang et al. (2023a), Wang\net al. (2024) and Huang et al. (2024) for example. Our work differs from their works in that we\napply the textural guidance jointly in both the training and sampling process, and with an emphasis\non enhancing real-time generation precision and speed. More detailed comparisons are provided in\nAppendix E, after we present a comprehensive description of our methodology.\nControlled diffusion models. Multiple works in controlled diffusion models are related to our\nwork in terms of methodology. Specifically, we adopt the idea of classifier-free guidance in training\nand generation, see Ho & Salimans (2022). To control the sampling process, Chung et al. (2022),\nSong et al. (2023) and Novack et al. (2024) guide the intermediate sampling steps using the gradients\nof a loss function. In contrast, Dhariwal & Nichol (2021), Saharia et al. (2022), Lou & Ermon\n(2023) and Fishman et al. (2023) apply projection and reflection during the sampling process to\nstraightforwardly incorporate data constraints. Different from these works, we design guidance\nfor intermediate steps tailored to the unique characteristics of symbolic music data and generation.\nWhile the meaning of a specific pixel in an image is undefined until the entire image is generated,\neach position on a piano roll corresponds to a fixed time-pitch pair from the outset. This new context\nenables us to develop novel implementations and theoretical perspectives on the guidance approach."}, {"title": "2 Background: Diffusion models for piano roll generation", "content": "In this section, we introduce the data representation of piano roll. We then introduce the formulations\nof diffusion model, combined with an application on modeling the piano roll data."}, {"title": "3 Challenges in symbolic music generation", "content": "While generative models have achieved significant success in text, image, and audio generation, the\neffective modeling and generation of symbolic music remains a relatively unexplored area. In this\nsection, we introduce two major challenges of symbolic music generation."}, {"title": "3.1 Harmonic precision", "content": "One challenge of symbolic music generation involves the high precision required for music gener-\nation. Unlike image generation, where a slightly misplaced pixel may not significantly affect the\noverall image quality, an \"inaccurately\" generated musical note can drastically affect the quality of\na piece. This phenomenon can be more explicitly characterized and explained in the framework of\nharmonic analysis.\nIn music, harmony refers to the simultaneous sound of different notes that form a cohesive entity in\nthe mind of the listener (M\u00fcller, 2015). The main constituent components of harmony are chords,\nwhich are musical constructs that typically consist of three or more notes. To create better harmonic\nalignment of the generated music, many literature of symbolic music generation, e.g., Min et al.\n(2023), von R\u00fctte et al. (2023) and Wang et al. (2024), leverage chords as a pivoting element in data\npre-processing, training and generation. In such works, chord-recognition algorithms are applied\nto training data to provide chords as conditions to the generative models. However, the complex-\nity of harmonic analysis and chord recognition lies in the existence of nonharmonic tones. The\nnonharmonic tones are notes that are not part of the implied chord, but are often used in music as\na combining element to create smooth melodic lines or transitions. The ambiguity of chords thus\ncomplicates automated chord recognition methods, often leading to errors.\nTherefore, in addition to chord analysis, we also consider key signatures\u00b3, which establish the tonal\ncenter of music. Unlike nonharmonic tones, out-of-key notes are less common, at least in many\ngenres, and produce more noticeable dissonance. For instance, a Gh note is considered as out-of-key in a Gb major context. While such notes might add an interesting tonal color when intentionally\nused by composers, they are usually perceived merely as mistakes when appearing in generative\nmodel outputs, see Figure 1 for example.\nWhy generative models struggle with out-of-key notes We now apply statistical theory to char-\nacterize why an out-of-key note is unlikely to be generated in a way that sounds \"right\" in context by\na symbolic music generation model. We note that a summary of non-standard notations is provided\nin Appendix A. Denote the probability of M = M as $P_M(M)$, and the conditional probability\nof M = M under a specific key signature sequence K as $P_M(M|K)$. Note that M is a specific\nrealization of the piano roll segment, sampled from a data distribution.\nLet $w_K(l) \\subset [1, H]$ denote the subset of pitch values that are out of key K(l)5, where K(l) denotes\nthe key signature at time l, and $W_K = \\{M \\in \\{0,1\\}^{L\\times H}| \\exists l \\in [1, L], \\exists h \\in w_K(l), s.t. M_{lh} = 1\\}$ denotes the subset of piano rolls with the presence of notes-out-of-key for key sequence K. The con-\nditional probability that M has at least one note-out-of-key conditional on key K is $P_M(\\omega|K) := \\sum_{M\\in W_K} P_M(M|K)$."}, {"title": "3.2 Rhythmic regularity", "content": "A second observation regarding symbolic music generation models is their tendency to produce ir-\nregular rhythmic patterns. While many composers typically maintain consistent rhythmic patterns\nacross consecutive measures within a 4-bar phrase, particularly in the accompaniment, such varia-\ntions frequently appear in the generated accompaniment of symbolic music generative models.\nSuch phenomenons can be explained by the scarcity of data and the high dimensionality hindering\nthe model's ability to capture correlations between different bars, even within a single generated\nsection. Additionally, the irregularity in generated patterns can stem from the presence of irregular\nsamples in many existing MIDI datasets. Without a sufficient quantity of data exhibiting clear\ncorrelations and repetition across measures, it is unlikely that the model will self-generate more\nhuman-like and consistent accompaniment patterns."}, {"title": "4 Methodology: Fine-grained textural guidance", "content": "In the previous section 3, we identified the unique challenges in symbolic music generation arising\nfrom the distinctive characteristics and specific requirements of symbolic music data. Together with\nthe scarcity of available high-quality data for training, this underscores the need for fine-grained\nexternal control and regularization in generating symbolic music. In this section, we present our\nmethodology of applying fine-grained regularization guidance to improve the quality and stability\nof the generated symbolic music.\nAn important characteristic of piano-roll data, crucial for designing fine-grained control, is that\neach position corresponds to a fixed time-pitch pair, providing a clear interpretation even before the\nfull sample is generated. This characteristic contrasts with other data types, such as image data,\nwhere the meanings of pixel values remain unclear until the image is fully generated, and individual\npixels can only be interpreted together with surrounding pixels in a convoluted manner. Therefore,\nwe accordingly design fine-grained conditioning and sampling correction/regularization, altogether\nreferred to as Fine-grained Textural Guidance (FTG) that leverage this characteristic of the piano\nroll data. We use \"texture\" to refer to harmony and rhythm together."}, {"title": "4.1 Fine-grained conditioning in training", "content": "We train a conditional diffusion model with fine-grained harmonic (C, required) and rhythmic (R,\noptional) conditions, which are provided to the diffusion models in the form of a piano roll Mcond.\nWe provide illustration of Mcond(C, R) and Mcond(C) via examples in Figure 3. The mathematical\ndescriptions are provided in Appendix D."}, {"title": "4.2 Fine-grained control in sampling process", "content": "As indicated by proposition 1 and empirical observations, providing chord conditions to model\ncannot prevent them from generating \u201cwrong notes\" that do not belong to the indicated key signature\nand context. Likewise, the rhythmic conditions also do not guarantee precise alignment with the\nprovided rhythm. Therefore, in this section we design a fine-grained sampling control to enhance\nthe precision of generation.\nWe aim to incorporate control at intermediate sampling steps to ensure the elimination of out-of-key notes, preventing noticeable inharmonious effects. Given key signature sequence K derived\nfrom chord condition C, let $w_K(l) := \\{l,w_K(l)\\}\\_{l=1}^L$ denote all out-of-key positions implied by K,\nthe generated piano-roll M is expected to satisfy $M\\in \\{0,1\\}^{L\\times H}\\backslash W_K$, i.e., $M_{lh} = 0$, for all\n$(l, h) \\in \\omega_K(l)$. In other words, the desired constrained distribution for generated X0 satisfies\n$P_O(X_O \\in W_K := \\{X|(l, h) \\in \\omega_K(l), s.t. X_{lh} > 1/2\\} |K)\n=\n0$.\nNote that in the backward sampling equation 2 that derives $X_{t-1}$ from $X_t$, we have for the first\nterm (Song et al., 2020a; Chung et al., 2022)\n$(\\frac{X_t - \\sqrt{1 - \\alpha_t}\\epsilon_{\\theta}(X_t, t)}{\\sqrt{\\alpha_t}})\n=\n``predicted X_0`` = E[X_0|X_t], t = T,T - 1, ..., 1$.\nIt is suggested in proposition 1 that the major reason leading to generated wrong notes lies in the\nincorrect estimation of probability density $p_X$, which in turn affects the corresponding score function\n$\\nabla_{X_t} log p_t(X_t)$. The equivalence $\\nabla_{X_t} log p_t(X_t) = -\\epsilon_{\\theta}(X_t,t)/\\sqrt{1 - \\alpha_t}$ therefore inspires us to\nproject $E[X_0|X_t]$ to the K-constrained domain $R^{L\\times H}\\backslash W_K$ by adjusting the value of $\\epsilon_{\\theta}(X_t, t)$ at\nevery sampling step t. This adjustment is interpreted as a correction of the estimated score.\nSpecifically, using the notations in 4.1, at each sampling step t, we replace the guided noise predic-\ntion $\\epsilon_{\\theta}(X_t, t|C, R)$ with $\\bar{\\epsilon_{\\theta}}(X_t, t|C, R)$ such that\n$\\bar{\\epsilon_{\\theta}}(X_t, t|C, R) = arg \\min_{\\epsilon} ||\\epsilon - \\epsilon_{\\theta}(X_t,t|C, R)||^2$\ns.t.\n$(\\frac{X_t - \\sqrt{1 - \\alpha_t}\\epsilon}{\\sqrt{\\alpha_t}}) \\in R^{L\\times H}\\backslash W_K.$\nThe element-wise formulation of $\\bar{\\epsilon_{\\theta}}(X_t, t|C, R)$ is given as follows, with calculation details pro-\nvided in Appendix C.2.\n$\\bar{\\epsilon_{\\theta},lh}(X_t, t|C, R) = 1\\{(l, h) \\notin \\omega_K(l)\\} \\cdot {\\epsilon_{\\theta},lh}(X_t, t|C, R)$\n$+1\\{(l, h) \\in \\omega_K(l)\\} \\cdot max_{\\epsilon} \\{\\epsilon_{\\theta},lh(X_t, t|C, R), (\\frac{1}{\\sqrt{1-\\alpha_t}})(\\frac{X_{t,lh}}{\\sqrt{\\alpha_t}} - \\frac{1}{2})\\}.$\nPlugging the corrected noise prediction $\\bar{\\epsilon_{\\theta}}(X_t, t|C, R)$ into equation 2, we derive the corrected\n$X_{t-1}$. The sampling process is therefore summarized as the following Algorithm 1."}, {"title": "5 Experiments", "content": "In this section, we present experiments, including empirical observations of generated examples and\nnumerical comparisons with baseline models, to demonstrate the effectiveness of our fine-grained\nguidance approach. We additionally create a demopage7 for demonstration, which allows for fast\nand stable interactive music creation with user-specified input guidance."}, {"title": "5.1 Empirical Observations", "content": "In this section, we provide empirical examples of how model output is reshaped by fine-grained\ncorrection in Figure 4. Notably, harmonic control not only helps the model eliminate incorrect\nnotes, but also guides it to replace them with correct ones."}, {"title": "5.2 Numerical Experiments", "content": "We focus our numerical experiments on accompaniment generation given melody and chord condi-\ntions. We briefly introduce the data representation and our model architecture in Section 5.2.1. We\ncompare with two state-of-art baselines: 1) WholeSongGen (Wang et al., 2024) and 2) GETMusic\n(Lv et al., 2023)."}, {"title": "5.2.1 Data Representation and Model Architecture", "content": "We represent music scores as image-like piano rolls for the generation target X, which is represented\nby a matrix of shape 2 \u00d7 L \u00d7 128 under the resolution of a 16th note, where L represents the total\nlength of the music piece, and the two channels represent note onset and sustain, respectively. In our\nexperiments, the model takes inputs with L = 64, corresponding to a 4-measure piece under time\nsignature 4/4. Longer music pieces can be generated autoregressively using the inpainting method.\nThe backbone of our model is a 2D UNet with spatial attention.\nThe condition matrix Mcond is also represented by a piano roll matrix of shape 2 \u00d7 L \u00d7 128, with the\nsame resolution and length as that of the generation target X. As stated in Section 4.1, our model can\ntake two kinds of condition matrix: one with both harmonic and rhythm conditions (Mcond (C, R)),\nthe other with harmonic condition only (Mcond(C)). For the accompaniment generation experiments,\nwe provide melody as an additional condition. Detailed construction of the condition matrices are\nprovided in Appendix F."}, {"title": "5.2.2 Dataset", "content": "We use the POP909 dataset (Wang et al., 2020a) for training and evaluation. This dataset consists\nof 909 MIDI pieces of pop songs, each containing lead melodies, chord progression, and piano\naccompaniment tracks. We exclude 29 pieces that are in triple meter, leaving 880 pieces for our\nexperiments. 90% of the data are used to train our model, and the remaining 10% are used for\nevaluation. In the training process, we split all the midi pieces into 4-measure non-overlapping\nsegments (corresponding to L = 64 under the resolution of a 16th note), which in total generates\n15761 segments in the entire training set."}, {"title": "5.2.3 Training and Sampling Details", "content": "We set diffusion timesteps T = 1000 with $B_0 = 8.5e-4$ and $B_T = 1.2e-2$. We use AdamW\noptimizer with a learning rate of 5e-5, $B_1 = 0.9$, and $B_2 = 0.999$. We train for 10 epochs with\nbatch size 16, resulting in 985 steps in each epoch.\nTo speed up the sampling process, we select a sub-sequence of length 10 from {1,\u2026\u2026,T} and\napply the accelerated sampling process in Song et al. (2020a). It takes 0.4 seconds to generate the\n4-measure accompaniment on a NVIDIA RTX 6000 Ada Generation GPU."}, {"title": "5.2.4 Task and Baseline Models", "content": "We consider accompaniment generation task based on melody and chord progression. We compare\nthe performance of our model with two baseline models: 1) WholeSongGen (Wang et al., 2024) and"}, {"title": "5.2.5 Evaluation", "content": "We generate accompaniments for the 88 MIDI pieces in our evaluation dataset. We introduce the\nfollowing objective metrics to evaluate the generation quality of different methods:\n(1) Chord Progression Similarity We use a rule-based chord recognition method from Dai et al.\n(2020) to recognize the chord progressions of the generated accompaniments and the ground truth\naccompaniments. Then we split all chord progressions into non-overlapping 2-measure segments,\nand encode each segment into a 256-d latent space use a pre-trained disentangled VAE (Wang et al.,\n2020b). We then calculate the pairwise cosine similarities of the generated segments and the ground\ntruth segments in the latent space. The average similarities with their 95% confidence intervals are\nshown in the first column of Table 1. The results indicate that our method significantly outperforms\nthe other two baselines in chord accuracy.\n(2) Feature Distribution Overlapping Area We assess the Overlapping Area (OA) of the distributions\nof some musical features in the generated and ground truth segments, including note pitch, duration,\nand note density. Similarly, we split both the generated accompaniments and the ground truth\ninto non-overlapping 2-measure segments. Following von R\u00fctte et al. (2023), for each feature f, we\ncalculate the macro overlapping area (MOA) in segment-level feature distributions so that the metric\nalso considers the temporal order of the features. MOA is defined as\n$MOA(f) = \\frac{1}{N} \\sum_{i=1}^N overlap(\\pi^{gen}_i(f), (\\pi^{gen}(f), \\pi^{gen}(f)),$\nwhere $\\pi^{gen}_i(f)$ is the distribution of feature f in the i-th generated segment, and $\\pi^{gen}(f)$ is the\ndistribution of feature f in the i-th ground truth segment. The MOA's for different methods are\nshown in the last 3 columns in Table 1. Our method significantly outperforms the baselines in terms\nof pitch distribution. The performance on duration and note density distribution is slightly lower than\nWholeSongGen, which makes sense because the WholeSongGen model leverages more background\ninformation."}, {"title": "A Summary of non-standard notations", "content": ""}, {"title": "C Proof of propositions and calculation details", "content": ""}, {"title": "C.1 Proof of Proposition 1", "content": "We first provide the detailed version of proposition 1, given as the following proposition 3. We\nintroduce the following definitions and assumptions as foundations of the detailed proposition 3.\nDefinition 1. (Property I) An estimator $\\hat{p}$ constructed from $\\{X_i\\}_{i=1}^n$, $X_i \\sim p(X)$, $X \\in R^d$ is said\nto satisfy property I with threshold q and constants $A_1, A_2 \\in (0,1)$, w.r.t. distribution p(X), if\n$\\forall s \\in \\{1,2,..., d\\}$ and $\\forall n \\in N$, we have\n$E_{\\hat{p}} \\{\\int_{R^{s-1}} \\int_{(-\\infty,q)} dX_s \\int_{R^{d-s}}|\\hat{p} - p|(Y_{s-1}, X_s, Z_{d-s}) \\geq A_1 E[TV(p, \\hat{p})]\\}$\n$E_{\\hat{p}} \\{\\int_{R^{s-1}} \\int_{(-\\infty,q)} dX_s \\int_{R^{d-s}}|\\hat{p} - p|(Y_{s-1}, X_s, Z_{d-s}) \\leq A_2 E[TV(p, \\hat{p})]\\}$\nNote that\n$TV (p, \\hat{p})\n=\n\\int_{R^d} |p-\\hat{p}|(X)dX, $"}, {"title": "C.2 Calculation detials in 4.2", "content": "Our goal is to find the optimal solution of problem (11). Since the constraint is an element-wise\nconstraint on a linear function of $\\epsilon$ and the objective is separable, we can find the optimal solution\nby element-wise optimization. Consider the (l, h)-element of $\\epsilon$.\nFirst, if $(l,h) \\notin \\omega_K(l)$, there is no constraint on $\\epsilon_{lh}$. Therefore, the optimal solution of $\\epsilon_{lh}$ is\n$\\epsilon_{\\theta,lh}(X_t, t|C, R)$.\nIf $(l, h) \\in \\omega_K(l)$, the constraint on $\\epsilon_{lh}$ is\n$\\frac{X_{t,lh}}{\\sqrt{\\alpha_t}} - \\frac{\\sqrt{1 - \\alpha_t} \\epsilon_{lh}}{\\sqrt{\\alpha_t}} < \\frac{1}{2},$\nwhich is equivalent to\n$\\epsilon_{lh} > \\frac{1}{\\sqrt{1 - \\alpha_t}} (\\frac{X_{t,lh}}{\\sqrt{\\alpha_t}} - \\frac{1}{2}).$\nThe objective is to minimize $|\\epsilon_{lh} - {\\epsilon_{\\theta},lh}(X_t, t|C, R)|$. Therefore, the optimal solution of $\\epsilon_{lh}$ is\n$\\epsilon_{lh} \\stackrel{min}{=} max \\{\\epsilon_{\\theta,lh}(X_t, t|C, R), \\frac{1}{\\sqrt{1-\\alpha_t}} (\\frac{X_{t,lh}}{\\sqrt{\\alpha_t}} - \\frac{1}{2})\\}.$"}, {"title": "C.3 Proof of Proposition 2", "content": "Proof. Under the SDE formulation, the denoising process can take the form of a solution to stochas-\ntic differential equation (SDE):\n$dX_t = \\frac{1}{2}[\\beta(t)X_t + \\beta(t)\\nabla_{X_t} log p_t(X_t)] dt + \\sqrt{\\beta(t)}dW_t,$\nwhere $\\beta(t/T) = T\\beta_t$, $W_t$ is the reverse time standard Wiener process. According to Song et al.\n(2020b), as T\u2192 \u221e, the solution to the SDE converges to the real data distribution po.\nIn the diffusion model, $\\nabla_{X_t} log p_t(X_t)$ is approximated by $-\\frac{\\epsilon_{\\theta}(X_t,t)}{\\sqrt{1 - e^{-\\int_0^t \\beta(s)ds}}}$. Therefore, the approximated reverse-SDE sampling process without harmonic guidance is\n$dX_t = \\frac{1}{2}[\\beta(t)X_t - \\beta(t)\\frac{\\epsilon_{\\theta}(X, t)}{\\sqrt{1-e^{-\\int_0^t \\beta(s)ds}}}] dt + \\sqrt{\\beta(t)}dW_t.$"}, {"title": "D Details of Conditioning and Algorithms", "content": ""}, {"title": "D.1 Mathematical formulation of textural conditions in section 4.1", "content": "Denote a chord progression by C, where C(l) denotes the chord at time l \u2208 [1,L]. Let $\\gamma_{C(l)} \\subset [1, H]$ denote the set of pitch index h that belongs to the pitch classes included in chord C(l).11, and let $\\gamma_R \\subset [1, L]$ denote the set of onset time indexes corresponding to rhythmic pattern R. We\ndefine the following versions of representations for the condition:\n\u2022 When harmonic (C) and rhythmic (R) conditions are both provided, the corresponding\nconditional piano roll $M^{cond}(C, R)$ is given element-wise by $M^{cond}_{ih}(C,R) = 1\\{l \\in \\gamma_R\\}1\\{h \\in {\\gamma_{C(l)}}\\}$, meaning that the (l, h)-element is 1 if pitch index h belongs to chord\nC(l) and there is onset notes at time l, and 0 otherwise.\n\u2022 When only harmonic (C) condition is provided, the corresponding piano roll $M^{cond}(C)$ is\ngiven element-wise by $M^{cond}_{lh}(C) = -1 - 1\\{h \\in {\\gamma_{C(l)}}\\}$, meaning that the (l, h)-element\nis -2 if pitch index h belongs to chord C(I), and -1 otherwise.\nFigure 3 provides illustrative examples of $M^{cond}(C, R)$ and $M^{cond}(C)$. The use of -2 and -1\n(rather than 1 and 0) in the latter case ensures that the model can fully capture the distinctions\nbetween the two scenarios, as a unified model will be trained on both types of conditions."}, {"title": "D.2 Additional algorithms in section 4.2", "content": "In this section, we provide the following algorithm: fine-grained sampling guidance additionally\nwith rhythmic regularization, fine-grained sampling guidance combined with DDIM sampling.\nLet B denote the rhythmic regularization. Specifically, we have the following types of regularization:\n\u2022 B1: Requiring exactly N onset of a note at time position l, i.e., $\\sum_{h \\in [1,H]} M_{lh} = N$\n\u2022 B2: Requiring at least N onsets at time position l, i.e.,\n$\\exists h \\subset [1, H]$, or $\\exists h \\subset [1, H]\\backslash \\omega_K(l)$ if harmonic regularization is jointly included\nsuch that $M_{lh} = 1$, and $|h| \\geq N$\n\u2022 B3: Requiring no onset of notes at time position l, i.e., $\\forall h \\in [1, H]$, $M_{lh} = 0$\nLet the set of M satisfying a specific regularization B be denoted as $M_B$, and the corresponding set\nof X be denoted as $\\mathcal{M_B}$, note that this includes the case where multiple requirements are satisfied,\nresulting in\n$\\mathcal{M_B} = \\mathcal{M}_{B_1,B_2,...} = \\mathcal{M}_{B_1} \\cap \\mathcal{M}_{B_2} \\cap ...$.\nThe correction of predicted noise score is then formulated as\n$\\bar{\\epsilon_{\\theta}}(X_t,t|C, R) = arg \\min_{\\epsilon} ||\\epsilon - {\\epsilon_{\\theta}}(X_t, t|C, R)||^2$\ns.t.\n$(\\frac{X_t - \\sqrt{1 - \\alpha_t} \\epsilon}{\\sqrt{\\alpha_t}}) \\in \\mathcal{M_B}.$\nFurther, we can perform predicted noise score correction with joint regularization on rhythm and\nharmony, resulting in the corrected noise score\n$\\bar{\\epsilon_{\\theta}}(X_t,t|C, R) = arg \\min_{\\epsilon} ||\\epsilon - {\\epsilon_{\\theta}}(X_t, t|C, R)||^2$\ns.t.\n$(\\frac{X_t - \\sqrt{1 - \\alpha_t} \\epsilon}{\\sqrt{\\alpha_t}})\n\\in (R^{L\\times H}\\backslash W_K) \\cap \\mathcal{M_B}.$"}, {"title": "E Comparison with Related Works", "content": "We provide a detailed comparison between our method and two related works in controlled diffusion\nmodels with constrained or guided intermediate sampling steps:\nComparison with reflected diffusion models In Lou & Ermon (2023), a bounded setting is used for\nboth the forward and backward processes, ensuring that the bound applies to the training objective\nas well as the entire sampling process. In contrast, we do not adopt the framework of bounded\nBrownian motion, because we do not require the entire sampling process to be bounded within a\ngiven domain; instead, we only enforce that the final sample outcome aligns with the constraint.\nWhile Lou & Ermon (2023) enforces thresholding on $X_t$ in both forward and backward processes,\nour approach is to perform a thresholding-like projection method on the predicted noise $\\epsilon_{\\theta}(X_t, t)$,\ninterpreted as noise correction."}, {"title": "F Detailed Data Representation", "content": "The two-channel version of piano roll with with both harmonic and rhythm conditions\n($M^{cond"}, "C, R)$) and with harmonic condition ($M^{cond}(C)$) with onset and sustain are represented as:\n\u2022 $M^{cond}(C, R)$: In the first channel, the (l, h)-element is 1 if there are onset notes at time l\nand pitch index h belongs to the chord C(l), and 0 otherwise. In the second channel, the\n(l, h)-element is 1 if pitch index h belongs to the chord C(l) and there is no onset note at\ntime l.\n\u2022 $M^{cond}(C)$: In both channels, the (l, h)-element is 1 if pitch index h belongs to the chord\nC(l), and 0 otherwise.\nIn each diffusion step t, the model input is a concatenated 4-channel piano roll with shape 4\u00d7L\u00d7128,\nwhere the first two channels correspond to the noisy target $X_t$ and the last two channels correspond\nto the condition Mcond (either $M^{cond}(C, R)$ or $M^{cond}(C)$). The output is the noise prediction $\\epsilon_{\\theta}$,\nwhich is a 2-channel piano roll with the same shape as $X_t$. For the accompaniment generation\nexperiments, we provide melody as an additional condition, which is also represented by a 2-channel\npiano roll with shape 2 \u00d7 L \u00d7 128, with the same resolution and length as X. The melody condition\nis also concatenated with $X_t$ and Mcond as model input, which results in a"]}