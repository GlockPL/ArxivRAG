{"title": "Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product", "authors": ["Pengxiang Lan", "Haoyu Xu", "Enneng Yang", "Yuliang Liang", "Guibing Guo", "Jianzhe Zhao", "Xingwei Wang"], "abstract": "Prompt tuning (PT) offers a cost-effective alternative to fine-tuning large-scale pre-trained language models (PLMs), requiring only a few parameters in soft prompt tokens added before the input text. However, existing PT approaches face two significant issues: (i) They overlook intrinsic semantic associations between soft prompt tokens, leading to high discreteness and limited interactions, thus reducing the model's comprehension and effectiveness in complex tasks. (ii) Due to the complexity of downstream tasks, long soft prompt is necessitated to improve performance, but prompt length correlates positively with memory usage and computational costs. Achieving high efficiency and performance remains an ongoing challenge. To address these issues, we propose a novel Low-pAraMeters Prompt Tuning (LAMP) method, which leverages prompt decomposition and compressed outer product. Specifically, the prompt decomposition module employs Truncated SVD to reduce training parameters and significantly lower the dimensionality of the soft prompt parameter space. It then utilizes a compressed outer product module to facilitate multiple interactions among prompt tokens, exploring their intrinsic associations to enhance knowledge representation. Finally, LAMP uses average pooling to reduce memory usage and training/inference time. Extensive experiments across six architectures and eight datasets demonstrate that LAMP outperforms state-of-the-art PT-based and LoRA-based methods in performance and efficiency.", "sections": [{"title": "1 Introduction", "content": "Pre-trained language models (PLMs) possess powerful learning capabilities to extract complex features and patterns from vast amounts of data (Devlin et al., 2019; Radford et al., 2019). In recent years, as the scale of large PLMs has rapidly expanded, computational costs have surged dramatically (Lan et al., 2024). Although full fine-tuning parameters of PLMs yields satisfactory results, it has become impractical, e.g., PaLM has 540B parameters and requires 6144 TPU v4 chips to train for 1200 hours (Chowdhery et al., 2023). Parameter-Efficient Fine-Tuning (PEFT) methods attempt to bridge this gap by achieving performance comparable to full fine-tuning with minimal computational resources and time costs (Houlsby et al., 2019; Hu et al., 2021; Lester et al., 2021). Among these methods, prompt tuning (PT) stands out for its efficiency and flexibility. It freezes the model parameters and exclusively trains the soft prompt tokens attached to the model's input, delivering performance on par with full fine-tuning (Lester et al., 2021; Xiao et al., 2023; Razdaibiedina et al., 2023; Lan et al., 2024). Notably, PT's trainable parameters are much lower than other PEFT methods (e.g., Adapter (Houlsby et al., 2019) and LoRA (Hu et al., 2021)) and do not grow exponentially as the scale of PLMs.\nAlthough these methods provide undeniable contributions, existing PT-based methods still suffer from two main challenges: First, in various tasks, PT initialization methods exhibit high discreteness, lacking exploration of the intrinsic semantic associations between tokens. The two mainstream prompt initialization methods\u2014random initialization and initialization from sampled vocabulary (i.e., the 5,000 most common tokens)\u2014help the model explore a broader parameter space during training. The sample vocabulary initialization, in particular, is widely utilized in pre-training for transfer learning (Vu et al., 2022; Asai et al., 2022) and multi-task learning (Wang et al., 2022; Zhong et al., 2024) due to its more informative nature. Nevertheless, as shown in Figure 1, we found that although this informative initialization provides prompt tokens with rich knowledge, these tokens remain isolated without establishing semantic connections after training. In NLP tasks, intrinsic semantic associations form the context and meaning of language, helping models better understand and represent complex language structures (Mikolov et al., 2013; Devlin et al., 2019; Vaswani et al., 2017). Prompt tokens leverage semantic knowledge to guide PLMs in producing outputs that better meet task requirements. Clearly, this intrinsic semantic association is essential for PT. This high discreteness overlooks capturing intrinsic semantic associations among soft prompt tokens, limiting the model's knowledge representation capabilities. Second, although PT does not require training the parameters of PLMs, adding soft prompts increases the total length of input embeddings. Figure 2 reveals the relationship between computational cost, trainable parameters, and prompt length. Previous research has shown that a long prompt 100 yields optimal PT performance, enabling PLMs to adapt to complex downstream tasks (Lester et al., 2021; Razdaibiedina et al., 2023; Xiao et al., 2023). However, such a length renders PT inefficient. This inefficiency stems from the inherent high complexity of PLMs (e.g., the quadratic complexity of Transformers) (Vaswani et al., 2017) and the fact that the storage of gradients and optimizer states is closely related to the number of trainable parameters (Guo et al., 2021). While some approaches (Xiao et al., 2023; Shi and Lipani, 2024; Lan et al., 2024) attempt to optimize standard PT, they still struggle with inefficiency and suboptimal performance.\nTo address the aforementioned challenges, we propose a novel efficient and effective low-parameters prompt tuning (LAMP) method through prompt decomposition and compressed outer product. Our motivation stems from the soft prompt exhibiting high dispersion and the \u201cintrinsic rank\" (Aghajanyan et al., 2021; Hu et al., 2021) in PEFT, which indicates that model fine-tuning can occur in a low intrinsic dimensionality space. Specifically, LAMP first employs Truncated singular value decomposition (SVD) with its inherent structure\u2014two low-dimensional singular vectors and singular values\u2014to transform the loosely related semantic knowledge in PT tokens into a more structured and interrelated form, while simultaneously reducing trainable parameters. It then aggregates the semantic knowledge from the Truncated SVD and leverages the compressed outer product to enable multi-level interactions of intrinsic semantics, enhancing the model's knowledge representation. Finally, LAMP reduces computational load by applying average pooling, which does not increase training parameters. Figure 2 demonstrates that the longer the prompt length, the more significant the reduction in computational cost and memory usage achieved by LAMP.\nThe main contributions of this paper are:\n\u2022 Our empirical study reveals that tokens in soft prompts exhibit high dispersion during training, lacking inherent semantic interactions among tokens to assist the model in comprehending and handling complex tasks, thereby limiting the model's knowledge representation capability.\n\u2022 We propose a novel low-parameter prompt tuning (abbreviated as LAMP) method that captures potential semantic interactions between prompt tokens through prompt decomposition and compressed outer product. LAMP achieves robust performance while significantly reducing computational costs (e.g., training time, memory usage, and trainable parameters)."}, {"title": "2 Method", "content": null}, {"title": "2.1 Preliminaries", "content": "Prompt Tuning. PT can maintain parameter efficiency as model size scales. This approach ensures that trainable parameters does not increase dramatically with model expansion, making it a preferred choice for many applications (Shi and Lipani, 2024; Lan et al., 2024). Let labelled training data (X, Y) = {xi, Yi}N i=1 for one target task T, the number of training data is N. The total parameters of PLM is \u0398 and each input text is xi. The embedding of x is represented as Ei \u2208 Rm\u00d7d, where m is maximum sequence length and d is the dimension of input embedding. The target prompt P\u2208 Rl\u00d7d is initialized, with l as the hyper-parameter determining the length of the soft prompt. This prompt is then concatenated with the fixed embedding Ei \u2208 Rm\u00d7d. Ei remains unchanged during gradient updates in training, resulting in a new input embedding [P; Ei] \u2208 R(l+m)\u00d7d\nThe formulation for the target task is as follows:\nLp = -\u03a3log P (yi| [P; Ei]; \u0398)\ni=1\nwhere Lp is a loss function only optimized with the prompt P. P(\u00b7) is maximizing the conditional probability of PT. The overall structure of PT is shown in Figure 3(a)."}, {"title": "2.2 LAMP: Low-parameters Prompt Tuning", "content": "Prompt initialization. We initialize the source soft prompt P from sampled vocabulary (Lester et al., 2021; Vu et al., 2022; Razdaibiedina et al., 2023; Asai et al., 2022), a strategy that embeds more semantic richness and prior knowledge than mere random initialization. Inadequate initialization can result in the discovery of suboptimal local minima, thus impairing the model's generalization capabilities.\nPrompt decomposition. Our innovative motivation stems from two aspects: (1) Soft prompt tokens exhibit high dispersion (as shown in Figure 1), neglecting the knowledge interactions between tokens; (2) Soft prompt also exhibit low \"intrinsic rank\" behavior (Xiao et al., 2023). Inspired by these findings and the core idea of Truncated SVD (Hansen, 1987), we employ Truncated SVD to reduce training parameters by decomposing the soft prompt P \u2208 Rl\u00d7d. The original SVD of P is formulated as follows:\nP = Udiag(Q)VT\nwhere U \u2208 Rl\u00d7min(l,d), V \u2208 Rd\u00d7min(l,d) are the singular vectors with orthogonal columns, U and V transforms highly dispersed tokens from the original PT into interrelated representations. Q \u2208 Rmin(l,d) comprises the singular values arranged in descending order (the larger the singular value, the more information it contains). The operation diag(Q) converts Q into a diagonal matrix, and VT represents the transpose of V.\nPrompt reconstruction. We define the soft prompt's low \"intrinsic rank\" as r and select the top-r singular values, Q[:r], which contain a rich amount of information arranged in descending order. The remaining singular values, Q[r:], are discarded and do not participate in training/inference. Consequently, as the Q dimension changes, singular values and vectors are redefined as {U[:r] \u2208 Rl\u00d7r, Q[:r] \u2208 Rr, V[:r] \u2208 Rd\u00d7r}. We can approximate the original information by storing only r (where r < d) singular values and their corresponding low-parameters singular vectors, achieving parameter compression, which is also why LAMP adopts Truncated SVD.\nThe trainable parameters are now reduced from \u2018l\u00d7 d' to \u2018l\u00d7r+r+r\u00d7d'. In experiments, we provide a detailed explanation of hyperparameter r and its impact on model performance. For instance, on the Llama2-7B (Touvron et al., 2023), when the prompt length increases from 100 to 500, traditional PT requires training 2,048k parameters, whereas our method, LAMP, only requires training (500 \u00d7 8+8+8 \u00d7 4096 = 36.8k) parameters. LAMP's advantage becomes more pronounced with longer prompt lengths or larger model scales, as it significantly reduces the computational cost by decreasing trainable parameters. LAMP has established a solid foundation for PT-based methods to excel across various domains."}, {"title": "Compressed outer product", "content": "Although using Truncated SVD to reduce the trainable parameters in prompt tuning is promising, directly applying dot products on the decomposed singular values and vectors can only partially capture the intrinsic associations between tokens. Specifically, dot products' inherent linear nature limits their ability to fully express the more complex, nonlinear interactions among prompt tokens. Considering that outer products can mine richer and more complex high-order interactions, we utilize the compressed outer product to further explore the intrinsic semantic associations between tokens in prompt tuning.\nFirstly, we utilize the dot product of singular values and singular vectors as the initialization input for the compressed outer product module:\nM = U[:r] diag (Q[:r]) \u2208 Rl\u00d7r,\nI = diag (Q[:r]) V[:r] \u2208 Rr\u00d7d.\nThis approach enables an initial aggregation of knowledge features between tokens, which helps to effectively represent and explore the underlying semantic knowledge associations. Due to compressed outer product can maintains the high-order structure while facilitating multiple layers of intrinsic semantic interactions. This approach effectively restores and enhances the complex information structures that might be lost in Truncated SVD, thereby enriching the knowledge representation capabilities of prompt tokens. The compressed outer product is formulated as follows:\nC = \u03a3M[:,i] \u2297 I[i,:] \u2208 Rl\u00d7d\ni=1\nwhere M[:,i] is the i-th column vector of M, I[i,:] is the i-th row vector of I, \u2297 denotes the outer product of two vectors. C \u2208 Rl\u00d7d is the resultant prompt after summing all the outer products. The introduction of compressed outer products enhances the representational power of prompt tuning. This approach enables the soft prompt to more effectively adapt to different downstream tasks through deep interactions between prompt tokens.\nWhile introducing compressed outer product does not create new trainable parameters, it does entail a slight increase in computational overhead due to its engagement in more complex higher-order interactions. Additionally, given the transformers' quadratic complexity, the prompt's length is proportional to the training duration. We consider employing average pooling operation to reduce training time:\nP'i,j = (1/p) \u03a3Cip+k,j\nk=0\nP'i,j represents the elements of the tensor P' \u2208 Rl/p\u00d7d after averaging pooling. This operation effectively compresses the l elements along the first dimension into l/p. It is also noteworthy that we explored a self-attention pooling strategy to dynamically filter prompt tokens in Appendix A.1; however, this strategy was not very effective and introduced additional trainable parameters."}, {"title": "2.3 Training and Inference", "content": "Only the parameters of U[:r] \u2208 Rl\u00d7r, Q[:r] \u2208 Rr, and V[:r] \u2208 Rd\u00d7r are optimized during the training process, while the backbone model (i.e., \u0398 and Ei) remained frozen as Figure 3(b). The reconstructed prompt P' is inserted before the input text embeddings. By P', Eq.1 is displaced by:\nLPT = -\u03a3log P(yi|[P'; Ei]; \u0398)\ni=1\nwhere [P'; Ei] \u2208 R(l/p+m)\u00d7d is a input embedding of PLMs through the connection of P' and Ei."}, {"title": "3 Experiments", "content": "In this section, we will answer these key research questions by conduct extensive experiments: RQ1: How does our LAMP performance compare with other SOTA baselines across different model scales and datasets? RQ2: How do few-shot adaptability and hyper-parameters optimization influence the LAMP? RQ3: How will the feature space of the soft prompt change after considering the intrinsic semantic associations between tokens?"}, {"title": "3.1 Evaluation Datasets and Metrics", "content": "Evaluation Datasets: Building upon prior studies in prompt tuning (Xiao et al., 2023), we employ eight NLP tasks from the SuperGLUE (Wang et al., 2019) and GLUE (Wang et al., 2018) benchmark and conduct multi-aspect experiments to evaluate the high efficiency and effectiveness of LAMP. The SuperGLUE benchmark includes more complex and challenging tasks among eight datasets than GLUE (Wang et al., 2018): CB (De Marneffe et al., 2019), WSC (Levesque et al., 2012), COPA (Roemmele et al., 2011), RTE (Giampiccolo et al., 2007), WiC (Pilehvar and Camacho-Collados, 2019), BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018) and ReCoRD (Zhang et al., 2018). MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), SST-2 (Socher et al., 2013) and MRPC(Bill, 2005) in GLUE benchmark. More details about datasets in Appendix A.2.\nMetrics: Consistent with previous work (Razdaibiedina et al., 2023; Xiao et al., 2023), the evaluation metric is F1 for MultiRC and ReCoRD, the evaluation metric is Accuracy for other tasks."}, {"title": "3.2 Baselines and Models", "content": "We compare LAMP with the following baseline approaches: Full Fine-tuning, which updates all parameters of PLMs; PT-based methods, where PT (Lester et al., 2021) inserts trainable continuous vectors, known as soft prompt, before the model's input, and its most advanced variants include Residual PT (Razdaibiedina et al., 2023), DePT (Shi and Lipani, 2024), EPT (Lan et al., 2024) and DPT (Xiao et al., 2023); LoRA-based methods include PiSSA (Meng et al., 2024), rsLoRA (Kalajdzievski, 2023), LoRA+ (Hayou et al., 2024), DoRA (Liu et al., 2024) and LoRA-GA (Wang et al., 2024). More details about Baselines can be found in Appendix A.3.\nWe aim to explore a high-performance PEFT method that minimizes trainable parameters. Trainable parameters are a crucial factor in our selection of baselines; hence, methods with more significant trainable parameters and modifying the transformer layers are not included as baselines for comparison. Such as Adapter (Houlsby et al., 2019) (prompt length is 100, 76.8k vs. 1.9M for T5-base) and its variant methods. Notably, EPT (Lan et al., 2024) has demonstrated superior performance compared to these PEFT methods. Furthermore, Xprompt (Ma et al., 2022) underwent rewinding training, and transfer learning (Vu et al., 2022; Asai et al., 2022) and multi-task learning (Wang et al., 2022) require pre-training. These methods are not directly comparable to LAMP."}, {"title": "3.2.1 Models Size", "content": "PT tends to underperform in smaller-scale models. Thus, we conducted primary experiments employing three T5 model variants (Raffel et al., 2020) (Small, 60M; Base, 220M; and Large, 770M) and validated the effectiveness of LAMP using T5-11B and Llama2-7B (Touvron et al., 2023)."}, {"title": "3.3 Training Details", "content": "T5 model as the backbone for our experiments; the hidden dimensions for the T5-base, T5-small, and T5 large are 512, 768, and 1,024, respectively. Following the experimental setup from Xiao et al. (2023), we set the soft prompt length to 100, rank r = 8 in Truncated SVD and batch size is 16. The models are trained 100 epochs using the AdamW (Loshchilov and Hutter, 2019) optimizer with an initial learning rate of 0.3. We employ the double quantization operation in QLoRA(Dettmers et al., 2023) for T5-11B and Llama2-7B. Other training details in Appendix A.4."}, {"title": "3.4 Overall Performance Comparison (RQ1)", "content": "Table 1 presents a comparison of LAMP against baseline methods on the SuperGLUE benchmark using various T5 model sizes. Notably, LAMP requires the fewest training parameters and demonstrates exceptional average performance across different scales of T5 models. LAMP outperforms the original PT by 7.58%, 9.08%, and 7.11% on T5-Small, T5-Base, and T5-Large, respectively. Detailed information on the standard deviation of LAMP can be found in Appendix A.5. LAMP's performance improvement is attributed to enhancing the model's knowledge representation by uncovering the intrinsic semantic interactions between soft prompt tokens. LAMP achieves outstanding performance while significantly reducing training parameters and computational costs.\nFrom the perspective of trainable parameters, LAMP and DPT are more efficient than other baselines; even though EPT outperforms DPT, EPT requires more trainable parameters. LAMP clearly outperforms DPT in several ways. First, DPT relies on randomly generated initial prompts that lack semantic richness, whereas LAMP leverages sample vocabularies to better aid the model in understanding complex language structures. Additionally, while DPT reduces trainable parameters, its prompt length remains at 100 when input into model, leading to inefficiency. In contrast, LAMP employs average pooling operations that neither harm performance nor increase trainable parameters, making it an efficient and effective novel prompt tuning method. LAMP demonstrates superior performance and requires significantly fewer training parameters than the latest LoRA-based PEFT methods, with detailed results provided in Appendix A.6."}, {"title": "3.5 Ablation Experiment Analysis (RQ2)", "content": "Few-shot adaptation. Following the few-shot experimental setup of Xiao et al. (2023), we randomly sampled 8, 16, and 32 training examples. Table 2 presents the results of all baselines on the Super-GLUE benchmark. All results are averaged over three runs with different random seeds on the T5-base model. We found that LAMP outperforms other baselines on most datasets in the few-shot setting, demonstrating its effectiveness. The few-shot performance of various methods across different datasets is detailed in Appendix A.7.\nSensitivity of Rank Size. The intrinsic rank r is the primary factor influencing the total number of trainable parameters in LAMP. We analyzed the impact of r \u2208 {4, 6, 8, 10, 12, 20} on LAMP performance using the T5-base model on the CB and BoolQ datasets within the SuperGLUE benchmark. As shown in Figure 4(a) and Figure 4(b), despite the minimal differences in trainable parameters of LAMP and DPT across different r values, LAMP consistently outperforms DPT and other baseline methods in most scenarios. This demonstrates the effectiveness of incorporating semantic knowledge into LAMP. The details of how the intrinsic rank r affects the changes in training parameters can be found in the Appendix A.8.\nEffect of Prompt Length. We conduct analyses using the RTE and WiC datasets within the Super-GLUE benchmark. To understand the impact of prompt length on the LAMP's performance, we maintained an \"intrinsic rank\" r of 8 for the soft prompt on the T5-Base model, varying the prompt lengths \u2208 {20, 100, 200}. Figure 4(c) and Figure 4(d) illustrates that LAMP consistently outperforms other baselines at prompt lengths of 20, 100, and 200. LAMP achieves optimal performance when the prompt length is set to 100, consistent with previous findings that 100 is the optimal hyperparameter for prompt length (Lester et al., 2021; Razdaibiedina et al., 2023). The experimental details of other datasets in the Appendix A.9.\nImpact of Model Scale. Based on quantification, we conducted experiments on T5-11B and Llama2-7B using randomly selected datasets and compared LAMP against baselines that initialize prompts with semantic knowledge from samples. As shown in Figure 5(a) and Figure 5(b), across different model architectures with more than 7B parameters (T5 with an encoder-decoder structure and Llama2 with a decoder-only structure), LAMP consistently helps models adapt to various downstream tasks, achieving superior performance. The experimental details of T5-3B can be found in the Appendix A.10. Meanwhile, Appendix A.11 details the changes in training parameters and memory usage associated with the model scale.\nBlocks of Average Pooling. Figure 5(c) illustrates the changes in model training time and performance on the SuperGLUE benchmark as the number of average pooling blocks increases. The larger the pooling block p, the shorter the prompt length input to the model, resulting in a more noticeable reduction in training time. Furthermore, we were pleasantly surprised to find that average pooling had minimal impact on performance, yet it provided significant advantages for PT. This finding offers a promising mentality for extending PT in various domains."}, {"title": "3.6 Interpretability (RQ3)", "content": "Figure 6 compares general PT before (i.e., blue points) and after (i.e., red points) extracting the intrinsic semantic associations between soft prompt tokens on the MultiRC and COPA datasets. To more intuitively reflect the discreteness of PT, we did not standardize the dimensions for comparison. The more extensive the x and y-axis ranges, the higher the degree of discreteness. The features extracted by LAMP exhibit more distinct clustering, indicating LAMP's superior ability to capture and represent the intrinsic structure and patterns of the data. By considering the interactions between prompt tokens, LAMP uncovers the intrinsic semantic associations among tokens to enhance knowledge representation. This enables PLMs to better grasp the semantic content of textual data.\nThe comparative results of discreteness between LAMP and original prompt tuning (PT) across other datasets are detailed in Appendix A.12."}, {"title": "4 Related Work", "content": "Parameter-efficient Fine-tuning. Parameter-efficient fine-tuning achieves strong results by training a small subset of parameters, thereby reducing computational costs and improving efficiency. AdapterDrop (R\u00fcckl\u00e9 et al., 2020) improves efficiency by removing unimportant adapters for a given task in each layer of the Transformer. BitFit (Zaken et al., 2021) only updates the bias terms while freezing most of the pre-trained model's parameters. LST (Sung et al., 2022) reduces training memory by running a small ladder network alongside the pre-trained network. LoRA (Hu et al., 2021) re-parameterizes incremental matrices through simple low-rank decomposition. KronA (Edalati et al., 2022) replaces the low-rank decomposition in LoRA with Kronecker product decomposition; PISSA (Meng et al., 2024) initializes the low-rank matrices with the weights of the pre-trained model, enhancing performance and efficiency. However, prompt tuning (Lester et al., 2021) stands out from the rest by achieving good results with training very few parameters.\nPrompt-based Fine-tuning. Unlike other PEFT methods, prompt-based fine-tuning methods sustain a controlled increase in trainable parameters, even with substantial model scaling. Prompt tuning (Lester et al., 2021) only adds the soft prompt to the input embedding layer of the model. DPT(Xiao et al., 2023) employs a re-parameterization strategy, using two low-rank matrices to replace the original soft prompt. DPT relies solely on random number generation for the soft prompt, resulting in weaker generalization and higher sensitivity to initialization. DePT(Shi and Lipani, 2024) decomposes the soft prompt into shorter prompts and pairs of low-rank matrices, which are then used to update the model's weights. EPT (Lan et al., 2024) leverages multi-space projection and prompt fusion to refine soft prompt knowledge, enhancing flexibility and balancing accuracy with computational efficiency for diverse tasks. Nevertheless, these PT-based methods need more efficiency and task-specific knowledge richness when dealing with long soft prompt. LAMP provides the ability to tailor prompts more precisely and effectively to the specific requirements of various tasks."}, {"title": "5 Conclusions", "content": "In this work, we observed that soft prompt tokens initialized randomly from the vocabulary lack intrinsic semantic associations to enhance knowledge representation. Additionally, PT-based methods face challenges balancing knowledge richness and computational cost in different tasks. Based on these issues, we approximate soft prompt by proposing a Low-parameter Prompt Tuning (LAMP) method, which utilizes two singular vectors and singular values. LAMP facilitates semantic knowledge interaction, allowing the soft prompt to incorporate more task-specific knowledge. It can serve as an efficient plugin for various PT-based tasks. Experimental results across three model scales (T5-Small, T5-Base, T5-Large) demonstrate that LAMP achieves high effectiveness and robustness with fewer trainable parameters."}, {"title": "Acknowledgements", "content": "This work is partially supported by the National Natural Science Foundation of China under Grant (No. 62032013, 62102074), the Science and Technology projects in Liaoning Province (No. 2023JH3/10200005)."}, {"title": "Limitations", "content": "While our method significantly reduces trainable parameters in NLP, its potential applications extend far beyond this domain. Its evaluation in areas beyond NLP and with other advanced large language models remains a work in the future. While our approach significantly reduces trainable parameters, further quantification of model parameters will also be explored in future research. The intrinsic semantic interactions between soft prompt tokens can be more effectively mined without increasing the number of trainable parameters. In future work, we will explore methods to enhance knowledge representation for PT."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Self-Attention Pooling", "content": "We propose utilizing a self-attention mechanism for the pooling operation to allow the soft prompt to adaptively assign different weights to prompt tokens -emphasizing key tokens while ignoring less important ones. The formula is expressed as follows:\nK = CW sa\nAweight = Softmax(K)\nP' = AT Aweight C\nwhere, Wsa \u2208 Rd\u00d7l/p is a learnable initialization weight matrix, and Aweight \u2208 Rl\u00d7l/p represents the attention weights. By applying Aweight to C \u2208 Rl\u00d7d, we achieve adaptive selection and pooling operation of prompt tokens, resulting in P' \u2208 Rl/p\u00d7d. The self-attention pooling operation introduces additional trainable parameters Wsa \u2208 Rdxl/p, and its performance is suboptimal. We hypothesize that this may be due to the compressed outer product effectively capturing the intrinsic semantic associations between prompt tokens. The dynamic weight assignment might disrupt these previously captured associations."}, {"title": "A.2 Dataset Details", "content": "Table 3 provides detailed information on the 8 datasets we used in SuperGLUE benchmark. The processing of all datasets follows the approach of Xiao et al. (2023)."}, {"title": "A.3 Baselines Details", "content": "Apart from full fine-tuning, due to the significant advantages of PT, all other baselines are variants based on PT. Descriptions of all PT-based baselines are as follows:\n\u2022 Full Fine-tuning: Updating all model parameters in the T5-models (Raffel et al., 2020) on each downstream task. It is the most fundamental method for comparing PEFT methods' performance and trainable parameters.\n\u2022 Prompt tuning (Lester et al., 2021): PT stands out in the PEFT approaches because it freezes the parameters of PLMs and only trains the attached soft (continuous) prompt vectors to the input text.\n\u2022 Residual Prompt tuning (Razdaibiedina et al., 2023): A PT-based variant (named Res PT) that utilizes a residual network to increase the flexibility of model selection for soft prompt token representations and improve the convergence rate.\n\u2022 DePT (Shi and Lipani, 2024): Decomposing the prompt into a shorter prompt and low-rank"}, {"title": "A.11 Change in LAMP Parameters", "content": null}, {"title": "A.11.1 Change in LAMP Parameters from Model Size", "content": "With intrinsic rank r = 8, Figure 9 illustrates the effect of model size on LAMP trainable parameters. As the model size increases, trainable parameters in original PT increases significantly, whereas LAMP mitigates this issue. Table 8 shows that when the prompt length l and intrinsic rank r remain constant, the higher the model size, the more significant the reduction in trainable parameters achieved by LAMP. LAMP also mitigates the impact of the hidden dimension on trainable parameters."}, {"title": "A.11.2 Variation of LAMP Parameters", "content": "Table 8 presents the variation of LAMP's trainable parameters with prompt length l\u2208 {20, 100, 1000, 5000, 10000} with r = 8. The longer the prompt length, the more LAMP downplays the impact of prompt length on trainable parameters, making LAMP's advantages more evident. For instance, in the Llama2-7B model, when the prompt length is set to 10,000, the number of"}]}