{"title": "KeyInst: Keyword Instruction for Improving SQL Formulation in Text-to-SQL", "authors": ["Xiping Liu", "Zhao Tan"], "abstract": "Text-to-SQL parsing involves the translation of natural language queries (NLQs) into their corresponding SQL commands. A principal challenge within this domain is the formulation of SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. However, the intrinsic disparity between the NLQ and the SQL poses a significant challenge. In this research, we introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by Large Language Models (LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query, thus facilitates a smoother SQL query formulation process. We explore two strategies for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. The former first generates KeyInst for question, which are then used to prompt LLMs. The latter employs a fine-tuned model to concurrently generate KeyInst and SQL in one step. We developed StrucQL, a benchmark specifically designed for the evaluation of SQL formulation. Extensive experiments on StrucQL and other benchmarks demonstrate that KeyInst significantly improves upon the existing Text-to-SQL prompting techniques.", "sections": [{"title": "1 Introduction", "content": "The task of Text-to-SQL parsing, which aims at translating natural language questions into executable SQL queries, has gained increasing attention in recent years, as it can help non-expert users quickly access information in the database without the need for technical background (Deng et al., 2021; Yu et al., 2020; Rajkumar et al., 2022; Ni et al., 2023). Text-to-SQL parsing faces two main challenges: schema linking and SQL formulation. Schema linking involves identifying the pertinent tables and columns in a database schema in response to an NLQ. SQL formulation refers to generating SQL queries that are not only syntactically correct but also semantically aligned with the natural language input.\nThis paper primarily focuses on the challenge of SQL formulation. Currently, most Text-to-SQL prompting methods induce Large Language Models (LLMs) to generate the target SQL directly using In-context Learning (ICL) (Nan et al., 2023; Pourreza and Rafiei, 2024a; Tan et al., 2024). However, the vast difference between natural language queries (NLQ) and SQL hinders precise query formulation. In previous works, the skeleton-aware decoder (Li et al., 2023) was proposed to alleviate this challenge by initially generating an SQL skeleton followed by the full query. An SQL skeleton is a basic framework of an SQL query consisting of SQL operators, without specific details such as column names, table names, or conditions. Incorporating SQL skeleton in prompting has also proven to be effective (Gao et al., 2023; Guo et al., 2023). In this work, we also use the SQL structure as a central element in SQL formulation, with a particular emphasis on identifying key SQL operators. For instance, in translating the NLQ \"List the customers' first and last names from the 10 least expensive invoices\", accurately identifying ORDER BY and LIMIT is crucial for formulating the correct SQL query.\nWe introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by LLMs. KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query. Recognizing that SQL queries corresponding to different NLQs require distinct keywords, KeyInst adapts dynamically to each query. This strategy effectively narrows the gap between NLQ and SQL, facilitating a smoother SQL query formulation process."}, {"title": "2 Methods", "content": "This paper introduces KeyInst to address the challenge of SQL formulation in Text-to-SQL parsing. The main idea is to analyze the NLQ to understand its intent, providing explicit guidance for SQL formulation by identifying essential keywords crucial for translating the NLQ into the target SQL. KeyInst is generated in real-time for each Text-to-SQL task.\nWe prepared over 6,200 KeyInst examples from the Spider training set, organized into a KeyInst set SKeyInst = {(Di, Qi, Ki, Si)}, where Di is the database schema, Qi is the question, Si is the SQL, and Ki is the KeyInst.\nEach KeyInst consists of two parts: question analysis and keyword suggestion, as shown in Figure 2. The question analysis is generated by prompting LLMs (see Appendix A). For keyword suggestions, we parse the SQL structure to identify all the keywords it uses, then filter out non-essential ones. Keywords are prioritized as follows: highest priority (GROUP BY, HAVING, ORDER BY, LIMIT, EXCEPT, INTERSECT, UNION, WHERE), second priority (SELECT, FROM). Lower priority keywords are only added if an SQL lacks higher priority keywords. Other Keywords(JOIN, COUNT, IN, and others) are excluded from the keyword suggestions. Without keyword prioritization, KeyInst would degrade into an SQL skeleton, which includes all the keywords of an SQL statement. More details about the skeleton can be found in Appendix B.\nWe implement the applications of KeyInst based on the KeyInst set SKeyInst, detailed in the following sections."}, {"title": "2.1 Pipeline Approach of KeyInst", "content": "We propose the pipeline approach as one application of KeyInst, generating SQL in two steps. First, generating the tailored KeyInst for the each Text-to-SQL task, then prompt LLMs with the generated KeyInst to generate SQL query. This section details this application."}, {"title": "2.1.1 Fine-tuned KeyInst Generator", "content": "One approach for KeyInst generation is to fine-tune a model to become a KeyInst generator. Using supervised fine-tuning, the input is a database shcema Di a question Qi, and the target output is the corresponding KeyInst Ki. The primary objective is to minimize the following loss function:\n$\\min \\frac{1}{N}\\sum_{i=1}^{N} C (M_{\\theta} (D_i, Q_i), K_i),$ (1)\nwhere L represents the loss related to the model's next token prediction, comparing the predicted KeyInst with the actual ground truth. This fine-tuned model, referred to as the fine-tuned KeyInst generator, analyzes the question and generates a tailored KeyInst, called KeyInst-FT, to prompt LLMs in SQL formulation."}, {"title": "2.1.2 In-context Learning KeyInst Generator", "content": "Another apporach for KeyInst generation is to prompt LLMs with ICL to generate KeyInst. We select a few examples (i.e., demonstrations) from the KeyInst set SKeyInst to form a few-shot prompt for generating a tailored KeyInst for a Text-to-SQL task. Each example contains a database schema Di, a question Qi and its corresponding KeyInst Ki. For each Text-to-SQL task, we select the top-m most similar examples based on masked question similarity (Gao et al., 2023) and combine them with the current database schema D and question Q to create a few-shot prompt. This few-shot prompt guides the LLMs to generate the tailored KeyInst,"}, {"title": "2.1.3 SQL formulation with KeyInst", "content": "SQL formulation follows KeyInst generation. A basic usage is to combine it with the database schema D, question Q, and KeyInst K to construct a zero-shot prompt that guides LLMs to generate SQL. This can be formulated as:\n$P_{LLms} (y | x) = P (y | prompt (D, Q, K)),$ (3)\nwhere x is the LLMs' input (i.e., the zero-shot prompt), and y is the expected SQL output.\nNotably, KeyInst functions as an instruction to enhance the SQL formulation capabilities of LLMs. It is highly extensible and can be effortlessly integrated with existing Text-to-SQL prompting methods. By appending KeyInst to these methods' prompts, their performance can be significantly improved. This will be analyzed in detail in \u00a74.3."}, {"title": "2.2 Single-Pass Approach of KeyInst", "content": "We propose a single-pass approach for KeyInst as an alternative application method of KeyInst. In this approach, a fine-tuned model simultaneously generates both KeyInst and SQL in a single pass. The generation of KeyInst serves as an initial reasoning step, and the fine-tuning process helps the model internalize this reasoning, thereby improving its ability to formulate SQL queries.\nThis involves supervised fine-tuning, where inputs are the database schema Di and the question Qi, and targets are the KeyInst Ki and the SQL statement Si. The objective is to minimize the empirical loss:\n$\\frac{1}{N}\\sum_{i=1}^{N}L (M_{\\theta} (D_i, Q_i), (K_i, S_i)),$ (4)\nwhere L represents the loss related to the model's next token prediction, comparing the predicted KeyInst and SQL with the actual ground truth.\nThe key difference between our fine-tuned model and a common Text-to-SQL fine-tuning model lies in the output. Our model first generates a KeyInst before generating the SQL, effectively reasoning about SQL formulation. Fine-tuning enables the model to remember this reasoning pattern, so it can spontaneously perform the reasoning when encountering a Text-to-SQL task."}, {"title": "3 StrucQL: A Structural Benchmark for Text-to-SQL", "content": "StrucQL is developed to allow researchers to swiftly and independently assess the SQL formulation capabilities of Text-to-SQL systems. Text-to-SQL errors can be categorized into semantic errors, which reflect schema linking capabilities, and structural errors, which pertain to SQL formulation skills.\nHowever, widely-used Text-to-SQL benchmarks such as Spider (Yu et al., 2018) and Bird (Li et al., 2024b) focus on overall parsing performance and lack mechanisms for isolating evaluations of SQL formulation. Previous studies have relied on expensive manual evaluations to gauge structural performance (Ning et al., 2024), underscoring the necessity for a specialized structural benchmark for Text-to-SQL systems.\nSQL structural errors fall into two categories: (1) syntax errors, such as mismatched parentheses, which make SQL unexecutable, and (2) structural misalignments with the NLQ, such as inappropriate keyword usage. While LLMs can easily generate syntactically correct SQL due to extensive pre-training, the real challenge is ensuring structural alignment with the NLQ. StrucQL, therefore, focuses on evaluating this alignment.\nWe developed StrucQL by modifying the Spider dataset and utilizing GPT4\u00b9 for assistance. StrucQL comprises 1050 examples and covers 7 types of SQL operation. Each example is dedicated to a single operation type. To mitigate schema linking difficulties in the Text-to-SQL task, we implemented schema simplification. This process reduces schema linking errors, thereby providing a clearer assessment of SQL formulation. Specifically, we replaced schema-related terms in the original NLQs with the corresponding table and column names, and filtered out tables and columns from the database schema that were irrelevant to the questions."}, {"title": "4 Experiments", "content": "In this section, we systematically assess the effectiveness of KeyInst. Our evaluation centers on two primary aspects: (1) comparing the performance of different applications of KeyInst, and (2) examining the performance improvements when KeyInst is integrated with current state-of-the-art (SOTA) Text-to-SQL prompting methods."}, {"title": "4.1 Setup", "content": "Models We selected five LLMs for our experiments: Gemma-7B-It (Team et al., 2024) (Gemma-7B), Llama-3-8B-Instruct\u00b2 (Llama3-8B), Llama-3-70B-Instruct\u00b2 (Llama3-70B), Claude-3-Opus-202402293 (Claude3), and GPT-4-Turbo-2024-04-091 (GPT4).\nHyperparameters For fine-tuning method, the Llama3-8B model is trained on Nvidia Tesla A100 GPUs, employing a batch size of 32 with a learning rate of 1*e-5. For the prompting method, we perform greedy decoding at a temperature of 7 = 0 to ensure reproducible results.\nBenchmarks We used the following benchmarks: StrucQL, Spider (Yu et al., 2018), and Bird (Li et al., 2024b). StrucQL, introduced in this paper, evaluates the SQL formulation of Text-to-SQL systems. Spider is a large-scale benchmark with 8,000 training samples and 1,034 development samples across multiple databases. The BIRD dataset features 12,751 question-SQL pairs, covering 95 large databases across 37 professional fields.\nMetrics We use execution accuracy (EX) to evaluate different methods. This metric compares the execution output of the predicted SQL query with that of the ground truth SQL query on same database instances.\nBaselines We selected three SOTA Text-to-SQL prompting methods as the baselines.\n(1) DIN-SQL (Pourreza and Rafiei, 2024a): This pipeline prompting method that involves schema linking, difficulty classification, SQL generation, and SQL self-correction.\n(2) DAIL-SQL (Gao et al., 2023): An efficient few-shot prompting method that selects demonstrations based on similarity of masked question and SQL skeleton. We use DAIL-SQL with 8 shots.\n(3) SC-SQL (Tan et al., 2024): This method integrates multiple Text-to-SQL reasoning paths and selects the best candidate result from these paths.\nOur methods We introduce KeyInst and apply it"}, {"title": "4.2 Results on StrucQL", "content": "We introduce StrucQL as a benchmark designed to evaluate the performance of Text-to-SQL systems in the challenge of SQL formulation. In this section, we analyze the performance of various applications of KeyInst on the StrucQL benchmark."}, {"title": "4.2.1 Comparison of KeyInst Applications", "content": "we compared two applications of KeyInst: the pipeline approach and the single-pass approach. The KeyLla, a Llama3-8B model fine-tuned with KeyInst, represents the single-pass approach. To ensure a fair comparison, we also used the Llama3-8B model within the pipeline to generate SQL. We utilized two variants of KeyInst: KeyInst-FT and KeyInst-ICL. Despite their different generation methods, both variants serve as part of the prompt to guide the Llama3-8B in SQL formulation.\nTable 2 demonstrates that the single-pass approach (i.e., KeyLla) outperforms the pipeline approach (i.e., KeyInst-FT and KeyInst-ICL) in terms of performance. KeyLla effectively internalizes KeyInst's reasoning for SQL formulation, leading to superior results. While the single-pass approach achieved the best performance, the pipeline approach can also achieve comparable results. Additionally, the pipeline approach have the advantage of being able to leverage more powerful LLMs, such as GPT4. Achieving similar performance through single-pass approach would be significantly more costly."}, {"title": "4.2.2 Comparison of KeyInst-FT and KeyInst-ICL", "content": "Table 3 presents the performance of KeyInst-FT and KeyInst-ICL on more powerful LLMs. Using KeyInst-FT to prompt GPT4 achieves the highest EX result at 84.3%. This outcome demonstrates the advantage of the pipeline approach, which can achieve excellent performance with low computational resources by leveraging the powerful natural language processing capabilities of LLMs.\nTable 3 also highlights the superior performance of KeyInst-FT over KeyInst-ICL. KeyInst-ICL provides overly detailed keyword suggestions, such as AVG, COUNT, JOIN, and IN (see Appendix D for examples). These keywords, defined as the lowest priority in \u00a72, are not expected to appear in KeyInst. Excessive detail can hinder LLMs'"}, {"title": "4.2.3 Ablation Study", "content": "Table 4 presents the results of the ablation study. Each KeyInst consists of two parts: question analysis and keyword suggestion (see Figure 2). The question analysis explains the NLQ of the current Text-to-SQL task, while the keyword suggestion provides potential SQL keywords for the current Text-to-SQL task. To assess the contribution of each component, we compared single-pass and pipeline approaches both with and without these parts. For the single-pass approach, we split the training data accordingly, and for the pipeline approach, we separated the KeyInst components when prompting the LLMs. The results in Table 4 indicate that the keyword suggestion plays a more significant role in the effectiveness of KeyInst."}, {"title": "4.2.4 Results of Baselines", "content": "Table 5 presents the performance of baseline methods (DIN-SQL (Pourreza and Rafiei, 2024a), DAIL-SQL (Gao et al., 2023), SC-SQL (Tan et al., 2024)) and our KeyInst-FT method on StrucQL. The results indicate that while these baseline methods, as SOTA Text-to-SQL prompting approaches, achieve commendable results on well-known benchmarks (e.g., Spider and Bird), there is still room for improvement in SQL formulation capabilities, particularly in set operations (EXCEPT, INTERSECT, UNION), where our method excels. We believe that explicitly mentioning SQL keywords relevant to the current Text-to-SQL task in the prompt is crucial for enhancing the LLMs' SQL formulation performance. This is supported by DAIL-SQL's strong performance, which is attributed to its consideration of SQL skeleton similarity when constructing few-shot prompts, thus the prompt may contain important SQL keywords that are relevant to the current task."}, {"title": "4.3 Results on General Benchmark", "content": "We also evaluated KeyInst on well-known benchmarks, such as Spider and Bird. KeyInst is instruction and can be easily integrated with existing Text-to-SQL prompting methods by appending KeyInst to their prompts. We used the GPT4 model to assess the performance of baseline methods with KeyInst.\nWe conducted experiments using KeyInst-FT (a variant of KeyInst). When used alone, KeyInst serves as a zero-shot prompting method. While it performs well compared to standard zero-shot methods, it does not match the current these SOTA Text-to-SQL prompting methods because it specifically addresses the SQL formulation challenge and"}, {"title": "4.4 Discussion", "content": "How to choose the application method for KeyInst? We propose two applications for KeyInst: single-pass and pipeline. When computational resources are abundant, the single-pass approach, which involves fine-tuning a model with KeyInst can maximize its SQL formulation capabilities. However, since computational resources are often limited, the pipeline approach becomes more advantageous as it can leverage more powerful models without extensive training. Therefore, we believe that the pipeline approach for KeyInst deserves more attention. Within this approach, fine-tuning a KeyInst generator (if resources allow) can produce more effective KeyInsts than using the in-context learning KeyInst generator.\nHow to use KeyInst? Due to KeyInst's lightweight design, KeyInst offers strong compatibility, especially evident in the prompting method. We do not recommend relying solely on KeyInst to solve Text-to-SQL tasks, as these tasks often also encounter the challenge of schema linking. We advocate for the integration of KeyInst with other Text-to-SQL prompting methods. This integration is straightforward because KeyInst is presented as an instruction within a prompt. The excellent compatibility of KeyInst holds significant potential for future research."}, {"title": "5 Relate Work", "content": "SQL formulation Previous works typically propose well-designed decoders to address SQL formulation challenge. (Wang et al., 2020; Cai et al., 2021; Qi et al., 2022). RESDSQL (Li et al., 2023) introduces a skeleton-aware decoder that first generates an SQL skeleton and then fills the slots, proving to be very effective. A new trend involves prompting LLMs (Chen et al., 2023; Liu et al., 2023), focusing on task decomposition, or selecting demonstrations for few-shot prompts. DIN-SQL (Pourreza and Rafiei, 2024a) uses a pipeline to sequentially address schema linking and SQL formulation, while SC-SQL (Tan et al., 2024) emphasizes result consistency (Wang et al., 2022). DAIL-SQL (Gao et al., 2023) selects demonstrations based on masked question and SQL skeleton similarity. Nan (Nan et al., 2023) and Guo (Guo et al., 2023) propose similar methods. These approaches rely on implicit information in demonstrations, leading to suboptimal SQL formulation. In contrast, KeyInst explicitly guides LLMs to use specific SQL keywords through tailored instructions.\nBenchmarks Popular benchmarks like Spider (Yu et al., 2018), and Bird (Li et al., 2024b) evaluate comprehensive Text-to-SQL capabilities. More challenging datasets like Spider-Syn (Gan et al., 2021a), Spider-DK (Gan et al., 2021b), and Ambiguity (Bhaskar et al., 2023) focus on schema linking. However, the field lacks a benchmark for SQL formulation performance. Therefore, we propose StrucQL to help researchers assess the SQL formulation capabilities of Text-to-SQL systems."}, {"title": "6 Conclusion", "content": "This paper introduces KeyInst, a dynamic instruction method explicitly highlighting essential SQL keywords likely to be included in the target SQL query. We explore two approaches for integrating KeyInst into Text-to-SQL parsing: the pipeline approach and the single-pass approach. In the pipeline approach, KeyInst is used to prompt LLMs. In contrast, the single-pass approach involves fine-tuning a model with KeyInst. Our results indicate that, for models of the same size, the single-pass approach outperforms the pipeline approach. However, the pipeline approach excels in flexibility, easily integrating with more powerful LLMs to achieve superior performance. Due to KeyInst's lightweight design, KeyInst integrates seamlessly with existing Text-to-SQL prompting methods, enhancing their performance. This compatibility suggests promising potential for future research in Text-to-SQL prompting."}, {"title": "Limitations", "content": "In this paper, we made an effort to demonstrate the effectiveness of KeyInst, but there are still some"}, {"title": "A Prompt of Question Analysis", "content": "In \u00a72, we constructed a KeyInst set, where the KeyInsts were pre-prepared. For the question analysis part of the KeyInsts, we used the following prompt to guide the GPT4 model. It is a few-shot prompt containing 7 demonstrations. The prompt is:\nPlease analyse the following natural language query.\nNatural language query: Please show the different statuses of cities and the average population of cities with each status.\nAnalysis: The question is asking for a list of different statuses of cities and the average population for cities within each status. This requires grouping the cities by their status and calculating the average population for each group.\nPlease analyse the following natural language query.\nNatural language query: What is the average longitude of stations that never had bike availability more than 10?\nAnalysis: The question is looking to calculate the average longitude of bike stations where the number of available bikes never exceeded 10. This requires filtering out stations based on a condition applied to their bike availability data.\nPlease analyse the following natural language query.\nNatural language query: List the writers of the books in ascending alphabetical order.\nAnalysis: The question is asking to retrieve a list of writers from the book table and sort them in ascending alphabetical order. This requires selecting the Writer column and ordering the results.\nPlease analyse the following natural language query.\nNatural language query: List the publisher of the publication with the highest price.\nAnalysis: The question is asking to identify the publisher of the publication that has the highest price. This requires sorting the publications by price in descending order and selecting the top result.\nPlease analyse the following natural language query.\nNatural language query: Show ids for all employees who don't have a certificate.\nAnalysis: The question is asking for the IDs of employees who do not possess any certificates. This requires comparing two sets of data: one from the Employee table and one from the Certificate table, and then finding the difference between these two sets.\nPlease analyse the following natural language query.\nNatural language query: Show names for all employees who have certificates on both Boeing 737-800 and Airbus A340-300.\nAnalysis: The question is looking for the names of employees who hold certificates for both the Boeing 737-800 and the Airbus A340-300 aircraft. This requires identifying employees who have certificates for both aircraft types and then retrieving their names.\nPlease analyse the following natural language query.\nNatural language query: Find courses that ran in Fall 2009 or in Spring 2010.\nAnalysis: The question is looking for courses that were offered either in the Fall semester of 2009 or in the Spring semester of 2010. This requires filtering records based on specific conditions for both the semester and the year."}, {"title": "B Comparsion of KeyInst and SQL skeleton.", "content": "In \u00a72, we assigned different priorities to SQL keywords and considered which keywords should be included in KeyInst's keyword suggestions based on these priorities. This approach was taken because general keywords (e.g., JOIN, IN, COUNT) do not directly reflect the query intent corresponding to NLQs. Instead, overly detailed information can increase the burden on the KeyInst generator and potentially affect the output of LLMs.\nWe conducted a comparative experiment where we did not set keyword priorities. In this scenario, KeyInst degraded into an SQL skeleton, as illustrated in Figure 3. The details of the comparative experiment are as follows: we replaced the KeyInst in the KeyInst set with SQL skeletons and fine-tuned the Llama3-8B model to become an SQL skeleton generator. This generator produces an SQL skeleton for each Text-to-SQL task, and this generated SQL skeleton is then used as part of the prompt to guide the LLMs in generating SQL.\nThe results in Table 7 show that using the skeleton is significantly less effective than using KeyInst-FT(a version of KeyInst). Although the skeleton (Figure 3) appears more specific on the surface compared to KeyInst, directly deriving an SQL skeleton from an NLQ is not easy. This often leads to unexpected errors, which can mislead the LLMs when generating SQL."}, {"title": "C An Example of Schema Simplification.", "content": "We constructed the StrucQL benchmark to intuitively evaluate a Text-to-SQL system's SQL formulation performance by minimizing schema linking's impact. This is achieved through schema simplification, which aims to reduce the complexity of schema linking in Text-to-SQL tasks, thereby decreasing the errors caused by incorrect schema links. Figure 4 provides an example of the schema-simplified question and schema. Specifically, we marked and modified schema-related words in the question. For instance, in Figure 4, 'name of the shop' was changed to 'shop.name'. Additionally, we filtered out tables and columns from the database schema that are irrelevant to the current question."}, {"title": "D Examples of KeyInst-FT and KeyInst-ICL", "content": "We propose two variants of KeyInst: KeyInst-FT and KeyInst-ICL KeyInst-FT is generated by a fine-tuned Llama3-8B model, while KeyInst-ICL is generated by guiding LLMs using In-Context learning. Our experiments demonstrate that KeyInst-FT performs better. Examples of KeyInst-FT and KeyInst-ICL are provided in Figure 5. These examples show that KeyInst-FT aligns more closely with the requirements of gold SQL. Specifically, KeyInst-FT consistently produces more accurate and contextually appropriate keyword suggestions. This comparison highlights the advantage of fine-tuning models for specific tasks."}, {"title": "E The usage of KeyInst", "content": "KeyInst is represented as a single instruction, which gives it excellent compatibility and allows it to integrate with existing Text-to-SQL prompting methods seamlessly. Specifically, each KeyInst is tailored to the current Text-to-SQL task. To use it, we place it with the current Text-to-SQL task, typically at the"}, {"title": "F Performance of LLMs with KeyInst on StrucQL", "content": "We evaluated the performance of various LLMs on StrucQL after using KeyInst (KeyInst-FT). The results in Table 8 show that KeyInst is a simple and effective method that significantly enhances the SQL formulation performance of LLMs. Notably, Llama3-70B with KeyInst is only 1.1% behind GPT-4 with KeyInst."}]}