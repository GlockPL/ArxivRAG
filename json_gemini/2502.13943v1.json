{"title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence", "authors": ["Yuliang Liu", "Junjie Lu", "Zhaoling Chen", "Chaofeng Qu", "Jason Klein Liu", "Chonghan Liu", "Zefan Cai", "Yunhui Xia", "Li Zhao", "Jiang Bian", "Chuheng Zhang", "Wei Shen", "Zhouhan Lin"], "abstract": "Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities. We provide our code on GitHub.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks. However, even most advanced LLMs struggle to generate correct solutions when facing complex reasoning problems, such as mathematical reasoning and code generation tasks (Huang et al., 2024; Tyen et al., 2024; Mirzadeh et al., 2024; Shen & Zhang, 2024). To address these challenges using the stepwise Chain of Thought (CoT) approach (Wei et al., 2023), various strategies have been proposed by the research community (Qin et al., 2024; DeepSeek-AI et al., 2025; Team et al., 2025). One promising method is training Process Reward Models (PRMs), which offer more fine-grained rewards at each reasoning step compared to Outcome Reward Models (ORMs), guiding the LLM to generate higher-quality responses than the original model output (Shao et al., 2024; Sessa et al., 2024; Gao et al., 2024).\nHowever, as illustrated in Figure 1, existing PRMs typically divide a model's response into multiple reasoning steps using rule-based methods, such as chopping with a pre-defined symbol. This results in a series of coarse reasoning step divisions that lack decision-making information at steps (Wang et al., 2024a; Lightman et al., 2023). Moreover, rule-based methods also face challenges when applied to tasks where the steps are difficult to define. Some studies have explored the application of PRMs at the level of individual tokens or fixed number of tokens (Lee et al., 2024; Luo et al., 2024), nevertheless, balancing annotation costs with the granularity of division remains a challenge. Although studies have demonstrated the advantages of PRMs over ORMs, these limitations, along with the high building costs, continue to constrain the broader adoption of PRMs (DeepSeek-AI et al., 2025).\nTo address these issues, we aim to find an automatic step-dividing method to divide reasoning solutions into more informative steps, in contrast to the coarse division by rule-based methods. As suggested by Kahneman (2011), the cognitive cost of reasoning varies depending on the difficulty of the decision or task. Additionally, a statistical analysis of common errors in reasoning tasks conducted by Roy & Roth (2016) revealed that many errors stem from incorrect numerical calculations or the misapplication of words, particularly verb misuse. This suggests that certain types of words or positions in the reasoning process require more attention.\nTherefore, our goal is to divide the reasoning responses at these key positions to ensure the valuable costs during inference and training. We find that by pivoting on the prediction confidence, the model can automatically identify the critical breaking points in the reasoning process. Accordingly, we propose AdaptiveStep, a method that divides reasoning steps based on model confidence (Hills & Anadkat, 2024). We conduct experiments on the PRM scenario, with the resulting PRM named the AdaptiveStep Process Reward Model (ASPRM). This dividing method yields highly informative step divisions, enabling downstream tasks (e.g., processing the reward model) to enhance performance.\nIn our experiments, we assess the effectiveness of ASPRM in mathematical reasoning and code generation tasks using the Best of N (BON) evaluation. For the mathematical reasoning task, we evaluate on GSM8k (Cobbe et al., 2021) and MATH500 (Lightman et al., 2023) dataset. For the code generation task, we collect a dataset named LeetCodeDataset containing 1,940 problems from LeetCode, along with the corresponding Python solutions and test cases, which include training and test splits to train and evaluate the PRM and further assess it on the Livecodebench (Jain et al., 2024).\nAdditionally, the most widely used PRM step-dividing method relies on fixed symbols, limiting the accuracy of the more fine-grained judgment ability of PRMs. We find that ASPRM can provide precise rewards to perform Token-level Value-guided Decoding (TVD) for reasoning tasks, offering another evaluation method by integrating PRM directly into the model inference process.\nIn mathematical reasoning tasks, ASPRM outperforms previous open-source methods in BoN evaluation. In addition,"}, {"title": "2. Related Works", "content": "Step-wise methods for LLMs reasoning: Chain-of-Thought (CoT) (Wei et al., 2023) reasoning has become a foundational approach in LLM reasoning. Scaling the number of tokens and steps in test time to tackle complex problems has become common practice (Team et al., 2025; DeepSeek-AI et al., 2025). In this paradigm, the model generates an intermediate step-wise solution before providing a final answer. As expectations for model performance on more complex tasks increase, methods for step-wise verification and alignment have also advanced rapidly, like PRM (Zhang et al., 2024; Wang et al., 2024a; Yuan et al., 2024) and step-wise RLHF (Chen et al., 2024; Lai et al., 2024; Wang et al., 2024b). Inference time step-wise methods also significantly enhance the model's reasoning capabilities, such as Monte Carlo methods (Feng et al., 2023), step-wise self-consistent (Zhao et al., 2024), step-wise beam search (Lee et al., 2024) and flexible divide-and-conquer methods (Yao et al., 2023; Hao et al., 2023) for planning.\nPRM for LLM reasoning and step-dividing methods: The importance of intermediate reasoning steps in LLMs for complex tasks was highlighted by Uesato et al. (2022),"}, {"title": "3. Methods", "content": "In this section, we first introduce how AdaptiveStep divides responses into reasoning steps, and then present how a PRM can be trained on these data, as shown in Figure 2. At last, we introduce Token-level Value-guided Decoding (TVD) that can get better responses using the trained PRM."}, {"title": "3.1. AdaptiveStep", "content": "Given a question $q \\in Q$, we can generate N responses with temperature-based random sampling using the language model $\\pi$. We denote generated responses as $\\{s^1,s^2,\\ldots,s^N\\}$ with $s^n \\in S$. (For ease of notation, we omit the dependence of the response $s^n$ on q.) In this way, we obtain a set of question-response pairs (Q\u00d7 S).\nTo divide the responses into reasoning steps, we use the probability of the sampled token as the metric for model confidence (Hills & Anadkat, 2024). Then we determine a threshold $\\tau$, which is based on a certain proportion of the token count, such that the tokens below this threshold become a breaking point.\nSpecifically, the model confidence can be written as\n$c_{s_i^n} = p(s_i | \\pi, q, s_{<i}^n)$ (1)\nwhere we use $s_{<i}^n$ and $s_i$ to denote the i-th token and the tokens prior to the i-th token in the response respectively. Low model confidence at the i-th token indicates that the model is hard to determine the token selection at the i-th position, and therefore this position may become the starting point of a new reasoning step.\nAccording to the above procedure, we divide the response $s^n$ into K reasoning steps $s^n = \\{r_1,r_2, ...,r_K\\}$ where the last token within each reasoning step is associated with below-the-threshold model confidence."}, {"title": "3.2. PRM Training", "content": "To train a PRM based on the question-response pairs with divided reasoning steps, we first need to estimate the target reward for each reasoning step and then train a PRM that can predict the reward.\nTo estimate the target reward, we mainly follow the heuristic rollout method proposed by Wang et al. (2024a). We rollout the response generation process J times starting from each reasoning step, resulting in rollouts denoted as $\\{p, r_1, ..., r_K, ,t_j\\}_{k\\in[K],j\\in[J]}$ where $t_j$ is the j-th trajectory starting from a partial response.\nThen, we estimate the target reward of this step based on the correctness of any decoded solution. We use hard estimation (HE) to estimate the reward for step the $r_k$. HE indicates whether any of the responses starting from the current partial response can reach a correct answer. For our implementation, in the code generation tasks, we define correctness as whether the solution can pass all test cases; in the math reasoning tasks, we define correctness as whether the answer matches the ground truth. Formally, the target reward can be estimated as\n$e_{r_k} = \\begin{cases} 1, & \\exists j \\in [J], \\{r_1, ..., r_k, t_j \\} \\text{ is correct} \\\\ 0, & \\text{otherwise} \\end{cases}$ (2)\nWith the target rewards estimated based on the rollouts, we can train PRM using the following loss:\n$C_{PRM} = \\sum_{k=1}^{K} [r_k \\log \\hat{r}^\\theta + (1 - r_k) \\log (1 - \\hat{r}^\\theta)],$ (3)\nwhere r is the target reward and $\\hat{r}^\\theta := R^\\theta(p,r_1,\\ldots,r_k)$ denotes the reward predicted by the PRM $R^\\theta$."}, {"title": "3.3. Token-level Value-guided Decoding", "content": "The TVD strategy leverages the PRM to guide token selection during language model decoding. Specifically, when the model encounters a low confidence score (Top-1 probability $C_p < \\tau$) in decoding, it triggers the PRM to evaluate the tokens associated with the highest M probability given by the policy model : $\\mathcal{S}_i = \\{s^*_i, s^2_i,...,s^M_i\\}$.\nAmong these candidates, the PRM selects the token it considers the best based on its learned reward estimation mechanism:\n$s_i = \\arg \\max_{s^m \\in \\mathcal{S}_i} R^\\theta (p, s_{<i}, s^m),$\" (4)\nwhere $s_i$ is the token selected as the optimal choice for the low-confidence decoding position, and $R^\\theta(\\cdot)$ represents the score given by the PRM.\""}, {"title": "4. Experiments and Analysis", "content": "In this section, we first show our experiment setup, including the dataset usage, model selection, baselines, metrics, and parameter setup. We then present the experimental results, followed by an analysis of the transferability, generalization, and features of the division."}, {"title": "4.1. Experiments Setup", "content": "Datasets and models: We use MetaMathQA (Yu et al., 2023) to train Mistral-V0.1 (Jiang et al., 2023) (Mistral), which is termed MetaMath-Mistral, to serve as \u03c0 in math domain, and use LeetCodeDataset\u00b9 training data to train Deepseek-Coder-Base (Guo et al., 2024), which is called LCD-DS to serve as in the code domain. To\nTo train ASPRM for code tasks, We collected 1,745 problems from the LeetCode problems as our training set and 175 problems as the test set. The test cases for these data are manually collected from the LeetCode website (excluding the test cases within the problem). The solutions are gathered from GitHub open-sourced repositories, mainly from https://github.com/doocs/leetcode, checked by GPT-4, and cross-verified with the test cases.\nget the math PRM training data, we sample MATH and GSM8k training datasets using MetaMath-Mistral and sample LeetCodeDataset training data using LCD-DS to generate code PRM training data. For evaluation, We use MATH500, the GSM8k test set, the LeetCodeDataset test set, and LiveCodeBench-V4 (Jain et al., 2024). To align with previous work and conduct further analysis, we train two math PRMs: ASPRM-L (based on Meta-Llama-3.1-8B (Grattafiori et al., 2024), which is called Llama in the following) and ASPRM-M (based on Mistral-V0.1), both with MetaMath-Mistral generated training data. And one code PRM: ASPRM-D (based on DeepSeek-Coder-Base) with LCD-DS generated training data.\nBaselines and metrics: There are several open-sourced PRMs in the math domain, we select Math-Shepherd (Wang et al., 2024a) and ER-PRM (Zhang et al., 2024) as our baselines. For the code domain, due to the limited availability of open-source code PRMs with competitive construction costs, we trained a code ORM as a baseline using the same data, with only the final rating position considered.\nFor all tasks, we evaluate the PRMs' performance using the Best of N (BoN) metric and further assess model capabilities with TVD. In math reasoning tasks, we evaluate whether the model's final answer matches the ground truth exactly. In the code tasks, we test the generated code by running it in a sandbox and checking if it passes all test cases. Following Wang et al. (2024a), we use the minimum PRM score across all scored steps as the PRM's final judgment for a given candidate in BoN.\nParameter Settings: We sample 30 times per data point and deduplicate the responses in Step 1. For labeling the PRM training data, we perform 8 rollouts per step using the same model \u03c0. This process generates 388k PRM training samples. We use MetaMath-Mistral-generated data to train the math PRM. And we get 49k PRM samples for the code"}, {"title": "4.2. Overall Results", "content": "BoN Results We report the BoN evaluation results for the math dataset in Figure 4, and for the code dataset in Figure 5, respectively.\nIn the math tasks, ASPRM-L performs best across Figure 4(a), 4(b) and 4(d) despite under more stringent conditions: the training data sources, and the construction costs and models. 1) For the training data sources, ASPRM only utilizes the GSM8k and MATH training sets during training data construction, while both ER-PRM and Math-Shepherd used the MATH test set (without using MATH500), which results in our performance being inferior to theirs on MATH500. 2) For the costs and models used in construction, the data construction costs for ASPRM is less than 70% of that for the other two methods and only used a single construct model. In addition to the above problems that lead ASPRM-M to poor performance in the MATH500 dataset, we attribute its performance in Figure 4(a) to the training dataset is constructed by a single model, constrained its test-time transferability.\nIn the code tasks results shown in Figure 5, ASPRM-D demonstrates superior judgment ability. As N increases, the robustness of ASPRM-D outperforms that of ORM.\nTVD Results We report TVD results in Table 1. In the math reasoning task, ASPRM has consistently shown an ability to enhance the reasoning capacity of the inference models. While the performance guided by ER-PRM and Math-Shepherd does not always demonstrate improvement, we hypothesize this is due to that the inference models already perform well with the greedy search on GSM8k, needing a more precise score to provide better guidance."}, {"title": "4.3. Transferability and Generalization Analysis", "content": "In this part, we investigate whether ASPRM demonstrates model transferability and rating position, in-domain, and cross-domain generalization capability, and the performance of mixed-domain data-trained PRM. In our experiments, unless otherwise specified, the BoN candidate generator and TVD inference model is MetaMath-Mistral.\nASPRM exhibit model transferability: Since the quality of training data generated by rollout depends on the polity \u03c0, we explore the transferability of training data of our method. We get 371k PRM training data generated by MetaMath-Llama and conduct the same process as MetaMath-Mistral. In Table 2, we find that training Mistral-V0.1 on data generated by MetaMath-Llama retains judgment ability, but its performance is weaker than that trained on data generated by the weak MetaMath-Mistral. This suggests that data generated through rollout has reasonable but limited transferability. Using multiple models for data construction, as in the Math-Shepherd, may be an effective strategy to enhance transferability.\nASPRM exhibit rating position generalization: We evaluate the rating position generalization of different PRMs and show the results in Table 3. Three setups are employed in our experiments: confidence, random, and hard, we explain the setting in the caption of Table 3. The performance of ER-PRM-L shows a significant difference between the hard and random setups, whereas the difference of ASPRM-L between the two setups is relatively small. Additionally, ASPRM-M performs better under the random setup than under the confidence setup, demonstrating its superior generalization ability in the rating position. We attribute this advantage to the diversity of rating point types in the ASPRM training data.\nASPRM exhibit in-domain generalization: We use GSM-Symbolic (Mirzadeh et al., 2024), which modified variables or sentences in the original GSM8k dataset, to test whether PRM can achieve in-domain generalization. We show our results in Table 4. We find that ASPRM exhibits strong in-domain generalization as it achieves better results in TVD than greedy search, and selects the right samples in Bo64.\nMixed data benefits downstream performance: Since both tasks are reasoning tasks, we explore whether mixing training data from different domains can enhance downstream performance. To this end, we conduct two experiments: 1) training Mistral on a mixed math and code dataset, and evaluating it on MATH500 and GSM8k; 2) training DeepSeek on an equal amount of randomly sampled math and code data, and evaluating it on LeetCodeDataset and LiveCodeBench. The results are shown in Table 6. We find that mixing data improves PRM performance on math datasets, while on code datasets, performance improves only in the TVD scenario on LiveCodeBench. We hypothesize this outcome is due to the following reason: for the math PRM, mixing long code domain training data improves the"}, {"title": "4.4. Feature Analysis", "content": "In this part, we discuss the features of the AdaptiveStep division used in ASPRM and its advantages.\nConstruction efficiency: The training data construction of ASPRM demonstrates superior efficiency in both domains. In the math domain, the training data for ASPRM is generated using only a single MetaMath-Mistral model, with 30 samples per data point and 8 times rollouts per step. In contrast, ER-PRM performs fewer samples but conducts 16 times rollouts, while Math-Shepherd uses multiple models for sampling and rollouts. The average number of steps per sample and sample counts for each method are presented in Appendix A.2. As a result, the data construction costs for ASPRM is less than 70% of that for the other two. In the code domain, there are 14.4 lines on average per answer for the LeetCodeDataset training set, whereas only 5.69 steps are required for our method on average.\nStatistical features of the division: There are several features and findings in the AdaptiveStep division statistics. For brevity, we refer to low-confidence tokens as \"decision tokens\" throughout this section. Taking the math PRM training data generated by Mistral as an example: 1) 3.85% tokens in mathematical expressions contribute 21.03% decision tokens; 2) only 2.7% decision tokens are newline tokens; 3) the inference model exhibits low confidence at semantic word points, particularly at Conjunction (29.00%), suggesting that continuous or transitional thinking is particularly challenging for the model.\nFor the code PRM training data: 1) the majority of decision points occur in the Code Comment type (80%), compared to the Code type (20%), even though Code Comments tokens account for only 19% of the total tokens; 2) a detailed analysis reveals that the Code Comment samples primarily fall into two subtypes: explaining what previous lines do and planning what to do in the following lines. The first subtype accounts for 9% of the samples, while the second accounts for 91%. This indicates that the inference model triggers more during the planning process than during the writing process when generating code; 3) by further analyzing the Code type, we find that Logical Operators, Block Begin Keyword, Control Statements and Loop Statements occupy a high proportion of low confidence proportion with a small number of tokens. This suggests that, in addition to pre-planning in Comment, the model still requires assistance at certain logical decision points during the writing process.\nThe statistical information indicates that the inference model is prone to performing low confidence in the calculation process, semantic word selection in mathematics reasoning tasks, and the planning process in code generation tasks."}, {"title": "5. Conclusion", "content": "In this paper, we propose a new reasoning step dividing method, AdaptiveStep, along with a corresponding Process Reward Model (PRM), ASPRM. We test the effectiveness of the PRM on mathematical reasoning and code generation tasks. To train the code PRM, we collect a function-level LeetCode dataset. We effectively integrate the PRM into the standard LLM inference process, achieving improvements over greedy search without additional inference overhead by token-level guidance. Our experiments on widely used datasets demonstrate robust performance with reduced computational costs. Furthermore, we analyze model transferability and generalization, showing that ASPRM exhibits both rating position, in-domain and cross-domain generalization. We also find that combining data from different domains further enhances PRM performance. Lastly, our feature analysis of the AdaptiveStep division confirms its effectiveness and informativeness."}, {"title": "6. Impact Statement", "content": "AdaptiveStep is an automatic, highly informative, and effective method for dividing reasoning steps. It can be easily applied to a wide range of complex tasks across various domains, such as code generation (as demonstrated in our paper) and AI-driven scientific reasoning. Furthermore, our exploration of the properties of AdaptiveStep PRM and its training data features will contribute to advancing process reward assignment in LLMs, potentially shaping the development of more general PRMs."}]}