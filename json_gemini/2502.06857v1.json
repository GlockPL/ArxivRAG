{"title": "Gemstones: A Model Suite for Multi-Faceted Scaling Laws", "authors": ["Sean McLeish", "John Kirchenbauer", "David Yu Miller", "Siddharth Singh", "Abhinav Bhatele", "Micah Goldblum", "Ashwinee Panda", "Tom Goldstein"], "abstract": "Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting.", "sections": [{"title": "1. Introduction", "content": "Existing works on scaling laws often restrict Transformer architectures to a small range of width-depth ratios (Porian et al., 2024), train on a small number of tokens, and fix training hyperparameters such as cooldown schedule across training runs (Hoffmann et al., 2022). These design choices, in turn, can dramatically influence the resulting scaling laws. If a scaling law is sensitive to such design choices, then it may only be useful for practitioners implementing similar setups to those that produced the scaling law. In practice, practitioners often take guidance from scaling laws that assume completely different design choices than their own implementation, often without understanding to degree to which these choices may impact optimal scaling.\nIn this work, we produce a vast array of model checkpoints"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Scaling Laws", "content": "Scaling laws address the trade-off between parameter count and number of training tokens, attempting to find the minimum loss possible for a language model with a constrained FLOP budget. Unfortunately, scaling laws treat model design and training as if it has a single dimension (parameter count). In reality, training is sensitive to many choices. Notably, Hoffmann et al. (2022) find significantly different fitted laws (Equation (1)) compared to Kaplan et al. (2020). Pearce & Song (2024) and Porian et al. (2024) attribute most of this discrepancy to the choice to exclude embedding parameters from the parameter count, both showing one law can be transformed into the other via controlled changes. Kaplan et al. (2020) justify including embedding parameters by showing that non-embedding parameters have a cleaner relationship with test loss. Scaling laws are also commonly included in many large model releases (Hu et al., 2024; Bi et al., 2024; Dubey et al., 2024).\nChoshen et al. (2024) collect both loss and benchmark performance metrics for a multitude of models and offer a practitioner's guide to fitting scaling laws. Most notably, they suggest 5 models are ample to fit a scaling law, and the early period of training should be excluded from the analysis. In contrast, Li et al. (2024b) show that selecting data according to different tokens-per-parameter ratios and using small grid searches when fitting scaling laws can cause big swings in outcomes. H\u00e4gele et al. (2024) suggest that a constant learning rate plus cooldown is preferable to a cosine learning rate schedule. The authors also find that stochastic weight averaging should be encouraged in scaling law analysis as it tends to lead to better models. Furthermore, Inbar & Sernau (2024) observe that FLOPs cannot be used to predict wall-clock time nor memory movement, and suggest that fast-training architectures may be preferred over those prescribed by scaling laws.\nThere are multiple works analyzing whether scaling laws can be used to predict downstream performance. Ruan et al. (2024) show that scaling laws can be predictive of benchmark performance. Caballero et al. (2023) suggest broken scaling laws that predict performance of both downstream and upstream tasks. Works in this vein are myriad; see our extended literature review in Appendix B."}, {"title": "2.2. The Role of Model Shape", "content": "Levine et al. (2020) find that, for large models, optimal depth grows logarithmically with width. Henighan et al. (2020) find there is an optimal aspect ratio for each modality they study which gives maximum performance, for example they find 5 to be optimal for math models. Petty et al. (2024) claim small (<400M) transformers have diminishing bene-"}, {"title": "2.3. Zero-shot Hyperparameter Transfer", "content": "The ability to train a series of models with extremely different parameter counts is an implicit requirement of any scaling law analysis. Work on zero-shot hyperparameter transfer across transformer model widths is mature (Yang et al., 2021; Everett et al., 2024; Hayou & Yang, 2023; Dey et al., 2024), but achieving transfer across diverse model depths is less well studied, especially in transformer language models (Bordelon et al., 2024)."}, {"title": "3. Designing Our Scaling Laws", "content": "We detail the design of our scaling laws, including model selection, the choice of learning rate, and curve fitting schemes"}, {"title": "Architecture.", "content": "To reduce the search space of all possible models we add some constraints, each of which are either based on precedent from a popular model series like Gemma (Team et al., 2024a;b), Llama (Touvron et al., 2023), Pythia (Biderman et al., 2023), or practical considerations such as hardware details.\nAll models have a head size of 128 because 256 is the maximum head dimension supported by the AMD implementation of Flash Attention 2 we utilize and we constrain our search to models with > 1 attention heads. We assume the simple convention of the Llama series where the head dimension is always the embedding dimension divided by the number of heads, implying that the embedding dimension (width) must be divisible by 128. Following conventions from the Gemma suite, we constrain the head count to be even to enable Grouped Query Attention (Ainslie et al., 2023) with a query to key ratio of 2 : 1 and we fix the intermediate size to be 4x the width of the model. We choose our vocabulary size to match the 50, 304 tokens in the Pythia tokenizer. While many of the architecture choices mirror those from Gemma, for simplicity we do not use logit softcapping nor do we tie the embedding and language modeling head weight matrices.\nWithin these constraints, we search the set of feasible models within target parameter count groups 50M, 100M, 500M, 1B and 2B with a tolerance of \u00b15%. At smaller scales we train up to 5 models at diverse widths and depths. At large parameter counts we train only three models, aiming for one \"standard\" aspect ratio (similar to existing models), one \u201cwide\u201d model, and one \u201cdeep\u201d model. We visualize the models we choose to train in Figure 2 overlaid with a selection of existing models from prior work. In the Appendix we plot the entire discrete set of all possible models under our constraints (Figure 22). Our 22 different models range from 50M to 2B parameters, spanning 11 widths from 256 to 3072 and 18 depths from 3 to 80."}, {"title": "Polishing the Gemstones.", "content": "For the main set of training runs, we train each model for 350B tokens of Dolma (Soldaini et al., 2024) data with a context length of 2048 and a world batch size of 2048 sequences. We use a linear learning rate warm up over 80 million tokens, and then train at a constant learning rate, which we adjust for model size as described in Section 3.1.\nIn service of future research based on our model suite, we open source checkpoints for all models at 2 billion token intervals, amounting to over 4, 000 checkpoints in total. We also open source the fitting code and logged metrics for all runs.\nWe also perform ablations over both cooldown and learning"}, {"title": "Training Details", "content": "We train with AdamW (Loshchilov & Hutter, 2017) with \u03b2 parameters 0.9 and 0.95 and a weight decay of 0.1. We do not apply weight decay to the bias or normalization parameters. All models are trained with tensor parallelism (Singh & Bhatele, 2022; Singh et al., 2024) over multiple nodes of AMD MI250X GPUs. To the best of our knowledge, this makes the Gemstone suite of models the largest collection trained on AMD GPUs."}, {"title": "3.1. Optimal Learning Rates for Gemstones", "content": "Training models across diverse architectures and scales requires learning rates that ensure both stability and near-optimal performance. Suboptimal learning rates risk misrepresenting scaling laws, as they could conflate architectural preferences with hyperparameter sensitivity. For the Gemstone models-varying in width, depth, and size-we address this challenge through a unified learning rate scal-"}, {"title": "Unified Learning Rate Scaling Rule", "content": "Existing scaling rules prescribe learning rates (lr) as lrbase/width for width scaling or lrbase/depth for depth scaling. Since Gem-stone models vary both dimensions, we propose a hybrid rule: lreff = lrbase/(width \u00d7 \u221adepth) This accounts for the compounding effect of gradient dynamics across width and depth, balancing update magnitudes during optimization."}, {"title": "Empirical Validation", "content": "To validate lrbase, we stress-test four extreme model shapes: wide (64 layers, 768 width) and deep (128 layers, 512 width) at 100M and 2B parameter scales. Each is trained for 2B tokens with lreff swept from 10-4 to 5 \u00d7 10-2. As shown in Figure 3 (left), optimal lreff varies widely across architectures. However, rescaling the x-axis by width \u00d7 \u221a depth collapses all curves onto a shared trend, revealing lrbase = 5 as the consistent optimum (right panel). This confirms our rule's efficacy for width-depth transfer."}, {"title": "Flaws in the Gemstones.", "content": "While lrbase = 5 achieves stable training for most models under the scheme described above, wider architectures (e.g., 256 width-depth ratio) occasionally exhibit loss spikes nonetheless. Despite these instabilities, via rollbacks and minor modifications to the learning rates for the most extreme models, all models in the suite are trained to 350B tokens without divergence. We discuss these issues and our solutions further in Appendix F.2."}, {"title": "Ablation Study", "content": "To assess sensitivity to lrbase, we replicate training for a subset of models with lrbase = 2.5 (e.g. dividing lreff by 2). While losses are marginally higher, scaling law fits remain robust, suggesting our conclusions are not artifacts of aggressive learning rates."}, {"title": "Scalable Parameter Initialization Rules.", "content": "Finally, stable training across model shapes and scales also requires model specific tweaks to parameter initialization (Yang et al., 2021). Following OLMO(1) (Groeneveld et al., 2024), we apply a parameter initialization strategy intended to enable stable training and learning rate transfer across scales. We initialize all parameters as truncated normal (\u03bc = 0, \u03b1 = \u22123\u00b7\u03c3,b = 3\u00b7 \u03c3) with modified variances dependent on the parameter type. We use \u03c3 = 1/\u221awidth except for the attention projections which are initialized as \u03c3 = 1/2 \u00b7 width \u00b7 (l + 1) and the MLP projections as \u03c3 = 1/\u221a2 \u00b7 (4 \u00d7 width) \u00b7 (l + 1) where in each case l is the layer index (not the total model depth) and the 4x factor comes from the relation of width to MLP intermediate dimension."}, {"title": "3.2. Fitting Scaling Laws", "content": "We fit scaling laws using methods similar to approach 1 and 3 from Chinchilla (Hoffmann et al., 2022). We fit all laws using the log perplexity of all trained models on a sample of 100 million tokens from a fixed, held-out validation set from the training distribution. We also collect log perplexity values for a range of open source models (Team et al., 2024a; b; Touvron et al., 2023; Dubey et al., 2024; Yang et al., 2024a;b) on the same validation data to allow for a comparison between our predictions and a selection of widely used models. We design a specialized FLOP counting function as we find that simple rules of thumb (e.g., FLOPs= 6 \u00d7 parameters (Hoffmann et al., 2022)) do not accurately account for differences in FLOPs between extremely wide and narrow architectures. We discuss this further and present our function in Appendix G.\nFollowing prior work, we plot the Epoch AI Replication (Besiroglu et al., 2024) of Chinchilla (Hoffmann et al., 2022) on all plots and use the coefficients for Kaplan plotted by Porian et al. (2024) which were extracted from the original paper (Kaplan et al., 2020)."}, {"title": "A More Robust Approach to Fitting Compute-Optimal Laws.", "content": "The first approach in Hoffmann et al. (2022) fits a scaling law by plotting the loss against FLOPs for a range of architectures, and then fitting a line to the pareto-optimal architecture for each FLOP count (see Figure 4). Following Hoffmann et al. (2022), we refer to this as \"Approach 1\". As we use a constant learning rate, we can use all recorded validation losses to fit our law. Hoffmann et al. (2022) and Kaplan et al. (2020) select model shapes so densely that they have a near-optimal architecture at each FLOP count. This works when all architectures lie in a 1D space (parameterized by parameter count), as each model is optimal in some FLOP regime, and the lower envelope is densely populated. In our two dimensional exploration (varying width and depth), some models are never optimal, and the ones that"}, {"title": "Our New Method: The Convex Hull.", "content": "We fit a lower convex hull to our loss curves. This hull is only supported by a sparse set of optimal models. This naturally excludes sub-optimal models that lie above the convex hull of optimality, and as we will show, this makes the resulting scaling law far more robust to model selection choices."}, {"title": "Why We Skip Approach 2.", "content": "Another method to fit scaling laws is to put model runs into isoFLOP bins and choose the best parameter count in each bin. Hoffmann et al. (2022) call this \"Approach 2\". Our 2-dimensional set of models do not finely cluster into isoFLOP bins, meaning our data is not easily amenable to Approach 2, hence we exclude this approach from our analysis. Hu et al. (2024) also eschew this approach."}, {"title": "Prescribing Optimal Widths and Depths by Fitting Power Laws.", "content": "The final approach described by Hoffmann et al. (2022) is to fit a parametric formula to the loss values with the ansatz\n\nL(p, T) = \\frac{A}{p^\\alpha} + \\frac{B}{T^\\beta} + \\epsilon\n\nwhere p is parameter count and T is tokens. We fit our models using L-BGFS (Liu & Nocedal, 1989) with a Huber loss (\u03b4 = 10-4) between the empirical log loss and the model prediction, and use multiple initializations following"}, {"title": "4. Experiments", "content": "In Section 4.1 we use our new convex hull fitting method to make a scaling law for the compute-optimal tokens-to-parameters ratio, and our new power law approach to provide a prescription for the compute-optimal width-to-depth ratio. We show how many seemingly innocuous design choices such as the learning rate schedule can significantly"}, {"title": "4.1. Sizing Up Our Scaling Laws Against Prior Laws and Industry Models", "content": "Approach 1. In Figure 4 (row one), we see our validation losses plotted as both a function of FLOPs (left) and GPU hours (right) for the first 100 billion tokens of training. We calculate GPU hours from the average recorded optimizer step time for each model.\nOur convex hull fits the data better than prior ap-proaches. Hoffmann et al. (2022)'s Approach 1 creates 250 logarithmically-spaced FLOPs bins per order of magnitude and then uses the models that achieve the best loss in each FLOPs bin to fit the scaling law (a line). However, for our data, their approach does not work very well because it in-cludes many points that are strictly suboptimal with respect to the minimal loss envelope. Our convex hull method omits these points, and fits the line with far fewer points.\nIn Figure 4 (row two), we see the prescription of the fitted laws for tokens per parameter. We see that the tokens per pa-rameter prescription of our Approach 1 fitting is also close to constant, like Hoffmann et al. (2022), but slightly higher, suggesting more tokens should be used per parameter in the model. We extend this plot showing predicted total param-eters, tokens, and over multiple ablations in Appendix C. We give a more detailed plot of each model's individual validation loss in Appendix F.\nApproach 3. This uses a parametric function to predict the loss when given parameter count and number of tokens. As our data is intentionally designed to cover a wide variety of"}, {"title": "4.2. A Rainbow of Scaling Laws", "content": "To demonstrate the sensitivity of scaling laws to design choices, we fit laws with various assumptions and model selection rules. We begin by fitting laws of the classical form (without width-depth), and use equation 4 from Hoffmann et al. (2022) to provide compute-optimal parameter count prescriptions. In Figure 6 we show the optimal predictions"}, {"title": "4.3. The Price of Stepping Off the Scaling Law", "content": "In the previous sections we provide prescriptions for the optimal width, depth, parameters, and tokens as a function of total training FLOPs. As we have seen, industry models don't always comply with our prescriptions. At the same time, the variability in our analyses suggests that scaling laws are inherently fuzzy, providing only rough estimates of optimal settings. With this in mind, we ask a natural question: if one happens to choose a sub-optimal point in design space, how bad can it really be?\nBy analyzing the cost of stepping off of the scaling law, we find that some kinds of design errors are more damaging than others. In particular, one pays a steep price for training models that are too narrow, and it is better to err on the side of too wide. We also find that training on more tokens than is strictly recommended (aka \"overtraining\") is typically"}, {"title": "If You Value Your Time, Train Wide Models.", "content": "We first show that in our training setup, training wider models is far more efficient than training deep models. In Figure 7, we reflect on the consequences of suboptimal architectural choices, by considering how much of a given resource\u2014FLOPs or GPU hours-would be \"overspent\" to reach any target loss value with the plotted architecture rather than the prescribed width and depth. We find that choosing to train \"skinny\" models (top left) wastes many FLOPs and GPU hours. The scale of overspend is quite different however, with the least efficient models only overspending about 50% on FLOPs but wasting more than 200% of the GPU hours spent by the best configuration. In other words, in the time taken to train a single (very) suboptimal model to the de-sired loss value, one could train three optimal-width-depth models. We note that while the time-optimal models tend to be the wider ones, this is probably due to our training scheme. Similar to other open-source efforts such as OLMo et al. (2024), we do not make any use of pipeline parallelism, and only employ tensor parallelism (using a hybrid data and tensor parallel algorithm similar to the ubiquitous Fully Sharded Data Parallel strategy). In summary, for stan-dard parallelism implementations, wider models are simply easier to scale, but as a result our observations regarding re-source overspending may not generalize to other parallelism strategies."}, {"title": "Scaling Laws Predict That Overtraining Is Efficient.", "content": "In Figure 5, we see the optimal predictions from our law of the form shown in Equation (2). We can shift those optimal points to simulate overtraining. To do this, we fix a FLOP budget and trace out a path of model sizes and correspond-ing token counts to remain within that budget. For each"}, {"title": "5. Limitations and Conclusions", "content": "We hope this work encourages a rich range of future work on the impact of width and depth within modern transformer architectures using the large amount of open source artifacts we produce. Future work should also extend to other hyper-parameters involved in training transformer architectures, such as the expansion factor. Although we endeavor to make our laws as generalizable as possible, we still expect that their applicability declines in training set-ups very different from our own."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Software and Data", "content": "We train all models using a fork of litgpt (AI, 2023) enhanced with AxoNN (Singh & Bhatele, 2022; Singh et al., 2024) tensor parallelism. We open source all models used in our analysis to Hugging Face (Wolf et al., 2020) and the logging from training on Weights and Biases in json format. All scaling-law fitting code is released on GitHub, with training code to be released shortly after publication."}, {"title": "B. Extended Related Works", "content": "Scaling laws are broadly applied to many areas of machine learning, such as machine translation. Ghorbani et al. (2021) split the parameters term into two, one for each of encoder and decoder, and similarly to Gordon et al. (2021) analyze the relationship between BLEU scores and scaling laws. Zhang et al. (2022) and Bansal et al. (2022) study the impact of architecture choice on the scaling law, finding increasing data or parameters can compensate for worse architectural decisions.\nScaling laws have also been applied to sparse architectures. Clark et al. (2022) analyze how the number of experts can be used in the law, studying both linear and quadratic interactions for many types of routing models. Yun et al. (2024) extend this, analyzing the trade offs between optimal training and optimal inference. Krajewski et al. (2024) find that with optimal settings a Mixture of Experts model always outperforms a transformer model at any computational budget. Frantar et al. (2023) focus on weight sparsity within foundation models, adding a multiplicative parameter on the parameters term in the law.\nThese techniques are not limited to generative text modeling only; they have also been applied to multi-model models. Henighan et al. (2020) find optimal model size can be described as a power law for model modeling including images and video. The authors also find that model size does not help \u2018strong generalization' for problem solving. Aghajanyan et al. (2023) analyze text, images, code and speech, presenting a scaling law to describe the competition between these modalities and describe a regime for optimal hyperparameter transfer from the unimodal to multimodal regimes. Liang et al. (2024) look at scaling laws for diffusion transformer models. Li et al. (2024a) analyze scaling laws for vision encoder commonly used to encode image inputs for transformer model backbones, finding increasing the size of the encoder alone can lead to performance degradation in some cases.\nFurther analyses using scaling laws have extended to analyzing finetuning and data limited scaling. Hernandez et al. (2021) find that finetuning is much more compute efficient when the pretraining ignored. Zhang et al. (2024) study parameter efficient finetuning regimes find a multiplicative law is better for the finetuning setting than the classical additive law used by others. Muennighoff et al. (2023) analyze the data constrained training regimes, finding epoching data up to four times is as good as training on deduplicated data in terms of reducing loss."}, {"title": "C. Ablations for Approach 1", "content": ""}, {"title": "C.1. Extended Paper Figures", "content": "In Figure 9, we plot an extended version of the Approach 1 plot we present in Figure 4."}, {"title": "C.2. Alternative Learning Rates", "content": "In Figure 10, we present the Approach 1 prescription when fitting on the learning rate ablation data."}, {"title": "C.3. Cooldown", "content": "In Figure 11, we present the Approach 1 prescription when fitting on the cooldown ablation data."}, {"title": "D. Ablations for Approach 3", "content": ""}, {"title": "D.1. Extended Paper Figures", "content": "In Figure 12, we plot an extended version of the Approach 3 plot we present in Figure 5."}, {"title": "D.2. Alternative Learning Rates", "content": "In Figure 13, fit Approach 3 laws to the dataset we trained with half of the optimal learning rate as described in Section 3.1. We again use a brute force approach as above to plot the results but to allow for precise comparison with later ablations we ignore the integer constraints on the number of heads, still enforcing that at least one head number be in every model. We remove this constraint for all models shown in Appendix D.2, Appendix D.3, Appendix D.4 and Appendix D.5."}, {"title": "D.3. Cooldown", "content": "In Figure 14, we fit Approach 3 laws to the subset of data for models for which we linearly decreased the learning rate to zero for."}, {"title": "D.4. Removing Smaller Token Counts", "content": "In Figure 15, we present the Approach 3 prescription when fitting on a dataset where all token counts less than 120 billion are removed."}, {"title": "D.5. Varying Delta in the Huber loss", "content": "So far we have fit all approach three laws with a Huber loss delta of 10-4. We now ablate this decision by refitting all laws with a delta of 10\u20133. We use an extremely large grid search of over 4 million initializations for the width-depth based law when fitting.\nTo begin we show the prescriptions of the Approach 3 laws if the integer constraints are removed, as we did for the learning rate and cooldown ablations in Figures 13 and 14 respectively.\nWe now compare all Approach 3 laws found with the increased delta. Specifically, we plot the full dataset laws with delta of 10-4 in Figure 16 and with 10-3 in Figure 17. We plot the learning rate ablation laws with delta of 10\u20134 in Figure 13 and with 10-3 in Figure 18. We plot the cooldown ablation laws with delta of 10\u20134 in Figure 14 and with 10-3 in Figure 19. In these figures, we see the difference for the full dataset, cooldown and learning rate ablations laws is minimal when changing the delta. We conclude with a cautionary figure about size of the grid search and the delta used in the Huber loss. In Figure 20, where we plot the exponents found by optimizing the Huber loss versus the size of the grid search used for optimization. We see that a delta of 10-5 is unstable for smaller grid sizes and including more tokens in the fitting data generally increases stability of the exponents found during optimization."}, {"title": "D.6. Alternative Law Forms", "content": "We also experiment with laws of the form shown in Equation (3). In Figure 21, we see that the prescriptions of this law are approximately in line with those of Kaplan et al. (2020). Unlike the laws for shown in Equation (2), these laws tend to prescribe that the width-depth ratio should go to zero as the FLOPs budget increases, i.e. prescribing an infinite-depth model in the infinite budget limit.\n\nL(w, d, p, T) = \\frac{A}{w^\\alpha} + \\frac{B}{d^\\beta} + \\frac{C}{T^\\gamma} + \\epsilon"}, {"title": "E. Data Sampling", "content": "We plot the entire space of all possible models subject to our design constraints discussed in Figure 22. While exploring the impact of finer grained depth differences during our experiments, we decided to add two additional models slightly outside the \u00b15% tolerance band at the 100M scale; for width = 512, in addition to the originally chosen depths of 12 and 13, we added 11 and 14; these appear as a dense collection of 4 points at the same width."}, {"title": "F. Training", "content": "Despite our best efforts to sufficiently mix the training data, we still see slight jumps in the global training loss when the training switches between chunks of data, hence we use validation loss to fit all laws as this is smooth."}, {"title": "F.1. Loss Curves", "content": ""}, {"title": "F.2. Additional Training Complications", "content": "Any gemstone naturally contains a small number of inclusions or fractures. We discuss a few of the minor imperfections in our model collection below.\nDealing with Training Instabilities After some of the widest models were trained beyond 50B tokens we began to observe unrecoverable loss spikes that were proceeded by small wobbles in the loss trajectory. Under the general intuition that the culprit was most likely that the width/depth ratios considered were simply too extreme for existing initialization and learning rate scaling approaches to handle, we reran some of the models with a \u201cpatch\" in place.\nWe modified the initialization rules and learning rate scaling factors to rescale the depth and layer indices of the model such that if width/depth > 256 scale variances and learning rates as if the depth of the model was actually depth'= \\frac{depth}{[(width/100)]}. The overall effect of the patch is to initialize and scale learning rates more conservatively, as if the aspect ratio were only 100 while keeping the original width of the model. We found this allowed us to complete training for a full set of 22 models out to 350B tokens for even our most extreme models.\nHowever, after 350B tokens, despite these efforts we observed that most extreme models which were patched still diverged anyway. While a partial cause of this could be the constant learning rate scheduler employed during training, concurrent work, from the authors of the original OLMo paper and codebase (Groeneveld et al., 2024) from which we derived some of our choices, reported that the initialization scheme dubbed the \u201cMitchell-init\u201d is indeed systematically prone to instabilities later on in training (OLMo et al., 2024). While an unfortunate finding, we were unable to rerun all of our experiments due to the consumption of significant non-fungible compute resources in the original experiments.\nModels Lacking Ablations Our cooldown ablation is from initial experiments below 100B tokens of training which do not use the patched learning rates scaling rules. This means there are minor discrepancies between the cooldown ablation and main set of training runs for the widest models from the three largest parameter count groups (1792 \u00d7 7, 2560 \u00d7 8, 3072 \u00d7 12). We also do not cool down the 100B token checkpoint for the 3072 \u00d7 12 model as it was experiencing a loss spike at that final point. Finally, we do not include ablations for the two width 512 models which do not fall into the \u00b15%"}, {"title": "G. FLOP counting matters", "content": "In Figure 24 we show that the common approximation of FLOPs per token= 6 \u00d7 parameters, miscounts the true FLOPS by a significant amount."}]}