{"title": "Hyper-Heuristics Can Profit From Global Variation Operators", "authors": ["Benjamin Doerr", "Johannes F. Lutzeyer"], "abstract": "In recent work, Lissovoi, Oliveto, and Warwicker (Artificial Intelligence (2023)) proved that the Move Acceptance Hyper-Heuristic (MAHH) leaves the local optimum of the multimodal CLIFF benchmark with remarkable efficiency. The $O(n^3)$ runtime of the MAHH, for almost all cliff widths $d > 2$, is significantly better than the \u0472($n^d$) runtime of simple elitist evolutionary algorithms (EAs) on CLIFF.\nIn this work, we first show that this advantage is specific to the CLIFF problem and does not extend to the JUMP benchmark, the most prominent multi-modal benchmark in the theory of randomized search heuristics. We prove that for any choice of the MAHH selection parameter $p$, the expected runtime of the MAHH on a JUMP function with gap size $m = O(n^{1/2})$ is at least \u03a9($\\frac{n^{2m-1}}{(2m - 1)!}$). This is significantly slower than the $O(n^m)$ runtime of simple elitist EAs.\nEncouragingly, we also show that replacing the local one-bit mutation operator in the MAHH with the global bit-wise mutation operator, commonly used in EAs, yields a runtime of min{1, $O(\\frac{e \\ln(n)}{n})^m \\frac{1}{m}$} O($n^m$) on JUMP functions. This is at least as good as the runtime of simple elitist EAs. For larger values of $m$, this result proves an asymptotic performance gain over simple EAs. As our proofs", "sections": [{"title": "1 Introduction", "content": "Evolutionary algorithms (EAs) have undergone intense mathematical analysis over the last 30 years [NW10, AD11, Jan13, ZYQ19, DN20]. Contrariwise, the rigorous analysis of hyper-heuristics is far less developed and has, so far, mostly focused on unimodal problems, which has led to several highly interesting results (see Section 2 for further detail).\nIn the first, and so far only, work on how hyper-heuristics solve multi-modal problems, that is, how they cope with the presence of true local optima, Lissovoi, Oliveto, and Warwicker have proven the remarkable result that the simple Move Acceptance Hyper-Heuristic (MAHH), which randomly mixes elitist selection with accepting any new solution, optimizes all CLIFF functions in time O($n^3$) (or less depending on the cliff width d). The MAHH thus struggles much less with the local optimum of this benchmark than several existing approaches, including simple elitist EAs (typically having a runtime of (nd) [PHST17]), the (1, \u03bb) EA having a runtime of roughly O($n^{3.98}$) (shown for $d = n/3$ only, which required a very careful choice of the population size) [HS21], and the Metropolis algorithm (with runtime \u0398($n^{d-0.5}/(log n)^{d-1.5}$) for constant d and a super-polynomial runtime for super-constant d).\nThe surprisingly good performance of the MAHH on CLIFF functions raises the question: Does the convincing performance of the MAHH on CLIFF generalize to other functions or is it restricted to the CLIFF benchmark only? To answer this question, we study the performance of the MAHH on the multimodal benchmark most prominent in the mathematical runtime analysis of randomized search heuristics, the JUMP benchmark. For this problem, with jump size $m \u2265 2$, only the loose bounds $O(n^{2mm^{-e(m)}})$ and \u03a9($2^{\\widetilde{\u0398(m)}}$) were shown in [LOW23]. These bounds allow no conclusive comparison with simple EAs, which typically have a \u0472($n^m$) runtime on JUMP functions.\nIn this work, we prove a general non-asymptotic lower bound for the runtime of the MAHH on JUMP functions, valid for all values of the problem size n, jump size $m \u2265 2$, and mixing parameter p of the hyper-heuristic."}, {"title": "2 Related Work", "content": "Since we perform a mathematical runtime analysis in this paper, we concentrate our review of related work on this domain. We refer to [BGH+13] for a general introduction to hyper-heuristics and to [NW10, AD11, Jan13, ZYQ19, DN20] for introductions to mathematical runtime analyses of randomized search heuristics."}, {"title": "2.1 Hyper-Heuristics", "content": "The mathematical runtime analysis of hyper-heuristics is a recent field that was started by Lehre and \u00d6czan [L\u00d613] by working on random mixing of mutation operators and random mixing of acceptance operators. Their work has led to numerous follow-up works, among which we now describe the most important ones. We refer to the survey [DD20, Section 8] for more details.\nAll mathematical runtime analyses of hyper-heuristics work with selection heuristics, that is, hyper-heuristics that try to choose wisely among a fixed set of given operators. The majority of these works consider the selection of mutation operators. The simplest selection hyper-heuristic is random mixing between operators, that is, selecting each time independently an operator from a given probability distribution. Random mixing of the 1-bit-flip and 2-bit-flip mutation operator was studied in [L\u00d613]. Their runtime analysis on ONEMAX (see [DD20, Theorem 6.8.1] for a corrected version of this result) indicated no advantages of mixing these two operators, a natural result for this unimodal benchmark. To show that mixing of mutation operators can be essential, the GAPPATH problem was constructed in [L\u00d613]. Neumann and Wegener [NW07] showed that the minimum spanning tree problem is such an example as well, without calling the studied algorithm a hyper-heuristic.\nWe note that the result of randomly mixing mutation operators can simply be regarded as a new mutation operator. For example, bit-wise mutation can be seen as a suitable mixing of k-bit-flip operators (k = 0,1,...,n) or the fast mutation operator of [DLMN17] can be seen as a mixing of bit-wise operators with different mutation rates. Therefore, more complex ways of selecting between different operators appear to be more interesting. In terms of runtime analyses, this was first done in [AL14], again to select between different operators, however, no superiority over random"}, {"title": "3 Preliminaries", "content": "We now formally define the MAHH algorithm, our considered standard benchmark functions, as well as some mathematical tools used in our runtime analysis. Throughout, we use the notation [1..n] to denote the set of integers {1,2,..., n}."}, {"title": "3.1 Algorithms", "content": "We will analyze the runtime of the MAHH algorithm applied to the problem of reaching the optimum of a benchmark function defined on the space of n-dimensional bit vectors.\nIn each iteration of the algorithm, one bit of the current vector x is chosen at random and flipped to create a mutation x'. This mutation is accepted with probability p (ALLMOVES operator), and is accepted only if the value of the benchmark function increases, with probability 1 \u2013 p (ONLYIMPROVING"}, {"title": "3.2 Benchmark Function Classes", "content": "We will now define the various benchmark functions that will be used to analyze the performances of the considered algorithms.\nFor all $x \u2208 {0,1}^n$, the ONEMAX benchmark is defined by\n$ONEMAX(x) = ||x||_1 = \\sum_{i=1}^n xj$.\nThe ONEMAX function has a constant slope leading to a global optimum placed at the all-ones bit-string $x^* = (1, . . ., 1)$. It is often used to evaluate the hillclimbing performance of randomized search heuristics. It is also the basis for the definition of the CLIFF and JUMP benchmarks.\nWhile we will not study the CLIFF benchmark in detail, for completeness we briefly define it now. The CLIFF benchmark was first proposed in [JS07]. For $d\u2208 [1..n]$, the function CLIFFd is defined by\n$CLIFF_d(x) = \\begin{cases} ONEMAX(x), & if ||x||_1 \u2264 n - d; \\ ONEMAX(x) \u2013 d+\\frac{5}{2}, & otherwise, \\end{cases}$\nfor all $x \u2208 {0,1}^n$. CLIFF functions consist of two ONEMAX-type slopes pointing towards the global optimum. The second one of these is constructed such that all its solutions except the global optimum have an objective value inferior to the best objective value in the first slope. Consequently, search heuristics that may accept inferior solutions may leave the local optimum by moving to an inferior solution on the second slope and improving this to the global optimum.\nIn this paper, we will present a runtime analysis of the MAHH hyper-heuristic on the JUMP benchmark. For $m \u2208 [1..n]$, the function JUMPm is defined for all $x \u2208 {0,1}^n$ via\n$JUMP_m(x) = \\begin{cases} m + ONEMAX(x), & if ||x||_1 \u2264 n -m or ||x||_1 = n; \\ ONEMAX(x), & otherwise. \\end{cases}$\nClearly, the main difference between CLIFF and JUMP functions is the shape of the valley of lower objective values surrounding the global optimum. For"}, {"title": "3.3 Mathematical Tools", "content": "We now introduce several of the fundamental mathematical tools, that we use to prove our upper bounds in Sections 5 and 6. In particular, we will be recalling the multiplicative and additive drift theorems as well as a simplified version of Wald's equation.\nWe begin with the multiplicative drift theorem, which provides upper bounds on the expected runtime when the progress can be bounded from below by an expression proportional to the distance from the target (which is set to zero in the formulation of Theorem 1).\nTheorem 1 (Multiplicative Drift Theorem [DJW12]). Let S \u2286 R be a finite set of positive numbers with minimum $S_{min}$. Let $(X_t)_{t\u22650}$ be a sequence of random variables over SU{0}. Let T be the random variable that denotes the first point in time t \u2208 N for which $X_t = 0$. Suppose further that there exists a constant \u03b4 > 0 such that\n$E[X_t - X_{t+1} | X_t = s] > \u03b4s$\nholds for all $s \u2208 S$ with Pr[$X_1 = s$] > 0. Then, for all $s_0 \u2208 S$ with Pr[$X_0 = S_0$] > 0,\n$E[T | X_0 = S_0] \u2264 \\frac{1 + \\log(s_0/S_{min})}{\u03b4}$\nWe continue with the additive drift theorem of He and Yao [HY01] (see also the recent survey [Len20]), which allows to translate an expected progress (or bounds on it) into bounds for expected hitting times.\nTheorem 2 (Additive Drift Theorem [HY01]). Let S \u2286 R\u22650 be finite and 0 \u2208 S. Let $(X_t)_{t>o}$ be a sequence of random variables over S. Let d > 0. Let T = inf{t > 0 | Xt = 0}. If for all t > 0 and all $s \u2208 S \\ {0}$ we have\n$E[X_t - X_{t+1} | X_t = s] \u2265 \u03b4,$\nthen\n$E[T] < \\frac{E[X_0]}{\u03b4}$"}, {"title": "4 Lower Bounds on the Runtime of the MAHH with One-Bit Mutation", "content": "We now begin with the mathematical runtime analysis of the MAHH. We first calculate an exact formula for the expected runtime of the MAHH on the JUMP benchmark when starting at a state with (n-1) one-bits in Theorem 4. From this, we easily obtain our lower bound for the runtime of the MAHH algorithm on JUMP functions with valley width $m = O(\\sqrt{n})$ in Corollary 6.\nTheorem 4 (Expected Duration of the Last Improvement). We consider the MAHH algorithm on JUMPm. Let T-1 be the time for the MAHH algorithm to reach the state with n one-bits, given a state with (n - 1) one-bits. For any $p > 0$, $n \u2208 N$, and $m \u2264 n$, we have\n$E[T_{-1}] = p^{-2m+1} \\sum_{k=0}^{n-m-1} p^k {n \\choose k} + p \\sum_{k=n-m}^{n-1} {n \\choose k} p^k.$\nFor our proof of this result, we recall a result of Droste, Jansen and Wegener [DJW00].\nLemma 5 ([DJW00, Corollary 5]). Consider a Markov process $(X_t)_{t\u22650}$ with state space [0..n]. Assume that for all t > 0, we have Pr[|$X_t - X_{t+1}$| \u2264 1] = 1. Let $p_i^+$ and $p_i^\u2212$ denote the transition probabilities to reach states i \u2013 1 and i + 1 from state i, respectively. Let $T_i^+$ denote the time to reach state i + 1 when starting in state i. Then\n$E[T_i^+] = \\sum_{k=0}^{i} \\prod_{l=k+1}^{i} \\frac{p_l^-}{p_l^+}$"}, {"title": "5 Upper Bound on the Runtime of the MAHH with Global Mutation", "content": "In Section 4 we showed that the MAHH has a highly noncompetitive runtime on the JUMP benchmark, i.e., a runtime of ($\\frac{n^{2m-1}}{(2m-1)!}$) for gap size"}, {"title": "6 Upper Bound on the Runtime of the MAHH with One-Bit Mutation", "content": "We will now derive an upper bound on the runtime of the MAHH on JUMPm in the case $p = \\frac{1}{m}$. It will, in particular, show that our lower bound in Section 4 is tight apart from factors depending on m only.\nFollowing the outline of the proof for the more complex bit-wise mutation operator in Section 5, the proof in this section is comparatively easy to establish. We begin by stating the main result of this section.\nTheorem 13. Let $m\u2208 [2..\\frac{n}{2}]$ and $p = \\frac{1}{m}$. Then the runtime T of the MAHH on JUMPm satisfies\n$E[T] = O(\\frac{n^{2m-1}}{m!m^{m-2}})$\nThe main proof idea is the same as for Theorem 7 in Section 5, i.e., in order to prove Theorem 13, we split the expected runtime T in two times,\nT = T1+T2,\nwhere T\u2081 is the time to reach a local or the global maximum from an arbitrary initial solution (hence also a random initial solution) and T2 is the additional time to reach the global optimum."}, {"title": "7 Conclusion", "content": "In this work, we conducted a precise runtime analysis of the MAHH on the JUMP benchmark, the most prominent multimodal benchmark in the theory"}]}