{"title": "Fully Autonomous AI Agents Should Not be Developed", "authors": ["Margaret Mitchell", "Avijit Ghosh", "Alexandra Sasha Luccioni", "Giada Pistilli"], "abstract": "This paper argues that fully autonomous AI agents should not be developed. In support of this position, we build from prior scientific literature and current product marketing to delineate different AI agent levels and detail the ethical values at play in each, documenting trade-offs in potential benefits and risks. Our analysis reveals that risks to people increase with the autonomy of a system: The more control a user cedes to an AI agent, the more risks to people arise. Particularly concerning are safety risks, which affect human life and impact further values.", "sections": [{"title": "1. Introduction", "content": "The sudden, rapid advancement of Large Language Model (LLM) capabilities-from writing fluent sentences to achieving increasingly high accuracy on benchmark datasets-has led AI developers and businesses alike to look towards what comes next. The tail end of 2024 saw \"AI agents\", autonomous goal-directed systems, begin to be marketed and deployed as the next big advancement in AI technology.\nMany recent Al agents are constructed by integrating LLMs into larger, multi-functional systems, capable of carrying out a variety of tasks. A foundational premise of this emerging paradigm is that computer programs need not be restricted to functioning as human-controlled tools designed for specific tasks; rather, systems now have the capacity to autonomously combine and execute multiple tasks without human intervention. This transition marks a fundamental shift towards systems capable of creating context-specific plans in non-deterministic environments. Many modern AI agents do not merely perform pre-defined actions, but are designed to analyze novel situations and take previously undefined actions to achieve goals.\nTo better understand the potential benefits and risks in current AI agent development, we review recent AI agent products\u00b9 alongside research on AI agents to document different potential benefits and risks aligned to human values. Our analysis reveals that risks to people increase with a system's level of autonomy: the more control a user cedes, the more risks arise from the system. As others (Chan et al., 2023) have previously noted, there is an urgent need to anticipate and address risks of increasing agency, and we do this via a value-based characterization. Particularly concerning are risks related to the value of safety for individuals (Section 5.2.10), which include loss of human life and open the door for privacy risks (Section 5.2.8) and security risks (Section 5.2.11). Compounding the issue is misplaced trust (Section 5.2.13) in unsafe systems, which enables a snowball effect of yet further harms. For example, the safety issue of \"hijacking\", wherein an agent is instructed by a malicious third party to exfiltrate confidential information, can create further harms as that information is used to compromise the user's public reputation or financial stability and to identify additional people as targets of attack (U.S. AI Safety Institute, 2025).\nGiven these risks, we argue that developing fully autonomous AI agents-systems capable of writing and executing their own code beyond predefined constraints-should be avoided. Complete freedom for code creation and execution enables the potential to override human control, realizing some of the worst harms described in Section 5. In contrast, semi-autonomous systems, which retain some level of human control, offer a more favorable risk-benefit profile, depending on the degree of autonomy, the complexity of tasks assigned, and the nature of human involvement."}, {"title": "2. Background", "content": ""}, {"title": "2.1. A Brief History of Artificial Agents", "content": "The idea of humans being assisted by artificial autonomous systems can be found throughout human history. Ancient mythology describes Cadmus (ca. 2000 BCE), who sowed dragon teeth that turned into soldiers. Aristotle speculated that automata could replace human slavery: \"There is only one condition in which we can imagine managers not needing subordinates, and masters not needing slaves. This condition would be that each instrument could do its own work, at the word of command or by intelligent anticipation\u201d (Aristotle, 1999). An early precursor to artificial agents was created by Ktesibios of Alexandria (ca. 250 BCE), who"}, {"title": "2.2. Current Landscape of Agentic Systems", "content": "In the 2020s, work on AI agents broadened the range of functionality that computer systems could provide while requiring less input from users. Newly available systems can now complete tasks previously requiring human interaction with multiple different people and programs, e.g., organizing meetings\u00b2 or creating personalized social media posts.\u00b3\nIn the physical world, autonomous system development has made significant advances in multiple domains. Autonomous vehicles represent one of the more visible applications, with systems capable of perceiving5 their environment and navigating without human input (Van Brummelen et al., 2018). These range from consumer vehicles with varying levels of autonomy to fully autonomous systems tested in controlled environments (Ballingall et al., 2020). The development of autonomous robots has similarly expanded, from industrial manu-"}, {"title": "3. Definitions", "content": ""}, {"title": "3.1. On AI Agents", "content": "Analyzing potential benefits and risks of Al agents requires understanding what an Al agent is, yet definitions and descriptions vary greatly. Within AI, the term \u201cagent\u201d is currently used for everything from single-step prompt-and-response systems to multi-step customer support systems.\u2079\nTo better understand what an AI agent is, we therefore review currently available AI agents and AI agent platforms (examples provided in footnotes throughout this document) as well as historical literature on the promise of AI agents (references throughout), and note the different functionalities described (Table 2). Definitions across these sources differ in terms of who and what is centered (the person, the computer system, or the workflow), the specificity and clarity of the language used, and the types of systems that the definition pinpoints (e.g., whether it distinguishes autonomous systems from automatic systems). Towards the goal of harmonizing these different perspectives for this research, we propose the following definition of \"AI agent\":\nComputer software systems capable of creating context-specific plans in non-deterministic environments.\nAlthough there is not full consensus on what an \u201cAI agent\u201d is, a commonality across recently introduced AI agents is that they act with some level of autonomy: given a goal, they can decompose it into subtasks and execute each one of them without direct human intervention. For example, an ideal Al agent could respond to a high-level request such"}, {"title": "3.2. On Agency", "content": "The concept of \"agency\" is central to debates about autonomous Al systems, yet its meaning and implications remain philosophically contested. In general terms, an agent is understood as an entity with capacity to act (Anscombe, 1957; Davidson, 1963). Applying this concept to artificial systems raises questions about the nature of those acts' intentionality: the philosophical literature commonly understands agency through the lens of intentional action, where actions are explained in terms of the agent's mental states (e.g., beliefs, desires) and their capacity to act for reasons (Davidson, 1963; Goldman, 1970), but artificially intelligent agents are not known to have mental states as historically discussed. This suggests that AI agents lack the fundamental characteristics of genuine agency (Frankfurt, 1971; Bratman, 1987; Velleman, 1992). As such, philosophical foundations supporting the development of \"agency\" in AI agents-and indeed, whether Al agents may be said to have"}, {"title": "4. AI Agent Levels", "content": "AI agents may be said to be more or less autonomous (or agentic), and the extent to which something is an agent may be understood on a sliding scale. Most writing on Al agents do not make such distinctions, which has contributed to recent confusion in both technical and public discourse about what AI agents are and what they are capable of (Lambert, 2024). Addressing this issue, a proposal of different gradations of \"AI agent\" has recently been put forward by multiple researchers (e.g., Kapoor et al. (2024); Ng (2024); Greyling (2024); Lambert (2024); Roucher et al. (2024), although there is not yet consensus on the specifics of each level. Drawing from these ideas and different descriptions of Al agents, we propose a leveled AI agent scale in Table 1. Levels are one way of categorizing; for a classic categorization with consensus, see Russell and Norvig (1995).\nOur proposed agentic levels correspond to decreasing input from a user and decreasing code written by the agent developers. On the other side of this coin, AI agents can control more of how they operate. This is a critical aspect of the AI agent scale to understand in order to inform how agents might be developed for the better: The more autonomous the system, the more we cede human control."}, {"title": "5. Values Embedded in Agentic Systems", "content": ""}, {"title": "5.1. Methodology", "content": "To examine the relationship between AI agent autonomy level and ethical implications, we conducted a systematic analysis of how agents are conceptualized and deployed across different contexts. Our investigation focused on how varying degrees of agent autonomy interact with value propositions in research and commercial implementations.\nSpecifically, we:\n1. Collected and categorized statements about what agents are, \u00b9\u00b9 their capabilities, benefits, and harms. This included industry surveys that captured how professionals across roles envision and use AI agents; \u00b9\u00b2 case studies of deployed autonomous systems;\u00b9\u00b3 and news articles on the rise of AI agents.\u00b9\u2074\n2. Identified recurring AI agent value propositions.\n3. Converged on a value taxonomy that had corresponding benefits, risks, or both in the reviewed literature.\n4. Analyzed the role of values with increased autonomy.\nThe different functionalities and values we present are not intended to be exhaustive, but rather to provide a start-"}, {"title": "5.2. Value taxonomy", "content": "We distinguish three main patterns in how agentic levels impact value preservation:\n\u2022 Inherent risks (\u2299), present at all autonomy levels due to limitations in an Al agent's base model(s).\n\u2022 Countervailing relationships (\u2193\u2191): Where increasing autonomy creates both risks and opportunities with respect to an ethical value.\n\u2022 Amplified risks (\u2191): Where increasing autonomy magnifies existing vulnerabilities."}, {"title": "5.2.1. VALUE: ACCURACY", "content": "The accuracy of an Al agent is modulated by the accuracy of the models it's based on. Accuracy of Al agents influence values such as reliability, utility, consistency, and safety.\nBenefit: When a system is accurate in how it responds to user requests and correctly aligns with developer goals, increased autonomy provides increased useful functionality.\nRisk: The models on which recent AI agents are based can be inaccurate. The commonly used LLMs are known to produce incorrect information that appears correct.\nApplication to agent levels: (\u2191). Inherent risk from AI agent base model(s) is amplified with increased autonomy. For example:\n\u2022 Simple Tool Call: Inaccuracy propagated to inappropriate tool selection.\n\u2022 Multi-step: Cascading errors compound risk of inaccurate or irrelevant outcomes.\n\u2022 Fully Autonomous: Unbounded inaccuracies may create outcomes wholly unaligned with human goals."}, {"title": "5.2.2. VALUE: ASSISTIVENESS", "content": "AI agents are often motivated as assistive for user needs, supplementing a user's abilities and increasing their efficiency in finishing multiple tasks simultaneously.\nBenefits: apabilities, such as an Al agent that helps a blind user navigate busy staircases. Al agents that are well-developed to be assistive could offer their users more freedom and opportunity; help to improve their users' positive impact within their organizations; and help users to increase their public reach.\nRisk: When agents replace people-such as when AI agents are used instead of employees-this creates job loss and economic impacts, driving a further divide between the people creating technology and the people who have provided data for the technology (often without consent). Further, assistiveness that is poorly designed could lead to harms from over-reliance or inappropriate trust (Section 5.2.13).\nApplication to agentic levels: (\u2193\u2191). By design, assistiveness increases as the AI agent level increases: Each increasing AI agent level provides for more assistive options, as the AI system requires less guidance from the developer or user."}, {"title": "5.2.3. VALUE: CONSISTENCY", "content": "Some sources motivate AI agents as helping with consistency (e.g., (Salesforce, 2024; Oracle, 2024)). We are not aware of rigorous work on the nature of AI agent consistency, although related work has shown that the LLMs that"}, {"title": "5.2.4. VALUE: EFFICIENCY", "content": "A common selling point of AI agents is that they may help users to get more tasks done more quickly, acting as an additional helping hand.\nBenefit: Ways systems might help with efficiency include organizing a user's documents so they can focus on spending more time with their family or pursuing work they find rewarding. A future self-driving AI agent may make routing decisions directly, and could coordinate with other systems for relevant updates, allowing users to reach their destinations more quickly.\nRisk: Trying to identify and fix errors that agents introduce-which may be a complex cascade of issues due to agents' ability to take multiple sequential steps-can be time-consuming, difficult, and stressful.\nApplication to agentic levels: (\u2193\u2191). The relationship between autonomy and efficiency is subject to the accuracy of the system and the control provided by the developer and user. When there is room for error, Al agents may create"}, {"title": "5.2.5. VALUE: EQUITY", "content": "AI agents may affect how fair and inclusive situations are.\nBenefit: Al agents can potentially help \"level the playing field\". For example, a meeting assistant might display how much time each person has had to speak. This could be used to promote more equal participation or highlight imbalances across gender or location. \u00b9\u2075\nRisk: The machine learned models underlying modern AI agents are trained on human data; human data can be inequitable, unfair, and exclusionary. Inequitable outcomes may also emerge due to sample bias in data collection (for example, over-representing some countries) and job loss from agents replacing human workers (see Section 5.2.2).\nApplication to agentic levels: (\u2191\u2193). The listed benefits and risks are largely inherent to the base model(s) an AI agent is built on, and so hold regardless of agent level. However, as AI agent autonomy increases, it becomes closer to an artificial worker compared to a tool, increasing the risk of job loss. On the other hand, AI agents that help to increase equity can help retain employees."}, {"title": "5.2.6. VALUE: FLEXIBILITY", "content": "This refers to the fundamental motivation within AI agent development of systems that can use diverse tools and engage in input/output relationships with multiple systems.\nBenefit: Flexibility can help increase a user's efficiency and speed, or provide assistance for multiple different needs.\nRisk: The more an agent is able to affect and be affected by systems, the greater the risk of malicious code and unintended problematic actions, compromising safety and security. For example, an agent connected to a bank account so that it can easily purchase items on behalf of someone would be in a position to drain the bank account. Because of this concern, tech companies have refrained from releasing AI agents that can make purchases autonomously.\u00b9\u2076\nApplication to agentic levels: (\u2191). Systems may become more flexible the higher the agentic level: As the ability to create new content increases, so too does the potential for content that connects more closely with different systems."}, {"title": "5.2.7. VALUE: HUMANLIKENESS", "content": "Current AI agents are designed to be approachable for people, engaging in human-like dialogue and actions.\nBenefit: Systems capable of generating human-like behavior offer the opportunity to run simulations on how different subpopulations might respond to different stimuli (Park et al., 2024b). This can be particularly useful in situations where direct human experimentation might cause harm or fatigue. Synthesizing human behavior could be used to predict dating compatibility, or forecast economic changes and political shifts. Another potential benefit currently undergoing research is companionship (Sidner et al., 2018).\nRisk: The benefits can be a double-edged sword: Humanlikeness can lead users to anthropomorphise the system, which may have negative psychological effects such as overreliance and addiction,\u00b9\u2077 inappropriate trust (see 5.2.13)-which can create safety harms and harms of associated values-dependence, and emotional entanglement, leading to anti-social behavior or self-harm.\u00b9\u2078 The phenomenon of \u201cuncanny valley\u201d adds another layer of complexity-as agents become more humanlike but fall short of perfect human simulation, they can trigger feelings of unease, revulsion, or cognitive dissonance in users.\nApplication to agentic levels: (\u2191). This value is realized from the machine learning models that power the agent, and uncanny humanlikeness is possible at the most basic level of AI agent (simple processors). As such, all levels of AI agent carry this value's benefits and risks."}, {"title": "5.2.8. VALUE: PRIVACY", "content": "Benefit: Al agents may offer some privacy in keeping transactions and tasks wholly confidential, aside from what is monitorable by the AI agent provider.\nRisk: For agents to work according to user expectations, the user may provide personal information such as where they're going, who they're meeting with, and what they're doing. Further, for the agent to be able to act on behalf of the user in a personalized way, it may also have access to applications and information sources that can be used to isolate further privacy information (for example, from contact lists, calendars, etc.). Users can easily give up control of their data for efficiency (and are more likely to when trusting the agent); if there is a privacy breach, the interconnectivity of different content brought by the AI agent can make things worse. For example, an AI agent with access to phone conversations and social media could share highly intimate information publicly without consent of those in-"}, {"title": "5.2.9. VALUE: RELEVANCE", "content": "Benefit: Similar to benefits of assistiveness and flexibility, agent outcomes can be uniquely relevant for each user.\nRisk: This personalization can amplify existing biases and create new ones: As systems adapt to individual users, they risk reinforcing and deepening existing prejudices, creating confirmation bias through selective information retrieval and establishing echo chambers that reify problematic viewpoints. The very mechanisms that make agents more relevant to users-their ability to learn from and adapt to user preferences-can inadvertently perpetuate and strengthen societal biases, making the challenge of balancing personalization with responsible AI development particularly difficult.\nApplication to agentic levels: (\u2193\u2191). The more freedom a system has to retrieve and formulate new content, the more potential there is to provide relevant information outside of constraints and resources set by users and developers."}, {"title": "5.2.10. VALUE: SAFETY", "content": "The ethical value of safety is a primary concern in the development of Artificial General Intelligence (AGI). Many of the benefits and concerns with respect to safety and AGI are also relevant to AI agents.\nBenefit: Robotic AI agents may help save people from bodily harm, such as agents capable of diffusing bombs, removing poisons, or operating in manufacturing or industrial settings that are hazardous environments for humans.\nRisk: The unpredictable nature of agent actions means that seemingly safe individual operations could combine in harmful ways, creating new risks that are difficult to prevent. (This is similar to Instrumental Convergence and the classic paperclip maximizer problem.\u00b9\u2079) It can also be unclear whether an AI agent might design a process that overrides a given guardrail, or if the way a guardrail is specified inadvertently creates further problems. If guardrails mitigate loss of human life, such as with autonomous surgeons or missile system operation, this is a severe risk. Therefore, making agents more capable and efficient through broader system access, more sophisticated action chains, and reduced human oversight conflicts with safety considerations.\nFurther, access to broad interfaces (for example, GUIs, as"}, {"title": "5.2.11. VALUE: SECURITY", "content": "Benefit: Potential benefits are similar to those for Privacy.\nRisk: Al agents present serious security challenges due to their handling of often sensitive data (customer or user information) combined with their safety risks, such as ability to interact with multiple systems and the by-design lack of detailed human oversight. This can lead to sharing confidential information even when their goals were set by good faith actors. Malicious actors could hijack or manipulate agents to gain access to connected systems, steal sensitive information, or conduct automated attacks at scale.\nApplication to agentic levels: (\u2191) Different AI agent levels affect security differently. For the first four, developers control the code the agent can access, providing a built-in ability to mitigate security outbreaks, e.g., by blocking communication with third parties. However, when an agent is able to create and execute new code (a fully autonomous agent), it's capable of creating breaches unforeseen by developers."}, {"title": "5.2.12. VALUE: SUSTAINABILITY", "content": "Benefit: It is hoped that AI agents may alleviate issues relevant to climate, such as by forecasting wildfire growth or flooding. Helping address efficiency issues, such as traffic efficiency, could decrease carbon emissions.\nRisk: The models current agents are based on bring negative environmental impacts, such as carbon emissions (Luccioni et al., 2024) and usage of potable water (Hao, 2024).\nApplication to agentic levels: ( \u2193\u2191). On one hand, the models on which AI agents are based bring with them environmental risks. On the other, the ability of AI agents to harness more information than humans and produce novel solutions outside of those foreseen by humans-an ability increased as autonomy increases-may lead to innovative approaches to addressing environmental issues."}, {"title": "5.2.13. VALUE: TRUST", "content": "Recent AI agent writing does not motivate how agents benefit or harm trust, but rather, that systems should be constructed to be worthy of our trust, shown to be safe (Section 5.2.10), secure (Section 5.2.11) and reliable.\nApplication to agentic levels: (\u2191). As the agentic level increases, human trust can lead to increased risks stemming from increased agent flexibility (Section 5.2.6) and issues in its accuracy (Section 5.2.1), consistency (Section 5.2.3), privacy (Section 5.2.8), safety (Section 5.2.10), security (Section 5.2.11), and truthfulness (Section 5.2.14)."}, {"title": "5.2.14. VALUE: TRUTHFULNESS", "content": "Risk: The deep learning technology modern Al agents are based on is well-known to be a source of false information (e.g., (Garry et al., 2024)), which can take shape in forms such as deepfakes or misinformation. Al agents can be used to further entrench this content, such as by tailoring output to current fears and posting on several platforms. This means that Al agents can be used to provide a false sense of what's true and what's false, manipulate people's beliefs, and widen the impact of non-consensual intimate content. False information propagated by AI agents, personalized for specific people, can also be used to scam them. Further risks emerge from inconsistent truthfulness, leading to inappropriate trust: A system correct the majority of the time is more likely to be inappropriately trusted when wrong.\nApplication to agentic levels: (\u2191). The more control an AI agent has over its environment and the resources available to it, the more it is able to define for itself what is true and false within its environment. Because the environments that AI agents may create for themselves are not identical to environments humans are in, the potential for less truthfulness, as based on human environments, increases."}, {"title": "5.3. Summary", "content": "Our analyses suggest that there are several forms of increased risk with increased agentic levels:\n\u2022 Risks that result from system inaccuracy (ACCURACY value, Section 5.2.1) and inconsistency (CONSISTENCY value, Section 5.2.3)\n\u2022 Risks of breaches of privacy (Section 5.2.8), safety (Section 5.2.10), and security (Section 5.2.11)\n\u2022 Risks of the wider spread of false information (TRUTHFULNESS value, Section 5.2.14)\n\u2022 Risk of loss of control outside human-set guardrails (FLEXIBILITY value, Section 5.2.6)\nThere is also the potential for increased benefit, particularly with respect to assistance (ASSISTIVENESS value, Section 5.2.2), efficiency (Section 5.2.4), equity (Section 5.2.5), relevance of outcomes (RELEVANCE value,"}, {"title": "6. Alternative Viewpoints", "content": "There are at least two alternative viewpoints to the views discussed in the paper.\n1. No gradations of \"AI agent\": Scholarship and marketing materials generally do not distinguish between different agentic levels. We believe this has created a common confusion on what is an AI agent and what is not. By distinguishing different levels and identifying the level of full autonomy as a specific type of AI agent, we hope to clarify misunderstandings and isolate this level of autonomy as particularly problematic in AI agent development.\n2. Support for building fully autonomous AI agents. Proponents of this view argue that full autonomy or \u201ccomplete agents\" are useful in order help us better understand human intelligence (Lambrinos & Scheier, 1996; Garland, 2015), and that \"strong\" AI systems could help to counterbalance human errors and irrationality (Garland, 2015). Others have put forward that Artificial General Intelligence (AGI) would be fully autonomous if realized (Totschnig, 2020), which would entail that developing AGI-a direct goal of multiple researchers and companies-opposes the position in this paper. Proponents of achieving AGI argue it could help us solve global problems, such as climate change and hunger (Lu et al., 2023), and provide significant economic gains (OpenAI, 2023). We contend that if AGI is to be developed, it should not be developed with full autonomy-humans should always maintain some level of control-and we hope that the distinctions we provide here across different agentic levels helps to inform future AGI goals."}, {"title": "7. Conclusion: Where do we go from here?", "content": "The history of nuclear close calls provides a sobering lesson about the risks of ceding human control to autonomous systems.\u00b2\u2070 For example, in 1980, computer systems falsely indicated over 2,000 Soviet missiles were heading toward North America. The error triggered emergency procedures: bomber crews rushed to their stations and command posts prepared for war. Only human cross-verification between different warning systems revealed the false alarm. Similar"}, {"title": "A. Agent definitions", "content": "The term \"agent\" has been used in many engineering contexts, including in references to software agent, intelligent agent, user agent, conversational agent, and reinforcement learning agent (Huyen, 2025).\nBelow, we provide a selection of AI Agent definitions that have informed this piece. Neither the list we provide here, nor the snippets of text quoted, should be taken as complete. Rather, they serve to illustrate the diversity and richness of AI agent definitions over the years. As humorously noted in Wooldridge & Jennings (1995): \u201cthe question what is an agent? is embarrassing for the agent- based computing community in just the same way that the question what is intelligence? is embarrassing for the mainstream AI community. The problem is that although the term is widely used, by many people working in closely related areas, it defies attempts to produce a single universally accepted definition.\"\nWe find stark differences in how AI agents are described, with ambiguous language a common practice in descriptions of products. For example, when materials describe agents as something that uses artificial intelligence\" (), they leave ambiguous what \u201cartificial intelligence\" refers to and the scope of technology included and excluded in the definition, such as whether simple prompt-response systems qualify as an agent. However, most descriptions of \"agents\u201d we reviewed entail that the system can take at least one step in program execution without user input."}, {"title": "B. Agent functionalities", "content": "This section provides a more detailed breakdown of different agent functionalities than in Table 2:\n\u2022 Proactivity: Related to autonomy is proactivity, which refers to the amount of goal-directed behavior that a system can take without a user directly specifying the goal (Wooldridge & Jennings, 1995). An example of a particularly \u201cproactive\" AI agent is a system that monitors your refrigerator to determine what food you are running out of, and then purchases what you need for you, without your knowledge. Smart thermostats are proactive Al agents that are being increasingly adopted in peoples' homes, automatically adjusting temperature based on changes in the"}]}