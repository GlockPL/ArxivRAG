{"title": "Time series forecasting based on optimized LLM for fault prediction in distribution power grid insulators", "authors": ["Jo\u00e3o P. Matos-Carvalho", "Stefano Frizzo Stefenon", "Valderi Reis Quietinho Leithardt", "Kin-Choong Yow"], "abstract": "Surface contamination on electrical grid insulators leads to an increase in leakage current until an electrical discharge occurs, which can result in a power system shutdown. To mitigate the possibility of disruptive faults resulting in a power outage, monitoring contamination and leakage current can help predict the progression of faults. Given this need, this paper proposes a hybrid deep learning (DL) model for predicting the increase in leakage current in high- voltage insulators. The hybrid structure considers a multi-criteria optimization using tree- structured Parzen estimation, an input stage filter for signal noise attenuation combined with a large language model (LLM) applied for time series forecasting. The proposed optimized LLM outperforms state-of-the-art DL models with a root-mean-square error equal to 2.24\u00d710-4 for a short-term horizon and 1.21\u00d710-3 for a medium-term horizon.", "sections": [{"title": "1. Introduction", "content": "Insulators in the conventional electrical power distribution and transmission system are installed outdoors and are susceptible to environmental conditions such as the presence of contamination, solar radiation, atmospheric discharges, and thermal variation [1]. These envi- ronmental stresses can result in a power outage if preventive or predictive maintenance is not handled [2]. A way of identifying where the power grid needs more attention is to perform inspections on the electrical system, either through imaging or using specific equipment to assess the con- dition of the system [3]. Besides monitoring using specific equipment such as ultrasound, radio interference, ultraviolet, and infrared cameras, an analysis can be done in terms of power grid contamination. The contamination of insulators is a subject that has been extensively stud- ied, because over time the contamination increases the surface conductivity of the insulators, resulting in flashover [4]. Specific techniques for measuring the contamination of insulators, whether this is saline contamination or based on kaolin (artificial contamination) are explored. Measuring leakage current is one way of determining how the contamination is influencing the insulator's degra- dation [5]. The higher the surface conductivity of the insulator, the greater the chance of this component experiencing leakage current, which leads to a flashover voltage in the power system that can result in a complete shutdown of the grid [6]. Considering the advances made in predicting time series using machine learning (ML)-based models, especially involving deep learning (DL) [7], using them to predict the increase in leakage current could be an alternative for improving the monitoring performance of electrical systems. Based on this premise, this paper proposes a hybrid model for predicting leakage current in electric power distribution insulators. Given the vast nomenclature of the field, the acronyms of this paper are standardized according to Table 1. The proposed hybrid model employs noise attenuation considering an input filter stage. The Christiano Fitzgerald (CF) asymmetric random walk [8], Hodrick-Prescott (HP) [9], season- trend decomposition using LOESS (STL) [10], multiple STL (MSTL) [11], empirical wavelet transform (EWT) [12], Butterworth [13], and empirical mode decomposition (EDM) [14] filters are analyzed, and the best filter is employed in the model input. For the STL and MSTL, the LOESS stands for locally estimated scatterplot smoothing. Based on the filtered signal, the LLM is used to perform the time series prediction, also known as timeLLM. To ensure that the best structure is used, the Optuna framework using the tree-structured Parzen estimator (TPE) is considered for hyperparameter tuning [15]. Con- sidering the proposed optimized LLM model applied for fault prediction, this paper has the following contributions: \u2022 It presents an innovative way of analyzing time series based on LLM models, and is a strategy that can be applied in the future considering the advances in this field. \u2022 By using a filter on the input signal, unrepresentative noises are disregarded, making the prediction model more assertive and promising in chaotic time series analysis. \u2022 Based on a strategy using TPE, the structure is hyper-adjusted ensuring that the optimal hyperparameters are used in the proposed model. The remainder of this paper is as follows: In section 2, related works about fault prediction in insulators are presented. Section 3 explains the proposed optimized LLM model. In section 4 the results of the application of the proposed model are discussed, firstly the evaluation of the filter is presented, followed by the tuning strategy. Considering a hypertuned structure, a statistical analysis and benchmarking are presented. Section 5 presents final remarks and directions for future research."}, {"title": "2. Related Works", "content": "Fault prediction in distribution grid insulators is critical for maintaining the reliability and safety of power systems [16]. Recent studies have explored various methodologies to predict in- sulator failures, focusing on analyzing leakage currents, employing ML techniques, and utilizing advanced signal processing methods. A way to identify faults in this context involves monitoring the time series data of the leakage current of insulators under contaminated conditions. Several authors have applied computer vision-based methods considering convolutional neu- ral networks (CNNs) for insulator fault identification [17]. Prominent in this area is the you only look once (YOLO) model, which was applied in its third generation by Yang et al. [18], in the fourth generation by Li et al. [19], in the fifth generation by Zhou et al. [20], considering a hypertuned version by Stefenon et al. [21], or using a hybrid version in [22]. Deng et al. [23] proposed a modified YOLO that can be computed on edge devices. The modification to make the algorithm more efficient was in the backbone of the YOLO. Instead of using a CSPDarknet53 (standard for the YOLO version that they considered), they applied a"}, {"title": "2.1. State-of-the-Art in Time Series Forecasting", "content": "In recent years, advances in ML and DL models have driven the development of more robust and accurate methods to predict time series. In this regard, DL-based approaches including recurrent neural networks (RNN) [54] and transformers [55], have shown promising performance. In [56] the authors considered the use of the temporal fusion transformer (TFT) model for time series. This model is an attention-based deep neural network architecture designed for multi- horizon time series forecasting, combining high performance with interpretability. The model incorporates static covariate encoders, gating mechanisms, variable selection networks, and hybrid temporal processing, which uses LSTMs for local patterns and self-attention to capture long-term dependencies. Oreshkin et al. [57] proposed the neural basis expansion analysis for time series forecasting (N-BEATS), which is a deep neural network architecture designed for univariate time series forecasting based on residual connections and multiple fully connected layers. The deep tempo- ral convolutional network (DeepTCN) was proposed in [58] and is a CNN architecture developed for probabilistic forecasting of multiple related time series. The model employs dilated causal convolutions, which guarantee dependence only on past inputs and capture long-range patterns"}, {"title": "3. Methodology", "content": "This paper considers the use of a filter input stage for noise attenuation and a hypertuned LLM model for time series forecasting. The time series is denoised to remove high frequencies and to have the focus of the analysis on the variation trend. To ensure that an optimal structure is assumed, the hyperparameters of the LLM are tuned via multi-criteria optimization based on TPE using the Optuna framework [67]. In this section, all considered techniques employed in the proposed optimized LLM model are explained."}, {"title": "3.1. Input Stage Filter", "content": "Filters are applied in this paper to reduce the high frequencies in the signal, thus focusing on predicting the trend, which represents the most significant variation in the temporal analysis that leads to failure after a certain time in conditions of high contamination. The decomposition is expressed as:\n\n\n\nwhere t\u2081 is the trend component, st is the seasonal (cyclical) component, and et is the remainder (residual) component [68]. The focus of the prediction here is the Tt because its variation represents the increase in the leakage current until it reaches the limit accepted by the insulator. The CF, HP, STL, MSTL, EWT, Butterworth, and EDM filters are analyzed for signal denoising, and the best filter is employed in the model input stage. The symbols used in the equation of these filters are presented in Table 3. The symbols that are used in both the filters and the model are presented in the section that explains the model architecture."}, {"title": "3.1.1. Christiano Fitzgerald Asymmetric Random Walk (CF) Filter", "content": "The CF filter is designed for the decomposition of time series data into seasonal and trend components. Unlike symmetric filters, the CF filter can operate asymmetrically, making it useful for real-time forecasting. The CF filter is based on the concept of band-pass filtering, where the goal is to isolate cycles within a specified frequency range [69]. The mathematical foundation of the filter involves approximating the ideal band-pass filter, which has a transfer function defined as:"}, {"title": "3.1.2. Hodrick-Prescott (HP) Filter", "content": "The HP filter is used in time series analysis to decompose a series into its trend and seasonal components. It is particularly for extracting the smooth long-term trend from noisy data [70]. The HP filter decomposes a time series yt into trend Tt and seasonal st components, previously defined in Eq. (1). The estimation of Tt is formulated as a minimization problem. Specifically, the objective function combines a goodness-of-fit term and a smoothness penalty:\n\n\n\n\n\nwhere the first term, , ensures that the estimated trend Tt fits the data well. The second term, , penalizes changes in the slope of the trend, effectively ensuring that Tt is smooth over time. The smoothing hyperparameter A controls the trade-off between these two objectives. The choice of A is subjective and may affect the decomposition. This issue can be solved with a A adjustment. When \u5165 \u2192 0, the trend Tt closely follows the original series yt, allowing for greater flexibility, on the other hand, when \u5165 \u2192 \u221e, the trend becomes a linear function, as the penalty on deviations from linearity dominates [71]."}, {"title": "3.1.3. Season-Trend Decomposition using LOESS (STL) Filter", "content": "The operation in STL decomposition is based on the application of LOESS, a regression method used to estimate a smooth function by fitting weighted local polynomials [72]. The LOESS procedure at any time point t involves: Initially, it defines the neighborhood around t by selecting points within a specified window determined by the smoothing hyperparameter \u5165, which controls the fraction of data used in the local fit. Thus, the weights are assigned to the observations within the neighborhood based on their distances from t using a kernel function, commonly the tricube kernel:\n\n\n\n\n\nFitting a weighted polynomial (typically linear or quadratic) to the data within the neigh- borhood, minimizing the locally weighted sum of squared residuals, given by\n\n\n\n\n\nwhere \u03b2\u03bf, \u03b21, . . ., \u1e9ep are the coefficients of the polynomial, and P is the degree of the polynomial (commonly P = 1 or P = 2). Once the decomposition is complete, the trend component Tt can be used for forecasting. For example, future values of yt may be predicted by evaluating the trend Tt using prediction models. The remainders are often modeled as a stochastic noise, assumed as white noise [73]."}, {"title": "3.1.4. Multiple Season-Trend Decomposition using LOESS (MSTL) Filter", "content": "The MSTL is a method used for analyzing time series data, particularly when multiple seasonal components and trends are present. The primary goal of MSTL is to decompose the time series into trend, seasonality, and remainder components according to Equation 1. Mathematically, for a univariate time series yt, the method can be represented as follows:\n\n\n\n\n\nwhere si,t is the seasonal component for the j-th seasonal frequency, where j = 1, . . ., M, and M is the number of seasonal components [74]. Like the STL, the MSTL employs the concept of LOESS to estimate each component iter- atively. LOESS operates by fitting a polynomial regression model locally for a subset of data points around each time index t, weighted by a kernel function [75]. The kernel assigns higher weights to points closer to t and lower weights to points farther away. In the MSTL decomposition process, the seasonal components sj,t are estimated first. Each seasonal component corresponds to a specific periodicity, and LOESS smoothing is applied after aggregating data for that periodicity. MSTL is robust to outliers through the use of LOESS, where weights are iteratively adjusted based on residuals to reduce the influence of extreme values [76]."}, {"title": "3.1.5. Empirical Wavelet Transform (EWT) Filter", "content": "The EWT is a signal decomposition method that is designed to extract meaningful frequency components from a signal. Unlike traditional wavelet transforms, which rely on predefined mother wavelets and fixed frequency partitions, the EWT constructs wavelet filters based on the spectral characteristics of the input signal. This adaptability makes it a promising method for time series forecasting, especially in cases where signals exhibit non-stationary behavior [77]. For each segment [Wk\u22121,Wk], a scaling function \u03c6\u3047(w) and a wavelet function \u03c8k(w) are con- structed. These functions are designed to satisfy orthogonality and completeness conditions over the defined frequency intervals. The k(w) is used to capture the low-frequency compo- nents, while the \u03c8k(w) isolates the band-limited frequency components [78]. The empirical scaling function and wavelet function in the Fourier domain can be defined as:\n\n\n\n\n\n\n\n\n\nwhere dk is the width of the transition band. After constructing the wavelet and scaling functions, the signal x(t) is decomposed into empirical wavelet coefficients using the inverse Fourier transform. The kth empirical wavelet component is given by:"}, {"title": "3.1.6. Butterworth Filter", "content": "The Butterworth filter is a signal processing technique used in time series analysis and forecasting due to its smooth frequency response and minimal distortion characteristics. It effectively separates high-frequency noise from low-frequency trends in time series data. The Butterworth filter is applied to have a maximally flat magnitude response in the passband, avoiding ripples in the frequency response [13]. The general transfer function for an nth-order Butterworth filter is given by:\n\n\n\n\n\nwhere w is the angular frequency, k is the cutoff frequency, and n is the order of the filter. The cutoff frequency k defines the boundary between the passband and the stopband. The filter's order, n, defines the steepness of the transition between the passband and the stopband. Higher- order filters yield sharper transitions but introduce higher computational complexity [80]. In time series forecasting, the Butterworth filter is often used as a low-pass filter to extract the low-frequency trend component of a time series. This is accomplished by suppressing high- frequency variations, which are typically associated with noise or short-term fluctuations while retaining the underlying trend [81]. To apply the Butterworth filter to discrete time series data, the transfer function is transformed into the Z-domain using the bilinear transformation:\n\n\n\n\n\nwhere S is the Laplace transform variable, T is the sampling period, and Z is the complex variable in the Z-domain. Substituting this transformation into the continuous-time transfer function yields the discrete-time transfer function:"}, {"title": "3.1.7. Empirical Mode Decomposition (EMD) Filter", "content": "The EMD is an adaptive, data-driven technique for analyzing non-linear and non-stationary time series data. It decomposes a signal into a finite set of components called intrinsic mode functions (IMFs) and a residual. The decomposition process begins by identifying the IMFs, which are defined by two conditions: the number of extrema and zero crossings in an IMF must either be equal or differ at most by one, and the mean value of the upper envelope and the lower envelope must be zero at every point [84]. To formalize the process, consider a time series x(t). The first step involves constructing envelopes for the signal using cubic splines to interpolate the local maxima and minima. De- noting the upper and lower envelopes as Cupper(t) and elower (t), respectively, their mean m(t) is calculated as\n\n\n\n\n\nThe mean is then subtracted from the original signal to produce a candidate IMF:\n\n\n\n\n\nThis process, known as sifting, is iterated until h(t) satisfies the IMF criteria. The first IMF (c1(t)), captures the highest frequency oscillations in the signal. To extract subsequent IMFs, c1(t) is subtracted from the original signal:\n\n\n\n\n\nwhere \u20ac1(t) becomes the new input signal for further decomposition. Repeating this procedure yields a set of IMFs {C1(t), C2(t), . . ., Cn(t)} and a final residual en(t), such that\n\n\n\n\n\nEach IMF isolates oscillatory modes of different scales, making EMD particularly suitable for analyzing signals where distinct temporal scales are present. For time series forecasting, consider that the extracted IMFs often exhibit simpler patterns than the original signal. These components can then be modeled independently using ML methods. Forecasting can proceed by predicting each IMF ci(t) individually by ML models [85]."}, {"title": "3.2. Prediction Model Architecture", "content": "LLM applied for time series, or timeLLM, is built upon the foundational structure of large language models by incorporating temporal reasoning directly into its architecture. The in- novation of this model lies in the integration of temporal embeddings, dynamic contextual adjustments, and attention mechanisms for time-dependent data. These features enhance the model's ability to handle sequential and time-sensitive information [86]. The symbols used in the equation of the model architecture and its hypertuning are presented in Table 4.\n\n\n\nTemporal embeddings encode time-related information, such as timestamps or relative du- rations, by mapping these into a latent space. Mathematically, given a sequence of events {N1, N2, ..., \u03b7\u03b7} occurring at corresponding timestamps {t1, t2,..., tn}, temporal embeddings T(t) map each timestamp ti to a vector representation vi \u2208 Rd, where d is the embedding dimension. These embeddings are learned jointly with the model hyperparameters, ensuring that temporal context is captured alongside linguistic features [87]. The temporal embeddings are integrated into the model's transformer layers. For each input token xi, the modified input representation is given by\n\n\n\n\n\nwhere E(xi) is the standard token embedding and T(ti) is the temporal embedding. This addition ensures that the model incorporates temporal information at the input stage. The attention mechanism in timeLLM is adapted to prioritize temporal dependencies. The scaled dot-product attention is modified to include a temporal weighting term. For query Q, key K, and value V matrices, the attention weights are computed as"}, {"title": "3.3. Model Hypertuning", "content": "For hyperparameter model tuning, the TPE is applied. TPE is a model-based optimiza- tion algorithm that builds on the Bayesian optimization, tailored for hyperparameter tuning [90]. The TPE algorithm uses probabilistic modeling to construct surrogate models of the objective function and guides the search for optimal hyperparameter configurations. In this paper, the TPE is applied for hypertuning the proposed optimized LLM model, the considered hyperparameters are the batch size, dropout, learning rate, and number of heads. The goal of hyperparameter optimization is to minimize or maximize an objective function f : X \u2192 R, where X is the hyperparameter space [91]. The TPE idea relies on modeling the conditional probability p(y | x), where x \u2208 X is a vector of hyperparameters and y = f(x) is the corresponding objective value. The TPE replace p(y | x) with two conditional densities, l(x) and g(x), based on a threshold y such that:\n\n\n\n\n\nwhere y is a quantile of the observed objective values {Y1, Y2, ..., Yn}, often chosen as a fixed percentile. The TPE algorithm thus rewrites the marginal likelihood p(x | y) using Bayes' theorem:\n\n\n\n\n\nwhere N = p(y) is the normalization constant. TPE focuses on maximizing the expected improvement (EI) by choosing hyperparameter configurations that are likely to improve upon the current best observations. The El criterion is reformulated in TPE by using the ratio of densities l(x)/g(x). Intuitively, TPE selects x to maximize this ratio, favoring regions of the search space that are more probable under l(x) (regions associated with good performance) while being less probable under g(x) (regions associated with worse performance) [92]."}, {"title": "4. Results and Discussion", "content": "This section presents the dataset used for the experiments, the analysis setup, the results of applying the proposed optimized LLM, and a benchmarking with other well-established DL methods. After defining the dataset and comparison settings used in this paper, the results of applying filters to reduce noise are discussed. From the filtered signal, the model is optimized using hypertuning. Once optimized, an analysis is handled concerning the variation in the forecast horizon, after which comparative results of the proposed model to other models are presented."}, {"title": "4.1. Dataset", "content": "The data set considered in this paper refers to leakage current measurements in an experi- ment on insulators subjected to artificial contamination. The experiment consists of increasing the level of contamination in insulators until a disruptive discharge occurs. The increase in contamination results in an increase in leakage current, which is the main indicator that a fault may occur [95]. Six insulators were evaluated in the experiment, four of which were discharged before the end of the experiment and were disregarded. Of the two insulators that withstood the increase in contamination without being discharged, only one had a linear increase in leakage current, and this was the insulator considered in this study. To reduce the complexity of the analysis, a downsample is handled in the pre-preprocessing stage, which means that instead of 96,800 recorded records being considered, corresponding to 26.9 hours of evaluation, 968 are the focus of the analysis. Electrical discharges occurred in many insulators after the leakage current exceeded 200mA, which is an appropriate threshold for predicting that a fault will occur. The considered signal of the leakage current in the insulator in question is shown in Figure 2."}, {"title": "4.2. Experiment Setup", "content": "The experiments utilized an NVIDIA RTX 3060 TI graphics processing unit with 120 GB of random-access memory. The models were implemented in Python. Processing time encom- passes the total duration required for both model training and testing. The evaluation metrics include RMSE, MAE, MAPE, and SMAPE, defined as follows:"}, {"title": "4.3. Filtering Analysis", "content": "In the pre-processing stage, to ensure a fair analysis, all the filters considered in this paper are applied using their default settings. The results of this evaluation in relation to the original signal are shown in Figure 3. This figure shows the sample index as the horizontal axis, since the downsample is applied to reduce the complexity of the analysis. Based on these results, the timeLLM model is applied to the original signal and the denoised signals by all the filters, as shown in Table 5. In this analysis, the default setup is used in the model considering a horizon equal to 60 steps ahead. The time needed to process the model with each of the filters was equivalent since the greatest computational effort is regarding the training stage of the DL model, not the usage of the filter."}, {"title": "4.4. Hypertuning Analysis", "content": "An important definition to be made in the model's tuning phase is the variability space (gap) of each hyperparameter. If the space is not properly defined, the model may struggle to find the optimum. A large search space makes it difficult to optimize the hyperparameters, and a small space can limit the search, causing the TPE to find values near the extreme of variation for each hyperparameter. To ensure that the model has been properly optimized, hypertuning with a large search space has been done beforehand, so the search space presented here takes into account the gap of best values found in the first experiment. Considering the gap of the first experiment, the hypertuning is done with the following values of batch size [10 to 20], number of heads [1 to 8], learning rate [0.001 to 0.01], and dropout [0.0 to 0.7]. The results of this analysis are shown in Figure 4. This analysis considered a time-LLM model with the EWT filter, using a horizon and input size equal to 20. Considering that all the optimal results of the hyperparameters evaluated were within the variation gap analyzed, the analysis was carried out properly. In Figure 4, the black dots represent the local minima of the gradient of each combination, considering Eq. (30) as the loss function. The lighter the gradient (white being the lightest color), the greater the loss function result and, in turn, the worse the result obtained. Con- versely, the darker the color of the gradient (in this case dark blue), the lower the loss function result and, in turn, the better the result, since in both cases the optimization function is to reduce the RMSE. In this optimization, the batch size hyperparameter had an importance of 13% in achieving the goal of reducing the RMSE, the learning rate had an importance of 17%, the number of heads in the model had an importance of 19%, and the dropout had an importance of 51%, making it the most important hyperparameter to optimize. The rank of the hyperparameter fitting attempts is shown in Figure 5. The values in the center of the plots are linear, as they only show the variation of one hyperparameter at a time. It should be noted that, for this case, the redder it gets, the higher the loss function (RMSE in this case), and, in turn, the worse the approximation of the output to the ground truth."}, {"title": "4.5. Multi-horizon Analysis", "content": "The purpose of the evaluation of horizons is to analyze the impact using a longer forecast horizon has on the model's performance. To this end, Table 6 presents a comparative analysis of various forecast horizons using the proposed optimized LLM model using the EWT as the input stage filter. A forecast of 5 steps ahead is considered a short-term horizon, and 60 steps ahead is considered a medium-term horizon. The results of this comparison showed that the longer the forecast horizon, the more difficult it is to produce a forecast with lower error. The time needed to train the model and carry out the test did not vary considerably when changing the forecast horizon, but the error had a big impact, showing how challenging it is to carry out forecasts with long forecast horizons. The MAPE values show that the model struggles to make predictions for horizons greater than 20 steps ahead since the values were higher than 30%. This challenge is related to the variability of the data, where non-linear time series are more difficult to predict in horizons that consider many steps ahead. This result shows that the proposed model is better suited to short-term than medium-term horizons. A visualization of the variation in the model's performance as the forecast horizon increases can be seen in Figure 7. For random sampling, a Monte Carlo approach is considered to train the model. The model shows a considerably promising result up to a Horizon equal to 20 steps ahead, after which greater difficulties are encountered in handling the forecast, especially after"}, {"title": "4.6. Statistical Analysis", "content": "To perform a statistical analysis, the proposed optimized LLM model was computed for 50 runs with different initialization weights (seed). The result of this evaluation is shown in Table 7. The mean, median, mode, range, standard deviation, 25th, 50th, and 75th percentile (%ile), interquartile range (IQR), skewness, and kurtosis for each performance metric (RMSE, MAE, MAPE and SMAPE) are presented. In general, the results of the statistical analysis show that the model has promising average values, making it a suitable model for the task discussed here. To give a visual presentation of the variability of the results in relation to the 50 runs, Figure 8 shows a box plot of the errors evaluated in this paper using the proposed optimized LLM model. This presentation shows some outliers with greater error than the average values obtained by the model. Considering that the initialization of the weights is random and that the vast majority of the results are close to the average (observing a logarithmic presentation), the results are promising. These results show that the model is stable, and based on that, a final comparative analysis is presented in the following."}, {"title": "4.7. Benchmarking", "content": "The comparative analysis presented here focuses on comparing our proposed method to other well-established DL architectures. This evaluation considers two different horizons: A short-term horizon of 5-steps ahead and a medium-term horizon of 60-steps ahead. The results of the short-term horizon are presented in Table 9, and the results of the medium-term horizon are presented are presented in Table 8."}, {"title": "5. Conclusion", "content": "In this paper, a hybrid model integrating a filtering input stage and an LLM applied for time series, optimized via the Optuna framework, was proposed for insulator fault prediction. The time series of the leakage current of the insulators is based on a high-voltage experiment with artificial contamination. Considering an evaluation of CF, HP, STL, MSTL, EWT, But- terworth, and EDM filters, the EWT showed more promising results with its default setup. Therefore, the EWT decomposed time series signals, mitigating noise and non-stationary ef- fects. These decomposed signals were subsequently forecasted by an optimized LLM, which demonstrated robust capabilities in modeling complex temporal dependencies inherent in insu- lator degradation patterns. The experimental results underscore the superiority of the optimized LLM framework in comparison to the state-of-the-art DL models, achieving an RMSE equal to 2.24\u00d710-4 for a short-term horizon (5 steps ahead) and 1.21\u00d710-3 for a medium-term horizon (60 steps ahead) in insulator fault detection. The proposed methodology enhances the reliability of power grid maintenance and provides a generalizable framework for predictive maintenance in industrial systems. Future work could explore the integration of real-time adaptive decomposition techniques, expand the model's interpretability for operational decision-making, and validate the framework on larger cross-domain datasets. Deploying the monitoring model on-site can promote practical implementation in smart grid applications."}]}