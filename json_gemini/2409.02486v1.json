{"title": "Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization", "authors": ["Cho-Ying Wu", "Yiqi Zhong", "Junying Wang", "Ulrich Neumann"], "abstract": "Indoor robots rely on depth to perform tasks like navigation or obstacle detection, and single-image depth estimation is widely used to assist perception. Most indoor single-image depth prediction focuses less on model generalizability to unseen datasets, concerned with in-the-wild robustness for system deployment. This work leverages gradient-based meta-learning to gain higher generalizability on zero-shot cross-dataset inference. Unlike the most-studied meta-learning of image classification associated with explicit class labels, no explicit task boundaries exist for continuous depth values tied to highly varying indoor environments regarding object arrangement and scene composition. We propose fine-grained task that treats each RGB-D mini-batch as a task in our meta-learning formulation. We first show that our method on limited data induces a much better prior (max 27.8% in RMSE). Then, finetuning on meta-learned initialization consistently outperforms baselines without the meta approach. Aiming at generalization, we propose zero-shot cross-dataset protocols and validate higher generalizability induced by our meta-initialization, as a simple and useful plugin to many existing depth estimation methods. The work at the intersection of depth and meta-learning potentially drives both research to step closer to practical robotic and machine perception usage.", "sections": [{"title": "I. INTRODUCTION", "content": "Much research learns depth from single images to fulfill indoor robotic tasks like collision detection [9, 23], navigation [36, 37], grasping [31, 39], or it benefits 3D sensing or learning good 3D representations for AR/VR and view synthesis [7, 30]. However, generalization is still a major issue in robustly estimating depth on unseen scenes or datasets, especially indoor scenes, since their composition varies widely, and objects are usually cluttered in the near field without an order. An intuitive solution is to learn from large-scale mixed datasets [27, 28, 43] or adopt pretrained auxiliary models as guidance [40], but they require extra information or exogenous models. Without those resources when training on data of limited appearance and depth variation (scene variety in this work), with an extreme case that only sparse and non-overlapping views are available, networks barely learn valid depth.\nInspired by meta-learning's advantages on domain gen-eralization and few-shot learning [8, 24], we dig into how meta-learning can be applied to pure\u00b9 single-image depth prediction. Conventional meta-learning focuses on image classification and follows few-shot multitask learning, where a task represents a distribution to sample data from [17]. Instead, we study a more complex problem of scene depth estimation: the difficulties first come from estimating per-pixel and continuous range values, in contrast to global and discrete values for image classification. Next, indoor RGB-D captures vary greatly. Even within a sequence, an adjacent frame to a close view of cluttered objects can be large spaces without objects.\nThis observation indicates that pure single-image depth estimation lacks clear task boundaries as conventional meta-learning [15]. The problem also differs from online depth adaptation, which treats each video sequence as a task [47, 48]. To address the specific challenges, we propose to treat"}, {"title": "II. RELATED WORK", "content": "Depth from Single Indoor Images. The task has gained higher popularity [20, 21] while more high-quality datasets be-come available, such as Hypersim [29], Replica [34], HM3D [26], and VA [40]. Some methods adopt surface normal [42], plane constraints [19, 20], advanced loss functions [1], auxiliary depth completion [14], mixed-dataset training [2, 27, 28, 43], or modules customized for depth estimation [18, 21, 44]. In contrast, our work focuses on designing a better learning scheme without extra data or exogenous models: we adopt the fundamental regression loss and off-the-shelf networks without loss of generalization in meta-learning methodology. Most prior works train/test on the same dataset without validating cross-dataset performance. We instead devise zero-shot cross-dataset evaluation protocols using recent high-quality synthetic and real captures to validate generalization from meta-initialization.\nGradient-based Meta-Learning. Meta-Learning principles [16] illustrate an oracle about learning how to learn, especially useful on domain adaption, generalization, and few-shot learning. Popular gradient-based algorithms such as MAML [8] and Reptile [24] are formulated as bilevel optimization problems using a base- and meta-optimizer. MAML uses gradients computed on the query set to update the meta-parameters. Reptile does not distinguish support and query sets and simply samples data from a task distribution for inner-loop exploration. We refer readers to [17] for a survey on algorithms.\nThe majority of meta-learning studies in vision community focuses on image [5, 25, 50] or pixel-level classification [3, 13, 22]. One pioneer [10] investigates single-image pose regression that naively regresses rotation angles for one synthetic object in each image. The study may be far from real use.\nFew works use meta-learning for depth but only on driving scenes with a much different problem setup [35, 38, 47, 48]. Many perform online learning and adaptation using stereo [47] or monocular videos [38, 48] by enforcing temporal consistency. They require affinity in nearby frames and meta-optimize within a single sequence. Our problem is arguably harder due to the pure single-image setting in highly diverse indoor structures. [35] works on single images but requires multiple driving datasets to build tasks and train on. We do not require multiple training sets, and we find their method is limited for indoor scenes by experiments."}, {"title": "III. METHODS", "content": "Definition. Single-image depth prediction learns a function $f_\\theta: I \\rightarrow D$, parameterized by $\\theta$, to map from imagery to depth. A training set $(I_{train}, D_{train})$, containing images"}, {"title": "B. Meta-Initialization on Depth from Single Image", "content": "Meta-Learning stage. In the first meta-learning stage, we adopt a meta-optimizer and a base-optimizer. In each meta-iteration, a fine-grained task B contains K samples that are sampled from the whole training set: $(I_k, D_k) \\sim (I_{train}, D_{train}), \\forall k \\in [1, K]$. Then we take L steps to explore gradient directions that minimize the regression loss, $L_{reg}$. We perform online augmentation, Aug, at each exploration step, including color jittering and left-right flip to craft multiple samples for a task. We get $(\\theta_{expl}^1, \\theta_{expl}^2 ...., \\theta_{expl}^{OL})$ from\n$\\theta'_{expl} \\leftarrow \\theta_{prior} - \\alpha \\nabla_{\\theta} \\Sigma_{k\\in [1,K]} L_{reg}(Aug(I_k), D_k; \\theta_{expl}^{prior})$.\nTo avoid over-fitting specific to gradient-based meta-learning [41] and improve generalization, after the inner steps, we do task augmentation, including mix-up and channel shuffle. We sample another fine-grained task B' and linearly blend B and B' at the bottleneck $\\mathcal{B} = f_E(I)$ after encoder $f_E$ and interpolate the depth groundtruth.\n$\\mathcal{B}_m = \\lambda_k f_E(I_k) + (1 - \\lambda_k) f_e(I'_k), D_m = \\lambda_k D_k + (1 - \\lambda_k)D'_k$,\nwhere $\\forall k \\in [1, K]$ and $\\lambda_k \\sim Beta(0.5,0.5)$ following the common mixup [46]. $\\mathcal{B}_m$ is passed into the decoder head supervised by $D_m$ with loss $L_{reg}$. We also perform channel shuffle at B's bottleneck $\\mathcal{B}$ with a channel size of $p$ by randomly choosing a subset of channels in $\\mathcal{B}$ to be replaced by the same channels in $\\mathcal{B}'$. After shuffles, we get $\\mathcal{B}^{cs}$, which is passed into the decoder head and supervised by B's depth groundtruth using the loss $L_{reg}$:\n$\\mathcal{B}^{cs} = r_k \\mathcal{B} + (1 - r_k) \\mathcal{B}' , r_k \\sim Ber(0.95)$,\nwhich means on average p/20 channels are shuffled with $\\mathcal{B}'$ by Bernoulli random variables, and we empirically find more shuffles can break down the representations of $\\mathcal{B}$. After inner loops and getting $\\theta_{expl}^{OL}$, either channel shuffle or mix-up is chosen with equal chances to use and optimize to get $\\theta_{aug}^{OL}$. Subsequent to the inner steps and task augmentation, we update the meta-parameters in Reptile style [24], i.e., following the explored weight updates in the inner steps\n$\\Theta_{meta} \\leftarrow \\Theta_{meta} - \\beta(\\Theta_{meta} - \\Theta_{aug}^{OL})$,\nwhere $\\alpha$ and $\\beta$ are respective learning rates (lr), and i and j denote inner and meta-iterations. After the last meta-iteration, we obtain meta-learned weights as the depth prior $\\theta_{prior}$.\nCompared with MAML [8], we find Reptile more suitable for training fine-grained tasks. In Reptile's paper, it is designed without support and query split, and thus it inherently does not require multiple samples in a task, which matches our fine-grained task definition. Next, first-order MAML computes gradients on the query set at the last inner step to update meta-parameters. Yet, without simple augmentation of color jittering and flipping, only one sample exists in each fine-grained task, and each fine-grained task differs greatly by random sampling from the dataset. Thus, if taking exploration on a support split and computing gradients on the query split, but the support and query samples are sampled from different scenes without any similarity, the gradients are nearly random and prevented from converging. This contrasts with video sequence as tasks, where they use MAML by exploiting affinity between frames [38, 48]. Reptile does not require"}, {"title": "C. Strategy Explanation", "content": "Meta-Initialization. For each meta-iteration, the base-optimizer explores its neighborhood with L steps. Compared to the usual single-step update, the meta-update first takes L-step amortized gradient descent with a lower learning rate to delicately explore local loss manifolds. Then it updates meta-parameters by direction from the inner steps but with a step size $\\beta$ towards $\\theta^\\prime$. The meta-learned $\\Theta_{prior}$ may underfit a training set since it does not wholly follow optimal gradients but with a $\\beta$ for control. However, it also forces the inner exploration to reach a better understanding and avoid anchoring on seen RGB-D local cues.\nProgressive learning perspective. The above strategy can be seen as progressive learning. The first-stage meta-learning avoids anchoring on seen local cues and gets coarse but smooth depth."}, {"title": "IV. EXPERIMENTS AND DISCUSSION", "content": "Aims. We validate our meta-initialization with four questions. Q1 Can meta-learning improve performances on limited scene-variety datasets (Sec. IV-A)? Q2 What improvements can meta-initialization bring compared with the most pop-ular ImageNet-initialization (Sec. IV-B)? Q3 How does meta-initialization help zero-shot cross-dataset generalization (Sec. IV-C)? Q4 How does better depth helps learning 3D representations (Sec. IV-D)?\nDatasets: We introduce the adopted datasets as follows.\n\u2022 Hypersim [29] has high scene variety with 470 synthetic indoor environments, from small rooms to large open spaces, with about 67K training and 7.7K testing images.\n\u2022 HM3D [26] and Replica [34] have 200K and 40K images, where we use data rendered in SimSIN [40]. HM3D has 800 scenes with high scene variety, and Replica has low scene variety with 18 overlapping scenes.\n\u2022 NYUv2 [33] contains 464 real indoor scenes with 654 testing images but with limited camera viewing direction.\n\u2022 VA [40] as a test set has 3.5K photorealistic renderings with arbitrary camera viewing directions.\nTraining Settings. ResNet and ConvNeXt are backbones to extract bottleneck features. We build a depth regression head following [12] that contains 5 convolution blocks with skip connection. Each convolution block contains a 3\u00d73 convolution, an ELU activation, and a bilinear 2\u00d7 upsampling layer. Channel-size of each convolution block is (256, 128, 64, 32, 16). Last, a 3\u00d73 convolution with a sigmoid activation is used to get 1-channel output depth maps. We set N = 5, L = 4, K = 32, (\u03b1, \u03b2) = (10-3,0.5) for ResNet, and (\u03b1, \u03b2) = (5 \u00d7 10\u22124,0.5) for ConvNeXt. At the supervised learning stage, we train models with a learning rate (lr) of 3 \u00d7 10-4 till convergence. Input size to the network is 256\u00d7256. L2 loss is used as Lreg.\nMetrics. Error metrics (in meters if having physical units): Mean Absolute Error (MAE), Absolute Relative Error (Ab-sRel), Root Mean Square Error (RMSE), Scale Invariant"}, {"title": "A. Meta-Learning on Data with Limited Scene Variety", "content": "We first examine the first-stage meta-learning on data with limited scene variety. We train N = 15 epochs of the first-stage meta-learning with (\u03b1, \u03b2) = (10-4, 0.5) and compare with the direct supervised learning by a learning rate lr = 10-4 with an equivalent update epochs of NLB = 30, where we also use the early stopping to prevent explicit overfitting. We do not use the online and task augmentation in the meta-learning. ResNet50 is used. The other hyperparameters are the same as given in Training Settings. Fig. 2 shows fitting to training data. Replica with limited scene variety is used to verify gains in low-resource situations. From the figure, the meta-learning is capable of identifying near/ far fields without irregularity, where the direct supervised learning struggles. Under this low-resource case, the meta-learning still induces a better image-to-depth mapping that delineates object shapes, separates depth-relevant/-irrelevant cues, and shows flat planes where rich depth-irrelevant textures exist. The observation follows the explanations in Sec. III-C.\nWe next numerically examine generalization to unseen scenes when training on data of different-level scene variety. HM3D (high-variety) and Replica (low-variety) are used as training sets, and VA is used for testing. Table I shows that models trained by the first-stage meta-learning substantially outperform the direct supervised learning with 16.2%-27.8% improvements. The advantage is more evident when trained on data with low scene variety. Table I further investigates the batch size K, and we find that smaller batch sizes lead to less generalization and higher variance in parameter update. Following Table I without augmentations, we study how the augmentations improve generalization in Table II. Applying the three techniques leads to the best results that further boost the performances of K = 32 meta-learning in Table I.\nComparison to other learning strategies to learn a prior. We first compare meta-initialization with simple pretraining with a strong weight decay (WD-pre), which is conventionally used to smooth weight updates, and gradient accumulation (GA) with L steps, which also learns weight updates from multi-step forwards. WD-pre baselines: We replace the first-stage meta-learning with supervised learning using a stronger weight decay (wd) and the same learning rate as the second stage. The regular second-stage supervised learning follows. GA baseline: We replace the meta-learning dual loop with supervised learning using gradient accumulation, which accumulates gradient for L = 4 steps and then updates the"}, {"title": "B. Meta-Initialization v.s. ImageNet-Initialization", "content": "We next examine the full Algorithm 1, obtain higher-quality depth from the subsequent supervised stage, and go beyond limited resources to train on higher scene-variety datasets. Intuitively, higher scene variety may diminish meta-learning's advantages in few-shot/low-resource learning [6]. However, such studies are necessary to validate meta-learning for a realistic purpose since depth estimators are practically trained on diverse environments. Comparison is drawn with baselines of the direct supervised learning without meta-initialization and begins from the standard ImageNet-initialization."}, {"title": "C. Zero-Shot Cross-Dataset Evaluation", "content": "Protocol and evaluation. To faithfully validate a trained model in the wild, we design protocols for zero-shot cross-dataset inference that also aims for sim-to-real purposes. High scene-variety and large-size synthetic datasets, Hypersim and HM3D, are used as the training sets. Real or more photorealistic VA, Replica, and NYUv2 serve as the test set, and their evaluations are capped at 10m. $D_{gt}$ and $D_{pred}$ are groundtruth and predicted depth. We use median-scaling in the protocol to compensate for different camera intrinsics, following common unsupervised or zero-shot depth evaluation protocols [11, 12, 43, 45], which computes a ratio $\\gamma$ = median($D_{gt}$) / median($D_{pred}$) and multiply it with the prediction to align with groundtruth scale in the evaluation. In Table V, compared with the ImageNet-initialization, the meta-initialization consistently improves in nearly all the metrics, especially $\\delta_1$ (on average +1.74 points).\nMeta-Initialization as plugins. To show wide applicability, we plug our meta-initialization into many recent high-performing architecture specialized for depth estimation, including BTS [19], DPT (hybrid and large size) [27], DepthFormer [21], AdaBins [1], GLPDepth [18], and NDDepth [32]. Comparison in Table VI shows using our meta-initialization consistently improves their original performances, giving them higher generalizability on zero-shot cross-dataset inference. Our meta-initialization is useful, especially since it only needs a few lines of code changes to modify learning schedules as a simple but effective piece."}, {"title": "D. Better Depth Supervision in NeRF", "content": "We show that more accurate depth from meta-initialization can better supervise the distance d a ray travels in NeRF. d is determined by the volumetric rendering rule [7]. In addition to the vanilla pixel color loss, we use distance maps"}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "This work investigates how meta-learning helps in pure single-image depth estimation and closely analyzes improve-ments. Meta-learning can learn smooth depth from global context (IV-A). It is a better initialization to obtain higher model generalizability verified on intra-dataset, zero-shot cross-dataset, and 3D representation evaluations (IV-B, IV-C, IV-D). From depth's perspective, this work studies an effective learning scheme to gain generalizability and validates with the proposed cross-dataset protocols. From meta-learning's perspective, this work proposes fine-grained task for a challenging pure single-image setting and studies a complex and practical goal of pixel-level real-valued regression.\nDiscussion: large foundation models. Foundation models generally require a large corpus of pretrained data and still need a finetune set to adapt to the downstream task. In this case, another RGB-D dataset is required for adapta-tion. In contrast, we only require an RGB-D set. If the size of a finetune set is not large enough, a large model may easily overfit by overparameterization and show worse generalization. We experiment finetuning foundation models using ConvNeXt-XXLarge and ViT-L/14 and compare with our meta-initialization. They are CLIP weights as initial parameters and further tuned on ImageNet22K. Finetuning those models does not win over our +Meta on Replica \u2192 VA and only show marginal gain on HM3D \u2192 VA."}]}