{"title": "DePrompt: Desensitization and Evaluation Personal Identifiable Information in Large Language Model Prompts", "authors": ["Xiongtao Sun", "Gan Liu", "Zhipeng He", "Hui Li", "Xiaoguang Li"], "abstract": "Prompt serves as a crucial link in interacting with large language models, widely impacting the accuracy and interpretability of model outputs. However, acquiring accurate and high-quality responses necessitates precise prompts, which inevitably pose significant risks of personal identifiable information (PII) leakage. Therefore, this paper proposes DePrompt, a desensitization protection and effectiveness evaluation framework for prompt, enabling users to safely and transparently utilize large language models. Specifically, by leveraging large model fine-tuning techniques as the underlying privacy protection method, we integrate contextual attributes to define privacy types, achieving high-precision PII entity identification. Additionally, through the analysis of key features in prompt desensitization scenarios, we devise adversarial generative desensitization methods that retain important semantic content while disrupting the link between identifiers and privacy attributes. Furthermore, we present utility evaluation metrics for prompt to better gauge and balance privacy and usability. Our framework is adaptable to prompts and can be extended to text usability-dependent scenarios. Through comparison with benchmarks and other model methods, experimental evaluations demonstrate that our desensitized prompt exhibit superior privacy protection utility and model inference results.", "sections": [{"title": "Introduction", "content": "The emergence of Large Language Models (LLMs) such as GPT-4 (Brown et al., 2020) has profoundly transformed the way we interact with technology. This transformation spans from intelligent assistants to customized content generation, showcasing their significant impact on our everyday lives and the field of natural language processing (NLP) (Young et al., 2018). Specifically, the role of prompts in fine-tuning (Lester et al., 2021) and applying LLMs is crucial. Prompts provide context and task guidance to the models, directing them to learn specific tasks and contexts, thereby influencing the way the models learn from data and generate results (Prasad et al., 2023). To enhance the quality of interaction with LLMs, whether in prompt tuning during the model training phase or prompt-based generation during the model inference phase, it is necessary to upload a large number of prompt samples for exploration and evaluation.\nHowever, prompts, as the link between users and LLMs, also bring about security and privacy risks. In security-sensitive scenarios, prompt-related security vulnerabilities mainly revolve around robustness (Liu et al., 2023c,b; Greshake et al., 2023) and privacy. While robustness can pose privacy threats (Li et al., 2023), this study focuses directly on researching privacy issues. Prompts may involve users' Personally Identifiable Information (PII) or sensitive information of organizations, directly leading to privacy breaches (Hong et al., 2023). Furthermore, LLMs have been shown to have the capability to memorize data through prompts, thereby resulting in indirect privacy leaks (Lukas et al., 2023). Privacy breaches can have serious consequences, especially in the case of PII leaks, which could lead to financial losses, susceptibility to social engineering attacks,"}, {"title": "Related works", "content": "In this section, we introduce some existing methods for prompt privacy anonymization, including PPDP and NLP, as well as some standard metrics for evaluating the protected text."}, {"title": "Privacy-preserving data publishing", "content": "PPDP is a privacy-enhancing technology designed to facilitate the public usage of data while preventing the disclosure of sensitive information (Gehrke, 2006), such as K-anonymity (Samarati, 2001) and differential privacy (Dwork, 2006). Hong et al. (Hong et al., 2024) proposed DP-OPT, the first private prompt generation mechanism. By transferring prompts through the Deep Language Network (DLN) and combining exponential differential privacy, they achieved competitive performance in generating private prompts. Similarly, Yu et al. (Yu et al., 2024) combined differential privacy with prompt tuning, with the additional focus on the annotation phase of prompt tuning. Therefore, they trained a private fine-tuning generator for prompt generation.\nThe above method only applies to structured datasets and cannot be tailored to individual prompt application scenarios. However, for prompt, semantics are more important than statistical distributions. Therefore, methods such as deletion, substitution, and generalization are still commonly used to protect personally identifiable information (PII) in textual content (Mamede et al., 2016). However, these methods often result in unacceptable utility loss of the data. Therefore, it is necessary to design a PPDP method suitable for individual prompts while maintaining a certain level of usability."}, {"title": "Natural language processing", "content": "NLP methods have not yet been applied to prompt privacy protection, primarily focusing on tasks such as detecting identifiers and privacy attributes. Liu et al. (Liu et al., 2019) designed a novel capsule-LSTM network to better capture entity-related information conveyed in clinical text, combining the strong expressive power of capsule networks with the sequential modeling capability of LSTM networks. Campanile et al. (Campanile et al., 2022) proposed an automatic clinical information de-identification method. This method first conducts advanced assessment of the amount and format of clinical record information, and subsequently performs detailed research on the overlap between selected clinical information types and protected health information (PHI). Meystre et al. (Meystre et al., 2014) proposed an automated solution using the BiLSTM-CRF model to remove private information from clinical records, effectively identifying protected health information from unstructured text. Liu et al. (Liu et al., 2023d) proposed the utilization of GPT-4 for medical text data processing and de-identification, though this implies the uploading of private data to a third party.\nAfter NLP recognition, entities are often assumed to be deleted or replaced, resulting in unnecessary loss of data utility. The definition of privacy is limited to predefined entity categories. When there is a need to dynamically change the privacy definition based on the context, data annotation and model training need to be re-performed, and privacy leakage risks still exist. Therefore, there is a need to dynamically define the concept of privacy, and to use PPDP methods appropriately after identifying privacy entities."}, {"title": "Evaluation metrics", "content": "Ni et al. (Ni et al., 2022) proposed a data anonymization evaluation framework for datasets in Internet of Things (IoT) scenarios. The framework assesses commonly used data anonymization algorithms based on privacy protection level, data utility, and performance. Pil\u00e1n et al. (Pil\u00e1n et al., 2022) proposed a novel benchmark and relevant evaluation metrics to assess the performance of text anonymization methods in the field of NLP. Additionally, they provided a new open-source annotated corpus. Benet et al. (Manzanares-Salor et al., 2022) proposed a re-identification attack on anonymous text, using state-of-the-art deep learning language models combined with background knowledge available to potential attackers, to provide an authentic assessment of risk disclosure.\nIn privacy measurement, existing evaluation metrics, such as single IR-based metrics, do not consider the relationship between privacy attributes and identifiers and the varying impact of different entities on the text. In utility measurement, most metrics focus on the overall dataset and cannot be tailored to individual data. Therefore, the existing evaluation metrics system is not applicable to prompt."}, {"title": "System and threat models", "content": "In this section, we first define the system and threat models of our framework."}, {"title": "System model", "content": "Our frameworl concentrates on the protection of PII in prompt, which consists of three entities, including a LLM user U, local protection mechanism for prompt DePrompt, and a LLM cloud service provider CSP, as shown in Figure 2. The commonly used symbols and corresponding definitions are listed in Table 1.\nLLM User. The LLM user provides prompts that can clearly and accurately express their intentions,"}, {"title": "Threat model", "content": "As shown in the Figure 3, in our threat model, CSP is considered honest-but-curious (Goldreich et al., 2019) and that the communication channel between local and cloud environments is susceptible to monitoring. In other words, Adversaries have access to the prompts submitted by U to the LLM CSP, as well as the LLM frameworks and interfaces used. However, they do not maliciously modify the prompts or the feedback results from the models. For a given model, we assumes that adversaries utilize white-box background knowledge for extracting and analyzing personal sensitive information, which is then applied to downstream tasks such as user profiling, thereby infringing upon individual privacy."}, {"title": "Preliminaries", "content": "This section begins with an introduction to fundamental concepts of data anonymization. Building on these concepts, we identify three critical attributes in Prompt-based data anonymization: semantic relevance, subject linkage, and entropy uncertainty, each essential for subsequent analysis. Additionally, we demonstrate the importance of LLM fine-tuning techniques as a significant tool for implementing attribute-based thinking within our framework."}, {"title": "Anonymization Key Terms", "content": "The current legal landscape, including legislations like the European General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA) in the United States, and China's Personal Information Protection Law (PIPL), typically mandates organizations to obtain explicit consent when collecting and processing personal data. Moreover, these regulations require appropriate measures such as encryption or anonymization of stored personal data to mitigate risks of data breaches and misuse. This underscores the significance of anonymization efforts, particularly in ensuring the security of personal data and complying with privacy laws. The following are some key terms pertaining to anonymization(Lison et al., 2021):\n\u2022 Direct Identifier refers to information that contains unique values capable of uniquely identifying an individual's identity. Examples of such information include ID card numbers, passport numbers, driver's license numbers, and more.\n\u2022 Quasi Identifier refers to indirectly identifiable information, while not capable of uniquely identifying an individual, can assist in confirming the identity of a specific person when combined with other information. There is no fixed type or scope for this information. Examples include gender, nationality, occupation, and more.\n\u2022 Confidential Attribute refers to highly sensitive and confidential information, the disclosure of which could have a significant impact on individuals. Examples include personal health conditions, religious beliefs, political views, and more.\n\u2022 Personally Identifiable Information refers to all information that can be used to identify, contact, or locate an individual. In this paper, we consider it as the union of Direct Identifiers, Quasi Identifiers, and Confidential Attributes."}, {"title": "Prompt-based anonymization attributes", "content": "Based on the requirement of anonymity and the characteristics of large language models, we propose three important features for PII in prompt: linkability(Kim et al., 2023), semanticity, and uncertainty. These features are incorporated into the DePrompt anonymization method, which is integrated throughout the entire anonymization framework."}, {"title": "Linkability", "content": "In using prompt, from the perspective of privacy leakage, the disclosure of privacy attributes alone may not necessarily pose significant risks. For example, when LLMs are employed in disease diagnosis, the symptom information contained in prompt cannot be directly linked to specific patients. Therefore, the crucial aspect of desensitization is to break the linkability between privacy attributes and identifiers.\nDefinition 1 Linkability in prompt. Let $L := {d_1,..., q_{n-1}, q_n,...,a_M}$ be M PII items relevant to a subject U. Each $d_m$ represents direct identifier, $q_m$ represents quasi identifier, and $a_m$ represents confidential attribute. Direct linkability refers to the scenario where $a_m$ can be associated with specific individuals U through specific $d_m$. On the other hand, Indirect linkability refers to the"}, {"title": "Semanticity", "content": "During the process of prompt desensitization, it is crucial to consider the correlation between different PII and semantics, as it determines the appropriate desensitization methods to be employed. In cases where there is little or weak correlation, methods such as deletion or generalization can be directly applied for data desensitization. However, if there is a strong correlation, desensitization methods with minimal semantic loss need to be utilized.\nDefinition 1 Semanticity in prompt. Let $S = {$s_1,s_2,...,s_K}$ be K semantic keywords and $L := {d_1, ..., q_{n\u22121}, q_n, ..., a_M}$ be M PII items from a prompt p. If a singular entity appears in both S and L, we refer to this entity as having privacy semantic relevance."}, {"title": "Uncertainty", "content": "The utilization of desensitization techniques such as generalization, K-anonymity, and differential privacy is fundamentally aimed at increasing the uncertainty of PII. This process can be understood as augmenting the information entropy of the data. By reducing data specificity, desensitization methods enhance the level of confusion in the data, thus fortifying the protection of individual identities and achieving the objective of privacy preservation.\nDefinition 3 Uncertainty in prompt. The $L := {d_1,...,I_{n\u22121},q_n, ..., a_M }$ represents PII vectors extracted from the prompt p. Uncertainty refers to the process of obfuscating confidential attributes $a_m$, thereby preventing the precise extraction of private information from p."}, {"title": "DePrompt", "content": "In this section, we first present our proposed framework DePrompt with an overview. Then the detailed processes and steps of DePrompt are introduced."}, {"title": "Overview", "content": "As mentioned above, prior to uploading to the CSP, the original prompt requires anonymization to mitigate the risk of privacy breaches. Unlike traditional data anonymization scenarios, anonymizing prompts necessitates ensuring their usability and precludes the use of conventional anonymization methods applicable to datasets, such as differential privacy or k-anonymity. Therefore, we propose DePrompt, a prompt anonymization framework deployable locally. The aim is to achieve the desensitization protection of PII within prompts while ensuring a certain level of prompt usability. Within DePrompt, we consider semanticity, linkability, and uncertainty to achieve a better balance between privacy and usability."}, {"title": "Construction of DePrompt", "content": "DePrompt primarily consists of pre-deployment fine-tuning of large language models and post-deployment semantic and privacy entity extraction, as well as adversarial generative desensitization. In the Figure 4, we illustrate the complete process of a prompt in DePrompt."}, {"title": "Large language model fine-tuning", "content": "Since DePrompt includes tasks such as scene recognition, privacy entity identification, and adversarial text generation, the model needs to possess the ability to comprehend complex scenarios and rich contextual information. It should also handle semantic associations and have the capacity to capture logical coherence between sentences, ensuring contextual consistency while generating adversarial text that is both perplexing and realistic. Therefore, we utilize LLM fine-tuning techniques to implement the model base for DePrompt, following the steps below:\nStep 1. Data Collection. From several publicly available prompt datasets, we gathered prompts with distinct scene characteristics. These were categorized into medical, daily, and financial scenes, and manually annotated for privacy entities.\nStep 2. Prompt Design. In light of the downstream tasks involved in DePrompt, it is necessary to design different prompts for subsequent fine-tuning. We extract the inputs from the collected prompts in order to restructure them. The specific formatting is illustrated in the Figure 5, with the top portion providing an example and the bottom portion outlining the precise format requirements.\nStep 3. LORA Fine-tuning. In this step, firstly, download the weights of the large model. Then, convert the downloaded model into the Hugging-Face format and expand the vocabulary. Finally, set the required number of layers for fine-tuning and utilize LORA for model training."}, {"title": "Entity extraction", "content": "Step 1. Semantic entity extraction. In this step, we utilize the TextRank algorithm (Mihalcea and Tarau, 2004) to construct a text as an undirected graph model. We employ graph ranking algorithms to calculate the importance of nodes, thereby extracting the top K keywords. This process enables us to obtain semantic entity vectors $S = {$s_1, $s_2,...,s_K}$, facilitating the analysis and comprehension of the semantic significance of the text.\nStep 2. Privacy definition. In DePrompt, we consider scenarios where large models are commonly used and where privacy requirements are high, encompassing daily, medical, and financial contexts. We have defined the following types of privacy entities:\nPERSON Personal statistical information, including name, age, gender, nationality, occupation, address, and employer.\nCODE Identification codes, including ID card numbers, employee ID numbers, residency permit numbers, and passport numbers.\nCONTACT Personal contact information, including personal phone numbers, email addresses, account details, and associated information.\nHEALTH Health status data, including medical history, allergy records, symptoms, and health examination results.\nMEDICAL Medical application data, including diagnostic outcomes, medication details, surgical records, and inpatient records.\nPAYMENT Payment data, including transaction records, expenditure amounts, and insurance information.\nASSET Financial asset-related information, including personal asset data, bank branch numbers, service point identifiers, fund codes, and securities codes.\nStep 3. Private entity extraction. In this phase, utilizing the locally fine-tuned large-scale model, we perform context recognition and privacy entity extraction based on the privacy definitions across various scenarios, resulting in privacy entity vectors $L := {d_1,..., q_{n-1}, q_n,\u2026\u2026\u2026,a_M}$."}, {"title": "Adversarial generative desensitization", "content": "As mentioned in Section 4.2, the aspects of semantics, linkability, and uncertainty are crucial in anonymous process. Leveraging the powerful text generation capabilities of LLM, we introduce adversarial generative anonymization as outlined in Algorithm 1.\nSemanticity (Line 1-5). After receiving the prompt query p from U, Locally extract S and L. We iterate through all the entities in L, and for semantically unrelated entities, traditional Scrub techniques such as deletion and masking can be employed to protect them.\nLinkability(Line 6-8). For semantically linked identifiers d and q, we leverage the powerful generative capabilities of fine-tuned LLM for Generative replacement. Imitating the prompt pattern for generation replacement, for instance, using \"Alice\" instead of \u201cJack\u201d. This generation disrupts the linkage between U and their private attributes $a_m$.\nUncertainty (Line 9-23). For semantically linked private attributes $a_m$, we initially employ a generative approach. Subsequently, we utilize Adversarial techniques to perturb these attributes using various special characters, rendering traditional Named Entity Recognition (NER) techniques incapable of automatically extracting the private attributes, while still enabling LLMs to comprehend and infer. We replace d, q, and a within the submitted prompt p by user U, and iterate the aforementioned process N \u2013 1 times. we concatenate a prompt that has been subjected to adversarial perturbation, containing the original private attributes, resulting in an N-length prompt vector pde, followed by a Shuffle operation. Finally, submit multiple prompt to LLM CSP to confuse the malicious attacker and get a LLM inference response."}, {"title": "Anonymization Evaluation", "content": "In this section, We develope a set of metric standards for privacy and usability concerning prompt input and large model inference output."}, {"title": "Privacy metircs", "content": "In the privacy measurement task, the approach outlined in research (Lukas et al., 2023) can be leveraged by employing advanced attacks on the prompt. Through the implementation of privacy attacks, potential privacy risks can be exposed, and the effectiveness of the adopted privacy protection measures can be assessed. In this paper, our primary focus lies in the utilization of attacks for PII extraction and identifier linkage."}, {"title": "Usability metircs", "content": "Anonymization methods inevitably lead to a loss of data utility. Effective anonymization methods strive to achieve lower utility loss and a high level of privacy protection. In evaluating the utility loss of prompts, it is important to consider prompt semantic loss, measures of usability in large model inference responses, and readability metrics.\nSemantic loss(SL). In the process of prompt desensitization, effectively measuring the semantic loss can help ensure that the model maintains an accurate grasp of task semantics even after desensitization. We using Sentence-BERT (Reimers and Gurevych, 2019), a pre-trained model. Based on the Siamese network structure, this model is trained by maximizing the loss of semantic similarity.\nInference loss(IL). The most straightforward and effective measure of utility is to compare the inference results of the large model before and after desensitization. This can help determine whether desensitization has affected the model's semantic understanding. We utilize cosine similarity for this purpose, as it is unaffected by dimensionality and is suitable for processing sparse vectors. The formula for calculating cosine similarity is as follows:\n$Cosine Similarity(A, B) = \\frac{AB}{|A| x |B|}$\nReadability loss(RL). We use Perplexity (PPL) as the readability metric. By comparing the PPL of the large model's inference results before and after desensitization, we can understand the impact of desensitization on the model's generation results, particularly with regard to semantics and fluency. The formula for Perplexity (PPL) is as follows:\n$PPL = 2^{-\\frac{1}{N} \\sum_i log P(w_i)}$"}, {"title": "Experiment evaluation", "content": ""}, {"title": "Experiment setup", "content": ""}, {"title": "Implementation", "content": "All programs in our experiments are implemented using Python language (version 3.8.18). The fine-tuning of the llama-2 model is conducted on various datasets based on PyTorch (version 1.13.1), Transformers (version 4.28.1), and PEFT (version 0.6.3).\nCSP used the GPT-3.5 API for our experiments.\nDePrompt runs on a Ubuntu 22.04 system server equipped with a V5000 GPU with 24GB of VRAM, alongside a 6-core processor and 24GB of RAM."}, {"title": "Datasets", "content": "In our experiments, the following datasets and models were utilized:\nHC3 is a corpus of Human-ChatGPT comparisons that aims to investigate how close ChatGPT is to Human Experts. To this end, it collect about questions from various public question answering datasets (e.g., medicine, law, finance QA) and the corresponding human answers and ChatGPT answers. The English samples in HC3 contain 24K questions, 59K human answers, and 27K Chatgpt answers (Guo et al., 2023).\nAlpacaGPT4 is deemed as an optimized version of Alpaca dataset, it includes 52K instruction tracking data samples for LLM fine-tuning. Compared to the original Alpaca dataset, AlpacaGPT4 also uses text-davinci-003 to complete the prompts, but generating the completions with GPT-4. Thus, the responses are of higher quality and lenght (Peng et al., 2023).\nUltraChat is an open-source, large-scale, multi round dialogue data project supported by Turbo APIs, aimed at promoting the construction of powerful language models with universal dialogue capabilities. It consists of three parts: Questions about the World, Writing and Creation, and Assistance on Existing Materials, including 1.4M examples of technology, art, entrepreneurship, and writing (Ding et al., 2023).\nSwype-instruct is a combination of multiple sources, including the GPT4All dataset, the Alpaca dataset from Stanford, custom generation using AllenAI augmentation, and some dataset augmentation from open-source Meta datasets. The dataset contains 0.88M instruction data for training and evaluating language models on various tasks (Srinivas, 2023)."}, {"title": "Named Entity Recognition", "content": ""}, {"title": "Scene recognition", "content": "From the aforementioned datasets in section 7.1.2, we collected 1000 data entries for each of the three scenarios: medical, everyday, and financial. Each set of test data was recorded and categorized as correct or incorrect, denoting accurate or inaccurate predictions, respectively. Subsequently, the prediction accuracy for each scenario and the overall accuracy were computed.\nFigure 6 compares our scheme with the BERT (Vaswani et al., 2017) and BERT_LSTM (Pandey and Singh, 2023) models, revealing that our scheme demonstrates superior accuracy in scene recognition tasks. The overall accuracy of our model in scene recognition is 98%. Notably, the highest predictive accuracy is achieved in the medical domain at 100%, attributed to the prevalence of specialized medical terminologies in this context. However, the recognition accuracy for everyday scenes is relatively lower, owing to the complexity and broad scope of textual data within this category."}, {"title": "Private entity extraction", "content": "In this experiment, our aim is to validate the efficacy of our approach in privacy entity recognition tasks. Initially, we assess the accuracy of our approach in defining privacy in section 5.2.2 and compare it with the BERT_CRF model and the BERT_LSTM_CRF model (Liu et al., 2023a). Subsequently, we proceed to perform experimental testing using the benchmark dataset (Pil\u00e1n et al., 2022).\nWe collected 1400 instances from the datasets in section 7.1.2, ensuring a minimum of 200 instances for each privacy entity type, and annotated them manually. As indicated in the Table 2, our approach outperforms the BERT and BERT_LSTM models in the accuracy of privacy entity recognition. The overall accuracy of our privacy entity recognition model's inference is 96%. The predicted accuracy for medical funding and payment privacy data is relatively low, possibly due to a scarcity of medical payment data in the dataset, which the model did not learn well.\nWe downloaded the benchmark dataset for experimental comparison, focusing on three methods outlined in the benchmark: Neural NER (ROBERTa), Presidio (default), and Presidio (+ORG), with specific results shown in Figure 7(a). From the figure, it is evident that our approach exhibits a certain advantage in both accuracy and recall, demonstrating its effectiveness on the benchmark dataset. We also analyzed the predicted accuracy of various privacy entities as defined in the benchmark, as depicted in Figure 7(b)."}, {"title": "Privacy and Utility Evaluation", "content": ""}, {"title": "Overall Performance", "content": "In this subsection, we compare our schemes with traditional anonymization methods, including Deletion, Tokenization, Masking, and Generalization, using the privacy and utility metrics defined in section 6. Examples of prompts after anonymization are shown in the Figure 8. We utilize the data described in the preceding section 7.2.2 and process the prompts using various anonymization methods before uploading them to the GPT-3.5 API to obtain inference results.\nAs illustrated in the Table 3, we set the relevant hyperparameters of our scheme to K=3 and N=5 for comparison with other methods. In terms of privacy assessment, Deletion yielded the optimal impact, while Generalization displayed the poorest performance. This discrepancy arises from the direct removal of PII in Deletion, contrasting with Generalization's retention of privacy attributes. Although our method did not achieve the best privacy protection effect, our adversarial generation approach can thwart an attacker's ability to ascertain the veracity of attributes within the privacy vector. Moreover, our method effectively disrupts the linkage between identifiers and attributes, thereby further reducing privacy risk.\nIn terms of prompt usability, our approach has achieved state-of-the-art results. Despite Deletion and Masking yielding the best privacy protection effects, the desensitized prompts generated by these methods generally lack usability, a similar issue observed with the Tokenization approach. While the Generalization method has achieved a certain balance between privacy and usability, it falls slightly short in both aspects compared to our approach. Therefore, these results show can be argued that our approach achieves the best current balance between privacy and usability."}, {"title": "Conclusion", "content": "In this paper, we investigate the tradeoff between privacy and utility in Prompt-based data anonymization. Specifically, our study focuses on three fundamental characteristics\u2014semanticity, linkability, and uncertainty. By leveraging fine-tuned LLM, we propose an adversarial generative desensitization approach for anonymizing the prompt. Additionally, we introduce an anonymization and evaluation framework specifically designed for prompts. Our experimental results unequivocally demonstrate the superiority of our proposed framework in terms of entity recognition performance and its ability to strike a favorable balance between privacy and utility. Moving forward, expanding this framework to a wider range of contexts where semantic understanding is crucial presents a compelling avenue for future exploration."}]}