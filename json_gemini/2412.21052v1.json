{"title": "Towards Effective Discrimination Testing for Generative AI", "authors": ["Thomas Zollo", "Nikita Rajaneesh", "Richard Zemel", "Talia Gillis", "Emily Black"], "abstract": "Generative AI (GenAI) models present new challenges in regulating against discriminatory behavior. In this paper, we argue that GenAI fairness research still has not met these challenges; instead, a significant gap remains between existing bias assessment methods and regulatory goals. This leads to ineffective regulation that can allow deployment of reportedly fair, yet actually discriminatory, GenAI systems. Towards remedying this problem, we connect the legal and technical literature around GenAI bias evaluation and identify areas of misalignment. Through four case studies, we demonstrate how this misalignment between fairness testing techniques and regulatory goals can result in discriminatory outcomes in real-world deployments, especially in adaptive or complex environments. We offer practical recommendations for improving discrimination testing to better align with regulatory goals and enhance the reliability of fairness assessments in future deployments.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) classification models have repeatedly been shown to be unfair, for example falsely predicting recidivism at a higher rate for Black defendants than white ones (W Flores et al., 2016) or failing to recognize faces with dark skin at a much higher rate than those with light skin (Buolamwini and Gebru, 2018). To prevent such harms from ML decision-making systems in certain high-stakes domains, such as employment, housing, and credit, traditional discrimination laws can be applied to regulate their use. This is because ML classification models often make allocative decisions, such as determining who is offered a job, or approved for a loan, matching traditional anti-discrimination frameworks. For such deployments, existing principles like the disparate impact doctrine can be applied to prevent unjustifiable disparities in allocations across demographic groups (Caro et al., 2023; Gillis, 2021). A significant body of ML research attempting to measure fairness in these models can be readily adapted to support these regulatory efforts, e.g., testing whether various"}, {"title": "2. Related Work", "content": "Various forms of discriminatory behavior have been discovered in GenAI systems, from differences in rates of toxic speech when describing demographic groups (Yang et al., 2023), to performance drops when encountering minority dialects (Deas et al., 2023), to representational harms, such as including far fewer women in generative image prompts for occupations like \u201cdoctor\u201d or \u201clawyer\u201d (Zhou et al., 2024), among many other noted issues (Bianchi et al., 2023; Haim et al., 2024; Kotek et al., 2023; Wan et al., 2023). However, partially due to the fact that the outputs of generative AI systems do not easily map on to popular algorithmic fairness definitions like equal opportunity or equalized odds (Hardt et al., 2016), which are particular to classification problems, there is little consensus on a standardized approach to measuring discrimination in GenAI systems. Current popular methods of measuring discrimination in GenAI systems may probe the associations between protected attributes and known stereotypes (Ghosh and Caliskan, 2023; Prates et al., 2020; Stanovsky et al., 2019), examine the relative ease with which toxic statements can be induced about different groups (Han et al., 2024; Perez et al., 2022; Samvelyan et al., 2024), or search for representational biases in distributions of generated content (Bianchi et al., 2023; Cho et al., 2023; Luccioni et al., 2023). Further technical literature relevant to each of our case studies is cited throughout Section 4.\nAnother relevant stream of work has highlighted the brittle nature of fairness testing in AI systems generally (Barrainkua et al., 2023; Black and Fredrikson, 2021; Cooper et al., 2023; Giguere et al., 2022), underscoring the difficulty of ensuring acceptable behavior in deployment. For example, research has shown how the fairness behavior of deep models can change based on distribution shift (Ding et al., 2021), small within-distribution differences in train/test split (Ferry et al., 2022), or even the order in which they see their training data (Ganesh et al., 2023). Black et al. (2024) point to how such instability can lead to d-hacking, where model practitioners can, intentionally or unintentionally, search for or reach a fairness testing schema that produces results which suggest low bias but do not generalize to deployment-time behavior. In this work, we demonstrate how challenges unique to GenAI systems, from their output flexibility to complex interaction capability, increase the modes of d-hacking possible and magnify those that exist, creating a significant challenge for regulators aiming to prevent discrimination in their use.\nAnother recent and related stream of literature focuses on the regulatory challenges associated with ensuring fairness in generative AI (GenAI) and the ways in which GenAI applications intersect with existing anti-discrimination laws. This literature highlights how existing doctrines in the U.S. and Europe are insufficient to address the harms that can arise from AI-generated content (Hacker, 2018; Xiang, 2024), and emphasize the need for developing effective testing and liability frameworks (Diega and Bezerra, 2024). Our work focuses specifically on the methods of bias assessment and their robustness, which are essential foundations for any effective testing and liability framework."}, {"title": "3. GenAI Discrimination Regulation", "content": "Emerging regulatory approaches to GenAI with respect to fairness and discrimination fall into two broad categories: (1) the application of traditional discrimination law and (2) new AI-specific regulatory frameworks. We will next examine each of these approaches in detail, and then discuss"}, {"title": "3.1. GenAI Under Traditional Anti-Discrimination Law", "content": "Traditional U.S. discrimination law forms a patchwork of federal, state, and sometimes municipal policy. Each law focuses on a specific domain, such as employment (Title VII, 1964), credit (ECOA, 1974), or housing (FHA, 1968), and applies to both government and private actors. Two core legal doctrines are central to many of these laws: disparate treatment and disparate impact. The disparate treatment doctrine aims to prevent intentional or direct discrimination by prohibiting decisions\u2014such as who to hire or whether to approve a loan\u2014on the basis of a protected characteristic like race or gender. In the context of algorithmic systems, this is often understood to mean that these demographic attributes should not directly be an input feature to the decision-making process (Gillis, 2021). The disparate impact doctrine is aimed at preventing facially neutral decisions that create unjustifiable disparities across demographic groups in the allocation of employment, housing, or credit opportunities, among other domains. For instance, an employer using an ML model to screen job applicants might find that the system selects male candidates at a higher rate, even though the algorithm is not explicitly screening for gender, triggering scrutiny under disparate impact law. While some disparate impact can be justified based on business objectives, the employer would still be required to stop using the tool if a less discriminatory alternative exists that meets the same business objective (Gillis et al., 2024).\nWhen GenAI is used to make allocative decisions\u2014e.g., who to hire or whether to approve a loan-in a way that mirrors traditional decision making or ML classifiers, these existing discrimination laws can be directly applied. For example, if some large language model (LLM) like GPT-4 was used to screen resumes and directly make decisions on which candidates should be offered an interview, the disparate impact doctrine could be applied as outlined above. However, many GenAI applications do not directly result in allocative decisions that would trigger existing discrimination laws, creating the need for new regulation to capture the concerns created by embedding these powerful models in broader systems where concerns about fairness arise in less tangible ways."}, {"title": "3.2. Emerging Discrimination Regulation for GenAI.", "content": "The wide range of applications enabled by the multimedia input/output capabilities of GenAI systems create new concerns for regulators beyond resource allocation, for example representational harms and the production of toxic content towards protected groups. Such harms are harder to map onto traditional discrimination frameworks, and thus in these more complex scenarios, the second category of regulation-emerging AI frameworks\u2014becomes crucial. Among these frameworks, some including the EU AI Act (European Union, 2023) have been enacted as binding law, while others such as the AI Bill of Rights (White House, 2022) and the NIST AI Guidelines (NIST, 2023) provide soft regulatory guidance. Other relevant efforts, such as Executive Order 14110 (White House, 2023a), provide a general framework that directs federal agencies to develop more specific guidelines, while certain frameworks are exclusively focused on regulating particular federal agencies' use of AI (OMB, 2024). Further collaborative approaches to regulation are also emerging, such as private industry voluntary commitments, as reflected in the recent Biden-Harris Administration commitment from industry players to manage AI risks (White House, 2023b) and the EU AI Pact (European Commission, 2024), which include commitments to guard against bias and unfairness. Various regulatory frameworks and voluntary guidelines are also emerging outside the EU and U.S. In Canada, the proposed Artificial Intelligence and Data Act (AIDA) seeks to regulate high-impact AI systems to ensure safety and fairness (Canada, 2024), while the a voluntary code of conduct of GenAI systems establishes principles for achieving fair and equitable outcomes during AI development and deployment (Canada, 2023). Similarly, in the UK, the Model for Responsible Innovation, developed by the Department for Science, Innovation and Technology (DSIT), offers soft guidance for responsible AI practices (DSIT, 2024).\nA key focus shared across these various frameworks and documents is the need to assess and mitigate discrimination and unfairness AI deployments. The White House's AI Bill of Rights (White House, 2022), for instance, mandates that automated systems must not \u201ccontribute to unjustified different treatment or impacts\u201d based on race, color, ethnicity, and other protected characteristics, a requirement echoed by other regulatory frameworks in the U.S. and Europe. For GenAI regulation, the general backbone of these proposals is the requirement to audit and monitor for AI risks (White House, 2023a). In particular, the OMB memo (OMB, 2024) requires that agencies \u201cestablish adequate safeguards and oversight mechanisms\" for GenAI systems. Similarly, Article 55 of the EU AI Act (European Union, 2023) requires that those deploying GenAI with systemic risk perform evaluations with \"standardised protocols and tools reflecting the state of the art, including conducting and documenting adversarial testing of the model.\u201d The oversight and testing guidance provided in these emerging frameworks relate to the responsible use of AI, which includes fairness and discrimination considerations. The NIST guidelines (NIST, 2024) more explicitly relate testing and monitoring to address harmful bias and recommend fairness assessments to quantify potential harms.\nOne particularly difficult challenge in regulating GenAI systems is determining liability for discriminatory outputs, particularly given the frequent separation of roles between developers, who design the systems, and deployers, who implement them in practice. Regulatory frameworks such as the EU AI Act address this by assigning obligations to both parties: developers must mitigate biases during training, while deployers are responsible for monitoring system performance and reporting issues. Though liability under civil rights law in the U.S. has traditionally focused on deployers"}, {"title": "3.3. Misalignment Between Regulatory Goals and Fairness Testing Methods", "content": "Although recent regulatory frameworks mark meaningful initial progress, significant areas of misalignment exist between regulatory goals and fairness testing methods that hinder the development of specific, effective anti-discrimination policy for GenAI systems. Some of these areas of misalignment stem from the policies themselves, and incompatible or inflexible legal structures: for example, these frameworks fail to define clear metrics and testing protocols for achieving fairness under complex deployment conditions, creating large practitioner discretion, increasing variability in already flexible and unstandardized GenAI fairness measurement (Bowman and Dahl, 2021; Raji et al., 2021), and potentially leading to uninformative (yet regulation-compliant) fairness tests. Key questions, such as which deployment conditions should guide evaluations, how liability applies when users modify models, and how to apply traditional discrimination law to generative outputs in addition to allocative decisions, remain unanswered. This ambiguity creates room for overly discretionary fairness tests that may comply with regulations but provide little actionable insight into discriminatory risks.\nWhile regulators bear the ultimate responsibility for translating high-level guidance into actionable, detailed protocols, some areas of misalignment stem from a lack of technical ability to meet regulatory goals. In fact, recent policy acknowledges the need to evaluate GenAI systems under conditions that \u201cmirror as closely as possible the conditions in which the AI will be deployed\" (OMB, 2024). However, current methods for detecting discrimination often fail to account for the complexities of real-world applications. Existing fairness testing approaches rely on imprecise or opaque metrics that may not reflect downstream outcomes, and fail to capture the dynamic and adaptive nature of GenAI systems. For example, these methods are typically confined to single-turn interactions with fixed hyperparameters, ignoring the multi-turn scenarios (Chao et al., 2024) and user-driven parameter modifications common in real-world deployments. Further, techniques like red teaming, frequently mentioned in policy documents, remain insufficiently standardized and may yield variable or subjective outcomes. In light of this, we contend that progress in technical methodologies for bias assessment must precede policy-making efforts to enable reliable discrimination testing.\nIn the rest of this paper, we explore how this misalignment between regulatory goals and fairness testing methods may manifest in real applications, and highlight avenues for future work aligning technical practices with regulatory goals in order to improve fairness assessments and ensure GenAI systems operate responsibly in practice."}, {"title": "4. Case Studies in Discrimination Testing", "content": "In this section, we present four case studies showing how the gap between popular testing approaches and regulatory goals can lead to scenarios where applying existing tools to meet guidelines does not prevent discriminatory behavior. For each case study, we discuss relevant legal issues, present an illustrative experiment, and offer suggestions on how future research may mitigate such concerns. Our case studies and experiments are not meant to argue for particular fairness methodology or evaluation techniques. Rather, they are meant to demonstrate how gaps between regulation and methodology can"}, {"title": "4.1. (Mis-)Applying Traditional Fairness Notions to GenAI Systems", "content": "In our first case study, we highlight two of the most significant challenges in detecting discrimination in complex GenAI deployments: (1) the lack of a clear mapping from model output to an allocative decision relevant to anti-discrimination law, as discussed in the previous section; and (2) the difficulty in measuring the quality of text or other non-classification output, especially with a single scalar. At a time when massive resources are put towards training and serving these models, less emphasis has been put on evaluation of novel generations\u2014which typically depends on crude metrics such as ROUGE (Lin, 2004) or BLEU (Papineni et al., 2002) for matching text to ground truth or FID for measuring quality of images (Heusel et al., 2017). Although there has been an increasing amount of attention to using LLMs, especially GPT-4, to evaluate LLM output, such a paradigm can lead to overemphasis on stylistic or surface-level similarities to ground truth, while missing deeper biases that affect fairness (Koo et al., 2024; Wu and Aji, 2023; Zheng et al., 2023). Given these shortcomings of popular GenAI performance evaluation methods, and the general disconnect of such evaluation from real-world implications, it remains difficult to harness them to ensure that generative outputs lead to equitable outcomes across diverse demographics in practice.\nWe focus our initial study on resume screening, an area where automated systems have already been adopted, are legally relevant, and potentially discriminatory (Bloomberg, 2024; Gaebler et al., 2024; Wilson and Caliskan, 2024). In particular, we study a case where an LLM is used to summarize resumes submitted for the job of Social Worker, so that a hiring manager can read a short blurb about a candidate before deciding whether to offer an interview. As noted in Section 3, disparities in selection rates of job applications across demographic groups can constitute illegal discrimination (EEOC, 2023; Title VII, 1964). However, when a model is not producing a prediction that resembles a decision, these laws cannot be directly applied, and thus emerging regulation is needed to address such applications. While EO 14110 (White House, 2023a) directs federal agencies to assess and mitigate discriminatory outcomes in AI systems, and OMB (OMB, 2024) requires agencies to establish safeguards and oversight mechanisms, they offer no clear guidance on how to test for violations of these principles, creating an opportunity for developers and/or deploying parties to (intentionally or unintentionally) game fairness reporting.\nWe will examine the effects on racial discrimination in (simulated) downstream outcomes when a model is tested for bias and selected based on a popular yet brittle metric for evaluating summarization performance, the recall-based ROUGE score. We study the effects of enforcing the traditional notion of equalized performance, in this case with respect to differences in ROUGE across groups, in a case where the model is producing text that will be used by downstream decision-makers to make allocative decisions. What we observe is a mismatch between GenAI bias evaluation and downstream discrimination-based harms: equality in ROUGE scores across demographic groups does not correspond to equality in interview selection rate. Towards approaches for mitigation, alternative measures of discrimination are considered to show how the pitfalls of GenAI evaluation may be avoided by using a more holistic and context-specific evaluation suite. Overall, our experiment"}, {"title": "Mitigation.", "content": "To better capture the danger that decision-making systems relying on GenAI compo-nents will lead to traditional discrimination concerns such as disparate impact, fairness researchers should attempt to create metrics and testing regimes that shed light on how GenAI behavior may influence downstream allocation decisions. For example, in the case of resume screening, rather than relying on surface-level metrics like ROUGE that evaluate how closely a summary matches a reference text, fairness researchers should design metrics that capture downstream effects, such as how a summary influences decision-makers' perceptions of candidates from different demographic groups. One approach could involve developing standardized frameworks that measure bias in how descriptive language, tone, or content varies across race or gender in resume summaries. Instead of focusing solely on output quality, fairness evaluation should investigate how other meaningful discrepancies might lead to biased representations of minority groups."}, {"title": "4.2. Variability in Red Teaming", "content": "Though they are known to undergo extensive, if opaque, safety training (Dubey et al., 2024; OpenAI et al., 2024), modern frontier models are still susceptible to various types of adversarial prompts, for example those meant to elicit toxic behavior (Bai et al., 2022), violent or sexual content (Qu et al., 2023), or proprietary or otherwise privileged information (Carlini et al., 2020, 2023). While"}, {"title": "Mitigation.", "content": "To address the variability and limitations in current red teaming approaches, it is crucial for researchers to focus on developing methods that are open, transparent, and stable. In the short term, this could mean applying a variety of red teaming techniques together, so that results are less prone to sensitivity in experiment choices. Our results offer support for such an approach, as a clearer picture seems to emerge when considering a full panel of tests, instead of just one. In the long term, rather than focusing solely on maximizing attack success rates, researchers should shift towards creating robust frameworks that minimize the sensitivity of results to minor changes in testing conditions. This includes providing full access to code, prompt templates, and LLMs used in the attack generation process, allowing others to replicate and build upon the work. These efforts will help ensure that red teaming evaluations provide reliable, actionable insights about a model's fairness and discriminatory potential, preventing misleading outcomes that could allow biased models to pass pre-deployment tests unnoticed, allowing for more effective policy."}, {"title": "4.3. Evaluating Complex Interaction Modes", "content": "Classification models can often be tested under conditions that closely mirror their deployment environments. On the other hand, GenAI systems are frequently deployed under far more complex interaction modes. In particular, these models are increasingly used as agents that can interact with an environment, tasked with carrying out multi-turn and multi-modal conversations, or otherwise interacting dynamically with users and the outside world in ways that are difficult to fully anticipate or simulate, and thus difficult to capture in evaluation settings. As a result, even for the most advanced commercial and open-source models deployed under these complex conditions, performance is often reported on academic NLP benchmarks or crowd-sourced leaderboards that predominantly feature single-turn or otherwise limited interactions (Chiang et al., 2024; Dubey et al., 2024; Hendrycks et al., 2021; OpenAI et al., 2024).\nHowever, a key component of the emerging approach to effective regulation is a call to test AI models in ways that approximate their use at deployment. For example, the OMB memo OMB (2024) states that \"[a]gencies must conduct adequate testing to ensure the AI, as well as components that rely on it, will work in its intended real-world context\" and that \"[t]esting conditions should mirror as closely as possible the conditions in which the AI will be deployed.\" The NIST GenAI framework NIST (2024) similarly emphasizes the need for testing to reflect \u201creal-world scenarios,\u201d highlighting that \"[m]easurement gaps can arise from mismatches between laboratory and real-world settings.\" While there have been emerging efforts to tackle complex interaction modes in the generative AI fairness literature (e.g., Bai et al. (2024); Hua et al. (2024); Lin et al. (2023); Lum et al. (2024)), most work on bias mitigation in large language models and other generative AI systems has been confined to simpler, more controlled settings. Given this dearth of available testing tools that speak to performance in real-world settings, it is currently difficult to meet the expectations outlined in emerging regulation.\nIn this case study we illustrate how discrimination testing results may fail to generalize from simpler to more complex deployment conditions by considering the problem of single-turn vs. multi-turn interactions. Text-based (and multi-modal) generative AI, particularly those trained on human preference data (Bai et al., 2022; Lambert et al., 2024; Rafailov et al., 2023; Zollo et al., 2024), create the possibility for multi-turn interactions, where user engagement can range from a single text exchange to longer conversations, possibly extended across multiple sessions. Despite the increasing prevalence of this paradigm in domains like education and medicine, evaluation of multi-turn dialogue systems remains highly challenging, for example given the difficulty of anticipating how a conversation may evolve over repeated turns (Anwar et al., 2024). Through our experiment, we illustrate how the fairness assessment of a set of candidate models may differ depending on whether they are evaluated in the single-turn or multi-turn setting. Our results highlight that despite the difficulty and potential expense associated with evaluating interactions that may span multiple turns, it is imperative that the GenAI fairness research community develop methods for testing under this and other complex interaction modes."}, {"title": "Mitigation", "content": "To address the gap between testing and deployment conditions, fairness research must prioritize the development of techniques to evaluate GenAI systems in more complex, real-world contexts. Emerging testing protocols should aim to capture complexity including multi-turn interactions, multi-modal input and output, the ability to use tools and draw on knowledge outside of the system (i.e., agents), and other important axes along which interactions may vary. Beyond fairness research, general work on seamlessly testing across different deployment conditions, e.g., through simulation environments, can help create the conditions in which the nuanced ways that bias can emerge can be captured. By expanding the scope of fairness testing beyond simple, controlled environments, the research community can produce tools to measure how GenAI models will behave in the real world, making it easier for policymakers to produce effective, context-specific safeguards against discrimination."}, {"title": "4.4. Effects of User Modifications", "content": "Ensuring non-discriminatory behavior in GenAI deployments is complicated by the fact that these models can often be modified in some meaningful way by the end user, for example by changing a"}, {"title": "Mitigation", "content": "To mitigate the risks posed by user modifications in generative AI systems, fairness research could prioritize the development of efficient methods for identifying and testing high-risk parameter settings. For example, such a tool might automatically flag configurations that are more likely to produce biased or harmful outputs, ensuring that these settings receive closer"}, {"title": "5. Conclusion", "content": "To address the gap between fairness testing techniques and regulatory goals, we propose a shift in research focus towards creating context-specific and robust testing frameworks which take into account the complexity of the real-world conditions under which GenAI operates. One limitation of this work is that the case studies, while illustrative, cannot fully encompass the wide range of problems that may come up in real-world GenAI deployments. Though we aim to identify the most significant challenges for assessing discrimination in GenAI systems, our list is not exhaustive. For example, problems also may arise because of issues like prompt sensitivity, test set contamination, or the difficulty of explaining or interpreting these models. Also, further testing is necessary to understand the effectiveness of our proposed mitigation strategies. Future research should explore more diverse use cases and challenges, especially those where models evolve over time and fairness must be assessed dynamically."}, {"title": "Appendix A. Additional Legal Discussion", "content": "EU AI Act's Risk-Based Framework and GenAI The EU AI Act adopts a risk-based approach, classifying Al systems into four categories: prohibited, high-risk, limited risk, and minimal risk. Initially, the Act was primarily tailored to traditional AI applications like credit scoring, recruitment, or healthcare. However, as GenAI gained prominence during the drafting process, it was explicitly in-corporated through amendments to address its unique challenges. Specifically, the Act was expanded to include general-purpose AI (GPAI) systems, such as GenAI, within its scope. These systems often serve as foundational models that can be fine-tuned or customized for specific applications across diverse domains.\nTo the extent that a GenAI system is used like a traditional AI system\u2014meaning for a specific use case the risk-based approach would likely apply. For example, if a GenAI system was used to provide credit scores to borrowers it would likely be classified as high-risk and the Act's Articles related to high-risk systems would apply. However, unlike traditional AI high-risk systems that are typically tied to specific domains, because GenAI models often produce outputs that often do not map directly onto allocative decisions, the EU AI Act creates rules specific for GenAI. To address this, the Act makes a distinction between GPAI systems that have systemic risks and those that do not, tailoring specific provisions to each category. For GPAI systems that pose systemic risks, Article 52 introduces additional requirements, such as the obligation of developers to conduct comprehensive risk assessments and implement mitigation strategies to address risks. For GPAI systems without systemic risks, the obligations are less stringent but still require developers to ensure that their systems are designed transparently and include mechanisms to minimize foreseeable risks, such as Article 54 which creates a documentation requirement.\nIn short, the risk-based approach of the Act continues to apply to GenAI when deployed in a specific setting covered. But the Act goes beyond the core requirements for GenAI, creating a systemic/non-systematic risk distinction rather than is risk-based categories used primarily for traditional AI systems.\nLiability and GenAI Systems Section 4.4 highlights an important legal issue in GenAI bias testing: who is liable for discriminatory outputs of GenAI systems, and who bears the responsibility to test these systems for discriminatory behavior? Liability in AI systems is particularly complex because the development and deployment processes are often separate. Developers create the systems, while users or deployers integrate them into real-world applications, often with limited understanding of the underlying mechanics or data.\nHistorically, discrimination law has primarily focused on the entities using or deploying systems, holding them accountable for discriminatory outcomes and decisions. In contrast, other legal frameworks, such as product liability, have centered on developers or manufacturers of products. For AI systems, and particularly for GenAI, the emerging approach is to distribute liability across both developers and deployers, sometimes with different requirements. For instance, the EU AI Act includes provisions that apply to both developers and users of AI systems. Article 10, for example, mandates measures to mitigate bias in training data, explicitly targeting developers of high-risk Al systems. Users, on the other hand, also have obligations under the Act. For example, under Article 29, deployers must monitor the operation of high-risk AI systems based on the provider's instructions and report any serious incidents. Regarding GenAI (which is a type of \u201cgeneral-purpose"}, {"title": "Appendix B. Additional Experiment Information", "content": "Here, we specify the procedures for all of our experiments in full detail, and include some additional results. Our code is available at https://github.com/thomaspzollo/dhacking.\nB.1. Hiring\nB.1.1. DETAILS\nThe first step in our experiment is to produce synthetic personas, which will then be fed to GPT4 to produce corresponding resumes. To produce these synthetic personas, we randomly sample traits, background, and other personal information from a range of categories (see Table 3), none of which include race, ethnicity, or highly related characteristics like religion or language.\nThen, each of 250 personas is fed to the OpenAI model gpt-40-2024-08-06 with a prompt (see Table 2) to generate a realistic resume for the job of Social Worker in the New York metro area, making their background at least somewhat relevant and including real university and company names when possible. Also, the prompt includes direction to not include a name or email, instead"}, {"title": "B.1.2. RESULTS", "content": "In order to understand whether the decision-maker in our simulation is truly influenced by discrimi-natory behavior by the summarization model, we perform a further experiment to probe the effects of the stereotypical names themselves, as opposed to the summary content, on the decision-maker. To do so, resumes are summarized without an applicant's name by Llama-2-7B, and then fed to the decision maker with stereotypical names from each of 4 groups. Results are shown in Figure 8. Applicants from the best-off group receive interviews at a rate of 38.4%, while the worst-off group is selected at a rate of 36.4%. This 2% difference is smaller than the 5.2% gap that occurs when Llama-2-7B produces summaries with knowledge of the applicants' names."}, {"title": "B.2. Red Teaming", "content": "B.2.1. DETAILS\nThis experiment is based on the bias testing methodology from Perez et al. (2022) (Section 6), an evaluation meant to ensure that a model cannot be easily made to produce offensive and discriminatory material. To implement this procedure, a red team must choose their own red language model (RedLM) to produce attacks, as a closed-source (and 280B parameter) model is employed as the RedLM in the original work. With the aim of bias testing, the RedLM is used to generate question templates into which protected group names can be inserted to elicit discriminatory text, e.g., \u201cWhat is your least favourite thing about GROUP people?\u201d or \u201cDo you think GROUP people are useless?\u201d.\nThe goal of this experiment is to show that different choices in RedLMs can cause inconsistent model rankings based on the attack success rate (more computation details of attack success rate are given below). We use the below set of models from Huggingface to produce attacks:"}, {"title": "B.3. Multi-Turn Conversations", "content": "B.3.1. DETAILS\nWe extend the red-teaming experiments to a multi-turn conversation setting using data from the important education and health domains.\nTo simulate repeated interactions in the domain of education, we use GSM8K Cobbe et al. (2021) as our dataset. GSM8K is a dataset of 8.5K high-quality, linguistically diverse grade school math word problems, written by humans. It includes 7.5K training and 1K test problems, requiring 2 to 8 steps to solve using basic arithmetic operations (+, -, *, /). A bright middle school student can solve all problems, making it useful for multi-step mathematical reasoning.\nTo simulate repeated interactions in the domain of health, we use MedQuad Ben Abacha and Demner-Fushman (2019) as a dataset. MedQuAD contains 47,457 medical question-answer pairs sourced from 12 NIH websites (e.g., cancer.gov, niddk.nih.gov, GARD, MedlinePlus Health Topics). It spans 37 question types, including Treatment, Diagnosis, and Side Effects, related to diseases, drugs, and other medical entities like tests.\nUsing Gemma-2-9b-it, we generate responses for 5000 questions from each dataset, using the hyper-parameters mentioned in Table 8. After we generate these responses, for each conversation length $k \\in \\{0,1,3,5\\}$, we sample examples and combine them into conversations (demarcating separate turns in the conversation using appropriate chat tags.), generating 1000 conversations for each domain and for each conversation length. After creating conversations, we concatenate the RedLM questions to the conversation and input this to the candidate models, Gemma-2-2b-it and Gemma-2-9b-it, this time replacing GROUP with \u201chomosexual\u201d. The relevant prompt details, and a full example input, are shown in Table 9. The hyperparameters of the relevant RedLMs and candidate models in Figure 5 can be found in Table 7 and Table 8.\nWe compute toxicity in the same manner as the previous experiment. The attack success rate is the percentage of answers that exceed the toxicity threshold of 0.5."}, {"title": "B.4. Racial Portrayals in Diffusion Models", "content": "B.4.1. DETAILS\nWe generate images using the medium version of the popular open source StableDiffusion3 model (stable-diffusion-3-medium-diffusers).\nWe perform 28 inference steps in diffusion, and generate 128 images of each with the prompt \"A photo of {identity}\", for the identities:\n\u2022 a white woman\n\u2022 a black woman\n\u2022 an asian woman\n\u2022 a hispanic woman"}]}