{"title": "REAL-WORLD DATA and Calibrated SIMULATION SUITE FOR OFFLINE TRAINING OF REINFORCEMENT LEARNING AGENTS TO OPTIMIZE ENERGY AND EMISSION IN BUILDINGS FOR ENVIRONMENTAL SUSTAINABILITY", "authors": ["Judah Goldfeder", "John Sipple"], "abstract": "Commercial office buildings contribute 17 percent of Carbon Emissions in the US, according to the US Energy Information Administration (EIA), and improving their efficiency will reduce their environmental burden and operating cost. A major contributor of energy consumption in these buildings are the Heating, Ventilation, and Air Conditioning (HVAC) devices. HVAC devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) agent is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many practical challenges. Most existing work on applying RL to this important task either makes use of proprietary data, or focuses on expensive and proprietary simulations that may not be grounded in the real world. We present the Smart Buildings Control Suite, the first open source interactive HVAC control dataset extracted from live sensor measurements of devices in real office buildings. The dataset consists of two components: six years of real-world historical data from three buildings, for offline RL, and a lightweight interactive simulator for each of these buildings, calibrated using the historical data, for online and model-based RL. For ease of use, our RL environments are all compatible with the OpenAI gym environment standard. We also demonstrate a novel method of calibrating the simulator, as well as baseline results on training an RL agent on the simulator, predicting real-world data, and training an RL agent directly from data. We believe this benchmark will accelerate progress and collaboration on building optimization and environmental sustainability research.", "sections": [{"title": "INTRODUCTION", "content": "Energy optimization and management in commercial buildings is a very important problem, whose importance is only growing with time. Buildings account for 37% of all US carbon emissions, with commercial buildings alone taking up a staggering 17% in 2023 (EIA). Reducing those emissions by even a small percentage can have a significant effect. In climates that are either very hot or very cold, energy consumption is much higher, and there is even more room to have a major impact. We believe this problem is one of the most important avenues for climate sustainability research, where even a small improvement over baseline policies can drastically reduce our carbon footprint.\nIn particular, HVAC systems account for 40-60% of energy use in buildings (P\u00e9rez-Lombard et al., 2008), and roughly 15% of the world's total energy consumption (Asim et al., 2022). Most office buildings are equipped with advanced HVAC devices, like Variable Air Volume (VAV) devices, Hot Water Systems, Air Conditioners and Air Handlers that are configured and tuned by the engineers, manufacturers, installers, and operators to run efficiently with the device's local control loops (Mc-"}, {"title": "OPTIMIZING ENERGY AND EMISSION IN OFFICE BUILDINGS WITH RL", "content": "In this section we frame energy optimization in office buildings as an RL problem. We define the state of the office building \\(S_t\\) at time t as a fixed length vector of measurements from sensors on the building's devices, such as a specific VAV's zone air temperature, gas meter's flow rate, etc. The action on the building \\(A_t\\) is a fixed-length vector of device setpoints selected by the agent at time t, such as the boiler supply water temperature setpoint, etc.\nMore generally, RL is a branch of machine learning that attempts to train an agent to choose the best actions to maximize some long-term, cumulative reward (Sutton & Barto, 2018). The agent observes"}, {"title": "RELATED WORKS", "content": "Considerable attention has been paid to HVAC control (Fong et al., 2006) in recent years (Kim et al., 2022), and while alternative approaches exist, such as model predictive control (Taheri et al., 2022), a growing portion of the literature has considered how RL and its various associated algorithms can be leveraged (Yu et al., 2021; Mason & Grijalva, 2019; Yu et al., 2020; Gao & Wang, 2023; Wang et al., 2023; V\u00e1zquez-Canteli & Nagy, 2019; Zhang et al., 2019b; Fang et al., 2022; Zhang et al., 2019b). As mentioned above, a central requirement in RL is the offline environment that trains the RL agent. Several methods have been proposed, largely falling under three broad categories.\nData-driven Emulators Some works attempt to learn a dynamics as a multivariate regression model from real-world data (Zou et al., 2020; Zhang et al., 2019a), often using recurrent neural network architecture, such as Long Short-Term Memory (LSTM) (Velswamy et al., 2017; Sendra-Arranz & Guti\u00e9rrez, 2020; Zhuang et al., 2023). The difficulty here is that data-driven models often do not generalize well to circumstances outside the training distribution, especially since they are not physics based."}, {"title": "THE DATASET STRUCTURE", "content": "Both the real-world data and simulated data are given in the same format. Following the RL paradigm, data is provided as a series of observations, actions, and rewards. In the case of the"}, {"title": "SIMULATOR DESIGN CONSIDERATIONS", "content": "A fundamental trade-off when designing a simulator is speed versus fidelity, as depicted in Figure 2. Fidelity is the simulator's ability to reproduce the building's true dynamics that affect the optimization process. Speed refers to both simulator configuration time, i.e., the time required to configure a simulator for a target building, and the agent training time, i.e., the time necessary for the agent to optimize its policy using the simulator.\nEvery building is unique, due to its physical layout, equipment, and location. Fully customizing a high fidelity simulation to a specific target building requires nearly exhaustive knowledge of the building structure, materials, location, etc., some of which are unknowable, especially for legacy office buildings. This requires manual \u201cguesstimation\u201d, which can erode the accuracy promised by high-fidelity simulation. In general, the configuration time required for high-fidelity simulations limits their utility for deploying RL-based optimization to many buildings. High-fidelity simulations also are affected by computational demand and long execution times.\nAlternatively, we propose a fast, low-to-medium-fidelity simulation model that was useful in addressing various design decisions, such as the reward function, the modeling of different algorithms. and for end-to-end testing. The simulation is built on a 2D finite-difference (FD) grid that models"}, {"title": "A LIGHTWEIGHT, CALIBRATED SIMULATION", "content": "Our goal is to develop a method for applying RL at scale to commercial buildings. To this end, we put forth the following requirements for this to be feasible: We must have an easily customizable simulated environment to train the agent, with high enough fidelity to train an improved control agent. To meet these desiderata, we designed a light weight simulator based on finite differences approximation of heat exchange, building upon earlier work (Goldfeder & Sipple, 2023). We proposed a simple automated procedure to go from building floor plans to a custom simulator in a short time, and we designed a calibration and evaluation pipeline, to use data to fine tune the simulation to better match the real world. What follows is a description of our implementation.\nThermal Model for the Simulation As a template for developing simulators that represent target buildings, we start with a general-purpose high-level thermal model for simulating office buildings, illustrated in Figure 3. In this thermal cycle, we highlight significant energy consumers as follows. The boiler burns natural gas to heat the water, \\(Q\\). Water pumps consume electricity \\(W_{b,p}\\) to circulate heating water through the VAVs. The air handler fans consume electricity \\(W_{b,in}\\), \\(W_{b,out}\\) to circulate the air through the VAVs. A motor drives the chiller's compressor to operate a refrigeration cycle, consuming electricity \\(W_c\\). In some buildings coolant is circulated through the air handlers with pumps that consume electricity, \\(W_{c,p}\\).\nWe selected water supply temperature \\(T_\\) and the air handler supply temperature \\(\\hat{T_s}\\) as agent actions because they affect the balance of electricity and natural gas consumption, they affect multiple device interactions, and they affect occupant comfort. Greater efficiencies can be achieved with these setpoints by choosing the ideal times and values to warm up and cool down the building in the workday mornings and evenings. Further tradeoffs include balancing the thermal load between hot water heating with natural gas and supply air heating with electricity using the air conditioner or heat pump units.\nFinite Differences Approximation The diffusion of thermal energy in time and space of the building can be approximated using the method of Finite Differences (FD)(Sparrow, 1993; Lomax et al., 2002), and applying an energy balance. This method divides each floor of the building into a grid of three-dimensional control volumes and applies thermal diffusion equations to estimate the temperature of each control volume. By assuming each floor is adiabatically isolated, (i.e., no heat is transferred between floors), we can simplify the three-spatial dimensions into a spatial two-dimensional heat transfer problem. Each control volume is a narrow volume bounded horizontally, parameter-"}, {"title": "SIMULATOR CALIBRATION", "content": "We now provide a full end-to-end demonstration of our calibration procedure, and show that our simulator, when tuned and calibrated, is able to make useful real-world predictions, and can train an RL agent to produce an improved policy over the baseline.\nSetup We calibrated the simulator using data from SB1, with two stories, a combined surface area of 93,858 square feet, and 170 HVAC devices. Using the configuration pipeline, we went from floor plan blueprints to a fully configured simulator for this building, a process that took a single technician less than three hours to complete.\nCalibration Data To calibrate our simulator, we took real-world data from three days, from Monday July 10, 2023 12:00 AM PST, to Thursday July 13, 2023 12:00 AM PST. The first two days were used as a train set, and the third day as validation of the calibrated performance on unseen data, as can be seen in Table 2. All times are given in US Pacific, the local time of the real building.\nCalibration Procedure We ran hyperparameter tuning for 4000 iterations, with the aim of optimizing the TS-MAE, as outlined in equation 1, over the train data. We reviewed the physical constants that yielded the lowest simulation error from calibration. Densities, heat capacities, and conductivities plausibly matched common interior and exterior building materials. However, the external convection coefficient was higher than under the weather conditions, and likely is compensating for the radiative losses and gains, which were not directly simulated. For details about the hyperparameter tuning procedure, including the parameters varied, the ranges given, and the values found that best minimized the calibration metric, see Appendix F.\nCalibration Results In Table 2, we present the predictive results of our calibrated simulator, on N-step prediction, for the train scenario, where N = 576, representing a two day predictive window, and the test scenario, where N = 288, representing a one day window. We calculated the TS-MAE, as defined in equation 1. We show results for the hyperparameters that best fit the train set, as well as for an uncalibrated simulator as a baseline. At no point was the validation data ever provided to"}, {"title": "DEMONSTRATION BENCHMARKING RESULTS", "content": "While we believe our benchmark will be useful for a variety of tasks, such as further use of the data to calibrate the simulator, in this section we highlight results on three important tasks that our suite is well suited to: training an RL agent on the simulator, training a time-series regression model to predict the real world data, and training an RL agent on the real data directly.\nTraining a Reinforcement Learning Agent on the Simulator To demonstrate the usefulness of our calibrated simulator on generating an improved policy, we used Soft Actor Critic (SAC) algorithm (Haarnoja et al., 2018) to train an agent, and then compared our agent with the baseline performance of running the policy currently used in the real building. Both actor and critic were feedforward networks. We ran hyperparameter tuning, again using the method from Golovin et. al. (Golovin et al., 2017), to choose the dimensionality of the critic network and actor network, the batch size, the critic learning rate and actor learning rate, and \u03b3.\nWe recorded the actor loss, critic loss, alpha loss, and return, over a two day period. The agents trained for 4,000 iterations. Using the \\(R_{3C}\\) reward, the baseline over this two day period had a return of -12.9, and our best agent had an improved return of -11.9, an 8% improvement over the baseline, as show in Table 3. For further training details, and an in depth performance comparison between the learned policy and the baseline, including a breakdown on setpoint deviation, carbon emissions, electrical energy, and natural gas energy, see Appendix H.\nTraining a Learned Dynamics Model Another important task is to use a sequence model to learn to predict the real world data, effectively learning a dynamics model that can then be used in turn in place of the simulator to train an agent. To demonstrate this approach, we trained an encoder-decoder LSTM(Hochreiter, 1997) to model the building dynamics. The model takes in a historical sequence of length N and outputs a prediction sequence of length M. At each timestep t in the sequence, the model is given an observation \\(O_t\\), action taken by the policy \\(A_t\\), and auxiliary state features (such as time of day and weather, that are useful as inputs but need not be predicted) \\(U_t\\), and for future timesteps, the model is trained to predict future observations, as well as future reward information (based on predicted energy use and carbon emissions) \\(E_t\\). The LSTM model is shown in Figure 24.\nTraining a Reinforcement Learning Agent on Real Data Building directly off of the above, we also trained an RL agent on the learned dynamics model, demonstrating the ability to learn a policy directly from data without involving the simulator. Like the simulator SAC agent, we were able to learn a policy that improved upon the baseline. For detailed analysis of this policy, see Appendix J."}, {"title": "LIMITATIONS AND CONCLUSION", "content": "The biggest limitation of our benchmark is that all buildings are located in California. We intend to remedy this in the near future by adding more buildings. Another limitation is that we only include data from a one year duration, and in the future we may add longer sequences, for year over year analysis. Our simulator also lacks a radiative heat model, and we hope further work can add this. In addition, our calibration focused on temperature, but in future work we hope to include energy consumption metrics as part of the calibration procedure.\nWe present a high quality interactive HVAC Control Suite, with real-world historical data from three buildings, as well as calibrated simulators for each building, and a novel, data-based, simulation calibration procedure. We also show promising initial results on key benchmark tasks. We believe this benchmark will facilitate collaboration, reproducibility, and progress on this problem, making an important contribution towards environmental sustainability."}, {"title": "REWARD FUNCTION DETAILS", "content": "We call our reward function the 3C Reward, because it is made up of a combination of three factors: Comfort, Cost, and Carbon. The purpose of the reward function is to provide the agent a feedback signal after each action about the quality of the current and past actions performed. We combine the different objectives described in Optimization Problem as a normalized, weighted sum of maintaining comfort conditions, electrical cost, and carbon cost:\n\\(R_{3c} = u \\times C_1 + v \\times C_2 + w \\times C_3\\)\nwhere \\(C_1\\) represents normalized comfort conditions, \\(C_2\\) normalized energy cost and \\(C_3\\) normalized carbon emission. Constants u, v, w represent operator preferences, allowing them to weight the relative importance of cost, comfort and carbon consumption.\nEach value \\(C_1\\), \\(C_2\\), \\(C_3\\), is bounded by the range [-1,0], where worst performance is -1 and the ideal performance upper-bound is 0 Thus the reward function in an agregate is formulated as an approximate regret function, bounded in the range [-1,0], and represents an offset from the best-case where comfort conditions are perfectly maintained, without consuming energy and emitting carbon. Each of the sub functions \\(C_1\\), \\(C_2\\), \\(C_3\\) will be elaborated next."}, {"title": "COMFORT LOSS FUNCTION (C1)", "content": "Besides zone air temperature, other factors such as ventilation, drafts, solar exposure, humidity and air quality affect human comfort and productivity in office buildings. However, for now we are focused solely on temperature as the indicator of the comfort level in the office buildings. As additional sensors are deployed and the other factors are measured, they should be considered in the definition of an enhanced comfort loss function.\nStudies have shown that a relationship exists between work performance and temperature. For example, in Sepp\u00e4nen, et al. 2006 (Seppanen et al., 2006), work performance was quantified as the mean time required to complete common office tasks (e.g., text processing, bookkeeping calculations, telephone customer service calls, etc.). Performance was shown to increase gradually with temperatures increasing up to 21-22\u00b0C and decreasing at temperatures beyond 23-24\u00b0C. Therefore, when temperatures deviate outside setpoints, the comfort loss should also be smooth and monotonically increasing.\nThus, the following rules were selected to govern the comfort loss function:\n1. Setpoints define the comfort standards, and no penalty should be applied whenever the zone temperature is within heating and cooling setpoints.\n2. Comfort is undefined when the zone is unoccupied: if the zone is unoccupied, comfort loss is zero, regardless of zone temperature.\n3. Comfort decays smoothly and monotonically as the temperatures drift from setpoints, and occupants are tolerant to small setpoint deviations. Therefore, small setpoint deviations should have a small comfort penalty, and the penalty should smoothly increase as the deviations increase.\n4. Large setpoint deviations should approach a maximum, bounded penalty, where a zone becomes completely intolerable for its occupants.\nThe comfort loss function represents a bounded penalty term for occupied zones that have zone air temperatures outside of setpoint, and covers three adjacent temperature intervals: below cooling setpoint \\(T_z < \\hat{T}_{heating}\\), inside setpoints \\(\\hat{T}_{heating} < T_z < \\hat{T}_{cooling}\\), and above cooling setpoint \\(\\hat{T}_{cooling} < T_z\\)\nWe propose a logistic sigmoid parameterized by A and A to represent the smooth decay (increase loss) of comfort below the heating and above the cooling setpoints. Parameter A is a stiffness coefficient that affects the slope of the decay and parameter A represents the offset in \u00b0C from the set point where halfway loss value (0.5) occurs. Additionally we define a step function \u03b4(k) = 1 when the zone has at least one occupant (k > 0), and \u03b4(k) = 0 otherwise.\n\\(h_z(T_z, k_z, \\hat{T}_{heating}, \\hat{T}_{cooling}) = \\begin{cases} \\frac{\\delta(k_z)}{1+e^{-\\chi(T_z-\\hat{T}_{heating}+\\Delta)}} -1 & T_z < \\hat{T}_{heating}\\\\ 0 & \\hat{T}_{heating} \\le T_z \\le \\hat{T}_{cooling} \\\\ \\frac{-\\delta(k_z)}{1+e^{-\\chi(T_z-\\hat{T}_{cooling}-\\Delta)}} & \\hat{T}_{cooling} < T_z \\end{cases}\\)\nThe chart below shows the comfort loss curve with common setpoints, where the horizontal axis represents zone air temperature and the vertical axis represents the loss. The heating and cooling setpoints were taken from data recordings."}, {"title": "ENERGY COST FUNCTION (C2)", "content": "The energy cost function \\(C_1(S_t)\\) is a normalized, aggregate cost estimate from consuming electrical and natural gas energy during one timestep. The cost function is the ratio of the actual energy used to the maximum energy capacity that ranges between 0: no cost incurred; and 1: maximum cost incurred.\n\\(C_2(S_t) = \\frac{\\text{actual energy cost}}{\\text{cost at max energy capacity}}\\)\nGeneral energy cost can be calculated as the product of the mean power applied, the time interval, and the cost per unit energy at the time of the interval, where we use W, W to represent electrical/mechanical energy, and power, and Q,Q to represent thermal energy and power from natural gas. Since all four terms contain the same interval \\(t_i - t_{i-1}\\), they cancel out, allowing us to use power instead of energy. As described above, pumps, blowers, and AC/refrigeration cycles consume electricity and water heaters/boilers consume natural gas. Therefore the total energy and cost is the sum of each energy consumer cost used over the interval:\n\\(C_2(S) = - \\frac{(W_a + W_m + W_p) \\times p_e(t) + Q_g \\times p_g(t)}{(W_{a,max} + W_{m,max} + W_{p,max}) \\times p_e(t) + Q_{g,max} \\times p_g(t)}\\)\nWhere \\(W_a\\) and \\(W_{a,max}\\) are the actual and max electrical power for the AC/refrigeration cycle, \\(W_m\\) and \\(W_{m,max}\\) are the actual and max electrical power for the blowers/air circulation, \\(W_p\\) and \\(W_{p,max}\\) are the actual and max pump electrical power, and \\(Q_g\\) and \\(Q_{g,max}\\) are the actual and max thermal power. Terms \\(p_e(t)\\) and \\(p_g(t)\\) are the electricity and gas price per energy incurred over the interval at time t.\nThe actual power terms in the numerator are estimated from the device observations and the device's fixed parameters using standard HVAC energy conversions. The max power terms in the denominator are derived from device ratings, which define the maximum operating nouns of the device."}, {"title": "CARBON EMISSION COST FUNCTION (C3)", "content": "Similar to the energy cost function, carbon emission cost function is a function of the electrical and natural gas power used during the interval. The carbon emission cost function \\(C_3\\) is a normalized, aggregate cost estimate from the emission of carbon mass by consuming electrical and natural gas energy during one timestep. The cost function is the ratio of the actual carbon used to the maximum carbon emitted that ranges between 0: no emission cost incurred; and 1: maximum emission cost incurred.\n\\(C_3(S_t) = \\frac{\\text{actual carbon mass emitted}}{\\text{maximum carbon emitted}}\\)\nThe carbon emission cost is similar to the energy cost function described above, except that we replace the price terms \\(p_e\\), \\(p_g\\) with emission terms \\(r_e\\), \\(r_g\\) that convert the power to carbon emission rates."}, {"title": "IMMEDIATE AND DELAYED REWARD RESPONSES", "content": "The reward function is a weighted average of maintaining temperature setpoints in occupied zones, while minimizing energy cost, and minimizing carbon emission. Both energy and carbon emission cost functions provide a low latency response, because actions have an almost immediate effect on the reward. For example, lowering the supply water temperature setpoint will reduce the flow of natural gas to the burner, bringing Q down in the next step. However, the effect of increasing water temperature on the comfort loss function may be delayed by multiple time steps, due to the thermal latency in the building. This thermal latency is due to inherent heat capacity and thermal resistance within the building that has a dampening effect on diffusing heat throughout the building. This means that some settings of u, v, w may cause undesirable effects. Experiments with the simulation indicate that too strong weights (e.g., u + v \u2265 0.6) toward energy cost and/or carbon emission may lead the agent to lower the water temperature, which can cause the VAVs to increase their airflow demand to compensate for a lower supply air temperature, since thermal energy flow is a tradeoff between air mass flow and water heating at the VAV's heat exchanger. Consequently, the increased airflow demand results in a much higher, delayed electrical energy consumption by the blowers to meet the zone airflow demand."}, {"title": "PROTO DEFINITIONS", "content": "Here, we will elaborate on the exact proto definitions used in the dataset.\nHaving applied the RL paradigm, the data in our dataset falls under the following categories:\n1. Environment Data General information about the environment, such as the number of devices and zones, and their names and device types. This is provided once per building environment\n2. Observation Data The measurements from all devices in the building (VAV's zone air temperature, gas meter's flow rate, etc.), provided at each time step\n3. Action Data The device setpoint values that the agent wants to set, provided at each timestep\n4. Reward Data Information used to calculate the reward, as expressed in energy cost in dollars, carbon emission, and comfort level of occupants, provided at each time step\nAs mentioned above, this data is stored in protos. This section provides the definition of each proto, categorizing them using the four categories above, with examples of each."}, {"title": "ENVIRONMENT DATA PROTOS", "content": "This is the data that provides, once per environment, details about the environment such as number of devices, and zones, etc. There are two proto definitions:\n1. ZoneInfo: The ZoneInfo message defines thermal spaces or zones in the building and provides zone-to-device association, which enables using the associated VAVs' zone air temperatures to estimate the zone's temperature.\n2. DeviceInfo: The HVAC devices in the building are defined in the DeviceInfo message. Each device exposes a map of observable_fields and action_fields. The observable_fields represent the observable state of the building in native units, and the action_fields are available setpoints exposed by the building that the agent may add to its action space. Currently observable_fields and action_fields are floating point values, but may be expanded to categorical values in the future."}, {"title": "ACTION DATA PROTOS", "content": "This consists of the device setpoint values that the agent wants to set, provided at each timestep. There are two relevant protos:\n1. ActionRequest\n2. ActionResponse\nThe Environment converts the action from the agent into an ActionRequest and sends it to the building. The building applies the request and returns an ActionResponse. The ActionRequest contains the UTC timestamp from the Environment, and a list of SingleActionRequests, one for each setpoint in the agent's action space. Each SingleActionRequest contains a tuple of the device_id, setpoint_name, and requested setpoint_value, in native units. The device_id must match with one of the device_ids in the DeviceInfos, and the setpoint_name must match with one of the action_fields of the associated device. The ActionResponse contains the building's UTC timestamp, the original ActionRequest, and a list of SingleActionResponses, one associated with each SingleActionRequest. The SingleActionResponse contains the associated SingleActionRequest, a response type enumeration, and a string for additional information."}, {"title": "REWARD DATA PROTOS", "content": "This includes information used to calculate the reward, as expressed in cost in dollars, carbon footprint, and comfort level of occupants, provided at each time step The Reward protos define the input and output messages for our 3C reward function (Cost Carbon and Comfort), which contains the code that converts them into a single scalar value, a requirement for most RL algorithms. There are two relevant protos:\n1. RewardInfo: The values that are used as inputs to calculate the reward\n2. RewardResponse: Containing the scalar reward signal obtained by passing the above functions into our 3C reward function\nThe building updates the RewardInfo at each timestep and provides the reward function necessary inputs to compute the 3C Reward Function. The data contained in theRewardInfo is bounded by the step's interval from start_timestamp to end_timestamp in UTC. The RewardInfo has mean energy rate estimates (i.e. power in Watts) that can be treated as constants over the interval. Given the interval and a constant rate value over the interval, the reported power in Watts can be easily converted into energy in kWh. The RewardInfo contains maps of three types of specialized data structures:\n\u2022 The ZoneRewardInfo message provides information about the zone air temperature measurements, temperature setpoints, airflow rate and setpoint, and average occupancy for the time step. Each instance is indexed by its unique zone ID.\n\u2022 The AirHandlerRewardInfo message describes the combined electrical power in W use of the intake/exhaust blowers, and the electrical power in W of the refrigeration cycle. Since a building may have more than one air handler, the air handler objects are values in a map keyed by the air handlers' device IDs.\n\u2022 The BoilerRewardInfo contains the average electrical power in W used by the pumps to circulate water through the building, and the average natural gas power in W used to heat the water in the boiler. Since there may be more than one hot water cycle in the building, each ZoneRewardInfo is placed into a map keyed by the hot water device's ID.\nThe reward function converts the current RewardInfo into the RewardResponse for the same interval as the RewardInfo. The agent's reward signal is agent_reward_value. Since the reward returned to the agent is a function of multiple factors, it is useful for analysis to show the individual components,m such as carbon mass emitted, and the electrical and gas costs for the step."}, {"title": "TRAINING AND EVALUATING A LEARNED DYNAMICS MODEL", "content": "Aside from being useful for offline training and for calibrating our simulator, the real world data can also be used to directly learn a regression model that approximates the building dynamics. This model can then be used to train a control agent.\nAs described in the main paper, to demonstrate this approach, building off of earlier work(Velswamy et al., 2017; Sendra-Arranz & Guti\u00e9rrez, 2020; Zou et al., 2020; Zhuang et al., 2023), we trained an LSTM to model the building dynamics. We used an encoder-decoder network, where the model takes in a historical sequence of length N and outputs a prediction sequence of length M. At each timestep t in the sequence, the model is given an observation \\(O_t\\), action taken by the policy \\(A_t\\), and auxiliary state features (such as time of day and weather, that are useful as inputs but need not be predicted) \\(U_t\\), and for future timesteps, the model is trained to predict future observations, as well as future reward information (based on predicted energy use and carbon emissions) \\(E_t\\). The LSTM model is shown in Figure 24.\nWe then trained the model to predict the next observation for 65 epochs, plotting training and validation loss, as shown in figure 25\nHowever, loss curves alone do not tell the full story of how well our regression model is reconstructing the signal of the dynamics, so we also included additional evaluations. We had the model predict 3 weeks into the future, and then compared the predictions with the ground truth data to ensure the"}, {"title": "REAL DATA SAC AGENT TRAINING DETAILS AND PERFORMANCE ANALYSIS", "content": "We then trained a SAC agent on the regression environment, much like how we did on the simulator. This gives us a baseline for how to generate a policy purely based on data, without use of the simulator. We used hyper-parameter tuning, and trained 200 agents. The chart in figure 27 shows agent reward progress as the number of trials increased."}]}