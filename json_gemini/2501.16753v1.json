{"title": "Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction", "authors": ["Hy Nguyen", "Srikanth Thudumu", "Hung Du", "Rajesh Vasa", "Kon Mouzakis"], "abstract": "Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into N chunks, where N is the number of heads. Each segment captures only a fraction of the original embedding's information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings \u2013 this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.", "sections": [{"title": "1. Introduction", "content": "Humans can anticipate short-term future events using visual information, a capability crucial for tasks such as autonomous driving [27], action recognition [7], motion prediction [7], and anomaly detection [10]. To enable machines to develop similar abilities, the task of Video Frame Prediction (VFP) has been extensively studied, involving the prediction of future video frames from recent past frames.\nVFP is challenging due to the complexity of real-world video dynamics and the inherent uncertainty of future events [32]. Capturing both spatial and temporal information effectively is essential [8]. Before Transformers [21], approaches typically combined sequence models (e.g., LSTM, RNN, GRU) with CNNs to extract spatio-temporal features from past frames [8, 22, 24, 26]. However, these models struggle with long-term dependencies, suffer from vanishing gradients, and are computationally expensive and error-prone over longer sequences [2], [8], [28]. In contrast, Transformer-based methods using Multi-head Self-Attention (MHSA) are more effective at handling long-range dependencies and allow for efficient parallel processing [1, 8, 16, 21, 28, 32].\nHowever, MHSA requires the input embedding to be divided into multiple chunks (corresponding to the number of attention heads), which can distort the learned latent space and dilute semantic information, reducing prediction accuracy. Although the Transformer-based VFP systems introduce additional modules to enhance prediction performance, such as the Local Spatio-Temporal block [16], Self-attention Memory (SAM) [8], or Temporal MHSA [28], etc, the core MHSA mechanism remains unchanged, allowing the issue of semantic dilution to persist. Furthermore, these VFP systems do not directly predict the next frame; instead, they predict the embedding of that frame. This requires a decoding step to reconstruct the predicted frame from its embedding. However, the loss function used to train these VFP systems is based on the error of the reconstructed frame, not the predicted embedding, leading to a discrepancy that can hinder effective model learning. For example, a common loss function used in VFP systems is structured as follows [6, 8, 16]:\n$L = L_1(X, \\hat{X}) + L_2(X, \\hat{X}) +P/S(X, \\hat{X})$"}, {"title": "2. Related Work", "content": "Before the introduction of the Transformer model [21], VFP systems primarily relied on sequence models such as LSTM, RNN, and GRU for temporal information, and image-to-vec models such as CNNs and Autoencoders for spatial information. These temporal and spatial models were often combined to process historical frame sequences and predict the next frame. ConvLSTM [18], for example, enhances traditional LSTM by using convolution operations instead of linear ones, achieving notable success in VFP. TrajGRU [19] extends GRU by incorporating dynamic, learnable spatial connections, enabling more effective spatiotemporal modeling. Other significant variants include PredRNN [22], PredRNN++ [23], Memory in Memory (MIM) [25], and Motion-Aware Unit (MAU) [3]. However, sequence models often face challenges such as handling long-term dependencies, vanishing gradient problems, high computational cost, slow training and inference, and susceptibility to error accumulation [28].\nVFP systems have evolved to more advanced techniques leveraging transformers and attention mechanisms, which"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Problem Formulation", "content": "The goal of single-frame VFP is to predict the next frame $I_{t+1}$ using the preceding $M$ frames from a video sequence ${I_t}_{t=1}^T$, where $I_t \\in \\mathbb{R}^{H \\times W \\times C}$ denotes the frame at time $t$, with height $H$, width $W$, and $C$ color channels. The task is modeled as:\n$\\hat{I}_{T+1} = f_{\\theta}(I_{T-M+1}, I_{T-M+2},\u2026\u2026\u2026, I_{T})$\nwhere $f_{\\theta}$ maps the sequence of input frames to the predicted next frame $\\hat{I}_{T+1}$. $f_{\\theta}$ is parameterized by $\\theta$.\nThe prediction error is optimized by minimizing the loss function $\\mathcal{L}$, typically defined as the difference between the"}, {"title": "3.2. Transformer-based VFP systems", "content": "Each frame $I_t$ is passed through a CNN-based model $g_{\\phi}$ (e.g., ResNet) to extract a high-dimensional feature representation:\n$e_t = g_{\\phi}(I_t), \\quad e_t \\in \\mathbb{R}^d$\nwhere $e_t$ is the frame embedding, $d$ is dimensionality, and $\\phi$ are the CNN parameters.\nThe embeddings of the last $M$ frames, $E = \\{e_{T-M+1}, e_{T-M+2}, ..., e_T\\}$, are input to a Transformer using multi-head self-attention (MHSA) to capture temporal dependencies. For each head $h_i$, the query, key, and value matrices are computed as:\n$q^{h_i} = W_q^{h_i} e_t \\quad k^{h_i} = W_k^{h_i} e_t \\quad v^{h_i} = W_v^{h_i} e_t$\nwhere $e_t^{h_i} \\in \\mathbb{R}^{d_h}$ (with $d_h = \\frac{d}{N}$) is the input for each head, and $N$ is the number of heads. This division causes semantic dilution in MHSA.\nThe attention score between embeddings $e_t$ and $e_{t'}$ for the $i$-th head is computed as:\n$a_{t,t'}^{h_i} = \\frac{exp(\\frac{q^{h_i} k^{h_i}}{\\sqrt{d_h}})}{\\sum_{t'=1}^{M} exp(\\frac{q^{h_i} k^{h_i}}{\\sqrt{d_h}})}$\nThe output for the $i$-th head is a weighted sum of the value matrices:\n$y_t^{h_i} = \\sum_{t'=1}^{M} a_{t,t'}^{h_i} v_{t'}$\nThe outputs from all heads are then concatenated to produce the final output for each embedding $e_t$:\n$y_t = [y_t^{h_1}; y_t^{h_2};...; y_t^{h_N}]$"}, {"title": "3.3. Proposed Method", "content": ""}, {"title": "3.3.1. Model Architecture", "content": "We propose a Semantic Concentration VFP (SC-VFP) model to address the limitations of Transformer-based VFP systems. The architecture consists of:\n1. Embedding Layer: Maps input frames to a lower-dimensional space via a Vision Transformer (ViT) [5]. A learnable classification token [CLS] aggregates spatial information from the entire input image, representing image frame embeddings.\n2. Semantic Concentration VFP (SC-VFP): Processes embeddings using an encoder-only Transformer, replacing the original MHSA block with our Semantic Concentration MHSA (SCMHSA) block to mitigate semantic dilution. Multiple encoder blocks process temporal information across sequences.\n3. Semantic Concentration Multi-head Self-attention (SCMHSA): Enhances traditional MHSA by processing the full embedding for each head, yielding richer representations. To manage the increased head dimensions, a learnable projection matrix $W_o$ retains the most relevant semantic information.\n4. Prediction Layer: Synthesizes temporal and spatial information via a Multi-Layer Perceptron (MLP) to predict the next frame's embedding."}, {"title": "3.3.2. Semantic Concentration Multi-Head Self-Attention (SCMHSA)", "content": "To address semantic dilution, SCMHSA enhances standard MHSA by preserving the full semantic content of the input embedding. Unlike traditional MHSA, which splits the embedding $e_t \\in \\mathbb{R}^d$ into $N$ smaller chunks $e_t^{h_i} \\in \\mathbb{R}^{d_h=\\frac{d}{N}}$ (for $N$ heads), SCMHSA feeds the complete embedding to each attention head, enabling holistic processing and mitigating semantic loss.\nBuilding on the standard Transformer architecture described earlier, SCMHSA introduces the following key modifications:\n1. Full Embedding for Each Head: Each attention head $h_i$ processes the entire embedding $e_t \\in \\mathbb{R}^d$, avoiding division:\n$q^{h_i} = W_q^{h_i} e_t \\quad k^{h_i} = W_k^{h_i} e_t \\quad v^{h_i} = W_v^{h_i} e_t$\n$W_q^{h_i}$, $W_k^{h_i}$, $W_v^{h_i} \\in \\mathbb{R}^{d \\times d'}$ are learned projection matrices, with $d'$ representing the head-specific projection dimensionality.\n2. Learnable Projection: The outputs of all heads are concatenated:\n$y_t = [y_t^{h_1}; y_t^{h_2};...; y_t^{h_N}] \\in \\mathbb{R}^{h \\times d'}$\nTo handle the increased dimensionality and retain key semantics, a learnable projection matrix, $W_o \\in \\mathbb{R}^{(h \\times d') \\times d}$, reduces the dimensionality back to $d$ while preserving the most relevant semantic information:\n$y_t^{final} = W_o y_t$\nSCMHSA provides:\n\u2022 Semantic Integrity: Each head retains the full semantic context, avoiding dilution.\n\u2022 Holistic Representation: All heads process complete embeddings for richer understanding.\n\u2022 Dimensionality Management: The learnable matrix $W_o$ filters irrelevant features, preserving key information."}, {"title": "3.3.3. Prediction Layer", "content": "The SC-VFP module outputs a sequence of vectors $Y = \\{y_1^{final}, y_2^{final},..., y_M^{final}\\}$, where $M$ is the number of previous frames, and $y_t^{final} \\in \\mathbb{R}^d$ represents the processed embedding of the $t$-th frame, enriched with the full semantic information. The Prediction Layer (implemented as an MLP), takes the output from SC-VFP module and generates a vector $\\hat{e}_{M+1} \\in \\mathbb{R}^d$ that predicts the embedding of the next frame in the sequence.\nThe prediction layer is designed to translate the temporal and spatial information captured by SC-VFP into an accurate prediction of the next video frame's embedding."}, {"title": "3.3.4. Our Loss Function", "content": "In existing Transformer-based VFP systems, the model predicts the next frame's embedding, but the loss is computed on the reconstructed frame [8], [16], [6], [28], which can introduce learning discrepancies. To address this, we propose a new loss function that directly optimizes the predicted embedding and enhances the effectiveness of the SCMHSA module.\nThe proposed loss function $\\mathcal{L}$ consists of two main components:\n1. Embedding MSE loss: This component directly measures the error between the true embedding $e_t$ and the predicted embedding $\\hat{e}_t$. This helps to ensure the model"}, {"title": "2. Semantic Similarity loss", "content": "Given that SCMHSA allows each head to receive the full input embedding, it is crucial to ensure that the heads capture distinct, non-overlapping semantic information. The semantic similarity loss achieves this by penalizing heads that produce similar outputs.\nFor each pair of heads $h_i$ and $h_j$ (where $i \\neq j$), this component computes the row-wise cosine similarity as the average angle distance between $h_i$ and $h_j$. The sum of these row-wise cosine similarities is averaged across all pairs of heads and all rows in the head vector. The Semantic Similarity loss can be expressed as:\n$\\mathcal{L}_{SS} = \\frac{1}{N(N-1)} \\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} \\frac{1}{M} \\sum_{k=1}^{M} |cos(h_{i,k}, h_{j,k})|$\nwhere $N$ is the number of heads, $M$ is the number of rows in each head vector (which is also the length of the input sequence), and $h_{i,k}$ is the $k$-th row of head $h_i$.\nThe total loss function $\\mathcal{L}$ is a weighted sum of the two components:\n$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\mathcal{L}_{SS}$\nwhere $\\lambda$ is a hyperparameter that controls the trade-off between the MSE loss and the Semantic Similarity loss."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "We conducted our experiments using four datasets: KTH, UCSD Pedestrian, UCF Sports, and Penn Action. Each training instance comprised six frames: five input frames and a sixth as the label. Instead of selecting five consecutive frames as input, we chose one frame for every five to avoid similarity issues with consecutive frames. This approach also reduces the amount of training data that needs to be processed. All input frames were resized to 224 \u00d7 224 for compatibility with the ViT model [5]."}, {"title": "4.1.1. \u039a\u03a4\u0397 [17]", "content": "The KTH dataset is widely used for human action recognition, containing 600 video sequences with a resolution of 160 \u00d7 120, and recorded at 25 fps. For our experiments, we use the walking and running classes."}, {"title": "4.1.2. UCSD Pedestrian [11]", "content": "The UCSD Pedestrian dataset features video footage from a stationary, elevated camera capturing pedestrian walkways to identify abnormal events. The dataset has two subsets: Peds1, with 34 training and 36 testing videos of people walking towards or away from the camera, and Peds2, with 16 training and 12 testing videos of pedestrians moving parallel to the camera. Each clip has 200 black-and-white frames at a 238 \u00d7 158 resolution."}, {"title": "4.1.3. UCF Sports [20]", "content": "The UCF Sports dataset contains 150 video sequences, each with a resolution of 720 \u00d7 480, and encompasses 10 action classes. This dataset is useful for studying human actions in sports contexts, offering diverse scenarios and movements."}, {"title": "4.1.4. Penn Action [30]", "content": "The Penn Action dataset contains 2,326 video sequences with a resolution of 640 \u00d7 480, covering 15 action classes, such as baseball pitching, pull-ups, and guitar strumming. It is valuable for action recognition tasks due to its diverse range of physical activities and detailed labeling of dynamic motions."}, {"title": "4.2. Metrics", "content": "Since our approach operates within the embedding space rather than the pixel space, common VFP metrics such as LPIPS [29] and SSIM are not applicable. Instead, we employ PSNR and MSE as our evaluation metrics. While PSNR is conventionally calculated using the pixel values, we modify it to utilize the embedding values."}, {"title": "4.3. Implementation Details", "content": "The proposed model was implemented using PyTorch\u00b9. Training and evaluation were conducted on an NVIDIA A100 40GB GPU. The model's architecture consisted of 6 encoder blocks, each with 6 attention heads and a frame embedding dimension of 768. The sequence length was set to 5. For optimization, we used the AdamW optimizer [9] with a learning rate of 1e-4. The batch size was set to 32, and the model was trained for 25 epochs. The dataset was split into training, validation, and test sets with a ratio of 0.7, 0.15, and 0.15, respectively. To ensure reproducibility, we fixed the random seed at 2023."}, {"title": "4.4. Results", "content": "We present the results of our proposed method, comparing it with the relevant and latest next-frame predictors that we know: PredRNN [22], SA-ConvLSTM [8], MIMO-VP [16], LFDM [15], VFP-ImageEvent [32], and ExtDM [31] across the four datasets previously mentioned. All methods were evaluated on the test set of these four datasets. The quantitative results and comparisons are summarized in Table 1. In addition to the quantitative comparison, we present a qualitative comparison in Figure 3. Unlike conventional next-frame prediction models, which directly predict the next frame, our proposed method predicts the embedding of the next frame instead. The ground truth embedding of the next frame is generated using ViT [5]. Figure 3 we visualize the error maps of the predicted embedding and the ground truth embedding for each method. The qualitative results demonstrate how closely the predicted embeddings from our method align with the ground truth. The darker the error map appears, the worse the prediction is to the ground truth. We also compare the cosine similarity between the embedding predicted by our method (SC-VFP) and the ground truth embedding, together with other methods in Figure 4."}, {"title": "4.4.1. Results on KTH", "content": "KTH is the only dataset where our method exhibited inferior performance compared to others. Specifically, SC-VFP achieved a PSNR score that was 7.01% lower than the best-performing method and an MSE score that was 59.94% lower (Table 1). This can be attributed to the relatively small size of the KTH dataset compared to the other three datasets used in our experiments. As a result, the issue of semantic dilution is less pronounced during training and testing on this dataset. In contrast, larger datasets contain more diverse semantics, increasing the potential impact of semantic dilution, which may result in more noticeable distinctions in performance."}, {"title": "4.4.2. Result on UCSD Pedestrian", "content": "On the UCSD dataset, our SC-VFP method outperformed all others, achieving the lowest MSE of 86.71 and the highest PSNR of 28.75. These results represent an 16.14% improvement in MSE and a 2.59% improvement in PSNR over the next best method (VFP-ImageEvent) (Table 1)."}, {"title": "4.4.3. Result on UCF Sports", "content": "On the UCF Sports dataset, which is larger and more complex than KTH and UCSD, we observe a more significant performance gap between SC-VFP and others. Specifically, the MSE was reduced by 38.3%, and the PSNR increased by 4.84% over the nearest competitor (ExtDM). These results further validate our hypothesis, suggesting that addressing semantic dilution is crucial for handling the more complex motion patterns found in larger datasets."}, {"title": "4.4.4. Result on Penn Action", "content": "Evaluating on the largest dataset (Penn Action), we observe a continued trend of improvement on larger and more complex datasets. SC-VFP once again establishes a substantial performance gap, outperforming the second-best method by 68.71% in MSE and 6.63% in PSNR. This demonstrates the scalability and robustness of our approach, confirming the suitability of SC-VFP for advanced video prediction tasks where fine-grained semantic information is crucial."}, {"title": "4.5. Ablation Study", "content": ""}, {"title": "4.5.1. Parameter Analysis", "content": "Our proposed SCMHSA model (42.7M parameters) introduces an increase in parameter count compared to the original Transformer-based model (31.4M parameters) due to its complete embedding processing in each attention head, which mitigates semantic dilution. This design results in approximately 1.35x more parameters than the baseline, enabling richer semantic representations and improving predictive accuracy, particularly on complex datasets. While our model has a higher parameter count relative to the original Transformer, experiments show that the additional parameters in SCMHSA significantly enhance the model's ability to capture complex spatiotemporal dynamics, leading to notable improvements in MSE and PSNR metrics across diverse datasets, as demonstrated in the next section. The parameter counts of our method and the original Transformer-based method are shown in Table 2."}, {"title": "4.5.2. Performance Analysis", "content": "To verify the contributions of SCMHSA and the Semantic Similarity loss modules, we performed an ablation study on the test set of four datasets (KTH, UCSD, UCF Sports, Penn Action). Table 3 demonstrates that excluding SCMHSA leads to varied impacts across different datasets. On the KTH dataset, SC-VFP without SCMHSA observes a better performance (0.45% lower on MSE and 0.07% higher on PSNR). In contrast, on the UCSD dataset, incorporating SCMHSA gains better performance, which decreases the MSE by 28.87% and enhances the PSNR by 3.97%. The effect is even more pronounced on the UCF Sports dataset, where the exclusion of SCMHSA significantly worsens the MSE by 45.29% and PSNR by 9.89%. Finally, on the Penn Action dataset, the inclusion of SCMHSA improves the MSE and PSNR values by 35.71% and 6.92%, respectively. These results underscore the critical role of the SCMHSA mechanism in mitigating the semantic dilution problem, thereby contributing significantly to accurate embedding predictions, particularly in larger and more complex datasets. Figure 5 illustrates the convergence trend of SC-VFP with and without SCMHSA on the training set of the Penn Action dataset. The results indicate that SC-VFP with SCMHSA not only achieves a lower loss but also converges more quickly compared to the variant without SCMHSA."}, {"title": "5. Conclusion", "content": "In this paper, we have addressed the issue of semantic dilution in Transformer-based video frame prediction (VFP) systems by introducing a novel solution called Semantic Concentration VFP (SC-VFP). Our approach leverages the Semantic Concentration Multi-head Self-attention (SCMHSA) mechanism and a new loss function designed to align more effectively with VFP outputs. This loss function enhances the ability of high-dimensional attention heads to capture unique, non-overlapping semantics. While SC-VFP demonstrated inferior performance on the smaller dataset (KTH), it achieved state-of-the-art results on three larger datasets (UCSD, UCF Sports, and Penn Action). These results underscore the suitability of SC-VFP for larger datasets that contain more diverse semantics, where the impact of semantic dilution is more pronounced and the need for fine-grained semantic information is critical."}]}