{"title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation", "authors": ["Philippe Hansen-Estruch", "David Yan", "Ching-Yao Chung", "Orr Zohar", "Jialiang Wang", "Tingbo Hou", "Tao Xu", "Sriram Vishwanath", "Peter Vajda", "Xinlei Chen"], "abstract": "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation - and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.", "sections": [{"title": "1 Introduction", "content": "Modern methods for high-fidelity image and video generation rely on two components: a visual tokenizer that encodes pixels into a lower-dimensional latent space and subsequently decodes, and a generator that models this latent representation. Although numerous works have improved the generators through scaling of Transformer-based architectures, the tokenizers themselves, predominantly based on convolutional neural networks (CNNs), have seldom been the focus of scaling efforts.\nIn this paper, we investigate whether visual tokenizers warrant the same scaling efforts as generators. To enable this, we first address two primary bottlenecks: architectural limitations and data scale. First, we replace convolutional backbones with a Transformer-based auto-encoder, specifically adopting the Vision Transformer (ViT) architecture enhanced with Llama, which has demonstrated effectiveness in large-scale training. Our resulting auto-encoder design, which we refer to as Vision Transformer Tokenizer or ViTok, combines easily with the generative pipeline in Diffusion Transformers (DiT). Second, we train our models on large-scale, in-the-wild image datasets that significantly exceed ImageNet-1K and extend our approach to videos, ensuring that our tokenizer scaling is not constrained by data"}, {"title": "2 Background", "content": "We review background on continuous visual tokenizers and then describe ViTok to enable our exploration."}, {"title": "2.1 Continuous Visual Tokenization", "content": "The Variational Auto-Encoder (VAE) is a framework that takes a visual input $X \\in \\mathbb{R}^{T \\times H \\times W \\times 3}$ (where $T = 1$ for images and $T > 1$ for videos) is processed by an encoder $f_e$, parameterized by $\\theta$. This encoder performs a spatial-temporal downsampling by a factor of $q \\times p \\times p$, producing a latent code. The encoder outputs parameters for a multivariate Gaussian distribution\u2014mean $z_m$ and variance $z_v$ with $c$ channel size.:\n$z \\sim N(z_m, z_v) = Z = f_e(X) \\in \\mathbb{R}^{\\frac{T}{q} \\times \\frac{H}{p} \\times \\frac{W}{p} \\times c}$,\nThe sampled latent vector $z$ is then fed into a decoder $g_\\varphi$, with parameters $\\varphi$, which reconstructs the input image $\\hat{X} = g_\\varphi(z)$. The primary objective of the auto-encoder is to minimize the mean squared error between the reconstructed and original images, $\\mathcal{L}_{REC}(X, \\hat{X})$. To regularize the latent distribution to a unit Gaussian prior which is necessary to recover the variational lower bound, a KL divergence regularization term is added which we refer to as $\\mathcal{L}_{KL}$. Recent advancements in VAEs used for downstream generation tasks incorporate additional objectives to improve the visual fidelity of the reconstructions. These include a perceptual loss based on VGG features $\\mathcal{L}_{LPIPS}$ and an adversarial GAN objective, $\\mathcal{L}_{GAN}$. The comprehensive loss function for the auto-encoder, $\\mathcal{L}_{AE}(X, \\hat{X}, Z)$, is formulated as:\n$\\mathcal{L}_{AE}(X, \\hat{X}, Z) = \\mathcal{L}_{REC}(X, \\hat{X}) + \\beta \\mathcal{L}_{KL}(Z) + \\eta \\mathcal{L}_{LPIPS}(X, \\hat{X}) + \\lambda \\mathcal{L}_{GAN}(X, \\hat{X})$\nwhere $\\beta$, $\\eta$, and $\\lambda$ are weights that balance the contribution of each term to the overall objective. We largely utilize the same overall loss, but ablate on the impact of each term in Section 3.4."}, {"title": "2.2 Scalable Auto-Encoding Framework", "content": "We now develop our visual tokenizer and pinpoint bottlenecks that we explore further in Section 3. The basic structure follows that of a variational auto-encoder (VAE) with an encoder-decoder architecture, but rather than relying on CNNs, we adopt a Vision Transformer (ViT) approach for better scalability. Our method builds on the ViViT framework to handle both images and videos. Specifically, a 3D convolution with kernel and stride size $q \\times p \\times p$ first tokenizes the input X into a sequence $X_{embed} \\in \\mathbb{R}^{B \\times L \\times C_f}$, where $L = \\frac{T}{q} \\times \\frac{H}{p} \\times \\frac{W}{p}$ and $C_f$ is the transformer's feature dimension. A ViT encoder then processes $X_{embed}$, and a linear projection reduces the channel width to produce a compact representation $Z = f_e(X_{embed}) \\in \\mathbb{R}^{B \\times L \\times 2c}$. Following the VAE formulation (Section 2), we recover $z \\in \\mathbb{R}^{B \\times L \\times c}$. We define\n$E = L \\times c,$\nwhich effectively controls our compression ratio by specifying the total dimensionality of the latent space. As Section 3 highlights, $E$ is pivotal in predicting reconstruction performance. Both $c$ and $E$ are very important for generative performance as well. Though $E$ can be influence also by the number of tokens $L$, so we can potentially keep $c$ low while increasing $L$ for increased $E$.\nFor the decoder, a linear projection upsamples $z$ from $c$ to $C_g$ channels, after which a ViT decoder processes the tokens to predict $X_{embed}$. Finally, a 3D transposed convolution recovers the original input resolution, producing $\\hat{X}$. This covers the high level process of Vision Transformer Tokenizer or ViTok. Figure 1 illustrates this process. We denote ViTok configurations by specifying their encoder size, decoder size, and patch/stride parameters $(q, p)$. For instance, ViTok S-B/4x16 indicates a small encoder, a base decoder, and a patch stride of $q = 4, p = 16$. Table 1 provides details on the ViTok sizes."}, {"title": "2.3 Experiment Setup and Training", "content": "We detail the training process for ViTok that will enable our exploration in Section 3.\nTraining stages. Due to the known instability of adversarial objectives in VAE frameworks, we stage our training of ViTok into two parts. Stage 1 uses only the MSE, LPIPS, and KL terms, following Equation 1 with $\\beta = 1 \\times 10^{-3}$, $\\eta = 1.0$, and $\\lambda = 0$. This setup ensures a stable auto-encoder that performs well. Stage 2 then introduces an adversarial loss, freezing the encoder $f_e$ while fine-tuning only the decoder $g_\\varphi$. Here, we switch to $\\beta = 1 \\times 10^{-3}$, $\\eta = 1.0$, and $\\lambda = 1.0$ in Equation 1. For images, this adversarial component follows standard GAN-based VAE techniques. For videos, we treat each frame independently by flattening the video into batches of frames, computing LPIPS and GAN losses on a frame-by-frame basis. This two-stage approach preserves the encoder's stability while enabling generative refinement in the decoder.\nArchitecture, datasets, and training details. We employ a Vision Transformer (ViT) setup for both our encoder and decoder, drawing on several modifications from Llama. In particular, we adopt SwiGLU and 3D Axial RoPE to better capture spatiotemporal relationships.\nSince we aim to scale our models without being constrained by data size, we train our auto-encoders on large-scale datasets. For images, we use the Shutterstock image dataset (450M images) and ImageNet-1K (1.3M images), evaluating reconstruction on the ImageNet-1K validation set and COCO-2017 validation set. For video training, we employ the Shutterstock video dataset (30M videos, each with over 200 frames at 24 fps), and validate on UCF-101 and Kinetics-700.\nStage 1 training runs for 100,000 steps, with a batch size of 1024 for images and 256 for videos. We then finetune for Stage 2 for another 100,000 steps, using a reduced batch size of 256 for images and 128 for videos. We use the AdamW optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, a peak learning rate of $1 \\times 10^{-4}$ (scaled by batch size \u00d7 frames), a weight decay of $1 \\times 10^{-4}$, and a cosine decay schedule. When a discriminator is used in Stage 2, we utilize StyleGAN and set the discriminator learning rate to $2 \\times 10^{-5}$, with a linear warmup of 25k steps. We use bfloat16 autocasting for all training, apply no exponential moving average (EMA) in Stage 1, and introduce EMA at 0.9999 in Stage 2.\nReconstruction evaluation metrics. To gauge reconstruction quality, we use Fr\u00e9chet Inception Distance (FID), Inception Score (IS), Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR). For video, we report rFID (frame-wise FID) and Fr\u00e9chet Video Distance (FVD) over entire videos, denoted as rFID and FVD respectively. We refer to these reconstruction-specific metrics as rFID, rIS, rSSIM, and rPSNR.\nGeneration experiments and metrics. To assess our tokenizers in a large-scale generative setting, we train a class-conditional DiT-L with 400M parameters for 500,000 steps and a batch size of 256, applying classifier-free guidance (CFG) on a DDIM sampler over 250 steps and CFG scales of 1.5 and 3.0. We apply the same Llama upgrades to our DiT as for our tokenizers. We measure generation quality using gFID and gIS computed over 50,000"}, {"title": "3 Bottlenecks, Scaling, and Trade-offs in Visual Tokenization", "content": "In Section 2, we introduced ViTok and outlined its training process. Here, we examine the impact of scaling three key factors-bottleneck size, encoder size, and decoder size on both reconstruction and generation performance. First, in Section 3.1, we examine scaling the primary bottleneck in reconstruction: the total number of floating points E (Equation 2) in the latent representation. Next, in Section 3.2, we test how this bottleneck effects generation results. Then, in Section 3.3, we analyze the impact of scaling the encoder and decoder size. Afterward, in Section 3.4, we analyze the decoder as an extension of the generative model and examine how the choice of objective in Equation 1 influences the trade-off in reconstruction. Finally, in Section 3.5, we extend our study to video data, highlighting key similarities and differences relative to image-based auto-encoding. Unless stated otherwise, all experiments in this section use Stage 1 training from Section 2.3 to ensure stable and consistent comparisons."}, {"title": "3.1 E as the Main Bottleneck in Image Reconstruction", "content": "In prior discrete cases performance depends on the number of tokens (L) and the size of the discrete codebook per token. For ViTok, the analogous factor is E (Equation 2), which proves to be the critical determinant of reconstruction performance. The bottleneck E is related to the number of pixels per floating point, $\\frac{T \\times H \\times W \\times 3}{E} = \\frac{H \\times W \\times 3}{E}$, representing the degree of compression applied.\nTo fully understand how E functions as a bottleneck, we performed an extensive sweep through various configurations of ViTok investigating performance on 256p image reconstruction. For our first experiment, we look to explore all combinations of patch size $p = \\{32,16,8\\}$ and channel widths $c = \\{4, 8, 16, 32, 64\\}$ which gives various E between $2^8$ to $2^{16}$. The patch size influences $L = \\frac{H \\times W}{p \\times p}$ and the amount of flops expended by the model due the quadratic nature of attention, while c dictates the extent of the bottleneck between the encoder and the decoder. For these experiments, we fixed the encoder size to Small and the decoder to Base (Table 1). Our findings on scaling E with 256p images are summarized in Figure 2. We provide more details and results in Appendix A.\nFigure 2 illustrates a strong correlation between E and rFID/rIS/rSSIM/rPSNR. This indicates that E is a significant predictor of the quality of the reconstruction, regardless of the shape of the code. Also, the behavior between different datasets reconstruction performance is similar with rFID changing slightly due to the size of the validation set difference (50k for ImageNet-1K vs 5k for COCO). Furthermore, for the same E,"}, {"title": "3.2 The Impact of E in Image Generation", "content": "In this section, we investigate how E influences performance in generative tasks by following the training protocol from Section 2.3 and using the same set of tokenizers evaluated in Figure 2. The results are in Figure 5.\nThe generative results exhibit a different trend compared to reconstruction, showing little to no linear correlation between log(E) and the generative metrics log(gFID) or gIS. Figure 5 reveals that each patch size has an optimal E, leading to a second-order trend. The optimal configurations are $p = 16, c = 16, E = 4096$; $p = 8, c = 4, E = 4096$; and $p = 32, c = 32, E = 2048$ for their respective patch sizes. Additionally, higher CFG settings tend to minimize the differences in gFID across various E values. However, for gIS, higher channel sizes (c > 32) and variants with poor reconstruction quality still result in poorer image quality, indicating that excessive channel sizes negatively impact performance despite CFG adjustments.\nCloser analysis reveals that a low E often bottlenecks the generative model, as the auto-encoder struggles with effective image reconstruction. Conversely, a high E, primarily driven by larger channel sizes (c), complicates model convergence and degrades both gFID and gIS metrics. These findings are corroborated by concurrent work that details a trade off between rFID and gFID in latent diffusion models. This highlights a critical trade-off in current latent diffusion models: E and c must be kept as low as possible to enhance generation performance while maintaining it high enough to ensure quality reconstructions. We provide generation visualizations for each tokenizer and trained DiT model in Appendix B."}, {"title": "3.3 Scaling Trends in Auto-Encoding", "content": "We aim to explore how scaling impacts auto-encoding in both reconstruction and generation tasks using ViTok. To test this, we fix the parameters to $p = 16, c = 16, L = 256, E = 4096$ for ViTok. We then conduct a sweep over different encoder and decoder sizesS-S, B-S, S-B, B-B, S-L, B-L, L-L defined in Table 1, following the same training protocol as described in Section 2.3. The results are reported in Figure 6 and 7.\nAs illustrated in Figure 6, the size of the encoder is not correlated with the reconstruction performance. In contrast, Figure 7 shows that the size of the decoder is positively correlated with the reconstruction"}, {"title": "3.4 A Trade-Off in Decoding", "content": "As shown in Section 3.3, increasing the size of the decoder improves reconstruction, suggesting that the decoder behaves more like a generative model for the input X and thus needs more computation than the encoder. To illustrate this, we compared how different losses balance traditional compression metrics (SSIM/PSNR) against generative metrics (FID/IS). SSIM/PSNR measure visual fidelity or how much of the original information is preserved, while FID/IS focus on visual quality and how closely outputs match the real dataset. This comparison shows how different choices of losses can shift the decoder's role from strictly reconstructing to more actively generating content.\nWe conducted these experiments on ViTok by fixing $p = 16, c = 16$, and $E = 4096$. We then trained with stage 1 and varied the LPIPS loss weight $\\lambda \\in \\{0.0, 0.5, 1.0\\}$ combined with the choice of L1 or L2 reconstruction loss (Equation 1). We also include our Stage 2 results following Section 2.3 to see the effect of the generative adversarial loss.\nFigure 10 shows a clear trade-off among these losses. Without perceptual loss, we get worse rFID/rIS scores but better rSSIM/rPSNR, indicating that a strict MSE-based approach preserves the most original information. Increasing $\\lambda$ gradually lowers SSIM/PSNR while improving FID/IS. Finally, fine-tuning the decoder with a GAN pushes these generative metrics further, achieving an rFID of 0.50 at the cost of lower SSIM/PSNR."}, {"title": "3.5 Video Results", "content": "We extend the application of ViTok to video tasks to examine the impact of E on video reconstruction and to investigate redundancy in video data. To enable a direct comparison with our image results, we maintain a resolution of 256p and utilize 16-frame videos at 8 fps for both training and evaluation. Tokenizing videos can result in very large sequence lengths; for example, a tubelet size of 4\u00d78 (with temporal stride q = 4 and spatial stride p = 8) for a video of dimensions 16\u00d7256\u00d7256 yields a sequence length of 4096 tokens. Therefore,"}, {"title": "4 Experimental Comparison", "content": "In this section, we compare our auto-encoders to prior work on image reconstruction at resolutions of 256p and 512p, as well as video reconstruction with 16 frames at 128p. We utilize the S-B/16 and S-L/16 ViTok variants for image tasks and the S-B/4x8, S-B/4x16, and S-B/8x8 ViTok variants for video tasks, as detailed in Table 1. Training these tokenizers follows the Stage 1 and Stage 2 protocol outlined in Section 2.3."}, {"title": "4.1 Image Reconstruction and Generation", "content": "We evaluate our models on image reconstruction and class-conditional image generation tasks using the ImageNet-1K and COCO-2017 datasets at resolutions of 256p and 512p. For image reconstruction, we compare our continuous tokenizer-based models against several state-of-the-art methods,"}, {"title": "4.2 Video Reconstruction and Generation", "content": "For our video comparison, our reconstruction metrics are computed on the UCF-101 training set and compared against state-of-the-art methods including TATS, LARP, and MAGViTv1/v2. The results are presented in Table 5. Our tokenizers demonstrate very competitive performance relative to prior work. Specifically, S-B/4x8 (1024 tokens) achieves state-of-the-art (SOTA) rFVD results compared to other CNN-based continuous tokenizers with the same total compression ratio. When applying further compression, the rFVD metrics show a slight degradation; however, they remain highly competitive with existing methods. Notably, our S-B/8x8 (512 tokens) variant matches the performance of LARP, which operates with 1024 tokens. Additionally, our approach significantly reduces FLOPs compared to Transformer-based prior method LARP, underscoring the efficiency and versatility of ViTok.\nWe further evaluate our models on class-conditional video generation using the UCF-101 dataset. We train a DiT-L model across all compression variants for 500K steps on the UCF-101 training set, computing gFID and gFVD metrics with a batch size of 256 and a learning rate of $1 \\times 10^{-4}$. The results are summarized in Table 7. ViTok achieves SOTA gFVD scores at 1024 tokens and maintains highly competitive gFVD scores at 512 tokens (\u00d78 by \u00d78 compression), representing the most efficient level of token compression for any tokenizer so far. At 256 tokens, ViTok's performance experiences a further decline but remains competitive within the field. Example video generations using our 1024-token configuration are illustrated in Figure 16."}, {"title": "5 Related Work", "content": "Image tokenization. High-resolution images have been compressed using deep auto-encoders, a process that involves encoding an image into a lower-dimensional latent representation, which is then decoded to reconstruct the original image. Variational auto-encoders (VAEs) extend this concept by incorporating a probabilistic meaning to the encoding. VQVAES introduce a vector quantization (VQ) step in the bottleneck of the auto-encoder, which discretizes the latent space. Further enhancing the visual fidelity of reconstructions, VQGAN integrates adversarial training into the objective of VQVAE. RQ-VAE modifies VQVAE to learn stacked discrete 1D tokens. Finally, FSQ simplifies the training process for image discrete tokenization to avoid additional auxiliary losses.\nWhile ConvNets have traditionally been the backbone for auto-encoders, recent explorations have incorporated Vision Transformers (ViT) to auto-encoding. ViTVQGAN modifies the VQGAN architecture to use a ViT and finds scaling benefits. Unified Masked Diffusion uses a ViT encoder-decoder framework for representation and generation tasks. TiTok introduces a 1D tokenizer ViT that distills latent codes from VQGAN. Finally, ElasticTok is concurrent work and utilizes a similar masking mechanism, though their paper focuses on reconstruction and does not try generation tasks.\nVideo tokenization. VideoGPT proposes using 3D Convolutions with a VQVAE. TATS utilizes replicate padding to reduce temporal corruptions issues with variable length videos. Phenaki utilizes the Video Vision Transformer (ViViT) architecture with a factorized attention using full spatial and casual temporal attention. MAGViTv1 utilizes a 3D convolution with VQGAN to learn a video tokenizer coupled with a masked generative portion. The temporal auto-encoder (TAE) used in Movie Gen is a continuous noncausal 2.5D"}, {"title": "6 Conclusion", "content": "In this paper, we explored scaling in auto-encoders. We introduced ViTok, a ViT-style auto-encoder to perform exploration. We tested scaling bottleneck sizes, encoder sizes, and decoder sizes. We found a strong correlation between the total number of floating points (E) and visual quality metrics. Our findings indicate that scaling the auto-encoder size alone does not significantly enhance downstream generative performance. Specifically, increasing the bottleneck size improves reconstruction quality but complicates training and negatively impacts generation when the latent space becomes too large. Additionally, scaling the encoder often fails to boost performance and can be detrimental, while scaling the decoder offers mixed results-enhancing reconstruction but not consistently improving generative tasks. These trends hold true for both image and video tokenizers, with our proposed ViTok effectively leveraging redundancy in video data to achieve superior performance in video generation tasks.\nThe best performing ViTok from our sweep achieves highly competitive performance with state-of-the-art tokenizers, matching rFID and rFVD metrics while requiring significantly fewer FLOPs. In benchmarks such as ImageNet, COCO, and UCF-101, ViTok not only matches but in some cases surpasses existing methods, particularly in class-conditional video generation. Our study highlights critical factors in the design and scaling of visual tokenizers, emphasizing the importance of bottleneck design and the nuanced effects of encoder and decoder scaling. We hope that our work will inspire further research into effective Transformer-based architectures for visual tokenization, ultimately advancing the field of high-quality image and video generation."}, {"title": "A Extra Experiments", "content": "We provide additional details on the implementaiton of ViTok. Our implementation is based on the Video-MAEv2 (Wang et al., 2023) codebase and inspired by the Big Vision codebase (Beyer et al., 2022). Utilizing PyTorch (Paszke et al., 2019), we employ Distributed Data Parallel (DDP) for efficient multi-GPU training, along with activation checkpointing, bfloat16 precision, and Torch Compile optimizations. For image models, we train using 8 NVIDIA H100 GPUs, where ViTok S-B/16 requires approximately 6-12 hours for stage 1 and 3-6 hours for stage 2 on 256p and 512p resolutions. In comparison, DiT image models take around 72-96 hours to train for 4 million steps on the same resolutions. For video models, ViTok S-B/4x8 is trained on 16 NVIDIA H100 GPUs, taking about 24 hours for stage 1 and 12 hours for stage 2 on 256p, 16-frame videos, and 12 hours for 128p, 16-frame videos. DiT video models require roughly 48-96 hours to train for 500k steps with a batch size of 256. Our transformer architecture is based on the Vision Transformer (ViT) (Dosovitskiy et al., 2021) and modified to incorporate elements from the Llama architecture, including SwiGLU (Shazeer, 2020) activation functions and 3D axial Rotary Position Embeddings (ROPE) (Su et al., 2024). The architecture consists of Transformer blocks (Vaswani et al., 2017) with multi-head self-attention and MLP layers, enhanced by residual connections (He et al., 2016) and layer normalization (Ba et al., 2016), closely following the Masked Autoencoder (MAE) design (He et al., 2022). Additionally, we integrate video processing code from Apollo (Zohar et al., 2024) and Video Occupancy Models (Tomar et al., 2024), enabling ViTok to effectively handle and exploit redundancy in video data, thereby improving both reconstruction metrics and compression efficiency. Overall, ViTok leverages advanced training techniques and architectural innovations to achieve state-of-the-art performance in image and video reconstruction and generation tasks."}, {"title": "A.1 Detailed 256p Image Results", "content": "We provide further detail of the ImageNet-1K validation reconstruction results from Figure 2 in Figure 17. Here we show different patch sizes and channels over E. This shows that regardless of patch size and FLOPs usage, E is highly correlated with the reconstruction perforance"}, {"title": "A.2 GAN Fine-tuning Ablation", "content": "In Figure 18, we study how various loss settings affect finetuning of the GAN decoder. Our goal is to highlight the trade-off and the decoder's transition toward more generative behavior. We use ViTok S-B/16 on 256p images, following the protocol in Section 2.3 for stage 2 fine-tuning from a model trained on stage 1.\nWe compare:\n\u2022 Finetuning the decoder with the same Stage 1 loss (no GAN).\n\u2022 Finetuning with discriminator learning rates (\\{1 \u00d7 10\u22125, 2 \u00d7 10-5,4 \u00d7 10-5\\}) and a GAN weight of 0.05.\n\u2022 Finetuning the full encoder/decoder with the GAN.\n\u2022 Using a higher GAN weight of 0.1 with a discriminator learning rate of 1 \u00d7 10\u22125.\nFrom Figure 18, the best setting is a GAN weight of 0.05 and a discriminator learning rate of 2 \u00d7 10-5. A higher discriminator learning rate causes training instabilities, while a lower rate degrades performance. Full finetuning works but does slightly worse than just finetuning the decoder. Finetuning without a GAN shows no improvement, confirming that GAN training is the primary driver of better results.\nFinally, we see an inherent trade-off: improving FID tends to worsen SSIM/PSNR, indicating that as the decoder focuses on visual fidelity, it shifts more toward generative outputs. This demonstrates the decoder's evolving role as a generative model to enhance visual performance."}, {"title": "A.3 Latent ViTok and Masked ViTok", "content": "In this section, we describe two variants of ViTok that provide different potential directions for tokenization. First we describe and evaluate our latent variation that does 1D tokenization and can form more arbitrary code shapes, then we discuss and evaluate our masking variant that allows for variable, adaptive tokenization.\nLatent ViTok Variation. Another variant of ViTok involves utilizing latent codes following Titok (Yu et al., 2024). Initially, after applying a tubelet embedding, we concatenate a set of 1D sincos initialized latent tokens with dimensions $l_{latent} \\times C_f$ to the tubelet token sequence $X_{embed}$. This combined sequence is then processed through the encoder and bottleneck using a linear layer. Subsequently, the tubelet tokens are discarded, and the latent tokens output by the encoder form $Z = l_{latent} \\times 2c$, from which we sample $z \\sim Z$. This gives us a 1D code with easy shape manipulation since L and c is arbitrarly decided and not dependent on p. In the decoder, z is upsampled to $C_g$, and we concatenate a flattened masked token sequence of length $L \\times C_g$ with the upsampled latent code $l_{latent} \\times C_g$. The decoder then predicts X in the same manner as the simple ViTok variation using the masked tokens. This approach allows for a more adaptive compression size and shape using self attention. Additionally, it accommodates arbitrary code shapes of different lengths than L, provided there is redundancy in the code. A trade-off compared to the simple ViTok is the increased total sequence length and computational cost (FLOPs) during encoding and decoding. We refer to this variant as Latent ViTok.\nWe train latent ViTok on stage 1 (Section 2.3) where we fix c = 16 and sweep the number of latent tokens L\u2208 \\{64, 128, 256, 512, 1024\\} to adjust E. The results are in Figure 19. Our simple variant outperforms the latent version for most values of E, although the latent version achieves slightly better rSSIM/rPSNR for certain choices of E. This indicates that the latent approach is a promising alternative to simple ViTok for more control over the latent space, but comes with an increased computational cost due to the longer sequence of concatenated tokens. We leave this implementation out of ViTok due to added complexity."}]}