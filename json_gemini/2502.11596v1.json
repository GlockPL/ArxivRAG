{"title": "LLM Embeddings for Deep Learning on Tabular Data", "authors": ["Boshko Koloski", "Andrei Margeloiu", "Xiangjian Jiang", "Bla\u017e \u0160krlj", "Nikola Simidjievski", "Mateja Jamnik"], "abstract": "Tabular deep-learning methods require embedding numerical and categorical input features into high-dimensional spaces before processing them. Existing methods deal with this heterogeneous nature of tabular data by employing separate type-specific encoding approaches. This limits the cross-table transfer potential and the exploitation of pre-trained knowledge. We propose a novel approach that first transforms tabular data into text, and then leverages pre-trained representations from LLMs to encode this data, resulting in a plug-and-play solution to improving deep-learning tabular methods. We demonstrate that our approach improves accuracy over competitive models, such as MLP, ResNet and FT-Transformer, by validating on seven classification datasets.", "sections": [{"title": "Introduction and Background", "content": "Tabular data is common in domains such as education [49], banking [33, 17] and medicine [20, 41, 48]. Thus, in recent years, tabular deep learning for predictive tasks has gained traction, with many architectures achieving state-of-the-art results [47]. A key challenge in applying neural networks to tabular data is determining how to effectively represent the features. Tabular data is typically represented with a mix of numerical and categorical features, living in high dimensions [25, 32]. A common approach to effectively representing features involves encoding numerical features differently from categorical ones [13, 1, 46, 40]. Once separated, various representation learning techniques are employed [4]. For example, numerical features are often transformed and then encoded using linear transformations, while categorical features are typically represented with lookup embeddings [6, 12, 45]. These embeddings provide a foundation for architectures that ensure permutation invariance at the feature level, such as transformers [42]. However, this approach is limiting because it separates features and trains individual embeddings from scratch. This is particularly restrictive for domains where data is scarce and usually consists of mixed numerical and categorical features.\nTabPFN [18] is a pre-trained tabular transformer trained on synthetic classification datasets, achieving competitive results and learning useful representations for various downstream predictors [31]. Despite its success, TabPFN has limited adaptability to real-world datasets, it only supports numerical data, and is constrained by the number of features and samples it can handle. Recently, TPBERTa[46] was proposed as an approach where features are first encoded separately based on their type, and then the ROBERTa [29] language model is fine-tuned across datasets. This method shows promising potential of utilising underlying LLM as a backbone, however it requires prior separate processing of numerical and categorical features, as well as tuning a relatively big model, which is limited by the number of features it can handle. Zhang et al. [50] showed the benefit of transforming tabular data to"}, {"title": "Methodology", "content": "Formal definition. Let D be a tabular dataset consisting of N samples and M features describing the input matrix X, used to predict the target vector y, where (Xi, Yi) denotes the input features and the label of the i-th row. Let g be an LLM-embedder that maps each feature m \u2208 M to a d-dimensional space RM\u00d7d. By mapping the whole input X, we obtain the embedded matrix E(X) \u2208 RN\u00d7M\u00d7d. Finally, a trainable model f (e.g., FT-Transformer) is trained on the embedded data to learn the mapping from the embedded input to the output, f(0; E) \u2192 \u0177.\nFeature Encoding with LLMs. We use LLMs as the encoding function g, which maps the input feature values to the LLM's embedding space. In particular, we first serialise the features into text and then embed the serialised text through the LLM to generate LLM-based embeddings. Previous work has shown that different prompting strategies have minimal impact [16]. Following this, we serialise the features using the template: This {col} is {value}.\nEmbedding adaptation. Next, we apply a shallow, one-layer neural network on top of the obtained LLM representations to project the LLM embeddings as inputs for the trainable model. The motivation behind this is twofold: (a) To ensure a fair comparison between models by projecting all embeddings to the same dimension, thus forcing the models to operate within the same dimensionality. (b) While the base variant models update their randomly initialised layers by fitting to the data, this approach introduces a similar adaptability mechanism to the frozen LLM embeddings, ensuring that they can be aligned with the downstream task."}, {"title": "Evaluation", "content": "We assess the impact of LLM-based embeddings on the classification accuracy on downstream tasks. To this end, we select seven datasets from three different domains: general, banking, and medical. Further details about the datasets can be found in Appendix D. To avoid contamination, we select the BGE [30] embedding as the LLM backbone representation, since it performs well on the MTEB benchmark [34], and works well for tabular data retrieval [23]. BGE was trained on text-only input formats without using any training steps, where data was translated from other formats (e.g., tables) into text to enhance the language modelling process.\nWe address three research questions: (a) Do LLM-based embeddings improve the performance of trainable predictors on downstream tasks? (b) Are LLM embeddings effective only across specific domains, or do they perform well in general contexts? (c) How does the choice of LLM affect the downstream performance?\nExperimental Setup. We evaluate the impact of training tabular neural networks with and without LLM-based embeddings. We fix the dimensionality of the embedding layer to 1024, independent of the model architecture, to ensure fairness. The weights of the LLM are frozen during model training. We perform 10 random train/test splits (with different random seeds), using a 70/30 stratified split of the data, and report the accuracy on the test set. We use 20% of the training split for validation. Models are trained for up to 100 epochs, with early stopping set to a patience of 10 and a minimum improvement delta of 0.01 in terms of validation loss."}, {"title": "Results and Discussion", "content": "Table 1 presents the results from the comparison between models with LLM embeddings (denoted as \"with LLM\") and the base variants (\u201cbase\u201d). We find that, on average, the LLM-based embeddings produce better results, outperforming the base trainable models."}, {"title": "Qualitative Visualisation of the Encoded Features", "content": "In this section, we present visualisations of selected features across different datasets, check Figure 3. We first embed the features using the proposed template and then project them into two dimensions via PCA. The visualisations show that both categorical and numerical features form meaningful clusters. For example, months, grade point average descriptions, and job titles group together. Interestingly, numerical values such as age, number of pregnancies, and medical measurements (e.g., total blood donated in c.c.) also cluster together. We attribute this semantic grouping to the primary influence of LLM-based embeddings on improved downstream performance."}, {"title": "Statistical Comparison", "content": "We conduct a hierarchical Bayesian t-test [3] across datasets and models to assess the statistical significance of the impact of LLM-based embeddings. In each setting, we use 7 datasets and perform 10 measurements per model. Following the recommendations in [3], we set the region of practical equivalence to 0.1%. Our results (consult Figure 4) demonstrate that our method is statistically more likely to outperform learned embeddings, supporting the applicability of our approach to current deep learning methods."}, {"title": "Limitations", "content": "A limitation of the proposed approach is its increased computational complexity. As each feature-value pair requires querying an additional LLM, complexity scales linearly with the number of features. While embeddings for finite-set features can be cached, continuous inputs require repeated querying, adding a persistent computational load. The results show that the choice of language model might influence downstream performance, contributing to the overall complexity. However, even small LLMs, tuned for embedding textual data, perform well, with BGE embeddings yielding strong results across tasks. Another limitation arises when feature names lack clear descriptions or are domain-specific and not well-covered by the LLM. To address this, integrating formal knowledge from knowledge bases, as suggested by [37], may provide a solution."}, {"title": "Related work", "content": "Feature Embeddings Embedding features is usually approached depending on the type of feature. Numerical features have traditionally been either transformed using a linear model [18, 13] or discretised by a variation of binning techniques [46, 19]. Binning as a technique for obtaining a pre-trained tabular deep learning model has shown promising results [27]. Categorical features are typically embedded via a lookup embedding layer [13, 19], with various techniques influencing their use [6, 12, 45]. Recently, tree-based feature embeddings have shown promising potential [46, 28]. Parallel to our work, [46] explored using word embeddings for embedding only feature names and categorical inputs, though they disentangled and embedded each word separately. To our knowledge, we are the first to explore LLM-based embeddings, both encoder- and decoder-based, for embedding of tabular data.\nLLMs and Tabular Data Learning on serialised tabular data to text has been prominent in mining relational tabular data [35, 26]. With the introduction of pre-trained models, a plethora of tabular models for table understanding and downstream prediction have been proposed, as shown in the recent survey [11]. The majority of these applications focus on serialising the inputs and fine-tuning large language models for prediction [16, 39, 50, 8]. Another line of work focuses on using LLMs as classifiers, where inputs are tokenised and mapped to the LLM vocabulary [46]. The idea of leveraging the potential of LLMs for transfer-learning across tables and feature encoding was shown as useful in [44], however they propose different encoding for different input types adding a complexity by design. Recently, focus to incorporate LLM priors by prompting them to order the input features and this was exploited by traditional ML models [51] showing promising results. LLMs showed remarkable potential as feature engineers as well [15]. However, using LLMs in this manner is either computationally heavy e.g. fine-tuning or requires careful prompt creation which can be laborious.\nLLMs and Text Embeddings Semantically embedding texts is one of the main tasks of interest in NLP, resulting in a benchmarking effort called the Massive Text Embedding Benchmark (MTEB) [34]. Traditionally, encoder-only model variants like sentence-based BERT [36] were popular, with"}]}