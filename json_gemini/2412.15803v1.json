{"title": "WebLLM: A High-Performance In-Browser LLM Inference Engine", "authors": ["Charlie F. Ruan", "Yucheng Qin", "Xun Zhou", "Ruihang Lai", "Hongyi Jin", "Yixin Dong", "Bohan Hou", "Meng-Shiun Yu", "Yiyan Zhai", "Sudeep Agarwal", "Hangrui Cao", "Siyuan Feng", "Tianqi Chen"], "abstract": "Advancements in large language models (LLMs) have unlocked remarkable capabilities. While deploying these models typically requires server-grade GPUs and cloud-based inference, the recent emergence of smaller open-source models and increasingly powerful consumer devices have made on-device deployment practical. The web browser as a platform for on-device deployment is universally accessible, provides a natural agentic environment, and conveniently abstracts out the different backends from diverse device vendors. To address this opportunity, we introduce WebLLM, an open-source JavaScript framework that enables high-performance LLM inference entirely within web browsers. WebLLM provides an OpenAI-style API for seamless integration into web applications, and leverages WebGPU for efficient local GPU acceleration and WebAssembly for performant CPU computation. With machine learning compilers MLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming the absence of performant WebGPU kernel libraries. Evaluations show that WebLLM can retain up to 80% native performance on the same device, with room to further close the gap. WebLLM paves the way for universally accessible, privacy-preserving, personalized, and locally powered LLM applications in web browsers.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have unlocked remarkable capabilities such as question-answering, code generation (Roziere et al. (2023)), and even reasoning (OpenAI (2024), Team (2024)). As the most capable models require server-grade GPUs, the models are typically hosted on the cloud during inference. However, open-source providers recently started releasing smaller models around 1 to 3 billion parameters that achieve competitive performance (Grattafiori et al. (2024), Abdin et al. (2024), Team et al. (2024), Hui et al. (2024)). Meanwhile, consumer devices have grown increasingly powerful: a 4-bit-quantized 3B model decodes 90 tokens per second on an Apple M3 laptop (Table 1). These trends make on-device LLM deployment both promising and practical. On-device deployment preserves privacy, enables personalization with local data, and unlocks new paradigms such as hybrid inference where cloud-based and on-device deployments co-exist (Qualcomm (2023)).\nThe web browser is an appealing platform for on-device deployment for three reasons. First, the browser is a natural agentic environment (Zhou et al. (2023)) for tasks like managing calendars, responding to emails, and creating documents \u2013 activities that could"}, {"title": "2 System Architecture and Key Components", "content": "WebLLM is a JavaScript framework that deploys LLMs locally in the client-side browser, enabling LLM-based features in web applications. Achieving this goal poses three challenges: WebLLM needs (1) a standardized API that web applications can easily incorporate; (2) adaptation to the browser's runtime environment; and (3) efficient GPU acceleration. As shown in Figure 1, WebLLM's architecture addresses these challenges by dividing the system into three parts correspondingly: a user-facing engine ServiceWorkerMLCEngine with endpoint-like behavior, an encapsulated MLCEngine that resides in the web worker (a background thread in JavaScript), and ahead-of-time compiled efficient WebGPU kernels."}, {"title": "2.1 An LLM Inference Engine", "content": "Web developers instantiate a lightweight ServiceWorkerMLCEngine in the web application frontend and treats it like an endpoint. The engine loads an LLM when specified, takes in an OpenAI-style request at any time, and streams back the output in an OpenAI-style response, which the web application can use to update the frontend.\nThis familiar, endpoint-like design brings several benefits. Endpoint-like APIs are JSON-in-JSON-out and thus have well-defined behavior. Besides, OpenAI-style API is widely adopted and makes WebLLM easy to integrate into existing projects. This design also allows WebLLM to extend to advanced features with minimal changes to the API. Advanced features that WebLLM supports with this API include: structured generation with JSON Schema and context-free grammar (Dong et al. (2024)), image input with vision language models (Abdin et al. (2024), Hui et al. (2024)), and loading multiple models in the same engine for applications like retrieval-augmented generation (Lewis et al. (2020))."}, {"title": "2.2 Adapting to the Browser Runtime", "content": "Unlike most LLM inference engines that are either C++ or Python-based, WebLLM is implemented in JavaScript. This non-conventional LLM runtime environment requires WebLLM to adapt to the technologies offered in browsers to ensure high performance.\nWeb workers LLM workloads are computationally heavy and could block the UI if run on the main thread. In JavaScript, web workers are used to separate heavy computation into background threads for a smooth UI. Therefore, WebLLM leverages web workers by having two engines: a lightweight frontend engine ServiceWorkerMLCEngine that is exposed to the web application, and a backend engine MLCEngine in the worker thread that actually computes the LLM workload (Figure 1). The two engines communicate via message-passing, and the messages are simply OpenAI requests and responses.\nWebGPU LLM inference requires GPU acceleration. WebGPU is a JavaScript API that allows web applications to leverage the device's GPU in the browser (Kenwright (2022)). WebGPU is also backend-agnostic: the same WebGPU kernel can run on devices with different GPU vendors, such as Apple laptops with M chips and laptops with NVIDIA GPUs. Therefore, WebLLM leverages WebGPU for any workload in LLM inference that requires GPU. We discuss in detail how such kernels are generated in \u00a72.3.\nWebAssembly Having WebGPU is not enough, as LLM inference also requires non-trivial computation on the CPU. WebAssembly (WASM) is a portable low-level bytecode that can be compiled from C++ code and run in JavaScript runtime with near-native performance (Haas et al. (2017)). Therefore, instead of re-implementing CPU workload in JavaScript, WebLLM leverages Emscripten (Zakai (2011)) to compile high-performance subsystems written in C++ into WebAssembly for various CPU workloads in LLM inference, including a grammar engine for structured generation (Dong et al. (2024)), sequence management in the paged KV cache (Team (2023)), and tensor manipulation for launching kernels (Chen et al. (2018)). This enables C++ code reuse for WebLLM without sacrificing performance."}, {"title": "2.3 GPU acceleration with WebGPU via MLC-LLM", "content": "GPU acceleration is crucial for high-performance LLM inference. WebGPU provides a standardized API to leverage GPU in JavaScript and abstracts out devices with different GPU vendors. However, unlike native backends such as CUDA, WebGPU does not have accelerated GPU libraries for common kernels. This makes it challenging to write high-performance customized GPU kernels such as PagedAttention and FlashAttention for WebGPU (Kwon et al. (2023), Dao et al. (2022)).\nWebLLM resolves this by leveraging machine learning compilation libraries MLC-LLM and Apache TVM to compile performant WebGPU kernels. MLC-LLM takes in any open-source model's implementation in Python, which uses techniques such as the aforementioned PagedAttention and FlashAttention, and compiles the model's computation into the backend of interest (in this case, WebGPU). Besides compiling to the specified target, MLC-LLM also provides both graph-level optimizations (e.g. kernel fusion) and operator-level optimizations (e.g. GEMM tiling) to ensure the kernel performance.\nMLC-LLM converts open-source models into two artifacts: converted weights and a WASM library. The WASM library contains both the WebGPU kernels and non-kernel functions in WebAssmebly. As shown in Figure 1, the models that WebLLM loads in are compiled ahead of time and hosted online."}, {"title": "3 Evaluation", "content": "We evaluate the performance of WebLLM by comparing its performance with MLC-LLM on the same Apple Macbook Pro M3 Max. The former leverages WebGPU kernels and a combination of JavaScript and WebAssembly runtime. The latter leverages Metal kernels and a combination of Python and C++ runtime. Our result shows that WebLLM preserves up to 80% of the decoding speed of MLC-LLM. There are various opportunities to further close the gap, including leveraging recent WebGPU features such as shuffling and a more careful optimization of WebLLM's runtime in general."}, {"title": "4 Conclusion", "content": "WebLLM demonstrates that high-performance, on-device LLM inference is feasible directly within the browser. As an open-source JavaScript framework, it empowers developers to integrate advanced LLM capabilities into web applications without sacrificing privacy or requiring server-level resources. WebLLM thus paves the way for accessible, personalized, and private LLM-driven experiences in everyday web usage."}]}