{"title": "Balance Benchmark: A Survey for Multimodal Imbalance Learning", "authors": ["Shaoxuan Xu", "Menglu Cui", "Chengxiang Huang", "Hongfa Wang", "Di Hu"], "abstract": "Multimodal learning has gained attention for its capacity to integrate information from different modalities. However, it is often hindered by the multimodal imbalance problem, where certain modality dominates while others remain underuti- lized. Although recent studies have proposed var- ious methods to alleviate this problem, they lack comprehensive and fair comparisons. In this paper, we systematically categorize various mainstream multimodal imbalance algorithms into four groups based on the strategies they employ to mitigate im- balance. To facilitate a comprehensive evaluation of these methods, we introduce BalanceBenchmark, a benchmark including multiple widely used multidimensional datasets and evaluation metrics from three perspectives: performance, imbalance degree, and complexity. To ensure fair compar- isons, we have developed a modular and extensi- ble toolkit that standardizes the experimental work- flow across different methods. Based on the ex- periments using BalanceBenchmark, we have iden- tified several key insights into the characteristics and advantages of different method groups in terms of performance, balance degree and computational complexity. We expect such analysis could inspire more efficient approaches to address the imbalance problem in the future, as well as foundation mod- els.", "sections": [{"title": "1 Introduction", "content": "Humans perceive the real world through multiple sensory modalities, such as visual, auditory, and haptic inputs. This rich interplay of modalities has driven extensive research into multimodal learning [Baltru\u0161aitis et al., 2019]. How- ever, recent studies have identified a critical challenge in this field: the multimodal imbalance problem, where certain modalities disproportionately dominate the behavior of mul-"}, {"title": "2 Multimodal imbalance learning", "content": "Multimodal learning aims to leverage diverse information from different modalities to enhance model performance [Baltru\u0161aitis et al., 2019]. However, recent studies have re- vealed the multimodal imbalance problem, where models tend to over-rely on some modalities while underutilizing oth- ers [Peng et al., 2022b]. This imbalance leads to suboptimal exploitation of the available multimodal information.\nWe consider a general multimodal learning framework for the illustration of imbalance phenomenon. Let $D_{train} = \\{(x_k, Y_k)\\}_{k=1}^N$ denote the multimodal training dataset. Each sample $x_k = (x_1, x_2,..., x_m)$ consists of m modalities, and $Y_k \\in \\{1,2,..., H\\}$ denotes the corresponding class label from H classes. In a multimodal model, each modality uses its own encoder $\\Phi^i(\\theta^i, \\cdot)$ with parameters $\\theta^i$. For simplicity,\n$f(x_k) = W[\\Phi_1; \\Phi_2; ... ; \\Phi_m] + b,$\\"}, {"title": "3 Taxonomy", "content": "In this section, we present our taxonomy for mitigating the multimodal imbalance problem based on the strategies for handling modality imbalance. As shown in Table 1, we cat- egorize these methods into four groups: Data, Feed-forward, Objective and Optimization. We also summarize the different types of imbalance indicator, which different methods use to evaluate the performance of different modalities."}, {"title": "3.1 Data", "content": "This part focuses on the method which enhances modality performance through targeted data processing strategies. Wei et al. [Wei et al., 2024a] propose a fine-grained evaluation method to facilitate multimodal collaboration. It evaluates modality-specific contributions at the sample level and em- ploys selective resampling techniques to enhance the discrim- inative capabilities of weak modality modalities."}, {"title": "3.2 Feed-forward", "content": "These methods alleviate the imbalanced learning across modalities by modifying the forward process during model training and inference. These methods can be categorized into two types based on where modifications are made.\nFeature Processing. The first type of methods adjust features during training. Adaptive Mask Co-optimization (AMCO) [Zhou et al., 2023] masks features of dominant modalities based on their performance, while On-the-fly Pre- diction Modulation (OPM) [Wei et al., 2025] drops its feature with dynamical probability in feed-forward stage.\nFusion Module. The second type achieves modality bal- ance by modifying the fusion mechanisms. Multimodal Learning with Alternating Unimodal Adaptation (MLA) [Zhang et al., 2024] uses dynamic fusion to integrate different modalities. It also employs an alternating optimization ap- proach to optimize unimodal encoders, minimizing interfer- ence between modalities. Greedy [Wu et al., 2022] utilizes the MMTM [Joze et al., 2020] architecture for intermediate fusion to boost the modality interaction. It also facilitates the learning of weak modality that indicated by conditional learn- ing speed, which is measured by the gradient change ratio."}, {"title": "3.3 Objective", "content": "Various methods for addressing modality imbalance in mul- timodal learning focus on modifying objectives. These meth- ods can be categorized into three main directions:\nFirstly, several methods modify the multimodal loss func- tion to mitigate the multimodal imbalance problem. For in- stance, Multi-Modal Cosine loss (MMCosine) [Xu et al., 2023] proposes a multimodal cosine loss, which effectively increases the learning proportion of weaker modalities by weight constraints and inter-symmetric constraints.\nSecondly, a group of methods leverage modality differ- ences for learning objectives to achieve balanced learning. MBSD [Liu et al., 2023] constrains the model using the Kullback-Leibler (KL) [Kullback and Leibler, 1951] diver- gence of prediction distributions between different modalities to reduce their distance. Calibrating Multimodal Learning (CML) [Ma et al., 2023] uses confidence loss derived from different modalities, which lowers the confidence of the dom- inant modality. LFM [Yang et al., 2024] bridges heteroge- neous data in the feature space through contrastive learning, reducing the distance between different modalities.\nThirdly, several approaches incorporate unimodal loss into the objectives to mitigate the imbalance problem. Uni-Modal Teacher (UMT) [Du et al., 2023] introduces a unimodal distillation loss, enhancing the learning of unimodal en- coders. Gradient-Blending (GBlending) [Wang et al., 2020] and MMPareto [Wei and Hu, 2024] utilize unimodal losses to solve the imbalance problem. GBlending [Wang et al., 2020] uses overfitting-to-generalization-ratio (OGR) as an indicator to show which modality is dominant and its corresponding weight, while MMPareto [Wei and Hu, 2024] borrows ideas from Pareto method [Sener and Koltun, 2018] to guarantee the final gradient is with direction common to all learning ob- jectives to boost the learning of weak modality."}, {"title": "3.4 Optimization", "content": "Recent studies have investigated optimization-based ap- proaches to mitigate the multimodal imbalance problem. Both On-the-fly Gradient Modulation (OGM) [Peng et al., 2022b] and Adaptive Gradient Modulation (AGM) [Li et al., 2023] aim to balance modality learning by slowing down the gradients of dominant modalities to provide more optimiza- tion space for weak modalities. Specifically, OGM [Peng et al., 2022b] uses performance score as an indicator to achieve this, while AGM [Li et al., 2023] employs a Shapley value- based method for gradient adjustment. Prototypical Modality Rebalance (PMR) [Fan et al., 2023] adjusts gradient magni- tudes based on category prototypes to accelerate the learning of weak modalities. Diagnosing & Re-learning (Relearning) [Wei et al., 2024b] uses re-initialization to reduce the depen- dence on dominant modalities while preventing weak modal- ities from learning excessive noise. ReconBoost [Hua et al., 2024] introduces an alternating-boosting optimization way to enhance the unimodal performance, which alleviates the im- balance problem."}, {"title": "4 Toolkit", "content": "To accompany BalanceBenchmark, we propose a comprehen- sive toolkit named BalanceMM, that incorporates 17 mul- timodal imbalance algorithms. Although these algorithms cover various methodological aspects, the toolkit provides a standardized implementation that unifies their evaluation and comparison. Due to its modular architecture, BalanceMM al- lows flexible integration of various datasets, modalities, back- bones and methods. This makes it extensible, allowing users to easily add new components to the overall framework."}, {"title": "4.1 Datasets and modalities", "content": "BalanceMM includes 7 datasets covering multiple modalities. These datasets include both bimodal and trimodal datasets, each with varying imbalance degrees, allowing for a more comprehensive evaluation of different methods. To stream- line the utilization of these datasets, we develop standardized data loaders for each dataset, ensuring consistency and repro- ducibility across experiments. A more detailed description of these datasets can be found in Section 5.1, where we discuss their characteristics in depth."}, {"title": "4.2 Backbones", "content": "To provide adaptability to different modalities, BalanceMM supports alternative backbones, including ResNet18 [He et al., 2016] and Transformer [Vaswani, 2017]. Vision Trans- former (ViT) [Dosovitskiy, 2020], which serves as a variant of Transformer specifically designed for vision tasks, is also supported. Users can choose to use a backbone trained from scratch or select a pre-trained version, depending on their specific needs. Designed as a plug-and-play component, the backbone integrates seamlessly into the workflow. Moreover, the toolkit is extensible, allowing users to easily incorporate new backbones for a wide range of applications."}, {"title": "4.3 Multimodal imbalance algorithms", "content": "BalanceMM covers 17 multimodal imbalance algorithms spanning 4 methodological categories defined in Section 3. As summarized in Table 1, these algorithms encompass vari- ous modality combinations and application domains, such as Computer Vision (CV), Natural Language Processing (NLP), and audio. A configuration-based workflow enables the acti- vation of any method with a single command, while maintain- ing the original specifications. The implementation of multi- modal imbalance algorithms is illustrated in Algorithm 1."}, {"title": "4.4 Evaluation metrics", "content": "BalanceMM offers unified evaluation metrics to assess mul- timodal imbalance methods by the criteria below."}, {"title": "4.5 Implementation pipeline", "content": "In Algorithm 1, we provide a reference implementation in the BalanceMM framework. The modular architecture of Bal- anceMM facilitates the efficient integration of various com- ponents. This not only makes the toolkit a powerful re- source for evaluating multimodal imbalance algorithms, but also streamlines the experimental workflow while maintain- ing robust performance and adaptability."}, {"title": "5 Datasets and benchmark", "content": "Balance Benchmark includes 7 datasets to evaluate different multimodal imbalance algorithms. These datasets include different types and numbers of modalities, as well as vary- ing degrees of imbalance. KineticsSounds [Arandjelovic and Zisserman, 2017], CREMA-D [Cao et al., 2014], Balance- dAV [Xia et al., 2023], and VGGSound [Chen et al., 2020] are audio-video datasets across various application scenarios. UCF-101 [Soomro, 2012] is a dataset with two modalities, RGB and optical flow. FOOD-101 [Wang et al., 2015] is an image-text dataset. And CMU-MOSEI [Zadeh et al., 2018] is a trimodal dataset (audio, video, text)."}, {"title": "5.2 Benchmark", "content": "BalanceBenchmark is the first comprehensive framework de- signed to evaluate multimodal imbalance algorithms. It ad- dresses three critical limitations of existing measurement ap- proaches. Firstly, to tackle the absence of standardized met- rics for imbalance analysis, we introduce a systematic evalua- tion protocol in Section 4.4, which measures three key dimen- sions in multimodal learning: performance, imbalance, and complexity. Secondly, to ensure reproducibility and fair com- parison of multiple methods, we maintain consistent experi- mental settings through a modular toolkit with unified data loaders and backbone support. Thirdly, to prevent overfitting to specific scenarios, we incorporate 7 diverse datasets span- ning different modality combinations such as audio-video,"}, {"title": "6 Experiments and analysis", "content": "We evaluated the effectiveness of all related methods dis- cussed in Section 3. Unimodal-1 refers to training the model using only the audio modality for KineticsSounds, CREMA- D, CMU-MOSEI, BalancedAV, and VGG. For UCF-101, it corresponds to the optical flow modality, while for FOOD- 101, it refers to the text modality. Unimodal-2 refers to training the model using only the video modality for Ki- neticsSounds, CREMA-D, CMU-MOSEI, BalancedAV, and VGG. For FOOD-101, it refers to image modality. For UCF- 101, it corresponds to the RGB modality. Unimodal-3 ap- plies only to CMU-MOSEI, where the model is trained us- ing the text modality. Baseline refers to the commonly used approach in multimodal imbalance learning, which employs concatenation fusion with a single multimodal cross-entropy loss function. As shown in Table 2, we conducted compre- hensive experiments using the proposed BanlenceBenchmark on 7 datasets. The results indicate that almost all related methods outperform the Baseline in terms of accuracy and F1 score, demonstrating that the multimodal imbalance problem is prevalent across various scenarios. Meanwhile, addressing this problem is crucial for improving model performance."}, {"title": "6.2 Analysis", "content": "Comparison of different categories of methods\nThe four categories of methods exhibit different character- istics when addressing the multimodal imbalance problem."}, {"title": "Relative balance", "content": "We conducted comprehensive experiments to investigate the relationship between model performance and the degree of imbalance. To quantify the imbalance degree, we utilized the Shapley-based method introduced in Section 4, where higher values indicate a higher imbalance degree and lower values reflect better balance between modalities. By adjusting the hyperparameters of various methods, we obtained different combinations of imbalance degree and performance. Specif- ically, we identified the points with the lowest imbalance de- gree and the highest performance.\nAs illustrated in Figure 3, we selected visualizations from two methods to demonstrate the relationship between per- formance and imbalance degree. The original baseline ex-"}, {"title": "Future work", "content": "Based on the analysis above, we provide several insights for future research in this field.\nHybrid strategies. Future research could explore hybrid strategies that integrate the strengths of different methods while mitigating their limitations. For instance, a more fine- grained adjustment of the learning objective could combine the advantages of both objective-based and optimization- based methods.\nPursue relative balance. When addressing the imbalance problem, it is important to recognize that different modalities inherently contain different amounts of information. There- fore, maintaining a relatively balanced state among modali- ties is preferable to blindly pursuing absolute balance. Fu- ture work could further explore efficient strategies to achieve relative balance across modalities, ensuring that models can effectively leverage the unique contributions of each modality\nMultimodal imbalance in foundation models. Existing methods for addressing multimodal imbalance remain limited to traditional neural networks and relatively small datasets. However, recent studies have identified the multimodal im- balance problem in mixed-modal foundation models [Team, 2024; Aghajanyan et al., 2023; The et al., 2024]. For example, studies on Chameleon [Team, 2024] shows that different modalities compete with each other with the stan- dard LLaMA architecture [Touvron et al., 2023]. Future work could extend network architectures to foundation models."}, {"title": "7 Conclusion", "content": "In conclusion, we introduce BalanceBenchmark, a unified benchmark for fair and comprehensive evaluation of mul- timodal imbalance algorithms. By incorporating a system- atic taxonomy, diverse evaluation metrics, a comprehensive dataset collection, and the modular toolkit BalanceMM, our benchmark enables thorough assessment of existing methods and provides a convenient tool for future work."}]}