{"title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis", "authors": ["Jiaqi Zhao", "Ming Wang", "Miao Zhang", "Yuzhang Shang", "Xuebo Liu", "Yaowei Wang", "Min Zhang", "Liqiang Nie"], "abstract": "Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMS PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.) Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMS and future research on PTQ approaches.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved remarkable success in text generation and various reasoning tasks, with representative ChatGPT"}, {"title": "2 Taxonomy", "content": "Most previous reviews on PTQ have also categorized quantization techniques, such as symmetric or asymmetric (Gholami et al., 2022) and group-wise or channel-wise quantization (Shen et al., 2020). Such taxonomies have become increasingly inadequate to meet the research requirements of subsequent studies as PTQ techniques continue to proliferate, because such taxonomies are relatively coarse-grained, making them challenging to conduct in-depth analysis of the characteristics of each category. In this section, we compile a comprehensive list of existing mainstream weight-only PTQ algorithms and categorize them based on their underlying principles. Specifically, they are classified into four categories: compensation-based strategy, rotation-based strategy, salience-based strategy and optimization-based strategy."}, {"title": "2.1\nCompensation-based Quantization", "content": "The strategy of compensation-based technique is to dynamically update the weights to compensate for quantization errors during the process. Specifically, these methods typically calculate the bad impact of quantization and then derive the required compensation in order to mitigate this impact.\nThis strategy is pioneered by GPTQ (Frantar et al., 2022) and is one of the most influential quantization techniques currently. GPTQ first reformulates the quantization error involving the Hessian matrix and then calculates the update formula for the unquantized weights after a specific weight is quantized, which can be expressed as:\n$\\delta = \\frac{w_q - \\text{quant}(w_q)}{[H^{-1}]_{qq}} \\cdot (H^{-1})_{:,q},$ (1)\nwhere \u03b4 denotes the optimal update of the unquantized weights, $w_q$ is the weight as position q and H indicates Hessian. Unlike LeCun et al. (1989) and Frantar and Alistarh (2022) which require a greedy search to identify the position that minimizes the errors for each quantization step, GPTQ partitions the weight matrix into multiple blocks and performs"}, {"title": "2.2\nRotation-based Quantization", "content": "The development of rotation-based methods stems from the observation that the distribution of pre-trained weights in LLMs does not facilitate the direct quantification, such as the existence of outliers. Targeting this issue, researchers typically apply transformations to process the weight matrix to enhance quantization performance.\nQuIP (Chee et al., 2024) is considered as the innovator of this strategy. Their insights reveal that quantization will be more effective when weights and proxy Hessian are incoherent, where a weight matrix W \u2208 Rn\u00d7m is \u00b5-incoherent if:\n$\\max_{i,j} |W_{ij}| = \\max_{i,j} |e_i^T W e_j| \\leq \\mu \\frac{||W||_F}{\\sqrt{mn}}.$ (2)\nSpecifically, QuIP multiplies a weight matrix by Kronecker-structured orthogonal matrices (Zhang et al., 2015) on the left and right. Such process can be thought of as a principled form of outlier reduction because the weights are similar in magnitudes, ensuring the rounding errors are not particularly large in any direction along the coordinate axes.\nInspired by QuIP, rotation-based methods rapidly gained traction. QuIP# (Tseng et al., 2024) employs Hadamard matrices (Halko et al., 2011) for rotation, achieving more efficient and superior quantization performance. QuaRot (Ashkboos et al., 2024) and SpinQuant (Liu et al., 2024c) further extend the Hadamard-rotation method to weight-activation quantization to eliminate the extreme outliers in activation channels."}, {"title": "2.3\nSalience-based Quantization", "content": "Salience-based strategy asserts the weights in LLMs exhibit varying degrees of importance, and quantization performance would be improved by selectively handling them based on their saliency. Generally, the motivation behind these algorithms primarily revolves around the criteria for determining salience and the treatment methods applied to the weights from different groups.\nMixed-precision quantization methods constitute the largest subset within salience-based tech-"}, {"title": "2.4 Optimization-based Quantization", "content": "A common characteristic of the three categories above is leveraging the intrinsic properties of the weights to influence the quantization outcome. Meanwhile, some researchers indicate that it is also effective to employ efficient optimization framework to update the quantization parameters. Since LLM weights are frozen and the optimization process is highly efficient, such approaches are also referred to as PTQ techniques\nOmniQuant (Shao et al., 2023) is the first to introduce optimization strategy into PTQ. To avoid insufferable computational resources cost during training, an efficient block-wise learning framework is proposed, where the output of full-precision blocks serves as supervisory information to update the clipping range of the scaling factors, achieving\nnearly lossless performance at 4-bit quantization. The optimization objective is:\n$\\arg \\min_{\\Theta_1,\\Theta_2} ||F(W, X) - F(Q_W(W; \\Theta), X)||, $ (4)\nwhere F means the mapping function for the current block and X denotes the full-precision activation. QW(\u00b7) represents the weight quantizer. \u0398 indicates learnable scaling factors.\nFollowing OmniQuant, CBQ (Ding et al., 2023) devises a two-branch framework to improve the robustness. LRQuant (Zhao et al., 2024) discovers the directional gaps between full-precision outputs and their quantized counterparts and propose a novel loss function named NLC loss to minimize the quantization error. AffineQuant (Ma et al., 2024) incorporates affine transformations into the PTQ process and optimizes the transformation matrix to reduce quantization errors.\nDespite our definitive taxonomy of existing milestone PTQ methods, the specific performance traits and suitable application contexts of each strategy are still unclear. The future researchers are still confused by the selection of foundational PTQ framework based on their requirements, this necessitating further analysis based on experiments."}, {"title": "3 Benchmarking PTQ in LLMs", "content": "To clearly grasp the specific performance trait of each PTQ strategy and provide useful recommendations, in this section we conduct extensive experiments to benchmark PTQ in LLMs. The detailed experimental results are listed in each subsection with corresponding analysis, conclusions and recommendations."}, {"title": "3.1 Experimental Settings", "content": "In our benchmark, we select AWQ (Lin et al., 2024), GPTQ (Frantar et al., 2022), OmniQuant (Shao et al., 2023), and QuIP (Chee et al., 2024) as representatives of the four PTQ strategies, owing to their superior performance and broad practical deployment. For quantized models, we mainly focus on the LLaMA family (LLaMA-1/2/3/3.1) (Touvron et al., 2023a,b; Dubey et al., 2024), the most widely deployed open-sourced LLMs. Besides, Mixtral (Jiang et al., 2024), DeepSeeK-MoE (Dai et al., 2024), Mamba (Gu and Dao, 2023), LLaVA-1.5 (Liu et al., 2024a), and VILA-1.5 (Lin et al., 2023) are also included. The evaluation metrics contains perplexity and average zero-shot accuracy"}, {"title": "3.2 Cross Bitwidth and Training Level\nRobustness", "content": "To comprehensively evaluate the four PTQ strategies, we use LLaMA families, which are the most widely deployed open-source LLMs. We present the average perplexity and accuracy in Table 1 and Table 2. For more details in each task please refer to Appendix B.2. In our exploration, we observe that even under identical experimental conditions, the performance of each PTQ strategy varies significantly across different bitwidths and training levels of LLMs.\nSalience-based Strategy Demonstrates Superiority at Higher-bit. As Table 1 and Table 2 shown, when applying 4-bit quantization, all baselines perform comparable and satisfactory, while AWQ holds a slight advantage. When it comes to 3-bit, the performance of different baselines begins to diverge but AWQ consistently performs the best. For example, For example, on LLaMA3-8B, AWQ achieves 7.98/61.23% and OmniQuant is 8.82/57.36% at 4-bit. Then at 3-bit the performance of OmniQuant decline visibly with 17.53/36.09%, but AWQ still achieves 9.83/54.65%. This suggests that the salience-based strategy is suitable for higher-bit PTQ.\nExtremely Low-bit PTQ Performance Varies with Training Level of LLMs. As the technical report (Dubey et al., 2024), the training dataset of LLaMA3/3.1 is several times larger than that of LLaMA/LLaMA2. The more extensive training leads to greater information loss when quantizing LLaMA3/3.1, especially for extremely low-bit PTQ, i.e., 2-bit. Our experimental results in Table 1 and Table 2 substantiate this inference and align with the recently proposed quantization scaling law (Kumar et al., 2024; Ouyang et al., 2024). However, previous research ignores to explore an effective PTQ pipeline for knowledge intensive LLMs. In this benchmark, we give a comprehensive insight through extensive experiments on undertrained LLMs (Table 1) and fully trained LLMs (Table 2) (Ouyang et al., 2024). According to the results, we observe that the performance of each PTQ strategy varies significantly with changes in the training level of LLMs at 2-bit, as summarized in three dimensions below:\n\u2022 Salience-based methods collapse on All LLMs: As listed in Table 1, AWQ exhibits extremely poor performance at 2-bit across all LLMs, completely losing any language capabilities, e.g., on LLaMA-65B AWQ only delivers 7.4e4/22.86%..\n\u2022 Optimization-based methods collapse on fully trained LLMs: Although OmniQuant performs well on LLaMA/LLaMA2, its performance drastically declines on more extensively trained LLaMA3/3.1, particularly at 3-bit where other baselines still maintain decent performance, OmniQuant has already collapsed. For instance, on 3-bit LLaMA3-70B, OmniQuant achieves 7.4e4/22.22%, whereas GPTQ is 7.16/40.51%.\n\u2022 Rotation-based and Compensation-based"}, {"title": "3.3 Cross-Architecture Robustness", "content": "LLMs constructed by stacking traditional transformer modules suffer from drawbacks such as high computational costs and poor long-sequence modeling capability. Consequently, various novel architectures for LLMs have emerged in recent years, such as MoE (Fedus et al., 2022) and Mamba (Gu and Dao, 2023). However, changes in model architecture may lead to potential issues of algorithm compatibility or performance degradation. Therefore, in this benchmark we further evaluate the four PTQ strategies on novel-architecture LLMs to analyze their cross-architecture robustness.\nThe evaluation results on Mamba 1.4B and 2.8B (Gu and Dao, 2023) are presented in Table 3. Mixtral 8\u00d77B (Jiang et al., 2024) and DeepSeekMoE-16B (Dai et al., 2024) are chosen for benchmarking MOE LLMs, where the results are shown in Table 4. For more details data please refer to Appendix B.2.\nSalience-based method cannot be generalized to Mamba and MoE LLMs. AWQ requires determining the scaling hyper-parameters which must based on the input activation during runtime. To ensure output invariance, this parameter must be integrated into the preceding linear layer. For Mamba, there is only an RMSNorm layer before in_proj layer while no linear layers precede the other projection layers, this making the output invariance cannot be ensured. For MoE LLMs, a router may choose different experts in the routing mechanism, which makes it hard to fuse the scaling hyper-parameters offline.\nOptimization-based method is highly unstable. As shown in Table 3, OmniQuant completely collapses on Mamba LLMs. Meanwhile, on Mixtral 8\u00d77B and DeepSeekMoE-16B, its performance is also unsatisfactory, particularly at 2-bit. The aforementioned two phenomena demonstrate that AWQ and OmniQuant exhibit poor cross-architecture generalization capabilities.\nRotation-based and compensation-based exhibit distinct robustness characteristics. The results in Table 3 and Table 4 indicate that GPTQ and QuIP achieve superior performance across various bitwidths for both MoE LLMs and Mamba. Taking the performance on LLaMA into consideration, it is evident that these two PTQ strategies maintain stability across different model-architecture. Interestingly, their robustness exhibit distinct nature. Specifically, in most cases, GPTQ performs better at higher bit, while QuIP is more suitable for 2-bit quantization. For example, GPTQ achieves 11.30/45.69% on Mamba-2.8B while QuIP is 12.30/45.11% at W4, but at W2 QuIP outperforms GPTQ with 119.95/28.23%. And on 2-bit DeepSeekMoE-16B, QuIP shows significant advantages with 23.29/32.79% compared with GPTQ. In addition, we can notice that QuIP exhibits unstable performance on MoE LLMs, especially on Mixtral 8\u00d77B where GPTQ significantly outperforms QuIP across all quantization bitwidths. For instance, GPTQ and QuIP exhibit"}, {"title": "3.4 Cross-Modality Robustness", "content": "Quantization may undermine the inherent cross-modal alignment capabilities, leading to degraded performance on multimodal tasks. In this benchmark, we further evaluate the average accuracy of the four PTQ strategies in visual-language reasoning tasks on VILA (Lin et al., 2023) and LLaVA (Liu et al., 2024a) to explore cross-modality robustness, and the results are presented in Table 5. For more details please refer to Appendix B.2.\nHigher-bit performance remains stable and comparable. Similar to LLMs, at 3/4-bit, the performance differences among various PTQ strategies are negligible, and all exhibit outstanding performance. For instance, on LLaVA1.5-13B the largest average accuracy gap is only 1.7% at 3-bit.\nRotation and compensation methods consistently demonstrate superiority at 2-bit. As shown in Table 5, the phenomenon at 2-bit is consistent with that in LLMs, where only models quantized by GPTQ and QuIP exhibit effective reasoning capabilities while AWQ and OmniQuant completely collapse. For instance, on VILA1.5-8B AWQ shows 7.69% and QuIP remains 54.18%.\nThe experimental observations suggest that at higher bit, any strategy exhibits commendable cross-modal robustness, whereas at extremely low-bit, rotation-based and compensation-based strategies emerge as the sole viable alternatives."}, {"title": "3.5 Trade-off among Bitwidth, Model Size\nand Performance", "content": "When deploying quantized LLMs in real system, people often grapple with the decision regarding the size and the bitwidth of the model to deploy. For instance, a straightforward question arises: Which one is better, a higher-bit smaller model or a lower-bit larger model? In order to clarify this confusion, in this benchmark, we comprehensively explore the trade-off between model size and quantization bitwidth from the performance perspective. The experimental results in previous tables indicate that the perplexity of text generation exhibits a strictly positive correlation with reasoning ability reflected in accuracy. As perplexity exhibits a much larger distribution range, we choose accuracy as the evaluation metric and then create intuitive visualizations to explore the trade-off (see Figure 2).\nThe ultra-large model at 2-bit even falls short in performance when compared to the 4-bit smallest model. The scaling law claims that, within the same LLM family, larger models generally exhibit superior performance. However, Figure 2 indicates that even the largest models, like LLaMA-65B, when quantized to extremely low-bit (2-bit), demonstrate inferior performance compared to the smallest models operating at 4-bit, such as LLaMA-7B. This observation remains valid across any LLM family, any architecture and any PTQ strategy as the results in the tables above.\nFor instance, LLaMA2-70B quantized to 2-bit using OmniQuant achieves 10.36/38.46%, which is worse than LLaMA2-7B at 4-bit with 6.55/51.11%; Mamba-2.8B at 2-bit is also worse than Mamba-1.4B at 4-bit quantized by any baseline. This fresh finding suggests that for the present moment, deploying smaller models at higher bitwidths appears to be the optimal choice. Additionally, specific research on extremely low-bit PTQ for ultra-large models seems essential. Most previous PTQ methods usually validated their performance across various model sizes, which ignored the fact that pushing ultra-large models to extremely low-bit does not surpass the performance of higher-bit smaller models so as to cause the unnecessary expenditure of computational resources. This highlights the need for the development of specialized designs for extremely low-bit PTQ on ultra-large models to ensure their performance exceeds that of higher-bit small models.\n3-bit is an effective and competitive target for PTQ. As illustrated in Figure 2, compared to the 4-bit smaller model, the performance advantage conferred by the large amounts of weights in the larger model remains fully evident at 3-bit, aligning with the scaling law. For example, LLaMA2-13B quantized by GPTQ exhibits 6.28/55.44%, outperforming 4-bit LLaMA2-7B with 6.45/52.86%. Given that 3-bit quantization still offers a considerable target bitwidth, it can serve as a viable quantization target for those seeking to harness the performance benefits of larger models."}, {"title": "4 Compensation-based PTQ: A Unified\nRobust Foundational Strategy", "content": "In Section 3, we have provided several recommendations for the selection of foundational PTQ strategy based on different requirements. But when developing novel PTQ algorithm, researchers usually strive for an ideal form, one where the designed method can be effectively generalized to any quantization scenario. To address this issue, in this section we further explore this valuable requirement to improve our benchmark.\nBased on the results in Section 3, it is evident that rotation-based and compensation-based strategies exhibit better generalization ability. Subsequently, we broadened our focus to more advanced algorithms within compensation-based and rotation-based strategies, i.e., VPTQ and QuaRot. However, when validating on Mamba and MoE"}, {"title": "5 Conclusion", "content": "In this paper, we introduce a novel comprehensive benchmark for PTQ methods in LLMs. Specifically, our benchmark first provide a comprehensive taxonomy for existing mainstream PTQ methods. Then with the baselines of each PTQ strategy, we conduct extensive experiments on LLMs with various training levels, model sizes, architectures and modalities. According to the evaluation results, we summarize the characteristic of each PTQ strategy and discover the modelsize-bitwidth trade-off. At last, with the comparative analysis, our benchmark offers valuable guidelines for quantized LLMs deployment and future PTQ algorithms development."}, {"title": "Limitation", "content": "Our benchmark addresses the shortcomings of existing PTQ surveys and provides guidance for future research. However, due to the rapid development and vast number of PTQ algorithms, it is impractical for us to conduct experiments using every approach across various models and scenarios to generate more comprehensive results for summarization. Additionally, because of differences in experimental environments, our experimental results may deviate from those originally reported. Nevertheless, to ensure fairness, all our experimental settings are identical. Lastly, our benchmark solely covers weight-only quantization, as the characteristics and strategies for weight-activation quantization are entirely different, and including them would significantly lengthen this paper. In the future, we will propose a specific benchmark for weight-activation quantization to fill the gap in quantization community."}, {"title": "Ethics Statement", "content": "This paper presents in-depth insights and recommendations associated with Large Language Models (LLMs) quantization, with the overarching goal of facilitating the widespread adoption and application of LLMs. In the current landscape, ethical concerns tied to LLMs, including the presence of hidden biases encoded in the models, are garnering heightened attention. Following our investigation, we assert that our proposed method does not further amplify the biases and contravene any ethical standards."}, {"title": "Appendix", "content": "A Background"}, {"title": "A.1 Quantization Preliminaries", "content": "Quantization aims to reduce inference and storage overheads by converting high precision floating-point values into their corresponding low precision integer counterparts (Nagel et al., 2021). The asymmetric weight-only quantization is formulated as:\nWq = \\frac{W}{S_q} = clamp([\\frac{W}{S_q}] + z_q, 0, 2^{b} - 1), (5)\nwhere $W \\in R^{n \\times m}$ and $W_q \\in R^{n \\times m}$ indicate full-precision and quantized weights respectively. [\u00b7] denotes round-to-nearest operator. $s_q$ is the scaling factor and $z_q$ is the zero-point."}, {"title": "A.2 Quantization Surveys and Benchmarks", "content": "Most model compression reviews summarize ample concepts, principles, and a coarse-grained classification of commonly used compression methods (Tang et al., 2024; Park et al., 2024; Miao et al., 2023; Wan et al., 2023; Zhu et al., 2023; Yang et al., 2024), but limited focus is placed on quantization. The other literature contributes a more detailed exploration of quantization. Wang et al. (2024) provides comprehensive explanations of quantization frameworks but lacks detailed taxonomy and summaries of their specific characteristics, so that the followers are still confused by how to select a basic framework to further develop. Gong et al. (2024) delves deeper into the existing methods and provides finer distinctions but no experiments are conducted to evaluate the performance of each category. Kurtic et al. (2024) explores the accuracy-performance trade-off for popular quantization formats via broad and automated benchmarks, but only 3 models are included into examination which undermines the persuasiveness of the study. Li et al. (2024) focus on synthesizing experimental results but offers limited valuable guidance for future research. It is evident that, there is still a lack of an technical and development guideline for LLMs PTQ which enables researchers to select correct model for deployment or foundational PTQ strategy based on their requirements. Our benchmark fills this gap by experimentally analyzing the characteristics of different PTQ strategies and providing recommendations for researchers to consider."}, {"title": "A.3 The Trend of PTQ Research", "content": "PTQ is more favored by LLMs compression due to its efficiency, convenience, and ease of reproducibility. As illustrated by Figure 3, in the first year of the emergence of LLMs (2022), 6 PTQ papers were published on Arxiv. By 2023, this number doubled, while there were only 6 QAT papers. By 2024, there was an explosive growth in the number of quantization papers, with 33 out of a total of 44 papers belonging to PTQ. Overall, PTQ papers account for 69.23% of the total number of quantization papers on LLMs, which demonstrates the intense attention on PTQ research."}, {"title": "B Details of Evaluation", "content": "B.1 Detailed Experimental Setup\nBaselines AWQ (Lin et al., 2024), GPTQ (Frantar et al., 2022), OmniQuant (Shao et al., 2023), and QuIP (Chee et al., 2024) serve as representatives of the four PTQ strategies in our taxonomy, owing to their superior performance and broad practical deployment in numerous released LLMs. All baselines perform channel-wise quantization and set up as their description.\nModels To demonstrate the universality and generalizability of our benchmark, we select LLaMA family, the most influential and widely used open-source LLM family, for performance evaluation. Specifically, this includes LLaMA-1 (7B to 70B) (Touvron et al., 2023a), LLaMA-2 (7B to 70B) (Touvron et al., 2023b), as well as LLaMA-3 and LLaMA-3.1 (8B and 70B) (Dubey et al., 2024), covering a substantial range of model sizes. Unlike other benchmarks, we further broaden our"}, {"title": "B.2 Detailed Evaluation Results", "content": "In this section, we present the detailed perplexity and reasoning accuracy on specific tasks. Table 7 and Table 8 show the detailed results on undertrained LLMs (correspond to Table 1). Table 9 and Table 10 shows the detailed results on fully trained LLMs (correspond to Table 2). Table 11 shows the detailed results on MoE LLMs (correspond to Table 4). Table 12 shows the detailed results on Mamba (correspond to Table 3). Table 13 shows the detailed results on Multimodal LLMs (correspond to Table 5)."}]}