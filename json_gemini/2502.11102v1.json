{"title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling", "authors": ["Hongliang Lu", "Zhonglin Xie", "Yaoyu Wu", "Can Ren", "Yuxuan Chen", "Zaiwen Wen"], "abstract": "Despite the rapid development of large language models (LLMs), a fundamental challenge persists: the lack of high-quality optimization modeling datasets hampers LLMs\u2019 robust modeling of practical optimization problems from natural language descriptions (NL). This data scarcity also contributes to the generalization difficulties experienced by learning-based methods. To address these challenges, we propose a scalable framework for synthesizing a high-quality dataset, named OptMATH. Starting from curated seed data with mathematical formulations (MF), this framework automatically generates problem data (PD) with controllable complexity. Then, a back-translation step is employed to obtain NL. To verify the correspondence between the NL and the PD, a forward modeling step followed by rejection sampling is used. The accepted pairs constitute the training part of OptMATH. Then a collection of rejected pairs is identified and further filtered. This collection serves as a new benchmark for optimization modeling, containing difficult instances whose lengths are much longer than these of NL4OPT and MAMO. Through extensive experiments, we demonstrate that models of various sizes (0.5B-32B parameters) trained on OptMATH achieve superior results on multiple modeling benchmarks, thereby validating the effectiveness and scalability of our approach.", "sections": [{"title": "1 Introduction", "content": "Automatic translation of natural language descriptions of optimization problems into solver-ready formats is a critical step in democratizing access to optimization techniques. This capability would enable individuals without expertise in optimization to leverage the power of optimization for solving real-world problems across various domains, including logistics [32], finance [18], and engineering [64]. Known as optimization modeling, this task has long been challenging due to the inherent ambiguity of natural language and the need for a deep understanding of optimization modeling principles. The manual process of formulating an optimization problem typically involves iterative refinement and demands significant mastery of the relevant techniques, making it time-consuming and inaccessible to many practitioners.\nRecent advances in Large Language Models (LLMs), such as ChatGPT [14], GPT-4 [59], and OpenAI's ol [58], have demonstrated remarkable capabilities in understanding natural language and performing complex reasoning tasks. Notably, the introduction of ol has significantly enhanced the performance of LLMs in mathematical reasoning, achieving state-of-the-art results on challenging datasets including AIME (2024), GPQA, and CodeForces. However, optimization modeling presents unique challenges. Unlike grade-school mathematics, where problems typically have a single correct solution, optimization problems can often be approached using multiple valid models, making the task semi-open-ended. Furthermore, optimization frequently relies on a vast body of empirical knowledge that is less formally structured than purely mathematical concepts. As a result, research has shown that directly applying LLMs to optimization modeling tasks yields suboptimal outcomes [63].\nLLMs for Optimization Modeling. To address this issue, recent efforts have explored various strategies. Research on leveraging LLMs for optimization modeling typically follows two main approaches. The first uses prompt engineering techniques to guide LLMs in generating or refining optimization models, without modifying the underlying model parameters. Examples include the NL4Opt competition [63], which aims to extract optimization formulations from natural language descriptions, and OptiMUS [3], which proposes an agent-based prompt engineering method. More recently, Autoformulation [6] combines Monte Carlo tree search with LLMs to address optimization modeling. While these methods rely heavily on the base capabilities of general-purpose LLMs, they do not yield fundamental advancements beyond refined prompting.\nThe second approach centers on fine-tuning, wherein model parameters are adapted using specially curated or synthesized optimization datasets. ORLM [71] introduces OR-Instruct, a data augmentation framework for optimization problems, and demonstrates performance gains via Supervised Fine-Tuning on a foundation model. LLMOPT [44] likewise applies a multi-instruction fine-tuning strategy and self-correction mechanisms. However, most of these methods generate small amounts of synthetic training data often of inconsistent quality and insufficient complexity. Consequently, their ability to generalize to more sophisticated optimization tasks is limited.\nLLM-Based Data Synthesis for Optimization. Data synthesis methods have become essential for addressing data scarcity and enhancing the performance of LLMs. According to [75], these methods can be broadly categorized into two main approaches: data augmentation and data synthesis. OR-Instruct [71, 76] exemplifies a data augmentation approach, following the self-instruct framework to expand existing datasets. On the other hand, MILP-Evolve [49] introduces an evolutionary framework designed to generate diverse mixed-integer linear programming (MILP) problems using LLMs. While MILP-Evolve represents a significant step forward in synthetic data generation, it does not address the critical challenge of translating natural language descriptions into mathematical optimization models.\nInstance Generation for Optimization. Recent research in MILP instance generation has evolved along two primary axes: learning-based structural synthesis and rule-based distribution expansion. The learning-based paradigm addresses data scarcity by developing generative models that preserve instance hardness and constraints. Examples include the bipartite graph variational autoencoder framework proposed by [31], the block decomposition operators for constraint matrices introduced in [52], the duality-driven feasibility guarantees established by [74], and the adaptive constraint modification mechanisms developed in [36]. The rule-based approach leverages instance space analysis techniques [4, 70, 5] to guide the generation of more diverse instance distributions [67, 9]. Alternatively, it may rely on manually selected features to control the characteristics of the generated instances [10].\nOur Contributions. We propose a scalable bidirectional synthesis framework that addresses the critical challenge of data scarcity in optimization modeling through triplet-aligned (NL, MF, PD) data generation and rigorous validation. Our framework uniquely integrates a closed-loop workflow with optimal value matching, ensuring semantic equivalence between NL, MF, and PD. This approach demonstrates exceptional scalability. The framework's domain adaptability is evidenced by coverage of 10+ real-world applications (e.g., logistics, energy, finance) through 53 seed generators, with manual analysis confirming 99.6% equivalence accuracy across all triplets.\nWe introduce OptMATH-Train, a large scale verified optimization modeling dataset containing rigorously validated (NL, MF, PD) triplets. Each triplet undergoes three-stage quality control: mathematical consistency checks (MF-to-PD compilation), semantic fidelity validation (PD-to-NL backtranslation), and solution equivalence verification through solver-based rejection sampling. From rejected instances, we curate OptMATH-Bench, a challenging benchmark comprising \"hard instances\" characterized by extended natural language contexts (2.9\u00d7 longer than MA\u039c\u039f EasyLP) and complex constraints. We further span it using various problems including LP, MILP, IP, NLP, SOCP. This benchmark provides the standardized evaluation for long-context optimization modeling.\nFinally, extensive experiments demonstrate the efficacy of our framework. Models trained on the OptMATH-Train dataset achieve state-of-the-art performance on multiple established modeling benchmarks, including NL4OPT and MAMO. These results definitively validate the effectiveness and scalability of our framework for generating high-quality optimization modeling datasets."}, {"title": "2 Backgrounds & Overview", "content": "In mathematical optimization theory, a canonical optimization problem can be formulated as:\n$\\begin{aligned}\n\\min_x & \\quad g(x), \\\\\n\\text{subject to } & \\quad c_i(x) = 0, \\quad i \\in E, \\\\\n& \\quad c_i(x) \\geq 0, \\quad i \\in I, \\\\\n\\end{aligned}$ (2.1)\nwhere $x \\in \\mathbb{R}^n$ denotes the decision vector. The objective function $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ assigns a scalar value to each candidate solution, which we seek to minimize. The constraint functions $c_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ define the feasible region through equality constraints indexed by $E$ and inequality constraints indexed by $I$.\nTo formalize the optimization modeling problem, we define several key concepts and illustrate them using a concrete example in Appendix D.1. For a specific optimization problem, we define NL as the natural language description, which corresponds to the \"Input-Natural Language Description\" in the example. We represent the LP/MPS file, the concrete mathematical expression (corresponding to the \"Output-Instance Formulation\"), and any other solver-ready formats, such as executable Python code with Gurobi shown in Appendix D.1, as problem data (PD). A common characteristic of these representations is that they allow us to obtain the optimal value of the problem by invoking a solver based on the PD. Mathematical formulation (MF) refers to formulation where concrete numbers are not yet specified, corresponding to the \"Output-General Formulation\" in the example. We emphasize that, in subsequent sections, we may use different forms of PD. However, these forms essentially carry the same information about the problem and can be inferred from the context without ambiguity. We use different forms of PD to facilitate their integration into the workflow and to enhance clarity in various contexts.\nModern solvers such as Gurobi and Mosek [38, 56] can efficiently solve problems stored with PDs using algorithms like interior-point methods [45]. However, practical challenges remain. In real-world applications, one of the main difficulties lies in converting informal NLs of problems into precise MFs. Moreover, extracting the PDs from NLs poses an additional significant challenge. Traditionally, this process has required deep optimization expertise [11], but recent advances in LLMs offer promising opportunities to automate this transformation.\nLet $A_\\theta$ represent an LLM parameterized by $\\theta$. The formulation for increasing the modeling capability of the LLM can be expressed as:\n$\\max_\\theta \\mathbb{E}_{(NL, MF, PD) \\sim D}[Q_{(NL, MF, PD)}(MF', PD')],$ (2.2)\ns.t. $(MF', PD') = A_\\theta(prompt_M(NL))$, (2.3)"}, {"title": "3 Feedback-Driven PD Generation", "content": "The mature development of the optimization community has provided us with access to many high-quality optimization PDs. These PDs are typically stored in standardized formats like MPS or LP files. To effectively leverage these resources, we began by curating over 50 seed problem classes sourced from a variety of optimization journals and websites (see Appendix A.2 for details). For each i-th problem class, we developed a corresponding instance generator $G_i$. This generator takes a problem-specific configuration as input and outputs a probability distribution over PDs. The distribution is designed to produce PDs with varying scales and complexities, which are controllable through adjustable configurations. Before delving into the controlled generation process for the PDs, we first explain how the complexity of PDs can be measured.\nMeasuring the Modeling Complexity. The complexity of formulating and solving a MIP problem depends on modeling choices such as the types of variables, the forms of constraints, and auxiliary modeling techniques employed. We introduce a scoring function $S$ defined as:\n$S(PD) = \\alpha_{bin}N_{bin} + \\alpha_{int}N_{int} + \\alpha_{cont}N_{cont}\n+ \\beta_{lin}N_{lin} + \\beta_{indic}N_{indic} + \\beta_{quad} N_{quad}\n+ \\beta_{gen} N_{gen} + \\beta_{BigM} BigM + d_{expr} L_{expr},$ (3.1)\nwhere $N_{bin}, N_{int}, N_{cont}$ are the number of binary, integer, and continuous variables, respectively. Similarly, $N_{lin}, N_{indic}, N_{quad}, N_{gen}$ represent the number of linear, indicator, quadratic, and general nonlinear constraints. The term $f_{BigM}$ is a factor reflecting the frequency of Big-M formulations, and $L_{expr}$ is the average number of terms per constraint and the objective function, which captures the structural information of the expressions.\nLastly, the weights $\\alpha_., \\beta_., BigM, d_{expr}$ are tunable parameters reflecting the contribution of each component to the overall complexity. To illustrate it, we provide an example in Appendix B.1 that considers a MIP problem incorporating multiple constraint types to optimize production costs for two products under resource constraints.\nSelecting Parameters to Control the Complexity. We now present the workflow in Algorithm 1 for selecting parameter configurations for instance generator $G_i$ to generate PDs fitting the complexity, feasibility, and solving time requirements. The prompt templates are illustrated in Appendix E.4. As formalized in Algorithm 1, the process begins by specifying target bounds. Then a template prompt$_\\Theta$ for initializing the configuration are incorporated. After obtaining the configuration, we generate $N$ PDs using it. We then evaluate the generated PDs through the complexity score, solving time, and feasibility satisfactory. Then, a feedback prompt$_{\\mathcal{RC}}$ is created based on the statistics of these metrics over the $N$ generated PDs. The LLM iteratively adjusts parameters based on feedback from solved instances, ultimately converging to a configuration that satisfy predefined criteria.\nThis ensures generated PDs remain both expressive and tractable by adhering to runtime thresholds."}, {"title": "4 The Data Synthesis Framework", "content": "This section presents our bidirectional scalable data synthesis framework. In this section, all of the PDs are in solver code form (see subsection 4.2 for more details). Let $L$ represents the LLM employed on the reverse data generation phase, and $A_\\theta$ for our fine-tuned AutoFormulator with weights $\\theta$. We define prompt$_I$, prompt$_C$, promptr as the prompt templates that accept certain inputs for the initial generation, self-criticism, and self-refinement stages. The algorithm is formalized in Algorithm 2, with an illustrative example of the backtranslation process shown in Figure 13.\nThe final OptMATH dataset $\\mathcal{D}$ is constructed by collecting all valid quadruples $(NL_{i,j}, MF_{i,j}, PD'_{i,j}, OV_{i,j})$ that pass the validation process, where $MF_i$ and $PD'_i$ represent the generated mathematical formulation and problem data using $A_\\theta$, $OV_{i,j}$ is the optimal value obtained by solving the problem specified by $PD'_{i,j}$. By leveraging our instance generators and the iterative refinement process, this algorithm enables scalable generation of high-quality data pairs. The mathematical equivalence between the generated formulations and the original instances is rigorously validated through rejection sampling, ensuring the reliability of our dataset. A comprehensive discussion of our quality control and rejection sampling can be found in Section 4.3."}, {"title": "4.1 Backtranslation Pipeline", "content": "To generate high-quality NLs of optimization problems at scale, we leverage a specific LLM as the foundation of our pipeline. Recent research has demonstrated that complex tasks often benefit from iterative refinement approaches rather than direct generation [53]. This observation aligns with human problem-solving processes in mathematics, which typically requires multiple attempts and refinements. Building upon this insight, we design a three-phase backtranslation pipeline that systematically improves the quality of generated descriptions through iterative refinement. All prompt templates used in this pipeline can be found in E.1.\nInitial Generation. Given the mathematical formulation $MF_i$ and the corresponding problem data $PD_{i,j}$ of a problem $j$ in $i$-class, the LLM generates an initial natural language description NL using the prompt template prompt$_I$. This stage requires the model to comprehend both the mathematical semantics and the instance parameters to produce a preliminary human-readable description.\nSelf-Criticism. Using prompt template prompt$_C$, the LLM evaluates the current description by examining the mathematical equivalence with $MF_i$, completeness of the constraints and objective functions, clarity and comprehensibility, and consistency of the parameters with $PD_{i,j}$. The criticism SC in iteration $k$ incorporates feedback from all previous iterations to guide improvements.\nSelf-Refinement. Based on the criticism, the model generates refined descriptions SR with the prompt template promptr. The refinement process focuses on improving the mathematical accuracy, completeness of the constraints, and clarity of the descriptions. This process iterates for T rounds until a satisfactory description $NL_{i,j}$ is obtained, with each iteration potentially improving the quality of the generated description. Based on our empirical analysis (see Appendix C.2), we set $T = 1$ in the final implementation."}, {"title": "4.2 Forward modeling", "content": "Building upon the NLs generated in subsection 4.1, we leverage AutoFormulator to transform them back into MFs and PDs in solver code form, enabling rejection sampling for quality validation. Given a NL as input, AutoFormulator produces two key outputs: a MF and corresponding PD in solver code form. While previous works [71, 44] adopted fixed output formats, our approach is not constrained to any particular format, as our primary goal is to obtain correct solver code, with the formulation serving as an intermediate reasoning step. To facilitate genuine mathematical modeling capabilities rather than superficial format mapping, we design diverse Chain-of-Thought (CoT) prompting strategies [78]. This approach generates multiple valid reasoning paths and formulation variants for the same problem, enriching our training data with diverse modeling perspectives and enhancing the model's mathematical reasoning capabilities. Detailed implementation of these CoT strategies is described in Appendix D.1."}, {"title": "4.3 Rejection Sampling", "content": "To ensure the quality and mathematical soundness of our generated optimization problem descriptions, we employ a rejection sampling strategy [83, 51] to filter and select high-quality samples from the generated candidates.\nAs illustrated in Algorithm 2, our rejection sampling mechanism relies on solution-based comparison to validate the generated samples. Specifically, for each generated natural language description $NL_{i,j}$, we use AutoFormulator to transform it into a mathematical formulation $MF'_j$, and solver code $PD'_j$, obtaining solution $OV'_j$. This solution is then compared with $OV_{i,j}$, obtained by directly solving the original instance $PD_{i,j}$. A sample is accepted into our dataset $\\mathcal{D}$ as a validated quadruple $(NL_{i,j}, MF'_{i,j}, PD'_{i,j}, OV_{i,j})$ if and only if $OV'_{i,j} = OV_{i,j}$.\nWhile this solution-based validation approach may not guarantee perfect equivalence (as problems with identical optimal values may represent different optimization problems), our manual analysis of randomly sampled instances (1% of the total dataset) reveals a remarkable 99.6% accuracy rate. We acknowledge that determining the exact equivalence between two mathematical formulations remains an open research question worthy of further investigation. Nevertheless, our current approach provides a practical and highly effective mechanism for ensuring dataset quality."}, {"title": "5 Fine-Tuning", "content": "5.1 Data Augmentation\nTo improve the diversity of our dataset, we use data augmentation to augment the training data. This method generates more non-standard problems compared to a data generator, enhancing the model's generalization performance. We create rules for problem rewriting, semantic substitution, constraint expansion, and numerical augmentation. For each instance, a randomly selected rule is used to prompt the LLMs to generate the corresponding augmented data. The detailed augmentation rules and prompt templates can be found in Appendix E.7.\nFor quality control, we employ a specific LLM $L$ to sample each augmented description twice independently, followed by the rejection sampling strategy described in Section 4.3. This process yields approximately 10 qualified augmented datasets for each problem, and this method was applied to augment 50 thousand instances to complement our original dataset."}, {"title": "5.2 Training the AutoFormulator", "content": "We adopt a supervised fine-tuning (SFT) approach to enhance the AutoFormulator's modeling capabilities. Specifically, we employ the LORA algorithm [42] for efficient parameter-efficient fine-tuning, which significantly reduces memory requirements while maintaining model performance by updating only a small set of adapter parameters. Using the OptMATH-Train dataset $\\mathcal{D}_{SFT} = \\{(NL_i, MF_i, PD_i)\\}_{Train}$, we train the model to generate both mathematical formulations and solver code given problem descriptions. For each training sample, the input consists of the problem description $NL_i$, while the target output is the concatenation of the formulation and solver code: $y_i = [MF_i; PD_i]$, where $[;]$ denotes sequence concatenation. The training objective follows the standard sequence-to-sequence loss:\n$L_{SFT}(\\theta) = -\\mathbb{E}_{(p,y) \\sim D_{SFT}} \\sum_{t=1}^{|y|} \\log P_\\theta(y_t | y_{<t}, p)$ (5.1)\nwhere $y_t$ represents the token at position $t$ in the target sequence, and $y_{<t}$ denotes all preceding tokens. This approach allows the model to learn the mapping from natural language problem descriptions to both mathematical formulations and solver code within a unified sequence-to-sequence framework."}, {"title": "6 Experiments", "content": "6.1 Statistics of the OptMATH Dataset\nFirst, using our generators, we generated a quality-filtered dataset containing over 600,000 LP files, which span 53 distinct problem types and are distributed across five hardness levels. For more details on the seed data class, please refer to Appendix A.2. To ensure computational feasibility, we impose a solving time threshold and employ a feedback pipeline that leverages an LLM to regulate both the complexity and feasibility of the generated instances. Further details on this process are provided in Appendix B. The distribution of file lengths across these LP files is visualized in Figure 2. As shown, the lengths range widely from 1,000 to 25,000 characters, capturing a rich variety of problem complexities. The proportions of different lengths are well-balanced, with a concentration on medium difficulty levels (which are already quite challenging compared to other benchmarks) and a gradual decline as the problems become harder. Additionally, the distribution confirms the effectiveness of our complexity control mechanism.\nWe further conducted a comparative analysis of problem lengths between OptMA\u03a4\u0397 and other benchmark datasets, with their average lengths shown in Figure 3. The analysis reveals that OptMATH presents significantly more complex problem descriptions compared to existing benchmarks. This increased complexity, manifested through longer problem descriptions, poses greater challenges for LLMS, as longer descriptions typically demand enhanced comprehension and reasoning capabilities."}, {"title": "6.2 Autoformulation", "content": "Evaluation Benchmarks and Metrics. We evaluate our fine-tuned model on three benchmarks: NL4OPT[63], MAMO[43], and our newly constructed OptMATH-Bench. Detailed descriptions of these benchmarks can be found in Appendix A.1. We use pass@1 accuracy as the evaluation metric, which specifically measures whether the optimal value obtained by the generated code matches the ground truth provided in the benchmark. The detailed matching criteria are described in Appendix A.1. Notably, since prompt design can significantly impact model performance, we maintain consistency by using the same prompt template across all model evaluations (see Appendix E.2 for details). Additionally, comprehensive details about our fine-tuning procedure are provided in Appendix D.3.\nMain Results. The primary results are presented in Table 1. First, our best-performing model, OptMATH-Qwen2.5-32B, achieves superior performance across all benchmarks, surpassing proprietary large language models such as GPT-3.5-Turbo[13], GPT4[59], and Deepseek-V3[50], despite these models having tens of times more parameters. Furthermore, our OptMATH-Qwen2.5-7B outperforms ORLM-LLaMA-3-8B, a model of comparable size, on all benchmarks and demonstrates performance only marginally inferior to Deepseek-V3. Collectively, these results demonstrate that training with OptMATH-Train significantly enhances the model's optimization modeling capabilities.\nAblation Study on Model Size. To investigate the effectiveness of OptMATH training across different model scales, we conducted experiments using Qwen2.5 models ranging from 0.5B to 32B parameters. Due to computational constraints, we used a randomly sampled subset of 100,000 training examples. As shown in Figure 6, all models exhibit substantial performance improvements after OptMATH-Train fine-tuning. Notably, we observe that while larger models generally achieve better absolute performance, the relative performance gains from OptMATH-Train training demonstrate diminishing returns as model size increases.\nAblation Study on Data Size. Figure 7 presents our comprehensive analysis of how varying amounts of training data influence the performance of Qwen2.5-1.5B model on OptMATH-Train. We observed significant improvements in the model's optimization modeling capabilities even with only a small fraction of the OptMATH-Train dataset. As we gradually increased the size of the training data, the performance gains became less pronounced, exhibiting a typical pattern of diminishing returns. Larger models exhibit smoother learning curves, while smaller models demonstrate greater sensitivity to additional training data, indicating higher potential for improvement through data scaling (detailed results across model sizes can be found in the Appendix D.4)."}, {"title": "7 Conclusion", "content": "In this paper, we introduce a bidirectional data synthesis framework for optimization modeling. It utilizes a two-step process: reverse data generation, where LLMs refine themselves in a loop to create diverse datasets, and autoformulation, where a specialized model translates natural language into mathematical representations. Our evaluation on NL4OPT, \u039c\u0391\u039c\u039f and OptMATH-Benchmarks demonstrated AutoFormulator's superior performance in generating accurate and well-formed optimization models compared to baseline approaches."}, {"title": "Impact Statements", "content": "This study introduces OptMATH, a dataset for optimization modeling, comprising a large-scale training set (OptMATH-Train) and a challenging benchmark (OptMATH-Bench). OptMATH has the potential to democratize optimization by enabling those without expertise to translate real-world problems into mathematical formulations. The OptMATH-Train dataset will significantly improve LLMs' ability to understand and model optimization problems. Furthermore, OptMATH's structured data facilitates the integration of optimization with advanced AI techniques like reinforcement learning, Monte Carlo Tree Search. Additionally, OptMATH-Bench provides a standardized benchmark for evaluating optimization modeling systems, pushing the boundaries of LLM capabilities. Ultimately, OptMATH can improve efficiency and decision-making across industries."}, {"title": "A.1 An Introduction of Different Benchmarks", "content": "We evaluated the modeling capabilities of our trained model on NL4OPT, \u039c\u0391MO, and our self-constructed dataset, OptMATH-Bench. Both MAMO and OptMATH-Bench have ground truth annotations, while the original NL4OPT dataset lacks ground truth. To address this, we utilized a LLM to generate initial ground truth for NL4OPT, followed by expert validation and correction for each data. As a result, we obtained the ground truth for the NL4OPT dataset. In addition, we have also analyzed these datasets in terms of problem scenarios and problem model types, and the distribution of scenarios for each dataset is shown in Figure 8, the distribution of problem types for each dataset is shown in Figure 4.\nNL4OPT[63] is a curated dataset derived from the NL4OPT Competition, where participants were tasked with developing automated methods to convert natural language problem descriptions into solver-ready code. This dataset primarily focuses on LP (Linear Programming) problems across various contexts, though the underlying mathematical models are relatively uniform, with more complex MIPS (Mixed Integer Programming and Scheduling) problems notably absent. For our experiments, we selected the test set from this dataset, filtered out low-quality examples, and retained a total of 245 high-quality instances.\nMAMO [43] introduces a novel optimization dataset to assess the modeling capabilities of LLMs. The dataset is divided into two main components, Easy_LP and Complex_LP, containing 652 and 211 instances, respectively. These components cover both LP and MILP problems, capturing a wide range of real-life scenarios. However, the dataset does not include any nonlinear programming (NLP) problems.\nOptMATH-Bench. As shown in Table 1, while our fine-tuned model achieves remarkable performance on NL4OPT and MAMO_EasyLP, these datasets alone are insufficient to comprehensively evaluate the model's optimization modeling capabilities. Moreover, both NL4OPT and MAMO datasets are limited to linear programming problems, making them less representative of the broader optimization landscape. To address this limitation, we constructed a more challenging dataset for large models, while also expanding the diversity of problem types OptMATH-Bench. This dataset includes a carefully curated selection of representative mathematical optimization problems that span a broad range of application scenarios, covering LP, MILP, IP, NLP, SOCP, and other common optimization problems. Additionally, the problems in OptMATH-Bench are inherently challenging, making them effective in distinguishing the modeling capabilities of the model.\nDuring evaluation, we observed that certain ambiguities in problem statements could cause the LLM to struggle in determining whether a variable is integer or continuous. To address this, we applied a rule-based substitution approach: as long as the optimal solution derived under either assumption (integer or continuous variable) matches the ground truth, we consider it a pass. To determine whether the optimal values are equivalent, we use the following formula:\n$\\frac{|Y_{pred} - Y_{label}|}{|Y_{label}| + 1} < \\epsilon$,\nwhere $\\epsilon$ is set to le-6."}, {"title": "A.2 Seed Classes", "content": "Our seed problem classes were curated by drawing from MIPLIB instances and integrating insights from both Chain-of-Experts[81] and peer-reviewed literature. For each instance, we conducted an in-depth analysis of its structure, starting from the problem description to identify its broader optimization category and further refining it into specific subclasses. To ensure theoretical accuracy, we consulted literature that provided detailed descriptions of these optimization subclasses. Based on these references, we formulated the mathematical representation of each subclass, systematically outlining sets (where applicable), parameters, decision variables, objective functions, and constraints. This step aimed to establish an abstract mathematical framework rather than focusing on specific instances.\nWe organized comprehensive metadata for each problem class in a structured metadata.json file, encompassing subclass names, references, reference links, and LaTeX-formulated mathematical expressions. An example of this metadata structure is provided in Appendix E.5. This systematic documentation not only ensures clarity but also facilitates dataset utilization and future extensions.\nNext, we focused on generating new problem instances. We implemented a custom Python class, Generator(), in generator.py, which contained a step-by-step algorithm to create instances of the identified subclasses (an example is provided in Appendix E.6). The input parameters and outputs were explicitly defined, with detailed specifications for each parameter's type and valid range documented in README.txt. We validated the generator by running test_generator() with default parameters to ensure the produced instances were both mathematically valid and practically meaningful.\nThrough this systematic and meticulous approach, we constructed a high-quality dataset of Problem Description (PD) generators that lays a solid foundation for generating natural language descriptions of optimization problems through Backtranslation Pipeline. This dataset is designed to be versatile and scalable, making it suitable for a wide range of applications in optimization research and practice."}, {"title": "A.3 OptMATH-Train", "content": "The OptMATH-Train dataset consists of over 150k reverse-generated samples and 50k augmented instances, forming a comprehensive collection of optimization problems. The dataset encompasses a rich variety of real-world application scenarios. As illustrated in Figure 9, the dataset covers over 10 major application domains spanning across both core business sectors and specialized industries, demonstrating extensive coverage of real-world optimization scenarios. The substantial proportions in logistics, supply chain, and manufacturing ensure robust representation of primary industrial applications, while the balanced inclusion of sectors like transportation, energy, and finance provides comprehensive coverage of specialized use cases. This thoughtful allocation of problems across different domains not only prevents data concentration but also maintains sufficient samples for each sector, enabling effective model training and evaluation.\nThe sequence length distribution of OptMATH-Train, as shown in Figure 10, exhibits a well-balanced profile for both input and output sequences. The distribution approximates a normal distribution with mild right-skewness, centered around 5,000 characters, demonstrating a natural variation in problem complexity. This balanced distribution pattern is particularly advantageous for model training, as it ensures sufficient context length for complex problem representation while maintaining computational efficiency. Furthermore, the moderate right-skewness encompasses challenging cases with extended sequences, which is essential for developing robust models capable of handling sophisticated optimization problems that require comprehensive reasoning and detailed solution steps."}, {"title": "B.1 An Example for Measuring the Complexity", "content": "Let binary variables $y_1, y_2 \\in \\{0,1\\}$ indicate whether products 1 and 2 are produced, integer variables $x_1, x_2 \\in \\mathbb{Z}^+$ represent production quantities, and a continuous variable $z \\geq 0$ denote total cost. The objective function minimizes total operational costs: $\\min z + 10y_1 + 8y_2$."}, {"title": "C.1 Backtranslation Pipeline", "content": "In our reverse generation pipeline, we employ Deepseek-V3[50] as our foundation model and configure its temperature parameter to 0.8 to enhance the diversity of generated problems. Furthermore, to achieve rich contextual diversity, we implement a random scenario assignment mechanism during the Initial Generate phase. This mechanism directs the LLM to synthesize problems that optimally integrate the mathematical characteristics with the designated scenario context. The detailed prompt is elaborated in Section E.1.\nThrough this backtranslation process, we initially generated approximately 120,000 easy optimization problems. As shown in Figure 15, the length distribution of problem descriptions exhibits a right-skewed pattern, with most problems containing 2,000 to 5,000 characters. After applying rejection sampling, around 40% of the generated problems were filtered out while maintaining a similar distribution pattern. This consistency in distribution before and after filtering suggests that our quality control process effectively removes low-quality samples without introducing length-related biases, ensuring the retained problems maintain natural and appropriate descriptive lengths. After multi-stage refinement including semantic verification and difficulty calibration, the pipeline ultimately produced 150,000 rigorously validated optimization problems. Together with 50,000 augmented instances, this curated collection forms our OptMATH-Train dataset, where each instance demonstrates: (1) Contextual alignment between mathematical formulations and real-world scenarios, (2) Controllable complexity levels matching specified difficulty tiers, and (3) Natural language expressions adhering to authentic problem-solving discourse patterns. The hierarchical quality assurance framework ensures the dataset's applicability for both educational interventions and benchmarking mathematical reasoning systems."}, {"title": "C.2 Ablation Study on the Impact of Self-Refine Iterations", "content": "To validate the effectiveness of each step in our backtranslation pipeline, we conducted comprehensive ablation studies. We first compared the accuracy between using only the Generate step versus implementing the complete pipeline with Generate, Self-criticize, and Self-refine steps. To investigate the impact of parameter $T$ on the acceptance rate of rejection sampling, we randomly selected 500 instances for evaluation, with results shown in Figure 14. The results demonstrate that our Self-Refine loop ($T > 1$) consistently outperforms direct generation ($T = 0$) in terms of acceptance rate. While there are some fluctuations in performance across different $T$ values, possibly due to the inherent hallucination tendencies of large language models, we observe that setting $T = 1$ achieves a satisfactory acceptance rate of 61.56%. Considering the trade-off between performance and computational efficiency (token usage), we adopt $T = 1$ in our final data synthesis process."}, {"title": "D.1 CoT Instructions of AutoFormulation", "content": "To support comprehensive mathematical modeling capabilities, we developed a diverse set of CoT instructions, which are detailed in Section E.3. These instructions vary in their decomposition approaches, intermediate reasoning steps, and presentation formats, providing multiple pathways for problem formulation. In Figure 16, we present one representative formulation pattern from our instruction set. This format includes three key components: a general mathematical formulation with standard notation, a detailed instance-specific formulation with complete parameter specifications, and the corresponding Python implementation using Gurobi. However, this represents just one of many possible formulation styles. Other formats in our instruction set may use different ordering of steps, alternative notation systems, or various levels of mathematical abstraction. The diversity in formulation patterns ensures that our dataset captures a wide range of valid mathematical modeling approaches while maintaining logical coherence and mathematical correctness."}, {"title": "D.2 Ablation Study on Augmentation", "content": "As mentioned in the previous section, the purpose of data augmentation is to increase the diversity of the dataset and generate more non-standard problems, which can help the fine-"}]}