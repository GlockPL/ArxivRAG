{"title": "LLM-based MOFs Synthesis Condition\nExtraction using Few-Shot Demonstrations", "authors": ["Lei Shi", "Zhimeng Liu", "Yi Yang", "Weize Wu", "Yuyang Zhang", "Hongbo Zhang", "Jing Lin", "Siyu Wu", "Zihan Chen", "Ruiming Li", "Nan Wang", "Zipeng Liu", "Huobin Tan", "Hongyi Gao", "Yue Zhang", "Ge Wang"], "abstract": "The extraction of Metal-Organic Frameworks (MOFs) synthesis con-\nditions from literature text has been challenging but crucial for the logical\ndesign of new MOFs with desirable functionality. The recent advent of\nlarge language models (LLMs) provides disruptively new solution to this\nlong-standing problem and latest researches have reported over 90% F1\nin extracting correct conditions from MOFs literature. We argue in this\npaper that most existing synthesis extraction practices with LLMs stay\nwith the primitive zero-shot learning, which could lead to downgraded ex-\ntraction and application performance due to the lack of specialized knowl-\nedge. This work pioneers and optimizes the few-shot in-context learning\nparadigm for LLM extraction of material synthesis conditions. First, we\npropose a human-AI joint data curation process to secure high-quality\nground-truth demonstrations for few-shot learning. Second, we apply\na BM25 algorithm based on the retrieval-augmented generation (RAG)\ntechnique to adaptively select few-shot demonstrations for each MOF's\nextraction. Over a dataset randomly sampled from 84,898 well-defined\nMOFs, the proposed few-shot method achieves much higher average F1\nperformance (0.93 vs. 0.81, +14.8%) than the native zero-shot LLM using\nthe same GPT-4 model, under fully automatic evaluation that are more\nobjective than the previous human evaluation. The proposed method is\nfurther validated through real-world material experiments: compared with\nthe baseline zero-shot LLM, the proposed few-shot approach increases the\nMOFs structural inference performance (R2) by 29.4% in average.", "sections": [{"title": "1 Introduction", "content": "Metal-Organic Frameworks (MOFs), a class of high performance porous mate-\nrial, have been widely applied to catalysis, gas storage, and groundwater reme-\ndiation [5] for its prestige in structural tunability and functional versatility [2].\nThese advantages are deeply rooted in the flexible yet logical synthesis con-\nfiguration of MOFs. Herein, precise and comprehensive knowledge of MOFs\nsynthesis conditions becomes extremely important to fully understand its struc-\ntural mechanism and discover new MOFs or sub-types, posing a fundamental\nchallenge to the whole discipline of MOFs and reticular chemistry [23].\nCurrently, there have been 100k+ MOFs successfully synthesized in the lab-\noratory. Their detailed synthesis conditions are often recorded by academic\nliterature in various textual or tabular formats. Machine learning methods, in\nparticular, text mining algorithms, are normally applied to the literature text\nto automatically extract synthesis conditions. However, the complexity and\nvolatility of free text limits the accuracy of synthesis condition extraction [13],\nwhich could jeopardize the effectiveness of downstream material applications\nover extracted synthesis data.\nThe emergence of large language models (LLMs) to some extent resolves\nthe problem of synthesis condition extraction from disparate forms of scientific\ntexts, due to their well-known expertise in the whole-spectrum of text mining\ntasks [3]. Recently, Zheng et al. [24], Dagdelen et al. [7], Polak and Morgan\n[16] have applied zero-shot or fine-tuned LLMs to extract synthesis conditions\nfrom experimental MOFs literature. They reported extraction performance of\nclose to 0.9 in F1 metric, but mostly over small datasets and evaluated by\nsubjective evaluations. It should be pointed out that the baseline zero-shot\nLLMs are notorious for their poor performance on sparse scenarios like MOFs\nsynthesis, which are infrequently covered by the general-purpose LLM training\ndata [6]. Therefore, evaluating the MOFs condition extraction performance with\nlarge-scale, real-life datasets become crucial for improving both the quantity\nand quality of MOFs synthesis knowledgebase. In addition, guided material\nexperiments over extracted synthesis conditions, which are rarely conducted in\nprevious works, should also be an important norm to evaluate the effectiveness\nof targeted synthesis condition extraction task.\nIn this work, we set out to overcome the notable limitations when applying\nprimitive zero-shot LLMs to the problem of MOFs synthesis condition extrac-\ntion from scientific texts. The main theme of this paper is to introduce the\nfew-shot in-context learning paradigm as the standard approach to augment\ngeneral-purpose LLMs on the material synthesis condition extraction problem.\nAs shown by our experiment results of Figure 1, in a dataset randomly sampled\nfrom 84,898 well-defined MOFs, the proposed few-shot method achieves much\nhigher average F1 performance (0.93 vs. 0.81, +14.8%) than the native zero-\nshot LLMs, both using the state-of-the-art GPT-4 Turbo model* [1], as shown\nin Figure 1."}, {"title": "2 Results", "content": "As shown in our technology pipeline of Figure 2, the MOFs literature dataset are\nfirst collected and pre-processed into compatible input format for LLMs (see Sec.\n5.1 for details). The latest high-performance LLM (i.e., GPT-4) is employed to\nextract 10 essential conditions for the synthesis of each MOF: metal precursor\nname & amount, organic linker name & amount, solvent name & amount, mod-\nulator name & amount, and synthesis reaction duration & temperature. The\nsynthesis extraction result is first evaluated on their literal accuracy with respect\nto an expert-curated ground-truth dataset, and then tested on the real-world\nscenarios of material structure inference and design. On the randomly sampled\n123 MOFs synthesis literature from all the 36177 MOFs, the extraction of 1230"}, {"title": "2.1 Human-AI Joint Data Curation", "content": "To introduce the few-shot LLM method, a prerequisite is to obtain a high-\nquality demonstration pool on the synthesis condition extraction task, i.e., the\nground-truth annotations. Traditionally, human annotations are the sole means\nto collect these examples for the few-shot learning. In this work, we also start\nwith a standard annotation protocol which includes three steps: 1) pilot anno-\ntations on 20 typical literature by the leading experts to reach consensus on the\nrigorous format of MOFs synthesis conditions; 2) batch annotations conducted\nby 6 experts over 180 MOFs synthesis paragraphs randomly chosen from the\nentire dataset. Each paragraph is double annotated by two experts to ensure\nreliability; 3) finalized annotations by only keeping the MOFs synthesis con-\nditions that are agreeing between the two experts, while removing annotated\nparagraphs that are inappropriate as examples (e.g., having more than one suite\nof MOFs synthesis conditions in the same paragraph). Eventually, we obtain a\nground-truth human annotation dataset composed of 147 suites of MOFs syn-\nthesis conditions. The full detail of our annotation approach and an online\nsoftware to assist the process is described in Sec. 5.2.\nUsing the human annotations developed above as examples, the performance\nof few-shot LLM models is depicted by the solid orange+triangle lines of Figure\n6. The average F1 metric rises from 0.81 (zero-shot) to the peak of 0.86 (K = 2),\nand does not increase any more. The random example selection shown by the\ndotted orange line oscillates slightly above the zero-shot performance. Both\nalgorithms over purely human annotation perform much worse than the new\nannotation approach described later (an average F1 as high as 0.93, solid blue\nline). It is hypothesized that the key limitation lies in the low data quality of\nfew-shot examples. For more information, we experiment with two other ways\nto generate annotated demonstrations. In the first trial, LLM is initially applied"}, {"title": "2.2 Few-Shot Large Language Model with Material Knowl-\nedge", "content": "Few-shot in-context learning with random examples\nIn the research area of natural language processing (NLP), few-shot in-\ncontext learning (FS-ICL) [6] generally refers to one typical learning paradigm\nto adapt the task-agnostic language models to various downstream tasks while\nachieving optimized performance on each task. In more detail, FS-ICL takes a\nfew prompted examples as input (known as shots), each composed of a context\nand a labeled completion, in addition to background prompts such as task de-\nscription (Figure 4). In the task of MOFs synthesis extraction for instance, a\ncontext refers to a paragraph containing all the synthesis conditions of a MOF\nand the labeled completion refers to the ground-truth synthesis conditions an-\nnotated and curated by human experts in our work. The top-right part of\nFigure 4 gives an example of the labeled completion. FS-ICL is often discussed\nin comparison to the fine-tuning (FT) paradigm, which updates the pre-trained\nlanguage models by incorporating a set of labeled examples via supervised learn-\ning. In both FS-ICL and FT, the final prediction is made by prompting a new\ncontext and asking the language model to complete it.\nThe main advantage of FS-ICL vs. FT lies in its versatility to work on"}, {"title": "2.3 Optimization for High-Throughput MOFs Synthesis\nExtraction", "content": "Though the proposed LLM-based synthesis extraction method achieves state-of-\nthe-art performance in our medium-scale validation set, scalability issues arise"}, {"title": "2.4 MOFs Structure Inference", "content": "To better validate the accuracy and potential of few-shot synthesis extraction\nmethod in downstream tasks, we set up a real-world MOFs synthesis-structure\ninference task and compared it with existing benchmark methods (zero-shot\nLLM). The specific task is to predict the microscopic property of MOFs: global\ncavity diameter, pore limiting diameter, largest cavity diameter, and framework\ndensity, using the synthesis conditions including metals, organic links, solvents,\nand reaction duration/temperature. We evaluate the task performance using co-\nefficient of determination (R2) of each inference model. The R\u00b2 metric effectively\nquantifies a model's explanatory power regarding the actual data variation and\nthe model accuracy. Therefore, it can be used to reflect the impact of different\nsynthesis conditions on MOFs microstructure.\nThe evaluation data is a subset of the CSD database [14], which encompasses\n5269 MOFs. As detailed in Sec. 5.1, these MOFs are carefully selected so that\neach MOF is described by only one scientific literature and the literature will\nonly have one synthesis paragraph. The resulting dataset ensures the validity\nof evaluation by exact correspondence between a MOF's microscopic structure\nand its extracted synthesis conditions.\nUsing the few-shot/zero-shot LLMs and other benchmark methods, the 10\nsynthesis conditions under study are extracted from a unique synthesis para-\ngraph linked to each of the 5269 MOFs. The raw textual conditions extracted\nare post-processed to improve data quality, such as synonym merging and stan-\ndardization of temperature/time scales (Sec. 5.3). On the LLM output by the\nfew-shot method, the top 100, 135, and 20 precursor names of metals, linkers,\nand solvents are selected, which leads to a smaller dataset of 800 MOFs. On the\nLLM by zero-shot method, the distribution of conditions are less longer-tailed,\nso that a stricter filter is applied to obtain the same number of 800 MOFs.\nThese precursor names are embedded into one length-198 feature vector by the\nmethods in Sec. 5.3, where serves as the input features in the material inference\ntask. The target outcome variables are the four microstructure property of a\nMOF. Their calculation procedure is described in Sec. 5.1.\nWe apply six machine learning models for the inference: Lasso Regression,\nBayesian Ridge Regression, AdaBoost, Random Forest, Gradient Boosting Re-\ngression, and Extreme Gradient Boosting (XGBoost). The first five models"}, {"title": "3 Methods", "content": "3.1\nSynthesis paragraph detection\nTo train a machine learning model for binary classification to determine whether\na paragraph is synthesized, we randomly obtained 440 papers from the database\nin Appendix A for annotation. Each paper was annotated by two different an-\nnotators to ensure inner annotator agreement. The 880 annotation tasks were\nassigned to four annotators who used our platform, shown in Figure 9, to anno-\ntate synthesis-related paragraphs. After annotation, only paragraphs annotated\nby both annotators were considered valid, while paragraphs annotated by only\none annotator were discarded. If there was an overlap in the positions of the\nparagraphs annotated by the two annotators, we found that mismatched para-\ngraphs often occurred because one annotator noted more synthesis parameters\nand thus marked a larger range. In such cases, the larger annotated paragraph\nwas considered valid. This method also resolved minor annotation deviations\nwithin a few characters, allowing two slightly different synthesis paragraphs to\nbe considered valid. This process yielded 1,349 valid annotated paragraphs.\nTo train the discrimination model, non-synthesis paragraphs were needed"}, {"title": "3.2 Few-Shot RAG Algorithms", "content": "To maximize the extraction performance of the model, we provide examples\nof extraction by human-AI annotation as demonstrations. By using a retrieve\nK demonstrations, the performance of LLMs in extracting synthesis conditions\non MOFs can be further improved [9, 12, 19]. Given the set of demonstrations\nD = d1, d2,..., dn and an input paragraph p, the top K similar demonstrations\nare obtained as:\nTop-K = sort((score(p, di), di)=1)[: k]\n(1)\nHere, the score is used to estimate the similarity between the embeddings of\ndocument di and paragraph p. The embedding models can be categorized into\ntraditional sparse vector encoders (e.g., TF-IDF, BM25 [18]) and semantic dense\nvector encoders (e.g., SBERT [8,17]) [10]. In our experiments, we compared\nthese two classes of retrieval methods and selected the one that performed best\nas the final approach.\nFor the traditional sparse vector retrieval method, we use the BM25 algo-\nrithm. BM25 is a probabilistic information retrieval model that ranks docu-\nments based on the frequency of query terms within the documents. It balances\nterm frequency (how often a term appears in a document) with inverse docu-"}, {"title": "4 Conclusion", "content": "This work studies the new paradigm of applying few-shot in-context learning to\nthe popular approach of LLM literature extraction for discovering MOFs syn-\nthesis conditions. It is shown through experiments that both the quality and the\nquantity of few-shot demonstrations are important in the studied scenario. We\nintroduce both a novel process of human-AI joint data curation to enhance few-\nshot demonstration quality and a calibrated BM-25 RAG algorithm to size the\noptimal few-shot quantity. Scalability issues regarding high-throughput MOFs\nsynthesis condition extraction are resolved using many practical methods such\nas offline synthesis paragraph detection and LLM-based coreference resolution.\nOur proposal is thoroughly evaluated using large-scale real-life MOFs dataset,\non both text extraction performance for synthesis condition discovery and the\ndownstream material task on structural property inference."}, {"title": "5 Appendix", "content": "5.1 MOFs Data\nCSD and the retrieved dataset\nWe base our work on the MOF subset of Cambridge Structural Database\n(CSD) [14] retrieved in June 2022, which lists 84,898 MOFs covering the bonding\nmotifs of all common MOFs in CSD. The entry of a MOF in the database\ncontains its structure in CIF format, the physical properties, a DOI linking to\nthe relevant publication, and a unique MOF ID.\nThe dataset is then pre-processed according to the goal of this work. First,\nthe full-text describing the MOFs under study should be available. Out of all\nthe 84,898 MOFs, 78,741 has non-empty DOIs. Since the same DOI could be\nlinked to multiple MOFs (one paper reporting more than one MOFs), there\nleaves 39,579 different DOI links after deduplication and 36,177 downloadable\npaper full-text. For the convenience of follow-up processing, we focus on the\nDOIs where the associated publication reports the information of only one MOF\nin CSD. This leads to a subset of 22,461 MOFs, each with a unique publication\nfile in PDF format.\nNext, the PDF of each MOF is converted to plain text [20] and segmented\ninto paragraphs. The high performance classification model in Sec. 3.1 is ap-\nplied to detect synthesis paragraphs enclosing the desired synthesis condition\ninformation. Again, for the sake of convenience and accuracy, we only consider\nthe 5,269 MOFs/publications that contain exactly one synthesis paragraph. An-\nother 12,606 publications do not have any synthesis paragraph, probably because\nthese papers are not related to MOFs experiments. The other 4,586 publications\nhave more than one synthesis paragraphs, as they are describing multiple MOFs\nor synthesis routes. Our pipeline could work with papers having more than one\nsuite of synthesis conditions, but the potential MOF-synthesis mismatch may\ndowngrade the application performance in evaluation. Therefore, throughout\nthis work we stick to the core dataset of 5,269 MOFs/publications and their\nunique synthesis paragraph.\nMicrostructure Property Computation\nFor material evaluation purpose, we also calculate structural and physical\nproperties of the 5,269 MOFs under consideration. The CIF file of each MOF is\nretrieved from CSD and input to the Zeo++ tool [22]. In total, four structural\nand physical properties are calculated: global cavity diameter, pore limiting di-\nameter, largest cavity diameter, and framework density. We set the probe ra-\ndius to 1.29A to simulate helium gas molecules, and the number of Monte Carlo\nsamples to 100,000 to ensure the accuracy of calculations. All Zeo++ param-\neters adhere to standard routines, guaranteeing that the computed properties\naccurately represent the behavior of gas molecules within the MOF structure."}, {"title": "5.2 Annotation Procedure for Synthesis Paragraphs and\nSynthesis Conditions", "content": "High-quality annotations are the cornerstone of few-shot in-context learning;\nonly accurate and highly coherence annotations can improve the precision of\nextracting. Therefore, we enlisted the help of eight experts in materials science\nand engineering to assist with the annotations. Additionally, we developed a\nbatch interactive annotation platform to enhance the convenience of the an-\nnotation process. During the annotation process, we discovered that the task\nwas challenging and had a high error rate done by human only, which led to\npoor model extraction performance when using erroneously annotated exam-\nples. Consequently, we implemented a comprehensive annotation process to\nimprove quality.\nSynthesis Paragraph Annotation\nTo annotate synthesis paragraphs for offline machine learning, 440 papers\nwere randomly obtained from the database in Appendix A. For inner anno-\ntator agreement, each paper was annotated by two different annotators. The\n880 annotation tasks were assigned to four annotators, who used our platform\nshown in Figure 9 to annotate synthesis-related paragraphs. After annotation,\nonly paragraphs annotated by both annotators were considered valid, and para-\ngraphs annotated by only one annotator were discarded. If there was an overlap\nin the positions of the paragraphs annotated by the two annotators, we found\nthrough checking the annotated data that the common mismatched paragraphs\noften occurred because one annotator noted more synthesis conditons and thus\nmarked a larger range for the synthesis paragraph. In such cases, the paragraph\nshould also be considered valid. Therefore, we treated the larger annotated\nparagraph as a valid synthesis paragraph. This method also resolved the issue\nof minor annotation deviations within a few characters, allowing two slightly dif-"}, {"title": "5.3 Post-processing of Synthesis Conditions", "content": "The raw synthesis conditions extracted by LLM-based method often suffer from\ndata quality issue, which potentially affects the downstream material inference\ntask. We introduce several data postprocessing methods to improve the quality\nof derived synthesis conditions so that the input data to the inference model\ncan be more formatted and densely distributed.\n5.3.1 Data Cleansing on Textual Conditions\nThe synthesis conditions include discrete names for Metal, Organic Linker, Sol-\nvent, and additives. These names often have different representations for the\nsame substance (e.g., \"H2O\" and \"Water\" both represent water, \"Cd(NO3)2.4H2O\""}, {"title": "5.4 Visual MOFs Synthesis Condition Extraction Engine\nand Database", "content": "Database and Engine\nTo streamline the entire workflow and efficiently organize the extraction re-\nsults from related papers, we developed the Visual MOFs Synthesis Extraction\nEngine and Database. Using our approach, we processed over 30,000 papers\nand extracted 57,081 synthesis paragraphs, on which we then performed syn-\nthesis condition extraction. To better view and analyze the vast amount of\nextraction results, we built a comprehensive database with 2 features: 1) Basic\nStatistics: The database provides basic statistics on all extraction results, in-\ncluding data on synthesis paragraphs and various synthesis conditions (Figure\n11). 2) Advanced Search Capabilities: This database is designed to support\nlogical expression searches for specific fields, allowing users to search for syn-\nthesis conditions, paper titles, and synthesis paragraph content with precision,\nand enables visualization of the retrieval results."}]}