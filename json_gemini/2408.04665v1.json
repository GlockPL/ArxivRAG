{"title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations", "authors": ["Lei Shi", "Zhimeng Liu", "Yi Yang", "Weize Wu", "Yuyang Zhang", "Hongbo Zhang", "Jing Lin", "Siyu Wu", "Zihan Chen", "Ruiming Li", "Nan Wang", "Zipeng Liu", "Huobin Tan", "Hongyi Gao", "Yue Zhang", "Ge Wang"], "abstract": "The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from literature text has been challenging but crucial for the logical design of new MOFs with desirable functionality. The recent advent of large language models (LLMs) provides disruptively new solution to this long-standing problem and latest researches have reported over 90% F1 in extracting correct conditions from MOFs literature. We argue in this paper that most existing synthesis extraction practices with LLMs stay with the primitive zero-shot learning, which could lead to downgraded extraction and application performance due to the lack of specialized knowledge. This work pioneers and optimizes the few-shot in-context learning paradigm for LLM extraction of material synthesis conditions. First, we propose a human-AI joint data curation process to secure high-quality ground-truth demonstrations for few-shot learning. Second, we apply a BM25 algorithm based on the retrieval-augmented generation (RAG) technique to adaptively select few-shot demonstrations for each MOF's extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the proposed few-shot method achieves much higher average F1 performance (0.93 vs. 0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under fully automatic evaluation that are more objective than the previous human evaluation. The proposed method is further validated through real-world material experiments: compared with the baseline zero-shot LLM, the proposed few-shot approach increases the MOFs structural inference performance (R\u00b2) by 29.4% in average.", "sections": [{"title": "1 Introduction", "content": "Metal-Organic Frameworks (MOFs), a class of high performance porous mate- rial, have been widely applied to catalysis, gas storage, and groundwater reme- diation [5] for its prestige in structural tunability and functional versatility [2]. These advantages are deeply rooted in the flexible yet logical synthesis con- figuration of MOFs. Herein, precise and comprehensive knowledge of MOFs synthesis conditions becomes extremely important to fully understand its struc- tural mechanism and discover new MOFs or sub-types, posing a fundamental challenge to the whole discipline of MOFs and reticular chemistry [23]. Currently, there have been 100k+ MOFs successfully synthesized in the lab- oratory. Their detailed synthesis conditions are often recorded by academic literature in various textual or tabular formats. Machine learning methods, in particular, text mining algorithms, are normally applied to the literature text to automatically extract synthesis conditions. However, the complexity and volatility of free text limits the accuracy of synthesis condition extraction [13], which could jeopardize the effectiveness of downstream material applications over extracted synthesis data. The emergence of large language models (LLMs) to some extent resolves the problem of synthesis condition extraction from disparate forms of scientific texts, due to their well-known expertise in the whole-spectrum of text mining tasks [3]. Recently, Zheng et al. [24], Dagdelen et al. [7], Polak and Morgan [16] have applied zero-shot or fine-tuned LLMs to extract synthesis conditions from experimental MOFs literature. They reported extraction performance of close to 0.9 in F1 metric, but mostly over small datasets and evaluated by subjective evaluations. It should be pointed out that the baseline zero-shot LLMs are notorious for their poor performance on sparse scenarios like MOFs synthesis, which are infrequently covered by the general-purpose LLM training data [6]. Therefore, evaluating the MOFs condition extraction performance with large-scale, real-life datasets become crucial for improving both the quantity and quality of MOFs synthesis knowledgebase. In addition, guided material experiments over extracted synthesis conditions, which are rarely conducted in previous works, should also be an important norm to evaluate the effectiveness of targeted synthesis condition extraction task. In this work, we set out to overcome the notable limitations when applying primitive zero-shot LLMs to the problem of MOFs synthesis condition extrac- tion from scientific texts. The main theme of this paper is to introduce the few-shot in-context learning paradigm as the standard approach to augment general-purpose LLMs on the material synthesis condition extraction problem. As shown by our experiment results of Figure 1, in a dataset randomly sampled from 84,898 well-defined MOFs, the proposed few-shot method achieves much higher average F1 performance (0.93 vs. 0.81, +14.8%) than the native zero- shot LLMs, both using the state-of-the-art GPT-4 Turbo model* [1], as shown in Figure 1."}, {"title": "Nevertheless, deploying few-shot LLMs to solve the current problem still faces multiple nontrivial challenges.", "content": "First, the superiority of few-shot LLMs depends on the data quality of their ground-truth demonstrations. In the sce- nario of MOFs synthesis extraction, obtaining ground-truth textual conditions scattered in scientific literature in numerous formats remains a daunting task. It would be extremely costly to apply traditional human annotation approach given that a change of material would require a totally new demonstration dataset. Second, the quantity of ground-truth demonstrations selected for each LLM extraction is also critical as high-performance LLMs are mostly commercial and charged by input size. For example, the fine-tuning technology is known to greatly improve the LLM performance [7], but will normally require hundreds of examples and a locally-stored large set of model weights. The application overheads to new synthesis extraction scenarios are quite high, thus reducing the adaptability of fine-tuning methods. In our case, minimizing the number of few-shot demonstrations would require an elaborate algorithm to select the demonstrations adaptively for each MOF's raw synthesis text. In this paper, we introduce two new methods to resolve the above challenges. First, on the preparation of ground-truth demonstrations, to our surprise, hu- man annotation and AI annotation show complementary advantages, not only in the annotation cost, but also in their output data quality. We then propose a human-AI joint data curation process, which enjoys the best of both worlds and offers the highest data quality in ground-truth demonstrations produced. Second, based on the popular retrieval-augmented generation (RAG) technique, we propose to apply the BM25 algorithm to adaptively select the best combi- nation of few-shot demonstrations for each MOF's synthesis extraction, whose performance significantly outruns the baseline random selection method. Our experiment results also suggest the most appropriate number of demonstrations for the trade-off between performance and cost. It is shown that a small overhead of 4-shots could already achieve the optimal performance, contrasting to tens to a hundred shots in other domains. In addition, we study the utility of different kinds of knowledge on our task when incorporated by LLM: the background knowledge on retrieved synthesis conditions, their application constraints on the numerical/textual format, and the few-shot demonstrations. Notably, the few-shot examples are shown to be the most critical. To our knowledge, we are the first to apply and optimize few-shot in-context learning LLM methods for the material synthesis condition extraction problem from scientific text. Moreover, we have considered the scalability issue for high-throughput syn- thesis extraction. The additional overhead includes the labor cost to acquire external knowledge (e.g., expert annotations on the literature text), the finan- cial cost to request LLM APIs, and the computational cost to potentially train in-house LLMs. For example, by the latest GPT-4 pricing model (10$ per 1M tokens), a single pass over all the 100k available MOFs synthesis literature (est. 10k words per literature) sums up to a non-negligible cost of 10k$, while per- formance tuning normally requires several passes. Three techniques adapted to large-scale material data are proposed. First, we learn an offline model to detect the most relevant synthesis paragraphs out of each literature, with an"}, {"title": "2 Results", "content": "As shown in our technology pipeline of Figure 2, the MOFs literature dataset are first collected and pre-processed into compatible input format for LLMs (see Sec. 5.1 for details). The latest high-performance LLM (i.e., GPT-4) is employed to extract 10 essential conditions for the synthesis of each MOF: metal precursor name & amount, organic linker name & amount, solvent name & amount, mod- ulator name & amount, and synthesis reaction duration & temperature. The synthesis extraction result is first evaluated on their literal accuracy with respect to an expert-curated ground-truth dataset, and then tested on the real-world scenarios of material structure inference and design. On the randomly sampled 123 MOFs synthesis literature from all the 36177 MOFs, the extraction of 1230"}, {"title": "2.1 Human-AI Joint Data Curation", "content": "To introduce the few-shot LLM method, a prerequisite is to obtain a high- quality demonstration pool on the synthesis condition extraction task, i.e., the ground-truth annotations. Traditionally, human annotations are the sole means to collect these examples for the few-shot learning. In this work, we also start with a standard annotation protocol which includes three steps: 1) pilot anno- tations on 20 typical literature by the leading experts to reach consensus on the rigorous format of MOFs synthesis conditions; 2) batch annotations conducted by 6 experts over 180 MOFs synthesis paragraphs randomly chosen from the entire dataset. Each paragraph is double annotated by two experts to ensure reliability; 3) finalized annotations by only keeping the MOFs synthesis con- ditions that are agreeing between the two experts, while removing annotated paragraphs that are inappropriate as examples (e.g., having more than one suite of MOFs synthesis conditions in the same paragraph). Eventually, we obtain a ground-truth human annotation dataset composed of 147 suites of MOFs syn- thesis conditions. The full detail of our annotation approach and an online software to assist the process is described in Sec. 5.2. Using the human annotations developed above as examples, the performance of few-shot LLM models is depicted by the solid orange+triangle lines of Figure 6. The average F1 metric rises from 0.81 (zero-shot) to the peak of 0.86 (K = 2), and does not increase any more. The random example selection shown by the dotted orange line oscillates slightly above the zero-shot performance. Both algorithms over purely human annotation perform much worse than the new annotation approach described later (an average F1 as high as 0.93, solid blue line). It is hypothesized that the key limitation lies in the low data quality of few-shot examples. For more information, we experiment with two other ways to generate annotated demonstrations. In the first trial, LLM is initially applied"}, {"title": "in a zero-shot mode to extract all synthesis conditions from the input paragraph.", "content": "The 1st-round LLM output is then used as the data annotation (examples) in the 2nd-round few-shot LLM in-context learning. The green+diamond lines in Figure 6 indicate that the performance with this AI-based annotation is still constantly below the best approach. In the final trial, we input synthesis paragraph without annotation as the examples (i.e., the lowest data quality). As expected, the red+square performance charts in Figure 6 achieves the lowest F1 and ACC. As more raw paragraphs are used, i.e., increasing K, both metrics drop. The explanation might be that more information without ground-truth distracts the LLM, rather than coaches it. The above results indicate that neither human annotations nor purely AI- generated examples achieve optimal data quality for LLM few-shot learning. To delve deeper into the issue, several leading MOF experts were consulted to evaluate all errors produced by the few-shot LLM method when using hu- man annotations as the sole examples and ground-truths. Out of 261 potential errors, 103 LLM outputs (39.5%) were identified as correct, 38 (14.6%) had certain issues but contributed to refining the corresponding ground-truth, and only 120 (45.9%) were true errors. The experts then compiled a revised set of ground-truth annotations, including the synthesis conditions for 123 individ- ual MOFs. The remaining 23 annotated conditions were deemed inappropriate because they either involved chiral MOFs with duplicate synthesis conditions and paragraphs or contained multiple MOF synthesis processes within a single paragraph. Although our technical framework can deal with the case of having multiple MOFs in a single paragraph, we chose the paragraphs describing the synthesis of only one MOF for more precise demonstrations. With this empirical experience, we propose a human-AI joint data curation process for the data quality optimization of ground-truth demonstrations in LLM-based few-shot learning paradigm. As shown in Figure 3(a), raw synthesis paragraphs are first processed by LLM in a zero-shot mode. Human experts then work on the initial AI annotation to achieve a best-effort human anno- tation (Figure 3(b)), which is the first round of reflection. After that, these human annotations are used as demonstrations in a LLM-based few-shot syn- thesis condition extraction Figure 3(c), which is the second round of reflection and generates few-shot AI annotations. Lastly, human experts combines human annotations and few-shot AI annotations into the human-AI joint annotation (Figure 3(d)), in the final round of reflection. We apply the few-shot LLM model over the final demonstrations with the highest-level of data quality. The best performance (F1=0.93 and ACC=0.9) is achieved at the point of most ap- propriate few-shot quantity (K = 4, as shown by the solid blue lines in Figure 6. We ascribe the superiority of human-AI joint data curation to three reasons, all due to the complementary nature of human expertise and AI's capacity. First, though human are excellent in flexible usage of material knowledge, they often fail to strictly follow pre-defined annotation rules. For example, to stan- dardize the solvent condition, it is required to leave out all modifiers of a common solvent. Human annotators sometimes extract \"hot water\" instead of \"water\","}, {"title": "2.2 Few-Shot Large Language Model with Material Knowledge", "content": "Few-shot in-context learning with random examples In the research area of natural language processing (NLP), few-shot in- context learning (FS-ICL) [6] generally refers to one typical learning paradigm to adapt the task-agnostic language models to various downstream tasks while achieving optimized performance on each task. In more detail, FS-ICL takes a few prompted examples as input (known as shots), each composed of a context and a labeled completion, in addition to background prompts such as task de- scription (Figure 4). In the task of MOFs synthesis extraction for instance, a context refers to a paragraph containing all the synthesis conditions of a MOF and the labeled completion refers to the ground-truth synthesis conditions an- notated and curated by human experts in our work. The top-right part of Figure 4 gives an example of the labeled completion. FS-ICL is often discussed in comparison to the fine-tuning (FT) paradigm, which updates the pre-trained language models by incorporating a set of labeled examples via supervised learn- ing. In both FS-ICL and FT, the final prediction is made by prompting a new context and asking the language model to complete it. The main advantage of FS-ICL vs. FT lies in its versatility to work on"}, {"title": "2.3 Optimization for High-Throughput MOFs Synthesis Extraction", "content": "Though the proposed LLM-based synthesis extraction method achieves state-of- the-art performance in our medium-scale validation set, scalability issues arise"}, {"title": "2.4 MOFs Structure Inference", "content": "To better validate the accuracy and potential of few-shot synthesis extraction method in downstream tasks, we set up a real-world MOFs synthesis-structure inference task and compared it with existing benchmark methods (zero-shot LLM). The specific task is to predict the microscopic property of MOFs: global cavity diameter, pore limiting diameter, largest cavity diameter, and framework density, using the synthesis conditions including metals, organic links, solvents, and reaction duration/temperature. We evaluate the task performance using co- efficient of determination (R\u00b2) of each inference model. The R\u00b2 metric effectively quantifies a model's explanatory power regarding the actual data variation and the model accuracy. Therefore, it can be used to reflect the impact of different synthesis conditions on MOFs microstructure. The evaluation data is a subset of the CSD database [14], which encompasses 5269 MOFs. As detailed in Sec. 5.1, these MOFs are carefully selected so that each MOF is described by only one scientific literature and the literature will only have one synthesis paragraph. The resulting dataset ensures the validity of evaluation by exact correspondence between a MOF's microscopic structure and its extracted synthesis conditions. Using the few-shot/zero-shot LLMs and other benchmark methods, the 10 synthesis conditions under study are extracted from a unique synthesis para- graph linked to each of the 5269 MOFs. The raw textual conditions extracted are post-processed to improve data quality, such as synonym merging and stan- dardization of temperature/time scales (Sec. 5.3). On the LLM output by the few-shot method, the top 100, 135, and 20 precursor names of metals, linkers, and solvents are selected, which leads to a smaller dataset of 800 MOFs. On the LLM by zero-shot method, the distribution of conditions are less longer-tailed, so that a stricter filter is applied to obtain the same number of 800 MOFs. These precursor names are embedded into one length-198 feature vector by the methods in Sec. 5.3, where serves as the input features in the material inference task. The target outcome variables are the four microstructure property of a MOF. Their calculation procedure is described in Sec. 5.1. We apply six machine learning models for the inference: Lasso Regression, Bayesian Ridge Regression, AdaBoost, Random Forest, Gradient Boosting Re- gression, and Extreme Gradient Boosting (XGBoost). The first five models"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Synthesis paragraph detection", "content": "To train a machine learning model for binary classification to determine whether a paragraph is synthesized, we randomly obtained 440 papers from the database in Appendix A for annotation. Each paper was annotated by two different an- notators to ensure inner annotator agreement. The 880 annotation tasks were assigned to four annotators who used our platform, shown in Figure 9, to anno- tate synthesis-related paragraphs. After annotation, only paragraphs annotated by both annotators were considered valid, while paragraphs annotated by only one annotator were discarded. If there was an overlap in the positions of the paragraphs annotated by the two annotators, we found that mismatched para- graphs often occurred because one annotator noted more synthesis parameters and thus marked a larger range. In such cases, the larger annotated paragraph was considered valid. This method also resolved minor annotation deviations within a few characters, allowing two slightly different synthesis paragraphs to be considered valid. This process yielded 1,349 valid annotated paragraphs. To train the discrimination model, non-synthesis paragraphs were needed"}, {"title": "3.2 Few-Shot RAG Algorithms", "content": "To maximize the extraction performance of the model, we provide examples of extraction by human-AI annotation as demonstrations. By using a retrieve K demonstrations, the performance of LLMs in extracting synthesis conditions on MOFs can be further improved [9, 12, 19]. Given the set of demonstrations D = d\u2081, d\u2082,..., d\u2099 and an input paragraph p, the top K similar demonstrations are obtained as: Top-K = sort((score(p, d\u1d62), d\u1d62)\u1d62\u208c\u2081\u207f)[: k]  (1) Here, the score is used to estimate the similarity between the embeddings of document d\u1d62 and paragraph p. The embedding models can be categorized into traditional sparse vector encoders (e.g., TF-IDF, BM25 [18]) and semantic dense vector encoders (e.g., SBERT [8,17]) [10]. In our experiments, we compared these two classes of retrieval methods and selected the one that performed best as the final approach. For the traditional sparse vector retrieval method, we use the BM25 algo- rithm. BM25 is a probabilistic information retrieval model that ranks docu- ments based on the frequency of query terms within the documents. It balances term frequency (how often a term appears in a document) with inverse docu-"}, {"title": "4 Conclusion", "content": "This work studies the new paradigm of applying few-shot in-context learning to the popular approach of LLM literature extraction for discovering MOFs syn- thesis conditions. It is shown through experiments that both the quality and the quantity of few-shot demonstrations are important in the studied scenario. We introduce both a novel process of human-AI joint data curation to enhance few- shot demonstration quality and a calibrated BM-25 RAG algorithm to size the optimal few-shot quantity. Scalability issues regarding high-throughput MOFs synthesis condition extraction are resolved using many practical methods such as offline synthesis paragraph detection and LLM-based coreference resolution. Our proposal is thoroughly evaluated using large-scale real-life MOFs dataset, on both text extraction performance for synthesis condition discovery and the downstream material task on structural property inference."}, {"title": "5 Appendix", "content": ""}, {"title": "5.1 MOFs Data", "content": "CSD and the retrieved dataset We base our work on the MOF subset of Cambridge Structural Database (CSD) [14] retrieved in June 2022, which lists 84,898 MOFs covering the bonding motifs of all common MOFs in CSD. The entry of a MOF in the database contains its structure in CIF format, the physical properties, a DOI linking to the relevant publication, and a unique MOF ID. The dataset is then pre-processed according to the goal of this work. First, the full-text describing the MOFs under study should be available. Out of all the 84,898 MOFs, 78,741 has non-empty DOIs. Since the same DOI could be linked to multiple MOFs (one paper reporting more than one MOFs), there leaves 39,579 different DOI links after deduplication and 36,177 downloadable paper full-text. For the convenience of follow-up processing, we focus on the DOIs where the associated publication reports the information of only one MOF in CSD. This leads to a subset of 22,461 MOFs, each with a unique publication file in PDF format. Next, the PDF of each MOF is converted to plain text [20] and segmented into paragraphs. The high performance classification model in Sec. 3.1 is ap- plied to detect synthesis paragraphs enclosing the desired synthesis condition information. Again, for the sake of convenience and accuracy, we only consider the 5,269 MOFs/publications that contain exactly one synthesis paragraph. An- other 12,606 publications do not have any synthesis paragraph, probably because these papers are not related to MOFs experiments. The other 4,586 publications have more than one synthesis paragraphs, as they are describing multiple MOFs or synthesis routes. Our pipeline could work with papers having more than one suite of synthesis conditions, but the potential MOF-synthesis mismatch may downgrade the application performance in evaluation. Therefore, throughout this work we stick to the core dataset of 5,269 MOFs/publications and their unique synthesis paragraph. Microstructure Property Computation For material evaluation purpose, we also calculate structural and physical properties of the 5,269 MOFs under consideration. The CIF file of each MOF is retrieved from CSD and input to the Zeo++ tool [22]. In total, four structural and physical properties are calculated: global cavity diameter, pore limiting di- ameter, largest cavity diameter, and framework density. We set the probe ra- dius to 1.29A to simulate helium gas molecules, and the number of Monte Carlo samples to 100,000 to ensure the accuracy of calculations. All Zeo++ param- eters adhere to standard routines, guaranteeing that the computed properties accurately represent the behavior of gas molecules within the MOF structure."}, {"title": "5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions", "content": "High-quality annotations are the cornerstone of few-shot in-context learning; only accurate and highly coherence annotations can improve the precision of extracting. Therefore, we enlisted the help of eight experts in materials science and engineering to assist with the annotations. Additionally, we developed a batch interactive annotation platform to enhance the convenience of the an- notation process. During the annotation process, we discovered that the task was challenging and had a high error rate done by human only, which led to poor model extraction performance when using erroneously annotated exam- ples. Consequently, we implemented a comprehensive annotation process to improve quality. Synthesis Paragraph Annotation To annotate synthesis paragraphs for offline machine learning, 440 papers were randomly obtained from the database in Appendix A. For inner anno- tator agreement, each paper was annotated by two different annotators. The 880 annotation tasks were assigned to four annotators, who used our platform shown in Figure 9 to annotate synthesis-related paragraphs. After annotation, only paragraphs annotated by both annotators were considered valid, and para- graphs annotated by only one annotator were discarded. If there was an overlap in the positions of the paragraphs annotated by the two annotators, we found through checking the annotated data that the common mismatched paragraphs often occurred because one annotator noted more synthesis conditons and thus marked a larger range for the synthesis paragraph. In such cases, the paragraph should also be considered valid. Therefore, we treated the larger annotated paragraph as a valid synthesis paragraph. This method also resolved the issue of minor annotation deviations within a few characters, allowing two slightly dif-"}, {"title": "5.3 Post-processing of Synthesis Conditions", "content": "The raw synthesis conditions extracted by LLM-based method often suffer from data quality issue, which potentially affects the downstream material inference task. We introduce several data postprocessing methods to improve the quality of derived synthesis conditions so that the input data to the inference model can be more formatted and densely distributed."}, {"title": "5.3.1 Data Cleansing on Textual Conditions", "content": "The synthesis conditions include discrete names for Metal, Organic Linker, Sol- vent, and additives. These names often have different representations for the same substance (e.g., \"H2O\" and \"Water\" both represent water, \"Cd(NO3)2.4H2O\""}, {"title": "5.3.2 Standardization of Numeric Conditions on Time and Temper- ature", "content": "After processing the discrete names in the synthesis conditions, we continued to parse and capture numerical data for time and temperature. These data may have quality issues such as inconsistent units and the presence of special characters. To address these issues, we performed the following normalization: Extracting and Formatting Data Using GPT-4, we extracted and formatted relevant data for time and tem- perature. Unit Standardization We defined standard units for each data type. For example, time was stan- dardized to hours, and temperature was standardized to Celsius (room temper- ature set at 25\u00b0C). Cleaning Special Characters."}, {"title": "5.3.3 Data Filtering by Synthesis Condition Distributions", "content": "After data cleansing and standardization, the distribution of different synthe- sis conditions becomes more centralized. As shown in Figure 10, the entity lists of both metal source and solvent are shortened. The number of unique organic linkers remains high due to its long-tailed distribution. In the applica- tion of MOFs microstructure property inference, we will only select these MOFs synthesized by top entities in metal source, organic linker, and solvent. For ex- ample, by default we apply a filter of (100, 135, 20), which select the MOFs having top-100 metal source in the ranked list of Figure 10(a), top-135 organic linker, and top-20 solvent. Note that for LLM models in comparison, different filters may be applied to ensure the same number of MOFs in the dataset."}, {"title": "5.3.4 Feature Embedding for Metal, Organic Linker, and Solvent Data", "content": "After disambiguation and merging, we obtained high-quality precursor/solvent data. To build accurate predictive models, we need to perform correspond- ing feature embedding to capture the material/structural characteristics of the precursor/solvent data. The specific steps are as follows: Obtaining Chemical Formulas and SMILES Using GPT-4, we obtained the chemical formulas and SMILES for the top 100 Metals and the top 20 Solvents after disambiguation and merging. For Organic Linkers, due to the complexity of their naming, GPT-4 could not accu- rately obtain the corresponding SMILES. Therefore, we manually collected the SMILES for the top 135 Organic Linkers after disambiguation and merging. Calculating Molecular Features Based on the obtained SMILES, we used RDKit to calculate the molecular features of Metals, Organic Linkers, and Solvents, including molecular weight, LogP values, the number of hydrogen bond donors and acceptors, Labute sur- face area, maximum molecular distance, molecular length, width, height, and topological polar surface area (TPSA). Calculating Metal Salt Features Using the Composition class from Pymatgen, we automatically inferred and assigned oxidation states for the chemical formulas of metal salts. Using the MultipleFeaturizer class from the Matminer library, we calculated a series of chemical features, including elemental properties, atomic orbitals, electron affin- ity, and electronegativity differences. Additionally, we included features of the metal elements contained in the MOFs, such as atomic mass, atomic radius, thermal conductivity, and detailed electronic configuration vector representa-"}, {"title": "5.4 Visual MOFs Synthesis Condition Extraction Engine and Database", "content": "Database and Engine To streamline the entire workflow and efficiently organize the extraction re- sults from related papers, we developed the Visual MOFs Synthesis Extraction Engine and Database. Using our approach, we processed over 30,000 papers and extracted 57,081 synthesis paragraphs, on which we then performed syn- thesis condition extraction. To better view and analyze the vast amount of extraction results, we built a comprehensive database with 2 features: 1) Basic Statistics: The database provides basic statistics on all extraction results, in- cluding data on synthesis paragraphs and various synthesis conditions (Figure 11). 2) Advanced Search Capabilities: This database is designed to support logical expression searches for specific fields, allowing users to search for syn- thesis conditions, paper titles, and synthesis paragraph content with precision, and enables visualization of the retrieval results."}, {"title": "Configurable Extraction:", "content": "The engine supports configuration for synthesis condition extraction, allowing users to adjust the sample quantity and selection method input into the large model."}, {"title": "Organized and Visualized Data:", "content": "The extracted conditions are systemati- cally organized and visualized for data interpretation and analysis."}, {"title": "Synthesis Visualization", "content": "The visualization system we designed can support users in analyzing synthe- sis paragraphs. Initially, users upload batch PDF papers and process through the LLM. Once extraction is complete, users can utilize the filtering panel to se- lect specific paragraphs for analysis. The overall performance panel (Fig.12(a)) then displays four key performance metrics of the LLM resolution, with a de- fault HeatMap (Fig.12(b).I) providing a detailed view of entity resolution perfor- mance across all evaluation metrics. Suppose further detail on specific metrics is needed. In that case, users can access the second tab (Fig.12(b).II), sliding down"}]}