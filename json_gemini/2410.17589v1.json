{"title": "Challenge on Sound Scene Synthesis: Evaluating Text-to-Audio Generation", "authors": ["Junwon Lee", "Modan Tailleur", "Laurie M. Heller", "Keunwoo Choi", "Mathieu Lagrange", "Brian McFee", "Keisuke Imoto", "Yuki Okamoto"], "abstract": "Despite significant advancements in neural text-to-audio generation, challenges persist in controllability and evaluation. This paper addresses these issues through the Sound Scene Synthesis challenge held as part of the Detection and Classification of Acoustic Scenes and Events 2024. We present an evaluation protocol combining objective metric, namely Fr\u00e9chet Audio Distance, with perceptual assessments, utilizing a structured prompt format to enable diverse captions and effective evaluation. Our analysis reveals varying performance across sound categories and model architectures, with larger models generally excelling but innovative lightweight approaches also showing promise. The strong correlation between objective metrics and human ratings validates our evaluation approach. We discuss outcomes in terms of audio quality, controllability, and architectural considerations for text-to-audio synthesizers, providing direction for future research.", "sections": [{"title": "1 Introduction", "content": "Sound is of paramount importance in the creation of an immersive user experience in multimedia content such as movies and games, not to mention real-time applications such as the metaverse. By generating sound that aligns with a target sound description, audio generation systems would offer creators a greater range of options and streamline the workflow, reducing time and cost.\nRecent advances in audio generation models have demonstrated considerable potential for automating and streamlining the process. However, the models face two significant challenges: a lack of audio quality that meets professional standards and a limited control over shaping desired sound sources."}, {"title": "2 Problem and Task Definition", "content": "In multimedia sound production, sound artists and engineers typically adhere to a structured process to create a final soundtrack. They first generate Foley sounds or collect samples from databases for each sound source. These samples are then edited to meet specific expectations regarding timbre, nuance, and temporal alignment. Finally, they mix all elements into a cohesive sound scene, often with music. TTA systems are designed to automate this process, but they face a number of challenges."}, {"title": "2.1 Problem in Current Text-To-Audio Systems", "content": "First, the quality of the generated audio is usually inadequate to meet commercial standards. Many TTA systems [1-4] generate audio waveforms at a 16 kHz sampling rate for training and inference efficiency, which is significantly lower than the industry standard of 48 kHz or higher. Second, their controllability through the text prompt is limited. Since controllability is crucial to achieve the desired sound characteristics, this limitation is a significant concern. The generated sound is rarely aligned with both the foreground and background sounds, i.e., their compositionality is noticeably limited. This happens particularly when there is a strong positive or negative correlation between the foreground and the background in the training set.\nEvaluation is another significant challenge, particularly because captions are incomplete descriptions of audio signals at varying levels of abstraction [5\u20137]. Fairly evaluating audio generation with a satisfaction score from such varied captioning styles presents considerable difficulties because of the seemingly endless possibilities of factors to consider. When evaluating a generated audio based on a caption People in a small crowd are speaking and a dog barks (from AudioCaps), for example, should the number of people speaking be considered? Does \"and\" imply the sounds to be sequential or simultaneous? How should all these factors be weighted to compute the satisfaction score? Once we answer these questions, how can we aggregate the score of this example with a score of a much simpler prompt? Although it may not be practically possible to answer all these questions, a simplified protocol should be defined to organize a public challenge in a fair manner."}, {"title": "2.2 Task Definition", "content": "In general, sound scene synthesis refers to the task of generating environmental sound scenes that can accompany events in multimedia content to enhance the narrative experience, excluding speech and music. This Sound Scene Synthesis task is built on last year's Foley sound synthesis challenge [8, 9], expanding the scope from Foley sounds to general sound scenes by generalizing the conditioning from a single predefined sound category to a natural language prompt. The audio output requirement is a 4-second, 32-bit, 32kHz, mono-channel audio waveform. Each submitted model is required to generate 250 audio files within a 24-hour period using the computing environment of Colab Pro+."}, {"title": "3 Official Dataset and Baseline System", "content": "Dataset Creation Prompts following the structure described in section 2.2 were crafted manually by the organizing team. We categorized foreground prompts into six categories: \"animal,\" \"vehicle,\" \"human,\" \"alarm,\" \"tool,\" and \"entrance.\" These foreground prompts are paired with five different backgrounds: \"crowd,\" \"traffic,\" \"water,\" \"birds,\" and \"no background,\" except that vehicles are not paired with traffic. The \"no background\" permits evaluation of clean monophonic foreground audios.\nThe level of detail in prompts was adjusted depending on the nature of the sound source. For example, the foreground prompt \"a jackhammer is pounding\" provides a clear and self-sufficient description. Qualifiers such as \"small\" or \"large\" would contribute little to the perception of a jackhammer sound, and the action associated with this source is restricted to \"pounding.\" In contrast, other prompts, such as \"a dog barking,\" benefit from more detailed descriptions, where variations in size (e.g., \"small dog\" vs. \"large dog\") or action (e.g., \"barking\" vs. \"whining\") could yield perceptually different audios. To maintain consistency across the dataset, we empirically balanced the complexity of foreground prompts, acknowledging that certain sounds carry more inherent information and, therefore, do not necessitate additional qualifiers or actions.\nA sound engineer from our team created 4-s audio files corresponding to each prompt based on sounds sourced mainly from Freesound.org but also from private libraries. In total, our dataset comprises 310 audio-captions, with approximately 50 in each foreground sound category and 60 per background category. The development and evaluation set contain respectively 60 and 250 of these audio-caption pairs. Two background categories, \"no background\" and \"birds,\" are excluded from the development set. Consequently, the evaluation set contains more samples with \"no background\" and \"birds\" compared to the development set.\nBaseline System We provided AudioLDM [1] as our baseline model. To ensure high quality and controllability, 9k hours of audio from 4 different sources were used for training. In addition, the model leveraged techniques such as the latent diffusion model and pretrained audio-text embedding [10], which made the training efficient. As the baseline model generates 10-second audio, which is longer than our configuration, we 1) chopped audio into 4-second segments with a hop size of 2 seconds and selected the largest energy segment, and 2) resampled it from 16kHz to 32kHz."}, {"title": "4 Evaluation", "content": "Following the previous challenge edition [8], we conducted a two-stage evaluation scheme including both objective and subjective evaluation.\nStep 1: Objective metrics To measure audio quality objectively, we adopted Fr\u00e9chet Audio Distance (FAD) [11]. We chose FAD as it is a widely used metric in audio generation to measure set-wise audio quality and semantics compared to the reference set. For the embedding used in FAD, we used PANNS CNN14 Wavegram-Logmel [12] (denoted as FAD-P) since it showed the highest correlation scores with perceptual rating [13, 14]. We provided an official evaluation software. \nStep 2: Subjective metrics Subjective evaluation of audio fit (\"how well the audio matches the sound of the prompt\") and perceptual quality (\"clarity, absence of artifacts and distortion\") was performed for the four submitted systems, the provided baseline system, and the Sound-Designer Reference evaluation set. Four prompts from each of the six foreground categories were selected, spanning the five background categories. First, 148 randomly ordered trials were presented online (via the toolkit Gorilla.sc) in six sections separated by foreground category. Category orders were varied across raters. Each audio was given a separate rating for its match to the foreground and background portion of the prompt on a scale from 0 (extremely poor) to 10 (extremely good). Subsequently, the same 148 sounds were presented in random order, without a prompt, and were rated for perceptual quality (0-10) regardless of content. Rating one sound per trial was better suited to this purpose than comparing multiple sounds because each sound was unique [15].\nFourteen raters, four from each top team and ten from system-blinded organizers and their lab members, rated sounds from all systems. To avoid bias, for each contestant and each prompt, each self-rating was replaced with a contestant's average responses to that prompt for all other systems; replacement ensured that simple removal of self-ratings would not uniquely raise or lower a system's average. The Final Rating of each system is a weighted sum of its Foreground Fit, Background Fit, and Audio Quality in a 2:1:1 ratio."}, {"title": "5 Results", "content": "A total of four systems were received for submission [16\u201319]. The (x, y) position represents the FAD score computed on the development set (FAD-P Dev) and the evaluation set (FAD-P Eval), respectively. First, the majority of systems exhibit a tendency to achieve lower FAD-P scores on the evaluation set when they are lower on the development set. This is anticipated, as the training is based, at least in part, on the development set. Second, it turns out that FAD-P Dev is a noisy measure to predict FAD-P Eval. The exception may be attributed to the presence of new sound sources in the evaluation set to prevent overfitting, which may result in performance discrepancies between the two sets.\nThe Spearman's correlation coefficient p of the ranking by FAD-P Eval and the final ranking is '0.900' (p = 0.037), while by FAD-P Dev it is '0.500' (p = 0.391). This discrepancy may be due to systems being overfitted to the development set or to the relatively small size of the development set since FAD is a biased metric [14, 20].\nTo validate the use of PANNs embedding in FAD calculation, we examined the correlation between FAD scores calculated from different embeddings and weighted summed scores, as illustrated in. The PANNs model demonstrated the highest Spearman's correlation coefficient of -0.94, in comparison to CLAP [10] and VGGish [21]. It is noteworthy that only the result of PANNS was statistically significant (i.e., p < 0.05). This result corroborates the previous study's findings [13].\nFAD-P shows a strong relationship with both foreground and background fit but a weak correlation with overall audio quality. This suggests that FAD-P primarily measures the audio-text correspondence, while it may be less sensitive to factors affecting overall quality, such as noise or generated artifacts.\nTo apply our dataset for evaluation, we additionally measured the FAD-P on the evaluation set with generated results from other open-source models [2-4, 22-27] (see Figure 1). Our findings revealed a consistent trend whereby scaling up resulted in enhanced performance, which aligns with the prevalent notion in the field of generative models. The number of model parameters was a more dominant factor than the model types (transformers or diffusion models) in general. However, there is one notable exception: Chung_KT [17] demonstrated promising performance in a lightweight GAN-based architecture. Secondly, it was observed that a model generating audio at a higher sample rate did not always achieve a better score in the evaluation set at 32 kHz. Currently, it is relatively under-optimized to train a model with a higher and more production-ready sampling rate.\nThe mean subjective ratings of Foreground fit, Background fit, and Audio Quality were appropriately low for the baseline system (3.3, 2.8, 3.8), appropriately high for the Reference Set (9.8, 8.8, 9.0), and moderately high for the top-ranked submitted system (5.8, 5.8, 6.0). The Background Fit (not shown) correlates highly (r=0.79) with the Foreground Fit, and Audio Quality correlates highly with both Foreground Fit (r=0.85) and Background Fit (r=0.87). Although system rankings vary across categories, and different rankings do not always reflect a large mean difference, the overall winner (submission 1) has the highest rating in most of the foreground categories. The Entrance Category proved the most challenging for generative systems, with no submissions rated a higher fit than the baseline system, while all submissions fared better than the baseline system in the Animal Category."}, {"title": "6 Conclusion", "content": "The Sound Scene Synthesis challenge has yielded important insights into text-to-audio generation for environmental sounds. Our evaluation protocol, combining FAD-P metrics and human ratings, revealed both progress and areas for improvement in audio quality, diversity, and controllability. The structured prompt format facilitated diverse captions while enabling effective evaluation. While larger models generally excelled, innovative lightweight approaches also showed promise. Performance varied across sound categories, with some showing substantial improvement over the baseline. The strong correlation between FAD-P and human ratings, particularly for sound source fit, validates its use as a reliable objective metric for future research.\nFuture work should focus on enhancing the nuance, temporal aspects, and spatial capabilities of generated sounds. Refining evaluation metrics to capture subtle qualitative differences will be crucial. As this task serves as a valuable benchmark for assessing generative audio models, future iterations could incorporate more sophisticated prompts and criteria. Success in this domain could pave the way for more complex audio generation tasks such as video-to-audio synthesis, potentially revolutionizing AI-driven audio production for multimedia content."}, {"title": "A Challenge Task Overview", "content": null}, {"title": "B Challenge Results", "content": null}, {"title": "C Subjective Evaluation", "content": null}]}