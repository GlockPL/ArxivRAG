{"title": "RoboSignature: Robust Signature and Watermarking on Network Attacks", "authors": ["Aryaman Shaan", "Garvit Banga", "Raghav Mantri"], "abstract": "Generative models have enabled easy creation and generation of images of all kinds given a single prompt. However, this has also raised ethical concerns about what is an actual piece of content created by humans or cameras compared to model-generated content like images or videos.\nWatermarking data generated by modern generative models is a popular method to provide information on the source of the content. The goal is for all generated images to conceal an invisible watermark, allowing for future detection and/or identification. The Stable Signature [1] finetunes the decoder of Latent Diffusion Models (LDM) such that a unique watermark is rooted in any image produced by the decoder.\nIn this paper, we present a novel adversarial fine-tuning attack that disrupts the model's ability to embed the intended watermark, exposing a significant vulnerability in existing watermarking methods.\nTo address this, we further propose a tamper-resistant fine-tuning algorithm inspired by methods developed for large language models [2], tailored to the specific requirements of watermarking in LDMs. Our findings emphasize the importance of anticipating and defending against potential vulnerabilities in generative systems. Code for our proposed method is available at https://github.com/GarvitBanga/RoboSignature.", "sections": [{"title": "I. INTRODUCTION", "content": "Many recent generative models have the ability to watermark their content, and quite a few are open-source. However, individual developers can remove watermarks on generated images by simply removing a line of code in the inference section of the model [1]. This poses a challenge for companies and researchers that create the models and would like to make their models open source while ensuring that the models generate watermarks as part of their generation process.\nIn an ideal scenario, the watermark should be inherently rooted in the model generation process, and the watermarks in the images/texts generated should be tamper-proof against image transformation attacks. Additionally, such open-source models themselves should be tamper-resistant, such that tampering with model weights does not allow generation of data without watermarks or incorrect watermarks.\nFernandez et al. [1] explored various image-related generation tasks on LDMs and applied their novel fine-tuning method that embeds an invisible watermark in images during the LDM decoder's image generation step. These tasks include text-to-image, image-editing, and in-painting. However, they pointed out that the fine-tuned decoder could be subject to network-level attacks. The models could be subject to certain adversarial fine-tuning regime that would make the model 'forget' to root the watermarks or root the incorrect watermark in the generation process while not harming the model's ability to generate images of good quality. The network levels attacks they pointed out are (i) model collusion and (ii) model purification. We introduce novel attacks based on random key generation that confuses the model such that it roots incorrect, random watermarks instead of the intended one.\nWhile contributions in attacking open-source models are important for exposing vulnerabilities to the wider research community to spread awareness of appropriate use cases, efforts are also needed to make models robust to such attacks. Tamirisa et al. [2] observed that safeguards enabled in Large Language Models (LLMs) can be easily bypassed using tampering attacks. These attacks typically involve a few steps of fine-tuning and have been proven effective in circumventing safeguards designed to prevent harmful information disclosure and unlearning. To mitigate such a threat, they proposed a novel fine-tuning algorithm to make LLMs tamper-resistant to adversarial fine-tuning while retaining their ability to answer informative questions and perform day-to-day tasks.\nA significant aspect of our contribution involves adapting the approach proposed by Tamirisa et al. [2] for LLMs to LDMs to enhance the watermark rooting procedure developed by Fernandez et al. [1], making it robust to the adversarial fine-tuning attacks we introduce. Importantly, our approach ensures that the core image generation capabilities of the model remain intact, preserving its utility for practical applications."}, {"title": "II. RELATED WORK", "content": "Diffusion models are a class of generative models capable of producing high-quality images by learning the data distribution through a process of noise addition and removal [3]. During the forward process (encoding), noise is progressively added to the input data over multiple steps, gradually transforming it into a noise-like representation. In the reverse process (decoding), a trained model iteratively removes the noise in small steps, reconstructing data samples that closely resemble the original distribution. This iterative de-noising process allows diffusion models to generate images with remarkable detail and fidelity.\nLatent Diffusion Models (LDMs) extend this approach by operating in a compressed latent space rather than directly on high-dimensional image data [4]. An auto-encoder structure is used to map images into a lower-dimensional latent representation, where the diffusion process takes place. Noise is added to these latent representations during the forward process, and the decoder iteratively removes this noise to reconstruct the data. Once trained, only the latent space and decoder are required for image generation, enabling LDMs to achieve efficient and high-quality results.\nStableDiffusion is a specific LDM-based model [4]. that we use for our research. It is capable of generating images based on textual prompts."}, {"title": "B. Watermarking", "content": "1) HiDDeN: A classical work in deep learning image watermarking literature is Hiding Data with Deep Networks (HiDDeN) [5]. Two sets of parameters are jointly optimized, the encoder network WE and the extractor network W. WE embeds a k-bit message into images robustly, while various adversarial transformation are applied to images during images. WE is trained to extract the original k-bit message. This work was inspired from the fact that minor noise perturbations can be added to images, that can be detected by neural networks, but are impervious to the naked eye.\nFormally We takes as input an image i and a watermark m\u2208 {0, 1}k. It outputs a residual image \u1e9e (essentially a minor noise perturbation) that is the same size as io. The residual image is scaled by a factor of a to produce an image with a watermark iw = \u03af\u03bf + \u03b1\u03b2. Next, a transformation is applied to iw from a set of transformations T. Examples are cropping and JPEG compression. W then extracts a soft message from the transformed image, m' = W(T(iw)). To train W effectively, binary cross-entropy (BCE) loss is used between the k-bit message m and the sigmoid \u03c3(m'). The formula for BCE loss is -\n\n$L_m = - \\sum_{i=1}^{k} m_i log(o(m')) + (1 - m_i) \\cdot log(1 - o(m'))$\n\n2) Stable Signature: Fernandez et al. [1] proposed Stable Signature, an active strategy combining image watermarking and Latent Diffusion Models. First, a HiDDen network is trained on the COCO dataset [6]. The encoder of this network is discarded. Only the extractor is used. Additionally, a particular k-bit string is chosen beforehand. This acts as the watermark key.\nNext, a pre-trained LDM model is chosen and the decoder of the model is fine-tuned. Specificaly, the encoder and the latent space layer weights are frozen during the backpropogation step. The decoder of a stable diffusion model is now fine-tuned to generate images with the watermarks encoded. Images generated by the LDM decoder are then fed into the HiDDen watermark extractor and an encoded message/watermark is extracted from them. This extracted watermark must match the watermark that was chosen before. The decoder essentially needs to be fine-tuned to embed the original watermark into any image it generates. This is ensured by the loss function. This process will be fine-tuned with 2 key components in the loss function given by\n\n$L = L_m + L_i$\n\nwhere Lm is the loss generated from the difference between the original watermark signature and the extracted watermark, and Li is the loss component derived from the difference between the generated image from this fine-tuned decoder Dm and the original decoder with no fine-tuning D. This is to ensure that the image generation with the message is not radically different from images originally generated images without watermarks.\nIt is also crucial that the model is able to understand luminance and contrast-masking to produce less perceivable watermarks. To do this, the decoder of the original Stable Diffusion model is used to produce an image without a watermark. The image perceptual loss between this image and the image produced with watermark controls the distortion between the images. This image perceptual loss is added along with the loss generated from the difference between the original watermark signature and the extracted watermark to train the weights of the decoder Dm.\nAny image produced by the LDM decoder would have that specific k-bit message embedded in it. This k-bit message could possibly be unique to that LDM decoder, as such an image produced could be traced back to the user in possession of that particular LDM decoder."}, {"title": "C. Network Level Attacks", "content": "Fernandez et al. [1] tested for various image level attacks that tried to remove the watermark from generated images will trying to keep image quality intact. They also pointed out an important class of attacks network level attacks. Where the LDM decoder's weights could be modified in a manner such that the model either forgets to root watermarks in generated image, or roots the incorrect watermark. The attacker's objective, in addition to corrupting the watermarking capability of the model, also includes preserving the quality of the generated images. There are primary two network level attacks discussed by Fernandez et al. [1] Such attacks are primarily adversarial fine-tuning based methods.\nThe model collusion attack is caused when multiple users that posses different LDM decoders collude. They can average the weights of their models to create a new model that generates images of a high quality but produces a new unique watermark key. This would help them escape the identification process and images produced would not trace back to them.\nThe model purification attack aimed at making the LDM decoder forget to add the watermark in its generation process. Essentially the model is fine-tined again but without the loss pertaining to the message-key. The idea is for the model to leave out the key generation. Fernandez et al. [1] experimentally showed that such an attack starts reducing the bit accuracy of the watermark extracted after a significant number of training steps. Also while the the bit accuracy reduces the peak signal to noise ratio (PSNR) value also reduces. Hence, image quality does not remain intact."}, {"title": "D. Tamper Resistant Fine-Tuning in LLMS (TAR)", "content": "Tamirisa et al. [2] made the observation that a few steps of adversarial fine-tuning could remove safety guardrails set in LLMs. They mainly focused on guardrails around Weaponization knowledge restriction and harmful request refusal. Weaponization knowledge restriction refers to a guardrail that ensures that the language model does not relay information regarding the use and manufacture of weapons while answering prompts that only request information for benign domains. Similarly, the guardrail of harmful request refusal disallows the model to produce harmful outputs. Tamirisa et al. [2] developed a novel fine-tuning algorithm called tamper resistant fine-tuning (TAR) that ensures any future adversarial fine-tuning does not succeed in removing guardrails already set in the model. They define the objective of TAR to keep the safety metric and the capability metric of the model high post fine-tuning. The capability metric of the model is defined as the performance of the model in generation, for example producing outputs pertaining to benign knowledge domains. The safety metric refers to the model's ability to adhere to the given guardrail. Formally, let the vanilla model's weights weights be 0, and let the guardrails be G. After guardrails have been set in the model, its weights are then transformed to \u03b8\u03b1. Post this TAR fine-tuning is performed on the model to produce \u03b8'. The aim is that after attacks from a set Atest are applied to \u03b8', capabilities_metric(0) and safety_metric(0) remain high."}, {"title": "III. METHODS", "content": "We propose a novel adversarial fine-tuning attack on StableDiffusion's decoder which has been previously fine-tuned by the Stable Signature method to root watermarks.\nThe Random Key Attack is a fine-tuning regime where in each training step, for Lm, the loss pertaining to message bit accuracy, the ground truth k-bit message is chosen randomly from the key space. In our work, and in Ferenandez et al.'s [1] approach, the watermark key is a bit string of size 48, leading to a key space size is 248. This fine-tuning regime is intended to confuse the model, causing it to generate images embedded with random keys rather than the intended watermark key.\nThe Gradual Random Key Attack is a variant of the Random Key Attack. Instead of producing a completely random key at every training step, the key is gradually modified, starting with a single random bit change and progressing towards a fully random key. The intent remains the same: the model should generate images with an incorrect embedded watermark key as the attack progresses."}, {"title": "B. Modified TAR Fine-Tuning", "content": "To make the model robust against adversarial fine-tuning, we adapt the Tampering Attack Resistance (TAR) proposed by Tamirisa et al. [2] to fit our framework.\nWe split the training data into two datasets: the capabilities_metric proxy dataset Dretain and the safety_metric proxy dataset DTR. Theses datasets are used to train two separate sets of gradients, GTR and gretain. During each outer training step, both sets of weights are updated.\nThe GTR gradients are computed by attacking the current iteration of the model, (0i\u22121), using K Random Key Attacks. Each attack consists of approximately 100 training steps. The attacked model obtained through these steps is denoted as attack(0-1). For each attacked model, we calculate the loss between the model's output and the original target key (Lm). Additionally, we compute the image reconstruction loss (Li), which measures the difference between the images generated by the attacked model and those generated by the original model after Stable Signature fine-tuning. These losses are combined to form the total loss LTR (as described in Algorithm 1). The GTR gradients are then computed as the average of the gradients from each LTR loss.\nSimilarly, the gretain gradients are computed using the original loss function, LORG. This loss ensures the output key matches the original target key (without any random attacks). The image reconstruction loss is also included as a component of LORG. Additionally, the L2 norm of the difference between the hidden states of the current iteration of the model and the original model is incorporated into the gretain gradients."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We conducted adversarial fine-tuning experiments on an LDM decoder that was previously fine-tuned to embed a specific watermark in the images it generates. These attacks aim to generate\n1) Random keys at each step (Table I)\n2) Gradually random or different keys across steps (Table II)\nThe fine-tuning process for the LDM decoder involved 100 steps with a batch size of 4, utilizing 400 images from the COCO dataset, similar to the setup described by Fernandez et al. [1]."}, {"title": "The low bit accuracies but high PSNR values highlight the dangers of such adversarial attacks - despite maintaining high image quality, as indicated by PSNR values, the bit accuracies dropped significantly which implies that a user would be unable to reliably trace the generated images back to the correct model of origin. This confirmed our hypothesis of the original model being susceptible to adversarial fine-tuning/network-level attacks. Notably, a 50% bit accuracy corresponds to completely random keys being generated. Thus, bit accuracies close to 50% indicate that the model is effectively incapable of rooting the intended target watermark key.", "content": ""}, {"title": "B. Results of Modified TAR fine-tuning", "content": "We conducted our modified TAR fine-tuning regime on LDM decoders that were already fine-tuned using the Stable Signature method for embedding watermarks. We used different configurations of outer training steps (N), different number of attacks to calculate GTR (K) and different number of train steps during each attack (attack steps) as shown in table III.\nAfter our modified TAR fine-tuning, high PSNR values (ranging from 35\u201351) were observed on the evaluation dataset, indicating that high-quality images were consistently generated. The bit accuracy relative to the target key was also notably high (96-99%) on the evaluation dataset across various training configuration\nWe further subjected the fine-tuned models to Random Key Attacks. Post-attack, the bit accuracy relative to the target key dropped to approximately 64-65%, while PSNR values remained in the range of 29-32.These values are depicted using the square markers in Figure 1. For comparison, Figure 1 also shows the bit accuracy and PSNR values for the original Stable Signature LDM decoder post-attack, shown using triangle markers."}, {"title": "V. CONCLUSION", "content": "Our analysis highlights that achieving tamper resistance for watermarking in Latent Diffusion Models (LDMs) is a tractable yet highly intricate challenge. While our proposed tamper-resistant fine-tuning method demonstrates minor improvements in bit accuracy against various tampering attacks, these enhancements fall short of addressing all vulnerabilities, especially in scenarios involving complex image transformations.\nA significant observation from our study is the absence of any underlying distribution among random bit strings. The possible key space for bit-strings is exceptionally large (248), with only one target key correct out of these possibilities. This makes the task of tamper resistance considerably challenging when compared to tasks like mitigating harmful content in the context of LLMs, where some inherent redundancy or distribution aids in the defense.\nFurther, unlike the robust safeguards established in the original TAR framework for LLMs, LDMs lack an equivalent baseline defense mechanism, leaving them inherently more susceptible to adversarial manipulations. Developing an effective initial safeguard for LDMs remains an open problem, underscoring the non-trivial nature of ensuring security and resilience in this domain."}]}