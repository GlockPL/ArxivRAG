{"title": "SIMULATION-BASED OPTIMISTIC POLICY ITERATION FOR MULTI-AGENT MDPS WITH KULLBACK-LEIBLER CONTROL COST", "authors": ["Khaled Nakhleh", "Ceyhun Eksin", "Sabit Ekin"], "abstract": "This paper proposes an agent-based optimistic policy iteration (OPI) scheme for learning stationary\noptimal stochastic policies in multi-agent Markov Decision Processes (MDPs), in which agents incur\na Kullback-Leibler (KL) divergence cost for their control efforts and an additional cost for the joint\nstate. The proposed scheme consists of a greedy policy improvement step followed by an m-step\ntemporal difference (TD) policy evaluation step. We use the separable structure of the instantaneous\ncost to show that the policy improvement step follows a Boltzmann distribution that depends on\nthe current value function estimate and the uncontrolled transition probabilities. This allows agents\nto compute the improved joint policy independently. We show that both the synchronous (entire\nstate space evaluation) and asynchronous (a uniformly sampled set of substates) versions of the OPI\nscheme with finite policy evaluation rollout converge to the optimal value function and an optimal\njoint policy asymptotically. Simulation results on a multi-agent MDP with KL control cost variant of\nthe Stag-Hare game validates our scheme's performance in terms of minimizing the cost return.", "sections": [{"title": "1 Introduction", "content": "Consider the two-agent MDP version [1] of the Stag-Hare game [2] where two hunters move on a gridworld to hunt\nhares or a stag (see Figure (1) for an illustration). Each hunter can only determine their own next position (local state)\nin this world by moving to a neighboring grid, but their costs are determined based on their joint position (state), and\nwhether they hunt a stag or a hare. The hunters can individually hunt a hare, but need to coordinate together to be able\nto hunt a stag. Considering the returns are the same amongst the hunters, this setting is an example of a multi-agent\nMarkov decision process (MDP), in which each agent has control of its local state but there is a common value function\n(identical interest) that depends on the joint state.\nWe assume the instantaneous costs are decomposed into two parts: an identical cost term that only depends on the\njoint state, and another term that captures the cost of control. The cost of control is measured by the KL divergence\nbetween the uncontrolled transition probabilities and transition probabilities collectively chosen by the agents. The KL\ncost represents the control cost the agents are willing to pay in order to modify the uncontrolled transition probability"}, {"title": "2 Related Work and Contributions", "content": ""}, {"title": "2.1 Simulation-Based Optimistic Policy Iteration", "content": "Optimistic, also known as simulation-based modified, policy iteration methods are preferred in settings with large state\nspaces due to their fast convergence [7]. However, the theoretical underpinnings of this practical success remain unclear.\nThe pioneering theoretical guarantee for OPIs with Monte-Carlo estimation established convergence assuming infinitely"}, {"title": "2.2 Decentralized Learning in Multi-Agent MDPs and Markov Games", "content": "The KLC-OPI scheme considers multiple agents that perform the policy improvement step independently of other agents\nusing an agent-local estimate of the value function (Lemma 4.1). Thus, the proposed scheme falls under the framework\nof multi-agent reinforcement learning (MARL) [14], which has seen growing interest in the context of learning in\nidentical interest or potential games [15, 16, 17], zero-sum Markov games [18, 19, 20], and multi-agent systems with\nKL control cost [21]. The considered multi-agent MDP framework is also equivalent to identical-interest Markov games\nor Markov cooperative games considered in [15, 22], where the costs/rewards are identical for all the agents.\nWork on Markov games consider either policy gradients, in which agents consider parameterized policies updated\nusing gradients computed through episodic returns [15, 23], or a combination of standard learning protocols, e.g.,\nbest-response, fictitious, with a standard reinforcement learning algorithm, e.g., Q-learning [19]. Policy gradient\nmethods are applicable to continuous state and action spaces' environments, but they suffer from large variance and the\nconvergence rate being sensitive to the choice of parameters.\nRecently, localized policy iteration methods for networked multi-agent systems [24] and zero-sum games [25] are\nshown to converge near globally optimal policy. These schemes are based on state space partitioning with policy\ndependent mapping that gives a uniform sup-norm contraction property which enables convergence of the algorithms to\nthe optimal value functions.\nThe proposed KLC-OPI scheme is a novel agent-based learning scheme that is shown to carry over the convergence\nproperties of OPI designed for single-agent MDPs to the multi-agent MDP with KL control cost setting. The KL cost\nstructure allows for continuous action spaces through the close-form solution to the policy improvement step."}, {"title": "3 Multi-Agent MDPs With KL Control Cost", "content": "We consider an infinite-horizon discounted n-agent MDP given as the tuple \u0393 := {N, S, {A_i}_{i=1}^N, P, p, C, \u03b3} with a finite\nnumber of players N := {1, ..., n}, finite joint state space S, and a continuous action space A = A_{i=1} \u00d7 A_{i=2} \u00d7...\u00d7 A_{i=n}.\nThe transition probability function P : S \u00d7 A \u2192 \u2206(S) determines the transition probability to the next joint state\n$s_{t+1} \\in S$ given the joint state $s_t \\in S$ and joint action profile a, at time step t \u2208 N\u207a. The initially sampled joint state is\nchosen from a prior p, for which we assume p(s) > 0 for all joint states s \u2208 S.\nThe intrinsic joint state cost function is defined as C : S \u2192 R.\nAs done in [3] for single-agent linearly solvable MDPs, in multi-agent MDPs with KL control cost, the one-step cost\nfunction is composed of two terms: the intrinsic joint state cost function C, and the control cost, measured using the\nKullback-Leibler (KL) divergence between the controlled and uncontrolled transition probability function. Finally,\n\u03b3\u2208 [0, 1) is the discount factor."}, {"title": "3.1 Stochastic Joint and Marginal Policies", "content": "In the KL control setting, agents pick a joint policy by re-weighting P with a continuous-valued action profile a \u2208 A\nthat directly specifies the transition probability from s to s' \u2208 S. Hence, the agents avoid the combinatorial search over\nthe state-action space for an action profile that is then applied to the transition probability function P.\nAssumption 3.1. The joint state s_t \u2208 S at time step t is composed of n sub-states where the sub-state s_{it} \u2208 Si can only\nbe controlled by agent i at time step t. The joint state is then st = [s_{1,t}, s_{2,t}, ..., s_{n,t}].\nGiven Assumption 3.1, the joint state space can be written as S = S_1 \u00d7 S_2 \u00d7 ... \u00d7 S_n. Moreover, and similar to\nproduct games [26], the multi-agent MDP with KL control cost transition structure is derived by taking the product of\nn Markov transition structures. Agent i \u2208 N only controls their sub-state transitions through a probability transition"}, {"title": "3.2 Decomposition of the One-Step Cost Function", "content": "Similar to single-agent linearly solvable MDPs [3, 4], the control cost is captured by the KL divergence between \u03c0\u03c1\u2080\nand the uncontrolled transition probability function P_0, i.e.,\n$D_{KL}(\\pi_{\\rho,\\cdot|s}||P_0(\\cdot|s)) := \\sum_{s'\\in S} \\pi_{\\rho}(s'|s) \\ln \\frac{\\pi_{\\rho}(s'|s)}{P_0(s'|s)} = \\mathbb{E}_{s' \\sim \\pi_{\\rho}(\\cdot|s)}[ \\ln \\frac{\\pi_{\\rho}(s'|s)}{P_0(s'|s)}] \\equiv \\mathbb{E}_{s' \\sim \\pi_{\\rho}(\\cdot|s)}[-\\ln P_0(s'|s)]$.\n(3)\nThe one-step cost function is then written as $q(s, \\pi_{\\rho_i}) = C(s) + D_{KL}(\\pi_{\\rho}(\\cdot|s)||P_0(\\cdot|s))$. We then define the value function\nunder \u03c0\u03c1\u2080 to be state-wise\n$V^{\\pi_{\\rho_i}}(s) := \\mathbb{E}_{s'\\sim \\pi_{\\rho_i}(\\cdot|s)}[\\sum_{t=0}^{\\infty} \\gamma^t C(s_t) + D_{KL}(\\pi_{\\rho}(s_t|s)||P_0(s_t|s_0))]$.\n(4)\nWith qmax being the maximum one-step cost, the value function is bounded by $q_{max}/(1 \u2013 \\gamma)$. Note that if the joint policy\n\u03c0\u03c1\u2080 is equal to P_0, then the KL control cost is zero and the next joint state s' is sampled according to $s' \\sim P_0(\\cdot|s)$, with\n$s' = [s'_1, s'_2, ..., s'_n]$. Given Assumption 3.3, prohibitive transitions to another joint state in a single transition and the\nDKL control cost boundedness conditions are met."}, {"title": "3.3 The KL Evaluation and KL Optimal Bellman Operators", "content": "We define the KL evaluation Bellman operator $T^{\\pi_{\\rho_i}}$ for \u03c0\u03c1\u2080 applied state-wise to the value function as\n$(T^{\\pi_{\\rho_i}}V)(s) = C(s) + D_{KL}(\\pi_{\\rho}(\\cdot|s)||P_0(\\cdot|s)) + \\gamma \\sum_{s'\\in S} \\pi_{\\rho_i}(s'|s)V(s').$\n(5)\nThe KL optimal Bellman operator, denoted as T, is the Bellman operator given an optimal joint policy, i.e., TV :=\n$min_{\\pi_{\\rho} \\in \\Pi} T^{\\pi_{\\rho}} V$. Then, the optimal value function V* satisfies $V^* = TV^*$. Moreover, we denote the obtained optimal joint\npolicy as $\\pi_{\\rho}^*$. Both the KL evaluation and the KL optimal Bellman operators have the three properties: monotonicity,\ndistributivity, and \u03b3-contraction. For any two arbitrary value functions V_1, V_2 \u2208 R^{|S|} where V_1 \u2264 V_2, $T^{\\pi_{\\rho_i}}$ exhibits\nmonotonicity such that $T^{\\pi_{\\rho_i}} V_1 \u2264 T^{\\pi_{\\rho_i}} V_2$. In addition, for any constant c_1 \u2208 R, and an all-ones vector \u0113, the operator\n$T^{\\pi_{\\rho_i}}$ has the distributivity property such that $T^{\\pi_{\\rho_i}} (V + c_1 \\cdot \u0113) = T^{\\pi_{\\rho_i}} V + \\gamma \\cdot c_1\u0113$. A single application of the Bellman\noperator gives \u03b3-contractions in the L\u221e-Norm with $||T^{\\pi_{\\rho_i}} V_1 - T^{\\pi_{\\rho_i}} V_2||_{\\infty}\u2264 \\gamma||V_1 - V_2||_{\\infty}$."}, {"title": "4 Multi-Agent Simulation-Based Kullback-Leibler Controlled Optimistic Policy Iteration\nKLC-OPI", "content": ""}, {"title": "4.1 The Scheme Description", "content": "The scheme is run by each agent i \u2208 N on iterations k = 0, 1, 2, ..., K independently of other agents. At each iteration\nk and for agent i, the scheme stores the value function estimate Vi,k used in evaluating the joint policy. Agent i performs\na greedy policy improvement step to obtain its independently calculated joint policy $\u03c0_{\u03c1_{o,k+1}}^{(i)}$ as\n$G(V_{i,k}) := arg \\min_{\\pi_{\\rho_{o,k+1}} \\in \\Pi} \u03c0_{\u03c1_{o,k+1}}^{(i)} V_{i,k} \u2286 S \u2192 \\Delta(S)$.\n(6)\nGiven that the joint state can be decomposed into an agent i \u2208 N sub-state si as stated in Assumption 3.1, agent i only\ncontrols their transitions to a sub-state s'i \u2208 Si using the marginal policy\n$\\pi_{\\rho_{o,k+1}}^{(i)}(s'|s) := \\sum_{j \\in N\\{i\\}} \\sum_{s'\\in S_j}\u03c0_{\u03c1_{o,k+1}}^{(i)}(s, s') $.\n(7)\nThe agents collectively evaluate their value function estimates by generating |S| synchronous and coupled m-step TD\nsampled trajectories using their marginal policies (7) with each trajectory starting at a joint state s \u2208 S. The term\nsynchronous denotes evaluating the value function estimates for all joint states in the state space, while the term coupled\nmeans that the joint state transitions are done simultaneously by all agents. The simulation-based policy evaluation step\nresults in the per joint state cost function q_1(s_t, \u03c0_{\u03c1_{o,k+1}}) for a timestep t using the m-step TD trajectories. Each agent\nuses the discounted sum of the returns to obtain an estimate of $(T^{\\pi_{\\rho_{o,k+1}}})^mV_{i,k}$ which we represent using the noise term\n\u03f5_{m,k} \u2208 R^{|S|} as follows,\n$(T^{\\pi_{\\rho_{o,k+1}}})^mV_{i,k} + \u03f5_{m,k} = \\sum_{t=0}^{m-1} \\gamma^t q_1(s_t, \\pi_{\u03c1_{o,k+1}}) + \\gamma^mV_{i,k}(s_{t=m})$.\n(8)\nAgent i's value function estimate is updated using the noisy returns and results in the estimate V_{i,k+1} at the end of\niteration k. We summarize the simulation-based KLC-OPI scheme run by each agent i \u2208 N in the following:\n$\u03c0_{\u03c1_{o,k+1}}^{(i)}= G(V_{i,k}),\nV_{i,k+1} = (I-A_k) V_{i,k} + A_k((T^{\\pi_{\\rho_{o,k+1}}})^mV_{i,k} + \u03f5_{m,k}),$\n(9)\nwhere I is the |S|\u00d7 |S| identity matrix, A_k is a |S| \u00d7 |S| diagonal matrix with joint state learning rates \u03b1_k(s) \u2208 R\u207a as\nits elements, and the rollout value is m \u2208 N\u207a. Note that m = 1 corresponds to value iteration with KL control cost\nKLC-VI, and letting m \u2192 \u221e gives a policy iteration with KL control cost KLC-PI rendition of the scheme. In addition,\nthe learning rates' matrix A_k is the same for all agents.\nSimilar to [Appendix 1.2 in [4]] for the single-agent linearly solvable MDP case, we show that the optimal joint policy\n(6) follows a Boltzmann distribution by applying the Cole-Hopf transformation to the Bellman equation with $(T^{\\pi_{\\rho_{o,k+1}}})^*$.\nLemma 4.1. At iteration k and for any i \u2208 N, the joint policy $\u03c0_{\u03c1_{o,k+1}}^{(i)}$ given P_0 that minimizes the discounted return of\nthe policy evaluation step in (9) follows a Boltzmann distribution\n$\\pi_{\\rho_{o,k+1}}(s'|s) = \\frac{P_0(s'|s)(Z_{i,k}(s'))^\\gamma}{\\sum_{s'\\in S} P_0(s'|s)(Z_{i,k}(s'))^\\gamma}$,\n(10)\nwhere Zi,k for agent i is the Cole-Hopf transformation of the value function such that $Z_{i,k}(s) = e^{-V_{i,k}(s)}$ state-wise in\niteration k."}, {"title": "4.2 Asymptotic Convergence", "content": "Our main result is that each agent's value function estimates Vi,k and individually calculated joint policy $\u03c0_{\u03c1_{o,k}} ^{(i)}$ converge\nto the optimal value function and an optimal joint policy respectively under the KLC-OPI iterations.\nTheorem 4.4. Assume the initial value function estimate is such that TV_{i,0} \u2013 V_{i,0} \u2264 0 for each agent i \u2208 N and\n\u03b1_k(s) = O(\\frac{1}{k}) state-wise. The value function estimate V_{i,k} and joint policy $\u03c0_{\u03c1_{o,k}} ^{(i)}$ iterates of i \u2208 N for the KLC-OPI\nscheme in (9) asymptotically converge to V* and to \u03c0\u03c1, respectively.\nThe convergence to optimal value function builds on two main assumptions. First one is that the initialized value\nfunction estimates i \u2208 N satisfy TV_{i,k=0} - V_{i,k=0} \u2264 0. The assumption is not restrictive since the initial value function\nestimate can be set to a large value state-wise such that a single application of the KL optimal Bellman operator\nwill guarantee a lower value function estimate. Our second assumption on the step size is standard in stochastic\napproximation and ensures that the steps are square summable but not summable [8, 13].\nSimilar to optimistic policy iteration with a deterministic policy [Proposition 1 in [27]], applying the operator $(T^{\\pi_{\\rho}}) ^{(i)}_{k+1}$\nm > 1 times in a single iteration of KLC-OPI does not guarantee a contraction in any norm nor monotonic improvement\nof the value function estimate, unlike exact policy iteration schemes where the monotonicity property is preserved\n[28, 29]. Hence, the KLC-OPI simulation-based policy evaluation step may result in a worse performing value function\nestimate, i.e., a larger discounted cost return, compared to the previous iteration's estimate.\nGiven the non-contracting, and in general without additional assumptions, non-monotonic improvement challenges\nof optimistic policy iteration schemes' updates, the updated value functions' performance has to be bounded using\nan analysis technique that does not utilize the contraction property. To prove asymptotic convergence to the optimal"}, {"title": "5 The Asynchronous KLC-OPI Scheme", "content": "For simulation-based policy iteration schemes, it can be computationally expensive to run a synchronous policy\nevaluation step with |S| m-step TD sampled trajectories especially if the joint state space is large. For this reason, we\nfocus on the asynchronous policy evaluation step of value function estimates using 1 \u2264 D \u2264 |S| joint states in each\niteration k. In a single iteration, a larger D value means that a larger subspace of the joint state space is evaluated at the\nexpense of additional sampling per iteration.\nIn order to prove the convergence of the asynchronous version of the scheme, we assume that the initial state distribution\np is a uniform distribution. Let Dk \u2286 S be the set of joint states that are evaluated at iteration k under the current joint\npolicy $\u03c0_{\u03c1_{o,k+1}}^{(i)}$ for any i \u2208 S. The set Dk of each iteration satisfies the following assumption."}, {"title": "6 Simulations", "content": ""}, {"title": "6.1 A Multi-Agent MDP: Stag-Hare", "content": "We consider a two-agent MDP with KL control cost variant [1] of the Stag-Hare game [2]. There are |N| = 2 hunters\nwho hunt on a 5 \u00d7 5 gridworld and can only move to adjacent grids in a single timestep.\nHunter i's sub-state is their grid location s_i = 0, 1, . . ., 24 such that there are |Si| = 25 possible grid locations. For the\ntwo hunters, this gives a total of |S| = |S1|\u00d7 |S2| = 625 joint states. The uncontrolled transition probability $P_{i,o}$ forces\nagent i \u2208 N to remain in their current sub-state w.p. 0.9, and transition to one of the b adjacent sub-states w.p. 0.1/b.\nThe gridworld has four hares and one stag at sub-state locations $s_h \u2208 {0,4, 20, 24}$ and $s_s \u2208 {12}$, respectively. When\none of the hunters reaches a hare's location, both hunters obtain a negative intrinsic joint state cost of -2 added to the\nKL control cost per timestep. If both hunters cooperate and move together to the stag's location, they would obtain a\nlower intrinsic joint state cost of -10 added to the KL control cost per timestep. The intrinsic joint state cost incurred\nfor each agent i \u2208 N in a single transition is\n$C(s) = -2\\cdot \\sum_{i=1}^{n} I\\{s_i \\in S_h\\} - 10 \\cdot I\\{\\{ s_i,s_j \\in S_s \\} > 1\\}$,\n(34)\nwhere I{-} being the indicator function."}, {"title": "6.2 ASYNC-KLC-OPI Simulation Results", "content": "We set the discount factor to be \u03b3 = 0.95, and the fixed episode time horizon m to be the average of a geometrically\ndistributed random variable with distribution geom(1\u2212y) such that m = 20. We test the scheme using D = [20, 40, 60, 80]\njoint states for K = 3000 iterations and show averaged results over 10 simulation runs in Figure (2).\nIn Figure 2(a), ASYNC-KLC-OPI converges faster to a minimum cost return as the number of sampled joint states D in a\nsingle iteration K increases. With a larger D value, the scheme evaluates a larger subset of the joint state space in one\niteration, and as a result it obtains a better joint policy for the next iteration compared with a smaller D value. However,\nthe average runtime per iteration increases as D increases and is 6.66, 14.57, 16.18, and 20.75 seconds for the selected\nD values, respectively.\nFigure 2(b) compares the obtained stochastic policy for D = 80 joint states against the optimal deterministic joint\npolicy. The optimal deterministic joint policy executes grid transitions that direct the agents to the stag's sub-state in\nthe lowest number of time steps. It can be seen from the figure that the stochastic joint policy gives a slightly better\nperformance for the joint states [20, 4], [5, 12] and [18, 14] in terms of the undiscounted cost return. For the joint state\n[11, 13] where each hunter starts in a cell neighboring the stag, the two policies achieve a similar performance. The cost\nreturn difference is due to the increased KL cost when using the optimal deterministic joint policy. Agents using the\noptimal deterministic joint policy transition to a selected joint state w.p. 1 which results in a higher KL cost compared\nwith a stochastic joint policy.\nFor Figure 2(c), we plot the L\u221e-Norm for the difference between iteration k value function estimates and the value\nfunction from iteration K = 3000. As shown, the convergence rate to the iteration K = 3000 value function estimate is\nfaster with a larger D value."}, {"title": "7 Conclusion", "content": "We presented a synchronous and an asynchronous simulation-based optimistic policy iteration schemes for multi-agent\nMDPs with KL control costs that are run independently by each agent. The separation between control costs and joint\nstate costs rendered the optimal joint policy to have a close-form solution in the form of a Boltzmann distribution that\ndepends on the current value function estimate and uncontrolled transition probabilities. Given standard assumptions on\nthe learning rates and the initial value function estimate, we showed the asymptotic convergence of both schemes to the\noptimal value function and an optimal joint policy. The convergence result applies to any simulation-based OPI scheme\nwith finite and noisy rollout returns on any MDP. For different number of sampled joint states in an iteration, simulation\nresults on a multi-agent MDP variant of the Stag-Hare game showed that the asynchronous scheme converges to a\nminimum cost return for the agents, with better performance than the optimal deterministic joint policy."}]}