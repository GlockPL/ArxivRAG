{"title": "SPIDER 2.0: EVALUATING LANGUAGE MODELS ON REAL-WORLD ENTERPRISE TEXT-TO-SQL WORKFLOWS", "authors": ["Fangyu Lei", "Jixuan Chen", "Yuxiao Ye", "Ruisheng Cao", "Dongchan Shin", "Hongjin Su", "Zhaoqing Suo", "Hongcheng Gao", "Wenjing Hu", "Pengcheng Yin", "Victor Zhong", "Caiming Xiong", "Ruoxi Sun", "Qian Liu", "Sida I. Wang", "Tao Yu"], "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising 632 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 17.0% of the tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation - especially in prior text-to-SQL benchmarks - they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Our code, baseline models, and data are available at https://spider2-sql.github.io.", "sections": [{"title": "INTRODUCTION", "content": "Automated code generation can serve as a crucial bridge between humans and data, assisting individuals in achieving difficult or monotonous tasks using complex data. A significant portion of existing data is stored in relational databases, where SQL serves as an essential interface that facilitates human interaction with these data. In this context, semantic parsing or text-to-SQL (Dahl et al., 1994; Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Li & Jagadish, 2014; Zhong et al., 2017; Yu et al., 2018) is an important technology that assists data analysts in performing routine queries, orchestrating data workflows, and accomplishing advanced business intelligence, thereby significantly reducing repetitive human labor and alleviating the burden on programmers. Large language models (LLMs) have demonstrated excellent capabilities in generating code (Chen et al., 2021; Austin et al., 2021), particularly in transforming natural language questions into SQL queries. Notably, methods based on GPT-4 achieved execution accuracy of 91.2% and 73.0% on the classic benchmarks Spider 1.0 (Yu et al., 2018) and BIRD (Li et al., 2024b), respectively.\nAlthough LLMs excel on these datasets, they often use non-industrial databases with few tables and columns, featuring simplistic SQL and questions that fall short of real-world complexity and overlook diverse SQL dialects. By contrast, real-world data are stored across a diverse array of"}, {"title": "2 BENCHMARK CONSTRUCTION", "content": "In this section, we introduce the task definition, general annotation pipeline, and dataset statistics for Spider 2.0, Spider 2.0-snow and Spider 2.0-lite. For concrete examples, refer to App.B."}, {"title": "2.1 TASK DEFINITION", "content": "Fig. 2 illustrates the task definition of both code agent setting and traditional text-to-SQL setting.\nCode agent task. Spider 2.0 is defined as a comprehensive code agent task. Given a question Q, a database interface I, and a codebase C (with project context, configuration, and documentation, illustrated in Fig. 1), the task is to iteratively modify the code (SQL/Python) C based on observations $O_k = execute(C, I, Q)$ until the final result A (text/table/database) is obtained. In other words, we use the final observation $O_k$ as an agent's answer to the question, i.e., $A = O_k$.\nText-to-SQL task. In contrast to Spider 2.0, Spider 2.0-snow and Spider 2.0-lite are formulated as self-contained tasks. Given database schema D, a natural language question Q, and auxiliary documentation $\\E$ as inputs, the text-to-SQL parser f(.) is required to output the corresponding SQL query $S = f(Q,D,\\E | \\theta)$, where $\\theta$ is the parameters of the parser. Spider 2.0-lite's database is hosted on diverse databases like Spider 2.0, while Spider 2.0-snow is entirely hosted on Snowflake, with a greater focus on text-to-SQL generation."}, {"title": "2.2 \u0391\u039d\u039dOTATION PIPELINE", "content": "Eight authors majoring in computer science, all highly proficient in SQL, carry out the data annotation process. The annotation pipelines consist of the following six steps:\n1) Database and SQL collection. We collect various databases from cloud data warehouses, including BigQuery public data, Snowflake Marketplace data, and other platforms, to ensure that they meet specific criteria: each database must contain more than 200 columns or have a nested schema structure. After filtering, we select 74 BigQuery, 54 Snowflake, 30 SQLite, 40 DuckDB, 10 PostgreSQL, and 5 ClickHouse databases. From the corresponding tutorials and forums, we gather 1,021 complex SQL queries, as well as 157 data transformation projects sourced from Fivetran and DBT (see App.B.2). To meet our criteria, the SQL queries must contain more than 50 tokens (tokenized by whitespace; for reference, the average token count of BIRD (Li et al., 2024b) is 30.9). Furthermore, queries must originate from real projects or tutorials, not from synthetic examples or corner cases. Ultimately, we retain 547 high-quality SQL queries and 78 DBT projects."}, {"title": "2) SQL rewrite to prevent data leakage", "content": "To avoid contamination and ensure the credibility of Spider 2.0's evaluation, annotators are required to rewrite each SQL and verify that they are bug-free. The rewrites are performed at two levels of increasing complexity: the surface and semantic levels, as detailed in Tab. 1. 84.2% of the examples underwent surface-level rewriting, while 42% experienced semantic-level rewriting. Annotators must ensure that the rewritten SQL executes successfully, completes in an acceptable time, and returns non-empty results. 85.98% of these SQL queries utilize advanced functions in various dialects (App.B.7.1), while 10.76% require additional DBT tools, posing challenges due to the need to integrate the project context."}, {"title": "3) Codebase and context setup", "content": "For each complex SQL query in Spider 2.0-lite and Spider 2.0-snow, we collect the external reference documents necessary to complete the task. Since the tasks span multiple database types, we gather documentation on SQL dialects and external functions, as shown in Tab. 18. Additionally, for Spider 2.0, we preserve the original codebase of the SQL-related project. For Spider 2.0, besides collecting reference documents, annotators also gather resources such as codebases, database interfaces to establish the context for each task (Fig. 1). Since some complex data transformation intentions may not be fully captured by a natural language question, annotators provide additional context, including data model descriptions (App.B.2) or predefined answer files (App.B.5), to maintain clarity while addressing potential ambiguities."}, {"title": "4) Natural language task instructions annotation", "content": "Annotators are required to write questions based on the SQLs and context gathered in Step 3, crafting two versions for different settings. The instructions are designed to balance both naturalness and unambiguity. Due to the differences between Spider 2.0 and Spider 2.0-lite/snow, code agent tasks demonstrate greater naturalness in its questions because it provides contexts and predefined files to guide the answers, while text-to-SQL tasks prioritize unambiguity, ensuring clearer and more straightforward specifications (see App.B.6 for differences). Annotators manually write the instructions, making them natural by avoiding blunt descriptions, removing ambiguity in the expected results, and ensuring that all SQL conditions are clearly mentioned. Also, the DBT-project tasks (see Fig. 1 and App.B.2), which are realistic data transformation coding scenarios, are exclusively used in Spider 2.0. Annotators craft task instructions based on the provided context. After the initial annotation, they verify the semantic equivalence between the SQL queries and instructions, paraphrasing for clarity with the help of LLMs."}, {"title": "5) Execution-based focused evaluation", "content": "In this step, annotators are required to obtain results from the databases programmatically and write evaluation scripts (details in App.A). The evaluation scripts can process the results in the form of strings, tables, and database files. It is important to note that in table-based evaluations, predicted results may include numerous columns, which might not exactly match the gold standard answers. This discrepancy often arises because some questions do not specify the columns that should be returned. To mitigate this, the evaluation scripts are specifically focused on the essential components of the answers, ignoring non-essential columns and emphasizing the core elements outlined in the instructions. This method facilitates targeted assessments of key columns for each task, thus significantly reducing the occurrence of false negatives. For Spider 2.0-lite and Spider 2.0-snow, these settings require that the output must be SQL, so the evaluation will compare the execution results of the SQLs using the table-based assessment method."}, {"title": "6) Quality control", "content": "To ensure the quality of our benchmark, each instruction, the gold SQL query, and evaluation script are reviewed by at least three annotators. We require the annotators to repeatedly review steps 3), 4), and 5) to ensure the correctness, naturalness, and unambiguity of the annotations. Consequently, 45% of the examples have errors identified by the first validators. After discussions and corrections, following the second round of iteration with the second validators, only 5% of the examples contain errors. Then we correct all errors and refine all annotations, and ultimately, all examples are deemed fully annotated. Additionally, we perform a \u201cred team\u201d assessment of our automatic evaluation by providing a set of false results to determine if they would be correctly classified as false, along with various correctly formatted results to verify their classification as true."}, {"title": "2.3 DATASET STATISTICS", "content": "We present a detailed statistical analysis of the features of Spider 2.0, Spider 2.0-snow and Spider 2.0-lite, comparing them with multiple previous datasets in Tab. 2, our datasets demonstrate strong complexity and realism in aspects such as databases, SQLs, and task scenarios."}, {"title": "Diverse database systems and SQL dialects", "content": "As shown in Fig. 3 and Tab. 3, our benchmarks feature a diverse array of database systems, including cloud data warehouses like BigQuery and Snowflake, locally hosted databases such as Postgres and ClickHouse, and lightweight systems like SQLite and DuckDB. This diversity distinguishes our benchmarks from previous work by encompassing various SQL dialects. Notably, 85.98% of the examples require the use of specialized functions from these dialects, with an average of 7.1 special functions utilized in each ground-truth SQL."}, {"title": "Real and complex database schema", "content": "As shown in Tab. 2, the databases in Spider 2.0 are equipped with large-scale schemas comprising extensive tables and columns, effectively mirroring real-world enterprise environments. As shown in Tab. 3, these databases are characterized by complex schema structures (e.g., multiple and nested schemas, partitioned tables; see Fig. 12 and Fig. 13), and dynamic tables that are updated daily. Additionally, the data encompasses a broad spectrum of complex types (Fig. 17), extensive volumes, and diverse scopes (Fig. 16), rendering it more diverse than previous datasets."}, {"title": "Challenging tasks across the data engineering pipeline", "content": "The examples in our benchmarks are collected from real tutorials and forums, covering a wide range of issues encountered in data pipelines, including data wrangling, data transformation, and data analysis (see App.B.1 for examples). The difficulty of these questions significantly exceeds that of previous SQL-related benchmarks, as the SQL queries in Spider 2.0 contain significantly more columns, tokens, and functions per query than those in prior work (see Tab. 2 and Fig. 19 for examples)."}, {"title": "Real projects scenarios with codebases and documents", "content": "As demonstrated in Tab. 2 and 3, tasks in both datasets require access to documentation, like external knowledge (App.B.4) and SQL dialect (App.B.7), necessitating a deep understanding of these resources. Compared to other prior works, for each task in Spider 2.0, we provide a codebase context to simulate a real workflow (App.B.5). More notably, some tasks introduce innovations such as project-level data transformation workflows built on DBT (App.B.2), a widely used tool for managing data transformations and analytics engineering. Successfully addressing these tasks requires navigating complex project codebases and databases, comprehending documentation, processing intricate contexts, and generating diverse queries through multi-step execution and reasoning."}, {"title": "3 EXPERIMENTS", "content": null}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "Evaluation metrics. For Spider 2.0, we use the Success Rate (SR) metric, which measures the proportion of task instances successfully completed. For Spider 2.0-lite and Spider 2.0-snow, the output for each task must be an SQL, we use the widely used metric Execution Accuracy (EX)(Yu et al., 2018; Li et al., 2024b). We employ the execution-based focused evaluation (App.A) to determine the success of each result for Spider 2.0 and assess the accuracy of SQL execution results for Spider 2.0-lite. The evaluation scripts are designed to accept output in the form of strings, tables, or database. For each example, an evaluation script is run for each example, producing a score of either 0 or 1. It is worth noting that in table-based evaluations, predicted results may contain numerous columns, leading to results that are not exactly the same as the gold answer. This occurs because, for some examples, questions do not explicitly specify which columns to return. The evaluation scripts are specifically focused on the essential components of the answers, disregarding irrelevant columns and concentrating on the core elements specified in the instructions.\nDifficulty level. We tokenize the gold SQL queries based on whitespace and classify their difficulty according to the number of tokens: < 80 tokens as Easy, 80 \u2013 159 as Medium, and $\\geq$ 160 as Hard\u00b9.\nLLMs. We experiment with state-of-the-art LLMs, including open-source representatives such as DeepseekCoder-V2.5 (Zhu et al., 2024), Qwen2.5-72B-Instruct (Team, 2024) and Llama-3.1-405B (Meta AI, 2024), and closed-source ones including Gemini-Pro-1.5 (Reid et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024) and GPT (OpenAI, 2023) families (GPT-4O, GPT-4 and o1-preview). Follow (Yang et al., 2024a; Chen et al., 2024), we use a temperature of 0.0 and truncate from the beginning of the input if still exceeding the max tokens limit required by the models.\nCode agent frameworks for Spider 2.0. We utilize several state-of-the-art frameworks, which have demonstrated excellent performance on other benchmarks. These include Reflexion (Shinn et al., 2023), CodeR (Chen et al., 2024), AutoEval (Pan et al., 2024). Inspired by React (Yao et al., 2022) and Intercode (Yang et al., 2023), we develop an agent framework called Spider-Agent, which is primarily focused on database-related coding tasks and projects. The framework allows for multi-turn interactions with the database via command-line interfaces until the final answer is obtained. The implementation details of Spider-Agent are shown in App.C.1.\nText-to-SQL methods for Spider 2.0-lite/snow. We evaluate several state-of-the-art and widely recognized text-to-SQL methods on Spider 2.0-lite and Spider 2.0-snow, including approaches based on prompting LLMs such as DIN-SQL (Pourreza & Rafiei, 2024), DAIL-SQL (Gao et al., 2024) and CHESS (Talaei et al., 2024), alongside SFT CodeS (Li et al., 2024a), which fine-tuned open-source models on extensive text-to-SQL corpora. DAIL-SQL and CHESS achieve the best performance among all accessible methods on the Spider 1.0 and BIRD benchmark, respectively. During implementation, we optimize the prompt organizations across all methods to better align with tasks, incorporating sampled cell values, external knowledge, and SQL dialect specifications (see Fig. 22)."}, {"title": "3.2 EVALUATION RESULTS", "content": "Existing LLMs are still far from being expert on real-world text-to-SQL workflow tasks. The o1-preview model achieves the highest performance, with a peak success rate of 17.01%, indicating significant potential for further improvement. It surpasses both GPT-4O and Claude-3.5-Sonnet across the Easy, Medium, and Hard cases, highlighting its superior reasoning capabilities. GPT-4 performs better on Easy tasks, but underperforms GPT-4O on Medium and Hard tasks. This discrepancy highlights the broad range of difficulties in Spider 2.0 and underscores its complexity. The open-source LLM Qwen-2.5-72B showed a performance of 6.17%, which is better than other open-source LLMs, but still has significant room for improvement."}, {"title": "4 ANALYSIS", "content": null}, {"title": "4.1 ANALYSIS OF DIFFERENT TASK TYPES", "content": "LLM-agent frameworks struggle to address project-level tasks. As shown in Tab. 6, the LM agent's performance on DBT-based project-level tasks is poor, solving only 12.82% of tasks with just 10 examples correct. This underscores the challenges in there tasks, which can be attributed to: (1) Data transformation projects often require multiple SQL queries to complete various models, necessitating a comprehensive understanding of the project. (2) These tasks involve complex context usage, demanding strong repository exploratory capabilities from the models. (3) Data is stored in databases, requiring the agent to transform data while exploring existing data, alongside SQL coding. Fig. 26 illustrates the action process of o1-preview successfully solving a task defined in App.B.2, while Fig. 27 is a failure case due to the failure to explore the information in the \"mrr.md\u201d file to solve a monthly recurring revenue classification.\nThe performance drops when external documents are required. From Tab. 7, we observe that when tasks involve external documents, the model performs poorly, correctly answering only 9 examples out of full dataset that accounts for just 10.98%. Through error analysis, we find that the model is not incapable of grounding complex documents information. These models typically have the correct problem-solving strategies and effectively explore the database, but fails at the most crucial step: grounding the complex requirements from the documents into SQLs. As the document shown in Fig. 14, the gold SQL is shown in Tab. 16. The failure case shows that the model cannot combine complex document with schema information and convert it into SQL query (Fig. 28).\nLLM-agent frameworks struggle interpreting databases with nested schema. As shown in Tab. 8, the model often performs poorly when handling columns with nested types. Nested columns are a common scenario in industrial-grade databases (see Fig. 12), where data is stored in array, dict formats within a single column. This poses significant challenges for LLMs in understanding the schema. As shown in Fig. 29, LLMs encounter schema linking errors due to an incomplete understanding of the information contained within nested fields. Most databases with nested types face the issue that models find it difficult to fully grasp the function of each nested column's internal information, while humans can comprehend the schema through multi-step reasoning and iterative understanding.\nLLM-agent frameworks face challenges handling different SQL dialects. As shown in Tab. 9, we analyze the examples in Spider 2.0 based on database type. The results indicate that examples of the Snowflake type in Spider 2.0 are the most challenging. To assess the impact of SQL dialects on performance, we randomly select 180 examples and hosted them on both BigQuery and Snowflake, using the same set of questions. We find that performance on BigQuery is 12.78%, while on Snowflake it is 6.6%, highlighting that subtle differences in SQL dialect syntax can lead to variations in SQL generation performance."}, {"title": "4.2 ERROR ANALYSIS OF SQL GENERATION", "content": "We conduct a detailed analysis of the errors encountered by both code agent frameworks and text-to-SQL parsing methods on randomly sampled 300 examples, as illustrated in Fig. 5. Representative errors along with their statistics and causal analysis are as follows.\nErroneous data analysis (35.5%). Compare to the previous benchmarks, Spider 2.0 and Spider 2.0-lite exhibit significantly complex data analysis demands that challenge the models' capabilities: 1) Dialect function usage (10.3%). This includes processing temporal (e.g., DATE_TRUNC) or geographic data (e.g., ST_DISTANCE). These functions require a nuanced understanding, which the models often fail to exhibit. 2) Advanced data calculation (7.5%). Model struggle with tasks like grouping samples to analyze trends within groups (using NTILE), or applying formulas for statistical values (e.g., CORR for Pearson correlation coefficients; STDDEV for standard deviation). 3) Intricate query planning (17.7%). Gold SQLs typically involve multiple nested queries, intermediate result processing through common table expressions (CTEs), or merging results from various sub-queries via set operations. However, models often inadequately handle these complexities. Refer to Fig. 6 for case studies on erroneous data processing.\nWrong schema linking (27.6%). This category includes errors with wrong tables and columns. For column linking errors (16.6%), the average number of columns per database in Spider 2.0-lite far exceeds those in other benchmarks (over 755 compared to approximately 54 in BIRD), making accurate column linking extremely challenging. Regarding table linking (10.1%), although examples from BigQuery support advanced syntax features like (TABLE_SUFFIX) and wildcard expressions, the models show limited flexibility in leveraging these features, even in few-shot setting.\nJOIN errors (8.3%). While foreign keys represent known schema relationships essential for valid SQL JOIN operations, databases in BigQuery often lack explicit foreign key. This omission forces models to infer potential keys based on column names and descriptions, leading to errors."}, {"title": "4.3 ANALYSIS OF DIFFERENT EXPERIMENTAL SETTINGS", "content": "Reference plan can significantly improve SQL generation performance. Annotators are required to provide detailed annotations for task instructions. While the original instructions are brief and conversational, the reference plan offers a comprehensive, step-by-step explanation of how to write each SQL. This approach decouples the exploration of the database from the process of generating text-to-SQL. An example is provided in App.B.1. As shown in Tab. 10, incorporating a reference plan resulted in an approximate 3% improvement in the EX of DAIL-SQL. By harnessing the latest o1-preview, which excels at code generation, 12.60% of the instances can be correctly solved. This suggests that some challenging instances can't be solved by directly generating the final SQL but benefit from a step-by-step approach using multiple CTE clauses.\nProviding oracle functions leads to a slight performance improvement. Considering that Spider 2.0 and Spider 2.0-lite involve SQL dialects from various database systems, we provide syntax and function documentation for each system to prevent the methods from suffering due to lack of syntax knowledge. For each example, we manually include the relevant function documentation that may be required, eliminating the need for a retrieval method and ensuring that the necessary syntax knowledge is readily accessible. As shown in Tab. 10, providing oracle SQL function documentation results in only a slight improvement in model performance. This suggests that, to a certain extent, models are capable of selecting appropriate functions and understanding their basic usage and syntax. However, the critical challenge lies in accurately utilizing these functions to reflect user intentions, as illustrated in Fig. 6(a).\nFew-shot prompting has little impact on performance. Spider 2.0-lite is not divided into train and dev sets, we manually select representative examples from the same SQL dialect as the SQL to be predicted, with distinct characteristics (encompassing multiple CTE or nested queries, or requiring intricate data processing) to serve as few-shot examples. Unexpectedly, few-shot in-context learning shows only marginal improvements in performance (see Tab. 11). This may be due to the gap between the simplistic text-to-SQL pre-training data used with LLMs and the complexity of the few-shot examples. Additionally, extensive schema prompts may hinder the model's ability to effectively assimilate information in the few-shot examples."}, {"title": "5 RELATED WORK", "content": "Code generation and text-to-SQL benchmark. As model capabilities advance, code generation benchmarks have become more complex and generalized. Many benchmarks (e.g., SQL-Spider (Yu et al., 2018), Bash-NL2Bash (Lin et al., 2018), Python-HumanEval (Chen et al., 2021)) treat code generation as seq2seq tasks. Many previous works (Lai et al., 2023; Yin et al., 2023; Huang et al., 2024; Chan et al., 2024) define code generation tasks for data science. MLAgentBench (Huang et al., 2023) and Intercode (Yang et al., 2024b) focus on interactive environments, while SWE-Bench (Jimenez et al., 2023) emphasizes repository-level coding tasks. Spider2-V (Cao et al., 2024) proposes data science and engineering benchmark in a multimodal setting. Particularly for the text-to-SQL task, WikiSQL (Zhong et al., 2017) is the first large-scale dataset for evaluating text-to-SQL methods. KaggleDBQA (Lee et al., 2021) includes database documentation, while SEDE (Hazoom"}, {"title": "6 CONCLUSION", "content": "We propose Spider 2.0, a benchmark for real-world enterprise-level text-to-SQL workflow tasks. It encompasses diverse database systems with various SQL dialects, large and complex database schemas, and challenging tasks across the data engineering pipeline, all set within real project scenarios including codebases and documentation. Despite being the most advanced LLMs (01-preview), they still perform poorly on Spider 2.0, achieving a success rate of only 18.8%, which underscores its status as a highly challenging benchmark. Spider 2.0 presents a novel challenge for text-to-SQL research, providing a direction towards more realistic and intelligent solutions."}]}