{"title": "Scores as Actions: a framework of fine-tuning diffusion models\nby continuous-time reinforcement learning*", "authors": ["Hanyang Zhao", "Haoxian Chen", "Ji Zhang", "David D. Yao", "Wenpin Tang"], "abstract": "Reinforcement Learning from human feedback (RLHF) has been shown a promising\ndirection for aligning generative models with human intent and has also been explored in\nrecent works [1, 9] for alignment of diffusion generative models. In this work, we provide\na rigorous treatment by formulating the task of fine-tuning diffusion models, with reward\nfunctions learned from human feedback, as an exploratory continuous-time stochastic control\nproblem. Our key idea lies in treating the score-matching functions as controls/actions, and\nupon this, we develop a unified framework from a continuous-time perspective, to employ\nreinforcement learning (RL) algorithms in terms of improving the generation quality of\ndiffusion models. We also develop the corresponding continuous-time RL theory for policy\noptimization and regularization under assumptions of stochastic different equations driven\nenvironment. Experiments on the text-to-image (T2I) generation will be reported in the\naccompanied paper [60].", "sections": [{"title": "1 Introduction", "content": "Diffusion models [37] can shape a simple noisy/non-informative initial distribution step-by-step\ninto a complex target distribution by a learned denoising process [13, 38, 41] and have shown\nthe remarkable capability to capture intricate, high-dimensional distributions make them the\nleading framework for high-quality and creative image generation, both unconditionally [6] or\nconditionally given additional text prompts [30, 31, 32, 36]; they are also rapidly finding use\nin other domains such as video synthesis [14], drug design [54] and continuous controls [17, 53].\nHowever, existing models still have limited abilities for needs like multiple objective compositions\n[10, 11], specific color and counts [23], and they may also suffer from sources of bias or fairness\nconcern [26] and may fail to produce reliable visual text, even being distorted. As such, there\nis great interest in improving them further in terms of either generated distribution quality, or\ncontrollability. Specifically, due to the emergence of human-interactive platforms like ChatGPT\n[27] and Stable Diffusion [31], there is increasing and substantial demand to let the generative\nmodels align with the user/human preference or feedback, which seems to be infeasible for the\ncurrent training process only targeted at maximizing the likelihood.\nInspired by such needs, [12] proposed a natural way to fine tune or optimize diffusion models:\nReinforcement Learning (RL, [43]). RL has already shown empirical success in enhancing Large"}, {"title": "1.1 More Related works", "content": "Our work is related to papers in several following domains."}, {"title": "2 Formulation and Preliminaries", "content": "In this section, we first review some necessary backgrounds of score-based diffusion models [41]\n(see [47] for a tutorial and more comprehensive review) and continuous-time RL [52]."}, {"title": "2.1 Score Based Diffusion Models", "content": "Diffusion Models. Consider perturbed data distributions evolve according to an SDE as the\nnoise intensifies [41], in which the forward SDE with state space $X_t \\in \\mathbb{R}^d$ is defined as:\n\n$dX_t = f(t, X_t)dt + g(t)dB_t$, with $X_o \\sim P_{data}(\\cdot)$, (1)\n\nwhere $(B_t, t > 0)$ is $d$-dimensional Brownian motion, and $f : \\mathbb{R}_+ \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ and $g : \\mathbb{R}_+ \\rightarrow \\mathbb{R}_+$\nare designed model parameters. To avoid technical difficulties, we assume that the stochastic\nprocesses (1) are well-defined, see e.g., [21, Section 5.3] or [42, Chapter 6] for background. Denote\n$p(t,\\cdot)$ as the probability density of $X_t$.\nSet time horizon $T > 0$ to be fixed, and run the SDE (1) until time $T$ to get $X_T \\sim p(T,\\cdot)$. The\ntime reversal $X^{rev}_t := X_{T-t}$ for $0 \\leq t \\leq T$, such that $law(X^{rev}_t) = law(X_{T-t})$ for every $t \\in [0,T]$,\nalso satisfies an SDE, under some mild conditions on $f$ and $g$:\n\n$dX^{rev}_t = \\left(-f(T-t, X^{rev}_t) + \\frac{1}{2} \\eta^2 g^2 (T \u2013 t)\\nabla_x \\log p(T \u2013 t, X^{rev}_t)\\right) dt + \\eta g(T \u2013 t)dB_t$, (2)\n\nin which $\\nabla_x \\log p(t, x)$ is known as the stein score function [41]) and $\\eta \\in [0, 1]$ is a constant. If\nstarting from the distribution $p(T, \\cdot) = law(X_T)$, the SDE (2) runs until time T and will generate\n(or recover) the desired $X^{rev}_T \\sim P_{data}(\\cdot)$. However, since distribution $p(T, \\cdot)$ is unknown and hard\nto sample from, diffusion models typically replace it with a prior distribution $p_{\\infty}(\\cdot)$ such that\n$p(T,\\cdot) \\approx p_{\\infty}(\\cdot)$ when $T$ is sufficiently large (which can motivate a smart choice of diffusion\ndynamics, see e.g. [46] for some discussions.). The prior distribution is thus independent of\n$p(0,\\cdot) = P_{data}(\\cdot)$, and this explains why DPMs generate distributions from \u201cnoise\u201d $p_{\\infty}(\\cdot)$. In\naddition, a special but important case by taking $\\eta = 0$ in the (2), this results to an ODE [41]:\n\n$dX^{ode}_t = \\left(-f(T \u2013 t, X^{ode}_t) + \\frac{1}{2} g^2 (T \u2013 t)\\nabla_x \\log p(T \u2013 t, X^{ode}_t)\\right) dt$, (3)"}, {"title": "Score Matching.", "content": "Since the score function $\\nabla_x \\log p(t, x)$ is also unknown, diffusion models learn\na function approximation $s_\\theta(t, x)$, parameterized by $\\theta$ (usually a neural network), to the true\nscore by minimizing the MSE or the Fisher divergence (between the learned distribution and\ntrue distribution), evaluated by samples generated through the forward process (1):\n\n$J(\\theta) = E_{t\\sim Uni(0,T)}E_{x\\sim p(t,\\cdot)} {\\lambda(t) [||s_\\theta(t, x) - \\nabla_x \\log p(t,x)||^2]},$ (4)\n\nin which $\\lambda : [0, T] \\rightarrow \\mathbb{R}_{>o}$ is a chosen positive weighting function. When choosing the weighting\nfunctions as $\\lambda(t) = g^2(t)$, this score matching objective is equivalent to maximizing an evidence\nlower bound (ELBO, [39, 15]) of the log-likelihood. The explicit score matching (ESM) objective\n(4) is equivalent to the following denoising score matching (DSM) [51] objective:\n\n$\\theta^* = \\arg \\max_\\theta E_{t\\sim Uni(0,T)} {\\lambda(t)E_{x_0\\sim P_{data}} E_{x_t|x_0} [||s_\\theta(t, x_t) - \\nabla_{x_t} \\log p(t, x_t|x_0)||^2]},$ (5)\n\nin which $x_t \\sim p(t,x_0)$. DSM objective is tractable and also easier to be optimized in the\nsense that $p(t, x_0)$ can be written in closed-form given a smart choice of diffusion dynamics, for\nexample, it is conditionally Gaussian if (1) is a linear SDE as in [41].\nAfter learning the optimal approximation $s_{\\theta^*}$ which minimizes the objective, diffusion models\nreplace the true score $\\nabla \\log p(t, x)$ by the learned score $s_{\\theta^*}(t, x)$ in (2) as an approximation to\n$X^{rev}$, which we denotes as $X^+_t$, such that:\n\n$dX^+_t = \\left(-f(T - t, X^+_t) + \\frac{1 + \\eta^2}{2} g^2 (T - t)s_{\\theta^*}(T - t, X^+_t)\\right) dt + \\eta g(T - t)dB_t$, $X^+_0 \\sim p_{\\infty}(\\cdot)$, (6)\n\nat time $T$ to achieve generation, or the corresponding ODE sampler ($\\eta = 0$) as:\n\n$dX^{ode}_t = \\left(-f(T - t, X^{ode}_t) + \\frac{1}{2} g^2 (T \u2013 t)s_{\\theta^*}(T \u2013 t, X^{ode}_t)\\right) dt$, $X^{ode}_0 \\sim p_{\\infty}(\\cdot)$. (7)\n\nThe well-known DDPM [13] sampler and DDIM [38] sampler can both be seen as a certain\ndiscretization/integration role of the SDE in (6) or the ODE in (7) as proved by [33, 41, 57, 58],\nwe include the more detailed discussions in Appendix A for completeness. We will follow the\ncontinuous-time prospective throughout this paper.\nFor the backward process (6) and (7), the score function approximations $s_{\\theta^*}(T \u2013 t, X^+_t)$ de-\ntermine the $law(X^+_t)s$ throughout the backward process with $t \\in (0,T]$, especially $law(X^+_T)$\nwhich we may actually be interested in. This is reminiscent of the actions or controls taken\nby agents in the RL/control theory and our formulation are thus motivated to treat this score\nfunction/approximation as a control over the backward process; for task of fine-tuning, $s_{\\theta^*}$ can\nbe seen as a pretrained control/policy we can both have access and we shall refer to."}, {"title": "2.2 Continuous-time RL", "content": "We start with a general formulation of the continuous-time RL under a finite horizon as a\nstochastic control problem, based on the same modeling framework as in [18, 52].\nDiffusion Process. Assume that the state space is $\\mathbb{R}^d$, and denote by $A$ the action space.\nLet $\\pi(\\cdot | t,x) \\in \\mathcal{P}(A)$ be a (state) feedback policy given the time $t \\in [0,T]$ and state $x \\in \\mathbb{R}^d$.\nA continuous RL problem is formulated by a distributional (or relaxed) control approach [55],"}, {"title": "3 Main Results", "content": "In this section, we formally formulate the task of fine-tuning diffusion models as a continuous-\ntime exploratory stochastic control problem under the finite horizon. The key component under"}, {"title": "3.1 Fine-tuning Diffusion Models as Stochastic Control", "content": "our formulation is to regard the backward process as a stochastic control process and treat the\nscore function approximation as action/control process.\nScore function as action. The backward procedure of diffusion models as defined in (6), when\nwe generate conditionally on an additional input/context which we denote as c. c can be a label\nfor generate imaging of a certain class or an input text prompt for text-to-image generation, and\nthe reverse process in the (latent) diffusion models becomes\n\n$dX^+_t = \\left(- f(T \u2013 t, X^+_t) + \\frac{1 + \\eta^2}{2} g^2 (T \u2013 t)s_{\\theta^{pre}} (T \u2013 t, X^+_t, c) \\right) dt + \\eta g(T \u2013 t)dB_t$, (13)\n\nin which we abuse the notation of $s_{\\theta^{pre}} (T \u2013 t, X^+_t, c)$ to represent the pretrained score function\nconditioning on $c$. Note that if we choose the b and $\\sigma$ in the continuous RL dynamics assumption\nin (10) as the diffusion term being $\\sigma(t) = g(T \u2013 t)$, and the drift term being:\n\n$b_{\\pi} (t, x, a) := \\left(- f(T \u2013 t, x) + \\frac{1 + \\eta^2}{2} g^2 (T \u2013 t)a\\right)$, (14)\n\nthen if we define a specific (deterministic) feedback control process as $\\alpha^{pre}_t = S_{\\theta^{pre}} (T-t, X_t^+, c)$,\nthe backward procedure can be rewritten as:\n\n$dX^+_t = b_{\\pi} (t, X^+_t, \\alpha_t) dt + \\eta \\sigma(t)dB_t$, (15)\n\nthus the action has a clear meaning as the place of score function (conditional on the input con-\ntext c). Thus we treat the current score approximation as a pretrained feedback control/action\nprocess, which we want to further optimize on for diffusion alignment.\nExploratory SDEs. Lies central in RL is the exploration. To encourage exploration, at time\nt, we can adopt a Gaussian exploration control/policy\n\n$\\alpha^+_t \\sim \\pi^{\\theta} (. \\vert t, X_t^+, c) = N(\\mu^{\\theta}(t, X_t^+, c), \\Sigma_t \\cdot I)$,  in which the mean $\\mu^{\\theta} (t, X_t^+, c)$ is approximated by\n$\\theta$ and the variance $\\Sigma_t$ is a chosen fixed exploration level for each $t$. We denote the corresponding\nprocess $X^+_t$ as $X^{\\theta}_t$ for brevity. Under this Gaussian parameterization, similar to discussion\nof (10), it suffices to consider the following SDE (it actually holds for general distribution\nbeyond Gaussian with mean $\\mu^{\\theta}(t, X_t^+, c)$) governed by a corresponding deterministic policy only\ndependent on the mean $\\mu^{\\theta}(t, X_t^+, c)$:\n\n$dX^{\\theta}_t = \\left[- f(T \u2013 t, X^{\\theta}_t) + \\frac{1 + \\eta^2}{2} g^2 (T \u2013 t) \\mu^{\\theta}(t, X_t^{\\theta}, c)\\right] dt + \\eta \\sigma(T-t)dt, X^{\\theta}_0 \\sim \\rho$, (16)\n\nand we denote the probability density of $X^{\\theta}_t$ as $\\rho_{\\theta}(t,\\cdot, c)$.\nWe emphasize that using the (conditional) score matching parameterization as the same as the\npolicy parameterization $\\mu^{\\theta^*} (t, x, c) = s^{\\theta^*} (T \u2013 t,x,c)$ recovers the backward procedure (6) in\nthe score-based diffusion models, that's why we refer our formulation enables 'score function\nas action'. This perspective is also reminiscent to classifier guidance, as classifier guidance for\ndiffusion models leverage the property of the conditional score as:\n$\\nabla \\log p(t, x | c) = \\nabla \\log p(t, c | x) + \\nabla \\log p(t, x)$,\n\nin order to generate conditionally on an extra label $c$ in our contents. $\\nabla \\log p(t, x)$ can be seen as\nan old unconditional control before fine-tuned, and $\\nabla \\log p(t, x | y)$ can be seen as a new control\nfor better conditional generation, and $\\nabla p(T, c | x)$ acts as a reward to guide the shift/difference\nfrom the old control to the new one. In more general cases when reward model is complicated,"}, {"title": "Regularization as Rewards.", "content": "We assume that we are given a reward model (RM), such that\nit can output the reward RM(x, c) given a generation $x \\in \\mathbb{R}^d$, which represents the human\npreference or any target we want to maximize of the current generation $X_T$ given input $c$. For\nexample, if the downstream task is text-to-image generation, we have RM(x, c) represents how\nthe generated image $X_T$ align with input prompt $c$. In addition, we also consider adding reg-\nularization: in our cases, there should be two sources of regularization: (i) regularization to\nencourage exploration as in our continuous RL formulation, like entropy regularization in [52]\nor [18]; (ii) regularization to prevent the model from overfitting to the reward or catastrophic\nforgetting, and thus failing to utilize the capabilities of the pre-trained diffusion models parame-\nterized by $\\Theta_{pre}$. Notice that here since we already used Gaussian Exploration with fixed variance\nto encourage exploration, we will not use additional regularization of source (i); for source (ii),\nin the same essence of previous work of tuning diffusion models by discrete-time RL [9, 27], we\nstill target at bounding the KL divergence of the final generation, i.e., our final optimization\nobjective yields:\n\n$E [RM(c, X_T^{\\theta}) \u2013 \\beta KL(\\rho_{\\theta}(T,\\cdot, c)||\\rho_{\\theta^{pre}} (T,\\cdot,c))],$ (17)\n\nin which $\\beta$ is a penalty constant, often needed to tune separately. However, instead of adding\nKL divergence directly to the objective like DPOK [9], we could \u2018smartly' transform this term\ninto an integration of expected $L^2$ penalty term between the mean of the current Gaussian policy\nand the pretrained/reference score along the path, thanks to our continuous-time formulation\nand the following theorem:\nTheorem 1. We have that, for any $c$, the discrepancy between the $\\rho_{\\theta}$ and $\\rho_{\\theta^{pre}}$ satisfies:\n\n$KL(\\rho_{\\theta} (T,\\cdot, c) ||\\rho_{\\theta^{pre}} (T,.,c)) = \\frac{1}{2} \\frac{1}{\\eta} \\sqrt{1 + \\eta^2} \\int_0^T E_{\\rho_{\\theta}(t,:\\vert c)}g^2 (T-t) ||\\mu^{\\theta} (t, X_t^{\\theta}, c) - s_{\\theta^{pre}}(t, X_t^{\\theta}, c)||^2dt$. (18)\n\nThe proof is enclosed in Appendix B.1. As a remark, it is important to let the expectation under\n$\\rho_{\\theta}$ in order for later online update. In addition, notice that\n$E_{\\rho_{\\theta}(t,:\\vert c)} ||a_t \u2013 s_{\\theta^{pre}} (t, X_t^{\\theta}, c)||^2 = E_{\\rho_{\\theta}(t,\\cdot\\vert c)} ||\\mu^{\\theta} (t, X_t^{\\theta}, c) \u2013 s_{\\theta^{pre}} (t, X_t^{\\theta}, c)||^2 + 2\\Sigma_t$,\nwe have that optimizing (17) is equivalent to optimizing the following objective:\n\n$\\eta = E \\left[\\int_{0}^{T} \\frac{\\beta}{\\left(\\frac{\\eta}{\\sqrt{1 + \\eta^2}}\\right)^2} \\frac{g^{2}(T-t)}{\\sigma_t^2} \\lVert\\mu^\\phi(t, X^\\phi_t, c) \u2013 s_{\\phi_{pre}} (t, X^\\phi_t, c)\\rVert^2 dt + RM(c, X_T^{\\theta})\\right]$ (19)\n\ninspired by the previous Theorem 1, and This aligns with our continuous-time RL formulation\nreviewed in (9), and we further investigate the methodologies for policy optimization under this\nformulation.\nWe first present the policy gradient formula for our finite horizon problem (9), which simplifies\nthe formula in [19] by utilizing the discussion in [20, 59]."}, {"title": "Theorem 2.", "content": "We have that the policy gradient of an admissable policy $\\pi^{\\theta}$ parameterized by $\\theta$ is:\n\n$\\nabla_{\\theta} \\eta^{\\theta} = E_{\\pi^{\\theta}} \\left[\\int_{0}^{T} \\nabla_{\\theta}\\log \\pi (a_t|t, X_t)q(t, X^{\\theta}_t, a_t^{\\theta}; \\pi^{\\theta})dt\\right]$ (20)"}, {"title": "Corollary 1.", "content": "We have that the policy gradient of an admissable policy $\\pi^{\\theta}$ parameterized by $\\theta$ is:\n\n$\\nabla_{\\theta} \\eta^{\\theta} = E_{\\pi^{\\theta}} \\left[\\int_{0}^{T} \\nabla_{\\theta}\\log \\pi (a_t|t, X_t) (r_n(t, X^{\\theta}_t, a_t^{\\theta}) + \\frac{1 + \\eta^2}{2}g^{2}(T-t)\\frac{\\partial V}{\\partial x} (t, X; \\pi^{\\theta})) dt\\right]$ (21)\n\nThis observation allows us only to estimate the value function, which we call as the continuous-\ntime case of Generalized Advantage Estimation (GAE) [34]."}, {"title": "3.2 Directly aligning deterministic sampler", "content": "We can also directly fine-tune an deterministic sampler instead of using the related SDEs. For\nexample, Stable Diffusion v1 adopts DDIM, which can be seen as an integration role of ODE;\nStable Diffusion v3 [7] is built upon Rectified Flow [24, 25]. In this work, we specifically consider\nthe case of fine-tuning an ODE-based model parameterized (by $\\phi$) as:\n\n$dX^{\\phi}_t = \\left(-f(T-t, X^{\\phi}_t) + \\frac{1}{2}\\beta^2 g^2 (T \u2013 t) \\mu^\\phi(t, X^{\\phi}_t, c)\\right)dt$, $X^{\\phi}_0 \\sim \\rho$. (22)\n\nFor brevity of analysis, we assume that the pre-trained model parameterized by $\\Theta_{pre}$ yields\nperfect score matching, i.e. $s_{\\Theta_{pre}}(t,x,c) = \\nabla_xp(t,x,c)$ for any $t, x, c$; Moreover we consider\n$\\rho = N(0, I)$, which indeed holds for forward process being VP-SDE. We consider the exploratory\nODE by taking $\\alpha^{\\phi} \\sim N(\\mu^{\\phi}(t, X_t^{\\phi}, c), \\sigma_t)$, similar to our exploratory SDE cases. To optimize\nthe objective in (17) in which $X_t$ now follows exploratory ODE (instead of SDE), we use the\nidea of MM algorithm, and then derive the policy gradient of a lower bound of the regularized\nobjective:"}, {"title": "Theorem 3.", "content": "We have for any context c, there exists a constant $\\tilde{C}$ such that:\n\n$KL(\\rho_{\\phi}(.,c)||\\rho_{\\phi^{pre}} (.,c)) \\leq C \\int_0^T g^2(T \u2013 t) ||\\mu^{\\phi^*} (t, \\cdot, c) \u2013 \\mu^{\\phi^{pre}} (t, \\cdot, c) ||_{H^1}dt$ (23)\n\nMore details of the theorem will be developed in the accompanied paper [60]. Notice that, we\nneed this separate argument since directly letting $\\eta \\rightarrow 0$ provides a vacuous bound in Theorem\n1. This theorem thus motivates our following objective as:\n\n$\\eta = E \\left[- \\beta \\tilde{C} \\int_{0}^{T} \\frac{g^{2}(T-t)}{\\sigma_t^2} \\lVert\\mu^\\phi (t, X^\\phi_t, c) \u2013 s_{\\phi_{pre}} (t, X^\\phi_t, c)\\rVert^2 dt + r(c, X_T^{\\theta})\\right]$ (24)\n\nfor which we reduce the $H^1$ norm to $L^2$ norm for computation efficiency, and it is thus the same\nas in SDE case despite that the expectation is now under the exploratory ODE. For practical\nconcern, since $\\tilde{C}$ is unknown, $\\beta \\tilde{C}$ can be tuned together to obtain final good results.\nContinuous-time. We can also derive the continuous-time policy gradient without prior time\ndiscretization as (which corresponds to Corollary 1 with $\\eta = 0$, though with different reward\nfunction definition):"}, {"title": "Corollary 2.", "content": "We have that the policy gradient of an admissable policy $\\pi^{\\theta}$ parameterized by $\\phi$\nis:\n\n$\\nabla_{\\phi} \\eta(\\pi^\\phi) = E \\left[\\int_{0}^{T} \\nabla_{\\theta}\\log \\pi (a_t^{\\theta}|t, X_t^{\\theta}) \\left(r(t, X^{\\phi}_t, a_t^{\\theta}) + b_\\theta(t, X_t^{\\theta}, a_t^{\\theta}) \\frac{\\partial V}{\\partial x} (t,x; \\pi)\\right) dt\\right]$ (25)"}, {"title": "4 Conclusions", "content": "In this work, we proposed a continuous-time reinforcement learning framework for fine-tuning\ncontinuous-time diffusion models, guided by a pre-trained reward function (from human feed-\nback). The work lays out the program; further development and experiments will be reported\nin forthcoming paper [60]."}]}