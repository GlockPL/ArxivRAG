{"title": "SPECIFICATIONS: THE MISSING LINK TO MAKING THE DEVELOPMENT OF LLM SYSTEMS AN ENGINEERING DISCIPLINE", "authors": ["Ion Stoica", "Matei Zaharia", "Joseph Gonzalez", "Ken Goldberg", "Hao Zhang", "Anastasios Angelopoulos", "Shishir G. Patil", "Lingjiao Chen", "Wei-Lin Chiang", "Jared Q. Davis"], "abstract": "Despite the significant strides made by generative AI in just a few short years, its future progress is constrained by the challenge of building modular and robust systems. This capability has been a cornerstone of past technological revolutions, which relied on combining components to create increasingly sophisticated and reliable systems. Cars, airplanes, computers, and software consist of components\u2014such as engines, wheels, CPUs, and libraries-that can be assembled, debugged, and replaced. A key tool for building such reliable and modular systems is specification: the precise description of the expected behavior, inputs, and outputs of each component. However, the generality of LLMs and the inherent ambiguity of natural language make defining specifications for LLM-based components (e.g., agents) both a challenging and urgent problem. In this paper, we discuss the progress the field has made so far through advances like structured outputs, process supervision, and test-time compute\u2014and outline several future directions for research to enable the development of modular and reliable LLM-based systems through improved specifications.", "sections": [{"title": "1 Introduction", "content": "Software has been one of the main drivers of economic growth over the past several decades, a trend famously captured by Andreessen in his influential blog post, \"Why software is eating the world,\" more than a decade ago [1]. More recently, AI-particularly Large Language Models (LLMs)\u2014has emerged as the next revolution, poised to disrupt the very software that has been \"eating the world\" [2, 3, 4]. However, to fully realize this vision, we need to build LLM-based systems with the same level of reliability and rigor as those found in established engineering disciplines, such as control theory, mechanical engineering, and software engineering.\nOne key tool that has historically enabled the rapid growth of engineering disciplines is specification. Specifications describe the expected behavior, inputs, and outputs of a system. These specifications have different levels of precision and come in many forms, including formal specifications, product requirements documents (PRDs), and user manuals. Specifications help developers in several ways: (1) decomposing complex systems into smaller components, (2) reusing existing components when possible, (3) verifying that a system works as intended (i.e., it meets its specification), (4) fixing the system when it does not, and (5) creating systems capable of making decisions without human intervention.\nIn this paper, we distinguish between two types of specifications: (i) a description of what the task should do, which we call a statement specification, and (ii) a description that enables a user to verify task solutions (outputs), which we call a solution specification. Essentially, statement specifications answer the question, \"What should the task accomplish?\" while solution specifications answer the question, \"How can one verify that the task's solution adheres to the statement specification?\" In traditional software systems, the statement specification can be captured in a Product Requirements Document (PRD), whereas the solution specification can be expressed as input-output tests (e.g., unit"}, {"title": "2 Challenges with today's LLMs: building modular and reliable systems", "content": "Over the past two decades, and especially in the past two years since the introduction of ChatGPT, generative AI has made enormous strides. While the rate of progress has been astounding, most of this progress has come from building larger and better models-models that often take many months, even years, to develop and require staggering computational and financial resources. Today, the cost of developing state-of-the-art models is on the order of hundreds of millions of dollars and is soon expected to climb to billions.\nThis state of affairs raises two challenges. First, the prohibitive costs constrain the development of these models to only a few companies. Second, the monolithic nature of these models makes it difficult to address issues when they produce incorrect outputs. Even if we have access to all the model's parameters, it is far from trivial\u00b2 to identify the root of the problem. This makes it challenging to fix hallucinations, arguably the biggest drawback of today's LLMs. These two challenges can significantly slow the rate of progress and ultimately hamper the growth of the AI industry.\nIn contrast, previous engineering-driven revolutions progressed not only by improving monolithic systems but even more so by building modular systems. Take a car, for example: it is built from a myriad of components, many of which are shared across different models (e.g., wheels, brakes, engines), and these components are produced by different teams within the same company or across companies (see Figure 1). Similarly, an operating system is built from numerous components (e.g., scheduler, file system, memory manager), each of which is composed of smaller components (e.g., device drivers), developed by hundreds of individuals.\nDecomposing complex systems into simpler ones and reusing existing components has significantly lowered the barriers to building complex systems. This approach makes it easier to fix and improve these systems by isolating faulty components and repairing or replacing them. Furthermore, modularity enables a broader set of participants to contribute to value creation, as it lowers the barrier to building new, complex systems from existing ones rather than creating them from scratch.\nPerhaps not surprisingly, given these historical patterns and advantages, we have recently seen efforts to build more modular AI systems [28, 29, 34]. However, despite these efforts, breakthroughs in AI have largely continued to come from a handful of frontier labs developing larger, more powerful models.\nIn an effort to turn this tide, in this paper, we draw from the rich body of work on building modular systems and apply these lessons to transitioning from monolithic to modular LLM systems. Such transformations have happened before. As engineering disciplines mature, they typically evolve from building large, monolithic artifacts to building modular ones, from using ad hoc processes to employing precise rules and standardized components.\nOne example is the automotive industry, which evolved from building bespoke cars-where much of the work was done by hand, and parts of the same car model were not interchangeable (e.g., a door of a Ford Model K [35] wouldn't fit another Model K car without adjustments)\u2014to building millions of cars from thousands of standard, interchangeable components. Another example is the computer industry, which transitioned from creating very expensive, one-of-a-kind computers (e.g., ENIAC [36] to building affordable PCs using standardized components. These evolutions dramatically lowered the cost and barrier to building new artifacts.\nIn addition to building modular LLM-based systems, another significant challenge is building reliable systems. As illustrated in Table 1, LLM-based products and services are prone to high-profile failures across a wide variety of scenarios. To a large extent, this is due to their intrinsic flexibility: LLMs are often asked to perform ambiguous, ill-defined tasks. While embracing ambiguity has been a key to their success, deploying LLM-based products in practical scenarios often requires them to operate reliably, ideally without human supervision.\nOnce again, we can draw inspiration from engineering disciplines, which have developed processes and techniques to ensure reliability over time. For instance, the construction industry has implemented a variety of techniques, tests, and standards to build structures capable of withstanding earthquakes and hurricanes. Similarly, the software industry has adopted development methodologies (e.g., waterfall, agile development) and rigorous testing procedures (e.g., formal verification [37, 5, 38], fuzz testing [13]) to create systems that can operate error-free for extended periods and control critical systems such as airplanes, cars, and nuclear power plants.\nBuilding modular systems is closely related to building reliable systems. Reliable systems are often built as modular systems by combining smaller, reliable components. For example, a reliable car consists of a reliable engine, reliable brakes, and so on. Similarly, a reliable backend service consists of a reliable web server, a reliable database, a reliable operating system, and so forth.\nIn general, there are five properties which allow us to build such systems:"}, {"title": "3 Specification, the foundation of software engineering", "content": "The goal of a software system is to perform a specified task. The specification provides a detailed description of what the task should achieve, including the required inputs and expected outputs. When clear from context, we will refer to \"the specification of the task implemented by a system\u201d simply as the \u201csystem's specification.\u201d\nIdeally, specifications should be unambiguous, leaving no room for interpretation about the system's correct behavior or output for any given input. As shown in Figure 2, a task specification should describe both (i) what the task should do (statement specification) and (ii) how should one verify the task's solution/outputs (i.e., solution specification). A task specification is said to be unambiguous if both its statement and solution specifications are unambiguous; otherwise it is ambiguous. Note that these two specifications are complementary. The statement specification alone cannot guarantee the desired output, as there could be bugs in the task's implementation. Thus, we need a solution specification to be able to check wether the solution produced by the task's implementation satisfies the specification.\nFigure 2 shows several simple examples of tasks with various degrees of ambiguity. Task \"Solve equation x\u00b3 x = 0 has both unambiguous statement and solution specifications. In contrast, task \"What is the longest river in the world?\" has an unambiguous statement specification, but its solution specification is ambiguous, as it doesn't provide the user with enough information to verify an answer, and it is unlikely the user has this information either. In particular, it is unlikely for a user that asks this question to know that \"Mississippi\", for instance, is a wrong answer.\nSpecifications do not exist in isolation; they typically assume a user (or another system) that can interpret them. For example, a user using a formal specification is expected to understand the formalism; a developer using copilot for Rust is expected to be familiar with Rust and with the code base; a user submitting a task to solve a math equation is expected to be able to check wether the solution is correct or not. Throughout this paper, unless otherwise noted, we assume an \"average\" user, not an expert in the intended user group. For example, in a programming task, we assume an average programmer rather than an expert; for a user asking a factual question, we assume they may not know the answer and might be misled by incorrect answers, like \"Mississippi\" in our \"What is the longest river in the world?\" example.\nWhen it comes to software systems, there are several frameworks for writing unambiguous specifications, including Coq/Gallina [5, 6], TLA+ [37], Alloy [38] and Lean [39]. For example, one could use Gallina to write unambiguous statement specifications, and Coq to generate unambiguous solution specifications as proofs that the generated programs indeed satisfy their statement specifications. Unfortunately, writing such specifications is far from trivial, so these frameworks end up covering a narrow set of use cases, e.g., use cases in which the cost of failure is very high. One example is chip design where flaws can lead to delays of months and huge financial losses (see here). Another example is a reactor control system in a nuclear plant, where a failure can lead to loss of lives.\nHowever, most software systems do not incur such prohibitive costs on failures. These include the majority of software systems we are using daily such as text editors, chat applications, and even databases or operating systems. In those cases, a failure (e.g., crash) has relatively low cost, and it is relatively easy to recover (e.g., just restart the system). As a result, many such systems can get away with weaker forms of specifications, e.g., solution specifications that define the outputs for most common, but not all, inputs. A common example is a solution specification that leverages unit tests which don't cover all code paths. Another example is a specification that uses preconditions to filter incorrect outputs and postconditions to filter most, but not all, incorrect outputs [40] (e.g., a postcondition can specify that a > b, but not specify precisely the relation between a and b).\nIt is important to note that an unambiguous specification does not necessary imply a deterministic behavior, e.g., the outputs of a system are deterministic. Indeed, an unambiguous specification can describe a stochastic system, such as"}, {"title": "4 The five properties of software engineering?", "content": "In this section, we introduce five desirable properties enabled by statement and solution specifications-verifiability, debuggability, modularity, reusability, and automated decision making-in the context of software systems. To illustrate these properties, we consider a running example using a database query engine as shown in Figure 4. A query engine consists of several components, (1) a parser which takes a query and produces an intermediate representation of the query such as an abstract syntactic tree (AST), (2) a query optimizer that takes the AST as input and applies various optimization techniques to generate an efficient query execution plan, and (3) a query execution component that executes the query plan to produce the final result. In addition, the query engine interacts with a data catalog that maintains metadata information (e.g., table schemas, location) needed by the query optimizer to produce the execution plan, and the database storing the actual data.\nAs illustrated in Figure 4, one of the properties of the query engine is modularity, i.e., the query engine consists of multiple software components. This modularity enables us to build these components in parallel and then assemble them together. A second related property is reusability which is the ability to use existing components when available. For example, instead of writing a query parser from scratch, we can use an existing library such as BlazingSQL [44] or JSQLParser [45], to speed up building the query engine.\nModularity and reusability are enabled by the fact that each component has a clear specification which, again, consists of the statement specification (e.g., PRD) and the solution specification (e.g., unit and integration tests). These specifications help ensure that the output of a component matches the expected input of the next component, and help us reason about the end-to-end behavior of the system. For instance, we expect the query optimizer to affect only the query performance, not the outputs produced by the query which should remain the same.\nWhile modularity and reusability can help building new systems from existing components, in many cases we need to build components from scratch. Implementing a new component requires two other properties: verifiability and debuggability. Verifiability is enabled by the solution specification which describes the correct outputs of the task implemented by the component. In most cases the solution specification consists of a set of input-output tests (e.g., unit tests, integration tests, statistical tests, etc). Alternatively, it can come in the form of preconditions and postconditions which filter out invalid inputs and outputs [40, 46]. In our query optimizer example, a verifier would check that none of the optimizations (e.g., push down predicates, join reordering) change the query result.\nDebuggability is the process of fixing the implementation if the verification fails. This involves locating the bug and fixing it. Debuggability is enabled by the hierarchical modularity of software systems. In our example, we can start with the query engine, and identify which component is buggy. Furthermore, within that component, we identify the module, function, and finally the sequence of instructions causing the bug. Then, we fix the bug. This top-down process of locating the bug is enabled by the fact that each component has a clear solution specification.\nA final property of software systems is automated decision-making. This enables a system to make decisions without human supervision. This property is key to addressing use cases where humans are too slow or error-prone. One"}, {"title": "5 Comparing LLM development and software development", "content": "The recent progress of LLMs has sparked the imagination of many with Andrej Karpathy famously stating that English is the hottest new programming language\" [49], which suggests that one can \u201cprogram\u201d LLMs by simply describing tasks in natural language.\nFigure 5 contrasts (a) developing a program for a given task using traditional software engineering with (b) accomplishing the same task using LLMs. With software engineering, a developer takes a task's statement specification and writes a program to implement that task. The program then takes an input and generates an output which can be verified using the solution specification. In contrast, an LLM takes a description (e.g., a high-level statement specification) of the task together with the task's input as a prompt and produces an output. Thus, LLMs have the potential to obviate the need for writing programs and for detailed specifications. When it comes to specifications, LLMs typically trade unambiguity for the ease of writing these specifications in natural language,\nFigure 6 shows a simple task to sort a list of integers, implemented by using both a program and an LLM. Figure 6(a) shows the statement specification in the form of a PRD, Figure 6(b) shows a Python program implementing the PRD, and Figure 6(c) shows a call to the program with input list: 3, 1, 4, 1, 5, 9. In contrast, Figure 6(d) shows an equivalent LLM solution consisting of a prompt that contains both a high-level statement specification of the task (i.e., \"Sort the following numbers in ascending order\") and the same input. Note that none of these cases shows a solution specification.\nWhile the example in Figure 6 works well and shows the full power of LLMs, unfortunately, in many cases the prompts are ambiguous and users don't get the desired answers. We discuss this challenge and approaches to address it next."}, {"title": "6 Task specifications in LLMS", "content": "When it comes to LLMs, we are faced with a conflict between (1) the accessibility and generality of the natural language (i.e., which allows anyone to specify any task), on one hand, and (2) its inherent ambiguity, on the other hand. Next, we examine approaches on how to resolve this conflict by making it easier to write clear (unambiguous) specifications for a growing number of LLM tasks. This will allow us to leverage engineering principles to implement these tasks and provide solutions for an increasing range of applications."}, {"title": "6.1 Are LLM prompts inherently ambiguous?", "content": "With LLMs, we specify tasks using prompts. We say that a prompt is inherently ambiguous if it is very hard, or infeasible, to disambiguate it. As before, we need to differentiate between statement and solution specifications. An example of a prompt that is ambiguous both in terms of statement and solution is \"Write a poem about a white horse in Shakespeare's style\". Such prompt is inherently ambiguous as there is no clear way to disambiguate it. Indeed, the user might not even know what a good poem looks like before seeing one!\nOn the other hand, consider the prompt \"How long does it take to go from Venice to Paris?\". This prompt has an ambiguous statement specification since it is not clear which \u201cVenice\" and which \u201cParis\u201d the user is referring to, i.e., is it the \"Venice\" in Italy or the one near Los Angeles? Is it the \"Paris\" in France or the one in Texas? Does the user intend to travel by car, train, or airplane? This prompt's statement specification is not inherently ambiguous as the user can easily disambiguate it by adding the missing information, as shown in Figure 3(b). However, note that the solution specification of this prompt is still ambiguous as it is not clear how to check whether the answer is correct from the information provided by the prompt alone.\nIn another example (see Figure 7(a)), consider the prompt, \"Write a Java program that counts the number of occurrences of the word \"The\u201d in a file. Similarly, this prompt has an ambiguous statement specification as it doesn't say whether the match should be case sensitive or not (though in this case one could argue that the user's intent was to have a case-sensitive match since she capitalized the first letter in \u201cThe\u201d). Again, the prompt's statement is not inherently ambiguous as a user can easily disambiguate it by adding \u201cMake the comparison case-sensitive\u201d, and, by doing so, she gets the desired result (see Figure 7(b)). At the same time, this prompt doesn't provide a solution specification either. However, one could add such specification by providing some unit tests, e.g., provide files with a known number of \"The\" words and extend the prompt to provide the expected number of \"The\" words for each test file."}, {"title": "7 Disambiguating LLM specifications (prompts)", "content": "As mentioned above, while most of LLM prompts are ambiguous, in many cases this ambiguity is not fundamental; it is merely a reflection of the fact that writing unambiguous specifications is often hard. In this section, we suggest several approaches to make it easier to disambiguate LLM prompts. We do so by drawing our inspiration from how humans deal with ambiguity when they communicate with each other."}, {"title": "7.1 How do humans deal with ambiguity?", "content": "People use natural language every day to describe tasks for others to perform, such as \"Buy one gallon of milk,\" \"Bring one more chair to the table,\" or \"Let's meet at the market hall at 7\" (see Table 2). These examples provide both a statement specification (i.e., what to do) and a solution specification (i.e., enough information to verify whether the task was performed correctly). For example, one can check that \"the milk was indeed bought,\" \"a new chair was brought to the table,\" and \"the person attended the meeting.\"\nHowever, since natural language is inherently ambiguous, the tasks it describes are often ambiguous as well, including the examples above. So, how do people handle this ambiguity? They typically rely on three main approaches.\nFirst, most people's interactions are multi-turn, e.g., dialogs, discussions, etc. This enables people to refine their task's description if ambiguous. If Alice asks Bill to do something, and Bill is confused, he will ask for clarifications. This will give Alice the opportunity to clarify the task's description, and Bill the opportunity to build more context about the task. This pattern is very common. Table 2 shows some examples of clarification questions: \"What kind of milk? What brand? By when do you need it?", "Which chair?": "or \"Which market hall? Which stall in the market hall? 7am or 7pm?\u201d\nSecond, people assume a task-aware context. Such context is built from performing similar tasks in the past or extra information related to the given task. Referring again to the tasks in Table 2, if this is not the first time you are buying milk, you probably already know which kind of milk or which brand you need to buy, so no need to ask. Also, if you know that the milk is needed for baking a cake for dinner, you might already know by when you need to buy the milk.\nThird, people often", "model": "ther people they communicate with. They do so by learning from the repeated interactions with these people, what they like and don't like, what is the most effective way to ask these people to perform a task, etc. We call this, people-aware context. In the example,", "7": "if you know the person asking you this is a late morning person, you can easily infer this must be 7pm not 7am.\nInterestingly, people leverage similar approaches when interacting with a chatbot like ChatGPT as well: they often use multi-turn interactions to refine their asks, assume the prior context from the conversation is available when asking the next question, and learn what the LLMs are good at or they are not got at (e.g., what kind of questions are more likely to produce hallucinations).\nIn summary, humans typically communicate iteratively by assuming some shared context. As such, they often leave details out of the conversation, as these details can be easily filled in via subsequent questions or can be inferred from shared context. Natural language has evolved over thousands of years to let humans communicate with each other efficiently. However, this does not necessarily make it the most effective way to communicate between an LLM and a human, two LLMs, or an LLM and an existing software system (e.g., function calling). Thus, it should come as no surprise that we need new techniques to improve communication in those cases."}, {"title": "7.2 Solutions for prompt disambiguation", "content": "In this section, we discuss several techniques to disambiguate LLM prompts drawing inspiration from how humans disambiguate their communication. Most of the techniques here focus on tasks' statement specifications.\nIterative disambiguation: As mentioned, in real life, people often ask questions to clarify the description of a task provided by another person. One example is a product manager refining a product requirement document (PRD) based on the feedback from a developer: when a developer is confused about the PRD, she asks questions, which typically lead to the program manager adding clarifications or new information to the PRD with the goal of disambiguating it.\nOne natural approach would be to mimic this process by having an LLM ask a human questions to help her disambiguate her prompt (see Figure 3(b)). But how can an LLM figure out whether a prompt is ambiguous? One possibility is to leverage the observation that an ambiguous prompt is more likely to generate different outputs than an unambiguous one.\u00b3 In particular, if an LLM generates multiple outputs for the same ambiguous prompt, the chances are some of these outputs will be different. Consider again our example in which we want to generate a program that counts the number of occurrences of the word \u201cThe\u201d in a particular document. By generating many programs we might end up with programs that provide different outputs depending on whether the comparison is case sensitive or not. By showing the differences between the programs to the user, we can help her realize that she needs to add the case sensitivity matching requirement to the prompt.\nA more advanced approach would not only show the differences between programs to the user, but also suggest how to disambiguate the prompt, e.g., \"Consider specifying whether the matching is case-sensitive or not?\". In another example, consider prompt \"Which is the fastest machine?\u201d. This prompt is also ambiguous. Is it the fastest computer? The fastest car? And, if it is the fastest car, is it the fastest road car? The fastest race car? Or maybe the fastest vehicle period, i.e., the kind of cars using jet engines that are purposely built for land speed records? Again, one way to disambiguate the original prompt is to use one or more LLMs to provide different outputs that expose the ambiguity (see Figure 8).\nYet another approach would be for the LLM to give answers that clearly state the assumptions it made in response to ambiguous prompts. For example, given the question \"How long does it take to go from Venice to Paris?\", an LLM might answer \"The flying time between Venice, Italy and Paris, France is 1h and 50min\u201d, (The words in italic capture the assumptions made by LLM in answering the question.) This will make it easy for the user to check whether this answers the question she had in mind and, if not, improve the prompt specification, e.g. \"How long does it take to drive between Venice, CA to Paris, TX?\u201d (see Figure 3(c)).\nNote the last two approaches might require finetuning or post-training an LLM to either provide suggestions to disambiguate prompts or state the assumptions for its outputs to fill the gaps in the prompts' specifications.\nThere is already an active area of related research in programming languages to identify the user's intent [50, 51, 52]. This research can be a promising base to build on.\nDomain-specific rules: People in the same profession or organization share context in the form of predefined rules or special semantics associated with words or phrases. For example, when a soldier hears from his superior that she should perform a task at 11:00, she knows this is 11am (not 11pm) since the military uses the 24h time format to tell the time. In another example, for software developers, words like \u201cclass\u201d, \u201cloop\u201d, \u201cvariable\u201d have very precise meanings; the same words are ambiguous in a colloquial conversation. Finally, a programming language adds another layer of syntax and semantics which further helps to disambiguate the meaning of statements written in that language.\nIn domains where there are strong rules or operating procedures we can use those rules and procedures to disambiguate the specifications. Examples are the civil or penal codes, standard-operating-procedure manuals in hospitals, labs, etc. Note that new rules are already being proposed specifically for LLMs, such as Constitutional AI [18]. Constitutional AI is a list of rules to align LLMs so that to minimize the harm they can cause. Some areas of research are (1) capturing these rules effectively in the prompt (e.g., select only the rules relevant to the task), (2) enforcing/verifying that these rules were followed by the output, i.e., solution specification. The latter is particularly challenging as most of these rules are not formal. Thus, formalizing these rules when possible is another area of research.\nFinally, another important area of research is defining new domain-specific rules. One example is capturing the laws and constraints in the physical world, e.g., people do not walk upside down unless they are in space, they do not walk through walls, etc. This is related to the ongoing work on world models [53, 54].\""}, {"title": "Monitor, learn & build context", "content": "Humans often create models (context) about the external world, including people they are interacting with. For example, we communicate and explain things differently to children, to our friends, to colleagues, or to our family. In some of these cases, we do so by learning what is the most effective way to communicate with a particular person after repeatedly interacting with that person (see Section Section 7.1). One area of research is to identify the prompts a model is good at (e.g., it excels in generating python programs for data processing) and not good at (e.g., it does a mediocre job on world problems). Also, we can learn the style of the prompt that maximizes the accuracy of model's answer. Based on these learnings, we can change prompts or route prompts to the best model to handle it.\n\"The stranger test\u201d: If there is no shared context available when prompting an LLM, one approach is to provide as much as possible information in the prompt, e.g., just assume the prompt is intended for a stranger. To illustrate this approach, consider a text-to-image task. Even though such a task is inherently ambiguous, we can still disambiguate it substantially by thinking of the prompt as a one-shot specification for a stranger. Consider you want to generate an image of the \"Golden Gate Bridge at sunset\" (see Figure 9(c)). Then you should provide as much information as you can to increase the probability you get the picture you'd likely like from someone who knows nothing about your artistic preferences, e.g., the picture should use pastel colors, there should be realistic proportions between the bridge and buildings, the sun should set opposite to the city, etc. Basically, you will try to externalize the specification you have in mind about how the picture should look like.\nAnother way to externalize such a specification is by showing the user various pictures and asking her to say which picture she likes and which she does not. Based on this, we can infer some user's preferences (e.g., pastel colors vs bright colors, photograph-like pictures vs cartoon-like pictures, etc) and use these preferences to enhance the prompt. Note this is another way of building context.\nStructured outputs: In an effort to improve reliability, many LLMs have started to provide support for describing their outputs and inputs in JSON format[20]. Furthermore, OpenAI has recently introduced \"structured outputs\" [23], a feature that not only ensures the output is in JSON format but that it adheres to a particular JSON schema. Furthermore, Trace [26] enables users to use pseudocode to specify a program and then use an iterative process to generate and refine the program. These constructs are one step closer to providing statement and solution specifications for programming tasks. In particular, a pseudocode provides a statement specification (i.e., it specifies what the task should do) while a schema provides a weak form of solution specification as it can be used to verify whether the output matches the given schema. However, the pseudocode can be still ambiguous, and an output schema captures mostly the syntax and not the output's semantics. As such, most of the disambiguation techniques discussed in this section apply to structured outputs as well. An interesting research direction is taking advantage of the additional structure and application domain (e.g., program synthesis) to develop more specific disambiguation techniques. Such techniques could borrow from and extend traditional formal specification research [55, 56]. Finally, while providing the pseudocde can help providing better solutions, writing such a pseudocode is not easy, especially for complex programs. One research direction here is to find the right balance between the level of detail of the pseudocode and the correctness of the generated program.\nInstruction following: Even if a prompt is unambiguous, it is of little use if the LLM cannot provide a solution for it. As such, the ability of the LLM to follow the prompt's specification is critical to eventually find a solution. Thus, a future direction is to improve an LLM ability to follow a prompt's specification, an already active area of research. This might involve not only test-time compute, but also pre-training and post-training."}, {"title": "8 The five engineering properties in the LLM context", "content": "In this section, we show how we can leverage the five engineering properties in the context of LLMs, and in the process identify some key challenges and suggest several approaches to address them."}, {"title": "8.1 Verifiability: challenges and solutions", "content": "Verifiability refers to the ability to assess whether the task's implementation adheres to the statement specification (e.g., a program's PRD). This reduces to checking that the task's outputs satisfy the solution specification (e.g., unit tests). Consequently, the verification depends strongly on the solution specification. If the solution specification is ambiguous, verification may fail.\nEven when a task has a clear solution specification, assessing whether its implementation works as intended is not always straightforward. This often requires additional context or tools. For instance, if we prompt an LLM to solve the equation \"x\u2074 + 3x \u2212 8 = 0\" (note that here the equation serves as both the statement and the solution specification), a user may have to use a calculator to verify that the generated solution is correctnes (see Figure 10(a)). In another example, an LLM might generate references to reputable documents, which the user may have to cross-check to verify the accuracy of the answer (see Figure 10(b)).\nNext, we propose several directions for future work aimed at improving solution specifications, as opposed to the statement specifications discussed in Section 7.2. Additionally, we suggest exploring the use of external tools to verify a task's implementation effectively.\nProof-carrying-outputs: An unambiguous statement specification is usually insufficient to determine whether an implementation truly satisfies the specification. We also need a solution specification. Consider a code generation task with a formal (unambiguous) statement specification. How can we ensure that the generated code satisfies the"}, {"title": "8.2 Debuggability: challenges and solutions", "content": "When the verification of a task's output concludes that the output is incorrect", "results.\nSelf-consistency": "One research direction is to generate multiple outputs for the same task, outputs that are expected to be consistent, and then check whether they are indeed consistent. Assume a task with a statement specification, but no solution specification. One possibility would be to use the statement specification to generate not only (1) the task's solution, but also (2) the solution specification, and (3) then"}]}