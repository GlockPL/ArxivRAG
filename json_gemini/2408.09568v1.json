{"title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair", "authors": ["Meghdad Dehghan", "Jie JW Wu", "Fatemeh H. Fard", "Ali Ounit"], "abstract": "[Context] Large Language Models (LLMs) have shown good performance in several software development-related tasks such as program repair, documentation, code refactoring, debugging, and testing. Adapters are specialized, small modules designed for parameter efficient fine-tuning of LLMs for specific tasks, domains, or applications without requiring extensive retraining of the entire model. These adapters offer a more efficient way to customize LLMs for particular needs, leveraging the pre-existing capabilities of the large model. Merging LLMs and adapters has shown promising results for various natural language domains and tasks, enabling the use of the learned models and adapters without additional training for a new task.\n[Objective] This research proposes continual merging and empirically studies the capabilities of merged adapters in Code LLMs, specially for the Automated Program Repair (APR) task. The goal is to gain insights into whether and how merging task-specific adapters can affect the performance of APR.\n[Method] In our framework, MergeRepair, we plan to merge multiple task-specific adapters using three different merging methods and evaluate the performance of the merged adapter for the APR task. Particularly, we will employ two main merging scenarios for all three techniques, (i) merging using equal-weight averaging applied on parameters of different adapters, where all adapters are of equal importance; and (ii) our proposed approach, continual merging, in which we sequentially merge the task-specific adapters and the order and weight of merged adapters matter. By exploratory study of merging techniques, we will investigate the improvement and generalizability of merged adapters for APR. Through continual merging, we will explore the capability of merged adapters and the effect of task order, as it occurs in real-world software projects.", "sections": [{"title": "I. INTRODUCTION", "content": "There are many efforts in Software Engineering (SE) to automate code-related tasks and facilitate developers' tasks using deep learning models. Bug prediction [1], bug fixing [2], [3], code generation [4], [5], comment generation [6], and commit message generation [7] are among these tasks. In practice, developers often need to train, test and validate different models for each of these tasks separately along with data collection and pre-processing, feature engineering, hyper-parameter tuning, and so on, to ensure efficient models are developed. However, repeating the process for each new task can be time-consuming and cumbersome. To train a new model that can perform well on a downstream task, there are several requirements, including an (often domain-specific) dataset, and sufficient computational resources. Prior research has shown that these requirements are not trivial to fulfill [8]. First, for each new task, a new model should be trained and deployed, which is costly. Additionally, by following this process, we will end up with several task-specific models, which are unable to leverage the strengths of other models, while they could benefit from related tasks to improve the performance within or outside their domain [9], [10].\nThe main question that we intend to investigate in this study is \u201cCan we re-use previously trained code-related adapters for a new SE task by merging them?\u201d Adapters are small modules used in Parameter-Efficient Fine-Tuning (PEFT) [11] of LLMs and have results on par with or better than fully fine tuning language models [12], [13]. The idea behind our study is rooted in the foundations of transfer learning, where the learned knowledge of a language model can be adapted to new tasks, domains, or languages [14]. The advantages of transfer learning in software engineering and code intelligence, have been shown by several recent works adopting Large Language Models (LLM) [15], [13]. Although fine-tuning is one of the main approaches for using language models, there are concerns about using several task-specific adapters or models. The issue of catastrophic forgetting [16] is another concern when fine-tuning a language model. Additionally, fully fine-tuning LLMS [17] becomes inefficient as multiple instances of the model for different tasks should be trained and deployed.\nTo address the above-mentioned issues, recent research shows the efficiency of merging models. Merging models enhances their performance compared to task-specific models [9], [10]. Merging models combines the parameters of individual task-specific models into a single model in order to leverage the knowledge of other models [9]. Note that merging models is intrinsically different from Multi-Task Learning (MTL) [18], [19]. In MTL, one model is jointly trained on two or more tasks; while in merging models, there exist multiple models, each of them trained separately on distinct tasks. Then, all of these models are merged together without additional training.\nSimilar techniques are developed for merging adapters. There are studies that merge multiple models [20], [21], [22] or adapters in natural language processing (NLP) domain [23], [24]. Adapters were also used for Multi-Task Learning [25]. Others adopted the idea of mixture-of-experts to inject a set of expert adapter layers in the transformer-based models in order to train multi-task models [26]. However, the research on merging adapters is scarce, and there is a research gap in investigating the merging ability of adapters for code-related tasks. This is specially important with the advent of Code LLMs. Though these models have shown promising results for many SE tasks, their computational cost is not negligible for companies and researchers. Thus, it is beneficial to re-use the trained models/adapters for new tasks by merging them, without additional training. There is no research that investigates if the current approaches affect the code-related tasks in the same way as in NLP and whether they would improve the performance of each task in the merged model.\nThe main goal of this study is to explore the performance of merged task-specific adapters and continual merging, in the context of Automated Program Repair (APR) task. We plan to investigate the idea of merging adapters for different SE tasks, using our framework, MergeRepair. First, we examine three merging techniques from the existing literature, being weight-space averaging[27], TIES-Merging [28] and DARE [22], in which all adapters are of equal importance in the merged model. Second, we explore our proposed merging paradigm, continual merging using all three merging techniques (i.e., weight-space averaging, TIES-Merging, and DARE). In continual merging, the order and weights of the adapters play an important factor, and the merged adapters do not have equal importance.\nTo conduct experiments, we will utilize the Low-Rank Adaptation (LoRA) [29] as adapter modules and train one instance of LoRA per task. LoRA optimizes low-rank decomposition of the weight matrices and, in this way, can achieve comparable results to fully fine-tuning [12], [13]. In our framework, MergeRepair, we will explore the merging capability of adapters injected in Code LLMs to improve the performance of a selected task. Our selected task is Automated Program Repair (APR), which is an active research area aiming to reduce the manual effort for software developers [30], and facilitates software development and maintenance [31]. Specifically, APR automates the process of generating the fixed code or patch, given code that contains bugs. We choose Development, Test, Improvement, and Misc tasks from the CommitPackFT dataset [32] for our study, as merging tasks to APR. This study could inspire researchers and practitioners to conduct similar work to reduce the effort of training new adapters and models and taking advantage of the existing ones for a new task. As merging models are a fairly new trend, we anticipate this work could pave the ground for new research avenues in this direction."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Automatic Program Repair has a long research background in the field of software engineering. Prior learning-based studies on program repair involve training deep neural nets to map the input buggy code to the fixed code snippet [3], [33]. With the advent of pre-trained language models, which have demonstrated promising performance for transfer learning, some studies have utilized these models to transfer the general knowledge of pre-trained language models to program repair task [34], [35]. By the emergence of larger language models, the research towards adapting these general-purpose models to downstream tasks has shifted to instruction-tuning these models to align them with the target domain of interest. This approach has been adopted in software engineering and there are works that study its potential for program repair [32], [36]. In this research, to develop the task-specific models, we will train adapters using instruction-tuning datasets and evaluate the models, containing the merged adapter, on the APR task. Although language models and PEFT methods are studied for APR in previous works [15], there is no current research that leverages the combination of similar task-specific adapters in order to study the performance of the APR task.\nAdapters, as parameter-efficient fine-tuning techniques, have emerged as a technique to optimize memory usage by reducing the number of trainable parameters during fine-tuning [11]. They have shown promising results compared to fully fine-tuned models while efficiently adapting large language models to downstream tasks. Subsequently, several adapter architectures have been proposed, including LoRA [29] and IA3 [37], each targeting specific layers and parameters of the transformer-based models. There are empirical studies on the utilization of adapters for SE tasks [8], [38]. Notably, recent studies have adopted adapters as a primary alternative method for efficiently fine-tuning large language models in specialized domains such as program repair [15].\nIn this study, we opt for LoRA as the adapter for fine-tuning the models across all target tasks. LoRA stands out as one of the most prevalent and widely used methods in the research community [12], [13].\nMerging models involves combining the parameters of two or more models, each trained on distinct domains/tasks, to create a unified model capable of addressing multiple tasks or domains [20]. Recent work has demonstrated that merging Llama2-7b-chat, a general-purpose chat model, with Meditron-7b, specialized for the medical domain, resulted in a merged model that outperformed its constituent models across both general and medical benchmarks [9]. The increasing number of merged models on the Open LLM leaderboard [39] further proves the success of applying this approach to various benchmarks. Several studies have proposed different merging techniques, such as weight-space averaging [27],"}, {"title": "III. RESEARCH QUESTIONS", "content": "The main goal of this study is to investigate the performance of merged task-specific adapters on the APR task. We will use two main scenarios for merging adapters, as shown in Figure 1 (explanation of each scenario is provided in Section V). To this end, we aim to answer the following research questions:\nRQ1: Does merging APR adapter with other task-specific adapters improve the performance of APR?\nMotivation: We are mainly interested to see whether the performance of APR task will be degraded or improved if we merge APR adapter with non-APR adapters.\nPrior studies have shown that merging adapters can improve the performance of single tasks in other domains [9]. In this RQ, we intend to investigate the potential improvement of merged adapters on the APR task, when APR is merged with four tasks of Development, Misc, Test & QA, and Improvement. The alternative way to achieve better performance having an already fine-tuned model, is to continue the fine-tuning process with additional data and computation. The potential improvement using merging adapters is essentially interesting because the merging process of various adapters is computationally cheaper than further fine-tuning of models. To study this RQ thoroughly, we start by merging APR with only one task and evaluate it on the APR benchmark. We will continue merging APR adapters with two, three, and all four tasks and assess the performance of the merged adapters on APR. In such a manner, we consider all subsets of four adapters merged with APR adapter.\nRQ2: Can merged adapters of other tasks generalize to APR?\nMotivation: Our main interest for this RQ is to understand if the performance of other task-specific adapters could be maintained for APR task when merged together.\nTraining a Code LLM on a new task is costly, and one of the goals of merging adapters is to expand the model to new tasks (i.e., out of domain data) without additional training [9]. Hence, we aim to analyze the generalizability of merged adapters to out-of-domain data. Particularly, we merge non-APR adapters together and evaluate them on the APR benchmark. For merging task-specific adapters, similar to RQ1, we start with adapters trained on a single task and experiment with merging up to four adapters.\nRQ3: How does continual merging of other task-specific adapters with the APR adapter influence the performance of APR?\nMotivation: In real-world software projects, accessing several task-adapters at a time is less applicable; instead, new datasets and tasks will be available as the project evolves. In such situations, the ability to adapt the merged adapter to new data and tasks becomes crucial.\nTo this end, we will investigate the continual learning capacity of merged adapters in this RQ, using our proposed"}, {"title": "IV. DATASETS", "content": "We plan to utilize the CommitPackFT dataset released by the OctoPack study [32]. This dataset is a refined version of the full CommitPack dataset, filtered by its publishers to make it suitable for instruction-tuning Code LLMs [32]. CommitPackFT contains 277 programming languages and 2GB of memory [32]. These two datasets contain code samples from GitHub repositories, including the code before and after the commit change. In particular, the dataset includes three main fields: commit messages, old contents (the file content before the commit), and new contents (the file content after the commit). The commit messages are the instructions used when instruction-tuning the models. Generally, these commits enhance the previous version of the code. A portion of the commits are related to bug-fixing commits tailored for the program repair task.\nThe Python split of the CommitPackFT dataset has been classified into five tasks by the publishers of the dataset. This classification shows the task information for each commit, which is done by using 1-shot prompting with the GPT-4 model. The task classification for other programming languages is not available. Therefore, our experiments are limited to the Python split of the dataset. shows the available tasks in the Python dataset. This dataset is used to train the task-specific adapters. We will conduct all the experiments for the Python split of the dataset, which contains 59, 113 samples."}, {"title": "V. EXECUTION PLAN", "content": "An overview of our approach, MergeRepair, is shown in Figure 1. First, we will train one instance of LoRA adapter for each task in the tuning phase. Then, in the merging phase, we will merge these task-specific adapters using two merging scenarios/paradigms: weight-space merging and continual merging. For the weight-space merging, we merge the parameters of all task-specific adapters with equivalent weights (i.e., influence) to investigate the potential improvement on APR (RQ1). This is done using three merging techniques. Subsequently, we merge non-APR adapters, again with equal weights, and evaluate the merged adapter on the APR task to study the generalizability to APR (RQ2). For the continual merging scenario, we aim to simulate the continual learning capability of merged adapters in which we sequentially add new task-specific adapters to the previous merged adapter (RQ3). In the following sections, we will explain the continual merging, merging methods, tasks, models, steps for each RQ, and evaluation metrics that we will employ to study the above-mentioned RQs.\nLet the parameters of the Code LLM be denoted by \\(\\theta_M\\). We will keep these parameters unchanged during instruction-tuning. For each target task, we will have a separate LORA adapter on top of the frozen model trained on that task. We refer to these trained adapters as task-specific adapters. Formally, having n available tasks denoted by \\(T_i = \\{T_1, T_2, ..., T_n\\}\\), we will also have n adapters, each with distinct weight parameter values. We define \\(I_{T_i}\\) as the influence of adapter \\(T_i\\) in the merged adapter.\nFor continual merging, we will maintain a single adapter as the merged adapter. Subsequently, we will merge the parameters of the current merged adapter with one new task-specific adapter at a time. Therefore, starting from the first task, i.e., \\(T_1\\) we will have \\(I_{T_1} = 1\\) in the first step. In the second step, we will add the second task-specific adapter, i.e., \\(T_2\\), resulting in \\(I_{T_1} = \\frac{2}{3}\\) and \\(I_{T_2} = \\frac{1}{3}\\) in the merged adapter. In the final step, n, the influence of all previously added adapters to the merged adapter will be similar to a geometric progression with \\(I_{T_n} = \\frac{1}{3}\\) for the last adapter and ratio of \\(\\frac{2}{3}\\). More formally, the final influence of task-specific adapters in the merged adapter, starting from \\(T_1\\) and ending with \\(T_n\\), would be acquired by \\(I_{T_i} = \\frac{1}{2^{n-i+1}-1}\\) for \\(i = \\{2,3,..., n\\}\\) and \\(I_{T_i} = \\frac{1}{2^{n-1}} \\) for \\(i = 1\\), since we consider the first adapter as the merged adapter in the first step.\nConsequently, in this scenario, the influence of the parameters of the previous task-adapters in the merged adapter will decrease, as we reach the final (i.e., n-th) adapter. Intuitively, the merged adapter attends to the new task while retaining less knowledge about the previous tasks.\nFor the continual merging scenario, different orders of task-specific adapters will be considered. The order of adding task-specific adapters is important as it will lead to different weight values for the parameters of the merged adapter. Having n different adapters, the total number of their permutations would be equal to n!. Hence, to adhere to the computational resources available to us, for the continual merging scenario, we will merge APR adapter with up to three other task-specific adapters, totaling 4! = 24 experiments required for each Code LLM.\nWe will experiment with three merging methods, all compatible with LoRA adapter that we aim to employ. Weight-space averaging is selected as the baseline merging method."}, {"title": "VI. RESULTS", "content": "The current document is submitted as a Registered Report. The results will be explained at a later stage. To further analyze the results, we will conduct two sets of analyses. One is aimed to study the internal adapter parameters of the models using the fraction of sign difference [24], i.e., the portion of parameters having opposite signs, of the participating adapters in the merging process. The sign of the parameters of the adapters is crucial as it is involved either indirectly or directly in the merging process, e.g., in TIES-Merging.\nAs another analysis, we will filter out the correct and incorrect generated samples of the models for the experiments. Then, we will study the correct-to-incorrect ratio of the generated samples when merging task-specific adapters. In particular, we will generate the output samples of the benchmark dataset using each task-specific adapter and analyze the semantic similarity of the samples whose correct/incorrect status changed during the merging process.\nSince the total number of experiments and evaluations will be significant in the entire study, we will apply this experiment for only some merging techniques, based on the obtained results."}, {"title": "VII. THREATS TO VALIDITY", "content": "Threats to internal validity refer to internal factors that might affect the reliability of the results. These factors in our study are related to model checkpoints and the dataset used. The model checkpoints that we will select are pre-trained without any further fine-tuning process such as instruction-tuning. Hence, the instruction-tuning process will align both models to generate more proper output code snippets. In all cases, we will use the best hyper-parameters and configurations, as recommended by the model developers. As we use the open source code and APIs, we anticipate low threat related to this factor. The other potential threat concerns the dataset used in our experiments. Although the task type of the records in the dataset has been obtained by prompting GPT-4 model, and might include incorrect labels, the same dataset is used for all experiments and models. Therefore, the results are affected in the same way by the used dataset. Please note that such noisy labels might exist in other datasets as well, and their effect needs to be evaluated in a separate study, which is out of the scope of our current study. Another threat can be related to the effect of model architecture and the model size. We do not conduct experiments on the effect of model architecture on the results, and we choose same size models, to reduce the related threats. It is note worthy that our results are more concerned with the merging capability of the trained adapters, and two models are chosen to reduce the bias related to the model.\nThreats to external validity relate to the generalizability of our findings. In this work, we conduct experiments only on automatic program repair, along with the other four tasks for Python language. Although the techniques used in the merged models/adapters can be adopted for other tasks and languages, the obtained results are limited to the used tasks and programming language and might not be applicable to other areas or languages.\nThreats to construct validity refers to misalignment among the test and what needs to be measured. In our work, we intend to compare the results for APR task, which is evaluated using pass@k metric. We found this metric to be more reliable than others, as in the benchmark, the fixed codes generated by the models will be evaluated by test cases.\nLastly, as the models are frozen and we keep all the models' configurations the same, the results are expected to be related to the merging techniques. We use different evaluation metrics, supported by statistical tests and additional analysis. Therefore, we expect low threats to the conclusion validity."}]}