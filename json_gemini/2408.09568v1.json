{"title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair", "authors": ["Meghdad Dehghan", "Jie JW Wu", "Fatemeh H. Fard", "Ali Ounit"], "abstract": "[Context] Large Language Models (LLMs) have shown good performance in several software development-related tasks such as program repair, documentation, code refactoring, debugging, and testing. Adapters are specialized, small modules designed for parameter efficient fine-tuning of LLMs for specific tasks, domains, or applications without requiring extensive retraining of the entire model. These adapters offer a more efficient way to customize LLMs for particular needs, leveraging the pre-existing capabilities of the large model. Merging LLMs and adapters has shown promising results for various natural language domains and tasks, enabling the use of the learned models and adapters without additional training for a new task. [Objective] This research proposes continual merging and empirically studies the capabilities of merged adapters in Code LLMs, specially for the Automated Program Repair (APR) task. The goal is to gain insights into whether and how merging task-specific adapters can affect the performance of APR. [Method] In our framework, MergeRepair, we plan to merge multiple task-specific adapters using three different merging methods and evaluate the performance of the merged adapter for the APR task. Particularly, we will employ two main merging scenarios for all three techniques, (i) merging using equal-weight averaging applied on parameters of different adapters, where all adapters are of equal importance; and (ii) our proposed approach, continual merging, in which we sequentially merge the task-specific adapters and the order and weight of merged adapters matter. By exploratory study of merging techniques, we will investigate the improvement and generalizability of merged adapters for APR. Through continual merging, we will explore the capability of merged adapters and the effect of task order, as it occurs in real-world software projects.", "sections": [{"title": "I. INTRODUCTION", "content": "There are many efforts in Software Engineering (SE) to automate code-related tasks and facilitate developers' tasks using deep learning models. Bug prediction [1], bug fixing [2], [3], code generation [4], [5], comment generation [6], and commit message generation [7] are among these tasks. In practice, developers often need to train, test and validate different models for each of these tasks separately along with data collection and pre-processing, feature engineering, hyper-parameter tuning, and so on, to ensure efficient models are developed. However, repeating the process for each new task can be time-consuming and cumbersome. To train a new model that can perform well on a downstream task, there are several requirements, including an (often domain-specific) dataset, and sufficient computational resources. Prior research has shown that these requirements are not trivial to fulfill [8]. First, for each new task, a new model should be trained and deployed, which is costly. Additionally, by following this process, we will end up with several task-specific models, which are unable to leverage the strengths of other models, while they could benefit from related tasks to improve the performance within or outside their domain [9], [10].\nThe main question that we intend to investigate in this study is \u201cCan we re-use previously trained code-related adapters for a new SE task by merging them?\u201d Adapters are small modules used in Parameter-Efficient Fine-Tuning (PEFT) [11] of LLMs and have results on par with or better than fully fine tuning language models [12], [13]. The idea behind our study is rooted in the foundations of transfer learning, where the learned knowledge of a language model can be adapted to new tasks, domains, or languages [14]. The advantages of transfer learning in software engineering and code intelligence, have been shown by several recent works adopting Large Language Models (LLM) [15], [13]. Although fine-tuning is one of the main approaches for using language models, there are concerns about using several task-specific adapters or models. The issue of catastrophic forgetting [16] is another concern when fine-tuning a language model. Additionally, fully fine-tuning LLMS [17] becomes inefficient as multiple instances of the model for different tasks should be trained and deployed.\nTo address the above-mentioned issues, recent research shows the efficiency of merging models. Merging models enhances their performance compared to task-specific models [9], [10]. Merging models combines the parameters of individual task-specific models into a single model in order to leverage the knowledge of other models [9]. Note that merging models is intrinsically different from Multi-Task Learning (MTL) [18], [19]. In MTL, one model is jointly trained on two or more tasks; while in merging models, there exist"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Automatic Program Repair has a long research background in the field of software engineering. Prior learning-based studies on program repair involve training deep neural nets to map the input buggy code to the fixed code snippet [3], [33]. With the advent of pre-trained language models, which have demonstrated promising performance for transfer learning, some studies have utilized these models to transfer the general knowledge of pre-trained language models to program repair task [34], [35]. By the emergence of larger language models, the research towards adapting these general-purpose models to downstream tasks has shifted to instruction-tuning these models to align them with the target domain of interest. This approach has been adopted in software engineering and there are works that study its potential for program repair [32], [36]. In this research, to develop the task-specific models, we will train adapters using instruction-tuning datasets and evaluate the models, containing the merged adapter, on the APR task. Although language models and PEFT methods are studied for APR in previous works [15], there is no current research that leverages the combination of similar task-specific adapters in order to study the performance of the APR task."}, {"title": "B. Adapters", "content": "Adapters, as parameter-efficient fine-tuning techniques, have emerged as a technique to optimize memory usage by reducing the number of trainable parameters during fine-tuning [11]. They have shown promising results compared to fully fine-tuned models while efficiently adapting large language models to downstream tasks. Subsequently, several adapter architectures have been proposed, including LoRA [29] and IA3 [37], each targeting specific layers and parameters of the transformer-based models. There are empirical studies on the utilization of adapters for SE tasks [8], [38]. Notably, recent studies have adopted adapters as a primary alternative method for efficiently fine-tuning large language models in specialized domains such as program repair [15].\nIn this study, we opt for LoRA as the adapter for fine-tuning the models across all target tasks. LoRA stands out as one of the most prevalent and widely used methods in the research community [12], [13]."}, {"title": "C. Merging", "content": "Merging models involves combining the parameters of two or more models, each trained on distinct domains/tasks, to create a unified model capable of addressing multiple tasks or domains [20]. Recent work has demonstrated that merging Llama2-7b-chat, a general-purpose chat model, with Meditron-7b, specialized for the medical domain, resulted in a merged model that outperformed its constituent models across both general and medical benchmarks [9]. The increasing number of merged models on the Open LLM leaderboard [39] further proves the success of applying this approach to various benchmarks. Several studies have proposed different merging techniques, such as weight-space averaging [27],"}, {"title": "III. RESEARCH QUESTIONS", "content": "The main goal of this study is to investigate the performance of merged task-specific adapters on the APR task. We will use two main scenarios for merging adapters, as shown in Figure 1 (explanation of each scenario is provided in Section V). To this end, we aim to answer the following research questions:\nRQ1: Does merging APR adapter with other task- specific adapters improve the performance of APR? Motivation: We are mainly interested to see whether the performance of APR task will be degraded or improved if we merge APR adapter with non-APR adapters.\nPrior studies have shown that merging adapters can improve the performance of single tasks in other domains [9]. In this RQ, we intend to investigate the potential improvement of merged adapters on the APR task, when APR is merged with four tasks of Development, Misc, Test & QA, and Improvement. The alternative way to achieve better performance having an already fine-tuned model, is to continue the fine-tuning process with additional data and computation. The potential improvement using merging adapters is essentially interesting because the merging process of various adapters is computationally cheaper than further fine-tuning of models. To study this RQ thoroughly, we start by merging APR with only one task and evaluate it on the APR benchmark. We will continue merging APR adapters with two, three, and all four tasks and assess the performance of the merged adapters on APR. In such a manner, we consider all subsets of four adapters merged with APR adapter.\nRQ2: Can merged adapters of other tasks generalize to APR? Motivation: Our main interest for this RQ is to understand if the performance of other task-specific adapters could be maintained for APR task when merged together.\nTraining a Code LLM on a new task is costly, and one of the goals of merging adapters is to expand the model to new tasks (i.e., out of domain data) without additional training [9]. Hence, we aim to analyze the generalizability of merged adapters to out-of-domain data. Particularly, we merge non-APR adapters together and evaluate them on the APR benchmark. For merging task-specific adapters, similar to RQ1, we start with adapters trained on a single task and experiment with merging up to four adapters.\nRQ3: How does continual merging of other task-specific adapters with the APR adapter influence the performance of APR? Motivation: In real-world software projects, accessing several task-adapters at a time is less applicable; instead, new datasets and tasks will be available as the project evolves. In such situations, the ability to adapt the merged adapter to new data and tasks becomes crucial.\nTo this end, we will investigate the continual learning capacity of merged adapters in this RQ, using our proposed"}, {"title": "IV. DATASETS", "content": "We plan to utilize the CommitPackFT dataset released by the OctoPack study [32]. This dataset is a refined version of the full CommitPack dataset, filtered by its publishers to make it suitable for instruction-tuning Code LLMs [32]. CommitPackFT contains 277 programming languages and 2GB of memory [32]. These two datasets contain code samples from GitHub repositories, including the code before and after the commit change. In particular, the dataset includes three main fields: commit messages, old contents (the file content before the commit), and new contents (the file content after the commit). The commit messages are the instructions used when instruction-tuning the models. Generally, these commits enhance the previous version of the code. A portion of the commits are related to bug-fixing commits tailored for the program repair task.\nThe Python split of the CommitPackFT dataset has been classified into five tasks by the publishers of the dataset. This classification shows the task information for each commit, which is done by using 1-shot prompting with the GPT-4 model. The task classification for other programming languages is not available. Therefore, our experiments are limited to the Python split of the dataset. Table I shows the available tasks in the Python dataset. This dataset is used to train the task-specific adapters. We will conduct all the experiments for the Python split of the dataset, which contains 59, 113 samples."}, {"title": "V. EXECUTION PLAN", "content": "An overview of our approach, MergeRepair, is shown in Figure 1. First, we will train one instance of LoRA adapter for each task in the tuning phase. Then, in the merging phase, we will merge these task-specific adapters using two merging scenarios/paradigms: weight-space merging and continual merging. For the weight-space merging, we merge the parameters of all task-specific adapters with equivalent weights (i.e., influence) to investigate the potential improvement on APR (RQ1). This is done using three merging techniques. Subsequently, we merge non-APR adapters, again with equal weights, and evaluate the merged adapter on the APR task to study the generalizability to APR (RQ2). For the continual merging scenario, we aim to simulate the continual learning capability of merged adapters in which we sequentially add new task-specific adapters to the previous merged adapter (RQ3). In the following sections, we will explain the continual merging, merging methods, tasks, models, steps for each RQ, and evaluation metrics that we will employ to study the above-mentioned RQs."}, {"title": "A. Continual Merging", "content": "Let the parameters of the Code LLM be denoted by $\\theta_M$. We will keep these parameters unchanged during instruction-tuning. For each target task, we will have a separate LORA adapter on top of the frozen model trained on that task. We refer to these trained adapters as task-specific adapters. Formally, having n available tasks denoted by $T_i = \\{T_1, T_2, ..., T_n\\}$, we will also have n adapters, each with distinct weight parameter values. We define $I_{T_i}$ as the influence of adapter $T_i$ in the merged adapter.\nFor continual merging, we will maintain a single adapter as the merged adapter. Subsequently, we will merge the parameters of the current merged adapter with one new task-specific adapter at a time. Therefore, starting from the first task, i.e., $T_1$ we will have $I_{T_1} = 1$ in the first step. In the second step, we will add the second task-specific adapter, i.e., $T_2$, resulting in $I_{T_1} = \\frac{2}{3}$ and $I_{T_2} = \\frac{1}{3}$ in the merged adapter. In the final step, n, the influence of all previously added adapters to the merged adapter will be similar to a geometric progression with $I_{T_n} = \\frac{1}{3}$ for the last adapter and ratio of $\\frac{2}{3}$. More formally, the final influence of task-specific adapters in the merged adapter, starting from $T_1$ and ending with $T_n$, would be acquired by $I_{T_i} = \\frac{1}{2^{n+1-i}}$ for $i = \\{2,3,..., n\\}$ and $I_{T_i} = \\frac{2^{n-1}}{2^n}$ for i = 1, since we consider the first adapter as the merged adapter in the first step.\nConsequently, in this scenario, the influence of the parameters of the previous task-adapters in the merged adapter will decrease, as we reach the final (i.e., n-th) adapter. Intuitively, the merged adapter attends to the new task while retaining less knowledge about the previous tasks.\nFor the continual merging scenario, different orders of task-specific adapters will be considered. The order of adding task-specific adapters is important as it will lead to different weight values for the parameters of the merged adapter. Having n different adapters, the total number of their permutations would be equal to n!. Hence, to adhere to the computational resources available to us, for the continual merging scenario, we will merge APR adapter with up to three other task-specific adapters, totaling 4! = 24 experiments required for each Code LLM."}, {"title": "B. Merging Methods", "content": "We will experiment with three merging methods, all compatible with LoRA adapter that we aim to employ. Weight-space averaging is selected as the baseline merging method."}, {"title": "C. Tasks", "content": "Table I shows all the tasks, including Development, Misc, Test & QA, and Improvement, along with the number of their samples. The \"Bug Fixes\" portion of this data is associated with the APR task and forms 19.05% of the dataset. The Misc category includes code samples from different domains, including configuration, dependencies, and documentation [32]. For all these tasks we will use instruction-tuning data to fine-tune the Code LLMs using LoRA. This training provides the task-specific adapters for our merging experiments. The pre-trained models, without instruction-tuning, will generate open-ended outputs that are not suitable for production use. This process aims to adapt the pre-trained models to generate human-aligned outputs given an instruction and an input/output pair of code samples [40]."}, {"title": "D. Models", "content": "We will use two state-of-the-art Code LLMs, StarCoder2 and Granite, for our experiment. These models are chosen as they are more recent than popular CodeLLMs like CodeLlama and Mistral. More importantly, StarCoder2 and Granite have the best performance for APR task. Both of these models are the same size, with three billion parameters each. Their checkpoints and source code are publicly available, allowing us to inject low-rank matrices of LoRA layers into the decoder blocks of these models and train only the added parameters. Another reason to choose StarCoder2 and Granite over other CodeLLMs is our restricted computational capacity. We ran experiments fine-tuning the two models using LoRA with a configuration that is reported by the models' authors to ensure the feasibility of the research and its correctness. This choice is not feasible for us with CodeLlama and Mistral."}, {"title": "E. Steps for Each RQ", "content": "For RQ1, we use merging with equal-weight merging (Figure 1) and add the other four tasks one by one to be merged with APR. As the order of tasks is not important in this part, we add tasks one by one to the APR adapter and compare the results with APR adapter on the benchmark dataset (see Section V-F). We will apply this process for each of the three merging methods, weight-space averaging, TIES-Merging, and DARE, separately and report the results.\nFor RQ2, we use the same process as in RQ1, however, we do not add the APR adapter in the merged adapter. Instead, we just test the merged adapter on APR to evaluate the generalizability of the merged adapter. We will report the results for each of the three merging methods and the combinations of the task-specific adapters.\nFor RQ3, we will use the continual merging (Figure 1 bottom of merging phase). For this purpose, we add the task-specific adapters one by one to build the merged adapter. In"}, {"title": "F. Evaluation Metrics", "content": "For all experiments we will report the pass@k and RobustPass@k scores of the models evaluated on the HumanEvalFix benchmark [32]. This benchmark is created as part of the HumanEvalPack benchmarks released by the same research. They manually produce bugs in the extended version of the original HumanEval benchmark which supports six programming languages, including Python.\nCompared to previous similarity-based metrics such as BLEU and CodeBLEU, execution-based metrics such as pass@k and RobustPass@k are more reliable to capture the functional correctness of generated codes. However, to calculate these scores, accessing to a dataset containing unit tests for all samples is required.\nIn pass@k metric, to check the correctness of the generated code samples, one of the generated candidates should pass all the test cases of the corresponding sample in the dataset. More formally, by sampling k candidates for each record or problem of the dataset among all n generated candidates, the pass@k metric could be calculated as follows:\n$\\text{pass@k} := E_{\\text{Problems}}\\left[1 - \\frac{\\binom{n}{k} - \\binom{c}{k}}{\\binom{n}{k}}\\right]$\n in which c is the number of correct samples that pass all of the unit tests of each specific problem. We will report pass@1 and pass@10 similar to previous works [32].\nCode LLMs have been shown to be sensitive to perturbations introduced in input prompts, which affects the reliability of the pass@k metric. Therefore, RobustPasss @k (RPs@k) [46] was proposed to evaluate the code generation robustness of Code LLMs. Following [46], we will introduce random perturbations in the HumanEvalFix benchmark to evaluate the models in a more robust manner that reflects practical applications To calculate the RobustPass@k metric, s random perturbations are introduced in the input prompt. n candidates are generated for each input prompt, resulting in nx s generated candidates named fi(xj), where 1 \u2264 i \u2264 n and 1 \u2264 j \u2264 s. Finally, the RobustPass@k metric is defined as below. Here, $rcs(x) = \\sum_{i=1}^{s} C_{i,s}(x)$ represents the worst-case correction for problem x. For each i, $C_{i,s}(x) = 1$ if and"}, {"title": "VI. RESULTS", "content": "The current document is submitted as a Registered Report. The results will be explained at a later stage. To further analyze the results, we will conduct two sets of analyses. One is aimed to study the internal adapter parameters of the models using the fraction of sign difference [24], i.e., the portion of parameters having opposite signs, of the participating adapters in the merging process. The sign of the parameters of the adapters is crucial as it is involved either indirectly or directly in the merging process, e.g., in TIES-Merging.\nAs another analysis, we will filter out the correct and incorrect generated samples of the models for the experiments. Then, we will study the correct-to-incorrect ratio of the generated samples when merging task-specific adapters. In particular, we will generate the output samples of the benchmark dataset using each task-specific adapter and analyze the semantic similarity of the samples whose correct/incorrect status changed during the merging process. Since the total number of experiments and evaluations will be significant in the entire study, we will apply this experiment for only some merging techniques, based on the obtained results."}, {"title": "VII. THREATS TO VALIDITY", "content": "Threats to internal validity refer to internal factors that might affect the reliability of the results. These factors in our study are related to model checkpoints and the dataset used. The model checkpoints that we will select are pre-trained without any further fine-tuning process such as instruction-tuning. Hence, the instruction-tuning process will align both models to generate more proper output code snippets. In all cases, we will use the best hyper-parameters and configurations, as recommended by the model developers. As we use the open source code and APIs, we anticipate low threat related to this factor. The other potential threat concerns the dataset used in our experiments. Although the task type of the records in the dataset has been obtained by prompting GPT-4 model, and might include incorrect labels, the same dataset is used for all experiments and models. Therefore, the results are affected in the same way by the used dataset. Please note that such noisy labels might exist in other datasets as well, and their effect needs to be evaluated in a separate study, which is out of the scope of our current study. Another threat can be related to the effect of model architecture and the model size. We do not conduct experiments on the effect of model architecture on the results, and we choose same size models, to reduce the related threats. It is note worthy that our results are more concerned with the merging capability of the trained adapters, and two models are chosen to reduce the bias related to the model.\nThreats to external validity relate to the generalizability of our findings. In this work, we conduct experiments only on automatic program repair, along with the other four tasks for Python language. Although the techniques used in the merged models/adapters can be adopted for other tasks and languages, the obtained results are limited to the used tasks and programming language and might not be applicable to other areas or languages.\nThreats to construct validity refers to misalignment among the test and what needs to be measured. In our work, we intend to compare the results for APR task, which is evaluated using pass@k metric. We found this metric to be more reliable than others, as in the benchmark, the fixed codes generated by the models will be evaluated by test cases.\nLastly, as the models are frozen and we keep all the models' configurations the same, the results are expected to be related to the merging techniques. We use different evaluation metrics, supported by statistical tests and additional analysis. Therefore, we expect low threats to the conclusion validity."}]}