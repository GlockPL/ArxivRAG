{"title": "Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera", "authors": ["Hiroki Tanioka", "Tetsushi Ueta", "Masahiko Sano"], "abstract": "The performance of ChatGPT and other LLMs has improved tremendously, and in online environments, they are increasingly likely to be used in a wide variety of situations, such as ChatBot on web pages, call center operations using voice interaction, and dialogue functions using agents. In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots. In this multimodal dialogue, mutual emotion recognition between the AI and the user will become important. So far, there have been methods for expressing emotions on the part of the AI agent or for recognizing them using textual or voice information of the user's utterances, but methods for Al agents to recognize emotions from the user's facial expressions have not been studied. In this study, we examined whether or not LLM-based AI agents can interact with users according to their emotional states by capturing the user in dialogue with a camera, recognizing emotions from facial expressions, and adding such emotion information to prompts. The results confirmed that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry.", "sections": [{"title": "I. INTRODUCTION", "content": "In August 2024, some teams of Center for Administration of Information Technology (abbreviated: AIT center) in Tokushima University will be moving out. Since the move will be to a different building, it was discussed to put up a sign or other information in the original room. Ideas for the signs included noting the location of the new rooms and a QR code for a Web page that would provide detailed directions. The idea is that it is effective to use both analog as well as digital methods. However, we are the AIT center that should develop the most digital services within the university. The initial motivation for this study is to take this opportunity one step further and open an online contact point.\nIn order to set up an online window, digital devices (terminals) such as tablets are essential. Some suggested that a university character (Tokupon) could serve as a receptionist on standby. If it goes that far, it would be like a receptionist robot. To realize this, a tablet terminal or similar device should be set up in front of the room before the move, ready for online meetings. Online meetings can be conducted remotely by a representative waiting in the new room or by a pseudo-Artificial Intelligence agent (AI agent). Therefore, we considered using ChatGPT one of the representatives of Large Language Models (LLMs), as the AI agent.\nIn this case, the person in charge of interacting remotely can see the other person's facial expression while conversing if the conversation is conducted in an online conference. However, LLMs usually recognize only textual information; some services, such as ChatGPT-40 [1] and Gemini [2], can recognize images and text simultaneously, but special prompts must be developed to capture them as the facial expressions of the interlocutor. In this study, we propose a method to recognize emotional information from the user's facial expression captured by a camera and hand it over to the LLM as a prompt together with the dialogue content."}, {"title": "II. RELATED RESEARCH", "content": "Recently, a technique called EmotionNet [3], which ap- plies convolutional neural networks to the recognition of emotions in human facial photos, has been proposed. Li- breFace [4] uses deep learning technology to recognize emo- tional expressions with higher accuracy than conventional methods. In addition, techniques for recognizing emotions from images, such as those using the Vision Transformer, have been proposed by Fuyan Ma et al [5]. Since ChatGPT- 4o is a multimodal AI, it can be recognize emotions from facial expressions. However, in this study, we employed FER instead of ChatGPT-40 because of the need for image recognition technology that can work locally (or embedded), taking personal information into consideration. FER is a Python library trained using Dataset [6]. The model is based on the Convolutional Neural Network proposed in Octavio Arriaga et al. [7].\nA survey article [8], which conducted a systematic re- view using the PRISMA protocol, found that task-based conversations are becoming more systematic, that there are improvements in the recognition rate of speech recognition, and that the appearance and expressiveness of the robot must"}, {"title": "III. APPROACH", "content": "Our proposed LLM-based facial expression recognition ChatBot is called FacingBot (FBot). We employ a python library FER. The FBot system configuration is shown in Figure 1. First, the user's face to be interacted with was gen- erated by ChatGPT-40 using the face of a Japanese woman with four different facial expressions: normal, smiling, angry, and sad, as shown in Figure 2. The generated face images were printed out on paper and recognized in the form of a photograph taken with the built-in camera of a laptop computer. Next, gpt-3.5-Turbo [10] was used as the LLM for dialogue in natural language. In addition to simply entering dialogue sentences, emotional information obtained using FER was added to the prompts in JSON(JavaScript Object Notation) format."}, {"title": "A. Emotional expressions in JSON format", "content": "Figure 3 shows an example of recognition results for a single smiling image. \"angry\u201d, \u201cdisgust\u201d, \u201cfear\u201d, \u201chappy\u201d, \"sad\", \"surprise\", and \"neutral\" each take a value between 0 and 1."}, {"title": "B. Query prompt with JSON", "content": "The prompt simply concatenated the message by string and the emotional information in JSON format, as expressed in the formula (1).\n\\[ [prompt] = [message] + [emotion(json)] \\] (1)\nFigure 4 is an example of a prompt when the message is \"Hello.\" and the query is made with a smile."}, {"title": "IV. EXPERIMENT", "content": "First, prepare multiple face images of one user interacting with gpt-3.5-turbo. Next, prepare several scenarios of interac- tion with gpt-3.5-turbo. Then, we will import and implement the FER and OpenAI libraries on Google Colaboratory using Python 3.10.12. In the experiment, the user interacts with gpt-3.5-turbo according to the scenarios, and we will investigate how different facial expressions affect gpt-3.5- turbo's response."}, {"title": "A. Preparation of face images", "content": "Using ChatGPT-40, four facial expressions were generated for a Japanese woman who came for a consultation. In this case, four types of facial expressions are used: Normal, which has no emotional expression, a smiling face, an angry face, and a sad face, simply because she is asking for advice. They were captured using the built-in camera of a laptop computer, and the results of emotion recognition by FER are shown in Figure 2. From the images, we found that women's facial expressions are either Happy or Neutral when normal, but almost Happy when smiling, Angry when angry, and Sad when sad, with higher score values for sad faces."}, {"title": "B. Dialogue scenario", "content": "Two dialogue scenarios were used: Case A was simply saying \"Hello.\" and by looking at the differences in FBot's responses when spoken to with a normal expression, a"}, {"title": "C. Results", "content": "Figure 5 shows how the user says \"Bye.\" with a smile in response to FBot. As shown in Table I, when spoken to with a smile, the response is that the speaker is pleased to be smiling. For instance, if the user inputed \"Hello.\" with smile face, FBot responded \"I'm glad you are happy! How can I help you today?\" On the other hand, if the face is angry or sad, there is a caring remark. For instance, if the user inputed \"Hello.\" with angry face, FBot responded \"I'm here to listen and support you. If you'd like to talk about what's on your mind or if there's anything I can do to help, feel free to share. Remember, it's okay to feel a range of emotions, and I'm here to offer assistance in any way I can. Take a deep breath, and know that you're not alone.\" These results confirm that it is possible to create a dialogue based on the user's emotional state by adding the emotion information recognized by FER in JSON format to gpt-3.5-turbo and sending it to a conversation using text strings."}, {"title": "V. DISCUSSION", "content": "The following is a discussion of the experimental results. First, we confirmed that gpt-3.5-turbo can recognize emo- tional states from users' facial expressions using FER. Next, we confirmed that gpt-3.5-turbo can respond to multimodal queries with emotion information according to the emotional state. We also confirmed that gpt-3.5-turbo does not respond according to the emotional state when the emotional state is ambiguous.\nOther issues that were found through the experiments are that emotion recognition using FER fluctuates the recognition results depending on the proximity, brightness, and angle of the face. Furthermore, there are still issues to be considered, such as whether to acquire the average or the maximum value of the emotion information within a certain period of time before the chat program speaks to the user. The average facial expression within a certain time period needs to be calculated, or the facial expression immediately after the chat string is sent may be important. This area needs to be investigated and studied, and the system design needs to be reviewed."}, {"title": "VI. CONCLUSION", "content": "In this paper, for multimodal interaction by AI agents, we designed and developed a system that, when a user makes an inquiry offline, sends visual information, especially emo- tional information obtained from facial expressions, along with the inquiry to the LLM to ensure that the system can respond according to the facial expressions of the user. Experimental results showed that LLM can respond differently to smiling, angry, and sad faces respectively. However, fur- ther research is needed on how to summarize the emotional information added to the text of the query by the user, since the facial expressions of the user are not always constant and the recognition results by FER are also variable. Based on the results of this research, the system could be applied to offline receptionist operations using Al agents, online receptionist"}]}