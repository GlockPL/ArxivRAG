{"title": "Data Formulator 2: Iteratively Creating Rich Visualizations with AI", "authors": ["CHENGLONG WANG", "BONGSHIN LEE", "STEVEN DRUCKER", "DAN MARSHALL", "JIANFENG GAO"], "abstract": "To create rich visualizations, data analysts often need to iterate back and forth among data processing and chart specification to achieve their goals. To achieve this, analysts need not only proficiency in data transformation and visualization tools but also efforts to manage the branching history consisting of many different versions of data and charts. Recent LLM-powered AI systems have greatly improved visualization authoring experiences, for example by mitigating manual data transformation barriers via LLMs' code generation ability. However, these systems do not work well for iterative visualization authoring, because they often require analysts to provide, in a single turn, a text-only prompt that fully describes the complex visualization task to be performed, which is unrealistic to both users and models in many cases. In this paper, we present Data Formulator 2, an LLM-powered visualization system to address these challenges. With Data Formulator 2, users describe their visualization intent with blended UI and natural language inputs, and data transformation are delegated to AI. To support iteration, Data Formulator 2 lets users navigate their iteration history and reuse previous designs towards new ones so that they don't need to start from scratch every time. In a user study with eight participants, we observed that Data Formulator 2 allows participants to develop their own iteration strategies to complete challenging data exploration sessions.", "sections": [{"title": "1 Introduction", "content": "From an initial design idea, data analysts often need to go back and forth on a variety of charts before reaching their goals. Throughout this iterative process, besides updating the chart specifications, analysts face the challenges to transform and manage different data formats to support these visualization designs. Iterative chart authoring is prevalent in exploratory data analysis [42], where analysts often discover new directions from initial charts. For example, after noticing that the line chart about renewable energy percentage in Figure 1 are quite dense for comparing different countries' trends, the analysts may want to filter it to show only top 5 CO2 emitter's trends, or visualize ranks of these countries each year instead. To achieve these, the analysts need different data transformations: the former requires filtering the data with each country's aggregated CO2 emission values, and the latter requires partitioning the data by year to compute each country's ranking. Similar challenges are also relevant in the data-driven storytelling context [40, 41], where authors needs to derive new data to refine chart designs (e.g., annotation). For example, to highlight which countries are leading in renewable energy adoption, the author would superimpose a trend line of global median adoption rates over the line chart; the author may later convert the chart into small multiples to tell a story about most sustainable countries from each continent. Again, these new designs require data transformation from the current results.\nManaging different data and chart designs together in these iterative authoring processes is challenging. As the analyst comes up with new chart designs, they need not only to understand the data format expected by the chart and tool, but also need to know how to use diverse transformation operators (e.g., reshaping, aggregation, window functions, string processing) in data transformation tools or libraries to prepare the data. Many AI-powered tools have been developed to tackle these visualization challenges (e.g., [2, 9, 26, 31, 54, 55]). These tools let users describe their goals using natural language, and they leverage the underlying AI models' code generation ability [1, 5] to automatically write code to transform the data and create the visualization. Despite their success, current tools do not work well in the iterative visualization authoring context. Most of them require analysts to provide, in a single turn, a text-only prompt that fully describes the complex visualization authoring task to be performed, which is usually unrealistic to both users and models.\n\u2022 First, despite free-form text prompts provide unbounded expressiveness for users to describe their visualization intent, they miss UI interactions' precision and affordance, making it difficult for users to clearly describe complex designs they come up in later iteration stages. For example, the user needs a verbose prompt to clearly elaborate which fields they would like to use in each visual channel to create a faceted bar chart; without it, AI models could misinterpret the intent and create undesired charts, requiring further disambiguation efforts from the user. and the system can provide immediate visual feedback to the user. In fact, writing high-quality prompts requires skill and efforts. Even with clear goals in mind, it is challenging for inexperienced users to clearly describe their intent [48, 64].\n\u2022 Second, existing AI-powered tools support only either single-turn or linear interactions with AI models, and therefore do not accommodate branching and backtracking that commonly occur in the iterative authoring process. To use single-turn text-to-vis tools in an iterative manner, users need to re-specify their intent from scratch each time they create a new design, even though the design update is minor. This not only is time consuming for the user, but also increases the chance of the AI model to fail the task, since the model needs to solve a complex task in one shot. While chat-based tools [26, 34, 66] support multi-turn interactions by reusing previous outputs in subsequent turns, they do not work well for branching contexts. When non-linear contexts are merged into a linear history, it is not only challenging for users to communicate which designs should be used towards next iterations, but also challenging for AI model to correctly retrieve relevant content from the long conversation history [14, 21, 65].\nTo overcome these limitations, we design a new interaction approach for iteratively chart authoring. Our key idea is to blend GUI and natural language (NL) inputs so that users can specify charts both precisely and flexibly, and we"}, {"title": "Chart specification with blended UI and NL inputs.", "content": "Resembling shelf-configuration UIs [40, 55], the concept encoding shelf allows users to drag existing data fields they wish to visualize and drop them to visual channels to specify chart designs. Differently, with concept encoding shelf, users can also input new data field names in the chart configuration to express their intent to visualize fields that they want from a transformed data. Then, they can provide a supplemental NL instruction to explain the new fields and ask the AI to transform data and instantiate the chart. This blended UI and NL approach for chart specification makes user inputs both precise and flexible. Since Data Formulator 2 can precisely extract chart specification from the encoding shelf, the user doesn't need verbose prompt to explain the design. By conveying data semantics using NL inputs, the user delegates data transformation to AI, and thus they doesn't need to worry about data preparation. This approach also improves the task success rate of AI models. Because Data Formulator 2 can infer the visualization script directly from UI input, the AI model only needs to generate data transformation code. With the chart design provided as contexts to the AI model, the model has more information to ground the user's instruction for better code generation."}, {"title": "Managing and leveraging iteration contexts with data threads.", "content": "Data Formulator 2 presents the user's non-linear iteration history as data threads and lets them manage data and charts created throughout the process. With data threads, users can easily navigate to an earlier result, fork a new branch, and reuse its context to create new charts. This way, users only need to inform the model how to update the previous result (e.g., \"show only top 5 CO2 emission countries' trends\", Figure 1) as opposed to re-describing the whole chart from scratch. When the user decides to reuse, the Data Formulator 2 tailors the conversation history to include only contexts relevant to that data to derive new result, allowing the AI to generate code with clear contextual information free from (irrelevant) messages from other threads. Besides general navigation and branching supports, data threads also provide shortcut for users to quickly backtrack and revise prompts to update recently created charts, which can be useful for analysts to explore alternative designs or correct errors made by AI."}, {"title": "2 Illustrative Scenarios: Exploring Renewable Energy Trends", "content": "In this section, we describe scenarios to illustrate users' experiences of creating a series of visualizations to explore global sustainability from a dataset of 20 countries' energy from 2000 to 2020. The initial dataset, shown in Figure 2-1, includes each country's energy produced from three sources (fossil fuel, renewables, and nuclear)"}, {"title": "2.1 Exploration with computational notebooks", "content": "Heather is an analyst who is proficient with a computational notebook and R libraries, ggplot2 and tidyverse. Because ggplot2 expects all data fields to be visualized on visual channels (e.g., x, y-axes, color, facet) are columns in the input data, Header uses tidyverse for data transformation.\nBasic charts. To start, Heather wants to visualize the amount of electricity produced from renewables per country over the years with a line chart to see \u201cif our planet is sustainable.\" Since the input data (table-1) includes all required fields, Heather creates the line chart with ease, by mapping columns Year\u2192 x, Electricity from renewables (Twh)\u2192 y and Entity\u2192 color (chart 1-A). She then creates another line chart for CO2 emission trends, mapping CO2 emissions (kt) to the y axis (chart \u2460-B). Heather is puzzled that China, the country with considerable increased use of renewable energy, also has the biggest increase in CO2 emissions. This is counterintuitive because renewables themselves would not cause CO2 emission increase. Thus, Heather decides to dive deeper.\nRenewable energy versus other sources. Heather suspects the CO2 emission increase is caused by a surge of fossil fuel consumption. To compare fossil fuel usage against renewables, she wants a faceted line chart that shows electricity from each energy source side by side (chart 2). To create the chart, Heather needs to have a data table with columns-Year, Electricity, Entity, and Energy Source-and map the columns to x, y, color, and facet, respectively so that the chart is divided into subplots based on values from the Energy Source column. Because table \u2460 stores electricity values across three columns in the wide format, Heather unpivots table \u2460 into the long format, to fold specified column names into values in the Energy Source field and corresponding values into the Electricity field. She then creates the desired chart \u2461 with the transformed data 2, and verifies her assumption: despite the increase of renewables usage, the usage of fossil fuel also grows significantly, leading to CO2 emission increase. This motivates Heather to explore renewable trends by visualizing trends of the percentage of electricity from renewables over all three resources.\nRenewable energy percentage and ranks. To visualize renewable energy percentage, Heather goes back to table \u2460 to derive a new column Renewable Percentage, by dividing Electricity from renewables (TWh) from the total produced electricity for each country per year. With the new data \u2462, Heather visualizes the renewable percentage trends in chart 3, which shows that the percentage increase is slower than their absolute value increase (as shown in chart 1).\nBecause many countries share similar renewable percentage, it is quite difficult to compare different countries' trends. Heather thus decides to create a visualization of countries' renewable percentage ranks to complement existing charts. To calculate ranks of each country among others per year, Heather uses a window function on table to partition the table based on Year, and apply the rank() function to Renewable Percentage to derive a new column Rank. With Rank mapped to y-axis, chart \u2463 allows Heather to clearly examine how different countries' ranks change in the last two decades; for example, Germany and UK are the two top ranked countries emerge from the bottom pack in 2000.\nRenewable trends from top CO2 emitters. Finally, Heather wants to focus on renewable percentage trends from top CO2 emission countries, which make most influences to global sustainability. Despite table \u2462 contains all columns to be visualized, Heather needs to filter it based on the countries' CO2 emission. To do so, Heather goes back to table to aggregate each country's total CO2 emission, sort it and find top five. Heather then uses this intermediate result to filter table \u2462 to obtain renewable percentage from top five CO2 emitters (shown as table \u2464) and creates chart 5."}, {"title": "2.2 Exploration with Data Formulator 2", "content": "Megan is a journalist who has a solid understanding about data visualization. She utilizes visualizations effectively in her work but she doesn't program. Megan can create and refine rich visualizations iteratively with Data Formulator 2 (Figure 3), which inherits the basic experience of shelf-configuration style tools. She can specify charts by mapping data fields to visual channels of the selected chart and provide additional contexts using natural language.\nBasic charts. Megan starts with line charts to visualize trends of electricity from renewables (Figure 2-1A). Since all three required fields are available from the input data, Megan simply selects chart type \u201cline chart\u201d in the encoding shelf and drags-and-drops fields to their corresponding visual channels (Figure 4-1). Data Formulator 2 then generates the desired visualization. To visualize the CO2 emission trends, Megan swaps y-axis encoding with CO2 emissions (kt)\u2192 y."}, {"title": "Renewable energy vs other sources.", "content": "Megan now needs to create the faceted line chart to compare electricity from all energy sources, which requires new fields Electricity and Energy Source. With Data Formulator 2, Megan can specify the chart using future fields and NL instructions in Concept Encoding Shelf (Figure 3-2) and delegate data transformation to AI.\nAs Figure 4-2 shows, Megan first drags-and-drops existing fields Year and Entity to x-axis and color, respectively; then, she types in names of new fields Electricity and Energy Source in y-axis and column, respectively, to tell the AI agent that she expects two new fields to be derived for these properties; finally, Megan provides an instruction \"compare electricity from all three sources\" to further clarify the intent and clicks the formulate button. To create the chart, Data Formulator 2 first generates a Vega-Lite spec skeleton from the encoding (to be completed based on information from the transformed data); it then summarizes the data, encodings, and NL instructions into a prompt to ask an LLM to generate a data transformation code to prepare the data that fulfils all necessary fields, which is then used to instantiate the chart skeleton. After reviewing the generated chart and data, Megan is satisfied and moves to the next task.\nData Formulator 2 also updates data threads (Figure 3-5) with the newly derived data and chart so that Megan can manage and leverage data provenance. For example, Megan can delete the chart, create/fork new charts from either the original or new data, or select an existing chart to iterate from."}, {"title": "Renewable energy percentage and ranks.", "content": "Megan proceeds to visualizing renewable energy percentage. Despite it required a different data transformation, Megan enjoys the same experience as the previous task: Megan drags-and-drops Year and Entity to x-axis and color (Figure 4-3), and enters the name of the new field \u201cRenewable Energy Percentage\u201d to y-axis; then, since Megan believes the field names are self-explanatory, she proceeds to formulate the new data without an additional NL instruction. Data Formulator 2 generates the desired visualization (Figure 5-1).\nTo visualize the ranks of countries based on their renewable percentage, Megan decides to continue it from the previous chart, which already computes renewable percentage. To do so, Megan duplicates the renewable percentage chart and update the y-axis field to another new field Rank and clicks \u201cderive.\u201d As shown in Figure 4-3, Megan's interaction positions the Concept Encoding Shelf in the contexts prior result as opposed to the original data, which conveys her intent of reusing the data towards the new one. With the context information, the AI model successfully derives the desired chart (Figure 2-4) even only with Megan's simple inputs."}, {"title": "Renewable trends from top CO2 emitters.", "content": "Next, Megan decides to visualize top-5 CO2 countries' renewable percentage trends. Megan again decides to iterate on the previous chart, otherwise she needs more effort to create a"}, {"title": "2.3 Comparison of Exploration Experiences", "content": "The experience of Heather and Megan exploring global sustainability (using data 1) demonstrates an inherently iterative process. Both of them started with a high-level goal without concrete designs in mind and gradually formed the design from explorations in various branches. This iterative exploration process required a series of data transformation and the management of provenance, and thus is challenging for people not proficient in data transformation and programming. Here, we compare their exploration experiences to highlight how Data Formulator 2 bridges Megan's skill gap, enabling her to achieve the analysis Heather, an experienced data analysis, performed."}, {"title": "2.3.1 Data transformation and chart creation.", "content": "When new designs are considered, Heather needs to prepare new data to accommodate the design, even when some designs are seemingly close (e.g., charts \u2462 and \u2464). This requires her to understand the data shape expected by the charts, choose the right transformation idiom (e.g., unpivot for table 2, join and union for table \u2465), and implement them with proper operators. Once the data is prepared, Heather"}, {"title": "2.3.2 Managing branching contexts.", "content": "During the exploration, Heather backtracks several times to reuse previous results toward new designs (e.g., chart \u2463\u2192 data \u2462\u2192 chart 5), creating three branches along the way. Because Heather programs in a notebook, she can either copy and adapt previous code snippets or reuse variables computed in previous iterations for new designs. This way, Heather lowers her specification efforts despite new designs are more complex. Heather's programming expertise is essential for her to manage the branching contexts in a linear programming environment.\nFor Megan, managing branching contexts with different version of data could be challenging without Data Formulator 2. Should Megan use a chat-based AI interface, she would need to prepare a verbose prompt to explain contexts and data transformation goals in detail each turn to avoid extra disambiguation efforts, especially when multiple branches are mixed in the chat history and the task becomes more complicated later on. Data Formulator 2's data threads address this challenge. Data threads not only provide a visualization for Megan to review history, but also let her visit previous states and reuse them towards new branches as Heather did. This way, Megan only needs to specify updates to be applied as opposed to describing the full design from scratch in one shot, and the AI model can generate results more reliably leveraging the contexts Megan provided. Shall Megan spot undesired results, she could also use data threads (Figure 3-\u2462) to rerun or backtrack one step to revise instructions, as opposed to restarting from the scratch."}, {"title": "3 The Data Formulator 2 System Design", "content": "As described earlier, Data Formulator 2 combines UI and NL interactions in a multi-modal UI to reduce analysts' visualization authoring efforts, and it provides data threads for users to navigate iteration history and specify new designs on top of previous ones. Data Formulator 2 employs the following system designs to support such interactions:\n\u2022 First, to allow users to specify chart design and data transformation goals from different paradigms (shelf-configuration UI versus NL inputs), Data Formulator 2 decouples chart specification and data transformation and solve them with different techniques (template instantiation versus AI code generation).\n\u2022 Second, to support reusing, Data Formulator 2 organizes the iteration history as data threads, treating data as first class objects. Data Formulator 2 enables users either to locate a chart from a different branch and follow up, or to quickly revise and rerun the most recent instruction leading to the current chart.\nWe next detail how Data Formulator 2 realizes these designs, and additional features designed to assist users to understand AI-generated results."}, {"title": "3.1 Multi-modal UI: Decoupling chart specification and data transformation", "content": "Data Formulator 2 decouples chart specification and data transformation so that users can benefit from both the precision of UI interaction to configure chart designs and the expressiveness of NL descriptions to specify"}, {"title": "3.2 Data threads: navigating the iteration history", "content": "During the iterative visualization process, the analyst needs to navigate their authoring history to locate relevant artifacts (data or charts) to take actions (delete, duplicate or followup). Data Formulator 2 introduces data threads to represent the tree-structured iteration history to support navigation tasks. In data threads, we treat data as the first class objects (nodes in data threads) that are connected according to the user's instruction provided to the AI model (edges), and visualizations are attached to the version of data they are created from. Centering the iteration history around data benefits user navigation because it directly reflects the sequence of user actions in creating these new data. This design also benefits the AI model: when user issues a follow-up instruction, Data Formulator 2 automatically retrieves its conversation history with the AI towards the current data and then instruct the AI model to rewrite the code towards new goals based on the retrieved history. This way, the AI model does not pose risk of incorrectly using conversation history from other branches to make incorrect data transformation. As shown in Figure 8, the code and the conversation history is attached to each data nodes. Each turn when the user provides a follow-up instruction, the AI model generates new code by updating the previous code (could be deletion, addition or both) to achieve the user's goal; this way, the code always takes the original data as the input with all information accessible. Comparing to an alternative design where we only pass current data to the AI model and asks it to write a new code to further transform it (i.e., reusing the data as opposed to reusing the computation leading to the data), our design has more flexibility to accommodate different styles of followup instructions either the user wants to further update the data (e.g., \u201cnow, calculate average rank for each country\u201d), revise previous the computation (e.g., \"also consider nuclear as renewable energy\") or creating an alternatives (e.g., \u201crank by CO2 instead\") since the AI has access to the full dialog history and the full dataset. In contrast, the data-only reuse approach restricts the AI model's access to only the current data, limiting its ability to support \"backtracking\u201d or \u201calternative design\" styles instructions."}, {"title": "3.3 Miscellaneous: inspecting results and styling charts", "content": "As an AI-powered tool, Data Formulator 2 lets the user verify AI-generated results and resolve mistakes made by AI. It displays transformed data, visualization, the code, and an explanation of the code in the main panel. This design accommodates various user verification styles identified by prior work [12, 54]: e.g., viewing high-level correctness from chart, inspecting corner cases in data, inspecting the transformation output, as well as understanding the transformation process from code. Data Formulator 2 utilizes a code explanation module to query the AI model to translate code into step-by-step explanations assist users to understand the process. Furthermore, despite data transformations generated in the later iteration stages can be complex, users only need to verify its correctness against its predecessor because Data Formulator 2 users create visualizations incrementally. This considerably lowers users' verification efforts, as we discovered in our study in Section 4. As previously mentioned in Figure 8, when the user discovers errors, they can take advantages of the data thread's iterative mechanism to rerun, follow up or revise instructions to correct results.\nBenefiting from the decoupled chart specification and data transformation processes, when the user wants to update visualization styles (e.g., change color scheme, change sort order of an axis, or swap encodings) that do not require additional data transformation, they can directly perform edits in the concept encoding shelf, by expanding the channel property and update parameters or swapping encoded fields. These updates are directly reflected in the Vega-Lite script and rendered in the main panel. Unlike interactions with AI which has a slightly delayed response time, this approach allows the user to achieve quick and precise edits with immediate visual feedback to refine the design."}, {"title": "3.4 Implementation", "content": "Data Formulator 2 is implemented as a React web application, with a backend Python server running on a Dv2-series CPU with 3.5 GiB RAM. Data Formulator 2 has been tested with different versions with OpenAI models, including GPT-3.5-turbo, GPT-4, GPT-4o and GPT-40-mini (we used GPT-3.5-turbo in our user study) all of which except GPT-4 can generally response within 10 seconds. Since the LLM generates code to manipulate data as opposed to directly consume data, data size does not affect its response time. Data Formulator 2 can sometimes be slow due to Vega-Lite rendering overhead (e.g., large dataset with > 20,000 rows, long data threads with > 20 charts), we envision that on-demand re-rendering of charts can improve its performance in deployment."}, {"title": "4 Evaluation: Iterative Exploratory Analysis", "content": "We conducted a user study to understand potential benefits and usability issues of Data Formulator 2, as well as strategies developed by users when iteratively creating visualizations in an exploratory data analysis session."}, {"title": "4.1 Study Design", "content": "Participants. After piloting and refining the design of user study and Data Formulator 2 with three volunteers, we recruited eight participants from a large company. Participants self rated their skills (Figure 9) on a scale (\"Novice,\u201d \u201cIntermediate,\u201d \u201cProficient,\u201d and \u201cExpert\") in the following aspects: (1) chart creation \u2013 experience with chart authoring tools or libraries, (2) data transformation \u2013 experience with data transformation tools and library expertise, (3) programming, and (4) AI assistants \u2013 experience with large language models (e.g., ChatGPT [1]) and prompting.\nSetup and procedure. Each study session, conducted remotely with screen sharing, consisted of four sections within a 2-hour slot. After a brief introduction of the study goal, participants were asked to follow step-by-step instructions in a tutorial presented in slides (~25 minutes). To make sure that they understood the tool and process, practice tasks (~15 minutes) were presented during and after the tutorial, where participants could ask questions as they worked through the tasks. Then, participants were given two study tasks to complete, where only clarification"}, {"title": "4.2 Results", "content": "Task completion. All participants successfully completed all 16 visualizations (Figure 9): participants took less than 20 mins on average to finish the seven charts in task 1, and about 33 mins for the nine charts in task 2. Since we let participants deviate from the main exploration task (e.g., in task 2, P4 asked to sort the bar chart for top profitable movies are based on their profits, even though it was not required), the recorded completion time is an overestimate of the actual task time. During the study, six participants asked for hints to get unstuck during tasks; we categorize them as follows:"}, {"title": "Iteration styles.", "content": "Data Formulator 2 lets users develop their own iteration strategies. We observed three major distinct styles of iteration, in terms of which tables or charts participants chose to derive a new chart.\nThe first type of users preferred to achieve a particular chart through small, incremental changes from an existing chart that shared either similar data fields or similar chart configuration. For example, P2 and P3 chose to create the line chart showing profit ratio trends overtime on top of the bar chart showing the average profit ratio per genre, and next visualized movies with highest profit ratio further on top, since they share the same derived field profit ratio. P2 mentioned \"I definitely like to be able to just work on top of that and like going forward by just giving a new prompt, because it remembers the context prior to the last one, it ends up generating the right data and visualization.\" P2 further commented that they did not like too much branching: \"...felt that it would be harder to go back to the source and fix every single time.\" P7 also preferred incremental changes, but with a focus on visual similarity as opposed to data similarity.\nIn contrast, the second type of users preferred to go back and re-issue a prompt to achieve all the changes from the initial data as succinctly as possible. For example, P1 mentioned that \u201c[I] like keeping it as terse as possible that will get me the right result.\" P4 also felt that sometimes it was more productive to just start over from the original dataset throwing out all iterations, especially when they failed to produce a desired outcome: \"when we had all of those failures, I went back to the original base dataset and then frame my question there.\"\nThe third type of users primarily think about the iterations in terms for adding (or retrieving) columns from the dataset. P5 preferred to first instruct Data Formulator 2 to add/remove columns from an existing data (e.g., bring back fields that might have been dropped in previous iterations as needed, or add a new field required for the desired chart), and then create visualization from the right data."}, {"title": "Organization of iteration history.", "content": "When asked about their rationale behind branching strategies, all participants agreed data threads are essential for managing iteration histories. Regarding their preferred organization style, P1 mentioned \"I don't like to pollute my workspace\" and \"I'd like to keep my workspace as clean as possible\" and thus they always chose to backtrack and fix previous instruction when encountering undesired results. P2, who mentioned \"going back created too much branching\u201d instead preferred follow through. P4 used prompts to help navigate iterations to find the one they were looking for: \"I was using the prompts as my anchor to figure out where"}, {"title": "Verification.", "content": "To proceed through iterative exploration, or repeat/correct a step, participants needed to verify that the chart or transformation was performed correctly. Some used the explanations of the code, some (even non-python programmers) used the actual code, and some used the result tables to validate the impact of the transformations. P3 mentioned \u201cas an expert, I like to see the prompt to the model, and then the code generated; but as a business user, I would imagine using more data, chart, and explanations.\u201d P4 mentioned \"[explanation] steps were really, really helpful in terms of figuring out whether it is doing the right thing as to what I'm asking it to do. That and also the data chart underneath.\u201d Interestingly, P7 stated that they preferred to use code rather than explanations of the code, but in the study, they used almost exclusively the explanations. They stated that they felt some pressure from the study environment not to spend too much time understanding code for which they were not familiar with, but they would trust code more. We also observed participants who developed trust in a workflow (by examining code and data tables) when it was straightforward, and then, they assumed the more complicated transformations built on top of these steps worked."}, {"title": "Miscellaneous.", "content": "Several users noted potential improvements of Data Formulator 2 for iterative chart authoring. P1 commented on how small interface variations might give different affordances. For instance, \u201cif there was a large view for data threads, it would encourage me to do more transformations and do more branching.\u201d P3 mentioned that they prefer AI to ask the user to disambiguate when the intent is unclear rather than trying to solve the task with unclear specification. P7 used instructions that were very detailed and sometimes incorrect, which in turn, made iteration more difficult, since it was difficult to incrementally modify these instructions. We discussed the potential of having templates or AI feedback for instruction crafting to reduce errors."}, {"title": "5 Related Work", "content": "Data Formulator 2 builds on top of existing work on data transformation, chart authoring, and AI-powered visualization tools."}, {"title": "LLM-powered visualization tools.", "content": "Large language models' code generation ability [1, 5, 24, 50] motivates the designs of new AI-powered visualizations tools [10, 26, 49, 55] that allows users to create visualization using high-level natural language descriptions. For example, given a dataset and a visualization prompt, LIDA [9] automatically generates a data summary and prompts the LLM to generate python code to transform data and generate visualizations. Because LLMs can struggle in understanding complex chart logics, ChartGPT [49] decomposes visualization tasks into fine-grained reasoning pipelines (e.g., data column selection, filtering logic, chart type, visual encoding), using chain-of-thoughts prompting [56] to guide LLMs to generate code step by step. Data Formulator [55] leverages LLMs to derive new data columns that can be used in traditional shelf-configuration UI. Because these tools focuses on single-turn user interaction with abstract NL descriptions, they are not suitable for iterative analysis where the analyst may branch or revise designs throughout. For multi-turn interactions, users can directly have conversation with LLMs in Code Interpreter [1] or Chat2Vis [26]: Code Interpreter equips the LLM with a Python interpreter so that the model can generate and execute code to transform data and create visualizations with the user interactively; Chat2Vis further includes visualization-specific prompts to help the model generate visualizations more reliably. Since these tools organize the dialog linearly, when the context contains branches, the user needs extract efforts to explain the task so that the model can retrieve the correct context, otherwise the model are more likely to produce undesired results [14, 21, 65]. Besides, since these tools are based on NL inputs, when the user has concrete designs in mind, they need additional efforts to elaborate the design clearly (especially when the design is complex) so that the model can produce their desired results."}, {"title": "Other AI and synthesis-powered tools.", "content": "Besides LLM-powered tools above, neural semantic parsing [6, 29, 31], and program synthesis-based tools [54] have also been developed to address the visualization challenge. For example, NL4DV [31] and NcNet [25] are natural language interfaces (NLIs) based on recurrent neural networks trained from parallel NL and chart specification corpus that can generate charts from NL queries. NL2Vis [62] and Graphy [6] use semantic parser to extract entities from the user's NL query and apply program synthesis techniques to compose chart specifications. Unlike LLM-based tools that can generate general purpose python programs to support expressive data transformation and visualization from abstract instructions, semantic parsing based NLIs are less expressive, requiring more concrete descriptions from the user and supporting only limited data transformation. In particular, these tools require tidy data inputs [58], and they do not support transformations like string processing, column derivation and reshaping. While programming-by-examples (PBE) techniques are developed to tackle data reshaping challenges in chart authoring (e.g., Falx [54] and Data Formulator [55]'s reshaping module), these tools require users to prepare low-level examples to demonstrate the transformation intent, which can be difficult for new users as it deviates from the high-level visualization workflow. Unlike LLM-based tools where the user can directly have conversation with the model to disambiguate inputs, semantic parsing and PBE-based tools develop special techniques for resolving ambiguous user intent. For example, DataTone [11] introduces disambiguation widgets to allow users to select alternative extracted entities in the generated queries to resolve ambiguity, and it paraphrases the generated query in NL to explain the result. Falx [54] renders charts from multiple versions of data consistent with user examples for user inspection.\nBenefit from LLMs, Data Formulator 2 supports a much wider range of data transformation and does not limit inputs to tidy data. Inspired by how prior work displays candidate results and explains code to help users understand system outputs [11, 12, 55], Data Formulator 2 displays generated code, data, chart and code explanation to assist user inspection. To resolve ambiguous outputs, the user can use data threads to follow up or backtrack and revise the their instructions."}, {"title": "Visualization grammars and interactive tools.", "content": "The grammar of graphics [60", "57": "Vega-Lite [45", "53": "where visualizations are built from mapping data columns to visual channels and lower-level chart properties. Comparing to more flexible and expressive languages like D3 [3", "22": "high-level grammars hide the computation process of linking data items to visual objects to reduce visualization efforts. Powered by these high-level grammars, interactive tools like Lyra [44", "23": "Charticulator [40"}]}