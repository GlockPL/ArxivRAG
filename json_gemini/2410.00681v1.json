{"title": "Advanced Arabic Alphabet Sign Language Recognition Using Transfer Learning and Transformer Models", "authors": ["Mazen Balatt", "Rewaa Awaad", "Hend Adel", "Ahmed B. Zaky", "Salah A. Aly"], "abstract": "This paper presents an Arabic Alphabet Sign Language recognition approach, using deep learning methods in conjunction with transfer learning and transformer-based models. We study the performance of the different variants on two publicly available datasets, namely ArSL2018 and AASL. This task will make full use of state-of-the-art CNN architectures like ResNet50, MobileNetV2, and EfficientNetB7, and the latest transformer models such as Google ViT and Microsoft Swin Transformer. These pre-trained models have been fine-tuned on the above datasets in an attempt to capture some unique features of Arabic sign language motions. Experimental results present evidence that the suggested methodology can receive a high recognition accuracy, by up to 99.6% and 99.43% on ArSL2018 and AASL, respectively. That is far beyond the previously reported state-of-the-art approaches. This performance opens up even more avenues for communication that may be more accessible to Arabic-speaking deaf and hard-of-hearing, and thus encourages an inclusive society.", "sections": [{"title": "I. INTRODUCTION", "content": "Sign language serves as a vital bridge between the hearing and deaf worlds [1], with Arabic Alphabet Sign Language (ArASL) holding particular significance due to the widespread use of Arabic. The development of efficient ArASL recognition systems represents not just a technological challenge, but a crucial step towards creating more inclusive societies in Arabic-speaking regions [2]. These systems have the potential to revolutionize communication, education, and social integration for the deaf and hard-of-hearing community.\nAdvanced Arabic Alphabets Sign Language (ArASL) recognition has significant societal impacts, including improving education, workplace integration, and public services access [3]. It can also improve healthcare access for deaf patients in rural areas, and provide real-time translation during emergencies [4]. Integrating sign language recognition technology into mainstream devices can raise awareness of deaf culture, leading to more inclusive policy-making and societal attitudes [5]. Thus, accurate and efficient Arabic Sign Language recognition is a crucial step towards digital inclusivity and equal access to information and services.\nRecent advancements in deep learning, especially in the domains of computer vision and sequence modeling, have opened new avenues for developing robust recognition systems. Transfer Learning [6] has emerged as a particularly promising approach, allowing researchers to leverage pre-trained models to enhance performance and generalization while reducing training time. This method shows great potential in unraveling the intricacies of ArASL and building scalable recognition systems.\nIn this work, we present our multifaceted contributions to the field of ArASL recognition, aiming to address these challenges and push the boundaries of what is possible in sign language technology:\ni We achieve superior recognition accuracy through the innovative application of transfer learning and state-of-the-art Transformer-based models.\nii We develop a scalable ArASL recognition framework that is adaptable to other sign languages, promoting wider applicability and impact.\niii We facilitate the creation of practical communication tools specifically designed for the Arabic-speaking deaf community, bridging the gap between technological advancement and real-world application."}, {"title": "II. RELATED WORK", "content": "Contemporary methodologies for facilitating sign language communication, including human interpretation [7], written communication [8], and Automatic Speech Recognition [9], while valuable, often exhibit limitations in scope and efficacy. The intricate and dynamic nature of sign languages, particularly Arabic Alphabets Sign Language (ArASL), presents significant challenges to conventional machine learning paradigms [10]. These traditional approaches frequently struggle to capture the nuanced gestures and diverse signing patterns characteristic of ArASL.\nRecent advancements in the field have yielded promising results. A notable study proposed a sign language recognition system utilizing transfer learning techniques for Arabic alphabets, leveraging the ArSL2018 dataset. The researchers implemented preprocessing techniques to mitigate class imbalance, including image resizing and data augmentation through horizontal and vertical shifts with zooming. Employing the EfficientNetB4 model, this approach achieved a commendable testing accuracy of 95%, demonstrating the potential of transfer learning in Arabic sign language recognition [11].\nAbdelghfar et al. introduced a deep learning approach for Qur'anic sign language recognition in 2024. Utilizing a subset of the ArSL2018 dataset, they addressed class imbalance through Random Oversampling, Synthetic Minority Over-sampling Technique, and Random Undersampling. Their QSLRS-CNN model attained impressive accuracies of 97.13% and 97.31% at the 100th and 200th epochs, respectively, surpassing existing models in performance [12].\nEl Baz et al. conducted a comprehensive study on Arabic alphabet sign language recognition using deep learning techniques and the RGB Arabic Alphabets Sign Language Dataset. Their methodology incorporated data preprocessing, including cleaning, resizing, and background removal, followed by data augmentation. The proposed architecture, comprising convolutional, pooling, dense, and dropout layers, achieved remarkable accuracies of 99.4% in training and 97.4% in validation [13].\nAl Nabih et al. explored a Vision Transformer (ViT)-based approach for Arabic sign language recognition. By fine-tuning a pre-trained ViT model on the ArSL2018 dataset, they achieved an outstanding accuracy of 99.3%, outperforming several recent CNN-based approaches [14].\nLahiani et al. conducted a comparative analysis of three pre-trained CNN-based architectures\u2014InceptionV3, VGG16, and MobileNetV2\u2014for Arabic alphabet sign language recognition. Utilizing transfer learning techniques on the ArSL2018 dataset, their study revealed superior performance with the MobileNetV2 network, achieving an accuracy of 96% [15].\nRenjith et al. proposed an innovative approach to sign language recognition by leveraging spatio-temporal features. Their method, applied to both Chinese Sign Language (CSL) and Arabic Alphabets Sign Language (ArASL), demonstrated promising results with accuracies of 90.87% for CSL and 89.46% for ArASL alphabet recognition [16].\nThe literature reveals the efficacy of various pre-trained models in recognition tasks. Architectures such as ResNet50 [17], MobileNetV2 [18], and EfficientNetB7 [19] have demonstrated remarkable results in image classification. More recently, transformer networks like Google's Vision Transformer (ViT) [20] and Microsoft's Swin Transformer [21] have revolutionized computer vision through self-attention mechanisms and hierarchical approaches. The superior performance of these pre-trained models, when fine-tuned for sign language recognition tasks, underscores their adaptability and effectiveness in this domain."}, {"title": "III. DATASETS", "content": "In this study, we utilize two datasets for Arabic alphabet sign language recognition: the Arabic Sign Language ArSL2018 dataset and RGB Arabic Alphabets Sign Language dataset (AASL)."}, {"title": "A. Arabic Alphabets Sign Language Dataset (ArASL2018)", "content": "A significant contribution to Arabic Sign Language (ArSL) recognition research was made by Latif et al. [22] with the introduction of the ArSL2018 dataset. This comprehensive dataset consists of 54,049 grayscale images (64x64 pixels) representing 32 Arabic sign language signs and alphabets. The images were collected from 40 participants of various age groups in Al Khobar, Saudi Arabia, using an iPhone 6S camera. The dataset includes variations in lighting, angles, and backgrounds to enhance its robustness. ArSL2018 is notable for being one of the first large, fully-labeled datasets for Arabic Sign Language, making it a valuable resource for researchers developing machine learning and computer vision applications for the deaf and hard of hearing community. The authors reported achieving high accuracy in their initial experiments, establishing a benchmark for future research. This dataset addresses a crucial need in the field, as it enables faster development and prototyping of assistive technology applications specific to Arabic sign language."}, {"title": "B. RGB Arabic Alphabets Sign Language Dataset (ArASL)", "content": "The RGB Arabic Alphabets Sign Language (AASL) dataset [23] consists of 7,857 labeled RGB images, representing 31 Arabic sign language alphabets. Collected from over 200 participants using various types of cameras (webcams, digital cameras, and phone cameras), the dataset captures a range of conditions including different lighting, backgrounds, and orientations. This diversity enhances its robustness for real-world applications. Experts validated and filtered the images to ensure high quality, making AASL an essential dataset for developing accurate Arabic sign language classification models."}, {"title": "IV. METHODOLOGY", "content": "The methodology for Arabic Alphabet Sign Language recognition consists of three key steps: data pre-processing, model selection with transfer learning, and model evaluation. Each stage plays a crucial role in ensuring the system's accuracy and reliability. The following sections will detail these procedures to provide a clear understanding of the research approach."}, {"title": "A. Data Preprocessing", "content": "Data preprocessing mainly helps in refining the dataset so that the deep learning models may be best trained. A few key tasks comprise the standardization of input data or enhancing its quality in the preprocessing pipeline.\nClass imbalance is one of the major issues to be tackled in this step. Few of the Arabic alphabet signs have much more samples than the rest, and hence using them would make predictions biased. Therefore, methods like under-sampling of majority classes and over-sampling of minority classes are put into practice. Each of the 28 Arabic alphabet signs must be represented decently while preparation of the dataset.\nAfter balancing classes, these images undergo grayscale conversion to reduce dimensionality, allowing a greater focus on the shape and texture of hand gestures. Since color information does not play an important role in recognizing hand signs, a gray-scale conversion of the problem at hand can help to simplify it further and reduce computational complexity.\nThe images are then uniformly resized to 224x224 pixels-a requirement to be compatible with some of these pre-trained models, such as ResNet50, VGG16, and MobileNetV2. This standardization of the image dimensions was performed for easy training of the model and to make all the input data uniform.\nNormalization of the pixel values scales them between a range of [0, 1]. This helps reduce light variations and adds more robustness to the model for different lighting conditions during inference.\nLastly, the data is divided into training, validation, and test subsets. A standard split of 70% for training, 15% for validation, and 15% for testing is done. This splitting ensures that the model is tested on unseen data for hyperparameter tuning and allows for a more realistic estimate of performances.\nAs shown in Figure 4, these steps, including class imbalance handling, grayscale conversion, image resizing, pixel value normalization, and data splitting, form the core of the preprocessing pipeline, leading to improved data quality and standardized inputs."}, {"title": "B. Model Selection and Transfer Learning", "content": "Our approach to Arabic Alphabet Sign Language recognition utilizes transfer learning with both Convolutional Neural Network (CNN) architectures and transformer-based models. We selected five pre-trained models to leverage their feature extraction capabilities learned from large-scale datasets: ResNet50, MobileNetV2, EfficientNetB7, Google Vision Transformer (ViT), and Microsoft Swin Transformer. For the CNN-based models (ResNet50, MobileNetV2, and EfficientNetB7), we employ a fine-tuning strategy. The early layers of these models are frozen to retain general features learned from ImageNet, while the final classification layers are replaced with new layers configured for our 28-class Arabic alphabet recognition task. Only these new layers and a few preceding layers are left unfrozen, allowing fine-tuning on the specific characteristics of our dataset.\nFor the transformer-based models (Google ViT and Microsoft Swin), we adopt a similar fine-tuning approach. These models, originally trained on large image datasets, are adapted to our specific task by modifying their classification heads while keeping the core transformer blocks mostly frozen.\nAll models are trained with a batch size of 32, using the Adam optimizer with an initial learning rate of 0.001. We employ the cross-entropy loss function to compute the difference between predicted and actual class labels. A StepLR scheduler is implemented to decay the learning rate by a factor of 0.1 every 10 epochs, aiding in better convergence. To prevent overfitting, we implement early stopping: training halts if the validation accuracy does not improve for 5 consecutive epochs.\nFigure 5 illustrates the transfer learning process, which is applicable to both CNN and transformer architectures. The input image is passed through the pre-trained backbone (ResNet50, MobileNetV2, EfficientNetB7, Google ViT, or Microsoft Swin), which extracts feature maps. These feature maps are then fed into new fully connected layers specifically designed for Arabic alphabet classification, leading to the final output predictions."}, {"title": "C. Model Evaluation", "content": "The performance of our models is evaluated using accuracy as the primary metric [24]. Accuracy provides a straightforward measure of the model's overall performance in classifying Arabic alphabet signs.\nAccuracy measures the proportion of correctly classified instances out of the total instances in the dataset. It is mathematically defined as:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+ FN}$  (1)\nWhile accuracy offers a clear indication of overall performance, it's important to note that in cases of class imbalance, it may not fully capture the model's effectiveness across all classes. However, given the balanced nature of our datasets (ArASL2018 and AASL), accuracy serves as an appropriate and sufficient metric for our evaluation.\nWe report accuracy for training, validation, and test sets to provide a comprehensive view of each model's performance and to assess potential overfitting or underfitting issues."}, {"title": "V. RESULTS", "content": "In this section, we present the results of our experiments on Arabic alphabet sign language recognition using fine-tuned CNN and transformer models. We evaluate the performance of these models on test datasets and compare their accuracy, training time, and overall efficiency to state-of-the-art methods."}, {"title": "A. ArASL2018 Dataset", "content": "Table I and Table II summarize the performance of both transfer learning and transformer-based models on the ArASL2018 dataset. The models were evaluated based on training, validation, and test accuracy, as well as the time taken for training.\nThe results indicate that transfer learning models generally outperform transformer-based models in terms of training efficiency, with MobileNetV2 achieving the fastest training time at 26.52 minutes while maintaining a competitive test accuracy of 99.48%. However, when test accuracy is prioritized over speed, transformer models like Microsoft Swin are superior, achieving a test accuracy of 99.6%, albeit with a significantly longer training time of 580.50 minutes. This suggests that while transfer learning models are more computationally efficient, transformer models may offer slight improvements in accuracy for more demanding applications.\nFigure 6 and Figure 7 illustrate the training and validation accuracy for MobileNetV2 and Microsoft Swin on the ArASL2018 dataset, respectively. MobileNetV2 shows faster convergence with fewer fluctuations compared to the transformer-based Microsoft Swin model. The stability of MobileNetV2 during training is particularly noteworthy, while Swin demonstrates a more gradual and fluctuating convergence pattern."}, {"title": "B. AASL Dataset", "content": "Similar trends were observed on the AASL dataset, as summarized in Table III and Table IV. Here, MobileNetV2 once again emerged as the most efficient model in terms of training time, completing the process in 146.02 minutes while achieving a test accuracy of 99.00%. On the other hand, the Google ViT transformer model achieved the highest test accuracy of 99.43%, albeit at the cost of a slightly longer training time.\nThis dataset further highlights the trade-off between accuracy and computational efficiency. MobileNetV2, while offering strong performance with minimal training time, does not surpass the accuracy of transformer-based models such as Google ViT, which consistently demonstrates superior test accuracy across datasets. However, the difference in training times between these two models on the AASL dataset is minimal, suggesting that transformer models may offer a viable alternative for applications where training time is less of a concern.\nFigure 8 and Figure 9 compare the training and validation accuracy of MobileNetV2 and Google ViT on the AASL dataset. While both models show efficient convergence, Google ViT consistently maintains a higher validation accuracy throughout the training process, indicating better generalization compared to MobileNetV2."}, {"title": "C. Comparison with Other Studies", "content": "Our approach outperforms several state-of-the-art models from other studies, as outlined in Table V. On the ArASL2018 dataset, our Microsoft Swin model surpasses the performance of Hu et al. [11], Abdelghfar et al. [12], and Alnabih et al. [14], achieving an impressive test accuracy of 99.60%. Similarly, on the AASL dataset, our Google ViT model reaches a test accuracy of 99.43%, improving upon the previous work of El-Sayed et al. [13] and Renjith et al. [16]."}, {"title": "VI. CONCLUSION", "content": "In this study, we presented an Arabic Alphabet Sign Language recognition approach using transfer learning with a transformer-based model, achieving state-of-the-art results with 99.6% test accuracy on ArASL2018 and 99.43% on the AASL dataset. Our approach demonstrates significant potential for enhancing communication technologies for the Arabic-speaking deaf and hard-of-hearing community. Future research could focus on implementing real-time translation, extending the method to full sentence recognition, and improving model robustness across diverse signing styles. Additionally, optimizing transformer models for resource-constrained devices would facilitate deployment in real-world applications. Expanding datasets and supporting multilingual sign language recognition could also broaden the system's impact, making it a valuable tool in assistive communication."}]}