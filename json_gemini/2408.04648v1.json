{"title": "PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models", "authors": ["Tikhonov Alexey"], "abstract": "We present PLUGH, a modern benchmark that currently consists of 5 tasks, each with 125 input texts extracted from 48 different games and representing 61 different (non-isomorphic) spatial graphs to assess the abilities of Large Language Models (LLMs) for spatial under- standing and reasoning. Our evaluation of API-based and open-sourced LLMs shows that while some commercial LLMs exhibit strong reasoning abilities, open-sourced competitors can demonstrate almost the same level of qual- ity; however, all models still have significant room for improvement. We identify typical reasons for LLM failures and discuss possible ways to deal with them. Datasets and evalua- tion code are released.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown re- markable capabilities, but there are still tasks that they struggle with. Spatial reasoning, understand- ing, and planning are among these challenging tasks (Valmeekam et al., 2023; Cohn, 2023; Mo- mennejad et al., 2023). In modern projects, this functionality is often implemented using exter- nal mechanisms such as Cognitive Architectures (Sumers et al., 2024). For example, in Simu- lacra (Park et al., 2023), a spatial tree is explic- itly queried, and prompts are formed based on the results.\nAs the quality of LLMs improves, evaluat- ing their progress becomes increasingly difficult (Tikhonov and Yamshchikov, 2023). However, we expect improvements in these aspects. To track this growth, we propose using our benchmark, which is aimed at assessing the quality of understanding the structure of space described in arbitrary fictional texts. Reconstructing such structures from text is a complex task (Ammanabrolu et al., 2020; Am- manabrolu and Riedl, 2021). On the one hand, spa- tial language understanding involves recognizing and reasoning about spatial semantics, e.g., spa- tial objects, relations, and transformations, in nat- ural language descriptions (Clements and Battista, 1992). On the other hand, extracting formal knowl- edge from fictional text implies narrative under- standing (Zhu et al., 2023).\nThe challenge, however, is how to objectively create ground truth annotations, as the concept of location and transitions between locations is inher- ently abstract and context-dependent.\nExisting works usually solve this problem by generating for- mal descriptions from graphs, but such descriptions are often monotonous and lack variability.\nRecently, text-based games have been frequently used as a unique source of information or as agents' playgrounds (Hausknecht et al., 2020; C\u00f4t\u00e9 et al., 2019; Urbanek et al., 2019), as they bridge the gap between formally functioning text-based interac- tive environments and fictional texts. Text-based games are also often used for generating bench- marks (Pan et al., 2023; Tan et al., 2023; Tikhonov, 2024). In (Peng et al., 2023), agents are trained to extract Knowledge Graphs from text stories, and they also use text-based games for data generation.\nWe propose using text-based games to construct such a benchmark, specifically a collection of games with known walkthroughs suitable for run- ning on the Jericho emulator (Hausknecht et al., 2020), resulting in pairs of diverse fictional texts and formal spatial structures.\nThis work contributes by:\n\u2022 Proposing a formal spatial reconstruction and reasoning benchmark with 5 different tasks:\nTask 1: Graph reconstruction\nTask 2a: Character's path reconstruction"}, {"title": "2 Approach", "content": "Text-based games represent a unique source of such information:\n\u2022 On the one hand, the player interacts with a partially observable, modeled environment through actions and observations conveyed in natural language. Thus, the game transcript is quite close to a natural linear fictional text (and can be transformed into one using mod- ern LLMs).\n\u2022 On the other hand, the game code strictly and formally defines the list of available locations and transitions between them. This informa- tion is already described by the game authors and can be extracted and used as ground truth.\nThe general schema of our approach is presented in Figure 1.\n\u2022 We used the Jericho engine (Hausknecht et al., 2020) and several dozen available games with known walkthroughs.\n\u2022 By replaying the walkthrough, we simultane- ously obtained the game transcript and the spatial graph (checking for location changes after each command)."}, {"title": "3 Tasks and Results", "content": "In this section we describe 5 novel tasks based on the data we collected and provide results of mod- ern LLMs evaluation, including 3 models from OpenAI (OpenAI, 2024), the latest model from Anthropic (Anthropic, 2024) and open-source mod- els, LLaMa3 (AI@Meta, 2024) and Mixtral (Jiang et al., 2024).\n3.1 Task 1: Graph Reconstruction\nTask Description: You will be provided with a short fiction text. Your task is to extract the men- tioned locations and compile a description of the locations graph in a graphviz format, undirected, without node descriptions, only with edges without labels for directly connected nodes.\nTarget Data: Ground truth graph description.\nMetrics: F1 scores for nodes and edges retrieval, the higher the better.\nNotes:\n\u2022 Some models ignore some instructions, pro- viding, for example, directed graphs or graphs with edge labels. Adding a few examples to the prompt usually significantly improves it.\n\u2022 It's not always possible to get identical node names (sometimes there are several options to name one node). To minimize the effects of these issues, we provided a relaxed (\"fuzzy\") parsing and matching algorithm. However, we ensure there are no false positives in that matching by avoiding similar location names in ground truth data and requiring that one of the matching items be a substring of another.\n\u2022 One may want to use different graph recon- struction metrics (for example, see Table 9), so we provide a modular code that can be used to add any additional metrics.\nResults for Task 1 for several modern models are presented in Table 2. We find no clear dependency pattern between task 1 metrics and graphs' prop- erties (like nodes, edges, and cycles) on available data.\n3.2 Task 2a: Character's Path Reconstruction\nTask Description: You will be provided with a short fiction text and a list of location names. Your task is to extract the main character's path as a sequence of visited locations, one by one, each on a new line.\nTarget Data: Ground truth path (according to the locations sequence in the walkthrough). The distribution of path lengths across the graphs in Task 2 is provided in Fig 3.\nMetric: Normalized Levenshtein distance, the lower the better.\nResults for Task 2a for several modern models are presented in Table 3.\n3.3 Task 2b: Reversed Character's Path Reconstruction\nTask Description: Same as Task 2a, but we ask models to produce the reversed path."}, {"title": "3.4 Task 3: Novel Shortest Path", "content": "Task Description: You will be provided with a short fiction text and a list of location names. Your task is to extract the shortest path between two given locations (source and target) as a sequence of visited locations starting from the source and ending with the target location, one by one, each on a new line.\nTarget Data: Shortest path between source and target locations (list of them if there are several shortest paths). In all cases, the length of the target path was 3 nodes.\nMetric: Normalized Levenshtein distance, the lower the better.\nNotes: We selected the pair of locations with the following properties:\n\u2022 There is at least one path from one to another in the segment's graph.\n\u2022 Each path has at least 3 steps (so we do not use directly connected locations).\n\u2022 At least one of these paths wasn't presented in the transcript."}, {"title": "3.5 Task 4: Temporal Hinted Shortest Path", "content": "Task Description: You will be provided with a short fiction text and a list of location names. Your task is to extract the shortest path between two given locations (source and target) as a sequence of visited locations starting from the source and ending with the target location, one by one, each on a new line.\nTarget Data: Shortest path between source and target locations (list of them if there are several shortest paths). The distribution of path lengths across the graphs in Task 4 is provided in Fig 4.\nMetric: Normalized Levenshtein distance, the lower the better.\nNotes: We selected the pair of locations with the following properties:\n\u2022 There is at least one path from one to another in the segment's graph.\n\u2022 Each path has at least 3 steps (so we do not use directly connected locations).\n\u2022 At least one of these paths wasn't presented in the transcript.\n\u2022 Instead of explicitly specifying the starting and ending locations, we use hints about where an object was first encountered or last seen in the narrative. For example, \"the place of the first encounter of nugget\" or \"the place where cereal was left.\""}, {"title": "4 Evaluation", "content": "The evaluation results provided in the previous sec- tion were calculated using generations sampled with temperature 0.01 (since some of the models deny usage of zero temperature). Before compari- son to the target data we prepocess and normalize sampled responses, to deal with some technical is- sues, like incorrect graph specification format or ambiguity in location naming (see the next subsec- tion for more details).\nAlthough different tasks rank models differently, one can speculate on the reasons and underlying patterns:\n\u2022 In Task 1, which can be considered a task of spatial summarization, the leader was Claude- 3-opus, followed closely by GPT-4 and GPT- 4o with minor differences.\n\u2022 In the other tasks, which can be considered tasks of spatial reasoning, the leaders on aver- age were GPT-4 and GPT-40, with Claude-3- opus slightly behind.\n\u2022 In Task 2b and Task 4, which requires ad- ditional reasoning (reversing the path and tracking object locations, correspondingly), all models except GPT-4o and GPT-4 showed significant degradation.\n\u2022 Among tested open-source models, LLaMA3 70B led, with Mixtral 8x22B following closely.\n\u2022 Smaller models (including GPT-3.5) found it significantly more challenging to understand the task setup, so their quality was substan- tially lower in the 0-shot setting. Still, it im- proved noticeably with few-shot examples.\n\u2022 Even the top models are still imperfect in solving any of the presented tasks, leaving room for further improvement. The following subsection analyzes some typical errors and discusses their origins, pointing out possible ways for future advancements."}, {"title": "4.1 Analysis of Typical Errors", "content": "In this subsection, we analyze typical problems that arise in models when solving the problems proposed in this paper and discuss how such issues can be detected and addressed in practice.\n\u2022 Incorrect formatting: Sometimes models, especially smaller ones, tend to ignore part of instructions, generating, in particular, directed graphs or graphs with extra information (e.g., labels on edges) (see Table 7 for example). To overcome this problem, we introduced a flex- ible parsing code that we used to preprocess generated texts (see implementation code).\n\u2022 Naming ambiguity: Sometimes models con- fuse or slightly change location names, which is inevitable, but normalization and fuzzy matching help a lot. Normalization includes lowercase, removal of articles, and removal of prepositions at the beginning of names (again, we recommend examining the metrics calcu- lation code for more details). Fuzzy matching threats two names equal if one of them is a substring of another. These efforts are espe- cially important in Task 1, because in other tasks we provide the model with the list of proper location names as a part of input. To illustrate the impact of normalization, we pro- vide the scores calculated with and without preprocessing for several models in Table 8.\n\u2022 Location hallucinations: Sometimes mod- els invent locations that were not visited in the narrative but were mentioned as some- thing seen in the distance. In other cases (es- pecially with smaller models), the model is not always able to grasp the concept of lo- cation and builds a generic nesting graph of the entities mentioned (including containers and objects); see Appendix D for an example. In both cases, this leads to additional false nodes in the graph, which can be illustrated by degradation precision while keeping recall on a relatively high level (see Table 9)."}, {"title": "5 Discussion", "content": "The task of defining and identifying locations and paths within a narrative is inherently complex and somewhat subjective. This complexity raises sev- eral questions about the nature and definition of spatial objects and relations in fictional texts.\nFirstly, one might question whether locations can be nested. For instance, is a closet within a grand- mother's room, where Little Red Riding Hood hid, a separate location or merely an object within a location? This ambiguity extends to whether an encompassing location should be described as a subgraph or just as an intermediate location con- necting other ones; see, for example, the Hundred Acre Wood in Winnie-the-Pooh (Figure 5). An- other debatable aspect is the distinction between a location and a container. Can a place like a \"glass table\" be considered a location? This might depend on the size of the characters and their ability to move around, as seen in Alice in Wonderland (see Figure 6). Moreover, should a location be immo- bile? Seems like it's not necessary, as locations can include a hot air balloon basket, an elevator, or a cab transporting characters during their conversa- tion. The concept of directly connected locations also requires discussion. If a character travels from home to work, how detailed should the description of their intermediate movements be?\nDespite the lack of universal answers to these questions, an average reader can intuitively respond to them without much thought. Modern LLMs, as demonstrated, can also reproduce the author's intended graph with sufficient accuracy (F1-score up to 78+%).\nThis observation leads us to the idea that, despite the apparent subjectivity and arbitrariness of defi- nitions, our perception of space described in a nar- rative is governed by more formal and predictable principles. For example, the description report- ing bias (the principle of omitting trivial things) suggests that writers tend to skip obvious infor- mation while writing texts. In (Fludernik, 1996), the author explores how narratives mimic natural conversation by selectively including or omitting details to focus on what is essential for the story. She argues that this selective reporting aligns with how people naturally communicate, highlighting significant events while excluding the mundane. Similarly, Chekhov's gun principle implies that mentioned ob- jects should be important to the plot.\nThus, while the task of defining and identifying locations in narratives is complex and subjective, our findings suggest that there are underlying prin- ciples guiding our perception of space in narratives. These principles can be leveraged to assess and improve the performance of LLMs in spatial rea- soning tasks, as demonstrated by the results of our benchmark. Future work should continue to ex- plore these principles and refine the methods for evaluating and enhancing the spatial reasoning ca- pabilities of LLMs."}, {"title": "6 Conclusion", "content": "In this paper, we introduced PLUGH, a benchmark designed to evaluate the spatial reasoning capabili- ties of LLMs based on fiction texts. Our evaluation of various models reveals significant variability in performance across different tasks, highlighting the strengths and weaknesses of current LLMs in spatial reasoning. Despite recent advancements, there is substantial room for improvement, particu- larly in addressing common issues such as incorrect formatting, naming ambiguity, and location hallu- cinations.\nOur findings underscore the importance of di- verse benchmarks and robust error analysis tech- niques in advancing the field of spatial reasoning. Future research should focus on developing mod- els with enhanced spatial reasoning capabilities and refining evaluation methodologies to provide a comprehensive assessment of model performance."}]}