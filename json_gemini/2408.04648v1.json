{"title": "PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models", "authors": ["Tikhonov Alexey"], "abstract": "We present PLUGH, a modern benchmark that currently consists of 5 tasks, each with 125 input texts extracted from 48 different games and representing 61 different (non-isomorphic) spatial graphs to assess the abilities of Large Language Models (LLMs) for spatial under-standing and reasoning. Our evaluation of API-based and open-sourced LLMs shows that while some commercial LLMs exhibit strong reasoning abilities, open-sourced competitors can demonstrate almost the same level of qual-ity; however, all models still have significant room for improvement. We identify typical reasons for LLM failures and discuss possible ways to deal with them. Datasets and evaluation code are released.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown re-markable capabilities, but there are still tasks that they struggle with. Spatial reasoning, understanding, and planning are among these challenging tasks (Valmeekam et al., 2023; Cohn, 2023; Momennejad et al., 2023). In modern projects, this functionality is often implemented using external mechanisms such as Cognitive Architectures (Sumers et al., 2024). For example, in Simu-lacra (Park et al., 2023), a spatial tree is explic-itly queried, and prompts are formed based on the results.\nAs the quality of LLMs improves, evaluat-ing their progress becomes increasingly difficult (Tikhonov and Yamshchikov, 2023). However, we expect improvements in these aspects. To track this growth, we propose using our benchmark, which is aimed at assessing the quality of understanding the structure of space described in arbitrary fictional texts. Reconstructing such structures from text is\na complex task (Ammanabrolu et al., 2020; Am-manabrolu and Riedl, 2021). On the one hand, spatial language understanding involves recognizing and reasoning about spatial semantics, e.g., spa-tial objects, relations, and transformations, in nat-ural language descriptions (Clements and Battista, 1992). On the other hand, extracting formal knowl-edge from fictional text implies narrative under-standing (Zhu et al., 2023).\nThe challenge, however, is how to objectively create ground truth annotations, as the concept of location and transitions between locations is inher-ently abstract and context-dependent.\nExisting works (Cohn, 2023; Mirzaee et al., 2021; Momennejad et al., 2023; Arabsheibani et al., 2023) usually solve this problem by generating for-mal descriptions from graphs, but such descriptions are often monotonous and lack variability.\nRecently, text-based games have been frequently used as a unique source of information or as agents' playgrounds (Hausknecht et al., 2020; C\u00f4t\u00e9 et al., 2019; Urbanek et al., 2019), as they bridge the gap between formally functioning text-based interac-tive environments and fictional texts. Text-based games are also often used for generating benchmarks (Pan et al., 2023; Tan et al., 2023; Tikhonov, 2024). In (Peng et al., 2023), agents are trained to extract Knowledge Graphs from text stories, and they also use text-based games for data generation.\nWe propose using text-based games to construct such a benchmark, specifically a collection of games with known walkthroughs suitable for run-ning on the Jericho emulator (Hausknecht et al., 2020), resulting in pairs of diverse fictional texts and formal spatial structures.\nThis work contributes by:\n\u2022 Proposing a formal spatial reconstruction and reasoning benchmark with 5 different tasks:\n\u2013 Task 1: Graph reconstruction\n\u2013 Task 2a: Character's path reconstruction"}, {"title": "2 Approach", "content": "Text-based games represent a unique source of such information:\n\u2022 On the one hand, the player interacts with a partially observable, modeled environment through actions and observations conveyed in natural language. Thus, the game transcript is quite close to a natural linear fictional text (and can be transformed into one using mod-ern LLMs).\n\u2022 On the other hand, the game code strictly and formally defines the list of available locations and transitions between them. This informa-tion is already described by the game authors and can be extracted and used as ground truth.\nThe general schema of our approach is presented in Figure 1.\n\u2022 We used the Jericho engine (Hausknecht et al., 2020) and several dozen available games with known walkthroughs.\n\u2022 By replaying the walkthrough, we simultane-ously obtained the game transcript and the spatial graph (checking for location changes after each command).\n\u2022 Since full-size transcripts and graphs turned out to be very diverse in size (from 3 to several hundred locations), we used a sliding window logic to find \"good\" segments of the walk-through.\n\u2022 A segment is considered good if its corre-sponding graph has 6 to 20 nodes, is con-nected, and sufficiently non-degenerate: the total number of basic cycles and leaf nodes should be at least 4 (thus excluding trivial lin-ear graphs). An example of a good segment is shown in Figure 2.\n\u2022 Then, the transcript of each segment was rewritten using the GPT-4 model into a fic-tion text (see the used prompt in Appendix A). An example of the text before and after rewriting is shown in Appendix B.\nTo validate the graph-text pairs, we performed the following checks:\n\u2022 The text should contain all node names as substrings.\n\u2022 The graph should not contain duplicate nodes or nodes whose names are substrings of each other.\nAs a result of this filtering, we obtained 125 seg-ments from 48 unique games. The average graph"}, {"title": "3 Tasks and Results", "content": "In this section we describe 5 novel tasks based on the data we collected and provide results of mod-ern LLMs evaluation, including 3 models from OpenAI (OpenAI, 2024), the latest model from Anthropic (Anthropic, 2024) and open-source mod-els, LLaMa3 (AI@Meta, 2024) and Mixtral (Jiang et al., 2024)."}, {"title": "3.1 Task 1: Graph Reconstruction", "content": "Task Description: You will be provided with a short fiction text. Your task is to extract the men-tioned locations and compile a description of the locations graph in a graphviz format, undirected, without node descriptions, only with edges without labels for directly connected nodes.\nTarget Data: Ground truth graph description.\nMetrics: F1 scores for nodes and edges retrieval, the higher the better.\nNotes:\n\u2022 Some models ignore some instructions, pro-viding, for example, directed graphs or graphs with edge labels. Adding a few examples to the prompt usually significantly improves it.\n\u2022 It's not always possible to get identical node names (sometimes there are several options to name one node). To minimize the effects of these issues, we provided a relaxed (\"fuzzy\") parsing and matching algorithm. However, we ensure there are no false positives in that matching by avoiding similar location names in ground truth data and requiring that one of the matching items be a substring of another.\n\u2022 One may want to use different graph recon-struction metrics (for example, see Table 9), so we provide a modular code that can be used to add any additional metrics.\nResults for Task 1 for several modern models are presented in Table 2. We find no clear dependency pattern between task 1 metrics and graphs' prop-erties (like nodes, edges, and cycles) on available data."}, {"title": "3.2 Task 2a: Character's Path Reconstruction", "content": "Task Description: You will be provided with a short fiction text and a list of location names. Your task is to extract the main character's path as a sequence of visited locations, one by one, each on a new line.\nTarget Data: Ground truth path (according to the locations sequence in the walkthrough). The distribution of path lengths across the graphs in Task 2 is provided in Fig 3.\nMetric: Normalized Levenshtein distance, the lower the better.\nResults for Task 2a for several modern models are presented in Table 3."}, {"title": "3.3 Task 2b: Reversed Character's Path Reconstruction", "content": "Task Description: Same as Task 2a, but we ask models to produce the reversed path."}, {"title": "3.4 Task 3: Novel Shortest Path", "content": "Task Description: You will be provided with a short fiction text and a list of location names. Your task is to extract the shortest path between two given locations (source and target) as a sequence of visited locations starting from the source and ending with the target location, one by one, each on a new line.\nTarget Data: Shortest path between source and target locations (list of them if there are several shortest paths). In all cases, the length of the target path was 3 nodes.\nMetric: Normalized Levenshtein distance, the lower the better.\nNotes: We selected the pair of locations with the following properties:\n\u2022 There is at least one path from one to another in the segment's graph.\n\u2022 Each path has at least 3 steps (so we do not use directly connected locations).\n\u2022 At least one of these paths wasn't presented in the transcript."}, {"title": "3.5 Task 4: Temporal Hinted Shortest Path", "content": "Task Description: You will be provided with a short fiction text and a list of location names. Your task is to extract the shortest path between two given locations (source and target) as a sequence of visited locations starting from the source and ending with the target location, one by one, each on a new line.\nTarget Data: Shortest path between source and target locations (list of them if there are several shortest paths). The distribution of path lengths across the graphs in Task 4 is provided in Fig 4.\nMetric: Normalized Levenshtein distance, the lower the better.\nNotes: We selected the pair of locations with the following properties:\n\u2022 There is at least one path from one to another in the segment's graph.\n\u2022 Each path has at least 3 steps (so we do not use directly connected locations).\n\u2022 At least one of these paths wasn't presented in the transcript.\n\u2022 Instead of explicitly specifying the starting and ending locations, we use hints about where an object was first encountered or last seen in the narrative. For example, \"the place of the first encounter of nugget\" or \"the place where cereal was left.\"\nResults for Task 4 for several modern models are presented in Table 6."}, {"title": "4 Evaluation", "content": "The evaluation results provided in the previous sec-tion were calculated using generations sampled\nclosely.\n\u2022 Smaller models (including GPT-3.5) found it significantly more challenging to understand the task setup, so their quality was substan-tially lower in the 0-shot setting. Still, it im-proved noticeably with few-shot examples.\n\u2022 Even the top models are still imperfect in solving any of the presented tasks, leaving room for further improvement. The following subsection analyzes some typical errors and discusses their origins, pointing out possible ways for future advancements."}, {"title": "4.1 Analysis of Typical Errors", "content": "In this subsection, we analyze typical problems that arise in models when solving the problems proposed in this paper and discuss how such issues can be detected and addressed in practice.\n\u2022 Incorrect formatting: Sometimes models, especially smaller ones, tend to ignore part of instructions, generating, in particular, directed graphs or graphs with extra information (e.g., labels on edges) (see Table 7 for example). To overcome this problem, we introduced a flex-ible parsing code that we used to preprocess generated texts (see implementation code).\n\u2022 Naming ambiguity: Sometimes models con-fuse or slightly change location names, which is inevitable, but normalization and fuzzy"}, {"title": "5 Discussion", "content": "The task of defining and identifying locations and paths within a narrative is inherently complex and somewhat subjective. This complexity raises several questions about the nature and definition of spatial objects and relations in fictional texts.\nFirstly, one might question whether locations can be nested. For instance, is a closet within a grand-mother's room, where Little Red Riding Hood hid, a separate location or merely an object within a location? This ambiguity extends to whether an encompassing location should be described as a subgraph or just as an intermediate location con-necting other ones; see, for example, the Hundred Acre Wood in Winnie-the-Pooh (Figure 5). An-other debatable aspect is the distinction between a location and a container. Can a place like a \"glass table\" be considered a location? This might depend on the size of the characters and their ability to move around, as seen in Alice in Wonderland (see Figure 6). Moreover, should a location be immo-bile? Seems like it's not necessary, as locations can include a hot air balloon basket, an elevator, or a cab transporting characters during their conversa-tion. The concept of directly connected locations also requires discussion. If a character travels from home to work, how detailed should the description of their intermediate movements be?\nDespite the lack of universal answers to these questions, an average reader can intuitively respond to them without much thought. Modern LLMs, as demonstrated, can also reproduce the author's intended graph with sufficient accuracy (F1-score up to 78+%).\nThis observation leads us to the idea that, despite the apparent subjectivity and arbitrariness of defi-nitions, our perception of space described in a nar-rative is governed by more formal and predictable principles. For example, the description report-ing bias (the principle of omitting trivial things) suggests that writers tend to skip obvious infor-mation while writing texts. In (Fludernik, 1996), the author explores how narratives mimic natural conversation by selectively including or omitting details to focus on what is essential for the story. She argues that this selective reporting aligns with how people naturally communicate, highlighting significant events while excluding the mundane.\nThus, while the task of defining and identifying locations in narratives is complex and subjective, our findings suggest that there are underlying prin-ciples guiding our perception of space in narratives. These principles can be leveraged to assess and improve the performance of LLMs in spatial rea-soning tasks, as demonstrated by the results of our benchmark. Future work should continue to ex-plore these principles and refine the methods for"}, {"title": "6 Conclusion", "content": "In this paper, we introduced PLUGH, a benchmark designed to evaluate the spatial reasoning capabili-ties of LLMs based on fiction texts. Our evaluation of various models reveals significant variability in performance across different tasks, highlighting the strengths and weaknesses of current LLMs in spatial reasoning. Despite recent advancements, there is substantial room for improvement, particu-larly in addressing common issues such as incorrect formatting, naming ambiguity, and location hallu-cinations.\nOur findings underscore the importance of di-verse benchmarks and robust error analysis tech-niques in advancing the field of spatial reasoning. Future research should focus on developing mod-els with enhanced spatial reasoning capabilities and refining evaluation methodologies to provide a comprehensive assessment of model performance."}, {"title": "A Conversion Prompt", "content": "The GPT-4 prompt we used to convert transcripts into fiction texts:\n{\"role\": \"system\", \"content\": \"You are the extremely powerful AI-powered fiction text writer. You will be provided with a transcript of an interactive fiction game some player sends actions, and a game returns observation. Your task is to rewrite it into a good-looking fiction text.\"},\n{\"role\": \"user\", \"content\": \"# Transcript <...>\"},\n{\"role\": \"system\", \"content\": \"\"\"Rewrite it into a good-looking fiction text put it into third person PoV, replace artificial looking text structures with normal narrative, remove redundant and repeating pieces.\nKEEP MENTIONS OF LOCATION NAMES DURING TRAVELS.\"\"\"},"}, {"title": "B Example of rewriting results", "content": "An example of the text before and after rewriting.\nBefore:\n<...>\nTunnel\nYou are in a narrow tunnel whose entrance has been blocked by gigantic, translucent boulders. Your only option is to continue to the northeast, where the tunnel gradually widens.\n> examine boulders\nThe boulders are, in reality, milky white crystals and fairly regular in shape, like giant diamonds.\n> ne\nCultural Complex\nThis imposing ante-room, the center of what was apparently the cultural center of the GUE, is adorned in the ghastly style of the GUE\u015b \"Grotesque Period.\" With leering gargoyles, car-toonish friezes depicting long-forgotten scenes of GUE history, and primitive statuary of pointy-headed personages unknown (perhaps very, very distant progenitors of the Flatheads), the place would have been best left undiscovered. North of here, a large hallway passes under the roughly hewn inscription \"Convention Center.\" To the east, under a fifty-story triumphal arch, a passageway the size of a large city boulevard opens into the Royal Theater. A relatively small and unobtrusive sign (perhaps ten feet high) stands nearby. South, a smaller and more dignified (i.e. post-Dimwit) path leads into what is billed as the \"Hall of Science.\"\n> n\nConvention Center Lobby\nYou are standing at the entrance of the GUE Convention Center. <...>\nAfter:\n<...> giant, translucent boulders tumbled from the ceiling, effectively sealing them inside. The only path forward lay to the northeast, where the tunnel gradually widened, beckoning them deeper into the unknown.\nThe boulders, upon closer inspection, revealed themselves as milky white crystals, shaped like enormous, uncut diamonds. Pushing forward, the adventurer entered the Cultural Complex, a grandiose ante-room adorned in the grotesque style of the GUE's 'Grotesque Period'. Leering gargoyles and cartoonish friezes depicted obscure scenes of forgotten history, surrounding primitive statues of pointy-headed figures. To the north, a hallway led to the Convention Center; to the east, a grand passageway opened into the Royal Theater.\nVenturing north, the adventurer found themselves in the lobby of the Convention Center. <...>"}, {"title": "C All non-isomorphic graphs", "content": ""}, {"title": "D Hallucinated locations example", "content": ""}]}