{"title": "PositionID: LLMs can Control Lengths, Copy and Paste\nwith Explicit Positional Awareness", "authors": ["Zekun Wang", "Feiyu Duan", "Yibo Zhang", "Wangchunshu Zhou", "Ke Xu", "Wenhao Huang", "Jie Fu"], "abstract": "Large Language Models (LLMs) demonstrate\nimpressive capabilities across various domains,\nincluding role-playing, creative writing, math-\nematical reasoning, and coding. Despite these\nadvancements, LLMs still encounter challenges\nwith length control, frequently failing to ad-\nhere to specific length constraints due to their\ntoken-level operations and insufficient train-\ning on data with strict length limitations. We\nidentify this issue as stemming from a lack\nof positional awareness and propose novel\napproaches-PositionID Prompting and Posi-\ntionID Fine-Tuning\u2014to address it. These meth-\nods enhance the model's ability to continuously\nmonitor and manage text length during genera-\ntion. Additionally, we introduce PositionID CP\nPrompting to enable LLMs to perform copy and\npaste operations accurately. Furthermore, we\ndevelop two benchmarks for evaluating length\ncontrol and copy-paste abilities. Our exper-\niments demonstrate that our methods signifi-\ncantly improve the model's adherence to length\nconstraints and copy-paste accuracy without\ncompromising response quality.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demon-\nstrated remarkable capabilities in various domains,\nsuch as role-playing (Wang et al., 2023b), creative\nwriting (Wang et al., 2024), mathematical reason-\ning (Shao et al., 2024), and coding (Roziere et al.,\n2023). These advanced LLMs often undergo multi-\ntask supervised instruction tuning (SFT), endow-\ning them with strong instruction-following abilities.\nAdditionally, an additional alignment training stage\nafter SFT further aligns LLMs with human values\nand needs. A notable outcome of this alignment\ntraining is that models tend to produce longer and\nmore detailed responses, thereby enhancing the\nfaithfulness, honesty, and helpfulness of their out-\nputs (Fu et al., 2022; Li et al., 2023; Dubois et al.,\n2024; Achiam et al., 2023).\nHowever, in many scenarios, longer responses\nare not necessarily better. Users may often prefer\noutputs that follow specific length constraints or\nconditions. Previous studies indicate that even the\nmost advanced LLMs, such as GPT-4 (Achiam\net al., 2023), struggle to precisely follow length\nconstraints (Zhou et al., 2023; Sun et al., 2023). For\nexample, when users request a model to generate\na text strictly within 500 words, the model often\nproduces outputs exceeding this requirement.\nWe hypothesize that the LLMs' weak adherence\nto length control instructions can be attributed to\ntwo primary issues:\n\u2022 The model's tokenizer typically operates at the\ntoken level rather than the word level, which\nmay mislead the model's perception of the\nword count.\n\u2022 The training data may contain a few examples\nwith strict length constraints (e.g., in 3 words),\nwith most data reflecting broad length control\nrequirements, such as \u201cshort\u201d or \u201clong\u201d.\nIntuitively, length control inherently requires an\nunderstanding of positional relationships within the\ntext. For example, if a model is tasked with gener-\nating a summary that is exactly 50 words long, it\nneeds to continuously monitor the number of words\nit has generated so far and how many words remain\nto meet the target length. This involves keeping\ntrack of its progress (i.e., the number of words be-\ning generated) within the text to ensure it stays\nwithin the specified limit. However, the model's\nfocus on token-level operations and the lack of\nstrict length-constrained training examples impair\nits ability to effectively and accurately monitor its"}, {"title": "progress within the text, leading to inaccuracies\nin following length control instructions. We refer\nto this lack of real-time awareness of the gener-\nated text's length by the models as the positional\nawareness issue.\nOur work is not the first to focus on improving\nthe model's positional awareness. For example,\nAbacus Embeddings (McLeish et al., 2024) en-\nhances the model's mathematical computation ca-\npabilities by adding positional embeddings to each\ndigit of a number. Similarly, Contextual Position\nEncoding (Golovneva et al., 2024) employs a gate\nmechanism based on the query-key map to deter-\nmine positional embeddings, making the positional\nembeddings context-aware and higher-level. This\ndesign improves the model's performance in tasks\nsuch as counting and selective copying. However,\nboth of them have several limitations: (1) modify-\ning the positional encoding is unfriendly to the off-\nthe-shelf LLMs, as it may degrade their generalist\ncapabilities, and it requires model re-training when\naltering the position encoding approach. (2) And\nthey are not suitable for the closed-source LLMs\nwhich only provide invoking APIs. Furthermore,\n(3) these methods are only applicable in limited sce-\nnarios, such as some toy tasks or arithmetic tasks,\nand they demonstrate limited effectiveness when\napplied to more complex real-world instructions.\nTo address these issues, we propose novel ap-\nproaches to enhance the model's positional aware-\nness: PositionID Prompting and PositionID Fine-\nTuning for length control tasks.\nIn these approaches, we use PositionID to de-\nnote the position of each unit in the text, where the\nunit can be defined as a word, a sentence, a para-\ngraph, etc., according to specific requirements. For\nexample, at the word-level, each word is assigned\na unique PositionID to indicate its position in the\nsequence, such as \u201cThe quick brown fox jumps\nover the lazy dog.", "The[1]\nquick[2] brown[3] fox[4] jumps[5] over[6] the[7]\nlazy[8] dog[9].": "hen the position ids are assigned.\nSimilarly, at the sentence-level, \u201cThe quick brown\nfox jumps over the lazy dog. A swift auburn fox\nleaps across a sleeping canine.\u201d can be converted\ninto \"The quick brown fox jumps over the lazy\ndog.[1] A swift auburn fox leaps across a sleeping\ncanine.[2]\" when the position IDs are assigned.\nAs shown in Figure 1, PositionID Prompting is a\ntuning-free technique that enables LLMs to count\nthe number of units continuously during text gener-\nation. PositionID Fine-Tuning involves training the"}, {"title": "model with data in a similar PositionID Prompting\nformat, thereby enhancing the model's positional\nawareness. Through different system prompts, this\nmethod allows flexible control to enable or disable\nthe PositionID prompting mode without compro-\nmising positional awareness.\nMoreover, we validate another interesting fea-\nture brought by explicit positional awareness,\nnamely, copy and paste (CP) abilities. To en-\nable this, we propose PositionID CP Prompt-\ning, which involves a three-stage tool-use mech-\nanism: (1) inserting position ids in the previous\ntext upon generating a \u201c<COPY>\u201d token, (2) con-\ntinuing generating the tool call \u201c<COPY>[tag=t]\n[desc=d] [start=s] [end=e]</COPY>", "instructions": "LenCtrl-Bench\nand CP-Bench. LenCtrl-Bench includes a training\nset of 28,135 samples to enhance the positional\nawareness of open-source models, and a test set\nof 2,817 samples. CP-Bench contains a test set\nof 182 samples carefully selected from the GPT-4 (Achiam\net al., 2023) synthesized samples and\nthe manually crafted samples.\nExperiments on LenCtrl-Bench demonstrate that\nthrough PositionID Prompting and PositionID Fine-\nTuning, models can more accurately follow length-\ncontrolled text generation instructions without com-\npromising response quality. Additionally, our work\nis the first to study the copy and paste tool-use\nabilities of current LLMs. Experiments on CP-\nBench confirm the effectiveness of our proposed\nPositionID CP Prompting. It can not only accu-\nrately copy text spans, but also demonstrate supe-\nrior response quality."}, {"title": "2 Datasets and Benchmarks", "content": "Due to the lack of instruction datasets and bench-\nmarks specifically designed to evaluate models'\nabilities to follow instructions with strict length\nconstraints and to perform accurate copying and\npasting, we construct two novel benchmarks:\nLenCtrl-Bench (Length-Controlled Text Gener-\nation Benchmark) and CP-Bench (Copy-Paste\nBenchmark). Both of these benchmarks are de-\nsigned with diverse instructions, aiming to compre-\nhensively evaluate the models' abilities to follow"}, {"title": "instructions while measuring their length control\nand copy-paste capabilities.", "content": null}, {"title": "2.1 LenCtrl-Bench", "content": "The LenCtrl-Bench is constructed from two\nwell-recognized and large-scale instruction-tuning\ndatasets, namely, Alpaca-52K (Taori et al., 2023)\nand OpenHermes 2.0 (Teknium, 2023), along with\none additional dataset for text summarization, i.e.,\nWikiHow (Koupaee and Wang, 2018). Alpaca-52K\nand OpenHermes 2.0 involve diverse real-world or\nGPT-synthesized instructions, but their response\nlengths are often relatively short. In contrast, due\nto the nature of text summarization, WikiHow in-\ncludes longer texts. By combining these three\ndatasets, we can construct a dataset with a broader\nlength distribution.\nTo evaluate models' length control ability with\ndifferent levels of length constraints, we design\nthree granularities: (1) word-level, (2) sentence-\nlevel, and (3) paragraph-level length constraints.\nWe determine the granularity of each sample ac-\ncording to the overall distribution balance and the\nlength of the source data response.\nWe use NLTK2 to count the number of words or\nsentences in the original responses of the source\ndatasets. For WikiHow, we use \u201c\\n\\n\u201d as a sep-\narator to count the number of paragraphs. Sub-\nsequently, we use GPT-4 (Achiam et al., 2023)\nto design 24 verbalized instruction templates for\nlength constraints, such as \u201cWith a response length\nof {length} word(s).\u201d, \u201cFrame your response with\n{length} sentence(s).\u201d, or \u201cExpand your response\nto {length} paragraph(s).\". The length constraint\ninstruction is then concatenated to the original in-\nstruction for each sample. And we use the original\nground-truth responses.\nMoreover, we remove non-English data, data\ncontaining code or mathematical expressions, data\nexceeding 1,000 words in length, and duplicated\nsamples. The resulting dataset includes 32,476\nsamples, which are divided into a training split and\na test split with a 10:1 ratio. Note that the training\nsplit serves as the SFT dataset for PositionID Fine-\nTuning when the position ids are assigned for the\nresponses (c.f., \u00a73.1).\nWe refer the reader to Appendix A for more\ndetails about LenCtrl-Bench, and to Appendix E\nfor examples in LenCtrl-Bench.\""}, {"title": "2.2 CP-Bench", "content": "The construction of CP-Bench adopts two ap-\nproaches: (1) constructing from certain off-the-\nshelf datasets with frequent textual span repeats,\nand (2) synthesizing by GPT-4 (Achiam et al.,\n2023). We brainstorm eight patterns for copy and\npaste, such as option selection, law article state-\nment, terminology reiteration, note-taking, URL\nrepeating, quotation, policy statement, and general\ntext span repeating. The construction approaches,\nthe source datasets, and the descriptions for these\nCP patterns can be found in Table 5, and their\ndemonstrations are presented in Appendix F.\nFor Approach (1), we collect the source data\nfrom open-source datasets (as detailed in Table 5).\nFor each sample in these datasets, we identify re-\npeated sentence blocks (i.e., a sequence of consec-\nutive sentences) and repeated spans under certain\nregex patterns (e.g., quoted texts, URLs, option\ncontents) as the parts to be copied and pasted. We\nremove samples with too short repeated parts un-\nless they match the aforementioned regex patterns.\nFor Approach (2), GPT synthesis, given a CP\npattern, GPT-4 is first asked to design 20-40 topics.\nGPT-4 then generates 20 samples for each topic\nwith the repeated spans marked by itself (using\nsquare brackets for example). The marked repeated\nspans are the parts to be copied and pasted.\nSubsequently, we process these textual forms\nof the parts to be copied and pasted into tool call-\ning forms. Specifically, for each group of iden-\ntical parts to be copied and pasted, all instances\nexcept for the first occurrence are replaced with\na paste tool call, and a copy tool call is inserted\nbefore the second occurrence. For example, for the\ntext \"My phone number is 1234567. If you miss\nme, please call 1234567 because 1234567 is my\nphone number,\" the converted form is \"My phone\nnumber is 1234567. If you miss me, please call\n{copy}{paste} because {paste} is my phone num-\nber,\" where {copy} and {paste} are the tool calls\ndetailed as follows.\nRegarding the copy and paste tool calls, the\nformat of {copy} is actually \u201c<COPY> [tag=t]\n[desc=d] [start=s] [end=e]</COPY>\u201d, and that of\n{paste} is actually \u201c<PASTE> [tag=t] </PASTE>", "<COPY>": "n", "<PASTE>": "and \u201c</PASTE>\u201d, de-"}, {"title": "3 Method", "content": "As shown in Figure 1, we have two pipelines: The\nPositionID Prompting and PositionID Fine-Tuning\nare designed for length control (\u00a73.1); The Posi-\ntionID CP Prompting is used for precise copying\nand pasting (\u00a73.2)."}, {"title": "3.1 PositionID for Length Control", "content": "PositionID Prompting. This approach is primar-\nily suitable for closed-source LLMs such as GPT-4\n(Achiam et al., 2023) due to their lack of public\navailability for training. As shown in Figure 1,\ncompared with vanilla prompting, which directly\nprompts the models with the user query and the\nlength constraint instruction, PositionID Prompt-\ning elicits the models to generate position IDs for\neach word, sentence, or paragraph unit using the\nprompt templates illustrated in Box C. For example,\nfor a 5-word output, Vanilla Prompting may gener-\nate \"The sun is shining brightly.\u201d, while PositionID\nPrompting generates \u201cThe 1 sun 2 is 3 shining 4\nbrightly 5.\" with the number after each word de-\nfined as the position IDs. This is the same case for\nsentence-level and paragraph-level position IDs.\nThis way, the model continuously counts the\nnumber of generated words (sentences, or para-\ngraphs) during the next token prediction, enhanc-\ning its positional awareness. The position signals\ninherent in the ever-growing context during the in-\nference stage enable the model to continuously and\nmore effectively control the progress of text gen-\neration, thereby achieving more strict control over\nthe generated text length.\nPositionID Fine-Tuning. To enhance positional\nawareness in open-source models for length con-\ntrol, we propose a novel approach called PositionID\nFine-Tuning. As shown in Figure 1, we train the\nlanguage model in a mixture of normal and Posi-\ntionID modes.\nIn the normal mode, the system prompt instructs\nthe model to follow standard length control guide-\nlines, such as: \u201cYou are an assistant that strictly fol-"}, {"title": "lows the length constraint.\" In this mode, the train-\ning labels do not include position IDs. In contrast,\nthe PositionID mode modifies the system prompt to\ninclude positional awareness instructions, such as:\n\"You are an assistant that generates your response\nwhile continuously counting the response length to\nfacilitate length control.\" In this mode, the training\nlabels are assigned position IDs.\nTraining the model in both modes effectively\ntransfers the positional awareness learned in the Po-\nsitionID mode to the normal mode. Consequently,\nduring the inference stage, the system prompt is\nset to the normal mode to ensure a clean response\ngeneration without position IDs.", "content": null}, {"title": "3.2 PositionID for Copy and Paste", "content": "PositionID CP Prompting. As shown in Fig-\nure 1, PositionID CP Prompting involves a three-\nstep function-calling mechanism that heavily relies\non the tool-use capabilities of LLMs (Qin et al.,\n2023; Wang et al., 2023a). These three steps are (1)\nthe pre-generation phase, (2) the copy tool calling\nphase, and (3) the paste tool calling phase.\nIn the pre-generation phase, the LLM generates\ntext up to the token \u201c<COPY>", "<COPY>": "oken, the LLM stops its text\ngeneration process and invokes external tools (e.g.,\nNLTK) to assign position IDs to each word. This\noperation modifies the context for the LLM due to\nthe insertion of numerous position IDs.\nDuring the copy tool calling phase, the visibility\nof position IDs allows the LLM to generate copy\ntool calls with precise position parameters, namely,\ns and e in \u201c<COPY>[tag=t] [desc=d] [start=s]\n[end=e]</COPY>\u201d. Upon generating the token\n\u201c</COPY>\u201d, the external copy tool copies the con-\ntent between position s and position e to an external\nclipboard, which stores the copied text spans asso-\nciated with their tags. The description parameter in\nthe copy tool calls serves as an auxiliary signal for\nthe LLM to understand the content represented by\nthe copy tool calls, facilitating subsequent pasting.\nFinally, in the paste tool calling phase,\nthe model generates paste tool calls, such as", "PASTE>[tag=t]</PASTE>": "based on the de-\nscription parameters of the copy tool calls to paste\nthe copied content tagged with t. When the token\n\u201c</PASTE>\u201d is generated, the paste tool executes\nand replaces the paste tool call span with the cor-"}, {"title": "responding copied content. The text generation\nprocess then continues until it either ends or en-\ncounters the next \u201c<PASTE>\u201d or \u201c<COPY>\u201d token,\nthereby repeating the corresponding process.", "content": null}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nBaselines For the length control task, we com-\npare our PositionID prompting with (1) zero-shot\nprompting, which specifies the length constraint in\nthe instruction without using position ids, (2) few-\nshot prompting, which follows the same instruction\nas zero-shot prompting, but includes three demon-\nstrations in the context, (3) CoT prompting, which\nleverages the Chain of Thought (CoT (Wei et al.,\n2022)) techniques for length control. CoT Prompt-\ning uses an additional prompt: \u201cPlease first gener-\nate your thoughts about how to control the length\nand then generate the content\u201d. (4) result-informed\nprompting, wherein the LLM is prompted with the\ninitial response obtained from zero-shot prompting\naugmented by the difference between the generated\nresponse length and the required length, thereby en-\nabling the model to refine its outputs. (5) truncation,\nwhich involves suppressing the end-of-sentence to-\nken and stopping the model's generation once the"}, {"title": "specified length is attained. The truncation-based\nbaseline is theoretically stringent in adhering to\nthe length constraint; however, it may not guaran-\ntee high-quality content generation. We compare\nour PositionID fine-tuning with (1) CFT, which\ninvolves constraint-specific fine-tuning with non-\nverbalized length control attributes (Zhou et al.,\n2023), and (2) InstructCTG, which is fined-tuned\nsolely on the normal mode of PositionID Fine-\nTuning (Zhou et al., 2023).\nFor the copy and paste task, we compare our\nPositionID CP prompting with the subsequent base-\nlines: (1) zero-shot non-CP prompting: the model\ndirectly generates responses following the instruc-\ntion. (2) few-shot CP prompting: similar to Posi-\ntionID CP Prompting, without inserting PositionID\nwhen using the copy tool.\nEvaluation metrics In terms of the length con-\ntrol task, we employ Rouge-L to assess the quality\nof the generated responses, and the Mean Absolute\nError (MAE) to evaluate the difference between\nthe length of the model's generated response and\nthat required by the instructions. We evaluate on\nthree levels of granularity: word-level, sentence-\nlevel, and paragraph-level.\nFor the copy-and-paste task, we adopt Rouge-"}, {"title": "4.2 Main results", "content": "We present our main results in Table 1, 2 and 3.\nOur key findings are summarized as follows:\nPositionID Prompting is effective for Length\nControl. Compared to other length control\nprompting baselines, our method achieves the best\nperformance across all levels. Additionally, re-\ngarding the Rouge-L metric, PositionID prompting\nachieves a score of 23.2, outperforming all other\nbaselines. This indicates that PositionID prompting\nnot only effectively controls the length of the gener-\nated responses but also maintains or even enhances\nthe quality of the responses.\nPositionID Fine-Tuning enhances model's po-\nsitional awareness. Furthermore, in our length\ncontrol fine-tuning experiments, we observe that\nPositionID fine-tuning consistently outperforms\nboth CFT and InstructCTG in terms of MAE met-\nrics across all levels. Notably, our method also\nachieves 19.8 in Rouge-L score, which is the clos-\nest to the zero-shot performance. This result sug-\ngests that our PositionID fine-tuning method not\nonly enhances the accuracy of response length con-\ntrol but also preserves the quality of the generated\nresponses compared to the baseline methods. At\nthe paragraph level, PositionID fine-tuning does\nnot yield the best MAE results compared to its\ninitialization. We will discuss this result in \u00a74.3.\nPositionID improves the functionality of the\ncopy and paste tools For the copy-and-paste\ntask, we evaluate from four perspectives in com-\nparison to few-shot prompting. (1) The successful\nuse of the tool: we notice a significant increase in\nthe CP Success Rate of PositionID CP Prompting.\nAdditionally, in terms of Consistency, our method\nachieves a higher win rate, implying that using the\ncopy tool with position ids can better capture the"}, {"title": "key information of instruction. (2) Accuracy of the\npaste position: PositionID CP Prompting also takes\nthe lead in PPL (8.4) and Clarity (33.7 winning\nrate). (3) Quality of the response content: Posi-\ntionID CP Prompting achieved 18.4 and 55.8% in\nRouge-L and Accuracy respectively, surpassing the\nbaseline. (4) Overall usage of the tools: PositionID\nscores 3.5 in tool-use proficiency, higher than 2.4\nof few-shot.", "content": null}, {"title": "4.3 Ablation Studies", "content": "Generality of PositionID Granularities for Posi-\ntionID Prompting. We further investigate how\ndifferent PositionID granularities influence the gen-\neration of responses. From Table 4, we observe that\nPositionID prompts at a specific level achieve the\nbest MAE scores at its corresponding granularity,\nsignificantly outperforming PositionID prompts at\nother levels for the same granularity. For exam-\nple, word-level PositionID Prompting performs the\nbest at the word-level length constraint, but is sub-\noptimal for sentence-level length constraint. From\nFigure 15 in the Appendix G, it's also apparent\nthat each prompt at varying levels exhibits a sig-\nnificant decrease in the precision of length control\nacross different granularities. This confirms that\nPositionID prompts at various levels can only en-\nhance the model's ability to control length at the\nspecific granularity, while adversely affecting at\nother granularities.\nEffectiveness of PositionID Prompting using Dif-\nferent Models. We explore the performance of\ndifferent models in length control tasks. We eval-\nuate four models: GPT-4, Yi-6B-Chat, Mistral-\n7B-Instruct (Jiang et al., 2023), and Vicuna-7B\n(Chiang et al., 2023), under both zero-shot and\nPositionID Prompting settings. The results are\npresented in Figure 2. Our findings reveal that:\n(1) GPT-4 outperforms all other models, achiev-"}, {"title": "ing the highest scores in MAE at different lev-\nels, and demonstrating strong results in Rouge-L;\n(2) Among the open-source models, Mistral-7B-\nInstruct stands out as the best performer, particu-\nlarly excelling at the word level, while Yi-6B-Chat\nranks last; (3) However, the usage of PositionID\nPrompting leads to the most notable improvement\nfor Yi-6B-Chat, reducing its MAE at the word\nlevel by 14.6. In contrast, Mistral-7B-Instruct and\nVicuna-7B show a decrease in performance when\nemploying PositionID Prompting.\nEffectiveness of PositionID Prompting and Po-\nsitionID Fine-Tuning under Different Length\nConstraints. Additionally, we try to understand\nhow the constraint length, as specified in the in-\nstructions, affects the model's output. We illustrate\nthe results of PositionID Prompting with GPT, and\nPositionID Fine-Tuning with Yi-6B-Chat in Figure\n3. We discover that:\n(1) At the word and sentence levels, MAE tends\nto rise with increasing constraint length, while the\nRouge-L score exhibits an opposite trend. This\nindicates that the model faces challenges in gen-\nerating longer texts: it not only loses precision in\ncontrolling the response length but also experiences\na decline in the quality of the output. One possi-", "content": null}, {"title": "ble reason for the increase in MAE could be the\nmodel's inability to accurately follow short length\nconstraint requirement (1-57), which leads to cu-\nmulative errors when generating longer sentences.\nThis could explain why PositionID did not per-\nform optimally at the paragraph level for fune-\ntuning setting, as shown in Table 1: given that\nparagraphs are typically longer than words and sen-\ntences, the model's capacity to utilize position ids\nmay decline.\n(2) At the paragraph level, the lowest MAE is\ntypically not observed at the beginning of the range\n(3-7 in Figure 3 (c)), but rather occurs later (12-\n17 in Figure 3 (c)). The Rouge-L score does not\nshow a clear trend of increase or decrease. In fact,\nwe find that the model tends to produce shorter\nparagraphs when required to generate more para-\ngraphs. For example, in the results of GPT-4 with\nPositionID Prompting at the paragraph level, as\nthe constraint length increases from 3-7 to 12-17,\nthe average paragraph length decreases from 71.8\ntokens to 57 tokens. As the constraint length in-\ncreases further, the paragraph length remains ap-\nproximately 56 tokens. This suggests that although\nmore paragraphs are required to be generated, the\ninterval between position ids decreases, potentially", "content": null}, {"title": "enhancing the model's sensitivity to text length and\nposition id reasoning.\nWe refer the reader to Appendix G for more ex-\nperiments including the more metrics for response\nquality evaluation, more results on the effect of\nconstraint length on prompting, more models for\nPositionID Fine-Tuning, and ablation experiments\non the mode mixing for PositionID Fine-Tuning.", "content": null}, {"title": "5 Related Works", "content": "Positional awareness of Large Language Mod-\nels (LLMs) significantly impacts their perfor-\nmance (Zhao et al., 2023; Touvron et al., 2023;\nSu et al., 2021; McLeish et al., 2024; Golovneva\net al., 2024). Techniques like Abacus Embed-\ndings (McLeish et al., 2024) and Contextual Po-\nsition Encoding (Golovneva et al., 2024) improve\ntasks such as mathematical computation and selec-\ntive copying but require model retraining and are\nlimited to open-source LLMs. They also do not\naddress length control or real-world copying and\npasting.\nPrevious research on length-controlled text gen-\neration (Qin et al., 2022; Zhou et al., 2023;\nSun et al., 2023) includes methods like In-\nstructCTG (Zhou et al., 2023), which uses fine-\ntuning with length constraints but is only applicable\nto open-source models. Our work introduces ap-\nproaches suitable for both closed-source and open-\nsource LLMs, focusing on length control and copy-\ning and pasting tasks."}, {"title": "6 Conclusion", "content": "In conclusion, our work introduces PositionID\nPrompting and PositionID Fine-Tuning to enhance\nlarge language models' positional awareness, en-\nabling more accurate adherence to length control\ninstructions without sacrificing response quality.\nAdditionally, we propose PositionID CP Prompting\nto address the challenge of copy-paste operations.\nBy explicitly marking the position of text units, Po-\nsitionID CP Prompting allows models to accurately\ncopy and paste specific spans of text. Our exper-\niments on LenCtrl-Bench confirm improvements\nin length control, while tests on CP-Bench demon-\nstrate the effectiveness of PositionID CP Prompting\nfor copy and paste, highlighting its capability to\nmaintain the integrity of the copied content and\nensure precise placement during paste operations."}, {"title": "Limitations", "content": "Despite the promising results of PositionID Prompt-\ning and PositionID Fine-Tuning, several limitations\nremain. First, these techniques require additional\nannotation and preprocessing steps, which may in-\ntroduce complexity and overhead in practical appli-\ncations. Second, the effectiveness of these methods\nis contingent on the granularity of PositionID as-\nsignments, which may vary across different tasks\nand use cases, potentially necessitating further fine-\ntuning and customization. Third, while our ap-\nproaches improve positional awareness, they may\nstill struggle with extremely fine-grained length\ncontrol requirements, such as generating text with\nvery long length requirements. Lastly, the general-\nizability of our techniques to closed-source models\nremains limited, as they may lead to double API\ncosts due to the additional generation of position\nids."}, {"title": "Ethics", "content": "Our work is based on Large Language Models\n(LLMs), which can generate potentially harmful\nand unfaithful responses. Our proposed method\naims to enhance the models' abilities for length\ncontrol, and copy-paste. Therefore, our method\npresents little ethical issues. Our constructed data,\nLenCtrl-Bench and CP-Bench, don't involve any\nsensitive data."}]}