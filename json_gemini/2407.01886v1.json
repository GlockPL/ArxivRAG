{"title": "Core Knowledge Learning Framework for Graph Adaptation and Scalability Learning", "authors": ["Bowen Zhang", "Zhichao Huang", "Genan Dai", "Guangning Xu", "Xiaomao Fan", "Hu Huang"], "abstract": "Graph classification is a pivotal challenge in machine learning, especially within the realm of graph-based data, given its importance in numerous real-world applications such as social network analysis, recommendation systems, and bioinformatics. Despite its significance, graph classification faces several hurdles, including adapting to diverse prediction tasks, training across multiple target domains, and handling small-sample prediction scenarios. Current methods often tackle these challenges individually, leading to fragmented solutions that lack a holistic approach to the overarching problem. In this paper, we propose an algorithm aimed at addressing the aforementioned challenges. By incorporating insights from various types of tasks, our method aims to enhance adaptability, scalability, and generalizability in graph classification. Motivated by the recognition that the underlying subgraph plays a crucial role in GNN prediction, while the remainder is task-irrelevant, we introduce the Core Knowledge Learning (CKL) framework for graph adaptation and scalability learning. CKL comprises several key modules, including the core subgraph knowledge submodule, graph domain adaptation module, and few-shot learning module for downstream tasks. Each module is tailored to tackle specific challenges in graph classification, such as domain shift, label inconsistencies, and data scarcity. By learning the core subgraph of the entire graph, we focus on the most pertinent features for task relevance. Consequently, our method offers benefits such as improved model performance, increased domain adaptability, and enhanced robustness to domain variations. Experimental results demonstrate significant performance enhancements achieved by our method compared to state-of-the-art approaches. Specifically, our method achieves notable improvements in accuracy and generalization across various datasets and evaluation metrics, underscoring its effectiveness in addressing the challenges of graph classification.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs have garnered considerable attention for their ability to represent structured and relational data across diverse fields, as noted in several studies [4, 34, 47, 55, 61]. Graph classification, a fundamental aspect of data analysis, focuses on predicting whole graph properties and has seen substantial research activity in recent years [42, 57, 73, 74]. This research has practical implications in various applications such as determining the quantum mechanical properties of molecules, including mutagenicity and toxicity [18], and identifying the functions of chemical compounds [28]. A wide array of graph classification methodologies have been developed, with the majority leveraging Graph Neural Networks (GNNs) to deliver strong performance [25, 62, 66, 72]. These methods typically utilize a neighbor-aware message passing mechanism coupled with a readout function to learn discriminative graph representations that effectively reflect the structural topology, thereby facilitating accurate classification.\nDespite its considerable potential, graph classification faces sev-eral significant challenges that hinder its broader adoption and ef-fectiveness. These challenges can be broadly categorized into three main areas: (1) Label Aspect: Graph classification models are often designed for specific tasks, which limits their ability to transfer knowledge to different prediction tasks. This lack of task-agnostic adaptability reduces the models' versatility and applicability across various domains [21]. Additionally, differences in labeling standards or the quality of annotations across domains can lead to inconsistencies in model predictions, thus affecting overall performance and generalizability. (2) Domain Shift Aspect: Graph classification models are generally trained on a single target domain, which diminishes their effectiveness when applied to diverse domains. Adapting these models to various target domains is a significant challenge due to variations in data distribution [46, 69, 75], which can degrade performance. Domain shifts, marked by changes in the data distribution between the source and target domains, amplify this challenge. Effective adaptation mechanisms are essential to maintain model robustness and enhance generalization capabilities. (3) Data Aspect: Graph classification struggles with effectively handling small-sample prediction scenarios. The lack of sufficient labeled data in the source domain, combined with data scarcity in the target domain, presents considerable challenges to the adaptation process, potentially leading to poor generalization performance [1, 12, 24, 35, 56]. Additionally, imbalanced data distributions between domains compound these difficulties, calling for strategies to alleviate the effects of data scarcity and ensure fair and accurate model predictions across different domains.\nIn this paper, we present a new framework meticulously crafted to address the shortcomings of existing graph classification meth-ods. Inspired by [40], the framework identifies and separates the"}, {"title": "2 RELATED WORK", "content": "Graph Classification Graph classification is a key task in graph-based machine learning, with wide applications in fields such as social network analysis, bioinformatics, and recommendation sys-tems. The adoption of Graph Neural Networks (GNNs) has consid-erably pushed forward the discipline by facilitating the modeling of complex structures and relationships present in graph data [26].\nThese models excel in a variety of tasks including node classifica-tion, graph classification, and link prediction [10, 19, 49]. Despite their successes, traditional GNNs often face difficulties in effec-tively capturing higher-order topological structures like paths and motifs [9]. In response, graph kernel methods have been developed to efficiently encapsulate structural information, offering a robust alternative to conventional GNN approaches [39, 71].\nUnsupervised Domain Adaptation Unsupervised domain adaptation (UDA) is a specialized area within machine learning focused on developing domain-invariant representations from a labeled source domain to be utilized in an unlabeled target domain. Traditional UDA methods typically involve aligning feature distri-butions between the source and target domains through techniques such as maximum mean discrepancy (MMD) [37] or adversarial training [14]. Recent progress in UDA has been directed towards creating more efficient and scalable solutions to address domain shifts. A notable trend involves the adoption of deep learning strate-gies, including deep adversarial domain adaptation (DADA) [53] and domain-adaptive contrastive learning (DaCo) [11]. These ap-proaches use neural networks to derive domain-invariant features. Additionally, methods like self-training [64] and pseudo-labeling [30] have been implemented to exploit unlabeled data in the target domain, enhancing the domain adaptation process.\nGraph Domain Adaptation Graph domain adaptation applies the concepts of unsupervised domain adaptation to graph-structured data. Its objective is to transfer insights from a labeled source graph to an unlabeled target graph, addressing challenges such as domain shift and label scarcity [21]. This issue is particularly pertinent in fields like social network analysis, where graphs depict social interactions, and bioinformatics, where graphs represent molecular structures. Existing methods mainly focus on how to transfer information from source graphs to unlabeled target graphs to learn effective node-level [5, 16, 60, 78] and graph-level [7, 63, 65, 70] representation. Despite recent progress, the field continues to face obstacles such as misalignment in category distributions between source and target domains and the absence of scalable, effective algorithms for graph domain adaptation. Tackling these issues ne-cessitates the development of robust, scalable algorithms capable of deriving domain-invariant representations from a limited amount of labeled data.\nFew-shot Learning Few-shot Learning (FSL) aims to train a model capable of generalizing to new classes based on only a small number of examples from those classes, often just one or a few. Meta-learning is a key strategy for FSL, enhancing the model's ability to generalize robustly. This technique involves extracting meta-knowledge that is applicable across a range of meta-tasks, enabling the model to adapt to new, unseen meta-tasks after suf-ficient meta-training. Meta-learning approaches for FSL can be divided into two main types: metric-based and optimization-based methods. Metric-based methods, such as Matching Network [54] and ProtoNet [51], focus on learning a metric to assess similarity between new instances and a few examples by mapping them into a metric space. For example, Matching Network achieves this by encoding the support and query sets separately to calculate similar-ities, while ProtoNet creates prototypes by averaging the support set representations for each class and classifies queries based on the Euclidean distance to these prototypes. On the other hand,"}, {"title": "3 PRELIMINARY", "content": "3.1 Graph Neural Networks\nConsidering the graph G = (V, E), let \\(h_v^{(k)}\\) represent the embed-ding vector of node v at layer k. For each node \\(v \\in V\\), we gather the embeddings of its neighbors from layer k - 1. Subsequently, the embedding \\(h_v^{(k)}\\) is updated iteratively by merging v's previous layer embedding with the embeddings aggregated from its neighbors. This process is formalized as follows:\n\\(h_v^{(k)} = C^{(k)}(h_v^{(k-1)}, A^{(k)}(\\{h_u^{(k-1)}\\}_{u \\in N(v)}))\\), (1)\nwhere N(v) represents the neighbors of v. \\(A^{(k)}\\) and \\(C^{(k)}\\) repre-sent the aggregation and combination operations at the k-th layer, respectively. At last, we summarize all node representations at the K-th layer with a readout function into the graph-level representa-tion, which can be formulated as follows:\nz = F (G) = READOUT (\\{h_v^{(K)}\\}_{v \\in V}); (2)\nwhere z is the graph-level representation of G and \\(\\Theta_e\\) denotes the parameter of our GNN-based encoder. K denotes the number of the graph convolutional layers. The readout function can be implemented using different ways, such as the summarizing all nodes' representations [62] or using a virtual node [32].\nAfter obtaining the graph representation z, we introduce a multi-layer perception (MLP) classifier H(.) to output label distributions for final classification as follows:\np = H(z), (3)\nwhere \\(p \\in [0, 1]^C\\) and \\(\\Theta_c\\) denotes the parameters of the classifier.\n3.2 Explainer of GNN\nFollowing the methodology in [40], we partition the entire graph into two components, denoted as \\(G_{total} = G_{sub} + G_{rest}\\). Here, \\(G_{sub}\\) represents the critical subgraph that significantly influences the predictions of the GNN, and is thus considered the explanatory graph. Conversely, \\(G_{rest}\\) includes the remaining edges that do not impact the GNN's predictions. To identify the essential subgraph \\(G_{sub}\\), the approaches described in [40, 76] focus on maximizing the mutual information between the labels and \\(G_{sub}\\):\n\\(\\max_{G_{sub}} MI(Y, G_{sub}) = H(Y) - H(Y|G = G_{sub})\\), (4)"}, {"title": "4 METHODOLOGY", "content": "In this scenario, Y represents the prediction made by the GNN when \\(G_{total}\\) is used as input. Mutual information quantifies the likelihood of Y when only a specific segment of the graph, \\(G_{sub}\\), is processed by the GNN. This concept derives from conventional forward propagation methods used to provide clear explanations of how the model functions. For example, the importance of an edge (i, j) is underscored when its removal leads to a significant change in the GNN's output, suggesting that this edge is critical and should be included in \\(G_{sub}\\). If an edge's removal does not substantially affect the output, it is considered non-essential for the model's decision-making. Since H(Y), the entropy of Y, is linked to the fixed parameters of the GNN during the explanation phase, the objective is to minimize the conditional entropy \\(H(Y|G = G_{sub})\\).\nOptimizing Eqn. 4 directly is impractical due to the \\(2^M\\) potential candidates for \\(G_{sub}\\), where M represents the total number of edges. To simplify this, we assume the graph follows the Gilbert random graph model [15], in which the edges of the subgraph are considered independent. Here, \\(e_{ij} = 1\\) indicates that the edge (i, j) is included, and 0 indicates it is not. Under this model, the probability of any graph configuration can be expressed as a product of individual probabilities:\n\\(P(G) = \\Pi_{(i,j)\\in V} P(e_{ij})\\). (5)\nAssuming the distribution of edge \\(e_{ij}\\) follows the Bernoulli dis-tribution: \\(e_{ij} \\sim Bern(\\Theta_{ij})\\). Then the Eqn. 5 can be rewrite as:\n\\min_{G_{sub}} H(Y|G = G_{sub}) = \\min_{G_{sub}} E_{G_{sub}}[H(Y|G = G_{sub})]\n\\approx \\min_{E_{G_{sub} \\sim q(\\Theta)}} [H(Y|G = G_{sub})], (6)\nwhere q() is the distribution of the core subgraph.\nThe purpose of learning core knowledge is to learn to determine the most essential subset of sample features. Focusing on the graph field, our purpose is to learn the subgraph structure that can represent the entire graph, and use the subgraph to replace the calculation of the entire graph. Motivated by [41], where the real-life graphs are with underlying structures, we develop the core knowledge learning (CKL) module to learn the core subgraph of graphs and then utilize the core knowledge for the downstream tasks learning.\n4.1 Core Knowledge Learning\nTo learn the core knowledge of a graph, we need to determine the important nodes subset from the whole graph. Traditional graph explainer methods [40, 41] direct learn the edges sampling process to determine the explainable subgraph. However, they typically assume the distribution of edges as prior knowledge, which is diffi-cult to satisfy in real scenarios. Thus, we propose the CKL module to learn the probability of edge and node selection. Specifically, given a graph G = (A,X), we first obtain the node embeddings with l-layer GNNs, and then map node \\(v_i\\), \\(v_j\\) and \\(e_{ij}\\) into the same feature space with multilayer perception (MLP):\nH = GNN(A, X), En = MLP(H), Ee = MLP(E), (7)\nwhere E denotes the features of edges.\nNode selection. We first calculate the node sampling probability \\(p_v\\) with the Sigmoid function to map the node embeddings into"}, {"title": "4.2 Graph Domain Adaptation Learning", "content": "[0, 1]:\n\\(p_v = Sigmoid(E_{no})\\). (8)\nA larger probability of node sampling will lead to a higher prob-ability of node mask with \\(m_v = 1\\), indicating the corresponding node v is important for the core knowledge learning. However, the node sampling process is non-differentiable [77], we relax \\(m_v\\) with Gumbel-softmax [13, 20]:\n\\(m_v = Sigmoid(\\frac{1}{t} \\log \\frac{p_v}{1-p_v} + \\log \\frac{u}{1-u})\\), (9)\nwhere t is the temperature parameter and u ~ Uniform(0, 1).\nEdge selection. After obtaining the probability of node sam-pling, we further evaluate the probability of edges corresponding to the sampled nodes. Specifically, we concat the embeddings of edge \\(e_{ij}\\) and the connected nodes \\(n_i\\), \\(n_j\\), and calculate the edge mask probability \\(m_{e_{ij}}\\) with:\n\\(E_{fusion} = Cat(E_{n_i}, E_{n_j}, E_{e_{ij}}), m_{e_{ij}} = Sigmod(E_{fusion})\\), (10)\nwhere Cat denotes the concat operation.\nWith the node and edge selection process, we mask the whole graph \\(G_{total}\\) to obtain the core graph \\(G_{sub}\\). Finally, we follow [76] to modify the conditional entropy with cross-entropy H(Y, \\(Y_{sub}\\)), where \\(Y_{sub}\\) is the prediction of the GNN model with \\(G_{sub}\\) as the input:\n\\min_{\\Theta} E_{G_{sub} \\sim q(\\Theta)}[H(Y|G = G_{sub})]. (11)\nwhere q() is the distribution of the core subgraph.\nFor efficient optimization of Eqn. 11, we simplify the conditional entropy with cross-entropy H(Y|\\(Y_{sub}\\)), where \\(Y_{sub}\\) is the output of subgraph \\(G_{sub}\\). With the simplification, we optimize Eqn. 11 with Monte Carlo approximation:\n\\min_{\\Theta} E_{G_{sub} \\sim q(\\Theta)}[H(Y|Y_{sub})]\n\\approx \\min_{\\Theta} \\frac{1}{K C} \\sum_{k=1}^{K} \\sum_{c=1}^{C} P(Y = c) \\log P(Y_{sub} = c)\n= \\min_{\\Theta} - \\frac{1}{K C} \\sum_{k=1}^{K} \\sum_{c=1}^{C} P(Y = c|G = G_{total}) \\log P(Y = c|G = G_{sub}), (12)\nwhere K is the number of sampled subgraphs, C is the number of labels, and \\(G_{sub}^k\\) denotes the k-th sampled subgraph.\nProblem setup. Denote a graph as G = (V, E, X) with the node set V, the edge set E, and the node attribute matrix X \\(X \\in R^{|V| \\times F}\\) with F denotes the attribute dimension. The labeled source domain is denoted as \\(D_s = \\{(G_i^s, y_i^s)\\}_{i=1}^{N_s}\\), where \\(y_i^s\\) denotes the labels of \\(G_i^s\\). The unlabeled target domain is \\(D_t = \\{G_i^t\\}_{i=1}^{N_t}\\), where \\(N_s\\) and \\(N_t\\) denote the number of source graphs and target graphs. Both domains share the same label space Y, but have different distributions in the graph space. Our objective is to train a model using both labeled source graphs and unlabeled target graphs to achieve superior performance in the target domain.\nThe extracted core subgraph is the underlying subgraph that makes important contribution to GNN's prediction and remaining is task-irrelevant part. Therefore, in the graph domain adaptation task, we measure the similarity of the source domain core subgraph and target domain core subgraph, ignoring the domain shift. Given two sampled subgraphs from source domain \\(G_{sub}^s = (V_s, E_s^s)\\) and target domain \\(G_{sub}^t = (V_t, E_t^t)\\), graph kernels calculate their similar-ity by comparing their substructure using a kernel function. In formulation,\nK (G_{sub}^s, G_{sub}^t) = \\sum_{o_1 \\in V_s} \\sum_{o_2 \\in V_t} k (G_{sub}^s (o_1), G_{sub}^t (o_2)) (13)\nwhere \\(G_{sub}^s (o_1)\\) represents the local substructure centered at node \\(o_1\\) and k(\u00b7, \u00b7) is a pre-defined similarity measurement. We omit \\(G_{sub}^s (o_1)\\) and leave k(\u00b7, \u00b7) in Eq. 13 for simplicity. In our implemen-tation, we utilize the Weisfeiler-Lehmah (WL) subtree kernel for the comparison of source and target core subgraph."}, {"title": "4.3 Few shot learning", "content": "Problem setup. The target is to learn a predictor from a set of few-shot molecular property prediction tasks \\(\\{T_{\\tau}\\}_{\u03c4=1}^{Nt}\\), and generalize to predict new properties given a few labeled molecules. The t-th task \\(T_\u03c4\\) predicts whether a molecule \\(x_{\u03c4,i}\\) with index i is active (\\(y_{\u03c4,i} = 1\\)) or inactive (\\(y_{\u03c4,i} = 0\\)) on a target property, provided with a small number of K labeled samples per class. This T is then formulated as a 2-way K-shot classification task with a support set \\(S_t = \\{(x_{\u03c4,i}, y_{\u03c4,i})\\}_{i=1}^{2K}\\) containing the 2K labeled samples and a query set \\(Q_t = \\{(x_{\u03c4,j}, y_{\u03c4,j})\\}_{j=1}^{N_t}\\) containing \\(N_t\\) unlabeled samples to be classified.\nIn order to achieve the goal of few shot learning on multiple tasks, we utilize the learned core subgraph as the input of GNN for prediction, i.e.,\n\\widehat{Y} = GNN (G_{sub} (\\Theta)), (16)\nwhere \\(\\Theta\\) is the parameters of task relevant embedding function and classifier and \\(\\Omega\\) denotes the collection of parameters of CKL molecular. The training loss L(\\(S_t\\), \\(f_{\\Theta, \\Phi}\\)) evaluated on \\(S_t\\) follows:\nL(S_t, f_{\\Theta, \\Phi}) = \\sum_{(x_{\\tau,i}, y_{\\tau,i}) \\in S_{\\tau}} y_{\u03c4,i} \\log(\\widehat{y_{\u03c4,i}}), (17)\nwhere \\(y_{\u03c4,i}\\) \\(\\in\\) \\(R^2\\) is a one-hot ground-truth. Observing that the Eqn. 17 contains two distinct parameters \\(\\Theta\\) and \\(\\Phi\\), we further use bi-level optimization methods to optimize them simultaneously:\n\\min_{\\Phi} F(\\Theta^*) = \\sum_{\\tau=1}^{Nt} L_{outer} (f_{\\Theta^*, \\Phi}(A, X, S_t)),\ns.t. \\Theta^* = \\sum_{\\tau=1}^{Nt} L_{inner} (f_{\\Theta, \\Phi} (A, X, S_t)), (18)\nwhere \\(L_{inner}\\) is the loss function in Eqn. 12 and \\(L_{outer}\\) is the loss function of Eqn, 17.\nInner optimization. We first optimize the parameter \\(\\Theta\\) with a gradient descent based optimizer by fixing \\(\\Phi\\),\n\\Theta_\u03c4 = \\Theta_{\u03c4-1} - \u03b1\\nabla L_{inner}(S_t, f_{\\Theta, \\Phi}), (19)\nwhere \u03b1 is the learning rate.\nOuter optimization. Following [12], we employ the gradient-based meta-learning strategy and initialize \\(\\Phi\\) with set of meta-training tasks \\(\\{T_{\\tau}\\}_{\u03c4=1}^{Nt}\\), which acts as the anchor of each task \\(T_\u03c4\\). Specifically, we fix parameter \\(\\Theta\\) and optimize \\(\\Phi\\) as \\(\\Phi_\u03c4\\) on \\(S_t\\) in each \\(T_\u03c4\\) during the outer optimization period. It is obtained by taking a few gradient descent updates:\n\\nabla L(S_t, f_{\\Theta, \\Phi}) = dL_{outer}\u2207(\\Phi) + dL_{outer}. (20)"}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Settings\nDatasets. For the graph domain adaptation task, we utilize 9 graph classification datasets for evaluation, i.e., Mutagenicity (M) [23], Tox21_AhR 1, FRANKENSTEIN (F) [45], and PROTEINS [8] (includ-ing PROTEINS (P) and DD (D)), COX2 [52] (including COX2 (C) and COX2_MD (CM)), BZR [52] (including BZR (B) and BZR_MD (BM)) obtained from TUDataset [43]. The details statistics are pre-sented in Table 1. Additionally, we follow [70] and partition M, T, F datasets into four sub-datasets based on edge density. For the few-shot learning task, we evaluate the experiments on widely used few-shot molecular property prediction, and the details of the datasets are introduced in Table 2.\nBaselines. For the graph domain adaptation task, we compare our CKL with different baselines: WL subtree [50] GCN [25], GIN [62], GMT [2], CIN [3], CDAN [38], ToAlign [59], MetaAlign [58], DEAL [68] and CoCo [70]. For the few-shot learning task, we compare CKL with Siamese [27], ProtoNet [51], MAML [12], TPN [35], EGNN [24], IterRefLSTM [1] and RAP [56].\nImplementation Details. In our CKL, we employ GIN [62] as the backbone of feature extraction. For the graph domain adaptation task, we utilize one of the sub-datasets as source data and the"}, {"title": "5.2 Performance on Different Domains", "content": "Tables 3 to 6 present the comparative results of CKL alongside other benchmark methods. Analyzing these results, we observe several key insights:\n\u2022 Superiority of Domain Adaptation Methods in Graphs: Domain adaptation strategies tailored for graphs consistently outperform traditional kernel and GNN-based methods. This suggests that conventional graph methodologies may struggle with adaptabil-ity across varying domains due to their limited expressive power. Therefore, the development of domain-invariant techniques is critical for the advancement of Graph Domain Adaptation (GDA). These domain-invariant methods prove essential not only in maintaining performance across diverse datasets but also in facil-itating the integration of graphs from disparate sources without loss of fidelity.\n\u2022 Robust Performance of GDA Techniques: Methods implemented in GDA demonstrate robust performance, notably surpassing traditional domain adaptation strategies. The success of these methods can be attributed to their ability to handle the inher-ent complexities in graph data. Achieving high-quality graph representations is a complex task, exacerbated by the structural and feature diversity within the graphs. This complexity renders traditional domain adaptation strategies less effective, thus high-lighting the specialized nature and effectiveness of graph-specific adaptation methods.\n\u2022 Efficiency of CKL: The proposed CKL method outshines other competing methods, showcasing its efficiency in core knowledge learning. This efficiency is largely due to CKL's focus on critical"}, {"title": "5.3 Performance on Few-shot Learning", "content": "Table 7 details the performance comparisons between CKL and a range of baseline methods in graph-based molecular encoding tasks. For this analysis, we have omitted results pertaining to Siamese and IterRefLSTM, as their implementation details and the outcomes of their evaluations on the ToxCast dataset are unavailable. The table clearly illustrates that CKL consistently achieves superior perfor-mance over other methods that also employ graph-based molecular encoders designed from the ground up. In particular, CKL not only surpasses all compared baselines but does so with a notable margin; it demonstrates an average performance improvement of 1.62% over the highest-performing baseline, EGNN. The performance reveals that methods incorporating few-shot learning techniques to decode relation graphs, such as GNN, TPN, and EGNN, deliver enhanced"}, {"title": "5.4 Flexibility of CKL", "content": "For the graph domain adaptation experiments, we use GIN as the backbone to extract the core subgraph feature. To show the flex-ibility of the proposed CKL, we replace the GIN with different GNN methods. In our implementation, we utilize GCN [25], Graph-Sage [17] and GMT [2] instead of GIN to show the flexibility of CKL. Additionally, we replace the WL subtree kernel with Graph Sampling [31], Random Walk [22] and Propagation [44].\nFigure 5 illustrates the comparative performance of several GNNs and graph kernels over four representative datasets. We have noted similar performance trends across additional datasets as well. The data indicate that among the various GNNs and graph kernels eval-uated, GIN and the WL subtree kernel consistently stand out as the top performers in the majority of cases. The superior performance of both GIN and the WL subtree kernel is likely due to their ex-ceptional capabilities in capturing complex graph structures and providing powerful node and graph-level representations. This con-sistent outperformance validates our selection of GIN and the WL subtree kernel as the primary methods for enhancing task perfor-mance in our graph domain adaptation efforts. The choice is further justified by their ability to effectively handle the complexities of diverse datasets, making them highly suitable for robust graph analysis and domain adaptation tasks."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce a novel approach named CKL that focuses on learning the core subgraph knowledge necessary for downstream tasks. Recognizing the essential role of the under-lying subgraph in GNN predictions, while considering the rest as task-irrelevant, we have developed a framework designed for graph adaptation and scalability learning. CKL includes several key components: the core subgraph knowledge submodule, the graph domain adaptation module, and the few-shot learning module, each aimed at addressing specific challenges in graph classification such as domain shifts, label inconsistencies, and data scarcity. Our com-prehensive experiments show that CKL significantly outperforms existing state-of-the-art methods, demonstrating notable advance-ments in performance."}]}