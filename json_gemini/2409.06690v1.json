{"title": "BENCHMARKING SUB-GENRE CLASSIFICATION FOR MAINSTAGE DANCE MUSIC", "authors": ["Hongzhi Shu", "Xinglin Li", "Hongyu Jiang", "Minghao Fu", "Xinyu Li"], "abstract": "Music classification, with a wide range of applications, is one of the most prominent tasks in music information retrieval. To address the absence of comprehensive datasets and high-performing methods in the classification of mainstage dance music, this work introduces a novel benchmark comprising a new dataset and a baseline. Our dataset extends the number of sub-genres to cover most recent mainstage live sets by top DJs worldwide in music festivals. A continuous soft labeling approach is employed to account for tracks that span multiple sub-genres, preserving the inherent sophistication. For the baseline, we developed deep learning models that outperform current state-of-the-art multimodel language models, which struggle to identify house music sub-genres, emphasizing the need for specialized models trained on fine-grained datasets. Our benchmark is applicable to serve for application scenarios such as music recommendation, DJ set curation, and interactive multimedia, where we also provide video demos.", "sections": [{"title": "1. INTRODUCTION", "content": "Being one of the most fundamental and important tasks in music information retrieval (MIR), music genre classification has mainly focused on broad genres. Despite continuous progress, existing datasets lack fine-grained labels that can capture the nuances within electronic dance music (EDM), and with 0/1 labels, they hardly manifest the category overlap in EDM data. Moreover, current universal models have subpar performance in specific tasks. The limitations are evident in the contexts of mainstage DJ sets, where tracks usually fall into sub-genres within the broad category of house music. Hence, the unique challenges presented by EDM need specialized datasets and algorithms tailored to its structural characteristics and its complexity with regard to production techniques.\nTo address this gap, we introduce a new benchmark specifically targeting at the classification of house music sub-genres. A dataset is designed to provide annotations from a list of 8 sub-genres. Unlike existing dataset HouseX [7], we introduce soft labeling instead of 0/1 categorical labeling to provide detailed and nuanced representation of the music. In addition, a baseline model is presented using spectral features [8, 9], building a foundation for future research in dance music genre classification. Furthermore, an application of our models in automated music visualization is prototyped to enhance visual experiences.\nOverall, this work aims to advance MIR for mainstage house music by offering a more comprehensive dataset and efficient baseline. Our key contributions are:\n(a) An improved annotation paradigm and a larger dataset.\n(b) A strong baseline for this dataset.\n(c) A VFX automation demo driven by trained models.\nThe rest sections are as follows: Sec. 2 reviews related work, Sec. 3 outlines our methodology, Sec. 4 presents our results, and Sec. 5 discusses potential applications, with conclusions in Sec. 6."}, {"title": "2. RELATED WORK", "content": "Traditional music genre datasets, like GTZAN [1], FMA [2] and MSD [10], often focus on broad genres like pop, country, and rock. The HouseX dataset [7] advanced classification on EDM but faced challenges like limited category richness and scale. In addition, previous works tackling drop detection [11, 12, 13] also highlight the emerging research interest in MIR for EDM. Alongside traditional deep learning methods, recent progress on multimodel large language models (MLLMs) like the Qwen-Audio series [5, 14] could also be used to classify generic audio."}, {"title": "3. METHODOLOGY", "content": "This section discusses our methodology, from the collection and annotation of the dataset, to the training of the model. Fig. 1 illustrates the pipeline. For simplicity, we refer to \"sub-genres\" of house music as \"genres\" from here unless otherwise specified. Statistics of our dataset is shown in Tab. 1. In the table, the first part lists the number of clips for the 8 genres in the training set and the validation set, while the second part gives some other general statistics."}, {"title": "3.1. Dataset", "content": "Our data preparation stage involves collection, annotation, and extraction of musical excerpts. The input data are represented as triplets, each comprising the raw audio (Xraw), its extracted acoustic features (Xfeat), and a corresponding label (Y), as illustrated by numbers from 1 to 3 in Fig. 1.\nOur dataset comprises over 1,000 selected tracks sourced from renowned international record companies. These tracks are stored as uncompressed raw audio (Xraw). For each track, we only consider the drop, which is the most representative segment of the style. To identify these segments, we detect excerpts where the volume consistently remains above a threshold ($V_{thres} = V_{max} - V_{margin}$), where Vmax denotes the maximum volume within the audio. Note that before detection, some rule-based smoothing process is applied to mitigate loudness fluctuations. After identifying the segments that meet the aforementioned criteria, several 7.5-second clips (around 4 bars) are randomly sampled from each segment, ensuring a comprehensive representation of the track.\nSubsequently, feature extraction is performed on each clip. Using Librosa [19], we compute: mel-spectrograms (Xmel), CQT-chromagrams (Xcqt), VQT-chromagrams (Xvqt), which are then stacked to produce the final audio feature matrix (Xfeat).\nAll tracks in the dataset are manually annotated with a total of eight distinct labels. The first four labels align with those previously defined by the HouseX dataset [7]. To improve classification granularity to a level suitable for covering most contemporary mainstage live sets, we introduce four additional genres. Recognizing that certain tracks may exhibit characteristics of multiple genres, we employ soft labeling techniques to add more information to our labels. We also"}, {"title": "3.2. Model", "content": "Given a dataset of audio features {Xreat} 1 with corresponding soft labels p(yi|Xreat) that represent probability distributions over possible genres, our objective is to train a neural network, parameterized by 0, to predict genre distributions qo (Yi Xeat) that closely match these soft labels.\nTo achieve this goal, we minimize the Kullback-Leibler (KL) divergence between the true distribution p(yi|Xfeat) and the predicted distribution qo (Yi Xfeat):\n$KL(p||99) = \\sum_{i=1}^{N} \\sum_{y_i \\in Y} p(y_i|X_{feat}) \\log (\\frac{p(y_i|X_{feat})}{q_{\\theta}(Y_i | X_{feat})})$\nDiscarding the part that does not depend on 0, this is equivalent to minimizing the cross-entropy loss:\n$L(0) = - \\sum_{i=1}^{N} \\sum_{y_i \\in Y} P(Y_i X_{feat}) log (9_0 (Y_i | X_{feat}))$.\nThe architecture of network @ is inspired by AST (audio spectrogram transformer) [20] trained on the AudioSet [21]. As shown in column (b) in Fig. 1, we derive a sequence of overlapping patches {X}\u2081 from each acoustic feature matrix Xfeat, where X is square with shape (3 \u00d7 224 \u00d7 224). Then each square feature patch is fed into a vision encoder, which essentially consists of conventional architectures used for image classification [15, 16, 17, 18]. The output for the sequence of input feature patches is a corresponding sequence of embeddings {Zemb}1. Positional encodings are then added to this sequence of embeddings, which are subsequently passed through several stacked transformer [22] encoder layers. The first (on the seq_len axis) output from this transformer is finally fed into a linear layer to get the predicted distribution."}, {"title": "4. RESULTS", "content": "This section presents the experiment results alongside our interpretations and findings. Tab. 2 provides numerical details and Fig. 2 shows the final embedding space after feature reduction by PCA, t-SNE [23] and UMAP [24].\nFour popular CNN/ViT architectures (with comparable number of parameters) serve as the feature extractor. Experiments show that all of these settings outperform Qwen2-Audio, with or without prompt on background knowledge. Again, our purpose is to justify the necessity of building specific dataset rather than \"beat\" MLLMs. In other words, MLLMs are quite likely to distinguish among EDM subgenres when provided with proper fine-tuning data; otherwise, they struggle to align textual background information with domain-specific audio features. In addition, models trained on soft labels perform uniformly better than those trained from (sharpened) 0/1 labels, which supports our claim that soft labeling provides richer information of the tracks. Moreover, models trained on composite data with chromagrams fail to surpass those trained solely on mel-spectrogram. We attribute this phenomenon to the domain gap between the RGB channels and the mel-CQT-VQT space. Future work on architectures using such composite data could focus on improving feature fusion techniques to achieve better results.\nWe also extracted the embeddings before the final linear layer and visualized them with dimension reduction techniques. The figures indicate that progressive house, bigroom and slap house are relatively well-separated, aligning with our annotations where most non-0/1 labels involve other genres."}, {"title": "5. APPLICATIONS", "content": "We propose some real-world scenarios for such classification algorithm. A straightforward application is music recommendation system tailored for listeners with specific preferences on certain sub-genres. Besides, this algorithm can boost productivity in multimedia contexts, such as automated MV generation and visuals generation given certain pre-defined rules [7]. For illustrative purposes, we prototyped a visual automation system simulated in Blender 3D."}, {"title": "6. CONCLUSION", "content": "We developed a comprehensive house music classification benchmark to address the limitations of existing datasets, such as limited sub-genre representation and inter-class overlaps. Our dataset employs continuous soft labeling to better capture track characteristics. Our proposed baseline methods outperform current MLLMs on this classification task, highlighting the significance of specialized datasets.\nFuture work will further scale up the dataset to better utilize the CQT and VQT feature spaces. Since labeling large datasets is impractical for the machine learning community alone, a collaborative approach with production experts is recommended. Future research should also extend beyond"}, {"title": "Appendix A: Classification Protocols", "content": "Due to space constraints, please find the definitions of the determinants we used in Tab. 3 in the \"misc\" subfolder of our code. Numerical scores are quantized from averages given by human music producers. We also provide qualitative descriptions of the 8 genres in the same folder. Note that the values of soft labels are not absolute per se. Even for professionals, one track can be interpreted in multiple ways. Soft labels are utilized to indicate \"Track X is genre A but also expresses some traits of genre B\", for example, so more information is contained in the label."}, {"title": "Appendix B: Experiment Settings", "content": "At least 24G GPU memory is required to complete one run with a batch size of 4. We assign one NVIDIA RTX A6000 GPU to each data setting (whether to use chromagram and whether to use soft labels). Completing 5 epochs for all 4 architectures in a single setting requires 1 day."}]}