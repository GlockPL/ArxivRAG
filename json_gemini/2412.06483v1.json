{"title": "SAFEWORLD: Geo-Diverse Safety Alignment", "authors": ["Da Yin", "Haoyi Qiu", "Kung-Hsiang Huang", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlook the geo-diversity of cultural and legal standards across the world. To demonstrate the challenges posed by geo-diverse safety standards, we introduce SAFEWORLD, a novel benchmark specifically designed to evaluate LLMs' ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SAFEWORLD encompasses 2,342 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria. To enhance LLMs' alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment training. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SAFEWORLDLM outperforms all competing models, including GPT-40 on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation. Our code and data can be found here: https://github.com/PlusLabNLP/SafeWorld.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as LLaMA [41] and GPT [27], are becoming integral to various AI applications, serving tens of millions of users globally. As their use increases, concerns around LLMs safety are rapidly growing. Recently, a wide range of studies focus on evaluating and reducing their toxic and harmful impact on users [15, 43, 13, 23, 48, 36, 21, 2, 5, 3]. Despite significant progress in this area, an essential factor often remains overlooked: geo-diversity. Recognizing and incorporating geographical variations [46, 45, 4, 11, 36, 6] is crucial in the LLM global application. In particular, in terms of the landscape of LLM safety, cultural norms and legal frameworks vary widely, resulting in diverse definitions of safe and acceptable behavior. As shown in Figure 1, while giving a green hat as a gift might be benign in many cultures, it is considered offensive in China. Likewise, legal ages for drinking and marriage differ significantly between regions. If a model fails to account for these cultural norms and local policies (i.e., cultural-legal guidelines), it can inadvertently cause unnecessary conflicts among individuals or even between nations and pose significant legal risks for local services. Therefore, to be both equitable and effective, LLMs must be calibrated to align with diverse cultural norms and legal standards worldwide.\nWe introduce SAFEWORLD, the first geo-diverse safety alignment evaluation benchmark, focusing on cultural and legal safety (\u00a73). SAFEWORLD evaluates an LLM's ability to generate helpful, safe, and appropriate responses in a global context. Constructed based on insights from our global user survey (Appendix A.2), SAFEWORLD comprises 2,342 high-quality diverse queries to simulate realistic, geo-diverse safety scenarios, validated through machine and human validations, which ensures alignment with cultural-legal guidelines from 50 countries and 439 regions/races.\nTo assess the quality of LLM responses to geo-diverse safety queries, we establish the three automatic evaluation protocols focusing on contextual appropriateness, accuracy, and comprehensiveness (\u00a74). Our evaluation reveals that LLaMA- and Mistral-series models can achieve comparable performance to GPT-3.5 and GPT-4-turbo on several dimensions. Although the cultural-legal guidelines used to construct SAFEWORLD queries are all derived from GPT-4-turbo's parametric knowledge, GPT-4-turbo still struggles with queries implicitly related to these guidelines and is even worse at providing appropriate response types than some open-source LLMs. This suggests that additional alignment methods may be necessary to effectively elicit and apply its learned knowledge in model responses.\nHow can we design effective approaches for geo-diverse safety alignment? Focusing on the widely used alignment method Direct Preference Optimization (DPO) [30] (\u00a75), we investigate how to synthesize training data for preference pairs that helps LLMs behave appropriately and accurately elicit factual knowledge. Specifically, we first synthesize training queries based on our repository of human-verified cultural-legal guidelines, GEOSAFEDB. Positive responses are then synthesized to align with the user queries and their corresponding cultural-legal guidelines. The negative responses in preference pairs are divided into two categories: Negative Response Category 1, which includes responses that correctly reference cultural-legal guidelines but do so inappropriately; Negative Response Category 2, which includes responses that are behaviorally appropriate but contain incorrect references to cultural-legal guidelines. Following the DPO alignment practices suggested by Huggingface Alignment Handbook [42], trained on top of Zephyr-7B-SFT-Full [42], our SAFEWORLDLM model outperforms all competitors, including GPT-40, across all three evaluated dimensions, along with a nearly 20% higher winning rate in helpfulness and harmfulness assessments by human evaluators from 9 countries. In addition, our SAFEWORLDALIGN training data proves to be useful for maintaining performance on general NLP and safety evaluation tasks while enhancing geo-diverse safety alignment.\nTo summarize, we make the following contributions: (1) We introduce SAFEWORLD, the first geo-diverse safety alignment evaluation benchmark for future real-world global AI applications. (2) We propose a multi-dimensional safety evaluation framework to assess the contextual appropriateness, accuracy, and comprehensiveness of responses, crucial for geo-diverse safety alignment. (3) We develop a geo-diverse safety alignment training method that enhances LLMs to outperform the advanced GPT-40 model in generating precise geo-diverse safety knowledge."}, {"title": "2 Related Work", "content": "Cultural Knowledge Bases and Evaluation Benchmarks. Early efforts to build cultural knowl-edge bases have primarily followed a top-down approach, extracting norm-related data from web"}, {"title": "3 SAFEWORLD", "content": "In this section, we detail the methodology for developing SAFEWORLD evaluation benchmark, designed to evaluate the geo-diverse safety alignment of LLMs. We first define geo-diverse safety (\u00a73.1), followed by the task definition (\u00a73.2), and the dataset construction process (\u00a73.3).\n3.1 Geo-Diverse Safety Definition\nInspired by the formal taxonomy proposed by [43], we identify three critical safety categories for geo-diverse contexts: (1) discrimination, exclusion, and toxicity; (2) malicious uses; and (3) misinformation harms. Building upon these categories, our SAFEWORLD benchmark emphasizes cultural safety and legal safety and we elaborate on the definition of these two dimensions:"}, {"title": "3.2 The Geo-Diverse Safety Alignment Task", "content": "SAFEWORLD aims to evaluate models' ability to respond appropriately, precisely, and helpfully to queries involving a culturally or legally sensitive content. The input to this task is a query x that may adhere to or violate specific cultural-legal guidelines ky = {k\u2081, ..., ky}, with an expected response type ry. These guidelines and response types vary by query type, detailed in \u00a73.3.2."}, {"title": "3.3 SAFEWORLD Construction", "content": "The benchmark creation involves two key stages: (1) constructing GEOSAFEDB, a cultural and legal geo-diverse safety database (\u00a73.3.1), and (2) formulating SAFEWORLD benchmark queries, each of which corresponds to cultural-legal guidelines in GEOSAFEDB (\u00a73.3.2).\n3.3.1 GEOSAFEDB Cultural-Legal Guideline Database Development\nThe first step towards SAFEWORLD involves creating a cultural and legal geo-diverse safety database, referred to as GEOSAFEDB, composed of the cultural-legal guidelines of various geographic back-grounds. It is beneficial for generating the queries indeed grounded to geo-diverse safety-related topics. We introduce a bottom-up approach, gathering country- and region/race-level guidelines via LLM prompting, followed by validation by native or local annotators.\nWe begin by selecting the top 50 most populous countries and use GPT-4-turbo to generate 100 unique, country-specific cultural-legal guidelines for each, ensuring geo-diversity in GEOSAFEDB. These guidelines undergo a rigorous multi-step verification process, combining both automated and human-based methods. Initially, verification is carried out using retrieval-augmented LLMs like Command-R and GPT-4-turbo, which validate the information against web-sourced data and pre-trained knowledge. Following this, geo-diverse human annotators from Amazon Mechanical Turk conduct a final round of validation, addressing common data quality issues encountered in prior research. Additionally, within a country, significant differences may exist between individual races and regions. For example, while India's national law prohibits cow slaughter due to the sacred status of cows in Hinduism, some states like West Bengal allow it. To capture these nuances, we extend our methodology by generating region/race-specific cultural-legal guidelines. This is achieved by prompting GPT-4-turbo based on the country-level guidelines, followed by another round of stringent machine and human validation to ensure the accuracy and representativeness of these region- and"}, {"title": "3.3.2 SAFEWORLD Query Generation", "content": "After building GEOSAFEDB, we proceed to construct queries that reflect real-life geo-diverse safety scenarios. To ensure that the queries align with relevant use cases, we conduct surveys with participants from diverse geographical backgrounds (Appendix A.2). Using the survey insights, we design four distinct query types, each tailored to a specific response type. Every query in SAFEWORLD includes a scenario that illustrates a culturally or legally sensitive (or insensitive) context, accompanied by a related question. Figure 3 overviews our query generation process. Below, we detail the steps for designing these queries. Figure 4 show SAFEWORLD query examples.\nSPECIFICANSWER. These queries involve scenarios that have already violated the cultural-legal guidelines of the queried country, race, or region. While the questions themselves might not be culturally or legally unsafe, LLMs should identify the specific guideline that has been violated in the context scenario when providing a response. To generate such SPECIFICANSWER queries, we create norm- or policy-violating scenarios and corresponding questions for each cultural-legal guideline gv \u2208 DC UDL, using carefully crafted prompts for GPT-4-turbo.\nCOMPREANSWER. For scenarios where no specific countries, races, or regions are mentioned, but potentially violate norms or laws of some communities, models should provide comprehensive"}, {"title": "4 SAFEWORLD Automatic Evaluation Framework", "content": "Our SAFEWORLD evaluation aims to assess how contextually appropriate, accurate, and comprehensive LLM responses are when addressing the four types of geo-diverse safety queries outlined in \u00a73. We implement the following evaluation protocols: (1) Response Type Matching (\u00a74.1); (2) Reference-Based Faithfulness and Coverage (\u00a74.2); (3) Reference-Free Factuality (\u00a74.3). Note that faithfulness and coverage are only applicable to SPECIFICANSWER and COMPREANSWER queries. These types of queries require generating responses that accurately mirror and encompass specific norms and policies outlined in the ground-truth guidelines. In contrast, REFUSETOANSWER and DOANSWER queries do not include these guidelines, as they involve responses that either directly address or avoid the topic without necessarily referencing cultural norms or policies explicitly.\n4.1 Response Type Matching\nAs described in \u00a73, each query is associated with an expected response type, denoted as R {SPECIFICANSWER, COMPREANSWER, REFUSETOANSWER, DOANSWER}. This evaluation protocol aims to determine whether the type of a model's generated response matches the expected response type. For example, in the case of DOANSWER queries, a model's response is considered a match if it addresses the query directly without raising any violation alerts. On the other hand, a response is deemed unmatched for REFUSETOANSWER queries if the model provides an answer when"}, {"title": "4.2 Reference-based Faithfulness and Coverage Evaluation", "content": "In the second evaluation dimension, we aim to determine whether models accurately identify and reference the norms or laws that are violated. To achieve this, we propose reference-based metrics to evaluate the faithfulness and coverage of model responses [29, 18, 17]. These metrics require first extracting the norms or policies from a model's response, denoted as k\u0177 = {k\u2081, ..., k\u2081}, using GPT-4-turbo. Faithfulness measures how accurately the model's response aligns with the ground-truth norms or policies. It is calculated as: FAITHFULNESS(ky, k\u00ba) = |k\u1ef9 \u2229 k\u1ef9|/|k\u0177| \u2208 [0, 1]. A higher faithfulness score indicates that the model's response is more precise in referencing the expected norms or policies. Coverage, on the other hand, evaluates the comprehensiveness of the model's response, indicating how well it captures the entirety of the ground-truth norms embedded in the query. It is defined as: COVERAGE(k\u1ef9, ky) = |k\u0177 \u2229 ky|/|ky| \u2208 [0,1]. A higher coverage score suggests that the model has referenced a more complete set of relevant norms or policies."}, {"title": "4.3 Reference-free Factuality Evaluation", "content": "To address situations where the norms or policies mentioned in the generated response are accurate but not covered by our annotated ground-truth norms or policies k\u00ba, we leverage the state-of-the-art retrieval-augmented LLM, Command-R. This model helps evaluate whether the norms or policies extracted from the model's response, k\u1ef9, can be verified using online sources. This process is crucial for assessing the factuality (i.e., factual accuracy) of the generated content, as discussed in prior works [19, 20]. Let kract \u2286 k\u1ef9 represent the subset of norms or policies that can be validated using information found on the web. We define factuality as: FACTUALITY(ku, kact) = |kact|/|ky| \u2208 [0, 1]. This metric measures the proportion of extracted norms or policies that are verifiable, providing a clearer indication of the response's factual accuracy."}, {"title": "4.4 LLM Evaluation Results", "content": "We conduct a comprehensive evaluation of six open-source (Zephyr, LLaMA-2, LLaMA-3, Mistral) and five proprietary (OpenAI and Cohere families) LLMs. Detailed information about the model versions can be found in Appendix Table 7. Each model is rigorously tested against our newly proposed SAFEWORLD benchmark. We assess model responses across the three dimensions outlined above. The results of these evaluations are presented in Table 2.\nLLAMA & Mistral vs. Proprietary LLMs. The LLaMA and Mistral-series models demonstrate impressive performance against proprietary LLMs. Notably, some models outperform GPT-3.5-turbo and Command-R/R+ in coverage, faithfulness, and factuality. Moreover, they even exceed GPT-4-turbo in response type classification. This success underscores the potential of open-source models to leverage relevant knowledge effectively, especially in addressing geo-diverse safety scenarios, achieving performance levels comparable to leading proprietary models like the GPT series.\nScrutiny on GPT and Command-R Performance. The query generation method described in \u00a73.3 uses cultural-legal guidelines generated by GPT-4-turbo to create the basis for test queries. This implies that GPT-4-turbo has internalized much of the cultural norms and policy knowledge"}, {"title": "5 Geo-Diverse Safety Alignment Training", "content": "To align model responses with geo-diverse safety standards and appropriate responding practices, we employ Direct Preference Optimization (DPO) [30], a commonly used alignment method. This method fine-tunes open-source LLMs to effectively address global user queries, ensuring safety and utility. It requires high-quality simulated user queries and response preference pairs, guiding models to generate more appropriate responses. This section outlines the creation of alignment training data, SAFEWORLDALIGN (\u00a75.2) and details the training settings (\u00a75.3).\n5.1 Direct Preference Optimization (DPO) Background\nDPO is a training paradigm that aligns models using preference pair data without relying on reinforce-ment learning. It aims to optimize the model by increasing the conditional probability of positive responses compared to negative ones. DPO training data consists of preference pairs, each with a user"}, {"title": "5.2 SAFEWORLDALIGN Alignment Training Data Generation", "content": "DPO training requires preference pair annotations, consisting of a user query Q, a positive response Rp, and a negative response Rn. We detail how we synthesize these three key components: (1) Training Query Generation: Detailed in \u00a73.3.2, this process relies on high-quality, human-verified annotations of cultural-legal guidelines to ensure the generated queries cover geo-diverse safety topics accurately and comprehensively. (2) Positive Response Generation: For a training user query Q regarding cultural norm or policy I, we generate a safe and useful positive response Rp that incorporate I using tailored prompts. Specifically, for a query of Type t, we deploy a custom prompt crafted to elicit responses that align with the desired characteristics for that query type. (3) Negative Response Generation: Motivated from the observations from \u00a74.4 about the current LLM's weaknesses, for a Type t training query Q related to a specific cultural norm or policy I, we create two distinct categories of negative responses for constructing the preference pairs:\nNegative Category 1 consists of negative responses that adhere to correct cultural-legal guideline I but correspond to a different response type t' where t' \u2260 t. Specifically, for a query of Type t, we utilize a prompt that tailors for generating the response with a different type t' that misaligns with the query type. For example, consider a SPECIFICANSWER query that demands an alerted response to a violated cultural norm or policy. A negative response for this category could be drawn from response DOANSWER, which fails to provide any reminders of the violation. This misalignment between the query and response type further encourages the model to acquire the desired behavior of LLMs when faced with diverse global user queries.\nNegative Category 2 consists of negative responses that match the user query type t but refer to incorrect cultural norms and policies I' where I' \u2260 I. For example, if the correct guideline is about infidelity to the wife or girlfriend, a negative response contains a perturbed incorrect guideline I' (e.g., the green hat is offensive to elders). Generating negative responses with the reference of incorrect guidelines I' via LLM prompting ensures these factual errors in the responses while being relevant with the user queries and encourages the model to precisely distinguish and memorize the correct cultural norms and policies. Note that since REFUSETOANSWER queries require only refusal and lack involved cultural norm and policy information, we do not generate responses for this negative response category across all REFUSETOANSWER queries."}, {"title": "5.3 Alignment Training Settings", "content": "Following the open-source LLM alignment method from the Huggingface Alignment Handbook [42], we employ DPO training on top of an initial reference policy, Zephyr-7B-SFT-Full, an already super-vised fine-tuned model. To maintain evaluation integrity, we exclude training queries involving cultural-legal guidelines present in the test set, preventing data leakage and ensuring rigorous testing of the model's ability to generalize. The final DPO training dataset, SAFEWORLDALIGN contains 45,746 preference pairs: 26,382 for Negative Category 1 and 19,364 for Negative Category 2. We refer to our alignment models as SAFEWORLDLM. See Appendix C for parameter details."}, {"title": "5.4 SAFEWORLDLM Evaluation Results", "content": "In this section, we provide an in-depth evaluation and analysis of the performance of our SAFE-WORLDLM on SAFEWORLD. Additionally, we conduct ablation studies to highlight the effectiveness of our specially constructed DPO training data. Our analysis spans both SAFEWORLD and general NLP and safety evaluation benchmarks, demonstrating the robust improvements our approach offers.\nMain Results. Table 2 and Table 3 highlight that our 7B SAFEWORLDLM-series LLMs sig-nificantly outperform nearly all competing models, including GPT-40, across all dimensions. It"}, {"title": "What if we enhance GPTs with additional guidance?", "content": "In Table 3, we compare our SAFEWORLDLM-series LLMs against various prompting baselines that provide explicit instructions for considering regional differences established upon the top-performing GPT-series model, GPT-4-turbo. Additionally, we include baselines that integrate both ground-truth cultural-legal guidelines and relevant guidelines retrieved from SAFEWORLD in the user prompt. We find that even if we provide explicit hints to GPT-4-turbo, our SAFEWORLDLM-series LLMs still demonstrate superior performance, un-derscoring the substantial benefits of additional safety alignment training. Although SAFEWORLDLM scores lower in faithfulness compared to GPT-4-turbo w/ Ground-Truth Guidelines, this difference is primarily because the baseline model directly utilizes ground-truth guidelines. We also notice that there are still occasional inconsistencies where GPT-4-turbo might not integrate the provided ground-truth guidelines into its responses, thereby resulting in lower coverage score. Overall, we observe that SAFEWORLDLM and its variants perform competitively even compared to a state-of-the-art models supplemented with additional human-written guidelines. This highlights the value of our alignment training method, which enhances the model's ability to identify and adapt to diverse cultural and legal norms across different regions."}, {"title": "Ablation Studies", "content": "To understand the impact of different components in our alignment training, we conduct ablation studies on different variants of SAFEWORLDLM. We tested three variants: (1) SAFEWORLDLM w/o Neg. Category 1 is the variant trained with only the preference pairs contain-ing the negative responses based on incorrect norm and policy knowledge. (2) SAFEWORLDLM w/o Neg. Category 2 is the model trained with only the preference pairs containing the negative responses with incorrect response types. (3) SAFEWORLDLM (50%) represents another variant trained using half of the total SAFEWORLD align training dataset, incorporating both types of nega-tive responses, designed for a fair comparison with the previous two variants thanks to the matched amount of training data. As shown in Table 3, the first two variants show distinct advantages. SAFE-WORLDLM w/o Neg. Category 1 shows better proficiency in factuality, while SAFEWORLDLM w/o Neg. Category 2 outperforms in response type matching. This can be attributed to the distinct training approaches: SAFEWORLDLM w/o Neg. Category 1 uses preference pair data that emphasizes the contrast between involved norms and policy contents, enabling it to generate more precise and factual responses. On the other hand, SAFEWORLDLM w/o Neg. Category 2 is tailored to better understand and align with the desired behaviors associated with global user query types. This disparity reveals that different negative response generation strategies can significantly enhance model performance in specific key evaluation dimensions critical to the SAFEWORLD benchmark. Furthermore, comparing SAFEWORLDLM (50%) with the former two variants shows that it achieves better performance across all evaluation dimensions, indicating that a more holistic improvement in model performance can be achieved by integrating diverse types of preference pairs."}, {"title": "5.5 General NLP and Safety Benchmark Evaluation Results", "content": "To further assess the impact of our SAFEWORLD training data on both general NLP and safety benchmarks, we conduct additional experiments to investigate that the geo-diverse safety align-ment does not compromise performance on downstream tasks. We select two general NLP tasks, MMLU [16] and HellaSwag [47], from the Open LLM Leaderboard on Huggingface. Following the leaderboard's few-shot evaluation framework, we provide 5-shot and 10-shot in-context examples for MMLU and HellaSwag, respectively. We also evaluate the models on two general safety benchmarks: Anthropic HH-RLHF [2] and BeaverTails [21]. Using the methodology from [36], we measure the proportion of harmless responses in the test sets as our primary safety metric, implemented through GPT prompting. We compare SAFEWORLDLM with the base model Zephyr-7B-SFT-Full to see the impact of SAFEWORLDALIGN on the general tasks.\nWe find that training on SAFEWORLDALIGN can achieve 96.5% and 80.2% harmless response ratios, significantly superior to Zephyr-7B-SFT-Full's performance of 59.3% and 74.2% on the two general safety benchmarks, HH-RLHF and BeaverTails. Additionally, we observe that SAFEWORLDALIGN'S performance on two general NLP tasks, MMLU and HellaSwag, is 56.6% and 78.5%, matching 56.8% and 78.5% performance of Zephyr-7B-SFT-Full, even though SAFEWORLDALIGN is designed for geo-diverse safety alignment. These findings suggest that SAFEWORLDALIGN enables models to significantly enhance geo-diverse and general safety alignment while maintaining performance on general NLP tasks. In Appendix D.2, we provide further analysis showing that combining SAFEWORLDALIGN with general alignment data, such as ULTRAFEEDBACK and SAFER-INSTRUCT, enhances performance beyond using ULTRAFEEDBACK and SAFER-INSTRUCT alone, respectively."}, {"title": "5.6 Human Evaluation", "content": "We further conduct human evaluation to showcase the effective-ness of SAFEWORLDLM according to the global annotator feed-backs. Following standard settings for evaluating LLMs' ability to follow instructions [2, 8], we recruit global annotators from 9 different countries as the users to compare and rate model re-sponses to geo-diverse safety queries based on helpfulness and harmlessness. We randomly sample 40 queries from each query type for the human evaluation. From Figure 7, we find that SAFE-WORLDLM achieves an 18-20% higher winning rate than GPT-40 in both dimensions, further demonstrating SAFEWORLDLM's effectiveness and its global acceptability among users."}, {"title": "5.7 Western vs. Non-Western", "content": "One of the key objectives of studying geo-diverse safety align-ment is to ensure models to perform equitably across both Western and non-Western countries, thereby delivering fair benefits to users worldwide. To achieve this, we analyze performance disparities between instances involving Western and non-Western countries, where smaller disparities indicate greater inclusivity. Notably, apart from the response type alignment dimension, SAFEWORLDLM demonstrates smaller disparities compared to GPT-40 and Command-R+. We attribute this improve-ment to the richer emphasis on non-Western knowledge in our training data, as illustrated in Figure 13. This focus likely contributes to the model's more balanced performance across different regions. These results highlight our commitment to developing inclusive models that cater effectively to a diverse global audience."}, {"title": "6 Conclusion", "content": "We introduce SAFEWORLD, a novel benchmark for evaluating safety alignment across diverse global contexts, ensuring LLMs cater to worldwide user needs. For comprehensively assess LLM response, we propose a multi-dimensional safety evaluation framework focusing on key dimensions needed for address user queries involving geo-diverse safety topics. Beyond evaluation, we present a geo-diverse safety alignment training method, encouraging models to acquire desired behaviors and precisely distinguish and memorize the cultural-legal guidelines. Our approach significantly enhances geo-diverse safety alignment, outperforming GPT-40, while maintaining strong performance on general NLP and safety tasks."}, {"title": "5.1 Direct Preference Optimization (DPO) Background", "content": "query Q, a positive response Rp, and a negative response Rn. The entire DPO training annotations can be represented as D = {(Q, Rp, Rn)}. The optimization objective for DPO minimizes:\nLDPO (\u03c0\u03b8; \u03c0ref) = -E(Q,Rp, Rn)~D [log \u03c3 (\u03b2log \u03c0\u03b8(Rp|Q) - \u03b2log \u03c0\u03b8(Rn|Q) - log \u03c0ref (Rp|Q) + log \u03c0ref (Rn|Q))] (1)\nwhere \u03c3 is the sigmoid function, \u03b2 is a hyperparameter, and \u03c0ref is the initial policy."}, {"title": "A SAFEWORLD Construction", "content": "The benchmark creation involves two key stages: (1) constructing GEOSAFEDB, a cultural and legal geo-diverse safety database (Appendix A.1), and (2) formulating SAFEWORLD benchmark queries, each of which corresponds to cultural-legal guidelines in GEOSAFEDB (Appendix A.3).\nA.1 GEOSAFEDB Development\nThe initial phase of SAFEWORLD focuses on developing GEOSAFEDB, a culturally and legally geo-diverse safety database. This database includes cultural norms and public policies from various geographic backgrounds. Previous methods face challenges such as limited relevance to safety concerns and compromised data quality, often due to top-down collection methods and insufficient annotation processes. To overcome these limitations, we propose a bottom-up approach that gathers country- and region-level guidelines through LLM prompting, followed by validation by native or local annotators, ensuring both accuracy and cultural and legal sensitivity."}, {"title": "A.2 Global User Survey Regarding Geo-Diverse Safety User Query Types", "content": "Before finalizing the global user query types for our study, as shown in Figure 15, we conduct a survey to better understand the response types that global users might expect in geo-diverse scenarios. We introduce three candidate response types, labeled as SPECIFICANSWER, COMPREANSWER, and REFUSETOANSWER, for participants to consider. Among the 21 respondents from 8 different countries, 11 expressed a preference for all three response types, while only 2 opted for none. Based on these insights, we decided to include all three query types in our study. Additionally, to enhance the complexity of the safety benchmark and to discourage models from overly frequent alerts about norm or policy violations, we incorporated DOANSWER queries into our evaluation."}, {"title": "A.3 Cultural Norms and Legal Policies/Laws Queries", "content": "After building GEOSAFEDB we proceed to construct queries that reflect real-life geo-diverse safety situations. To identify the most relevant use cases, we conduct surveys with participants from diverse geographical backgrounds. Based on the survey results (Appendix A.2), we design four distinct query types, each tailored to elicit a specific response type. Each query in SAFEWORLD includes a scenario that presents a culturally or legally sensitive context, accompanied by a relevant question. Figure 10 illustrates our query generation process. For more details on the generation prompts, please refer to the Supplemental Material. Below, we detail the steps involved in creating these queries. Figure 11 show SAFEWORLD query examples."}, {"title": "B Automatic Evaluation Framework", "content": "B.1 Evaluated Models\nWe conduct a comprehensive evaluation of six open-source (Zephyr, LLaMA-2, LLaMA-3, Mistral) and five proprietary (OpenAI and Cohere families). Detailed information about the model versions can be found in Table 7."}, {"title": "C Alignment Training Settings", "content": "Following the open-source LLM alignment method outlined in the Huggingface Alignment Hand-book [42], we employ the DPO training on top of an initial reference policy, Zephyr-7B-SFT-Full, an"}, {"title": "D Further Discussions", "content": "In this section, we seek to answer a couple of additional research questions.\nD.1 How reliable is our evaluation framework?\nWe conduct experiments to assess whether our LLM-based automatic evaluation framework aligns with human evaluations across four dimensions. To this end, we randomly sample 60 responses generated by five models from our evaluation results, calculating Pearson correlation (p) and Kendall's tau (T) scores. We utilize Llama-3-70B-Instruct and GPT-4-turbo as the base models for the evaluation metric. As shown in Table 9, our results indicate a notably strong correlation (>0.7) across all dimensions between human judgments and our evaluation framework when using GPT-4-turbo. In contrast, Llama-3-70B-Instruct demonstrates only moderate correlation. Given these findings, we prioritize GPT-4-turbo for our evaluators due to its superior alignment with human assessments.\nD.2 Does using more alignment data yield better performance?\nGeneral NLP Benchmark Evaluation. We initially explore the effect of integrating SAFEWORLD training data with the commonly used ULTRAFEEDBACK DPO data, which is aimed at enhancing"}, {"title": "D.3 Which types of queries are LLMs better/worse at?", "content": "Figure 16 illustrates the performance breakdown of various models when handling different types of queries. The scores for each dimension represent the average of the scores presented in Ta-ble 2 and Table 3. Notably, both open-source and proprietary LLMs, such as Mistral-7B-Instruct, Llama-3-8B-Instruct, Command-R-Plus, GPT-4-turbo, and GPT-40, generally perform poorly on norm/policy queries, with the exception of NORMDOANSWER. In contrast, our alignment model, SAFEWORLDLM consistently outperforms the other LLMs across all query types."}, {"title": "E Limitations", "content": "Coverage of Countries. Our SAFEWORLD benchmark currently focuses on the top 50 most populous countries, which limits its scope by excluding cultural and legal norms from less populous nations. This narrow coverage may result in the omission of valuable geo-diverse perspectives that are crucial for achieving a truly comprehensive understanding of global safety norms. Expanding the dataset to include a wider range of countries in future work will be essential for ensuring more inclusive and well-rounded geo-diverse safety alignment.\nLimited Scope of Query Types. Although our study encompasses a range of query types, it does not capture the full spectrum of scenarios and subtleties present in real-world geo-diverse safety contexts. This limitation may result in the underrepresentation of certain response types and cultural-legal nuances in our analysis. To enhance the robustness of LLM performance evaluation in"}, {"title": "F Supplementary Materials", "content": "Can you imagine a VIVID", "tomorrow\", \"next week\" in the scenario.\nHere are some examples": {"norm": {"in": "n# Scenario: [SCENARIO", "norm": ["SPECIFIC_VIOLATED_NORM"], "tomorrow\", \"next week\".\nHere are some examples": {"norm": {"in": "n# Scenario: [SCENARIO", "norm": ["SPECIFIC_NORM"], "Scenario": ". No more details and explanations. Here is the given scenario: {Input scenario}.\nCan you follow the examples to generate an INFORMAL, CASUAL, and COMPREHENSIVE question (no interjections like \"hey\", \"hi\", no clauses starting with \"without\") like what you talk to your best friend, based on the given scenario? Besides the INFORMAL and CASUAL question, share your thoughts that motivate the question generation in one sentence.\nDO NOT question ANYTHING ABOUT IF THE scenario IS APPRO"}}}}}]}