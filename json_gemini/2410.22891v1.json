{"title": "VPO: Leveraging the Number of Votes in Preference Optimization", "authors": ["Jae Hyeon Cho", "Minkyung Park", "Byung-Jun Lee"], "abstract": "Direct Preference Optimization (DPO) trains a language model using human preference data, bypassing the explicit reward modeling phase of Reinforcement Learning from Human Feedback (RLHF). By iterating over sentence pairs in a preference dataset, DPO enhances generation quality by increasing the likelihood of producing preferred sentences over less favored ones. Preference datasets are typically created by selecting preferred sentences through a voting process involving multiple individuals, as opinions can vary due to the subjective nature of human preferences. While the number of votes offers insight into whether a sentence pair is clearly preferable or controversial, current methods do not fully leverage this information. In this paper, we introduce a technique that leverages user voting data to better align with diverse subjective preferences. We employ the Bayesian Minimum Mean Square Error (Bayesian MMSE) estimator to model the probability that one generation is preferable to another. Using this estimated probability as a target, we develop the Vote-based Preference Optimization (VPO) framework, which incorporates the number of votes on both sides to distinguish between controversial and obvious generation pairs. We show that previous algorithms, such as DPO and Identity Preference Optimization (IPO), can be extended using the proposed framework, termed VDPO and VIPO. Our experiments demonstrate that these proposed algorithms outperform various existing methods, including their base algorithms.", "sections": [{"title": "1 Introduction", "content": "In general-domain applications of language models (LM), the model should be aligned with human values, such as helpfulness, honesty, and harmlessness. Pre-training and supervised fine-tuning (SFT) enable the development of models with notable capabilities across a wide range of natural language processing (NLP) tasks (Wei et al. 2021; Wang et al. 2023). However, additional training using pairwise preference data is often employed to further align the model with human values.\nPreference alignment methods, such as reinforcement learning from human feedback (RLHF, Stiennon et al. 2020; Ouyang et al. 2022) and direct preference optimization (DPO, Rafailov et al. 2023), have shown significant successes in enhancing the human usability of language models. Consequently, these preference optimization processes are now considered essential in the development of state-of-the-art large LMs (Achiam et al. 2023; Team et al. 2023).\nGiven pairwise preference data with labels indicating which response is preferred, RLHF trains a reward model to align with these preferences, enabling the evaluation of a language model's outputs. Subsequently, the language model is trained using a reinforcement learning algorithm to maximize the expected reward of its generated responses. In contrast, DPO provides an alternative approach by directly adjusting the generation probabilities of the language model based on preference labels. This method eliminates the need for a separate reward modeling phase, thereby reducing computational costs.\nHowever, we note that the current labels in pairwise preference datasets may provide limited information in these processes. Human preference is inherently subjective, and not all pairwise generations can be easily classified as simply better or worse, as judgments vary among individuals. As a result, voting or scoring processes are often utilized to gather preference data, yet this additional information has largely been overlooked in previous studies on preference alignment.\nIn this paper, we introduce a simple yet effective method to better utilize the rich side information inherent in human preference datasets. Our approach models the underlying target preference probability using the Bayesian Minimum Mean Square Error (MMSE) estimator, enabling the model to distinguish between learning from clear-cut examples (those with a high vote/score gap) and contentious cases (those with a low vote/score gap). We term this framework as Vote-based Preference Optimization (VPO), and extend established algorithms such as DPO and Identity Preference Optimization (IPO, Azar et al. 2024) into VDPO and VIPO, respectively, demonstrating the broad applicability of our approach.\nIn the experiments, we empirically demonstrate the following:\n\u2022 VDPO and VIPO outperform existing algorithms, achieving improved generation quality and training stability.\n\u2022 Our framework is also adaptable to utilize AI feedback in scenarios where costly human voting information is unavailable."}, {"title": "2 Preliminaries", "content": "In this section, we outline the standard procedures for training a general-purpose language model prior to aligning it with human values. The process begins with the following two steps:\nPretraining To provide the model with general capabilities, it is trained on a large corpus using next token prediction, commonly referred to as teacher forcing.\nSupervised finetuning Following pretraining, supervised finetuning is performed to equip the model with specific abilities required for the target domain tasks. During this phase, the model is trained on a dataset specifically curated for the intended tasks. We refer to the model after this step as $T_{ref}$ henceforth."}, {"title": "2.1 RLHF: Alignment via Reward Modeling", "content": "The standard RLHF process consists of two steps.\nReward model training The reward model is trained using human preference data to align its judgments with human values. The human preference dataset is composed of the triplet $D = {x, Y_1, Y_2}$, where x is the context, and y1 and y2 are response pairs given the context. We define the events:\n$Y_1 = y_1$ is favored over $y_2$, $Y_2 = y_2$ is favored over $y_1$.\nThe probability of these events is modeled using a Bradley-Terry model, which is defined as follows:\n$Pr(Y_1|x) := \\frac{exp(r(x, y_1))}{exp(r(x, y_1)) + exp(r(x, y_2))}$"}, {"title": "RL finetuning", "content": "After training a reward model, a regularized RL algorithm is used to maximize the expected reward while ensuring the model does not deviate significantly from the initial model $T_{ref}$:\n$\\max_\\theta E_{x~D, y~\\pi_\\theta (y|x)} [r(x, y) \u2013 \\beta D_{KL}(\\pi_\\theta(\\cdot|x)||T_{ref}(\\cdot|x))]$.\nThis approach ensures that the updated model achieves high reward, meaning strong alignment with human preferences, while preserving the general language capabilities of the reference model."}, {"title": "2.2 DPO: Alignment without Reward Modeling", "content": "Direct preference optimization Training an additional reward model, along with using reinforcement learning to fine-tune a model, involves numerous complex engineering challenges. DPO provides an alternative approach by directly training the language model on the preference dataset by substituting reward model with its closed-form solution. Assuming y1 is preferred over y2, the DPO objective is defined as follows:\n$\\max_\\theta E_p [log\\sigma (r(x, y_1) \u2013 r(x, y_2))]$\nwhere $r(x, y) = \\beta log \\frac{\\pi_\\theta(y|x)}{T_{ref}(y|x)} + Blog Z(x)$,\nand $Z(x) = \\sum_y T_{ref}(y|x) exp( \\frac{r(x, y)}{\\beta})$.\nDPO simplifies the training process by directly leveraging the dataset, thereby enhancing both stability and efficiency.\nReward divergence A significant drawback of DPO, as highlighted by Azar et al. (2024), is that its objective is theoretically prone to divergence. When considering a single data point (x, y1, y2), the DPO objective continually drives an increase in the margin r(x, y1) \u2013 r(x, y2) without bound. In practice, this results in an inflated scale of the implicit reward function, which undermines the regularization towards Tref. This is one reason why DPO often becomes unstable when trained over multiple epochs and requires early stopping.\nTo address this issue, Identity Preference Optimization (IPO, Azar et al. 2024) and conservative DPO (CDPO, Mitchell 2023) have been proposed, both of which stabilize training by adjusting the objective."}, {"title": "3 Related Works", "content": "Alignment without reward modeling Since the introduction of DPO, several studies have focused on improving the efficiency of preference alignment algorithms. As briefly introduced above, Azar et al. (2024) mathematically analyzed the issue of diverging rewards in DPO and proposed IPO as a potential solution. Ethayarajh et al. (2024) introduced Kahneman-Tversky Optimization (KTO), which utilizes the Kahneman-Tversky human utility function to better align with human values. Hong, Lee, and Thorne (2024) presented the Odds Ratio Preference Optimization (ORPO), a reference model-free approach that eliminates the dependency on a baseline model, thereby simplifying the optimization process.\nAlthough various improvements to DPO are being explored, they still share the limitation of not fully utilizing side information beyond the binary indication of more or less preferred. In this paper, we propose enhancing existing algorithms by incorporating additional side data. The improvements we suggest are orthogonal to the advancements made by these existing methods and can be seamlessly integrated into all of these approaches.\nNoise in preference labels Several studies have examined the potential for preference labels to be noisy due to human subjectivity. While the primary objective of cDPO (Mitchell 2023) was to address the issue of reward divergence, the algorithm was formulated with the assumption that preference labels may contain noise. To further enhance the robustness of learning in noisy environments, Chowdhury, Kini, and Natarajan (2024) developed robust DPO (rDPO), which is specifically designed to minimize the impact of noise in preference labels.\nAs we will demonstrate, our VPO framework can also be interpreted as modeling the level of noise in preference labels using side information. In cDPO and rDPO, this noise level is assumed to be constant and is tuned as a hyperparameter. In contrast, our approach offers a straightforward and intuitive method for estimating noise levels in the preference dataset, building on a similar framework."}, {"title": "4 Method", "content": "In standard protocols for constructing human preference datasets, each generation pair is typically evaluated multiple times by different evaluators to account for the variability in their judgments. Although the number of votes from these evaluators is usually recorded during dataset creation, this information has often been underutilized in previous methodologies. Below, we provide a detailed illustrative example to emphasize this point.\nTo this end, we propose modeling the target preference probability: p(Y1|x, v1, v2) and p(Y2|x, V1, V2), where v1 and v2 represent the number of votes for y1 and y2, respectively. In previous approaches, it is typically assumed that Y1 is the preferred response, assigning p(Y1|x) = 1 and p(Y2|x) = 0. Instead, we employ a Bayesian approach to model the target preference probability, taking into account the number of votes v1, v2 collected during dataset construction. This approach allows for a nuanced interpretation of vote counts, enabling the distinction between different vote distributions, ranging from controversial response pairs to more obvious ones."}, {"title": "4.1 Modelling Targets with Bayesian MMSE", "content": "To better align with the human preference through the finite number of assessments in the preference dataset, we adopt a Bayesian approach. We begin by defining the prior distribution of the target preference probability when no voting information is observed:\np(Y1|x) ~ Beta(c, c) where c > 0 is a hyperparameter."}, {"title": "Theorem 1", "content": "(Pishro-Nik 2014) Bayesian MMSE estimator is solution to the following:\n$\\OMMSE(v_1, v_2) = \\argmin_\\theta \\int (\\theta - \\theta)^2 p(\\theta|v_1, v_2) d\\theta$.\nUsing Bayesian MMSE estimator allows us to convey the implication of various number of votes of response pairs without introducing additional stochasticity to the training."}, {"title": "4.2 Vote-based Preference Optimization (VPO)", "content": "Adopting the Bayesian MMSE estimator as the target preference probability, p(Y1|x, v1, v2) = OMMSE(v1, v2), creates a versatile framework that can be generalized to extend various preference optimization algorithms. We refer to these collection of extended algorithms as the Vote-based Preference Optimization (VPO) framework, which enables a more nuanced understanding of subjective human preferences.\nCross entropy with generalized targets In previous approaches, including RLHF and DPO, the (implicit) reward function is trained by maximizing the log-likelihood, as shown in Eq. (1). This can be interpreted as assuming p(Y1|x) = 1 as the target and using a cross entropy objective. By adopting the generalized target probability p(Y1|x, v1, v2) from VPO, we now obtain a generalized reward loss function:\n$\\max_r E_D \\sum_{i=1}^2 P(Y_i|x, v_1, v_2) log \\sigma(P_r (Y_i|x)).$\nThis objective functions as an adaptive label smoothing mechanism, ensuring that the reward function learns to have a large reward margin for substantial vote gaps and a smaller reward margin for narrower vote gaps."}, {"title": "Vote-based Direct Preference Optimization (VDPO)", "content": "To implement our approach within DPO, we generalize Eq. (2) using the target preference probability from VPO:\n$\\max_\\theta E_D [p(Y_1|x, v_1, v_2) log \\sigma (r(x, y_1) \u2013 r(x, y_2))\n+p(Y_2|x, v_1, v_2) log \\sigma (r(x, y_2) \u2013 r(x, y_1))]$,\nwhere r(.) is defined as in Eq. (2). In addition to differentiating response pairs with varying vote ratios, as discussed in Mitchell (2023), including both the more preferred and the less preferred responses contributes to more stable training by addressing the issue of reward divergence."}, {"title": "Vote-based Identity Preference Optimization (VIPO)", "content": "While IPO (Azar et al. 2024) was introduced to address reward divergence, it can still benefit from distinguishing pairs with varying vote ratios by incorporating VPO. Its objective is:\n$\\min_\\theta \\min E_D\\bigg(\\frac{r(x, y_1) \u2013 r(x, y_2)}{2 \\beta} \u2013 \\frac{1}{2} \\bigg)^2,$\nwhich tries to fix the reward margin to be $\\frac{\\beta}{2}$. This objective is derived from:\n$\\min E_D [ (r(x, y_1) - r(x, y_2) \u2013 \\beta^{-1}p(Y_1|x))^2\n+ (r(x, y_2) - r(x, y_1) - \\beta^{-1}p(Y_2|x))^2]$,\nwith p(Y1|x) = 1 and p(Y2|x) = 0. Adopting VPO by substituting p(Y1|x, v1, v2) in, up to a constant we get:\n$\\min_\\theta E_D\\bigg( \\frac{r(x, y_1) \u2013 r(x, y_2)}{2 \\beta} - \\frac{2p(Y_1|x, v_1, v_2) \u2013 1}{2} \\bigg)^2.$\nBy leveraging vote-based information, VIPO adjusts the reward margin to be proportional to the strength of human preference, up to a maximum of $\\frac{\\beta}{2}$."}, {"title": "5 Experiments", "content": "In this section, we outline the experimental settings used to evaluate the performance of the proposed VPO framework. Additional details about the experiments are available in Appendix B."}, {"title": "5.1 Training Details", "content": "Data Our experiments utilize two widely recognized binary human preference datasets: the Stanford Human Preferences dataset (SHP, Ethayarajh, Choi, and Swayamdipta 2022) and the UltraFeedback Binarized dataset (UFB, Cui et al. 2023).\n\u2022 The SHP dataset consists of Reddit data, where the voting score is calculated by subtracting the number of negative votes from the number of positive votes and then adding one to the result. We use the voting scores directly as v\u2081 and v\u2082 for computing the target preference probability p(Yix, V1, V2).\n\u2022 In contrast, the UFB dataset employs GPT-4 for score annotation, with scores ranging from 1 to 10. To account for the different scaling of this scoring mechanism compared to the number of votes in human-annotated datasets, we exponentiated the scores before computing the target preference probability.\nWe follow the convention of limiting the preference dataset to a maximum of five pairwise comparisons per post to effectively manage the large number of comments associated with certain posts in the SHP dataset.\nModel In our study, we employ two pretrained models: the Pythia 2.8B model (Biderman et al. 2023) and the LLaMA 7B model (Touvron et al. 2023).\nFor training on the SHP dataset, we follow the methodology outlined by Ethayarajh et al. (2024). For the SFT phase, we utilize a combination of datasets, including Anthropic HH (Ganguli et al. 2022), SHP, and OpenAssistant (K\u00f6pf et al. 2024). For the UFB dataset, SFT is performed exclusively using the UFB dataset.\nFollowing the SFT phase, we apply a range of preference alignment techniques to the fine-tuned model. To ensure consistency in our comparisons, we fix the hyperparameters \u03b2 = 0.1 and c = 1."}, {"title": "5.2 Evaluation Method", "content": "Evaluating how well a language model aligns with human values ideally requires human assessment. However, due to the high costs associated with large-scale human evaluation, we employ automatic evaluation methods that have demonstrated strong agreement with human judgments.\nTo assess model performance, we generate outputs using two sets of prompts: one from the test set (in-domain) and another from the Alpaca Farm dataset (out-of-domain, Dubois et al. 2023). We then conduct a comparative analysis of these outputs using the Alpaca Eval 2.0 framework (Li et al. 2023), which provides a standardized and comprehensive evaluation methodology. For evaluating the outputs, we use GPT-4-Turbo as the annotator, which is the default setting in Alpaca Eval 2.0. We report both the win rate and the length-controlled win rate.\nThe SHP dataset covers 18 different domains; for evaluation, we randomly select 20 samples from each domain. For the evaluation of UFB, we randomly select 500 examples from its training set. When evaluating with Alpaca Farm, we use all 805 prompts."}, {"title": "6 Results and Analysis", "content": "In this section, we empirically assess the proposed framework. Section 6.1 presents our main results, demonstrating the strong performance of the VPO framework. In Section 6.2 and 6.3, we explore the key characteristics of VPO. In Section 6.4, we examine in detail the differences in the generations produced by the proposed algorithms."}, {"title": "6.1 Performance Assessment", "content": "On SHP dataset In Table 3, we present the performance of models aligned with various algorithms. The results demonstrate that our proposed algorithms (VDPO and VIPO) consistently outperform the baseline algorithms (DPO and IPO) as well as other methods (KTO, CDPO, rDPO) in terms of win rates against the SFT model, across both standard and length-controlled evaluations.\nFor the Pythia 2.8B model, VDPO achieved the highest win rate in the Alpaca domain at 57.05% surpassing both DPO (55.92%) and IPO (56.35%). The performance gap between VDPO and cDPO underscores the importance of differently estimating the target preference probability depending on the response pair.\nOn UFB dataset Our experimental results on the UFB dataset, as presented in Table 4, demonstrate that our framework led to significant overall performance improvements. Notably, VDPO exhibited a marked enhancement in performance compared to DPO within the learned domain environment, with the win rate increasing by 7.3% and the LC win rate increasing by 4.69%."}, {"title": "6.2 Analysis of Generation Lengths", "content": "According to our proposed objectives, VPO should prioritize learning from data that is preferred by a substantial voting gap, while reducing emphasis on data with a narrower voting gap. Interestingly, this hypothesis was confirmed simply by measuring the length of generations from the aligned models.\nAt the top of Table 5, we measured and reported the lengths of preferred responses by dividing them into two groups: one consisting of responses with a small voting gap and the other with a large voting gap. In the SHP dataset, we observed that responses with a small voting gap are shorter, while in the UFB dataset, responses with a large voting gap are shorter.\nOn the other hand, at the bottom of Table 5, we measured and reported the lengths of generated outputs from the Pythia 2.8B model, aligned using four different algorithms\u2014DPO, VDPO, IPO, and VIPO\u2014across both the SHP and UFB datasets. It can be noted that:\n\u2022 Overall, responses in the UFB dataset are longer than those in the SHP dataset, and all aligned models reflect this difference.\n\u2022 In the SHP dataset, responses with a large voting gap were observed to be longer, consequently, VPO algorithms generated longer outputs on this dataset.\n\u2022 Conversely, in the UFB dataset, responses with a large voting gap were shorter. As expected, VPO algorithms produced shorter outputs on this dataset, and notably, VDPO generated outputs that were half the length of those produced by DPO.\nThese results demonstrate that our algorithm effectively prioritizes learning from responses favored by a larger voting gap, thereby confirming its intended functionality."}, {"title": "6.3 Prevention of Reward Divergence with VDPO", "content": "As described in Section 2.2, one issue with DPO is that its implicit reward function tends to diverge during training. Without early stopping, the reward scale increases indefinitely and deviates from the reference policy, as regularization is effectively ignored.\nOne approach to mitigate reward divergence is to apply label smoothing, as done in cDPO (Mitchell 2023), which allows for a small e probability that a less preferred response may be favored. It has been shown that even a small e can prevent indefinite reward scaling. Similarly, our proposed VDPO can be viewed as using the Bayesian MMSE estimator, which is non-zero, in place of e and it is expected to address the reward divergence issue effectively.\nFigure 2 illustrates how the reward margin-the difference in reward between preferred and non-preferred responses-evolves during preference alignment. Since VPO reduces the reward margin by focusing less on training responses with a small voting gap, the figure shows that VPO algorithms have a smaller reward margin compared to their base algorithms. In the case of DPO and VDPO, DPO exhibits reward divergence, while VDPO effectively manages this issue, resulting in a converged reward margin. Examples of overfitted generations from DPO, caused by reward divergence, are provided in Appendix D."}, {"title": "6.4 Qualitative examples", "content": "Table 6 presents sample outputs from the LLaMA 7B model. The responses from VDPO and VIPO exhibit noticeable improvements over other baselines, demonstrating correctness, coherence, and clarity. SFT shows a lack of coherence, which could be addressed with further preference alignment, while DPO tends to produce overfitted results due to reward divergence.\nInterestingly, we frequently observed that IPO responses, though less clear, were more engaging. We speculate that this reflects the nature of the Reddit data used in the SHP dataset, where such engaging but less helpful responses often receive a considerable number of upvotes (examples in Appendix C). Our framework effectively learns by appropriately weighting different response pairs based on their upvotes, resulting in clearer responses by avoiding an overemphasis on these mediocre responses."}, {"title": "7 Conclusion", "content": "In this paper, we present a novel approach called Vote-based Preference Optimization (VPO), which estimates target preference probability based on the number of votes each response has received. Our method allows for more accurate alignment with human values by considering the subjectivity of annotators. We empirically demonstrate the strong performance of our algorithm across various experimental settings."}]}