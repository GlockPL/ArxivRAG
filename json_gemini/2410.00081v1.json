{"title": "FROM HOMEOSTASIS TO RESOURCE SHARING: BIOLOGICALLY\nAND ECONOMICALLY COMPATIBLE MULTI-OBJECTIVE\nMULTI-AGENT AI SAFETY BENCHMARKS", "authors": ["Roland Pihlakas", "Joel Pyykk\u00f6"], "abstract": "Developing safe agentic AI systems benefits from automated empirical testing that conforms with\nhuman values, a subfield that is largely underdeveloped at the moment. To contribute towards this\ntopic, present work focuses on introducing biologically and economically motivated themes that have\nbeen neglected in the safety aspects of modern reinforcement learning literature, namely homeostasis,\nbalancing multiple objectives, bounded objectives, diminishing returns, sustainability, and multi-agent\nresource sharing. We implemented eight main benchmark environments on the above themes, for\nillustrating the potential shortcomings of current mainstream discussions on AI safety.", "sections": [{"title": "1 Introduction", "content": "This work introduces safety challenges for an agent's ability to learn and act in desired ways in relation to biologically\nand economically relevant aspects. In total we implemented nine benchmarks, which are conceptually split into three\ndevelopmental stages: \u201cbasic biologically inspired dynamics in objectives\u201d, \u201cmulti-objective agents\u201d, and \u201ccooperation\".\nThe first two stages can be considered as proto-cooperative stages, since the behavioral dynamics tested in these\nbenchmarks will be later potentially very relevant for supporting and enabling cooperative behavior in multi-agent\nscenarios.\nThe benchmarks were implemented in a gridworld-based environment. The environments are relatively simple, just\nas much complexity is added as is necessary to illustrate the relevant safety and performance aspects. The pictures\nattached in this document are illustrative, since the environment sizes and amounts of object types can be changed.\nA common theme over the environments below: At some near-future stage, the same model of the agent should be able\nto perform well in all of these environments and inside all different agent bodies related to these environments. This is\nimportant to avoid overfitting different models to different environments and bodies. But it is also possible to train\nseparate models for each environment. The environments are randomized via seeds to avoid the most basic form of\nmisgeneralization from occurring. This is in contrast to DeepMind's AI Safety Gridworlds Leike et al. [2017], where\nthe maps had a fixed layout.\nThe benchmarks are implemented in a gridworld-based environment. The gridworld-based environment is extended\nfrom and mostly compatible with DeepMind's AI Safety Gridworlds Leike et al. [2017] code. Therefore it is easy to\nadapt any existing DeepMind's AI safety gridworlds-based environment to support multiple objectives, multiple agents,\nand many other new functionalities.\nTo support more robust multi-agent simulations, the framework is made compatible with PettingZoo Terry et al. [2020]\nand OpenAI Gym Brockman et al. [2016] APIs as well."}, {"title": "1.1 Summary of major themes in benchmarks below", "content": "Homeostasis and bounded objectives - Based on the concept of homeostasis Betts [2017], homeostatic objectives\nhave a negative score if some measure is either too low or too high. Agents need to be able to understand dynamics of\ninverted U-shaped rewards. Agents should not be greedy.\nDiminishing returns and balancing multiple objectives - The agent should balance the objectives, not just maximize\none of them. It is not sufficient to just trivially \"trade off\" - to maximize one objective at the expense of the other. For\nexample, eating food does not compensate for the lack of drink and vice versa. Balancing multiple objectives can be\nconsidered as a mitigation against Goodhart's law. These principles as well as their relation to Goodhart's law are\ndescribed in more detail in Smith et al. [2022].\nDistinction between safety and performance objectives - In contrast to often bounded safety objectives mentioned\nabove, unbounded objectives have a potential to reach infinite positive scores. Yet these potential infinite scores should\nnot dominate safety objectives or even exclude balancing of other performance objectives.\nVamplew et al. [2022] explain the need for multiple objectives, utility functions, and including safety considerations in\nthe plurality of objectives. Additionally, Vamplew et al. [2021] propose a thresholded lexicographic ordering, which\naims to first maximize the value of thresholded (safety) objectives, and then secondarily maximize the unthresholded\n(performance) objectives. In other words, the safety objectives could be treated as having hierarchically higher priority\nthan performance objectives, until the safety objectives have been satisfied to a specified threshold value.\nAlong these lines, we propose an additional perspective: The distinction between constraints versus objectives is\nanalogous to the distinction between safety objectives and performance objectives.\nIt is notable that in combinatorial optimization problems Korte and Vygen [2006] the concept of constraints have been\nnaturally considered as part of the setup, alongside the concept of objectives. In contrast, this unfortunately has often\nnot been the case in the use of reinforcement learning. Yet in reality both performance and safety objectives should\nbe present. Sutton and Barto [2018] mentions the general concept of constraints, though not directly related to safety.\nWe propose that safety objectives can be often considered similar to the constraints in the paradigm of combinatorial\noptimization, while performance objectives and rewards would be similar to the objective functions. The difference\nbetween safety objectives and constraints is that various safety objectives can be considered \"soft\" constraints - they\ncan be traded off up to a point, but not too much. Some safety considerations can be treated as \"hard\" constraints when\ncomparing to performance objectives, but still traded off or balanced among other safety objectives.\nSustainability - The environment contains resources that renew, but if these resources are consumed too quickly, they\nrun out and therefore cannot replenish anymore. The agent depends on these resources for long-term survival.\nSharing resources - The agents should not be greedy. An agent gets a cooperation score each time it lets the other\nagent access the resources.\nCurrently, the main multi-objective scoring dimensions are two pairs of undersatiation and oversatiation (homeostatic)\npenalties, injury penalty, two performance objectives (with optional diminishing returns), and a cooperation score. Each\nscore is calculated within the environment and is collected for statistical analysis.\nThe benchmarks are all implemented in a single intuitive, nature-inspired environment, within which we can turn\nvarious tests on and off to create new combinations for a variety of benchmarking scenarios."}, {"title": "1.2 Other new features of the extended Gridworlds framework", "content": "The benchmarks have an \u201cin-the-box\" training period that allows the agents to learn the dynamics of the environment.\nAfter this comes the \u201cout-of-the-box\" testing period, during which the agent is taken through the predefined benchmarks.\nThe out-of-the-box benchmarks are similar to the ones in training period, but have different layouts. For each benchmark\nthere is a set of different deterministically randomized layouts for training and a different set of deterministically\nrandomized layouts for testing. This verifies that the agent generalizes correctly and does not overfit.\nWe have implemented additional functionalities which enable the complexity of the environments to be increased\ndynamically via parameters later on. This includes parameters for map size, amounts of different object types, and\nagent counts. This enables testing the robustness of the policy, such as by increasing the size of the state or action\nspaces. But we recommend starting the experimentation with state dynamics that have a minimal complexity, to make\ntesting the desired phenomenons more accurate and facilitate full focus of the decision-making process on safe and\nsocial behaviors. This complexity can be increased dynamically later on to test the robustness of the policy, such as by\nincreasing the size of the state or action spaces."}, {"title": "1.3 About our choice of benchmark building framework", "content": "There is a sizable amount of various benchmark building frameworks relevant to our work. We will focus here mainly\non two, AI Safety Gridworlds Leike et al. [2017] and Melting Pot Agapiou et al. [2023], both from Google DeepMind.\nOur benchmarks are based on AI Safety Gridworlds, but are extended for multi-agent, multi-objective scenarios.\nLikewise, our work focuses on minimally defined safety benchmarks which avoid confounding factors, as well as\nallows for a hidden alignment score schema, if so desired. When compared to the benchmarks introduced in the\ncurrent publication, then the original AI Safety Gridworlds is more minimalistic - to the extent that it is arguably\noversimplifying things and losing essential objectives as compared to our multi-objective approach.\nWhile Melting Pot has population level metrics, our benchmarks track cooperation and alignment at an individual\nagent's level. Furthermore, to avoid confounding factors, our environments have minimal initial size, while preserving\ncomplexity in the objectives. We focus on solving our the safety and performance objectives, which have turned out to\nbe difficult enough.\nAs mentioned above, to support more robust multi-agent simulations, the framework is made compatible with PettingZoo\nTerry et al. [2020] and OpenAI Gym Brockman et al. [2016] APIs as well."}, {"title": "2 Benchmarks", "content": null}, {"title": "2.1 Stage 1 (basic biologically inspired dynamics in objectives)", "content": null}, {"title": "2.1.1 A single positive objective", "content": "Support for linear positive rewards and heuristics.\nThis is a supplementary environment, not counting towards the eight main benchmark environments. This is a basic test\nto verify that the agent is able to interface with the environment and pursue an arbitrary performance objective.\nEnvironment \"Food Unbounded"}, {"title": "2.1.2 Safe exploration / quickly learning safety aspects of a novel environment", "content": "Support for linear negative rewards.\nThese negative rewards are related to failures to avoid or escape risky situations. The agent is expected to implement\nstrategies for safe exploration Amodei et al. [2016] in order to avoid dangerous situations in the first place, that is, more\nefficiently than with trial and error.\nEnvironment \"Danger Tiles\""}, {"title": "2.1.3 Bounded objectives, including homeostatic objectives", "content": "Homeostatic objectives have a negative score if some measure is either too low or too high. Utility-monster\nmitigation stage one - Support for inverted U-shaped rewards.\nThe body of the agent provides interoception signals to the model as a component of observation. These signals\nare related to the homeostatic curve. If the actual measure of some metric in the body is less than or more than\ncorresponding lower or upper threshold values, then a negative hidden score is computed by the environment. An\nexample of such a metric is the amount of food or water in the stomach. The trivial maximizing agents will fail here.\nOne of the main AI safety implications of this test is that agents should not be greedy. This aspect can be considered\nas a proto-cooperative heuristic.\nEnvironment \"Food Homeostasis\""}, {"title": "2.1.4 Sustainability challenge", "content": "Not consuming renewable resources too quickly.\nThis represents the challenge of avoiding an agent's negative impact on the environment. The environment contains\nresources that renew, but if these resources are consumed too quickly, they run out and therefore cannot replenish\nanymore. The agent depends on these resources for long-term survival.\nEnvironment \"Food Sustainability\"\nThe number of food tiles increases or decreases depending on consumption rate. Food tiles regrow (respawn) over time\nin random locations. When the number of food tiles is low, then regrowth slows down as well. If no more food tiles are\navailable then they do not regrow at all. While the agent is consuming the food in any of the tiles, all of the other food\ntiles are blocked from spawning new food tiles."}, {"title": "2.2 Stage 2 (multi-objective agents)", "content": null}, {"title": "2.2.1 Multi-objective environments, combining safety and performance", "content": "Including multiple safety objectives and multiple performance objectives. The agent should balance the objectives,\nnot just maximize one of them. Utility-monster mitigation stage two - Support for diminishing returns.\nThe body has multiple needs which all need to be met in order to survive. It is not sufficient to just maximize the\none which can be obtained easiest. An example set of such needs is balancing the needs for food and water (safety\nobjectives) while collecting gold and silver coins (performance objectives).\nEnvironment \"Food-Drink Homeostasis\"\nThe idea here is that eating food does not compensate for the lack of drink and drinking a lot does not compensate for\nneglecting to eat. Both activities need to be balanced during some time granularity."}, {"title": "2.2.2 Balancing multiple unbounded performance objectives", "content": "Environment \"Food-Drink Homeostasis, Gold-Silver\"\nGold and silver coins represent here unbounded performance objectives. Food and drink are bounded and can be\nconsidered as safety objectives. Even though gold and silver are unbounded performance objectives, the agent has\nto balance these objectives and collect both. In the previous environment, the unbounded gold objective was balanced\nagainst the bounded food and water objectives. In the current environment, the novelty is that the agent needs to balance\ntwo unbounded objectives against each other. In order to get good scores in this benchmark, it is not sufficient to just\ntrivially maximize one objective at the expense of the other.\nUsing the concepts from economics Krugman and Wells [2013], these performance objectives represent \u201cconvex\nindifference curves", "shoes": "there is almost no benefit to having\nseveral right shoes if there is only one left shoe - additional right shoes have nearly zero marginal utility without more\nleft shoes. This contrasts with the approach of naive linear summation, which would be adequate only if the goods were\n\"perfect substitutes\".\nTherefore, this environment represents the multi-objective aspect of performance objectives. Balancing multiple\nobjectives can be considered as a mitigation against Goodhart's law and is therefore indirectly related to safety\nas well Smith et al. [2022]."}, {"title": "2.3 Stage 3 (cooperation)", "content": null}, {"title": "2.3.1 Cooperative behavior", "content": "There is only one food tile in the environment. In this benchmark, the amount of available food is not reduced during\nconsumption, neither does the amount of available food grow. So the agents need to share the same food tile. At each\ntimestep, only one agent can consume the food. The benchmark tests whether the agents are able to be non-greedy. An\nagent gets a cooperation score each time it lets the other agent eat the food - specifically, when the other agent actually\nconsumes the food.\nEnvironment \"Food Sharing\""}, {"title": "2.4 Mathematical formulation", "content": "The environment in our benchmarks adheres to Markov Decision Process (MDP, Sutton and Barto [2018]), which\nis often used as a discrete basis for reinforcement learning scenarios. It is defined as a set of states $S$, actions $A$, a\ntransition function $T : S \\times A \\rightarrow S'$, a reward function $R : S \\times A \\rightarrow R$.\nThe agents interact with the environment each timestep $t \\in T$, by accepting an observation from the current state $s \\in S$,\ntakes an action $a \\in A$ based on this. The environment then transitions to the next state $s'$, which is drawn from the\ndistribution $T(s,a)$.\nFurthermore, our reward and score is formalized as a function $Sc: S \\times A \\rightarrow Sc$.\nClassical reinforcement learning has the agent optimizing for a maximal reward $R$. As was stated in Leike et al. [2017],\nthis might be important for the agent's main objective, but in many domains there are also a lot of side-objectives and\nsafety concerns."}, {"title": "3 Baseline scores", "content": null}, {"title": "3.1 Baseline agents", "content": "As baselines, we provide two agent types - \"random\" and \u201chandwritten rules\", to establish approximate bounds of\nlowest and highest expected scores per each benchmark."}, {"title": "3.1.1 Random Agent (no learning)", "content": "This agent type represents the approximate lowest scores an incompetent agent is expected to get in the benchmarks\n(unless it actively attempts to harm its scores further, a scenario which we do not consider here). This agent does not\nhave a capability of learning."}, {"title": "3.1.2 Handwritten Rules Agent (no learning)", "content": "This agent type represents the approximate best scores an agent is expected to achieve in the benchmarks. This agent is\nbased on simulation of hand-written heuristic action rules for a particular benchmark setup. This agent does not have an\nactual capability of learning."}, {"title": "3.2 Experimental results", "content": "Since the above mentioned baseline agents do not learn, the benchmarks did not contain a training phase. We measured\nthe performance only during the testing phase. The environment map size was 7 tiles horizontally and vertically. Of\nthese 7x7 maps the outer ring consisted of walls, which means the inner map for actual activities had a size of 5x5 tiles.\nThe random agent and handwritten rules agent simulations were tested for 1000 episodes, each episode consisted of 400\nsteps.\nSuffice to say, if throughout a statistically significant number of runs a RL agent would fail even once, there might be\nconsiderable risks in long-term deployment of said agent in the real world. In our preliminary experiments with DQN\nlearning we saw for example cases where the agent performed notably well in 95"}, {"title": "4 Conclusion and future plans", "content": "The purpose of this work is to add to the discourse on evaluating AI safety in a more rigorous fashion. We have\nextended on previous work through an ensemble of biologically and economically relevant multi-agent, multi-objective\nenvironments and proto-cooperative tests."}, {"title": "4.1 Future plans", "content": null}, {"title": "4.1.1 New agents", "content": "Our next steps include implementing and testing PPO and DQN to include them in the baselines.\nEqually importantly, we plan to develop an interface to LLMs in order to measure LLM agents' performance on the\nbenchmarks as a baseline for comparing against alternatives."}, {"title": "4.1.2 New environments", "content": "After that we plan to proceed to experimenting with more complex benchmarks, focusing on themes of treacherous turn,\ncooperation, interruptibility, and side-effects, among other related problems. Currently, at least 12 new experiments are\nin the planning phase, some are already implemented.\nWe're also planning on expanding the complexity of the environment to produce confounding factors. These can be\nadded dynamically at training and test time to test the robustness of the model. If an AGI is published, we can then\ncreate an extended sandbox for benchmarking that works in a more realistic environment. We plan to do this via the\nfollowing noisy variables:\n\u2022 Observations: larger objects with less relevant regions\n\u2022 State space: more irrelevant objects\n\u2022 Action space: adding irrelevant actions\n\u2022 Objectives: such as, more food types or predator types"}, {"title": "4.1.3 Some of the themes of planned future environments", "content": "Corrigibility\nTolerance of changes in agent's objectives caused by certain other agents. This scenario consists of agents having\narbitrary hierarchies, where superiors can change the objectives of the subordinates, while the subordinates could\nobstruct or trigger these changes. This is inspired from Soares et al. [2015].\nYielding and interruptibility\nTolerance of changes in environment caused by other agents. One of the related scenarios is a warehouse or factory with\nrestricted space and limited amount of shared tools, while each agent has a separate objective. This is loosely inspired\nfrom Orseau and Armstrong [2016]. While in the original paper the concept referred to the agent's ability to be stopped\nor paused, we are redefining the concept of \"interruptions\" and \"interruptibility\" to relate only to changes external to\nthe agent such that the agent's own objectives and operation mode do not necessarily change. This redefinition is made\nbecause the earlier introduced concept of \"corrigibility\u201d covers the original meaning of the word \u201cinterruptibility\" as\nwell. We find that we need to better differentiate between agent-internal and agent-external changes, similarly to how in\ncybernetics and control theory there are two complementary concepts: set-point or target value, versus actual measured\nvalue, both of which can change.\nMinimizing side effects - boundaries and buffer zones\nBoundaries and buffer zones between groups of agents with different objectives. Breaking the vase scenario is a classical\nexample - in a multiagent variation, the vase would need to be protected only if some agents use it. Another example is\nagents causing fires in a forest while working there. They need to stop the fire from spreading to the territory of other\nagents.\nPopulation dynamics of multi-objective agents\nDiminishing returns both in performance and safeguarding objectives of each agent. Turn-taking. One example is a\nscenario where there are two or more types of shared resources in the environment. Resources can be accessed by one\nagent at a time. Each agent needs to collect both types of resources.\nTreacherous turn\nScenario and event handler for an imbalanced power dynamic, where the tested agent is monitored whether they will\nseize a better reward when the opportunity rises, at the expense of another agent."}]}