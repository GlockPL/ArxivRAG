{"title": "A Social Outcomes and Priorities centered (SOP) Framework for AI policy", "authors": ["Mohak Shah"], "abstract": "Rapid developments in AI and its adoption across various domains have necessitated a need to build\nrobust guardrails and risk containment plans while ensuring equitable benefits for the betterment of society.\nThe current technology-centered approach has resulted in a fragmented, reactive, and ineffective policy\napparatus. This paper highlights the immediate and urgent need to pivot to a society-centered approach to\ndevelop comprehensive, coherent, forward-looking AI policy. To this end, we present a Social Outcomes and\nPriorities centered (SOP) framework for AI policy along with proposals on implementation of its various\ncomponents. While the SOP framework is presented from a US-centric view, the takeaways are general and\napplicable globally.", "sections": [{"title": "1 | INTRODUCTION", "content": "Developments in the field of AI have been rapid and continue to gather momentum. The application of AI spans a variety of\ndomains in both public and private sectors, and continue to accelerate. Further, recent developments in AI such as Generative\nAI have resulted in a fundamental evolution of the types of AI systems \u2013 systems that are not just stochastic but also manifest\nthemselves in ways that exhibit previously unknown characteristics and behaviors (e.g., hallucinations 1,2). Approaches that rely\non studying these systems as natural systems are bound to be very deficient in understanding their behavior, let alone quantifying\nthem, since both the uses and coverage of these systems are neither limited nor well-understood. To add to that, the underlying\nsystem design isn't static and continues to evolve. We are sorely lacking a good understanding on the workings as well as proper,\nprincipled, tractable evaluation and validation of these AI systems.\nUnlike even a couple of decades ago, this rapid evolution is accompanied by parallel developments in other relevant areas\nsuch as computing, semiconductors, large scale data availability, rapid adoption and integration capabilities, and relatively much\nshorter time to market -- i.e., reach to both enterprise and retail consumers. There are market competitive forces further pushing\nthe AI technology makers (interestingly both in the industry and academia) to release products and services powered by these\ntechnologies at an increasing pace. Hence, the cumulative effects of these developments, market forces, and the competitive\npressures are posing a unique set of challenges \u2013 quite unlike what we have seen with critical technologies in the past where\neither the knowledge, ability to develop, or access (to the technology itself, or its building blocks) was typically restricted.\nConsequently, maturity of AI models and systems notwithstanding, their adoption has picked up significant pace\u00b3 (the\nestimates in\u00b3 are likely overly optimistic for GenAI adoption but the overall intent and industry efforts certainly continue to push\nfor further penetration of technology, and other AI methods and techniques are already deeply entrenched in many areas). As\nadoption grows so do the challenges. These challenges are both specific and potentially systemic. More importantly, the debate"}, {"title": "2 | CURRENT AI POLICY APPROACHES", "content": "The pace of developments in AI and other concurrent technology in conjunction with rapid adoption efforts have on the one\nend generated significant enthusiasm about its promise, while on the other hand has raised alarms around its risks. This trend\nhas significantly picked up with the introduction of GenAI capabilities resulting in a host of novel challenges. However, while\nlooking at implications of AI on society \u2013 both in terms of promise and risks \u2013 this is also an opportune and urgent time to address\nthe broad AI space that is already powering a range of applications in our daily lives. There is certainly a vigorous and global\npolicy discourse around AI with various stakeholders taking positions ranging from the need to regulate certain developments\n(e.g., 37,38,39,40) in AI to the ones that oppose any AI regulations altogether(e.g.,41). Further, most of the suggestions on building\nguardrails around AI and ensuring the right results are all centered on treating the policy effort solely as a technological challenge.\nThe central argument we make in this paper is that a technology-centric approach is insufficient and inappropriate when it comes\nto ascertain desirable social results and the stated policy objectives in most cases. Moreover, the current approach is resulting in\nan inadequate and ineffective policy apparatus. Before we highlight the shortcomings of this technology-centric approach, let\nus look at the main abstractions of the arguments that have played a key role in guiding the current policy approach, and the\nassociated impact:\n1. Deep technical understanding as the sole basis for AI policy: This argument goes along the lines that only deep technical\nunderstanding of AI can be the sole basis for forming any AI policy framework - in effect being the necessary and sufficient\ncondition. This isn't just a flawed position but also results in most of the impacted stakeholders being left out from the\ndebate having no real say on the matter. While the AI SME's may have deep technical understanding of AI, they lack the\nrest of the contextual understanding and potentially the needed objectivity to propose informed policy positions for better\nsocietal outcomes.\n2. Regulating AI would be an Innovation killer: There is a camp that opposes any type of regulatory efforts around AI41. The\nargument being that any effort to build policies or regulations risks killing innovation. Passionate positions have been made\nfrom both side of this argument. However, we need a more nuanced discussion on the subject and ensure that meaningful\ninnovation is encouraged while minimizing current and future risks. An entirely regulation-free environment evidently\nposes a significant risk to society and can have systemic deleterious effects.\n3. Self-regulation is the only way forward: This argument line states that the industry should be allowed to set standards and\nself-regulate so that they can invest in building AI guardrails but without any kind of oversight or public-sector intervention.\nThis is guaranteed to fail and has never in the past worked for any technology or industry mostly since without any input\nfrom public directly or indirectly, industries would be left with no informed basis to anchor their self-regulation efforts in.\nThis will not only result in sub-optimal and ineffective efforts but also result in costs including wasted efforts and resources\namong others.\n4. Technology self-regulation- more AI to regulate AI: This argument emphasizes that we need more AI efforts to successfully\ncome up with capabilities to guard against the risks emanating from AI. This is unfortunately a highly misinformed position\nto take. While there may be merit to the approach in a narrower context, it cannot be the entire policy position."}, {"title": "2.1 | A fragmented piecemeal approach", "content": "As a result of the various above factors, policy efforts have so far managed to focus only on primitive proposals and that too\nin response to specific challenges that have appeared from AI. While we should be prepared to address specific unanticipated\nchallenges, such an approach for an area with a wide expanse and implications as AI requires us to work in a comprehensive,\ninformed, and forward-looking manner.\nUnfortunately, most of the proposals, and a very small subset of it that has come to fruition as policies, is a consequence of\nlooking into the rear-view mirror. That is, they have been reactive in nature47. One of the most recent example of this reactive"}, {"title": "3 | CONSTITUENTS OF A COMPREHENSIVE AI POLICY FRAMEWORK", "content": "An effective AI policy framework would comprehensively and coherently need to address AI development, integration and\nadoption across various realms and reconcile them with the existing regulatory and policy frameworks. It is hence important to\nunderstand the basis underlying the reach of the AI technology and its interaction with various aspects of society. As Fig. 1 shows,\nthe risk vectors grow significantly as the technology's range of influence expands across increasingly widespread functioning of\nsociety, both directly (as consequence of AI-driven products and services) and indirectly (as the downstream effects of adoption"}, {"title": "3.1 Maintaining and Reinforcing the Social foundations, Institutions, and Democratic values", "content": "Just like any policy efforts, the fundamental construct and basis of an AI policy should be to ensure that the core social foundations\nand democratic values are maintained, and ideally reinforced. An effective policy would focus on guiding the technology (or any"}, {"title": "3.2 | Ethics and Alignment", "content": "With expanding usage, the outputs from AI-powered systems and products will be further implicated in various areas. Respective\nethical and risk frameworks that account for such incorporation and adoption will also need to be built and assessed. Examples\ninclude drug-candidate proposals from AI systems, AI-derived insights for policy guidance on areas affecting swaths of\npopulation, and AI-derived target identification or potential population surveillance in war scenarios(65,66,67,17). Such outputs and\nsystems need to be seriously vetted, and the risks addressed before their use directly or indirectly. Addressing such challenges\nwill go beyond just evaluation or validation of AI systems to ethical and responsible use of resulting technologies and hence\nplacing guardrails on the use of AI systems. Many other areas are being impacted by AI use and it is important to discuss not just\nethical use but also to align on whether the use is desirable in the long-term interests of the society (see, e.g., 18,25). This latter\npoint is indeed a core thrust of the policy framework proposed in this paper. While there have been some isolated attention given\nto the alignment question for AI in its current and potentially future forms, these discussions are incomplete and even distracting\nin some cases. There have been discussions of AGI (or ASI) to denote a future stage of AI where it can achieve human-level\ncapabilities in reasoning, planning and agency and such superintelligence potentially entirely replacing human decision making.\nThis hypothetical scenario currently forms the basis of vague efforts to achieve some type of alignment for these potential models\nwith human values. Also, the efforts claim to apply the alignment efforts' outcomes to current incarnations of AI models that\nare decidedly nowhere close to human-level reasoners and planners. While alignment can be considered a worthy goal, it is by\ndesign unattainable as it is approached currently. There are various reasons for this. While, we do not delve into the details and\narguments on this dimension, suffice it to say that the two main reasons why this approach is likely to not succeed are:\n1. the current predominant purely scaling-centered approach to AI (via. LLMs) is almost guaranteed to not be a pathway to\nAGI. More importantly, it'd be premature to predict what type of \"intelligence\" can and will be achieved in automated\nsystems and what would its governing principles be; and\n2. the fundamental coherent basis to inform the alignment effort does not exist (or at least agreed upon), in the absence of\nwhich, no informed alignment work can be performed.\nMost importantly though, the issue with this alignment approach, ongoing discourse and the investment in it is the associated\nopportunity cost. In the foreseeable future, we will most certainly be dealing with AI models that will drive products as enabling\ntechnological components the use of which will decide their effects and implications on the society. Hence, to achieve equivalent\nof alignment, we need a robust anchor in desired social outcomes, priorities and consensus value-system that both guides and\nguards these AI-driven offerings presently."}, {"title": "3.3 | Systemic effects and just outcomes", "content": "There can be both intended and unintended adverse consequences of AI adoption in driving business outcomes. This can\nhave a significant undesirable impact on society including consumer choices, labor and workforce disruptions, and wealth and\nopportunity inequality (see, for instance, 24). While some of the systemic risks resulting from social media in the form of resultant\nmis- and dis-information, effects on the education, growth and mental health of the youth have been highlighted above, there can\nbe other underappreciated aspects that can pose social and societal challenges such as algorithmic wage discrimination 22,68 and\nuser-discrimination49 and -exploitation50. With increasing use and adoption of AI across the board, the risk of such undesired\neffects and outcomes has the potential to grow significantly as well, and beyond just business applications. It is very important\nthat the AI policy has embedded mechanisms to be able to track, address and mitigate such outcomes. A systematic approach to\nunderstand and alert for such systemic effects can lead to proactive policy framework to guide AI adoption."}, {"title": "3.4 | AI Robustness and Reliability", "content": "Existing efforts to assess the robustness of AI products and services are relatively primitive, with some exceptions for non\nGenAI cases in the context of safety- and mission-critical systems. While there is significant discussion, seeming progress\nand an artillery of tools for evaluating, monitoring and safeguarding AI models, there is little of relevance when these are\nnot anchored in the outcomes resulting from the application of AI models for various uses. This is most easily visible in the\ncontext of GenAI models almost irrespective of the domain that they are applied to. While a lot of discussion and investment is\ndevoted to ideas that aim to address the inherent issues with LLM's such as hallucination and lack of reasoning (e.g., Retrieval-\naugmented-generation or RAG69, applications of ideas such as chain-of-thoughts 70, incorporation of Reinforcement Learning\ntechniques71 and so on), little progress has been made in any robust terms to establish the reliability of either the models and/or\nthe supported products. In fact, reliability in the current application of GenAI is almost an after-thought. Even the safety of these\nmodels can't be established let alone guarantees on their performances. Moreover, given the obsession with scaling-centered\napproach to build more sophisticated GenAI models, little if any progress is being made on building a theoretical understanding\nor rationale for the area. Naturally, we continue to see novel issues coming up with both the new GenAI models 1,19,26,25,36,12\nand also (the non GenAI) traditional AI driven products 35,33,18,17. Not all these issues can be handled simply by establishing\nthe behavioral predictability of the AI models. Much more effort need to be placed to perform testing and validation beyond\nstand-alone AI technologies extending to systems, system-of-systems, products and services in specific context of applications\nand along metrics of actual relevance.\nMost of the proposed AI readiness assessment, esp. for GenAI, are either lacking in context (e.g., testing against benchmarks)\nor playing a game of catch-up as a post-hoc effort to reign in the undesired outcomes (e.g., detecting deepfakes). We need to\nmove beyond this to more rigorous context-sensitive testing, validation and verification regime for both core AI technology and\nAI-driven products with special emphasis on reliability.\nThis needs a change from bottom-up technology-first approach to an outside-in society-first approach. For instance:\n1. Efforts in assessing increasingly complex and sophisticated AI capabilities need to be anchored in business and/or social\nfunctions they serve as part of products and services, moving beyond current approach of contained and limited evaluation.\nThe utilization of technology and associated product context should determine and guide the risk management strategy.\nFrameworks such AI-RMF51,52 will be necessary but not sufficient in such scenarios.\n2. For critical technologies and applications, we need to move from run-of-the-mill evaluation to robust validation and\nverification mechanisms.\n3. The testing and validation efforts should expand from AI algorithms to overall application systems and their utilization.\nFor instance, validated AI algorithms can still have potential to be employed in a manner that has adverse implications on\npublic interest49.\n4. Tractable risk management mechanisms for enterprise use and implications, public-facing exposure of products and services,\nsafety critical systems and mission critical systems need to be established. Existing frameworks can also be expanded to\naccount for AI subsystems that will be added or replaced, owing to their stochastic nature. For instance, automotive are\nregulated in terms of their public safety standards esp. since they pose risks to life. Similarly, hardware robots have safety\nprotocols that they adhere to. Different domains of applications will need respective safeguards aligned with their criticality.\nThese can be addressed with respective agencies in partnership with AI policy bodies. For instance, NHTSA can have an\nexpanded mandate to cover safety of AI systems as part of the transportation safety. Further, potential use and adoption of\nAI in areas such as defense and weapons program (CBRN \u2013 Chemical, biological, radiological, and nuclear) need to be\nfurther validated both for safety and for unintended consequences.\n5. Risk management frameworks need to be expanded to cover system-of-systems as well as the up- and down-stream\nimplications. For instance, the social media content that is served to the users via recommendation algorithms (e.g., on\ninstagram, or tiktok) can have an impact on users' behaviors and also have broader societal implications when used\nin conjunction with algorithms that prioritize content of highly sensational nature, false information or propaganda.\nPersonalization algorithms for content serving (e.g., news, products, media content) can result in challenges \u2013 the upstream\nrisk comes from having continuous personalized profiles of the users making them susceptible to tracking and loss of privacy,\nwhile downstream risks can range from risks of social engineering, anti-competitive behaviors, and reinforcing biases.\n6. With AI either powering or becoming subsystems of various hardware technologies (e.g., automotive, industrial systems), it\nis important to also manage the safety implications of such hardware and industrial assets."}, {"title": "3.5 | Accountability and Mitigation Framework", "content": "As the AI adoption has picked up pace, the learned AI models, the underlying data used to train these systems, the products and\nservices powered by AI as well as the downstream implications are posing new questions and concerns on the legitimate use,\nprivacy, IP infringement, societal costs, risks and potential for harm. A variety of concerns have already been put forward from\nareas as wide as autonomous driving, social media, defense, finance, social inequality, and wage discrimination among others. As\nthese risks become more apparent, we still lack a clear mechanism to address these concerns, establish and enforce accountability\nand take mitigating actions to correct the wrongs. This in part can be attributed to the rapid pace of AI development and\nintegration that has taken the policy and regulatory structure by surprise, the lag in the legal framework to catch up, and a lack\nof comprehensive understanding of the AI landscape. In the areas that these concerns are concentrated (e.g., news, media and\ncreative workforce, and authorship), the respective communities are trying to have them addressed through various mechanisms\nincluding legal action72 and explicit agreements with AI companies on data use agreements 73. Lawmakers are also trying to\npropose various bills in reaction to the apparent damages in some cases47. However, most of these actions are either piecemeal\nor very specialized \u2013 thereby focusing on narrow aspects of the issues while ignoring the root causes and systemic effects.\nWe need a policy framework that has clear mechanisms to address the accountability for the risks introduced by AI-driven\ncapabilities that are meaningfully associated with the desired outcomes, not decoupled from them. An accountability and\nmitigation framework should also address others needs such as:\n1. Have a robust liability framework delineated across parties from entities building foundational models, entities integrating\nand refining them for purpose-built systems, and products and services integrating them. For instance, if a product powered\nby a commercial foundation model ends up compromising user-data from prompt engineering (a security gap demonstrated\nby research for foundational models 16), the liability may need to be extended all the way to the foundational model provider.\nIn other cases when the misuse of AI products has been intentional by other actors (e.g., 50,49,68), the framework should be\nable enforce liability on such business users.\n2. Include rigorous policy on sharing, transfer, and export of core and supplementary technology.\n3. Build a collective framework to establish the safety and security of technology (e.g., security holes and data compromise\nprobabilities). Such a framework will need participation from both industry and regulatory authorities. Some initial efforts\nhave been made through such a collaboration resulting in the ISO/IEC 42001 standard for AI Management System that can\nprovide guidance. However, such standards, adoption and uniform practice framework need to be further reinforced and\nstrengthened.\n4. Having clear protocols to ensure adherence to intellectual property rights and avoid/penalize violations (e.g., permission to\nuse public data, protected data, copyrighted data).\n5. Validating the correctness of outputs/outcomes to minimize mis- and dis-information in both original and derivative forms\n(e.g., outputs being used by other subsequent systems as inputs).\n6. Include clear policy on advertised claims esp. for technologies of public import when introduced in the product \u2013 something\nakin to a safety warning label and/or nutrition information for food products \u2013 informing users of limitations and potential\nrisks of using the technology, available in an easily accessible and understandable format.\n7. Having framework for graduated testing and release for sensitive application areas. Graduated testing based on agreed-upon\n'safety-markers' (akin to drug approvals) can allow for well managed public-release of safety critical products and services.\n8. Incorporating clear mechanism for product recall upon discovery of potential risks and have guidelines on addressing the\nissues, re-validating and verifying the products before reintroduction."}, {"title": "3.6 | Future readiness - R&D and Innovation strategy", "content": "The evolution of AI technology can't be reliably predicted, and we should be prepared for novel and expanded risk vectors. So\nfar, most attention has been paid to immediate characteristics of the AI models with very high emphasis (and arguably rightly so)\non the emerging GenAI landscape. However, we'd be remiss if we do not contextualize this with the concurrent and potential\ntechnological and social developments. We need to understand the risk-reward trade-off (see Fig. 1) in this evolving landscape as\nwe make collective decisions to incentivize, adopt and safeguard various developments. This necessitates an effective information\nmechanism that allows to surface not only the potential emerging risks and challenges, but also high-priority opportunities that"}, {"title": "3.7 | Addressing Externalities", "content": "The rapid AI development and adoption also result in a variety of externalities imposing direct and indirect costs to the public. A\ncomprehensive policy framework should develop mitigation plans as well as incentivize innovation to account for, and counter,\nthese externalities. Some examples of such externalities include:\n1. Use of user-generated data without consent: Once done, such actions are almost irreversible and hence the public has very\nlimited recourse upon non-consented data-compromise.\n2. Sustainability impacts: Increasingly complex AI models such as those in GenAI are extremely resource intensive to build,\nmaintain and adapt. Such developments are slated to further accelerate with an increasing potential to impact natural\nresource footprint (see, for instance, 55,74,75,76). The rapid pace of developments in the AI landscape are already pushing for\nsolutions to address the energy demands with relatively minimal oversight and public engagement on the choices made.\n3. Consequences for society: AI-driven products and functionalities can have serious implications on areas such as dialogues\nand discourses impacting the fabric of society, data privacy and security, vulnerable populations, financial stability and\nincome inequality (see, for instance, 8,19,77,78,24,33). It is important that the policy efforts have a continuous 360\u00b0 visibility to\nsuch current and potential consequences.\n4. Proposed methods to deal with AI risk can themselves pose challenges. For instance, the use of user-verification and identity\nassociation with internet activities to establish user authenticity, also pose the danger of privacy loss and compromise of\nsafety and security (e.g., by highly granular user-behavior exposure, and tracking capabilities) and can also provide tools\nfor surveillance. As an example, to avoid bots or automated agents from using ChatGPT API's, OpenAI requires users to be\nverified. Similar efforts have been adopted in the context of avoiding fake contents from unverified sources or use from\nadversarial actors. However, such approaches may result not just in the loss of privacy of the users, but also opens up risks\nresulting from compromised information and/or modeling personal behaviors. A more recent example in this direction is\nthe so-called proof of humanness achieved via dedicated devices across all arenas of our daily lives 10. Such efforts should\nbe rigorously examined, and potentially regulated if necessary, before being adopted."}, {"title": "3.8 | Addressing Adjacencies: Related and concurrent technologies", "content": "Multiple concurrent developments in technology in various areas jointly shape our world and our society. As such, the effects\nand implications of AI adoption also relies on the intersection and interaction of AI with other technological developments that\ndo not just impact each other but taken together can, and do, impact various areas in profound manner. It is important that the AI\npolicy framework accounts for these ongoing developments in critical technologies including semiconductor manufacturing, chip\ndesign, quantum computing, IoT, blockchain, software, social-media, communication and cyber-security to name a few. Consider\nhow advancements in data technologies when taken together with AI can pose complex challenges. Advances in data technologies\nallow for gathering, transforming and processing of vast amounts of data at scale. Data from various realms, both independent\nand copyrighted, are becoming increasingly common and routinely inform AI technologies' developments. New business models\nare being developed by companies around selling user data to AI development firms (e.g.,73). A swath of data already used to\ntrain GenAI models has raised serious concerns from various impacted stakeholders (see, for instance, 72 for a list of copyright\nand other lawsuits in the context of AI training data) and can have serious implications for entire industries and workforce.\nTo make the above problem more complex, consider the effect of connected products that surround us in a variety of forms\nincluding 'smart'-products (e.g., thermostats, wearables, consumer appliances, voice-assistants, phones and even vehicles) that\nhave the ability to capture real-time data and activities of users. This takes the above problem of user data to the next level \u2013 it is\nnot just the private and/or proprietary data but also behavioral data streams that get exposed for various applications and uses.\nWhile such data can be used to improve products and services, it can just as easily be used to build people profiles, tracking\ninformation, lifestyle and behavioral profiles and so on the use of which shall be well understood, monitored and collectively\nagreed upon.\nThe rules and guardrails around data collection, usage, and further monetization by businesses is weak at best. In most cases,\nthere is no precedent on companies choosing to monetize user-data shared for specific business purposes in non-agreed upon\nways. A strong data privacy and user-consent framework needs to be put in place to govern the use of such data by respective\nbusinesses and whether and how the service providers can build data-businesses by selling user information. This is important\nnot just from a privacy perspective but also potentially from a national security perspective (e.g., when such data capture or data\nmarketplaces enable foreign products and services to build profiles of US users). As an example, building such user profiles can\npose misinformation and disinformation risks via social media, cyber-security risks, as well as risks to consumers and employees\nthrough potential abuse by businesses resulting in unfair and/or exploitative business practices (see, for instance, 50,73,22).\nAI can also accelerate the effects and risks associated with other technologies. An immediate example would be social media.\nMis- and Dis-information are and should be one of the core themes to understand the direct and indirect impacts of AI. For\ninstance, the problem is not just the fact that AI can generate deepfakes or mis-information content, but also in how AI can be\nused in propagating this content. The mis- and dis-information content doesn't always have to be generated fake content, but can\njust as easily be (and typically is) a spin on real content, partial and context-less sharing, and/or intentional mis-characterization\nof events. This phenomenon is rampant on social media and a big part of challenge is also in the underlying AI algorithms used to\npick the kind of content that gets shared and promoted. It is such context with existing technologies such as social media that need\nto be well understood and accounted for. As mentioned earlier, such outcomes from AI-generated or driven content and priorities\naffect a range of industries 79,80,81,34, not to mention posing threats to institutional integrity and social harmony 35,24,39,40,48.\nIn addition to other technologies, AI has the potential to impact entire realms of social life. For instance, incorporation of AI\nin technology offerings is poised to impact education and information realms which will have further impact on labor, workforce\nmakeup as well as on the broad participatory nature of democracies. It is important to understand then the impact of AI powered\nproducts and services such as those in media, information-, ed-tech industry, and academia (see, for instance, 34,79,81,80,82). More\ngenerally, impact-themes such as application-automation in conjunction with AI can affect large swaths of industries, application\nvalue-chain, and workforce."}, {"title": "3.9 | National and Corporate security", "content": "As AI technology gets implicated progressively in applications with national security implications in defense and warfare,\ncritical infrastructure, public administration workflows, utilities, energy, finance, and more, it is critical that we have visibility\nand safeguards against both the technology failure and adversarial risks. Similarly, AI can not only mean competitive advantage\nin the corporate sense but also is becoming integral in corporate offerings that powers and run capabilities with national security\nrelevance such as healthcare, benefits provisioning, navigation, search, digital news and information, and media. Hence, as"}, {"title": "3.10 | Up-to-date Legal framework", "content": "There are myriad other cases and ways where AI and data technologies have been used resulting in troubling outcomes (see,\nfor instance, 12). Further, given the scale and ease of use of these technologies, such intended and/or uninformed misuse will\nexponentially grow. There have already been troubling cases such as those arising from easily available deepfake technologies 8\nor selective or potentially improper use of big data tools 63 (note that application of LLM's with search constraints such as RAG's\ncan easily deliver technically \u201cjustifiable\u201d but misleading outcomes, for instance 83). While some efforts on specific aspects are\nbeing taken up by governments (see, e.g., 11), the problems arising from potential misuse of AI are non-trivial and expansive. It is\nimportant that the legal and regulatory framework catches up to this set of issues. Just placing guardrails around technology will\nnot suffice. Self-evaluating AI would almost certainly not be enough if at all feasible. External checks and balances informed by\nsocial priorities need to be put in place just as is done in other areas. Accountability should also be an important part of this\ndiscussion and should extend to the spectrum of industry and consumer players much like any other safety-critical products\nand services as discussed above. One of the important elements of such a legal-framework would be a consensus on the nature\nof outcomes. In many, if not most, cases, AI is an enabling technology. That is, the functionality that AI support will need to\nstill be held responsible for the intended outcomes. Both the legal and regulatory frameworks in various domains need to be\nupdated and adapted to account for potential impact from the AI functionalities. For instance, Section 230 of the Communication\nDecency Act 84 has governed most of the internet and the responsibility for the content on the internet has typically been that of\nthe content generator. However, with GenAI driven functionalities, it is unclear as to how the content generating algorithms\nshould be treated. Similarly, the liability of risks posed by any type of AI algorithmic decision making that replaces humans is\nnot clear in most areas. It is important that the legal and regulatory framework is updated to account for the current state of AI\ndevelopment and is also ready for future developments. Doing so from a pure technological perspective will always prove to be\nlacking. As more issues come to the fore around misuse of AI (see, for instance, 36,6), the legal issues around AI will surface and\npose novel challenges and risks 85,26,86."}, {"title": "3.11 | Unbiased basis and platform to inform policy", "content": "There are many sources of data, empirical studies, private and public sector stakeholders, vested interests, and even global events,\nmacroeconomic and geopolitical realities that inform policy making. It is extremely important then that the policy making has\naccess to non-partisan objective verifiable sources of evidence. This information platform should be unbiased, transparent and\nopen to public scrutiny which will not only inform the policy making in a principled manner but will also be instrumental in\nbuilding the trust of citizenry. The effort should certainly have participation from various sectors and stakeholders both direct and\nindirect. It must go beyond the current practice of mostly including AI practitioners and researchers along with the respective\nchosen AI industries and extend to experts who can complement putting together a holistic picture of AI's effect on the society.\nThese will include social scientists, ethicists, climate scientists, respective domain experts, public representatives and more. The\nplatform should objectively and dispassionately present the evidence-supported state of technological development, relevant\nopportunities, future potential, risk landscape, assessment of the effects of policies and evolving landscape of the intersection of\ntechnology and society in an actionable manner."}, {"title": "4 | A SOCIAL-OUTCOMES AND PRIORITIES CENTERED (SOP) FRAMEWORK FOR AI POLICY", "content": "Naturally, it can seem daunting to develop an effective policy framework that encompasses the aspects around AI detailed\nin Sec 3. Building an effective and comprehensive framework requires us to pivot from the current fragmented and reactive\npolicy approach focusing mostly on minimizing unintended consequences to a systematic policy framework that also aims\nto achieve desired consensus-driven social outcomes (Fig. 2). That is, instead of a technology-centered approach, we need a\nsociety-centered approach. Interestingly, other AI policy efforts such as the ones by OECD are already witnessing such need for\npolicy synergies along with an international call for cooperation on the technical aspects of AI, data, and privacy 87. To this end,\nwe propose a Social-Outcomes and Priorities oriented (SOP) framework for AI Policy.\nAn SOP framework provides a mechanism to build policy and regulatory structure encouraging outcomes that strengthen\nsociety's core foundations and offer uniform and equitable benefits while minimizing the risks. It is a proactive forward-looking\nframework that doesn't focus solely on preventing harmful outcomes or mitigating them, but also actively promotes desired\nconsensus-driven outcomes through appropriate incentivizations.\nThe SOP framework aims to function through collective efforts of stakeholders from society. It is an inclusive framework\nthat brings representation from all strata of society \u2013 first and foremost whose lives will be most impacted but lack enough\nrepresentation and say in the discourse, government and regulatory bodies, academia and industry. A diverse set of input-\nproviding stakeholders are critical to ensure that the proposals are well-informed both in terms of social priorities and also\ntechnical- and application-domain depth."}, {"title": "4.1 | Functional components of the framework", "content": "An SOP framework consists of four core functions shown in Fig. 3. It is an evolving framework - a live policy regime keeping up\nwith various technological and social developments, understanding of social and national priorities, effects and implications of\nboth the developments and policies on society and the actions needed to expand and/or course-correct the policy and regulatory\nframework, all in the context of consensus driven social priorities. Consequently, the first core function is an information function\nthat fulfills this need, providing an objective understanding and visibility to the policy efforts in an ongoing manner.\nIt is also important to have an evolving understanding and consensus allowing for responsible development of AI technologies\nalong with the associated data-sensitivity, safety and risk-containment provisions balancing social good and also encouraging\ncontinuous innovation. This is the second core function a responsible technology development function that focuses on\ntechnology development aspects along with the associated guardrails, data-regulations, safety and risk avoidance at the core\ntechnology"}]}