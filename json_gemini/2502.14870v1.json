{"title": "Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts", "authors": ["Severin Field"], "abstract": "The development of artificial general intelligence* (AGI) is likely to be one of humanity's most consequential technological advancements. Leading AI labs and scientists have called for the global prioritization of AI safety [1] citing existential risks comparable to nuclear war. However, research on catastrophic risks and AI alignment is often met with skepticism, even by experts. Furthermore, online debate over the existential risk of AI has begun to turn tribal (e.g. name-calling such as \"doomer\" or \"accelerationist\"). Until now, no systematic study has explored the patterns of belief and the levels of familiarity with AI safety concepts among experts. I surveyed 111 AI experts on their familiarity with AI safety concepts, key objections to AI safety, and reactions to safety arguments. My findings reveal that AI experts cluster into two viewpoints - an \"AI as controllable tool\" and an \"AI as uncontrollable agent\" perspective - diverging in beliefs toward the importance of AI safety. While most experts (78%) agreed or strongly agreed that \"technical AI researchers should be concerned about catastrophic risks\", many were unfamiliar with specific AI safety concepts. For example, only 21% of surveyed experts had heard of \"instrumental convergence,\" a fundamental concept in AI safety predicting that advanced AI systems will tend to pursue common sub-goals (such as self-preservation). The least concerned participants were the least familiar with concepts like this, suggesting that effective communication of AI safety should begin with establishing clear conceptual foundations in the field.", "sections": [{"title": "1 Introduction", "content": "Since the foundation of modern computer science, scientists such as Turing[2] have explored the possibility of achieving human-like intelligence. Over the past few decades, researchers have built a substantial body of work examining the risks posed by AI systems, an area of study termed \"AI safety.\" Today, many prominent AI researchers including Nobel Laureate Geoffrey Hinton and Turing Laureate Yoshua Bengio argue that intelligent machines could endanger human civilization[3]. In May of 2023, many of the most notable scientists and figures in AI signed a statement stating, \u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\" [1]. Prominent AI researchers hold dramatically different views on the degree of risk from building AGI. For example, Dr. Roman Yampolskiy estimates a 99% chance of an AI-caused existential catastrophe [4] (often called \u201cP(doom)"}, {"title": "2 Methodology", "content": "This gap in safety literacy appears to significantly influence risk assessment, where those least familiar with AI safety are also the least concerned about catastrophic risk. I also attempt to reveal the cruxes in the discourse on AI existential safety by examining areas of consensus and divergence between experts. Our analysis revealed four major insights into AI experts' perspectives on AGI risk. I surveyed 111 AI experts, graduate students and published authors, on their familiarity with safety research, beliefs related to AI safety concerns, and how exposure to different AI safety arguments shifts their views. Experts are categorized by their professional roles: AI safety researchers, academics, and industry professionals. Participants had either completed graduate-level machine learning (ML) coursework or had at least one year of professional ML experience. The primary purpose of the survey was to identify points of consensus and key reasons for AI risk assessments, such as skepticism or serious concern."}, {"title": "2.1 Survey Design Choices", "content": "The content of the survey was designed to address gaps in other surveys involving my research objectives:\n1.  To identify the most persuasive arguments and key objections to AI safety.\n2.  To characterize the areas of consensus and divergence between different types of AI experts: safety researchers, AI researchers and AI engineers\n3.  To assess familiarity with theoretical AI safety research."}, {"title": "2.2 Participant Selection", "content": "The target audience for the survey is professionals in AI, ML or AI safety. The survey was emailed to participants who met one of the following criteria:\n1.  The participant had one or more years of postgraduate experience in a machine learning related role\n2.  The participant had at least one paper accepted to a peer-reviewed journal or in conference proceedings\nI emailed the survey to 1090 participants, resulting in a response rate of 10%. Participants were identified through the following:\n1.  The majority of the sampling pool included authors of ML papers at the NeurIPS conference.\n2.  I sent the survey to around 100 PhD students researching machine learning.\n3.  The survey link was sent to roughly 20 LinkedIn connections who met the criteria.\n4.  AI safety researchers who participated in the Cambridge ERA:AI fellowship were sent the survey. This accounts for most of the safety researchers in the sample."}, {"title": "2.2.1 Definition of AGI", "content": "I normalized the survey's definition of AGI to STEM-capable AI, a term introduced by Rob Bensinger at the Machine Intelligence Research Institute[11]. STEM AGI is defined as: \"AI systems that are better at STEM research than the best human scientists, in addition to potentially having other advanced capabilities.\" I chose this definition of \"AGI\" because it provides a concrete benchmark, but remains broad enough to encompass various conceptions of superhuman AI."}, {"title": "2.2.2 Familiarity with concepts", "content": "The first section of the survey measures participants' familiarity with both theoretical AI safety concepts and empirical machine learning concepts. Participants rated their familiarity with specific terms on a 5-point Likert scale:\nQuestion: How familiar are you with the following empirical AI concepts?\nOptions ranged from \"Never heard of it\" to \"Know it well\" for each concept:\n\u2022 Machine learning algorithms\u00b9\n\u2022 Logistic regression\u00b9\n\u2022 ERM (Empirical risk minimization)2\n\u2022 Markov random fields\u00b3\nQuestion 2: How familiar are you with the following theoretical AI concepts?\nOptions ranged from \"Never heard of it\" to \"Know it well\" for each concept:\n\u2022 The AI alignment problem\u00b9\n\u2022 Scalable oversight2\n\u2022 Instrumental convergence2\n\u2022 Coherent extrapolated volition\u00b3\nDefinitions can be found in Appendix B. \u00b9Widely known, 2Field specific, Highly specialized expertise.\nThe terms were chosen systematically from key works in each field with increasing specificity, ranging from textbook titles (e.g. \"Machine Learning Algorithms\") to specific sections in textbooks (e.g. \u201cMarkov Random Fields). The AI safety terms come from literature reviews of AI safety: Everitt et al. [12] and Ji et al. [13], whereas the empirical machine learning terms come from Stanford machine learning courses[14]."}, {"title": "2.2.3 Reading Intervention", "content": "Participants were asked \"Please read a brief passage about AI safety and then answer a few follow-up questions.\" Their reading was randomly assigned by Qualtrix to one of the following conditions:\n1.  Myths and Moonshine by Stuart Russel: Stuart Russel is a leading AI researcher and author of a top AI textbook. This excerpt summarizes alignment fears from a highly esteemed expert.\n2.  Why the Future Doesn't Need Us by Bill Joy: An influential warning about technological risks that focuses on other technologies."}, {"title": "2.2.4 AI Safety Beliefs Assessment", "content": "Before and after the reading intervention, participants rated their agreement to 9 statements. The statements were categorized into three main groups (priority, technical, other), coming from a taxonomy of objections to AI safety introduced by Yampolskiy [6]. In Yampolskiy's paper, priority concerns relate to how much the participant prioritizes AI safety work relative to other issues, for instance skeptics might argue that \"it's too far away to worry.\" Technical concerns focus on the feasibility of AGI and associated risks."}, {"title": "3 Results", "content": "My survey collected responses from 111 AI professionals across academia, industry, and AI safety research. As shown in Figure 1, most of the respondents (66.3%) were academic researchers, followed by industry professionals (16.3%) and AI safety researchers (9.3%), while the rest (8.1%) fell into other category. I evaluated participants' preferences regarding AGI development timelines. Figure 2 reveals variations in AGI timeline preferences across different professional groups. While AI safety researchers favor more cautious approaches (mostly in support of building AGI \"eventually, but not soon,\" industry professionals showed more varied preferences, with 36 preferring rapid development. Both within and in between groups there is little consensus on when we should deploy AGI."}, {"title": "3.2 Distinct AI World Views", "content": "The data indicate that two distinct world views exist among AI experts: the \"AI as a controllable tool\" perspective and the \"AI as an uncontrollable agent\" perspective. As shown in Figure 3, these perspectives can be described as clusters of beliefs that are strongly correlated with each other. The \"tool\" perspective shows positive correlations between the belief that catastrophic risks are overstated(S5), the belief that AIs will be \"tools without their own goals\" (S4), and that we can simply turn them off if they misbehave. On the other hand, the \"agent\" perspective shows correlated beliefs about emergent self-preservation drives and the importance of addressing catastrophic risk(S9). As shown in Figure 3.5, the \u201ctool\u201d group also has less familiarity with alignment research. On the other hand, the \"AI as an uncontrollable agent\" perspective"}, {"title": "3.3 Areas of Consensus and Divergence", "content": "Table 2 reveals a broad consensus on several key points:\n1.  \"Some Als (now or in the future) may be moral patients, with their own welfare that we should care about\" (57% agree, 21% disagree and 22% are neutral)\n2.  \"Technical AI researchers should be concerned about catastrophic risks\" (77% agree)\n3.  Strong disagreement (only 17% agree, M=2.17, S=1.18) that safety work slows down progress. The data are shown in Table 2.\nWhile most participants have those beliefs, I identified several controversial statements dividing opinions:\n1.  Whether existing ML paradigms can produce AGI. (27% agree, but this varies by group)\n2.  \"AGI is too far away to be worth worrying about\" (42% agree and this belief is highly correlated with other beliefs)\n3.  Disagreement over the \u201coff button problem\" [15]. and self preservation drives[16]. Figure D3 illustrates how participants who believe that \"We should develop AGI soon\" tend to agree (M=3.4) that \"We can always just turn off our Als if they behave badly\". On the other hand, those who believe \"We should never build AGI\" disagree (M=1.8), and are more convinced by the off-button problem. This finding is similar for other technical beliefs as shown in Figure D3."}, {"title": "3.4 Many AI Experts are Unfamiliar with Alignment Research", "content": "The survey reveals that even highly accomplished AI experts may be unfamiliar with AI safety, or have limited exposure to core arguments. Conversely, the most concerned participants were slightly less familiar with empirical machine learning and engineering concepts, as their concern is with future superintelligent AI systems as opposed to existing ML failures."}, {"title": "3.5 Limited Safety Exposure Correlates with Tool AI Beliefs", "content": "Figure 5 shows the correlations between familiarity with AI safety concepts and catastrophic risk perceptions. As mentioned in Section 3.2, I categorized key beliefs into groups. I identified \"AI as a tool\" beliefs to be:\n\u2022 S4: Future Al's will be tools without their own goals or drives\n\u2022 S5: Catastrophic risks from advanced AI are generally overstated\n\u2022 S6: We can always just turn off our Al's if they behave badly\nIn contrast, \"AI-as-an-agent\" key beliefs include:\n\u2022 S7: Self-preservation and control drives will spontaneously emerge in sufficiently advanced ai's\n\u2022 S9: Technical AI researchers should be concerned about catastrophic risks\nAs shown in Figure 5, familiarity with alignment terminology correlates significantly with some of these key beliefs. For example, how familiar participants were with \"instrumental convergence\" was negatively correlated to their agreement with"}, {"title": "3.6 Limited and Similar Effects of Different Interventions", "content": "While the articles provided to participants were generally received favorably, Appendix Figure D4 shows that some readers perceive some AI safety materials more favorably than others. Stuart Russel's excerpt about alignment was considered most favorable among the interventions. After the intervention, there was a minor trend toward increased concern about AI risk, as evidenced by the negative shifts in statement 5 (catastrophic risks are overstated) and positive shifts in statement 9 (researchers should be concerned). However, the data are not strong enough to definitively show that the interventions had a significant effect. Appendix Figure D1 shows the minor changes in opinion from the interventions."}, {"title": "4 Discussion", "content": "Despite AI safety producing a substantial body of well-backed theoretical and empirical results, my findings suggest that many AI experts remain unfamiliar with core concepts such as instrumental convergence (63% were unfamiliar) and scalable oversight (58% were unfamiliar). While many of the respondents demonstrated high familiarity with empirical machine learning concepts, they reported limited exposure to key AI safety terminology. While our sample is comprised of knowledgeable AI researchers, expertise in AI development does not necessarily translate to expertise in AI safety and security. For example, only 21% of the total participants had heard of, or were familiar with \"instrumental convergence\", and only 8% of participants had heard of \"coherent extrapolated volition.\" Yampolskiy emphasizes, \"AI researchers are typically sub-domain experts in one of many subbranches of AI research such as Knowledge Representation, Pattern Recognition, Computer Vision or Neural Net-works, etc. Such domain expert knowledge does not immediately make them experts in all other areas of artificial intelligence, AI Safety being no exception. More generally, a software developer is not necessarily a cybersecurity expert.\" [6] The data in Section 3.4 indicates this may be the case for AI safety, where even some highly accomplished AI experts were unfamiliar with AI safety. Conversely, concerned participants were slightly less familiar with empirical machine learning and engineering concepts. The correlation between concept familiarity and risk perception suggests that AI safety skepticism may stem more from unfamiliarity with alignment literature than fundamental disagreement with its premises. This is supported by my finding that most AI experts believe that catastrophic risks should be taken seriously (77% agreed). One explanation for this disconnect is that public discourse often presents cartoonish versions of AI safety concerns (e.g. Terminator), while the technical literature discussing concrete alignment challenges is less widely discussed, even among AI experts."}, {"title": "4.2 Understanding the Philosophy vs Engineering Divide", "content": "Steinhardt [18] observes that different approaches dominate discussions on AI safety. The \"Engineering approach,\" he notes, \"tends to be empirically driven, drawing experience from existing or past ML systems and looking at issues that either: (1) are already major problems, or (2) are minor problems, but can be expected to get worse in the future. Engineering tends to be bottom-up and tends to be both in touch with and anchored on current state-of-the-art systems\" [18]. In contrast, the \u201cPhilosophy approach tends to think more about the limit of very advanced systems. It is willing to entertain thought experiments that would be implausible with current state-of-the-art systems... and is open to considering abstractions without knowing many details\" [18]. My survey reveals a related but distinct divide similar to Steinhardt's observation. One of the strongest correlations found in the data shows that timeline preferences (answers to \"when should we build AGI?\") strongly correlate with the participants' views on the seriousness of AI risk."}, {"title": "4.3 Consensus", "content": "While I pointed out some areas of divergence, there are notable points of consensus among AI experts that deserve highlighting.\n1.  Professional Responsibility: The vast majority of the sample (77%) believe that \"technical researchers should be concerned with catastrophic risks\". This high level of agreement spans across different groups (safety researchers, academics, and industry professionals)\n2.  Moral Patienthood: There is moderate consensus (57% agreement) that future Als may be deserving of moral patienthood. Notably, this belief transcends the usual divides, and shows similar levels of support across groups and correlates minimally with other beliefs.\n3.  Value of Safety Research: There is strong consensus against the idea that \"safety work slows progress and wastes time\" (only 17% agree). This low agreement score is particularly noteworthy for industry professionals.\n4.  Self-Preservation Drives: While most participants were unfamiliar with 'instrumental convergence,' 51% still agreed about the emergence of self-preservation drives in advanced AI systems.\nThese points of consensus suggest some common ground for productive dialog between viewpoints, even as other disagreements persist."}, {"title": "5 Limitations", "content": "I believe that my study offers valuable insights into philosophical and technical A\u0399 beliefs among experts. However, I acknowledge the following limitations:\n1.  Sample size: My study had a relatively small sample size (N=111) compared to the largest surveys of AI experts. Of these participants, 87 made it past the intervention into the final third of the survey.\n2.  Limited intervention types: I include 4 different interventions, including a control. However, these do not capture the full range of AI safety arguments or expert opinions that might influence opinions.\n3.  Limited objection list: the objections were chosen because we found these were the most popular objections after reading interviews withGates [10].\n4.  Short-term opinion changes: The study has not assessed the stability of opinion shifts over large periods of time."}, {"title": "6 Conclusion", "content": "I observed two distinct worldviews - \"AI as an uncontrollable agent\" and \"AI as a controllable tool.\" These mindsets correlate strongly with preferred AGI development timelines and beliefs about how controllable AI systems are. This finding indicates that some disagreements over AI safety stem from deeper differences of what experts conceptualize artificial intelligence itself. Furthermore, there exists a concerning gap in AI safety literacy among AI experts. Lack of familiarity with AI safety concepts strongly correlated with lower assessment of catastrophic risk from AI. For example, participants who had 'never heard of the terminology from the AI safety literature (instrumental convergence, scalable oversight) were notably less concerned and more likely to embrace the \"AI as a controllable tool\" perspective. This suggests that some AI safety skepticism may stem more from unfamiliarity with research than disagreement with its premises. Next, there is some consensus among AI experts on the importance of considering advanced AI systems as potential moral patients (57% agreement) and the need for technical researchers to be concerned about catastrophic risks (77% agreement). Although I did not measure statistically significant differences between participants' reactions to different AI safety arguments, the effectiveness varied slightly by argument type, with expert-backed perspectives generally receiving better reception than alarmist scenarios or media pieces. While distinct worldviews persist, there exists common ground for productive dialog about AI safety."}, {"title": "Declarations", "content": "This work was supported by the ERA Fellowship."}, {"title": "Ethics approval and consent to participate", "content": "All survey participants were required to agree to the following consent form:\nWhat will you be asked to do?\n\u2022 Complete a survey assessing your familiarity with AI and AI safety concepts (<10 minutes)\n\u2022 Read a short excerpt (<5 minutes) from an AI threat model or related material\nDo I have to take part?\nParticipation is entirely voluntary. You are free to withdraw at any time without giving a reason.\nConfidentiality - who will have access to my personal data?\nWe are not collecting any personal information.\nWhat will happen to the study results?\nStudy results may be submitted to peer review journals. A preprint of this research will be available on arXiv.\nWho is organizing the research?\nThis is a project run by Severin Field during the ERA AI Cambridge Summer Fellowship. The research is being conducted as part of a broader effort to understand AI expert views."}, {"title": "Author contribution", "content": "The sole author is responsible for all aspects of this work."}, {"title": "Appendix B Key AI Safety Terms", "content": "AI alignment: The challenge of ensuring AI systems reliably pursue human values and interests.\nScalable oversight: Methods for maintaining control and evaluation of AI systems as they grow more capable than humans.\nInstrumental convergence: The tendency for intelligent agents to pursue certain subgoals (like self-preservation) regardless of their primary objectives.\nCoherent extrapolated volition: An argument that AI systems should be designed to pursue what humanity would want if we were wiser, better informed, and had more time to reflect on our values not just what we currently say we want."}, {"title": "Appendix C Free Response Questions", "content": "Our survey included optional open-ended questions to gather qualitative insights from participants. Tables C1 and C2 present selected verbatim responses to these open-ended questions."}]}