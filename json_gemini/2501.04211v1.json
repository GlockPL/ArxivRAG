{"title": "CURing Large Models: Compression via CUR Decomposition", "authors": ["Sanghyeon Park", "Soo-Mook Moon"], "abstract": "Large deep learning models have achieved remarkable success but are resource-intensive, posing challenges in computational cost and memory usage. We introduce CURing, a novel model compression method based on CUR matrix decomposition, which approximates weight matrices as the product of selected columns (C) and rows (R), and a small linking matrix (U). We apply this decomposition to weights chosen based on the combined influence of their magnitudes and activations. By identifying and retaining informative rows and columns, CURing significantly reduces model size with minimal performance loss. It preserves the original network's input/output structures, retains important features such as non-negativity, and the compressed model's activation patterns align with the original, thereby enhancing interpretability.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of deep learning has led to the development of increasingly large models that have achieved remarkable success across various domains. These models, while powerful, come with substantial computational costs and memory requirements, making them challenging to deploy in resource-constrained environments. In many practical applications, there is a critical need for models that are both accurate and efficient.\nOne approach to bridge this gap posed by compressing large models is the use of pruning then Parameter-Efficient Fine-Tuning (PEFT) as a form of healing. In this context, models are first pruned to reduce their size by eliminating less significant parameters. The pruning process, however, can lead to a loss of precision and degrade the model's performance. Therefore, PEFT methods are then employed to fine-tune (retraining) the remaining parameters efficiently healing the model to restore its performance. This combination enables the development of compact models that maintain high levels of accuracy where resources are limited. However, even with the use of PEFT, fine-tuning large models still requires considerable computational resources.\nMatrix decomposition is a promising approach for model compression, reducing neural network size while preserving key information. By approximating the original matrix under low-rank conditions, it effectively minimizes storage and memory requirements without requiring retraining. However, highly information-preserving decompositions are often computationally expensive, both in factorization and in selecting elements to prune, making them less practical for large-scale models. Additionally, decomposition can disrupt the original characteristics of weight matrices, such as explainability, as the factorized components consist of entirely new parameters distinct from the original matrix.\nTo address the challenges of model compressing-retraining overhead, massive process time, and losing characteristics, we propose CURing, a novel model compression technique based on CUR matrix decomposition. By leveraging CUR decomposition's strong original-approximation feature, CURing inherently heals the damage caused by compression. Unlike the (structural) pruning methods, CURing preserves the input/output dimensions, avoiding structural changes, while reducing the number of parameters by decomposing the original weight matrix \\(W\\) into the low-rank matrices C, \\(Uo\\), and R. Furthermore, by adding a square matrix \\(\\Delta U\\) to the linking matrix \\(Uo\\) derived from CUR decomposition, CURing itself functions as a PEFT. This allows for further healing or task-specific adaptation, similar to LoRA. Since \\(\\Delta U\\) is a square matrix, it has maximum expressiveness within the same rank, enabling CURing to be the best effective fine-tuning method like MoRA.\nIn summary, the key contributions of this paper are:\n\u2022 We introduce a novel neural network compression technique based on CUR decomposition, fast and effectively reducing model size while maintaining performance. We demonstrate that our approach allows for"}, {"title": "2. Related Work", "content": "2.1. Pruning\nPruning reduces neural network size by removing or zeroizing significant weights or neurons. Layer-wise Pruning removes specific layers to improve efficiency. In a recent study, similar layers were identified and removed by measuring angular distances between layer outputs in large GPT-style models, excluding the last layer. After the pruning, fine-tuning using LoRA compensated for performance loss. Similarly, study proposed pruning less important layers by leveraging cosine similarity, with retraining required to restore pruning damage. Another study explored selective removal of layers from decoder-based language models while keeping the first and last layers to preserve performance. In other work, measuring the persistence of topological features in each layer led to the removal of layers when adjacent layers showed high similarity . Attention Pruning removes unnecessary attention heads. It was shown that only some attention heads in multi-head attention are important, and others can be removed without affecting performance . The recent research shows that making feed-forward network and query, key, value matrices sparse with most elements zero is possible with minimal performance degradation . These studies suggest that, in LLMs, some layers and weights of attention can be replaced with low-rank approximations, supporting CURing's approach.\nStudies about structural pruning often remove components based on rows or columns. Such methods alter network structure, leading to issues like incompatibility with existing adapters or merging mismatches . CURing retains certain rows and columns in weight matrices (thus removing less important ones) but maintains original input/output structures, avoiding these issues. It also preserves properties like negative features, maintaining interpretability.\nWe can use additional information for better pruning. The Fisher information matrix was used to perform precise pruning based on parameter influence on output distributions , then LoRA was used to correct distortions from pruning. A method using input feature activations along with weight magnitudes for pruning was proposed, allowing immediate use without retraining . Incorporating additional information can lead to better performance at the same compression rate.\n2.2. Model Compression\nLow-Rank Approximations have been widely used for model compression. Self-attention matrices inherently possess low-dimensional characteristics, demonstrated via performance and Singular Value Decomposition (SVD) analysis . Compression was also performed by lowering rank via SVD-based matrix factorization . Parameters were reduced by 1.8\u20133.8 times using SVD and weight sharing, but retraining was required . Acceptable performance without retraining was achieved using SVD-based compression . Low-rank approximation through SVD in transformer FFN layers showed lower loss when pruning later (near-output) layers. Further, SliceGPT employs the Principal Component Analysis (PCA) technique to compress the weight matrices meticulously.\nInterpolative Decomposition (ID) has been used for compression, maintaining performance without extensive retraining by preserving original weight matrix information . The recent study, STAT, uses QR decomposition to identify and remove less important parts, then generating correction matrices to minimize damage and maintain structure, eliminating the need for retraining. CURing, like STAT, inherently possesses a correction effect but achieves this faster in a single decomposition step without structural considerations, saving significant time (hours vs. minutes). While ID offers similar interpretability, CUR decomposition quickly obtains decomposed matrices . Also, for very large and sparse matrices, CUR can be more memory-efficient than ID.\n2.3. Parameter-Efficient Fine-Tuning\nParameter-Efficient Fine-Tuning (PEFT) updates only a small number of parameters to adapt models efficiently. LORA, as in Figure 1a, learns two additional low-rank matrices during fine-tuning. However, asymmetric low-rank matrices may have limitations due to low expressive power. To overcome this, MoRA uses square matrices for high-rank expressiveness with the same parameter efficiency, employing human-defined non-parameterized operators (comp, decomp) to compress and expand dimensions (Figure 1b). CURing enables parameter-efficient high-rank updates via a trainable square matrix U, achieving maximum rank for the same number of parameters. By interpreting U as \\(Uo + \\Delta U\\), CURing maximizes"}, {"title": "2.4. Knowledge Distillation", "content": "automatic healing without retraining.\n\u2022 Our method preserves the original network's structure after compressing. CURing even preserves the original weight's characteristics such as negativeness, so can be easily integrated with other compression techniques such as pruning. Unlike other decomposition techniques, CURing offers high interpretability by directly utilizes values from the original neural network, retaining essential structural and semantic information.\n\u2022 We show that CURing is also a parameter-efficient fine-tuning method itself, allowing a relatively high rank value within the constraint of same trainable parameters, and therefore enabling high-informative adaptation.\nKnowledge Distillation transfers knowledge from a large model to a smaller one . Layer-wise differences between student and teacher models were expressed as mean squared error (MSE) loss for training. Models were able to be compressed by training with block-specific losses . Our proposed CURing shows sufficient performance without retraining but employs distillation with the original model for additional healing. Distillation on the C4 dataset compensates for loss due to low-rank decomposition. Although distillation was conducted on the C4 dataset, its performance is task-independent; experiments confirm excellent results on datasets like Wikitext, BoolQ, and MMLU."}, {"title": "3. CUR Matrix Decomposition", "content": "Matrix decomposition techniques are widely used for dimensionality reduction, data compression, and efficient computations. A promising application is in compressing neural network models by approximating their weight matrices with low-rank representations. The assumption is that model's core information which can be represented within lower rank exist; under this assumption, layers and components that have less impact (present less changes) can have their rank reduced.\nCUR decomposition approximates an original matrix \\(W \\in \\mathbb{R}^{m\\times n}\\) as:\n\\begin{equation}\nW \\approx CUR,\n\\end{equation}\nwhere \\(C = W [:, q] \\in \\mathbb{R}^{m\\times r}\\) consists of selected columns from \\(W\\), \\(R = W [p, :] \\in \\mathbb{R}^{r\\times n}\\) consists of selected rows from \\(W\\), and \\(U \\in \\mathbb{R}^{r\\times r}\\) is a small square matrix capturing interactions between these rows and columns. The integer vectors \\(p, q \\in \\mathbb{N}\\) are \\(r\\)-distinct selected indices. CUR decomposition can approximate the original matrix well by properly selecting rows and columns, based on their importance (e.g., \\(l_2\\)-norms or leverage scores ).\nOnce the matrices \\(C\\) and \\(R\\) are obtained, the core matrix \\(U\\) is computed:\n\\begin{equation}\nU = C^{\\dagger}WR^{\\dagger},\n\\end{equation}\nwhere \\(C^{\\dagger}\\) and \\(R^{\\dagger}\\) are the pseudoinverses of \\(C\\) and \\(R\\), respectively . Computing \\(U\\) using pseudoinverses is optimal with respect to the Frobenius norm .\n3.1. DEIM-CUR\nVarious methods exist for sampling rows and columns efficiently. Algorithms using random sampling probabilities based on norms or leverage scores allow for fast approximation , and even random selections yield satisfactory performance . However, these methods often require selecting more rows and columns than the target rank \\(r\\) to achieve bounded error performance.\nBuilding on the Discrete Empirical Interpolation Method (DEIM) selection algorithm , the DEIM-CUR decomposition offers a deterministic approach by selecting exactly \\(r\\) rows and \\(r\\) columns corresponding to the rank \\(r\\), leading to more accurate approximations under the constraint of limited selected rows and columns . Since the main purpose of this study is compression to reduce memory usage, adopting DEIM-CUR is more appropriate compared to other methods that require much more rows and columns.\nThe DEIM-CUR factorization provides a strong approximation of a matrix \\(W \\in \\mathbb{R}^{m\\times n}\\) with a bounded error. According to the studies , the CUR approximation is bounded within a factor of \\((\\sqrt{n_p} + \\sqrt{n_q})\\) relative to the error of the optimal rank-\\(r\\) solution \\(\\sigma_{r+1}\\) .\nTheorem 3.1. Let \\(W \\in \\mathbb{R}^{m\\times n}\\) and \\(1 < r < \\min(m, n)\\). The rank-\\(r\\) singular value decomposition of \\(W\\) is expressed as \\(W \\approx PEQ^T\\), where \\(P \\in \\mathbb{R}^{m\\times r}\\) and \\(Q \\in \\mathbb{R}^{n\\times r}\\) consist of the leading \\(r\\) left and right singular vectors, respectively. Suppose the integer vectors \\(p, q \\in \\mathbb{N}^r\\) contain \\(r\\)-distinct indices selected using the DEIM algorithm from \\(P\\) and \\(Q\\), respectively (\\(p = DEIM(P)\\) and \\(q = DEIM(Q)\\)). The DEIM-CUR factorization defines the matrices \\(C = W [:, q] \\in \\mathbb{R}^{m\\times r}\\), \\(R = W [p, :] \\in \\mathbb{R}^{r\\times n}\\), and \\(U = C^{\\dagger}WR^{\\dagger} \\in \\mathbb{R}^{r\\times r}\\). The error bound of the DEIM-CUR factorization is:\n\\[\n||W - CUR||_2 \\le (\\sqrt{n_p} + \\sqrt{n_q})\\sigma_{r+1},\n\\]\nwhere \\(\\sigma_{r+1}\\) is the first neglected singular value of \\(W\\), and the finite error constants are defined as \\(n_p = ||(P [p,:])^{-1}||_2\\) and \\(n_q = ||(Q [:, q])^{-1}||_2\\).\nFollowing the recent research , the DEIM-CUR factorization provides an improved and interpretable error bound given by:\n\\[\n\\sqrt{n_p} < \\sqrt{\\frac{3}{2}}, \\quad \\sqrt{n_q} < \\sqrt{\\frac{3}{2}}.\n\\]\n3.2. Parameter Reduction\nCUR decomposition effectively reduces the number of parameters in the model. Specifically, the total number of parameters in C, U, and R is smaller than in the original matrix W when the condition \\(mn > mr + r^2 + rn\\) is met, where \\(r\\) represents the rank. Specifically, we use \\(r < \\min(m, n)\\):\n\\[\nr \\leftarrow \\min \\left( \\sqrt{\\frac{(m^2+6mn+n^2)-(m+n)}{2}}, \\quad \\max \\left(2^{[\\log_2{(1000)}]}, 2^{8} \\right) \\right).\n\\end{equation}\nThe constraint to ranks that are powers of 2 ensures compatibility with hardware acceleration requirements. Additionally, we impose an upper bound rmax to maintain C and R as low-rank matrices, while ensuring that the trainable matrix U remains parameter-efficient."}, {"title": "4. CURing", "content": "CUR decomposition's ability to maintain the original matrix's properties while reducing dimensionality makes it a suitable choice for compressing large neural network weight matrices without significant loss of information. It preserves properties like sparsity and non-negativity . This is beneficial for interpretability and maintains the original characteristics of weight matrices. Moreover, CUR decomposition retains the original input and output dimensions (m and n).\nOur proposed method, CURing, compresses neural network models by reducing the rank of certain layers' weights using CUR matrix decomposition. By identifying layers that contribute less to the model's performance, we replace their weights with low-rank approximations, significantly reducing model size without substantial loss in functionality.\nWe are focused on applying CURing to compress transformers, which are the foundation of most large-scale models. Specifically, we focus on factorizing the Multi-Head Attention (MHA) and Feed-Forward Network (FFN) components of the transformer. We apply compression specifically targeting the Query, Key, and Gate weights in the Llama architecture (See the Figure 3b and 3c). We ignore biases for simplicity. Through experimental evaluation, we also found that applying CUR decomposition to them offers a favorable balance between size reduction and performance loss than other weights combination, within the constraint of max rank size of 256.\n4.1. Layer Selection\nSeveral researches indicate that layers not playing significant roles can be removed in LLMs . That means, we can replace them with low-rank representations, without loss of significant performance damage. We focus on reducing the rank of layers exhibiting minimal changes\u2014specifically, where the distance between their outputs is small.\nTo measure representation similarity, we compute the angular distance between the output representations of a layer and those of a subsequent layer. The angular distance between two hidden states of layer \\(n-1\\) and \\(n\\), \\(h_{n-1}\\) and \\(h_n\\), is defined as:\n\\begin{equation}\nd(h_{n-1}, h_n) = \\frac{1}{\\pi} \\arccos \\left( \\frac{h_{n-1} \\cdot h_n}{|| h_{n-1} ||_2 ||h_n||_2} \\right),\n\\end{equation}\nwhere \\(\\cdot\\) is the inner product over the hidden states of the last non-padded token of the sequence, and \\(|| \\cdot ||_2\\) denotes the \\(l_2\\)-norm. The hidden states are obtained during calibration and average over all calibration data. In experiments, we use 128 Colossal Clean Crawled Corpus (C4) dataset.\nSmall angular distance implies that layers maintain similar information; thus, the later layer can be replaced with a low-rank approximation without significantly affecting performance. In other words, for two similar layers, we perform CUR decomposition on the later layer, as shown in the Figure 3a. We retain the last layer, as it is essential for model performance consistent with findings in .\n4.2. CUR Decomposition on Weights\nAfter selecting the layers, we compress the weights in each layer using CUR decomposition. For each weight, to select rows and columns for CUR decomposition, we employ the WANDA method alongside the Discrete Empirical Interpolation Method (DEIM) .\nWANDA utilizes both weight magnitudes and activation information, advancing the selection criterion from a basic approach (considering only magnitudes) to a more sophisticated one (additionally considering changes). This enriched information allows for the sensitive detection of fine weight influences, enabling effective pruning. When Leveraging WANDA to select rows and columns for CUR decomposition, it results in a higher-quality approximation that incorporates more informative aspects. As illustrated in Figure 2a, the information matrix \\(S\\) is computed by multiplying the absolute values of the weights with the input activations (outputs of the preceding layer). We use the 128 C4 dataset to collect activations, a process performed concurrently with the computation of per-layer angular distances.\nIn DEIM-CUR, for a given target rank \\(r\\), the indices of the most important \\(r\\) rows and \\(r\\) columns are selected based on the Singular Value Decomposition (SVD) of informative matrix \\(S \\approx PEQ^T\\). This process is illustrated in Figure 2b. Using the selected indexes, we extract C and R from the original matrix \\(W\\) (Figure 2c). We then compute the core matrix U to approximate the original weight matrix \\(W\\). The core matrix U is calculated using the pseudoinverses of C and R, following the Equation 1.\nConsider a fully connected (FC) network defined as:\n\\begin{equation}\nf_{W,W_2}(x) = \\gamma(xW)W_2,\n\\end{equation}\nwhere \\(\\gamma\\) is the activation function and biases are omitted for simplicity. For convenience, we write \\(f_W\\) instead of \\(f_{W,W_2}\\). We assume that the FC layer has sufficiently many hidden units. Moreover, let any continuous \\(f \\in C(K)\\) be defined on a compact set \\(K \\subset \\mathbb{R}^{m}\\) (thus \\(x \\in K\\)), and let \\(\\gamma(\\cdot)\\) be"}, {"title": "4.3. Decomposing Multi-Head Attentions", "content": "L-Lipschitz continuous for some real constant \\(L > 0\\):\n\\begin{equation}\n||\\gamma(a) - \\gamma(b) ||_2 \\le L||ab||_2 \\quad \\text{for all } a, b.\n\\end{equation}\nUnder these conditions, and building upon prior findings on approximation errors , the CUR-factorized network \\(f_{CUR}\\) satisfies the following error bound:\nTheorem 4.1. Let \\(f_{CUR} \\in C(K)\\) be defined on a compact set \\(K \\subset \\mathbb{R}^{m}\\) with an L-Lipschitz activation \\(\\gamma\\). Suppose a rank-\\(r\\) DEIM-CUR factorization (\\(W \\approx CUR\\)) is applied to the fully connected layer. Then \\(f_{CUR}\\) approximates any continuous \\(f \\in C(K)\\) within an error bound of \\((\\epsilon + \\delta)^2\\):\n\\[\n||f - f_{CUR}||_2 \\le (\\epsilon + \\delta)^2,\n\\]\nif the following inequality is satisfied:\n\\[\n\\sigma_{r+1} \\le \\frac{\\delta}{L(\\sqrt{n_p} + \\sqrt{n_q})(||W_2||_2||K||_2)^{-1}},\n\\]\nwhere \\(n_p\\) and \\(n_q\\) are finite error constants from Theorem 3.1, and \\(\\sigma_{r+1}\\) is the \\((r + 1)\\)-th singular value of the original matrix \\(W\\) (i.e., the first neglected singular value).\nHere, \\(\\epsilon\\) is the universal approximation error associated with the full-rank matrix \\(W\\), and \\(\\delta\\) is the additional error introduced by the low-rank \\((r)\\) CUR decomposition. The proof of Theorem 4.1 is provided in Appendix A.1.\nBy applying CUR factorization (e.g., on Q, K, or Gate) before activations, followed by a linear map, MHA and FFN layers can be viewed as FC-like structures, allowing the same approximation error bounds to hold.\nIn Transformer architectures, the Multi-Head Attention (MHA) mechanism plays a critical role in capturing contextual relationships within the input sequence . Each MHA layer consists of multiple attention heads, each with its own set of query (Q), key (K), and value (V). To compress the MHA layers, we apply CUR decomposition specifically to the \\(W^Q\\) and \\(W^K\\) matrices, which are responsible for generating Q and K, respectively. Figure 3b illustrates the decomposition of MHA.\nGiven an input sequence \\(X \\in \\mathbb{R}^{l\\times d_{model}}\\, where \\(l\\) is the sequence length and \\(d_{model}\\) is the model dimension, the weight matrices for the \\(i\\)-th head are defined as:\n\\[\nW_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{model}\\times d_k},\n\\]\nwhere \\(d_k\\) is the dimension of the queries and keys. To simplify and align with the Llama architecture, we set the hidden dimension of \\(W^V\\) to \\(d_k\\), the same as that of queries and keys. Now, the queries, keys, and values are computed by projecting the input X using the weight matrices:\n\\[\nQ_i = XW_i^Q, K_i = XW_i^K, V_i = XW_i^V \\in \\mathbb{R}^{l \\times d_k}.\n\\]\nFor each attention head, the output is computed using the attention mechanism as follows:\n\\begin{equation}\nHead_i(X) = Attention(Q_i, K_i, V_i)\n=\\text{Softmax} \\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right)V_i.\n\\end{equation}"}, {"title": "4.4. Decomposing Feed-Forward Networks", "content": "Intuitively, \\(P_{Head}\\) represents a significant rank reduction in the matrix for context interpretation in transformers, based on the idea that layers causing minimal output changes do not require high context interpretation capacity. Furthermore, the core matrices \\(U^{\\{Q,K\\}}\\) are learnable, enabling fine-tuning to adjust the model's context-capturing ability and achieve the best parameter efficiency at rank-\\(r\\).\nUnlike previous studies that focused on simple two-layer networks with a single hidden layer , our research investigates Llama-like Feed-Forward Networks (FFNs), which are commonly used in today's large language models. We consider the FFN to consist of a Gate, Up, and Down projections , with corresponding weights \\(W^{Gate}\\), \\(W^{Up}\\), and \\(W^{Down}\\). Figure 3c represents the structure of the FFN and presents a brief overview of its decomposition.\nGiven an input vector \\(X \\in \\mathbb{R}^{l\\times d_{model}}\\, the FFN output is calculated as:\n\\begin{equation}\nFFN(X) = \\text{SiLU}(XW^{Gate}) \\odot XW^{Up} \\)W^{Down},\n\\end{equation}\nwhere \\(W^{Gate}, W^{Up} \\in \\mathbb{R}^{d_{model} \\times d_{inter}}\\) and \\(W^{Down} \\in \\mathbb{R}^{d_{inter} \\times d_{model}}\\, with \\(d_{inter}\\) representing the intermediate dimension. Llama employs the SiLU activation function . The gate projection part, \\(P_{FFN}\\), effectively controls the flow of information.\nTo compress the FFN, we apply CUR decomposition to the weight \\(W^{Gate}\\). Specifically, we approximate this weight matrix as:\n\\[\nW^{Gate} \\approx C^{Gate} U^{Gate} R^{Gate}\n\\]\nwhere \\(C^{Gate} \\in \\mathbb{R}^{d_{model} \\times r}\\) contains selected columns from \\(W^{Gate}\\); \\(R^{Gate} \\in \\mathbb{R}^{r \\times d_{inter}}\\) consists of selected rows; and \\(U^{Gate} \\in \\mathbb{R}^{r \\times r}\\) is the core matrix. With this approximation \\(W^{Gate} \\approx W^{Gate}\\), the FFN computation becomes:\n\\begin{equation}\nFFN(X) = \\text{SiLU}(XC^{Gate} U^{Gate} R^{Gate}) \\odot XW^{Up} \\)W^{Down},\n\\end{equation}\nBy applying CUR decomposition to \\(W^{Gate}\\), the number of parameters in the FFN is reduced while preserving the original input/output dimensions, thus maintaining compatibility with the original model architecture. With the learnable weight \\(U^{Gate}\\) in \\(P_{FFN}\\), the model can adjust the emphasis and suppression of information passed to the next layer."}, {"title": "4.5. Layer-wise Knowledge Distillation", "content": "Although retraining is not strictly required due to the inherent correction provided by CUR decomposition, additional training (healing) can be beneficial when the model undergoes substantial compression or when further performance improvement is desired.\nIn the healing process, we allow only U to be updated, while the matrices C and R remain fixed. Further, the core matrix U is interpreted as \\(U = U_o + \\Delta U\\), where \\(U_o\\) is initialized to the value of U, and \\(\\Delta U\\) starts as a zero matrix. During healing, \\(U_o\\) remains fixed while \\(\\Delta U\\) is iteratively updated. As illustrated in Figure 1, this formulation allows healing to be intuitively viewed as a Parameter-Efficient Fine-Tuning (PEFT) method, where \\(\\Delta U\\) corresponds to the trainable component.\nHowever, the goal here is not task-specific adaptation as PEFT does, but to restore overall performance while mitigating catastrophic forgetting. To achieve this, we use layer-wise Knowledge Distillation (KD), as illustrated in Figure 3d. We employ a layer-wise Mean-Squared-Error (MSE) loss between the teacher (original) and student (compressed) model outputs. This approach aligns with techniques in prior work and has proven effective in preserving the performance of compressed models . KD also acts as implicit regularization , since the soft outputs of the teacher guide the student to prevent overfitting and constrain excessive parameter growth. Therefore, catastrophic forgetting on previously learned information is mitigated, even if we use only one kind of corpus (e.g., C4) for healing.\nA structural approach that fixes the C and R also mitigates forgetting. Intuitively, only U is updated, and so the optimization is restricted to a subspace determined by C and R. Similar to MoRA's comp/decomp modules , C and R project parameters to and from a lower-rank space. However, unlike MORA, CURing's fixed C and R impose additional constraints that regulate the update directions of U. This mitigates catastrophic forgetting, as observed in CURLORA .\nTo analyze this more formally, let us revisit the single fully-connected network defined in Equation 3, where the activation function \\(\\gamma(\\cdot)\\) is Lipschitz continuous with constant L, as stated in Equation 4. For an input batch X with b as the batch size, the MSE between the original output \\(f_W(X)\\) and its CUR approximation \\(f_{CUR}(X)\\) is given by:\n\\begin{equation}\nMSE = \\frac{1}{b} ||f_W(X) - f_{CUR}(X)||^2_F.\n\\end{equation}\nMeanwhile, we consider the Frobenius norm-based loss:\n\\[\n\\mathcal{L}(U) = ||W - CUR||^2_F.\n\\]\nInstead of considering the MSE directly, we use \\(\\mathcal{L}(U)\\) in our analysis, as the MSE is upper-bounded by \\(\\mathcal{L}(U)\\). This allows us to analyze the network at the level of weight matrices, where minimizing \\(\\mathcal{L}(U)\\) also optimizes the MSE.\nTheorem 4.2. Let \\(f_W\\) and \\(f_{CUR}\\) represent the outputs of a fully connected network with weights W and their CUR factorized matrices C, U, and R, respectively. Suppose the activation function \\(\\gamma(\\cdot)\\) is Lipschitz continuous with constant L, and the input batch X (of size b) is sufficiently diverse and uniformly distributed. Then, the MSE satisfies the following upper bound:\n\\begin{equation}\nMSE(X) = \\frac{1}{b} ||f_W(X) - f_{CUR}(X)||^2_F \\leq \\frac{1}{b} L^2 ||X||^2_F ||W_2||^2_F \\mathcal{L}(U),\n\\end{equation}"}, {"title": "5. Experiments", "content": "where \\(\\mathcal{L}(U) = ||W - CUR||^2_F\\) is the Frobenius norm-based loss.\nUsing \\(\\mathcal{L}(U)\\), we can investigate the subspace restriction of U as follows:\nTheorem 4.3. Given \\(W \\approx CUR \\in \\mathbb{R}^{m\\times n}\\), let \\(C \\in \\mathbb{R}^{m\\times r}\\) and \\(R \\in \\mathbb{R}^{r\\times n}\\) be fixed, while \\(U \\in \\mathbb{R}^{r\\times r}\\) is the only trainable matrix. Consider the loss function:\n\\[\n\\mathcal{L}(U) = ||W - CUR||^2_F.\n\\]\nThe gradient of this loss with respect to U, denoted as \\(\\nabla_U\\mathcal{L}(U)\\), always lies in the set:\n\\[\n\\nabla_U \\mathcal{L}(U) \\in \\{C^T M R^T\\},\\nonumber\n\\]\nwhere \\(M = CUR - W \\in \\mathbb{R}^{m \\times n}\\).\nThe proofs of Theorems 4.2 and 4.3 can be found in Appendices A.2 and A.3, respectively.\nWe further consider the optimization problem:\n\\[\nU^* = \\arg \\min_U \\mathcal{L}(U) = \\arg \\min_U ||W - CUR||_F.\n\\]\nThe solution \\(W^* = CU^* R\\) is the best Frobenius norm approximation to W, that is, \\(||W^*||_F \\approx ||W||_F\\). As healing progresses to minimize \\(\\mathcal{L}(U)\\), the compressed model's representation CUR approaches \\(W^*\\). Thus, scales becomes aligned with those of the original weights. By constraining the update directions and scales, the healing process imposes structural regularization on the changes to U (i.e., restricting \\(\\Delta U\\)). Semantically, this enhances the context-interpreting performance of \\(P_{Head}\\) and \\(P_{FFN}\\) while mitigating forgetting.\nEmpirically, after KD, the Frobenius norm difference between W and CUR decreases, so the student's norm no longer overshoots the teacher's. In experiments, we performed KD using the C4 dataset , excluding the data used for calibration in measuring layer-wise angular distances and accumulating WANDA input activations. Remarkably, we observed that the model's performance across multiple tasks was quickly restored with only about 100 steps of fine-tuning. This demonstrates the efficiency of our approach.\nWe evaluate CURing across multiple datasets and settings to assess its compression efficiency, performance retention, and healing capabilities. Calibration (calculating WANDA and angular distances) and healing data are drawn from the C4 training set, with no overlap between the two. The use of C4 provides superior performance in calibration compared to other corpora . By default, we calibrate on 128 examples. We evaluate models"}, {"title": "6. Discussion", "content": "on the C4 validation subset, WikiText2, BoolQ (0-shot), and MMLU (5-shot). For MMLU, we use 32 samples per category from 57 categories. The context length is capped at 128; the detailed hyperparameters appear in Appendix B. All experiments are conducted on a single NVIDIA H100 80GB GPU.\n5.1. Compression Performance\nWe apply CURing to three models\u2014Llama3.1-8B, Mistral-7B, and Orca2-7B\u2014to investigate compression overhead, size reduction, and performance impact. We target the \\(W^Q\\), \\(W^K\\), and \\(W^{Gate}\\) weight matrices in each model.\nTable 1 presents the time required for CURing as we vary the number of compressed layers (1 to 30), excluding the first and last layers as discussed in Section 4.1. The max rank is fixed at 256 (other ranks are discussed in Appendix C.2). As the number of target layers increases, the total decomposition time grows linearly. For instance, compressing 10 layers takes about 2 minutes, while compressing all 30 layers requires 6\u20137 minutes. This is significantly faster than other compressing methods, such as SliceGPT , which takes about 44 minutes to compress Llama2-7B on a single H100 , due to the overhead from PCA and residual-connection handling. By leveraging the efficient CUR decomposition method and avoiding complex structural considerations, CURing achieves remarkable speed.\nAs also detailed in Table 1, compressing 10 layer of Llama3.1-8B results in a size reduction of approximately 2.66 GiB (about 9%). Increasing the number of compressed layers proportionally lowers model size. Unlike approaches that remove entire layers , CURing preserves each layer in a low-rank form. Although pruning can achieve more substantial size reductions, it often demands retraining to restore performance, whereas CURing typically retains acceptable performance without further training.\nFigure 4 compares the performance of CURing with different numbers of compressed layers. We report perplexity on C4 and WikiText2, along with zero-shot or few-shot"}, {"title": "5.2. Healing", "content": "accuracies on BoolQ and MMLU. For Llama3.1-8B, the uncompressed baseline (\\(x = 0\\)) yields perplexity values of 23.79 on C4 and"}]}