{"title": "Integrating Artificial Intelligence and Geophysical Insights for Earthquake Forecasting: A Cross-Disciplinary Review", "authors": ["Zhang Ying", "Wen Congcong", "Sornette Didier", "Zhan Chengxiang"], "abstract": "Earthquake forecasting remains a significant scientific challenge, with current methods falling short of achieving the performance necessary for meaningful societal benefits. Traditional models, primarily based on past seismicity and geomechanical data, struggle to capture the complexity of seismic patterns and often overlook valuable non-seismic precursors such as geophysical, geochemical, and atmospheric anomalies. The integration of such diverse data sources into forecasting models, combined with advancements in AI technologies, offers a promising path forward. AI methods, particularly deep learning, excel at processing complex, large-scale datasets, identifying subtle patterns, and handling multidimensional relationships, making them well-suited for overcoming the limitations of conventional approaches.\nThis review highlights the importance of combining AI with geophysical knowledge to create robust, physics-informed forecasting models. It explores current AI methods, input data types, loss functions, and practical considerations for model development, offering guidance to both geophysicists and AI researchers. While many AI-based studies oversimplify earthquake prediction, neglecting critical features such as data imbalance and spatio-temporal clustering, the integration of specialized geophysical insights into AI models can address these shortcomings.\nWe emphasize the importance of interdisciplinary collaboration, urging geophysicists to experiment with AI architectures thoughtfully and encouraging AI experts to deepen their understanding of seismology. By bridging these disciplines, we can develop more accurate, reliable, and societally impactful earthquake forecasting tools.", "sections": [{"title": "1 Introduction and Motivation", "content": "Despite decades of research, earthquake forecasting remains in an exploratory stage, with current methods still falling short of achieving performance levels that would provide meaningful benefits for society. Forecasting models based primarily on past seismicity and other geomechanical measurements have shown limited progress, appearing to exhibit diminishing returns on the large deployed research efforts, likely because the available techniques are insufficient to extract useful information from the complex seismicity patterns. Moreover, the information they rely on may be too narrow. These methods, while valuable, focus predominantly on geo-mechanical information (mostly on seismic source data and space-time seismicity patterns, and sometimes on geodetic and deformation information, fault zone properties, subsurface structure data, plate kinematics), which may overlook critical aspects of earthquake precursors.\nTo overcome these limitations, there is a growing recognition of the need to integrate non-seismic and non-mechanical information into earthquake forecasting models. Traditional approaches, which rely heavily on seismic and geomechanical data, struggle to capture the full complexity of earthquake precursors. Expanding the scope of data inputs to include geophysical and geochemical anomalies\u2014such as atmospheric and ionospheric precursors (total electron content, low-frequency electromagnetic emissions, geomagnetic field variations, atmospheric electric field), thermal infrared anomalies, gas and hydrochemical emissions, biological and ecological indicators, gravitational and tidal anomalies and even satellite-based remote sensing data[1, 2]-can provide a more comprehensive picture of the conditions leading to earthquakes. Incorporating diverse data sources allows for a holistic view that bridges gaps in traditional methods and leverages insights from a broader range of disciplines.\nHowever, the heterogeneity and sheer volume of these data, combined with their intricate interdependencies, pose a significant analytical challenge. Extracting meaningful, multivariate, and multidimensional information from such complex datasets exceeds the capabilities of conventional statistical and computational techniques. This is where AI becomes indispensable. With its capacity to process large-scale, diverse data, identify subtle patterns, and handle multidimensional relationships, AI offers the tools necessary to transform earthquake forecasting. By integrating advanced AI methods with data from many varied sources augmented by geophysical knowledge, it will become possible to not only enhance predictive accuracy but also uncover new insights into the precursors and mechanisms of seismic events. This review highlights the potential of such an approach to push the boundaries of current forecasting capabilities and deliver meaningful societal benefits.\nFor seismologists and geophysicists, this comprehensive review offers an in-depth exploration of current AI methods applied to earthquake forecasting. We detail the types of input data utilized, the different machine learning models, the loss functions commonly employed, and key practical considerations in model development. Additionally, we examine the future integration of AI technologies with geophysical expertise, highlighting how this synergy can significantly improve the accuracy and reliability of earthquake forecasting.\nOur aim is to equip domain experts with limited AI experience with the foundational knowledge needed to choose appropriate models, select relevant data inputs, and implement effective training strategies for creating high-quality forecasting systems tailored to their specific needs. For those already proficient in both geophysics and AI, we present cutting-edge techniques, future directions, and advanced methodologies that can help push the limits of current models, addressing existing challenges and paving the way for more powerful forecasting tools.\nWhile many AI researchers have shown interest in earthquake forecasting and proposed various models, our review reveals that many of these approaches oversimplify the problem, often treating it as a straightforward binary classification or regression task, with little attention to the underlying physical processes. Critical features of earthquake data\u2014such as the severe imbalance between positive and negative samples and the spatio-temporal clustering of seismic events\u2014are frequently overlooked. Moreover, without a deep understanding of geophysics, performance evaluations in many studies are often overly optimistic, failing to reflect real-world conditions.\nWe collected 142 relevant papers on earthquake prediction and forecasting using machine learning from 1994 to the present (Q4-2025), and the information about these papers is available at https://github.com/AI-earthquake/citations-for-Predicting-future- Californian-earthquakes-with-deep-neural-networks/tree/main. This review addresses these common oversights and provides guidance on building specialized models that are both practically effective and grounded in physical principles. By doing so, we aim to bridge the gap between the AI and geophysics communities, helping AI experts gain the geophysical insights necessary to develop more robust and credible earthquake forecasting models. Our goal is to ensure that these models are not only technically sound but also recognized and validated by the geophysics community, fostering true interdisciplinary collaboration."}, {"title": "2 Fundamentals of earthquake forecasting", "content": "In this section, we aim to assist non-expert earthquake forecasting researchers in locating valuable learning resources and swiftly establishing a professional earthquake forecasting system. We also highlight various organizations and platforms currently involved in earthquake forecasting, detailing the models they employ. This approach is intended to enable future AI experts to conduct prospective testing after developing their own earthquake forecasting models and to identify robust benchmarks for comparison. Those already proficient in the field can feel free to skip this part."}, {"title": "2.1. Differences between an earthquake forecast and an earthquake prediction", "content": "The terms \"earthquake forecast\u201d and \u201cearthquake prediction\u201d are often confused, even among seismologists, and especially by non-experts.\nEarthquake prediction refers to the precise determination of when, where, and with what magnitude an earthquake will occur. This approach aims to provide exact details about specific earthquake parameters, including the date, location, and intensity. An example of this would be an attempt to predict that a magnitude 6.5 earthquake will strike San Francisco on June 15th, 2025, at 10:30 AM. Such predictions require an exactness that is extremely difficult to achieve due to the complexity and unpredictability of earthquake-generating processes within the Earth's crust. Historically, earthquake prediction has proven to be highly elusive, and most claims have turned out to fail or to be incomplete.\nIn contrast, earthquake forecasting focuses on estimating the likelihood of seismic activity over well-defined time frames, spatial regions and magnitude range, generating probabilistic estimates rather than predicting specific events. In other words, while a prediction is more like a point prediction, a forecasts is a probabilistic statement. For example, a forecast might indicate a 30 percent chance of a magnitude 6.0 or greater earthquake occurring within the next 10 years in the Los Angeles region. This approach does not aim to specify the exact time or location of an event but instead provides a probability based on historical data, geological knowledge, and patterns of past seismicity. The spatial and temporal windows, as well as the magnitude range and number of earthquakes, can be tailored to different needs. For instance, a forecast could state the probability to observe at least three earthquakes of magnitude 4.0 or higher occurring between midnight today and midnight tomorrow within the official city boundaries of Tokyo, Japan. This flexibility allows forecasting models to be adapted for various scales and applications, from long-term regional assessments to short-term, localized hazard estimates.\nThe key distinction is that forecasting aims to provide useful information about earthquake risks in a manageable and realistic way, helping communities prepare for possible seismic hazards without attempting to specify the exact details of when and where the next big quake will strike. Because of its probabilistic nature, forecasting is seen as more reliable and scientifically grounded. The term \"forecasting\" is widely accepted in the statistical seismology community, as it reflects more modest and achievable objectives than the exacting demands of prediction, making it a more practical tool for disaster preparedness and risk mitigation."}, {"title": "2.2. Testing the performance of earthquake forecasting models", "content": "Despite the development of a large variety of methods, earthquake prediction remains controversial, particularly when it comes to major earthquakes. As noted by Huang et al. [3], progress has been slow, with only a few notable cases of successful predictions. These rare successes are often overshadowed by the numerous failures, leading many to dismiss successful predictions as mere coincidences or \u201clucky guesses.\" Let us however mention the unique approach and the strength of the Chinese earthquake prediction program that developed over a decade from 1966 to 1976 [2], but which has been abandoned for decades for complex political and organisational reasons and is now witnessing a recent renewal. One should also not forget the Russian earthquake prediction program launched by Vladimir Keilis-Borok and his collaborators and now led by Vladimir Kossobokov focused on a pattern recognition approach to earthquake prediction invented by Gelfand et al [4]. The real-time testing of the M8 and M8-MSc algorithms started in 1992 and continues until present with a six-months periodicity. The statistical significance of these on-going predictions can be found in Refs.[5, 6, 7, 8].\nSeveral factors contribute to the skepticism among scientists regarding the feasibility of developing reliable earthquake prediction methods.\n\u2022 Slow Progress: Research in earthquake prediction has been painstakingly slow, with very few verifiable successes.\n\u2022 Inconsistent Results: The few cases of success are vastly outnumbered by instances of failure, reinforcing the notion that successful predictions are more due to chance than a reliable method. Many past claims of success have proven irreproducible [9].\n\u2022 Lack of clarity on the underlying mechanism(s): As Freund et al. (2021)[1] pointed out, the lack of a clear physical mechanism linking non-seismic precursors (such as atmospheric or ionospheric anomalies) to earthquakes has led to widespread criticism, even from within the seismology and physics communities.\n\u2022 Lack of rigorous statistical testing methodology, in particular for works involving proposed non-seismic precursors [10].\nThis history highlights the critical need for rigorous verification, testing, and validation of models before they can be deemed reliable for practical use [10]. Leaving the realm of earthquake predictions to focus on the more scientifically robust approach to earthquake forecasting, seismologists have developed structured and systematic methods for assessing the effectiveness and reliability of forecasting models, providing a clearer path toward improving their real-world applicability. Three key principles for testing earthquake forecasting models have emerged [11].\n(i) Clear Parameterization: Forecasters must define clear parameters, including latitude, longitude, magnitude, and time range. Predictions may optionally include depth range, focal mechanisms, or other measurable earthquake characteristics.\n(ii) Objective Success Criteria: The success or failure of a forecast must be easily and objectively determined, without requiring further interpretation.\n(iii) Use of Prior Data: Models should be constructed using data available before the prediction time, to avoid introducing biases from data that would not be available in a real-world scenario. While this statement is a priori obvious, it can become subtly violated in statistical tests using past data that attempt to reproduce real-life real-time situations.\nThese principles are intended to ensure fair and objective evaluations of forecasting models, making it easier to assess their practical value without ambiguity. Rule (iii) is particularly important to trust that pseudo-prospective forecasting methods can be applied to real-world situations.\nTo rigorously evaluate earthquake forecasting models, prospective, retrospective and pseudo-prospective testing methods are employed [11].\n(1) Prospective Testing: In prospective testing, forecasts are made in real-time and evaluated against earthquakes that occur after the forecast is issued. This approach is the gold standard because it eliminates biases that could arise from knowing future data. Delayed prospective testing, in which models are isolated from post-forecast data, is another method that helps ensure unbiased assessments.\n(2) Retrospective Testing: Retrospective testing compares forecasts against past earthquake data, which the modelers may already know. While valuable, this method can be prone to bias, as the modelers may unconsciously tailor their models based on the known data.\n(3) To bridge the gap between retrospective and prospective testing, a hybrid method called pseudo-prospective testing is often used. In this approach, models are calibrated using data only up to a certain time, $t_0$, and then forecasts are made for a period after $t_0$. Although this method mimics the conditions of prospective testing, it carries a risk of bias, as the modelers may be influenced by their knowledge of future data, even if indirectly.\nIn summary, rigorous prospective or pseudo-prospective testing is essential for evaluating the practical utility of earthquake forecasting models. While retrospective testing has its uses, only by testing models in a forward-looking, unbiased manner can we gain reliable insights into their true performance. Ensuring that these models can withstand such scrutiny is critical for their adoption in real-world disaster preparedness and risk mitigation."}, {"title": "2.3. Benchmarks for earthquake forecasting models", "content": "\"Benchmarks\" refer to standards used to evaluate and compare performance, quality, or effectiveness. This topic originally belonged to the previous sub-section 2.2. However, among AI experts unfamiliar with the intricacies of earthquake forecasting, many studies evaluating AI-based forecasting models suffer from notable methodological flaws and a lack of rigor, undermining their credibility and professional standards. Therefore, we believe it is necessary to dedicate a separate section to benchmarks to emphasize their importance.\nWhen assessing the performance of earthquake forecasting models, a significant knowledge gap exists between seismologists and AI experts[12]. AI experts often treat earthquake forecasting merely as a classification problem, overlooking key spatio-temporal features of earthquakes, such as the imbalance between positive and negative samples and the pronounced clustering of earthquakes in both space and time. On the other hand, statistical seismologists tend to adopt more specialized evaluation metrics, comparing their models with better benchmarks.\nLet us review a longstanding debate concerning earthquake forecasting. DeVries et al.[13] built a deep learning network with 13,000 parameters to forecast the spatial distribution of aftershocks, and this work was published in the prestigious journal Nature. However, Mignan and Broccardo[14] reanalyzed these findings by applying a two-parameter logistic regression model (equivalent to a single neuron) and achieved results identical to those in the original study[13]. Furthermore, they found that the accuracy of this deep and complex neural network was no greater than that provided by a simple empirical law. We believe that the main reason for this phenomenon lies in the disconnect between AI and the field of geophysics. Due to a lack of understanding of specialized knowledge in earthquake forecasting, researchers often focus solely on high scores from evaluation metrics and hastily claim the success of their models, overlooking the deeper principles of seismology and the challenges present in practical applications.\nSuch issues are not isolated cases. The study by DeVries et al. [13] is frequently cited primarily because it was published in one of the world's most prestigious journals, known for its highly selective peer review process. After reviewing 77 articles on the application of neural networks to earthquake prediction/forecasting from 1994 to 2019, Mignan and Broccardo[12] discovered that only 47% of these studies included a comparison to a baseline. Of those that did, 22% used a Poisson null hypothesis or randomized data as the baseline, while the remaining 78% compared their results to other machine learning methods utilizing the same features and data. In our survey extending that of Mignan and Broccardo[12] to 141 papers, we arrive at similar conclusions. Only 66.9% of works using AI to forecast earthquakes were compared to a baseline, most of which are other machine-learning methods or simple Poisson null hypothesis. When we want to demonstrate that a newly developed model surpasses existing limitations, we should compare our model with the best current models, including both the best AI models and the most advanced geophysical or statistical seismological models. However, most previous studies evaluate their work primarily from the perspective of AI research, focusing on the use of state-of-the-art AI technologies or the development of improved artificial neural networks. However, they often neglect to benchmark their models against existing, high-performing geophysical models, which is critical for demonstrating their practical value in earthquake forecasting.\nDue to this significant cognitive bias, Mignan and Broccardo[12] noted in their review that these artificial neural network (ANN) models do not appear to provide new insights into earthquake predictability, as they fail to offer convincing evidence that these models outperform simple empirical laws. In our extended investigation, we found that four studies from 2023 compared their models with reasonably strong geophysical models, six of which made comparisons with some versions of the general class of ETAS (epidemic type aftershock sequence) models. We will discuss these studies in detail later.\nIn summary, one of the biggest issues with previous research is the lack of comparison between their models and powerful benchmarks, a point that has been repeatedly emphasized in several past specialized studies[11, 15]."}, {"title": "2.4. Recommended resources for non-experts in earthquake forecasting", "content": "This section seeks to provide non-expert earthquake forecasters with essential resources to build a foundational understanding of statistical seismology and professional earthquake forecasting. Additionally, it introduces key Operational Earthquake Forecasting (OEF) platforms and relevant institutions, enabling newcomers to quickly integrate into the geophysics and statistical seismology communities.\nOur first recommendation is to start with the Community Online Resource for Statistical Seismicity Analysis (CORSSA), a platform specifically designed for students, beginners, as well as researchers in statistical seismology. It is presently in hiatus but it has been organized and authored by globally renowned statistical seismologists and earthquake forecasting experts, including Andrew J. Michael, Stefan Wiemer, Jiancang Zhuang and several other dedicated scientists. The platform provides educational resources, software tools, and best practices aimed at improving the application of statistical methods in earthquake science. CORSSA offers tutorials, articles, and guidance on how to apply these methods in real-world research. On this platform, users can learn about statistical seismology models such as the ETAS model, the Reasenberg and Jones model, as well as various de-clustering methods for earthquake catalogs, earthquake catalog completeness algorithms, professional earthquake forecasting model evaluation metrics and so on. For more detailed information, visit http://corssa.org/en/home/.\nWe also recommend the review paper titled \"Developing, Testing, and Communicating Earthquake Forecasts: Current Practices and Future Directions\" by Mizrahi et al. [11] on Operational Earthquake Forecasting. This comprehensive review is an essential resource for anyone involved in earthquake forecasting, risk mitigation, and public communication. Based on discoveries in statistical seismology, scientists have proposed numerous models, many of which have been applied to operational earthquake forecasting (OEF). OEF refers to the authoritative, near-real-time application of earthquake forecasting, with countries like Italy, New Zealand, and the United States already having operational systems in place, and more nations expected to follow. Key models used in OEF include the Reasenberg and Jones model[16], various versions of the Epidemic-Type Aftershock Sequence (ETAS) model[17], the Epidemic-Type Earthquake Sequence (ETES) model[18], the Short-Term Earthquake Probability (STEP) model[19, 20], the Every Earthquake a Precursor According to Scale (EEPAS) model, and ensemble models that integrate multiple approaches. Among these, sophisticated versions of the ETAS model has consistently demonstrated the best performance, particularly in aftershock forecasting. The review[11] serves as an invaluable guide for those seeking to engage in earthquake forecasting research. The review highlights the significance of transparency, benchmark comparisons, prospective testing, and reproducibility in developing effective earthquake forecast models. Furthermore, it emphasizes the critical importance of collaborating with end-users to ensure that earthquake forecast outputs are both socially relevant and practically applicable. Providing authoritative guidance on the subject, this review strikes a balance between scientific rigor and practical applications for risk management, making it an excellent frontier guide for professional earthquake forecasters.\nThis section concludes with an introduction to several established and emerging earthquake forecasting platforms, as well as research organizations dedicated to advancing earthquake forecasting. As Mizrahi et al. [11] already provide a detailed overview of the official Operational Earthquake Forecasting platforms, we will only give a brief introduction. The Italian OEF system generates forecasts but currently limits public access to its detailed models and code. Information may be available through authorized channels, but specific resources are not openly shared. The New Zealand system offers publicly accessible forecasts and has made efforts to communicate its findings. However, the underlying models may not be fully open-source. More information can be found on GNS Science. The United States Geological Survey (USGS) provides a range of earthquake-related data and resources, including aftershock forecasts. They also share some models and methodologies openly, which can be found on their website https://www.usgs.gov/programs/earthquake-hazards.\nSince its inception in California in 2007, the Collaboratory for the Study of Earthquake Predictability (CSEP)[21, 22] has been conducting forecast experiments in a variety of tectonic settings and at a global scale. It has been operating four testing centers on four continents to automatically and objectively evaluate models against prospective data. This research organization has been focused on earthquake forecasting, dedicated to assessing and validating the performance of earthquake forecasting models. It has promoted collaboration among different research teams by providing a unified testing framework and datasets, enhancing the scientific rigor and reliability of earthquake forecasting. Forecasters have been able to submit their forecasts to this platform, thus completing prospective testing. Its operations are scaling down as new initiatives emerge to modernize and streamline its relatively cumbersome testing infrastructure. See https://cseptesting.org.\nThe RichterX platform[23] is a real-time earthquake forecasting system that utilizes a version of the ETAS model [24] to construct scenarios and calculate forecast probabilities in adjustable space-time-magnitude windows. Designed for global accessibility, this platform integrates extensive historical seismic data with real-time monitoring to generate accurate forecasts of future earthquake events. It features a user-friendly interface that allows scientists, researchers, and the general public to access real-time predictions and alerts. Additionally, users can submit their own predictions to the Richter X platform and compete against the ETAS model; those who outperform the platform's model will be rewarded. The creators of the platform aim to further explore the potential of earthquake prediction through this competitive approach. The website for the Richter X platform is https://www.richterx.com.\nSince 1992, Vladimir G. Kossobokov and collaborators have been sharing intermediate-term, middle- and narrow-range earthquake predictions on a global scale through biannual monthly emails shared with a select group of approximately 150 experts in earthquake prediction research worldwide, using algorithms called M8 and and M8-MSc[25, 26]. The focus is on predicting earthquakes with magnitudes of 7.5 or greater and 8.0 or greater within predefined large spatial regions.\nAoyuX is an emerging online research platform focused on enhancing earthquake predictability in China. Developed by the Institute of Risk Analysis, Prediction, and Management (RisksX) at the Southern University of Science and Technology (SUSTech), in collaboration with multiple research institutions, including the China Earthquake Networks Center and Beijing Normal University, with support from the National Natural Science Foundation of China - Earthquake Science Joint Fund and the Special Fund of the China Seismic Experimental Site. In Chinese mythology, \"Aoyu\" is a legendary creature with a dragon's head and a fish's body, believed by ancient Chinese to cause earthquakes. The \"X\" represents the unknown and infinite in Western culture, symbolizing goals and hopes. The AoyuX platform aims to use cutting-edge statistical seismology tools for forward earthquake forecasting research. It will also serve as an online research platform for forward earthquake forecasting competitions with other models. In the future, the AoyuX platform will integrate geophysical observation data, artificial intelligence (AI). AoyuX will also create a visualization platform to improve communication between experts and the public, aiding collaborative efforts in monitoring and assessing major earthquakes. At the time of writing, AoyuX shares with Chinese experts a monthly earthquake forecasting report for the China Seismic Experimental Site of Sichuan-Yunnan region."}, {"title": "3 A quick review of AI algorithm used for earthquake forecasting", "content": "In Section 3.1, we present a concise overview of the AI algorithms and the principles behind them, as applied in previous studies. Section 3.3 explores the potential role of AI technology in earthquake forecasting, emphasizing its integration from a seismological perspective."}, {"title": "3.1. AI algorithms used in earthquake prediction", "content": ""}, {"title": "3.1.1. Traditional Machine Learning Models", "content": "Decision Tree. A decision tree [27] is a non-parametric supervised learning algorithm used for classification and regression tasks. It is structured hierarchically, consisting of a root node, internal nodes, and leaf nodes. The root node contains the entire sample set, internal nodes correspond to feature attribute tests, and leaf nodes represent the outcomes of the decisions. The construction of a decision tree typically involves three steps: feature selection, decision tree generation, and decision tree pruning. Feature selection aims to identify features that are highly correlated with the classification results, typically based on the information gain criterion. Decision tree generation begins after selecting the features. Starting from the root node, the algorithm computes the information gain for all features at the node and selects the feature with the highest information gain as the node's feature. Sub-nodes are then created based on the different values of this feature. This process is recursively applied to each sub-node until the information gain becomes negligible or there are no more features to select. Decision tree pruning is primarily aimed at combating \"overfitting\" by proactively removing certain branches to reduce the risk of overfitting. Common decision tree algorithms include the Iterative Dichotomiser 3 (ID3), the C4.5, and the Classification and Regression Tree (CART) algorithms.\nBuilding on the classical decision tree, many extensions have been developed. For instance, Random Forest, introduced by Leo Breiman in 2001 [28], is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or the mean prediction of the individual trees (regression). Another extension, Inverse Boosting Pruning Trees [29], proposed by Tong et al. in 2019, is an ensemble method that combines an improved AdaBoost algorithm with pruning decision trees for classification. Additionally, Boosted Decision Tree Regression, proposed by Jerome H. Friedman in 2001 [30], is a variant of boosting where the model iteratively focuses on reducing the residual errors of the predictions by combining multiple weak learners (in this case, decision trees) into a robust predictive model for regression tasks.\nSupport Vector Machines (SVM). SVM [31] is a supervised learning model used for classification and regression tasks. SVM operate by finding the optimal hyperplane that maximizes the margin between different classes in a dataset. The core components of SVM include support vectors, the hyperplane, and the margin. Support vectors are the data points that are closest to the hyperplane and influence its position and orientation. The hyperplane is a decision boundary that separates different classes, while the margin is the distance between the hyperplane and the nearest support vectors. According to the margin, SVM can be divided into hard margin SVM and soft margin SVM. The hard margin SVM is applicable when the data are linearly separable. It finds the hyperplane that not only maximizes the margin but also ensures that all data points are correctly classified without any errors. This approach, however, is very sensitive to outliers and may not be suitable for datasets with overlapping classes or noisy data. The soft margin SVM, introduced to handle non-linearly separable data, allows for some misclassifications to achieve a better overall model fit. It introduces slack variables to permit certain data points to lie within the margin or on the wrong side of the hyperplane. The degree of tolerance for misclassifications is controlled by a regularization parameter C. A higher value of C seeks to reduce misclassifications, potentially at the cost of a smaller margin, while a lower value of C allows more flexibility and a larger margin, possibly tolerating more classification errors.\nK-Nearest Neighbor (KNN). The KNN algorithm [32] is a simple yet powerful supervised learning method widely used in both classification and regression tasks. The fundamental principle of KNN is that similar data points tend to be close to each other in the feature space. When making predictions, KNN identifies the 'k' nearest neighbors to the query point and bases its prediction on these neighbors. In classification tasks, KNN assigns the query point to the most common class among the neighbors while, in regression tasks, it predicts the average value of the neighbors. The effectiveness of KNN is influenced by several key factors, such as the choice of distance metric, the choice of the 'k' value, and the distribution of the data. Common distance metrics include Euclidean, Manhattan, and Minkowski distances, each of which affects how similarity is assessed. The selection of 'k' is critical: a smaller 'k' may lead to overfitting, making the model overly sensitive to noise, while a larger 'k' can smooth predictions but may result in underfitting. Although KNN is straightforward and easy to implement, it can become computationally expensive when handling large datasets, particularly as the dimensionality increases. To address these challenges, dimensionality reduction techniques or data structures such as KD-Trees [33] are commonly employed.\nNa\u00efve Bayes. Na\u00efve Bayes [34] is a simple yet powerful family of probabilistic classifiers based on Bayes' theorem, which assumes that features are conditionally independent given the class label. Although this simplifying assumption often does not hold in reality, Na\u00efve Bayes classifiers still perform exceptionally well in practical applications, especially in high-dimensional data environments. The algorithm works by calculating the posterior probability of each class given a set of features and assigning the data point to the class with the highest posterior probability. This process involves using Bayes' theorem to update the probability estimate for each class as more features are considered, with the estimates being gradually refined. Na\u00efve Bayes is typically divided into three variants: Gaussian Na\u00efve Bayes for continuous data, Multinomial Na\u00efve Bayes for discrete data, and Bernoulli Na\u00efve Bayes for binary data. The algorithm is particularly well-suited for tasks involving large feature spaces, performs well even with small training datasets, and is robust to irrelevant features. However, its performance may be negatively impacted if the independence assumption is severely violated or if there are strong interactions between features.\nPolynomial Regression. Polynomial Regression [35] is an advanced form of linear regression that models the relationship between independent and dependent variables using an nth-degree polynomial. Unlike linear regression, which fits a straight line, polynomial regression allows for the fitting of a curve, enabling the capture of more complex, non-linear relationships within the data. The model parameters are typically estimated using the least squares method, which minimizes the sum of the squared differences between the observed values and the predicted values. The flexibility of polynomial regression makes it particularly useful when the data exhibits non-linear trends that a straight line cannot accurately represent. However, as the degree of the polynomial increases, the risk of overfitting also rises, meaning the model may start to fit the noise in the data rather than the true underlying pattern. To mitigate this issue, regularization techniques such as Ridge regression or Lasso regression can be applied. Therefore, careful selection of the appropriate polynomial degree is crucial to balance the model's complexity and its ability to generalize to new data.\nMixture of regressions. Mixture regression [36] is a probabilistic approach for modeling data generated by a combination of sub-populations within an overall population. It assumes that the dataset arises from K distinct regression functions, each representing a specific cluster. Data points are probabilistically assigned to these clusters, and a corresponding regression function is fitted to each cluster. This method combines clustering and regression into a unified framework, with parameters and cluster assignments iteratively refined using algorithms such as Expectation-Maximization (EM) to maximize the likelihood of the data. This approach captures heterogeneity by identifying distinct relationships within subsets of the data while accounting for the overlap between clusters. The flexibility of this method makes it applicable to both linear and non-linear regression models, allowing it to effectively handle scenarios where multiple generating processes underlie the observed data.\nLogistic Regression. Logistic Regression [37] is a widely used statistical method for binary classification problems. The core idea is to use the logistic function to map predicted values to a probability range between 0 and 1, thereby estimating the probability that a given input point belongs to a specific class. The model calculates a probability value through a linear combination of input features, representing the likelihood that the input belongs to the positive class. In implementing logistic regression, the Maximum Likelihood Estimation (MLE) method is commonly used to determine the model's parameters, maximizing the likelihood of the observed data. Once trained, the logistic regression model can be used to classify new data and output a predicted probability based on the input features. To prevent overfitting, L1 or L2 regularization can be introduced, which adds a penalty term to the loss function to constrain the magnitude of the coefficients.\nMultinomial Logistic Regression is an extension of logistic regression that can handle classification tasks with more than two categories. The principle behind it is to calculate a logistic regression model for each class and then use the softmax function to convert the outputs of these models into a probability distribution over the classes. Each probability represents the likelihood that the input data belongs to a particular class, and the model ultimately selects the class with the highest probability as the prediction result."}, {"title": "3.1.2. Deep Learning Models", "content": "Feed-forward Neural Network (FNN) . A FNN [38", "39": "is a deep learning model specifically designed for processing data with a grid-like topology, most commonly used for analyzing images and videos. CNNs effectively capture and extract spatial hierarchical features from data by utilizing a combination of convolutional layers, pooling layers, and fully connected layers. The core characteristics of CNNs include local connectivity, weight sharing, and down-sampling. The convolutional layer is the central component of a CNN, and it uses kernels that slide over the input data to extract local features such as edges and corners. By employing weight sharing, CNNs reduce model complexity while retaining essential feature information. Pooling layers are used to downsample feature maps, reducing the spatial dimensions of the data, thereby lowering computational costs and"}]}