{"title": "LLM+AL: Bridging Large Language Models and Action Languages\nfor Complex Reasoning about Actions", "authors": ["Adam Ishay", "Joohyung Lee"], "abstract": "Large Language Models (LLMs) have made significant\nstrides in various intelligent tasks but still struggle with\ncomplex action reasoning tasks that require systematic\nsearch. To address this limitation, we propose a method\nthat bridges the natural language understanding capabili-\nties of LLMs with the symbolic reasoning strengths of ac-\ntion languages. Our approach, termed LLM+AL, leverages\nthe LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's profi-\nciency in automated reasoning based on encoded knowledge.\nWe compare LLM+AL against state-of-the-art LLMs, includ-\ning CHATGPT-4, CLAUDE 3 OPUS, GEMINI ULTRA 1.0,\nand 01-PREVIEW, using benchmarks for complex reasoning\nabout actions. Our findings indicate that, although all meth-\nods exhibit errors, LLM+AL, with relatively minimal human\ncorrections, consistently leads to correct answers, whereas\nstandalone LLMs fail to improve even with human feedback.\nLLM+AL also contributes to automated generation of action\nlanguages.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have made significant\nstrides in various intelligent tasks (Brohan et al. 2023; Ko-\njima et al. 2022; Huang et al. 2023; Zeng et al. 2022; Yao\net al. 2024; Besta et al. 2024), yet they often struggle with\ncomplex reasoning about actions, particularly in problems\nthat demand systematic search. An emerging alternative is\nto use an LLM as a semantic parser to convert natural lan-\nguage into symbolic representations, such as Python pro-\ngrams (Gao et al. 2023; Nye et al. 2021; Olausson et al.\n2023), Planning Domain Definition Language (PDDL) (Liu\net al. 2023; Guan et al. 2023; Xie et al. 2023), and logic\nprograms (Ishay, Yang, and Lee 2023). These symbolic rep-\nresentations are then processed by dedicated symbolic rea-\nsoners.\nHowever, these methods have limitations. As demon-\nstrated in this paper, for complex reasoning tasks, LLMs al-\nmost always fail to generate Python programs for searching\nfor solutions, except in cases where the problem is a typical\nsearch task that LLMs may have memorized from the train-\ning corpus. Even in such instances, when small variations\nare introduced, LLMs struggle to adapt to the changes.\nUsing LLMs to generate PDDL could address a broader\nrange of action reasoning problems. However, PDDL is de-\nsigned as a standard language for task planners, thus many\nforms of knowledge about actions beyond task planning can-\nnot be effectively handled by this method.\nThis paper presents a novel method that bridges the nat-\nural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages (Gelfond\nand Lifschitz 1998; Giunchiglia et al. 2004). Our approach,\ntermed \"LLM+AL,\" leverages the LLM's strengths in se-\nmantic parsing and commonsense knowledge generation\nalongside the action language's proficiency in automated\nreasoning based on encoded knowledge.\nAction languages are particularly well-suited for this pur-\npose. They have an intuitive, natural language like syn-\ntax, feature formal semantics, and are supported by effi-\ncient computational tools. Action languages are designed for\nmore knowledge-intensive reasoning than PDDL, encom-\npassing not only task planning problems but also temporal\nprediction problems, which involve predicting what would\nhappen if a sequence of actions is executed, and postdic-\ntion problems, where one infers the initial state given the\ncurrent state and a past sequence of actions. Even when fo-\ncused solely on task planning problems, action languages of-\nfer greater expressivity, such as representing indirect effects\nto address the ramification problem (e.g., the banana's loca-\ntion is determined by the monkey's location if the monkey is\nholding it; thus, any action that moves the monkey will indi-\nrectly affect the banana's location), defaults (e.g., by default,\na pendulum swings back and forth unless it is being held),\nand additive fluents (e.g., the final balance is calculated by\nadding (or subtracting) the contributions of concurrent trans-\nactions). In particular, this paper leverages one of the latest\nmembers in this family, the action language BC+ (Babb and\nLee 2015, 2020) due to its simplicity and expressivity, as\nwell as the availability of the efficient BC+ reasoner called\nCPLUS2ASP (Babb and Lee 2013).\nThe LLM+AL pipeline leverages an LLM effectively\nacross multiple stages, each serving a different purpose.\nFirst, given a reasoning problem in natural language, we use\nan LLM to generate a program signature and extract com-\nmonsense and domain-specific knowledge. Next, the LLM\nis tasked with converting this knowledge into BC+ rules,\nguided by a prompt that details BC+ syntax and semantics"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 LLMs for Planning", "content": "Several works have proposed applying LLMs to planning\ntasks (Huang et al. 2022; Brohan et al. 2023; Huang et al.\n2023; Singh et al. 2023; Yao et al. 2023). For instance, Say-\nCan (Brohan et al. 2023) combines high-level actions with\nvalue functions, grounding LLMs in an environment. In-\nner Monologue (Huang et al. 2023) integrates environmen-\ntal feedback, including human feedback, into its pipeline,\nthereby enhancing robustness against agent errors. However,\nas noted in (Valmeekam et al. 2022, 2023), these methods\nstruggle with more complex planning tasks.\nOne approach to address these limitations is using an\nLLM as an interface for symbolic reasoning engines. This\nincludes generating executable Python code, as explored in\nrecent work where natural language and Python program\npairs are used to produce code for reasoning tasks (Olaus-\nson et al. 2023; Gao et al. 2023; Chen et al. 2023b; Lyu et al.\n2023; Singh et al. 2023). While this approach offloads much\nof the computation to the Python interpreter, it is not well-\nsuited for planning tasks that involve constraints that are not\neasily expressible in a procedural language.\nAnother alternative is the use of Planning Domain Defini-\ntion Language (PDDL) with LLMs. Some studies (Liu et al.\n2023; Xie et al. 2023) have focused on translating English\ninstructions into PDDL goals, assuming pre-existing PDDL\naction descriptions. Since only an instance file or goal needs\nto be generated, this setting is considerably simpler. Some\nrecent works embrace a human-in-the-loop approach with\nLLMs, using human feedback when constructing domain\nmodels and executing plans (Guan et al. 2023; Huang et al.\n2023; Yao et al. 2023). Closest to our approach, Guan et al.\n(2023) employed an LLM for generating PDDL descriptions\nbut noted that many manual corrections by PDDL experts\nwere necessary due to errors in GPT-4's translations, which\nimpacted their execution by a PDDL solver.\nA number of recent works show some success using\nLLMs to iteratively revise their own output, surpassing base-\nline LLM performance while bypassing expensive human\nfeedback (Madaan et al. 2024; Kim, Baldi, and McAleer\n2024). In particular, LLMs are well-suited for self-revision\nwhen they have access to external forms of feedback, such as\nexternal knowledge or tools (e.g., a code interpreter) (Kamoi\net al. 2024; Stechly, Valmeekam, and Kambhampati 2024b;\nGuan et al. 2023)."}, {"title": "2.2 Action Languages", "content": "Action languages, such as A (Gelfond and Lifschitz 1993),\nB (Gelfond and Lifschitz 1998), C (Giunchiglia and Lif-\nschitz 1998), C+ (Giunchiglia et al. 2004), BC (Lee and\nMeng 2013), and BC+ (Babb and Lee 2020), represent sub-\nsets of natural language specifically designed for describing\nactions and their effects. These languages are often viewed\nas high-level notations of answer set programs (Lifschitz\n2008; Brewka, Eiter, and Truszczy\u0144ski 2011), structured to\neffectively represent transition systems. Key research top-\nics in this field include the exploration of their expressive\npossibilities, such as indirect effects, triggered actions, de-\nfaults, and additive fluents (Giunchiglia et al. 2004; Gelfond\nand Lifschitz 1998; Lee and Lifschitz 2003; Inclezan and\nGelfond 2016). Such languages offer greater expressiveness\nthan PDDL, which has been well-studied in the literature\n(Eyerich et al. 2006; Jiang et al. 2019). Despite the rich body\nof research surrounding action languages, a significant chal-\nlenge remains: automation of action language generation,\nwhich we address in this paper.\nConstants in BC+ are categorized into 'fluent' and 'ac-\ntion' constants. For instance, in the Blocks World domain,"}, {"title": "3 Our Method", "content": "Our framework, as depicted in Figure 1, comprises four\nprincipal components: BC+ Signature Generation, English\nKnowledge Generation, BC+ Rule and Query Generation,\nand Self-Revision. The BC+ Signature Generation is re-\nsponsible for defining necessary symbols. English Knowl-\nedge Generation involves extracting and structuring rele-\nvant information from the natural language problem descrip-\ntion, while BC+ Rule Generation focuses on translating this\nstructured knowledge into formal BC+ rules, thereby bridg-\ning the gap between natural language understanding and\nsymbolic reasoning. Finally, Self-Revision iteratively refines\nthe BC+ signature, rules, and query, with feedback from the\nBC+ reasoner when run on a set of queries generated by the\nLLM. The result is a correct program (and a correct plan), or\na program requiring a typically small number of corrections\nto yield a correct plan."}, {"title": "3.1 Input", "content": "The input is a natural language description of the problem,\nincluding descriptions of types, objects, and actions involv-\ning them, along with a query in natural language. For ex-\nample, the input for the Missionaries and Cannibals puzzle\n(MCP) is given in Appendix A.1."}, {"title": "3.2 BC+ Signature Generation", "content": "Given the problem description in English, this step gener-\nates a signature in BC+ syntax. Writing such a BC+ program\ntypically starts with understanding the problem and consid-\nering the dynamics (knowledge) required, thinking about\nwhat fluent and action constants are useful, and then writ-\ning rules about them. We present the LLM with the problem\nand prompt it to generate important parts of the problem\nin natural language before signature generation. The Sig-\nnature Generation prompt (See Appendix B.1) contains an\nintroduction to BC+ and a few example translations of the\nEnglish description to important knowledge, an analysis of\nconstants and their natural language reading, and finally, a\nBC+ signature. Only the natural language reading of con-\nstants and signature are passed to the rest of the pipeline.\nFor the MCP puzzle, the generated signature can be found\nin Appendix A.2."}, {"title": "3.3 Knowledge Generation", "content": "We leverage an LLM to extract relevant knowledge from a\nproblem description to be used by the BC+ reasoner. This\nis achieved by using a prompt which includes instructions\nand few-shot example problem inputs, natural language\nreadings of constants, and signatures, paired with knowl-\nedge about some action domains. The knowledge generated\nbroadly falls into two categories, commonsense knowledge\nand domain-specific knowledge. Commonsense knowledge\nis about information not explicitly stated in the problem de-\nscription and usually is in the form of cause and effect. For\nexample, for the MCP domain, this step correctly generates\nthe commonsense knowledge \"crossing a vessel causes the\nlocation of the vessel to change\" and \"crossing a vessel\ncauses the number of a group at a location to decrease by\nthe amount of members on the vessel.\u201d Enumerating such\ncommonsense knowledge can be tedious and easy to miss.\nThus, we find it useful to use an LLM for this task. Domain-\nspecific knowledge is directly tied to the given information\nabout the problem, for instance, \u201cMissionaries should not\nbe outnumbered by cannibals, or they will be eaten.\u201d Our\nprompt examples are designed so that each knowledge sen-\ntence is translated into a causal law in BC+. The full prompt\nfor extracting knowledge is shown in Appendix B.2. The"}, {"title": "3.4 Rules and Query Generation", "content": "Further leveraging the capabilities of LLMs, we use them\nto convert natural language into a symbolic representation.\nThis is done with a prompt that contains a brief introduction\nof action language BC+ and few-shot examples of natural\nlanguage knowledge translated into BC+ rules. This prompt,\ntogether with the natural language reading of constants, sig-\nnature, and English knowledge generated in the previous\nsteps, is given as input to the LLM, which then generates\nBC+ causal laws. The full prompt is given in Appendix B.3.\nFor example, the knowledge, crossing a vessel causes its\nlocation to change is turned into:\n$\\cross (V) \\text{ causes loc(V)=Loc if going (V) =Loc.}$"}, {"title": "3.5 Self-Revision", "content": "The programs generated so far may have errors, either syn-\ntactically or semantically. This step leverages an LLM to\nrevise the generated BC+ program based on the reasoner's\noutput.\nSatisfiability Check. First, without considering any ini-\ntial or goal state, the pipeline performs a satisfiability check,\nwhich runs the BC+ reasoner to ensure if the generated BC+\nsignature and rules (without a query) are satisfiable. Typical\nerrors detected in this step are syntax errors and reported by\nthe BC+ reasoner. Based on the output message of the BC+\nreasoner (e.g., syntactic restrictions on some causal laws are\nnot observed), the LLM is prompted to update the signature\nand/or rules accordingly. This step repeats until either they\nare satisfiable, in which case the pipeline proceeds to Sam-\nple Query Generation, or the maximum number of allowed\nrevisions is reached, in which case the pipeline ends on this\nstep, settling on the current signature, rules, and main query.\nSample Query Generation. Next, the pipeline prompts the\nLLM to generate a small set of simple sample queries. These\nqueries will later serve to check that the program correctly\nimplements domain-specific rules and constraints (e.g., ac-\ntions lead to expected changes in the state, preconditions for\nactions are respected, etc.). The LLM is instructed to ap-\npend either \"(satisfiable)\" or \"(unsatisfiable)\", depending on\nwhether the query is expected to be satisfiable or unsatisfi-\nable based on the LLM's discretion. For example, a sample\nquery for the Missionaries and Cannibals puzzle could in-\nvolve an action which is is expected to be disallowed, such\nas crossing on the boat with more than the allowed capacity,\nwhich would be appended with \"(unsatisfiable)\".\nSample and Main Query Feedback. The pipeline auto-\nmatically executes each sample query and the main query\nusing the BC+ reasoner, and the outputs are provided to\nthe LLM. The LLM is then tasked with verifying that the\noutputs align with the domain and revising the BC+ sig-\nnature, causal laws, main query, and/or sample queries as\nneeded. If no changes are required or the maximum num-\nber of revisions is reached, the resulting BC+ program and\nthe BC+ reasoner's output are finalized. Otherwise, the pro-\ncess repeats. See Appendix A.6. for an example of the Self-\nRevision step."}, {"title": "4 Experiments", "content": "Benchmarks. We consider benchmarks focusing on com-\nplex reasoning. The first set is from (McCarthy 1998), where\nMcCarthy proposed several variations of the well-known\nMissionaries and Cannibals puzzle. The second set consists\nof several well-known puzzles along with our own varia-\ntions.\nBaselines. For the baseline LLMs, we use CHATGPT-4,\nCLAUDE 3 OPUS, GEMINI 1.0 ULTRA, and 01-PREVIEW.\nThese baseline models are provided with problem descrip-\ntions in natural language, as outlined in Section 3.1, and\ntasked with finding a solution. Additionally, we evaluate"}, {"title": "4.1 Experiment Results", "content": "Benchmark Performance. As shown in Table 1, the base-\nline LLMs perform poorly on the MCP elaboration prob-\nlems. Neither CLAUDE 3 OPUS nor GEMINI ULTRA 1.0\nsolves any MCP problems correctly, while CHATGPT-4\nsolves only three. 01-PREVIEW does better, solving 7 prob-\nlems correctly. CHATGPT-4+CODE produces correct plans\nfor 5 elaborations and occasionally generates solutions that\nare mostly correct but include minor issues that can be easily\nfixed manually. These cases are denoted with A in Table 1.\nLLM+AL2 automatically solves 7 MCP problems. For the re-\nmaining problems, an average of 3.1 manual corrections to\nthe generated BC+ program are required to produce correct\nplans.\nWe further include superficial variations of standard puz-\nzles and observe that, even for these simple elaborations, the\nresults are similar, as shown in Table 2. These variations in-\nvolve minor changes to the initial states (e.g., in Tower of\nHanoi variations, disks are distributed among pegs in no par-\nticular order). The baseline LLMs perform poorly on these\nvariations, with CHATGPT-4 and 01-PREVIEW solving 1\nand 2 variations correctly, respectively. While CHATGPT-\n4+CODE performs better than the baseline LLMs, it still\nstruggles with these variations. In contrast, LLM+AL out-\nperforms the baseline LLMs, and for the problems it fails\nto solve correctly, an average of only 2.2 corrections are re-\nquired to produce the correct output.\nEffectiveness of Self-Revision. Self-Revision provides a\nnotable improvement in the quality of the generated BC+\nprograms. Across all 30 problems in Tables 1 and 2, only\n22.3% (7/30) of programs generated prior to the Self-\nRevision step are executable without syntax errors. This per-\ncentage increases substantially to 86.7% (26/30) after the\nSelf-Revision step. Similarly, the proportion of programs\nthat produce correct answers when run through the BC+ rea-\nsoner rises from 16.6% (5/30) before Self-Revision to 50%\n(15/30) afterward. In terms of issues requiring correction,\nthere are 70 issues in the BC+ programs prior to the Self-\nRevision step, but this number decreases to 42 following"}, {"title": "4.2 Analysis", "content": "LLMs struggle to consistently adhere to state con-\nstraints. Of the 17 problems in Table 1, 14 are solv-\nable, meaning they have valid plans to reach the goals.\nCLAUDE 3 OPUS and GEMINI 1.0 ULTRA fail to solve any\nof them correctly, while CHATGPT-4 produces 11 incor-\nrect solutions. Among these incorrect plans, the state con-\nstraint-mandating that missionaries must not be outnum-\nbered by cannibals-is frequently violated, despite clear in-\nstructions to adhere to it. Notably, 89.7% (35 out of 39)\nof these violations occur within the first three steps of the"}, {"title": "5 Conclusion", "content": "We propose LLM+AL, a framework that bridges LLMs with\naction languages, enabling them to complement each other.\nCompared to the direct use of LLMs, LLM+AL achieves\nmore robust and accurate reasoning about actions by lever-\naging the expressiveness and formal reasoning capabilities\nof action languages. While the generation of action language\ndescriptions traditionally requires human expert knowledge,\nLLM+AL simplifies this process through an automated pro-\ncess. Additionally, we employ a Self-Revision mechanism,\nan iterative approach in which an LLM generates sample\nqueries to test the correctness of its previously generated\nBC+ program. Based on feedback from the BC+ reasoner,\nthe LLM revises its program, significantly improving the\nquality of the final output. While some mistakes may persist\nin the final programs, the generative capabilities of LLMs\nmake creating action descriptions significantly easier com-\npared to crafting them from scratch. It is likely that future\nLLM improvements will further reduce such errors. More-\nover, fine-tuning LLMs could further enhance the perfor-\nmance of LLM+AL provided it is feasible."}]}