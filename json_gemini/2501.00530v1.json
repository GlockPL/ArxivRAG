{"title": "Superposition in Transformers: A Novel Way of Building Mixture of Experts", "authors": ["Ayoub Ben Chaliah", "Hela Dellagi"], "abstract": "Catastrophic forgetting remains a major challenge when adapting large language models (LLMs) to new tasks or domains. Conventional fine-tuning often overwrites existing knowledge, causing performance degradation on original tasks. We introduce Superposition in Transformers, a novel architecture that leverages autoencoders to superimpose the hidden representations of a base model and a fine-tuned model within a shared parameter space. By using B-spline-based blending coefficients and autoencoders that adaptively reconstruct hidden states based on the input data distribution, our method effectively mitigates catastrophic forgetting and enables a new paradigm of \"in-model\" superposition. This approach preserves original model capabilities while allowing compact domain-specific expertise to be added, and it supports dynamic switching between model states during inference.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) such as GPT-3[1] and GPT-4[2] have shown remarkable performance on various language tasks. However, adapting these models to new tasks or domains often leads to catastrophic forgetting, where newly learned information overwrites older knowledge [3]. This challenge becomes critical in continual learning or domain adaptation settings, where preserving performance on original tasks is essential [4].\nMixture of Experts (MoE) [5] models mitigate catastrophic forgetting by delegating different distributions or tasks to distinct experts. However, MoE solutions typically require large parameter expansions and introduce complex gating. In contrast, we propose a more parameter-efficient method: we blend the hidden states of two models a base model and a fine-tuned model using trainable blending coefficients derived from B-splines. We then reconstruct the original state from the blended weights via the autoencoders at inference time, allowing each model's states to co-exist in a superposed fashion with minimal interference."}, {"title": "1.1 Contributions", "content": "\u2022 Autoencoder-Based Superposition: We demonstrate how autoencoders can reconstruct hidden states from either the base or fine-tuned model, effectively gating the representation space to prevent catastrophic forgetting.\n\u2022 Jointly Learned B-Spline Blending: By training blending coefficients jointly with the autoencoders, we find a balance that preserves each model's capabilities while introducing minimal new parameters.\n\u2022 Parameter Efficiency and Practicality: This approach retains the majority of each model's parameters in frozen form and only learns relatively small auxiliary modules, making it feasible for real-world use."}, {"title": "1.2 Motivation and Positioning", "content": "Most existing mixture-of-experts methods expand parameter count significantly by introducing distinct expert modules and gating networks [6, 7]. Parameter-efficient fine-tuning approaches such as Adapters [8] or LORA [9] often rely on modifying or appending new weights to the base model. In contrast, our method produces a single merged set of parameters-via B-spline blending\u2014while training autoencoders to reconstruct the original hidden states on demand. During training, both the base and fine-tuned model parameters are accessed in a frozen manner to generate hidden states, but in the end, only the new blended model parameters (plus the autoencoders) remain. This design merges the capabilities of two experts into one compact parameter space and mitigates catastrophic forgetting by preserving each model's knowledge in a superposed, reconstructable form."}, {"title": "2 Background and Related Work", "content": null}, {"title": "2.1 Mixture of Experts", "content": "Mixture of Experts (MoE) dynamically allocates different experts to different inputs, improving capacity and specialization. While effective, MoE methods often increase parameter count and rely on gating networks [7]. Our method similarly aims to leverage multiple \"expert\" models but does so via a shared parameter space and lightweight autoencoder modules."}, {"title": "2.2 Parameter-Efficient Transfer Learning", "content": "Recent techniques like Adapters [8] and LoRA [9] reduce the cost of fine-tuning large models by injecting small trainable components. We differ in that we freeze both the base and fine-tuned models; instead of adding new layers directly on top of them, we blend their hidden states and reconstruct them adaptively. This approach can be viewed as orthogonal to Adapters or LORA, potentially combining well with these methods."}, {"title": "2.3 Superposition and Polysemantic Neurons", "content": "In neural networks, superposition refers to efficiently encoding multiple features or tasks within the same parameter space [10]. Large LLMs often exhibit polysemantic neurons-units that respond to multiple unrelated concepts [11]. Our approach further promotes polysemanticity by forcing the network to encode the states of two distinct models in overlapping neurons, constrained by autoencoder bottlenecks."}, {"title": "2.4 Neural Network Compression and Model Merging", "content": "Prior methods for merging or compressing models [12, 13] generally aim to reduce memory overhead. By contrast, our framework focuses on preserving both original models in a single parameter space without over-writing. The added overhead is small (primarily the autoencoders and the blending coefficients) compared to training or storing two separate models."}, {"title": "3 Proposed Method", "content": null}, {"title": "3.1 Overview", "content": "We merge a base model $M_{base}$ and a fine-tuned model $M_{fine}$ into a single \u201csuperposed\u201d model $M_{merged}$. To do this, we:"}, {"title": "3.2 Blending Hidden States Using B-Splines", "content": null}, {"title": "3.2.1 Motivation", "content": "Averaging or naively mixing model weights can degrade performance, as it does not adapt to which features are critical to each model. Instead, we blend hidden states directly, allowing each layer to remain mostly intact (frozen). The a-values are learned with a mechanism that encourages smooth transitions across layers."}, {"title": "3.2.2 Formulation", "content": "For layer l, let $h^{base}$ and $h^{fine}$ be the hidden states from the base and fine-tuned models, respectively. We define\n$h_{l} = (1 - \\alpha(l)) h^{base} + \\alpha(l) h^{fine}$,\nwhere\n$\\alpha(1) = clamp (\\sum_{i=1}^{N} c_{i} B_{i,k}(1) + b_{l}, 0, 1)$.\n\u2022 ${c_{i}}$ are trainable control points for a B-spline of degree k.\n\u2022 $b_{l}$ is a layer-specific bias term.\n\u2022 The B-spline basis functions $B_{i,k}(\u00b7)$ ensure smoothly varying $\\alpha(l)$.\nWe freeze the weights of both $M_{base}$ and $M_{fine}$, focusing on learning only ${c_{i}}$, ${b_{l}}$, and the autoencoders."}, {"title": "3.3 Merged Model Architecture", "content": null}, {"title": "3.3.1 Merging Post-Training", "content": "After training, one option is to produce a single model whose parameters reflect a final hard merge of the two original models:\n$\\theta_{l} = (1 \u2013 \\alpha(l)) \\theta^{base} + \\alpha(l) \\theta^{fine}$,\nwhere $\\theta^{base}$ and $\\theta^{fine}$ are the layer parameters. This yields a standalone model with minimal overhead. Alternatively, one may keep the B-spline plus autoencoders as a gating system that dynamically adapts to inputs."}, {"title": "3.3.2 Forward Pass with Merged Model", "content": "During inference, the model computes hidden states for each layer via blended embeddings and self-attention (incorporating $\\alpha(l)$) and optionally applies an autoencoder to refine $h_{l}$, reconstructing the hidden states that best suit the input's domain. This structure preserves each model's domain knowledge while introducing minimal overhead."}, {"title": "3.4 Autoencoders for State Reconstruction", "content": null}, {"title": "3.4.1 Architecture", "content": "At each selected layer 1, an autoencoder compresses the blended hidden state $h_{l}$ into a bottleneck and reconstructs it:\n$z_{l} = Encoder(h_{l}), \\hat{h}_{l} = Decoder(z_{l})$.\nWe train it to match $\\hat{h}$ with either $h^{base}$ or $h^{fine}$. We skip embedding layers and the final layer norm to maintain alignment with the original model heads."}, {"title": "3.4.2 Minimizing Information Loss", "content": "Because $\\alpha$ and the autoencoder parameters are trained jointly, $\\alpha$ naturally adjusts so that the autoencoder can accurately reconstruct either $h^{base}$ or $h^{fine}$. This synergy ensures that if reconstruction error is high, $\\alpha(l)$ shifts to a region that better preserves salient features from the respective domain."}, {"title": "3.4.3 Role of Autoencoders in Encouraging Polysemanticity", "content": "Each autoencoder's bottleneck forces the network to encode crucial details from both models within a limited dimension. This encourages polysemantic neurons, since the same hidden units may now carry signals relevant to both tasks. A narrower bottleneck heightens polysemantic pressure but can also degrade domain-specific detail; a wider bottleneck reduces polysemantic pressure but can maintain more domain specificity."}, {"title": "3.4.4 Extending to a 2D-alpha Model (Optional)", "content": "One can extend $\\alpha(l)$ to a vector for per-dimension blending. In this scenario, local features (extracted by convolution layers) and global features (captured by a low-rank adapter) are combined in the autoencoder. While more expressive, this 2D-alpha approach also increases complexity and may demand careful tuning to avoid overfitting."}, {"title": "4 Training Procedure", "content": null}, {"title": "4.1 Objectives", "content": "We optimize two core objectives:\n\u2022 $L_{LLM}$: Standard language modeling loss, preserving fluency and coherence.\n\u2022 $L_{Recon}$: Reconstruction error between $\\hat{h}$ and the target hidden state $h^{base}$ or $h^{fine}$.\nAn additional alpha regularization term $L_{alphaReg}$ may be applied to keep $\\alpha$ smooth or centered."}, {"title": "4.2 Loss Functions", "content": "We generally use:\n$L = \\lambda_{LM} L_{LLM} + \\lambda_{Recon} L_{Recon} + \\lambda_{Alpha} L_{AlphaReg}$,\nwhere $\\lambda_{LM}$, $\\lambda_{Recon}$, and $\\lambda_{Alpha}$ balance the relative importance of each component. $L_{Recon}$ is typically Mean Squared Error (MSE) or L2 distance over the hidden states."}, {"title": "4.3 Optimization", "content": "We freeze all parameters of $M_{base}$ and $M_{fine}$. The trainable variables are the B-spline control points ${c_{i}}$ and biases ${b_{l}}$ as well as the autoencoder weights and biases per selected layer.\nBy iterating through mini-batches, we compute hidden states from both models (frozen), blend them via $\\alpha(l)$ to produce $h_{l}$ and use an autoencoder to reconstruct $h_{l}$ into either $h^{base}$ or $h^{fine}$. We then backpropagate the reconstruction error to update $\\alpha$ and the autoencoder parameters. The final forward pass for language modeling also contributes to $L_{LLM}$, ensuring that fluency is maintained."}, {"title": "4.4 Role of Labels", "content": "Labels indicating whether an input belongs to the base or fine-tuned domain can guide which hidden state the autoencoder targets. While optional in principle, these labels can significantly speed up convergence and improve accuracy of reconstruction for each domain."}, {"title": "5 Experiments and Results", "content": "In this section, we evaluate the proposed merged model, by integrating a base GPT-2 model and a fine-tuned GPT-2 model trained on French text. We focus on demonstrating how the autoencoders enable the superposition of transformer blocks from different fine-tunes, allowing the merged model to effectively combine representations from both models. Our analysis includes performance metrics, hidden state reconstruction and the emergence of polysemantic neurons."}, {"title": "5.1 Experimental Setup", "content": "We compare the following models:\n\u2022 Base Model: GPT-2 trained on English data.\n\u2022 Fine-Tuned Model: GPT-2 fine-tuned on a French corpus.\n\u2022 Merged Model: Combines the base and fine-tuned models using layer-wise blending with learned $\\alpha$ values and incorporates autoencoders to enable superposition."}, {"title": "7 Conclusion", "content": "The proposed method represents a significant advancement in enabling large language models (LLMs) to integrate knowledge from multiple domains or tasks without sacrificing foundational capabilities or requiring extensive parameter growth. By leveraging autoencoders and blending mechanisms, the approach facilitates efficient knowledge integration making it a practical and resource-efficient enhancement to LLMs.\nExperimental results highlight improvements in perplexity reduction, reconstruction accuracy, and hidden state alignment with base and fine-tuned models. Techniques like t-SNE visualizations further validate the model's ability to adapt representations dynamically. This adaptability enables seamless processing of diverse inputs and enhances multi-domain integration.\nBeyond merging two models, this method promises broader applications, such as multilingual processing, seamless language switching, and integrating symbolic reasoning with domain knowledge.\nLooking ahead, this approach paves the way for the development of models capable of integrating diverse skills and knowledge domains, representing a step toward more flexible, powerful, and general-purpose AI systems. The potential applications are vast and exciting."}, {"title": "Example Scenario:", "content": "* Input: \"Solve for x: 2x + 3 = 7. Then, explain the significance of the solution in real-world applications.\u201d\nExpected Behavior:\n* First Part (Symbolic Reasoning): The model uses the symbolic reasoning experts to solve the equation.\n* Second Part (Domain Explanation): The model switches to the domain knowledge expert to provide an explanation."}, {"title": "B.1 Loss Functions", "content": "The losses involved in the different architectures combines:\nReconstruction Loss $L_{Recon}$ This loss measures how well the autoencoders reconstruct the blended hidden states. It uses both Mean Squared Error (MSE) and the L2 distance between the reconstructed hidden states and the target hidden states.\n$L_{Recon} = \\sum_{l=1}^{L}[ \\lambda_{MSEE} ||h_{l} - h_{l}^{target} ||^{2} + \\lambda_{L2} ||h_{l} - h_{l}^{target} ||^{2}]$\nL represents the number of layers in the transformer model.\nLanguage Modeling Loss $L_{LM}$ This loss measures the standard language modeling objective using cross-entropy between the predicted logits and the next token in the sequence.\n$L_{LM} = E [\\sum_{t} log P_{\\theta}(w_{t}|w_{<t})]$\nAlpha Regularization Loss $L_{AlphaReg}$ The alpha regularization loss encourages the control points of the B-spline-based blending coefficients to adhere to desirable properties. It ensures smooth and interpretable blending, avoiding overfitting to noisy representations. It includes three components:"}, {"title": "Smoothness Loss:", "content": "Ensures that adjacent control points are close to each other, minimizing abrupt changes in blending behavior.\n$L_{Smoothness} = \\sum_{i=1}^{N-1} ||c_{i} - c_{i-1}||^{2}$\nN represents the number of control points in the B-spline interpolation."}, {"title": "Centrality Loss:", "content": "Penalizes deviations of the control points from a central value (e.g., 0).\n$L_{Centrality} = \\sum_{i=1}^{N} ||c_{i}||^{2}$"}, {"title": "Mean Bias Loss:", "content": "Penalizes deviations of the mean layer-wise bias from zero to encourage balanced adjustments across layers.\n$L_{MeanBias} = \\mu_{l}^{2}$\nwhere $\\mu_{l}^{2}$ is the mean of the layer-wise bias."}, {"title": "Variance Bias Loss:", "content": "Encourages the variance of the layer-wise bias to match a desired target variance $\\sigma_{target}$.\n$L_{VarianceBias} = (\\sigma_{l}^{2} \u2013 \\sigma_{l}^{target})^{2}$\nwhere $\\sigma_{l}^{2}$ is the variance of the layer-wise bias.\nThe total alpha regularization loss is:\n$L_{AlphaReg} = \\lambda_{Smoothness} L_{Smoothness}+\\lambda_{Centrality} L_{Centrality}+\\lambda_{MeanBias} L_{MeanBias}+\\lambda_{VarianceBias} L_{VarianceBias}$"}, {"title": "The Total Losses", "content": "The total 1D loss combines the language modeling loss and the reconstruction loss. Each component is weighted to balance their contributions during training.\n$L_{1D} = \\lambda_{Recon} L_{Recon} + \\lambda_{LM} L_{LM}$\nThe total 2D loss combines all the aferomentionned losses weighted to balance their contributions during training.\n$L_{2D} = \\lambda_{Recon} L_{Recon} + \\lambda_{LM} L_{LM} + \\lambda_{AlphaReg} L_{AlphaReg}$"}]}