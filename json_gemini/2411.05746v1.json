{"title": "CONTINUOUS-TIME ANALYSIS OF ADAPTIVE OPTIMIZATION AND NORMALIZATION", "authors": ["Rhys Gould", "Hidenori Tanaka"], "abstract": "Adaptive optimization algorithms, particularly Adam and its variant AdamW, are fundamental components of modern deep learning. However, their training dynamics lack comprehensive theoretical understanding, with limited insight into why common practices\u2014such as specific hyperparameter choices and normalization layers-contribute to successful generalization. This work presents a continuous-time formulation of Adam and AdamW, facilitating a tractable analysis of training dynamics that can shed light on such practical questions. We theoretically derive a stable region for Adam's hyperparameters (\u03b2, \u03b3) that ensures bounded updates, empirically verifying these predictions by observing unstable exponential growth of parameter updates outside this region. Furthermore, we theoretically justify the success of normalization layers by uncovering an implicit meta-adaptive effect of scale-invariant architectural components. This insight leads to an explicit optimizer, 2-Adam, which we generalize to k-Adam-an optimizer that applies an adaptive normalization procedure k times, encompassing Adam (corresponding to k = 1) and Adam with a normalization layer (corresponding to k = 2). Overall, our continuous-time formulation of Adam facilitates a principled analysis, offering deeper understanding of optimal hyperparameter choices and architectural decisions in modern deep learning.", "sections": [{"title": "1 CONTINUOUS-TIME FORMULATION OF ADAM", "content": "In this section we present a continuous-time formulation of the Adam (and AdamW) optimizer which forms the theoretical foundation for the rest of the paper. We derive a continuous-time expression for the Adam gradient update in Section 1.1 (utilized in Section 2 to derive a condition for bounded updates), and in Section 1.2 we formulate Adam as a second-order differential equation (used in Section 3 to interpret the implicit effect of scale invariance and motivate the k-Adam optimizer). We provide brief descriptions of relevant optimizers (momentum, RMSprop, Adam, AdamW) in Appendix A. Experimental details are left to Appendix F.\nNotation. We write $||x|| := \\sqrt{\\sum_i x_i^2}$ and $||x||_\\infty := \\max_i |x_i|$ for $x \\in \\mathbb{R}^P$, and denote the inner product $(x, y) = \\sum_i x_i y_i$. We denote elementwise squaring by $x^2$. We consider a loss function $L : \\Theta \\rightarrow \\mathbb{R}$ where $\\Theta \\subset \\mathbb{R}^P$, and $\\theta_n \\in \\Theta$ denotes a model's parameters after n discrete updates, with corresponding gradient $g_n := \\nabla_\\theta L(\\theta_n)$, and initial parameter $\\theta_0$. We will write $g_{0:n} \\equiv (g_0,..., g_n)$."}, {"title": "1.1 PARAMETER UPDATE IN CONTINUOUS-TIME", "content": "Neglecting weight decay for the moment (i.e. \\( \\lambda = 0 \\)), Adam and AdamW possess the discrete-time update rule\n\\[\n\\theta_{n+1} = \\theta_n - \\eta u_n, \\text{ where } u_n := \\frac{\\sqrt{1-\\beta_n+1}}{\\sqrt{1-\\gamma_n+1}} \\frac{m_n}{\\sqrt{v_n}}\n\\tag{1}\n\\]\nwith learning rate \\( \\eta \\), and adaptive hyperparameters (\u03b2, \u03b3) \u2208 [0,1]2 with corresponding moving-averages\n\\[\nm_n := \\gamma m_{n-1} + (1 - \\gamma)g_n, \\qquad v_n := \\beta v_{n-1} + (1 - \\beta)g_n^2\n\\]\nwhere (m\u22121, v\u22121) := (0,0). We say that un is an adaptive-normalization of the gradients $g_{0:n}$ (see Appendix N for more details).\nWe will work in continuous-time, at a timescale of \\( n^p \\), such that e.g. mn = m(tn), with tn := n\u03b7 for some p\u2208 R (typically p = 1). We can then derive continuous-time expressions for the moving-averages mn and vn (details left to Appendix C), which, to leading order in n\u03b7, take the form\n\\[\nm(t) = \\int_0^t d\\tau K_\\gamma(\\tau, t)g(\\tau), \\qquad v(t) = \\int_0^t d\\tau K_\\beta(\\tau, t)g(\\tau)^2,\n\\tag{2}\n\\]\n\\[\nK_\\alpha(\\tau, t) := \\frac{1-\\alpha}{\\eta^p \\alpha} exp\\left(-\\frac{1-\\alpha}{\\eta^p \\alpha}(t - \\tau)\\right)\n\\]"}, {"title": "1.2 CONTINUOUS PARAMETER DYNAMICS VIA DIFFERENTIAL EQUATION", "content": "From Equation (1) we can also determine \\(\\theta_n = \\theta(t_n)\\) in continuous-time via a governing differential equation (details left to Appendix C). To second-order, AdamW with weight decay \\(\\lambda\\) is governed by the differential equation\n\\[\n\\lambda \\eta \\theta(t) + \\frac{\\eta^2 p}{2} \\ddot{\\theta}(t) + \\dot{\\theta}(t) = -\\eta u(t)\n\\tag{4}\n\\]\nIn combination with the continuous-time expression for u(t) (Equation (3)), we can numerically solve Equation (4) (method described in Appendix G) to obtain a continuous-time parameter trajectory \u03b8(t). We demonstrate that the continuous-time trajectory (for timescale p = 1) associated with Equation (4) closely agrees with the true discrete-time trajectory in Figure 1. In Section 3 we will use Equation (4) to interpret the implicit effect of scale invariance."}, {"title": "2 THEORY OF ADAPTIVE HYPERPARAMETERS", "content": "We now begin to apply the continuous-time framework presented in the previous section to understanding aspects of adaptive optimization from a theoretical perspective. In this section we will present a theory-driven account of adaptive hyperparameter choice for Adam/AdamW, understanding the values of (\u03b2, \u03b3) that result in stable training and effective generalization.\nFirst we will use the continuous-time expression for Adam's update (Equation (3)) to derive a theoretical region of hyperparameter space B+ for which updates are provably bounded (Section 2.1). We will then empirically verify that this region indeed exhibits stable training, with unstable training in the complementary region B- (exhibiting a predictable exponential growth) (Section 2.2), and observe how generalization performance varies in the regions B+ and B- (Section 2.3)."}, {"title": "2.1 DERIVING A CONDITION FOR BOUNDED UPDATES", "content": "The continuous-time expression for Adam's update (Equation (3)) has an immediate consequence in regards to bounding the max-update $||u_n||_\\infty = ||\\theta_{n+1} - \\theta_n||_\\infty/\\eta$. The max-update quantifies the maximal parameter change (across all parameters) at a given step, hence upper bounds on this quantity also hold identically for the parameter change of any arbitrary parameter. We have the result,\nMax-update bound. The max-update can be bounded as\n\\[\n||u_n||_\\infty \\le \\frac{\\sqrt{1 - \\beta \\eta+1}}{\\sqrt{1 - \\gamma \\eta+1}} \\sqrt{\\frac{\\beta}{1-\\beta}} \\sqrt{\\frac{1-\\gamma}{\\gamma}} B_\\eta(\\beta, \\gamma)\n\\tag{5}\n\\]\nwhere\n\\[\nB_\\eta(\\beta, \\gamma) := \\begin{cases}\n1/\\sqrt{C(\\beta, \\gamma)}, & C(\\beta, \\gamma) > 0, \\\\\n\\sqrt{\\eta}, & C(\\beta, \\gamma) = 0, \\\\\n\\exp(\\eta |C(\\beta, \\gamma)|/2)/\\sqrt{|C(\\beta, \\gamma)|}, & C(\\beta, \\gamma) < 0\n\\end{cases}\n\\]\ndefining \\( C(\\beta, \\gamma) := (2\\beta(1 - \\gamma) - \\gamma(1 - \\beta))/\\beta \\gamma \\).\nWe highlight that this bound is only possible because of the specific form of Adam's update u(t): a moving-average of g divided by the square-root of a moving-average of g2 (Equation (3)), i.e. an adaptive-normalization of g (see Appendix N), which allows us to apply the Cauchy-Schwarz inequality. We can therefore view this result as a theoretical justification for the form of Adam's update. We leave the derivation of this bound to Appendix C.\nWe define the bounded-update region\n\\[\nB_+ := \\{(\\beta, \\gamma) : C(\\beta, \\gamma) > 0\\},\n\\]\nand the complementary region\n\\[\nB_- := \\{(\\beta, \\gamma) : C(\\beta, \\gamma) < 0\\}\n\\]\nFrom Equation (5), we can bound the max-update by a constant independent of n when (\u03b2, \u03b3) \u2208 B+. Outside of B+, the bound depends on n and diverges over training as n \u2192 \u221e. This is suggestive that we may observe stable training when (\u03b2, \u03b3) \u2208 B+, and that the max-update may grow exponentially (at a rate/exponent proportional to |C(\u03b2, \u03b3)|) when (\u03b2, \u03b3) \u2208 B\u2212. Indeed, we will verify this phenomena empirically in Section 2.2. We comment on the case (\u03b2, \u03b3) \u2208 B0 in Appendix D. It is easy to show that \u03b2 > \u03b3 is a sufficient condition for (\u03b2, \u03b3) \u2208 B+, meaning that the choice of hyperparameters typically chosen in practice - e.g. the PyTorch default values (\u03b2, \u03b3) := (0.999, 0.9) \u2013 lie within B+.\nFor the following, we define the level curves\n\\[\nB_c := \\{(\\beta, \\gamma) : C(\\beta, \\gamma) = c\\},\n\\]\nand consider normal curves perpendicular to these level curves, visualized in Figure 2. We will denote the (unique) normal curve passing through the point (\u03b2, \u03b3) by $C_{\\beta,\\gamma}$. In Section 2.2 and Section 2.3 we will analyse how C(\u03b2, \u03b3) correlates with training stability & generalization performance by observing how such properties change as we move along the normal curve $C_{\\beta,\\gamma}$ in hyperparameter space, since C(\u03b2, \u03b3) varies monotonically along a normal curve (by construction)."}, {"title": "2.2 EMPIRICAL ANALYSIS OF MAX-UPDATE", "content": "We now consider the empirical implications of Equation (5), assessing whether these bounds indeed hold in practice and whether C(\u03b2, \u03b3) is predictive of training stability. We train a decoder-only transformer model on a shakespeare text dataset (details left to Appendix F) and observe how the max-update $||u_n||_\\infty$ evolves over training iterations n. Specifically, in Figure 3, we consider the normal curve $C_{\\bar{\\beta},\\bar{\\gamma}}$ passing through the typical hyperparameter values (\\( \\bar{\\beta}, \\bar{\\gamma}\\) = (0.999, 0.9), taking 64 points (\u03b2, \u03b3) uniformly along this curve (which we visualize in Figure 3c). For each point (\u03b2, \u03b3), we plot the max-update trajectory (Figure 3a), finding that the theoretical bounds of Equation (5) are satisfied in both regions B+ and B-, observing well-behaved bounded growth in B+, whereas B- exhibits exponential growth at a rate that appears correlated with |C(\u03b2, \u03b3)| as predicted by Equation (5). We more closely verify this phenomena in Figure 3b, finding that in B\u2212, the slope d log ||un||\u221e/dn has a near-perfect agreement with the theoretically predicted growth rate of |C(\u03b2, \u03b3)|/2. It is surprising that not only are the theoretical bounds (Equation (5)) satisfied in practice, but the bounds are very accurate models of the true empirical dynamics, with exponential growth occurring almost immediately after entering B\u2212. We look more closely at the results of Figure 3a in the case of (\u03b2, \u03b3) \u2208 B+ in Appendix J. We comment on correcting the exponential growth in B\u2013 via learning rate annealing in Appendix L.\nThese results partly justify the success of typical values (\u03b2, \u03b3) = (0.999, 0.9) in practice: these values lie in B+ and hence benefit from theoretical guarantees regarding training stability (which, as we have seen, are faithful to practice). Can we obtain a more fine-grained picture as to what choices of (\u03b2, \u03b3) within the region B+ will result in successful generalization? We will now explore this."}, {"title": "2.3 PROPERTIES OF GENERALIZATION IN B+", "content": "We will now assess how generalization performance varies in B+, and particularly, how the value of C(\u03b2, \u03b3) correlates with generalization performance. In Figure 4 we observe how test loss varies along the normal curve $C_{\\bar{\\beta},\\bar{\\gamma}}$, taking 128 points uniformly along the entire curve (visualized in Figure 4c). We see that larger values of C(\u03b2, \u03b3) display faster generalization, as seen in Figure 4a and highlighted at iteration 1000 in Figure 4b. After sufficient training, i.e. at iteration 3000 in Figure 4b, all points in B+ achieve roughly the same test loss. The test loss exhibits a rapid increase after entering the region B-, which is to be expected given the unstable exponential growth in the max-update shown in Section 2.2.\nThese results suggest that for a given normal curve, the point (\u03b2, \u03b3) with the largest value of C(\u03b2, \u03b3) generalizes the fastest along the curve. We provide supporting evidence for this claim, finding similar results for the normal curve that instead passes through the hyperparameter values (0.95, 0.9), in Appendix K. This would further justify why the values (\u03b2, \u03b3) succeed in practice (since \u03b2 \u2248 1, hence C(\u03b2, \u03b3) is large relative to other points along its normal curve), however unlike the results of Section 2.2, this claim lacks an explicit supporting theoretical result; it is only suggestive from Equation (5) that a larger value of C(\u03b2, \u03b3) may correlate with better training properties. We leave direct theoretical guarantees regarding the rate of generalization to future work."}, {"title": "3 IMPLICIT EFFECT OF SCALE INVARIANCE", "content": "The presence of scale-invariant architectural components has become ubiquitous in modern machine learning. One particular instance of such components are normalization layers, such as layer-norm (Ba et al., 2016), batch-norm (Ioffe & Szegedy, 2015), and qk-norm (Dehghani et al., 2023; Gilmer et al., 2023). It has been observed in practice that such normalization provides an implicit beneficial effect to training, compared to e.g. explicitly fixing/constraining the weight norm after each update step (Tanaka & Kunin, 2021; Lubana et al., 2021; Santurkar et al., 2019). In this section we will apply the continuous-time framework of Section 1 to interpreting this implicit role of scale invariance, allowing us to gain an understanding as to how normalization layers contribute to successful generalization.\nWe will first briefly discuss scale-invariant maps (Section 3.1) and then use the second-order differential equation for Adam/AdamW (Equation (4)) to solve for the norm dynamics and uncover an implicit meta-adaptive effect of scale invariance, which we then convert into an explicit optimizer, 2-Adam (Section 3.2). Finally, we will introduce the k-Adam optimizer a generalization of Adam and 2-Adam - and compare its performance with Adam for training a CNN on CIFAR10 (Section 3.3)."}, {"title": "3.1 SCALE-INVARIANT MAPS", "content": "A function f: \u0398 \u2192 R is scale-invariant with respect to a weight W \u2208 W CO if and only if f(\u03b8) remains unchanged under the transformation W&W for all a \u2208 R. As a concrete example, qk-norm (Dehghani et al., 2023; Gilmer et al., 2023) applies a layer-norm to the query and key projections in a transformer, with the attention logits zij of a particular attention head taking the form,\n\\[\nz_{ij} := LN(W_Q x_i)^T LN(W_K x_j)\n\\]\nfor embeddings xi \u2208 Rd, query matrix WQ, key matrix WK, and layer-norm LN(\u00b7). As a result, the attention logits zij are scale-invariant with respect to WQ and WK, and hence the loss function L is equivalently scale-invariant. One can show (see Appendix I) that if L is scale-invariant with respect to a weight W, then\n\\[\n\\langle W(t), g_W(t) \\rangle = 0\n\\tag{6}\n\\]\nat all times t, where $g_W(t) := \\nabla_W L(\\theta(t))$ is the gradient associated with W. Equation (6) will be utilized in the following to show that scale-invariant maps possess an implicit meta-adaptive effect."}, {"title": "3.2 UNCOVERING AN IMPLICIT META-ADAPTIVE EFFECT", "content": "To theoretically analyse the implicit effect of scale invariance, we will use the second-order differential equation for AdamW described by Equation (4) and choose a timescale of p = 1 (we verified the accuracy of this setup in Section 1.2). For an arbitrary scale invariant weight W (e.g. a key/query matrix under qk-norm), the continuous-time dynamics of W are governed by\n\\[\n\\frac{1}{\\eta^2p} \\ddot{W}(t) + \\dot{W}(t) + \\lambda W(t) = -u_W(t)\n\\tag{7}\n\\]\nto second-order, with uw defined analogously to u (replacing g with gw in Equation (3)). From here we can derive (see Appendix H) the following expression for the weight norm ||W(t)||, \n\\[\n||W(t)||^2 \\approx ||W(0)||^2 e^{-2\\lambda t} + \\int_0^t d\\tau e^{-2\\lambda(t-\\tau)} (\\eta ||u_W(\\tau)||^2 - 2 \\langle W(\\tau), u_W(\\tau) \\rangle)\n\\tag{8}\n\\]\nWe empirically verify that Equation (8) is accurate to the true discrete trajectory for ||W(t)||2 in Figure 5. In order to derive Equation (8) we did not need to use the property of scale invariance. We will now make use of this property, in combination with the following two simplifying assumptions:\nAssumption 1. We will assume that\n\\[\n\\langle W(t), g_W(t) \\rangle \\approx 0 \\quad \\forall \\tau < t\n\\]"}, {"title": "3.3 THE k-ADAM OPTIMIZER", "content": "The 2-Adam optimizer applies an adaptive-normalization procedure twice with respect to hyperparameters (\u03b21:2, \u03b31:2). The k-Adam optimizer extends this concept, applying an adaptive-normalization procedure k times with respect to hyperparameters (\u03b21:k, \u03b31:k), defined by the update rule\n\\[\nk\\text{-Adam:} \\quad \\theta_{n+1} := \\theta_n - \\eta u_n^{(k)}\n\\tag{11}\n\\]"}, {"title": "Algorithm 1 k-Adam update rule", "content": "for all i = 1,..., k, where (\u03b2, \u03b3) := (0.999, 0.9) are the typical Adam/AdamW hyperparameter values. In Appendix M we motivate the inverse exp strategy; the other strategies have been chosen heuristically. We note that for these strategies, (\u03b2i, \u03b3i) \u2208 B+ for all i = 1, . . ., k (since \u03b2i > \u03b3i) hence we expect stable training as a result of Equation (12).\n\\[\n||u_n^{(i)}||_\\infty \\leq \\frac{\\sqrt{1 - \\beta_i \\eta+1}}{\\sqrt{1 - \\gamma_i \\eta+1}} \\sqrt{\\frac{\\beta_i}{1-\\beta_i}} \\sqrt{\\frac{1-\\gamma_i}{\\gamma_i}} B_\\eta(\\beta_i, \\gamma_i)\n\\tag{12}\n\\]"}, {"title": "5 DISCUSSION", "content": "In this work we have presented a continuous-time framework for Adam, facilitating a mathematically tractable analysis of training dynamics. We have demonstrated the utilty of this framework for answering questions of practical interest regarding hyperparameter choice and the role of normalization.\nWe have seen that the derived bound on the max-update (Equation (5)) is surprisingly accurate to empirical training dynamics, however our bound does not rigorously justify (only suggests) why we observe a faster rate of generalization for larger C'(\u03b2, \u03b3) along a given normal curve, hence there is still progress to be made for a complete understanding of adaptive hyperparameter choice.\nIn this paper we do not perform a rigorous analysis of k-Adam's performance \u2013 i.e. evaluating its performance across various datasets and architectures as our focus in this paper is a theory-driven analysis of training dynamics. We note that the meta-adaptive effect we describe in this paper is not a complete account of the benefits of normalization layers. Normalization layers possess additional benefits, such as avoiding rank collapse (Daneshmand et al., 2020; Dong et al., 2023; Noci et al., 2022) and contributing towards the smoothness of the loss landscape (Santurkar et al., 2019; Kohler et al., 2018; Karakida et al., 2019; Lyu et al., 2023); it is unclear whether such phenomena are related to the meta-adaptive effect we describe in this paper."}, {"title": "A DESCRIPTIONS OF OPTIMIZERS", "content": "Momentum (Polyak, 1964) features an adaptive gradient direction \u2013 updating by a moving-average of the gradient - as described by the update rule,\n\\[\n\\text{Momentum:} \\quad \\theta_{n+1} := \\theta_n - \\eta m_n,\n\\]\n\\[\nm_n := \\gamma m_{n-1} + (1 - \\gamma)g_n\n\\]\nfor n = 0, 1, 2, . . ., with moving-average hyperparameter \u03b3 and m\u22121 := 0.\nRMSprop (Tieleman & Hinton, 2012) features an adaptive learning rate \u2013 normalizing the update gradient gn by a moving-average of the squared gradient $g_n^2$ as described by the update rule,\n\\[\n\\text{RMSprop:} \\quad \\theta_{n+1} := \\theta_n - \\eta \\frac{g_n}{\\sqrt{v_n}},\n\\]\n\\[\nv_n := \\beta v_{n-1} + (1 - \\beta)g_n^2\n\\]\nwith v\u22121 := 0. Note that $g_n^2$ describes an element-wise squaring, and the division $g_n/\\sqrt{v_n}$ is also element-wise. Here we have neglected weight decay; we will discuss weight decay strategies (coupled vs decoupled) below.\nAdam (Kingma & Ba, 2017) and its variant AdamW (Loshchilov & Hutter, 2019) are a combination of momentum and RMSprop, and also use a bias correction factor, which we justify in Appendix B. Neglecting weight decay, Adam & AdamW are equivalent, with update rule\n\\[\n\\text{Adam/AdamW:} \\quad \\theta_{n+1} := \\theta_n - \\eta \\frac{\\hat{m}_n}{\\sqrt{\\hat{v}_n}},\n\\]\n\\[\n\\hat{m}_n := \\frac{1}{1 - \\gamma_n+1} m_n, \\qquad \\hat{v}_n := \\frac{1}{1 - \\beta_n+1} v_n,\n\\]\n\\[\nm_n := \\gamma m_{n-1} + (1 - \\gamma)g_n, \\qquad v_n := \\beta v_{n-1} + (1 - \\beta)g_n^2\n\\]\nwhere a tilde denotes bias correction. The difference between Adam and AdamW comes from how they apply weight decay: Adam uses coupled weight decay, equivalent to transforming the loss L(\u03b8)\u2192L(\u03b8) + \u03bb||\u03b8||2 such that $g_n \\rightarrow g_n + \\lambda \\theta_n$, and AdamW uses decoupled weight decay, which involves subtracting \u03bb\u03b7\u03b8\u03b7 from the RHS of the update rule, i.e.\n\\[\n\\text{AdamW:} \\quad \\theta_{n+1} := \\theta_n - \\lambda \\eta \\theta_n - \\eta \\frac{\\hat{m}_n}{\\sqrt{\\hat{v}_n}}.\n\\]\nWe note that the PyTorch implementation of RMSprop uses coupled weight decay, i.e. RMSprop is Adam with \u03b3 = 0."}, {"title": "B MOTIVATING BIAS CORRECTION IN ADAM", "content": "Consider an exponential moving average of a sequence {x0, x1, x2,...} of tensors,\n\\[\ny_n := \\beta y_{n-1} + (1 - \\beta)x_n\n\\]\n\\[\n= (1 - \\beta)(x_n + \\beta x_{n-1} + \u00b7\u00b7\u00b7 + \\beta^n x_0)\n\\]\nConsider the stationary case with E[xn] independent of n, then\n\\[\nE[y_n] = E[x_n](1 - \\beta)(1 + \\beta + \u00b7\u00b7\u00b7 + \\beta^n) = E[x_n](1 - \\beta^{n+1})\n\\]\nhence if we want an unbiased exponential moving average, we should instead consider a bias-corrected form of yn:\n\\[\n\\hat{y}_n := \\frac{1}{1 - \\beta^{n+1}} y_n\n\\]\nsuch that\n\\[\nE[\\hat{y}_n] = E[x_n]\n\\]"}, {"title": "C DERIVING THE PARAMETER UPDATE BOUND", "content": "First we will derive a continuous-time expression for mn by Taylor expanding its definition,\n\\[\nm(t_n) = \\gamma m(t_n - \\eta^p) + (1 - \\gamma)g(t_n)\n\\]\n\\[\n= \\gamma m(t_n) - \\eta^p \\gamma \\dot{m}(t_n) + (1 - \\gamma)g(t_n) + O(\\eta^{2p})\n\\]\nand so to leading order,\n\\[\n\\dot{m}(t) + \\frac{1-\\gamma}{\\eta^p \\gamma} m(t) = \\frac{1-\\gamma}{\\eta^p \\gamma} g(t)\n\\]\n\\[\n\\frac{d}{dt} \\left(e^{\\frac{1-\\gamma}{\\eta^p \\gamma} t} m(t)\\right) = e^{\\frac{1-\\gamma}{\\eta^p \\gamma} t} \\frac{1-\\gamma}{\\eta^p \\gamma} g(t)\n\\]\n\\[\nm(t) = \\int_0^t d\\tau e^{-\\frac{1-\\gamma}{\\eta^p \\gamma}(t - \\tau)} \\frac{1-\\gamma}{\\eta^p \\gamma} g(\\tau)\n\\]\n\\[\n= \\int_0^t d\\tau K_\\gamma(\\tau, t)g(\\tau)\n\\]\nSimilarly for vn,\n\\[\n\\dot{v}(t) + \\frac{1-\\beta}{\\eta^p \\beta} v(t) = \\frac{1-\\beta}{\\eta^p \\beta} g(t)^2\n\\]\n\\[\n\\Rightarrow v(t) = \\int_0^t d\\tau e^{-\\frac{1-\\beta}{\\eta^p \\beta}(t - \\tau)} \\frac{1-\\beta}{\\eta^p \\beta} g(\\tau)^2\n\\]\n\\[\n= \\int_0^t d\\tau K_\\beta(\\tau, t)g(\\tau)^2\n\\]\nAs a result, the ratio of moving-averages present in Adam's update u(t) takes the form,\n\\[\n\\frac{m(t)}{\\sqrt{v(t)}} = \\eta^{-p/2} \\sqrt{\\frac{1 - \\gamma}{\\gamma}} \\sqrt{\\frac{\\beta}{1 - \\beta}} \\frac{\\int_0^t d\\tau \\exp\\left(-\\frac{1-\\gamma}{\\eta^p \\gamma}(t - \\tau)\\right) g(\\tau)}{\\sqrt{\\int_0^t d\\tau \\exp\\left(-\\frac{1-\\beta}{\\eta^p \\beta}(t - \\tau)\\right) g(\\tau)^2}}\n\\tag{13}\n\\]\nNote that this describes an element-wise division of tensors. This expression is particularly amenable to the Cauchy-Schwarz inequality, which says that for (square-integrable) functions f, g: R \u2192 R,\n\\[\n\\left| \\int dx f(x)g(x) \\right|^2 \\le \\int dx f(x)^2 \\int dx g(x)^2\n\\]\nWe can apply the Cauchy-Schwarz inequality (element-wise) to the integral in the numerator of Equation (13) to provide an upper bound,\n\\[\n\\left| \\int_0^t d\\tau \\exp \\left(-\\frac{1-\\gamma}{\\eta^p \\gamma}(t - \\tau)\\right) g(\\tau) \\right|\n\\]\n\\[\n= \\left| \\int_0^t d\\tau \\left[ \\exp \\left(-\\frac{1-\\gamma}{2\\eta^p \\gamma}(t - \\tau)\\right) \\right] \\left[ \\exp \\left(-\\frac{1-\\gamma}{2\\eta^p \\gamma}(t - \\tau)\\right) g(\\tau) \\right] \\right|\n\\]\n\\[\n\\le \\sqrt{\\int_0^t \\frac{d\\tau}{\\sqrt{1-\\exp(-Ct/\\eta^p)}} \\exp\\left(-\\frac{1-\\beta}{\\eta^p \\beta}(t - \\tau)\\right)} \\sqrt{\\int_0^t \\frac{d\\tau}{\\sqrt{1-\\exp(-Ct/\\eta^p)}} \\exp\\left(-\\frac{1-\\beta}{\\eta^p \\beta}(t - \\tau)\\right) g(\\tau)^2}\n\\]"}, {"title": "D EMPIRICAL ANALYSIS OF BO", "content": "Equation (5) suggests that ||un||\u221e may scale like \u221a\u03b7 when (\u03b2, \u03b3) \u2208 B0. Do we observe this in practice? In Figure 7 we instead see bounded behaviour analogous to that of the region B+ (as seen in Figure 3), i.e. there is bounded growth rather than a \u221a\u03b7 scaling in the max-update. We note that the bound Equation (5) is not violated; the predicted upper bound is indeed met, though the prediction is just not tight relative to the true dynamics empirically. We suspect that the condition C(\u03b2, \u03b3) = 0 may be sensitive to numerical precision issues in practice, which is perhaps the reason why we observe behaviour analogous to C(\u03b2, \u03b3) > 0 instead."}, {"title": "E ASSUMPTIONS OF SECTION 3.2", "content": "Assumption 1. To verify Assumption 1 we consider the transformer model setup described in Appendix F (same setup Figure 5) with qk-norm (Dehghani et al., 2023; Gilmer et al., 2023) enabled, such that the loss is scale invariant under each query and key matrix. In Figure 8 we plot the cosine similarity\n\\[\nsim(W_{100}, g_n) := \\frac{\\langle W_{100}, g_n \\rangle}{||W_{100}|| ||g_n||} \\in [-1, 1]\n\\]\nfor iterations n = 1, ..., 100, for each query and key matrix W in the model (a total of 32 matrices). We observe a cosine similarity of \u2248 0 at n = 100 (supporting Equation (6)) and an increase in cosine similarity as n decreases, however the cosine similarity still remains negligible. We also demonstrate that without qk-norm, the cosine similarity is no longer well behaved.\nAssumption 2. To verify Assumption 2 we choose 16 random parameter values from random query/key matrices W, and plot their trajectory in regular training vs. their trajectory when using coarse-graining on W. We display the results in Figure 9, finding a strong agreement."}, {"title": "F EXPERIMENTAL SETUPS", "content": "Architecture details. Throughout the paper we consider a nanoGPT model (Karpathy, 2022) \u2013 which is a decoder-only transformer model (Vaswani et al., 2023) \u2013 and a CNN model (Lecun et al., 1998). The nanoGPT model architecture we use has 6 layers, with 6 attention heads per layer. The embedding dimension is 384. We use a dropout of 0.2. When training we use the shakespeare dataset associated with the nanoGPT repository (Karpathy, 2022), using character-level tokenization, for a max input length of 256 tokens. The CNN model architecture we use is shown in Figure 10 which we use for Figure 6.\nContinuous-time trajectory. For experiments that require continuous-time trajectories, we use the method described in Appendix G.\nFigure 1. We use the method described in Appendix G \u2013 with K = 100, p = 1 and \u03b7 = 10\u22123 \u2013 to numerically solve Equation (4), obtaining a continuous-time trajectory in"}]}