{"title": "Task Addition in Multi-Task Learning by Geometrical Alignment", "authors": ["Soorin Yim", "Dae-Woong Jeong", "Sung Moon Ko", "Sumin Lee", "Hyunseung Kim", "Chanhui Lee", "Sehui Han"], "abstract": "Training deep learning models on limited data while maintaining generalization is one of the fundamental challenges in molecular property prediction. One effective solution is transferring knowledge extracted from abundant datasets to those with scarce data. Recently, a novel algorithm called Geometrically Aligned Transfer Encoder (GATE) has been introduced, which uses soft parameter sharing by aligning the geometrical shapes of task-specific latent spaces. However, GATE faces limitations in scaling to multiple tasks due to computational costs. In this study, we propose a task addition approach for GATE to improve performance on target tasks with limited data while minimizing computational complexity. It is achieved through supervised multi-task pre-training on a large dataset, followed by the addition and training of task-specific modules for each target task. Our experiments demonstrate the superior performance of the task addition strategy for GATE over conventional multi-task methods, with comparable computational costs.", "sections": [{"title": "1 Introduction", "content": "Molecular property prediction is a key area of computational chemistry, aimed at developing models that map molecular structures to their properties [1]. These properties can vary from fundamental characteristics such as electron affinity and critical compressibility factor to complex and specific properties like chromophore lifetime. These predictions are crucial for accelerating the development of next-generation materials, optimizing chemical synthesis, and understanding molecular interactions.\nHowever, accurately predicting molecular properties poses a significant challenge due to the complex relationships between molecular structures and their properties. Transfer learning has emerged as an essential technique to address the scarcity of labeled data and the high dimensionality of feature spaces in this field [2]."}, {"title": null, "content": "By leveraging knowledge gained from related tasks or domains, transfer learning enables models to generalize better to target tasks with limited training data, enhancing prediction accuracy and robustness. This is particularly beneficial in molecular property prediction, where experimental data collection is often expensive and time-consuming.\nIn the context of molecular property prediction, Geometrically Aligned Transfer Encoder (GATE) has been introduced as a promising foundation approach for knowledge transfer [3, 4]. GATE aligns the geometrical shapes of latent spaces across tasks to transfer mutual information. However, GATE's pairwise transfer between tasks results in $O(N^2)$ computational complexity as the number of tasks increases, making it computationally expensive.\nTo address this issue, we propose a task addition approach for GATE, a two-stage framework that reduces the computational cost while maintaining effective knowledge transfer. In the first stage, supervised pre-training is performed on a large dataset. In the second stage, modules for each target task are added and trained on smaller datasets of the target task while the parameters pre-trained on source tasks remain fixed. This approach allows pre-trained models to be reused for diverse target tasks, significantly reducing training time.\nIn this paper, we introduce task addition for GATE to minimize computational cost while maintaining generalizability. The key contributions of this work are as follows:\n\u2022 We extend GATE with a task addition approach.\n\u2022 Task-added GATE outperforms single-task learning (SINGLE) and task-added multi-task learning (MTL) across various molecular property prediction tasks.\n\u2022 The training time for task-added GATE is significantly faster than training MTL models from scratch and comparable to SINGLE and task-added MTL.\n\u2022 Our results show that task-added GATE is less dependent on the choice of source tasks, unlike task-added MTL, which heavily depends on source tasks."}, {"title": "2 Task Addition by Geometrical Alignment", "content": "Geometrical alignment is an effective method in a multi-task prediction setups. However, as shown in [4], the algorithm requires significant computational power as the number of tasks increases. To address this, we hereby introduce a specific mathematical description of the task addition method for GATE to accelerate the algorithm with minimal loss of prediction power.\nIn the GATE algorithm, the core assumption is that the essence of prediction performance lies in the geometrical characteristics of the corresponding latent space of a given input. In multi-task learning, different downstream tasks induce various latent spaces, yet the input remains equivalent for molecular property prediction tasks. Therefore, the primary strategy is to align the geometrical shapes of latent spaces from different prediction tasks to maximize the utilization of mutual information.\nThe fundamental architecture of GATE is precisely described in [3], including the algorithm diagram, thorough mathematical structure, experiments, and an introduction to Riemannian differential geometry. The architecture of the task-added GATE is depicted in Figure 1. As shown in the figure, a molecule is initially represented using the Simplified Molecular-Input Line-Entry system (SMILES), which serves as the input. It is then embedded into a real-number embedding vector. This embedding process is mandatory since the GATE algorithm requires infinitesimal perturbations around the given input vector. Consequently, the universal embedding vector must be capable of acquiring perturbation points independent of the choice of tasks.\nThe embedding vectors are then fed into the task-specific encoders to generate task-specific latent vectors, $z_n$. For this process, we utilize Directed Message Passing Neural Network (DMPNN) [5] and conventional Multi-Layer Perceptron (MLP) layers. These latent vectors serve two main purposes: one is to compute prediction values for tasks, and the other is to align the latent spaces.\nTo ensure model's accuracy, we first introduce a simple regression loss. After the latent vectors pass through the task-specific head network, the final predicted value should closely match to the given"}, {"title": null, "content": "label. We use a simple Mean Squared Error (MSE) loss for this purpose.\n$l_{reg} = \\frac{1}{K} \\sum (y_i - \\hat{y_i})^2$ (1)\nHere, $K$, $y_i$, and $\\hat{y_i}$ represent the number of target tasks to add, the target label, and the predicted value, respectively. In the multi-task extended GATE architecture, the regression loss is summed over the entire number of tasks, resulting in the computation complexity of the algorithm being $O(N^2)$. However, as the number of tasks is now restricted to the number of target tasks, the entire process becomes significantly faster.\nThe other, and the most important part of the algorithm, is the alignment of latent spaces. To align the geometrical shapes of these latent spaces, one should know the specific mapping relation between one another. In the mathematical description, the mapping should be formulated by coordinate transformation induced by a Jacobian at an arbitrary point.\n$\\hat{z} = \\sum_j \\frac{\\partial z_i}{\\partial z_j}z_j $ (2)\nDeriving an analytic form of the Jacobian from data-driven methods without assuming the underlying geometry is typically impossible. We bypass this issue by predicting the transformed vector directly using a neural network. Neural networks are generally smooth and differentiable due to the backpropagation learning scheme, allowing us to assume the latent space is also smooth and differentiable. Hence, it is plausible to assume the latent space as Riemannian. The diffeomorphism invariance of Riemannian geometry ensures that a locally flat frame can be found anywhere on a manifold, which we utilize to align latent geometries.\nWe set up autoencoder models to map a vector in latent space to a locally flat frame on a universal manifold and vice versa, as depicted in Figure 2. Specifically, each encoder, $\\phi$, maps a latent vector from a task-specific latent space to a vector in a locally flat frame, while each decoder, $\\phi^{-1}$, maps a vector from the locally flat frame to the task-specific latent space. Unlike the original GATE, since this situation involves task addition, the models for source tasks should be pre-trained. Therefore, the mapping models can be categorized into two types: those with fixed parameters and those with learnable parameters. Mapping from source tasks should be fixed.\n$z_\\alpha = Transfer_{\\alpha \\to LF}(z_\\alpha)$ (3)\n$\\hat{z}_\\alpha = Transfer^{-1}_{LF \\to \\alpha}(\\hat{z}_{LF})$ (4)"}, {"title": null, "content": "And mapping from the target task should be learnable.\n$\\hat{z}_{t} = Transfer_{t \\to LF}(z_{t})$ (5)\n$z_{t} = Transfer^{-1}_{t}(\\hat{z}_{LF})$ (6)\nHere, $\\alpha$ denotes the source task, and $t$ indicates the target task. The index $\\alpha$ ranges from 1 to the number of source tasks. For example, if there are 1 to 10 different source tasks and one new target task is added, then $\\alpha$ ranges from 1 to 10.\nConsidering $Transfer_{\\alpha \\to LF}(z_{\\alpha})$ as an example, where $\\alpha$ is set to 4, this indicates that the latent vector of the 4th task is mapped to a locally flat frame on a universal manifold. One can introduce a simple autoencoder loss that forces the input latent vector and reconstructed vector to be equal for the mapping networks.\n$l_{auto} = \\sum_{\\alpha} MSE(z_\\alpha, \\hat{z}_\\alpha)$ (7)\nAdditionally, a different set of losses can be formulated to align the geometrical shapes of latent spaces. So far, we have not imposed any constraints on a mapping network that should map a latent vector to a locally flat frame. As introduced in [3], we will introduce three different kinds of constraints afterward.\nOne loss involves matching latent vectors on the locally flat frame that are mapped from different task-specific latent spaces. This loss aims to align the geometry point-wise by requiring latent vectors on a locally flat frame to be equal to one another. This loss is simply induced by the fact that the input of the model always starts from the same molecule regardless of the task choice. Hence, while the task-specific latent vectors may differ, if the mapping encoder correctly maps the vectors to the locally flat frame on the universal manifold, the mapped vectors from different tasks should be equal on the locally flat frame. We call this loss the consistency loss.\n$l_{cons} = \\sum_{\\alpha} MSE(\\hat{z}_{LF}, \\hat{z}_{LF,t})$ (8)\nAs mentioned earlier, $\\hat{z}_{LF}$ and $\\hat{z}_{LF,t}$ represent vectors on the locally flat frame from the source and target latent spaces, respectively. One can introduce another loss function, namely the mapping loss, to"}, {"title": null, "content": "enhance the bonding between latent spaces. This loss requires the predicted value for a given task using a standard downstream route to be equal to the predicted value for the same task using a detoured route.\n$z_{t} = Transfer_{\\alpha \\to LF}(z_{\\alpha})$ (9)\n$\\hat{y}_{t} = Transfer^{-1}_{LF \\to t}(\\hat{z}_{LF})$ (10)\nHere, $\\hat{z}_{t}$ indicates a latent vector of the target task mapped from the vector in the source task latent space by the source mapping and target inverse-mapping networks. Although $\\hat{z}_{LF, t}$ originates from the source task latent space, the source mapping network should map the vector to a locally flat frame, and the target inverse-mapping network should theoretically map the vector to the same latent vector in the target latent space. Therefore, if $z_t$ and $\\hat{z}_{t}$ go through the same head network, the predicted value should be the same. This loss is called the mapping loss.\n$l_{map} = \\sum_{\\alpha} MSE(y_t, \\hat{y}_{\\alpha \\to t})$ (11)\n$y_t$ represents a predicted value from a standard downstream route, and $\\hat{y}_{\\alpha \\to t}$ indicates a predicted value from a detoured route.\nDespite these losses, geometrical alignment remains insufficient as they only align geometry at specific points. In general, neural networks often enjoy a vast number of degrees of freedom, providing enough flexibility to significantly distort the geometrical shape. Hence, even though the geometry is matched at given points, the surrounding shape around those points may still not be aligned.\nTo impose stronger constraints, one should consider not only a specific point but also the geometrical shape around a given input point. Typically, this is done by assuming an analytic form of the metric and computing Riemannian properties such as curvatures. However, finding the analytic form of the metric is nearly impossible. Therefore, we will impose geometrical constraints without knowing the analytic form of the metric.\nThe key notion is the distance between points on a curved space. While a distance can be easily defined in Euclidean space, it is not as straightforward in curved space. One should solve the geodesic (freely falling motion) equation to find the geodesic path between points and integrate the infinitesimal distances along the path with metric weighting.\n$S^2 = \\int \\sum_{\\mu} \\sum_{\\nu} g_{\\mu \\nu} dx^\\mu dx^\\nu$ (12)\nHowever, we now know that one can always find a locally flat frame on a Riemannian manifold. By utilizing this fact, if one is trying to compute the distance between a point on a locally flat frame and a its infinitesimal perturbations, the distance is simplified into a mere Euclidean distance.\n$S^2 = \\sum_{\\mu} \\sum_{\\nu} g_{\\mu \\nu} dx^\\mu dx^\\nu$ (13)\nHere, $a$ represents a given latent vector, while $b$ denotes a perturbed vector around the latent vector $a$. As we assume the perturbation is infinitesimal, the distance between a given point and its perturbations can be reduced to the following form:\n$S = |b - a|$ (14)\nThis distance should also be the same for both source and target tasks after mapping into a locally flat frame. The loss that ensures this equality is called the distance loss.\n$l_{dis} = \\frac{1}{M} \\sum_{\\alpha} C_\\alpha \\sum_{i} MSE(|\\hat{z}_{LF, t}^i|, |\\hat{z}_{LF}^i|)$ (15)\nHere, $M$ is the number of perturbation points around the input vector, $C_\\alpha$ is the weight of distance losses according to the source tasks, and $\\delta^i$ is the displacement between given data and their perturbations. Specific descriptions of loss terms are as follows.\n$\\delta_\\alpha = |(\\hat{z}_{LF})^i - (z^i)|$, $\\delta_t = |(\\hat{z}_{LF, t})^i - (z^i)|$ (16)\n$\\hat{z}^i_{\\alpha} = Transfer_{\\alpha \\to LF}(Encoder_{\\alpha}(x^i))$ (17)\n$\\hat{z}^i_{t} = Transfer_{t \\to LF}(Encoder_{t}(x^i))$ (18)"}, {"title": null, "content": "The index $i$ denotes ith perturbation point.\nThe distance loss is the key term in this architecture. This loss restricts the local geometrical shape around input vector points, significantly reducing the vast number of degrees of freedom of the model. As mentioned earlier, the latent space should be smooth and differentiable, limiting the freedom to retain the shape of local geometry around input vectors and preventing drastic deformations\u00b9.\nGathering all the introduced loss terms gives rise to a total loss function for the architecture.\n$l_{tot} = l_{reg} + \\alpha l_{auto} + \\beta l_{cons} + \\gamma l_{map} + \\delta l_{dis}$ (19)\nThe total loss contains numerous hyperparameters that need to be tuned. These parameters affect the prediction performance of the model. Hence, some of the hyperparameters should be tuned with care to achieve superior performance. For instance, parameters $\\gamma$, $\\delta$, and $C_\\alpha$ are sensitive to performance, while others are not. Therefore, we often leave the other parameters as 1 and focus on finding the best set of parameters for $\\gamma$, $\\delta$, and $C_\\alpha$. If the relationship between tasks is well known by domain knowledge, it is a very good strategy to begin with."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Experimental Setup", "content": null}, {"title": "3.1.1 Datasets", "content": "To evaluate our algorithm, we used 20 datasets, each corresponding to a different molecular property, sourced from three open databases: PubChem [6], Ochem [7], and CCCB [8]. Each property was standardized by subtracting the mean and dividing by the standard deviation. Detailed information about each dataset is provided in Appendix A. We split each dataset into training and test sets using an 80:20 ratio. For evaluating model performance for extrapolation, we split the dataset based on the scaffold [9]. The training set was further uniformly split into four folds for cross-validation."}, {"title": "3.1.2 Models", "content": "We employed ten tasks as source tasks for the supervised pre-training of GATE and MTL. The remaining ten tasks were used as target tasks for task addition, individually. For each target task, we added the regression unit for the target task to the pre-trained model, resulting in ten different models for GATE, each for a target task.\nTo benchmark the performance of GATE, we also trained MTL models in task addition setup. In the pre-training stage, a pre-trained MTL model with a shared encoder and task-specific heads for source tasks were trained. Then, for each target task, task-specific head was added to the model architecture and trained on the target task while the pre-trained parameters were frozen. We also trained ten task-added MTL models, each for a target task.\nFor performance comparison, we trained GATE and MTL on all 20 tasks from scratch (referred to as 'Vanilla20' models), serving as reference performance for task-added GATE and MTL. To compare computational costs, we trained 10 MTL and GATE models from scratch (referred to as 'Vanilla11'), trained on 11 tasks (10 source tasks + one target task).\nIn summary, we trained the following three different types of models for both GATE and MTL:\n\u2022 Task-added model: Pre-trained on 10 source tasks followed by addition and training of one target task.\n\u2022 Vanilla20: Trained on all 20 tasks from scratch.\n\u2022 Vanilla11: Trained on 10 source tasks and one target task from scratch.\nLastly, we also trained single-task models ('SINGLE') on each target task without multi-task learning. The same architecture for encoders and heads was used across all models for fair comparison. A detailed descriptions of the model architecture and hyperparameters are provided in Appendix Table 3 and 4, respectively."}, {"title": "3.2 Results", "content": null}, {"title": "3.2.1 Task Addition Reduces Computational Costs", "content": "The primary motivation for task addition is to reduce the computational complexity of GATE. We analyzed training times for SINGLE, MTL, and GATE, as shown in Table 1. Task-added MTL was the fastest to train, even taking less time than SINGLE. Task-added GATE was 13 % slower than SINGLE, but 9.29 times faster than vanilla11 MTL. Task-added MTL was 10.9 times faster than vanilla11 MTL, while task-added GATE was 39.13 times faster than vanilla11 GATE The significant improvement in the training speed of GATE with task addition is due to the time complexity of vanilla GATE being O(N2), compared to MTL's O(N). In summary, task-added GATE is significantly faster than training vanilla MTL or GATE from scratch, and its training time is comparable to that of SINGLE."}, {"title": "3.2.2 Task-Added GATE Enables Knowledge Transfer", "content": "The other goal of task addition is to maintain model performance compared to training the entire model from scratch. The performance of SINGLE, task-added MTL, and task-added GATE is illustrated in Figure 3. The average Root Mean Squared Error (RMSE) of task-added MTL, SINGLE, and task-added GATE across 10 target tasks are 0.742, 0.670, and 0.647, respectively. Similarly, the average Pearson correlation of task-added MTL, SINGLE, and task-added GATE across 10 target tasks are 0.527, 0.662, and 0.688, respectively. Detailed numerical results can be found in Appendix Table 5. These metrics demonstrate that GATE outperforms SINGLE, indicating that knowledge transfer through task addition enhances performance."}, {"title": null, "content": "In contrast, MTL underperforms compared to SINGLE, suggesting that not all MTL architectures are suitable for task addition. This observation aligns with findings reported in previous studies"}, {"title": null, "content": "[10, 1]. Overall, these results indicate that GATE generalizes well in an extrapolation setting by using task-specific latent vector for each task, minimizing interference that might be caused by strongly shared latent vector in MTL.\nThis effect is further highlighted by comparing the correlation recovery rates of GATE and MTL in Figure 4. On average, task-added GATE recovered 98.3% of the Pearson correlation of vanilla20 GATE, while task-added MTL achieved 70.4% of the Pearson correlation of vanilla20 MTL. GATE shows higher correlation recovery rates than MTL for all ten tasks. In the worst-case scenario, task-added GATE achieves a recovery rate of 79.3% for CMQ, whereas task-added MTL only recovers 15.1 % for CML. This highlights that task-added GATE shows robust performance regardless of the target task, whereas the performance of task-added MTL heavily depends on the nature of the target task. In summary, these results demonstrate that GATE effectively transfers knowledge from source tasks, while MTL shows limited performance in task addition setup."}, {"title": "3.2.3 Dependence on Source Tasks", "content": "To analyze the factors that affect the performance of task addition, we examined the correlation between source tasks and target tasks. Specifically, for each source-target task pair, we selected molecules that have labels for both source and target tasks. If a source and target task share at least ten molecules in common, we calculated the absolute Pearson correlation between their labels to measure the degree of relatedness between source and target tasks. Figure 5 reports the maximum absolute correlation of source tasks for each target task.\nFor MTL, the correlation recovery rate tends to decrease as the maximum correlation between the target task and the source tasks decreases. This indicates that if no source task is closely related to the target task, the performance of multi-task learning (MTL) can decrease. On the other hand, the correlation recovery rate of task-added GATE is less dependent on the correlation with source tasks. This explains why task-added MTL performs worst for CML, which has a maximum related source task correlation of 0.387. Conversely, the correlation recovery rate of GATE for CML is 0.988, achieving a 7.3 % increase compared to SINGLE. These results demonstrate the ability of task-added GATE to effectively extract mutual information even in challenging situations."}, {"title": "4 Discussion", "content": "In this study, we proposed and evaluated a two-stage, multi-task learning approach called task addition for the GATE algorithm. Task addition aims to enhance performance on target tasks with limited data while minimizing computational complexity. This is achieved through leveraging supervised multi-task pre-training on a large dataset, followed by the addition of task-specific modules. Task-added"}, {"title": null, "content": "GATE demonstrated significant performance improvements over SINGLE and task-added MTL, with more efficient training times compared to vanilla MTL and GATE trained from scratch.\nThe GATE's unique approach to knowledge transfer via geometrical alignment enhances the model's performance. Unlike conventional multi-task learning (MTL), which shares latent vectors directly, GATE transfers knowledge through a universal locally flat (LF) space, aligning the geometrical shapes of latent spaces across tasks. This method mitigates negative transfer effects and ensures efficient information flow from source to target tasks.\nIn the context of molecular property prediction, where data scarcity and high dimensionality are significant challenges, the GATE algorithm offers a robust solution. Our findings indicate that task-added GATE can maintain the performance of its vanilla counterpart while achieving training times comparable to SINGLE and task-added MTL models. This suggests that GATE is particularly well-suited for the efficient screening of novel compounds with desired properties by adding related tasks to the model.\nWhile our results are promising, there are areas for further investigation. We used supervised multi-task learning as a pre-training strategy. Recently, self-supervised learning has been widely adopted as a pre-training strategy, and recent studies have showed that self-supervised pre-training followed by multi-task fine-tuning can improve performance [11]. Adopting such strategies to GATE may further enhance the model's performance.\nIn summary, the proposed task addition approach for GATE offers a powerful tool for improving performance on target tasks with limited data in molecular property prediction. By effectively balancing computational efficiency and task performance, the GATE algorithm stands out as a promising candidate for addressing the challenges of data scarcity in this domain."}, {"title": "A Detailed Explanation of Datasets", "content": "We evaluated task addition using 20 molecular property datasets. During preprocessing, we excluded data with incorrect units, typographical errors, and measurements taken under extreme conditions. Before training, we standardized all datasets by their mean and standard deviation. Below, we provide the physical meaning of each molecular property.\n\u2022 Acentric Factor (ACF) : A measure of the non-sphericity of molecules, which quantifies how much the behavior of a fluid deviates from that of a spherical molecule\n\u2022 Absorbance Maximum Wavelength (AW) : The wavelength at which a substance absorbs the maximum amount of light\n\u2022 Boiling Point (BP) : The temperature at which a compound changes state from liquid to gas at a given atmospheric pressure.\n\u2022 Critical Compressibility Factor (CCF): A dimensionless number that describes the behavior of a substance at its critical point.\n\u2022 Collision Cross Section (CCS): A measure of the probability of interaction between particles, representing the effective area that one particle presents to another for a collision to occur.\n\u2022 Chromophore Emission Max (CME) : The wavelength at which a chromophore emits the maximum amount of light upon excitation.\n\u2022 Chromophore Emission FWHM (CMF) : The width of the emission spectrum at half of its maximum intensity, indicating the range of wavelengths emitted by the chromophore.\n\u2022 Chromophore Life Time (CML) : The average time a chromophore remains in an excited state before returning to its ground state.\n\u2022 Chromophore Quantum Yield (CMQ) : The efficiency of photon emission by a chromophore, defined as the ratio of the number of photons emitted to the number of photons absorbed.\n\u2022 Decomposition (DC) : The process by which a chemical compound breaks down into simpler substances or elements.\n\u2022 Dipole Moment (DM) : A measure of the separation of positive and negative charges in a molecule, indicating the polarity of the molecule.\n\u2022 Electron Affinity (EAF) : The amount of energy released when an electron is added to a neutral atom or molecule in the gas phase to form a negative ion.\n\u2022 Flash Point (FP) : The lowest temperature at which a liquid can form an ignitable mixture in air near its surface, indicating its flammability.\n\u2022 HOMO (HM) : The highest energy molecular orbital that is occupied by electrons in a molecule under normal conditions.\n\u2022 LUMO (LM) : The lowest energy molecular orbital that is unoccupied by electrons, which can accept electrons during a chemical reaction.\n\u2022 Log P (LP) : The logarithm of the partition coefficient between octanol and water, indicating the hydrophobicity or lipophilicity of a compound.\n\u2022 Molar Extinction Coefficient (MEC) : A measure of how strongly a chemical species absorbs light at a given wavelength per molar concentration.\n\u2022 Melting Point (MP) : The temperature at which a solid turns into a liquid at atmospheric pressure.\n\u2022 pKa (PKA) : The negative logarithm of the acid dissociation constant, indicating the strength of an acid in solution.\n\u2022 Refractive Index (RI): A measure of how much light is bent, or refracted, when entering a material from another medium."}, {"title": "B Architecture and Hyperparameters", "content": "Training process for GATE is described in [3]. Our model is composed of five different types of modules, whose parameter sizes are listed in Table 3."}]}