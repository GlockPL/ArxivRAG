{"title": "Structurally Consistent MRI Colorization using Cross-modal Fusion Learning", "authors": ["Mayuri Mathur", "Anav Chaudhary", "Saurabh Kumar Gupta", "Ojaswa Sharma"], "abstract": "Medical image colorization can greatly enhance the interpretability of the underlying imaging modality and provide insights into human anatomy. The objective of medical image colorization is to transfer a diverse spectrum of colors distributed across human anatomy from Cryosection data to source MRI data while retaining the structures of the MRI. To achieve this, we propose a novel architecture for structurally consistent color transfer to the source MRI data. Our architecture fuses segmentation semantics of Cryosection images for stable contextual colorization of various organs in MRI images. For colorization, we neither require precise registration between MRI and Cryosection images, nor segmentation of MRI images. Additionally, our architecture incorporates a feature compression-and-activation mechanism to capture organ-level global information and suppress noise, enabling the distinction of organ-specific data in MRI scans for more accurate and realistic organ-specific colorization. Our experiments demonstrate that our architecture surpasses the existing methods and yields better quantitative and qualitative results. Our source code will be released here https://github.com/graphics-research-group/MRI-Colorization-Cross-Modal-Fusion-Learning.git", "sections": [{"title": "INTRODUCTION", "content": "Radiometric images like MRI and CT contain grayscale pictures of human anatomy used for non-invasive clinical diagnosis. Only trained radiologists can visually recognize and distinguish anatomical structures in radiometric imaging data. Recently, deep-learning techniques have offered automated organ-based structural recognition for color transfer in the medical domain and improved the visual experience of human anatomy. Extensive work has been done for natural image colorization by utilizing deep-learning techniques [1]\u2013 [3], however these techniques underperform in the medical domain. The existing colorization architectures are more suited for natural images and their performance degrades when adapted to the medical domain. This is primarily due to the fact that many such approaches [2], [3] incorporate pre-trained networks like VGG [4] that are trained only on natural images. There are limited contributions to medical image colorization. The existing medical colorization methods have limited color variations in anatomical structures, some techniques [5] use 3D convolutional layers in their frameworks with larger memory requirements for optimization. Some techniques [6], [7] colorize partial sections of the body efficiently but are incapable of the diverse color synthesis for anatomical structures of the full body. While most methods [8]\u2013[11] focus on segmentation which cannot be used for colorization since these methods only capture global information and are inefficient in fetching diverse colors and textures of the various organs. We propose a novel semi-supervised architecture for structurally consistent MRI colorization to overcome the aforementioned limitations in the medical domain. In our method, the MRI data is not completely registered with the Cryosection data. Deformations persist in the MRI data even after performing a rigid registration followed by deformable registration using B-splines deformation, therefore, the available Cryosection segmentation data cannot be paired with MRI data. Our method builds the cross-domain organ-color and organ-structure association between partly registered heterogeneous data. It integrates the Cryosection segmentation semantics with the perceptually similar semantics of the MRI data during cyclic generation across domains. This is achieved by the color adaptation dual decoder module used in our proposed colorization generator in the forward pass of our cycle-consistent network for bridging multimodal information of colors and structures. One decoder is designed for building an association between organ regions and colors using the semantics of segmentation data while the other decoder performs MRI colorization by leveraging this association during optimization. The source MRI data contains noise that introduces ambiguity in differentiating between various organ textures that disrupt the colorization process. To overcome this limitation and to further enhance the performance of building color and anatomical region association in our architecture, we introduce skip connections between the two decoders using a compression-activation mechanism that suppresses noise and captures the global information of diverse organs. To the best of our knowledge, our architecture provides full body colorization of partly registered MRI data. The major contributions of our work are as follows:\n\u2022 A novel architecture to colorize the whole body that can effectively capture anatomical structures of the MRI data and colors of Cryosection data.\n\u2022 Integration of Cryosection segmentation information in the proposed cross-modality adaptation framework for a robust semantic and color-texture correlation.\n\u2022 A multiscale module to handle varying resolutions of input MRI images."}, {"title": "RELATED WORK", "content": "a) Cross modality synthesis: Various deep-learning- based techniques [8], [12]-[17] are utilized in cross-modality"}, {"title": "APPROACH", "content": "We perform experiments on a subset of the Visible Korean Human dataset [26]\u2013[28] that contains Cryosection, CT, MRI, and Cryosection segmentation images of a male subject. For our approach, we perform MRI volumetric registration with Cryosection volume by using deformable B-spline registration available in the Elastix [29] library. The Cryosection segmentation data consists of 54 subclasses corresponding to 13 anatomical systems of the human body.\nOur method aims to perform colorization by transferring textural details of the Cryosection image to an MRI image with minimum structural alterations in the latter. It comprises of generative adversarial network that leverages cyclic cross- modality fusion technique [30]. Our architecture offers a dual decoder module in the color-transferring generator to adapt distinguishable colors and textures of various organs. It also facilitates cross-modal correlation between the segmentation of Cryosection data and perceptually similar structures of partly registered data. We design skip connections of our architecture with compression and excitation mechanism [20] to capture global semantic information about organ level and noise suppression. We extend the capability of our architecture for multiscale invariance to interpret organ details at different resolutions by incorporating the hierarchical spatial variation module in the encoder of the colorization generator."}, {"title": "Colorization architecture", "content": "Our proposed colorization architecture is designed using the generative adversarial network with a cyclic cross-modality adaptation technique [30], as shown in as shown in Fig. 1."}, {"title": "Compression-activation generators", "content": "Our proposed framework implements channel compression and activation mechanisms [20] to all the skip connections of its architecture. The compression-activation blocks incorporate linear layers that replace the conventional convolutional residual blocks. The compression-activation technique is used to capture the organ-level global information. As this technique emphasizes the important features, it tends to suppress less useful ones. Here, it reduces the impact of noise in the MRI data. The ability to capture global information of anatomical classes is leveraged to maintain the structural consistency of the source MRI data in the colorized MRI data. In this mechanism, compression extracts channel-wise descriptor embedding by aggregating the spatial and channel-wise information of the feature from the preceding convolutional layer. Activation is performed by applying a sigmoid-based gating"}, {"title": "Multiscale module", "content": "We implement a multiscale module in the generators Gm\u2192\u00ea and Gc\u2192m, and pass three inputs corresponding to the original resolution and its downsampled versions. Both the input MRI image m and the input Cryosection image c are downsampled by a factor of two resulting in $m_2 \\in \\mathbb{R}^{\\frac{w}{2} \\times \\frac{h}{2}}$ and $m_4 \\in \\mathbb{R}^{\\frac{w}{4} \\times \\frac{h}{4}}$, where w and h correspond to height and width of m respectively and the same is implemented for the input Cryosection c keeping the number of channels intact. The multiscale module comprises three convolutional submodules where each submodule operates on each m, m2 and m4 respectively. The features extracted from these submodules are fused by summing the feature maps of m2 and m4 and concatenating with the feature of m. The fused feature is passed to the succeeding layers of the generator."}, {"title": "Approach formulation", "content": "Our approach focuses on colorizing MRI images while preserving structural integrity across domain shifts. We accomplish this by optimizing the fidelity of our colorization using cyclically consistent adversarial loss, structural similarity loss, and segmentation loss.\na) Cyclic consistency loss: We incorporate cyclic cross- modality adaptation losses [30] which are adversarial loss Ladv and cycle consistency loss Lrec. Modality translation is achieved by minimizing the adversarial loss [32] within the mappings Gm\u2192\u00ea and Gc\u2192m\n$L_{adv} = \\mathbb{E}_{c \\sim p_c} [log D_c(c)] + \\mathbb{E}_{m \\sim p_m} [log(1 \u2013 D_c(G_{m \\rightarrow \\hat{c}}(m)))] + \\mathbb{E}_{m \\sim p_m} [log D_m(m)] + \\mathbb{E}_{c \\sim p_c} [log(1 \u2013 D_m(G_{c \\rightarrow m}(c)))], $ (1)\nwhere De and Dm discriminate the cross-modal distribution adaptability by classifying the synthesized data of respective data as real and pseudo. Reconstruction loss used in cycle consistency is computed between the input and the reconstructed modalities. For an MRI sample m, a cascade of generators produce the reconstructed MRI image $m = G_{c \\rightarrow m}(G_{m \\rightarrow \\hat{c}}(m))$. Similarly, for a Cryosection image c, the reconstructed Cryosection image $\\hat{c} = G_{m \\rightarrow \\hat{c}}(G_{c \\rightarrow m}(c))$. We formulate the reconstruction loss as\n$L_{rec} =\\mathbb{E}_{c \\sim p_c} [||G_{m \\rightarrow \\hat{c}}(G_{c \\rightarrow m}(c)) \u2013 c||_1] + \\mathbb{E}_{m \\sim p_m} [||G_{c \\rightarrow m}(G_{m \\rightarrow \\hat{c}}(m)) \u2013 m||_1].$(2)\nThe total Cyclic Adversarial Loss is then defined as\n$L_{cyc} = L_{adv} + L_{rec},$ (3)"}, {"title": "Structural Adaptation Loss", "content": "Our method incorporates a variant of Structural Similarity Index Measure (SSIM) [33] for structurally consistent color adaptation. We compute a local SSIM map using an image's luminance, contrast, and structure over small patches of variable sizes. In our approach, we compute the local SSIM values over four patch sizes of 3 \u00d7 3, 5\u00d75, 7\u00d77, and 9 \u00d7 9 and aggregate these four resulting SSIM maps in the loss term. The local SSIM value at an image index (i, j) over a patch is defined as follows\n$SSIM_{i,j} (m, \\hat{c}) = l_{ij} (m, \\hat{c}) k_{ij} (m, \\hat{c}) t_{ij} (m, \\hat{c}),$ (4)\nwhere $l_{ij} (m, \\hat{c})$ denotes the luminance similarity, $k_{ij} (m, \\hat{c})$ denotes the contrast similarity and $t_{ij} (m, \\hat{c})$ denotes the local structural similarity between m and $\\hat{c}$ over a patch:\n$l_{ij} (m, \\hat{c}) = \\frac{2\\mu_{m,ij}\\mu_{\\hat{c},ij} + C_1}{\\mu^2_{m,ij} + \\mu^2_{\\hat{c},ij} + C_1},$\n$k_{ij} (m, \\hat{c}) = \\frac{2\\sigma_{m,ij}\\sigma_{\\hat{c},ij} + C_2}{\\sigma^2_{m,ij} + \\sigma^2_{\\hat{c},ij} + C_2},$\n$t_{ij} (m, \\hat{c}) = \\frac{\\sigma_{m\\hat{c},ij} + C_3}{\\sigma_{m,ij}\\sigma_{\\hat{c},ij} + C_3}$\nHere, $\\mu_{m,ij}$ and $\\mu_{\\hat{c},ij}$ represent the mean values, $\\sigma_{m,ij}$ and $\\sigma_{\\hat{c},ij}$ represent the standard deviations, and $\\sigma_{m\\hat{c},ij}$ represents the covariance between m and $\\hat{c}$ over a patch centered at image index (i, j). Constants C1, C2, and C3 are small positive constants to prevent instability when the denominator approaches zero (see Wang et al. [33] for details).\nWe compute three SSIM losses Lssim(M,c), Lssim(c,m), and Lssim(c, c') for enforcing structural similarity between the pairs of images (m, $\\hat{c}$), (c, m), and (c, c'), respectively. Lssim(m,c) is defined as the mean of all values in a local SSIM map summed over different patch sizes:\n$L_{ssim}(m, c) = \\sum_{b \\in \\{3,5,7,9\\}} mean (SSIM_{b \\times b} (m, \\hat{c})).$ (5)\nLssim (m, c) and Lssim (c, c') are computed similarly. The total SSIM loss is given by\n$L_{ssim} = L_{ssim(c, m)} + L_{ssim (m, \\hat{c})} + L_{ssim (c, c')}.$ (6)"}, {"title": "Segmentation loss", "content": "Segmentation loss: The Visible Korean Human dataset provides segmentation data corresponding to the Cryosection images. The same cannot be paired with the MRI data due to non-rigid deformations between Cryosection and MRI images. We unify the feature space of both MRI and segmentation data via a pseudo Cryosection image c' synthesized from the decoder Fx c'. The cross-entropy (CE) loss function [31] Lseg used to update parameters of Gm\u2192\u0109 is given by\n$L_{seg} = CE(s, S_{c \\rightarrow \\hat{s}}(c)),$ (7)\nwhere CE between two multi-class binary maps s, \u015d \u2208 R\u00b3 \u2192 {0,1} is defined as:\n$CE(s, \\hat{s}) = \\frac{1}{wh} \\sum_i \\sum_j \\sum_k s_{ijk} log \\hat{s}_{ijk}, $ (8)\nwhere l is the total number of segmentation classes. The complete objective of our approach is given by loss L = Lcyc+ Lssim + Lseg."}, {"title": "RESULTS AND ANALYSIS", "content": "Our model consists of convolutional layers along with residual connections. The discriminator consists of five convolutional layers with LeakyReLu activations followed by a final linear layer with Sigmoid activation. The input spatial size of the multimodal networks is 256 \u00d7 256. The segmentation dataset has 46 accurate segmentations of Cryosection images. We optimize all models using the Adam optimizer, with learning rates set to 10-\u00b3, 10-4, and 10-6 for generators, discriminators, and segmentation models respectively. Training is conducted for 150 epochs with a batch size of 28 using Pytorch with distributed learning across two Nvidia A100 GPUs with 80 GB of memory each. We extract 12,000 slices of resolution (256x256) at random planes from the volumetric MRI, Cryosection, and Segmentation data for the dataset after partial volumetric registration of MRI and Cryosection data. The train set comprises 10,000 samples and the test set comprises 2,000 samples. The source code for our approach will be released later for research purposes.\nDuring inference the primary generator Gm\u2192\u00ea takes input MRI image for colorization. While Cryosection and segmentation ground truth images are needed during the training of our model, these are not required at inference time. The results of our colorization are displayed in Fig. 2 for four sample images taken across the human body. These results demonstrate the structural accuracy of our colorization in alignment with the input MRI, as well as the detailed and vivid textures of the ground truth Cryosection images. Since our approach utilizes organ-level segmentation information, it clearly demonstrates how different tissues, such as bones, muscles, cartilage, liver, and others, are accurately colorized. Furthermore, to demonstrate the scale-invariance of our approach, we present colorization results for input MRI images of varying sizes: 256\u00d7256, 128\u00d7128, and 64\u00d764 in Fig. 2. We observe that as the resolution decreases, our method effectively handles the changes in structural information for colorization, maintaining minimal perceptual loss in the colorized outputs."}, {"title": "Evaluation metrics", "content": "We use image comparison metrics to evaluate our approach, utilizing evaluation metrics categorized broadly into structural similarity, color similarity, and textural similarity. We include the following evaluation metrics on the testing set in our comparisons: colorfulness (CF) score [34], Structural Similarity Index (SSIM) [33], Multiscale Structural Similarity Index (MS-SSIM) [35], Feature Similarity Index (FSIM) [36], and Structural Texture Similarity Index (STSIM) [37]. Note that the generated colorized MRI images should be structurally similar to the input greyscale MRI and the colors in the output should be similar to Crysoction data. CF depicts the diversity and vividness of colorized MRI however, a high CF score does not always signify good visual quality. Therefore, we use \u25b3CF [3] which computes the difference between the CF scores of the Cryosection image and the generated one. SSIM is used to measure the structural consistency between the colorized MRI and the input MRI. Multiscale Structural Similarity Index (MS-SSIM) [35] helps in capturing different levels of image"}, {"title": "Comparisons with state-of-the-art", "content": "We compare the results of our method with the state-of-the- art colorization methods for which we selected five competing colorization methods: ChromaGAN [1], ColorFormer [2], DDColor [3], APS [17], and ALDM [16]. All the comparisons are conducted by training these methods on the Visible Korean Human dataset.\na) Quantitative comparisons: We note that the coloriza- tion methods ChromaGAN, ColorFormer, and DDColor utilize the pre-trained VGG network and are originally trained on natural images. These methods lack knowledge of medical image distribution, so we retrain them on our medical dataset. In contrast, the two methods APS and ALDM are specifically designed to work with medical images and transform between CT and MRI images. We retrain these methods on our dataset to adapt them for MRI and Cryosection image modalities."}, {"title": "Ablation study", "content": "To evaluate the significance of different components within our proposed deep-learning model, we performed a comprehensive ablation study. The results of the ablation study are shown in Table II and Fig. 5.\nA1: Without cycle consistency In this experiment, MRI colorization is achieved by discarding Lrec from Lcyc and optimizing only the forward pass of the cyclic generative adversarial network (GAN) with segmentation semantics. As shown in Fig. 5, this experiment led to distortions in the internal structures of the colorized MRI. In our architecture, the cyclic modality adaptation technique leverages the information of the source image reconstruction, aiding the structural consistency across multimodal unregistered data, as shown in the Fig. 6.\nA2: Without segmentation network For this experiment, the semantic segmentation module Scs and segmentation loss Lseg are removed from our architecture, and only pseudo Cryosection synthesis executes cross-modality semantic information fusion for colorization. The absence of segmentation information leads to degradation in organ color variations in the colorized MRI as seen in the visual comparisons in Fig. 5.\nA3: Without segmentation network and pseudo Cryosection In this variant, we discard the semantic segmentation module along with the decoder Fx c' that performs pseudo Cryosection synthesis. Consequently, the Lssim loss is computed from the input MRI and the colorized MRI. As shown in Fig. 5, the colorized MRI is devoid of structural and color correspondence.\nA4: Segmentation on reconstructed Cryosection Here, we remove the pseudo Cryosection synthesis module Fx c' and propagate the reconstructed Cryosection synthesized from the cyclic generative network to the segmentation network. Pseudo Cryosection is used for building semantic associations between input MRI and ground truth Cryosection that are only partially registered. As seen in Fig. 5, this results in degradation in the color distribution of the colorized MRI upon removing Fx c'.\nA5: Without compression-activation mechanism In this variant, the compression-activation blocks in the skip connections are replaced with conventional convolutional layers to show the effectiveness and performance of our architecture comprising of compression-activation mechanism. The colorized MRI synthesized using this variant has limited color variations for anatomical regions and region-based global information since compression-activation mechanism facilitates noise suppression. As shown in the Fig. 6, finer structures in the colorized MRI are absent."}, {"title": "DISCUSSION AND CHALLENGES", "content": "Our proposed colorization architecture performs realistic color transfer from the ground truth Cryosection data to an input MRI. We trained our architecture on MRI and Cryosection data with partial registration between the two modalities. Our system is capable of building a semantic association between the above two modalities via the segmentation of ground truth Cryosection. Comparison with the state-of-the-art methods shows superior performance of our method which can preserve MRI structures and synthesize colors and textures of ground truth Cryosection. Currently, our current approach is memory intensive since cross-modality networks require two pairs of generators and discriminators for the respective adapting distributions along with the segmentation network of corresponding modalities. This is not a limitation, however, it requires a high-end computing resource for training and it would be beneficial to reduce this memory constraint. The colorized MRI from our method inherits the noise and lack of high-frequency details from the input MRI. Our future work is to incorporate a method for improving the resolution of the input MRI data to match with the high details of the ground truth Cryosection. Further, the segmentation data used in our method corresponds to the Cryosection data only. As a future work, we would like to modify the presented architecture to segment MRI data as an unsupervised task based on the segmentation data of Cryosection."}, {"title": "CONCLUSION", "content": "The primary contribution of this work is the development of a generator architecture for synthesizing colorized MRI images with Cryosection-like textural details of various anatomical structures. Our cross-modality adaptation-based deep learning architecture is multiscale, and captures global structural details. By utilizing segmentation information, our model effectively adapts the color distribution of the ground truth Cryosection data, ensuring coherence between the anatomical structures of the input MRI and the resulting colorized images. Our approach demonstrates superior performance in both colorization and texture synthesis compared to existing methods."}, {"title": "COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted using the Visible Korean Human data made available by KISTI via an agreement."}]}