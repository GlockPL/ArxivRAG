{"title": "Generative Model-Based Fusion for Improved Few-Shot Semantic Segmentation of Infrared Images", "authors": ["Junno Yun", "Mehmet Ak\u00e7akaya"], "abstract": "Infrared (IR) imaging is commonly used in various scenarios, including autonomous driving, fire safety and defense applications. Thus, semantic segmentation of such images is of great interest. However, this task faces several challenges, including data scarcity, differing contrast and input channel number compared to natural images, and emergence of classes not represented in databases in certain scenarios, such as defense applications. Few-shot segmentation (FSS) provides a framework to overcome these issues by segmenting query images using a few labeled support samples. However, existing FSS models for IR images require paired visible RGB images, which is a major limitation since acquiring such paired data is difficult or impossible in some applications. In this work, we develop new strategies for FSS of IR images by using generative model-ing and fusion techniques. To this end, we propose to synthesize auxiliary data to provide additional channel information to complement the limited contrast in the IR images, as well as IR data synthesis for data augmentation. Here, the former helps the FSS model to better capture the relationship between the support and query sets, while the latter addresses the issue of data scarcity. Finally, to further improve the former aspect, we propose a novel fusion ensemble module for integrating the two different modalities. Our methods are evaluated on different IR datasets, and improve upon the state-of-the-art (SOTA) FSS models.", "sections": [{"title": "1. Introduction", "content": "Semantic segmentation classifies each pixel of an image to a specific category, providing precise scene comprehension and object localization at the pixel level across numerous applications [43]. While RGB images are ubiquitous in computer vision, infrared (IR) images capture temperature-related details, enhancing visibility in challenging conditions such as low illumination, smog, and fog [7, 22, 24]. Thus, visual/RGB and IR images offer complementary information. However, in numerous applications, acquiring visual and IR data concurrently is impossible, including in defense systems [10], maritime object recognition [19, 41], and fire safety [3,20]. This has led to interest in semantic segmentation of standalone IR images [24, 29].\nHowever, IR image segmentation models face several challenges. First, obtaining large-scale annotated IR datasets is difficult due to expensive sensors and restricted access to data from defense and commercial applications [7]. This scarcity of annotated IR datasets limits the training of deep learning (DL) models, particularly with supervised learning [5]. Additionally, unseen or rare classes are common in IR imaging applications, posing a challenge for supervised DL models to generalize to new classes [32, 40]. Lastly, thermal cameras typically produce low-resolution, low-contrast IR images with unclear boundaries [24], and the single gray-scale channel provides limited information compared to RGB's three channels, which leads to performance issues in existing segmentation models [24, 29, 45].\nFew-shot segmentation (FSS) addresses these challenges by learning effective segmentation from a small number of labeled examples, potentially mitigating the lack of training data and improving generalization to unseen classes [9, 23, 30, 32, 39, 40, 42, 49]. This approach is particularly critical for defense applications of IR, as the increasing complexity of battlefield environments introduces new categories of targets, such as UAVs, weapons, and obstacles. This relevance also extends to firefighting. However, FSS models have found limited use for IR data due to issues including scarcity of IR data and the lower number of channels containing information compared to RGB images [2, 48, 51].\nIn cases where real data is insufficient, as is often the case with IR datasets, synthetic data can enhance DL model performance [46]. While image-to-image (I2I) translation methods can augment IR images from RGB images, they depend on RGB images and corresponding segmentation masks, requiring further annotation [47], especially if they lack corresponding masks. Thus, augmenting IR data while maintaining semantic categories poses a challenge.\nOn the other hand, to address channel number differences, various fusion networks [13, 36, 37] have been proposed to leverage IR image properties by fusing them with RGB data. Combining IR and visible spectra can synergistically enhance segmentation models, as they provide distinct insights. Multi-modal FSS frameworks [2,48,51] have been developed for paired RGB-IR datasets, improving IR image segmentation. However, these methods are limited to paired RGB-IR datasets, which are difficult to obtain in practice.\nTo tackle these challenges, we propose to use generative DL models along with associated novel fusion networks as an alternative strategy for synthesizing images from unpaired data [12, 52], as depicted in Fig. 1(b-c), aiming to improve the performance of FSS models for IR images without requiring paired RGB data. To this end, we propose the use of two generative DL methods for enhancing the performance. First, we generate lightness images (L value from the LAB color space) from existing IR images, maintaining the same gray channel while featuring more distinct boundaries, thereby augmenting the training data and mitigating the scarcity of IR data in public databases without additional annotation work. Second, we synthesize RGB images from the IR and lightness images generated in the initial steps and utilize them as auxiliary information through an additional meta-learning component to emphasize color space similarity. Afterward, a novel fusion process combines predictions from the two meta learners and the base learner, as shown in Fig. 1(d).\nOur FSS approach for IR images is evaluated on SODA (Segmenting Objects in Day And night) [24] and SCUT-SEG [45] datasets, demonstrating improvement over existing strategies.\nOur main contributions are :\n\u2022 We propose using generated lightness images as augmented training data to address the scarcity of IR data. Additionally, we exploit multi-modal data augmentation by generating synthesized RGB images to provide additional channel information.\n\u2022 We adapt the FSS task for IR images, and propose an FSS model with a novel meta learner and a multi-modal fusion ensemble module.\n\u2022 Our method achieves significant improvements on SODA and SCUTSEG datasets compared to SOTA FSS models.\n\u2022 We demonstrate that utilizing fusion techniques with generative methods yields promising results without paired RGB-IR data."}, {"title": "2. Related Work", "content": "In this section, we provide background on FSS methods for RGB and IR images, as well as I2I translation with generative diffusion models."}, {"title": "2.1. Few-Shot Segmentation", "content": "FSS is a few-shot learning strategy for segmenting objects of interest in images using only a few annotated samples. A common FSS architecture involves two branches [32]. The first branch generates classifier parameters by processing support images, capturing relevant information. The second branch, the segmentation branch, receives a query image and produces a segmentation mask using the parameters from the first branch.\nOutstanding achievements in FSS have been made by leveraging prototype learning methods, a type of metric-based approach [21, 33, 38, 40]. For instance, masked average pooling [49] enhanced the extraction of class representative prototype vectors from the support set. PFENet [39] introduced a non-parametric prior mask generation method based on cosine similarity of high-level features, improving generalization. HSNet [26] used hyper-correlation squeeze networks to leverage multi-level feature correlations and employed lightweight 4D convolutions for fine-grained segmentation. An attention-based multi-context guiding network [17] emphasized context information from small to large scales to guide query branches globally. To address biases in previous frameworks reliant on meta-learning, BAM [23] proposed a novel FSS approach with three components: a meta learner designed to recognize novel, unseen classes, following the conventional meta-learning paradigm; a base learner trained in a supervised manner on known base classes; and an ensemble module that integrates coarse predictions from both learners for accuracy.\nBuilding on the architecture of BAM and previous frameworks, MSANet [18] further enhanced the FSS model by introducing two guiding modules into the meta learner: the multi-similarity module and the lightweight attention block. The multi-similarity module calculates visual correspondences between features extracted from intermediate and high-level layers of support and query images, establishing meaningful relationships between the two sets of features. The lightweight attention module, composed of convolutional neural networks, enables the meta learner to focus on the ideal target in the query image."}, {"title": "2.2. Infrared Image Few-Shot Segmentation", "content": "Segmentation of IR images has seen advances despite its challenges. A major limitation when relying solely on IR images for segmentation has been related to issues such as obscure boundaries, low resolution, and low contrast. To overcome these issues, methods [24, 29] applying edge information have been suggested, improving the performance of the segmentation models. Furthermore, visual-thermal spectrum fusion networks [13, 25, 36, 37] have been proposed to compensate for the individual limitations of visual and IR images, and to incorporate the strengths of both modalities. As part of these advancements, visual-thermal FSS methods that combine FSS and visual-thermal fusion framework have been proposed to address data sparsity and the growing generalization challenge of unseen classes [2, 48, 51]. However, these approaches still face the problem of acquiring paired IR-RGB data, imposing significant limitations in existing applications. Therefore, an approach that only utilizes IR images for FSS is necessary."}, {"title": "2.3. Generative Diffusion Models", "content": "Diffusion models have achieved remarkable success in unsupervised I2I translation [8, 15, 34, 35]. One such diffusion model, DDPM [15] generates data by progressively denoising through a series of steps. This model initiates a forward process, which gradually adds Gaussian noise to the data over T time steps:\n$x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon, \\epsilon \\sim N(0, I)$ (1)\n$q(x_t|x_{t\u22121}) = N(x_t; \\sqrt{1 \u2013 \\beta_t}x_{t\u22121}, \\beta_tI)$ (2)\nwhere $\\beta_t$ controls the amount of noise added at each step. Following the forward process, the reverse process gradually generates images from a desired distribution through iterative denoising, maximizing data likelihood:\n$P_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x,t))$ (3)\nwhere $\\mu_\\theta$ is a learned function, and $\\Sigma_\\theta$ is known variance with a preset schedule. After training the reverse process, the sampling generates a denoised image $x_0$ from a noise vector $x_T$ using reverse diffusion. For each time step t from T to 1, update the image to approximate $x_0$ as follows:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t) + \\sigma_tz_t, z_t \\sim N(0, I)$ (4)\nwhere $\\alpha_t$ and $\\bar{\\alpha}_t$ control the variance schedule, $\\epsilon_\\theta(x_t, t)$ is the neural network's noise estimate at step t, and $\\sigma_t$ is the standard deviation of the noise added at each step.\nAlthough denoising diffusion models improve sample quality and offer more stable training, they often require numerous inference steps [1], presenting computational challenges. An alternative approach [28] is to utilize an adversarial conditional diffusion approach, integrating non-diffusive and diffusive models to address these challenges for I2I translation. Here, non-diffusive models utilize generative adversarial networks (GANs) with cycle-consistent architectures for unsupervised training [11, 52], generating conditioning images $x_0$ and $y_0$ to guide denoising. Meanwhile, diffusive models take these source-conditioning images [31] and efficiently sample images using adversarial projectors $G_x$ and $G_y$ [44] to capture reverse transition probabilities over large step sizes:\n$x_t = \\sqrt{1 - \\gamma_t} x_{t-k} + \\sqrt{\\gamma_t} \\epsilon, \\epsilon \\sim N(0, I)$ (5)\n$q(x_t | x_{t\u2212k}) = N(x_t; \\sqrt{1 \u2013 \\gamma_t} x_{t\u2212k}, \\gamma_t I)$ (6)\nwhere $k \\gg 1$ denotes the step size, the denoising generators $G_x$ and $G_y$ produce $x_0$ and $y_0$ respectively, by implementing the distribution as follows:\n$P_\\theta(x_{t\u2212k} | x_t,Y) := q(x_{t\u2212k} | x_t, x_0 = G(x_t,y,t))$ (7)\nMeanwhile, for two adversarial diffusion processes, discriminators $D_x$ and $D_y$ respectively evaluate pairs $(x_t, x_{t-k})$ and $(y_t, \\hat{y}_{t-k})$, to determine their authenticity, as well as pairs $(x_t, x_{t-k})$, $(y_t, y_{t-k})$. After completing small reverse diffusion steps, the final denoised image can be sampled from the distribution $x_0 \\sim P_\\theta (x_0 | x_k, Y)$."}, {"title": "3. Methodology", "content": "To enhance the performance of FSS for IR data, we incorporate additional data derived from outputs of generative diffusion networks using fusion models. Our main motivation is to prioritize semantic preservation and enhance contrast or color information in IR images to improve the performance of IR FSS models. To this end, we leverage generative diffusion models to synthesize 'Generated RGB Images' for auxiliary information, which are used in conjunction with novel fusion networks for integrating features from IR and these auxiliary data. Additionally, we also use 'Generated Lightness Images' for data augmentation to address IR data scarcity issues."}, {"title": "3.1. Few-Shot Segmentation Baseline", "content": "For completeness, we summarize the key features of the baseline network, MSANet [18], which we use for FSS. MSANet, inspired by the BAM architecture [23], consist of three main parts including a base learner, a meta learner, and an ensemble module as illustrated in Fig. 1(a). Further architectural and training details are provided in SuppMat.\nThe base learner is PSPNet [50], with a ResNet-50/101 [14]. Trained on known base classes in a supervised manner, it predicts regions of base classes in query images and suppresses falsely activated regions in the meta learner's output, yielding $F_{Base}$. The base learner's loss is:\n$L_b = \\frac{1}{N_{batch}}\\sum_{i=1}^{N_{batch}} CE(softmax (F_{Base}^{(i)}, m_{Base}^{(i)}),$ (8)\nwhere CE denotes the cross-entropy loss, $m_{Base}^{(i)}$ represents the ground truth of the base classes in the query set, and $N_{batch}$ is the batch size.\nThe meta learner aims to recognize unseen classes by discerning relationships between these features. Using intermediate to high-level features from the support set, the meta learner employs masked average pooling to create class prototype vectors and an attention module for foreground-focused feature mapping. It also generates a prior mask similar to PFENet [39] and utilizes a multi-similarity module across multiple layers from both support and query sets to establish visual correspondences. The loss for the meta learner is calculated using binary cross-entropy (BCE):\n$L_m = \\frac{1}{N_{episode}} \\sum_{i=1}^{N_{episode}} BCE(softmax (F_{Meta}^{(i)}, m_{Meta}^{(i)}),$ (9)\nwhere $F_{Meta}$ is the binary meta prediction mask, $m_{Meta}$ are the labels for unseen classes in the query set, and $N_{episode}$ is the total training episodes per batch.\nThe ensemble module enhances segmentation accuracy by refining predictions $F_{Meta}$ and $F_{Base}$ from both learners through an adjustment and an ensemble phase. In the former phase, it derives an adjustment factor map from the differences between query and support image pairs, and refines $F_{Meta}$'s foreground and background. In the latter phase, this adjusted background is combined with with $F_{Base}$ to produce the final background map. These processes yield the final prediction map $F_{final}$. The loss for final prediction is:\n$L_f = \\frac{1}{N_{episode}}\\sum_{i=1}^{N_{episode}} BCE(softmax (F_{final}^{(i)}, m_{final}^{(i)}),$ (10)\nWe designate this network as our baseline for the novel modifications outlined in Fig. 2 and Section 3.3."}, {"title": "3.2. Image-to-Image Translation", "content": "We adopt the adversarial conditional diffusion model in [28] as a generative network to perform I2I translation as shown in Fig. 3. This model is trained using unpaired IR-RGB images from a publicly available IR-RGB dataset. Afterward, the trained model generates two types of new datasets: generated lightness dataset for data augmentation and generated RGB dataset for auxiliary information, derived from existing IR images, as outlined in Fig. 4."}, {"title": "Data Augmentation (IRAug)", "content": "To mitigate the scarcity of IR data in public databases, we adapt a data augmentation strategy using generative models. RGB images from IR-RGB data are first converted to the LAB color space. We then employ unpaired I2I translation, where IR images are randomly selected without paired lightness domain (L) images. This approach generates lightness images $IR_L$ from existing IR images, preserving their grayscale properties while enhancing boundaries. This method effectively expands the training dataset for IR image-based tasks without requiring additional annotations, thereby diversifying data distributions and improving model robustness. We define the dataset $IR_{Aug}$, which comprises both IR and $IR_L$."}, {"title": "Auxiliary Information (RGB Aux)", "content": "A significant innovation in our work is the use of generative models to complement the limited contrast in IR images, which complicates the adoption of existing FSS models designed for RGB images. This challenge primarily arises from spectral differences between IR and visible spectra. We hypothesize that enhancing the model with plausible colors to distinguish object boundaries and features could enhance the FSS process. To achieve this, we employ a generative model for unpaired I2I translation on the IR-RGB dataset, resulting in $RGB_{IR}$.\nFurthermore, we also propose to augment this strategy by converting previously obtained lightness images $IR_L$ into the RGB domain, yielding $RGB_L$. Thus, $RGB_{IR}$ and $RGB_L$ datasets serve as auxiliary information in our methodology, and is denoted as the dataset $RGB_{Aux}$. The major benefit of our approach is that we can pair augmented IR images $IR_{Aug}$, comprising both IR and $IR_L$, with generated RGB datasets, $RGB_{Aux}$."}, {"title": "3.3. Novel Fusion for Auxiliary Information", "content": "Extracting Informative Features from ($RGB_{Aux}$). We propose a strategy employing an encoder and a novel component, the Meta Learner RGB, which is structurally identical to the Meta Learner IR, to leverage auxiliary information from the $RGB_{Aux}$ dataset, as shown in Fig. 2 and further detailed in SuppMat. Initially, the encoder is trained using both $IR_{Aug}$ and $RGB_{Aux}$ data to capture shared features across the IR and RGB domains during the base learner training stage. The base learner is shared across two domains and produces two probability maps: one from the IR domain query set and another from the RGB domain query set. Subsequently, utilizing features from the pre-trained encoder, Meta Learner RGB evaluates the similarity between support and query sets in RGB domain, while Meta Learner IR performs a similar task in IR domain, leveraging shared weights from atrous spatial pyramid pooling (ASPP) module [6] and the decoder. Consequently, this approach yields two meta predictions and two base predictions.\nFusion for IR and RGB Domains. To harness these essential predictions, we propose a fusion ensemble module, as depicted in Fig. 2. This module comprises two identical ensemble modules and convolutional layers. Each ensemble module processes meta and base predictions separately in the IR and RGB domains, resulting in foreground/background predictions for each domain. Finally, foreground predictions from each domain are merged using 1 \u00d7 1 convolutional layers, and similarly, background predictions are merged, resulting in final fore/background predictions. This fusion ensemble method effectively integrates features from different modalities. For the loss, we include $L_{RGB}$ and $L_{RGB}$, defined in an analogous manner to Eq. (8) and Eq. (9), yielding our proposed total loss:\n$L_{total, proposed} = L_f + L_m + L_b + L_{RGB} + L_{RGB}$ (11)\nNote our novel architectural changes only lead to a modest increase of 2.18% and 1.60% in total parameters over the baseline, using ResNet-50 and ResNet-101, respectively."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Few-Shot Segmentation Baseline", "content": "Since the IR FSS problem has not been extensively explored in the literature, there are no established baselines. We choose MSANet as our baseline FSS model and enhance its performance using generative model-based fusion techniques. We conduct experiments on two IR datasets, evaluating four methods: baseline with IR; method1, combining IR and simple augmentation-based learning with $IR_L$; method2 and method3, which integrate the baseline method with additional meta learners and fusion networks. method2 uses IR and $RGB_{IR}$, while method3 utilizes the full set of proposed auxiliary data generated by the diffusion model $IR_{Aug}$ and $RGB_{Aux}$ data."}, {"title": "4.2. Data", "content": "IR Datasets. To demonstrate the effectiveness of our approach, we utilize two IR image datasets: SODA [24] and SCUTSEG [45]. SODA dataset contains 2,168 IR images (1,168 for training, 1,000 for testing). It features labeled images of 20 semantic regions from diverse real-world scenes, encompassing both indoor and outdoor environments with varying lighting conditions, imaging blur, and resolutions. SCUTSEG dataset consists of 2,010 nighttime driving IR images (1,345 for training, 665 for testing) depicting common objects encountered during driving across 9 classes.\nDuring the meta learner training stage, each dataset follows the FSS methodology [32], with object categories evenly divided into four folds"}, {"title": "5. Conclusion", "content": "In this paper, we explore FSS of IR images, which holds significant promise and importance for various fields. FSS tackles the challenge of limited IR data for supervised training and the emergence of new classification classes. Our proposed techniques and results demonstrate the potential of using generative DL models and fusion ensemble networks to enhance FSS performance for IR images. Specifically, we use generative models for data augmentation and more critically to generate auxiliary information to tackle issues related to limited contrast in IR images. For the latter, we introduce a novel fusion ensemble module to use information from auxiliary generated data. Our methods improve FSS performance without requiring paired IR-RGB datasets as previous methods did."}]}