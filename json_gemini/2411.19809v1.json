{"title": "Q-learning-based Model-free Safety Filter", "authors": ["Guo Ning Sue", "Yogita Choudhary", "Richard Desatnik", "Carmel Majidi", "John Dolan", "Guanya Shi"], "abstract": "Ensuring safety via safety filters in real-world robotics presents significant challenges, particularly when the system dynamics is complex or unavailable. To handle this issue, learning-based safety filters recently gained popularity, which can be classified as model-based and model-free methods. Existing model-based approaches requires various assumptions on system model (e.g., control-affine), which limits their application in complex systems, and existing model-free approaches need substantial modifications to standard RL algorithms and lack versatility. This paper proposes a simple, plugin-and-play, and effective model-free safety filter learning framework. We introduce a novel reward formulation and use Q-learning to learn Q-value functions to safeguard arbitrary task specific nominal policies via filtering out their potentially unsafe actions. The threshold used in the filtering process is supported by our theoretical analysis. Due to its model-free nature and simplicity, our framework can be seamlessly integrated with various RL algorithms. We validate the proposed approach through simulations on double integrator and Dubin's car systems and demonstrate its effectiveness in real-world experiments with a soft robotic limb.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning has shown tremendous progress in a variety of decision making and control problems, particularly games [1], locomotion [2], and robot manipulation [3]. However, deploying RL in real-world robotic operations presents unique challenges, with safety being a paramount concern. Unfortunately, devising effective safety mechanisms is challenging. To tackle the issue of safety, various tools from control theory such as control barrier functions (CBFs) [4]\u2013[6], model predictive safety certification [7], Hamilton-Jacobi reachability (HJR) [8], have been integrated with the standard reinforcement learning pipeline. These methods require accurate models of robotic systems, which are often unavailable. To address this issue, recent research has shifted towards model-free approaches [9]. However, these approaches [10], [11] are either not robust, requiring near-perfect safety value function approximations [12], or drastic changes to the standard model-free RL pipeline [13]. In this paper, we introduce a model-free safety filter that has theoretical safety guarantees in the optimal case, but is still robust to suboptimal conditions and fits seamlessly into the standard model-free RL pipeline."}, {"title": "A. Related Work", "content": "Model-free Safe Reinforcement Learning: Model-free Safe Reinforcement Learning (RL) is frequently formulated as a Constrained Markov Decision Process (CMDP) [14]. A CMDP extends a standard MDP by incorporating an additional cost signal to identify state-action pairs that violate constraints. By defining cost thresholds, it is possible to derive policies with low failure probabilities. Popular methods for solving CMDPs include Lagrange multiplier methods [15]\u2013[17], projection methods [18], and penalty function methods [19]. Another line of work in model-free RL focuses on learning a safety critic to filter out unsafe actions and is also the inspiration behind our proposed approach. Safety Q-functions for Reinforcement Learning (SQRL) [20] learns a safety critic that predict the future failure probability and uses the critic to constrain the nominal policy. The framework involves pre-training the safety critic and then fine-tuning the policy on target tasks using the learned safety precautions.\nExpanding on this approach, Recovery RL [21] additionally learns a recovery policy along with safety critic. They utilize two separate policies for task and recovery to learn safely without compromising task performance.\nIn contrast to [21], [22] focuses on jointly optimizing performance and safety. They modify the reward design proposed in [23] by incorporating the safety critic into the reward to prevent exploration in unsafe regions during training. [24] introduces a binary safety critic by leveraging the idea that safety is a binary property. Our formulation also follows the safety critic structure but introduces a novel reward design to ensure safety.\nModel-based Safe Reinforcement Learning: Model-based safe RL methods are more sample efficient compared to model-free methods. Recent studies integrate CMDPs with model-based RL to minimize training violations and speed up learning [23], [25], [26]. Safe Model-Based Policy Optimiza-"}, {"title": "II. PRELIMINARIES", "content": "Consider a car traveling along a road (Fig. 2) where areas like the curb are designated as unsafe states ($\\mathcal{X}_{unsafe}$) due to hazards. If the car is traveling at 80km/h and is only 1m from the curb, a collision is inevitable even with maximum deceleration, defining this state as the irrecoverable state ($\\mathcal{X}_{irrec}$). In contrast, if the car is 1km away from the curb, there are various control actions that can ensure safety, categorizing this state as an absolutely safe state ($\\mathcal{X}_{safe}$). Therefore, we can divide the state space $\\mathcal{X}$ into three subspaces: $\\mathcal{X}_{safe}$, $\\mathcal{X}_{irrec}$ and $\\mathcal{X}_{unsafe}$."}, {"title": "Definition 1.", "content": "$\\mathcal{X}_{safe}$: The set of states for which there is always an action to keep out of the unsafe region and stay in the same set. For example a speeding car far from any obstacles.\nMathematically, if $x_t \\in \\mathcal{X}_{safe}$ then $\\exists u \\in \\mathcal{U}$ where $x_{t+1} = f(x_t, u)$ and $X_{t+1} \\in \\mathcal{X}_{safe}$. $f$ denotes system dynamics, $X_t$ denotes a state in the system at time t, $u$ stands for an action to take at $x$ and $\\mathcal{U}$ denotes the set of possible actions to take. This implies that $\\mathcal{X}_{safe}$ is forward invariant."}, {"title": "Definition 2.", "content": "$\\mathcal{X}_{irrec}$: The set of states of which is currently not in $\\mathcal{X}_{unsafe}$, but will inevitably reach the unsafe region despite all actions taken. For example, a speeding car very close to an obstacle. Due to its speed, despite applying a maximum breaking deceleration, there is nothing to prevent its collision.\nMathematically, if $X_t \\in \\mathcal{X}_{irrec}$ then $X_{t+T} \\in \\mathcal{X}_{unsafe}$ for any $u_t$, $u_{t+1}...., u_{t+T} \\in \\mathcal{U}$ where $T$ is a positive integer."}, {"title": "Definition 3.", "content": "$\\mathcal{X}_{unsafe}$: The set of states that is human defined to be unsafe either to other humans or the system.\nFurthermore, these subspaces have the following relations:\n$\\mathcal{X} = \\mathcal{X}_{safe} \\cup \\mathcal{X}_{irrec} \\cup \\mathcal{X}_{unsafe}$                                                                                    (1)\n$\\mathcal{X}_{unsafe} \\cap \\mathcal{X}_{safe} = \\mathcal{X}_{unsafe} \\cap \\mathcal{X}_{irrec} = \\mathcal{X}_{safe} \\cap \\mathcal{X}_{irrec} = \\emptyset$                                (2)\nIdentifying irrecoverable states is challenging due to system dynamics, but crucial as entering $\\mathcal{X}_{irrec}$ leads to $\\mathcal{X}_{unsafe}$."}, {"title": "B. Q Learning", "content": "We propose to use Q-learning for constructing a safety filter that filters hazardous actions and keeps the system in $\\mathcal{X}_{safe}$. In Q-learning, the goal is to learn the expected discounted cumulative reward for a state-action pair in a Markov Decision Process (MDP) [27]. The Q-value is updated during training using the Bellman update rule, under deterministic dynamics:\n$Q(x, u) = r(x, u, x') + \\gamma \\max_{u'} Q(x', u')$                        (3)\nwhere $r$ is the reward function and $\\gamma$ is the discount factor. The value function is defined as:\n$V(x) = \\max_{u} Q(x, u)$                                                                                     (4)\nWe propose learning optimal Q-function and value function $Q_{safe}$ and $V_{safe}$, respectively using a specialized reward"}, {"title": "III. METHODOLOGY", "content": "In this work, we propose a Q-learning-based, model-free reinforcement learning approach to construct a safety filter. A block diagram of the framework is shown in Fig. 1. Our method leverages the fact that $Q_{safe}$ and $V_{safe}$ can be learned off-policy, enabling the simultaneous training of a task-specific policy $\\pi_{task}$ and the safety policy $\\pi_{safe}$.\nThe two policies are trained in parallel but remain decoupled by a gating mechanism that sorts observations into separate replay buffers based on episodic conditions. This decoupling allows $\\pi_{task}$ to be swapped with any task during testing. Our theoretical analysis focuses on the formulation of $r_{safe}$ and its impact on $Q_{safe}$, $V_{safe}$, and $\\pi_{safe}$, as $\\pi_{task}$ can be any task policy."}, {"title": "A. Formulation and Analysis", "content": "1) Reward Formulation: We define the safety reward function $r'_{safe}(x, u, x')$ as:\nr'_{safe}(x, u, x') = \\begin{cases}\n    \\frac{1}{\\gamma\\epsilon(1 - \\gamma)}l(x), & x, x' \\notin X_{unsafe} \\\\\n    -1, & x \\notin X_{unsafe}, x' \\in X_{unsafe} \\\\\n    -1, & x, x' \\in X_{unsafe}\n\\end{cases}\n(5)\nwhere $x$ is the state comprising of time $t$, position and velocity. $x'$ is the state the system transitions to after applying control $u$, and $\\gamma \\in (0, 1)$ is the discount factor. The function $l(x) \\in (0,1]$ increases as the system moves deeper into the safe region; for example:\nl(x) = \\frac{d(x)}{\\max_{x \\in X_{unsafe}} d(x)},(6)\nwhere $d(x)$ is the positive signed distance to $X_{unsafe}$. For $x$ where $d(x) = 0$, i.e. on the boundary of $X_{unsafe}$, we consider $x \\in X_{unsafe}$\nDuring training, we optimize the $Q_{safe}$ function using standard Bellman updates using $r_{safe}$.\nAssumption 1. An episode terminates at the time step when the agent reaches the unsafe region or at maximum episode length, $T$.\nWe consider a finite episode length scenario as from Eq. 5, when $x \\in X_{safe}$ and $x' \\in X_{unsafe}$, the reward can become unbounded if $t$ is very large. Therefore, we define $T$ as the episode length during training to bound the reward and prevent divergence.\nOur design of $r_{safe}$ aims to ensure a clear separation on $V_{safe}$ between states in $X_{safe}$, and those in $X_{irrec}$. While an intuitive approach is to add a large negative penalty upon entering the unsafe region [23], fixed penalties diminish"}, {"title": "2) Analysis:", "content": "The true value function $V_{safe}$ has the following properties."}, {"title": "Property 1.", "content": "If $x \\in X_{safe}$, then the optimal value of the value function satisfies the following: $0 < V_{safe}(x) \\le \\frac{1-\\gamma^T}{1-\\gamma}$"}, {"title": "From the definition of", "content": "$X_{safe}$, $X_{safe}$ is forward invariant and thus the agent accumulates the discounted reward $l(x)$ for $T$ steps. Since, $0 < l(x) < 1$, therefore:\n$\\sum_{i=0}^{T} \\gamma^i. 0 < V_{safe}(x) = \\sum_{i=0}^{T} \\gamma^{i}l(x) \\leq \\sum_{i=0}^{T} \\gamma^{i}$\n$\\rightarrow 0 <V_{safe}(x) \\leq \\frac{1 - \\gamma^{T+1}}{1 - \\gamma}$                                                                                   (7)\nSince $l(x)$ is continuous and increases as the system moves away from $X_{unsafe}$, so are values of $V_{safe}$."}, {"title": "Property 2.", "content": "If $x \\in X_{irrec}$, then the optimal value of the value function satisfy the following $-\\frac{1}{1-\\gamma} + \\gamma < V_{safe}(x) < 0$.\nIn the irrecoverable region, episodes terminate in a finite number of steps with a large negative reward upon entering the unsafe region. Therefore, for $x \\in X_{irrec}$, the optimal value function $V_{safe}(x)$ is represented by the following summation:\n$V_{irrec}(x) = \\sum_{i=N+1}^{T} - \\gamma^{i} + \\sum_{i=0}^{N-1} \\gamma^{i}l(x)$,\nwhere $N$ is a positive integer representing the last time step in $X_{safe}$. Since $0 < l(x) \\leq 1$, substituting and simplifying:\n$\\frac{-1 + \\gamma^{T+1} - \\gamma^{N+1}}{1 - \\gamma} < V_{safe}(x) \\leq \\frac{-1 + \\gamma^{T+1} - \\gamma}{1 - \\gamma}$\nas $\\gamma \\in (0,1)$.                                                                                   (8)"}, {"title": "Property 3.", "content": "If $x \\in X_{unsafe}$, then the optimal value of the value function satisfy the following $V_{safe}(x) = -\\frac{1 - \\gamma^{T+1}}{1-\\gamma}$.\nWhen $x \\in X_{unsafe}$, the episode terminates immediately with a penalty of -1. Therefore, this terminal state accumulates the discounted cumulative reward of -1 for $T$ steps:\n$V_{safe}(x) = - \\sum_{i=0}^{T} \\gamma^i = -\\frac{1 - \\gamma^{T+1}}{1-\\gamma}$                                                                                  (9)\nThese properties show that $V_{safe}(x)$ generally increases from $X_{unsafe}$ to $X_{safe}$, especially the transition from $X_{irrec}$ to $X_{safe}$. This indicates that the boundary between $X_{irrec}$ and $X_{safe}$ corresponds to a threshold value distinguishing these two regions in $V_{safe}(x)$. Fig. 3 depicts a conceptual visualization of the $V_{safe}(x)$."}, {"title": "Theorem 1.", "content": "Given $Q_{safe}$ and if $x \\in X_{safe}$ and $\\pi_{filter}(x)$ is followed, then $x' \\in X_{safe}$ where $x'$ is the state of the system after taking $\\pi_{filter}(x)$ from $x$.\nProof. We proceed to prove by contradiction, assuming first that $x' \\notin X_{safe}$. Letting $\\pi_{filter}(x) = a$, from the Bellman Optimality equation of Q-Learning:\n$Q_{safe}(x, a) = r_{safe}(x, a, x') + \\gamma \\max_{u \\in \\mathcal{U}} Q_{safe}(x', u)$\n$= r_{safe}(x, a, x') + \\gamma V_{safe}(x')$\nSince $x' \\notin X_{safe}$, that implies:\n$r_{safe}(x, a, x') < 0$; $\\gamma V_{safe}(x') < 0$; $Q_{safe}(x, a) < 0$\nFrom conditions outlined for $\\pi_{filter}(x)$ in Eq.12:\n$a = \\underset{u \\in \\mathcal{U}}{\\text{argmax}} Q_{safe}(x, u)$\nHowever\n$Q_{safe}(x, a) = Q_{safe} (x, \\underset{u \\in \\mathcal{U}}{\\text{argmax}} Q_{safe}(x, u))$\n$= \\underset{u \\in \\mathcal{U}}{\\text{max}} Q_{safe}(x, u) = V_{safe}(x) < 0$\nAccording to Eq. 11, $x \\in X_{irrec}$ or $x \\in X_{unsafe}$, which is a contradiction."}, {"title": "Theorem 1 demonstrates", "content": "that the filtering scheme described in Eq. 12 selects actions that keep the system within $X_{safe}$. Moreover, the scheme intervenes only when the system is about to exit $X_{safe}$, thereby minimally impacting the performance of the task policy."}, {"title": "B. Implementation", "content": "To implement $\\pi_{filter}$ and keep the system within $X_{safe}$, the task policy action is replaced by a greedy policy that maximizes the learned $Q_{safe}$ under $r_{safe}$. We use Soft Actor-Critic (SAC) [28] for simulations with continuous action spaces and Deep Q-Learning (DQN) [29] for real-world validation."}, {"title": "1) Simultaneous Training of Task Policy and Safety Policy:", "content": "Leveraging the off-policy nature of SAC and DQN agents, we simultaneously train a task agent that maximizes task reward and a safety agent that maximizes the proposed safety reward. Both agents share the environment and rolled-out data but maintain independent replay buffers. A gating mechanism prevents the safety agent from collecting observations once the system enters the unsafe region. Initially, observations are added to both agents' replay buffers with their respective rewards. When the safety agent reaches the unsafe region, it stops receiving new observations, while the nominal agent continues until the episode ends. This approach minimizes interference with the task agent, preserving task performance, and ensures the safety policy remains valid for any task policy despite simultaneous training and shared data.\nDue to Assumption 1, the safety agent does not observe the unsafe region, as the episode terminates once the unsafe region is reached. Therefore a supervised loss, similar to the loss used in [11], is introduced to the safety agent in addition to the standard RL losses to classify whether a state belongs to $X_{unsafe}$. The loss is defined as follows:\n$L_{unsafe} = ||V_{safe}(x) + \\frac{1 - \\gamma^{T+1}}{1-\\gamma} -||, \\forall x \\in X_{unsafe}$                                                                                   (13)\nwhere $V_{safe}$ denotes the learned value function."}, {"title": "2) Tuning the threshold:", "content": "Converging to $V^{*}_{safe}$ is challenging, and while $\\epsilon_2 = 0$ is optimal in theory, it may not represent the boundary for $V_{safe}$. Our filter also loses guarantees in suboptimal cases. However, if $V_{safe} \\approx V_{safe}^{*}$, a threshold separating $X_{safe}$ and $X_{irrec}$ should exist due to the increasing nature of $V_{safe}$.\nAs Fig. 3 shows, raising $\\epsilon_2$ gives more conservative safe region estimates, leading $\\pi_{filter}$ to select safer actions, reducing the risk of irrecoverable states. Starting from $\\epsilon_2 = 0$, one can tune how conservative the learned filter will act, enabling practical use of suboptimal $Q_{safe}$ in the safety filter. Section 4 demonstrates that increasing the threshold ensures safety despite discrepancies between the learned and true value and Q functions."}, {"title": "IV. RESULTS", "content": "We experimentally validate our method in simulations and on real hardware. Simulations involve a double integrator system for basic validation and a Dubin's car environment for static obstacle avoidance and to compare with existing methods. On real hardware we demonstrated the filter's capability to constrain a soft robotic limb within a specified region."}, {"title": "A. Simulation Environments", "content": "1) Double Integrator: The double integrator state is two-dimensional, with position $x$ and velocity $\\dot{x}$. Safe bounds are $\\pm 2$ m and $\\pm 3$ m/s, with absolute maximum bounds of $\\pm 4$ m and $\\pm 5$ m/s. The input acceleration is bounded between $\\pm 2$ m/s\u00b2, and the task is to reach a goal at 1.8 m (neon green dashed line in Fig. 4).\nThe two-dimensional state allows easy visualization of the learned value function to empirically validate our theory. In Fig. 4, safety policy actions (acceleration) are shown on the left, and the value function on the right. The solid black contour represents the analytical safe set [13]. The black dashed lines denote the learned safe set where $\\epsilon_2 = 0$, which lies entirely within the analytical safe set. The blue dashed line represents the contour for $\\epsilon_2 = 90$; as per our formulation, this contour is smaller than the $\\epsilon_2 = 0$ contour, leading to more conservative estimates of $X_{safe}$, which is observed empirically.\nMoreover, on the left of Fig. 4, at the boundary of the analytical safe set, the actions align with intuition: when approaching the right boundary at high speed, the action is a leftward force (red), and vice versa. These empirical results are consistent with our theoretical expectations."}, {"title": "2) Dubin's car:", "content": "Because the double integrator's goal lies within the safe region, the nominal agent remains safe by simply achieving the task, which does not fully demonstrate our safety filter's impact. Therefore, we tested our method in the Dubin's car environment, featuring complex nonlinear dynamics and conflicts between the nominal task and safety, e.g. the shortest path to goal goes through $X_{unsafe}$. In this environment, safety bounds are $\\pm 2$ m with a central keep-out region of radius 1 m. The absolute maximum bounds are defined with an input velocity of 1.2 m/s, aiming for a goal at (1.8 m, 1.8 m) with a radius of 0.5 m. As shown in Tab. I,"}, {"title": "B. Hardware Experiments", "content": "1) Hardware Setup: To experimentally validate the proposed scheme, we implemented it on a soft robotic limb similar to the setup used in [33] and is depicted in Fig. 6. The soft limb is injection-molded from silicone (Smooth-Sil 945) and actuated by four shape memory alloy (SMA) coils (Flextinol) positioned at the up, down, left, and right orientations. The coils contract when heated by a Pulse Width Modulated (PWM) controlled electrical current that are applied at the coil tips. When the temperature rises above 90\u00b0C, due to the phase transition of the SMA, the limb bend in a specific direction, increasing the bending angle.\nA capacitive bend sensor (Bend Labs Digital Flex Sensor - 2-Axis, 4 Inch) is located in the center to detect the bending angles of the limb. The bending angles ($\\theta_x$, $\\theta_y$) are defined as the angle between the tangent at the limb tip and the vertical, measured along both the x and y axes. Since the soft limb's actuation depends on heat increased heat leads to greater contraction\u2014and due to the malleable nature of silicone, the system exhibits highly nonlinear dynamics. Furthermore, we control only the PWM signals that induce heating to raise the temperature, but relying only on passive cooling to lower it. These factors complicate the dynamics, rendering an analytical mapping from PWM signals to limb states intractable."}, {"title": "2) Data Setup:", "content": "To enable efficient training, instead of naively deploying RL training on the hardware, we first create a data driven simulator to approximate the system dynamics. Data was collected through random motor wob-"}, {"title": "3) RL Setup:", "content": "To validate our method, we trained the safety policy using DQN with the observation space $x = [\\theta_x, \\theta_y, \\dot{\\theta_x}, \\dot{\\theta_y}]$ and an action space of four actions\u2014move upper right ($u_1$), upper left ($u_2$), lower left ($u_3$), and lower right ($u_4$)\u2014as depicted in Fig. 6. The safety policy was trained using our approximated dynamics simulator to constrain movements within $\\pm 30\u00b0$ on both x and y axes. The constraint boundary is shown as a green circle in the right figure of Fig. 5, i.e., $X_{safe}$ represents states where $||\\theta|| < 30\u00b0$. For the task policy, we used a predefined action sequence:\n$u_1$ for 3 seconds, $u_4$ for 3 seconds, $u_3$ for 6 seconds, $u_2$ for 12 seconds, $u_1$ for 6 seconds, and $u_3$ for 3 seconds, aiming to trace a diamond-shaped trajectory. The real-life rollout of this sequence is shown in orange on the right of Fig. 5. The task of the safety filter is to constraint the limb movement within the $||\\theta|| < 30\u00b0$ boundary."}, {"title": "4) Hardware Results:", "content": "As shown in the right of Fig. 5, the safety policy successfully keeps the limb within the safe region, as the blue trajectories, which represent the recorded limb trajectory with our safety filter, are entirely contained within the safe region denoted by the green circle. However, due to the sim-to-real gap between the learned dynamics and the real limb dynamics, $\\epsilon_2 = 90$ was required for our scheme to work in real life, resulting in a very conservative filter. Despite this conservatism, the empirical results validate the robustness of our method in maintaining system safety, even in the presence of a significant sim-to-real gap."}, {"title": "V. CONCLUSION", "content": "We introduce a method to identify safe, unsafe, and irrecoverable states through reward shaping in standard reinforcement learning. We devise a policy filter with theoretical safety guarantees under optimal conditions, which, with threshold tuning, also performs well under suboptimal conditions like large sim-to-real gaps.\nOur method allows simultaneous training of a task-specific policy and a safety policy that generalizes to any task policy. It matches existing methods in simulations and is validated on a real-world, highly nonlinear soft silicone limb, making it more versatile, easier to train, and more general than previous methods.\nUnfortunately, while the independence of task and safety policy in our framework aids generalization, it may hinder the task performance when safety and task objectives conflict greatly, potentially causing deadlocks. Future research could focus on developing a filter that retains generalizability without significantly impeding the task policy. Additionally, although our method is robust to suboptimalities, it lacks safety guarantees under suboptimal conditions. Extending the formulation to provide safety guarantees even in suboptimal scenarios is another valuable research direction."}]}