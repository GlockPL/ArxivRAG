{"title": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference Feedback", "authors": ["Najmeh Forouzandehmehr", "Nima Farrokhsiar", "Ramin Giahi", "Evren Korpeoglu", "Kannan Achan"], "abstract": "Personalized outfit recommendation remains a complex challenge, demanding both fashion compatibility understanding and trend awareness. This paper presents a novel framework that harnesses the expressive power of large language models (LLMs) for this task, mitigating their \"black box\" and static nature through fine-tuning and direct feedback integration. We bridge the item visual-textual gap in items descriptions by employing image captioning with a Multimodal Large Language Model (MLLM). This enables the LLM to extract style and color characteristics from human-curated fashion images, forming the basis for personalized recommendations. The LLM is efficiently fine-tuned on the open-source Polyvore dataset of curated fashion images, optimizing its ability to recommend stylish outfits. A direct preference mechanism using negative examples is employed to enhance the LLM's decision-making process. This creates a self-enhancing AI feedback loop that continuously refines recommendations in line with seasonal fashion trends. Our framework is evaluated on the Polyvore dataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank, and complementary item retrieval. These evaluations underline the framework's ability to generate stylish, trend-aligned outfit suggestions, continuously improving through direct feedback. The evaluation results demonstrated that our proposed framework significantly outperforms the base LLM, creating more cohesive outfits. The improved performance in these tasks underscores the proposed framework's potential to enhance the shopping experience with accurate suggestions, proving its effectiveness over the vanilla LLM based outfit generation.", "sections": [{"title": "1 INTRODUCTION", "content": "The central problem addressed in this research lies at the intersection of technology and fashion, two rapidly evolving fields. The challenge is to create an automated, personalized outfit recommendation system that not only understands fashion compatibility but is also sensitive to current trends and individual preferences. Given the proliferation of online shopping and the ever-increasing consumer demand for personalized experiences, the development of such a system holds significant practical relevance and economic potential. Personalized outfit recommendation has witnessed a revolution driven by advancements in artificial intelligence (AI) and natural language processing (NLP). Recent years have seen a surge of deep learning-based approaches tackling two key tasks: understanding fashion compatibility and capturing current trends. This literature review delves into existing research efforts along these lines, highlighting their strengths, limitations, and paving the way for our proposed framework."}, {"title": "1.1 Graph Neural Networks (GNNs)", "content": "By modeling product relationships as a graph, GNNs reason about outfit compatibility by considering global outfit coherence Han et al. [2]. This approach tackles outfit compatibility at multiple levels (coarse- grained and fine-grained categories), leading to richer information propagation and improved recommendation accuracy. Additionally, research by Li et al. [6] proposes a hierarchical graph that captures both user-outfit and outfit-item relationships. This approach unifies compatibility modeling and personalized recommendation. It leverages embedding propagation for effective information aggregation and user preference refinement, achieving superior performance. While GNN-based approaches excel at modeling garment relationships for outfit recommendation, they face several hurdles. Data sparsity, especially for uncommon items, can hinder effective learning. Large-scale recommendation systems might struggle with GNNs' scalability demands. Additionally, their complex inner workings (such as message passing and encoder- de- coder architecture) make interpretability challenging, limiting error correction and fine-tuning recommendations. New users and items encounter the \"cold start\" problem due to the lack of connections in the graph, leading to subpar suggestions."}, {"title": "1.2 Transformer-based Approaches", "content": "While GNNs offer valuable insights into garment relationships, recent advancements in Transformer-based architectures have opened new avenues for outfit generation. These models excel at capturing long-range dependencies and complex semantic relationships within text, making them highly suitable for understanding fashion descriptions and generating cohesive outfit suggestions. Transformer-based approaches like OutfitTransformer Sarkar et al. [11] leverage self-attention mechanisms to capture the intricate relationships between individual clothing items within an outfit. This holistic view enables them to generate outfits that are not only stylistically compatible but also consider global coherence and overall aesthetics. Unlike GNNs, Transformers lack inherent scalability limitations and can effectively handle large datasets of fashion items and descriptions. This opens up their potential for application in large-scale recommendation systems. Transformers, while promising for their efficiency and interpretability, face limitations in capturing the crucial aspects of trend awareness and personalized context for effective outfit generation. Their focus solely on item embeddings limits their ability to consider broader factors like occasion, weather, user preferences, and current trends, potentially leading to unrealistic, impractical, or unoriginal suggestions. Additionally, inherent biases in the training data can be perpetuated, generating outfits that lack inclusivity or fail to reflect individual style needs. While the complex nature of self-attention mechanisms allows for understanding individual item relationships, it hinders transparency in the decision-making process, making it difficult to integrate user feedback and refine recommendations effectively. This limited grasp of context and trends stands in stark contrast to fine-tuned Large-scale Language Models (LLMs), which leverage their vast pre-trained knowledge to reason about these factors, generating more adaptable and contextually relevant outfit suggestions, albeit with potential trade-offs in computational cost and explainability."}, {"title": "2 PRELIMINARY", "content": null}, {"title": "2.1 Parameter-Efficient Training (PEFT)", "content": "Parameter-Efficient Fine-Tuning (PEFT) enables adopting large pre-trained to specific task by training a small set of parameters. An important example of PEFT, Low-rank Adaptation (LoRA) Hu et al. [4] hypothesizes that the weight updates in pre-trained models have a low intrinsic rank during adaptation. For a pre-trained weight matrix, $W \\in R^{d\\times k}$, it is updated with a low-rank decomposition $W + \\Delta W = W + AB$, where $A \\in R^{d\\times r}$ and $B \\in R^{d\\times k}$. The rank $r < min(d, k)$. During training, W is frozen with no gradient updates, while A and B are trainable. This is the reason why LoRA training is much more efficient than full fine-tuning. In the Transformer structure, LoRA only adapts the attention weights ($Wq, Wk, Wu, Wo$) and freezes all other layers, including MLP and normalization layers. This manner is simple and parameter efficient."}, {"title": "2.2 Reinforcement Learning from Human Feedback (RHLF)", "content": "Reinforcement Learning from Human Feedback (RLHF), which aims to improve the natural language understanding capabilities of Language Learning Models (LLMs) by integrating human feedback during the training process. The RLHF method comprises three main stages:\n(1) Supervised Fine-Tuning (SFT): The RLHF process typically starts with the fine-tuning of a pre-trained LLM through supervised learning on high-quality data, targeting the downstream tasks of interest such as dialogue, summarization, and more. This stage results in the creation of a model, denoted as SFT.\n(2) Preference Sampling and Reward Learning: In the second phase, the SFT model is given prompts x, which generate pairs of answers (y1, y2) ~ \u03c0SFT (y | x). These pairs are then shown to human labelers who express their preference for one answer. This preference is denoted as Yw > y\u0131 | x where yw and y represent the preferred and less preferred completions amongst (y1, y2), respectively. A reward model r(x, y) is then trained to rate the quality of the generated responses.\n(3) RL Fine-Tuning Phase: In this phase, the SFT model (policy) is optimized under PPO reinforcement learning framework, with rewards calculated by Inverse Reinforcement Learning (IRL) using Kullback-Leibler (KL) divergence. The per-token probability distributions from the RL policy are compared to those from the initial model to compute a penalty on the difference between them. This penalty is designed as a scaled version of the KL divergence between the sequences of distributions over tokens, denoted as KLr. This KL divergence term prevents the RL policy from deviating significantly from the initial pre-trained model with each training batch, ensuring the generation of reasonably coherent text snippets. In the third step, a PPO RL Schulman et al. [12] formulation is used to fine-tune the model based on the reward function trained in the previous step."}, {"title": "2.3 Direct Preference Optimization (DPO)", "content": "DPO has emerged recently as a viable alternative to RLHF for preference alignment, optimizing the policy model directly without needing to train a separate reward model and sample rewards through reinforcement learning Rafailov et al. [10]. It has shown comparable performances with RLHF in summarization and chatbot use cases on language models, and maintains strong performance in higher temperature sampling. At the same time, it avoids the unstable and brittle process of training models with RL. DPO implicitly optimizes the same objective as existing RLHF algorithms (i.e., reward function with a KL-divergence term) discussed above. Specifically The DPO framework can be outlined in 2 steps:\n(1) For a given prompt x generate completion pairs (y1, y2) by sampling from the reference policy tref (y | .). These are then annotated with human preferences to form a comprehensive dataset D = {(x\u00b2, yw, y})}}~1\nN\ni=1\n(2) The next step involves refining the language model \u03c0\u03c1 to reduce the DPO loss function LDPO considering the established tref, dataset D, and the target \u00df, where\n$L_{DPO} = E_{(x,yw,y1)} \u03c3\u03b2log\\frac{\u03c0\u03c1 (\u03c8\u03b9 | \u03a7)}{tref (yl | x)} -\u03b2log \\frac{\u03c0\u03c1(yw|x)}{Aref (Yw | x)}$\n\u00df is a parameter controlling the deviation from the base reference policy. Ideally, one prefers to capitalize on existing publicly available preference datasets to avoid the necessity of creating new samples and collating human preferences. Comprehensive details concerning the implementation and the hyperparameters are discussed in Rafailov et al. [10]."}, {"title": "3 METHODOLOGY", "content": "Our proposed model comprises four fundamental components: MLLM image captioner, PEFT prompt generator, a pretrained large language model Mistral 7B, DPO prompt generator module. Figure 1 illustrates the overview of our framework. Our framework takes as input each outfit's constituent item images. The components' design and implementation details are provided below:"}, {"title": "3.1 MLLM Image Captioner", "content": "Multimodal Large Language Models (MLLMs) such as LLaVA Liu et al. [8] are constructed by connecting a pre-trained vision encoder with a LLM. The vision encoders are usually from CLIP Radford et al. [9] so that they can inherently extract semantically aligned visual features. The visual features are then adapted by a specialized light-weight module to map them into the hidden space of LLMs, so they can be jointly processed with the textual inputs by the LLM. Through multimodal training, the MLLMs learn to generate responses given the visual and textual inputs. We used Llava as image captioning tool to provide detailed but short description of item images in each human curated outfits from Polyvore dataset, using the following prompt:"}, {"title": "3.2 PEFT Prompt Generator", "content": "To align the pre-trained LLM model that recommends style cohesive outfits, we first perform supervised fine-tuning (SFT) on a pre-trained LM (Mistral 7B) on a small collection of labeled data. To collect the data, we generate specific prompts using items captions for each input x and target output y pair based on the downstream tasks (CP and FTTB)."}, {"title": "3.2.1 Fill In The Blank (FITB) Task", "content": "In this task, a sequence of clothing and accessory items is provided, and LLM must choose an item from multiple choices compatible with other items to fill in the blank. This is a very practical scenario in real life, e.g., a user wants to choose a pair of shoes to match their pant and coat. An illustration of this task can be seen in Figure 2."}, {"title": "3.2.2 Outfit Compatibility Prediction (CP)", "content": "The task of compatibility prediction forecasts the compatibility of all items within an outfit. Examples of this task can be seen in Figure 3. To perform this task for each outfit from Polyvore disjoint training dataset, first we generate the item descriptions using image captioner step then create the prompt as follows:\n\"Human: As a fashion consultant, your task is to evaluate the overall style compatibility of a list of clothing item descriptions. You need to assign a single compatibility score between 0 and 1 for the entire list. A score of 1 indicates that the items are very compatible style-wise and can be combined to create a cohesive outfit. A score of 0 indicates that the items are not compatible at all. List of clothing item descriptions (Complete Outfit) Output: Compatibility score (0-1). Assistant: (Correct Score)\""}, {"title": "3.3 Pretrained LLM", "content": "For this research purpose we chose Mistral 7B Jiang et al. [5] as the base LLM model, Mistral 7B developed by Mistral AI stands out as a powerful large language model (LLM) with 7 billion parameters. This decoder-based model utilizes a sliding window attention mechanism, allowing for efficient processing of long sequences with a theoretical attention span of 128K tokens. Additionally, its grouped query attention (GQA) enables faster inference and reduces cache size requirements. Notably, Mistral 7B surpasses Llama 2 13B on various demonstrating performance processing tasks."}, {"title": "3.4 DPO Prompt generator", "content": "To further enhance the alignment of Mistral LLM after LoRA fine tuning, we used DPO training as next training stage, the DPO training requires a dataset of prompts, each with two possible completions (preferred and dispreferred), we used the following adapted prompts for FITB and CP training tasks."}, {"title": "3.4.1 Fill In The Blank (FITB)", "content": "\"You have two lists: the first list contains item descriptions that make up an incomplete outfit, and the second list contains additional item descriptions as options to complete the look. Your task is to select exactly one item from List 2 that best complements each item in List 1, considering factors like style, color, and overall aesthetic. List 1 (Incomplete outfit) List 2 (Options to complete the outfit), Chosen: (Correct Option), Rejected: (Incorrect Option)\""}, {"title": "3.4.2 Outfit Compatibility Prediction (CP)", "content": "\"As a fashion consultant, your task is to evaluate the overall style compatibility of a list of clothing item descriptions. You need to assign a single compatibility score between 0 and 1 for the entire list. A score of 1 indicates that the items are very compatible style-wise and can be combined to create a cohesive outfit. A score of 0 indicates that the items are not compatible at all. List of clothing item descriptions (Complete Outfit), Chosen : (Correct score), Rejected: (1-Correct score)\""}, {"title": "4 EVALUATION", "content": "For evaluation, we compare our proposed method with two baselines: plain LLM, and PEFT LLM using LoRA on two different tasks:\n(1) Outfit Compatibility Prediction (CP) task that predicts the compatibility of items in an outfit.\n(2) Fill in the Blank (FITB) task that selects the most compatible item for an incomplete outfit given a set of candidate choices (e.g., 4 candidates).\nThe Polyvore Outfits dataset has two sets, the disjoint and non-disjoint sets. In the disjoint set, the training split items (and outfits) do not overlap with the validation and test splits.\nFor this evaluation purpose we only focus on disjoint sets. The disjoint set comprises of 16995 training and 15154 test outfits. For the standard compatibility prediction and FITB tasks, we evaluate our model on the Polyvore Outfits dataset. We randomly selected 1000 outfits from the train set and generated PEFT and DPO prompts for each CP and FTIB training tasks. The training is done on one NVIDIA T4 GPU with a local batch size of 1 pair and gradient accumulation of 4 steps. We utilize AdamW optimizer with a learning rate of 2 \u00d7 10-4 with 0. 3 warm-up steps and linear decay for PEFT training. We use same optimizer with a learning rate of 1 \u00d7 10-8 with 0.1 ratio warm-up steps and linear decay for the DPO training. The DPO temperature parameter \u00df set be 0.1. The backbone model is trained for 3 epochs on our dataset."}, {"title": "4.1 Outfit Compatibility Prediction (CP)", "content": "The goal of this task is to measure the compatibility of an outfit. Our compatibility model predicts a score that indicates the compatibility of the overall outfit. We compare the performance with the baseline approaches in Table 1 by using the standard metric AUC, which measures the area under the receiver operating characteristic curve. The purposed approach outperforms the other baseline approaches significantly."}, {"title": "4.2 FITB", "content": "The goal of this task is to choose the best item from the entire database. For the FITB task we use accuracy. From Table 2, we observe that the purposed approach outperforms the other two baselines on the non-disjoint test dataset."}, {"title": "5 CONCLUSIONS", "content": "This paper presents a novel framework for personalized outfit recommendation that leverages the power of large language models (LLMs) while mitigating their limitations through fine-tuning and direct feedback integration. We bridge the visual-textual gap by employing multimodal language models (MLMs) for image captioning, enabling the LLM to extract style and color features from fashion images. The future of this framework lies in expanding its capabilities by incorporating additional modalities like user context (location, occasion) to provide a richer understanding of preferences, while exploring advanced feedback mechanisms like click-through analysis to refine the learning process."}]}