{"title": "CIA: Controllable Image Augmentation Framework Based on Stable Diffusion", "authors": ["Mohamed Benkedadra", "Dany Rimez", "Tiffanie Godelaine", "Natarajan Chidambaram", "Hamed Razavi Khosroshahi", "Horacio Tellez", "Matei Mancas", "Benoit Macq", "Sidi Ahmed Mahmoudi"], "abstract": "Computer vision tasks such as object detection and segmentation rely on the availability of extensive, accurately annotated datasets. In this work, We present CIA, a modular pipeline, for (1) generating synthetic images for dataset augmentation using Stable Diffusion, (2) filtering out low quality samples using defined quality metrics, (3) forcing the existence of specific patterns in generated images using accurate prompting and ControlNet. In order to show how CIA can be used to search for an optimal augmentation pipeline of training data, we study human object detection in a data constrained scenario, using YOLOv8n on COCO and Flickr30k datasets. We have recorded significant improvement using CIA-generated images, approaching the performances obtained when doubling the amount of real images in the dataset. Our findings suggest that our modular framework can significantly enhance object detection systems, and make it possible for future research to be done on data-constrained scenarios. The framework is available at: github.com/multitel-ai/CIA.", "sections": [{"title": "I. INTRODUCTION", "content": "The performance of deep learning models is dependent on the quality and diversity of the dataset they were trained on. Unfortunately, the creation of such high-quality and accurately annotated datasets is often challenged by the scarcity of data and the substantial costs associated with annotation [1], especially in specialized and evolving Computer Vision tasks. Hence, other strategies are commonly used to enhance dataset quality, like Active Learning [2] and Data Augmentation methods [3] (image rotation, flipping, color adjustment, etc). However, these methods modify images with simple and often content-agnostic transformations, limiting their ability to introduce completely new information into the dataset. This limitation led us to the exploration of Generative AI models like Stable Diffusion [4] that can generate entirely new images. Through the usage of ControlNet [5] with predefined features extracted from the original image, we can tailor the generation process to meet specific task requirements. This creates an unprecedented opportunity to augment datasets beyond traditional methods. Concurrently, when dealing with"}, {"title": "II. RELATED WORKS", "content": "Data augmentation has become an indispensable strategy for enhancing the quality and diversity of visual datasets and improving models' performances in Computer Vision tasks. Shorten and Khoshgoftaar's comprehensive review [6] extensively explores the diverse range of techniques employed, spanning from fundamental geometric transformations [3] to sophisticated generative methods such as Stable Diffusion [4] and ControlNet [5]. These advanced techniques can generate novel content and scene conditions. For example, introducing new variations in weather, people position, object appearance, image style, etc. This essential for mitigating the limitations posed by inadequate datasets, ultimately enhancing the performance and reliability of models. Chen et al. [7] preform scale-aware data augmentation strategies for region dependent tasks. They focus on adding objects of different scales through computationally efficient, zoom-in/out operations. Ghiasi et al. [8] expanded on this approach by copy-pasting the zoomed objects at various scales into different backgrounds."}, {"title": "A. Stable Diffusion for Data Augmentation", "content": "Eliassen and Ma [9] demonstrated how Stable Diffusion, combined with Active Learning, can effectively re-balance classification datasets, notably outperforming traditional over-sampling methods on CIFAR-10. Trabucco et al. [10] demonstrated the efficacy of text-to-image diffusion models in creating synthetic images for data augmentation. Similarly, Azizi et al. [11] highlighted how synthetic data from diffusion models can enhance ImageNet [12] classification. By fine-tuning text-to-image models, they achieved class-conditional models with impressive fidelity. For region dependent tasks such as object detection and in-stance segmentation, Ge et al. [13] introduced a text-to-image synthesis paradigm leveraging DALL-E [14]. Their method generates diverse labeled data by utilizing segmentation masks to separately produce foregrounds (objects) and backgrounds (scenes). However, the approach exhibits limitations in the quality of the generated samples due to the artificial merging of generated objects onto backgrounds, resulting in a notice-able discontinuity between the foreground and background elements. Wu et al. [15] focused on image augmentation with Con-trolled Diffusion. Their method significantly boosts perfor-mance with minimal training data. Although these studies collectively showcase the transfor-mative impact of Stable Diffusion models, none of them offer a complete and reliable pipeline for generative data augmen-tation. They do not provide an easy to set up tool for quickly and efficiently testing the different augmentation strategies, or for employing quality metrics to evaluate synthetic data."}, {"title": "B. Synthetic Images Quality Assessment", "content": "Assessing the quality of the synthesized samples presents a notable challenge. The complexity stems from the subjective and multidimensional nature of \"quality\", as its definition can defer depending on the intended application of the generated data. The literature often highlights the use of Image Quality Assessment (IQA) metrics for evaluating visual quality. Active Learning metrics can be used for assessing the potential impact of the data on model training.\n1) IQA Metrics: IQA metrics focus on different fea-tures and patterns in the image to quantify its qual-ity. Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [16], measures contrast, luminosity, distortion, etc, to quantify anomalies in generated images. The Neural Im-age Assessment (NIMA) model [17] employs a CNN trained to measure the aesthetics and realism of synthetic images, and outputs a distribution of scores that represent different criteria. As introduced in [18], ClipIQA leverages the power of large-scale pre-trained vision-language models to predict image quality without reference images. It was trained on specific features related not only to quality but to the general look, feel, content, and context of the image.\n2) Active Learning metrics: Unlike Model-Agnostic met-rics, Model-Aware quality metrics rely on the discriminative task's model for quality assessment. Active learning sampling strategies are often employed to assess the quality of the synthetic data by predicting its impact on model performance. Uncertainty based and diversity based methods [2] are the most common. In the example of object detection using YOLO, we can use the detection confidence score as a measure of the distance between a new synthetic image and the average distribution of real images used to train the baseline model."}, {"title": "III. PROPOSED CIA FRAMEWORK", "content": "CIA is composed of four modules, as seen in Fig.2. Initially, an Extraction module performs feature extraction from original images, to acquire the control features that maintain the integrity of the dataset's intrinsic characteristics. These features are used in the next phase by the ControlNet to condition the output of Stable Diffusion, thus adding an extra control sequence beyond the conventional text prompt. The Generation module takes in the extracted features combined with text prompts to synthesize new images. The prompts are either manually specified, or automatically gen-erated. Optionally, In order to put constraints on the resulting dataset quality, the Quality Assessment module can filter the generated images using chosen quality metrics, which allows for retaining only the highest-quality images. The final stage of the pipeline is the Train and Test block. Through training different models, we can explore the effects of using various combinations of original and synthetic data on task performance."}, {"title": "A. Extraction", "content": "We begin by extracting features specific to the chosen ControlNet. Although custom extractors can be added, a few are implemented by default in CIA and cover some popular domains, from extracting human features through poses [19] or faces [20], to broader generic features like edges [21] and segmentation masks [22]."}, {"title": "B. Generation", "content": "Several generators could be obtained by combining the cho-sen ControlNet model with any compatible Stable Diffusion model. Once the Diffusion model is chosen, the generator G is able to generate the synthetic image $S_i$, for each extracted feature $F_i$ from an image $I_i$, and the text prompt $C_i$ (the caption of the original image). Such that $S_i$ is given by $S_i = G(F_i, C)$.\nTo introduce more diversity in generated images, we use modified captions $C'$. Many methods could be used to generate prompts, such as LLMs (e.g., LLama2 [23]). However, the default prompt generator $T$ of CIA follows a simple imple-mentation. It takes a prompt $C_i$ and a vocabulary to produce a new prompt $C'$. For example, $T$ modifies $C_i$ = a man in a red shirt, by substituting words from a vocabulary: {v0: [man, woman, child], v1: [red, black, yellow]}. a possible modified caption could be $C_i'$ = a woman in a yellow shirt. We can generate many modified captions $C_{ij}$, meaning $j$ possibilities of synthetic images generated from a single real image where $j \\in \\{0, 1, 2, ..., (\\prod_{k=0}^{K-1} |v_k|) - 1\\}$. The new text prompts are the input prompts of Stable Diffusion. Its output is conditioned by the control features"}, {"title": "C. Quality Assessor and Sampler", "content": "To assess the quality of synthetic images in $D'$, we introduce a Quality Assessment module that filters out low quality images. Here, the quality of $S_i$ can be defined according to any metric suitable for the task. The quality score $q_i$ is then computed using the selected metric $Q$ from the set of Quality Metrics $Q$ such that $q_i = Q(S_i)$. The quality metrics imple-mented in CIA includes IQA and Active Learning metrics."}, {"title": "D. Train and Test", "content": "We can train and test multiple models for the task at hand. Through modifying generation parameters, we can choose the amount of synthetic data in this training set. Optionally, if the Quality Assessment module is used, we can control the quality thresholds of the added synthetic data. Performances of the model are evaluated on a validation set during training, and on a test set after training. Both sets are constituted of real images only."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We preformed a case study on human object detection to prove the framework's effectiveness. In this toy example, we only have access to a limited dataset that leads to suboptimal performances. The goal is to study how to optimally improve performances, by adding CIA-generated synthetic images. The Generation parameters of Stable Diffusion were not optimized and kept constant. YOLOv8n [24] was used as the object detection model. For each experiment, it was trained for 300 epochs using the training parameters from [25] with the SGD optimizer. Experiments were conducted on subsets of Common Objects in Context (COCO) [26] and Flickr30k Entities [27]."}, {"title": "A. Datasets", "content": "COCO was processed to focus on a subset of images containing only one instance of the \"PERSON\" class, where objects take an area between 5% and 80% of the image. In Flickr, objects are labelled with textual segments without con-sistent class annotations. We processed the textual descriptions to automatically annotate the images with the \"PERSON\" class. The inconsistency in Flickr's annotations provided a robust stress testing ground for CIA, simulating the variability and imperfection common in real-world datasets. Three types of training sets were created for both datasets :\n1) Baseline: Contains real images. One lower $D_{250}$ (250 images) and one upper $D_{500}$ (500 images) baselines were used as basis for comparison.\n2) Synthetic: To evaluate the impact of adding synthetic images on object detection performance, a larger synthetic dataset $D_{1250}$ (1250 images) was generated by using five distinct auto-generated captions ($C_1, ..., C_f$) for each sample in $D_{250}$. Multiple datasets were then created with different proportions (250,500,...,1250) of synthetic images sampled from $D'_{1250}$ and added to $D_{250}$."}, {"title": "B. Experiments", "content": "With these datasets, three experiments presented hereafter were conducted. The first one was done using both COCO and Flickr images, while only COCO was used for the two others.\n1) ControlNet effect: To analyze the effects of choosing a good ControlNet that fits the task, we compared several models. Four models were chosen. Some tailored for people detection (OpenPose or MediaPipe), and others are more generic (Canny Edge and Segmentation), and suitable for various types of datasets and contexts. All four are compatible with Stable Diffusion v1.5 (runwayml/stable-diffusion-v1-5) from the Hugging Face platform. We added a deficient extraction module to the case study, to understand how the usage of bad conditions affects the results. We used the Segmentation extractor, with a transposed seg-mentation mask as a condition instead of the true segmentation mask. As a result, this new extraction module, False-Segmentation, generated bad quality images. Not only the shape and position of the label bounding box are affected, but the content of the image is not necessarily coherent with the label anymore.\n2) Data Augmentation additivity: This experiment aims at illustrating the first claim stated in Section I, i.e. CIA augmentation can independently be used along with other data augmentation methods. Let's call this property additivity."}, {"title": "V. RESULTS", "content": "In this section, the results of the case study for the three aforementioned experiments are presented before being dis-cussed to highlight the possibilities of CIA."}, {"title": "A. ControlNet effect", "content": "Evaluating the influence of different ControlNets on en-hancing YOLOv8's object detection capabilities, focuses on variations in mAP. This evaluation reveals the significance of ControlNet choice on perfor-mance. While most ControlNets led to an increase in mAP compared to $D_{250}$, and $D_{ablation}$, none matched $D_{500}$ perfor-mance. Notably, Mediapipe exhibits a decline in performance. This could be explained by images where the object deviates from the original bounding box (Fig.1). We then tested False-Segmentation, and obtained similar results to Mediapipe. This confirms that the choice of the ControlNet needs to be consistent with the task domain. On the contrary, Canny Edge, OpenPose, and Segmentation contributed positively to mAP. This improvement was notable up to 750 synthetic samples, beyond which mAP increase was considered not significant."}, {"title": "B. Data augmentation additivity", "content": "The second study aimed to analyze the impact of adding synthetic images to other data augmentation techniques to determine the value of synthetic data. The results show that using synthetic data never leads to lower performances for efficient ControlNets at all data augmentation levels, as demonstrated by the higher"}, {"title": "C. Sampling with quality metrics", "content": "The aim of this experiment was to refine the pipeline for op-timal outcomes. The results are similar for all ControlNets: none of the sampling strategies significantly outperformed random sampling. This suggests that prioritizing images based solely on features like visual quality or diversity, may not be the most effective strategy for model improvement."}, {"title": "VI. DISCUSSION", "content": "This case study provides guidelines for using the CIA framework effectively. We demonstrated that adding synthetic images generated with the appropriate ControlNet can enhance detection performance. These images can also be used in conjunction with basic data augmentation. The analysis of the influence of sampling methods indicates that the diversity of the generated images may not be optimal. Exploring other hyperparameters during the generation process may lead to better results. Nonetheless, it is still far superior to classical methods even at very high levels of augmentation. An overview of the different types of images that can be produced with the five studied ControlNets was already given. However, displays the introduction of new patterns in the images. Changes in the background, point of view, and style can be observed. Such differences could be of high interest depending on the task. This is merely a glimpse of the generation possibilities, that can be tailored through the prompt and the Stable Diffusion model choice, both of which can be easily modified with the CIA framework."}, {"title": "VII. CONCLUSION", "content": "CIA offers a plug-and-play capability for developing, testing, and evaluating custom image generation pipelines. This framework has the potential to have a significant impact on the field of computer vision by providing researchers with a powerful tool for augmenting datasets and exploring new metrics and diffusion models. We demonstrated the capabil-ities of CIA in augmenting limited object detection datasets. But, the adaptability of the CIA framework allows for easy extension to other tasks like classification, segmentation or tracking. It allows for the incorporation of custom Diffusion models, ControlNet models and quality metrics to further adapt CIA to any application. Moreover, through its modularity, a module can easily be replaced or added. For example, adding other generative AI methods (not based on Stable Diffusion)."}]}