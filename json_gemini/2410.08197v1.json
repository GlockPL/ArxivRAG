{"title": "FROM EXPLORATION TO MASTERY: ENABLING LLMS\nTO MASTER TOOLS VIA SELF-DRIVEN INTERACTIONS", "authors": ["Changle Qu", "Sunhao Dai", "Xiaochi Wei", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Jun Xu", "Ji-rong Wen"], "abstract": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate the\nlimitations inherent in their pre-training data. In this process, tool documentation\nplays a crucial role by providing usage instructions for LLMs, thereby facilitat-\ning effective tool utilization. This paper concentrates on the critical challenge\nof bridging the comprehension gap between LLMs and external tools due to the\ninadequacies and inaccuracies inherent in existing human-centric tool documenta-\ntion. We propose a novel framework, DRAFT, aimed at Dynamically Refining\ntool documentation through the Analysis of Feedback and Trails emanating from\nLLMs' interactions with external tools. This methodology pivots on an innovative\ntrial-and-error approach, consisting of three distinct learning phases: experience\ngathering, learning from experience, and documentation rewriting, to iteratively\nenhance the tool documentation. This process is further optimized by implementing\na diversity-promoting exploration strategy to ensure explorative diversity and a\ntool-adaptive termination mechanism to prevent overfitting while enhancing effi-\nciency. Extensive experiments on multiple datasets demonstrate that DRAFT's\niterative, feedback-based refinement significantly ameliorates documentation qual-\nity, fostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Tool learning (Mialon et al., 2023; Qin et al., 2023b; Schick et al., 2024; Qu et al., 2024b), which\nintegrates external tools with large language models (LLMs), has significantly enhanced the capability\nof LLMs to address complex real-world tasks (Nakano et al., 2021; Qin et al., 2023a; M. Bran et al.,\n2024). By leveraging external tools, LLMs are able to mitigate the limitations of outdated pre-training\ndata and the text-in-text-out interface, enabling them to access up-to-date information, interact with\ndynamic environments, and take actions beyond their original scope (Zhuang et al., 2024; Wang\net al., 2024a). To effectively utilize these external tools, LLMs are typically provided with tool\ndocumentation as context (Shen et al., 2024; Song et al., 2023; Xu et al., 2023). This documentation\nprovides essential information on how tools function, their potential uses, and the ways in which\nthey can be leveraged to solve complex tasks. By incorporating tool documentation within the task\ninstructions, LLMs can leverage their in-context learning abilities to understand and utilize the tools\nefficiently (Wei et al., 2022; Hsieh et al., 2023). Therefore, tool documentation is an indispensable\ncomponent driving the success of tool learning, serving as a bridge between LLMs and external tools.\nHowever, existing tools primarily originate from pre-established, human-engineered code reposi-\ntories and are not explicitly tailored for the utilization of LLMs from their inception, let alone the\ncorresponding tool documentation. In fact, orchestrating an ideal documentation for an external\ntool that adapts to the specific requirements of LLMs remains a challenging endeavor. First, the\noriginal human-crafted tool documentation is typically created with human intuition in mind, and\nis often fraught with inconsistency and ambiguities, as it primarily caters to human understanding"}, {"title": "2 METHODS", "content": "In this section, we will first introduce the overview of DRAFT, and then provide detailed explanations\nof the three included learning stages. The learning algorithm is presented in Algorithm 1.\n2.1 OVERVIEW OF DRAFT\nTo address the challenges of inadequate, ambiguous, and outdated tool documentation that hinder\nLLMs from effectively utilizing external tools, we propose DRAFT, a framework that iteratively\nrefines tool documentation to bridge the comprehension gap between LLMs and tools. As illustrated\nin Figure 2, DRAFT operates through three interconnected phases: experience gathering, learning\nfrom experience, and documentation rewriting. In the experience gathering phase, an explorer\nsimulates diverse tool usage scenarios, collecting data on how the LLM interacts with the tool based\non the current documentation, thus uncovering misunderstandings and limitations. The learning\nfrom experience phase involves an analyzer examining this data to identify discrepancies between\nintended and actual tool usage, pinpointing ambiguities or inaccuracies in the documentation, and\nproposing targeted improvements. In the documentation rewriting phase, a rewriter integrates these\ninsights to update the documentation, enhancing clarity and alignment with the tool's functionalities.\nThrough this trial-and-error framework, DRAFT is capable of simulating the process by which\nhumans acquire proficiency in tool usage through repeated interactions and hands-on experiences,\nthereby automating the creation of tool documentation specifically designed for LLMs. Furthermore,\nby employing the diversity-promoting exploration strategy and tool-adaptive termination mechanism,\nDRAFT efficiently converges on optimized documentation, enabling LLMs to utilize external tools\nmore effectively despite the initial documentation shortcomings."}, {"title": "2.2 EXPERIENCE GATHERING", "content": "In the experience gathering phase, we design an\nExplorer AE to simulate plausible scenarios in\nwhich the tool may be utilized. This approach par-\nallels the manner in which individuals investigate the\npotential applications of a new tool when they are\nunable to comprehend the accompanying manual.\nSpecifically, at the i-th iteration, the Explorer gen-\nerates an exploration instance $e_i$ based on the current\ntool documentation $t_{i\u22121}$, next-step exploration direc-\ntion $d_{i-1}$ from the Rewriter, and the previous his-\ntory $H_i = {(e_j, r_j)|j < i}$, which consists of prior exploration instances $e_{<i}$ and their corresponding\nreturn results of the tool $r_{<i}$. This process is formalized as follows:\n$e_i = M_E(t_{i-1}, d_{i-1}, H_i)$,\nwhere $e_i$ consists of a user query $e$ related to the tool and the necessary parameters $e$. The initial\ntool documentation is denoted as $t_0$, representing the raw documentation provided in the dataset.\nAfter generating $e_i$, the Explorer invokes the tool to obtain the result $r_i$ returned by the tool.\nGiven that tool utilization often involves complex parameter ranges, combinations, and potential error\nsources, it is crucial to ensure diversity in the exploration phase to cover a wide spectrum of possible\nscenarios. To address this, besides maintaining a record of all previously explored queries\u2014which\nwe provide to the Explorer to instruct it to generate instances that differ from those already\ngenerated\u2014we also implement a diversity-promoting exploration strategy:\nSimilarity Constraint. When generating a new instance, the Explorer calculates the cosine\nsimilarity between the new generated query $e$ and all prior queries $e_f$ for $j < i$, using embedding\nvectors obtained from OpenAI's text-embedding-ada-002.\nThe similarity is computed as:\n$\\max_{j<i} sim(e, e_f) < \u03c6$,"}, {"title": "2.3 LEARNING FROM EXPERIENCE", "content": "Analogous to how humans learn-acquiring familiar-\nity with new tools through practical experiences and\nthen consulting manuals to deepen understanding-the\ninsights gained during the experience gathering phase\nprovide a foundation for informed and targeted enhance-\nments to the documentation. Thus, building upon the\nexperience gathered in the first phase, the second phase\nfocuses on analyzing this data to refine the tool docu-\nmentation. In this phase, we introduce an Analyzer\ndesigned to identify and address issues within the cur-\nrent tool documentation, thereby guiding the Rewriter in making effective revisions.\nFormally, at the i-th iteration, the Analyzer takes the following inputs: current tool documentation\n$t_{i-1}$, exploration instance $e_i$, tool feedback $r_i$ provided by the Explorer, and the history of\ndocumentation revisions $T_i = {t_j | j < i}$. Then the Analyzer analyzes these inputs to identify\nissues and generate revision suggestions $s_i$:\n$s_i = M_A(t_{i-1}, e_i, r_i, T_i)$,\nTo ensure that the Analyzer can provide high-quality and relevant revision suggestions, we establish\nseveral evaluation criteria, including consistency with tool outputs, comprehensiveness, and concise-\nness without irrelevant information (as detailed in Appendix B). These criteria enable Analyzer\nto identify and assess issues present within the existing tool documentation. By considering the\nhistorical evolution of the documentation through the revision history $T_i$, the Analyzer gains\nvaluable insights into past updates, helping it avoid redundant or repetitive suggestions and focus on\nareas that still require improvement. The prompt template for Analyzer is illustrated in Figure 4.\nFurthermore, the Analyzer delivers its feedback in natural language, offering detailed and nuanced\nguidance to the Rewriter for subsequent updates. This approach contrasts with providing mere\nscalar feedback, as it ensures the Rewriter receives comprehensive insights that facilitate accurate\nand effective revisions, ultimately enhancing the clarity and usability of the tool documentation."}, {"title": "2.4 DOCUMENTATION REWRITING", "content": "Building upon the experiences gathered and the revision suggestions obtained from the previous two\nphases, the final phase focuses on refining the tool documentation to enhance its clarity, accuracy, and\nusability, ensuring it aligns with the comprehension capabilities of LLMs. This phase also provides\nsuggestions for future exploration directions in the next iteration of the experience gathering phase.\nSpecifically, we design a Rewriter to synthesize information from the exploration instances $e_i$\nand the corresponding tool return results $r_i$ provided by the Explorer, as well as the revision\nsuggestions $s_i$ from the Analyzer. It is important to note that the Rewriter also takes into\naccount the rewrite history $T_i$, which includes all previous versions of the tool documentation up\nto iteration i. By integrating these inputs, the Rewriter produces an updated version of the tool\ndocumentation $t_i$ and provides suggestions for the next round of exploration directions $d_i$. This\nprocess is formalized as:\n$d_i, t_i = M_R(t_{i\u22121}, e_i, r_i, s_i, T_i)$,\nBy incorporating the revision history into its process, the Rewriter ensures that each version of\nthe documentation builds upon its predecessors, resulting in continuous improvements in clarity,\naccuracy, and usability. The prompt template to get Rewriter is shown in Figure 5.\nFurthermore, recognizing that different tools vary in\ntheir complexity and the ease with which LLMs can\ncomprehend them, we implement a tool-adaptive ter-\nmination mechanism to adaptively determine when\nto cease modifications for each tool. Analogous to\nrecipes requiring different levels of expertise, some\ntools may reach optimal documentation faster than\nothers. We consider the iterative process to have con-\nverged when there is minimal change between two\nconsecutive versions of the documentation, indicat-\ning that the Rewriter has sufficiently aligned the\ndocumentation with the LLM's understanding.\nSpecifically, we measure the degree of change $\u0394$\nbetween iterations by calculating both the word-match metric (e.g., BLEU score (Papineni et al.,\n2002)) and the semantic-match metric (e.g., cosine similarity of embeddings):\n$\u0394=\\frac{sim(e_i, e_{i-1}) + BLEU(t_i, t_{i\u22121})}{2}$,\nwhere $e$ and $e_{-1}$ are the embedding vectors of $t_i$ and $t_{i\u22121}$ obtained using OpenAI's text-embedding-\nada-002. The function sim(,) calculates the cosine similarity between the semantic embedding\nvectors of two documentation versions, and BLEU(,) measures the n-gram overlap between them.\nIf $\u0394$ exceeds a predefined termination threshold $T$, we stop the iterative modifications.\nThis tool-adaptive termination mechanism offers several advantages: First, it enhances efficiency by\nceasing iterations when the documentation is adequately aligned with the LLM's comprehension,\nconserving computational resources and time. Second, it prevents unnecessary modifications that\ncould lead to overfitting, thus optimizing the quality of the documentation. By employing both\nthe BLEU score and cosine similarity, we ensure a balanced assessment of structural and semantic\nalignment, ultimately yielding high-quality documentation tailored for effective LLM utilization."}, {"title": "2.5 DRAFT'S STRENGTHS", "content": "In this section, we outline the key strengths of the proposed DRAFT framework. First and foremost,\nDRAFT operates in a fully automated manner, which significantly reduces resource consumption\nin comparison to the time-consuming and labor-intensive manual modifications that are typically\nrequired. Furthermore, by employing a trial-and-error methodology, DRAFT continuously updates\ntool documentation based on feedback regarding tool usage obtained from LLMs, thereby enhancing\nthe alignment between tool documentation and the operational understanding of LLMs. Additionally,\nDRAFT is capable of dynamically maintaining an accurate and up-to-date representation of evolving\nfeatures within the tool documentation as the tools develop. It also provides inherent explainability, as\nthe entire process is presented in natural language. Users can easily track the history of modifications\nand seamlessly integrate expert insights into the updating process."}, {"title": "3 EXPERIMENTS", "content": "3.1 EXPERIMENTAL SETUP\nDatasets. To verify the effectiveness of DRAFT, we conduct experiments on two benchmarks:\nToolBench and RestBench. ToolBench (Qin et al., 2024) is a large-scale benchmark of real-world\nAPIs collected from RapidAPI and BMTools, commonly used to evaluate the capability of LLMs\nin tool usage. Due to budget constraints, we focus on the most challenging subset of ToolBench,"}, {"title": "3.2 EXPERIMENTAL RESULTS", "content": "We present our experimental results in Table 1. Based on these results, we have the following\nobservations: While EasyTool can slightly improve experimental performance, it does not incorporate\nthe experience feedback from LLMs to iteratively revise the tool documentation. As a result, it cannot\nfully align with the understanding of LLMs. In contrast, our method effectively addresses these issues\nand achieves more significant improvements. We observe that all LLMs achieve better performance\nwhen using the tool documentation modified by DRAFT. This indicates that although our tool"}, {"title": "3.3 FURTHER ANALYSIS", "content": "How does the number of iteration rounds affect the performance of tool learning? A key\nfeature of DRAFT is its iterative refinement of tool documentation with a tool-adaptive termination\nmechanism. We examine the effectiveness of these designs by analyzing how the number of iteration\nrounds influences the performance of downstream tool learning. The results presented in Figure 6\ndemonstrate a general trend where performance enhances with an increasing number of iterations,\nfollowed by a subsequent decline. This suggests that iterative modifications are crucial for enhancing\ntool documentation quality and the ability of LLMs to utilize tools effectively. Such modifications\nfacilitate the exploration of a wider array of examples and the incorporation of additional feedback\nderived from the tool usage experiences of LLMs, ultimately refining the tool descriptions to achieve\nsuperior performance. However, a decline in performance is observed after a certain number of\niterations. This decline may be attributed to the introduction of redundant information as the number\nof iterations increases, potentially leading to overfitting. Therefore, we implement a tool-adaptive\ntermination mechanism to prevent performance degradation and ensure optimal results.\nDoes using other models as back-\nbones also ensure cross-model gen-\neralization? Our preliminary experi-\nments have demonstrated that employ-\ning GPT-4o as the backbone to re-\nfine tool documentation by integrat-\ning its own usage feedback results in\nrevised documentation that exhibits"}, {"title": "4 RELATED WORK", "content": "Tool Learning. Recent studies have highlighted the potential of LLMs to utilize external tools in\naddressing complex problems (Qu et al., 2024b; Wang et al., 2024d). With the aid of external tools,\nLLMs can obtain up-to-date information (Nakano et al., 2021; Gou et al., 2024a;b), enhance domain-\nspecific knowledge (M. Bran et al., 2024; Zhang et al., 2024a), process multi-modal information (Sur\u00eds\net al., 2023; Gao et al., 2024c), and more. Existing tool learning approaches can be categorized into\ntwo types: tuning-based and tuning-free methods (Gao et al., 2024b). Tuning-based methods enhance\nthe tool-using capabilities of LLMs by fine-tuning them on tool-related datasets (Patil et al., 2023;\nHao et al., 2024; Yang et al., 2024; Qin et al., 2024; Liu et al., 2024). However, this approach is\nonly applicable to open-source models and requires substantial computational resources. In contrast,\ntuning-free methods provide LLMs with tool documentation and a few demonstrations (Wei et al.,\n2022; Hsieh et al., 2023; Paranjape et al., 2023; Du et al., 2024; Shi et al., 2024), relying on the\nin-context learning ability of LLMs to understand how to use tools. This approach requires no\nadditional training and allows for the plug-and-play integration of external tools. However, this\nmethod necessitates high-quality tool documentation that is aligned with the comprehension of\nLLMs (Yuan et al., 2024). In this paper, we propose a method to align with LLMs understanding and\nimprove the quality of tool documentation to enhance the tool-using capabilities of LLMs.\nLearning from Feedback. Recent studies show that LLMs can improve their initial responses through\nself-correction, leading to improved performance (Shinn et al., 2024; Madaan et al., 2024; Pan et al.,\n2024; Huang et al., 2024). However, some researchers observe that relying exclusively on self-\ncorrection without external feedback may yield minimal improvements or worsen performance (Zhao\net al., 2024). In contrast, incorporating learning from feedback has been shown to improve various\ntasks (Jin et al., 2023; Gao et al., 2024a; Wang et al., 2024c; Pan et al., 2024; Welleck et al., 2022;\nZhang et al., 2024b). The forms of feedback are categorized into scalar and natural language\ntypes (Gou et al., 2024a). Scalar feedback provides coarse-grained information and typically serves\nas a reward signal in reinforcement learning frameworks (Ziegler et al., 2019), while natural language\nfeedback provides detailed insights and is used in prompts for LLMs to enhance performance (Jin\net al., 2023). The sources of feedback are diverse, including humans (Ouyang et al., 2022), critic\nmodels (Nathani et al.), external tools (Wang et al., 2024b; Qiao et al., 2024), and even the LLM itself.\nTo ensure that tool documentation genuinely reflects the purpose of the tool, we obtain feedback by\nactually using the tool to get the returned results, thereby producing high quality tool documentation."}, {"title": "5 CONCLUSION", "content": "In this paper, we highlight that the misalignment between the existing, primarily human-centric tool\ndocumentation and the interpretive requirements of LLMs acts as a pivotal barrier obstructing the\nfull potential of tool learning with LLMs. To remedy this, inspired by trial-and-error, we introduce\nDRAFT, a dynamic and self-improving framework specifically designed to iteratively refine tool\ndocumentation based on direct interactions and feedback loops between LLMs and external tools.\nThrough extensive experimentation, our findings substantiate the assertion that our proposed DRAFT\nmarkedly enhances the alignment between tool documentation and the operational understanding of\nLLMs, thereby fostering more effective tool usage."}, {"title": "APPENDIX", "content": "A MORE EXPERIMENTS\nWe also conduct analysis experiments on three datesets using other LLMs. As illustrated in Table 5,\nboth GPT-40-mini and LLama-3-70B display trends that are consistent with those observed in GPT-40.\nThe absence of our proposed diversity-promoting exploration strategy and tool-adaptive termination\nmechanism results in a decline in performance, thereby underscoring the importance of our design\ninnovations. An intuitive explanation posits that, in the absence of the diversity-promoting exploration\nstrategy, the examples generated during each round of exploration may exhibit significant similarity.\nThis similarity could render multiple iterations indistinguishable from a single iteration, thereby\nundermining the advantages of repeated exploration. Likewise, the absence of the tool-adaptive\ntermination mechanism may result in excessive iterations, which could produce redundant information\nand contribute to overfitting.\nB DETAILED PROMPTS\nTable 6 provides a comprehensive overview of the prompts utilized during the three learning stages\nof DRAFT, which include experience gathering, learning from experience, and documentation\nrewriting.\nC CASE STUDY\nC.1 TOOLBENCH\nTable 7 displays the comparison of original tool documentation and modified versions using DRAFT\nin some cases from the ToolBench dataset. Each case highlights a specific issue found in the raw\ndocumentation, including incompleteness, ambiguity, redundancy, and inaccuracy, all of which are\neffectively addressed by our proposed method.\nC.2 RESTBENCH\nTable 8 displays the comparison of original tool documentation and modified versions using DRAFT\nin some cases from the RestBench dataset. Although the overall quality of the tools in RestBench is\ngenerally superior to that of ToolBench, there remain significant issues, including irrelevance, ambigu-\nity, and incomplete information. This further highlights the necessity of employing our method, which\niteratively refines the documentation through interactions with the tools. By systematically addressing\nthese issues, our approach guarantees that the documentation evolves in a way that enhances clarity,\nrelevance, and completeness. This, in turn, minimizes the likelihood of misunderstandings or misuse\nof the APIs and enhances the ability of LLMs to utilize external tools."}]}