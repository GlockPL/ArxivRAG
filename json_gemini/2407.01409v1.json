{"title": "Dynamic Few-Shot Learning for Knowledge Graph Question Answering", "authors": ["Jacopo D'Abramo", "Andrea Zugarini", "Paolo Torroni"], "abstract": "Large language models present opportunities for innovative Question Answering over Knowledge Graphs (KGQA). However, they are not inherently designed for query generation. To bridge this gap, solutions have been proposed that rely on fine-tuning or ad-hoc architectures, achieving good results but limited out-of-domain distribution generalization. In this study, we introduce a novel approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the efficiency of in-context learning and semantic similarity and provides a generally applicable solution for KGQA with state-of-the-art performance. We run an extensive evaluation across multiple benchmark datasets and architecture configurations.", "sections": [{"title": "1 Introduction", "content": "The growth of the Semantic Web has led to the creation and storage of vast amounts of structured knowledge (Hitzler, 2021; Shadbolt et al., 2006), organized into massive Knowledge Graphs (KGs) such as Wikidata (Pellissier Tanon et al., 2016), DBpedia (Lehmann et al., 2014), and FreeBase (Bollacker et al., 2008). The scale of these KGs, with over 109 million items in Wikidata alone,\u00b9 has made extracting relevant information from them increasingly challenging. This led to the emergence of Knowledge Graph Question Answering (KGQA), whose goal is to answer natural language questions posed over KGs.\nA typical KGQA system consists of three main components: Entity Linking (EL), Relation Linking (RL), and Query Genration (QG). Starting from a natural language question q, EL and RL return a set of entities Eq and relations Rg therein. The QG module, crucially, takes q, Eq and Rq and generates a SPARQL query that produces the answer."}, {"title": "2 Related work", "content": "Early research in KG query generation was rule-based (Guo et al., 2005; Owens et al., 2008), template-based (Zenz et al., 2009; Unger et al., 2012) or search-based. For example, G\u00f6rlitz et al. (2012) developed a query generation heuristic to predict the final SPARQL representations by exhaustively checking all possible combinations of query patterns. However, manual or semi-manual approaches hit scalability issues with KGs like WikiData and DBpedia. More recent approaches belong to two main streams: information-retrieval based methods and Text-to-SPARQL approaches.\nInformation Retrieval KGQA. This family of methods involves the identification of sub-graphs relevant to q. Approaches include divide-and-conquer (Kim et al., 2023), fact retrieval based on linked entities (Baek et al., 2023), more complex methods involving hops, relation predictions, and triple sampling (Wu et al., 2023), or Evidence Pattern Retrieval (EPR) through structural dependency modeling (Ding et al., 2024).\nText-to-SPARQL. With the recent wave of decoder-based LLMs such as GPT (Brown et al., 2020), Mixtral (Jiang et al., 2024), and LLamA (Touvron et al., 2023), generative AI was also used to translate q into SPARQL queries. Notably, Zou et al. (2021) introduced a text-to-SPARQL model that leverages a relation-aware attention decoder and a pointer network encoder, incorporating three separate scaled dot-product attention mechanisms to generate SPARQL queries that capture entity, relation, and keyword representations. Banerjee et al. (2022) experimented with various models, including T5 (Raffel et al., 2020), BART (Lewis et al., 2019), and Pointer Generation Networks (See et al., 2017), to explore their efficacy in KGQA tasks. Rony et al. (2022)'s SGPT employs a stack of transformer encoders to extract linguistic features from q and GPT-2 as a decoder. However, this architecture is limited by its inability to capture connections among entities and relations in the underlying knowledge graph, leading to errors in generating triple sequences in the final SPARQL queries. Pliukhin et al. (2023) presented a one-shot generative approach, where the prompt is augmented with a KG fragment required to construct the query and a question-subgraph query example.\nDespite promising results, these architectures are prone to systematic errors. One such error, the so-called \"triple-flip\", refers to the reversal of subject and object positions in the generated SPARQL triples, yielding wrong, often empty answers. Qi et al. (2024) addressed this issue by developing TSET, a fine-tuned T5 model with a pretraining stage called Triplet Structure Correction. This approach aims to deepen the model's understanding of triple order, establishing state-of-the-art performance on major KGQA datasets.\nExample Selection in Few-Shot Learning. In-context learning (ICL) is a paradigm that leverages reasoning through analogies. A task description, question, and demonstration context are usually concatenated to create a prompt, which is then input into an LLM for prediction. Unlike fine-tuning, ICL performs predictions without gradient updates (Dong et al., 2023). Few-Shot Learning is a type of ICL where the demonstration context includes a few examples. Owing to the effectiveness of ICL and the obvious advantage of building systems that don't need domain-specific training, a great deal of research and engineering efforts have been devoted to designing suitable prompts. ICL has been successfully applied to many NLP problems, including QA (Chada and Natarajan, 2021; Chen et al., 2023) and KGQA (Li et al., 2023). Some studies have also focused on the selection of in-context examples. In particular, Liu et al. (2022) developed KATE, an unsupervised retriever that utilizes k-nearest neighbors and distance metrics (e.g., L2 distance and cosine similarity) to select in-context examples for tasks such as sentiment analysis, table-to-text generation, and question answering. Levy et al. (2023) explored the incorporation of diverse demonstrations into prompts for compositional semantic parsing task, demonstrating that such diversity leads to better structural coverage in target utterances. Kim et al. (2022) leveraged the generative capabilities of pre-trained language models to generate demonstrations for each class in downstream tasks, conditioned on test inputs and class information. Gonen et al. (2022) found that selecting examples based on perplexity, in particular lower perplexity, is an effective strategy. However, to the best of our knowledge, example selection has not yet been applied to KGQA."}, {"title": "3 Method", "content": "Given a collection of natural language questions Q and a knowledge graph G := (E,R, F), where E are entities, R are relations, and F \u2286 E X R X E are facts, KGQA is the problem of answering questions in Q based on G. KGQA can be framed as a text-to-SPARQL task, where a question q must be translated into a SPARQL query sq to be executed on G by a SPARQL engine, to return a (possibly empty) answer a. The entities and relations in q, denoted as Eq and Rq, may be, and usually are, extracted from q before generating sq. Hence, query generation can be tackled as a conditional text generation problem given q, Eq and Rq. Within the scope of ICL, Pe is a pre-trained LLM and the conditional input Eq, Rq, q is combined with other contextual information C, such as additional instructions, guidelines, constraints and demonstrations, all expressed via natural language text. Accordingly, the generated query is:\nSq = arg max Po(s|C, Eq, Rq, q). (1)\nS\n3.1 Dynamic Few-Shot Retrieval\nIn few-shot ICL, the choice of demonstrations to inject in the prompt can significantly affect performance. Usually, few-shot examples are predetermined representative instances of the task, hand-picked during prompt design. Conversely, we aim to retrieve good examples dynamically, based on their relevance to the input question. Inspired by Liu et al. (2022), we adopt a retrieval approach based on the similarity between a question q and a set of previously answered text-to-SPARQL examples collected in a storage S, where each example is a tuple including a question x, its entities Ex and relations Rx, and the associated SPARQL query sx. The question, its entities and relations (q, Eq, Rq) are mapped onto a vector representation eq \u2208 Rd using a sentence encoder. To properly feed such information to an encoder-only LM, we concatenate question, entities and relations in a single input sequence q := [q, Eq, Rq]. Likewise, we encode each example x \u2208 S into a vector ex \u2208 Rd and then compute the similarity between the target question and the storage:\nscore(q, x) = sim(eq, ex), \u2200x \u2208 S, (2)\nwhere the sim is a similarity function. Based on such a scoring, we retrieve the k-most similar examples S and include them as demonstrations in the in-context prompt.\n3.2 In-Context Prompt\nThe in-context prompt has three parts. The first is the task description, instructing the LLM with a numbered list of guidelines on the output format and on the available information. The second, highlighted in Figure 1 with a green block, contains the k retrieved demonstrations. Each demonstration consists of a question, its entities and relations, denoted as gold entities/relations, all paired with their SPARQL query delimited by <SPARQL></SPARQL> tags. The ### symbol delimits each single example. The final part is the input question, associated with its gold entities and relations. The answer returned by the LLM prompted as such is then parsed to extract the generated text enclosed in <SPARQL></SPARQL> tags. The resulting query Sq is executed by a SPARQL engine on G to yield the answer to q. We call our approach Dynamic Few-Shot Learning (DFSL).\n3.3 Multi-Query Generation\nA typical challenge faced by LLMs in SPARQL query generation is the understanding of what is the subject and what is the object of a relation, an information the model does not have. This problem is called triple-flip error (Qi et al., 2024). LLMs often end up in swapping the subject with the object in the query, almost choosing one way or the other randomly. Thanks to DFSL, this issue may be alleviated whenever there are similar demonstrations in the in-context prompt that clarify the subject-object roles. To further reduce triple-flip errors, we propose the generation of multiple SPARQL queries by retaining all the final hypotheses generated during beam search. The model uncertainty in placing subject and object is likely to be reflected in the beam search exploration. Intuitively, both triple-ordering hypotheses are considered plausible by the model. Thus, instead of just returning the most probable sequence s according to Equation 1, we keep the whole b queries { sq,1, . . ., Sq,b} formulated by beam search. We use DFSL-MQ to denote such a multi-query extension of DFSL.\nAnswer Selection. Executing multiple queries inevitably leads to multiple possible answers. Therefore, we must define an answer selection criterion. We designed two heuristics: Largest Set (LS) and First Set (FS). LS executes all the b queries, obtaining with each query sq,j a (possibly empty) answer set Aj. LS then selects, among {A1, . . ., Ab}, the largest one\u00b2, i.e:\nA = arg max(|A1|, ..., |A6|),\nAi\nthe rationale being that incorrect candidates will likely have empty results. However, LS can be misled into selecting answers from under-constrained queries that return many irrelevant instances. FS adheres to the natural beams ordering by selecting the first query that yields a non-empty answer set."}, {"title": "4 Experiments", "content": "In this section, we aim to study the effects of each component involved in our DFSL approach. We evaluate DFSL and its extension DFSL-MQ on four KGQA datasets. In our investigation, we consider different backbones and we compare with multiple baselines and state-of-the-art solutions.\n4.1 Datasets\nTo assess the flexibility and robustness of our approach, we evaluate it on four heterogeneous KGQA benchmarks based on two different Knowledge Graphs (Wikidata, DBpedia).\nQALD-9 DB. QALD-9 (Ngomo, 2018) is a dataset from the Question Answering over Linked Data (QALD) challenge series. It comprises 408 training questions and 150 test questions. Unlike the other KGQA benchmarks, the SPARQL queries are meant for a DBpedia Knowledge Graph. We refer to it as QALD-9 DB to emphasize that.\nQALD-9 plus. QALD-9 plus extends QALD-9 on new languages and transfers SPARQL queries from DBpedia to Wikidata. Although some queries were not portable to Wikidata due to the absence of corresponding information, it still comprises 371 training questions and 136 test questions. In our experiments, we only consider English questions.\nQALD-10. QALD-10 (Usbeck et al., 2023) is the latest dataset in the QALD series, designed to increase the complexity of gold SPARQL queries. It consists of 412 training questions extracted from QALD-9 plus Wikidata. The test set was created from scratch, comprising 394 test questions that express real-world information needs. Test questions significantly differ from those in training.\nLC-QUAD 2.0. LC-QuAD 2.0 (Dubey et al., 2019) is a large-scale dataset grounded on Wikidata. It consists of 30,226 simple and complex questions: 24,180 in training, and 6,046 in test. Questions are diverse. They include single- and multi-fact, boolean, count, and other query types. LC-QuAD 2.0 allows us to gauge the DFSL performance against a large text-to-SPARQL storage.\n4.2 Backbones\nMixtral 8x7B. Based on the Sparse Mixture of Experts (SMOE) architecture (Fedus et al., 2022), Mixtral 8x7B (Jiang et al., 2024) is a 46.7B parameters model. Among the backbones adopted in this paper, Mixtral is the smallest. Moreover, thanks to the characteristics of its SMoE architecture, less than 13B are active at each inference step, making Mixtral particularly efficient.\nLlama-3 70B. Built upon the Llama architecture (Touvron et al., 2023), Llama-3 70B has been trained on 15T tokens, a 650% increase from its predecessor, Llama 2. At the moment we are writing, Llama-3 70B is one of the best-performing open-weights LLMs available.\nCodeLlama 70B. Initialized from Llama2 70B, CodeLlama (Rozi\u00e8re et al., 2024) is a specialized version fine-tuned on 1T tokens of code-heavy data. Therefore, we expect CodeLlama to be particularly suitable for SPARQL query generation.\n4.3 Baselines\nPlain Question. This is a naive baseline where we feed an LLM only with the task description and the question q. Without in-context examples nor any entity or relation associated with q, the LLM can only rely on its parameter memory.\nZero-Shot Learning. Here we do not provide any demonstrative example in the prompt. However, unlike the plain question baseline, we do inject golden entities and relations into the prompt. With reference to Figure 1, the In-Context prompt remains the same but without the green-like block containing the demonstrations.\nFew-Shot Learning. The prompt is filled with a single set of k manually selected examples, used for all the questions in the test set. The examples were chosen to maximize diversity and cover different kinds of queries\u00b3.\nMulti Query Prompting (DFSL-MQP). As an alternative to our proposed multi-query generation (DFSL-MQ), this baseline consists in a naive multi-query prompting strategy. Essentially, we ask the model to generate more queries to answer the question. To ease the creation of inverted subject-object queries that can solve triple-flip errors, we extend the prompt to explicitly ask the model to produce this kind of SPARQL queries. Answer selection uses LS and FS heuristics, like with DFSL-MQ.\n4.4 Experimental Setup\nImplementation. In our experiments, the training set of each dataset serves as storage for the retrieval of the k most similar examples (see the next paragraph for details on k tuning) with DFSL. Examples are encoded with a sentence transformer4, all-mpnet-base-v25, and sim is defined as the cosine similarity. Inference is performed via beam search in both DFSL, where b is set to 3, and DFSL-MQ, with b set to 10. All the experiments were run on a cluster of 4 NVIDIA A100 GPUs.\nNumber of Few-shot Examples. We first analyzed how the number of few-shot examples k retrieved by DFSL affects the performance. We chose among k = {1,3,5,7} and evaluated DFSL with Llama 3 70B backbone on the four datasets. The results shown in Figure 3 suggest that values of k greater than one perform comparably well on smaller benchmarks, while on LC-QUAD 2.0, where there are about 25 thousands examples as storage, increasing k seems to be beneficial. This may be due to the increased likelihood of finding similar examples in larger datasets as k grows. We set k = 5 for all the forthcoming experiments, which is a good trade-off across all the datasets.\n4.5 Results\nImpact of Dynamic Examples. To measure the importance of retrieving few-shot examples dynamically, we compare DFSL on different backbones against Zero-Shot and Few-Shot Learning baselines. Results are outlined in Table 1.\nIn terms of backbones, Llama 3 consistently outperforms both Mixtral and CodeLlama in zero-shot learning scenario, whereas in few-shot, results are generally comparable between Llama-3 and CodeLlama. Such a strong Llama 3 zero-shot performance may be caused by some sort of data contamination, however we leave such an investigation for future works.\nBoth few-shot learning and DFSL generally yield substantial gains with respect to zero-shot baseline on all the backbones and datasets. An exception occurs in QALD-10 with Llama-3. Notably, when comparing DFSL and Few-shot Learning baseline, we can see our approach improving F1 scores by a large margin in LC-QUAD 2.0, QALD-9 Plus and QALD-9 DB, with F1 increasing up to 21 absolute points6. In QALD-10 instead, where the test set has a different distribution from its training, there are no significant differences between DFSL and the standard few-shot learning approach. Indeed, an approach like DFSL brings little benefits when the storage only contains unrelated examples.\nOverall, DFSL with CodeLlama achieved the greatest performance with respect to all the other configurations. Therefore, we adopt CodeLlama as our backbone in the following DFSL experiments.\nImpact of Multi-Query Generation. Here we investigate DFSL-MQ, the multi-query approach extending DFSL. We evaluate both answer selection strategies, LS and FS, and compare them against the plain DFSL and the multi-query prompting baseline described in Section 4.3. All the results are outlined in Table 2.\nHaving multiple queries is not necessarily beneficial. Indeed, the multi-query prompting baseline under-performs in three datasets out of four with respect to (single query) DFSL, regardless of the answer selection method adopted. DFSL-MQ instead proves to be generally beneficial. Both Largest Set and First Set heuristics are effective when the hypotheses come from the beams. Furthermore, FS consistently outperforms LS, even by substantial margins in QALD-9 DB.\nIn-context Learning vs Fine-tuning. Up to this point, we have assessed DFSL in the scope of In-Context Learning approaches. In Table 3 instead, we compare our approach against state-of-\n4.6 Ablation studies\nDifferent Example Encoding. As described in Section 3.1, to compute the embeddings we concatenated the textual input made of the question and its list of entities and relations. Here, we gauge the impact of this additional information on DFSL performance. In Figure 2 we compare DFSL, with a variant where we only embed the natural language question q, without any additional data concatenated. The evaluation carried out in all the benchmarks and with all the backbones, demonstrates that such information improves the quality of the generated queries.\nAbsence of gold information. In KGQA, text-to-SPARQL generation usually relies not only on the question itself, but also on entities and relations associated to it. Here we assess DFSL when either the entities Eq or the relations Rq, or both are missing. The information is removed throughout the entire process. For example, when removing entities, we discard them from both the storage and the prompt. Even the embeddings for the retrieval are computed by encoding an input without any entity concatenated in q, i.e. becoming q = [q, Rq]. We report this on QALD-9 DB dataset. By observing the results outlined in Table 4, it is clear that, without full knowledge of the entities and the relations required for generating the query, the LLM performance drops significantly. Nonetheless, even in the case where no information is given (DFSL w/o Eq, Rq), the presence of dynamic demonstrations is essential, yielding a 33+ absolute F1 increase compared to plain question baseline."}, {"title": "5 Conclusion", "content": "In this paper, we introduced DFSL, a novel approach to Knowledge Graph Question Answering. This method leverages semantic search to dynamically retrieve relevant examples from the training set, enriching the prompt for LLMs to improve the generation of SPARQL queries. We conducted comprehensive experiments on four publicly available datasets based on two widely-used KBs, DBpedia and Wikidata. By employing three different state-of-the-art LLMs as backbones, we demonstrated that DFSL achieves superior performance compared to both standard in-context learning techniques and state-of-the-art models fine-tuned on the downstream task. We further conducted an extensive evaluation of DFSL through ablation studies to measure the impact of hyper-parameters, different backbones, embedding methods, answer selection strategies, and the inclusion or exclusion of entities and relations information associated to a question. The code will be released publicly upon acceptance of the paper. In the future, we plan to study the effectiveness of DFSL in cognate domains like text-to-SQL.\nLimitations\nWe recognize some limitations in our work. Our experiments are all on English-based datasets, where notoriously LLMs are better performing. Moreover, the massive pre-training of those LLMs on a vast portion of the Web, may expose those models to unintended data contamination. Experiments only focused on LLMs with large number of parameters, without investigating the behaviour of smaller models. To encode examples, we limited the investigation to what kind of text to encode (just the question, or the question and its entities and relations), without exploring different embedding models, similarity criteria or other input concatenation strategies. We leave these investigations to future work."}]}