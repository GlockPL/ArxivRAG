{"title": "CueTip: An Interactive and Explainable Physics-aware Pool Assistant", "authors": ["SEAN MEMERY", "KEVIN DENAMGANAI", "JIAXIN ZHANG", "ZEHAI TU", "YIWEN GUO", "KARTIC SUBR"], "abstract": "We present an interactive and explainable automated coaching assistant called CueTip for a variant of pool/billiards. CueTip's novelty lies in its combination of three features: a natural-language interface, an ability to perform contextual, physics-aware reasoning, and that its explanations are rooted in a set of predetermined guidelines developed by domain experts. We instrument a physics simulator so that it generates event traces in natural language alongside traditional state traces. Event traces lend themselves to interpretation by language models, which serve as the interface to our assistant. We design and train a neural adaptor that decouples tactical choices made by CueTip from its interactivity and explainability allowing it to be reconfigured to mimic any pool playing agent. Our experiments show that CueTip enables contextual query-based assistance and explanations while maintaining the strength of the agent in terms of win rate (improving it in some situations). The explanations generated by CueTip are physically-aware and grounded in the expert rules and are therefore more reliable.", "sections": [{"title": "1 INTRODUCTION", "content": "Imagine interacting and reasoning in natural language with an automatic assistant about shot options for a game of pool. Now, given a description of balls on a table, imagine querying a language model (LM) to predict their positions after a specific shot is taken. While LMs are excellent at conversational exchange, they are inherently ill-suited to reason about dynamics in a physical system. Even if the initial state, shot dynamics, and all physical parameters can be described exactly, the LM is likely to hallucinate predictions that are physically implausible.\nWe use an off-the-shelf LM and a physics simulator [Kiefl 2024] in closed loop to address the above problem. Rather than attempting to imbue gargantuan LMs with knowledge of pool-table physics, our insight is to instrument a physical simulator for pool to output a natural language trace composed of a few simple events. We translate this insight into CueTip a natural language assistant for a variant of pool. We demonstrate CueTip's ability to provide contextual help based on a user query specific to a given state of the table, by suggesting relevant shots along with explanations. An overview of this process is provided in Figure 1. The assistant is also adapted to empirically mimic the shot-selection strategy of any pool playing agent. CueTip accepts a set of predefined expert rules, i.e. heuristics, that are used to evaluate the quality of a shot for a given state. The explanations generated by the assistant are rooted in these rules.\nThere is considerable interest in reasoning about physics with LMs using strategies such as training or fine-tuning [Liu et al. 2022; Wang et al. 2023a], using LMs to perform planning [Feng et al. 2024; Huang et al. 2023; Valmeekam et al. 2023], or pure prompt engineering [Polverini and Gregorcic 2024; Yeadon et al. 2024]. Our approach could be categorized as using LMs to help planning [Kambhampati et al. 2024]. That is, we devise a mech- anism where the LM recommends plans in consultation with a simulator but the tuning of that plan is domain-specific. Finally, the tuned plan is explained by the LM with respect to relevant domain-specific guidelines. Our design of CueTip is adaptable and supports a wide range of queries, it can be tuned easily to mimic an agent (used for determining shots) and accommodate additional expert rules (used as guidelines for shot suggestion and explanations).\nWe introduce an interactive and explainable natural-language assistant for pool. Within this context, our contributions are:\n(1) Physics awareness: we build contextual, physics-aware rea- soning into the assistant;\n(2) Explainability: we develop a mechanism for generating reli- able explanations, grounded in predetermined domain-expert rules;\n(3) Modularity: we decouple the assistant from the underlying agent at the cost of pretraining a small multilayer perceptron network- which serves as a neural surrogate of the agent; and\n(4) Uncertainty awareness: our mechanism accommodates for uncertainty around the value of shots."}, {"title": "2 BACKGROUND", "content": "2.1 Related Work\nPool AI. Pool has a long history of use as a playground for artificial intelligence research. There was a prominent trend on de- veloping performant agents for pool [Alian 2003; Greenspan et al. 2008; Sang and Cheung 1994; Smith 2007]. Many of these agents competed in the Computational Pool Tournament [Archibald et al. 2010] including CUECARD [Archibald et al. 2009]. Few of these works emphasise explainability, although there is a recent impe- tus in this direction [Fragkiadaki et al. 2015; Tung et al. 2019]. There is some interest in mimicking human decision-making through feature analysis of expert games and a learned network, resulting in improved long-term planning [Tung et al. 2019]. We similarily develop a scheme to build an empirical surrogate of a pool-playing agent, but additionally imbue it with interactivity and explainability.\nExplainable AI and LMs. The topic of explainability in re- lation to LMs has become increasingly popular [Luo and Specia 2024; Zhao et al. 2023]. Explainability has assumed an important role given an increased appreciation of the limitations of LMs, for example in planning and reasoning [Kambhampati et al. 2024] or amplifying biases [Gallegos et al. 2024]. Their inability to pro- vide explanations [Lanham et al. 2023; Turpin et al. 2024] has led to innovative schemes such as local explanations [Chuang et al. 2024; Ferrando et al. 2022] (where an LM's output is explained, for example, through feature attribution), perturbation methods [Lundberg and Lee 2017] (features of the input are removed to observe changes in output), and gradient-based methods [Sun- dararajan et al. 2017] (gradient accumulation is utilised to mea- sure the importance of specific inputs). These methods measure relations between features but have low generalization across domains and are not well suited to online decision-making.\nLMs and Physics Environments. Physical reasoning is an inherently out-of-domain problem for LMs, however many works have attempted to leverage their next-word prediction capabili- ties within physical environments. Following the emergence of math-word problems to benchmark LMs on their math reasoning capabilities [Cobbe et al. 2021; Hendrycks et al. 2021; Lu et al. 2024], [Ding et al. 2023] proposed the PhysQA dataset consisting of physics-word problems to evaluate their physics reasoning capabilities. This form of evaluation is limited, though, as it does not account for dynamic physics interactions. This is a gap that Mind's Eye [Liu et al. 2022] starts to address by incorporating the MuJoCo physics simulator [Todorov et al. 2012] to inform question answering via fine tuning of a LM but it does not utilise the simulator to ground the LM's reasoning. [Lyu et al. 2024] improves upon the latter by using a tool integration of the Mu-JoCo physics simulator for LMs being tested on various scientific problems. Some examples that blend online reasoning and dy- namic physics utilise a Minecraft environment [Fan et al. 2022; Wang et al. 2023c], but they typically evaluate LM based agents performance on more general tasks such as exploration and skill acquisition, as opposed to specifically focusing on physics rea- soning and understanding. Following investigations of intuitive physics [Kubricht et al. 2017] in human beings [Baillargeon 1995; Baillargeon et al. 1985; Kim and Spelke 1999; Spelke et al. 1992] and machine learning [Battaglia et al. 2016, 2013; Piloto et al. 2022; Smith et al. 2019; Watters et al. 2017], [Jassim et al. 2024] proposes the GRASP benchmark to evaluate more systematically intuitive physics understanding in multimodal LMs. Results stress a lack of perceptual understanding as a major shortcoming, that [Balazadeh et al. 2024] proposes to improve upon via simulation-to-reality fine-tuning of a modular, specialised visual LM. These studies take place in open-loop contexts, i.e. without execution of actions by the LM in the environment. Recently, building upon [Voudouris et al. 2022], which solely focused on improving the ecological validity of evaluating object permanence, [Mecattaf et al. 2024] pro- posed the LLM-AAI benchmark to measure multiple but limited intuitive physics items in several LMs with execution of actions in ecologically valid environments, which are shown to have internal validity towards precisely evaluating intuitive physics understand- ing and reasoning. It remains to propose similar environments for all intuitive physics items. Some works have incorporated the physics simulator in closed-loop reasoning along with an LM [Cherian et al. 2024; Memery et al. 2024]. These results high- light their unsuitability due to a lack of physical 'common-sense' coupled with an ineptness in interpreting numerical simulation traces.\nLearning from Domain Expert Rules. Domain expert rules played a foundational role in artificial intelligence through Expert Systems [Lindsay et al. 1993; Ryan 2017; Yanase and Triantaphyl- lou 2019], which encoded domain knowledge as explicit if-then statements to emulate expert decision-making. While traditional expert systems faced scalability challenges, modern approaches are exploring the integration of expert knowledge with LMs to combine structured reasoning with natural language capabilities [Arsevska et al. 2016; Louie et al. 2024; Silva-Rodr\u00edguez et al. 2023]. This integration offers potential advantages: domain heuristics can provide reliable constraints on LM outputs, while LMs can make expert knowledge more flexible and generalizable. These approaches suggest promising directions for developing systems that balance structured domain expertise with natural language understanding."}, {"title": "2.2 Setup", "content": "Let $x \\in X$ represent the state of a pool table and $\\theta \\in \\Theta$ represent shot parameters $(v, \\alpha, \\beta, a, b)$ consisting of the velocity of the cue stick, $v \\in [0, 5]$, its angles (azimuth, $\\alpha \\in [0,360]$, and elevation, $\\beta \\in [0,90]$) and the point of contact on the cue ball (for imparting spin), $(a, b) \\in [-0.5, 0.5]^2$. A standard pool agent $\\pi: X \\rightarrow \\Theta$ maps a given state of the table to a shot (ideally an optimal play) for that state. We assume that we are given a simulation function $f : X \\times \\Theta \\rightarrow X$, so that a shot by a standard agent takes a table state $x$ to $f(x, \\pi(x))$.\n3Pool: A Simplified pool game. We define a variant of 9-ball pool for two players, 3Pool, where the table contains a white (cue) ball and 6 colour balls. Each player is assigned a pre-determined set of 3 colour balls. The game begins with a state $x$ where the balls are in random positions. A player wins when they have pocketed their assigned set of three balls. At each turn, a player (say Alice) strikes the cue ball with a stick according to parameters $\\theta$ taking the table's state to $f(x, \\theta)$. If the shot results in one of Alice's balls being pocketed without committing a foul, then Alice may play again; otherwise Alice's opponent plays the next shot. Alice's shot is deemed a foul if: the cue ball makes first contact with her opponent's ball; if the cue ball makes contact with no ball; or if the cue ball is pocketed. Simulating a complete game involves executing a sequence of shots by Alice and her opponent until one of them wins (pockets their balls). We adapt an existing simulator [Kiefl 2024] to build a physics-based environment which we call 3Pool.\nRules to evaluate shots. We adopt a set of rules from the text- book [Alciatore 2004], where domain experts detail heuristics to evaluate the state of a table resulting from a shot. Some rules are assessments of difficulties associated with certain configurations within a state. We choose $m = 29$ rules (listed in Appendix A within supplemental material) that apply to 3Pool, representing them as $R = \\{R_i, r_i\\}, i = 1, 2, 3, ..., m$ where $R_i \\in L$ is a descrip- tion of the rule in natural language and $r_i: X \\times \\Theta \\rightarrow [0, 1]$ is a rule-evaluation function that evaluates the relevance of the rule to a state-shot pair.\nAgents evaluated. Poolmaster [Dussault and Landry 2006], denoted as $\\pi_{pm}$, is a tournament-winning pool agent that op- timizes shot selection through position-based value estimation. The agent uses an objective function that considers target ball potting, cue ball positioning, and shot legality. Shot evaluation considers both direct and indirect shots through hand-crafted difficulty coefficients, with indirect shots transformed via table mirroring for unified assessment. Shot selection is performed through a grid-based search, where the table is discretized into a $G \\in \\{15, 30\\}$ grid. For each position, the agent evaluates dis- cretized shot parameters to identify positions that maximize the objective function, ultimately selecting the shot with the high- est combined immediate and future value. We also implement a simple greedy baseline agent $\\pi_g$ that computes shot angle $\\alpha$ via trigonometric approximation (other shot parameters are sampled randomly) to pocket each target ball in its closest pocket. $\\pi_g$ per- forms shot selection to maximize the number of potted target balls using random selection among equivalent shots. CueTip can use any agent that fits the definition of $\\pi(x)$."}, {"title": "3 INTERACTIVE AND EXPLAINABLE POOL ASSISTANT", "content": "We explain three important aspects of CueTip. In Section 3.1 we introduce our simple scheme to represent simulation traces as sequences of events described textually. Then, in Section 3.2 we define our notion of interactive and explainable agents and explain the three modules of CueTip that communicate with a physics simulator and an LM. Finally, in Section 3.3, we explain how we train a simple neural network to behave as an empirical surrogate for any given agent and how this surrogate encodes uncertainty.\n3.1 Event-based representation of simulation traces\nTo facilitate language-based reasoning, we instrumented the simu- lation as $f_e: X \\times \\Theta \\rightarrow X \\times L$ to output $L \\in S$, an ordered sequence $L = [L_0, L_1, L_2, ...]$ of predetermined events where each event $L_i$ is a concatenation of two string tokens followed by information tokens:\n(1) ball-ball-<bid1>-<bid2> when ball bid1 collides with ball bid2;\n(2) ball-cushion-<bid> when ball bid bounces off cushion; and\n(3) ball-pocket-<bid>-<pid> when ball bid falls into pocket pid\nwhich serve as an abstraction of state information ignoring details such as geometry and dynamics attributes (velocity, spin, etc.). Although such an encoding of simulation traces is lossy, it enables qualitative comparisons of shots and their outcomes.\n3.2 Interactivity and explainability\nWe introduce agents $I : X \\times L \\rightarrow \\Theta \\times L$. Given a contex- tual reasoning query $Q$ in natural language $L$ and state $x$, each such agent $I(x, Q)$ returns a shot $\\theta$ along with an explanation $E \\in L$ that is rooted in domain expert rules. We design a family of such agents using three sequential stages: A recommender hy- pothesizes shots relevant to a state of the table $x$ and contextual query $Q$; a tuner optimizes the recommended shots to obtain $\\theta^*$ as the shot that best addresses the contextual query; and finally an explainer that generates explanations $E$ based on rules $R$. The recommender first queries an out-of-the-box LM to generate po- tential sequences of events $L = l(Q,C), L, Q, C \\in L$ where $Q$ combines the user query with a prompt requesting relevant event sequences and $C$ is a context specifying a description of state and a list of ball IDs for the agent to target. Chain-of-Thought prompting [Wei et al. 2022] is utilised, to allow the recommender to plan the shots it will suggest before producing the event se- quences. Details of this process are included in Appendix B. The central role of the recommender is to find $N_r \\in N$ hypotheti- cal shots $\\theta_k$ that result in a sequence of simulation events $L_k$ (via $f_e(x, \\theta_k)$) that resembles $L$. We achieve this via minimization of $d_1 (L, L_k) - (|L_k| + ||\\theta_k||)$ using simulated annealing, where $d_L (L, L_k)$ yields the longest common (ordered) subsequence of $L$ and $L_k$ that necessarily begins with the first event in $L$. The second term is used to penalise exaggerated values. Finally, each shot has a strategy label $s_k \\in \\{offensive, defensive, none\\}$ and a difficulty label $d_k \\in \\{easy, medium, hard, none\\}$ based on the shot's desired characteristics: $(s_k, d_k) = l(Q, C, L_k)$. These labels are used in Section 3.4.\nGiven $x$ and $\\{\\theta_k\\}$, the tuner first evaluates $x_k = f_e(x, \\theta_k)$ for each $k$ and then evaluates $r_k$ for each $k$ using the expert rules so that $r = r_i(x_k, \\theta_k), i = 1, 2, ..., m$. For some rules, this requires knowledge of the result of executing the shot which we obtain via $f(x, \\theta_k)$. The tuner then iteratively optimizes $\\theta_k$ until the expected value (explained in 3.3) of the shot is maximized. The tuner returns the highest-value shot $\\theta^*$ as the shot selected by $I(x, Q)$ along with $r^*$, the corresponding vector of rule function evaluations. The explainer e augments a skeleton prompt $P$ con- taining general information such as a description of the task, table specifications, text-encodings of all events and some example se- quences with contextual information. The contextual information $C_e$ includes the positions of balls in $x$, the dictionary of shot pa- rameters and a list of rules and their value functions evaluated on $(x, \\theta)$. Each field is quantized and encoded as a string. The final explanation is generated by an LM as $E = l(P_e)$ where the augmented prompt $P_e = e(P, C_e)$. We refer readers to Appendix B for details and examples of $P_e$ and $C_e$.\n3.3 Incorporating uncertainty via neural approximation\nWe build an empirical surrogate $\\tilde{\\pi}$ for any agent $\\pi$ by training it to approximate the distribution $p(v | x, \\theta, \\pi)$ of the expected win rate of a shot $\\theta$ given a table state $x$. Since our goal is to use $\\tilde{\\pi}$ instead of $\\pi$ within an explainable agent $II$, for explanations to be grounded in the set of expert rules $R$, we define $\\tilde{\\pi}$ to be a mapping from $r \\in [0, 1]^m$ (rather than from $(x, \\theta)$). The elements of $r$ are $r_i (x, \\theta)$. We discretize $p(v | x, \\theta, \\pi)$ using $n$ bins and therefore $\\tilde{\\pi}: [0,1]^m \\rightarrow [0, 1]^n$. At test time, we use the evaluated $\\tilde{p} = (r)$ to assess the quality of the agent's shot. A high expectation indicates a good payoff and a low variance implies robustness to imperfections in execution.\nOur environment models execution errors as Gaussian noise $\\epsilon \\sim N(0, \\sigma^2)$ injected into each shot $\\theta$ given a table state $x$. The execution of the perturbed action results in a stochastic state vari- able $x_1 = f(x, \\pi(x) + \\epsilon)$. The value of $\\theta$ is the expected win rate $v \\in [0, 1]$ for player 1, which may be estimated via Monte Carlo average over $M$ sampled $x_1$ and simulating $N$ complete games. Instead, we estimate the probability distribution $p(v | x, \\theta, \\pi)$ which encodes uncertainty surrounding the quality of the shot. We use a simple Monte Carlo method, that averages n-bin histograms $\\tilde{p} \\in [0, 1]^n$ of the $M$ expected win ratio $v_j, j = 1, 2, ..., M$ (for each sampled $x_1$). Ideally this would be a tight distribution with a peak at $v = 1$. We use these samples to train our neural adap- tor: $[0,1]^m \\rightarrow [0,1]^n$ (see Algorithm 1), which consists of a simple multi-layer perceptron network (MLP) with six layers of size 256 and ReLU activation functions. We train this using a Cross-Entropy loss function with a batch size of 128, a learning rate of 0.005, a dropout rate of 0.25 over 25 epochs.\n3.4 Incorporating strategy and difficulty\nWhile the strategy and difficulty labels s and d characterize a shot, we additionally define binary strategy vectors $w_o, w_d \\in \\{0, 1\\}^m$ containing classification across rules (on offensive vs defensive). Then we use these to calculate the strategy score $v_s$ and the difficulty score $v_d$ for each shot. If $r_t$ is the rule evaluations asso- ciated with a shot $(x_t, \\theta_t)$ being tuned, and if $s$ is defensive, then $v_s = (w_d-w_o)r_t$. If $s$ is offensive then $v_s = (w_o-w_d)r_t$. We cal- culate a difficulty score $v_d = H_{max}-|H_t-H_d|$where $H_t = H(\\tilde{\\pi}(r_t))$ is the entropy of the approximate value distribution and $H_d$ is the mean entropy of the $d^{th}$ section of the distribution of entropies across all samples in the dataset- we partition the entropy distri- bution across training samples into 3 difficulty sets: low [0, 0.4), medium [0.4, 0.8) or high.\nThe tuner in Section 3.2 combines these terms with uncertainty to perform shot optimization, to obtain $\\theta^*$ by maximizing the objective function $E[\\tilde{p}] + v_s + v_d$ for each shot $\\theta_{\\kappa}$.\n3.5 Summary\nGiven an agent $\\pi$, we first train its neural surrogate $\\tilde{\\pi}$ that can rapidly approximate the distribution of win-rates of $r$ as player 1 from any state $x$. Then, we embed the surrogate within the tuner of CueTip, resulting in agent $II$ which can be queried and provides explanations in natural language. That is, for each agent $\\pi$ we can associate its three variants as a tuple $(\\pi, \\tilde{\\pi}, II)$. We implemented comparisons for the PoolMaster agent $(\\pi_{pm}, \\tilde{\\pi}_{pm}, II_{pm})$ and for a simple greedy agent $(\\pi_{g}, \\tilde{\\pi}_{g}, I_{g})$ as described in Section 4.2.1."}, {"title": "4 EXPERIMENTS", "content": "4.1 Qualitative evaluation across diverse queries\nWe demonstrate the interactive capabilities of CueTip through four qualitative examples of diverse queries in Figure 5. Given an initial state $x_0$ (shown in the center of the figure), each query $Q \\in L$ is executed as $(\\theta^*, E) = II(x_0, Q)$ with 5 candidate shots and 300 optimization steps as hyperparameters. The recommender generates candidate shots which are optimized by the tuner to select $\\theta^*$, followed by explanation generation via the explainer component $E$. The results of this are presented in Figure 5 to demonstrate the system's interactive shot generation and expla- nation capabilities.\n4.2 Quantitative evaluation of performance, explainability\n4.2.1 Performance. We evaluate agent performance through pair- wise games in our 3Pool environment of three versions each of two core agents- PoolMaster (pm) and greedy (g). For each of these agents, we have the agent $(\\pi_{pm}, \\pi_g)$, their neural surrogates $(\\tilde{\\pi}_{pm}, \\tilde{\\pi}_g)$ and their CueTip versions $(II_{pm}, II_g)$. For each pair of these 6 agents, we simulate 100 games with 50 games each as player 1 (to mitigate potential starting position bias). The mean percentage win rates over the 100 games (and standard deviations) are tabulated in Table 1 with each cell showing the win rate of the agent on the row as player 1 against the agent on the column as player 2. Standard deviations were calculated by fitting binomial distributions.\nThe hyperparameters for each agent are defined as follows: For pm, the search grid size G is set to [15, 30], with shot parameters $\\Theta = \\{v, \\beta, a, b\\}$ to be optimised with 5 discretized intervals each. For neural-based agents $(\\tilde{\\pi}, II)$, we set the number of candidate shots K = 3 and use N = 300 simulated annealing steps. For $\\tilde{\\pi}$, the K candidate shots begin as randomly sampled parameters, before being tuned. Importantly, for II, the query prompt provided to the agent is simply: \"Find the best shot for me in this position\". The networks for each agent $\\{\\pi_{pm}, \\tilde{\\pi}_g, II_{pm}, II_g\\}$ are trained on 2500 state-shot pairs generated from the $\\pi_{pm}$ or $\\pi_g$ agents, as described in Section 3.3.\n4.2.2 Relevance to expert rules. The reliability of the explana- tions provided by CueTip hinges on the ability of the LM to assess the relevance of each expert rule to a given state-shot pair. This is used by the explainer within its context $C_e$. We ver- ified that the LM estimates relevances that match ground truth rule evaluations. The dataset for this is generated through a two- phase sampling process. First, we extract state-shot pairs $(x, \\theta)$ from the network training dataset and apply K-means clustering (K = 25) to their rule evaluation vectors, then sample uniformly from each cluster to create a diverse evaluation set that captures varying shot characteristics. State-shot pairs $(x_r, \\theta_r)$ for evalua- tion are sampled from this dataset, evaluating the ground truth $r_i (x, \\theta), i = 1, 2...., m$ and classifying them according to a Likert scale quantizing the [0, 1] range of $r_i$ into bins:\nOur experiment constructs C from $(x_r, \\theta_r, r_r)$ (as done by the explainer described in Sec. 3.2) and uses this to augment a skeleton test prompt $P_t$. This prompt queries the LM to predict the vector of Likert values $s_t$ by comparing the keys $K_t$ given the context $C_r$. Further details are presented in Appendix C. Ideally, the distance $s_t- s_r$ between the test and reference values should be < 1. Figure 3 plots the mean distances (orange bars) for each rule (X- axis) along with error bars across the dataset. The plot on the right side shows the aggregate over all rules. Figure 3 also shows distances (blue bars) from a control method where the context is constructed from only the state-shot pair $(x_r, \\theta_r)$ instead of $(x_\\gamma, \\theta_\\gamma, r_\\gamma)$.\n4.2.3 User study. We evaluate explanation quality through a comparative user study between CueTip and a baseline LM. Our participant pool U consists of 100 users recruited through Prolific [Prolific 2025]. Each participant evaluated 20 unique state-shot pairs sampled from the diverse dataset described in Section 4.2.2. Each evaluation instance presents participants with an animated GIF visualization of the state-shot pair $(x_i, \\theta_i)$ with simulated frames, an explanation generated by either CueTip or the baseline model, and a 7-point Likert scale for quality assessment. Partic- ipants provide ratings $r_{u,i} \\in \\{1, ..., 7\\}$ for each instance, with a mandatory 10-second viewing period before rating is allowed. Examples of the user prompt are presented in Appendix E. The baseline model follows the same explanation generation proto- col as CueTip (Section 3.2) but without access to rule function evaluations $r$, while retaining access to the natural language rule descriptions $R_i$. Post-study, participants self-report their pool expertise $l_u \\in \\{None, Low, High\\}$. Figure 4 presents the rating distributions seperated by expertise level."}, {"title": "5 RESULTS", "content": "5.1 Qualitative evaluation of our agent\nFigure 5 presents four queries given to CueTip. The generated shots are seen to be tailored to the query in each case. For ex- ample, given the query \"I am feeling defensive, find me a defen- sive shot\", CueTip generates event sequences with strategy label s = defensive and difficulty label d = medium (not illustrated). The tuning process, guided by these labels, produces a shot $\\theta^*$ that maximizes defensive positioning, as evidenced in the motion trajectory visualization. The explainer component grounds this shot in expert rules $R_i$, where defensive rule $R_5$ (safety oppor- tunity identification) applies positively (hence shown in green) while offensive rule $R_{13}$ (target ball potting prioritization) does not apply (hence shown red), demonstrating the effectiveness of the strategy-aware optimization.\nFor \"Show me a trick shot!\", CueTip demonstrates its capac- ity to generate a high-difficulty shots by setting s = offensive and d = hard. The selected shot $\\theta^*$ exhibits multiple complex characteristics: a long distance to travel and multiple cushion col- lisions. The neural adaptor $\\tilde{\\pi}$ predicts an expected value of 0.34, indicating significant execution difficulty (not illustrated). The explainer identifies relevant rules including $R_{14}$ (distance-based difficulty scaling) and $R_{29}$ (multi-cushion complexity), reflecting its high-risk nature.\n5.2 Quantitative evaluation of our agent\n5.2.1 Performance. The win rates in Table 1 show that the neural surrogate $\\tilde{\\pi}_{pm}$ performas best, achieving the highest win rate. No- tably, $\\tilde{\\pi}_{pm}$ achieves a win rate exceeding 50% against its training baseline $\\pi_{pm}$ (winner of the computational pool tournament). Re- sults suggest that the neural adaptor's shot optimization strategy (with stochasticity and uncertainty) surpasses the agents used to train the adaptor. The greedy surrogate, $\\tilde{\\pi}_g$, exhibits reduced performance with a 35% win rate against $\\pi_{pm}$, highlighting the importance of the underlying agent used during training. The in- teractive agents $II_{pm}$ and $II_g$ maintain strong performance, with $II_{pm}$ achieving a 59% win rate against $\\pi_{pm}$. The interactive agents $II_{pm}$ and $II_g$ exhibit a consistent performance reduction compared to their non-interactive counterparts, highlighting the trade-off between performance and interactivity.\n5.2.2 Relevance to expert rules. Figure 3 plots the results of the experiment presented in Section 4.2.2. Since the error is < 1 on average, for the orange bars, the experiment verifies that a combination of textual Likert keys and rule evaluation vector $r_r$ ensures that the LM's evaluation of the current state is aligned with the reference. The blue bars represent a higher error (p- values < 1e - 4 in a two-sample Kolmogorov Smirnov test, except for rule 7). Thus $r_\\gamma$ plays a vital role in grounding the explanations in the expert rules.\n5.2.3 User study. Figure 4 presents the rating distributions, seper- ated by user expertise, from our experiment described in Sec- tion 4.2.3. The distributions show that CueTip-generated explana- tions consistently achieve higher mean ratings across all exper- tise levels $l_u \\in \\{None, Low, High\\}$. For users with no prior pool experience, we observe a moderate difference in mean ratings ($\\mu_{CueTip}$ = 4.83, baseline = 4.32, $\\Delta \\mu$ = 0.51). This difference in- creases slightly for users with low experience ($\\mu_{CueTip}$ = 5.22, $\\mu_{baseline}$ = 4.61, $\\Delta \\mu$ = 0.61), and is most pronounced among highly experienced users ($\\mu_{CueTip}$ = 5.24, $\\mu_{baseline}$ = 4.13, $\\Delta \\mu$ = 1.11). The rating differential in the High Experience group suggests highly experienced players believe that CueTip's explanations bet- ter reflect true shot characteristics. We attribute this to CueTip's access to the expert rule evaluations $r$, which grounds explana- tions in accurate shot information rather than relying on a LM's interpretation of the expert rules $R_i$."}, {"title": "6 IMPLEMENTATION AND DISCUSSION", "content": "Winrates and Potting Rates. Analysis of the Section 4.2.1 exper- iments reveals an important distinction between win rates and potting rates (defined as the proportion of shots that successfully pot a target ball). The baseline $\\pi_g$ achieves the lowest potting rate at 29%, consistent with its poor performance, while $\\tilde{\\pi}_{pm}$ demon- strates strong performance with 82%. $II_{pm}$ achieves the highest potting rate at 93%, indicating highly effective shot recommenda- tion and optimization. Notably, $\\tilde{\\pi}_{pm}$ achieves the highest win rate despite a relatively low potting rate of 62%, suggesting a more nuanced strategy that balances offensive and defensive play. This demonstrates that optimal performance in pool requires strategic shot selection beyond simple potting optimization, a capability effectively captured by the neural surrogate approach.\nLM impact on reliability. We employ zero-shot chain-of-thought (CoT) reasoning with majority-voting self-consistency [Wang et al. 2023b; Wei et"}]}