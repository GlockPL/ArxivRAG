{"title": "Mixture of Experts based Multi-task Supervise\nLearning from Crowds", "authors": ["Tao Han", "Huaixuan Shi", "Xinyi Ding", "Xiao Ma", "Huamao Gu", "Yili Fang"], "abstract": "Existing truth inference methods in crowdsourcing aim to map redundant labels\nand items to the ground truth. They treat the ground truth as hidden variables\nand use statistical or deep learning-based worker behavior models to infer the\nground truth. However, worker behavior models that rely on ground truth hidden\nvariables overlook workers' behavior at the item feature level, leading to imprecise\ncharacterizations and negatively impacting the quality of truth inference. This\npaper proposes a new paradigm of multi-task supervised learning from crowds,\nwhich eliminates the need for modeling of items's ground truth in worker behavior\nmodels. Within this paradigm, we propose a worker behavior model at the item\nfeature level called Mixture of Experts based Multi-task Supervised Learning from\nCrowds (MMLC). Two truth inference strategies are proposed within MMLC. The\nfirst strategy, named MMLC-owf, utilizes clustering methods in the worker spectral\nspace to identify the projection vector of the oracle worker. Subsequently, the labels\ngenerated based on this vector are considered as the inferred truth. The second\nstrategy, called MMLC-df, employs the MMLC model to fill the crowdsourced\ndata, which can enhance the effectiveness of existing truth inference methods.\nExperimental results demonstrate that MMLC-owf outperforms state-of-the-art\nmethods and MMLC-df enhances the quality of existing truth inference methods.", "sections": [{"title": "Introduction", "content": "Truth inference in crowdsourcing aims to derive accurate results from noisy data provided by online\nworkers. Existing truth inference methods can be broadly classified into two categories: weakly\nsupervised and supervised approaches. In the weakly supervised approach, unknown ground truth is\ntreated as hidden variables. This involves utilizing statistics from workers' noisy answers to calculate\nresults directly. Alternatively, it entails creating worker behavior models and employing unsupervised\nlearning methods such as the EM algorithm to estimate unknown parameters and infer the ground\ntruth. The weakly supervised approach can further be classified into statistical learning and deep\nlearning methods based on whether it consider the features of the items. Statistical learning methods,\nsuch as Majority voting (MV) Imamura et al. (2018), Dawid&Skene model (DS) Dawid and Skene\n(1979), and homologous Dawid-Skene model (HDS) Karger et al. (2011); Li and Yu (2014), do\nnot incorporate item features. In contrast, deep learning methods like Training Deep Neural Nets\nGaunt et al. (2016) take item features into account. In the supervised approach, a classifier model\nis first constructed with item features as the input and ground truth as the output. Then, a worker\nbehavior model is created based on a confusion matrix that establishes the relationship between the\nitem's ground truth and the worker labels. On this basis, supervised learning is implemented using\nthe classifier model and the worker behavior model by treating the worker labels as supervisory\ninformation. Finally, the output of the classifier model is used as the inferred ground truth. In recent\nyears, various truth inference methods based on supervised learning have been proposed, such as\nCrowdlayerRodrigues and Pereira (2018), CoNALChu et al. (2021), and UnionNetWei et al. (2022).\nHowever, the worker behavior model based on the confusion matrix faces challenges in effectively\ncapturing variations in feature characteristics across different items. Neglecting these variations in\nworker behavior under different conditions can result in inaccurate representations of worker behavior,\nconsequently impacting the quality of truth inference. For example, in handwritten digit recognition,\nworkers generally have high accuracy. Suppose there are two items: one closely resembles the digit\n\"1,\" but its ground truth is actually \"7,\u201d and the other is a normal \"7.\u201d The former receives many labels\nas \"1,\" while the latter rarely gets labeled as \"1.\" Under the worker behavior model based on the\nconfusion matrix, it is difficult to accurately model the labeling behavior of such high-difficulty items.\nTherefore, there is a high probability that the model will interpret the former with \u201c1\u201d as the ground\ntruth, leading to incorrect judgments. The quality of truth inference is influenced by uncertainty\nfrom hidden variables, the method's data adaptability, and the accuracy in characterizing worker\nbehavior. The purpose of this paper is to develop a supervised model that can achieve high-quality\ntruth inference with a worker behavior model at the item feature level.\nIn this paper, we propose Multi-task Supervised Learning from Crowds (MLC), a novel paradigm\nfor crowdsourcing learning. Unlike traditional paradigm, MLC does not rely on the ground truth\nof the items but instead focuses on understanding the unique behavior of individual workers across\ndifferent items. When multiple workers handle the same item, they share the item's features, leading\nto a multi-task learning paradigm. Within this paradigm, we propose a method called Mixture of\nExperts-based Multi-task Supervised Learning from Crowds (MMLC). MMLC does not utilize a\nsingle classifier but instead creates multiple expert modules. The outputs from these expert modules\nserve as the bases of the worker spectral space. Each worker is represented by his or her projection\nvector in the spectral space that characterizes their behavior. The worker behavior model provides a\nmore precise depiction of their behavior across different items by accurately modeling the workers'\nbehavior on item features. However, it is important to note that the model itself cannot determine\nthe ground truth. To address this limitation, we introduce two truth inference strategies based on\nMMLC. The first strategy involves analyzing the distribution of workers' projections in the worker\nspectral space. We identify the projection of the oracle worker by applying clustering methods,\nand consider its labels as the ground truth. This approach is referred to as Oracle Worker Finding\nof MMLC (MMLC-owf). The second strategy leverages the sparsity of crowdsourced data to fill\nthe original dataset with MMLC outputs, generating a new crowdsourcing dataset. Existing truth\ninference methods can then be applied within this framework, which is called Data Filling of MMLC\n(MMLC-df). The main contributions are as follows:\n\u2022 We introduce a novel paradigm of multi-task supervised learning from crowds and propose a new\nworker behavior model called MMLC. This model is well-suited for crowdsourcing learning\nand offers a more accurate way to characterize worker behavior in item labeling.\n\u2022 We leverage MMLC to identify the oracle worker for labeling items as the ground truth, referred\nto as MMLC-owf. Experimental results demonstrate that the labels obtained using this method\nexhibit higher quality compared to state-of-the-art methodes.\n\u2022 We introduce a truth inference framework called MMLC-df, which leverages the MMLC model\nto fill sparse crowdsourced data. This framework then applies truth inference methods to\ndetermine the ground truth. Experimental results demonstrate that MMLC-df significantly\nenhances truth inference methods, leading to higher-quality results."}, {"title": "2 Related Work", "content": "Weakly supervised approach: This class of methods focuses on modeling the relationship between\nworkers' noisy answers and the ground truth. In this approach, the ground truth is treated as a hidden\nvariable, and weakly supervised learning techniques are employed to infer the ground truth. The most\ndirect and widely used method is Majority Voting (MV) Sheng et al. (2017). It involves counting\nthe responses from workers and considering the answer with the majority of votes as the ground\ntruth. However, MV overlooks the variations in worker behavior during the labeling process and\ntreats all workers' labels equally. To address this limitation, Tao et al. Tao et al. (2020) proposed a\nmethod that considers the deterministic information of majority and minority categories separately.\nThey establish a voting model that takes into account the labeling quality of workers in different\nsituations. The Dawid-Skene (DS) Dawid and Skene (1979) method utilizes a confusion matrix\nto describe each worker's behavior when handling an item. The EM algorithm is then used to\nestimate the worker's confusion matrix parameters and the ground truth. HDS Raykar et al. (2010)\nassumes equal probabilities of wrong worker selections while evolving the confusion matrix. GLAD"}, {"title": "3 Problem Formulation", "content": "This section provides a formal problem description. Our main goal is to create a worker behavior\nmodel and achieve joint learning of worker abilities by utilizing multi-task learning to infer the\nground truth. Let W = {wj} denote the worker set, where wj represents an individual worker, and\nX = {xi} denote the set of items, where xi represents a single item to be labeled. The labels for each\nitem belong to the category set K = {k}. We use Yij to denote the category label assigned by worker\nwj to item xi. We have an indicator function Iij, where Iij = 1 if Yij exists and Iij = 0 otherwise.\nConsequently, we obtain the crowdsourced triples dataset D = {< Xi, Wj, Yij > |Iij = 1}. With\nregards to truth inference in crowdsourcing, we provide the following definition:\nDefinition 1 (Problem of Truth Inference (TI Problem)) By modeling and learning from the\ncrowdsourced label dataset D, the problem of truth inference aims to find a function $g^* : X \\rightarrow K$\nsuch that\n$g^* = arg min \\sum_{i=1}^{\\left|X\\right|} L (z_i, g(x_i)) + \\lambda \\left\\|g\\right\\|_H$.   (1)\nDefinition 2 (Problem of Multi-task supervise Learning from Crowds (MLC Problem)) Let\n$S_j = {(\\langle x_i, w_j \\rangle,Y_{ij})}_{x_i\\in x}$ denote the crowdsourced training dataset for worker $w_j$, where\n$X_j = {x_i|I_{ij} = 1}$. The labels provided by worker j can be regarded as the j-th task for the\ncorresponding item. Consequently, we obtain the dataset as $S = \\bigcup_j S_j$. The problem of multi-task\nsupervised learning from crowds is to find a worker behavior function $f^* : X \\times W \\rightarrow K$ such that\n$f^*  = arg min_f \\sum_{w_j \\in W}  \\sum_{x_i \\in X_j} L (Y_{ij}, f(\\langle x_i, w_j \\rangle)) + \\lambda \\left\\|f\\right\\|_H.$ (2)\nWe can observe that the solution to MLC problem does not directly address TI problem. Therefore,\nwe provide two approaches to tackle this issue. The first approach is to identify an oracle worker\n$W_{oracle}$ based on the distribution of workers. We then consider the labels provided by this oracle\nworker as the ground truth, that is,\n$g^*(x_i) = f^*(\\langle X_i, W_{oracle} \\rangle)$.  (3)\nThe second approach considers the sparsity characteristic of crowdsourced data, where workers do\nnot annotate every item. Consequently, we can utilize the results of MLC to generate a new dataset\nfor inference. The new crowdsourced data can be defined as follows:\n$D' = D \\bigcup {\\langle x_i, w_j, Y_{ij} \\rangle | Y_{ij} = f^*(\\langle X_i, w_j \\rangle), I_{ij} = 0}$.  (4)"}, {"title": "4 Proposed Methodology", "content": "To address the MLC problem, we propose a Mixture of Experts based Multi-task Supervised Learning\nfrom Crowds (MMLC) model. This model utilizes mixture of experts to characterize the varying\nattention of workers towards different item features,, aiming to capture the feature-level behavior\ndifferences of workers when dealing with various items. The framework of the model is shown in\nFig. 1. It consists of three main modules: expert module, gate module, and output module.\nIn the expert module, each item is processed by a feature extractor to obtain an item feature vector xi.\nThen, the item feature vector is fed into m expert modules, where each module captures the unique\ncharacteristics of worker behavior associated with different feature information. Each expert module\nperforms transformations and compressions on the feature vector, resulting in the output matrix of the\nexpert module: $U(x_i) = (u_1(x_i), u_2(x_i), ..., u_E(x_i))$. Each expert sub-module follows the same\nstructure, consisting of multiple layers of fully connected neural networks with ReLU activation\nfunctions in each layer. For each expert sub-module ue, the high-dimensional feature vector xi is\ntransformed into a low-dimensional vector $u_e(x_i)$.\nIn the gate module, a gate network is constructed to control the selection of expert modules. This\ngate network takes worker data as input and generates a projection vector of the worker in the\nworker spectral space, with a length of E. The bases of the worker spectral space are the outputs\nof the expert sub-modules. Specifically, the module takes the one-hot encoded vector representing\neach worker wj as input. After passing through a fully connected ReLU layer, the data proceeds\nthrough an attention layer and a softmax layer. Finally, it produces a worker projection vector\n$v(w_j) = (v_1(w_j), v_2(W_j), ..., v_E(w_j))^T$ with a length of E. The projection of worker wj in the\nworker spectral space with the expert sub-modules as the bases is:\n$proj_{U(x_i)}(w_j) = \\sum_{e=1}^E v_e(w_j)u_e(x_i)$. (5)\nIn the output module, the worker's labels for the item are generated. The output module generates\nlabels for each worker based on their chosen expert modules. It involves mapping worker behavior\nthrough the gate network, which includes weighting and summing the outputs of the expert modules.\nSubsequently, through a fully connected ReLU layer and a softmax layer, the model produces the\nlabel output of worker wj for item xi as follows:\n$f_\\theta(< x_i, w_j >) = \\Omicron (proj_{U(x_i)}(w_j))$, (6)\nwhere $\\omicron ()$ denotes the output function, and $\\Omicron$ is the parameter set of the functions U, v, and $\\omicron$ within\nthe MMLC model. The MMLC model deals with a classification problem with |K| categories. The\nnetwork's output is a |K|-dimensional vector, where each element represents the predicted probability\nof a category.\nThe model's loss function combines a cross-entropy loss term with a regularization term. The loss\nfunction is formulated as follows:\n$L_\\Theta = \\frac{1}{|W|} \\sum_{w_j \\in W} \\sum_{x_i \\in X_j} \\sum_{k \\in K} \\ell (Y_{ij}, f_\\theta(< x_i, w_j >)) + \\lambda \\left\\|\\Theta\\right\\|_F,$ (7)\nThe first term denotes the multi-class cross-entropy loss, while the second term represents the\nregularization of the model's parameter set to prevent overfitting. In the equation, A is the\nregularization coefficient, and $\\left\\| \\cdot \\right\\|_F$ denotes the Frobenius norm. By minimizing the loss function,\nwe can obtain the final model $M^* : f_{\\theta^*}( \\cdot )$. This model uses the function $f_{\\theta^*}$ to predict the labels of\nworker wj for item xi, where $\\Theta^*$ represents the optimized parameters.\nTruth Inference by Oracle Worker Finding (MMLC-owf): The MMLC model does not directly\ngenerate the ground truth for inference. To address this issue, this section proposes a method for\ninferring the ground truth by identifying the oracle worker's projection vector in the worker spectral\nspace. Specifically, each worker is theoretically associated with a projection in the worker spectral\nspace, representing their unique characteristics. Workers are distributed in the spectral space. We\nassume the existence of an omniscient oracle worker who possesses a projection vector in the spectral\nspace and is capable of providing the ground truth in the MMLC model. Therefore, by identifying\nthe projection vector of the oracle worker in the worker spectral space, we can consider its output as\nthe inferred truth. If we treat any worker as a random expression of the oracle worker's error, then the\ncenter of the worker's distribution projected onto the spectral space can be regarded as the projection\nvector of the oracle worker, that is,\n$V_{oracle} = \\tau (v(W))$, (8)\nwhere the function $\\tau( \\cdot )$ is used to determine the distribution center, which can be found using methods\nsuch as kernel density estimation, mean, median, etc. According to the MMLC model, the outcome\nof the Oracle Worker Finding method (MMLC-owf) for inferring the ground truth of item xi can be\nexpressed as follows:\n$f_{\\theta^*} (< x_i, W_{oracle} >) = \\Omicron (U(x_i)V_{oracle})$.  (9)\nTruth Inference Framework by Data Filling (MMLC-df): In addition to the MMLC-owf method,\nwe propose a truth inference framework using data filling under the MMLC model called MMLC-df,\nwhich utilizes the sparsity of crowdsourced data. A new crowdsourced dataset D' is constructed\nthrough data filling as follows:\n$D' = D \\bigcup {< x_i, w_j, Y_{ij} > | Y_{ij} = f_{\\theta^*} (< x_i, w_j >), I_{ij} = 0}$. (10)\nSubsequently, any truth inference method applied to this new crowdsourced dataset can infer higher-\nquality ground truth compared to that obtained from the original data."}, {"title": "5 Experiments", "content": "In this section, we verify the effectiveness of our method through experiments. Our MMLC-owf\nmethod is a truth inference method, and we compare it with the following baselines: MVSheng\net al. (2008) directly uses majority voting to determine the ground truth; DSDawid and Skene\n(1979) employs a confusion matrix to characterize the labeling behavior of workers and uses the EM\nalgorithm to infer the ground truth; HDSKarger et al. (2011) simplifies the DS method by assuming\nthat each worker has the same probability of being correct under different truth values and equal\nprobabilities for incorrect options; FDSSinha et al. (2018) is a simple and efficient algorithm based\non DS, designed to achieve faster convergence while maintaining the accuracy of truth inference;\nMax-MIGCao et al. (2019) utilizes the EM algorithm to integrate label aggregation and classifier\ntraining; CONALChu et al. (2021) distinguishes between common noise and individual noise by\npredicting a joint worker confusion matrix using classifiers; CrowdARCao et al. (2023) estimates\nworker capability features through classifier prediction and models the reliability of joint worker\nlabels.\nOur truth inference framework, MMLC-df, incorporates a core component that performs data filling.\nWe compare it with the following baselines: G_MV Sheng (2011) utilizes truth inference results\nfrom the MV algorithm to evaluate worker ability and assign new labels accordingly; G_IRT Baker\net al. (2017) utilizes joint maximum likelihood estimation to estimate parameters of the IRT model,\nsuch as worker abilities and item difficulties, and generates new labels based on these parameters;\nTDG4Crowd Fang et al. (2023) learns the feature distributions of workers and items separately using\nworker models and item models. An inference component is used to learn the label distribution and\ngenerate new labels.\nWe conduct experiments using three crowdsourced datasets with item features:\n\u2022 LableMe Rodrigues and Pereira (2018); Russell et al. (2008): This dataset consists of 1000\nimages categorized into 8 classes, with a total of 2547 labels provided by 59 workers. Each\nimage is represented by 8192-dimensional features extracted using a pre-trained VGG-16 model.\n\u2022 Text Dumitrache et al. (2018): This dataset comprises 1594 sentences extracted from the\nCrowdTruth corpus, categorized into 13 groups. The dataset includes 14,228 labels provided\nby 154 workers. Each sentence is represented by 768-dimensional features extracted using a\npre-trained BERT model.\n\u2022 Music Rodrigues et al. (2014): This dataset consists of 700 music compositions, each with a\nduration of 30 seconds, and categorized into 10 groups. It includes 2,945 labels provided by 44\nworkers. Each music composition is represented by 124-dimensional features extracted using\nthe Marsyas Rodrigues et al. (2013) music retrieval tool.\nTo accommodate the feature scales of the three experimental datasets, our model's architecture varies\naccordingly. For the LableMe dataset, our model employs 16 expert modules, each comprising 3 fully\nconnected ReLU layers, with a final layer output dimension of 32. For the Text and Music datasets,\nwe utilize 10 expert modules. Each module consists of 3 and 2 fully connected ReLU layers, with\noutput dimensions of 32 and 16, respectively. We adopt the settings from the Max-MIG, CoNAL, and\nCrowdAR truth inference methods, we adopt the settings from their source code for the LableMe and\nMusic datasets. Since there is no source code available for the Text dataset, we adopt the settings used\nin the LableMe dataset. Regarding the TDG4Crowd data filling algorithm, we utilize the settings\nfrom its source code. The remaining methods do not use deep network structures and rely on default\nsettings."}, {"title": "5.1 Evaluation of Oracle Worker Finding (MMLC-owf)", "content": "Main Result: Our method, MMLC-owf, was evaluated alongside seven other methods through\nfive rounds experiments. The average accuracy results are shown in Tab. 1. In our method, we\nutilized kernel density estimation (KDE) to compute the projection vector of the oracle worker in\nthe worker spectral space. Our method, MMLC-owf, achieved the highest accuracy in the Text and\nMusic datasets. In the LableMe dataset, it ranked second, with only a 0.4% difference from the\ntop-performing CrowdAR method. Deep learning-based methods typically produce better results\nwhen analyzing datasets with high-dimensional item features like LableMe. In datasets with fewer\nfeatures, the advantage of deep learning methods was not significant.\nImpact of Redundancy: We examine how varying levels of redundancy affect the accuracy of our\nmethod. Due to the varying redundancy of data items, a maximum redundancy parameter R is set.\nWe randomly keep R labels for items with more than R labels and discard the rest. This process\ngenerates a dataset with a maximum redundancy R. By conducting five repeated experiments and\naveraging the accuracy and standard deviation, the results are shown in Fig. 3. As the average number\nof worker responses increases, all methods show an upward trend in their results. The analysis\nof various redundancy levels across the datasets indicates that higher redundancy levels are more\nadvantageous for our method. Our method can effectively utilize worker behavior descriptions on\ndatasets with higher redundancy but may face underfitting on datasets with lower redundancy.\nClustering Methods in Oracle Worker Finding: Our method, MMLC-owf, utilizes a clustering\nmethod to determine the center of the distribution of the projection vector of workers in the worker\nspectral space as the projection vector of the oracle worker. Here, we examine how different\nclustering methods affect truth inference outcomes. We compare three clustering methods: kernel\ndensity estimation (KDE), Mean, and Median, as well as their worker-weighted variants: KDE-W,\nMean-W, and Median-W. Worker weights are calculated based on the proportion of items answered\nby each worker relative to the total number of items, considering data imbalance. In addition, we\noptimize the projection vector in the worker spectral space using the ground truth of the items as\nan upper-bound method for clustering. The parameters of the expert modules and output modules\nare fixed in the pre-trained MMLC model, referred to as \u201cTruth.", "Truth\"\nmethod, which represents the theoretical upper limit with clustering methods, achieved accuracy rates\nof 96.32%, 91.25%, and 92.97% on the three datasets respectively. The quality of the ground truth\ngenerated by oracle workers using six clustering methods still slightly deviates from theoretical upper\nlimits. This implies that the MMLC-owf method can provide high-quality ground truth by optimally\nprojecting the worker spectral space, approaching the theoretical upper limit. The model has strong\nexpressive ability, with a small gap between the projected spectral space and the ground truth. There\nis potential to enhance MMLC-owf by choosing a more effective clustering method.\nWorker Distribution in Worker Spectral Space": "We assume that each worker is an oracle worker\nwith random errors in their expression. The center of the distribution of workers projected onto the\nspectral space corresponds to the projection vector of the oracle worker. To validate this assumption,\nwe employed the IOSMAP dimensionality reduction method to reduce the worker projection vectors\nobtained from the MMLC model to 2D, resulting in the scatter plot shown in Fig. 4. We calculated\nthe accuracy of each worker on the dataset, where the closer the point's color on the graph is to green,\nthe worker's accuracy is higher. The closer the point's color is to red, the lower the worker's accuracy.\nThe plot also shows the projection obtained by the KDE method for the oracle worker, represented by\nblue asterisks. From the distribution of worker projections, although the shapes of the distributions\ndiffer across datasets, there is a noticeable trend where workers with higher accuracy tend to cluster\ncloser to the projection of the oracle worker. This observation demonstrates a clear tendency towards\naggregation and provides some degree of confirmation for the validity of our assumption."}, {"title": "5.2 Evaluation of Data Filling (MMLC-df)", "content": "Main Result: We compared MMLC-df with three filling methods: G_MV, G_IRT, and TDG4Crowd.\nWe used the filled data with eight truth inference methods from the previous section to infer the"}, {"title": "6 Conclusion", "content": "This paper introduces a novel crowdsourced learning paradigm called MLC. Within this paradigm,\nwe propose a feature-level worker behavior model called MMLC. Based on this model, we develop\ntwo truth inference methods: MMLC-owf, which uses oracle worker finding, and MMLC-df, a\ntruth inference framework based on crowdsourced data filling. Experimental results demonstrate\nthe superior performance of MMLC-owf compared to other methods. Furthermore, we assess the\ntheoretical upper performance limit of the MMLC-owf method, demonstrating its potential to enhance\nclustering method selection and validate its strong performance. The experiments also validate the\neffectiveness and stability of the MMLC-df framework in enhancing truth inference methods through\ncrowdsourced data filling. Furthermore, we observed that our model exhibited better performance on\ndatasets with a higher number of annotations per worker. On real crowdsourcing platforms, workers\ncontinuously engage in annotation tasks, resulting in an increasing average number of annotations\nper worker. Consequently, our model holds significant practical value for real-world applications."}]}