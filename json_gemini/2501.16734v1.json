{"title": "Distilling Large Language Models for Network Active Queue Management", "authors": ["Deol Satish", "Shiva Raj Pokhrel", "Jonathan Kua", "Anwar Walid"], "abstract": "The growing complexity of network traffic and demand for ultra-low latency communication require smarter packet traffic management. Existing Deep Learning-based queueing approaches struggle with dynamic network scenarios and demand high engineering effort. We propose AQM-LLM, distilling Large Language Models (LLMs) with few-shot learning, contextual understanding, and pattern recognition to improve Active Queue Management (AQM) [RFC 9330] with minimal manual effort. We consider a specific case where AQM is Low Latency, Low Loss, and Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative decoding and reinforcement-based distilling of LLM by tackling congestion prevention in the L4S architecture using Explicit Congestion Notification (ECN) [RFC 9331] and periodic packet dropping. We develop a new open-source experimental platform by executing L4S-AQM on FreeBSD-14, providing interoperable modules to support LLM integration and facilitate IETF recognition through wider testing. Our extensive evaluations show L4S-LLM enhances queue management, prevents congestion, reduces latency, and boosts network performance, showcasing LLMs' adaptability and efficiency in uplifting AQM systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Since their inception in the 1990s, Active Queue Management (AQM) algorithms have relied on mathematical models to develop optimal rule-based control systems. Early algorithms, such as Random Early Detection (RED) [1], used queue length as the primary metric to track and manage network buffers. Modern AQMs, like Controlled Delay (CoDel) [2], Proportional Integral Controller Enhanced (PIE) [3], and Common Applications Kept Enhanced (CAKE), have shifted to queue delay as the key metric, providing more direct control over user-perceived latency and improving network performance. Despite these advancements, rule-based AQMs face significant challenges in today's dynamic and heterogeneous networking environments, which include advances in WiFi, 5G/6G, optical fibers, and satellites. Emerging network environments experience rapid fluctuations in link quality, traffic patterns, and interference levels, often outpacing the adaptive capabilities of formulaic approaches. Addressing such dynamics and heterogeneous network challenges using rule-based methods requires the development of complex, computationally intensive rules that demand periodic (re)engineering and continuous parameter tuning based on specific network dynamics\u00b2 [6].\nDeep learning offers a promising alternative, shifting the focus toward learning-based AQMs that minimize reliance on predefined mathematical models [7]. Supervised learning (SL) methods are frequently employed for prediction tasks, such as traffic classification and bandwidth estimation, while reinforcement learning (RL) is utilized for decision-making problems, including congestion control, adaptive bitrate streaming, and cloud cluster job scheduling [8]. In AQM, deep neural networks (DNNs) and RL have been applied to packet traffic prediction and optimization tasks [5]. Compared to rule-based systems, learning-based methods (e.g., DNN, RL, etc.) excel at adapting dynamically to evolving network conditions and leveraging data-driven insights to handle complex scenarios effectively [5]. However, challenges persist, including scalability, robustness, and computational efficiency, which must be addressed to fully realize the potential of ML-driven AQMS, which are summarized as follows.\n1. Designing and tuning ML-based AQM models demands significant effort, including architecture design, hyperparameter optimization, and algorithm selection, requiring extensive experimentation and computational resources, which can hinder deployment in dynamic networks.\n2. ML-based AQM often struggles to generalize across varying network conditions, with models trained on static traffic performing poorly under real-world fluctuations, limiting their reliability compared to traditional rule-based systems.\n3. Continuous retraining of ML-based AQM is essential to adapt to evolving networks, requiring diverse, high-quality datasets and extensive preprocessing, which increases operational costs and complicates commercial deployment."}, {"title": "A. LLMs over AQM: Challenges & Distillation", "content": "Leveraging Large Language Models (LLMs) like GPT, Opt, and Llama [9], [10] into AQM can address the high costs of developing task-specific DNNs for solving networking challenges [11]. Pretrained on vast text datasets, LLMs excel in contextual understanding, reasoning, and generalization [10] making them ideal for tasks like AQM. Their attention mechanism enables dynamic analysis of traffic parameters and dependencies, allowing proactive congestion management and adaptability to evolving network conditions. The distillation of LLM for AQM offers a universal framework to improve the Internet with minimal modifications, which can perhaps be achieved by adopting new RL ideas for the distillation of knowledge [10], [12]. However, key challenges include:\ni) Diverse Inputs: AQM data includes time series (e.g., queue delay) and scalar values (e.g., burst allowance), requiring specialized encoders.\nii) Inference Latency: Token-by-token output generation causes delays, disrupting AQM's rapid update cycles (45 ms). Hallucinated outputs further exacerbate the delays.\niii) Distillation Costs: Distilling LLMs for AQM requires time-consuming fine-tuning, especially in decision-making tasks using reinforcement learning, and demands significant computational resources for large models.\nWhile adopting LLMs can unify and enhance AQM, the above mentioned challenges must be addressed for effective AQM-LLM design and deployment."}, {"title": "B. Our Design and Contributions", "content": "We propose AQM-LLM and develop it over L4S. L4S-LLM is an adapted LLM framework developed for efficient congestion prevention with L4S. L4S-LLM leverages a foundational LLM model for decision-making in the L4S queues with minimal manual modifications, by incorporating three key modules:\nState Encoder: A multifaceted encoder processes diverse network data (e.g., queue delay, packet drops) into token-like embeddings, enabling the LLM to understand and utilize network metrics effectively.\nL4S-LM Head: Replacing the default language model head, the L4S-LM head maps LLM outputs directly to congestion prevention actions (enqueue, drop, or mark) in a single inference, reducing response time and preventing hallucinations.\nData-Driven Low-Rank L4S Adaptation (proposed LoRA): This module finetunes the LLM using RL with distillation over the pre-collected data from existing algorithms, eliminating lengthy real-time interactions. By employing Low-Rank Adaptation (LoRA), finetuning costs are drastically reduced, with a 64% decrease in GPU memory usage and a 15.1% reduction in training time.\nL4S-LLM addresses low-latency needs by proactively identifying traffic patterns and taking optimal congestion-prevention actions. Its strong generalization capability ensures high performance on unseen network scenarios.\nOur key contributions in this paper are as follows.\n1) Identification of key challenges and advantages of using LLMs for network queue management.\n2) Development of L4S-LLM with a multi-faceted encoder, specialized L4S-LM head, and the proposed LORA scheme for improved RL-based distillation and finetuning of the L4S-LLM.\n3) Evaluation demonstrating improved queue management, congestion mitigation, and latency reduction across unseen data and network environments."}, {"title": "L4S-LM Head Design & Development", "content": "Conventional LM heads rely on iterative multiround inference to generate valid responses, resulting in substantial latency. To overcome this limitation, we propose the novel L4S-LM head, which generates a valid response in a single inference round by predicting a probability distribution over possible actions. This design streamlines the response generation process, significantly accelerating inference. Drawing inspiration from speculative decoding, the L4S-LM head advances beyond sequential token-by-token generation by simultaneously evaluating multiple potential outcomes, enhancing both computational efficiency and scalability (without degrading any accuracy). Furthermore, L4S-LM offers a robust framework to improve AQM performance while reducing manual effort and resource consumption."}, {"title": "C. Literature", "content": "Networks often suffer from 'bufferbloat' [13], [14], where excessive packet buffering leads to high latency and jitter. To mitigate this, AQM schemes were developed, which detect congestion and either drop packets or use ECN to signal TCP congestion control algorithms to adjust the congestion window. Rule-based AQMs like CoDel [15], PIE [3], and CAKE depend on manually created rules, but these static approaches struggle to adapt to dynamic network traffic patterns.\nLearning-based AQM algorithms address these limitations by leveraging deep learning techniques to automate decision-making. Early works, such as PFED [16], forecast traffic using MMSE prediction to regulate queue length and penalize misbehaving flows. Advanced approaches include ECN-based algorithms using LSTM architectures for traffic prediction [8], QP-AQM for adaptive queue management with Q-learning traffic predictors [17], and Deep Q-Network-based AQMs for intelligent packet dropping and differential QoS [18].\nWhile these methods improve adaptability, they face challenges like high engineering effort, computational cost, and resource-intensive training. Their reliance on continuous learning limits generalization across diverse network environments, hindering practical deployment in real-world settings. LLMs [19] like ChatGPT, PaLM, Llama2, and OPT are advanced deep neural networks built on the Transformer architecture, with widespread applications across diverse domains. LLMs process input and output as token sequences, converting text into embeddings and predicting subsequent tokens using an auto-regressive mechanism. The self-attention mechanism allows LLMs to focus on relevant parts of the input, enabling them to handle complex language patterns and nuances effectively."}, {"title": "II. RESEARCH DESIGN", "content": "The L4S architecture, defined in RFC 9330 [4], enhances QoE and QoS by integrating ECN with advanced TCP and AQM techniques. Central to this is the Dual-Queue Coupled AQM4 [RFC 9332], which prioritizes latency-sensitive L4S flows over traditional Classic queues, reducing delay at the cost of minor performance trade-offs for Classic flows. Complementing this, the Prague TCP variant works synergistically with DualPI2 and ECN to minimize packet loss and latency while maintaining high throughput.\nDualPI2 uses ECN (RFC 3168 [20]) to mark packets during congestion, resorting to packet drops only under severe congestion. ECN leverages two bits in the IP header to signal congestion using four codepoints, with CE (1) indicating congestion. Upon receiving CE-marked packets, the receiver informs the sender via an ECE flag, prompting the sender to adjust congestion control and respond with a CWR flag to acknowledge the notification. This mechanism mitigates delays, reduces packet loss, and minimizes retransmission, leading to lower latency, improved throughput, and decreased head-of-line blocking [21].\nIn this paper, we develop an approach that uses the extraordinary capabilities of LLM to prevent congestion in the L4S DualPI2 by deciding the appropriate action to be taken in that specific scenario. Mainly, its job is to decide between packet dropping, packet enqueueing, and most importantly, ECN notification. LLMs are trained on billions of parameters that absorb extensive knowledge, allowing them to show extraordinary abilities such as planning, pattern mining, problem-solving, and generalization to unseen conditions.\nOur objective is to utilize the LLMs' prediction capabilities to enhance the efficacy and foresight in identifying network congestion within the AQM framework. If there is congestion ahead, the LLM will inform TCP using the ECN notification or will directly drop the packets if the delay is excessive."}, {"title": "A. Architecture of AQM-LLM", "content": "In this section, we detail the design of AQM-LLM, a framework that leverages Large Language Models to optimize AQM tasks, including packet enqueueing, dropping, and ECN marking. AQM-LLM comprises three core components: (i) the L4S-LLM State Encoder, (ii) the L4S-LLM Head, and (iii) Data-driven Low-Rank L4S Adaptation. Below, we provide an overview of these components, focusing on the L4S-LLM State Encoder.\nThe L4S-LLM State Encoder processes task-specific information such as queue size, delay, and other AQM metrics. It converts raw input data from various modalities into a format compatible with LLMs by projecting these inputs into a token space tailored to the specific model. This ensures seamless integration with LLMs, where the token space size adapts to the underlying model.\nThe encoder consists of two main components: (a) Feature Encoder: Extracts meaningful features from raw AQM metrics such as queue delay and size; (b) Linear Projector: Maps these features into token-like embeddings aligned with the LLM's input requirements. Together, these components enable efficient processing of diverse AQM inputs, ensuring compatibility and effectiveness in LLM-driven queue management.\nFeature Encoder: To extract features from raw input data, we employ specialized feature encoders that are tailored to each type of data. A key design choice is leveraging pre-existing, optimized encoders rather than manually building them from scratch. Linear Projection: Although the feature encoders are highly capable, the dimensions of the extracted features may not match the token space required by the LLM. To resolve this mismatch, we utilize trainable linear or embedding layers to project the extracted features into the appropriate token space. These embedding layers learn to map features to token-like embeddings compatible with the LLM, enabling seamless integration. Additionally, we apply layer normalization [22] to the output embeddings, ensuring improved training stability and performance. This approach ensures that the data is optimally formatted for the LLM while maintaining robust and efficient processing."}, {"title": "B. L4S Language Modelling Head", "content": "Using the above state encoder, the LLM is able to transform the raw input data of different modalities into embeddings or high-level features. These features are then directed to the L4S-LM head to directly generate answers. We designed a custom L4S Language Modelling head, implemented as a trainable linear layer to generate task-specific answers based on the processed features from the LLM. Unlike the LM head, our custom L4S-LM head constrains answers to fall within a valid range of possible answers, ensuring the reliability of the LLM in our task. For example, in our case, we define three discrete actions: 0 refers to enqueuing the packet, 1 refers to dropping a packet from the queue, and 2 indicates applying the CE(1) flag to signal congestion for L4S enqueue management. Traditionally, the LM head requires multiple rounds of inference to generate a single valid response, which leads to significant delays in answer generation. To address this challenge, we developed the L4S-LM head, to generate a valid response in a single inference round by predicting a probability distribution over possible actions. This new approach is inspired by speculative decoding, where instead of generating responses token by token, the model predicts multiple possible outcomes simultaneously,"}, {"title": "C. Low-Rank Adaptation", "content": "In this section, we propose a data-driven low-rank L4S adaptation to efficiently fine-tune our LLM to reduce computation overhead and answer generation time for our AQM-specific task. This adaptation allows us to use an offline approach to fine-tune our model, instead of having to interact with the network environment and increase overhead directly. It comprises two main components (A) a data-driven pipeline for AQM decision-making and (B) a low-rank data adaptation approach (LoRA) which reduces the number of parameters that need to be fine-tuned from 7B parameters to just 1% of the parameters, for faster fine-tuning. LoRA has two advantages (A) it decreases the time for fine-tuning significantly, and (B) It helps us retain the foundational knowledge of LLM.\n1) RL-based L4S Adaptation: In the case of a task similar to the typical Markov Decision Process, instead of using the typical traditional RL approach, we adopt a data-driven RL technique. The standard RL technique is impractical due to the significant interaction time required between the LLM and the environment. To overcome this, we propose a modified RL adaptation pipeline based on the efficient data-driven RL technique [23]\u2013[25]. This method eliminates the need for any real-time interaction by utilizing pre-collected experience datasets from existing networking algorithms. This approach enables fine-tuning the model to optimize reward. The intuitive rationale behind this approach is to allow the LLM to improve its policies by observing the behaviours of existing networking algorithms [24]. It learns why certain actions lead to better performance, as well as understanding why certain actions result in poor performance [25]. In this method, we only collect all our data at once to use throughout our training process, significantly decreasing the training time for our LLM.\nThe proposed RL adaptation pipeline functions by first collecting experience trajectories from an existing policy (DualPI2 from FreeBSD AQM) into a dataset $D_{rl}$ = {$T_1$,...,$T_{|D|}$}, where each trajectory T = {$r_t$, $s_t$, $a_t$}$_{t=1}^{T}$ is composed of rewards r, states s and actions a, where T denotes the episode length. The actions are defined as, 0 - Enqueue, 1 - Drop, and 2 - MARKECN. The state consists of 8 features: {queue_type, burst_allowance, drop_probability, current_queue_delay, accumulated_probability, total_bytes total_drops, packet_length}.\nState: $s_t = (p_t, qd_t, pdrop_t, lenb_t, totb_t)$,\nAction: $a_t = p_{kt}^{L4S}$,\nReward: $R_t = p_{kt} ien / (qdelay_t + 1)$,\nIn the trajectory, the reward rt is substituted by the return $R_t = \\sum_{t=1}^{T} i r_i$, representing the cumulative rewards generated from the state $s_t$ moving forward. Furthermore, in certain tasks, the state or action may comprise multiple components of information. For example, in the context of L4S, the state includes historical queue delays. This complexity requires a finer level of discretization of both state and action spaces to ensure that all relevant aspects of the system are adequately captured.: $s_t = \\{s_t^1, ..., s_t^7\\}$, $a_t = \\{a_t^1, ..., a_t^m\\}$. The updated representation of the trajectory is then expressed as:\nT = {$R_t, s_t^1, ..., s_t^7, a_t^1, ..., a_t^m$}$_{t=1}^{T}$\nFollowing the trajectory representation presented earlier, we fine-tune the LLM to learn the distribution of cumulated returns. For each training step, we randomly sample batches of data from the dataset:\n$d = \\{R_t, s_t^1, ..., s_t^7, a_t^1, ..., a_t^m\\}_{t=t-w+1}$"}, {"title": "D. Computation Overhead(s)", "content": "Figure 15 illustrates the number of parameters fine-tuned when using different LLM models. We have utilized LoRA to reduce the number of fine-tuned parameters, as both models originally contain over a billion parameters, significantly impacting training time. Llama2 originally comprised 7 billion parameters, but using LoRA with a rank of 128, the number of trainable parameters is reduced to approximately 70 million, while maintaining performance nearly identical to full fine-tuning. Similarly, OPT, which initially contains 1.3 billion trainable parameters, is reduced to 25 million parameters using LoRA of the same rank. In contrast, GPT-2 and T5-LLM, with 125 million and 220 million parameters, respectively, were used without parameter reduction. The rank value of 128 was chosen because it effectively balances parameter efficiency and model performance, enabling comparable results to full fine-tuning while significantly reducing computational and memory requirements."}, {"title": "E. Discussions and Limitations", "content": "Several critical limitations must be addressed before considering the commercial deployment of the proposed L4S-LLM framework in network routers. This section outlines the primary challenges affecting deployability and overall effectiveness:\n\u2022 Online Training/Testing: The framework requires real-time evaluation within the L4S architecture to address deployment challenges. While the current L4S-LLM framework was trained and tested using a data-driven approach with experimental data from L4S DualPI2 AQM and various TCP congestion control algorithms, this process involved offline training and testing. Real-time deployment within a virtual router demands further optimization of response generation times and consideration of the resource constraints of the target routers.\n\u2022 Time Constraints: Deployment requires a significant reduction in L4S-LLM's response generation time to ensure effective performance in real-world applications.\n\u2022 Hardware: Approaches for fine-tuning and distilling LLMs rely on high-end hardware, including an RTX 4090 GPU, 128 GB of RAM, and a 13th-generation i9 processor. Commercial network routers typically lack such resources. Therefore, model parameter size must be reduced to address memory constraints, alongside CPU and GPU optimizations. A potential solution involves transitioning to SLMs, potentially by using correlated knowledge distillation (CKD) [12], to replicate LLM capabilities on resource-limited devices.\n\u2022 Energy: Continuous operation of LLMs requires substantial energy, typically supported by high-performance GPUs and CPUs. The significant energy demands and associated costs pose a challenge to commercial viability. Major technology firms are exploring nuclear-powered energy solutions to meet their LLM needs. Achieving commercial feasibility will require balancing enhanced performance with reduced energy and operational costs, necessitating further advancements."}, {"title": "IV. CONCLUSION", "content": "This research develops and presents AQM-LLM, a framework that adapts LLMs for AQM and evaluates it within the L4S architecture. L4S-LLM dynamically manages congestion, optimizes queue performance and minimizes latency through preemptive ECN signalling and packet-dropping mechanisms. The framework achieves significant computational efficiency using low-rank adaptation, reducing memory usage by 64%, and a data-driven reinforcement learning pipeline, which streamlines fine-tuning. The open-source experimental platform of L4S-AQM on FreeBSD-14 facilitates AQM-LLM future research and assists IETF recognition. Although L4S-LLM demonstrates transformative improvements in latency, throughput, and efficiency, further optimization through techniques like pruning, quantization, and distillation toward Small Language Models based AQM is necessary to address resource constraints, paving the way for scalable and intelligent queuing solutions."}]}