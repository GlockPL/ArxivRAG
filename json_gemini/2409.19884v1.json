{"title": "SWIM: SHORT-WINDOW CNN INTEGRATED WITH MAMBA FOR EEG-BASED AUDITORY SPATIAL ATTENTION DECODING", "authors": ["Ziyang Zhang", "Andrew Thwaites", "Alexandra Woolgar", "Brian Moore", "Chao Zhang"], "abstract": "In complex auditory environments, the human auditory system possesses the remarkable ability to focus on a specific speaker while disregarding others. In this study, a new model named SWIM, a short-window convolution neural network (CNN) integrated with Mamba, is proposed for identifying the locus of auditory attention (left or right) from electroencephalography (EEG) signals without relying on speech envelopes. SWIM consists of two parts. The first is a short-window CNN (SWCNN), which acts as a short-term EEG feature extractor and achieves a final accuracy of 84.9% in the leave-one-speaker-out setup on the widely used KUL dataset. This improvement is due to the use of an improved CNN structure, data augmentation, multitask training, and model combination. The second part, Mamba, is a sequence model first applied to auditory spatial attention decoding to leverage the long-term dependency from previous SWCNN time steps. By joint training SWCNN and Mamba, the proposed SWIM structure uses both short-term and long-term information and achieves an accuracy of 86.2%, which reduces the classification errors by a relative 31.0% compared to the previous state-of-the-art result. The source code is available at https://github.com/windowso/SWIM-ASAD.", "sections": [{"title": "1. INTRODUCTION", "content": "Humans have a remarkable ability for selective hearing. Given complex auditory environments with multiple speakers, multiple conversations and overlapping speech, the human auditory system can selectively ignore and filter out the speech of other speakers by focusing on the specific speaker of interest, a phenomenon known as the cocktail party effect [1]. However, this auditory task proves challenging for individuals with hearing impairments. The reduced spectral and temporal resolution of the early auditory system associated with hearing loss adversely affects cocktail party processing and hearing aids have a limited ability to overcome these deficits. A possible solution would be to enhance the voice of the attended speaker, but first they must be identified. One way could be to identify the attended speaker via eye gaze or gestures, but these methods are not sufficiently robust. A more effective solution could involve using the listener's neural activity, as measured through EEG or other devices, to directly determine the attended speaker, a process known as auditory attention decoding.\nPrevious works in auditory attention decoding have primarily studied stimulus reconstruction, where post-stimulus brain activity is used to decode and reconstruct the envelope of the attended speech stimulus [2-4]. More recent studies focused on the direct decoding of the spatial location of the attended speaker [5-15], a paradigm known as auditory spatial attention decoding (ASAD). ASAD eliminates the need for a clean speech stimulus, thus advancing the development of practical neuro-steered hearing prostheses. To develop high-performance ASAD for neuro-steered hearing, the models used need to satisfy the following abilities: 1) The ability to detect and respond to the change in the user's auditory attention rapidly in a short time window; 2) the ability to use the features embedded in the long-term EEG signals to improve the detection of the auditory attention direction in the current window. These abilities help the model to achieve not only stable detection of the listener's long-term auditory attention focus on the same speaker, but also accurate and immediate detection of attention changes when the listener intends to focus on a different speaker. In the current existing body of literature, convolutional neural networks (CNNs) are applied to EEG signals to achieve high-performance short-term auditory attention decoding for low-latency ASAD [5]. Transformer-based models including STAnet [14] and XAnet [15], are developed to use the attention mechanisms to model long-term temporal information in EEG signals. By combining these two structures, it is possible to achieve both long-term stable detection with short-term rapid response abilities using a single model.\nIn this paper, we propose a novel model structure short-window CNN integrated with Mamba [16] (SWIM) that can model both short-term and long-term EEG signal patterns by stacking CNN and Mamba for ASAD. The CNN acts as a feature extraction from raw EEG signals, enabling the acquisition of highly efficient and accurate local spatial-temporal patterns for auditory attention within the current time step. The Mamba sequence model uses the features extracted by the CNN as the input features at each time step, incorporating long-term temporal information carried from previous time steps to assist in judging the auditory attention of the current time step. Compared to other commonly used sequence models including recurrent neural network (RNN) and Transformer, Mamba which is improved based on the state space model (SSM), combines the advantages of high inference efficiency from RNN and the high training efficiency from Transformer, making it more suitable for the deployment to cost-sensitive neuro-steered hearing devices.\nSpecifically, the short-window CNN (SWCNN) uses a short CNN kernel time window to focus on local patterns of the EEG signals within each 1-second (s) window. Window overlapping and time masking data augmentation methods along with multitask training are used to improve SWCNN. Further by combining two SWCNN models trained with different input channel configurations, an accuracy of 84.9% is achieved in the Leave-one-speaker-out setup of"}, {"title": "2. RELATED WORK", "content": "ASAD Models. The ASAD task does not require clean speech envelopes as input, making it suitable for practical applications in neuro-steered hearing devices. Numerous different models have been proposed for ASAD, such as traditional methods like common spatial patterns (CSP) [6]. Graph neural networks [25] use EEG graph transformed from EEG data for leveraging the positioning information of EEG channels [8, 9]. The neuroscience-inspired spiking neural network offers a fast, accurate, and energy-efficient ASAD [7]. DenseNet-3D converts EEG data into a three-dimensional (3D) arrangement containing spatial-temporal information [10]. Transformer-based models including STAnet [14] and XAnet [15] use attention mechanism in decoding. CNNs are model architectures that have shown good performance and efficiency in extracting features from EEG for ASAD tasks [5].\nSequence Models. Sequence models are typically used to model dependencies in time series data. RNN, as the earliest proposed sequence model, capture historical information through hidden states, and their variants [26] are widely used in many sequence modelling tasks. Transformer [27], the recent prevalent sequence model, use the attention mechanism to handle the temporal dependencies, addressing RNN's issues with parallel training across time and long sequence memory loss. Transformers are highly scalable and have led to the development of applications like large language models.\nMamba [16,28], a recently proposed sequence model of prevalence, is an improvement based on SSM. Mamba combines the benefits from both Transformer and RNN, which can be trained by parallelised across time and is good at modelling long sequences like Transformer, while performing computational and memory-efficient streaming inference like RNN. Overall, Mamba combines the fast autoregressive inference of RNN with the efficient parallel training of the Transformer, balancing performance and efficiency.\nEEG Data Augmentation and Multitask Training. Data augmentation and multitask training are commonly used in deep learning. Given that ASAD datasets are typically of limited sizes, often comprising only dozens to tens of hours of data, the application of data augmentation and multitask training becomes crucial for making full use of EEG data and labels. Many methods have been proposed in the realm of EEG data augmentation [29-38]. The approaches typically involve exploring different overlapping between sliding windows [39, 40], and time masking that sets a portion of the signal to zero [41]. There are also data augmentation methods operating in the frequency domain [41] and in channel dimension [42, 43]. With regards to multitasking, reconstruction of the speech envelope has been considered as an auxiliary task to improve ASAD accuracy [11]."}, {"title": "3. METHODS", "content": "SWIM consists of two parts: a short-window CNN (SWCNN) and a Mamba sequence model. SWCNN acts as an EEG feature extractor, responding rapidly and reliably within a short window. Mamba captures the temporal dependencies across different time steps over long EEG sequences, using the information from previous time steps to improve the prediction of the current time step. Additionally, EEG data augmentation techniques are applied to both SWCNN and SWIM. The following sections will introduce SWCNN, Mamba, SWIM and EEG data augmentation techniques."}, {"title": "3.1. Short-Window CNN", "content": "The SWCNN in Fig. 1 is improved based on the CNN structure used in [5]. The major structural differences are: 1) the kernel time window is reduced from 17 to 5; 2) the channels of CNN are increased from 5 to 16, and the size of the fully connected (FC) layer is increased from 5 to 64 and batch normalisation (BatchNorm) is added.\nThe EEG signal is represented as a matrix in $R^{64 \\times T}$, indicating the presence of 64 channels and T-lengthed decision window. The initial layer within SWCNN incorporates a CNN layer characterised by a kernel dimension of 64 \u00d7 5, a padding schema of (0, 2), and an output channel count of 16. This layer is succeeded by using BatchNorm and a ReLU activation function. Consequently, the output generated by this CNN layer is structured as a $R^{16 \\times 1 \\times \\tilde{T}}$ matrix. After the CNN layer, an average pooling operation is employed, which performs an averaged pooling across time. This operation effectively condenses each time series into a singular value, resulting in an output dimensionality of $R^{16 \\times 1 \\times 1}$. Following the pooling operation,\nSWCNN incorporates two FC layers. The initial FC layer accepts an input in $R^{16}$, projects this input into a $R^{64}$ space, and is further processed by a ReLU activation function. The subsequent FC layer, in turn, processes the $R^{64}$ input to produce an output in $R^{18}$."}, {"title": "3.2. The Mamba Model", "content": "Mamba is a sequence model designed to achieve highly efficient and accurate long sequence modelling. Mamba is developed based on SSM [44, 45], which can be represented as follows\n$h_n = A h_{n-1} + B x_n$ (2)\n$y_n = C h_n$. (3)\nBy compressing history information into the state $h_n$, SSM can handle very long sequences while maintaining low computational and memory requirements. Due to the good properties of SSM, it has efficient parallel training like the Transformer and fast auto-regressive generation like the RNN during inference.\nMamba introduces two key improvements to SSM. First, Mamba incorporates a selection mechanism related to the input-to-state matrix, making the information filtering mechanism of SSM input-dependent, achieving comparable effectiveness to the Transformer, which is called Selective SSM (S6). Second, Mamba optimizes the S6 algorithm on hardware, achieving a theoretical time complexity of $O(n)$ and space complexity of $O(1)$ during training, while the Transformer's theoretical time complexity is $O(n^2)$ and space complexity is $O(n)$, thus Mamba is computationally more efficient than Transformer.\nMamba is highly suitable for our target scenario: A practical EEG-assisted hearing aid device should respond to the change of the user's auditory attention rapidly in low latency while being able to leverage long-term patterns embedded in the previous time steps to aid such short-window judgments. The history length could span hours or even days, placing high demands on the model's inference time complexity. Compared to Transformer, Mamba's inference time complexity is O(1) instead of O(n), which is more suitable for streaming ASAD in practical scenarios. Additionally, hearing aid devices often have limited memory, and Mamba's low storage complexity of O(1) is better suited for edge deployment compared to the Transformer's storage complexity of O(n)."}, {"title": "3.3. From Short Window to Long Sequence", "content": "While SWCNN can perform accurate and efficient ASAD based on short windows, it does not use the long-term information embedded in the previous time steps of the EEG signals, which limits its performance upper bound when making more rapid detection of focus changes with a small time window shift. In contrast, Mamba can achieve highly accurate long sequence modelling by leveraging the temporal dependencies to enhance the performance of the decision of the current time step. Therefore, it is natural and beneficial to extend from a short window to a long sequence. To achieve this, SWIM, a novel structure combining SWCNN and Mamba is proposed for ASAD. This approach retains SWCNN's advantage of rapid response in based on local patterns within a short window while further improving the accuracy and latency by leveraging the temporal dependency from the entire EEG signals.\nAs shown in Fig. 2, SWCNN with classification head removed extracts 64-dim hidden features from the 64 channels and T-length decision window of raw EEG signals. Mamba does not directly act on raw EEG data. Instead, it uses features extracted by SWCNN as input. The hidden features extracted from the current window are concatenated with those extracted from previous CNN decision windows along the time dimension to form a matrix in $R^{64 \\times N}$. This matrix is used as Mamba's input, and Mamba outputs a binary classification result for the current window, indicating the direction of auditory spatial attention (left or right).\nThe Mamba model structure consists of three parts. First, the Mamba backbone network has 3 layers of Mamba blocks, each with a model dimension of 64 and a state dimension of 16. Second, there is an FC layer between the output from SWCNN and the Mamba backbone network, with an input dimension of 64 and an output dimension matching the Mamba model dimension. Third, following the Mamba backbone network, there is an average pooling layer and a classification head. The output size of the Mamba backbone network matches the input size, resulting in an output matrix of $R^{64 \\times N}$. For classification, the average pooling layer reduces the output to a 64-dimensional vector, which is then processed by the classification head, an FC layer that converts the 64-dimensional vector to a 2-dimensional output for classifying auditory attention direction."}, {"title": "3.4. EEG Data Augmentation Techniques", "content": "Two data augmentation techniques are proposed to act on the EEG matrix directly, to facilitate the model's exposure to more diverse EEG data segments and durations.\n1. Overlapping: EEG data is segmented into many overlapped decision windows as shown in Fig. 3, thereby increasing the diversity of the data samples available for training.\n2. Time Masking: The time masking is conducted by SpecAugmentation [46]. $t$ consecutive time steps $[t_0, t_0 + t)$ are masked as shown in Fig. 3. t is randomly drawn from a uniform distribution ranging from 0 to a predefined time mask parameter T, ensuring variability in the extent of masking applied across different instances. Subsequently, the starting point $t_0$ is chosen from the interval $[0, T \u2013 t)$."}, {"title": "4. EXPERIMENTAL SETUP", "content": "Our experiments used the widely used KUL dataset [17], which comprises EEG recordings from 16 subjects with normal hearing. Each subject underwent 8 trials, with each trial lasting 6 minutes. Subjects were instructed to focus on one of two competing speakers that were simultaneously active, positioned at \u00b190\u00b0 in the azimuth direction, representing the left and right locations respectively. The dataset features four stories delivered by three speakers: Speaker1 and Speaker2 narrate Story1 and Story2 respectively, while Speaker3 tells Story3 and Story4. The dataset was recorded using a 64-channel BioSemi ActiveTwo EEG system, followed by pre-processing with down sampling to 128Hz and bandpass filtering. At last, the data is"}, {"title": "4.2. Evaluation Approach", "content": "The dataset is partitioned according to three strategies:\n1. Every-trial: The evaluation approach ensures that the training, validation, and test datasets encompass data from Every-trial. Specifically, the initial 70% of data from each trial is allocated to the training set, the subsequent 15% to the validation set, and the final 15% to the test set.\n2. Leave-one-speaker-out: The dataset consists of data from three speakers. Speaker1 and Speaker2 each narrated a single story, and Speaker3 narrated two stories. To ensure there is sufficient training data, the strategy will focus on leaving"}, {"title": "4.3. Model and Training Implementation", "content": "SWIM was trained using the PyTorch framework on an NVIDIA RTX 4090 GPU. The Adam optimiser [47] was used and the Cosine Annealing Learning Rate Scheduler was used to adjust the learning rate over the training epochs. Regularisation approaches including early stopping based on validation accuracy and weight decay are used to prevent overfitting. Specifically, SWCNN was trained with a batch size of 64, initial learning rate of le-3, maximum epoch number of 100 and weight decay of le-3. SWIM was trained with a batch size of 32, maximum epoch number of 5 and weight decay of 0. When training SWIM, the parameters of SWCNN are loaded from a pre-trained version and fine-tuned with an initial learning rate of 1e-5. The Mamba part starts with an initial learning rate of le-3. When training SWCNN and SWIM, the learning rate was varied from 1e-5 to le-2, the batch size was varied from 32 to 128, and the weight decay was varied from 0 to 1e-1. The hyperparameters were selected according to the results of the validation set."}, {"title": "5. RESULTS AND ANALYSIS", "content": "In this section, the results of three experimental setups are reported. If without explicit specification, the results are derived using a Leave-one-speaker-out setup with a decision window length of 1 second. All experiments are repeated three times with different random seeds, and the mean of the results is reported."}, {"title": "5.1. Improvement from SW CNN", "content": "SWCNN has a shorter kernel time window and more channels than the structure in [5] and adds multitask training. Without using data augmentation, multitask training or model combination, an ablation study was provided in Table 1, showing that the improvement comes from the reduced kernel time window and the increased number of CNN channels. The results found that the use of a long kernel time window with Avg-Pool is inferior to the short kernel time window. Conversely, the increase of CNN channels allows SWCNN to capture more EEG patterns and improve ASAD accuracy."}, {"title": "5.2. Data Augmentation on SW CNN", "content": "Two data augmentation methods, overlapping and time masking, were applied to SWCNN. The following will explore the improvements brought by each method.\nFig. 4(a) depicts the influence of overlapping-based data augmentation, without using time masking or multitask training. The overlapping ratio a is the ratio of the overlapping region length to the decision window length. It is observed that decoding accuracy improves with an increase in a to 0.75, which benefits from the growth of the amount of training data. Further increase a to 0.875 results in a decrease in accuracy, despite the amount of training data being doubled compared to that at a a = 0.75, indicating that SWCNN could suffer from excessive repetition of information.\nFig. 4(b) shows the influence of using time masking-based data augmentation alone. The time masking ratio $\u03b2 = T/T$ is defined as the ratio of the maximum length of the masked region to the decision window length. It is observed that increasing $\u03b2$ improves ASAD accuracy, meaning that increasing data diversity through time masking enhances the model's performance."}, {"title": "5.3. Model Combination of SW CNN", "content": "In this section, a setup with hyperparameters $\u03b1 = 0.75, \u03b2 = 1$, and $\u03b3 = 0.05$ was used as our standard-setting thereafter. Nine channels ('Fpz', 'Fp1', 'AF3', 'F5', 'Fp2', 'AF4', 'F6', 'AF7', 'AF8') located near the eyes are selected and only their data is used to train SWCNN with standard setting, denoted as $M_{NineChan}$. SWCNN trained on all channels with the standard setting is denoted as $M_{AllChan}$. The final output is calculated as $y = 0.5 Softmax(M_{AllChan}(x)) + 0.5 Softmax(M_{NineChan}(x))$, and the system is denoted as \"SWCNN combined\". All results of the three experimental setups are shown in Table 2. By combining models from the two distinct configurations, a final accuracy of 84.9% in the Leave-one-speaker-out setup is achieved, representing a 4.9% improvement over the previous SOTA benchmark. New SOTA benchmarks are established across all three setups."}, {"title": "5.4. SWIM", "content": "SWIM loads the parameters of SWCNN trained in a standard setting, while other parameters are randomly initialized. When training SWIM, the total window length is set to 5s, with the last second as the current window and the rest as the previous windows. SWIM needs to determine the auditory attention direction of the subject in the last second. SWCNN slides over the 5s window with a window length of 1s and a step size of 0.125s. For data augmentation, the overlapping ratio is set to 0.75, and the time masking ratio is set to"}, {"title": "5.5. Channel Importance of SW CNN", "content": "Each of the 64 channels on the test set is sequentially masked, and the ASAD accuracy using SWCNN with the standard setting is tested to evaluate the impact of the absence of each channel on accuracy. The importance of each channel is determined by the accuracy difference achieved with all channels and the accuracy obtained when excluding the specific channel.\nAs depicted in Fig. 5, our findings suggest that channels near the eyes contribute most to successful classification, potentially due to an eye-gaze bias [24]. This bias is characterised by the subject's unconscious tendency to direct their gaze towards the speaker they are attending to. As EEG equipment can inadvertently record these gaze patterns, it allows for the potential exploitation of this gaze information to boost ASAD performance, either intentionally or unintentionally; alternatively, these channels may simply be the most informative, independent of eye-gaze influence. In conclusion, the channels near the eyes should be utilised if they help decode in practice,"}, {"title": "5.6. Model Pattern Analysis", "content": "Why does the Every-trial setup significantly outperform other setups in terms of accuracy? One hypothesis is that the model learns temporal features due to the consistent attending location label within each trial, a continuous duration [24]. Essentially, the model may identify a test segment as part of a particular trial based on these temporal features and correctly predict the attention location from the corresponding training set. The Every-trial setup's training set includes parts of all trials, facilitating the learning of these temporal features. In contrast, other setups lack this advantage as their training sets exclude trials from the test set, making temporal feature learning challenging.\nTo verify this assumption, we devised an experiment similar to the Every-trial setup on SWCNN, but with a variation in the training set. Rather than using the first 70% of each trial, specific durations, such as 0-10% or 10-20%, were used. This approach keeps the amount of training data constant, with the temporal distance between the training and test sets being the sole variable. If our hypothesis holds, accuracy should increase as the training set gets temporally closer to the test set. Our findings depicted in Fig. 7 largely align with the hypothesis that the model learns temporal features in the Every-trial setup."}, {"title": "6. CONCLUSIONS", "content": "In this paper, SWIM, a short-window CNN integrated with Mamba is proposed. Benefiting from the short window CNN (SWCNN) and Mamba's long-term sequence modelling ability, SWIM can achieve a highly accurate, efficient and agile response to the user's auditory attention focus. By applying data augmentation strategies, multitask training, and model combination to SWCNN, an accuracy of 84.9% in the leave-one-speaker-out setup is achieved, surpassing the previous state-of-the-art benchmark by 4.9%. SWCNN also sets new benchmarks in leave-one-subject-out and every-trial setups. By utilizing long-term historical information, SWIM achieves a final accuracy of 86.2% in the leave-one-speaker-out setup. Additionally, an analysis of the importance of each channel confirms that channels near the eyes are crucial for decoding accuracy, although it is unclear if this is due to eye-gaze bias. Finally, we confirm that the model becomes over-fitted to the temporal dependency of each trial in the every-trial setup."}]}