{"title": "Efficient and Universal Neural-Network Decoder for Stabilizer-Based Quantum Error Correction", "authors": ["Gengyuan Hu", "Wanli Ouyang", "Chao-Yang Lu", "Chen Lin", "Han-Sen Zhong"], "abstract": "Quantum error correction is crucial for large-scale quantum computing, but the absence of efficient decoders for new codes like quantum low-density parity-check (QLDPC) codes has hindered progress. Here we introduce a universal decoder based on linear attention sequence modeling and graph neural network that operates directly on any stabilizer code's graph structure. Our numerical experiments demonstrate that this decoder outperforms specialized algorithms in both accuracy and speed across diverse stabilizer codes, including surface codes, color codes, and QLDPC codes. The decoder maintains linear time scaling with syndrome measurements and requires no structural modifications between different codes. For the Bivariate Bicycle code with distance 12, our approach achieves a 39.4% lower logical error rate than previous best decoders while requiring only 1% of the decoding time. These results provide a practical, universal solution for quantum error correction, eliminating the need for code-specific decoders.", "sections": [{"title": "1 Main", "content": "Quantum computing systems are highly susceptible to environmental noise, making quantum error correction (QEC) (1, 2) essential for practical implementations. QEC protects quantum information by encoding logical qubits across multiple physical qubits, analogous to classical error correction. However, quantum error correction is fundamentally more challenging due to constraints imposed by quantum mechanics, including the no-cloning theorem (3) and measurement-induced wavefunction collapse.\n\nThe stabilizer formalism (4) has emerged as the leading theoretical framework for quantum error correction, providing a systematic approach to encode quantum information. While the encoding process is straightforward in the stabilizer formalism, the decoding problem presents a significant challenge, having been proven NP-complete (5) to achieve optimal accuracy. As a result, a practical decoder which allows real-time error suppressing on a quantum computer requires a well designed trade off between accuracy and decoding speed. Although efficient specialized decoders exist for certain codes such as the surface code (6, 7), many advantageous schemes like quantum low-density parity-check (QLDPC) codes (8) and color code (9) still suffers from either low decoding accuracy or long decoding latency.\n\nTwo primary approaches have emerged for achieving efficient decoding beyond the surface code. The matching-based approach, which has demonstrated both high accuracy and computational efficiency for surface codes, focuses on decomposing complex encoding schemes into surface code-like sub-blocks for decoding (10). The belief propagation (BP) based approach adapts classical BP"}, {"title": "1.1 Decode on the Tanner graph", "content": "In the quantum circuit model, quantum computations are executed through sequences of elementary quantum gates acting on qubits, analogous to classical logic gates in digital circuits. However, physical implementations are susceptible to errors during all operations, including gate operations, qubit initialization, and measurements. This realistic scenario is characterized by the circuit-level noise model.\n\nTo protect quantum information against such noise, stabilizer codes are employed. A [[n, k, d]] stabilizer code is defined by a set of n \u2212 k stabilizer generators, represented by a 2n \u00d7 (n \u2212 k) binary symplectic matrix H = [\u00b7|\u00b7]. Each row in H represents a stabilizer generator, and each column of each half corresponds to a qubit. The matrix elements specify Pauli operators through the mapping:\n\n(0|0) \u2192 1, (0|1) \u2192 Z, (1|0) \u2192 X, (1|1) \u2192 Y\n\nsuch that each stabilizer generator is a product of Pauli operators on the encoded qubits.\n\nOur approach begins by representing this stabilizer structure as a Tanner graph (FIG.1). In this representation, stabilizer generators become check nodes and qubits become data nodes, with edges indicating non-zero matrix elements. The Tanner graph serves not only as a mathematical equivalent representation for a stabilizer code but also as an abstracted description of the code's hardware topology. Signals from the quantum device implementing the described code can be naturally embedded into the graph for feature extraction.\n\nDuring quantum error correction, measurements from each stabilizer generator are collected in successive cycles until the logical qubit measurement is required. The collected measurement results from previous cycles, called syndromes, are used to correct the logical measurement results through decoding. To connect the decoding problem with the syndrome, we extend the basic Tanner graph by incorporating nodes and edges representing logical observables, which are also defined as products of Pauli operators. We then embed all syndrome data into the extended Tanner graph G(Vdata, Vcheck, Vlogical, Estabilizer, Elogical), treating each time as a slice of a temporal dynamic graph. This formulation transforms the decoding problem into a temporal graph classification task, seeking a mapping f:\n\nf : G\u2081 ({St}) \u2192 C, S\u2081 \u2208 F,C\u2208 Fs, C\u2208 F\n\nwhere {S} represents the syndrome sequence, and C is a binary vector indicating logical qubit flips,"}, {"title": "1.2 Efficient universal decoding across different code families", "content": "with ns being the number of syndromes and k the number of encoded logical qubits. This binary nature of inputs and outputs reflects the discrete measurement outcomes in quantum experiments.\n\nThe graph formulation's expressiveness distinguishes it from existing neural decoders constrained by topological rigidity. Conventional neural approaches, such as Convolutional Neural Networks (CNNs) tailored for surface codes, require redesigning network architectures when applied to codes spanning different geometries (e.g., 3D toric codes) or non-local structures (e.g., QLDPC codes). In contrast, our extended Tanner graph G offers a topology-agnostic interface: its nodes and edges dynamically adapt to any stabilizer code's connectivity, whether geometric or algebraic. By grounding the neural architecture directly in G's graph isomorphisms, the decoder permits a single neural model to learn decoding strategies across code families, where prior neural decoders required per-code specialization.\n\nThe neural network decoder, whose architecture mirrors the extended Tanner graph structure, can be trained using data from either quantum simulators or real quantum processors. This approach enables the decoder to learn both the underlying noise patterns and circuit-specific error correlations directly from the data, without requiring explicit noise process modeling.\n\nOur previous section established the theoretical framework for universal neural decoders, yet significant challenges persist in optimizing two critical dimensions: modeling the topological structure of quantum codes and capturing temporal dependencies in syndrome sequences. To address these challenges, we propose a three-phase neural decoding architecture :\n\n1. Spatial Feature Extraction: Embedded multiplicative message passing (MMP) operators process syndrome patterns on Tanner graph slices, explicitly leveraging parity-check constraints.\n\n2. Temporal Context Integration: Historical syndrome correlations are captured through linear attention mechanisms, preserving linear inference complexity (O(T)) while enabling parallel training.\n\n3. Logical Error Prediction: Final decoding decisions are generated via topology-grounded graph pooling, incorporating code-specific boundary conditions."}, {"title": "1.3 Robustness to noise strength", "content": "The computational complexity constraint fundamentally shapes our architecture design. To enable real-time decoding for quantum devices, the decoder must maintain linear scaling in the time dimension. While recurrent neural networks (RNNs) naturally satisfy this requirement, their known limitations in long-sequence modeling and training instability make them suboptimal. Instead, we adopt modern linear attention layers (24, 25) - a technique matured through large language model development - which combine parallelizable training with efficient recurrent inference through matrix decomposition techniques.\n\nOur spatial processing employs multiplicative rather than additive message passing, driven by the algebraic structure of stabilizer measurements: simultaneous violations of a stabilizer generator cancel modulo 2. This property is natively captured by first normalizing node features to (-1,1), then applying element-wise multiplication during message aggregation. The resulting product terms directly reflect parity conservation laws.\n\nTo rigorously evaluate the performance of our decoder, we conduct quantum memory experiments using diverse quantum error correction (QEC) codes, including two major code families: triangular color codes with code distances ranging from d = 3 to d = 11, and BB codes represented by two distinct configurations, [[72, 12,6]] and [[144, 12, 12]], where [[n, k, d]] denotes an encoding scheme that uses n physical qubits to encode k logical qubits with a code distance of d. Both families belong to the CSS code subclass (Calderbank-Shor-Steane codes), which allows for independent X- and Z-basis decoding. This structural property simplifies the temporal decoding graph Gt into a static graph G containing only half the stabilizer check nodes, thereby significantly reducing decoding complexity. The simulation employs a uniform depolarizing noise model with an error parameter p = 0.005, reflecting realistic near-term quantum computing conditions (26, 27) For comprehensive benchmarking, we compare our neural network decoder against BP-OSD. Across all test cases, we maintain consistent neural network architectures while strategically adapting graph connectivity configurations and optimizing hyperparameters for each specific code structure. Further details on the noise model can be found in the Supplementary Material.\n\nAs shown in Fig. 2, we quantify decoder performance through two critical fault-tolerance metrics: logical error rate (y-axis) and encoding rate (x-axis). Our neural network decoder demonstrates universal superiority over BP-OSD, achieving a 27.0% average reduction in logical error rates. The performance gap widens substantially for larger codes: In the [[144, 12, 12]] BB code implementation, our decoder attains a 39.4% error rate reduction .\n\nWe also evaluate the decoding speed of the decoders, with the results presented in Fig. 3. The neural decoder demonstrates significantly faster performance compared to the BP-OSD decoder, particularly for large codes with a high number of cycles. This speed advantage stems from the decoder's linear computational complexity along the time dimension and its parallelizable structure across the qubit dimension. For instance, on the BB [[144, 12, 12]] code, the neural decoder achieves a two-orders-of-magnitude speedup at 12 cycles, and this advantage grows to nearly three orders of magnitude at 36 cycles. Although we have not yet fully optimized the decoder for inference, the per-cycle latency is already approximately 1 ms/cycle. Furthermore, we demonstrate the potential throughput when leveraging the full computational power of a GTX4090 device. This scalability underscores the practicality of our method for fault-tolerant quantum architectures.\n\nTo benchmark the performance of our decoder against more existing approaches, we also conducted comparative evaluations using the surface code the most widely researched quantum error correction architecture. Fig. 4 presents analysis results from the Sycamore surface code dataset (23), which encompasses quantum memory experiments for rotated surface codes with distances d = 3 and d = 5 across 25 syndrome extraction cycles. The surface code variant under investigation is the rotated XZZX variant which is not CSS, it has a different number of syndrome measurement at the initializing cycle that establishes it as an prototype for evaluating our framework's capability to handle dynamic temporal graph structures. Following the experimental protocol established in prior work (19), we partitioned the dataset into validation and test sets based on index parity, with final performance metrics obtained by averaging across measurement bases, qubit configurations, and parity selections. Adopting the pretrain-finetune paradigm, we first trained our model on simulated data before performing experimental data adaptation. The proposed decoder demonstrates superior error suppression capability, achieving lower logical error rates while maintaining enhanced parameter efficiency compared to existing solutions.\n\nSince our neural network decoder learns an implicit noise model from training data rather than relying on explicit noise parameters, adapting it to new noise models is challenging. Therefore, it"}, {"title": "2 Discussion", "content": "is important to evaluate its performance across different physical error rates p.\n\nFig. 5 compares our neural network decoder with the BP-OSD decoder on both BB code and Color code under varying error rates. The neural network decoder, trained at p = 0.005, is applied directly across all error rates without fine-tuning. In contrast, the BP-OSD decoder incorporates the actual error rates as prior information during decoding. Our decoder outperforms BP-OSD across all test cases, even with the performance gap widening at lower error rates - far from the training condition of p = 0.005.\n\nTo quantify the performance scaling, we analyze the sub-threshold behavior using the established scaling formula (28, 29):\n\nLER = A(p/Pth)ank/2\n\nwhere p is the physical error rate, n is the block length, representing the number of physical qubits needed in a minimum encoding unit, and LER is the logical error rate. Among the fitting parameters, two are particularly significant: the threshold physical error rate pth and the scaling factor \u1e9e. The threshold pth represents the maximum physical error rate at which increasing code distance reduces logical error rate, while \u1e9e quantifies the effectiveness of increasing block length.\n\nOur experiments show that the neural decoder improves the color code parameters from Pth = 0.006056, \u03b2 = 0.4108 to pth = 0.006610, \u03b2 = 0.5060, and the BB code parameters from Pth = 0.007282, \u03b2 = 0.8458 to pth = 0.006472, \u03b2 = 1.250. These results reveal two distinct patterns: For the color code, the significant higher threshold and similar scaling factor suggest consistent improvement across all code sizes. For the BB code, the significantly higher scaling factor indicates that larger block lengths benefit more from the neural decoder in small p region, highlighting the limitations of BP-OSD for large-scale codes.\n\nIn this work, we have developed a universal framework for quantum error correction decoding using graph neural networks. Our decoder demonstrates superior performance in both accuracy and speed compared to existing approaches. While the framework's architecture is conceptually straightforward, its consistent success across diverse test cases validates the potential of neural network-based approaches."}]}