{"title": "VACoDe: Visual Augmented Contrastive Decoding", "authors": ["Sihyeon Kim", "Boryeong Cho", "Sangmin Bae", "Sumyeong Ahn", "Se-Young Yunt"], "abstract": "Despite the astonishing performance of recent Large Vision-Language Models (LVLMs), these models often generate inaccurate responses. To address this issue, previous studies have focused on mitigating hallucinations by employing contrastive decoding (CD) with augmented images, which amplifies the contrast with the original image. However, these methods have limitations, including reliance on a single augmentation, which is restrictive for certain tasks, as well as the high cost of using external knowledge. In this study, we address these limitations by exploring how to utilize multiple image augmentations. Through extensive experiments, we observed that different augmentations produce varying levels of contrast depending on the task. Based on this observation, we introduce a novel method called VACoDe, Visual Augmented Contrastive Decoding. This method adaptively selects the augmentation with the highest contrast for each task using the proposed softmax distance metric. Our empirical tests show that VACoDe outperforms previous methods and improves output quality in various vision-language tasks. Additionally, VACoDe can be universally applied across different model types and sizes without additional training or the use of external models and data.", "sections": [{"title": "1 Introduction", "content": "Pre-trained Large Vision-Language Models (LVLMs) [23, 34, 38, 7, 16, 15, 25] have gained prominence due to their capability to understand multiple data formats, especially vision and language, simultaneously. These models have demonstrated exceptional performance in various tasks like zero-shot image classification [25, 33], image-text retrieval [33, 16], visual question answering [7, 23], and image captioning [16, 15]. Different from earlier encoder-based models like CLIP [25], most recent large-scale VLMs, such as LLaVA [23], MPlugOWL [34], MiniGPT-4 [38], and InstructBLIP [7], utilize autoregressive transformers to expand their functionality, enabling them to generate more complex outputs.\nHowever, language decoders sometimes produce incorrect outputs, a phenomenon often called hallucination. Among various methodologies [31, 26, 27], one promising approach is contrastive decoding (CD) [17], which generates final answers by examining candidate responses and leveraging their contrastiveness. In detail, they operate in two stages: (1) generating output distributions given original and contrastive prompts each, and (2) subtracting the two output distributions to reduce the likelihood of hallucinated tokens. The effectiveness of this approach depends on how well the"}, {"title": "2 Preliminaries", "content": "Here, we provide a concise summary of background information to aid in understanding this research. We specifically discuss Vision-Language Models, visual data augmentation, and contrastive decoding.\nGenerative Large Vision-Language Models (LVLMs). LVLMs are among the most prominent multi-modality models. They process pairs of input image v and text (e.g., question) q, denoted as (v, q), and generate answers by utilizing the visual information within v. In this paper, we primarily focus on generative LVLMs, rather than CLIP-like [25] models. These generative LVLMs produce tokens one at a time in sequence similar to LLMs. The mathematical expression for this process is:\n$Yt \\sim P(Yt|v, q, Y<t)$.\nHere, $p(\u00b7)$ represents the softmax of the output of the vocabulary set, and $y<t$ denotes the tokens generated up to but not including the timestamp t. Like LLMs, LVLMs are also prone to hallucination [18, 21, 29], where the model erroneously assigns higher probabilities to tokens that do not factually exist in the provided image.\nVisual Augmentation (VA). VA consists of long-established techniques that modify visual data to produce desired images for computer vision research, such as enhancing sharpness, adjusting color jitters, and more. While some augmentation techniques, like mixup [36] or CutMix [35], require combining more than two images, our discussion focuses on single-image augmentation operations for simplicity. We focus on a specific framework:\n$\u03c5' = \u039f\u03bf\u2208\u0391(\u03bd)$,\nwhere o represents an augmentation operation within the set A. In this paper, we employ the augmentations A = {color, flip, random crop, random erase, sharp, edge, noise}."}, {"title": "Contrastive Decoding (CD)", "content": "CD approach was first introduced in the language domain [17]. It operates by generating two outputs using two different models: an expert model that produces the original outputs and an amateur model that generates contrastive outputs. CD is then performed based on the contradictions between them. This approach has also been explored in the VLM domain by using manipulated images to create contrastiveness. This method involves using the image to remove unrelated information, such as hallucinations, by subtracting the image with amplified contrastive information from the original one. The process operates as follows:\n$PCD(y|v, Oo, q) = SOFTMAX ((1+x)f(y\\v, q) \u2013 af (y|O\uff61(v), q))$, (1)\nwhere $f(\u00b7)$ is the output logit obtained from VLM, $O\uff61(v)$ as augmented image and $\\alpha$ as hyperparam-eter for the strength of contrastiveness. To amplify the hallucination inherent in the VLM, VCD [14]"}, {"title": "3 VACoDe: Visual-Augmented Contrastive Decoding", "content": "This section explores the impact of VAs on LVLMs, focusing specifically on CD. In essence, we demonstrate that certain VAs cause either contrast or persistence, implying that the output distribution of the augmented image either varies or stays consistent with the distribution of the original image for the given query. Furthermore, we detail our discovery that contrastive augmentation can be identified using the proposed score, which relies on the softmax distance. Building on these insights, we present a novel algorithm named VACoDe, which leverages both the original and augmented images for CD."}, {"title": "3.1 Appropriate Augmentations Enhance the Contrast", "content": "Initially, we investigate the effect of augmentation on LVLM output. We hypothesize that for each query, there is a specific set of VAs, termed the contrastive augmentation set, which causes the LVLM to produce incorrect answers. These VAs essentially alter the input image, resulting in the loss of key features necessary for correctly answering the query. For instance, if the query is about color, augmentations like color inversion can lead to an incorrect response. In this section, we experiment on the MME benchmark [9] to test our hypothesis and present our findings."}, {"title": "3.2 Maximizing Contrast: Selecting Augmentation with the Biggest Distance", "content": "To address the challenge of selecting contrastive augmentation, we first set our intuition that the augmentation resulting in the most different output can serve as a contrastive augmentation. To measure the difference, we use one of the useful metrics, the L2 norm, called distance D, defined as follows:\n$D(p(v), p(O(v))) = || (p(v) \u2013 p(O(v) ||_2$, where $p(v) = SOFTMAX(f(y|v, x))$. (3)\nNote that it can be changed to other types of distance metrics, such as the Ln norm, KL divergence, and so on. Analysis of this metric is also included in Appendix A."}, {"title": "3.3 VACODE: Visual-Augmented Contrastive Decoding", "content": "Based on the above observations, we propose VACoDe to automatically select an appropriate augmentation for each query by utilizing the distance D. The entire procedure is summarized in Algorithm 1. In the initial decoding phase with the given question, we adaptively select contrastive augmentation by calculating the distance metric D and choosing the augmentation with the maximum D. This chosen augmentation \u00f4 is then used for the remainder of the decoding process. This approach is adopted because computing the output for all augmentation candidates at every step is too resource-intensive. Once the contrastive augmentation is determined, LVLM calculates the probability of tokens PvacoDe,t using Eq. (1). Subsequently, among the whole vocabulary V, the candidate vocabulary set Vcand \u2208 V is defined to select a more reliable token following the original CD algorithm [17]. This process is repeated iteratively to generate the output words Yt.\nFor VACoDe, we use two scenarios to define candidate augmentations: all and selection. All uses all the augmentations as augmentation candidate set A. However, some augmentations may be ineffective or replaceable depending on the given query types or the augmentations in the set. In this case, excluding these augmentations may work as a way to eliminate noisy augmentations. So we introduce the selection strategy that leverages validation to choose a subset of augmentations A' \u2208 A to use only the more effective augmentations. Detailed settings and methods are explained in Appendix D."}, {"title": "4 Experiments", "content": "In this section, we aim to validate the superiority of the proposed algorithm through both qualitative and quantitative analyses."}, {"title": "4.1 Experimental Settings", "content": "Datasets and evaluation metrics. We conduct experiments using three datasets: MME [9],\nMMBench [24], and VQAv2 [10]. Each dataset consists of image-question pairs to evaluate how well LVLM generates robust and correct answers to various questions.\n\u2022 MME is an LVLM evaluation dataset with granular question categories, including 10 categories from the perception tasks and 4 from the cognition tasks. The labels consist of 'Yes' or 'No,' and performance is measured by MME score, which is derived from accuracy. In this paper, we evaluate the perception category as our method focuses on observation ability.\n\u2022 MMBench is a dataset of image-question pairs from 20 categories to validate how skillfully LVLM performs on various vision-language tasks with given option labels. For evaluation, we incorporate CircularEval which rotates the positions of the possible option labels in a circular manner."}, {"title": "4.2 Experiment Results", "content": "Result on each category. Table 2 shows the MME score of CD using different augmentations on the MME dataset for each perception category using the LLaVA-1.5 13B model. First, looking at the single method, as mentioned in Section 3.1, if each single visual augmentation corresponds to a contrastive augmentation on the given question, it improves CD performance. Although it does not improve the performance in all other question categories, the total MME score increases. This means LVLMs are likely to provide incorrect answers when image augmentations are applied. For instance, in the case of questions about recognizing celebrities or landmarks, humans can answer the corresponding"}, {"title": "4.3 Discussion", "content": "Analysis on the selection strategy. Among the augmentation strategies all and selection, selection shows better performance in Table 2 and Table 3. This indicates that our approach to eliminating noisy augmentations is effective and highlights the importance of using only the most effective augmentations to achieve better performance. This approach not only proves its efficacy but also provides users with guidance on choosing the optimal subset of augmentations from various options.\nAnalysis of the combination of visual augmentations. In this section, we evaluate different combinations of augmentations to estimate the impact of each augmentation. For simplicity, we limit the augmentation set to {color, flip, random crop}. Table 4 shows the effect of using all augmentation candidates in the set and the impact of excluding each one individually. According to the results, VACoDe performance using all three augmentations, color, crop and flip, shows higher performance than other sub-combinations. Specifically, when color or flip augmentation is removed from the augmentation set, performance in the color and position categories significantly decreases. Considering each augmentation has a different contrastive effect, the results confirm that providing an appropriate combination of VAs for A can provide proper contrast for a given task.\nQualitative study. In this section, we discuss examples of using VACoDe in MMBench with LLAVA-13B as illustrated in Figure 7. The first example in Figure 7a demonstrates an instance where LVLM incorrectly predicts as \"sheep is behind the car.\" When edge augmentation is applied by VACODE, it exacerbates LVLM's confusion, increasing the likelihood of an incorrect answer. However, CD corrects this by addressing the disadvantage and generates the correct answer. In Figure 7b, the original image prediction is 'top-left' for an empty space. After flipping the image, the model correctly identifies 'bottom-left' as the accurate answer for the augmented image. It's important to note that 'top-left' has a high probability in both images, indicating a bias in the LVLM to the given image and the question. Applying CD reduces this bias, resulting in the correct output."}, {"title": "5 Related Works", "content": "Visual augmentation. In the computer vision domain, visual augmentation has been employed to increase the diversity of sample data, thereby helping to overcome the challenges associated with acquiring large training datasets and mitigating overfitting issues in environments with limited samples. Traditional augmentations include changes in color, cropping, and flipping. Additionally, there are more advanced techniques such as erasing [13, 8, 37], and other techniques such as mixup [36] and CutMix [35]. Furthermore, the automatic application of multiple augmentations has been explored [6, 19].\nSome studies in LVLMs employ VA to achieve the desired output in various methods. FGVP [32] adds blur to the background of the image, leaving the main object clear to emphasize it. To focus on each object in the image, [3, 28, 20] use multiple cropped images, each focusing on a single object to generate the desired output, while [11] uses inpainting to erase objects to measure the correlation between objects.\nContrastive decoding. CD [17] was introduced in the NLP domain using two differently sized language models. It leverages contrastive output by subtracting the small model's probability from the larger model's to retain the strengths of the large model while eliminating the weaknesses that are evident in the small model. There are variants like DOLA [5] which utilizes contrast in layer-level outputs and Instructive Decoding [12] uses two contrastive instructions to generate an output opposite to the original output.\nRecently, similar approaches have been applied in LVLMs, utilizing contrastive images to guide the model in generating accurate text, mainly focusing on reducing hallucination in LVLM [18, 21, 29]. VCD [14] demonstrates that adding noise to the image can elevate the hallucination inherent in LVLMs, subsequently applying CD to manage the hallucination. Another work CRG [30] employs a black bounding box from external data to conceal the object relevant to the question, amplifying hallucination, while HALC [2] uses multiple different cropped images from the detection model and explores multiple pairs of cropped images to find pairs that amplify the information in the cropped image. These works address methods to manage hallucination in LVLMs using a single type of augmentation, which has limitations in generating enough contrast for various types of questions. Unlike previous studies, VACoDe explores multiple augmentations and selects the most effective one to answer the question. Moreover, it does not require additional training or an external model, providing direct perturbation to the image."}, {"title": "6 Conclusion", "content": "In this paper, we introduce VACoDe for utilizing multiple augmentations by adaptively choosing contrastive decoding. Initially, we examined the effects of various augmentations and found that their effectiveness depends on the type of question. Specifically, each query has key features that act as clues for answers, and contrastive augmentations can modify these features. Therefore, selecting the contrastive augmentation that creates a significant contrast is essential for improving CD. Based on this, we propose an algorithm called VACoDe, which selects augmentation by the largest distance D. Experiments show that VACoDe outperforms other methods across different datasets and underscores the importance of selecting appropriate augmentations.\nLimitation. Our method selects the appropriate contrastive augmentation among augmentation candidates. No matter how well VACoDe works and the appropriate augmentation is selected for the given task, if there is no sufficient contrastive augmentation for the task among the candidates, it is difficult to expect a significant performance gain.\nFuture work. Future work includes implementing an automatic search for candidate augmentation sets suitable for the target task. Additionally, investigating the relationship between visual contrast and language contrast in LVLMs suggests a further direction for expanding this study."}, {"title": "A Ablation on Distance metric D", "content": "In this section, we examine comprehensive several additional ablation experiments that are consid-erable in the environment in which VACoDe is applied. Based on these ablation results, we expect VACoDe to have universally high robustness and be able to perform various tasks, models, and inferences.\nWe perform experiments using several common distance measures to define our distance function D that VACoDe uses to select which VA will produce high contrast. The experiment is performed in the MME dataset using the LLaVA-1.5 13B model. Also, we use the average softmax Gain directly to check the effect. In detail, softmax Gain on the correct answer label obtained when applying the distance measure candidate Di used in the experiment and the VAs used in Figure 2 to VACoDe for all samples. In order to control the variables of VAs that contain randomness, each experiment performs a total of 5 experiments with different seeds on the entire MME dataset and then measures softmax Gain through the average."}, {"title": "B Analysis of Different Model Sizes", "content": "We showed that VACoDe is proper for general LVLMs and has a significant effect on performance by experimenting with three different models LLAVA-1.5, InstructBLIP, and Qwen-VL on various types of datasets at the Section 4. In this ablation, we conduct an experiment using LLaVA-1.5 7B, 13B and InstructBLIP 7B, 13B to check the effect of the model size on VACoDe. MME dataset is used for this experiment. We measured the performance for the perception category and the total performance for each model."}, {"title": "CEffect of Different Sampling Strategies", "content": "We perform analysis studies on different sampling strategies to see how VACoDe is affected by sampling methods other than basic regular decoding. In this experiment, 4 sampling techniques are applied: (1) Top P sampling (specifically, p = 0.9), (2) Top K sampling (specifically, k = 50), (3) Temperature sampling (specifically, T = 0.7/1.5). Top P sampling is a method in which the only token candidates in the distribution on cumulative probability p can be selected as the next token. This has the effect of preventing noise samples with too low a probability to be extracted from candidates. Top K sampling uses only the top k candidates from the highest probability for sampling. In temperature sampling, temperature scaling is applied to the softmax to calculate the next token logits. When temperature T is low, the possibility of selecting a high-probability candidate group increases, and the possibility of choosing low-probability candidates decreases. It has the effect of increasing the probability of more static responses. Conversely, when the temperature T is large, the chance of choosing among the high-probability candidates decreases, and the low-probability candidates increases. It has the effect of increasing the possibility of making more diverse responses."}, {"title": "D Selection Strategy", "content": "Removing noisy augmentations via acceptence threshold. Using the distance D, we expect to select a VA that shows high-performance improvement when used on CD. However, there may exist cases where some VAs cannot be appropriate contrastive augmentation for a specific task overall. In this case, these VAs contribute less to performance improvement than other VAs on average and can sometimes become noise that prevents other VAs from being used as contrast. We use the Acceptance Threshold, a simple baseline that eliminates the noise VAs. To discover the suitableness of VAs for the target task, in the sample sub-dataset, we utilize the LVLM's first token generation distance by VACoDe for each VA. Let ci be the number of times that VA\u017c selected as contrast VA among a total of M VAs. For the N data samples and acceptance threshold T, candidate VAs with Ci < TN TM are treated as unsuitable for this task and removed. Throughout the main experiments, we used the acceptance threshold of T = 0.5."}, {"title": "E Experiment Details", "content": "In this paper, all reported our experiment used LVLM models can run on a single 48 GB NVIDIA RTX A6000. In the process of applying VACoDe, our model requires inference as the number of VAs used in the first step only, and each subsequent generation step requires twice token generation stages."}]}