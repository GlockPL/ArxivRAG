{"title": "VACoDe: Visual Augmented Contrastive Decoding", "authors": ["Sihyeon Kim", "Boryeong Cho", "Sangmin Bae", "Sumyeong Ahn", "Se-Young Yunt"], "abstract": "Despite the astonishing performance of recent Large Vision-Language Models (LVLMs), these models often generate inaccurate responses. To address this issue, previous studies have focused on mitigating hallucinations by employing contrastive decoding (CD) with augmented images, which amplifies the contrast with the original image. However, these methods have limitations, including reliance on a single augmentation, which is restrictive for certain tasks, as well as the high cost of using external knowledge. In this study, we address these limitations by exploring how to utilize multiple image augmentations. Through extensive experiments, we observed that different augmentations produce varying levels of contrast depending on the task. Based on this observation, we introduce a novel method called VACoDe, Visual Augmented Contrastive Decoding. This method adaptively selects the augmentation with the highest contrast for each task using the proposed softmax distance metric. Our empirical tests show that VACoDe outperforms previous methods and improves output quality in various vision-language tasks. Additionally, VACoDe can be universally applied across different model types and sizes without additional training or the use of external models and data.", "sections": [{"title": "Introduction", "content": "Pre-trained Large Vision-Language Models (LVLMs) [23, 34, 38, 7, 16, 15, 25] have gained promi-nence due to their capability to understand multiple data formats, especially vision and language, simultaneously. These models have demonstrated exceptional performance in various tasks like zero-shot image classification [25, 33], image-text retrieval [33, 16], visual question answering [7, 23], and image captioning [16, 15]. Different from earlier encoder-based models like CLIP [25], most recent large-scale VLMs, such as LLaVA [23], MPlugOWL [34], MiniGPT-4 [38], and InstructBLIP [7], utilize autoregressive transformers to expand their functionality, enabling them to generate more complex outputs.\nHowever, language decoders sometimes produce incorrect outputs, a phenomenon often called hallucination. Among various methodologies [31, 26, 27], one promising approach is contrastive decoding (CD) [17], which generates final answers by examining candidate responses and leveraging their contrastiveness. In detail, they operate in two stages: (1) generating output distributions given original and contrastive prompts each, and (2) subtracting the two output distributions to reduce the likelihood of hallucinated tokens. The effectiveness of this approach depends on how well the contrasting prompts are constructed. While it is relatively straightforward to create contrastive text prompts by replacing original words with their opposites or random words [12], image prompts require a more deliberate approach, as there is no clearly defined augmentation strategy that creates contrastive information.\nThere have been a few works on generating contrastive images [14, 30, 2], which aim to increase sample variance by manipulating features in images through the addition of noise or cropping. In some cases, it has been shown that generating contrastive images using their static method can effectively modify features to create contrastiveness. However, applying fixed augmentations to all samples cannot always guarantee contrastive images, as the salient features in an image may vary depending on the text question in vision-language tasks.\nWe illustrate an example in Figure 1, where the model receives position-related question \u201cwhere is the cat?,\" and generates the incorrect output, left. If a position-related augmentation such as flipping is applied to a given image, the output distribution would likely be heavily skewed towards left. Thus, the model can generate the correct answer with the CD method by decreasing the probability of the hallucinated token. However, when applying color augmentation, where color is a less relevant feature to the target question, the color-augmented output distribution may be similar to the output distribution from the original image. Subsequently, the contrastive decoded logit with color augmentation may still have the wrong answer. In light of this, to generate appropriate and sufficient contrastiveness to ensure the model provides the correct answer, selecting the proper augmentation operation is significantly required.\nContributions. In this paper, we address the challenge of enhancing CD performance by formulating the selection of the proper augmentation. Our contributions are summarized as follows:\n\u2022 We explore the effect of visual augmentation on LVLMs. Our findings indicate that each augmentation has a distinct impact, altering the output distribution of VLMs and subsequently affecting the response. From the CD perspective, the choice of proper augmentation is critical: selecting contrastive augmentations that introduce beneficial contrast can enhance performance, while persistent augmentations can lead to a decline in performance.\n\u2022 Based on the findings, we introduce an algorithm called VACoDe that selects the most contrastive augmentation to empower CD capability without additional training or using external models. The"}, {"title": "Preliminaries", "content": "Here, we provide a concise summary of background information to aid in understanding this research. We specifically discuss Vision-Language Models, visual data augmentation, and contrastive decoding.\nGenerative Large Vision-Language Models (LVLMs). LVLMs are among the most prominent multi-modality models. They process pairs of input image v and text (e.g., question) q, denoted as (v, q), and generate answers by utilizing the visual information within v. In this paper, we primarily focus on generative LVLMs, rather than CLIP-like [25] models. These generative LVLMs produce tokens one at a time in sequence similar to LLMs. The mathematical expression for this process is:\n$Y_t \\sim P(Y_t|v, q, Y_{<t})$.\nHere, $p(\u00b7)$ represents the softmax of the output of the vocabulary set, and $y_{<t}$ denotes the tokens generated up to but not including the timestamp t. Like LLMs, LVLMs are also prone to hallucination [18, 21, 29], where the model erroneously assigns higher probabilities to tokens that do not factually exist in the provided image.\nVisual Augmentation (VA). VA consists of long-established techniques that modify visual data to produce desired images for computer vision research, such as enhancing sharpness, adjusting color jitters, and more. While some augmentation techniques, like mixup [36] or CutMix [35], require combining more than two images, our discussion focuses on single-image augmentation operations for simplicity. We focus on a specific framework:\n$\u03c5' = \u039f_{o \\in A}(\u03bd)$,\nwhere o represents an augmentation operation within the set A. In this paper, we employ the augmentations A = {color, flip, random crop, random erase, sharp, edge, noise}. Examples are illustrated in Figure 2. The descriptions of the augmentations are: (1) color: color inversion, (2) flip: horizontal flip followed by vertical flip, (3) crop: cropping a random part of the image, (4) erase: randomly erasing part of the image, (5) sharp: adjusting image sharpness, (6) edge: extracting edge textures, and (7) noise: adding diffusion noise. Note that we use the default noise setting from VCD [14].\nContrastive Decoding (CD). CD approach was first introduced in the language domain [17]. It operates by generating two outputs using two different models: an expert model that produces the original outputs and an amateur model that generates contrastive outputs. CD is then performed based on the contradictions between them. This approach has also been explored in the VLM domain by using manipulated images to create contrastiveness. This method involves using the image to remove unrelated information, such as hallucinations, by subtracting the image with amplified contrastive information from the original one. The process operates as follows:\n$PCD(y|v, Oo, q) = SOFTMAX ((1+x)f(y\\v, q) \u2013 af (y|O\uff61(v), q)),$\nwhere $f(\u00b7)$ is the output logit obtained from VLM, $O\uff61(v)$ as augmented image and $\\alpha$ as hyperparam-eter for the strength of contrastiveness. To amplify the hallucination inherent in the VLM, VCD [14]"}, {"title": "VACoDe: Visual-Augmented Contrastive Decoding", "content": "This section explores the impact of VAs on LVLMs, focusing specifically on CD. In essence, we demonstrate that certain VAs cause either contrast or persistence, implying that the output distribution of the augmented image either varies or stays consistent with the distribution of the original image for the given query. Furthermore, we detail our discovery that contrastive augmentation can be identified using the proposed score, which relies on the softmax distance. Building on these insights, we present a novel algorithm named VACoDe, which leverages both the original and augmented images for CD.\nAppropriate Augmentations Enhance the Contrast\nInitially, we investigate the effect of augmentation on LVLM output. We hypothesize that for each query, there is a specific set of VAs, termed the contrastive augmentation set, which causes the LVLM to produce incorrect answers. These VAs essentially alter the input image, resulting in the loss of key features necessary for correctly answering the query. For instance, if the query is about color, augmentations like color inversion can lead to an incorrect response. In this section, we experiment on the MME benchmark [9] to test our hypothesis and present our findings.\nExperimental setting. First of all, we provide a summary of the MME dataset [9] and the experimental settings in detail. MME benchmark categorizes question types into 14 groups, such as color, count, position, existence, and more. We concentrate on questions related to color, existence, and position to thoroughly investigate the influence of VAs. We manually select contrastive augmentations for each query type, based on our hypothesis that they can produce incorrect outputs. Specifically, depending on the query type, we choose the contrastive augmentation pairs as outlined in Table 1. Note that the remaining two augmentations for each type are considered persistent augmentations. In subsequent experiments, we use these three augmentations and three query types to evaluate the augmentation effect by assessing softmax outputs with LLaVA-1.5.\nContrastive augmentations decrease performance. Using the manually selected query-augmentation pairs, we analyze their outputs both qualitatively and quantitatively. For qualitative analysis, as described in Figure 3, we show examples of contrastive and persistent augmentations for given queries. In summary, when the LVLM encounters a contrastively augmented image, it fails to produce the correct answer. As shown in Figure 3a, when the model is asked a color-related question with the original image, it provides the correct answer. However, when given the color-augmented image, which is a contrastive augmentation on the color-type query, the model generates an incorrect answer. Conversely, if a flip augmentation, i.e., persistent augmentation, is applied, the model correctly predicts the answer. This is because flipping does not alter the key color features that are needed to respond accurately. Similarly, for the existence-type query, as shown in Figure 3b, random cropping corresponds to a contrastive augmentation since it removes the objects needed to answer correctly. For instance, the portion containing the \"chair\" may be cropped out, leading to an incorrect response. However, coloring the image does not affect the presence of the \"chair,\" allowing the model to provide the correct answer. Qualitatively, there are VAs that can manipulate key features crucial for answering the question correctly. These results are also reflected in the quantitative results, as shown in Figure 4. Figure 4a shows the MME score difference when augmentation is applied. For instance, when the question type is \"existence,\" applying random cropping to the input image lowers the MME score. It means that contrastive augmentation can lead to a performance decrease.\nAdditionally, we aim to verify whether this contrastive augmentation can be advantageous in a CD setting. To assess this, we measure the softmax gain, called Gain score, as follows:\n$Gain(v, q, YGT, 0) = PCD(YGT|v, O, q) \u2013 SOFTMAX(f(yGT|v, q)).$\nThis score measures the increase of the softmax on the ground truth value from the original decoding to the CD output to examine how much advantage CD can incur. As illustrated in Figure 4b, utilizing CD methods with contrastive augmentation results in a significant increase in the Gain score. For instance, in the case of existence-type questions, we observe the highest gain when using contrast augmentation, i.e., random crop. Since we rely on impractical information, such as manually selected contrastive augmentations, the remaining challenge is to identify the contrastive augmentation for each query without human intervention.\nMaximizing Contrast: Selecting Augmentation with the Biggest Distance\nTo address the challenge of selecting contrastive augmentation, we first set our intuition that the augmentation resulting in the most different output can serve as a contrastive augmentation. To measure the difference, we use one of the useful metrics, the L2 norm, called distance D, defined as follows:\n$D(p(v), p(O(v))) = || (p(v) \u2013 p(O(v) ||_2, where p(v) = SOFTMAX(f(y|v, x)).$\nNote that it can be changed to other types of distance metrics, such as the Ln norm, KL divergence, and so on. Analysis of this metric is also included in Appendix A.\nChoosing augmentation with distance D. To verify our hypothesis a bigger distance D can have the most contrastiveness \u2013 we measure distance D following Eq. (3) and the Gain score defined as Eq. (2), under the aforementioned experimental conditions. To examine the correlation between the distance D and the Gain score, we sort the augmentations based on the distance D, and analyze the average Gain score on each ranking. As shown in Figure 5, we confirm that the augmentation with the greatest D results in the biggest average increase in the Gain score. This implies that selecting the augmentation with the highest D yields the best performance improvement. Additionally, the top-ranked group shows a higher increase than other single augmentations. It also indicates that augmentation integration is successful using distance D. Moreover, Figure 6 shows how frequently each augmentation is selected as having the highest D score with the knowledge of the question type. The most frequent augmentations correspond to contrastive augmentation, which aligns with intuition in Section 3.1. This suggests using the augmentation with the largest D to select the contrastive augmentation o for each query q. While this is an impractical test, it is valuable for understanding the impact of the distance metric."}, {"title": "VACODE: Visual-Augmented Contrastive Decoding", "content": "Based on the above observations, we propose VACoDe to automatically select an appropriate augmentation for each query by utilizing the distance D. The entire procedure is summarized in Algorithm 1.\nIn the initial decoding phase with the given question, we adaptively select contrastive augmentation by calculating the distance metric D and choosing the augmentation with the maximum D. This chosen augmentation \u00f4 is then used for the remainder of the decoding process. This approach is adopted because computing the output for all augmentation candidates at every step is too resource-intensive. Once the contrastive augmentation is determined, LVLM calculates the probability of tokens PvacoDe,t using Eq. (1). Subsequently, among the whole vocabulary V, the candidate vocabulary set Vcand \u2208 V is defined to select a more reliable token following the original CD algorithm [17]. This process is repeated iteratively to generate the output words Yt.\nFor VACoDe, we use two scenarios to define candidate augmentations: all and selection. All uses all the augmentations as augmentation candidate set A. However, some augmentations may be ineffective or replaceable depending on the given query types or the augmentations in the set. In this case, excluding these augmentations may work as a way to eliminate noisy augmentations. So we introduce the selection strategy that leverages validation to choose a subset of augmentations A' \u2208 A to use only the more effective augmentations. Detailed settings and methods are explained in Appendix D."}, {"title": "Experiments", "content": "In this section, we aim to validate the superiority of the proposed algorithm through both qualitative and quantitative analyses.\nExperimental Settings\nDatasets and evaluation metrics. We conduct experiments using three datasets: MME [9], MMBench [24], and VQAv2 [10]. Each dataset consists of image-question pairs to evaluate how well LVLM generates robust and correct answers to various questions.\n\u2022 MME is an LVLM evaluation dataset with granular question categories, including 10 categories from the perception tasks and 4 from the cognition tasks. The labels consist of 'Yes' or 'No,' and performance is measured by MME score, which is derived from accuracy. In this paper, we evaluate the perception category as our method focuses on observation ability.\n\u2022 MMBench is a dataset of image-question pairs from 20 categories to validate how skillfully LVLM performs on various vision-language tasks with given option labels. For evaluation, we incorporate CircularEval which rotates the positions of the possible option labels in a circular manner.\n\u2022 VQAv2 is a dataset containing open-ended questions paired with images. This allows for a proper evaluation of how expertly the model can utilize the given visual information rather than simply using the learned language priors of the decoder. We randomly select 30, 000 samples from the VQAv2 evaluation dataset to validate our method.\nFor the reliability of the results, we report performance using the average of the results of 5 different seed runs for MME and MMBench, and a single run for VQAv2.\nModels. We evaluate the performance of VACoDe on three baseline LVLM foundation models: LLAVA-1.5 [22], InstructBLIP [7] and Qwen-VL [1]. Specifically, we use pretrained LLAVA-1.5 and InstructBLIP with Vicuna [4] 13B language decoder, and Qwen-VL with Qwen 7B backbone. Ablation studies on model size can be found in Appendix B.\nCD methods. We use 7 augmentations in Figure 2 for the augmentation set A. CD with each single augmentation is used as a baseline, and note that a single noise addition augmentation is equivalent to the VCD [14] method. We exclude other methods from the baseline that incorporate external models or data. When applying VACoDe, we employ both all and selection strategies. The selected augmentations vary depending on the models or datasets. For example, on the MME benchmark, the LLaVA-13B model utilizes four specific augmentations: color, edge, crop, and flip for selection.\nImplementation details. For the main experiment, we choose a = 1 and \u03b2 = 0.1 for VACoDe. Additionally, we use T = 1 and p = 1 for the sampling strategy, which employs the softmax distribution for the next token generation. Ablation studies on decoding strategies can be found in Appendix C.\nExperiment Results\nResult on each category. Table 2 shows the MME score of CD using different augmentations on the MME dataset for each perception category using the LLaVA-1.5 13B model. First, looking at the single method, as mentioned in Section 3.1, if each single visual augmentation corresponds to a contrastive augmentation on the given question, it improves CD performance. Although it does not improve the performance in all other question categories, the total MME score increases. This means LVLMs are likely to provide incorrect answers when image augmentations are applied. For instance, in the case of questions about recognizing celebrities or landmarks, humans can answer the corresponding"}, {"title": "Discussion", "content": "Analysis on the selection strategy. Among the augmentation strategies all and selection, selection shows better performance in Table 2 and Table 3. This indicates that our approach to eliminating noisy augmentations is effective and highlights the importance of using only the most effective augmentations to achieve better performance. This approach not only proves its efficacy but also provides users with guidance on choosing the optimal subset of augmentations from various options.\nAnalysis of the combination of visual augmentations. In this section, we evaluate different combinations of augmentations to estimate the impact of each augmentation. For simplicity, we limit the augmentation set to {color, flip, random crop}. Table 4 shows the effect of using all augmentation candidates in the set and the impact of excluding each one individually. According to the results, VACoDe performance using all three augmentations, color, crop and flip, shows higher performance than other sub-combinations. Specifically, when color or flip augmentation is removed from the augmentation set, performance in the color and position categories significantly decreases. Considering each augmentation has a different contrastive effect, the results confirm that providing an appropriate combination of VAs for A can provide proper contrast for a given task.\nQualitative study. In this section, we discuss examples of using VACoDe in MMBench with LLAVA-13B as illustrated in Figure 7. The first example in Figure 7a demonstrates an instance where LVLM incorrectly predicts as \"sheep is behind the car.\" When edge augmentation is applied by VACODE, it exacerbates LVLM's confusion, increasing the likelihood of an incorrect answer. However, CD corrects this by addressing the disadvantage and generates the correct answer. In Figure 7b, the original image prediction is 'top-left' for an empty space. After flipping the image, the model correctly identifies 'bottom-left' as the accurate answer for the augmented image. It's important to note that 'top-left' has a high probability in both images, indicating a bias in the LVLM to the given image and the question. Applying CD reduces this bias, resulting in the correct output."}, {"title": "Related Works", "content": "Visual augmentation. In the computer vision domain, visual augmentation has been employed to increase the diversity of sample data, thereby helping to overcome the challenges associated with acquiring large training datasets and mitigating overfitting issues in environments with limited samples. Traditional augmentations include changes in color, cropping, and flipping. Additionally, there are more advanced techniques such as erasing [13, 8, 37], and other techniques such as mixup [36] and CutMix [35]. Furthermore, the automatic application of multiple augmentations has been explored [6, 19].\nSome studies in LVLMs employ VA to achieve the desired output in various methods. FGVP [32] adds blur to the background of the image, leaving the main object clear to emphasize it. To focus on each object in the image, [3, 28, 20] use multiple cropped images, each focusing on a single object to generate the desired output, while [11] uses inpainting to erase objects to measure the correlation between objects.\nContrastive decoding. CD [17] was introduced in the NLP domain using two differently sized language models. It leverages contrastive output by subtracting the small model's probability from the larger model's to retain the strengths of the large model while eliminating the weaknesses that are evident in the small model. There are variants like DOLA [5] which utilizes contrast in layer-level outputs and Instructive Decoding [12] uses two contrastive instructions to generate an output opposite to the original output.\nRecently, similar approaches have been applied in LVLMs, utilizing contrastive images to guide the model in generating accurate text, mainly focusing on reducing hallucination in LVLM [18, 21, 29]. VCD [14] demonstrates that adding noise to the image can elevate the hallucination inherent in LVLMs, subsequently applying CD to manage the hallucination. Another work CRG [30] employs a black bounding box from external data to conceal the object relevant to the question, amplifying hallucination, while HALC [2] uses multiple different cropped images from the detection model and explores multiple pairs of cropped images to find pairs that amplify the information in the cropped image. These works address methods to manage hallucination in LVLMs using a single type of augmentation, which has limitations in generating enough contrast for various types of questions. Unlike previous studies, VACoDe explores multiple augmentations and selects the most effective one to answer the question. Moreover, it does not require additional training or an external model, providing direct perturbation to the image."}, {"title": "Conclusion", "content": "In this paper, we introduce VACoDe for utilizing multiple augmentations by adaptively choosing contrastive decoding. Initially, we examined the effects of various augmentations and found that their effectiveness depends on the type of question. Specifically, each query has key features that act as clues for answers, and contrastive augmentations can modify these features. Therefore, selecting the contrastive augmentation that creates a significant contrast is essential for improving CD. Based on this, we propose an algorithm called VACoDe, which selects augmentation by the largest distance D. Experiments show that VACoDe outperforms other methods across different datasets and underscores the importance of selecting appropriate augmentations.\nLimitation. Our method selects the appropriate contrastive augmentation among augmentation candidates. No matter how well VACoDe works and the appropriate augmentation is selected for the given task, if there is no sufficient contrastive augmentation for the task among the candidates, it is difficult to expect a significant performance gain.\nFuture work. Future work includes implementing an automatic search for candidate augmentation sets suitable for the target task. Additionally, investigating the relationship between visual contrast and language contrast in LVLMs suggests a further direction for expanding this study."}, {"title": "A Ablation on Distance metric D", "content": "In this section, we examine comprehensive several additional ablation experiments that are consid-erable in the environment in which VACoDe is applied. Based on these ablation results, we expect VACoDe to have universally high robustness and be able to perform various tasks, models, and inferences.\nWe perform experiments using several common distance measures to define our distance function D that VACoDe uses to select which VA will produce high contrast. The experiment is performed in the MME dataset using the LLaVA-1.5 13B model. Also, we use the average softmax Gain directly to check the effect. In detail, softmax Gain on the correct answer label obtained when applying the distance measure candidate Di used in the experiment and the VAs used in Figure 2 to VACoDe for all samples. In order to control the variables of VAs that contain randomness, each experiment performs a total of 5 experiments with different seeds on the entire MME dataset and then measures softmax Gain through the average.\nFigure 8 shows the result of affectness of different distance D functions. In this experiment, we use L1, L2, L3, L\u221e, Cosine similarity, Kullback-Leibler Divergence (KL divergence), and Earth mover's distance (EM distance) as distance candidates. The x-axis of the results in Figure 8 means the candidate distance names used, and the y-axis means the average softmax gain improved compared to regular decoding obtained through VACoDe when each distance is used as a measurement. From the results, we can check that L1, L2, and L3 norms show high performance improvement almost no difference overall. This means that any of these can be used in the algorithm as a distance function at a similar level. However, in the case of L\u221e and KL divergence, it can be seen that the actual performance improvement is much smaller compared to others. These show very low-performance improvement compared to the L2 distance, which we used in the main experiment, meaning they are improper measurements for estimating the expected contrast of VAs. The other two distances, cosine similarity and EM distance, performed higher than KL divergence but did not perform higher than L2 norm for the entire MME dataset. Based on this result, we empirically confirmed that using L2 norm as our main VACoDe distance D is a meaningful standard through experiments with these distance measures and the results shown throughout our main experiments."}, {"title": "B Analysis of Different Model Sizes", "content": "We showed that VACoDe is proper for general LVLMs and has a significant effect on performance by experimenting with three different models LLAVA-1.5, InstructBLIP, and Qwen-VL on various types of datasets at the Section 4. In this ablation, we conduct an experiment using LLaVA-1.5 7B, 13B and InstructBLIP 7B, 13B to check the effect of the model size on VACoDe. MME dataset is used for this experiment. We measured the performance for the perception category and the total performance for each model."}, {"title": "CEffect of Different Sampling Strategies", "content": "We perform analysis studies on different sampling strategies to see how VACoDe is affected by sampling methods other than basic regular decoding. In this experiment, 4 sampling techniques are applied: (1) Top P sampling (specifically, p = 0.9), (2) Top K sampling (specifically, k = 50), (3) Temperature sampling (specifically, T = 0.7/1.5). Top P sampling is a method in which the only token candidates in the distribution on cumulative probability p can be selected as the next token. This has the effect of preventing noise samples with too low a probability to be extracted from candidates. Top K sampling uses only the top k candidates from the highest probability for sampling. In temperature sampling, temperature scaling is applied to the softmax to calculate the next token logits. When temperature T is low, the possibility of selecting a high-probability candidate group increases, and the possibility of choosing low-probability candidates decreases. It has the effect of increasing the probability of more static responses. Conversely, when the temperature T is large, the chance of choosing among the high-probability candidates decreases, and the low-probability candidates increases. It has the effect of increasing the possibility of making more diverse responses."}, {"title": "D Selection Strategy", "content": "Removing noisy augmentations via acceptence threshold. Using the distance D, we expect to select a VA that shows high-performance improvement when used on CD. However, there may exist cases where some VAs cannot be appropriate contrastive augmentation for a specific task overall. In this case, these VAs contribute less to performance improvement than other VAs on average and can sometimes become noise that prevents other VAs from being used as contrast. We use the Acceptance Threshold, a simple baseline that eliminates the noise VAs. To discover the suitableness of VAs for the target task, in the sample sub-dataset, we utilize the LVLM's first token generation distance by VACoDe for each VA. Let ci be the number of times that VA\u017c selected as contrast VA among a total of M VAs. For the N data samples and acceptance threshold T, candidate VAs with Ci < TN TM are treated as unsuitable for this task and removed. Throughout the main experiments, we used the acceptance threshold of T = 0.5."}, {"title": "E Experiment Details", "content": "Experiment computation resource.\nIn this paper, all reported our experiment used LVLM models can run on a single 48 GB NVIDIA RTX A6000. In the process of applying VACoDe, our model requires inference as the number of VAs used in the first step only, and each subsequent generation step requires twice token generation stages."}]}