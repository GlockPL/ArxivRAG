{"title": "Enhancing Temporal Modeling of Video LLMs via Time Gating", "authors": ["Zi-Yuan Hu", "Yiwu Zhong", "Shijia Huang", "Michael R. Lyu", "Liwei Wang"], "abstract": "Video Large Language Models (Video LLMs) have achieved impressive performance on video-and-language tasks, such as video question answering. However, most existing Video LLMs neglect temporal information in video data, leading to struggles with temporal-aware video understanding. To address this gap, we propose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling through a novel Time Gating module (TG). The TG module employs a time gating mechanism on its sub-modules, comprising gating spatial attention, gating temporal attention, and gating MLP. This architecture enables our model to achieve a robust understanding of temporal information within videos. Extensive evaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass, and NEXT-QA) demonstrates that our TG-Vid model significantly outperforms the existing Video LLMs. Further, comprehensive ablation studies validate that the performance gains are attributed to the designs of our TG module.", "sections": [{"title": "1 Introduction", "content": "The advancement of Large Language Models (LLMS) (Touvron et al., 2023; Chiang et al., 2023) has greatly fueled multi-modal research, such as Image LLMs (Liu et al., 2024b; Bai et al., 2023; Dai et al., 2023; Liu et al., 2024a) which have achieved success on image-and-language downstream tasks (Goyal et al., 2017). Inspired by Image LLMs, many recent efforts manage to empower LLMs to understand video data (Maaz et al., 2023; Li et al., 2023c; Liu et al., 2024d). The typical architecture of these Video LLMs comprises a pretrained vision encoder (Radford et al., 2021; Sun et al., 2023), a pretrained LLM (Chiang et al., 2023), and a connection module in between (Zhu et al., 2023; Dai et al., 2023).\nDespite the impressive performance demonstrated by Video LLMs (Maaz et al., 2023; Xu et al., 2017; Yu et al., 2019; Jang et al., 2017), a recent study (Liu et al., 2024e) reveals that most Video LLMs perform comparably to, or even worse than, Image LLMs. This discrepancy arises because existing video benchmarks can often be adequately addressed by single-frame bias (Lei et al., 2022; Buch et al., 2022), without the need for capturing the temporal dynamics of videos. To better evaluate the temporal modeling capability, multiple temporal-sensitive benchmarks have been developed (Liu et al., 2024e; Li et al., 2023c; Xiao et al., 2021) that cannot be solved by simply relying on single-frame bias as a shortcut.\nIn this paper, we aim to enhance the temporal modeling ability of Video LLMs and evaluate our model on the temporal-sensitive benchmarks. Specifically, we propose a temporal-aware Video LLM (TG-Vid) in this work, featuring a novel Time Gating module (TG) to enhance temporal modeling. This TG module comprises three sub-modules, gating spatial attention, gating temporal attention, and gating MLP, simultaneously capturing spatial and temporal information. A recent relevant work ST-LLM (Liu et al., 2024d) also tries to enhance temporal modeling, by directly utilizing BT-Adapter (Liu et al., 2024c) which applies spatio-temporal attention in parallel to the vision encoder. In contrast, our work builds gating spatio-temporal attention on top of the vision encoder, and our gating mechanism imposes effective module-specific control over each sub-module of the TG module. As validated by experiments, our design achieves better performance on temporal-sensitive benchmarks.\nWe conduct comprehensive experiments on three temporal-sensitive video benchmarks (i.e., MVBench (Li et al., 2023c), TempCompass (Liu"}, {"title": "2 Related Work", "content": "Video Large Language Models. Benefited from the reasoning power of large language models (LLMs) (Zhang et al., 2022; Brown et al., 2020; Touvron et al., 2023; Chiang et al., 2023; Zhao et al., 2023), Video LLMs (Li et al., 2023b; Maaz et al., 2023; Zhang et al., 2023; Lin et al., 2023; Tang et al., 2023; Ren et al., 2024; Wang et al., 2024c; Tan et al., 2024) have shown impressive performance on video-and-language tasks, such as video question answering (Xu et al., 2017; Jang et al., 2017; Yu et al., 2019; Maaz et al., 2023; Xiao et al., 2021). However, most existing Video LLMs inherit the design of Image LLMs (Zhu et al., 2023; Liu et al., 2024b; Dai et al., 2023) and overlook the temporal modeling that is critical for video data, leading to unsatisfactory capability on temporal-aware video understanding (Li et al., 2023c; Liu et al., 2024e). For example, TempCompass (Liu et al., 2024e) reveals that the temporal understanding ability of most Video LLMs is on par with or even weaker than Image LLMs. In this work, we propose a temporal-aware Video LLM, featuring a new architecture of time gating module to enhance video temporal modeling.\nVideo Temporal Modeling. Modeling temporal information has been a long-standing topic in video research. Early work utilizes 3D convolutional networks (CNNs) to achieve spatio-temporal video modeling (Carreira and Zisserman, 2017; Feichtenhofer et al., 2016; Tran et al., 2015). To reduce training costs, subsequent CNN-based models explore factorizing convolutions across spatial and temporal dimensions (Sun et al., 2015; Tran et al., 2019, 2018; Xie et al., 2018; Feichtenhofer, 2020). Further, by leveraging the superiority of Transformer in processing sequences, TimesFormer (Bertasius et al., 2021) and ViViT (Arnab et al., 2021) employ Transformer-based architectures to enhance spatio-temporal modeling via spatial and temporal attention. Beyond single action, a line of work seeks to learn the temporal ordering of actions in procedural activities (Bojanowski et al., 2014; Chang et al., 2020; Zhao et al., 2022; Zhong et al., 2023). More recently, pretrained image-language models (Radford et al., 2021) are transferred to video tasks (Ni et al., 2022; Pan et al., 2022; Luo et al., 2022; Fang et al., 2021; Liu et al., 2024c), such as action recognition and video retrieval. Unlike these works, we extend the idea of spatio-temporal attention to Video LLMs, targeting at temporal-sensitive VideoQA and filling the gap of video modeling in Video LLMs."}, {"title": "3 Methodology", "content": "In this section, we introduce our Time Gating Video LLM (TG-Vid). Fig. 1 provides an overview of our model architecture. To enhance the temporal modeling of a Video LLM (comprising an LLM, a vision encoder, and a connection module), we propose a Time Gating (TG) module with a novel module-specific time gating mechanism."}, {"title": "3.1 Preliminary", "content": "Given a video input with T frames, a pretrained vision encoder (Sun et al., 2023) extracts patch embeddings for each frame and concatenates them into video embeddings $V \\in \\mathbb{R}^{T \\times L_v \\times D_v}$, where $L_v$ denotes the number of patch embeddings in each frame and $D_v$ denotes the dimension of patch embeddings. On the other side, given the text input, we employ the text embedder of a pretrained LLM (Chiang et al., 2023) to obtain the text embeddings $T \\in \\mathbb{R}^{L_T \\times D_T}$, where $L_T$ denotes the number of text tokens and $D_T$ denotes the dimension of the text embeddings. This video and text encoding process is common in Video LLMs methods (Li et al., 2023c; Wang et al., 2024b; Liu et al., 2024d).\nOur model design extends spatio-temporal attention from ViViT (Arnab et al., 2021) and TimesFormer (Bertasius et al., 2021). We provide the background knowledge of spatio-temporal attention. For clarity, we first formulate the vanilla N-layer Spatio-Temporal module (ST), which is placed between the vision encoder and the QFormer. Each ST layer comprises a spatial attention, a temporal attention, and a two-layer MLP. Given the input $V^l \\in \\mathbb{R}^{T \\times L_v \\times D_v}$, the l-th layer of ST ($V^0$ is set as V) can be formulated as:\n\\begin{align}\nV^l_S &= \\text{ReshapeS}(V^{l-1}) \\tag{1}\\\\\nY^l_S &= \\text{MSA}(\\text{LN}(V^l_S)) + V^l_S \\tag{2}\\\\\nV^l_T &= \\text{ReshapeT}(Y^l_S) \\tag{3}\\\\\nY^l_T &= \\text{MSA}(\\text{LN}(V^l_T)) + V^l_T \\tag{4}\\\\\nV^l_M &= \\text{ReshapeM}(Y^l_T) \\tag{5}\\\\\nV^{l+1} = Y^l_M &= \\text{MLP}(\\text{LN}(V^l_M))+V^l_M \\tag{6}\n\\end{align}\nwhere LN(\u00b7) denotes layer normalization (Ba et al., 2016), MSA(\u00b7) denotes multi-head self-attention, and MLP(\u00b7) denotes a two-layer MLP. ReshapeS(\u00b7) reshapes $V^{l-1} \\in \\mathbb{R}^{T \\times L_v \\times D_v}$ as $V_S^l \\in \\mathbb{R}^{T \\times L_v \\cdot D_v}$, ReshapeT(\u00b7) reshapes $Y_S^l \\in \\mathbb{R}^{T \\times L_v \\cdot D_v}$ as $V_T^l \\in \\mathbb{R}^{L_v \\times T \\cdot D_v}$, and ReshapeM(\u00b7) reshapes $Y_T^l \\in \\mathbb{R}^{L_v \\times T \\cdot D_v}$ as $V_M^l \\in \\mathbb{R}^{T \\times L_v \\times D_v}$."}, {"title": "3.2 Time Gating Module (TG)", "content": "The vanilla ST module can model the spatio-temporal information in video inputs. However, directly inserting a randomly initialized ST module into Video LLM results in unstable training and sub-optimal performance. To address this issue, we propose a novel Time Gating Module (TG), featuring a time gating mechanism to impose constraints on each sub-module (i.e., a gating spatial attention, a gating temporal attention, and a gating MLP) of the TG module. These gating sub-modules allow our TG to focus dynamically on relevant information in both spatial and temporal aspects, enhancing the temporal modeling ability of Video LLM.\nUnlike previous research works (Sung et al., 2022; Liu et al., 2024c) that utilize gating mechanism conditioned solely on a trainable but module-agnostic scalar (e.g., $a \\in \\mathbb{R}^{1}$) or vector (e.g., $b \\in \\mathbb{R}^{D_v}$), the gating function $Gating(\\cdot)$ in our TG is module-specific and conditioned on both the input and output of the sub-module. Specifically, gating spatial attention is implemented as:\n\\begin{align}\n\\hat{Y}^l_S &= \\text{MSA}(\\text{LN}(V^l_S)) \\\\\nY^l_S &= \\text{Gating}(V^l_S, \\hat{Y}^l_S) + V^l_S \\nonumber\\\\\n&= \\sigma(\\text{Cat}(V^l_S, \\hat{Y}^l_S)W_S) \\odot \\hat{Y}^l_S + V^l_S \\tag{7}\n\\end{align}\nwhere $\\sigma(\\cdot)$ is a sigmoid function, Cat(\u00b7) denotes concatenate operation, $W_S \\in \\mathbb{R}^{2 \\cdot D_v \\times D_v}$ is a linear projection, and $\\odot$ denotes element-wise product. Similarly, gating temporal attention and gating MLP are implemented as follows:\n\\begin{align}\n\\hat{Y}^l_T &= \\text{MSA}(\\text{LN}(V^l_T)) \\\\\nY^l_T &= \\sigma(\\text{Cat}(V^l_T, \\hat{Y}^l_T)W_T) \\odot \\hat{Y}^l_T + V^l_T \\tag{8}\\\\\n\\hat{Y}^l_M &= \\text{MLP}(\\text{LN}(V^l_M)) \\\\\nY^l_M &= \\sigma(\\text{Cat}(V^l_M, \\hat{Y}^l_M)W_M) \\odot \\hat{Y}^l_M + V^l_M \\tag{9}\n\\end{align}\nwhere $W_T \\in \\mathbb{R}^{2 \\cdot D_v \\times D_v}$ and $W_M \\in \\mathbb{R}^{2 \\cdot D_v \\times D_v}$."}, {"title": "3.3 Time Gating Video LLM", "content": "By inserting the N-layer TG module between the frozen vision encoder and the frozen QFormer, we propose TG-Vid, a Time Gating Video LLM. The output video embeddings of the pretrained QFormer are flattened as $V_Q \\in \\mathbb{R}^{T \\cdot L_q \\times D_v}$, where $L_q$ denotes the length of query tokens for each frame. Subsequently, $V_Q$ is projected into the text embedding space and concatenated with the text embedding T as follows:\n\\begin{equation}\nVT = [V_QW_{VT}, T] \\tag{10}\n\\end{equation}\nwhere $W_{VT} \\in \\mathbb{R}^{D_v \\times D_T}$ is a trainable linear projection, and $VT \\in \\mathbb{R}^{(T \\cdot L_q+L_T) \\times D_T}$ is the input into the LLM. Same as previous Video LLMs, our TG-Vid model is trained on next token prediction."}, {"title": "4 Experiments", "content": "Compared with existing Video LLMs, we evaluate our TG-Vid on three temporal-sensitive video understanding benchmarks (i.e., MVBench (Li et al., 2023c), TempCompass (Liu et al., 2024e), and NEXT-QA (Xiao et al., 2021; Buch et al., 2022)).\nMore details of datasets, implementation, experiment results and visualization are provided in the Appendix."}, {"title": "4.1 Main Results", "content": "Tab. 1, Tab. 2, and Tab. 3 show our main results on MVBench, TempCompass, and NEXT-QA, respectively. Our TG-Vid model achieves the best performance and surpasses previous methods by a large margin across all benchmarks. For example, compared with the closest competitor ST-LLM, our TG-Vid-220K achieves +1.5 on MVBench, +2.1 on TempCompass, +3.2 on NEXT-QA ATP-hard, and +3.2 on NEXT-QA Val. These impressive results demonstrate a consistent finding that our TG-Vid model can capture temporal information more effectively, attributed to the TG designs."}, {"title": "4.2 Ablation Studies", "content": "Given the comparable performance of TG-Vid-220K and TG-Vid-197K, the ablation studies are based on the latter for efficiency consideration.\nTG Module. In Tab. 4, row 3 significantly outperforms row 1 by a large margin (+3.0), demonstrating the effectiveness of our TG module in empowering temporal-aware video understanding.\nTime Gating Mechanism. Row 3 significantly surpasses row 2 (+1.5), underscoring the crucial role of the time gating mechanism in enhancing video temporal modeling.\nTG Components. The results in Tab. 4 indicate that each sub-module of TG module contributes to performance improvement. Notably, the proposed gating temporal attention provides the most significant enhancement (from 54.7 to 56.0), further validating the necessity of temporal modeling."}, {"title": "5 Conclusion", "content": "In this paper, we focus on developing a Video LLM, TG-Vid, to overcome the struggles of the existing Video LLMs in temporal-aware video understanding. Specifically, we propose a novel Time Gating module (TG) with a time gating mechanism, to enhance the temporal modeling ability of TG-Vid. Comprehensive experiments and ablation studies conducted on three temporal-sensitive benchmarks (i.e., MVBench, TempCompas, and NExT-QA) indicate that TG-Vid outperforms the existing Video LLMs by a large margin. These results demonstrate the effectiveness of our TG design in enhancing temporal modeling, thereby empowering our TG-Vid with a strong ability of temporal-aware video understanding.\nLimitations. Our proposed TG-Vid model has achieved strong performance on the temporalsensitive video understanding benchmarks. However, there are still some limitations: (1) Despite that our TG module can significantly enhance the temporal modeling of the Video LLM, integrating it into Video LLM requires additional computation; (2) Similar to the existing Video LLMs, our TG-Vid model has the potential to inherit the undesired biases from the training dataset and the pretrained LLMs; (3) The focus of this work is on temporal modeling. Whether the proposed TG-Vid model and the TG module can be generalized to other video-and-language tasks, such as long video understanding, is worth exploring in future research."}, {"title": "Appendix", "content": "In the appendix, we provide more details of: (1) the statistics of the training dataset; (2) the implementation details and hyper-parameters for training; (3) additional ablation study on the number of layers of the proposed TG module; (4) additional ablation study on the training output format; (5) additional ablation study on the designs of TG module; (6) the comprehensive results of MVBench (comprising twenty sub-tasks) and TempCompass (comprising three sub-tasks); (7) the visualization of time gating."}, {"title": "A Training Dataset Statistics", "content": "To ensure a fair comparison with the state-of-the-art Video LLM, ST-LLM (Liu et al., 2024d), our TG-Vid-220K model utilizes the same training dataset as ST-LLM, as detailed in Tab. 5. For the training dataset of our TG-Vid-197K model, we filter out the Conversation-VideoChatGPT and VQAWebVidQA datasets to improve training efficiency. Moreover, the training dataset with 220K video-text pairs is also a subset of the training dataset of VideoChat2 (Li et al., 2023c), which contains 1.9M video-text pairs."}, {"title": "B Implementation Details", "content": "Following ST-LLM, we adopt the Vicuna-7B-v1.1 (Chiang et al., 2023) as our pretrained LLM and the EVA-ViT-g (Sun et al., 2023) as our pretrained vision encoder. The QFormer is also initialized from the pretrained InstructBLIP (Dai et al., 2023), while our TG module is randomly initialized. Following the designs of LLaMA (Touvron et al., 2023), the self-attention inside the TG module is implemented as self-attention with rotary position embeddings (RoPE) (Su et al., 2024). Similarly, the activation function of MLP inside the TG module is implemented as the SwiGLU activation function. Our TG-Vid model is subsequently trained on the video instruction tuning dataset, which is described in Appendix A.\nDuring training, the vision encoder and the QFormer are frozen, while other modules of TGVid are trainable. The training of TG-Vid-197K costs about 7 hours and the training of TG-Vid220K costs about 13 hours. Both of these trainings are conducted on 8 A100 GPUs (each GPU has 80G memory)."}, {"title": "C Number of Layers of TG", "content": "As shown in Fig. 2, we ablate the depth of the TG module. The results reveal that all of the models with the TG module significantly surpass the model without the TG module, demonstrating the effectiveness of our proposed TG module in empowering temporal-aware video understanding. Moreover, the results also indicate that the 3-layer TG module achieve the best performance. Therefore, we use N = 3 in Tab. 6 by default."}, {"title": "D Training Output Format", "content": "To improve the training efficiency for LLM decoding, we explore modifying the training output format. The original output format form VideoChat2-IT (Li et al., 2023c) is \u201c(A) chase the dog.\", while ours is modified as a direct output format: \u201cA\u201d. Following the instruction used in LLaVA1.5 (Liu et al., 2024a), we also add an instruction \u201cAnswer with the option's letter from the given choices directly.\" in the text input.\nAs shown in Tab. 7, the performance after modification is slightly decreased but the performance is still comparable. Therefore, we utilize the direct output format as our training output format for efficiency consideration."}, {"title": "E Designs Borrowed from LLaMA", "content": "As mentioned in Appendix B, we borrow two designs (i.e., RoPE and SwiGLU) from LLaMA into the implementation of the TG module. In this section, we ablate these designs, as shown in Tab. 8. The results demonstrate that these designs can slightly improve the model performance. Therefore, we introduce these designs into the implementation of our TG module."}, {"title": "F Comprehensive Results of MVBench and TempCompass.", "content": "Due to the space limitation of the main paper, we present the comprehensive results of MVBench (comprising twenty sub-tasks) and TempCompass in (comprising three sub-tasks) in Tab. 9 and Tab. 10, respectively."}, {"title": "G Visualization of Time Gating.", "content": "To gain more insight into how time gating works, we provide some visualizations in xx. To be specific, we visualize the heatmap of the gate values produced by our time gating mechanism, based on the following steps:\n\u2022 Step 1: Randomly select an input sample (with a T-frame video).\n\u2022 Step 2: For the input sample, obtain its gate values of the gating temporal attention in the first layer of TG (i.e., the result of the sigmoid function in Equation (8)), denoted as $G \\in \\mathbb{R}^{L_v \\times T \\cdot D_v}$. $L_v$ denotes the number of patch embeddings in each frame and $D_v$ denotes the dimension of patch embeddings.\n\u2022 Step 3: Visualize the gate values. Specifically, reshape G as $G \\in \\mathbb{R}^{L_v \\times T \\times D_v}$ and adopt averagepooling along the $D_v$ dimension to obtain $\\hat{G} \\in \\mathbb{R}^{L_v \\times T}$. Finally, visualize $\\hat{G}$ with a heatmap visualization H.\nTo conduct a detailed analysis, we repeated steps 1-3 twice. Given two different input samples from MVBench, SampleA (with a $T_A$-frame video) and SampleB (with a $T_B$-frame video), we obtain $G_A$ and $G_B$, and visualize the corresponding heatmaps $H_A$ and $H_B$. Our observations are as follows:\n\u2022 For videos with different durations (i.e., different numbers of frames), the shape of G can adapt accordingly ($\\hat{G}_A \\in \\mathbb{R}^{L_v \\times T_A}$ and $\\hat{G}_B \\in \\mathbb{R}^{L_v \\times T_B}$). Therefore, for different input samples, the time gating mechanism can adapt to the content of specific input and performs temporal-sensitive control, thereby enhancing the temporal modeling ability of the model.\n\u2022 For all patch embeddings at the same time (i.e., the same frame), the corresponding gate values of $\\hat{G}$ change dynamically, revealing that the time gating mechanism can discern the information at different spatial locations and provides dynamic, fine-grained control.\n\u2022 For all patch embeddings at the same spatial location, the corresponding gate values of $\\hat{G}$ also change dynamically, which demonstrates that the time gating mechanism can also distinguish the temporal dynamics and provide fine-grained control."}]}