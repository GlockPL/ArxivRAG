{"title": "Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review", "authors": ["Di Wu", "Xian Wei", "Guang Chen", "Hao Shen", "Xiangfeng Wang", "Wenhao Li", "Bo Jin"], "abstract": "Embodied multi-agent systems (EMAS) have attracted growing attention for their potential to address complex, real-world challenges in areas such as logistics and robotics. Recent advances in foundation models pave the way for generative agents capable of richer communication and adaptive problem-solving. This survey provides a systematic examination of how EMAS can benefit from these generative capabilities. We propose a taxonomy that categorizes EMAS by system architectures and embodiment modalities, emphasizing how collaboration spans both physical and virtual contexts. Central building blocks, perception, planning, communication, and feedback, are then analyzed to illustrate how generative techniques bolster system robustness and flexibility. Through concrete examples, we demonstrate the transformative effects of integrating foundation models into embodied, multi-agent frameworks. Finally, we discuss challenges and future directions, underlining the significant promise of EMAS to reshape the landscape of AI-driven collaboration.", "sections": [{"title": "Introduction", "content": "Embodied multi-agent systems (EMAS) have garnered growing interest due to their significant potential in domains such as smart transportation, logistics, and manufacturing [YJ+13, IS+18]. By integrating physical embodiments ranging from autonomous vehicles to robotic manipulators with multi-agent systems (MAS) [DKJ18], EMAS offers a decentralized, collaborative approach that can handle complex tasks with remarkable efficiency. Despite these advantages, designing and implementing effective EMAS remains a non-trivial endeavor, often requiring specialized knowledge of cybernetics, extensive training data, and careful reinforcement-learning paradigms [LB08, OD23].\nIn traditional MAS, agents collaborate by dividing responsibilities, sharing state information, and collectively adapting to dynamic environments [DKJ18]. While these principles have led to impressive success in certain niches, conventional approaches face critical limitations in generalizing to new tasks [MS+22], scaling to large agent populations [CTE+22], and coping with unexpected environmental changes [WR04]. They often rely on narrowly trained models, which can be brittle or constrained to specific domains [YZ+23]. These shortfalls underscore the urgency for more flexible and robust solutions that can thrive in open-ended and rapidly changing embodied scenarios.\nRecent breakthroughs in foundation models (FMs, e.g., large language models, FMs, or vision-language models, VLMs) [ZLL+24] have opened new avenues for advancing MAS toward more adaptive and generative behaviors. By equipping agents with natural language capabilities, contextual reasoning, and the ability to generate novel solutions, FM-based MAS transcend some of the limitations inherent in purely signal-driven or reinforcement-based frameworks [GCW+24, CLH+24, LP+24] .\nThese \"generative agents\" can communicate in semantically rich ways, collaborate with human-level fluency, and rapidly adapt strategies in response to unforeseen challenges. As such, FM-powered agents could transform how multi-agent collaboration unfolds both in physical spaces populated by embodied devices and in virtual spaces where agents share abstract knowledge and tasks.\nAgainst this backdrop, the field of EMAS stands poised to benefit from these recent advances in FMs. By combining physical embodiments with generative multimodal intelligence, future systems may embrace a broader design space that incorporates complex perception, high-level linguistic and visual reasoning, and adaptive decision-making. However, existing literature surveys on embodied AI and multi-agent systems often treat these fields in isolation, leaving critical gaps at their intersection [IS+18, DYT+22, GCW+24, MS+24, HRS24]. A systematic view of how FM-based generative agents can best be integrated into EMAS is still emerging.\nThis survey aims to provide a comprehensive and structured examination of the current state of generative multi-agent collaboration in Embodied AI, as shown in Figure 1. First, we introduce a taxonomy that classifies existing EMAS solutions based on the number of models and types of embodiments in Section 2, highlighting how collaboration can arise both among physical agents and in purely virtual semantic environments. Next, we explore the major building blocks of multi-agent collaboration\u2014system perception, planning, communication, and feedback and examine how each of these components can be designed to leverage FM-based generative capabilities in Section 3. Moving beyond theoretical perspectives, we delve into practical applications in Section 4, illustrating how generative multi-agent collaboration enhances functionality across diverse embodied scenarios.\nTo the best of our knowledge, this is the first survey to systematically address the convergence"}, {"title": "Collaborative Architectures", "content": "Building upon the key challenges and opportunities outlined in the previous section, this section presents an overview of collaborative architectures in EMAS, as shown in Figure 2. In particular, we address how generative multi-agent systems leverage either extrinsic collaboration across multiple embodied entities or intrinsic collaboration among multiple FMs within a single embodied entity. We also cover hybrid approaches that combine these strategies to meet diverse system requirements. The goal is to provide a structured understanding of how multi-agent collaborations can be orchestrated to maximize adaptability, scalability, and task alignment, especially when integrated with FMs."}, {"title": "Extrinsic Collaboration", "content": "In scenarios where collaboration unfolds among multiple embodied entities, known as extrinsic collaboration, agents physically or virtually interact to accomplish shared objectives. Drawing from the longstanding multi-robot and conventional MAS literature, extrinsic collaboration can be organized using either a centralized or a decentralized strategy. These approaches offer differing trade-offs related to scalability, communication overhead, and global versus local control."}, {"title": "Centralized Architecture", "content": "In a centralized policy framework, a single unified model controls multiple robots or agents, offering centralized task allocation and decision-making. The centralized model assigns tasks based on agent capabilities and system objectives, ensuring coordination across agents by providing a global perspective. Studies have explored language-based [LTW+24, OA+24, CYZ+24] and code-based [KVM24, ZQW+24] task allocation.\nThe centralized model also plays a key role in decision-making by synthesizing information from all agents to make final decisions, ensuring coherence. For example, [YKC23] uses a centralized model for navigation target determination, [TXL+20] uses it for interactive question answering with a 3D-CNN-LSTM QA model, and [GAZ+24] employs it for deadlock resolution in multi-robot systems by guiding a leader robot's actions.\nThe centralized control strategy ensures coordination by using a single model for task allocation and decision-making. Its advantages include optimal task distribution and consistent decisions. However, it can be limited by system complexity, high computational demands, and scalability issues in large or dynamic environments."}, {"title": "Decentralized Architecture", "content": "In a decentralized strategy, each model independently controls its corresponding embodied entity, providing greater flexibility and scalability. Early studies used reinforcement learning for decentralized control, but the rise of FMs has enabled agents to handle diverse tasks autonomously [CJ+24], forming more advanced decentralized systems.\nFMs enhance decentralized systems by leveraging the reasoning capabilities to improve individual decision-making based on local partial observations. For example, [ZWL+24] utilizes a world model to assist multi-agent planning, where each individual predicts the behavior of other agents through the world model and infers its own plan. Similarly, [AF+23] introduces an auxiliary theory-of-mind reasoning FM to interpret the actions and needs of partner agents, thereby supporting individual decision-making.\nFurthermore, with the reasoning and communication capabilities of FMs, FM-based agents exhibit emergent sociality. [CJ+23] reveals that when not explicitly instructed on which strategy to adopt, FM-driven agents primarily follow the average strategy, representing an egalitarian organizational structure among agents. Other research [GHL+24, CJ+24] highlight the potential benefits of more structured roles within the team. This suggests that, similar to human social structures, FM agents can exhibit emergent behaviors that optimize collaboration by adapting to organizational frameworks, enhancing their collective ability to tackle complex tasks."}, {"title": "Intrinsic Collaboration", "content": "While extrinsic collaboration deals with multiple robots and embodied entities, intrinsic collaboration occurs within the internal structure of a single system that may contain multiple FMs. This concept resonates with the recent push for collaborative workflows among various FM modules, each specializing in different roles, to jointly handle increasingly complex tasks. Such internal orchestration expands traditional notions of multi-agent coordination by focusing on consolidated decision-making within a single embodiment.\nEach FM in this workflow assumes a specific function or role to collaboratively complete a task. Research has applied this paradigm to embodied learning systems, such as [QZL+24], which uses modules like planner, partoller, and performer for task-solving in a Minecraft sandbox, and [SSY+24], which decomposes tasks into observer, planner, and executor roles. LLaMAR [NO+24] also employs a plan-act-correct-verify framework for self-correction without oracles or simulators. Intrinsic collaboration can improve system functionality by enhancing planning accuracy, safety, and adaptability. For example, [LY+23] uses FM-based fast-mind and slow-mind for collaborative plan generation and evaluation, while LLaMAC [ZMR+23] employs multiple critics and an assessor to provide feedback and improve robustness."}, {"title": "Hybrid Collaboration Architectures", "content": "In many real-world applications, drawing a strict boundary between extrinsic and intrinsic collaboration is neither practical nor advantageous. Instead, hybrid collaborative architectures combine these strategies to exploit the strengths of centralized, decentralized, and internal FM workflows. As embodied tasks grow in complexity, the flexibility to mix different levels of collaboration, both among robots and within an agent's internal structure, becomes increasingly valuable.\nIntrinsic collaboration enhances model capabilities through modular FMs and can be applied in centralized and decentralized systems. For example, CoELA [ZDS+24] uses five modules-perception, memory, communication, planning, and execution\u2014while [YPY+24] builds agents with observation, memory, and planning modules for decentralized robot collaboration. Centralized models can also use modular FMs, such as [WTL+24], which employs a task- and action-FM for task assignment.\nCentralized and decentralized strategies can be combined, with different stages of a task utilizing different approaches. Inspired by the centralized training with decentralized execution (CTDE) framework in multi-agent reinforcement learning (MARL), [CYZ+24] and [ZC+24] propose centralized planning with decentralized execution, where global planning guides task execution, maximizing the synergy between global oversight and local autonomy.\nBy showcasing these varying architectures, we illustrate how practitioners can effectively orchestrate multi-agent collaboration in EMAS across different levels of granularity and control. The next section builds on this architectural perspective by examining how key system components-perception, planning, communication, and feedback-can be designed to leverage FM-based generative capabilities for more robust and adaptive multi-agent collaboration."}, {"title": "Advancing Collaborative Functionality", "content": "Building upon the architectural insights from Section 2, which examined how multi-agent collaboration can be orchestrated at the structural level, we now pivot to the functional building blocks that drive effective teamwork among embodied agents. Specifically, we highlight how perception, planning, communication, and feedback mechanisms can be designed to harness the generative capabilities of FMs. By focusing on these key modules, we illustrate how EMAS solutions can more robustly interpret the physical environment, formulate and adapt plans, exchange information, and iteratively learn from both their own behaviors and the environment itself. This approach complements the collaboration architectures introduced previously, offering a finer-grained perspective on enabling dynamic and context-aware cooperation among embodied agents."}, {"title": "Perception", "content": "Although a generative model may derive semantic knowledge from text and vision, embodied agents must actively sense and interpret the physical world. This entails handling 3D structures, dynamic conditions, and real-time interactions [LCB+24]. Consequently, the perception module is paramount, as it conveys detailed environmental features to subsequent models, ensuring that generative capabilities are grounded in tangible contexts [PH+24]."}, {"title": "Physical Perception for FM", "content": "The simplest means of providing physical context to an FM is to supply a verbal description of the environment. Although such prompts may be crafted manually, many approaches augment linguistic descriptions with automated tools. For instance, some studies [MJ+24, CZR+23] use visual models to detect and describe objects, while others [BCF+23, HW+23] employ affordance learning to enrich the FM's understanding of how objects can be operated upon in a physical setting. Beyond passively receiving information, recent work enables agents to decide when and what type of information to observe, facilitating active perception. For example, [QZL+24] allows the FM to query a fine-tuned model about environmental details; the responses inform a progressively constructed scene description."}, {"title": "Collaborative Perception", "content": "In multi-agent systems, collaborative perception aims to merge complementary sensory inputs from different agents, enhancing overall performance [YYZ+23]. Within autonomous driving or drone fleets, this often arises through sensor-level data sharing or output-level fusion [SRC24]. In FM-based systems, collaborative agents may collectively build a global memory of the environment by aggregating each agent's local maps or visual data. For instance, [YKC23] fuses the semantic maps derived from RGBD inputs of multiple agents, and [TXL+20] employs 3D reconstruction of each agent's observations to form a holistic 3D status and semantic memory of the shared environment."}, {"title": "Planning", "content": "Planning constitutes a core module of multi-agent embodied systems, enabling agents to strategize based on states, goals, and individual capabilities. Effective planning is crucial for task assignment, coordination, and seamlessly integrating the capabilities of generative FMs."}, {"title": "Planning format", "content": "Planning methods often employ either language-based or code-based formats. Language-based planning uses natural language to guide task flows, achieving intuitiveness and ease-of-adaptation, especially with the advent of advanced FMs [MJ+24, YKC23]. By contrast, code-based methods utilize structured programming or domain-specific notations (e.g., PDDL) for higher precision. [KVM24] uses Python code to frame overall task flow, and [ZQW+24] converts tasks into PDDL problems for allocation to multiple robots."}, {"title": "Planning process", "content": "Beyond individual decision-making, multi-agent collaboration demands consensus-building, conflict resolution, and resource sharing. In centralized systems, a single model frequently allocates sub-tasks. For example, [LTW+24] generates action lists based on each agent's capability, [OA+24] integrates FMs and linear programming to solve task partitions, while [CYZ+24] exploits \"robot resumes\" for FM-based discussions around task assignment. In decentralized systems, agents communicate directly to optimize their collective plans, supported by robust information exchanges that will be explored in the following subsection."}, {"title": "Communication", "content": "Communication is central to MAS, enabling agents to share situational updates, coordinate tasks, and reach consensus. Unlike traditional approaches that require painstaking communication protocol design, generative agents can exploit the zero-shot language generation abilities of FMs, reducing the complexity of building efficient communication interfaces.\nFollowing [SWJ+22], we categorize multi-generative-agents communication patterns in embodied AI to three main structures:\n\u2022 Star: A virtual central agent controls the flow of messages, broadcasting plans or directives to other agents.Much work with centralized architecture has explored this approach [KVM24, YKC23]\n\u2022 Fully Connected (FC): Every agent communicates freely with every other agent, leveraging FM-driven messages. For instance, [MJ+24] uses inter-FM dialogues between two robotic arms to coordinate manipulation tasks. In CoELA [ZDS+24], each agent references current state information via memory retrieval, generating communication content through an FM.\n\u2022 Hierarchical: A leadership structure emerges to boost scalability and reduce communication overhead. [CJ+24, LYZ+24, GHL+24] show how leadership roles channel or filter communications, improving efficiency and outcomes."}, {"title": "Feedback", "content": "Embodied tasks are complex and uncertain, making feedback mechanisms essential for agent improvement. Feedback enables agents to adjust and optimize behavior, allowing continuous learning based on the current state, environmental changes, or external guidance."}, {"title": "System Feedback", "content": "System feedback refers to information generated internally before an action is taken. This involves agents or a centralized model revisiting their initial plans to identify flaws or potential improvements. Several works [LZD+24, CYZ+24, ZMR+23] implement a multi-agent discussion phase post-plan generation, refining action lists through peer feedback. [CAD+24] and [ZQW+24] employ FM checkers to validate code-based plans, ensuring syntactic correctness. Meanwhile, [ZYB+24] devises advantage functions to evaluate and iteratively refine plans, and [LY+23] applies an FM to predict plan consequences, followed by another FM that rates plan quality, thus driving iterative enhancements."}, {"title": "Environmental Feedback", "content": "Environmental feedback surfaces after executing actions in the physical (or simulated) world. Many studies log real-world outcomes to guide future decisions. For example, [LTW+24] and [YPY+24] store action results in memory for future planning references, whereas [QZL+24] and [NO+24] assess the root cause of failures and adapt their action plans accordingly. Additionally, multi-agent organizational structures can be reconfigured mid-task in response to environmental signals. [CSZ+23] dynamically updates the roles, and [GHL+24] employs a critic FM to evaluate agent performance, even reorganizing leadership."}, {"title": "Human Feedback", "content": "External human guidance can offer nuanced interventions and strategic directions unattainable through purely automated systems. For instance, [PL+23] identifies ambiguous or infeasible task instructions warranting human assistance, while [WHK24] and [RDB+23] integrate conformal prediction to measure task uncertainty and trigger human help requests. Beyond soliciting assistance, [CK+23] and [SH+24] permit human operators to refine on-the-fly robot actions through spoken instructions, improving task success rates.\nIn sum, perception, planning, communication, and feedback emerge as foundational pillars for translating high-level collaborative architectures into practical, generative multi-agent solutions. Whether agents collaborate extrinsically through distributed configurations or intrinsically via multiple FMs within a single embodied, robust supporting modules ensure adaptability and resilience in real-world settings.\nThe next section delves into concrete application domains, illustrating how these functional modules synergize to tackle diverse embodied tasks. By bridging architectural principles (Section 2) with modular functionalities and grounding them in real-world scenarios, we aim to offer a comprehensive view of how generative multi-agent collaboration can be effectively realized in EMAS."}, {"title": "Downstream Tasks: From Simulation to Real-World Deployment", "content": "Building on the architectures and functional modules, this section examines how generative multi-agent collaboration moves from controlled simulation environments to real-world applications. Although many advances are validated through virtual platforms, these simulation insights lay the groundwork for tackling the complexities of intelligent transportation, household robotics, and embodied question answering."}, {"title": "Simulation Platforms", "content": "Earlier sections introduced how multi-agent collaboration can be structured and functionally enabled. Simulation environments now enter as a crucial layer for testing these designs, allowing researchers to systematically refine agent interactions without incurring real-world operational costs or risks."}, {"title": "Grid-World Paradigms", "content": "Grid worlds feature cell-based structures that focus on decision-making and path planning while abstracting away physical details. By adopting an FM-based translator-and-checker framework, [CAD+24] improves multi-agent performance on grid tasks, while [ZMR+23] introduces feedback mechanisms to enhance grid transportation. [CAZ+24] further evaluates various FM-driven multi-robot architectures in a grid setup, underscoring how these simplified worlds facilitate quick validation of collaborative designs."}, {"title": "Game-Based Collaboration Scenarios", "content": "Game-based platforms like Overcooked provide clear rules, time constraints, and forced coordination among agents [YJ+24, AF+23, ZYB+24]. FM-coordination extends to other structured games such as Hanabi and Collab Games, showcasing that generative approaches are adaptable to diverse team-based challenges. For more open-ended tasks, Minecraft [WXJ+23, PC+24] pushes the envelope with larger environments and indefinite goals. Recent work [PC+24, ZC+24, QZL+24] focuses on collaborative exploration, while others [CJ+24, CSZ+23, ZMC+24] tackle resource collection or structure building."}, {"title": "Advanced 3D Environments and Robotic Simulations", "content": "Realistic simulators aim to mirror real-life complexity more closely. AI2-THOR [KM+17] offers meticulously rendered indoor scenes and is used for multi-agent household tasks [KVM24, WHK24, LLG+22, SSY+24]. Similarly, VirtualHome-Social [GHL+24], BEHAVIOR-1K [LTW+24], and Habitat-based benchmarks [CYZ+24] enable agents to develop collaborative strategies in object manipulation and navigation. Such platforms help bridge the gap between algorithmic development and physical deployment."}, {"title": "Emerging Applications", "content": "Armed with validated architectures and robust functional modules, researchers have begun to face the ultimate frontier: translating simulator learnings into viable physical deployments. From intelligent transportation to household robotics, the following subsections spotlight how generative multi-agent collaboration is being adapted to meet real-world demands, illustrating both the maturity and remaining challenges of these systems."}, {"title": "Intelligent Transportation and Delivery", "content": "Multi-agent collaboration in intelligent transportation covers UAV/UGV coordination for cargo delivery and environmental monitoring. Early approaches mainly leveraged MARL, but FM-driven solutions are now emerging. [GW+24] explores FM-based initial task allocation for surveillance missions, and [WTL+24] applies generative models to assign tracking targets, suggesting that language-guided strategies can adapt swiftly to dynamic scenarios."}, {"title": "Household Assistance Robotics", "content": "Many 3D simulation benchmarks, including AI2-THOR and Habitat, were originally crafted to emulate domestic environments. Domestic tasks such as \"clearing the table\" or following instructions like \"Turn on the desk and floor lamp and watch TV\" demand robust perception, planning, and communication. Studies [KVM24, WHK24, LGZL24, MJ+24, ZYB+24] demonstrate how multiple agents can share roles, interpret commands, and divide complex tasks. Generative models further streamline coordination, enabling adaptive task assignment and richer human-robot interactions."}, {"title": "Beyond Exploration: Embodied Question Answering", "content": "Embodied Question Answering (EQA) involves active exploration and reasoning in 3D spaces. Unlike tasks that emphasize physical interactions, EQA focuses on gathering and interpreting information, often requiring an advanced understanding of spatial layouts, object relationships, or event histories. Multi-agent versions often leverage team-based sensing for global memory and consensus [TGG+23, TXL+20, PD+24]. [CZR+23] positions agents with specialized functions to contribute key information, showcasing how FM-driven collaboration can integrate observations into coherent answers.\nIn highlighting these simulation benchmarks and real-world applications, this section underscores a key trajectory in EMAS: leveraging structured testbeds for proof-of-concept, then transitioning solutions to high-stakes domains. Having established where and how generative multi-agent collaboration can be deployed, the subsequent sections will address remaining challenges and outline prospective frontiers for EMAS research."}, {"title": "Open Challenges and Future Trends", "content": "As the field of multi-agent collaboration in embodied AI systems is continuing to develop, there remain several open challenges and promising future directions. Despite the progress made, numerous real-world obstacles persist, limiting the application of embedied systems. This section identifies key challenges and outlines potential areas of exploration and innovation to address these issues."}, {"title": "Benchmarking and Evaluation", "content": "One major challenge is lacking standardized evaluation criteria. While significant strides have been made in benchmarking individual agents and single-agent systems, there is a notable gap for the evaluation of embodied multi-agent collaboration. Existing benchmarks focus on task-specific metrics, and fail to account for the complexity of interactions, coordination, and emergent behaviors, that arise in multi-agent settings. There is an urgent need for unified evaluation standards for the holistic performance, including factors such as scalability, adaptability, robustness, and collective intelligence. The development of benchmarks is crucial to ensure consistent across different domains, and enabling meaningful comparisons between various multi-agent frameworks."}, {"title": "Data Collection and Heterogeneity", "content": "Another challenge in multi-agent collaboration is the data scarcity and heterogeneity for embodied systems. Collecting large-scale, high-quality datas of different systems with diverse physical characteristics and capabilities is an arduous task. The variation in hardware, sensor, and environmental interactions leads to inconsistence, making it difficult to generalize across systems and tasks. The real-world data available could be limited, hindering training and evaluating effectively. Additionally, most works in multi-agent collaboration are conducted in simulated environments, due to practical constraints, and only a few studies employ real-world data. Hence, there is a pressing need for standardized data collection, as well as innovative methods to transfer between simulation and real-world applications, to bridge the gap between theory and reality."}, {"title": "Foundation Models for Embodied AI", "content": "The development of foundation models, particularly for embodied agents, is poised to be a transformative breakthrough in the field of multi-agent collaboration. Currently, generative agents primarily rely on FMs to perform complex tasks, and naturally the next step is to build specifical foundational models designed for embodied systems. These models serve as a core framework for multi-agent collaboration, integrating perception, decision-making, and action. Recent works, such as RT-1 [BB+22] and RDT [LW+24], made significant strides in robot foundation models for adaptable and scalable systems. The evolution of foundation models will lay the groundwork for more seamless multi-agent collaboration, enabling agents with comprehensive capabilities and teamworks in dynamic environments. However, challenges remain to extend single-agent FMs to multi-agent, requiring novel architectures and methodologies."}, {"title": "Scalability of Agents", "content": "Currently, numbers of agents involved in collaborative multi-agent systems remains small. Scaling up the number of agents will lead to the increased complexity and difficulty of computation, communication, coordination, task allocations, and resource management. Moreover, maintaining stability and robustness in large-scale multi-agent systems requires sophisticated orchestration and coordination techniques. Researches on scalable architecture, efficient communication protocol, and collaborative tactics will be essential to unlocking the full potential of large-scale embedied systems. The development to optimize agent workflows and patterns will be crucial for scaling up these systems in a resource-awareness manner."}, {"title": "Human-Centric Collaboration", "content": "The integration of robots into human-centered environments remains a critical topic. In many applications, multi-agent systems need to collaborate with not only each other but also human. Ensuring that robots can work seamlessly alongside humans in dynamic and unstructured environments requires the development of human-robot interaction (HRI) protocols that consider human cognitive capabilities, preferences, and limitations. Human-robot collaboration introduces additional challenges, such as safety, adaptability, and trustworthy. Researches on human-robot teamwork, shared autonomy, and intuitive interfaces will be vital for fostering productive and safe collaboration between humans and robots, particularly for healthcare, industrial automation, and service robots."}, {"title": "Theoretical Foundations and Interpretability", "content": "Current approaches of embodied multi-agent collaborations, particularly those involving FMs, often lack a solid theoretical foundation. While substantial progress has been made in developing practical systems, the understanding is very limited about the underlying principles and collective intelligence that emerges to govern agent interactions. A deeper theoretical exploration of the dynamic cooperation, including the roles of communication, coordination, and consensus, is essential for advancing the field. Furthermore, the reliability and interpretability of embedied multi-agent systems and models is critical, especially for safety-critical environments, such as automatic drive and smart railway."}, {"title": "Related Work", "content": "Although numerous surveys have examined embodied AI or MAS individually, few efforts have tackled the critical overlap between these fields, leaving significant knowledge gaps unaddressed. Early studies on embodied AI [HXJ+23, FTT+23, MS+24] focus on single-agent perception-action loops. They discuss autonomy and sensorimotor learning in depth, yet devote limited attention to collaborative paradigms. Similarly, [DYT+22] and [LCB+24] explore how agents interact with environments but still assume solitary agents with limited capacity for distributed teamwork.\nIn contrast, recent surveys on FM-driven multi-agent systems [GCW+24, CLH+24, LP+24] showcase promising results in semantic communication and emergent coordination, especially in virtual environments. However, these contributions remain detached from physical embodiment, where hardware constraints, sensor noise, and kinematic coordination pose significant challenges. Meanwhile, classic robotics surveys [IS+18, YJ+13] laid the groundwork for cooperative swarm behaviors but lack generative capabilities that facilitate role adaptation or zero-shot planning.\nSeveral specialized reviews provide partial bridges. For instance, [HRS24] advances language-based human-robot interaction, yet overlooks non-linguistic coordination crucial in industrial or warehouse settings. Likewise, [SHP24] integrates FMs with MARL but treats embodiment mostly as an implementation detail. Consequently, none of these views examine physical grounding, collaborative intelligence, and generative models under a unified lens.\nOur survey addresses this gap by synthesizing insights from three converging axes: (i) embodied AI's physical imperatives, (ii) multi-agent systems' collaborative intelligence, and (iii) generative models' adaptive reasoning. We propose a novel taxonomy that reconciles embodiment multiplicity (physical agents) with model multiplicity (virtual agents). Through case studies in cross-modal perception and emergent communication, we show how FMs overcome conventional multi-agent limitations in real-world embodied contexts. Finally, we identify underscored challenges, such as Sim2Real transfer for generative collectives, bridging the divide between robotics and FM-based coordination. Our work thus establishes conceptual foundations for a new generation of embodied systems, where physical constraints and generative collaboration progress in tandem."}, {"title": "Conclusion", "content": "This survey investigates a popular and potential research area, i.e. multi-agent collaboration in embodied systems, which focuses on how generative foundation models can be integrated into embodied multi-agent systems. We emphasize how FM-based generative agents facilitate dynamic collaboration and emergent intelligence, and systematically explore multi-agent collaboration architectures from both intrinsic and extrinsic perspectives, focusing on key technologies such as perception, planning, communication and feedback mechanisms. Various applications range from grid world exploration to household assistance in embodied scenarios are studied to demonstrate the potential of FM-based EMAS to address complex problems, and discuss the associated challenges and opportunities in this rapidly evolving field. We hope this survey can serve as a valuable lamp for researchers, practitioners, and stakeholders, that offers a comprehensive understanding of the current landscape and inspires more advanced and scalable solutions of dynamic seamless collaboration for embodied multi-agent AI."}]}