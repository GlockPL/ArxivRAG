{"title": "Building Trust in Black-box Optimization: A Comprehensive Framework for Explainability", "authors": ["Nazanin Nezami", "Hadis Anahideh"], "abstract": "Optimizing costly black-box functions within a constrained evaluation budget presents significant challenges in many real-world applications. Surrogate Optimization (SO) is a common resolution, yet its proprietary nature introduced by the complexity of surrogate models and the sampling core (e.g., acquisition functions) often leads to a lack of explainability and transparency. While existing literature has primarily concentrated on enhancing convergence to global optima, the practical interpretation of newly proposed strategies remains underexplored, especially in batch evaluation settings. In this paper, we propose Inclusive Explainability Metrics for Surrogate Optimization (IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the transparency, trustworthiness, and explainability of the SO approaches. Through these metrics, we provide both intermediate and post-hoc explanations to practitioners before and after performing expensive evaluations to gain trust. We consider four primary categories of metrics, each targeting a specific aspect of the SO process: Sampling Core Metrics, Batch Properties Metrics, Optimization Process Metrics, and Feature Importance. Our experimental evaluations demonstrate the significant potential of the proposed metrics across different benchmarks.", "sections": [{"title": "Introduction", "content": "Interpretability and reliability are foundational elements of any optimization approach (Carvalho, Pereira, and Cardoso 2019), serving as essential pillars upon which trust and acceptance are built. While explainability is a well-studied concept for machine learning (ML) models (Linardatos, Papastefanopoulos, and Kotsiantis 2020), the application of these principles to black-box optimization algorithms, such as Surrogate Optimization or Bayesian optimization (Rodemann et al. 2024), remains relatively unexplored. Black-box optimization (BBO) techniques aim to find the best parameter configurations or designs that optimize a target function, which is often unknown and costly to evaluate. The general idea is to sample the solution space strategically to discover promising areas.\n\nThe importance of BBO cannot be overstated. It is widely used in various fields such as engineering design (Chen, Chiu, and Fuge 2019), hyperparameter tuning for ML models (Turner et al. 2021), and scientific lab experiments (Angermueller et al. 2020). These optimization techniques are crucial when the objective function is complex, expensive to evaluate, or lacks an explicit analytical form. With an efficient exploration and exploitation of the solution space, BBO helps identify optimal solutions that would be otherwise infeasible to find using traditional methods.\n\nDespite their pivotal role in ML and other applications, black-box optimizers (acquisition functions) are frequently perceived as opaque and inscrutable. This lack of transparency hinders user trust and limits their broader applicability. Consequently, improving our understanding and interpretation of these optimization approaches can significantly enhance their usability and reliability in practical settings.\n\nExample: The \"Robot Pushing\" problem involves two robot hands working together to push objects toward a designated goal. The movements of the robot hands are influenced by 14 parameters, including hand trajectory, location, rotation, velocity, and direction. This complexity requires rigorous experimentation and precise optimization to achieve the desired outcomes. In this context, batch evaluation is crucial because it allows for efficient usage of computational resources and enables parallel processing. Evaluating a large batch of sample points simultaneously demands a transparent and explainable selection process. Consider the scenario where a BBO algorithm selects a batch of 50 configurations for evaluation. Engineers need to understand the rationale behind the choice of each configuration in the batch. This understanding is necessary to build confidence and trust in the optimizer's decisions and the overall optimization outcomes. For instance, engineers need to know if the selected configurations explore a diverse range of trajectories and velocities, or if they exploit the previously successful parameters. A transparent explanation of these choices can demonstrate how the algorithm balances exploration and exploitation, ensuring that the optimization process is effective and reliable.\n\nRecent research has highlighted the importance of explainability in BBO. For instance, Adachi et al. discussed that Bayesian optimization (BO) often struggles to gain user trust due to its opaque nature. They introduced the Collaborative and Explainable Bayesian Optimization (CoExBO) framework, which uses preference learning to incorporate human knowledge and Shapley values to quantify the contribution of each feature to the acquisition function. Simi-"}, {"title": "Related Work", "content": "Research in BBO has concentrated on managing the exploration-exploitation trade-off (Frazier 2018), traditionally in a sequential evaluation setting (Jones, Schonlau, and Welch 1998; Srinivas et al. 2009). However, the introduction of batch sampling has created a paradigm shift, as it is not only necessary for numerous experimental evaluation scenarios but also accelerates convergence, enhances exploration, and improves robustness by leveraging parallel computational resources (Desautels, Krause, and Burdick 2014; Balandat et al. 2020).\n\nExplainable AI (XAI) is a growing field focused on making artificial intelligence systems more transparent and understandable to humans (Arrieta et al. 2020). Researchers have developed various techniques to elucidate the inner workings of complex models, ranging from local explanation methods such as LIME (Local Interpretable Model-agnostic Explanations) (Ribeiro, Singh, and Guestrin 2016) and SHAP (SHapley Additive exPlanations) (Lundberg and Lee 2017) to global interpretation strategies such as decision trees and rule-based systems (Linardatos, Papastefanopoulos, and Kotsiantis 2020; Sharma et al. 2024). However, research on interpretability in black-box optimization is sparse. Only a few articles in the XAI literature have addressed the explainability of black-box optimizers. For example, (Seitz 2022) proposes a gradient-based explainability for GPs as the primary surrogate model in BO to create uncertainty-aware feature rankings. Explainable Meta-heuristic (Singh, Brownlee, and Cairns 2022) mines surrogate fitness models with the objective of identifying the variables that strongly influence solution quality for providing an explanation of the near-optimal solution. Recently, (Adachi et al. 2023) introduced Collaborative and Explainable Bayesian Optimization (CoExBo) specifically for lithium-ion battery design. This framework integrates human expertise into BO through preference learning and utilizes Shapley values to explain its recommendations. Co-ExBo begins by aligning human knowledge with BO via preference learning, followed by proposing multiple options from which the user can choose based on additional Shapley value insights. In contrast, Shapley-assisted human-BO col-"}, {"title": "Inclusive Explainability Metrics for Surrogate Optimization", "content": "In this section, we introduce the Inclusive Explainability Metrics for Surrogate Optimization (IEMSO) framework, designed to enhance the explainability and trustworthiness of SO approaches. The IEMSO framework is structured around four primary categories of model-agnostic metrics, each targeting a specific aspect of the SO process."}, {"title": "Sampling Core Metrics", "content": "This category of metrics focuses on providing explanations for individual sample points selected for expensive evaluation in each iteration. They offer insights into the logic behind the sampling process. Sample points can be explained based on their contribution to the selected subset (batch), their location in the solution space, and their contribution to SO objective (exploration-exploitation trade-off).\n\nPoint Coordinate Exploration (PCE) is a metric that quantifies the extent to which the selected sample points have covered the full range of each coordinate or feature within the predefined bounds.\n\nDefinition 1. Given a set of n evaluated sample points $D = {x_1,x_2,...,x_n}$, where each $x^i$ is a d-dimensional vector and each coordinates $x_j, j = 1,2,...,d)$ is bounded by $[lb_j, ub_j]$, the PCE is defined as the normalized range covered by the sample points in that coordinate, as\n$PCE_j = \\frac{max(x_j^1,...,x_j^n)-min(x_j^1,...,x_j^n)}{ub_j-lb_j}$. Hence, the average PCE is $PCE(D) = \\frac{\\sum_{i=1}^d PCE_j}{d}$.\n\nA PCE value close to 1 indicates that the sample points have sufficiently explored the range of each coordinate $[lb_j, ub_j]$. Conversely, a PCE value approaching 0 indicates that the sample points have explored only a limited fraction of each coordinate's range. This metric is simple to compute and offers a clear measure of coverage for each dimension. By normalizing exploration extent, PCE ensures consistency across different coordinates, helping to assess whether the sampling strategy is effectively exploring the search space which is a key factor in optimization.\n\nMean Distance from Prior Evaluations (MDPE) is a metric designed to quantify the dissimilarity (i.e., distance) of each individual sample point from the previously evaluated data points. In SO, sample points that exhibit greater distance from the previously evaluated points are predominantly important for the exploration objective, whereas points that are similar or in close proximity to the evaluated set are primarily selected for exploitation purposes.\n\nDefinition 2. Given a set of n evaluated sample points $D = {x_1,x_2,...,x_n}$, and the selected subset of candidate points $S = {z^1,z^2,...,z^k}$, the MDPE is defined as the average pairwise Euclidean distance between each sample point $z^i \\in S$ and the set of evaluated points D, $MDPE(z) = \\frac{1}{n} \\sum_{x \\in D} ||z^k - x^i||$, where $||z^k - x^i||$ denotes the Euclidean distance between the points $z^k$ and $x^i$.\n\nThe Mean Distance from Prior Evaluations (MDPE) metric offers several key advantages. Firstly, it is intuitive, providing a clear and understandable measure of how different or distant the new sample points are from the already evaluated points. This clarity aids in the easy interpretation of the sampling strategy's effectiveness. Secondly, MDPE effectively highlights the balance between exploration and exploitation, a crucial aspect of optimization tasks. Distant points indicate exploration, while points closer to previously evaluated ones indicate exploitation. Finally, MDPE offers a quantitative measure to assess the diversity and novelty of the selected sample points, enabling a rigorous evaluation of the optimization process.\n\nContribution to the Hypervolume of Exploration-Exploitation (CHEE) is a metric that quantifies the expected and actual impact of each sample point on exploration and exploitation objectives, both prior to and following the expensive evaluation.\n\nDefinition 3. Consider a surrogate optimization approach characterized by an exploitation metric $\\mu(x)$ and an exploration metric $\\sigma(x)$, where $x \\in X$ represents a point in the solution space X. Let $\\hat{\\mu}(x)$ and $\\hat{\\sigma}(x)$ denote the surrogate model's predictions for the exploitation and exploration metrics, respectively. The estimated Pareto set $P'$ constructed on the estimated exploration and exploitation metrics is defined as $P' = {x \\in X | \\nexists x' \\in Xs.t.\\hat{\\mu}(x') \\leq \\mu(x), \\hat{\\sigma}(x') \\leq \\hat{\\sigma}(x)}$. The hypervolume (HV) of the Pareto set $P'$ with respect to a reference point r is $HV(P', r) = Volume (\\bigcup_{x\\in P'} [x, r])$. The contribution of a point x \u2208 S to the hypervolume of exploration-exploitation is then defined as $\\bigtriangledown HV(x, P',r) = HV(P',r) - HV(P' \\setminus {x},r)$.\n\nThis metric evaluates the change in hypervolume resulting from the inclusion or exclusion of a sample point, thus measuring its contribution to both exploration and exploitation. The use of hypervolume as a measure is a well-established method in multi-objective optimization to evaluate the quality of solutions. It provides a clear and interpretable way to assess the impact of adding or removing points from the Pareto set. The exploration metric \u03c3(x) can be interpreted in different ways depending on the SO approach. When using a surrogate model, \u03c3(x) often represents the surrogate variance, which captures the model's uncertainty about the prediction at point x. Alternatively, in a model-agnostic SO approach, \u03c3(x) can be represented by a distance metric d(x), which measures the dissimilarity or novelty of the point x relative to other points in the dataset. This flexibility allows the framework to be adapted to various optimization scenarios and objectives."}, {"title": "Batch Properties Metrics", "content": "This category of metrics is dedicated to providing explanations for the selected batch of sample points. These metrics"}, {"title": "Sampling Core Metrics", "content": "evaluate the batch based on its aggregated contribution to diversity (both intra-batch and inter-batch diversity), aggregated objective values, and its impact on the SO objective, specifically the exploration-exploitation trade-off.\n\nDensity of the selected subset (DES) is a metric that estimates the entropy of the batch. The entropy of a batch of sample points provides a measure of the uncertainty or randomness within the batch. In the context of surrogate optimization, higher entropy indicates greater diversity among the sample points, while lower entropy suggests that the points are more similar to each other. Entropy, in this context, can be calculated using techniques like Kernel Density Estimation (KDE)(Chen 2017), to estimate the underlying probability density function of a dataset without assuming a specific parametric form, making it suitable for complex distributions.\n\nDefinition 4. For a batch of points $S = {X_1,X_2, ..., X_k}$, the DES metric can be defined using differential entropy. Let p(x) represent the KDE estimate of the probability density function for the points in S. The differential entropy H(S) is given by $H(S) = \\int_x p(x) log p(x) dx$. In practice, for a finite sample, the differential entropy can be approximated as $H(S) \\approx -\\sum_{i=1}^klogp(x_i)$, where p(x_i) is the KDE estimate at point x_i.\n\nBy calculating the differential entropy of the batch, the DES metric provides insights into how well the batch balances exploration and exploitation. High DES values indicate a well-distributed set of sample points, contributing to effective exploration of the solution space.\n\nDiversity of the selected subset (DIS) quantifies the spread or coverage of the selected set of points in a multidimensional space by measuring the volume of the geometric shape spanned the points. Inspired by the diverse sampling concept from Determinantal Point Processes (DPPs) (Kulesza, Taskar et al. 2012), we aim to utilize the determinant of the kernel matrix L to measure the batch diversity.\n\nThe DIS metric quantifies the spread or coverage of the selected set of points in a multidimensional space. This metric is inspired by concepts from Determinantal Point Processes (DPPs) (Kulesza, Taskar et al. 2012), which measure diversity based on the volume spanned by the points.\n\nDefinition 5. Given a batch of points S, the diversity can be measured using the determinant of the kernel matrix L, where $L_{ij}$ is computed using a similarity kernel function. DIS(S) is then given by $DIS(S) = \\sum_{\\sigma \\in S_n}sign(\\sigma) \\prod_{i=1}^n L_{i,\\sigma(i)}$, where \u03c3 is a permutation of the indices 1, 2, . . ., n, and sign(\u03c3) is the sign of the permutation. When the kernel is a Radial Basis Function (RBF), $L_{ij} = exp(\\frac{-||x_i-x_j||^2}{2\\sigma^2})$ (Steinwart, Hush, and Scovel 2006).\n\nAverage Batch Distance (ABD): metric quantifies the dissimilarity between a batch of selected sample points and a set of previously evaluated data points. This metric is crucial for assessing the balance between exploration (distant points) and exploitation (close points).\n\nDefinition 6. Let $D = {X_1, X_2, ..., X_n}$ be a set of n previously evaluated sample points, and let $S = {Z_1, Z_2,..., Z_k}$"}, {"title": "Optimization Process Metrics", "content": "These metrics provide a holistic view of the overall optimization process, including convergence and performance.\n\nPartitioning-Based Solution Space Analysis (PSSA) metric focuses on providing an explanation of the solution space, which is commonly considered as a d-dimensional hyperrectangular space. Accurately locating each sample point or the selected batch of points within this space is crucial prior to expensive evaluations. To achieve this, we utilize partitioning methods to divide the solution space into"}, {"title": "Optimization Process Metrics", "content": "distinct regions, facilitating a better understanding of the distribution and relationships within the data.\n\nDefinition 8. Let $F = {(x_i, Y_i) | i = 1,2,..., n}$ be a d-dimensional dataset, where $x_i \\in R^d$ represents an evaluated sample point and $y_i$ is the corresponding expensive target value. A partitioning method divides the dataset into m regions, ${R_1, R_2, ..., R_m }$, where each region $R_k$ is a subset of the solution space. The region $R_k$ can be defined as $R_k = {x \\in X | Condition_k(x)}$, where $Condition_k (x)$ specifies the criteria that determine the membership of x in region $R_k$.\n\nAs an example, consider decision trees as a partitioning method. In decision trees, the dataset is partitioned based on decision rules at each node, where each decision rule splits the data into two branches based on a threshold value of a specific feature. This process is repeated recursively, creating a hierarchical partitioning of the solution space. Each node in the decision tree splits the data based on a decision rule of the form $x_j < v$, where $x_j$ is the j-th feature and v is a threshold value. Each partition (leaf node) corresponds to a region $R_k$ in the d-dimensional space. The region $R_m$ associated with leaf node m can be defined as $R_m = {x \\in X | \\land_{(j,v)\\in L_m} (x_j \\leq v \\lor x_j > v)}$, where $L_m$ is the set of splitting rules leading to leaf m.\n\nBy leveraging the explainability of decision trees, the PSSA metric systematically partitions the solution space and derives interpretable rules that elucidate the relationships between the features and the target values. This approach enables a clearer understanding of the solution space and facilitates the identification of regions that warrant further exploration or exploitation.\n\nConvergence Rate (CR) tracks the rate at which the optimization process approaches the optimal solution. It is measured by the decrease in the objective function value over iterations.\n\nDefinition 9. Given an objective function f(x), let $I = {f(x^{(1)}), f(x^{(2)}), ..., f(x^{(T)})}$ be the sequence of best known objective function values over T iterations. The Convergence Rate is defined as the average rate of decrease in the best known objective function value per iteration $CR= \\frac{\\sum_{t=2}^{T} \\frac{f(x^{(t-1)})-f(x^{(t)})}{f(x^{(t-1)})}}{T}$.\n\nThis metric provides insight into how quickly the SO converges toward the optimal solution.\n\nOptimization Stability (OS) metric evaluates the stability of the optimization process, ensuring it is not overly sensitive to initial conditions or random variations.\n\nDefinition 10. Let S be a set of m optimization runs with different initial conditions or random seeds, resulting in objective function values ${f(x_1), f(x_2), ..., f(x_n)}$ at convergence. The Optimization Stability is defined as the standard deviation of the final objective function values at convergence $OS = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m (f(x_i)-\\bar{f})^2}$, where $\\bar{f}$ is the mean of the objective function values at convergence $\\bar{f} = \\frac{\\sum_{i=1}^n f(x_i)}{n}$."}, {"title": "Feature Importance", "content": "This group of metrics focuses on evaluating the significance of individual features in the context of surrogate optimization. By quantifying the contribution of each feature to various objectives and models, these metrics provide insights into the behavior and performance of the optimization process, enhancing its interpretability.\n\nFeature Importance for Exploration and Exploitation (FIEE) is a metric that evaluates the contribution of each feature to the exploration and exploitation objectives.\n\nDefinition 11. Given a set of input features $x = (x_1,x_2,...,x_d)$, the contribution of each feature $x_j, \\forall j \\in {1, ...,d}$, to the exploration objective is denoted by $\\eta_j$, and to the exploitation objective by $|\\tau_j|$. These contributions can be calculated using feature importance techniques such as SHAP values, permutation importance, or other model-specific methods.\n\n$\\eta_j = SHAP_{\\sigma}(x_j)$ or $\\eta_j =$ Permutation Importance$(x_j)$,\n\n$|\\tau_j| = SHAP_{\\mu}(x_j)$ or $|\\tau_j| =$ Permutation Importance$(x_j)$,\n\nwhere SHAP(xj) and Permutation Importance(xj) represent the SHAP value and permutation importance of feature $x_j$ with respect to the exploration metric \u03c3(x), respectively, and SHAP\u00b5(xj) and Permutation Importance(xj) represent the SHAP value and permutation importance of feature xj with respect to the exploitation metric \u00b5(x), respectively.\n\nFeature Importance for the Black-Box Objective Function (FIBB) is a metric that evaluates the contribution of each feature to the black-box objective function.\n\nDefinition 12. The contribution of each feature $j\\in {1, ..., d}$ to the black-box objective function is denoted by $\\phi_j$. This contribution can be calculated using data-driven feature importance methods such as SHAP values, permutation importance, or other relevant techniques,\n\n$\\phi_j = SHAP_{f}(x_j)$ or $\\phi_j =$ Permutation Importance$_{f}(x_j)$,\n\nwhere SHAP f(x) and Permutation Importance f(x) represent the SHAP value and permutation importance of feature xj with respect to the black-box objective function f(x).\n\nThese metrics help in understanding the relative importance of features and their evolution across iterations, enhancing interpretability.\n\nFeature Importance of Surrogate Model (FIS) is a metric that quantifies the contribution of individual features to the predictions generated by the surrogate model. It utilizes point-wise feature importance techniques, such as Shapley Values (Lundberg and Lee 2017), to offer a detailed analysis of how each feature influences the predicted outcomes.\n\nDefinition 13. Given a surrogate model $\\hat{f}$ and a set of input features $X = (x_1,x_2,...,x_d)$, the contribution of"}, {"title": "Feature Importance", "content": "data point $x_i$ can be quantified using feature importance scores $\\phi_j(x_i)$. The FIS metric for each feature j is then defined as the average feature importance score over all data points $FIS_j = \\frac{\\sum_{i=1}^{n}\\phi_j(x_i)}{n}$, where $\\phi_j(x_i)$ is the feature importance score of feature j for the i-th data point, and n is the total number of data points. Feature importance scores $\\phi_j(x_i)$ can be obtained using various techniques, such as SHAP values, permutation importance, or other model-specific importance measures like Multivariate Adaptive Regression Splines (MARS) (Friedman 1991) and Decision Trees (DT) (Song and Ying 2015).\n\nWe emphasize the advantages of using explainable surrogate models such as MARS or DT compared to other models like Gaussian Processes (GPs) (Rasmussen and Nickisch 2010). Although GPs are powerful for capturing complex, non-linear relationships, they lack inherent mechanisms for directly extracting feature importance."}, {"title": "Results", "content": "In this section, we elaborate on the impact and applicability of the proposed explanation metrics, employing the well-known synthetic and real-world benchmark problems from black-box optimization literature.\n\nBaselines: We consider the standard Monte-Carlo-based batch Bayesian acquisition functions such as qEI, qUCB, qMES, and qGibbon as described in (Wilson et al. 2017; Balandat et al. 2020). Moreover, we consider DEEPA (Nezami and Anahideh 2024) as a Pareto-based SO baseline which has been shown to be effective for high-dimensional BBO problems.\n\nExperiment Set-up: In our experiments, the size of the initial evaluated set is assigned based on the 2* (d+1) formula where d refers to the number of features. For the 10d test problems, we maintain a batch size of k = 8, a candidate set of 1000 randomly generated sample points. For the 2d instances, we use a batch size of k = 4, a candidate set of 100 randomly generated sample points. For the Levy 6d test problem, the batch size is set to 4, the candidate set includes 1000 points. For the Robot Pushing and Rover Trajectory benchmarks, the batch size is set to k = 50.\n\nResults: Figure 9 illustrates the application of the previously introduced metrics across various benchmark problems, enhancing explainability and fostering user trust.\n\nFigures 1 and 2 demonstrate how Batch Properties Metrics provide valuable insights, enabling the user to compare batch selections across different baselines. For the 10-dimensional Rastrigin test problem, the ABD plot indicates that qGibbon tends to select points that are more distant from the evaluated set for batch evaluation compared to other baselines. This characteristic makes qGibbon an excellent choice for users prioritizing exploration. This behavior aligns with the Gibbon acquisition function, which is equivalent to a Determinantal Point Process (DPP) when specific quality functions and similarity kernels are used. By maximizing the determinant of the selected subset, it ensures that a diverse set of points is chosen for evaluation.\n\nFigure 3 presents the PCE and MDPE from the Sampling Core Metrics for the 14-dimensional Robot Pushing"}, {"title": "Results", "content": "problem. By leveraging PCE plots during local convergence, users can actively guide the optimization process to select points with more diverse coordinate values, thereby reducing the risk of budget inefficiency. For instance, the DEEPA strategy tends to select sample points with a higher range of values in the second dimension compared to the 14th dimension in the final iterations. Additionally, the MDPE plot proves valuable at any stage of the optimization process, providing a clear visualization of the point-wise distances between the evaluated set and the sample points chosen by a given acquisition function. For example, in the last few iterations, qMES tends to focus on points that are closer (with lower distance values) compared to other baselines, making it more exploitative.\n\nFigures 4 and 5 depict the PSSA from the Optimization Process Metrics for the Branin and Robot Pushing problems. PSSA enables users to monitor promising regions within the solution space (highlighted in yellow in Figure 5 ) and locate newly selected sample points by revealing the partitioning rules (Figure 4). For example, in the Robot Pushing problems, we can calculate the number of points selected from each partition based on the Decision Tree (DT) rules in each iteration (k = 50). Additionally, PSSA assists users in adjusting the sampling strategy if excessive exploitation or exploration is detected within specific areas of the solution space. Moreover, plots in Figure 6 provide insight into the performance of different SO baselines. For the 6-dimensional Levy, DEEPA outperforms other baselines in terms of convergence rate, while qGibbon shows greater stability (less variation) across the final iterations.\n\nFigures 7a and 7b show the top 10 features (coordinates) utilizing FIBB and FIEE (exploration) from the Feature Importance Metrics for the high-dimensional Rover Trajectory test problem. For example, dimensions 32 and 19 significantly contribute to BBO exploration objective (Maxmin distance) while the first and last 3 dimensions have more impact on the true black-box objective value.\n\nMoreover, Figures 8a and 8b show the FIS metric for the 6-dimensional Levy test problem. Figure 8a utilizes GP as the surrogate model and calculates MARS feature importance based on the observed data in each iteration. Figure 8b uses MARS which is an explainable surrogate model and utilizes the surrogate's feature importance in each iteration. The user can utilize these results to monitor how the surrogate's important coordinates change across iterations."}, {"title": "Conclusion", "content": "In this paper, we introduced Inclusive Explainability Metrics for Surrogate Optimization (IEMSO), a comprehensive framework designed to enhance transparency and build trust in black-box optimization (BBO) methods. Our approach addresses significant gaps in the current state of explainability for surrogate optimization, providing a set of model-agnostic metrics that can be applied across different surrogate frameworks and optimization scenarios. Through extensive experimental analysis on both synthetic and real-world benchmark problems, we demonstrated the effectiveness of IEMSO in offering valuable insights into various"}, {"title": "Conclusion", "content": "stages of the optimization process. The metrics were divided into four main categories: Sampling Core Metrics, Batch Properties Metrics, Optimization Process Metrics, and Feature Importance Metrics. Each category serves a unique purpose in elucidating different aspects of surrogate optimization, from explaining sampling strategies to understanding feature importance and evaluating the overall optimization process. Our results show that IEMSO not only enhances the interpretability of optimization outcomes but"}, {"title": "Conclusion", "content": "also enables experts to make informed decisions throughout the optimization process. By offering both intermediate and post-hoc explanations, IEMSO ensures that the decision-making processes are transparent, interpretable, and trustworthy, thereby increasing the reliability and applicability of surrogate optimization in complex, real-world scenarios. Future work could focus on expanding the set of explainability metrics and developing a user-friendly interface to further enhance the tool's applicability."}]}