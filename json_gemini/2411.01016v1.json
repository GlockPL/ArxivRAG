{"title": "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "authors": ["Cheng Yang", "Yang Sui", "Jinqi Xiao", "Lingyi Huang", "Yu Gong", "Yuanlin Duan", "Wenqi Jia", "Miao Yin", "Yu Cheng", "Bo Yuan"], "abstract": "The emergence of Mixture of Experts (MoE) LLMs has significantly advanced the development of language models. Compared to traditional LLMs, MoE LLMs outperform traditional LLMs by achieving higher performance with considerably fewer activated parameters. Despite this efficiency, their enormous parameter size still leads to high deployment costs. In this paper, we introduce a two-stage compression method tailored for MoE to reduce the model size and decrease the computational cost. First, in the inter-expert pruning stage, we analyze the importance of each layer and propose the Layer-wise Genetic Search and Block-wise KT-Reception Field with the non-uniform pruning ratio to prune the individual expert. Second, in the intra-expert decomposition stage, we apply the low-rank decomposition to further compress the parameters within the remaining experts. Extensive experiments on Qwen1.5-MoE-A2.7B, DeepSeek-V2-Lite, and Mixtral-8\u00d77B demonstrate that our proposed methods can both reduce the model size and enhance inference efficiency while maintaining performance in various zero-shot tasks. The code will be available at https://github.com/xiaochengsky/MoEI-2.git", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently demonstrated remarkable language understanding and generation proficiency, excelling in complex tasks (Achiam et al., 2023; Touvron et al., 2023a; Wu et al., 2020). However, deploying these models presents substantial challenges due to their significant storage and computational demands. To overcome these issues, the Mixture-of-Experts (MoE) LLM has been proposed (Jiang et al., 2024), which activates only a subset of its parameters during training and inference. For instance, with a smaller model size, the Mixtral-8\u00d77B model with a total of 47B parameters surpasses the performance of dense Transformer models like LLaMA-2-70B (Touvron et al., 2023b). Additionally, Qwen1.5-MoE-A2.7B (Bai et al., 2023) demonstrates highly competitive performance compared to other 7B models, and the recently introduced DeepSeekv2 MoE (DeepSeek-AI, 2024) achieves performance levels comparable to GPT-4, demonstrating the powerful capabilities of MoE models.\nMoE models have garnered significant attention recently due to their ability to dynamically select subsets of parameters for each input, enabling efficient handling of diverse tasks. Despite their potential, a notable challenge with MoE models is that they are still burdened by substantial parameter size and computation cost. For example, Mixtral-8\u00d77B (Jiang et al., 2024) not only has 47B parameters but also activates 13B parameters during inference. While this architecture allows for scalability and flexibility, it also introduces complexities and huge memory in deployment and inference, particularly when considering resource constraints and efficiency. Consequently, decreasing and maintaining these large-scale models remains a critical area of research.\nModel compression techniques, such as pruning, knowledge distillation, and quantization, have been utilized to slim the model size. (Lu et al., 2024) proposed to reduce the parameter count of MoE models by expert pruning, but it does not reduce the parameters during inference efficiently. (Li et al., 2024) merges several experts into one and applies the low-rank decomposition to further reduce the model size. Although this approach achieves a good compression ratio and performance, it requires calibration and fine-tuning for each downstream task individually, which is not suitable for large-scale LLMs, and time costs are very high."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Mixture-of-Experts LLMS", "content": "MoE-LLMs have gained significant attention in recent years due to their ability to scale efficiently while maintaining high performance. MoE models divide the network into several experts and dynamically select a subset of these experts for each input, which reduces computational overhead and enhances scalability. (Shazeer et al., 2017) introduced the MoE model in their work on the Sparsely-Gated Mixture-of-Experts Layer, and (Lepikhin et al., 2020) further advanced the MoE architecture by demonstrating its scalability to trillions of parameters while retaining manageable computation costs by distributing the experts across multiple devices. With the recent advancements in decoder-only architecture(Touvron et al., 2023a), MoE models built on this structure have become increasingly popular (Jiang et al., 2024). In this paper, we focus on how to build an end-to-end framework to solve post-training expert pruning and decomposition for MoE LLMs to decrease computation and storage."}, {"title": "2.2 Compression on MoE LLMs", "content": "Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019, 2020), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022; Yuan et al., 2023; Wang et al., 2024), and methods like early exit (Xin et al., 2020). Building on these techniques, pruning, and sparsity is crucial for MoE models, which often have up to 95% of parameters dedicated to experts. Pruning MoE models involves removing less important experts or neurons to reduce the number of active parameters during inference. For example, (Kim et al., 2021) retains the most activated experts to enhance machine translation MoE models, while (Koishekenov et al., 2022) introduces gate statistics-"}, {"title": "3 Method", "content": "In this section, we introduce the details of our proposed framework, MoE-I2, which consists of three stages: Inter-Expert Pruning stage (Sec. 3.1), Intra-Expert Decomposition stage (Sec. 3.2), and fine-tuning stage (Sec. 3.3). The overall pipeline is shown in Figure 1."}, {"title": "3.1 Inter-Expert Pruning", "content": "In this stage, our goal is to prune individual unimportant experts to reduce the parameter size and computational cost. It raises two crucial questions: (1) Given an overall pruning ratio, how many experts should be pruned in each layer? (2) How to determine which experts to prune?"}, {"title": "3.1.1 Layer Importance Analysis", "content": "To answer the first question, we start by analyzing the importance of each layer. The layer importance of i-th layer, denoted by $I_i$, is defined as the average loss degradation by removing individual experts within this layer. Specifically, to calculate $I_i$ in the i-th layer, we first calculate the expert importance. We consecutively pruning j-th expert in the i-th layer, denoted by $e_{i,j}$, where j = 1,2,\u2026\u2026, $M_i$. The $M_i$ represents the total number of experts in the i-th layer. Next, each pruned model predicts the next token with the calibration samples. The expert importance of $e_{i,j}$ is calculated as:\n$L_{i,j} = \\sum_B L(X, \\{E\\} \\setminus \\{e_{i,j}\\})$\nFollowing this paradigm, we demonstrate the layer importance for Mixtral-8\u00d77B (Jiang et al., 2024), Qwen1.5-MoE-A2.7B (Bai et al., 2023), and DeepSeek-V2-Lite (DeepSeek-AI, 2024) as shown in Figure 2. Note that the previous work (Lu et al., 2024) overlooks the varying importance of layers and simply applies a uniform pruning ratio to each layer, leading to a suboptimal solution. In contrast, our analysis shows that some models perform in ways that largely diverge from this strategy. For example, the analysis of DeepSeek-V2-Lite (Figure 2) reveals that layer importance rapidly increases with depth, indicating that deeper layers are more sensitive than shallower ones."}, {"title": "3.1.2 Inter-Expert Pruning Strategy", "content": "To answer the second question, it is required to identify a combination of N experts that have the least impact on prediction loss. Previous work (Lu et al., 2024) utilizes brute-force search to find the least impactful combination of N experts within each layer. However, this method presents two significant drawbacks. First, the brute-force search has high time complexity, making it extremely time-consuming, especially when pruning the MoE with a large number of experts. For example, Qwen1.5-MoE-A2.7B and DeepSeek-V2-Lite have 60 and 64 experts per layer, respectively. If 25% of experts need to be pruned, (Lu et al., 2024) needs to traverse $C_{15}^{60}$ and $C_{16}^{64}$ times for each layer respectively, which is unacceptable in terms of time consumption. Second, it restricts the search space within the current layer, only achieving a local optimum and potentially missing a more globally optimal solution.\nTo mitigate these challenges, we leverage Genetic Search (Grefenstette, 1993; Alam et al., 2020) with KT-Receptive Field methods to enhance search efficiency and concurrently identify the least im-"}, {"title": "Layer-wise Genetic Search.", "content": "To avoid extreme time consumption caused by brute-force search (Lu et al., 2024), we leverage the genetic search to select the M candidate combinations in each layer. For the i-th layer, we define all possible pruning combinations as $CP_i$. Here, $P_i$ represents the number of experts to be be pruned in the i-th layer. Given that there are $M_i$ experts in the i-th layer, $C^{M_i}_{P_i}$ denotes the number of combinations for selecting $P_i$ experts to prune from the total of $M_i$ experts.\nIn the initial stage of Genetic Search, we first initialize a population $\\{CP_{i,1}, CP_{i,2},..., CP_{i,N}\\}$, where the population size N = 100. We then calculate the loss for each combination in the population:\n$L = \\sum_B ||F_i(X) - F_i(X, \\{E_i\\}\\setminus CP_{i,n}))||_F$\nWe select the combinations with the smallest loss from $CP_{i,n}$ as parents. Using union and random sampling, we generate offspring combinations. Each individual in the offspring population undergoes some mutations, where a few experts to be pruned are randomly replaced. This process is repeated iteratively in 50 steps and we can obtain the optimal a few combinations of expert pruning as candidate combinations in the i-th layer."}, {"title": "Block-wise KT-Reception Field.", "content": "After obtaining then candidate combinations, we only keep K best combinations with the smallest loss in each layer as the candidate combinations to be used for the block-level optimization. We aim to select one of the K combinations from each layer such that they minimize the output loss. During this selection process, instead of only considering the importance of experts in just the current layer (Lu et al., 2024), we extend the scope of candidate selection from one layer to T layers, achieving a block-wise combination. Specifically, we partition all layers into $\\lceil \\frac{N}{T} \\rceil$ blocks. Within each block, we select the combination in a brute-force scheme. Given K candidates in each layer, and considering there are T layers in one block, we traverse all possible combinations by selecting one combination from each of T layers, yielding a total of $K^T$ options. Subsequently, we calculate the output loss and select the optimal combinations for pruning. The pipeline is shown in Figure 3.\nExpert Pruning. Given the to-be-pruned experts, we conduct the expert pruning operation by removing the entire expert in a structured manner."}, {"title": "3.2 Intra-Expert Decomposition", "content": "In this stage, we propose to further compress the remaining experts in a fine-grained way by perform-"}, {"title": "3.2.1 Expert Importance Analysis", "content": "As mentioned in (Chi et al., 2022), each expert has varying levels of importance. To achieve better compression performance, instead of applying a uniform compression ratio, we aim to retain more parameters in the important experts and fewer in the less important ones. That leads us to assign higher ranks to the more important experts and lower ranks to the less important ones. Therefore, to calculate the varying ranks, we analyze the relative importance of each expert. Based upon the previous analysis in Sec. 3.1.1, we adopt the same importance metric, $I_{i,j}$ in Eq. 1, as the expert importance.\nTo determine the varying ranks of each expert,"}, {"title": "3.2.2 Intra-Expert Decomposition Strategy", "content": "Singular Value Decomposition (SVD) is a general technique to reduce parameter size by decomposing a large dense matrix into two smaller low-rank matrices. Compared to the Vanilla SVD, which only focuses on the initial weight matrix, (Wang et al., 2024) generates activation by truncation-aware data whitening and provides hierarchical closed-form updates for model compression. Inspired by SVD-LLM (Wang et al., 2024) working on dense models, we extend SVD-LLM to MoE models by integrating the non-uniform ranks $R_{i,j}$ in Sec. 3.2.1."}, {"title": "3.3 Efficient Fine-tuning", "content": "To mitigate performance degradation caused by the two-stage compression, we fine-tune the MoE by updating the weights. Instead of adjusting all weights, we integrate LoRA (Hu et al., 2021), a low-rank approximation technique, into the post-training of the pruned model. The overall algorithm is illustrated in Alg. 1."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Settings", "content": "Model Settings. To demonstrate the effectiveness of our method, we conducted experiments on three MoE models: Qwen1.5-MoE-A2.7B (14.3B), DeepSeek-V2-Lite (16B), and Mixtral-8\u00d77B (47B). Mixtral-8\u00d77B has a larger number of parameters and relatively fewer experts (8 experts per layer in total 32 layers). On the other hand, Qwen1.5-MoE-A2.7B and DeepSeek-V2-Lite have fewer parameters but a greater number of experts (60 and 64 experts per layer in a total of 24 and 26 layers, respectively).\nEvaluation and Datasets. To evaluate the performance in a task-agnostic setting, we mainly adopt LLama-Pruner (Ma et al., 2023) evaluation methodology, conducting zero-shot task classification across common sense reasoning datasets such as BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-easy (Clark et al., 2018), ARC-challenge (Clark et al., 2018), and OpenbookQA (Mihaylov et al., 2018). Meanwhile, our model evaluates results in multiple-choice tasks or generates answers in open-ended generation tasks (Gao et al., 2021). Furthermore, we supplement our evaluation with a zero-shot perplexity (PPL) analysis on WikiText2 (Merity et al., 2016) and PTB (Marcus et al., 1993)."}, {"title": "4.2 Main Results", "content": "MoE-I2 Results. Table 1 presents the zero-shot performance of the models after applying the MoE-I2 framework. It is evident that pruning 25% of the expert parameters results in only a slight performance loss. However, after finetuning the compressed mode with only 2 epochs, the performance can even surpass that of the original model, especially with an improvement of over 2% on the DeepSeek-V2-Lite model. This observation suggests that pruning 25% of the experts in the first step is lossless. In the second step, we choose to further compress the pruned model with an approximate 40% compression ratio via low-rank decomposition. Finally, we perform the finetuning stage. As a result, we can see that while ensuring a reduction of more than 50% in expert parameters, the model's performance is largely preserved.\nZero-shot Performance Comparisons with Existing Methods.\nTable 2 shows the zero-shot performance of the pruned model by comparing Wanda (Sun et al., 2023), EEP (Lu et al., 2024), and our Inter-Expert Pruning method under the same sparsity rate. Our method demonstrates significant advantages over Wanda and EEP.\nPPL Comparisons with Existing Methods. Table 3 shows the zero-shot perplexity(PPL) of the pruned model by comparing EEP, and our Inter-Expert Pruning method under the same sparsity rate. Our method demonstrates significant advantages over EEP.\nInference Speedup with Existing Methods. Table 4 shows the speedup of three models by comparing Wanda (Sun et al., 2023), EEP (Lu et al., 2024), and MoE-I\u00b2 method."}, {"title": "4.3 Ablation Studies", "content": "Comparison of MoE-I\u00b2 and its Components. Table 5 demonstrates the necessity of the components within the MoE-I2 framework. It shows that MoE-I\u00b2 has a significant advantage when compared to applying only Inter-Expert Pruning or Intra-Expert Decomposition individually."}, {"title": "5 Conclusion", "content": "In this paper, we explore the efficiency of current large-scale MoE models and propose a general end-to-end compression framework, MoE-I2, that addresses the issue of parameter redundancy in MoE models. In our approach, we first conduct the layer importance analysis and Inter-Expert Pruning for different MoE models. Subsequently, we perform the expert important analysis based on the pruned model, ensuring appropriate target ranks of each expert when performing the Intra-Expert Decomposition. Our MoE-I\u00b2 framework significantly reduces the parameters of MoE models maintaining high performance. In the future, we aim to support a wider variety of MoE models with larger parameters, enhancing their deployability."}, {"title": "Limitations", "content": "Our proposed framework, MoE-I2, can perform end-to-end compression on any MoE model and adaptively find suitable pruning and decomposition strategies for the target MoE model. By compressing the model at multiple fine-grants, we ensure optimal compression while maintaining model performance, making it more suitable for deployment. Despite these advantages, due to computational limitations, we have not yet tested our framework on larger MoE models such as Mixtral-8\u00d722B (141B), and DeepSeek-V2 (236B). We aim to gradually test these larger MoE models in future work."}, {"title": "Ethics Statement", "content": "Our research focuses on developing an end-to-end framework for the compression of Mixture-of-Experts (MoE) large language models (LLMs). By enhancing model compression techniques, we aim to significantly reduce the model size and improve inference efficiency, ensuring these improvements do not come at the cost of performance. While our work contributes to the advancement of deploying sophisticated LLMs more effectively, we recognize the ethical considerations inherent in this field. These include the need to address potential biases in the models, ensure the responsible and"}]}