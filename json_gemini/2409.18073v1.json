{"title": "Infer Human's Intentions Before Following Natural Language Instructions", "authors": ["Yanming Wan", "Yue Wu", "Yiping Wang", "Jiayuan Mao", "Natasha Jaques"], "abstract": "For AI agents to be helpful to humans, they should be able to follow natural language instructions to complete everyday cooperative tasks in human environments. However, real human instructions inherently possess ambiguity, because the human speakers assume sufficient prior knowledge about their hidden goals and intentions. Standard language grounding and planning methods fail to address such ambiguities because they do not model human internal goals as additional partially observable factors in the environment. We propose a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), aiming for better natural language instruction following in collaborative embodied tasks. Our framework makes explicit inferences about human goals and intentions as intermediate reasoning steps. We implement a set of Transformer-based models and evaluate them over a challenging benchmark, HandMeThat. We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-to-end approaches. We also compare our implementation with strong baselines, including Chain of Thought prompting on the largest available pre-trained language models, and find that FISER provides better performance on the embodied social reasoning tasks under investigation, reaching the state-of-the-art on HandMeThat.", "sections": [{"title": "1 Introduction", "content": "Building AI assistants that can interact with people in a shared environment and follow their instructions would unlock assistive robotics and free up domestic labor. Toward this broad goal, we need to address the problem of \"translating\" realistic natural language instructions into actions executable by robots. The conventional way that people formulate this problem is grounded language learning, which aims at mapping abstract natural language phrases to concretely executable actions. However, these approaches miss an important component of many human-robot collaborative tasks, which is that the language humans tend to use in everyday scenarios is inherently ambiguous. Human speakers assume that listeners possess prior knowledge, leading them to omit certain information for efficiency (Grice 1975; Sperber and Wilson 1986; Clark 1996; Dennett 1987; Gergely et al. 1995). Resolving this ambiguity depends on leveraging other sources of information (e.g., human internal goals and historical actions) that are partially observable to the robot.\nConsider the example shown in Fig. 1, where a human is tidying up a room. In the middle of her actions, she asks a robot for help, saying \"Could you pass that from the sofa?\" This instruction does not appear to be solvable without further information about the person's underlying intention. While such internal mental states are not directly observed, agents can infer them from human's past actions. Specifically, if the robot can observe that in previous steps, the human put several books into a box one by one, it can infer that she intends to use that box to store all the books. Based on this guess, the robot can check if there are any remaining books on the sofa and then hand them to the person.\nGenerally speaking, the ambiguity in the instruction mainly arises from two aspects. First, the human assumes sufficient prior knowledge about her hidden intentions (Dennett 1987; Gergely et al. 1995), which is based on the common sense knowledge that people tend to group similar items together when tidying up, and the observation that the human is gathering books. Second, people make trade-offs between accuracy and efficiency of communication (Grice 1975; Sperber and Wilson 1986; Clark 1996). This leads to the challenge of building AI agents that can follow efficient, ambiguous speech that people naturally adopt when giving directions.\nWe consider the case where there's a human and a robot collaborating in a shared environment. The human is working on some tasks, and specifies a sub-task for the robot to help with by giving a natural language instruction. Past methods (e.g., language grounding) attempt to directly complete the specified command from the given instructions, since the human only acts as a disembodied issuer of instructions and is not another active agent in their environments. The human, as another partially observable factor in the environment, has been overlooked. In this paper, we present a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), which suggests that we should introduce the human's intention as explicit variables for the model to draw inferences about. By leveraging this structure, our framework opts to decompose the problem into two parts \u2013 social reasoning and embodied reasoning. Specifically, social reasoning is aimed at predicting the sub-task for which the human is asking for assistance, which can be inferred from the context"}, {"title": "2 Related Work", "content": "2.1 Grounded language learning\nIn order for AI to be useful to people in our homes and natural environments, non-experts need to be able to communicate with Al agents using natural language. This issue has long captured the attention of researchers (Winograd 1972; Siskind 1994), and the primary challenge involves mapping natural language to concrete meanings within the physical en-\nvironment. Several studies explore language-conditioned task completion in specific environments (Shridhar et al. 2020; Suglia et al. 2021; Kojima, Suhr, and Artzi 2021). With the emergence of LLMs, many works discussed grounding language by leveraging pre-trained LLMs (Blukis et al. 2021; Nair et al. 2022; Zellers et al. 2021). A prominent example is SayCan (Ahn et al. 2022), which proposed extracting the knowledge in LLMs by using them to score the likelihood that a subtask available to the robot will help complete a high-level instruction. Although the above studies may incorporate common sense reasoning about language as well as information within the physical environment, their instructions explicitly express human intentions. For example, the most ambiguous instruction solved by SayCan is, \u201cI spilled my coke, can you bring me something to clean it up?\u201d where the ambiguity can still be easily resolved given that the sponge is the only cleaning tool in the environment. In contrast, we address the problem that realistic human instructions omit certain information for efficiency, making them much more ambiguous, and necessitating inferring human intentions to fill in the gaps.\n2.2 Collaborative communication\nWe consider the case where the human and the robot are working in a shared environment, which is closely related to the literature on collaborative communication (e.g. Two Body Problem (Jain et al. 2019)). CerealBar (Suhr et al. 2019), DialFRED (Gao et al. 2022) and TEACh (Padmakumar et al. 2022) introduce collaborative tasks where the human works as a disembodied issuer of instructions, possibly responding to robot's questions via explicit messages. In contrast, we consider the problem in which the AI assistant needs to consider both explicit messages in natural language and the implicit information in observed human actions. Further, we assume that instructions are not exhaustively describing the required information, but are generated based on a trade-off between informativeness and communication cost. To this end, we focus on the HandMeThat (Wan, Mao, and Tenenbaum 2022) (HMT) benchmark, that calls for the ability to consider both explicit and implicit messages when following ambiguous instructions. The previous state-of-the-art work (Cao et al. 2024) on HMT performs iterated goal inference over the goal space in symbolic representation. However, it requires hand-crafted, pre-defined structures and extensive domain knowledge, which is not applicable in real-world scenarios.\n2.3 Goal recognition\nIn our method, we hope to infer the human's intentions based on the observed historical actions, which is related to goal recognition problem (Lesh and Etzioni 1995; Baker, Tenenbaum, and Saxe 2007; Levesque 2011; Meneguzzi and Pereira 2021). Most of the works are based on the assumption of rationale that an agent should make (approximately) optimal decisions towards the goals every step (Dennett 1987; Gergely et al. 1995). Understanding human intentions in embodied environments has also been studied in many works; for example in Watch-and-Help (Puig et al. 2021) the AI must infer the human's goal from demonstrations, but no natural language is involved. Some recent works (Ying et al."}, {"title": "3 FISER: Follow Instructions with Social and Embodied Reasoning", "content": "3.1 Problem Formulation\nA human-robot Markov Decision Process is described as a tuple $(S, A_{h,r}, T, U, R^r, \\gamma, T)$. $s \\in S$ are object-oriented states including the locations, status and type of each object and agent. $A_{h,r}$ is the joint action space with $A_h, A_r$ being the sets of actions available to the human and the robot, respectively. $T: S \\times A_{h,r} \\times S \\rightarrow {0,1}$ is the transition function where $T(s, a_{h,r}, s') = 1$ if and only if taking actions $a_{h,r}$ at state $s$ gives $s'$ as the next state. $U$ is a set of instructions that the human can give to the robot. $R: S \\rightarrow R$ is a reward function for the robot, $\\gamma$ is the discount factor, and $T$ is the horizon. Throughout the paper, we consider a scenario with only a single round of instruction following for the robot. In each episode, starting from an initial state $s_0$, the human begins working in the environment, and the robot is waiting. Human stops at a time step $t' < T$, leading to a trajectory $T_{t'} = (s_0, a_h, s_1, a_h,..., s_{t'-1}, a_{t'-1})$ and a final state $s_{t'}$. Then the human produces a natural language instruction $u \\in U$ that asks the robot for help. Given $T_{t'}, s_{t'}$ and $u$, the robot needs to interact with the environment by taking a sequence of actions ${a}_{t>t'}$ to maximize its discounted rewards $\\sum_{t=t'}^T [\\gamma^{t-t'} R^r (s_{t+1})]$.\n3.2 Modeling the Human's Intentions\nA straightforward solution to the human-robot MDP may treat $T_{t'}$ and $u$ as additional state information. However, in reality, $T_{t'}$, $u$, and $R^r$ have important correlations: when the human is taking actions and producing instructions, their behavior can be modeled as optimizing for an internal reward function $R^h : S \\rightarrow R$, which is not revealed to the robot. Our insight into this broad problem class is to leverage the causal relation between the human's behavior and"}, {"title": "3.3 Step-wise Reasoning over Human Intentions", "content": "Our model, FISER, builds on top of the factorized human-robot MDP formulation above. We formulate the problem into the social and embodied reasoning phases.\nSocial Reasoning: Robot's Task Recognition\nThe robot needs to disambiguate the natural language instruction $u$ into an understandable and executable task within its own goal space based on the observation of current state $s_{t'}$ and the historical trajectory $T_{t'}$. Therefore, we hope to estimate a function TR, such that $TR(s_{t'}, T_{t'}, u) \\rightarrow G_r$.\nSocial Reasoning: Human's Plan Recognition.\nWe further propose a variant that explicitly estimates the human's underlying overall plan $G_h$ based on $T_{t'}$, and replaces that trajectory by the predicted goal when doing instruction disambiguation. However, since recognizing the full plan is usually intractable, we opt to also take in $u$ and $s_{t'}$, and directly predict the predicate $p^* \\in G_h$ (subgoal) that the human wants the robot to help with. Therefore, we learn two functions PR and TR, such that $PR(s_{t'}, T_{t'}, u) \\rightarrow p^*$ and $TR(s_{t'}, p^*, u) \\rightarrow G_r$.\nEmbodied Reasoning: Grounded Planning.\nOnce the robot goal $G_r$ is obtained, the problem is reduced to a pure grounding and planning task. We can replace the ambiguous natural language instruction $u$ by the accurately expressed robot goal $G_r$. The final grounded planning function GP should satisfy that $GP(s_{t'}, T_{t'}, G_r) \\rightarrow {a}_{t>t'}$, which is basically learning a typical goal-conditioned robot policy $\\pi(a | s_{t'}, T_{t'}, G_r)$.\nSince the functions TR and PR involve natural language inputs, language models are required for these two modules. For GP, we can either implement planning algorithms or use neural networks, since all inputs can be symbolic."}, {"title": "4 Transformer-based Model Implementation", "content": "We implement a Transformer-based model following our framework, illustrated in Fig. 3. We assume all inputs are rendered in texts, and the model needs to predict action strings.\n4.1 Inputs\nSince small-scale language models cannot process excessively long inputs, we divide the information into four parts, including world state description ($s_{t'}$), human's trajectory ($T_{t'}$), language instruction ($u$), and the model's past outputs.\nWorld state description.\nWe consider an object-centric representation for the world state. Specifically, the world state is described as a sequence of object tokens. For each object in the world, we fuse the information of its category (object type and genre), attributes (e.g., size, color, is-open), and spatial relation (inside or on top of another object) into one single embedding, which we called as an object token. The human and the robot are also treated as two special \"objects\".\nHuman's trajectory.\nA straightforward way to represent the human's trajectory is to directly use a paragraph of text to describe the action sequence, e.g., \"the human picks up book#1 from the table\". To better align it with the world state, we replace the embeddings for object names (\u201cbook#1\") by the object tokens that we obtained in the world state description.\nLanguage instruction.\nThe natural language instruction sentence is tokenized and then directly turned into embeddings.\nModel's past outputs.\nEvery time the agent takes a step, the environment returns a sentence describing the effect of its action, i.e., the update in observations. In each episode, such sentences for past steps are concatenated and served as an extra input, e.g.,"}, {"title": "4.3 Encoder Layers", "content": "Each encoder layer is composed of four Transformer layers and a Modality Interaction module.\nTransformer Layers.\nWe use four separate Transformer encoder-only layers to process the inputs. We remove the positional encoding for the world state description, because we do not expect the model to learn an ordering of objects.\nModality Interaction.\nIn order to fuse the information from the four parts of inputs, we design a modality interaction module (an MLP) within each layer, following the architecture proposed by GreaseLM (Zhang et al. 2021). We reserve a special \u201cinteraction\u201d token at the front of each part of inputs. These tokens are expected to gather respective information in the Transformer layers and then interact with each other through this MLP. The updated special tokens will then replace the original first token in each part of inputs.\n4.4 Prediction Layers\nNow we introduce our predictors for intermediate reasoning steps and the final actions. All the predictions are trained with cross entropy loss over corresponding supervisions.\nSocial Reasoning: Human's Plan Recognition.\nWe assume that there is a vocabulary of concepts that allow us to represent human goals as first order logic predicates (e.g., $(\\exists y, box(y), \\forall x, book(x) \\Rightarrow in(x,y))$). While such logical predicates could be more complex, here we assume human plans follow a simpler form: a Q(uantifier), a S(ubjective), a V(erb), and an O(bjective) (e.g., (for-all, book, inside, box)). Therefore, the human's plan recognition module of our model needs to predict a tuple of four tokens. The prediction is conditioned on the embeddings of the four inputs. Specifically, we calculate the log-likelihood over all tuples as follows, where the predictions of the Subjective and Objective are conditioned on the predicted values of Quantifier and Verb:\n$\\log Pr[Q, S, V, O] \\approx \\log Pr[Q] + \\log Pr[V] \\\\\n+ \\log Pr[S | Q, V] + \\log Pr[O | Q, V]$,\nSocial Reasoning: Robot's Task Recognition.\nThe main target of the entire social reasoning phase is to predict the task assigned to the robot, such as a specific object to manipulate. Note that in this step the model needs to specify a concrete object in the world, while the Subjective and Objective predictions in previous plan recognition step are object types."}, {"title": "5 Experiments", "content": "We evaluate our framework by training a Transformer-based model from scratch for the challenging HandMeThat benchmark (Wan, Mao, and Tenenbaum 2022), and then compare them with multiple competitive baselines, including the state-of-the-art prior work on HMT, and the CoT prompting on the largest available pre-trained language models. We will investigate the following hypotheses through empirical analysis.\nH1: Explicitly modeling underlying human intentions works better than directly predicting actions.\n(a) Separating the social and embodied reasoning steps by explicitly recognizing the robot's task is beneficial.\n(b) Explicitly recognizing the human's plan further helps with the social reasoning stage.\nH2: Pre-trained LLMs, despite having access to common-sense knowledge, do not adequately perform the complex social and embodied reasoning in this task. Incorporation of domain-specific knowledge through CoT can help.\n5.1 HandMeThat Environment\nWe evaluate our models over the HandMeThat (version 2) dataset (Wan, Mao, and Tenenbaum 2022). It introduces a household ambiguous instruction following task rendered in text. HandMeThat instructions are split into four difficulty levels, and the gaps between levels correspond to different challenges. The instruction in a Level 1 task has no ambiguity-it is a pure planning task. A Level 2 task requires social reasoning where a robot can successfully accomplish the task if it can also infer the goal from the human trajectory. On Level 3, the robot needs to further consider pragmatic reasoning in language use. For example, if there are multiple books everywhere in the room and only one coat, which is on the sofa, and both the book and the coat are helpful to the human's goal, the human might say \"Could you pass that from the sofa?\". Using pragmatics, we can understand the human is asking for the book, since \"from the sofa\" is required to disambiguate which book the human is referring to, but if the human wanted the coat they could simply ask for it. The final Level 4 contains tasks with inherent ambiguities that cannot be resolved with the existing information, but can potentially be resolved with a strong prior over what human is likely to do. For example, taking one more fruit will complete the goal of packing picnics, but there are many kinds of fruits in the refrigerator to choose from. From the perspective of completing the goal, any fruit will do, but human preferences may make a difference-the human may want apples instead of bananas at this time.\nWe evaluate all the models on their success rates in achieving the robot's goal. Note that the original evaluation metric in HandMeThat additionally considers the number of robot's steps. An agent can trivially improve success rate with increased steps, by simply searching all objects in a brute force, trial-and-error fashion. We believe that enumeration over objects is not realistic in the real world, so we restrict our experiments to one trial (to be completed within 4 or 5 steps).\n5.2 Model Details\nBaseline models."}, {"title": "6 Results", "content": "We evaluate all the models over the HandMeThat (version 2) dataset in the fully observable setting. Overall, our best-performing Transformer+FISER model achieves a 64.5% success rate on average, achieving the state-of-the-art on the HandMeThat benchmark. The main results are presented in Table 1. Now we discuss the previously stated hypotheses.\nH1: Explicitly modeling human intentions works better than directly predicting actions.\n(a) Separating the social and embodied reasoning steps by explicitly recognizing the robot's task is beneficial. For both prompted GPT-4 Turbo and Transformer-based models, explicitly predicting the robot's task significantly improves the success rates across all difficulty levels, which supports our hypothesis that separating the social and embodied reasoning steps is beneficial in these complex reasoning tasks. The comparison between two different training schemes of our Transformer-based models are presented in Table 2. Results show that training in a multi-staged manner works better than end-to-end in our tasks. It may imply that the low-level grounded planning (embodied reasoning) is requiring a sufficiently different task representation from inferring human's internal goals (social reasoning), that allowing gradients from the embodied reasoning module to flow into the social reasoning module actually hurts performance. It is a further support on empirical side that we should make explicit inferences about human intentions as intermediate reasoning steps.\n(b) Explicitly recognizing the human's plan further helps with the social reasoning stage. When we further include the Human's Plan Recognition (PR) stage, we find that it only helps for the most ambiguous cases (like in Level 4). For GPT-4 Turbo, adding PR is showing approximately the same performances as normal FISER method. We attribute this to the fact that pre-trained LLMs are not good at leveraging hierarchical predictions to improve on this task. For Transformer-based models, introducing PR gives better performance on Level 4, but is harmful to the simplest Level 1. We attribute the poor performance in Level 1 to the fact that such simple tasks do not require knowing the humans' high-level goal. Therefore, forcing the model to predict this information reduces the model's capacity to focus on planning for low-level actions. On the other hand, the improved performance on Level 4 shows that explicit human's plan recognition helps to better learn priors over human intentions. Even on these intrinsically ambiguous tasks, the model can leverage the strong prior to take helpful actions.\nH2: Pre-trained LLMs, despite having access to common-sense knowledge, do not adequately perform the complex social and embodied reasoning in this task. Incorporation of domain-specific knowledge through CoT can help. Results show that training much smaller, more efficient Transformer-based models from scratch is exhibiting about 70% increased performance than prompting state-of-the-art pre-trained LLMs. GPT-4 Turbo's results on Level 1 show it has the capability to do some level of embodied reasoning when given explicit tasks. However, the performance drop on subsequent levels indicates that the required knowledge to solve HandMeThat tasks is not fully covered by common-sense knowledge in pre-trained LLMs. With well-designed prompt engineering (PE) that contains some domain knowledge (e.g., goal space and few-shot examples), GPT-4 Turbo improves significantly across all difficulty levels. However, even with careful CoT prompts and few-shot examples, it is far from small-scale Transformer models across all levels."}, {"title": "7 Conclusion", "content": "We study the challenging HandMeThat benchmark, comprising ambiguous instruction following tasks requiring sophisticated embodied and social reasoning. We find that existing approaches for training models end-to-end, or for prompting powerful pre-trained LLMs, are both insufficient to solve these tasks. We hypothesized that performance could be improved by building a model that explicitly performs social reasoning to infer the human's intentions from their prior actions in the environment. Our results provide evidence for this hypothesis, and show that our approach, Follow Instructions with Social and Embodied Reasoning (FISER), enhances performance over the most competitive prompting baselines by 70%, setting the new state-of-the-art for HandMeThat."}]}