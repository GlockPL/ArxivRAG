{"title": "Revisiting Weight Averaging for Model Merging", "authors": ["Jiho Choi", "Donggyun Kim", "Chanhyuk Lee", "Seunghoon Hong"], "abstract": "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively separates core task-specific knowledge and nuisance noise within the fine-tuned parameters into the top and lower singular vectors, respectively, allowing us to reduce inter-task interference through its low-rank approximation. We evaluate our method on eight image classification tasks, demonstrating that it outperforms prior methods by a significant margin, narrowing the performance gap with traditional multi-task learning to within 1-3%.", "sections": [{"title": "1. Introduction", "content": "Model merging has emerged as an efficient way for constructing multi-task learners [18]. Unlike traditional multi-task learning approaches that directly train a single model on multiple tasks [1], model merging leverages individually fine-tuned models and fuses their parameters to create a model that preserves their original capabilities. This approach eliminates the need to prepare training data or store separate sets of parameters for each task, thereby reducing the costs associated with modern deep neural networks, which often require large amounts of data and numerous parameters. Consequently, model merging has been favored in various applications, including federated learning [25], model compression [32], and continual learning [10].\nAlongside fundamental observations about mode connectivity in parameter space [6-8], model merging through direct interpolation between fine-tuned parameters has become prevalent in the literature. For example, the most straightforward approach is simply averaging the parameters of different models, a method known as weight averaging [3, 10, 13, 20, 33]. However, when the knowledge encoded in each model differs significantly, weight averaging can lead to substantial interference between parameters. This interference often results in considerable performance degradation compared to traditional multi-task learning approaches, especially as the number of merged models increases.\nTo mitigate the limitations of the weight averaging, recent approaches have leveraged the task arithmetic framework [12], which allows for the extrapolation of the parameters. In this framework, models are assumed to be fine-tuned from a common initialization, enabling the definition of task vectors as the directions pointing from the initialization to the fine-tuned parameters in the parameter space. Among several arithmetic operations on task vectors, addition with a scaling coefficient has shown to be effective in merging models trained on various tasks. Subsequent approaches have improved this arithmetic by either resolving interference between task vectors [35] or applying test-time adaptation techniques for better scaling coefficient [36].\nIn this paper, we revisit the weight averaging strategy through the lens of task arithmetic. We begin by formulating weight averaging as a task arithmetic that induces centered task vectors around the weight average itself. We then observe that applying low-rank approximations on the centered task vectors dramatically improves the performance of the merged model produced by the corresponding task arithmetic. When an appropriate rank is chosen, this approach largely outperforms the original task arithmetic and even advanced task vector-based variants. We also note that the optimal rank can be consistently found throughout different experimental settings. To understand this surprisingly high performance, we provide both theoretical insights and empirical evidence based on the spectral analysis of task vectors. We conjecture that centering the task vectors leads to the isolation of core task-specific knowledge from the interfering noise present in their top and lower singular vectors.\nBased on this observation, we propose a novel training-free model merging approach called Centered Arithmetic"}, {"title": "2. Related Work", "content": "Multi-task Learning Multi-task learning (MTL) aims to construct a single model that efficiently handles multiple tasks in terms of parameter and inference efficiency, typically by training the model using all available training data for each task [1]. However, the increasing size of deep learning models [21, 30] makes retraining computationally infeasible when new tasks are added. Moreover, accessing the data distribution of each task is often challenging due to privacy concerns, rendering it practically difficult to utilize all training data. While MTL opts to improve generalization and enhance the performance of each task by learning multiple tasks simultaneously, it often results in decreased performance compared to models trained on individual tasks due to task conflicts [19, 27].\nParameter Manipulation and Task Arithmetic To overcome the limitations of traditional multi-task learning, various methods have been proposed to construct a single model capable of handling multiple tasks by manipulating model parameters alone [11, 28, 33]. However, due to the non-convexity of loss functions and the complexity of the loss landscape in deep learning tasks, naively summing the parameters of models trained on different tasks often leads to significant performance degradation on each task [17, 23]. Intriguingly, Ilharco et al. [12] demonstrated that arithmetic operations in parameter space using task vectors-differences between the parameters of task-specific fine-tuned models and those of a common pretrained model-consistently influence model behavior. Specifically, adding task vectors enhances performance on multiple tasks (task addition), while subtracting a task vector diminishes performance on the associated task (task negation). This suggests that task vectors capture task-specific knowledge in a way that can be algebraically manipulated. Ortiz-Jimenez et al. [24] analyzed this phenomenon, attributing it to weight disentanglement where different directions in the weight space of a model independently govern distinct regions in the input space, allowing the model to manipulate these regions separately for specific tasks and described it as an emergent property inherent to pretrained parameters.\nModel Merging and Task Interference Building upon the findings of Ilharco et al. [12], multi-task models can be constructed without retraining or access to training data by performing a weighted sum of task vectors. However, task interference still leads to reduced performance on individual tasks. To mitigate this issue, recent studies have proposed methods such as Ties-Merging [35], which trims parameters with small magnitudes in the task vectors and resolves sign conflicts among parameters with large magnitudes through majority voting across tasks. Similarly, Yang et al. [36] introduced a method that employs test-time adaptation [31] to dynamically determine the coefficients for the weighted sum of task vectors based on the input data distribution, thereby reducing task interference. Nonetheless, these methods are limited in further reducing task interference because they rely on the fixed definition of task vectors from the pretrained parameters."}, {"title": "3. Preliminary", "content": "Problem Setup We address the problem of model merging, which seeks to combine multiple task-specific models into a single model capable of performing all tasks. Let $\\theta_1,\\cdots, \\theta_T$ denote the parameters of the models trained for $T$ different tasks and $L_t(\\theta)$ represent the corresponding loss function for task $t$. Our objective is to merge the parameters to produce $\\theta^*$ that can perform all tasks, i.e., $L_t(\\theta^*) \\approx L_t(\\theta_t)$ for each task $t = 1,...,T$. By merging models in the parameter space, we need neither an access to"}, {"title": "4. Revisiting Weight Averaging", "content": "In this section, we revisit the weight averaging from the perspective of task vectors. First, we empirically show that the weight average $\\theta_{\\text{avg}}$ serves as a good initialization for defining task vectors with low-rank approximations (Section 4.1). Then we provide an in-depth analysis of the intriguing properties (Section 4.2). Based on these, we propose a novel model merging approach by inducing task vectors from the weight average."}, {"title": "4.1. Intriguing Properties of Weight Averaging", "content": "We begin by showing that the weight averaging in Eq. (1) can be rewritten in the form of task arithmetic in Eq. (2) as follows:\n$$\nA(\\lambda) = \\theta_{\\text{avg}} + \\lambda \\sum_{t=1}^T (\\theta_t - \\theta_{\\text{avg}}),\n$$\nNote that $A(\\lambda) = \\theta_{\\text{avg}}$ always holds regardless of $\\lambda$. Eq. (3) provides a useful insight into understand weight averaging: the weight average $\\theta_{\\text{avg}}$ is itself the result of task arithmetic by posing $\\theta_{\\text{avg}}$ as an initial point for the vectors rather than $\\theta_0$. The induced task vectors $\\tilde{\\tau}_t = \\theta_t - \\theta_{\\text{avg}}$ can be viewed as centered, i.e., summing to zero, while the original task vectors from $\\theta_0$ are generally uncentered. Considering the inferior performance of weight averaging over task arithmetic, it may also seem to suggest that centering the task vectors with $\\theta_{\\text{avg}}$ is disadvantageous.\nHowever, we observe an intriguing trend when we apply the rank reduction on the centered task vectors $\\tilde{\\tau}_t$. Suppose the model consists of $L$ layers and let $\\theta^l \\in \\mathbb{R}^{m\\times n}$ be the weight matrix at $l$-th layer of rank $r$. Then we apply the centered task arithmetic defined in Eq. (3) layer-wise, with low-rank approximation on the task vectors as follows:\n$$\nA_k(\\lambda) = \\theta_{\\text{avg}} + \\lambda \\sum_{t=1}^T \\text{SVD}_k(\\theta_t^l - \\theta_{\\text{avg}}^l), \\quad \\forall l \\leq L,\n$$\nwhere $\\text{SVD}_k(\\theta)$ denotes low-rank approximation of $\\theta$ with top-$k$ singular vectors, i.e., $\\text{SVD}_k(\\theta) = \\sum_{i=1}^k \\sigma_i u_i v_i^T$, where $u_i$ and $v_i$ denote the $i$-th left and right singular vectors obtained by Singular Value Decomposition (SVD), respectively."}, {"title": "4.2. Analysis on Singular Vectors", "content": "To understand the intriguing properties of weight averaging discussed in Section 4.1, we propose a conjecture on the intrinsic structures of the task vectors with further analysis. We hypothesize that the interesting behavior of centered task vectors comes from a separation of knowledge contained in their singular vectors. The type of information represented by task vectors can be regarded as either core knowledge that is necessary to solve each task, or noise that comes from the randomness of fine-tuning. Our conjecture is that in the centered task vectors $\\tilde{\\tau}_t$, core knowledge is concentrated in top singular vectors, while noise is dispersed (ambient) in lower singular vectors. Conversely, in the original task vectors $\\tau_t$, we suspect that both core knowledge and noise are spread across all singular vectors.\nSuch isolation of core knowledge and noise in centered task vectors would allow their low-rank approximations to effectively retain the essential task-specific information with less interference, which explains the \"bell-curve\" behavior in Figure 1. When the rank is small, the performance of the centered task arithmetic rapidly increases by continuously absorbing the core knowledge. After absorbing all core knowledge, the remaining noise components cause severe interference in the parameter space, thus the performance drops with larger rank. At the full rank, the core knowledge and noise exactly counteract each other and the merged model reduces to the weight average. On the other hand, this explains why the performance of the original task arithmetic monotonically increases with rank; the core knowledge is distributed across all singular vectors.\nTo consolidate the conjecture, we perform the low-rank approximation in reverse order, where we preserve lowest $k$ singular vectors for computing $\\text{SVD}_k$ in Eq. (4). The result of the arithmetic with reverse-ordered rank reduction is shown in Figure 2. We observe an inverted bell-curve of the centered task arithmetic (orange solid line), where the accuracy gradually decreases as the rank increases, followed by a rapid rise back to the weight averaging at high ranks near the full rank. This supports our conjecture, where the gradual performance drop is attributed to the noise from lower singular vectors, and the rapid rise near the full rank is attributed to the core knowledge from top singular vectors. The behavior of the original task arithmetic (blue solid line) is also consistent with our conjecture, where the performance still increases monotonically as rank increases, due to the distributed knowledge.\nTo further validate the conjecture, we apply direct rout-"}, {"title": "4.3. Model Merging with CART", "content": "Based on the previous discussions, we propose a training-free model merging approach called Centered Arithmetic with Rank-reduced Task vectors (CART). The method is simple; given $T$ parameters $\\theta_1,\\cdots, \\theta_T$ individually trained for each task $t$, we apply Eq. (4) to obtain merged weight matrices of the model. Compared to the original task arithmetic, CART introduces an additional hyperparameter $k$. However, as we discuss in the experiment (Figure 5a), retaining 8% of the rank yields stable performance across different settings.\nThanks to the generality, CART can be plugged in any merging method that leverages the task arithmetic framework. For example, AdaMerging [36] exploits test-time adaptation to improve the performance. This introduces task-wise and layer-wise merging coefficients $\\lambda_{t,l}$ and adapts them during test time by minimizing the Shannon entropy on the test data. Combined with this variant, Eq. (4) can be transformed into\n$$\nA_k^l = \\theta_{\\text{avg}}^l + \\sum_{t=1}^T \\lambda_{t,l} \\cdot \\text{SVD}_k(\\theta_t^l - \\theta_{\\text{avg}}^l), \\quad \\forall l \\leq L,\n$$\nwhich we refer as CART++ in our experiments."}, {"title": "5. Experiments", "content": "In this section, we present our experimental results. First, we describe the experimental setup in Section 5.1, followed by a comparison of our method with state-of-the-art model merging techniques in Section 5.2. Finally, we provide an in-depth analysis of our method, exploring its unique behavior, scalability, and sensitivity in Section 5.3."}, {"title": "5.1. Experimental Setup", "content": "Tasks and Models Following the prior arts in model merging [12, 35, 36], we conducted experiments on eight vision classification tasks spanning diverse domains and varying numbers of labels: Cars [14], DTD [4], EuroSAT [9], GTSRB [29], MNIST [15], RESISC45 [2], SUN397 [34], and SVHN [22]. Additionally, to examine the effects across different model scales, we employed the ViT-B-32 and ViT-L-14 architectures of CLIP [26]. Following the previous settings [12, 35, 36], we exploit task-specific classification heads obtained by the CLIP text encoder, to evaluate the merged model in each task. We report the classification accuracy at each task and their average.\nBaseline Methods Following Yang et al. [36], we categorize our baselines into several groups. The first group consists of non-merging approaches, including zero-shot application of the pre-trained model (Pretrained), individually fine-tuned models for each task (Individual), and multi-task learning with joint training (Traditional MTL), which respectively represent the lower- and upper-performance bounds for model merging. The second group includes weight averaging methods, such as naive Weight Averaging and its advanced variations, including Fisher Merging [20] and RegMean [13]. The third group includes approaches based on task vectors such as Task Arithmetic [12] and Ties-Merging [35] that aims to reduce task interference. For task vector-based approaches, we additionally consider the method with test-time adaptation that adapts the merging coefficient layer-wise such as AdaMerging [36].\nImplementation Details When merging the model, we applied the low-rank approximation of Eq. (4) to only the matrix component of the parameters such as weight matrices in MLP and project matrices in attention layers, while non-matrix components such as biases or ones in normalization layers are set to standard weight averaging. For hyperparameters, we observe that choosing $k = 0.08 \\cdot r$, (i.e., 8% of full rank) and $\\lambda = 1.0$ works consistently the best across datasets and models."}, {"title": "5.2. Main Results", "content": "Table 1 presents the comparison with various model merging baselines with ViT-B-32 backbone. First, it shows that the performance of naive weight averaging is far below the upper bound (Individual and Traditional MTL) and generally performs worse compared to the approaches based on task vector (Task Arithmetic and Ties-Merging). However, we observe that applying the proper low-rank approximation to weight averaging by our method significantly boosts the performance, surpassing the prior art based on task vector (Ties-Merging) up to 10.6%. Considering that"}, {"title": "5.3. Analysis", "content": "Loss Landscape Visualization To investigate the effect of rank reduction in CART, we visualize the loss landscapes around the weight average $\\theta_{\\text{avg}}$. Following Li et al. [16], we vectorize parameters from all layers into a single flat vector, then project them onto a 2D space spanned by the first two principal components of the parameters. Figure 4 shows contour plots of the loss landscapes for each task, illustrating the trajectory of CART as the rank $k$ from zero to full. Consistent to Figure 5, the trajectory forms a circular path revolves around the weight average, visiting the loss basins of each task. As discussed in Section 4.2, the top and lower singular vectors of the centered task vectors play distinct roles in the parameter space. We observe a common basin of all tasks near the weight average (located in the top-right area of each plot), where the top singular vectors of the centered task vectors guide $\\theta_{\\text{avg}}$ toward this common basin. Conversely, the lower singular vectors introduce noise that causes the parameters to deviate from the common basin. Combined with the scaling coefficient $\\lambda$, which controls the radius of the trajectory, CART enables more dynamic exploration of the loss landscapes than the linear movements of original task arithmetic, leading to a significant improvement in performance.\nScalability on Number of Tasks In this experiment, we assess the impact of the number of tasks on model merging performance. To this end, we construct a subset consisting of $N$ tasks sampled from eight tasks, and report the average performance obtained by all combinations of subsets i.e., $\\binom{8}{N}$, while increasing $N$. Figure 6 illustrates the results. We observe that methods based on simple averaging and task arithmetic exhibit significant performance degradation as the number of tasks increases. This indicates that the inter-task interference intensifies with the increasing number of tasks. On the other hand, the performance of our method remains relatively consistent across varying numbers of tasks. As discussed in Section 4.2, it suggests that eliminating less significant singular vectors of centered task vectors can effectively reduce task interference, leading to a much more scalable approach for model merging.\nOptimal Rank for Low-Rank Approximation Since the performance of our method depends on the low-rank approximation of the task vector, we investigate the impact of rank ($k$ in Eq. (4)) in merging performance. Figure 5 presents the per-task performance with varying ranks on two models, ViT-B-32 and ViT-L-14. Although the over-"}, {"title": "6. Conclusion", "content": "In this study, we have revisited the effectiveness of weight averaging from the perspective of model merging. By conducting a detailed analysis to understand why it performs well, and observing these phenomena in the loss landscape, we confirmed that the centered task vectors induced from weight averaging effectively isolates the core knowledge from interfering noise in their singular vectors. Our proposed method, CART, significantly outperformed previous methodologies by a substantial margin across various classification tasks in 8 vision classification datasets. Particularly, through exhaustive experiments on various combinations of tasks, we demonstrated that our method is scalable with respect to the number of merging tasks. Moreover, since our method can be expressed within the framework of task arithmetic, it allows for incremental application of methodologies in this field. We have shown that CART++, an extension of our method, can achieve performance comparable to traditional multi-task learning."}, {"title": "A. Implementation Details", "content": "We provide the implementation details of our experiments.\nDetailed Experimental Settings The baseline codes and pre-trained weights used in our experiments are available in the repository provided by task arithmetic [12]. This includes fine-tuned checkpoints for the eight vision classification tasks (Cars [14], DTD [4], EuroSAT [9], GT-SRB [29], MNIST [15], RESISC45 [2], SUN397 [34], and SVHN [22]) across both ViT-B-32 and ViT-L-14 architectures [5]. We used the CLIP [26] implementations from the OpenCLIP \u00b9 library, which provides the pretrained weights.\nSearching the Scaling Coefficient In CART, we performed a grid search for the scaling coefficient $\\lambda \\in (0.05, 0.1, 0.2, ..., 1.0)$ and selected $\\lambda$ that yielded the best performance, following the task arithmetic settings [12]. In CART++, which employs test-time adaptation to optimize the scaling coefficients, we adopted the settings from AdaMerging [36]. Using the test dataset for each task, we iteratively optimized the scaling coefficients separately until the evaluation accuracy converged to an upper bound. We provide the pseudocode for CART and CART++ in Algorithm 1, where the test-time adaptation of CART++ is described in Algorithm 2.\nB. Further Analysis on Loss Landscapes\nWe provide further analysis on the loss landscapes near the model parameters obtained by CART and task arithmetic.\nMulti-Task Loss Landscape To directly compare the behaviors of CART and task arithmetic in the parameter space, we visualize the loss landscapes of them with respect to the multi-task loss (e.g., $L = \\sum_{t=1} L_t$). As observed in the left panel of Figure 7, CART can closely approach the common basin induced by the multi-task loss, with its trajectory rotating from $\\theta_{\\text{avg}}$ as the rank increases. In contrast, for task arithmetic, we observed that even as the rank increases (up to full rank), it converges to a point farther from the common basin than CART, which is consistent with Figure 1. In the right panel of Figure 7, we examine the trajectory of task arithmetic with varying $\\lambda$. We observe that when $\\lambda = 0.125$, the trajectory moves toward the weight average, and as $\\lambda$ increases (e.g., to 1), there is a tendency to move away from the common basin. This trend aligns with the results presented by Ilharco et al. [12].\nTask-wise Loss Landscapes In Figure 8, we investigate the loss landscape around the fine-tuned weights $\\theta_t$ for each task. We confirm that the $\\theta_t$ are located within the basin"}]}