{"title": "Entropy Bootstrapping for Weakly Supervised Nuclei Detection", "authors": ["James Willoughby", "Irina Voiculescu"], "abstract": "Microscopy structure segmentation, such as detecting cells or nuclei, generally requires a human to draw a ground truth contour around each instance. Weakly supervised approaches (e.g. consisting of only single point labels) have the potential to reduce this workload significantly. Our approach uses individual point labels for an entropy estimation to approximate an underlying distribution of cell pixels. We infer full cell masks from this distribution, and use Mask-RCNN to produce an instance segmentation output. We compare this point-annotated approach with training on the full ground truth masks. We show that our method achieves a comparatively good level of performance, despite a 95% reduction in pixel labels.", "sections": [{"title": "1. Introduction", "content": "Histopathology is extremely important in detecting and assessing medical conditions, particularly in cancer [20][9]. As in many other areas of medicine Deep Learning has shown great potential to aid in computerised pathology, aiming to reduce both human error and human workload.\nCell instance segmentation has typically been the task of choice for deep learning based approaches, since the output provides the cell localisations and extent, which are easy for clinicians to verify and correct if needed. However the limitation behind such an approach is the vast amount of labeled data needed to train the models. These cell segmentation masks are especially time consuming due to the thousands of cells on any given slide, making expert annotation either prohibitively expensive or incredibly protracted. Even then, given the sheer number of cells we still expect these ground truths to be slightly inaccurate.\nDue to the challenges involved in acquiring such a dataset, weakly supervised learning is a natural fit in this scenario, as it can significantly reduce the labeling requirement. We provide a novel approach for weakly supervised cell detection. It first uses uncertainty to \"bootstrap\" the weak labels, and then refines the \"bootstrapped\" predictions into individual cell detection instances.\nRather than performing segmentation and checking mask overlap. we carry out cell deection. For clinical purposes, tasks such as predicting a bounding box for each cell provide a similar level of information, localising the cell within the image, giving an estimate of its size, and only omitting precise shape information. There is strong evidence that size analysis provides valuable information about pathol-"}, {"title": "2. Related Work", "content": "2.1. Cell Detection and Segmentation\nThe dataset and results we report are used for nuclei detection rather than full cell detection but, for obvious reasons, the methods involved in the two are closely related. As such we discuss methods relevant to both.\nAutomated Cell detection and segmentation methods have been an area of research for many years due to the large number of cells involved in histopathology as well as the subjectivity of the field. Watershed-based segmentation approaches have been historically popular such as those in [4] and [11] as these work well given a good contrast and marker proposal approach. Other popular approaches include Voronoi-based methods such as those in [10] and [23], since the Voronoi tiling of each cell's centroid is a good approximation of cell morpohology as discussed in more detail in [10]. The dual of the Voronoi diagram, the Delaunay triangulation, is also a useful classical procedure in this context and can be used as a way of separating clustered cells as described in [21].\nAs Deep Learning has risen in popularity as a methodology, cell detection and segmentation has naturally adopted it as a strategy and has seen significant benefit. Supervised deep learning approaches have emerged that are able to achieve levels of accuracy similar to human annotators as described in [6] and [18]. Both of these approaches address the issue of training data which is a conspicuous problem within this task and they do so in different ways. In [18] the authors use an automatic training set generation process by converting fluorescent nuclei and cytoplasm into masks via the use of an existing tool. They also create a manually segmented test set but note that even this small proportion of their overall data is a significant number of cells to segment manually. In [6] they take an adjacent but distinct approach, gradually creating a dataset of one million labeled cells via a human-in-the-loop process. As these approaches show, there is considerable benefit to be gained from deep learn-ing approaches to this problem but they inevitably run up against the issue of labeling."}, {"title": "2.2. Weakly Supervised Cell Segmentation", "content": "Given the obvious potential of deep learning approaches and the labeling limitation of traditional supervised learning approaches, many weakly supervised methods have been proposed to tackle this challenge.\nPoint or scribble annotations are the weak label of choice in the vast majority of methods as they are significantly easier to acquire than full masks and still provide a great deal of information about cell count and localisation which can be leveraged in order to acheive good results in cell detection and segmentation tasks.\nMany methods approach the problem via a multitask approach. In [2] the network uses Repel encoding, clustering and a Voronoi diagram as combined losses for their network to optimise and [17] uses a similar approach for their final loss function, optimising over cluster and Voronoi losses as well. This multitask approach allows for different aspects of the weak label information as well as any prior learned output to be leveraged efficiently.\nMany approaches also base themselves around known information about cell morphology or distribution. In [22] for example the network is based around learning a set of shapes for cell boundaries in order to improve its precision around the cell border. In [16] the centriod of each cell is used as the training data which is then expanded upon in order to segment entire cells.\nThe Voronoi diagram of the cell annotations or predictions is used at some point in many methods as shown in [17][2], because it provides a quick way to reduce the problem. This is because each point within a cell should generally be closer to its point label than any other point label. As a result only one instance will be contained within each partition of an accurate Voronoi diagram, which makes separating instances much easier.\nSome methods approach the problem using a self-supervised or co-supervised training strategy as in [24] and [19] this allows for a weak initial mask to be improved via consensus between simultanously trained networks."}, {"title": "2.3. Our Contributions", "content": "We distance ourselves from the above approaches in that we are primarily focusing on using a Bayesian Deep learn-"}, {"title": "3. Methods", "content": "3.1. Data\nThe data used in this paper was taken from the PanNuke dataset [5] a cancer nucleus instance segmentation and classification dataset across 19 tissue types. The central idea behind this dataset is that it aims to provide a dataset which represents the \"clinical wild\" better than existing histopathology datasets; methods trained on past datasets have tended to underperform when translated to real clinical scenarios. We believe that this kind of dataset represents the most challenging test for a weakly supervised approach. We used 2, 656 images from this dataset split 80 : 20 into training and test sets. The images were all 256\u00d7256 pixels."}, {"title": "3.2. Entropy Bootstrapping", "content": "The entropy bootstrap step is designed to take the point annotations provided and convert them into a \u201cprior\" segmentation which can then be crefined down into a more precise instance segmentation.\nWe first pass the images and point annotations into a Bayesian Segmentation Network in order to get a measure of the entropy over the labels. Specifically we used the Functional Variational Inference formulation outlined in [1]. This network estimates the probability of each pixel belonging to each class $p(X = x)\\forall x \\in C$. In our case this is binary classification into the cases $x =$ Nucleus and $x =$ Not Nucleus for each pixel. This probability distribution over the classes of all the pixels which we can take the entropy given by:\n$H(x) = - \\sum_{x \\in C} p(x)log(p(x))$ (1)\nThis measures the average uncertainty in class prediction across the image.\nOur point annotations are designed to replicate the scenario of an annotator looking over all the nuclei and placing a dot in each one. Given no guidance we assume that this dot could be placed in any random point within the nucleus.\nIn order to approximate this scenario with a clinician placing dots we take an existing dataset of instance segmented nuclei [5] and randomly choose points within each nucleus to place our dots. In practice we do this by sampling a random pixel within each nucleus and adding a small neighborhood around it to a binary segmentation mask. Given the random sampling of the pixel within the nucleus we assume that we can roughly express this \u201csparsification\" of the ground truth labels by the conditional probability:\n$P(X_L = Nucleus|x_T = Nucleus) = \\epsilon$ (2)\nThis is the probability that a pixel will be labeled as Nucleus in our point annotations given that it is a Nucleus pixel in"}, {"title": "3.3. Instance Conversion", "content": "The conversion process from our approximation of $P(C_T)$ to an instance segmentation has a number of issues to confront. Nuclei are often clustered and since we are usually looking at a two-dimensional slice through a three-dimensional tissue there can be overlapping nuclei. In addition to the separation issues from overlapping, we also have many cases where nuclei will be hard to distinguish from the background due to poor contrast. We may also have cases where the ground truth labels have missed some obvious nuclei amidst the large number of annotations.\nIn order to convert the distribution over the nucleus pixels into a set of instances we first apply a deterministic process, shown in Fig. 3:\nFirstly we apply gaussian blur over the entropy estimate to reduce the noise that will interfere with later steps. We then generate the Voronoi diagram of the input point annotations and take the edges. We subtract these edges from the entropy distribution in order to delineate between adjacent instances as much as possible.\nWe then apply gaussian adaptive thresholding to generate a binary segmentation of our separated entropy distribution, where the threshold value is given by a gaussian-weighted sum of values in the neighborhood of each pixel. This has to be done locally due to the nucleus clustering issue. The minima of our entropy distribution in regions with many nuclei are much higher than minima in more sparsely populated regions of the image. This results in a"}, {"title": "3.4. Training and evaluation using Mask-RCNN", "content": "Our instance masks generated by the watershed are likely to have rough borders and there is the potential of under- or over-segmentation of the nuclei in these masks which will interfere with alignment with the ground truth masks. As such we aim to use Mask-RCNN [8] to try and improve these instance masks.\nThe rationale for why this should improve the masks is that if our watershed instances are good enough then they begin to look like ground truth masks which have been eroded by noise. From a learning perspective we expect that it should be easier for the network to learn the features that all the masks have in common, which represent good nucleus features, than it will be to learn the precise erosion needed to learn the watershed masks accurately. In particular Mask-RCNN has a multiscale feature embedding which allows for higher level features of the nucleus to be taken into account than the watershed is able to see.\nAs such we assume that given the watershed masks localise most of the nuclei and segment a significant proportion of each nucleus then we can use Mask-RCNN to improve the watershed masks significantly. This assumption of how Mask-RCNN will behave does rest on not introducing too many systematic errors across the watershed masks. This would introduce new easy-to-learn features which would harm performance as Mask-RCNN would learn to predict common mistakes. A good example of such an issue would be single masks over clustered nuclei, this kind of mistake would be easy to learn for Mask-RCNN and would compare very poorly against the ground truth.\nOne nice property of our work up until this step is that we have thus far used statistically grounded or deterministic processes giving the process a lot of transparency. We can propagate the entropy distribution all the way up to our watershed masks to give an uncertainty interpretation of our predictions however with the Mask-RCNN step we lose this. While Mask-RCNN does give a confidence output as part of its prediction, the confidence of a deep learning model is known to not be correlated well with uncertainty [13]. With a more uncertainty-based final step we could make our process more transparent and provide a reliable output uncertainty but this is reserved for future work."}, {"title": "3.4.1 Implementation Details", "content": "For our experiments we used Mask-RCNN [8] with a number of different backbone choices, ResNet50 [7], full size Swin Transformer [12] and small size Swin Transformer. We used imagenet pretrained weights. We trained with the AdamW optimiser [14] and linear learning rate decay with a base learning rate of 0.00008."}, {"title": "4. Results and Discussion", "content": "4.1. Assessing Entropy\nDetermining Appropriate Metrics\nIn order to proceed with the process we have to assess the quality of our entropy distribution and in particular its suitability for use as a \"prior\" for our instance masks. As such we need to determine how well the distribution correlates with the ground truth masks.\nWe can threshold the entropy at different probability values in order to assess how well the distribution matches the ground truth at these different thresholds. A straightforward way to do this is to take the Dice score of the thresholded entropy when compared with the background. Plotting these Dice values against the entropy threshold we can produce a graph as in the left plot in Fig. 4. The limitation of this Dice measure when we are mostly interested in detection and lo-"}, {"title": "4.2. Assessing Deterministic Instance Generation", "content": "Determining Appropriate Metrics\nIn order to assess how well our instance generation step performs at generating instance masks we need to compare with the ground truth instance masks. A natural way to do this is via an mAP50-like metric since this is a widely used standard for object detection, however we lack confidence values for our instance masks so direct use of the metric is not possible. We instead choose to measure the detection rate at different IoU thresholds across the test set. IoU is the intersection over union, measuring the extent to which two regions match as:\n$IoU = \\frac{X_{pred} \\cap x_{gt}}{X_{pred} + X_{gt} - X_{pred} \\cap X_{gt}}$ (5)\nHere $X_{pred}$ and $x_{gt}$ are the sets of predicted pixels and ground truth pixels respectively. Detection rate is given by:\n$Da = \\frac{TP_\\alpha}{N_{nuclei}}$ (6)\nWhere $TP_\\alpha$ is the number of predictions which overlap with a ground truth box with IoU greater than threshold value $\\alpha$ and $N_{nuclei}$ is the total number of nuclei in the ground truth.\nWe calculate this value for a number of different thresholds from 0.25 to 0.75. We expect to perform poorly near 0.75 as this indicates precise alignment of the predictions and the ground truth which we do not expect to achieve for a number of reasons. Firstly we are using watershed on the image pixels to generate our masks which will result in jagged boundaries which will never align precisely with the smooth boundaries given in the ground truth. Secondly, even though we are confident that we are separating the nucleus clusters well, given our Voronoi diagram is generated from random points within the nuclei we are unlikely to be separating exactly on the nucleus boundary, causing imprecise bounding boxes.\nWe expect to perform well at a threshold value of 0.25 since this is the most forgiving to imprecise prediction boxes however this is a poor metric of detection since it will allow for a prediction of a whole cluster of nuclei to be marked as correct in many cases and we do not want to reward this behaviour. As such a prediction threshold of 0.5 is considered to be a reasonable metric to base assessment around. This threshold indicates that a prediction is well localised and scaled compared to the nucleus and minimises correct predictions for nucleus clusters but allows for detection with the imprecise nucleus borders we expect to see. This justification is backed up by mAP50 being considered one of the core assessment metrics for object detection in general [3]."}, {"title": "4.3. Assessing Mask-RCNN output", "content": "Determining Appropriate Metrics\nMask-RCNN outputs are generally assessed on an instance basis using the bounding box and segmentation mAP measures. For bounding box the IoU is calculated by comparing the masks and for segmentation it is done by comparing the masks directly. Naturally we expect the bounding box performance to be better than the segmentation performance and since fundamentally we are interested in detection and localisation we can treat the bounding box metric as our main metric of quality. As discussed in Sec. 4.2 we are interested mostly in the values at IoU greater than 0.5 so we report the values for thresholds of 0.5 and 0.75 as shown in Tab. 1 and Tab. 2."}, {"title": "5. Conclusions and Further Work", "content": "In addition to proposing a novel weakly supervised nuclei instance detection method based on entropy estimation, we also wish to generate discussion over the appropriate choice of metrics for assessing such tasks.\nWe show that the masks generated with our relatively simple method can be used with an off-the-shelf instance segmentation model to approach the performance seen when training with the full set of original instance masks. We also find that Mask-RCNN has limitations in this scenario, indicating that better results could be achieved with a domain-specific final step, to be investigated in future.\nIn addition we would like to investigate a more uncertainty-focused approach which can take advantage of the entropy distribution better or provide grounded uncertainty estimates of output predictions.\nAnother advantage of an uncertainty based approach would be the ability to deal with ambiguous or missed labels in a more principled manner. This would enable an approach better suited to the reality of nuclei detection.\nWe can also extend this to multiclass detection fairly easily if our point annotations have class labels attached as we can reintroduce this information when making the watershed masks. This would provide valuable additional information in the output."}, {"title": "6. Appendix A", "content": "Starting from our entropy definition of:\n$H = - \\sum_{x \\in C} p(x)log(p(x))$ (7)\nWe use the fact that we are doing a binary labelling process to define:\n$P(X_L = nucleus) = P(C_L)$ (8)\n$P(X_L = background) = P(C_\\bar{L}) = 1 - P(C_L)$ (9)\n$P(x_T = nucleus) = P(C_T)$ (10)\n$P(x_T = background) = P(C_\\bar{T}) = 1 - P(C_T)$ (11)\nWhere $X_L$ is the label of pixel x which is input into the network and $x_T$ is the ground truth label of pixel x.\nOur labeling approach is such that we accurately label all the background pixels. As such we make the assumption that given a background pixel from the ground truth, we label it correctly in the input labels with probability 1:\n$P(C_L|C_\\bar{T}) = 1$ (12)\nThe second component of our approach is that we label the foreground or nuclei pixels accurately only with small probability $\\epsilon << 1$. So our probability that a nucleus pixel in the ground truth will be labeled as a nucleus pixel in our final labels is:\n$P(C_L|C_T) = \\epsilon$ (13)\nExpanding the entropy we get:\n$H = -P(C_L)logP(C_L) - P(C_\\bar{L})logP(C_\\bar{L})$ (14)\nWe can use Bayes theorem to rewrite P(CL) as:\n$P(C_L) = P(C_L| C_T)P(C_T) + P(C_L|C_\\bar{T})P(C_\\bar{T})$ (15)\nand we can use that $P(C_L|C_\\bar{T})$ is the complement of our assumption in 12 to show:\n$P(C_L|C_\\bar{T}) = 1 - P(C_\\bar{L}|C_\\bar{T}) = 0$ (16)\nwhich we substitute into 15 get:\n$P(C_L) = \\epsilon P(C_T)$ (17)\nThis now allows us to rewrite our entropy from 14 as:\n$H = -\\epsilon P(C_T)log\\epsilon P(C_T) - (1 - \\epsilon P(C_T))log(1 - \\epsilon P(C_T))$ (18)\nsubstituting x = P(CT) and rearranging we can then write the entropy in terms of $\\epsilon$ and x:\n$H = -\\epsilon x log\\epsilon - \\epsilon x log x - (1 - \\epsilon x)log(1 - \\epsilon x)$ (19)\nIn order to see which term dominates in this expression for small $\\epsilon$ we look at the relative rates at which each term tends to zero as $\\epsilon \\rightarrow 0$. We can see that $\\epsilon x log(x)$ goes to zero faster than $\\epsilon x log(\\epsilon)$ as:\n$\\lim_{\\epsilon \\rightarrow 0} \\frac{\\epsilon x log(\\epsilon)}{\\epsilon x log(x)} = \\lim_{\\epsilon \\rightarrow 0} \\frac{log(\\epsilon)}{log(x)} = 0$ (20)\nand similarly we can show that:\n$\\lim_{\\epsilon \\rightarrow 0} \\frac{(1 - \\epsilon x)log(1 - \\epsilon x)}{\\epsilon x log(x)} = 0$ (21)\nThis means that for sufficiently small $\\epsilon$ the $-\\epsilon x log(\\epsilon)$ term dominates and the entropy behaves as:\n$H = -elog(\\epsilon)P(C_T)$ (22)\nFor small $\\epsilon$ this elog(\\epsilon) term is a small positive constant, so we can then write:\n$H \\propto P(C_T)$ (23)\nAs observed."}, {"title": "7. Appendix B", "content": "We also report the results of ablation studies into varying parameters in the sampling procedure.\n7.1. Varying Point Radius\nIn our point annotation creation procedure we sample our random point from within each ground truth instance mask and then convert it into an input mask by adding a neighborhood around the point. This is done to mimic point labels that a clinician would create with a stylus when labeling nuclei. Our choice of radius for this virtual \"pen\u201d has two effects. If the radius is too large then it will likely create a mask which spills over the nucleus border, introducing additional uncertainty, but also introducing more total correct pixels into the labels. If the radius is too small then the network may have too little information to learn on, resulting in a poor output.\n7.2. Percentage of Point Annotations\nWe also carried out a study into the effect of reducing the percentage of point annotations used in training on entropy performance. This represents another axis along which labels can be made \u201cweaker\u201d, by reducing the number of instances labeled at all. We expect that decreasing the percentage of instances labeled will decrease the efficacy of our method but it is interesting to see how the entropy varies as this is changed. Of course because there are a very large number of nuclei labeled across the entire dataset, well over 100, 000, a reduction by 50% still leaves us with a very large number of labeled nuclei.\n7.3. Position Error Experiments\nAnother type of \u201cweakening\u201d we could apply to these point annotations is introducing random translation of the sampled points. This would be emulating a particularly lazy or rushed annotator, or an error in the labeling software. This would result in the points being place near but not exactly where the annotator intended. We emulate this effect by adding gaussian noise to the coordinates of the point annotations to introduce a position error of up to 5 pixels. We expect that this will worsen performance as we will sometimes be labeling background incorrectly but a small position error should not have a drastic effect on our approach."}]}