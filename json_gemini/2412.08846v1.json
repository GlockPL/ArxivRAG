{"title": "Exploring Large Language Models on Cross-Cultural Values in Connection with Training Methodology", "authors": ["Minsang Kim", "Seungjun Baek"], "abstract": "Large language models (LLMs) closely interact with humans, and thus need an intimate understanding of the cultural values of human society. In this paper, we explore how open-source LLMs make judgments on diverse categories of cultural values across countries, and its relation to training methodology such as model sizes, training corpus, alignment, etc. Our analysis shows that LLMs can judge socio-cultural norms similar to humans but less so on social systems and progress. In addition, LLMs tend to judge cultural values biased toward Western culture, which can be improved with training on the multilingual corpus. We also find that increasing model size helps a better understanding of social values, but smaller models can be enhanced by using synthetic data. Our analysis reveals valuable insights into the design methodology of LLMs in connection with their understanding of cultural values.", "sections": [{"title": "1 Introduction & Related Work", "content": "Large Language Models (LLMs) have become a core technology in many real-world applications to closely interact with humans. Thus, it is important that LLMs understand the cultural values and diversities of human societies. In particular, since LLMs are widely used as general assistants (Achiam et al., 2023; Team et al., 2023), they should generate responses which capture the cultural context of target users. For example, people in some countries may think that divorce is largely unacceptable, but those in other countries may consider it as an unpleasant but acceptable practice.\nRecently, there have been studies on how language models judge moral norms. (Schramowski et al., 2022) investigated whether English language models such as BERT have moral bias like humans in English culture. (Ramezani and Xu, 2023) proposed a methodology for probing moral norms of English language models across diverse cultures. In their experiments, models trained in English predict empirical moral norms across countries worse than the English moral norms. Thus, the authors argue that the moral norms inferred by LLMs have potential risks of bias, and the moral norms suggested by the models should not be regarded as definitive values of morality. Recently, (Arora et al., 2023) examined bidirectional language models such as BERT (Devlin, 2018) and RoBERTa (Liu, 2019) on cross-cultural values in different countries. While the authors extensively analyzed the LLMs' predictions, they did not examine the links between training/design choices for LLMs and the human-likeness of LLMs' judgments.\nIn this paper, we extensively study the progress of LLMs in cultural values across countries using World Value Survey (Haerpfer et al., 2022). We provide detailed observations on the capability and limitations of LLMs and their relation to the design choices for LLMs. We systemically study recent models such as LLaMA (Touvron et al., 2023; Dubey et al., 2024), Phi (Abdin et al., 2024), and Yi (Young et al., 2024) on diverse and debatable categories of cultural values. In particular, we discuss how pre-training methodology (Touvron et al., 2023; Team et al., 2024; Young et al., 2024; Abdin et al., 2024) and model attributes such as model sizes, training corpus, alignment, etc., affect the LLMs' understanding of human cultures. We expect that our findings provide key insights on design methodology to improve the quality of cultural judgments by LLMs across countries.\nOur findings are summarized as follows: i) LLMs similarly judge Socio-Cultural Norms with humans, but are less similar in Social Systems and Progress. ii) LLMs' assessment of cultural values tends to be biased toward Western cultures. iii) However, cultural diversity can be improved by training on the multilingual corpus. iv) Larger models have a stronger cultural-awareness. v) Synthetic data can distill cultural knowledge of larger"}, {"title": "2 Methodology of probing LLMs", "content": "2.1 Dataset - World Value Survey\nWe use the World Value Survey (WVS) (Haerpfer et al., 2022) dataset to probe LLMs's knowledge of cultural values across countries. WVS contains questions on diverse topics of social values. There are 209 questions from 12 categories in 55 countries. To probe LLM, we convert all questions and answers to multiple-choice tasks using the template in Fig. 5 in Appendix A.1. Each question has K candidate answers $a_k$. A sample question: \u201cIs the death penalty justifiable?\u201d has $K = 3$ candidate answers: (A) Never Justifiable, B) Neutral C) Justifiable}. In our experiment, K takes values from {3,5,7,9}. Table 2 in Appendix A.3 shows dataset statistics of WVS.\n2.2 Method - Scoring Cultural Values\nBelow we describe a method to measure the correlation between the answers provided by LLM and humans. We individually assign scores to candidate answers which enables quantifying correlations in the choice of answers. Let $s_k$ denote the score assigned to k-th candidate answer for $k \\in [K]$. We let $s_k$ take values between -1 and 1. Since there are K answers, we regularly distribute $s_1, ..., s_K$ over interval [-1,1]. For example, the question \u201cIs the death penalty justifiable?\u201d has candidate answers never justifiable, neutral, or justifiable. We assign the scores -1, 0, and 1 to the candidate answers. For each question, We will compute the mean score which is the expectation of score over the distribution of answers generated by LLMs and humans, and compare the correlations of their average scores.\nFirstly, we define the distribution of answer candidates chosen by LLMs. Let $p_k$ denote the probability of generating k-th answer candidate for $k \\in [K]$ conditional on the question. $p_k$ is defined to be the likelihood (softmax output) of answer k of LLM model P:\n$p_k = P(w_{i} | w_{i-1}, ..., w_{1})$ (1)\nwhere $w_i$ denotes the i-th token, $l_k$ denotes token length of k-th answer candidate, and $c$ denotes the length of question contexts. We normalize $p_k$'s to yield a probability distribution $P_k = p_k/(\\Sigma_{i=1}^{K} p_i)$. Thus, the mean score of LLMs is given by $\\Sigma_{i=1}^{K} p_k s_k$.\nSecondly, we compute the distribution of answer choices by humans. The distribution denoted by $r_k$, $k \\in [K]$, is simply set to the empirical distribution of choosing answer candidate k for the given question. The statistics of answer distribution by the survey participants are available in the WVS dataset. Thus, the mean score of humans is given by $\\Sigma_{i=1}^{K} r_k s_k$\nFinally, we compute the Pearson's correlation (Freedman et al., 2007) between mean scores over questions to measure the degree of agreement between LLM and humans."}, {"title": "3 Experiment & Analysis", "content": "3.1 Experimental settings\nBaselines. We evaluate various open-source LLMs such as phi-series (Li et al., 2023; Gunasekar et al., 2023; Abdin et al., 2024), Yi (Young et al., 2024), Llama-2 (Touvron et al., 2023), and Llama-3 (Dubey et al., 2024). We chose the open models because the model information such as sizes and training tokens are publicly available, which enables detailed analysis. The models are categorized into small or large models. The numbers of parameters for small models are between 1B and 8B parameters; for large models, the sizes are 13B, 34B, and 70B. If a chat model associated with the base model is available, we include it in the analysis to examine the effects of alignment. \n3.2 Exploring LLMs' Judgments on WVS and connections to Training Methodology\nWe examine how similar the LLMs' inference to that of humans in various categories of cultural values, and its relation to training methods. We aggregate the mean scores and compute their correlations over WVS dataset to measure the similarity.\nObs. 1. LLMs and Humans judge similarly on Socio-Cultural Norms, but less so on Social Systems and Progress. Fig. 1 show the ratings by LLMs and humans across various topics in descending order of similarity between LLMs and humans. We roughly divide topics into two topic groups based on similarity: we observe that LLMs tend to agree with humans on social-cultural norms group, but less so on social systems and progress group. Since LLMs are trained from mas-"}, {"title": "4 Conclusion", "content": "In this paper, we extensively examine how LLMs make judgments of cultural values. In our experiments, current LLMs have limitations of cross-cultural judgments specific to questions categorized in societal systems and progress and tend to be biased toward Western culture. However, we also find that training multilingual data can transfer cross-cultural knowledge to English. In addition, since larger models have a better perception of cultural values, training synthetic data created by larger models can be helpful. Finally, alignment successfully trains the model of human likeness."}, {"title": "5 Limitations", "content": "Despite our extensive study of various language models (LLMs) across diverse categories of cultural values, our research remains confined to the World Value Survey dataset. Consequently, we plan to further investigate the properties of LLMs\u2019 cultural judgment using other datasets in the future. Furthermore, due to the lack of technical details (Li et al., 2023; Gunasekar et al., 2023; Abdin et al., 2024), we conduct a preliminary analysis of the properties of synthetic data. Although we indirectly present our rationale for this analysis as derived from other well-known synthetic data (Ben Allal et al., 2024), we aim to provide a comprehensive overview."}]}