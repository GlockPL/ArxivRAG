{"title": "Approximating Auction Equilibria with Reinforcement Learning*", "authors": ["Pranjal Rawat"], "abstract": "Traditional methods for computing equilibria in auctions become computationally intractable as auction complexity increases, particularly in multi-item and dynamic auctions. This paper introduces a self-play based reinforcement learning approach that employs advanced algorithms such as Proximal Policy Optimization and Neural Fictitious Self-Play to approximate Bayes-Nash equilibria. This framework allows for continuous action spaces, high-dimensional information states, and delayed payoffs. Through self-play, these algorithms can learn robust and near-optimal bidding strategies in auctions with known equilibria, including those with symmetric and asymmetric valuations, private and interdependent values, and multi-round auctions.", "sections": [{"title": "Introduction", "content": "Auctions are a time-tested way for sellers to discover prices and allocate items to those who value them the most. As marketplaces and platforms become increasingly virtual, there is an even greater scope to design markets to improve efficiency, revenue, fairness, etc. Game-theoretic analysis has shed much light on strategic behavior in simple auctions with few bidders and simple rules. These insights have been successfully applied to spectrum, procurement, and resale auctions (Milgrom 2004).\nHowever, game-theoretic analysis has struggled to make inroads when auctions begin to get complex: many bidders, multiple items, multiple rounds, varying information disclosures, and payment rules. As we move beyond the textbook cases, it quickly becomes apparent that solving these auctions is highly challenging. The few equilibria that we know in these settings are obtained through very clever arguments that simplify the problem considerably through symmetry and capture extreme cases (Wilson 1987, Cho et al. 2024).\nAuctions are typically modeled as Bayesian games of incomplete information. Conitzer and Sandholm (2008) show that computing pure equilibria in 2x2 auctions is NP-complete, i.e., it is computationally infeasible for any algorithm to guarantee a solution in polynomial time. While we know that mixed equilibria must exist (Nash 1951), finding such equilibria is PPAD-hard (Daskalakis, Goldberg, and Papadimitriou 2009). Cai and Papadimitriou (2014) demonstrated the PP-hard complexity of finding exact Bayesian Nash equilibria in specific simultaneous auctions. This and other results lead Cai and Papadimitriou (2014) to conclude that \u201cwe now know that the auctions of Vickrey and Myerson are isolated areas of light in a sea of dark\u201d.\nIn parallel to these developments, recent advances in AI have led to breakthroughs in solving games of imperfect information. Reinforcement learning is an active area of research and has shown remarkable progress in finding powerful strategies in games like Chess, Go, and Poker. The central ingredient in these cases is a flexible representation of the value of states and an iterative method to obtain feedback and refine policies. The success of reinforcement learning in games of perfect and imperfect information motivates their use in finding theoretical market equilibria.\nIn this paper, I show that simple but sophisticated policy gradient algorithms can converge to pure Nash equilibrium in simple auctions through self-play. These algorithms use reinforcement learning, or learning through exploration and feedback, to continually refine strategies by playing against themselves. I focus on approximate equilibria, where participants play e-best responses to each other. At the theoretical"}, {"title": "Bayesian Games", "content": "Bayesian games are characterized by players with incomplete information, typically about other players' types, which are private. Each player's type is drawn from a known probability distribution, and this type influences their strategy. Formally, a Bayesian game is defined as G = (I,V,O, A, f, u), where I is the set of players, V represents players' private types, O is the set of possible observations, A is the action space, f is the probability density function of types and observations, and u represents the utility functions of the players."}, {"title": "Bayes Nash Equilibria", "content": "In Bayesian games, players select actions based on their observations and types. A strategy for player i is a mapping from observations and types to actions, denoted \u03b2\u1d62 : O\u1d62 \u00d7 V \u2192 A\u1d62. The central solution concept is the Bayesian Nash Equilibrium (BNE). A strategy profile (\u03b2\u2081, ..., \u03b2\u2099) is a BNE if, given the strategies of other players, no player can improve their expected utility by unilaterally deviating from their strategy. Formally, a strategy profile is a BNE if for each player i:\nE[U\u1d62(\u03b2\u1d62(O\u1d62, V\u1d62), \u03b2\u208b\u1d62(O\u208b\u1d62, V\u208b\u1d62), V\u1d62)] \u2265 E[U\u1d62(a\u1d62, \u03b2\u208b\u1d62(O\u208b\u1d62, V\u208b\u1d62), V\u1d62)] \u2200a\u1d62 \u2208 A\u1d62.\nThis ensures that each player's strategy is optimal, given the strategies of others and their private information.\nAn e-Bayesian Nash Equilibrium (e-BNE) relaxes this condition slightly. A strategy profile is an e-BNE if no player can improve their expected utility by more than \u20ac \u2265 0 by deviating from their strategy. Formally, (\u03b2\u2081, ..., \u03b2\u2099) is an e-BNE if for each player i:\nE[u\u1d62(\u03b2\u1d62(O\u1d62, V\u1d62), \u03b2\u208b\u1d62(O\u208b\u1d62, V\u208b\u1d62), V\u1d62)] \u2265 E[U\u1d62(a\u1d62, \u03b2\u208b\u1d62(O\u208b\u1d62, V\u208b\u1d62), V\u1d62)] \u2013 e \u2200a\u1d62 \u2208 A\u1d62.\nThis allows for small deviations in optimality, but still ensures that no player can significantly improve their outcome by deviating."}, {"title": "Example: First-Price Auction with Two Bidders", "content": "Consider a first-price sealed-bid auction with two bidders, each with a private valuation drawn from a uniform distribution on [0,1]. The bidders simultaneously submit bids, and the highest bidder wins and pays their bid. Each bidder's objective is to"}, {"title": "Auctions in the Reinforcement Learning Framework", "content": "Reinforcement learning (RL) provides a framework for agents to learn optimal strategies through interaction with an environment. We recast Bayesian games, specifically auctions, into the RL framework, allowing bidders to learn equilibrium strategies through repeated play. The goal is for each bidder to converge toward a Bayesian Nash equilibrium by applying RL algorithms.\nIn RL, an agent interacts with the environment over discrete time steps. At each time step t, the agent observes a state s\u209c, selects an action a\u209c according to a policy \u03c0(a\u209c|s\u209c), receives a reward r\u209c, and transitions to a new state s\u209c\u208a\u2081. The key components in the context of auctions are:"}, {"title": "States, Actions, and Rewards", "content": "The state of player i, s\u1d62 \u2208 S\u1d62 = V\u1d62 \u00d7 O\u1d62, includes their private valuation v\u1d62 and any observations o\u1d62. In sealed-bid auctions, o\u1d62 may be empty; in dynamic auctions, it may include previous bids or signals.\nThe action a\u1d62 \u2208 A\u1d62 is the bid chosen by player i. The action space can be continuous or discrete, depending on the auction design.\nThe reward function r\u1d62(s\u1d62, a\u1d62) depends on the auction outcome, determined by the"}, {"title": "Policy and Value Functions", "content": "Each agent maintains a policy \u03c0\u1d62(a|s\u1d62), defining the probability distribution over actions based on the current state. The policy is parameterized (e.g., by \u03b8) and updated over time to maximize expected rewards. Initially, policies are stochastic to encourage exploration.\nAgents estimate a value function V\u1da0(s\u1d62), representing the expected return when starting from state s\u1d62 and following policy \u03c0\u1d62. This helps assess state quality and reduces variance in policy updates."}, {"title": "Learning Process", "content": "The learning involves episodes, each corresponding to an independent auction. At each episode:\n1. State Observation: Agent i observes s\u1d62 = (v\u1d62, o\u1d62).\n2. Action Selection: Agent selects a\u1d62 according to \u03c0\u1d62(a\u1d62|s\u1d62).\n3. Auction Outcome: Actions determine the outcome; agent receives reward r\u1d62.\n4. Experience Collection: Agent collects (s\u1d62, a\u1d62, r\u1d62, s\u1d62'); in single-shot auctions, s\u1d62' may not exist.\n5. Policy Update: Agent updates policy parameters using collected experience.\nIn multi-agent settings, each agent's optimal policy depends on others' policies. Convergence requires agents' policies to stabilize, which is challenging since the environment is non-stationary from any single agent's perspective. Therefore,"}, {"title": "Example: First-Price Auction", "content": "In a first-price auction with two bidders, each bidder's valuation v\u1d62 is drawn from [0, 1]. The state is s\u1d62 = v\u1d62, and the action is the bid a\u1d62. The reward is as defined earlier. Each bidder maintains a policy \u03c0\u1d62(a\u1d62|s\u1d62;\u03b8\u1d62). Through repeated auctions, bidders collect experiences (s\u1d62, a\u1d62, r\u1d62) and update \u03b8\u1d62 using policy gradients. Initially random policies ensure exploration. Over time, policies converge toward the equilibrium strategy \u03b2(v\u1d62) = v\u1d62/2 for two bidders."}, {"title": "Algorithm", "content": null}, {"title": "Policy Gradient Methods", "content": "Policy gradient methods focus on directly optimizing the policy function, denoted as \u03c0\u03b8(a|s), with the objective of maximizing the expected cumulative return. The policy \u03c0\u03b8(a|s) represents the probability of taking action a given state s, parameterized by the parameter vector \u03b8. The expected cumulative return is defined as:\nJ(\u03b8) = E_{\u03c0\u03b8} [\u2211_{t=0}^{T} r_t],\nwhere r\u209c is the reward received at time step t, and T is the time horizon of an episode. To optimize this objective, the gradient of the expected return with respect to the policy parameters \u03b8 is computed as:\n\u2207_{\u03b8}J(\u03b8) = E_{\u03c0\u03b8} [\u2211_{t=0}^{T} \u2207_\u03b8 log \u03c0_\u03b8(a_t|s_t) A_t],\nwhere A\u209c = A^{\u03c0\u03b8}(s\u209c, a\u209c) is the advantage function. The advantage function measures the relative quality of action a\u209c in state s\u209c compared to the average performance of the policy in that state, effectively capturing how much better or worse an action is compared to others available in the same state.\nNeural networks are typically employed to parameterize the policy \u03c0\u03b8(a|s). For environments with continuous action spaces, the policy outputs the parameters of a Gaussian distribution:\n\u03c0\u03b8(a|s) = N(\u03bc\u03b8(s), \u03c3\u03b8(s)),\nwhere \u03bc\u03b8(s) and \u03c3\u03b8(s) represent the mean and standard deviation of the distribu-"}, {"title": "Proximal Policy Optimization (PPO)", "content": "Proximal Policy Optimization (PPO) enhances the stability and reliability of policy gradient methods by constraining the extent to which the policy can change during each update. This is achieved through a clipped objective function that prevents large, potentially detrimental updates to the policy parameters. The core objective function of PPO is defined as:\nL^{CLIP}(\u03b8) = E_t[min(r_t(\u03b8)A_t, clip(r_t(\u03b8), 1 \u2013 \u03f5, 1 + \u03f5)A_t)],\nwhere r\u209c(\u03b8) is the probability ratio between the new policy and the old policy:\nr_t(\u03b8) = \\frac{\u03c0_\u03b8(a_t|s_t)}{\u03c0_{\u03b8_{old}}(a_t|s_t)},\nand \u03f5 is a small hyperparameter that defines the permissible range for r\u209c(\u03b8). The clipping function ensures that the ratio r\u209c(\u03b8) remains within the interval [1 \u2013 \u03f5, 1 + \u03f5]. This mechanism effectively limits the policy update to a region where the new policy does not deviate excessively from the old policy, thereby maintaining training stability.\nThe clipped objective can be elaborated as follows:\nL^{CLIP}(\u03b8) = E_t\\begin{cases} r_t(\u03b8)A_t, & \\text{if } |r_t(\u03b8) - 1| \u2264 \u03f5 \\\\ (1+\u03f5)A_t, & \\text{if } r_t(\u03b8) > 1 + \u03f5, A_t \u2265 0 \\\\ (1 - \u03f5)A_t, & \\text{if } r_t(\u03b8) < 1 \u2212 \u03f5, A_t \u2264 0 \\\\ r_t(\u03b8)A_t, & \\text{otherwise} \\end{cases}\nThis formulation ensures a balance between exploration and exploitation by pre- venting the policy from making overly aggressive updates that could destabilize the learning process."}, {"title": "Other Components", "content": "Beyond the clipped objective, the PPO algorithm integrates several critical components to facilitate effective learning. The policy parameters, denoted by \u03b8, are updated by maximizing the clipped objective function in conjunction with an entropy regularization term, H(\u03c0\u03b8(\u00b7|st)). The entropy term encourages exploration by penalizing policies that become too deterministic, thereby promoting a more diverse set of actions. The influence of the entropy term is controlled by the coefficient \u03b2, which determines the weight given to entropy regularization relative to the primary objective.\nFor each training iteration, a set of trajectories D\u2096 = {(s\u209c, a\u209c, r\u209c)} is collected by interacting with the environment using the current policy \u03c0\u03b8\u2096. From these trajectories, advantage estimates \u00c2\u209c and return estimates R\u209c are computed. The advantage estimates guide the policy updates by indicating the relative value of actions, while the return estimates are utilized to update the value function V\u1db2(st), parameterized by \u03a6. The value function is trained to minimize the value function loss L^{VF}(\u03a6), defined as the mean squared error between the predicted values and the estimated returns:\nL^{VF}(\u03a6) = \\frac{1}{|mini-batch|} \u2211_t (V_\u03a6(s_t) \u2013 R_t)^2.\nBoth the policy and value function updates are performed using mini-batch stochastic gradient descent, iterating over subsets of the collected trajectories. This approach ensures efficient and stable learning by leveraging batch processing and gradient-based optimization. The value function aids in reducing the variance of the policy gradient estimates, while the advantage estimates refine the policy's action selection based on the relative performance of actions in specific states."}, {"title": "Pseudocode", "content": "To consolidate the aforementioned components, the PPO algorithm is summarized in the following pseudocode."}, {"title": "Experiments", "content": null}, {"title": "First Price Auction", "content": "In a first price auction with two bidders and independent private valuations drawn from a uniform distribution v ~ UNIF(0, 1), the symmetric Bayesian Nash Equilibrium (BNE) bidding strategy is b*(v) = v/2."}, {"title": "First Price Auction with Power Distribution", "content": "Here, bidders\u2019valuations are drawn from a power distribution f(v) = v^{1/2}. The symmetric BNE bidding strategy is b*(v) = v/3."}, {"title": "First Price Auction with Risk Aversion", "content": "In this setting, bidders are risk-averse, and the utility of the winner is given by U\u1d62 = \u221av\u1d62 - b\u1d62. The symmetric BNE bidding strategy becomes: b*(v) = 2v/3."}, {"title": "First Price Auction with Asymmetric Bidders", "content": "Bidders have asymmetric valuation distributions: Agent 1's valuation v\u2081 ~ UNIF(0, 1.33), and Agent 2's valuation v\u2082 ~ UNIF(0,0.8). The asymmetric BNE is complex and does not have a simple closed-form expression."}, {"title": "First Price Auction with Reserve Price", "content": "A reserve price r = 0.25 is introduced below which bidders automatically lose. Valuations are drawn from v ~ UNIF(0, 1)."}, {"title": "Second Price Auction", "content": "In a second price auction with two bidders and valuations v ~ UNIF(0, 1), the symmetric BNE bidding strategy is truthful bidding: b*(v) = v."}, {"title": "All-Pay Auction", "content": "In an all-pay auction with two bidders and valuations v ~ UNIF(0, 1), the symmetric BNE bidding strategy is: b*(v) = v\u00b2/2"}, {"title": "Third-Price Auction", "content": "With three bidders and valuations v ~ UNIF(0,1), the symmetric BNE bidding strategy in a third-price auction is: b*(v) = 2v."}, {"title": "First Price Auction with Common Values", "content": "Bidders receive signals x\u1d62 = k\u1d62 + t where k\u1d62, t ~ UNIF(0, 1), and the common value is \u03c5 = (x\u2081+x\u2082)/2. The symmetric BNE bidding strategy is:b*(x) = 2x/3."}, {"title": "Second Price Auction with Common Values", "content": "With three bidders, valuations v ~ UNIF(0, 1), and independent signals x ~ UNIF(0, 2v), the symmetric BNE bidding strategy is: b*(x) = 2x/(2 + x)."}, {"title": "Korean Auction", "content": "In the Korean auction, two bidders have private valuations v ~ UNIF(0,1). They receive a signal x = 1 if the bidder holds the highest bid in round 0; otherwise, x = 0. The auction proceeds in rounds indexed by t = 0, 1."}, {"title": "Conclusion", "content": "In this study, we presented an RL-based approach to approximate equilibria in auctions, offering a framework for bidders to learn optimal strategies through repeated interactions. We validated this approach against theoretical benchmarks, showing its effectiveness in simple auction settings such as first-price auctions.\nThere are several directions for future work. First, a robust hyperparameter configuration needs to be established, ensuring the approach works consistently across different types of auctions. Addressing the issue of policy collapse as standard deviation declines is critical, possibly requiring more sophisticated methods like robust learning rate annealing or second-order methods such as Trust Regions. Incorporating model-based insights could improve sample efficiency and accelerate convergence. Lastly, extending the framework to handle high-dimensional action spaces, such as those in simultaneous auctions, will be an essential step in scaling the approach to more complex auction environments."}]}