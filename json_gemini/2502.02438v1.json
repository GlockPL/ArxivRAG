{"title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment", "authors": ["Yaling Shen", "Zhixiong Zhuang", "Kun Yuan", "Maria-Irina Nicolae", "Nassir Navab", "Nicolas Padoy", "Mario Fritz"], "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-STEAL), the first stealing attack against medical MLLMS. ADA-STEAL relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.", "sections": [{"title": "Introduction", "content": "In recent years, the development of medical multimodal large language models (MLLMs) has garnered widespread attention due to their potential to revolutionize healthcare. These models could support clinical decision-making (Seenivasan et al. 2022; Chen et al. 2024a; Yuan et al. 2024b), enhance diagnostic accuracy (Yuan et al. 2024a; Chen et al. 2024b), and promote equitable distribution of medical resources (Jia et al. 2024; Yuan et al. 2023; Zhang et al. 2023). One of the most important use cases is radiology report generation, where the medical MLLM takes a radiology image (e.g., chest X-ray) as input and generates a detailed diagnostic report. Since medical data is usually not publicly available and medical expertise is scarce, a well-performing medical MLLMs becomes valuable intellectual property (IP). However, these models are potentially vulnerable to model stealing attacks (Tram\u00e8r et al. 2016; Orekondy, Schiele, and Fritz 2019a), which replicate the functionality of a machine learning model through black-box access. This threat is particularly significant in the medical field because a duplicated model could conflict with IP and privacy regulations, and could also facilitate further attacks. By designing a payload on the copied model, malicious actors can then attack the original model, e.g., with a transfer jailbreak attack (Huang et al. 2024) to compel a medical MLLMs to output fraudulent or fabricated medical information.\nModel theft typically involves two steps: (i) creating a transfer dataset by querying the victim model with public or synthetic data (Orekondy, Schiele, and Fritz 2019a; Truong et al. 2021) to obtain pseudo-labels; and (ii) training the attacker model using these pseudo-labels as ground truth."}, {"title": "Related Work", "content": "Knowledge distillation. Knowledge distillation (Bucilu\u0103, Caruana, and Niculescu-Mizil 2006; Hinton, Vinyals, and Dean 2015) helps transfer the knowledge from a complex and larger \"teacher\" model to a compact and simpler \"student\" model, which is similar to our victim-attacker design. However, unlike knowledge distillation, where the student model has the same data distribution as the teacher model's training data, in our problem formulation, the attacker has no prior knowledge of the victim's black-box model, e.g., unknown architecture, data distributions or training parameters. Although data-free knowledge distillation (Fang et al. 2019; Micaelli and Storkey 2019; Choi et al. 2020) further assumes the absence of the teacher model's training data, its requirement of white-box access to the teacher model for backpropagation is a major difference to our setup.\nModel stealing. Model stealing, or model theft, typically has one of two objectives: exact replication of the model or its components, or functionality replication, where the attacker aims to mimic the model's behavior. The first type focuses on extracting the model's hyperparameters (Wang and Gong 2018), architecture (Oh et al. 2018), or learned parameters (Tram\u00e8r et al. 2016). The second type (Orekondy, Schiele, and Fritz 2019a; Truong et al. 2021; Zhuang, Nicolae, and Fritz 2024), involves training a model that mimics the victim's performance without prior knowledge of its training data or architecture. In this work, we present the first functionality model stealing attack against MLLMs for radiology report generation. While previous methods like Knockoff Nets (Orekondy, Schiele, and Fritz 2019a) replicate medical image classifiers using natural images, they deal with a much smaller output space compared to text generation. Bert-Thieves (Krishna et al. 2020), another related approach, targets language models, but does not handle images and benefits from publicly available text data that shares a similar distribution with the victim model. In contrast, medical data is hard to obtain, and only a specific subset of the vocabulary is relevant in this context, making it more challenging for the attacker, who may lack prior knowledge in the medical domain.\nSecurity of multimodal large language models. With the ability to understand and reason about different data types, MLLMs are vulnerable to evasion attacks targeting each data modality, such as malicious image and text constructs (Schlarmann and Hein 2023; Liu et al. 2024). In contrast to these attacks that are designed to induce erroneous or disallowed responses, the model stealing attack we present aims to mimic the functionality of the victim medical MLLM for radiology report generation, using only black-box access to the victim model."}, {"title": "Threat Model", "content": "In this section, we formalize the threat model for stealing black-box medical MLLMs in radiology report generation. First, we introduce preliminary concepts and notations. We then formalize the victim model as well as the attacker's objective and knowledge based on the real-world setup.\nNotations. We model MLLMs predicting the probability of the next token y\u0131 given the preceding language tokens y<1, the input image X, and the instruction prompt T. The final output is the answer Y = {y\u0131}{=1. Each token y is drawn from vocabulary V. This is formalized as:\n$$p(Y|X,T) = \\prod_{l=1}^{L}p(y_l|y_{<l}, X,T),$$"}, {"title": "Adversarial Domain Alignment", "content": "The attacker aims to replicate the radiology report generation functionality of a black-box medical model without access to medical datasets. To achieve this, they would ideally optimize the parameters \u03b8\u03b1 of the attacker model Ma to minimize the token prediction loss L on the victim dataset Dv:\n$$\\min_{\\theta_a} \\frac{1}{D_v} \\sum_{(X_v, Y_v) \\in D_v} [\\sum_{l=1}^{L} L(M_a(X_v), Y_l)],$$\nSince the attacker does not have access to Dv, they need to construct an own dataset Da for training:\n$$D_a = \\{(X_a, Y^*) | X_a \\sim P_a, Y^* = M_v(X_a)\\},$$\nwhere Xa can be sourced from a publicly available non-medical dataset with distribution Pa, and Y* is the pseudo-report predicted by the victim model. However, using non-medical images as queries yields homogeneous reports that barely explore the output vocabulary space of the victim model. Moreover, the attacker lacks the prior knowledge to guide the exploration and diversity of radiology reports. To address these challenges, we propose Adversarial Domain Alignment (ADA-STEAL), which initially diversifies the reports and then aligns Pa with Pv through data augmentation based on adversarial attacks. In turn, this enables the attacker to obtain more varied, medically relevant reports from the victim. To this end, the objective of the attacker is:\n$$\\minimize_{\\theta_a, \\delta} \\sum_{(X_a,Y^*)\\in D_a} [L(M_a(X_a), Y^*) + L(M_a(X_a + \\delta), \\tilde{Y})],$$\nwhere \u03b4 is the adversarial perturbation on image Xa to elicit a more diverse and medically relevant report \u1ef8 from the victim model, which is later replaced by its proxy, the attacker model. The goal is for the perturbed query data to better approximate the distribution Pv.\nThe overall method consists of three steps: (I) attacker model training to mimic the victim model; (II) medical report enrichment to diversify the victim's pseudo-reports; and (III) domain alignment to shift the attacker query image distribution towards the medical image distribution. Three steps can be iterated until the query budget B is exhausted."}, {"title": "Attacker Model Training", "content": "Following standard model stealing, the attacker queries the victim model with initial non-medical images from the distribution Pa and receives radiology report outputs. The attacker model Ma is then trained to minimize the loss in Equation (5) on the attacker's dataset Da:\n$$L(M_a(X_a), Y^*) = -\\sum_{l=1}^{L}log p(y_l^* | M_a(X_a)_{<l})$$\nOnce trained, this model serves as the proxy for the victim model in step III to design adversarial perturbations to be transferred to the victim. The dataset Da will be iteratively updated with aligned data following step III."}, {"title": "Medical Report Enrichment", "content": "Repetitive medical pseudo-reports from natural images limit the attacker model's ability to generalize to real medical images, which often feature varied abnormalities. Since the attacker lacks the expertise to design accurate reports, we incorporate an additional open-access large language model (LLM) as our oracle model Mo to generate a more diverse and medical-relevant report \u1ef8. Here, \u1ef8 ~ Mo(To), where To is the prompt for the oracle model as follows. The report \u1ef8 will be used in the next step as the desired output for the query image. We emphasize that Mo does not benefit from the image modality."}, {"title": "Domain Alignment", "content": "Training the attacker model Ma on Pa using the victim model's predictions makes Ma a proxy for the victim model. This suggests that Ma can map Xa to Y, even with initial low accuracy. We hypothesize that an input X can be generated to produce a relevant report Y by reversing the mapping, X = M\u207b\u00b9(Y). We adapt the Fast Gradient Sign Method (FGSM) (Goodfellow, Shlens, and Szegedy 2015) to generate adversarial perturbations \u03b4 on image Xa with the attacker model Ma and the new report \u1ef8:\n$$\\delta = \\epsilon \\cdot sign(\\nabla_x L(M_a(X_a),\\tilde{Y})),$$\nwhere \u03f5 is the magnitude of the adversarial perturbations. The attacker can query the victim model with the generated image Xa + \u03b4 and update the attacker transfer dataset Da:\n$$D_a = \\{(X_a + \\delta, M_v(X_a + \\delta))\\}.$$"}, {"title": "Experimental Setup", "content": "We now introduce the setup used in our experiments, including models, datasets, attacker image distribution, baseline attacks, evaluation metrics, and implementation details."}, {"title": "Experimental Results", "content": "Model stealing performance. The main experimental results on the two aforementioned test datasets are shown in Table 3. First, both attack strategies improve the performance of radiology report generation compared to the original MLLM. This outcome highlights the vulnerability of the medical MLLMs to model stealing attacks. Second, ADA-STEAL outperforms the other attack in all metrics, which confirms the effectiveness of our proposed model stealing method in enhancing the diversity of the attacker set. Such diversity may come from the incorporation of medical report enrichment and adversarial image generation that align attacker predictions with the expert knowledge of the oracle LLM. Third, when comparing between datasets, the performance achievements of ADA-STEAL tested on IU X-RAY are higher than that of MIMIC-CXR. This might be because IU X-RAY is relatively small and has less diverse image-text mappings, making it easier for attackers to mimic the image-to-text generation functionality. Finally, there is no clear winner in terms of the model architecture used by the attacker, but rather the performance depends on both the attack strategy and the measured metric.\nAblative analysis. To investigate the effect of adversarial attacks in domain alignment as well as oracle LLM diversification in medical report enrichment, an abrogation study is conducted, examining the impact of the non-utilization of the oracle model in ADA-STEAL (denoted as ADA w/o \u1ef8). Under the same experimental setup of IDEFICS and"}, {"title": "Discussion", "content": "Extended scope. Our current setup applies to victim models that are deterministic and use beam search decoding. We see no fundamental limitation in our methodology that would prevent us from extending our work to models with stochastic outputs or that vary the output decoding strategy.\nReports' drift. The reports generated by the oracle LLM are only based on a prompt and no images. We identify a risk that these reports are not accurate or of good quality. However, we notice in practice that data diversity close to the target domain is sufficient to elicit data diversity in the victim model. Moreover, the reports are not used to train the stolen model, further limiting their potential impact.\nDefenses. Our work does not evaluate the ADA-STEAL against model stealing defenses. Current defenses (Orekondy, Schiele, and Fritz 2019b; Kariyappa"}, {"title": "Conclusion", "content": "In this paper, we show for the first time that an attacker can successfully steal the functionality of radiology report generation from a medical MLLM without access to the victim data distribution. Our attack ADA-STEAL produces a diverse dataset for stealing by leveraging adversarial attacks for domain alignment and an oracle model for report enrichment. Experimental results on two medical datasets demonstrate the effectiveness of ADA-STEAL, which outperforms existing methods directly adapted to the task of radiology report generation. We encourage medical MLLM owners to consider the risk of model theft and to protect their assets."}]}