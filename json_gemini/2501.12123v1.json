{"title": "FedCLEAN: byzantine defense by CLustering Errors of Activation maps in Non-IID federated learning environments", "authors": ["Mehdi Ben Ghali", "Reda Bellafqira", "Gouenou Coatrieux"], "abstract": "Federated Learning (FL) enables clients to collaboratively train a global model using their local datasets while reinforcing data privacy. However, FL is susceptible to poisoning attacks. Existing defense mechanisms assume that clients' data are independent and identically distributed (IID), making them ineffective in real-world applications where data are non-IID. This paper presents FedCLEAN, the first defense capable of filtering attackers' model updates in a non-IID FL environment. The originality of FedCLEAN is twofold. First, it relies on a client confidence score derived from the reconstruction errors of each client's model activation maps for a given trigger set, with reconstruction errors obtained by means of a Conditional Variational Autoencoder trained according to a novel server-side strategy. Second, we propose an ad-hoc trust propagation algorithm based on client scores, which allows building a cluster of benign clients while flagging potential attackers. Experimental results on the datasets MNIST and FashionMNIST demonstrate the robustness of FedCLEAN against Byzantine attackers in non-IID scenarios and a close-to-zero benign client misclassification rate, even in the absence of an attack.", "sections": [{"title": "1 Introduction", "content": "In recent years, Federated Learning (FL) has attracted considerable interest as a privacy-preserving machine learning paradigm [40,32,25] in several domains, such as healthcare [36], transportation [36], and personal devices [28]. In a typical FL scenario, several clients collectively contribute to training a global model. At first, a central server initiates the global model and sends it to the clients so that they can train it locally on their private data. Model updates are sent back to the server, which aggregates them into a new global model. These two steps are repeated until the global model converges. As clients' data remain stored locally, privacy is ensured [42]. However, even though FL systems preserve privacy, their decentralized nature makes them vulnerable to poisoning attacks, where one or"}, {"title": "1.1 Related Works", "content": "As stated above, the first defenses against malicious updates were the development of robust aggregation algorithms. FedAvg [41] is one of the earliest and most commonly used aggregation methods in FL. It consists of using a weighted average of the updates sent by clients to actualize the global model. Clients' dataset sizes are used as weights. Doing so dilutes the effect of sparse attackers but is not sufficient when their number increases. It is also not robust to FL attacks that amplify updates to dominate the global average [48], and ineffective in non-IID scenarios where an attacker has a larger dataset compared to benign clients. GeoMed [26] uses the geometric median of clients' updates as a global update. KRUM [8] uses one of the local updates to generate a global model update. The chosen local update minimizes the sum of distances to its nearest neighbors. Other aggregation schemes have been proposed since [38,34,1,23], but they focus on training performance rather than security. All of these methods have been shown to be ineffective under Byzantine attacks [24].\nTo better respond to attacks, \"detect and remove\" defense systems have been recently proposed. They aim at excluding malicious updates before the aggregation step. One first class of methods regroups systems which rely on anomaly detection techniques based on autoencoders. Li et al. [37] were the first to propose such a defense. It trains a copy of the global model on the test dataset on the server side, while training at the same time a variational autoencoder (VAE) on surrogate vectors randomly sampled from the weights of this model at each epoch. Although this solution performs better than aggregation-based defenses under Byzantine attacks, it requires training a VAE on an entire server-side dataset to achieve good results. This is quite unrealistic in real-life scenarios, as the server most likely won't have access to a significant training dataset.\nFedCVAE [24] later proposed an improvement using an untrained conditional variational autoencoder (CVAE). When the central server receives updates from"}, {"title": "1.2 Contribution", "content": "In this work, we propose FedCLEAN, an extension of FedCAM, a solution introduced by Bellafqira et al. [6]. Similar to FedCAM, FedCLEAN analyzes the GeoMed normalized CVAE reconstruction errors of clients' activation maps for given inputs of a trigger set. Clients with high reconstruction errors are rejected based on an anomaly detection strategy. While FedCAM achieves top byzantine robustness compared to concurrent methods in IID scenarios [6], it fails to achieve good results in the context of non-IID data, as we will see in Section"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Federated Learning (FL)", "content": "A Common FL Scenario In this work, we considered an FL setting where N clients $\\{C_k\\}_{k=1..N}$ with their datasets $\\{D_k\\}_{k=1..N}$ participate in a training session orchestrated by a central server S to train a global model $M_G$. During each FL round t, S scatters the current global model $M_t$ to a subset of U randomly selected clients. Each client updates the received model on its local dataset for a number of local epochs and sends back the updated model to S. The server then uses an aggregation algorithm such as FedAvg [41] to get an updated version of the global model $M_t^{t+1}$ from the clients' updated models $\\{M_u^t\\}_{u=1..U}$. The above training steps are then repeated until convergence or a stopping criterion is met.\nNon-IID Data in Federated Learning One major statistical challenge in FL is the so-called Non-Independent and Identically-Distributed (Non-IID) hypothesis [33,55]. Indeed, in most real-life applications [19], local datasets are client-specific, and data collection processes are often inconsistent across clients,"}, {"title": "2.2 Conditional Variational Autoencoders (CVAE)", "content": "Autoencoders (AEs) [30] are a class of unsupervised generative neural networks that aim to reconstruct a given input from a latent representation encoding. AEs are made of: i) an encoder $f$ that maps input vectors from the initial feature space to a lower-dimensionality representation space called the latent space; and ii) a decoder that retrieves input vectors from their latent representations. The objective of an autoencoder is to minimize the reconstruction error (RE) between the initial and reconstructed vectors. They have been historically used for dimensionality reduction [4,30], model pre-training [20,7], and generative tasks [52]. An important property of AEs is that, when applied to out-of-distribution vectors that deviate from the original training distribution, these deviations are amplified in the latent space, and much higher reconstruction errors are observed. This makes AEs considered as a valuable tool for anomaly detection in a wide range of applications [43].\nAutoencoders having a fixed-dimensional latent space show some limitations for datasets with varying complexities. This has motivated the introduction of Variational Autoencoders (VAEs) [35] with a variational latent space based on Bayesian inference methods [18]. Conditional-VAEs (CVAEs) [45] are an extension of VAEs that leverages conditional probabilities to guide the encoding and decoding process following a provided condition. In practice, what differentiates a CVAE from a traditional VAE is the addition of a condition vector y as an input of both the encoder and decoder Thus, CVAEs enhance the capabilities of VAEs by making them compatible with supervised learning and allowing them to work in a task-guided manner [16]. They have been extensively used for a number of applications, including anomaly detection and detecting attackers in federated learning [24,6,11]."}, {"title": "2.3 GeoMed and Activation Maps", "content": "In a Euclidean space, the geometric median (GeoMed) of a set of points is a point minimizing the $L_2$ distance to all the points in the set. GeoMed is a robust approximation of the centroid of a given set of points under the presence of outliers [26]. This has led to its extensive use in Byzantine-robust federated learning as a robust aggregation method [13] or to detect attackers [24].\nIn a typical deep learning model, the data is processed through feed-forward layers. Each hidden layer takes in the previous layer's output, applies a linear transformation parametrized by a set of weights (e.g., a convolution or weighted sum), and then applies a nonlinearity function. The output of the layer's weighted transformation is referred to as the activations of that layer. \"Activation maps\", or the set of activation maps extracted from a model for a given input, are characteristic of a model and the way it processes data. They have been used for applications such as explainability [47] and model watermarking [15,5,56]. It has been observed that in many real-life applications, the activation maps of a model tend to follow an intrinsic distribution [15,44]."}, {"title": "3 Threat Model and Defense Requirements", "content": "In this section, we present the threat model and security requirements considered in this work. In our FL scenario, we assume that the clients never share their datasets and only communicate their updated models to the server. Regarding the server, it has access to a sample of the testing set and the updated models received at each round (i.e., the server can inspect the inner layers and weights of the models clients send back after training)."}, {"title": "3.1 Threat Model", "content": "Adversary's Objective In this work, we consider a \"Byzantine\" adversary A, who wishes to carry out Byzantine attack during the training phase of an FL model. To do so, A controls a number of the FL clients who submit poisoned model updates to S during the FL process with the objective of disturbing the convergence of the global model. Formally, A attempts to fulfill the following objectives:\nO1: If the attack succeeds, the global model's accuracy on test data should collapse.\nO2: The attacked model should not be able to recover from the accuracy drop by pursuing further training in the absence of the attacker.\nAdversary's Capabilities As in [3,46,22,11,24,10], we consider a strong adversary who has full control over m < N/2 \"malicious\" clients. A has full access to these clients' datasets, models, and communication channels. In particular,\nA can conduct training using local datasets when clients are selected by the"}, {"title": "3.2 Security Requirements", "content": "To effectively defend against a Byzantine attacker, it is required that a filtering-based defense system D satisfies the following requirements [46]:\nR1: Block all malicious clients. As discussed above, the Byzantine adversary A is able to break the model given one single successful round of attack. As a result, in order to be robust to such an adversary, D must detect all malicious clients updates at any round.\nR2: Do not disrupt training. D should not negatively impact the training. In other words, D must have high precision and not misclassify benign clients. It must also preserve the test accuracy achieved in the no-attack and no-defense scenario.\nWe will see that our proposal responds to all these objectives and security requirements."}, {"title": "4 Proposed defense: FedCLEAN", "content": ""}, {"title": "4.1 FedCLEAN Principle", "content": "The idea behind FedCLEAN is to analyze the normalized activation maps (NAMs), obtained by subtracting the GeoMed value from activation maps of one or several layers of $M_G$ for some given inputs referred as a trigger set $T_\\tau = (X_\\tau, Y_\\tau)$, where $X_\\tau$ and $Y_\\tau$ are the set of samples and their labels, respectively. We assume that the NAMs obtained with malicious model updates will correspond to outliers that can be identified by means of an CVAE and GeoMed based anomaly detection strategy.\nMore clearly, before the FL process begins : FedCLEAN : i) makes a copy of the initial global model; ii) trains the copy locally for several epochs on a small dataset called the \"trigger set\"; iii) extracts the NAMs associated with the trigger set from different epochs of training; iv) trains a CVAE to reconstruct these NAMs, using their corresponding labels $Y_\\tau$ as a condition. The training process is detailed in 4.2. By doing so, the CVAE learns the latent distribution of NAMs associated with each class.\nOnce trained, the CVAE is used by S at each epoch to reconstruct for each client updates the NAMs corresponding to the trigger set. Malicious NAMs, which fall out of the learned distribution, result in high reconstruction errors. To discriminate malicious NAMs from benign ones, FedCLEAN makes use of a clustering procedure relying on an ad-hoc trust propagation algorithm. We discuss it in subsection 4.3."}, {"title": "4.2 CVAE training constraints", "content": "As stated above, the CVAE model is trained by the server S considering a simulated a benign model $M_G^0$ based on the server trigger set $T_\\tau$. One of the constraints we assumed in building FedCLEAN is that $T_\\tau$ is of very small size compared to the client data. To train the CVAE well, one thus needs to augment the training set. The solution we adopted is to use the activation maps (AMs) computed at different rounds of the training of $M_G^\\prime$. But by doing so, we have to take into account the temporal drift of $M_G^\\prime$ activation maps that impact CVAE generalization capabilities. In fact, AMs are more heavily affected by noise and very different at the very beginning of the training of $M_G^\\prime$ than after a certain number of epochs. To overcome this issue, and as illustrated in Fig. 2 the CVAE training set is constituted of activation maps collected after some warmup epochs of the training of $M_G^\\prime$. At each epoch, AMs are GeoMed normalized so as to better control AMs amplitude differences along the training of $M_G^\\prime$. Indeed, with\na GeoMed recomputed at each round, we have found that the distribution of the deviations from GeoMed remains stable. Anyway, even with a small trigger set, this technique allows to constitute a training set that captures the distribution of\nbenign NAMs assuming certain hypotheses.\nA second issue we had to face when training the CVAE is that it is sensitive\nto \"posterior collapse\" [29,9,14], which causes it to converge to minima that\nignore training inputs. More clearly, the traditional training of a CVAE, consists\nin maximizing the Evidence Lower Bound (ELBO) in the following loss:\n$L(x, y; \\theta, \\phi) = -ELBO(x, y; \\theta, \\phi)$\n$= - (E[log P_\\theta (x|z, y)] - \\frac{1}{2} [\\sigma_\\phi (x)^2 + \\mu_\\phi (x)^2 - 1 - log(\\sigma_\\phi (x)^2))$\n$= MSE(g_\\theta (z, y), x) + D_{KL}(q_\\phi (z|x)||p(z))$\n(1)\nwhere: $g_\\theta$ is the CVAE decoder; $q_\\phi$ is the latent space distribution given by\nthe CVAE encoder; z is a latent representation sampled from this distribution;\nand, (x, y) the CVAE input sample and its label; $D_{KL}$ is the Kullback-Leibler"}, {"title": "4.3 Clustering using trust propagation", "content": "A commonplace practice in CVAE anomaly detection and FL defenses [6,24,37] is to threshold error values to decide of an anomaly. Usually, this is the average reconstruction error of all clients. However because of this, in the absence of an attacker, nearly half of the clients even if they are all benigns are wrongly flagged as malicious. Well-adjusted static thresholds could allow for better filtering. In an IID scenario, such reference values could be implied from the error values of CVAE during training. However, in a Non-IID scenario, real client reconstruction error values cannot be deduced from the sample training scenario. We thus opted for a simple clustering algorithm instead.\nTo avoid systematically excluding benign clients, our clustering algorithm is able to automatically output either one or two clusters depending on whether malicious updates are present. This is a challenge as deciding the number of clusters based on data properties is an active problem in cluster analysis [21], and most algorithms require fixing the number of clusters beforehand as a user-defined parameter [21]. In addition, clustering algorithms vary widely in terms of \"parameter reliance\", i.e. the dependence on user-defined parameters and the tolerance to different ranges of such parameters. Our proposal has a single parameter that is interpretable (i.e. not rely on guessing).\nIt sequentially retrieves all benign clients participating in training, through what we call a \"trust propagation\" algorithm. It relies on a single relative parameter, which is a fraction in [0, 1] that can be selected in a data-agnostic manner, which it then uses to dynamically compute the distance measures it needs to perform clustering. Our idea relies on the fact that when reconstructing benign client activation maps, the measured reconstruction error is a residual from the randomness and uncertainty within CVAE. More clearly, when reconstructing out-of-distribution attacker NAMs, the reconstructed vector is an arbitrary response. Thus, the reconstruction error we measure is akin to measuring error"}, {"title": "5 Evaluation", "content": ""}, {"title": "5.1 Experimental setup", "content": "FL setting We evaluate our proposition on Non-IID image classification tasks using two public datasets : MNIST [17] and FashionMNIST [51] datasets. The FL is orchestrated by a server that applies FedCLEAN as defense D before each aggregation step with 100 total clients. 50% of clients are randomly selected to participate in training at each round, and we use FedAVG [41] to aggregate model updates. As global model MG, we use a standard CNN architecture that achieves good results on both datasets in the absence of attackers. In all our experiments, we set 30% of the client population to be attackers.\nNon-IID Data distribution We distribute the samples of MNIST and FashionMNIST datasets among clients to synthesize a Non-IID data distribution. As mentioned above, we craft distributions that achieve label skew and quantity"}, {"title": "5.2 Considered Byzantine attacks", "content": "Four attacks tested and parametrized in the literature were tested [12,24,6]:\nSign-Flipping attacks: Attackers reverse the signs of their model updates by multiplying them by a factor of $-\\xi$. This is akin to reversing gradient descent and leading the model in the direction of errors. Let $w_i$ be the weights of the model update after local training. Attackers send back $\\omega = -\\xi w_i$."}, {"title": "5.3 Experimental results", "content": "Tables 1 and 2 report the results of testing on different distributions of the\nMNIST and FashionMNIST datasets. We use three standard metrics to measure\nthe effectiveness of each defense system: the global model's accuracy on the test\ndataset (ACC), the attacker detection false negative (FN), which refers to the\nfraction of attackers that manage to get through the defense system, and benign\ndetection false positive (FP) which represents the fraction of blocked benign\nclients.\nFor a byzantine attack, the defense requirement R1 requires FN to be 0% in\norder for the defense to be considered successful. The defense also needs to not\ndisrupt the training (R2), which means ACC should be as close as possible to\nthe benchmark no defense and no attack accuracy, and FP should be as close as\npossible to 0%.\nAs we can observe, FedCLEAN is the only defense system that achieves 0%\nof FN against all types of Byzantine attacks in all tested scenarios. We also"}, {"title": "6 Conclusion & future work", "content": "We introduced FedCLEAN, an autoencoder-based defense system designed to identify malicious clients in Non-IID federated learning environments from updated models' activation maps. With two core components: A framework for training CVAEs for an FL defense system leveraging model activation maps normalized with GeoMed, DKL annealing, and a realistic trigger set requirement. And a dynamic one-parameter client selection algorithm that minimizes the exclusion of benign clients.\nWe demonstrate that, in image classification benchmarks, FedCLEAN is robust against Byzantine attacks under a Non-IID client data distribution and achieves top performance in benign client missclassification tests. Future work will focus on broader machine learning applications, backdoor attacks, as well as improving our method by enhancing CVAE training through adaptive regularization, and implementing a fully non-parametric trust propagation algorithm."}]}