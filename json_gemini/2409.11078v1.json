{"title": "MonoKAN: Certified Monotonic Kolmogorov-Arnold Network", "authors": ["Alejandro Polo-Molina", "David Alfaya", "Jose Portela"], "abstract": "Artificial Neural Networks (ANNs) have significantly advanced various fields by effectively recognizing patterns and solving complex problems. Despite these advancements, their interpretability remains a critical challenge, especially in applications where transparency and accountability are essential. To address this, explainable AI (XAI) has made progress in demystifying ANNs, yet interpretability alone is often insufficient. In certain applications, model predictions must align with expert-imposed requirements, sometimes exemplified by partial monotonicity constraints. While monotonic approaches are found in the literature for traditional Multi-layer Perceptrons (MLPs), they still face difficulties in achieving both interpretability and certified partial monotonicity. Recently, the Kolmogorov-Arnold Network (KAN) architecture, based on learnable activation functions parametrized as splines, has been proposed as a more interpretable alternative to MLPs. Building on this, we introduce a novel ANN architecture called MonoKAN, which is based on the KAN architecture and achieves certified partial monotonicity while enhancing interpretability. To achieve this, we employ cubic Hermite splines, which guarantee monotonicity through a set of straightforward conditions. Additionally, by using positive weights in the linear combinations of these splines, we ensure that the network preserves the monotonic relationships between input and output. Our experiments demonstrate that MonoKAN not only enhances interpretability but also improves predictive performance across the majority of benchmarks, outperforming state-of-the-art monotonic MLP approaches.", "sections": [{"title": "1. Introduction", "content": "Artificial neural networks (ANNs) are the backbone of modern artificial intelligence (Lecun et al., 2015), (Goodfellow et al., 2016). These computational systems are designed to recognize patterns and solve complex problems through learning from data, making them highly effective for tasks such as image and speech recognition (Hinton et al., 2012), predictive analytics (Liu et al., 2017) or many others (Sarvamangala and Kulkarni, 2022), (Xu et al., 2020). By mimicking the brain's ability to process information and adapt through experience, ANNs have revolutionized fields ranging from computer vision to autonomous systems (Sarvamangala and Kulkarni, 2022), (Voulodimos et al., 2018), and their development continues to drive forward the capabilities of machine learning and artificial intelligence as a whole.\nDespite their impressive capabilities, ANNs face significant challenges regarding interpretability. As ANNs grow more complex, their decision-making processes become increasingly opaque, often described as \"black boxes\" due to the difficulty in understanding how specific inputs are translated into outputs. This lack of transparency can be problematic in critical applications such as healthcare or finance, where understanding the rationale behind decisions is crucial for trust and accountability (Cohen et al., 2021), (Tjoa and Guan, 2021). Furthermore, the complexity of ANNs makes it hard to find and fix biases in the models, which can lead to unfair or harmful results. Addressing these interpretability issues is essential to ensure that ANNs can be safely and effectively integrated into high-stakes environments.\nIn response to the interpretability challenges of ANNs, the field of explainable artificial intelligence (XAI) has grown substantially in the last decades. Numerous studies have emerged aiming to demystify their inner workings (Zhang et al., 2020), (Pizarroso et al., 2022), (Morala et al., 2023). These studies represent critical strides toward making neural networks more transparent and trustworthy, facilitating their adoption in fields where understanding and accountability are paramount.\nHowever, it is often the case where interpretability alone is insufficient in some critical applications (Rudin, 2019). Therefore, in some fields, it is a requisite to certify that the model predictions align with some requirements imposed by human experts (Cohen et al., 2021). Partial monotonicity is an example where incorporating prior knowledge from human experts into the model might sometimes be necessary. For instance, in university admissions, it is reasonable to expect that, all other variables being equal, an applicant with a higher GPA should have a higher probability of being accepted. If the model's predictions do not follow this monotonic relationship, it could lead to unfair and unethical admission decisions. For instance, an applicant with a 4.0 GPA being rejected while an applicant with a 3.0 GPA is accepted, all other factors being equal, would be seen as unfair and could indicate bias in the model.\nAs a result, the training of partial monotonic ANNs has become a prominent area of research in recent years. To tackle this issue, two primary strategies have emerged (Liu et al., 2020). First of all, there are some studies that enforce monotonicity by means of a regularization term that guides the ANNs training towards a partial monotonic solution (Sivaraman et al., 2020), (Gupta et al., 2019), (Monteiro et al., 2022). However, these approaches verify monotonicity only on a finite set of points, and hence none of the previous studies can certify the enforcement of partial monotonic constraints across all possible input values. Therefore, it is necessary to use some external certification algorithm after the training process to guarantee partial monotonicity. Regarding this type of algorithm, few examples are found in the literature (Liu et al., 2020), (Polo-Molina et al., 2024). On the other hand, some studies propose designing constrained architectures that inherently ensure monotonicity (Runje and Shankaranarayana, 2023), (Daniels and Velikova, 2010), (You et al., 2017), (Nolte et al., 2022). Although these methods can guarantee partial monotonicity, they often come with the trade-off of being overly restrictive or complex and challenging to implement (Liu et al., 2020).\nEven though some of the aforementioned methods can lead up to certified partial monotonic ANNs, traditional Multi-layer Perceptron (MLP) architectures still have significant difficulties with interpretability. The complex and often opaque nature of the connections and weight adjustments in MLPs makes it challenging to understand and predict how inputs are being transformed into outputs. Therefore, existing approaches to obtaining monotonic MLPs hardly generate both interpretable and certified partial monotonic ANNs, often requiring post-hoc interpretability methods.\nTo address some of the aforementioned difficulties related to interpretability, a new ANN architecture, called the Kolmogorov-Arnold Network (KAN), has been recently proposed (Liu et al., 2024). Unlike traditional MLPs, which rely on the universal approximation theorem, KANs leverage the Kolmogorov-Arnold representation theorem. This theorem states that any multivariate continuous function can be decomposed into a finite combination of univariate functions, enhancing the interpretability of the network.\nHowever, the functions depicted by the Kolmogorov-Arnold theorem can be non-smooth, even fractal, and may not be learnable in practice (Liu et al., 2024). Consequently, a KAN with the width and depth proposed by the Kolmogorov-Arnold theorem is often too simplistic in practice to approximate any function arbitrarily well using smooth splines.\nTherefore, although the use of the Kolmogorov-Arnold representation theorem for ANNs was already studied (Sprecher and Draghici, 2002), (K\u00f6ppen, 2002), the major breakthrough occurred when Liu et al. (2024) established the analogy between MLPs and KANs. In MLPs, the notion of a layer is clearly defined, and the model's power comes from stacking multiple layers to form deeper architectures."}, {"title": "2. Kolmogorov-Arnold Networks", "content": "As mentioned before, while Multi-Layer Perceptrons (MLPs) draw their inspiration from the universal approximation theorem, our attention shifts to the Kolmogorov-Arnold representation theorem."}, {"title": "3. MonoK\u0391\u039d", "content": "This section introduces the necessary theoretical development and the proposed algorithm for generating a set of sufficient conditions to ensure that a KAN is partially monotonic w.r.t. a specific subset of input variables. First of all, the concept of partial monotonicity will be explained, as well as the way to ensure monotonicity in cubic Hermite splines. Subsequently, the main theorem outlining the sufficient conditions for a KAN to be partially monotonic will be presented. Finally, the proposed algorithm to ensure that a KAN meets these conditions will be described."}, {"title": "3.1. Partial Monotonicity", "content": "To begin with, let us start by presenting the concept of partial monotonicity. Intuitively, a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is increasing (resp. decreasing) partially monotonic w.r.t the rth input, with $1 \\leq r \\leq n$, whenever the output increases (decreases) if the rth input increases. Mathematically speaking, a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is increasing (resp. decreasing) partially monotonic w.r.t. its rth input if\n$f(x_1, ..., x_r, ..., x_n) \\leq f(x_1, ..., x'_r, ..., x_n), \\forall x_r \\leq x'_r$\n(resp. $f(x_1, ..., x_r, ..., x_n) \\geq f(x_1, ..., x'_r, ..., x_n)$).\nTherefore, $f$ will be partially monotonic w.r.t. a subset of its input variables ${x_{i_1}, \\dots , x_{i_k}}$ where $0 \\leq k \\leq n$, if Eq. (4) holds for each $i_j$ simultaneously with $j \\in {1, \\dots , k}$."}, {"title": "3.2. Monotonic Cubic Hermite Splines", "content": "As mentioned before, each function to be learned in a KAN layer is univariate, allowing for various parameterization methods for each 1D function. While the original formulation presented in (Liu et al., 2024) employs B-splines to approximate these univariate functions, in this paper, it is proposed the use of cubic Hermite splines. The advantage of cubic Hermite splines lies in the well-known sufficient conditions for monotonicity (Fritsch and Carlson, 1980), (Ar\u00e0ndiga et al., 2022). Besides, for a sufficiently smooth function and a fine enough grid, the resulting cubic Hermite spline converges uniformly to the desired function (Hall and Meyer, 1976).\nA cubic Hermite spline, or cubic Hermite interpolator, is a type of spline where each segment is a third-degree polynomial defined by its values and first derivatives at the endpoints of the interval it spans. Consequently, the spline is $C^1$ continuous within the interval of definition.\nTo formally define a cubic Hermite spline, consider a set of knots $x_k$, values $y_k$ and derivative values $m_k$ at each of the knots $x_k$ given by $X = {(x_k, y_k, m_k) | \\forall k \\in I = {1,2,..., n}}$. Then, the cubic Hermite spline $p$ is a set of $n - 1$ cubic polynomials such that, in each subinterval $I_k = [x_k, x_{k+1}]$, it is verified that\n$p(x_k) = y_k, \\forall k \\in I$\n$p'(x_k) = m_k, \\forall k \\in I$.\nTherefore, the above conditions ensure that the spline matches both the function values and the slopes at each data point. Furthermore, on each subinterval $I_k = [x_k, x_{k+1}]$, the cubic Hermite spline $p$ can be expressed in its Hermite form as\n$p(t) = h_{00}(t)y_k + h_{10}(t)(x_{k+1} - x_k)m_k +\nh_{01}(t)y_{k+1} + h_{11}(t)(x_{k+1} - x_k)m_{k+1}$,\nwhere $t = \\frac{x-x_k}{x_{k+1}-x_k}$ and $h_{00}, h_{10}, h_{01},h_{11}$ are the Hermite basis functions defined as follows\n$h_{00}(t) = 2t^3 - 3t^2 + 1$,\n$h_{10}(t) = t^3 - 2t^2 + t$,\n$h_{01}(t) = -2t^3 + 3t^2$,\n$h_{11}(t) = t^3 - t^2$.\nOnce the terminology of cubic Hermite splines has been established, we now consider the conditions required for the resulting spline to be monotonic as shown in Fritsch and Carlson (1980). According to Eq. (5), to achieve an increasing (resp. decreasing) monotonic cubic Hermite spline, it is necessary that $y_k \\leq y_{k+1}, \\forall k\\in I$ (resp. $y_k \\geq y_{k+1}, \\forall i\\in I$). Additionally, to ensure monotonicity, it is clear that considering\n$d_k = \\frac{y_{k+1} - y_k}{x_{k+1} - x_k}$\nthe slope of the secant line between two successive points $x_k$ and $x_{k+1}$, then the derivative at each point within the interval $I_t$ must match the sign of $d_r$ to maintain monotonicity. Specifically, if $d_k = 0$, then both $m_k$ and $m_{k+1}$ must also be zero, as any other configuration would disrupt monotonicity between $x_k$ and $x_{k+1}$. These conditions, stated in the following lemma, establish necessary conditions for a cubic Hermite spline to be monotonic.\nLemma 1 (Necessary conditions for monotonicity, (Ar\u00e0ndiga et al., 2022, Theorem 1.1)). Let $p_k$ be an increasing (resp. decreasing) monotone cubic Hermite spline of the data $X = {(x_k, y_k, m_k), (x_{k+1}, y_{k+1}, m_{k+1})}$ such that the control points verify that $y_k \\leq y_{k+1}$ (resp. $y_k \\geq y_{k+1}$). Then\n$m_k\\geq0$ and $m_{k+1}\\geq 0$.\n(resp. $m_k\\leq 0$ and $m_{k+1}\\leq 0$)\nMoreover, if $d_k = 0$ then $p_k$ is monotone (in fact, constant) if and only if $m_k = m_{k+1} = 0$.\nFor the more general case when $d_k \\neq 0$, Fritsch and Carlson (1980) introduced the parameters $\\alpha_k$ and $\\beta_k$, defined as\n$\\alpha_k := \\frac{m_k}{d_k}$\n$\\beta_k := \\frac{m_{k+1}}{d_k}$\nThese parameters provide the necessary framework for establishing sufficient conditions for monotonicity.\nLemma 2 (Sufficient conditions for monotonicity, (Fritsch and Carlson, 1980, Lemma 2 and \u00a74)). Let $I_k = [X_k, X_{k+1}]$ be an interval between two knot points and $p_k$ be a cubic Hermite spline of the data $X = {(x_k, Y_k, m_k), (X_{k+1}, Y_{k+1}, m_{k+1})}$ such that the control points verify that $y_k < Y_{k+1}$ (resp. $y_k > Y_{k+1}$). Then, the cubic Hermite spline $p_k$ is increasingly (resp. decreasingly) monotone on $I_k$ if\n$\\alpha_k := \\frac{m_k}{d_k} \\geq 0, \\qquad \\beta_k := \\frac{m_{k+1}}{d_k} \\geq 0$\n(resp. $\\alpha_k := \\frac{m_k}{d_k} \\leq 0, \\qquad \\beta_k := \\frac{m_{k+1}}{d_k} < 0$)\nand\n$\\alpha_k^2 + \\beta_k^2 \\leq 9$.\nBy adhering to these conditions, one can ensure that the cubic Hermite spline remains monotonic over its entire domain."}, {"title": "3.3. Mathematical Certification of Partial Monotonicity of a KAN", "content": "Having introduced the definition of partial monotonicity and the necessary conditions for a cubic Hermite spline to be monotonic, we now present the main theoretical result, which provides a set of sufficient conditions for a KAN to be certified partial monotonic. For simplicity, we will assume that the KAN is partially monotonic with respect to the rth input. Therefore, when handling multiple monotonic features, the same conditions applied to the rth input will be applied to each monotonic feature."}, {"title": "3.4. MonoKAN Algorithm", "content": "Finally, it is presented the proposed algorithm that ensures that a KAN fulfills the sufficient condition stated in Theorem 3, certifying the network as partially monotonic. To achieve this, it is proposed that the learned parameters are clamped at each training epoch. Therefore, by ensuring that the updated parameters meet the sufficient conditions, the algorithm ensures the KAN's partial monotonicity.\nConsider $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, a KAN with L layers, expected to be partially monotonic with respect to the rth input ($1 \\leq r \\leq n$). According to Theorem 3, certifying the proposed constraints ensures the KAN's partial monotonicity. For this purpose, we propose a clamping method, described in applyCons Algorithm, that adjusts the parameters at each epoch, ensuring they stay within the permissible range. Therefore, as mentioned in Section 3.3, this clamping has to be applied to the activations coming out of the rth input feature and the succesive layers. A pseudocode of the algorithm is described in MonoKAN Algorithm."}, {"title": "4. Experiments", "content": "To assess the practical applicability of the proposed method, we conduct experiments across multiple datasets and benchmark against the latest state-of-the-art algorithms. Although no other study presents a certified partial monotonic KAN, the idea is to compare the results obtained against the existing MLPs approaches found in the literature.\nTo ensure a fair and consistent comparison, we adopted the experimental procedures established by Liu et al. (2020) and Sivaraman et al. (2020), which are widely accepted in the literature. Therefore, for each dataset, the experiments were conducted three times to report the mean and the standard deviation. Consequently, although we benchmark our results against the state-of-the-art monotonic architectures (Nolte et al., 2022), (Runje and Shankaranarayana, 2023), we reran their experiments using the methodology of Liu et al. (2020) and Sivaraman et al. (2020). This approach ensures methodological consistency and allows for a fair comparison across the different studies.\nIn the initial set of experiments, we used the following datasets proposed by Liu et al. (2020): COMPAS (Angwin et al., 2023), a classification dataset with 13 features, including 4 monotonic ones; Blog Feedback Regression (Buza, 2014), a regression dataset with 276 features, 8 of which are monotonic; and Loan Defaulter, a classification dataset with 28 features, 5 of which are monotonic.\nFor the second set of experiments, we employed datasets specified by Sivaraman et al. (2020): the Auto MPG dataset,"}, {"title": "4.1. Results", "content": "The obtained results after performing the experiments are summarized in Tables 1 and 2. As observed, the results obtained by the proposed MonoKAN outperform the state-of-the-art in four out of five datasets, while in the remaining experiment, our method ranks as the second-best option.\nWhen considering the number of parameters for each model, KAN remains competitive compared to most of the approaches proposed in the literature. However, in cases where the number of input variables is substantial, KAN exhibits a higher parameter count than some approaches. This increase in parameters for datasets with numerous inputs is due to KAN's architecture, which generates at least one spline in the first layer for each input. Consequently, the model complexity and the number of parameters grow proportionally with the number of input variables, impacting the overall efficiency and computational requirements of KANs for datasets with a large number of variables.\nOn the other hand, it is important to note that MonoKAN inherits all the additional advantages of using KAN architectures compared with traditional MLPs described in the introduction, especially its enhanced interpretability arising from being easier to visualize. For instance, in Figure 2, we can observe a trained MonoKAN model using the Auto MPG dataset. This illustration highlights the specific relationships between each input variable and the output, demonstrating the certified decreasing partial monotonicity concerning input variables $x_2$, $x_3$ and $x_4$. The visualization effectively showcases how the MonoKAN model maintains monotonic behavior w.r.t these selected features. This is crucial for understanding and verifying the model's adherence to monotonic constraints, which can be essential for applications requiring reliable and interpretable predictions. Additionally, MonoKAN provides insight into the model's behavior and performance, allowing for a deeper analysis of the variable interactions and their impact on the output."}, {"title": "5. Conclusion", "content": "This paper proposes a novel artificial neural network (ANN) architecture called MonoKAN, which is based on the Kolmogorov-Arnold Network (KAN). MonoKAN is designed to certify partial monotonicity across the entire input space, not just within the domain of the training data, while enhancing interpretability. To achieve this, we replace the B-splines, proposed in the original formulation of KAN, with cubic Hermite splines, which offer well-established conditions for monotonicity and can uniformly approximate sufficiently smooth functions. Our experiments demonstrate that MonoKAN consistently outperforms existing state-of-the-art methods in terms of performance metrics. Moreover, it retains the interpretability benefits of KANs, enabling effective visualization of model behavior. This combination of interpretability and certified partial monotonicity addresses a crucial need for more trustworthy and explainable AI models. Future research will focus on extending the architecture to splines of arbitrary degrees and investigating the effects of pruning, a key characteristic of KANs."}, {"title": "CRediT authorship contribution statement", "content": "Alejandro Polo-Molina: Writing \u2013 review & editing, Writing - original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. David Alfaya: Writing \u2013 review & editing, Validation, Supervision, Resources, Project administration, Methodology, Funding acquisition. Jose Portela: Writing \u2013 review & editing, Validation, Supervision, Resources, Project administration, Methodology, Funding acquisition."}, {"title": "Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "Data availability", "content": "All datasets used in this research are available online."}, {"title": "Acknowledgments", "content": "This research was supported by funding from CDTI, with Grant Number MIG-20221006 associated with the ATMOSPHERE Project and grant PID2022-142024NB-I00 funded by MCIN/AEI/10.13039/501100011033."}, {"title": "A. Proof of Theorem 3", "content": "This appendix presents a proof of Theorem 3 that states a sufficient condition for a Kolmogorov Arnold Network (KAN) to be partially monotonic. First of all, let us start by presenting a proposition that states that the composition of univariate monotonic function is also monotonic.\nProposition A.1. Let $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ and $g : \\mathbb{R} \\rightarrow \\mathbb{R}$ be two continuous functions. Then\n1. $g \\circ f$ is increasingly monotonic if both $f$ and $g$ are increasingly monotonic.\n2. $g \\circ f$ is decreasingly monotonic if $f$ is decreasingly monotonic and $g$ is increasingly monotonic.\nProof."}, {"title": "2. Assume $f$ is decreasingly monotonic and $g$ is increasingly monotonic.", "content": "Assume $f$ and $g$ are increasingly monotonic. By definition, $\\forall x_1, x_2 \\in \\mathbb{R}$ such that $x_1 \\leq x_2$, we have $f(x_1) \\leq f(x_2)$ and $g(y_1) \\leq g(y_2) \\forall y_1, y_2 \\in \\mathbb{R}$ with $y_1 \\leq y_2$. Therefore, consider any $x_1, x_2 \\in \\mathbb{R}$ such that $x_1 \\leq x_2$. Then,\n$f(x_1) \\leq f(x_2)$.\nApplying $g$ to both sides, since $g$ is increasing, we get\n$g(f(x_1)) \\leq g(f(x_2))$.\nThus, $g \\circ f$ is increasingly monotonic.\nAssume $f$ is decreasingly monotonic and $g$ is increasingly monotonic. By definition, $\\forall x_1, x_2 \\in \\mathbb{R}$ with $x_1 \\leq x_2$, we have $f(x_1) \\geq f(x_2)$ and $g(y_1) \\leq g(y_2) \\forall y_1, y_2 \\in \\mathbb{R}$ with $y_1 \\leq y_2$. Consider any $x_1, x_2 \\in \\mathbb{R}$ such that $x_1 \\leq x_2$. Then,\n$f(x_1) \\geq f(x_2)$.\nApplying $g$ to both sides, since $g$ is increasing, we get\n$g(f(x_1)) \\geq g(f(x_2))$.\nThus, $g \\circ f$ is decreasingly monotonic.\nConsequently, to obtain a KAN that is increasingly (resp. decreasingly) partially monotonic with respect to the $r^{th}$ input, it is sufficient to ensure that the $n_1$ activations from the $r^{th}$ input in the first layer are increasingly (decreasingly) monotonic and that for the rest of the nodes from the following layers, where the activation function outputs generated by the $r^{th}$ input are considered part of the input, are also increasingly monotonic. Therefore, according to the above proposition, the KAN would be increasingly (decreasingly) partially monotonic. Considering this idea, it is obtained Theorem A.2 that gives a sufficient condition for a KAN to be partially monotonic."}, {"title": "Theorem A.2. Given $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$a KAN with L layers and K knots in the interval of definition I = [$x^1$, $x^K$], then if the basis function b is increasingly monotonic and the following conditions are met.", "content": "Given $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$a KAN with L layers and K knots in the interval of definition I = [$x^1$, $x^K$], then if the basis function b is increasingly monotonic and the following conditions are met.\n1. $\\omega^{0,j}_\\Phi \\geq 0, \\omega^{0,j}_b \\geq 0, \\omega^{0,j,r}_\\Phi \\geq 0, \\omega^{0,j,r}_b \\geq 0$, (resp. $\\omega^{0,j}_\\Phi \\geq 0, \\omega^{0,j}_b \\geq 0, \\omega^{0,j,r}_\\Phi \\leq 0, \\omega^{0,j,r}_b \\leq 0$)\n2. $y^{k+1}_{0,j,r} \\geq y^{k}_{0,j,r} \\;i.e. \\;d^{0,j,r}_k \\geq 0$, (resp. $y^{k+1}_{0,j,r} \\leq y^{k}_{0,j,r} \\;i.e. \\;d^{0,j,r}_k \\leq 0$)\n3. if $d^{0,j,r}_k = 0, m^{0,j,r}_k = m^{0,j,r}_{k+1} = 0$,\n4. if $d^{0,j,r}_k > 0, m^{0,j,r}_k \\geq 0, m^{0,j,r}_{k+1} \\geq 0$, (resp. if $d^{0,j,r}_k < 0, m^{0,j,r}_k \\leq 0, m^{0,j,r}_{k+1} \\leq 0$)\n5. if $d^{0,j,r}_k > 0, (\\alpha^{0,j,r}_k)^2 + (\\beta^{0,j,r}_k)^2 \\leq 9$,\n6. $\\omega^{l,j}_\\Phi \\geq 0, \\omega^{l,j}_b \\geq 0$,\n7. $y^{k+1}_{l,j,i} \\geq y^{k}_{l,j,i} \\;i.e. \\;d^{l,j,i}_k \\geq 0$,\n8. if $d^{l,j,i}_k = 0, m^{l,j,i}_k = m^{l,j,i}_{k+1} = 0$,\n9. if $d^{l,j,i}_k > 0, m^{l,j,i}_k \\geq 0, m^{l,j,i}_{k+1} \\geq 0$,\n10. if $d^{l,j,i}_k > 0, (\\alpha^{l,j,i}_k)^2 + (\\beta^{l,j,i}_k)^2 \\leq 9$,\nwhere $\\alpha^{l,j,i}_k := \\frac{m^{l,j,i}_k}{d^{l,j,i}_k}, \\beta^{l,j,i}_k := \\frac{m^{l,j,i}_{k+1}}{d^{l,j,i}_k}$ and $1 \\leq l < L - 1, \\forall 1 < k < K - 1, 1 \\leq i \\leq n^l, 1 \\leq j \\leq n^{l+1}$, then $f$ is increasingly (resp. decreasingly) partially monotonic w.r.t the $r^{th}$ input.\nProof. Let us prove the theorem by induction over the number of layers of the KAN. Without loss of generality, we will consider the case of increasing monotonicity. The case for decreasing monotonicity is followed by analogous arguments.\nBase Case (n = 1)\nSuppose that $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a KAN with 1 layer such that the KAN's structure is [n, 1]. Therefore, by Eq. (6),\n$\\hat{y} = f(x) = \\sum_{i=1}^{n_0} (\\omega^{0,1}_\\Phi \\Phi_{0,1,i}(x_{0,i}) + \\omega^{0,1}_b b(x_{0,i})) + \\theta_{0,1}$.\nConsidering conditions (1) - (5) and Lemma 1 and Lemma 2, then it is clear that $\\Phi_{0,1,r}$ is monotone. Additionally, the proposed linear extrapolation of the cubic Hermite spline, with slopes $m_l$ to the left of $x^1$ and $m_R$ to the right of $x^K$, ensures that the spline $\\Phi_{0,1,i}$ is $C^1$ continuous and monotonic across $\\mathbb{R}$, not just within the data domain. Therefore, the linear combination of these monotonic functions, with positive coefficients, remains monotonic, and the composition of monotonic functions is also monotonic (by Proposition A.1). Thus, $f$ is partially monotonic with respect to the $r^{th}$ input across $\\mathbb{R}^n$.\nInduction Step. Suppose true the result for l layers and let us prove it for the (l + 1)th layer.\nConsidering a KAN $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ with structure [n, ..., $n^l$, $n^{l+1} = 1$]. Then by Eq. (6),\n$\\hat{y} = f(x) = \\sum_{i=1}^{n_l} (\\omega^{l,1}_\\Phi \\Phi_{l,1,i}(x_{l,i}) + \\omega^{l,1}_b b(x_{l,i})) + \\theta_{l,1}$.\nBy conditions (6) - (10) and Lemma 1 and Lemma 2, $\\Phi_{l,1,i}$ is monotone $\\forall 1 \\leq i \\leq n^l$. Moreover, as $x_{l,i}$ is obtained from the input $x_0$ as a KAN with structure [n,...,$n^{l-1}$,1] which satisfies all the hypothesis of the Theorem, then, by the induction hypothesis, $x_{l,i}$ is partially monotone w.r.t. the $r^{th}$ input. Therefore, considering Proposition A.1 and the same reasoning as in the base case, f is partially monotone w.r.t. the $r^{th}$ input across $\\mathbb{R}^n$."}]}