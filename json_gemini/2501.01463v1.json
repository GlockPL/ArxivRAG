{"title": "Goal Recognition using Actor-Critic Optimization", "authors": ["Ben Nageris", "Felipe Meneguzzi", "Reuth Mirsky"], "abstract": "Goal Recognition aims to infer an agent's goal from a sequence of observations. Existing approaches often rely on manually engineered domains and discrete representations. Deep Recognition using Actor-Critic Optimization (DRACO) is a novel approach based on deep reinforcement learning that overcomes these limitations by providing two key contributions. First, it is the first goal recognition algorithm that learns a set of policy networks from unstructured data and uses them for inference. Second, DRACO introduces new metrics for assessing goal hypotheses through continuous policy representations. DRACO achieves state-of-the-art performance for goal recognition in discrete settings while not using the structured inputs used by existing approaches. Moreover, it outperforms these approaches in more challenging, continuous settings at substantially reduced costs in both computing and memory. Together, these results showcase the robustness of the new algorithm, bridging traditional goal recognition and deep reinforcement learning.", "sections": [{"title": "1 Introduction", "content": "Goal recognition (GR) is a fundamental problem in AI where the objective is to infer the terminal goal of an observed agent from a sequence of observations (Sukthankar et al. 2014). GR techniques can be used for estimating people's paths (Vered and Kaminka 2017), recognizing actions in a kitchen for a service robot to fetch the required ingredients (Kautz, Allen et al. 1986; Shvo et al. 2022), or even alerting an operator when a client executes a suspicious sequence of actions in e-commerce software (Qin and Lee 2004). A recognition task contrasts a sequence of observations, represented as discrete, symbolic actions, with a domain theory that describes the domain's possible actions to infer the likely goal from the observations. This process inherits three fundamental limitations of using a symbolic domain theory. First, using such domain theories forces the recognizer to rely on accurate and relevant symbols and limits the size of feasible problems. Second, these underlying models often represent an optimal plan for a goal but require additional extensions to handle noise in the recognition process. Third, recognizing goals in continuous domains necessitates a discretization process that may jeopardize recognition (Kaminka, Vered, and Agmon 2018)."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Recognition Problems", "content": "Goal and Activity recognition are related but distinct tasks (Van-Horenbeke and Peer 2021; Meneguzzi and Pereira 2021; Mirsky, Keren, and Geib 2021; Sukthankar et al. 2014), which have three main components. First, the environment specifies the problem's dynamics and settings. Second, the observed agent represents the agent that acts in the environment and pursues some objective. We note that often, this observed agent is called an actor, but we use observed agent instead to differentiate it from the Actor in the Actor-Critic algorithm description. Third, the observer watches the observed agent as it acts in the environment. Beyond those similarities, each recognition problem has its own properties. Activity Recognition (AR) problems focus on labeling a sequence of possibly noisy sensor inputs with a single usually unstructured label, denoting a low-level activity of the observed agent, while Goal Recognition (GR) is the most abstract level of inference focusing on identifying the highest-level goal or overarching objective of the agent's actions. The problem formulation in this work provides a synthesis of AR and GR, as it requires a sequence of low-level sensor data as input and outputs a prediction about the goal of the observed agent. We use a similar formulation for goal recognition as planning (Ram\u00edrez and Geffner (2009)):\nDefinition 1 A Goal Recognition (GR) problem is a tuple (\u039e, G, O), such that \u039e is domain theory, G is an exhaustive and mutually exclusive set of potential goals, and O is a set of observations. A solution for a GR problem is a goal g \u2208 G that O explains according to \u039e.\nThe domain theory is a description of the environment. It can be prescribed in various ways, including plan graphs (Avrahami-Zilberbrand and Kaminka 2005; E-Mart\u00edn, R-Moreno, and Smith 2015), a planning domain (Ram\u00edrez and Geffner 2009; Masters and Sardina 2019; Pereira, Oren, and Meneguzzi 2020), or a Markov Decision Process (MDP) (Ramirez and Geffner 2011; Amado, Mirsky, and Meneguzzi 2022). As we combine GR and AR, such that both the goal and the observations can be either a symbolic state representation (e.g., coordinates in space), or other raw data such as images or floating point numbers vectors. Each goal g can be expressed as a formula, a set of labels over possible goal states, or a reward function over alternative sets of these states. Similarly, the observations O can be either a sequence of symbolic states or raw data. Finally, the observer can perceive all actions and state sets or only part of them."}, {"title": "2.2 Planning Domain Definition Language (PDDL)", "content": "PDDL is a formal language for describing planning problems, and introduced to standardize the representation of problems in automated planning and to serve as a common standard for benchmarking planning algorithms. A PDDL problem description typically consists of a domain file and a problem file. The domain file specifies the predicates, types, constants, and operators (actions) available, while the problem file defines the specific objects, initial state, and goal conditions.\nSymbolic GR approaches relies on having a deliberate domain representation (PDDL for example) which is susceptible to noise, and require complete and precise specification usually made by a domain expert manually ((Ram\u00edrez and Geffner 2009; Ram\u00edrez and Geffner 2010)). This representation makes them often difficult to scale and costly in terms of real time computation."}, {"title": "2.3 Reinforcement Learning", "content": "This work focuses on Reinforcement Learning (RL)-based recognition. A commonly used domain model in RL is MDP (Sutton and Barto 2018). MDP is used to model decision-making processes, and can encapsulate both discrete and continuous state and action spaces.\nDefinition 2 An MDP M, is a 4-tuple (S, A, P, R) such that S are the states in the environment, A is the set of actions the agent can execute, P is a transition function, and R is a reward function. A transition function P(s' | s, a) returns the probability of transitioning from state s to state s' after taking action a, and a reward function R(s, a, s') returns the reward an agent obtains when it transitions from s to s' using action a.\nThe solution to an MDP is a policy \u03c0, which in RL often consists of a stochastic function \u03c0(\u03b1 | s) that defines the probability of the agent taking action a \u2208 A in state s \u2208 S. A policy \u03c0* is optimal if it maximizes the expected return of all states of the MDP. In Deep RL, NNs estimate these policies. In DRACO, learned policies represent the potential behavior of an observed agent rather than the desired behavior of the learner itself, and hence we can shape the reward differently for learning each goal-dependent policy."}, {"title": "3 DRACO", "content": "DRACO relies on the assumption that the environment behaves as an MDP (Definition 2) and in practice, we only need to represent states and actions as our domain theory \u039e from Definition 1:\nDefinition 3 A GR as RL problem is a tuple (\u039e, G, O) such that \u039e = (S, A) is a set of n-dimensional states and m-dimensional actions, both of which can be discrete or continuous, S \u2286 R\u207f and A \u2286 R\u1d50; G \u2286 S is a set of potential goals, out of which we assume the observed agent pursues exactly one; and O = s\u2080, a\u2080, s\u2081, a\u2081, s\u2082, a\u2082, . . . is a sequence of state-action observations, such that s\u1d62 \u2208 S and a\u1d62 \u2208 A.\nThis definition is purposefully broad, allowing for different possible instantiations of the problem: States can be images or continuous data such as a robot's joints' location and velocities. This representation obviates the need to manually prescribe labels or symbols to specific states. While we do not assume explicit transition dynamics to be part of the problem, we assume having access to the environment or some model of the environment, such as a sampling model or simulator. These assumptions free us from explicitly prescribing action dynamics as part of a recognition problem and position our solution as a model-free RL technique.\nFigure 1 illustrates the recognition process in DRACO. It starts by receiving a set of goals (|G| = 2 in the figure), and the observation sequence O (|O| = 3 in the figure). To solve the GR problem, we first discuss how DRACO learns a set of policies, one policy for each goal, using simulated interactions. Then, we explain how DRACO compares the observation sequence O against each goal-oriented policy. Key to this comparison is the metric for processing observations and comparing them to policies. In this work, we propose two different metrics. The algorithm then outputs a distribution over the set of goals to represent how likely each goal to explain the observation."}, {"title": "3.1 Policy Learning", "content": "GR often decouples the description of the problem, including the set of recognizable goals, from a specific observation sequence (Ramirez and Geffner 2011; Masters and Sardina 2019). There is a clear gain in precomputing these policies, as the learning process is the costliest component of the pipeline, and we can assume prior knowledge of the goals in realistic domains. Consistent with this, DRACO learns a set of policies offline, one policy \u03c0g for each goal g\u2208 G. A significant strength of this approach is that it does not require the learned policies to represent agents pursuing each goal optimally. Rather, they only need to hold that a policy for goal g\u1d62 is more similar to the agent's behavior of pursuing g\u1d62 than to the agent's behavior of pursuing any other goal.\nDRACO learns goal-dependent policies \u03c0g using Actor-Critic policy gradient methods. These algorithms naturally support continuous state and action spaces, which are crucial in many physical environments, such as robotics, trajectory recognition, etc. We use the sampling model of the environment to compute the policy \u03c0g for each candidate goal g. We assume our model can simulate environmental dynamics but does not contain explicit reward information. This assumption is consistent with most real-world environments, such as movement in robotic arms, which do not include explicit reward specifications. Given this model, for each goal g, DRACO generates a different reward function, meant to shape the agent's preferences to reach that goal.\nThe reward function has a large influence on the learning part of the framework. Sometimes, the sparsity of the positive reward might require us to shape it further to help the agent learn how to reach the goal. Several goal-conditioned RL algorithms apply here (Andrychowicz et al. 2017; Durugkar et al. 2021; Kaelbling 1993; Schaul et al. 2015), and transfer learning techniques can be used to learn new policies for new goals quickly, thus enabling DRACO to scale quickly with the number of goals. However, to minimize variability in this work, in our empirical evaluation we replace these sophisticated algorithms with a simple aggregated reward. To train the goal-dependent policy \u03c0g, we consider the reward for action a in state s to be:\n$R_g(s, a) = (-1) * ||a(s) \u2013 g||_{L_1}$"}, {"title": "Learning Architectures", "content": "This work focuses on Actor-Critic (AC) approaches to learn the goal-dependent policies since they are the leading approaches in policy-based continuous RL. Each AC algorithm requires two NNs per goal (actor NN and critic NN), so its total number of trained NNs is 2 \u00d7 |G|. The Actor network outputs a probability of taking each action, the size of its output is the number of valid actions. By contrast, the critic network, which outputs the expected value for being in this particular state, has a single neuron in its last layer. Once DRACO computes these two NNs, it uses the Actor's output to generate policies for each goal (\u03c0g) and an observation sequence O for GR: inferring the goal (g\u2208 G) that O explains. The reason that we chose to use the Actor network's output rather than the Critic network's is that the Actor network outputs an action rather than a value, which is easier to compare to the observation sequence."}, {"title": "3.2 Likelihood Estimation of Observations (inference)", "content": "Following the Bayesian formulation from Ram\u00edrez and Geffner (2010), we compute the probability P(g | O) of a goal g\u2208G conditioned on observations O by computing the probability P(O | g) of the observations conditioned on the goals. The key challenge here is to estimate the likelihood of O given g, which is the likelihood of taking the action a when in state s according to \u03c0g. Formally, \u03c0g(\u03b1 | s) = P(a | s, g). For this estimation, we need to design a distance function between an observation sequence of the form s\u2080, a\u2080, s\u2081, a\u2081, . . . and a policy \u03c0g(a\u1d62 | s\u1d62). This function should be able to handle continuous spaces to enable goal recognition in such environments. We can then compute an aggregate distance \u0394 of the observations O to the observations we expect an agent to generate if it follows a policy towards that goal at each step in the observations. This metric allows us to compute the probability of the observations for each goal using the softmin of such distances:\n$P(O | g) = softmin(\u0394(\u039f, \u03c0_g))$  g\u2208G\nThen, using the assumption of exhaustive and mutually exclusive goal hypotheses from Definition 1, we can compute the probability of each goal hypothesis by normalizing over the individually computed conditional probabilities in Eq. 3.\n$P(g | O) = \\frac{P(O | g)P(g)}{\\sum_{g_i \u2208 G} P(O | g_i)}$\nWe develop three functions to serve as our distance metrics to measure the differnece between an observation and policy \u03c0\u03c1. Since observation can be precieved in two main formats: state only list, and state and action tuples list, in this work we suggest metrics for both observations formats."}, {"title": "3.3 Observation distance functions", "content": "Max utility for State-only Os applies to state-only observations and is defined by a sequence of states < s\u2081,..., s\u2096 >. In the AC algorithm family, the critic network estimates V(S), the expected return from s following a particular policy \u03c0. V(s) expresses how good it is to be in s, and the higher V (s) the more the network thinks s promise. We take advantage of the ability of AC to estimate V (S), and present a state-only distance based on this estimation.\n$D_4(O) = D_4(<s_1,..., s_k>) = \\frac{1}{\\sum_{s_i \u2208 O_s}} V(s_i)$\nIn this work, as explained in Equation 2, the goal that minimizes the distance between the policy and the given observation is the goal the framework suggests the agent aims. Having said that, as the observation explains the goal better, we aim to minimize the result of distance mathematical computation.\nSince in this work we use a non positive simple reward function explained in Eq 1, as the V(s\u2096 \u2208 O\u209b) becomes smaller (meaning that the state is less attractive), the state-only distance (Eq 4) becomes larger and vice versa. If an observation contains more promising goals, their \u2211\u209b\u2096\u2208O\u209b V(s\u2096) will be larger causing D\u2084(O) to become lower (sum of non positives) and eventually be assessed as a more promising goal candidate.\nWasserstein distance computes the amount of work to convert one distribution to another (Vallender 1974).\n$W_p(P, Q) = \\sum_{i=1}^{k} ||X_i \u2013 Y_i||_{L_1}$\nEq. 5 encodes the Wasserstein distance measure for one-dimensional distributions, with 1 statistical moment, for two empirical distributions P and Q with samples X\u2081, ..., X\u2096 and Y\u2081, ..., Y\u2096 respectively. We adapt this equation to compute the spot Wasserstein distance between an observation o = s, a and a goal-dependent policy \u03c0, from which we sample actions \u0101 (denoted by \u00f1(s)):\n$W_p(s, \u03b1, \u03c0) = ||a \u2212 (\u00e3 ~ \u03c0(s))||_{L_1}$\nAs this score looks at a single observation at a time, we omit the sum over n samples. Finally, we compute the Wasserstein distance between the observation sequence and the goal-dependent policy.\n$\u0394_{wp}(\u039f,\u03c0) = \u03bc({W_i | W_i = W_p(s_i, a_i, \u03c0)\u2200s_i, a_i \u2208 O})$\nZ-score function is a statistical measure that computes, for a value x sampled from a population, the number of standard deviations \u03c3 from the statistical means \u03bc so that Z(x) = (x \u2212 \u03bc)/\u03c3. As seen in Figure 2, this computation is exactly what the inference process is looking for: how likely a sample (an observed action) is under some distribution (the goal-based policy). Recall that policies, by default, learned via the AC methods encodes a Gaussian distribution. Out of this Gaussian distribution, we can extract descriptive statistics, regardless of whether the action space is continuous or discrete, and leverage it to compute z-score. Specifically, Eq. 8 encodes the spot z-score of a single observation of state s, and action a against the action expected of policy \u03c0 for the same state.\n$Z(s, \u03b1, \u03c0) = \\frac{|\u03b1 \u2013 \u03bc(\u03c0(s))|}{\u03c3(\u03c0(s))}$\nHere \u03bc, and \u03c3 refer, respectively, to the mean and standard deviation of the numeric representation of the actions applicable in state s. We compute the distance for an entire observation sequence by taking the mean of the spot Z-scores:\n$\u0394_z(\u039f, \u03c0) = \u03bc({z_i | z_i = Z(s_i, a_i, \u03c0)\u2200s_i, a_i \u2208 O})$"}, {"title": "4 Empirical Setup and Testbed", "content": "We evaluate DRACO in two OpenAI-Gym (Brockman et al. 2016) based setups: MiniGrid (Chevalier-Boisvert et al. 2023), and Panda-gym (Gallou\u00e9dec et al. 2021). As DRACO supports both discrete and continuous setups, we compare its performance with leading state-of-the-art approaches suited for these environmental setups. For each domain, we generate GR problems by creating configurations within a shared initial setup and obstacle placement.\nBaselines. (1) Plan Recognition as Planning: Ramirez and Geffner (R&G) is the standard baseline for symbolic GR and leverage planner executions for GR (Ramirez and Geffner 2011). Since it relies on a PDDL domain representation that is either manually crafted (Vered and Kaminka 2017) or generated from labelled data (Granada et al. 2020), it cannot be trivially used in continuous domains, so we use it for evaluating MiniGrid. (2) GRAQL (Amado, Mirsky, and Meneguzzi 2022) learns a Q-table per goal and uses them to compute the likelihood of each goal hypothesis. GRAQL's tabular nature limits it to discrete domains, so we discretize domains to compare it with DRACO in continuous setups.\nAlgorithms. In this evaluation, DRACO learns policies via Proximal Policy Optimization (PPO) (Schulman et al. 2017), from Stable Baselines (Raffin et al. 2021) and infers using of Z-score and Wasserstein distance function metrics proposed in section 3.3. GRAQL and DRACO require offline domain's learning, whereas R&G doesn't require training, but need to execute a planner in inference time. This implementation exemplifies how DRACO performs with an off-the-shelf DRL algorithm. GRAQL uses KL-divergence for inference in MiniGrid, but its performance in discretized-continuous environments is often poor because of the high number of actions, which makes the probability that the observation and the policy converge on the same action infeasible. To address it, in the discretized environments, we use the mean of the action distances to the observation instead.\nHyperparameters. While GRAQL and DRACO use different training algorithms, they share various hyperparameters, such as number of episodes, learning rate, and discount factor. Throughout the executions, optimizing the approach's performance was the leading motivation. We used the hyperparameter values that maximized each approach. In MiniGrid, we compare GRAQL and DRACO after 100K episodes, discount factor (\u03b3) 0.99, and learning rate (\u03b1) 0.001. In Panda-Gym, we tracked the learning's performance under multiple episode configurations until convergence. The optimal discretization factor for GRAQL is 0.03 cm. Similarly, in Panda-Gym, GRAQL's learning rate (\u03b3) was 0.01 while DRACO's was 0.0006. For R&G, we created a PDDL-based domain theory using PDDL-generator (Seipp, Torralba, and Hoffmann 2022).\nMetrics. We leverage standard prediction metrics from machine learning, to the context of goal recognition, to \"correctly classify\" the true goal means it has the highest probability. As such, the meaning of these metrics for GR is as follows: Accuracy and precision measure how often a method correctly classifies the correct and incorrect goals, respectively, and recall measures how often a method identifies the correct goal as the most likely. Finally, we evaluate the framework's confidence in its prediction. The confidence metric is the difference between the probabilities of the two most probable goals divided by the most probable goal's probability. Thus, if two (or more) most likely goals are equiprobable, confidence is zero.\nDomain 1: MiniGrid. The motivation to use MiniGrid is the ability to be represented in multiple formats and to model complex GR problems. We created three environment formats, one for each algorithm: PDDLs for R&G (complete environment model), symbolic state representation for GRAQL (x and y coordinates, angle) and a visual representation for DRACO (image), making it possible to compare the different approaches in the same GR problem. Note that each representation varies in the information each algorithm perceives. MiniGrid problem consists of an agent and objective which are the red triangle and the bright green cells in Figure 3 respectively. The agent's task is to reach the objective by taking actions: turn right, turn left, move forward, do nothing. We evaluate the three approaches on the setups depicted in Figures 3a, 3b, 3c: 2-Goal and 3-Goal are obstacle-free environments, and Lava has 3 goal candidates and lava cells. For each setup, we generated 10 different GR settings.\nDomain 2: Panda-gym Panda-gym is a drastically different domain than MiniGrid: it has continuous state and action spaces, and it realistically presents a Franka Emika Panda robot arm (Gallou\u00e9dec et al. 2021) using the PyBullet physics engine. This domain allows us to test DRACO's boundaries and performance on a continuous state space that is not an image. This environment consists of a single arm with the objective of reaching a colored sphere (Figure 3d). Since this environment is fully continuous and does not inherently support discrete state spaces, we provide a discretization of the environment in order to run the GRAQL baseline on it. Specifically, we tested discretization granularities of 0.01 0.05 cm and eventually chose 0.03 cm as this value yields the best performance for our GRAQL baseline. To stress out approach, we ran both DRACO and GRAQL on three different GR setups, with 2,3, and 4 goal spheres (Figure 3d illustrates Panda-Gym 3-goals problem). For each setup, we generated 10 different GR settings.\nDomain ambiguity We assess the level of domain ambiguity using the worst-case distinctiveness (WCD) metric (Keren, Gal, and Karpas 2014). WCD represents the maximum steps an optimal agent can take before an observer can differentiate the correct goal. Since WCD accounts only for discrete states, in Panda-gym, we consider all states in a perimeter of 0.01 cm around a state to be the same."}, {"title": "5 Results", "content": "Our tests use a different algorithm from our learning methods to generate observations, namely Asynchronous Actor-Critic (A2C) (Mnih et al. 2016). For each combination of algorithm and instance, the observation sequence O varies its degree of observability: 10%, 30%, 50%, 70%, and full observability. We generate an observation sequence with x% observability by taking a full sequence and removing each observation along the trajectory with x% probability. Similarly, we generate noisy observations by appending suboptimal actions to x% of the states.\u00b9 For each execution in every environment, we use the same trajectories for all algorithms. We show the metrics averaged over 10 settings with varying degrees of observability (13 configurations per execution), totalling 540 GR problem instances.\nPerformance Table 2 shows results for MiniGrid. All approaches perform overwhelmingly well, with perfect scores on all measures when the observability is 0.5 or higher. However, this result misses part of the story: while GRAQL's and R&G's true goal confidence score was close to the other goals' score (9% and 13% difference on average respectively), DRACO had very different scores between the true goal and the other candidates (71% on average), making DRACO more confident in its predictions. Moreover, DRACO's results dominate GRAQL's and R&G results in the more challenging, low observability (10%, 30%), problem variants. It is a reasonable outcome because DL can make significant inroads to recognizing patterns and variance instead of valuing the current state. Moreover, our use of NN-based policies makes DRACO more forgiving to missing observations, as the likelihood of these missing actions is e rather than zero, and this difference is accumulated with the number of missing observations. Lastly, as this table shows, DRACO and GRAQL both outperform the R&G approach in most of these discrete domains. Importantly, DRACO dominates both approaches in the confidence metric, inferring higher probabilities for the correct goal, whereas GRAQL and R&G often disambiguate poorly. Note that the accuracy metric in this paper differs from Ram\u00edrez and Geffner (2010), which refers to the ratio of problems where the recognizer ranks the true goal (i.e., True Positives) with the highest likelihood, and the number of problems tested. This measure differs from standard ML accuracy, which also factors in the successful ranking of the correct goal (i.e., True Positives + True Negatives). In our experiments, we report accuracy using the latter definition.\nFigure 4 reports the average F-Score for all Panda-Gym problems running GRAQL in a discretized environment version and DRACO in the continuous environment representation, both with the Z-score and the Wasserstein distance metrics. Figure 4a shows the mean performance with full observability, and Figures 4b- 4c show the mean performance under partial and noisy observations. First, DRACO's performance is similar and often superior to GRAQL, often reaching the best results regardless of the distance metric used. GRAQL struggles in generalizing continuous and relatively large environments. This gap happens due to one of the main differences between DL and tabular methods - DL approximates states similar to visited ones while Q-Learning does not, affecting the number of episodes GRAQL requires to learn how to solve a problem. Figures 4a and 4b compare the approaches under increasing episodes and observability percentage, respectively. These figures emphasize the superiority of DRACO over GRAQL across the board in continuous environments. GRAQL performance improves when episodes increase but generalizing a large problem for GRAQL takes a substantially more amount of computations than it takes for DRACO. However, in the partial observation scenario (Figure 4b), as the observability percentage increases DRACO's performance increases as well while GRAQL's remains about the same, thus emphasizing its inability to learn the problem (even for the 100% observability problem). Figure 4c illustrates performance under noisy (but complete) observations. DRACO outperforms GRAQL with low variance, showing its substantial resilience to noise. DRACO was barely impacted by the 10% and 20% noise ratio, preserving its performance from the full-observable non-noisy observation. Indeed, the gap between DRACO and GRAQL increases as the noise ratio decreases. Such a difference in performance is again likely due to its leverage of DL to the learning stage and the richer representation state available to DRACO. Additionally, Figure 4 also compares the different distance metrics proposed in this paper in section 3.2. Figure 4a shows a slight dominance of DRACO using Wasserstein distance over DRACO using Z-score when trained for < 125K but for 150K+ episodes, this trend changes. The number of trained episodes impacts the agent's optimality, making the Z-score metric better for scenarios where the trained agent policy is more optimal. This behavior happens because the Z-score metric takes into consideration the standard deviation of the policy distribution which takes more episodes to stabilize. Figures 4b, 4c shows a dominance of Wasserstein over Z-score, for observability levels of < 50% and with noisy observations. Wasserstein is better under high uncertainty stemming from the observations, as the Z-score metric is more sensitive to high variability that skews it from the right goal."}, {"title": "Scalability", "content": "Scalability. Scalability evaluation has three main factors: storage, training, and execution time. First, in terms of storage consumption, DRACO uses a constant 8 \u00b10 MB while GRAQL 3190\u00b1700 MB. This difference is primarily due to the huge storage needs of the Q-table used by GRAQL. By contrast, DRACO uses constant memory because it stores only the network architecture and neurons' values. Even though these neurons' values change along the training time, their memory consumption remains constant. Second, DRACO's average training time was 13.52 \u00b1 4.3 minutes while GRAQL's was 1108 \u00b1 463 minutes. We trained both approaches on the same commodity Tesla K40 server. This difference is due to two factors: (1) GRAQL relies on the CPU for its computation, whereas DRACO uses a GPU; and (2) GRAQL's storage-intensive operations are more expensive. Notably, R&G does not require training time, yet its online time is significantly higher than the other algorithms, as it runs planners in inference time. Specifically, DRACO and GRAQL's online runtime takes \u2248 0.12 and \u2248 0.1 seconds on average respectively while R&G takes \u2248 2 seconds. Third, as discussed in section 3.2, GRAQL and DRACO load their policies before inference. GRAQL's Q-tables loading time is often not negligible and takes an average of \u2248 3.4 minutes per table while DRACO's NNs loading takes seconds.\nTo conclude, we evaluate the paper's two main contributions: the DRACO framework and the proposed distance metrics. DRACO scaled better than the state-of-the-art GR algorithms in storage consumption, computation time, and performance. DRACO requires no domain expert to craft an elaborate model like R&G and instead learns via interactions. It achieves better or equal performance results compared to GRAQL and R&G, with both metrics performing overwhelmingly well. Wasserstein was slightly better in less observable spaces and under noisy observations."}, {"title": "6 Related Work", "content": "Goal recognition research can be categorized into two main categories: symbolic and data-driven GR. Symbolic approaches leverage planning, search, and parsing techniques to infer which goal best explains a sequence of symbols (Geib and Goldman 2009; Pereira, Oren, and Meneguzzi 2020). These approaches heavily rely on a domain expert to provide a problem and environment formulation, which makes them difficult to scale and susceptible to noise. By contrast, data-driven GR obviates or mitigates the need for a domain expert. Some learning-based approaches rely on process mining techniques (Polyvyanyy et al. 2020; Ko et al. 2023). These approaches compare an observation sequence to past traces, which makes them brittle when handling large or continuous action spaces. Recent work lifts the requirement of domain experts by relying on tabular RL or DL (Zeng et al. 2018; Min et al. 2014). Amado, Mirsky, and Meneguzzi (2022) develop a GR approach based on tabular RL techniques to learn the domain model, and Fang et al. (2023) extend this work to robotics simulators. Like many other RL algorithms, these algorithms assume that the agent's environment is an MDP. Both Chiari et al. (2022) and Maynard, Duhamel, and Kabanza (2019) use DL to solve GR problems. They propose neural network (NN) architectures that get observations as input and return the probability of reaching each goal. Although the DL approaches presented in these papers present major improvements, it also faces several difficulties. Chiari et al. (2022) discretises the continuous environment and hence suffers from the same discretisation issues discussed earlier. Maynard, Duhamel, and Kabanza (2019) mitigate this by working directly with images of the domain. These approaches require extensive data collection, making them highly inflexible to changes and previously unseen states. DRACO provides a compromise between these approaches: it uses unstructured input, but a distinct NN is allocated for each potential goal, so learning is focused and efficient. DRACO can recognize goals within all the available fluents of the domain as long as interacting with the environment (or a simulation) is possible, which is often a limitation in machine learning approaches for GR (Zhuo et al. 2020)."}, {"title": "7 Conclusion", "content": "This paper provides a novel approach for end-to-end goal recognition in continuous domains. Its contributions include the DRACO algorithm, two distance metrics to compare observed trajectories to a policy, and a testbed of unstructured goal recognition environments and various problems. DRACO is shown to be more accurate, have higher confidence, and scale better than existing approaches, both in discrete and continuous domains. DRACO has many advantages over symbolic and tabular RL approaches. First, it works in both continuous and discrete domains without having the recognition process affected by the quality of a training set. Such approaches are limited by the states and goals within the dataset. Second, its memory footprint and sample efficiency scale better, as earlier algorithms depend on the size of the state and action spaces. Third, it doesn't require a domain-expert to provide the problem's formulation. Finally, value-based RL techniques like Q-learning often fail continuous domains due to instability and poor convergence, leading to suboptimal policies.\nWhile DRACO is promising for robust goal recognition, it has three limitations: First, access to simulations: DRACO requires executing multiple instances to learn the goal-dependent policies. While this does not require DRACO to perfectly replicate the actor's policy, it should be able to produce policies that are different enough from one another. In the future, we aim to extend our work to use policies learned from Imitation Learning (Hussein et al. 2017) or Learning from Observation (Zhu et al. 2020) techniques. Second, number of goals: While DRACO's online inference is fast, it still needs to learn a new policy for each goal the actor might pursue. Potential ways to overcome this are using universal value functions (Schaul et al. 2015) or transfer learning techniques (Taylor and Stone 2009). Third, learning quality: More complex domains may require elaborate learning techniques. DRACO's PPO can be replaced by more sophisticated RL algorithms. For example, image-processing techniques (Gedraite and Hadad 2011) or policy acquisition"}]}