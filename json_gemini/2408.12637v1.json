{"title": "Building and better understanding vision-language models: insights and future directions", "authors": ["Hugo Lauren\u00e7on", "Andr\u00e9s Marafioti", "Victor Sanh", "L\u00e9o Tronchon"], "abstract": "The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.", "sections": [{"title": "1 Introduction", "content": "Vision-language models (VLMs), that take images and texts as inputs and output texts, are highly effective in various applications such as document and figure understanding (Hu et al., 2024), solving visual mathematical problems (Gao et al., 2023), or converting webpage screenshots into code (Lauren\u00e7on et al., 2024). The advancement of powerful open large language models (Touvron et al., 2023; Jiang et al., 2023; Team et al., 2024) and vision encoders (Zhai et al., 2023; Sun et al., 2023; Radford et al., 2021) allows researchers to build upon these unimodal pre-trained models to create advanced VLMs that solve these tasks with increasing accuracy (Dai et al., 2023; Liu et al., 2023; Bai et al., 2023; Lin et al., 2023; Li et al., 2023; Wang et al., 2023). Despite advancements in the field, the literature highlights a variety of divergent design choices across key aspects of the development pipeline, indicating a lack of consensus. For instance, while many recent models (Koh et al., 2023; Li et al., 2023; Liu et al., 2023) have chosen to concatenate the sequence of image hidden states with the sequence of text embeddings before feeding it as input to the language model, the Llama 3-V model (Dubey et al., 2024) use interleaved Transformer-based cross-attentions to fuse the visual information into the LLM, similar to Flamingo (Alayrac et al., 2022). These different core choices in VLM development, often not ablated or justified in research papers, make it challenging to distinguish which decisions impact model performance and assess the compute and data efficiency trade-offs associated with each method. In this paper, we begin by guiding the reader through the main research questions in the field, offering a detailed overview of the latest VLM approaches to address these challenges, along with the strengths and weaknesses of each. Specifically, we focus on (a) the various architectures used to connect pre-trained language models with vision encoders, (b) the different types of data employed in VLM training, their utility, and the typical stage at which they are introduced, (c) the training methods for VLMs, which are often divided into multiple stages for efficiency and stability, and (d) the challenges encountered in model evaluation. We propose future research directions, particularly around data, to enhance model performance. Building on this overview, we then walk through the practical steps for building Idefics3-8B2, a powerful VLM trained efficiently, using only open datasets and a straightforward pipeline. Idefics3-8B significantly outperforms its predecessor, Idefics2-8B, particularly in document understanding tasks, with a 13.7-point improvement on DocVQA (Mathew et al., 2021). To especially boost the capabilities on this task, we created the Docmatix\u00b3 dataset, which includes 2.4 million images and 9.5 million QA pairs derived from 1.3 million PDF documents\u2014a 240-fold increase in scale compared to previous open datasets. We release our model alongside the datasets used for its training."}, {"title": "2 Analyzing architectural choices in VLMS", "content": "Since the introduction of Frozen (Tsimpoukelli et al., 2021) and Flamingo (Alayrac et al., 2022), most VLMs have been built on top of unimodal pre-trained backbones, a language model and/or a vision encoder, rather than training entirely new models from scratch (Koh et al., 2023; Li et al., 2023; Liu et al., 2023). The availability of powerful open-source LLMs (Dubey et al., 2024; Jiang et al., 2023; Team et al., 2024) and image encoders (Zhai et al., 2023; Sun et al., 2023; Radford et al., 2021), which are increasingly expensive to train, enables researchers to leverage these models to create high-performing VLMs at a reduced cost (Dai et al., 2023; Koh et al., 2023; Liu et al., 2023; Vallaeys et al., 2024). These two pre-trained models are usually connected with either a cross-attention or a self-attention architecture."}, {"title": "2.1 Connecting unimodal pre-trained models", "content": "Since the introduction of Frozen (Tsimpoukelli et al., 2021) and Flamingo (Alayrac et al., 2022), most VLMs have been built on top of unimodal pre-trained backbones, a language model and/or a vision encoder, rather than training entirely new models from scratch (Koh et al., 2023; Li et al., 2023; Liu et al., 2023). The availability of powerful open-source LLMs (Dubey et al., 2024; Jiang et al., 2023; Team et al., 2024) and image encoders (Zhai et al., 2023; Sun et al., 2023; Radford et al., 2021), which are increasingly expensive to train, enables researchers to leverage these models to create high-performing VLMs at a reduced cost (Dai et al., 2023; Koh et al., 2023; Liu et al., 2023; Vallaeys et al., 2024). These two pre-trained models are usually connected with either a cross-attention or a self-attention architecture."}, {"title": "2.1.1 Cross-attention architecture", "content": "The cross-attention architecture is introduced in Flamingo (Alayrac et al., 2022). The image hidden states encoded by the vision backbone are used to condition the frozen language model using freshly initialized cross-attention layers that are interleaved between the pretrained language model layers. The keys and values in these layers are obtained from the vision features, while the queries are derived from the language inputs. In practice, a cross-attention block is inserted after every four Transformer blocks in the LLM, adding newly initialized parameters equivalent to roughly 1/4th of the LLM's size. This significant increase in parameters enhances the model's expressivity, allowing it to achieve strong performance without unfreezing the LLM during training, thereby preserving the pre-trained LLM's performance on text-only tasks. Idefics1 (Lauren\u00e7on et al., 2023) and OpenFlamingo (Awadalla et al., 2023) are open replications of Flamingo. More recently, Llama 3-V (Dubey et al., 2024) also adopted this approach to adapt Llama 3 to multimodality."}, {"title": "2.1.2 Self-attention architecture", "content": "In the self-attention architecture (or fully-autoregressive architecture), introduced in FROMAGE (Koh et al., 2023) and BLIP2 (Li et al., 2023), the output of the vision encoder is treated as tokens and concatenated to the sequence of text tokens. The entire sequence is then passed as input to the language model. The sequence of visual tokens can be optionally pooled into a shorter sequence, making the model more efficient both during the training and at inference. We refer to the layers that map the vision-hidden space to the text-hidden space as modality projection layers. Most recent VLMs have now adopted this design, including Llava (Liu et al., 2023), Qwen-VL (Bai et al., 2023), DeepSeek-VL (Lu et al., 2024), SPHINX (Lin et al., 2023), VILA (Lin et al., 2023), MiniGemini (Li et al., 2024), Monkey (Li et al., 2023), MM1 (McKinzie et al., 2024), Idefics2 (Lauren\u00e7on et al., 2024), MiniCPM-V (Yao et al., 2024), InternLM (Dong et al., 2024) or InternVL (Chen et al., 2024)."}, {"title": "2.1.3 Which architecture performs best?", "content": "The performance comparison between these two main types of architectures was explored in Lauren\u00e7on et al. (2024). The pre-trained unimodal models are Mistral-7B (Jiang et al., 2023) for the LLM and SigLIP-SO400M (Zhai et al., 2023) for the vision encoder. The model with the self-attention architecture has a total of 8.3B parameters, including 740M newly initialized, while the model with the cross-attention architecture has a total of 10B parameters, including 2.5B newly initialized. The authors demonstrate that the cross-attention architecture significantly outperforms when the backbones are kept frozen during training. However, when parts of the vision encoder and language model are trained with LoRA (Hu et al., 2022), adding an extra 200M trainable parameters distributed across both models, the cross-attention architecture performs worse despite having more parameters overall. Nonetheless, this study did not evaluate the performance of the VLMs on text-only benchmarks. Intuitively, when parts of the language model are unfrozen during training, we need to incorporate data from the LLM training data mixture into the VLM training data to maintain performance on text-only benchmarks."}, {"title": "2.1.4 Impact of the pre-trained backbones on performance", "content": "Various studies find that the performance of each standalone unimodal pre-trained backbone correlates with the performance of the resulting VLM. For instance, in (Lauren\u00e7on et al., 2024), the authors demonstrate that replacing the language model from LLaMA-1-7B (Touvron et al., 2023) (35.1% on MMLU (Hendrycks et al., 2021)) with Mistral-7B (Jiang et al., 2023) (60.1% on MMLU) leads to a substantial improvement across benchmarks. Analogously, replacing CLIP-ViT-H (Radford et al., 2021) (78.0% on ImageNet (Deng et al., 2009)) with SigLIP-SO400M (Zhai et al., 2023) (83.2% on ImageNet), also leads to a substantial performance improvement across all benchmarks, without changing the total number of parameters of the VLM. Because vision encoders are often trained on different datasets and optimized for various tasks, some models, like SPHINX (Lin et al., 2023), combine representations from multiple encoders, such as DINOv2 (Oquab et al., 2023) and CLIP (Radford et al., 2021), to create a richer sequence of visual embeddings, though this comes at the expense of computational efficiency. Recent research has heavily focused on improving open language models (Touvron et al., 2023; Dubey et al., 2024; Team et al., 2024; Jiang et al., 2023; Zheng et al., 2024; Conover et al., 2023; Mehta et al., 2024; Abdin et al., 2024; Hu et al., 2024; DeepSeek-AI et al., 2024; Bai et al., 2023). In contrast, few open-vision encoders have been released, with SigLIP-SO400M standing out due to its favorable performance-to-parameter ratio with only 400M parameters. This suggests a need for extensively trained open-source vision encoders at scale."}, {"title": "2.2 Examining the other architectural choices", "content": "2.2.1 Is a vision encoder really necessary?\nInstead of employing a vision encoder, Fuyu (Bavishi et al., 2023) feeds image patches directly into the language model after applying a simple linear projection to adjust the dimensions. This architecture offers two main advantages: it is independent of another pre-trained model and preserves all the information from the original image. The latter point is crucial since the original image details might be necessary for accurately responding to the prompt. On the other hand, a pre-trained vision encoder transforms an image into a representation that is independent of the user's prompt. As a result, vision encoders aim to capture as much information as possible and can still miss details pertinent to the prompt. VisFocus (Abramovich et al., 2024) attempts to address this drawback by incorporating the user's prompt into the vision encoder. However, this approach is less natural in interleaved image-text conversations, where prompts may refer back to previous questions. Despite these advantages, this architecture has not yet demonstrated superior performance. Fuyu scores significantly lower on benchmarks compared to the best models of similar size released around the same time. PaliGemma (Beyer et al., 2024) also experimented with this approach and reported a notable drop in performance compared to using a pre-trained vision encoder. The authors suggest that bypassing a vision encoder pre-trained on billions of images could lead to longer training times to achieve similar performance. Furthermore, handling image representation within the language model might decrease its performance on text-only benchmarks. Even if this approach outperformed others on multimodal benchmarks, most VLMs are still not evaluated on text-only benchmarks, making it unclear whether omitting a vision encoder affects text benchmark performance. Finally, this approach has not been tested yet with an efficient pooling strategy that does not significantly reduce information by operating directly on raw pixels. Looking ahead, for tasks like video understanding or extension to other modalities, it will be important to develop an architecture that can efficiently reduce the number of visual tokens passed to the language model to maintain a reasonable sequence length."}, {"title": "2.2.2 How should we connect the vision encoder to the language model?", "content": "Many models, such as FROMAGE (Koh et al., 2023) and LLaVA (Liu et al., 2023), use a simple linear layer between the vision encoder and the LLM, ensuring that all encoded visual information is retained since no pooling strategy is applied. However, this approach results in a long sequence of visual tokens, making training and inference less efficient. To address this, Qwen-VL (Bai et al., 2023) reduces the number of visual tokens by using a single-layer cross-attention module between a group of embeddings and the image hidden states. Similarly, Idefics2 (Lauren\u00e7on et al., 2024) employs a cross-attention module within a perceiver resampler (Jaegle et al., 2021; Alayrac et al., 2022), demonstrating that the number of visual tokens can be compressed to as few as 64 (divided by 77) while maintaining performance for most tasks, except those that require extensive OCR capabilities. InternLM-XComposer2-4KHD (Dong et al., 2024) also shows that increasing the number of visual tokens per image is primarily necessary for benchmarks focused on OCR tasks, such as InfoVQA (Mathew et al., 2022) and DocVQA (Mathew et al., 2021). Despite the efficiency of the perceiver resampler, its use has been challenged in several papers, which suggest leveraging the 2D structure of images more effectively. For instance, HoneyBee (Cha et al., 2024) introduces the C-Abstractor, which reintroduces 2D positional embeddings to the visual features, followed by ResNet blocks (Xie et al., 2017). In mPLUG-DocOwl-1.5 (Hu et al., 2024), the H-Reducer is introduced, using convolutions to divide the number of image hidden states by 4. InternVL (Chen et al., 2024) also achieves a fourfold compression using a simple pixel shuffle strategy. Recently, MiniCPM-V 2.6 (Yao et al., 2024), like Idefics2, chose the perceiver resampler with 64 learnable embeddings but enhanced it by adding 2D positional embeddings."}, {"title": "2.2.3 The image-splitting strategy: a trick to increase the number of visual tokens", "content": "Introduced in UReader (Ye et al., 2023) and SPHINX (Lin et al., 2023), the image splitting strategy involves dividing an original image into multiple sub-images, each of which is encoded separately by the vision encoder. The number of tiles can be fixed, such as consistently using four crops per image, or it can vary depending on the image's original resolution, with the image split every N pixels, for example. When the number of tiles is based on the original resolution, the model is trained with varying numbers of visual tokens. This approach is particularly advantageous during inference: for simpler tasks, fewer visual tokens are needed, saving computational resources, while more computing can be allocated by increasing the image resolution for tasks that require intensive OCR. This flexibility is highly beneficial for models designed to excel both at reasoning on a single image with high computational resources and at processing videos with many frames while maintaining a reasonable sequence length by using a lower resolution for each frame. Most vision encoders are designed for relatively low, fixed image resolutions and are not well-suited for processing large images. The image-splitting strategy addresses this by enabling the use of off-the-shelf pre-trained vision encoders at their original resolution, simply by feeding multiple smaller sub-images to the encoder instead of the original large image. Since the vision encoder's weights are shared across each sub-image, this approach also enhances training efficiency. However, since the tiles of an image are not independent, encoding each one separately can be suboptimal and may result in a loss of global context. To address this, the current strategy involves adding the downscaled original image to the list of tiles, resizing it to match the resolution supported by the vision encoder. While this helps retain some of the overall context, it's not a perfect solution, as the reduced resolution of the original image makes it difficult to capture finer details and its resolution depends on the original image's resolution. Can we do better than the image-splitting strategy? An alternative to the image-splitting strategy and a promising direction for future research is to develop a vision encoder that can natively process images of varying resolutions, including very large ones, without changing the original aspect ratios, potentially incorporating a mechanism for handling long-context efficiently. This model could be trained efficiently using the Patch'n'Pack (Dehghani et al., 2023) strategyate a different number of visual tokens per image based on the original resolution, enabling the entire image to be encoded directly without the need to crop it into multiple sub-images."}, {"title": "3 Training methods and datasets for VLMs", "content": "Training VLMs typically occurs in multiple stages, primarily due to (a) the limited availability of high-quality data at scale, (b) memory constraints for efficient training, and (c) stability issues. During these stages, progressively higher-quality data is introduced, the maximum image resolution is gradually increased, and more model parts are unfrozen. As discussed in the previous section, the process begins with two unimodal pre-trained backbones: a language model and a vision encoder."}, {"title": "3.1 Multi-stage pre-training", "content": "The primary goal of pre-training is to align the backbone models and train the newly initialized parameters in the model. This is achieved using large-scale datasets to expose the VLM to a wide variety of examples to build extensive knowledge and improve robustness against out-of-domain data. To preserve the initial performance of the LLM, some models, like VILA (Lin et al., 2023) and LLaVA-NeXT (Liu et al., 2024), begin training by freezing the backbone models and focusing solely on the newly initialized parameters (the connector) until a satisfactory performance level is achieved. Afterward, the vision encoder and/or the language model can be gradually unfrozen. If instabilities arise, or if there's a need to enhance the model's expressivity while adding more regularization than full unfreezing, a LoRA (Hu et al., 2022) approach can be effective even during the pre-training phase (Lauren\u00e7on et al., 2024). To efficiently train on a large number of images, the image resolution is typically kept low at the start of training and gradually increased over time. Once the resolution is sufficiently high, datasets containing large images, such as PDFs, can be incorporated into the training data. In the following paragraphs, we will discuss the various types of data typically used during this process."}, {"title": "3.2 Fine-tuning", "content": "Similar to the approach commonly used with LLMs (Touvron et al., 2023), fine-tuning is typically done in two stages: supervised fine-tuning (SFT) followed by an alignment phase. The literature offers many high-quality datasets containing diverse images and covering a wide range of tasks. They are often annotated by humans, ensuring accurate QA pairs. Although most of them are relatively small individually, when combined, they provide a sufficient number of examples for an effective SFT. Inspired by previous work on LLMs (Wei et al., 2022; Sanh et al., 2022), InstructBLIP (Dai et al., 2023) and M3IT (Li et al., 2023) were among the first to introduce curated mixtures of academic datasets for fine-tuning VLMs. Building on these efforts, The Cauldron (Lauren\u00e7on et al., 2024) introduced a collection of 50 high-quality datasets covering a broad range of tasks, including general visual question answering, counting, captioning, text transcription, document understanding, chart/figure analysis, table understanding, visual reasoning, geometry, spotting differences between two images, and converting screenshots into functional code. Each dataset in this compilation is formatted into a standardized question/answer format, and when multiple QA pairs exist per image, they are combined into a multi-turn conversation. However, a drawback of academic datasets is that their answers tend to be concise, which may lead the model to generate similarly brief responses, which are often less preferred by users. A potential solution is to use an LLM to expand and rephrase the answers, as in M3IT (Li et al., 2023) and Llava 3-V (Dubey et al., 2024). There are several reasons to include an alignment stage following supervised fine-tuning. The first objective is to align the model's output with human preferences, making it more intuitive and better at following complex instructions. Additionally, as demonstrated in RLHF-V (Yu et al., 2024), this stage effectively reduces hallucinations, where the model might describe objects or details not actually present in the image. It also enhances model safety by minimizing the risk of generating harmful content. It also may further improve overall model performance. RLAIF-V (Yu et al., 2024) provides a dataset of 80K preference pairs, used in the training of MiniCPM-V 2.5 (Yao et al., 2024). VLFeedback (Li et al., 2023) offers 380K comparison pairs, where model responses sampled from 12 VLMs are ranked by GPT-4V (Achiam et al., 2023). Similarly, SPA-VL (Zhang et al., 2024) generates 100K preference pairs through a comparable approach. DPO (Rafailov et al., 2024) is then commonly applied to these datasets during the alignment phase."}, {"title": "4 Challenges in evaluating VLMs", "content": "The earliest and most popular multimodal benchmarks, such as VQAv2 (Goyal et al., 2017), OKVQA (Marino et al., 2019), TextVQA (Singh et al., 2019), and COCO Captioning (Lin et al., 2014), are mainly open-ended. These benchmarks rely on specific ground-truth answers for each question, so even minor variations in the model's responses can lead to a score marked as incorrect. This method of evaluation tends to favor models that produce answers closely aligned with the benchmark's expected format or writing style. For example, VQAv2, which assesses general real-world image understanding, typically expects short answers, often just one or two words. Even when the evaluation prompt clearly specifies this format, models like Gemini 1.0 Ultra (Team et al., 2023) and GPT-4V (Achiam et al., 2023) achieve scores of 77.8 and 77.2, respectively. These scores are notably lower than those of much smaller models that include a small portion of VQAv2 in their fine-tuning data: MM1-3B-Chat (McKinzie et al., 2024) reaches 82.0, and moondream2 achieves 79.4 with only 1.9B parameters. This discrepancy highlights the challenge of evaluating different models without letting the benchmark's template influence the results. One potential way to mitigate this bias is to perform few-shot evaluations, although this approach is less effective than training on the benchmark training set, and is not currently used for evaluating instruct models. However, the level of ambiguity in these evaluations can vary by benchmark. For instance, TextVQA and DocVQA (Mathew et al., 2021) require the model to read and extract text directly from an image without rephrasing it, which reduces ambiguity. In MathVista (Lu et al., 2024), where answers are always numerical, each question is paired with specific instructions, such as indicating whether the answer should be an integer or a float rounded to two decimal places. Recently proposed, the LAVE metric (Ma\u00f1as et al., 2024) consists of asking an LLM to evaluate whether the response generated by the VLM is correct, given the ground truth and the specific question, thereby reducing the template problem. Another way to reduce ambiguity is to use benchmarks that include multiple-choice questions (MCQs), where the model selects the correct option by choosing the corresponding letter. Many recent benchmarks have adopted this approach, such as MMMU (Yue et al., 2024), MMStar (Chen et al., 2024), and MMBench (Liu et al., 2023)."}, {"title": "4.1 Open-ended and multiple-choice benchmarks", "content": "The earliest and most popular multimodal benchmarks, such as VQAv2 (Goyal et al., 2017), OKVQA (Marino et al., 2019), TextVQA (Singh et al., 2019), and COCO Captioning (Lin et al., 2014), are mainly open-ended. These benchmarks rely on specific ground-truth answers for each question, so even minor variations in the model's responses can lead to a score marked as incorrect. This method of evaluation tends to favor models that produce answers closely aligned with the benchmark's expected format or writing style. For example, VQAv2, which assesses general real-world image understanding, typically expects short answers, often just one or two words. Even when the evaluation prompt clearly specifies this format, models like Gemini 1.0 Ultra (Team et al., 2023) and GPT-4V (Achiam et al., 2023) achieve scores of 77.8 and 77.2, respectively. These scores are notably lower than those of much smaller models that include a small portion of VQAv2 in their fine-tuning data: MM1-3B-Chat (McKinzie et al., 2024) reaches 82.0, and moondream2 achieves 79.4 with only 1.9B parameters. This discrepancy highlights the challenge of evaluating different models without letting the benchmark's template influence the results. One potential way to mitigate this bias is to perform few-shot evaluations, although this approach is less effective than training on the benchmark training set, and is not currently used for evaluating instruct models. However, the level of ambiguity in these evaluations can vary by benchmark. For instance, TextVQA and DocVQA (Mathew et al., 2021) require the model to read and extract text directly from an image without rephrasing it, which reduces ambiguity. In MathVista (Lu et al., 2024), where answers are always numerical, each question is paired with specific instructions, such as indicating whether the answer should be an integer or a float rounded to two decimal places. Recently proposed, the LAVE metric (Ma\u00f1as et al., 2024) consists of asking an LLM to evaluate whether the response generated by the VLM is correct, given the ground truth and the specific question, thereby reducing the template problem. Another way to reduce ambiguity is to use benchmarks that include multiple-choice questions (MCQs), where the model selects the correct option by choosing the corresponding letter. Many recent benchmarks have adopted this approach, such as MMMU (Yue et al., 2024), MMStar (Chen et al., 2024), and MMBench (Liu et al., 2023)."}, {"title": "4.2 Challenges in model evaluation during the pre-training stage", "content": "There is a significant discrepancy between the performance of VLMs at the pre-training stage versus after fine-tuning. For instance, Idefics2-base (Lauren\u00e7on et al., 2024) scores 57.9 on TextVQA (Singh et al., 2019) using 8 in-context examples and less than 55 on DocVQA (Mathew et al., 2021) during pre-training. However, after fine-tuning, it achieves 70.4 on TextVQA and 67.3 on DocVQA in a zero-shot setting, without employing the image-splitting strategy. As noted earlier, these open-ended tasks are less influenced by the specific template expected by the benchmark. One reason for this gap is that the model only starts learning the specific task of visual question answering (beyond just image captioning or text transcription) during the fine-tuning stage-unless a third pre-training stage is conducted using large synthetic VQA datasets, as described in Figure 2, which offer examples more aligned with the ones present in benchmarks. When instruction data is omitted during pre-training, more complex tasks like document understanding may perform poorly, and the impact of development choices in the VLM may only become evident after fine-tuning, leading to a delayed feedback loop. This delay can make pre-training ablations misleading. For example, in Idefics2, the authors found no noticeable improvements during pre-training when using 128 visual tokens instead of 64 with their architecture. While this held true for most tasks, the benefit of using more visual tokens per image became apparent in OCR tasks after fine-tuning with the image-splitting strategy. Therefore, to obtain more accurate insights during pre-training ablations, we recommend incorporating instruction data into the data mixture."}, {"title": "4.3 Risk of contamination and overoptimization in some benchmarks", "content": "Some benchmarks are derived from the validation or test sets of existing academic datasets. For instance, MathVista (Lu et al., 2024), a leading benchmark for evaluating reasoning and math capabilities, shows signs of potential contamination. We found that at least 6.6% of the questions include images from the training sets of academic datasets often used in supervised fine-tuning, and 2.2% feature both an image and a question that is identical or highly similar. Additionally, this benchmark often includes questions that are especially difficult to answer unless the model has encountered them during training. For example, we find that at least 6.1% of the questions in MathVista ask variations of the question, What is the age gap between these two people in the image?. Variants of this question are also abundant on KVQA (Shah et al., 2019). Therefore, models incorporating in their fine-tuning data will have an advantage for MathVista. Ultimately, benchmarks should be used to measure model performance, not as a training objective. Fine-tuning on similar examples can boost scores, but it provides little evidence for the model's ability to generalize to real-world scenarios. Thus, we encourage researchers to exclude images used in the benchmarks they evaluate from their supervised fine-tuning data."}, {"title": "5 Idefics3: adapting Llama 3 to multimodality", "content": "In this section, we detail the construction of Idefics3, a VLM based on Llama 3.1 (Dubey et al., 2024) and SigLIP-SO400M (Zhai et al., 2023). First, we begin by preparing the dataset used for training."}, {"title": "5.1 Dataset preparation", "content": "Our approach mainly takes the datasets used in the training of Idefics2 (Lauren\u00e7on et al., 2024) while also adding complementary datasets for supervised fine-tuning to expand the range of tasks covered. These datasets are detailed below."}, {"title": "5.1.1 Extending The Cauldron", "content": "As previously mentioned, The Cauldron (Lauren\u00e7on et al., 2024) is a collection of 50 high-quality datasets from existing literature. We have expanded this collection by adding 6 more datasets: Cord-v27 for training models to output information in JSON format, LNQA for large-scale real-world visual question answering, ShareGPT-40 and IIW-400 (Garg et al., 2024) for generating detailed captions, Geo170K (Gao et al., 2023) for tasks involving geometry, and Docmatix for document understanding."}, {"title": "5.1.2 Enhancing document understanding capabilities with Docmatix", "content": "Document understanding is a critical business application for VLMs. Yet, only a few open-source datasets are available for boosting the performance of models in this area, and they typically include only a limited number of examples. For instance, DocVQA (Mathew et al., 2021) offers 10K images and 40K QA pairs, InfographicVQA (Mathew et al., 2022) contains 2K images and 10K QA pairs, and VisualMRC (Tanaka et al."}]}