{"title": "Building and better understanding vision-language models: insights and future directions", "authors": ["Hugo Lauren\u00e7on", "Andr\u00e9s Marafioti", "Victor Sanh", "L\u00e9o Tronchon"], "abstract": "The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.", "sections": [{"title": "Introduction", "content": "Vision-language models (VLMs), that take images and texts as inputs and output texts, are highly effective in various applications such as document and figure understanding (Hu et al., 2024), solving visual mathematical problems (Gao et al., 2023), or converting webpage screenshots into code (Lauren\u00e7on et al., 2024). The advancement of powerful open large language models (Touvron et al., 2023; Jiang et al., 2023; Team et al., 2024) and vision encoders (Zhai et al., 2023; Sun et al., 2023; Radford et al., 2021) allows researchers to build upon these unimodal pre-trained models to create advanced VLMs that solve these tasks with increasing accuracy (Dai et al., 2023; Liu et al., 2023; Bai et al., 2023; Lin et al., 2023; Li et al., 2023; Wang et al., 2023).\nDespite advancements in the field, the literature highlights a variety of divergent design choices across key aspects of the development pipeline, indicating a lack of consensus. For instance, while many recent models (Koh et al., 2023; Li et al., 2023; Liu et al., 2023) have chosen to concatenate the sequence of image hidden states with the sequence of text embeddings before feeding it as input to the language model, the Llama 3-V model (Dubey et al., 2024) use interleaved Transformer-based cross-attentions to fuse the visual information into the LLM, similar to Flamingo (Alayrac et al., 2022). These different core choices in VLM development, often not ablated or justified in research papers, make it challenging to distinguish which decisions impact model performance and assess the compute and data efficiency trade-offs associated with each method.\nIn this paper, we begin by guiding the reader through the main research questions in the field, offering a detailed overview of the latest VLM approaches to address these challenges, along with the strengths and weaknesses of each. Specifically, we focus on (a) the various architectures used to connect pre-trained language models with vision encoders, (b) the different types of data employed in VLM training, their utility, and the typical stage at which they are introduced, (c) the training methods for VLMs, which are often divided into multiple stages for efficiency and stability, and (d) the challenges encountered in model evaluation. We propose future research directions, particularly around data, to enhance model performance.\nBuilding on this overview, we then walk through the practical steps for building Idefics3-8B2, a powerful VLM trained efficiently, using only open datasets and a straightforward pipeline. Idefics3- 8B significantly outperforms its predecessor, Idefics2-8B, particularly in document understanding tasks, with a 13.7-point improvement on DocVQA (Mathew et al., 2021). To especially boost the capabilities on this task, we created the Docmatix\u00b3 dataset, which includes 2.4 million images and 9.5 million QA pairs derived from 1.3 million PDF documents\u2014a 240-fold increase in scale compared to previous open datasets. We release our model alongside the datasets used for its training."}, {"title": "Analyzing architectural choices in VLMS", "content": ""}, {"title": "Connecting unimodal pre-trained models", "content": "Since the introduction of Frozen (Tsimpoukelli et al., 2021) and Flamingo (Alayrac et al., 2022), most VLMs have been built on top of unimodal pre-trained backbones, a language model and/or a vision encoder, rather than training entirely new models from scratch (Koh et al., 2023; Li et al., 2023; Liu et al., 2023). The availability of powerful open-source LLMs (Dubey et al., 2024; Jiang et al., 2023; Team et al., 2024) and image encoders (Zhai et al., 2023; Sun et al., 2023; Radford et al., 2021), which are increasingly expensive to train, enables researchers to leverage these models to create high-performing VLMs at a reduced cost (Dai et al., 2023; Koh et al., 2023; Liu et al., 2023; Vallaeys et al., 2024). These two pre-trained models are usually connected with either a cross-attention or a self-attention architecture."}, {"title": "Cross-attention architecture", "content": "The cross-attention architecture is introduced in Flamingo (Alayrac et al., 2022). The image hidden states encoded by the vision backbone are used to condition the frozen language model using freshly initialized cross-attention layers that are interleaved between the pretrained language model layers. The keys and values in these layers are obtained from the vision features, while the queries are derived from the language inputs. In practice, a cross-attention block is inserted after every four Transformer blocks in the LLM, adding newly initialized parameters equivalent to roughly 1/4th of the LLM's size. This significant increase in parameters enhances the model's expressivity, allowing it to achieve strong performance without unfreezing the LLM during training, thereby preserving the pre-trained LLM's performance on text-only tasks.\nIdefics1 (Lauren\u00e7on et al., 2023) and OpenFlamingo (Awadalla et al., 2023) are open replications of Flamingo. More recently, Llama 3-V (Dubey et al., 2024) also adopted this approach to adapt Llama 3 to multimodality."}, {"title": "Self-attention architecture", "content": "In the self-attention architecture (or fully-autoregressive architecture), introduced in FROMAGE (Koh et al., 2023) and BLIP2 (Li et al., 2023), the output of the vision encoder is treated as tokens and concatenated to the sequence of text tokens. The entire sequence is then passed as input to the language model. The sequence of visual tokens can be optionally pooled into a shorter sequence, making the model more efficient both during the training and at inference. We refer to the layers that map the vision-hidden space to the text-hidden space as modality projection layers. Figure 1 highlights the different components of the self-attention architecture.\nMost recent VLMs have now adopted this design, including Llava (Liu et al., 2023), Qwen-VL (Bai et al., 2023), DeepSeek-VL (Lu et al., 2024), SPHINX (Lin et al., 2023), VILA (Lin et al., 2023), MiniGemini (Li et al., 2024), Monkey (Li et al., 2023), MM1 (McKinzie et al., 2024), Idefics2 (Lauren\u00e7on et al., 2024), MiniCPM-V (Yao et al., 2024), InternLM (Dong et al., 2024) or InternVL (Chen et al., 2024)."}, {"title": "Which architecture performs best?", "content": "The performance comparison between these two main types of architectures was explored in Lauren\u00e7on et al. (2024). The pre-trained unimodal models are Mistral-7B (Jiang et al., 2023) for the LLM and SigLIP-SO400M (Zhai et al., 2023) for the vision encoder. The model with the self-attention architecture has a total of 8.3B parameters, including 740M newly initialized, while the model with the cross-attention architecture has a total of 10B parameters, including 2.5B newly initialized. The authors demonstrate that the cross-attention architecture significantly outperforms when the backbones are kept frozen during training. However, when parts of the vision encoder and language model are trained with LoRA (Hu et al., 2022), adding an extra 200M trainable parameters distributed across both models, the cross-attention architecture performs worse despite having more parameters overall.\nNonetheless, this study did not evaluate the performance of the VLMs on text-only benchmarks. Intuitively, when parts of the language model are unfrozen during training, we need to incorporate data from the LLM training data mixture into the VLM training data to maintain performance on text-only benchmarks."}, {"title": "Impact of the pre-trained backbones on performance", "content": "Various studies find that the performance of each standalone unimodal pre-trained backbone correlates with the performance of the resulting VLM. For instance, in (Lauren\u00e7on et al., 2024), the authors demonstrate that replacing the language model from LLaMA-1-7B (Touvron et al., 2023) (35.1% on MMLU (Hendrycks et al., 2021)) with Mistral-7B (Jiang et al., 2023) (60.1% on MMLU) leads to a substantial improvement across benchmarks. Analogously, replacing CLIP-ViT-H (Radford et al., 2021) (78.0% on ImageNet (Deng et al., 2009)) with SigLIP-SO400M (Zhai et al., 2023) (83.2% on ImageNet), also leads to a substantial performance improvement across all benchmarks, without changing the total number of parameters of the VLM.\nBecause vision encoders are often trained on different datasets and optimized for various tasks, some models, like SPHINX (Lin et al., 2023), combine representations from multiple encoders, such as DINOv2 (Oquab et al., 2023) and CLIP (Radford et al., 2021), to create a richer sequence of visual embeddings, though this comes at the expense of computational efficiency.\nRecent research has heavily focused on improving open language models (Touvron et al., 2023; Dubey et al., 2024; Team et al., 2024; Jiang et al., 2023; Zheng et al., 2024; Conover et al., 2023; Mehta et al., 2024; Abdin et al., 2024; Hu et al., 2024; DeepSeek-AI et al., 2024; Bai et al., 2023). In contrast, few open-vision encoders have been released, with SigLIP-SO400M standing out due to its favorable performance-to-parameter ratio with only 400M parameters. This suggests a need for extensively trained open-source vision encoders at scale."}, {"title": "Examining the other architectural choices", "content": ""}, {"title": "Is a vision encoder really necessary?", "content": "Instead of employing a vision encoder, Fuyu (Bavishi et al., 2023) feeds image patches directly into the language model after applying a simple linear projection to adjust the dimensions. This architecture offers two main advantages: it is independent of another pre-trained model and preserves all the information from the original image. The latter point is crucial since the original image details might be necessary for accurately responding to the prompt. On the other hand, a pre-trained vision encoder transforms an image into a representation that is independent of the user's prompt. As a result, vision encoders aim to capture as much information as possible and can still miss details pertinent to the prompt. VisFocus (Abramovich et al., 2024) attempts to address this drawback by incorporating the user's prompt into the vision encoder. However, this approach is less natural in interleaved image-text conversations, where prompts may refer back to previous questions.\nDespite these advantages, this architecture has not yet demonstrated superior performance. Fuyu scores significantly lower on benchmarks compared to the best models of similar size released around the same time. PaliGemma (Beyer et al., 2024) also experimented with this approach and reported a notable drop in performance compared to using a pre-trained vision encoder. The authors suggest that bypassing a vision encoder pre-trained on billions of images could lead to longer training times to achieve similar performance.\nFurthermore, handling image representation within the language model might decrease its per- formance on text-only benchmarks. Even if this approach outperformed others on multimodal benchmarks, most VLMs are still not evaluated on text-only benchmarks, making it unclear whether omitting a vision encoder affects text benchmark performance.\nFinally, this approach has not been tested yet with an efficient pooling strategy that does not signifi- cantly reduce information by operating directly on raw pixels. Looking ahead, for tasks like video understanding or extension to other modalities, it will be important to develop an architecture that can efficiently reduce the number of visual tokens passed to the language model to maintain a reasonable sequence length."}, {"title": "How should we connect the vision encoder to the language model?", "content": "Many models, such as FROMAGE (Koh et al., 2023) and LLaVA (Liu et al., 2023), use a simple linear layer between the vision encoder and the LLM, ensuring that all encoded visual information is retained since no pooling strategy is applied. However, this approach results in a long sequence of visual tokens, making training and inference less efficient. To address this, Qwen-VL (Bai et al., 2023) reduces the number of visual tokens by using a single-layer cross-attention module between a group of embeddings and the image hidden states. Similarly, Idefics2 (Lauren\u00e7on et al., 2024) employs a cross-attention module within a perceiver resampler (Jaegle et al., 2021; Alayrac et al., 2022), demonstrating that the number of visual tokens can be compressed to as few as 64 (divided by 77) while maintaining performance for most tasks, except those that require extensive OCR capabilities. InternLM-XComposer2-4KHD (Dong et al., 2024) also shows that increasing the number of visual tokens per image is primarily necessary for benchmarks focused on OCR tasks, such as InfoVQA (Mathew et al., 2022) and DocVQA (Mathew et al., 2021).\nDespite the efficiency of the perceiver resampler, its use has been challenged in several papers, which suggest leveraging the 2D structure of images more effectively. For instance, HoneyBee (Cha et al., 2024) introduces the C-Abstractor, which reintroduces 2D positional embeddings to the visual features, followed by ResNet blocks (Xie et al., 2017). In mPLUG-DocOwl-1.5 (Hu et al., 2024), the H-Reducer is introduced, using convolutions to divide the number of image hidden states by 4. InternVL (Chen et al., 2024) also achieves a fourfold compression using a simple pixel shuffle strategy. Recently, MiniCPM-V 2.6 (Yao et al., 2024), like Idefics2, chose the perceiver resampler with 64 learnable embeddings but enhanced it by adding 2D positional embeddings."}, {"title": "The image-splitting strategy: a trick to increase the number of visual tokens", "content": "Introduced in UReader (Ye et al., 2023) and SPHINX (Lin et al., 2023), the image splitting strategy involves dividing an original image into multiple sub-images, each of which is encoded separately by the vision encoder. The number of tiles can be fixed, such as consistently using four crops per image, or it can vary depending on the image's original resolution, with the image split every N pixels, for example.\nWhen the number of tiles is based on the original resolution, the model is trained with varying numbers of visual tokens. This approach is particularly advantageous during inference: for simpler tasks, fewer visual tokens are needed, saving computational resources, while more computing can be allocated by increasing the image resolution for tasks that require intensive OCR. This flexibility is highly beneficial for models designed to excel both at reasoning on a single image with high computational resources and at processing videos with many frames while maintaining a reasonable sequence length by using a lower resolution for each frame.\nMost vision encoders are designed for relatively low, fixed image resolutions and are not well-suited for processing large images. The image-splitting strategy addresses this by enabling the use of off-the-shelf pre-trained vision encoders at their original resolution, simply by feeding multiple smaller sub-images to the encoder instead of the original large image. Since the vision encoder's weights are shared across each sub-image, this approach also enhances training efficiency.\nHowever, since the tiles of an image are not independent, encoding each one separately can be suboptimal and may result in a loss of global context. To address this, the current strategy involves adding the downscaled original image to the list of tiles, resizing it to match the resolution supported by the vision encoder. While this helps retain some of the overall context, it's not a perfect solution, as the reduced resolution of the original image makes it difficult to capture finer details and its resolution depends on the original image's resolution.\nCan we do better than the image-splitting strategy? An alternative to the image-splitting strategy and a promising direction for future research is to develop a vision encoder that can natively process images of varying resolutions, including very large ones, without changing the original aspect ratios, potentially incorporating a mechanism for handling long-context efficiently. This model could be trained efficiently using the Patch'n'Pack (Dehghani et al., 2023) strategyate a different number of visual tokens per image based on the original resolution, enabling the entire image to be encoded directly without the need to crop it into multiple sub-images."}, {"title": "Training methods and datasets for VLMs", "content": "Training VLMs typically occurs in multiple stages, primarily due to (a) the limited availability of high-quality data at scale, (b) memory constraints for efficient training, and (c) stability issues. During these stages, progressively higher-quality data is introduced, the maximum image resolution is gradually increased, and more model parts are unfrozen. Figure 2 illustrates the key stages of training and the types of datasets used at each stage. As discussed in the previous section, the process begins with two unimodal pre-trained backbones: a language model and a vision encoder."}, {"title": "Multi-stage pre-training", "content": "The primary goal of pre-training is to align the backbone models and train the newly initialized parameters in the model. This is achieved using large-scale datasets to expose the VLM to a wide variety of examples to build extensive knowledge and improve robustness against out-of-domain data. To preserve the initial performance of the LLM, some models, like VILA (Lin et al., 2023) and LLaVA-NeXT (Liu et al., 2024), begin training by freezing the backbone models and focusing solely on the newly initialized parameters (the connector) until a satisfactory performance level is achieved. Afterward, the vision encoder and/or the language model can be gradually unfrozen. If instabilities arise, or if there's a need to enhance the model's expressivity while adding more regularization than full unfreezing, a LoRA (Hu et al., 2022) approach can be effective even during the pre-training phase (Lauren\u00e7on et al., 2024).\nTo efficiently train on a large number of images, the image resolution is typically kept low at the start of training and gradually increased over time. Once the resolution is sufficiently high, datasets containing large images, such as PDFs, can be incorporated into the training data.\nIn the following paragraphs, we will discuss the various types of data typically used during this process. Examples of the most common ones are illustrated in Figure 3."}, {"title": "Image-text pairs", "content": "Image-text pair datasets are generally created by crawling the web, downloading images, and extracting the corresponding alt-text from the original HTML files. Due to the ease of collecting these raw image-text pairs and their effectiveness in establishing strong alignment between images and text, many large-scale datasets have been created, such as LAION (Schuhmann et al., 2022) with 5B images, COYO (Byeon et al., 2022) with 700M images, and DataComp (Gadre et al., 2024) with 12.8B images.\nHowever, the alt-texts in these datasets are often noisy, ungrammatical, or too brief, making training challenging. Recent approaches have achieved better results by using synthetic re-captioning, where the same images from the original datasets are re-captioned using another model (McKinzie et al., 2024; Betker et al., 2023; Lauren\u00e7on et al., 2024). For example, LAION COCO (Schuhmann et al., 2022) re-captioned 600 million images from LAION-5B using an ensemble of BLIP (Li et al., 2022) and two CLIP models (Radford et al., 2021). Similarly, VeCap (Lai et al., 2023) combines the original alt-text with a synthetically generated caption from LLaVA (Liu et al., 2023) to create a dataset of 300 million samples.\nWhile these efforts have mainly focused on generating high-quality captions for given images, less attention has been paid to the initial selection of \"good\" images, which remains a promising area of research. This is important given the high proportion of web images that may not be useful for VLM training (e.g., logos, icons, portraits of non-public figures). Synth2 (Sharifzadeh et al., 2024) addresses this by reversing the usual process, starting with LLM-generated captions and then using a Text-to-Image model to generate corresponding images. Furthermore, studies such as SNIP-Dedup Webster et al. (2023) and SemDeDup (Abbas et al., 2023) have shown that by applying image deduplication, it is possible to train on just half of the LAION dataset with only a minimal reduction in performance compared to using the full dataset."}, {"title": "Interleaved image-text documents", "content": "Training on interleaved image-text documents, also called web documents, was first introduced in Flamingo (Alayrac et al., 2022) using the proprietary M3W dataset. OBELICS (Lauren\u00e7on et al., 2023) is an open-source dataset of interleaved image-text documents, containing 141 million documents and 353 million images. This dataset was constructed from HTML files obtained from Common Crawl dumps, which were carefully filtered. The resulting documents maintain the original linearity of images and texts as they appeared on the websites, while removing spam and ads.\nThe authors highlight several advantages of using web documents in the training data mix: (a) it enhances in-context learning abilities, (b) it improves the model's ability to understand an arbitrary number of images interleaved with text, and (c) it exposes the model to a much wider distribution of texts than what is available in standard image-text pair datasets. This aligns with findings from MM1 (McKinzie et al., 2024), which showed that interleaved data is instrumental for few-shot and text-only performance. OBELICS has been used in the training of various VLMs, including MM1 (McKinzie et al., 2024), Idefics2 (Lauren\u00e7on et al., 2024), and BLIP-3 (Xue et al., 2024). Recently, the scale of these datasets has been significantly expanded, with MINT-1T (Awadalla et al., 2024) growing to 1T documents and 3.4B images, and OmniCorpus (Li et al., 2024) reaching 2.2B documents and 8.6B images.\nModel-based filtering on educational content, similar to the approach in Phi-3 (Abdin et al., 2024) and Fine Web-Edu (Penedo et al., 2024), remains unexplored for these multimodal datasets and could likely offer significant improvements."}, {"title": "PDF documents", "content": "Two primary datasets for PDF documents paired with their text transcriptions are OCR-IDL (Biten et al., 2022) and PDFA4. OCR-IDL includes 26M pages of industry documents, while the English-only filtered version of PDFA contains 18M pages sourced from Common Crawl, offering greater diversity than OCR-IDL. Both datasets were created using OCR extraction tools to obtain corresponding texts and their locations within the documents, which can be linearized into a full document transcription. Idefics2 (Lauren\u00e7on et al., 2024) used these datasets directly during pre-training, an approach also adopted at scale in Llama 3-V (Dubey et al., 2024) to enhance performance on document understanding tasks."}, {"title": "Synthetic data", "content": "sh foundational skills such as (a) image captioning, (b) handling an arbitrary number of images interleaved with diverse texts, and (c) text transcription, all of which are essential for tackling more complex tasks. These datasets are abundant, as they are primarily built by crawling the web, ensuring a broad distribution of texts and images, enhancing robustness against rare examples. However, these datasets fall short in addressing many of the tasks that users typically require, such as document understanding or visual math reasoning, which are significantly more challenging. Relying on generalization or the limited examples in current fine-tuning datasets to master these tasks is not ideal.\nIn the training of LLMs, synthetic data has proven to be highly effective (Zheng et al., 2024; Gunasekar et al., 2023; Liu et al., 2024; Dubey et al., 2024). Given the recent advancements in VLMs, which now solve many real-world examples with high accuracy, creating and training on large-scale synthetic datasets is a logical step. These datasets can be tailored to include examples that closely resemble the tasks users will likely request, making them more relevant than the data used in earlier training stages.\nThe main categories of synthetic data that could be used are outlined below."}, {"title": "Image captioning", "content": "The leading dataset for images paired with detailed captions is PixelProse (Singla et al., 2024). This dataset, built using images from CC12M (Changpinyo et al., 2021), CommonPool (Gadre et al., 2024), and RedCaps (Desai et al., 2021), contains captions generated by Gemini 1.0 Pro (Team et al., 2023). Despite being smaller in scale with 17M images, PixelProse offers richer descriptions and uses a stronger model for caption generation, making it an improvement over LAION COCO. Future improvements could include a more diverse, filtered, and deduplicated set of images, better models to reduce potential hallucinations in the generations, and various prompts for stylistic diversity. A similar dataset, ShareGPT-405, re-captions images using GPT-40 to obtain 57K examples."}, {"title": "Real-world visual question answering", "content": "Datasets in this category contain QA pairs about real- world images, covering topics like identifying people or objects, understanding subtle scenes, counting, color identification, or spatial positioning. The leading dataset in this area is LNQA6, with 300K images sourced from Localized Narratives (Pont-Tuset et al., 2020) and 1.5M QA pairs."}, {"title": "Text reading in natural images", "content": "In LLAVAR (Zhang et al., 2023), the authors use OCR tools to extract text from real-world images in the LAION-5B dataset (Schuhmann et al., 2022), resulting in 420K samples. Similar approaches are seen in MiniCPM-V (Yao et al., 2024) and Llama 3-V (Dubey et al., 2024). The key advantage of these datasets is their scalability and the unique distribution of text in natural images compared to PDF documents, which enhances the model's ability to tackle tasks like TextVQA (Singh et al., 2019)."}, {"title": "Text transcription", "content": "The leading dataset for text transcription is PDFA, mentioned above. However, linearizing texts coherently from bounding boxes can be challenging, and math equations are often inaccurately transcribed or omitted, an area where models like Nougat (Blecher et al., 2023) excel. Additionally, figures and tables are often poorly transcribed by OCR tools. A better strategy for text transcription would involve combining a traditional OCR tool, a document-specialized model like Nougat, and a robust VLM to judge, refine, and merge the outputs of these models."}, {"title": "Document understanding", "content": "Understanding documents from images is complex, making the generation of quality synthetic QA pairs challenging even for advanced VLMs. However, accurate text transcriptions from document images can be obtained with OCR tools, and text-only LLMs are performant at generating QA pairs from these transcriptions. This approach was used to create the dataset Docmatix, introduced in detail later in this paper, which includes 1.3M documents up to 4 pages long and 9.5M QA pairs. Enhancements could involve generating more diverse questions, such as summarizing a paragraph, and employing a strong VLM to filter out erroneous generated QA pairs."}, {"title": "Chart understanding", "content": "ChartGemma (Masry et al., 2024) uses Gemini 1.5 Flash (Reid et al., 2024) to generate 160K QA pairs for chart analysis, covering a range of questions like summarizing insights, converting charts to Markdown tables, and assessing the validity of stated facts based on the chart."}, {"title": "Table understanding", "content": "A dataset for table understanding can be created by either using a strong VLM with table images taken from the web, or by synthetically generating tables with an LLM, rendering them to images, and generating QA pairs with the LLM. However, to our knowledge, there is currently no large-scale open-source synthetic dataset available for this task."}, {"title": "Reasoning with chain-of-thought", "content": "In Meteor (Lee et al., 2024), the authors developed a proprietary dataset to enable a model to answer complex questions using a chain-of-thought strategy (Wei et al., 2022). They began by collecting challenging QA pairs from academic datasets, where the answers were provided without explanations. Then, they employed Claude 3 Haiku (Anthropic, 2024) to generate detailed and comprehensive rationales for these answers. These rationales were finally filtered by GPT-4V (Achiam et al., 2023) to ensure quality, resulting in a final set of 1.1M question-rationale-answer triples."}, {"title": "Visual mathematical reasoning", "content": "Even the most advanced VLMs currently struggle with complex mathematical reasoning and geometry tasks. Generating synthetic data directly from a teacher model is problematic because the teacher often fails to provide correct answers. Instead, datasets like Geo170K (Gao et al., 2023) and MAVIS-Instruct (Zhang et al., 2024) are created by augmenting small and accurate academic mathematical datasets using an LLM. In AlphaGeometry (Trinh et al., 2024), the authors train a model exclusively on synthetically generated geometric problems, enabling it to solve olympiad-level challenges effectively."}, {"title": "Converting web screenshots into HTML code", "content": "To develop models capable of efficiently converting web screenshots into functional HTML code, WebSight (Lauren\u00e7on et al., 2024) introduced a fully synthetic dataset comprising 2M pairs of HTML code and their corre- sponding screenshots. The HTML and TailWind CSS code were generated using DeepSeek-Coder (Guo et al., 2024), merged into a single file, and then filtered and rendered to obtain the web screenshot. Instead of relying on a general LLM coder, further improvements could be achieved by using a specialist LLM fine-tuned specifically for HTML and CSS generation, enabling the creation of more diverse and visually appealing websites. In InternLM-XComposer-2.5 (Zhang et al., 2024), in addition to the WebSight dataset, the authors built a proprietary dataset that includes HTML and CSS files from The Stack v2 (Lozhkov et al., 2024) which were heavily filtered to remove external links and irrelevant content. This approach benefits from more diverse websites in the dataset, though it may introduce challenges with potentially noisy, lengthy, or difficult-to-learn examples."}, {"title": "Locating objects in an image", "content": "Determining the exact positions of objects within an image by generating bounding boxes around them is useful for various applications, such as enabling a VLM to navigate the web by selecting where to click based on positional output. In BLIP3-GROUNDING-50M (Xue et al., 2024), large-scale grounding datasets are created by using a diverse set of images, where objects and their locations are identified using open-world image tagging and object detection models."}, {"title": "Fine-tuning", "content": "Similar to the approach commonly used with LLMs (Touvron et al., 2023), fine-tuning is typically done in two stages: supervised fine-tuning (SFT) followed by an alignment phase."}, {"title": "Which datasets should be used for the SFT?", "content": "The literature offers many high-quality datasets containing diverse images and covering a wide range of tasks. They are often annotated by humans, ensuring accurate QA pairs. Although most of them are relatively small individually, when combined, they provide a sufficient number of examples for an effective SFT.\nInspired by previous work on LLMs (Wei et al., 2022; Sanh et al., 2022), InstructBLIP (Dai"}]}