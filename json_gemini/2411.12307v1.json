{"title": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production", "authors": ["Junhua Liu", "Yong Keat Tan", "Bin Fu", "Kwan Hui Lim"], "abstract": "Accurate multi-turn intent classification is essential for advancing conversational Al systems. However, challenges such as the scarcity of comprehensive datasets and the complexity of contextual dependencies across dialogue turns hinder progress. This paper presents two novel approaches leveraging Large Language Models (LLMs) to enhance scalability and reduce latency in production dialogue systems. First, we introduce Symbol Tuning, which simplifies intent labels to reduce task complexity and improve performance in multi-turn dialogues. Second, we propose C-LARA (Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework that employs LLMs for data augmentation and pseudo-labeling to generate synthetic multi-turn dialogues. These enriched datasets are used to fine-tune a small, efficient model suitable for deployment. Experiments conducted on multilingual dialogue datasets demonstrate significant improvements in classification accuracy and resource efficiency. Our methods enhance multi-turn intent classification accuracy by 5.09%, reduce annotation costs by 40%, and enable scalable deployment in low-resource multilingual industrial systems, highlighting their practicality and impact.", "sections": [{"title": "1 Introduction", "content": "Dialogue systems are critical for automating interactions between customers and agents, streamlining communication and enhancing user experience. They play a pivotal role in international e-commerce platforms, addressing the increasing demand for instantaneous and efficient customer service. Intent classification, a fundamental aspect of natural language understanding in dialogue systems, involves identifying users' goals from their inputs, thereby minimizing waiting times and operational costs [22]. User interactions frequently evolve into multi-turn dialogues when detailed information is required, complicating the development of multi-turn intent classification (MTIC) models, despite their similarity to standard text classification tasks. Additionally, real-world multilingual systems require scalable solutions that uphold inclusivity and ethical standards, particularly in low-resource settings. This complexity arises from the need to consider contextual factors like historical utterances and prior intents. Without a proper understanding of session context, the system risks misinterpreting user intentions, which may result in incorrect applications or irrelevant responses [24]. Consequently, MTIC within dialogue system presents significant challenges.\nThe first challenge is that the length of intents in industrial dialogue systems is longer compared to general text classification tasks. Figure 1 shows that the real intents comprise several words in our knowledge base because operators(Ops) typically assign intents"}, {"title": "2 Problem Formulation", "content": ""}, {"title": "2.1 Multi-Turn Intent Classification", "content": "Multi-Turn Intent Classification (MTIC) involves identifying the intent I of the final query qn from a predefined set I, based on a sequence of user queries Q = {qi}_{i=1}^n in a chatbot session. This task relies on the conversational context C = {qi}_{i=1}^{n-1}, which includes prior queries. Context-dependency adds complexity, requiring models to interpret nuanced conversational dynamics and evolving user intentions. Each intent I has a local-language title y and a hierarchical English category z (e.g., Indonesia: y = 'Cara membatalkan pesanan', z = 'Logistics > Order > Cancellation')."}, {"title": "2.2 Supervised Fine-tuning", "content": "Supervised Fine-tuning (SFT) adapts pre-trained large language models (LLMs) for specific tasks using labeled datasets. This process achieves high benchmark accuracy through task-specific supervision.\nProblem Definition. Given a dataset D = {(xi, yi)}_{i=1}^N, where xi is an input query and yi is the corresponding label, the objective is to optimize model parameters @ to maximize the conditional likelihood p(yi|xi; 0):\nLSFT(0) = - \\frac{1}{N} \\sum_{i=1}^N log p(y_i|x_i; 0)."}, {"title": "2.3 Symbol Tuning", "content": "Unlike methods replacing task labels with unrelated symbols [21], our Symbol Tuning approach focuses on intent classification. Verbose labels in industrial systems disperse semantic information, hindering model performance. To address this, we compress labels into concise phrases using GPT-4. For example, \"Request to Cancel Order\" becomes \"Cancel Order,\" serving as compact semantic anchors that enhance shallow and deep layer representations.\nMathematical Formulation. Let the original intent label be L = {t1, t2, ..., tm}. The compressed label L', with n < m, is generated by optimizing:\nL' = argmin_{L'} C(L') + \\&(L', L),\nwhere: - C(L'): Compactness of L' (e.g., token count). - &(L', L):\nSemantic divergence, computed as:\n&(L', L) = 1 - cosine_sim(\\$(L'), \\$(L)),\nwith $() as an embedding function.\nObjective Function. Given D = {(xi, Li)}_{i=1}^N, where Li is the original label, the supervised fine-tuning loss becomes:\nLST(0) = -\\sum_{(x,L')~D} log p(t_j|t_{<j}, x; 0),\nj=1\nwhere t_{<j} denotes preceding tokens in L'.\nPerformance Implications. Replacing verbose labels L with compact L' reduces token processing and improves classification accuracy, streamlining intent recognition tasks."}, {"title": "3 Solutions", "content": ""}, {"title": "3.1 Symbol Tuning on LLM", "content": "To address intent classification tasks, we utilize generative models rather than conventional discriminative or regressive approaches. Our Symbol Tuning (ST) method involves supervised fine-tuning (SFT) of an LLM with compressed intent labels. Given a complete chat session S = {q1, I1, ..., qn-1, In-1, qn}, the model is trained to generate the representative question in corresponding to the correct intent In of the final query qn. Queries and intents are structured in a natural question-answering flow, as illustrated below:\nSYSTEM: \"A chat between a curious user and an artificial intelligence assistant. The assistant provides helpful, detailed, and polite responses to the user's questions.\"\nUSER: \"{q_1}\"\nASSISTANT: \"The intent title is {r_1}.\"\nUSER: \"{q_n}\"\nASSISTANT: \"The intent title is {r_n}.\"\nThe generated rn is compared with intents in I using cosine similarity in the embedding space to ensure semantic alignment between the model's output and predefined intent titles.\nCompressed Generation. Intent representative queries r often consist of approximately 12 tokens, making them inefficient as generation targets. To address this, we employ an LLM to compress r into concise phrases, typically two words, while preserving their semantic essence. This process ensures that each compressed intent label rc is unique. If duplicates occur, the model iteratively increases the word count until uniqueness is achieved. This compression reduces the average length of rc to four tokens, optimizing it for generation tasks and improving classification accuracy. This approach enhances classification accuracy by reducing semantic dispersion in labels, ensuring more focused information propagation through LLM layers.\nCross-Lingual Labels. In non-English markets, intent labels r are compressed into English while retaining the original language for input queries Q. Leveraging English, the predominant language in LLM pretraining corpora, simplifies label generation and enhances model performance in multilingual settings. This cross-lingual strategy reduces complexity and improves alignment with pretraining distributions. This strategy leverages the strengths of pre-trained LLMs while accommodating multilingual data, offering a scalable solution for cross-lingual intent classification."}, {"title": "3.2 Consistency-aware Linguistics Adaptive Retrieval Augmentation", "content": "To enhance in-context learning, we propose the Consistency-aware, Linguistics Adaptive Retrieval Augmentation (C-LARA) framework. Building upon the LARA model [9], C-LARA incorporates a fine-tuned single-turn model Mc within a retrieval-augmented pipeline. This framework enables zero-shot Multi-Turn Intent Classification (MTIC) using only single-turn demonstrations. Unlike LARA, which is computationally intensive in real-time, C-LARA operates offline as a pseudo-labeling tool, generating high-quality multi-turn data for training lightweight classification models.\nSpecifically, the LARA pipeline can be complex and resource-intensive to implement for real-time systems. Hence, we use this method offline as a multi-turn data pseudo-labeling tool to train a smaller classification model. The training method mirrors that of the single-turn classifier Mc following the paper, adding pseudo-labeled multi-turn data to the original data comprising only single-turn samples.\nSince this is not a real-time task, the pipeline response time is not a critical consideration, hence self-consistency checking was performed on the LLM outputs to ensure the quality of pseudo-labels. For this check, as shown in Figure 3, the in-context learning phase is run three times per sample, with the in-context demonstrations sorted in three orders according to their similarity scores to the session queries: ascending, descending, and random. This approach to self-consistency checking method can also be implemented when"}, {"title": "3.2.1 Hierarchical Text Classification(HTC)", "content": "Mc is an ensemble of label-attention encoder and a hierarchical-aware tree-based encoder with 3-layered global and local intent classifiers.\nThe label-attention encoder has one classifier head for each intent layer. Each classifier head has one hidden linear layer to obtain the layer intermediate output L\u2081, which encodes the layer information. This layer information will be utilised in the input of the next layer classifier head.\nL\u2081 = \\begin{cases} [HW} + b}, & \\text{if } l = 1, \\\\ [(H \\oplus L_{i-1})W +b1, & \\text{if } l > 1, \\end{cases}\nwhere W\u00b9 \u2208 Rd\u00d7d for l = 1 and W\u00b9 \u2208 R2d\u00d7d for l > 1. b} \u2208 Rd, I is the layer number, \u2295 denotes tensor concatenation. Finally, we obtain the local logits H^{local} for each layer classes by using another linear layer\nH^{local} = L_i W^2 + b^2, W^2 \u2208 R^{d\u00d7|I_l|},b^2 \u2208 R^{|I_l|}\nwhere |I1| is the number of classes in the layer.\nHowever, the label-attention model is unaware of the overall hierarchical structure. Therefore, we ensemble it with another method. We refer to HiTIN [26] for the implementation of a state-of-the-art HTC global approach. In this method, a tree network is constructed based on the simplified original taxonomy structure, and the messages are propagated bottom-up in an isomorphism manner, which complements the label-attention model used. The embedding for leaf nodes are obtained by broadcasting the text representation H. After the tree isomorphism network propagation, all embedding from all layers are aggregated to form single embedding, and a classification layer is used to obtain the logits H^{global} of all tree"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "The dataset used in our experiments is derived from the conversation history of a large e-commerce platform. It includes user queries"}, {"title": "4.2 Metrics", "content": "The primary evaluation metric is the accuracy of predicted labels for the final query qn in each conversation session Q. Metrics accounting for class imbalance were not considered, as the sampled sessions reflect the distribution of online traffic across intents, providing a realistic approximation of live performance."}, {"title": "4.3 Implementation Details", "content": "Symbol Tuning on LLM. FastChat framework is used to fine-tune 7B LLMs using LoRA method on their q_proj, v_proj, o_proj, and k_proj modules with a learning rate of 2e-5 over 10 epochs. The 7B models used are Llama-2-7B (for SG) and SeaLLM-7B-chat (for ID) on Hugging Face. Before the models are fine-tuned on the multi-turn intent recognition task, they are further pre-trained on ShareGPT dataset with the same setting above, and the weights are then merged. For the sake of simplicity, we will refer to the LLMs further pre-trained on ShareGPT dataset as base models. During training for intent classification task, loss is calculated on all the model output including those after history queries. During inference, greedy decoding strategy is used to generate the target r part, the prefix \"The intent title is \" is not generated but instead appended at the end of the prompt. When the generated label has no exact match with any r in I, gestalt string matching is used to find the closest one.\nHTC with C-LARA. The in-house Hierarchical Text Classification (HTC) model is a BERT-based model fine-tuned using the combination of the pseudo-labeled multi-turn data and existing single-turn"}, {"title": "4.4 Baseline settings", "content": "For a fair comparison, we adopt three methods fine-tuned on HTC model (Mc) as our global baselines across two methods:\n(1) Single-turn method: where only the last query of a session is considered by Mc;\n(2) Naive concatenation: all queries are concatenated together before being fed into Mc;\n(3) Selective concatenation: where a concatenation selection model is trained to select the most suitable historical query with the last query to serve as the input to Mc.\nST on LLM. In SG, except Llama2-7B, we also tried to continue pre-training the base models on in-domain corpus to strengthen the language understanding of local languages and the corresponding slang used, as humans usually converse with the chatbot in a non-formal way. We term the domain specific base model as Domain-Llama2-7B. In ID, we switched Llama2-7B model to SeaLLM-7B-chat [11] which was introduced specifically for languages in South East Asia.\nThe ST approach was adapted for supervised intent recognition using compressed generation targets (rc) and cross-lingual labels (CL_label). These adjustments optimized performance by simplifying the generative task while maintaining semantic integrity. Comparisons with baseline methods in Table 2 show that ST achieves competitive results in English markets but faces challenges in non-English settings due to limitations in pre-training for low-resource languages.\nHTC with C-LARA. This experiment uses Vicuna-13B as our base model for pseudo-labeling within LARA and C-LARA. We designed three pipelines with four prompt templates in [9] to demonstrate that using C-LARA for pseudo-labeling can effectively improve the HTC model's performance in multi-turn classification tasks. The detailed introduction is listed as follows:\n\u2022 LARA: Using LARA directly as a classifier.\n\u2022 LARA-PL: Using LARA as a naive pseudo-labeling tool and fine-turn HTC model with generated data.\n\u2022 C-LARA: Useing C-LARA to filter out the noise and generate high-quality data to fine-tune the HTC model."}, {"title": "4.5 Offline Experiments", "content": "Symbol Tuning on LLM. Table 2 illustrates the effectiveness of Symbol Tuning (ST) on LLMs. Compressing the generation target r reduces task complexity and improves accuracy by 5.09% in the SG market. This compression also mitigates hallucination, reducing instances of unmatched generated labels from 2.5% to 0%.\nInterestingly, this technique also stopped LLM hallucination, i.e. generating label with no match in the I. The hallucination rate without using compressed r is about 2.5%. In ID, which is a non-English market, we find that cross-lingual label which changes the generation target to English rather than in the local language also improved the performance by 2.53%. Using different base models which were trained specifically on the in-domain corpus or"}, {"title": "4.6 Online Deployment Evaluation", "content": "ST on LLM.. Using the LMDeploy framework, LARA weights were merged with the 7B base model, enabling faster inference times. Deployed on a single 32GB V100 GPU, the Symbol Tuning (ST) approach achieved an average latency of 170ms at 0.5 QPS in the SG market. In contrast, C-LARA models converted to ONNX format (1.1GB per model) achieved an average latency of 80ms at 1 QPS on an 8-core CPU machine with 16GB memory, demonstrating superior scalability and cost-efficiency.\nC-LARA. We deploy C-LARA across all eight markets. The models were first converted to ONNX format, reducing their size to 1.1GB. Deployed on an 8-core CPU machine with 16GB memory, C-LARA achieved an average latency of 80ms at 1 QPS, which is less than half the latency of the ST on LLM method. This deployment significantly reduced both costs and complexity, making it more scalable for industrial applications. Due to its versatility, an Auto-Training Portal (ATP) ecosystem is built around the LARA-PL method (Fig. 4). ATP enables seamless and continuous improvements for the chatbot's multi-turn intent recognition system. Using online chat logs, local operations teams can update the Knowledge Base (KB) by adding new intents and crafting example queries. Subsequently, they can trigger C-LARA for pseudo-labeling multi-turn chat logs, generating data to train lightweight models. Once training is complete, the models are deployed through the portal for online A/B testing, creating an iterative cycle of improvement. For fair comparisons, the version of the KB (intents and single-turn training data) was kept consistent across control and test groups."}, {"title": "4.7 Online Performance", "content": "We leverage the following two metrics:\n(1) Resolution rate (RR) which is measured by the rate of user completing the answer flow, not transferring to live agent, and not giving bad rating to the answer.\n(2) Customer Service Satisfaction (SCSAT) where users will be asked about their satisfaction towards our chatbot for chatbot only sessions (no intervention from live agents). The score is calculated by # good rated sessions/(# good rated sessions + # bad rated sessions).\nWe use the selective concatenation method as the baseline for all experiments, with paired t-test to evaluate statistical significance."}, {"title": "5 Ablation Study", "content": ""}, {"title": "5.1 Effect of Target Length", "content": "We investigate how the amount of information in ST generation target affects the intent recognition performance using two rather extreme approaches and their conversation semantic fluidity."}, {"title": "5.1.1 Longer Target Length", "content": "To achieve this, the model is trained to summarize all queries in Q before outputting the target r. For instance, the new output format of model will be \u201cYou are asking about {summary}. So, the intent title is {rn}\". The rationale is to utilize the summarization ability of LLMs to better understand the context. For our training data, the summaries are obtained by prompting the original LLM backbones in a zero-shot manner. We chose this over increasing the length of r statically to impose more information on the model's generation target. Table 6 demonstrates the impact of increasing the target length in Symbol Tuning (ST). Extending the generation target to include query summaries decreases performance by 3.82%. While this approach enhances semantic coherence, excessive information overloads the model, reducing its ability to focus on the core intent classification task."}, {"title": "5.1.2 Shorter Target Length", "content": "The approach of compressing r was inspired by [21]. Hence, we also tried to replace rs with completely meaningless symbols, while keeping the generation prefix of \"The intent title is \". Compressing target labels to purely symbolic representations results in a significant 8.91% performance drop, as shown in Table 7. This highlights the importance of preserving semantic richness in target labels for generative fine-tuning. Effective compression methods must retain key information from the original labels to avoid loss in classification accuracy.. Thus, when compressing rs, it is important to choose a method that can preserve the information in original rs as much as possible."}, {"title": "5.2 Impact of Self-consistency in MTIC", "content": "Using our multi-turn test sets, we evaluate the performance of MTIC with and without self-consistency checking. We remove the samples with inconsistent outputs and calculate the precision of the remaining samples. On average, 12% of test samples will be removed in each market. Incorporating self-consistency checking into MTIC evaluations improves accuracy across all prompt variations, as shown in Table 4. By removing approximately 12% of test samples with inconsistent outputs, this method effectively filters out erroneous predictions, ensuring higher-quality pseudo-labels and more reliable results. This ensures the quality of pseudo-labels."}, {"title": "5.3 Effect of Model Size", "content": "For fair comparison between LLM ST and C-LARA, we use vicuna-7b-v1.5 as the base model with prompt P and Pformatted, without self-consistency checking. The results of LLM ST method are taken from the best of each market reported in this paper, including base models pre-trained on in-domain corpus, so it should have the advantage over Vicuna-7B-v1.5. Table 5 compares C-LARA and LLM ST using models of the same size (Vicuna-7B-v1.5) without self-consistency checking. Despite the simpler pipeline, C-LARA consistently outperforms LLM ST, avoiding the complexity of multi-turn sample crafting. However, smaller models exhibit reduced instruction-following capabilities, as demonstrated by the lower performance of Pformatted compared to P. One interesting observation here is that the performance of C-LARA when using Pformatted is now lower than P. LLMs of smaller size could be weaker in instruction following, and in this sense the semantic meaning of the labels in demonstrations are more critical. Prepending meaningless characters before labels can negatively affect the understanding of labels for smaller LLMs."}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 Synthetic Data Generation", "content": "The scarcity of annotated dialogue data, particularly in low-resource languages, has driven research into synthetic data generation. Borisov et al. [1] proposed a method leveraging auto-regressive generative models to create realistic tabular datasets, highlighting their utility in data augmentation. Similarly, Li et al. [6] demonstrated that synthetic data generated by LLMs can significantly enhance model performance in classification tasks. Additionally, Tang et al. [18] utilized synthetic data to craft challenging examples for fact-checking, improving the factual accuracy of LLM outputs."}, {"title": "6.2 Modeling Multi-turn Dialogue Context", "content": "Multi-turn dialogue modeling is essential for dialogue understanding tasks. Early methods used bidirectional contextual LSTMs [4] to capture context-aware utterance representations for tasks such as MultiWOZ intent classification [2]. Other approaches, such as multi-channel graph convolutional networks, were applied to query classification in E-commerce [25].\nRecent advancements leverage pre-trained language models (PLMs) as sentence encoders [17], particularly for emotion recognition in conversations (ERC). For instance, Lee and Lee [5] encoded both context and speaker memory using PLMs, while Qin et al. [14] incorporated multi-turn information from utterances and dialogue structure through fine-tuning. Despite their effectiveness, these methods depend heavily on multi-turn training datasets, which are difficult to acquire in real-world e-commerce settings [7]. In contrast, our approach employs LLMs within an augmentation-based pipeline to generate multi-turn data, enabling zero-shot intent classification using smaller models."}, {"title": "6.3 LLM on text classification", "content": "Recent studies have explored the applicability of LLMs across various domains. Chae and Davidson [3] investigated LLMs for sociological text classification, demonstrating their potential in social science research. In financial intent detection, Loukas et al. [10] analyzed the trade-offs between performance and cost when using LLMs for text classification. Liu et al. [8] employed GPT-40 to perform zero-shot classification on multi-level semi-structured text with retrieval augmentation. Wei et al. [20] highlighted the benefits of fine-tuning LLMs on domain-specific datasets, improving performance in legal document review. Wei et al. [21] introduced symbol tuning, where natural language labels were replaced with unrelated symbols during fine-tuning to enhance classification. Our work differs by compressing longer intent labels into semantically meaningful phrases, enabling easier generation and improving accuracy for tasks with a large number of classes."}, {"title": "7 Conclusion", "content": "Multi-turn intent classification plays a critical role in modern dialogue systems. Unlike typical classification tasks, real-world intent classification often involves varying intent lengths, posing unique challenges. In this work, we introduced Symbol Tuning to fine-tune large language models (LLMs) with compressed intents. Our experiments demonstrated that shortening intents improved accuracy by 5.09% compared to using original intents.\nAdditionally, we proposed C-LARA, an augmentation-based pipeline for generating high-quality multi-turn datasets using self-consistency validation. Training smaller models with pseudo-labeled data generated by C-LARA yielded a 1.06% average performance improvement. Empirically, C-LARA significantly reduces annotation costs by automating pseudo-labeling based on the user's latest utterance in dialogue history, improving model iteration efficiency. Furthermore, training smaller models offers computational efficiency, enabling scalable deployment and online inference.\nFuture Work. Moving forward, we aim to incorporate features such as user profiles and order history into C-LARA to support more diverse dialogue tasks. We also plan to explore cross-lingual transfer and advanced tokenization techniques to enhance performance in low-resource languages."}]}