{"title": "You need to MIMIC to get FAME:\nSolving Meeting Transcript Scarcity with a Multi-Agent Conversations", "authors": ["Frederic Kirstein", "Muneeb Khan", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "abstract": "Meeting summarization suffers from limited\nhigh-quality data, mainly due to privacy restric-\ntions and expensive collection processes. We\naddress this gap with FAME, a dataset of 500\nmeetings in English and 300 in German pro-\nduced by MIMIC, our new multi-agent meet-\ning synthesis framework that generates meet-\ning transcripts on a given knowledge source\nby defining psychologically grounded partici-\npant profiles, outlining the conversation, and\norchestrating a large language model (LLM)\ndebate. A modular post-processing step re-\nfines these outputs, mitigating potential repeti-\ntiveness and overly formal tones, ensuring co-\nherent, credible dialogues at scale. We also\npropose a psychologically grounded evalua-\ntion framework assessing naturalness, social\nbehavior authenticity, and transcript difficul-\nties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5\nin naturalness), preserves speaker-centric chal-\nlenges (3/5 in spoken language), and introduces\nricher information-oriented difficulty (4/5 in\ndifficulty). These findings highlight that FAME\nis a good and scalable proxy for real-world\nmeeting conditions. It enables new test sce-\nnarios for meeting summarization research and\nother conversation-centric applications in tasks\nrequiring conversation data or simulating social\nscenarios under behavioral constraints\u00b9.", "sections": [{"title": "1 Introduction", "content": "Meetings underlie collaboration and decision-\nmaking in corporations, academia, and govern-\nment. Meeting summaries help record key discus-\nsion points, update absentees, and capture to-dos\n(Zhong et al., 2021; Hu et al., 2023; Laskar et al.,\n2023). While AI-based summaries are available on\nplatforms such as Zoom\u00b2, Microsoft Teams\u00b3, they\ntypically build on limited English data that does not\nrepresent the diversity of real meetings, e.g., multi-\nlingual sessions, specialized discussions (Kirstein\net al., 2025a). Data scarcity for training and test-\ning meeting summarization systems persists due to\nprivacy and intellectual property concerns, along\nwith expensive manual annotation (Ben Abacha\net al., 2023). Existing corpora, such as AMI (Mc-\ncowan et al., 2005), ICSI (Janin et al., 2003), and\nMeetingBank (Hu et al., 2023), offer only a narrow\nrange of scenarios, which primarily revolve around\nstaged business, academic, or parliamentary meet-\nings. Non-English resources like FREDSum (Ren-\nnard et al., 2023), containing manually transcribed\nand annotated political debates, remain sporadic\nand underscore the lack of linguistic diversity.\nResearchers have explored synthetic transcripts\nto address this scarcity, but many methods are sub-\noptimal in scalability and realism. Single-model,\nomniscient continuation (Qiu and Pan, 2024; Zhou\net al., 2024a) can produce dialogues lacking ac-\ntual knowledge and speaker interplay, while crowd-\nsourced role-plays are expensive (Mccowan et al.,\n2005; Thulke et al., 2024). Automated heuristics\n(e.g., noising, swapping) often yield disjointed con-\nversations (Chen and Yang, 2021; Park et al., 2022;\nLiu et al., 2022). These approaches struggle to bal-\nance large-scale generation with authentic group\ndynamics and credible topic evolution, even though\nthey can process and generate thousands of tokens.\nWe introduce MIMIC (Multi-agent IMItation of\nConversations, see Figure 1), a movie-production-\ninspired framework based on multi-agent debate\n(Liang et al., 2024; Du et al., 2024). MIMIC sum-\nmarizes a knowledge source, expands the summary\ninto an agenda, and orchestrates psychologically\ngrounded agents debating turn-by-turn, allowing\ninterruptions (e.g., phone calls). A modular refine-\nment step mitigates repetitions or overly formal\nspeech, ensuring plausible and coherent discourse\nat scale. With MIMIC, we generate FAME (FAke"}, {"title": "2 Related Works", "content": "Meeting summarization datasets. Most meet-\ning summarization research relies on a few standard\nEnglish-only corpora (Kirstein et al., 2025a), no-\ntably AMI (Mccowan et al., 2005), i.e., staged busi-\nness, and ICSI (Janin et al., 2003), i.e., academic.\nWhile QMSum (Zhong et al., 2021) and Meeting-\nBank (Hu et al., 2023) broaden coverage to parlia-\nmentary sessions and city councils, non-English\ndata remains sparse (e.g., FREDSum (Rennard\net al., 2023) for French and ELITR (Nedoluzhko\net al., 2022) for Czech). We address this gaps by\nreleasing FAME, a corpus of 800 synthetic En-\nglish and German meetings spanning diverse topics,\nmeeting formats, and speaking styles.\nSynthetic dialogue generation. Existing syn-\nthetic meeting data generation approaches often\nrely on relatively simple text continuation by LLMs\n(Qiu and Pan, 2024) or heuristics, e.g., noising,\nswapping (Chen and Yang, 2021; Park et al., 2022;\nLiu et al., 2022), risking superficial turn-taking\nand unrealistic participant behavior (Kirstein et al.,\n2025a). Small-scale manual simulations (Thulke\net al., 2024) offer greater realism but are costly\nand hard to scale. Recent tools like Google's\nNotebookLM (Google, 2024) and Nvidia's PDF-to-\nPodcast (Nvidia, 2025) transform documents into\ntwo-speaker podcasts but lack multi-participant\ngroup dynamics. In contrast, MIMIC simulates\nturn-by-turn interactions among psychology-based\nagents with their own memory, allowing spon-\ntaneous debates and evolving stances. A post-\nprocessing module addresses common LLM flaws\n(e.g., repetition, vocabulary), ensuring high-quality,\nnaturally flowing conversations."}, {"title": "3 The MIMIC Methodology", "content": "We propose MIMIC, a multi-agent framework\ninspired by movie production to generate syn-\nthetic meeting transcripts from a knowledge source\nthrough multi-agent debate (see Figure 1). The ba-\nsic idea is to summarize the knowledge source to\ndistill its key highlights to be discussed in a multi-\nagent-LLM setup. This setup emulates a discussion\namong participants with distinct personas and their\nprivate memory of the knowledge source, including\nreal-world dynamics such as turn-taking, disagree-\nments, clarifications, and topic continuity. MIMIC\noperates in three phases, i.e., pre-production, pro-\nduction, and post-production, with seven stages\noverall (splitted 3/2/2 between each phase). We\nexplain here the methodological background of\nMIMIC, the corresponding prompts, and imple-\nmentation details are covered in Appendices C\nand H. In Appendix I, we present the result of each\nstage using an example knowledge source."}, {"title": "3.1 Pre-Production Phase", "content": "This phase establishes foundational elements, in-\ncluding the meeting's target summary, participant\nroles, and an agenda-like outline.\nStage 1: Content Brainstorming. Given a\nsource text, we prompt an LLM to extract hierar-\nchical topics and subtopics, following the approach\noutlined in Paoli (2023). An LLM composes an\nabstractive target summary following Gao et al.\n(2024), guided by five human-written QMSum sum-\nmaries for consistent brevity, style, and structure.\nThis summary covers the later discussion points\nand topic flow, acting as a basic outline.\nStage 2: Casting. We define participant profiles\nsuited to the meeting context. Each profile contains\na functional role (e.g., project manager, technical\nexpert), background (e.g., experience, qualifica-\ntions), domain expertise, and a distinct perspective\n(e.g., favoring practical solutions). An LLM itera-\ntively creates these profiles, ensuring complemen-\ntary viewpoints without redundancy. Next, each\nprofile receives a speaking style, including tone\n(e.g., formal), language complexity (e.g., jargon),\ncommunication style (e.g., assertive), plus filler\nwords (e.g., \u201cum,\u201d \u201cyou know\u201d) and catchphrases\nto align the person's language with their role.\nWe distributed select knowledge-source para-\ngraphs to each participant based on expertise (Li\net al., 2025), introducing knowledge imbalances"}, {"title": "3.2 Production Phase", "content": "This phase simulates turn-by-turn dialogue among\nmultiple LLMs and validate each scene for quality.\nStage 4: Filming. We generate transcripts scene\nby scene, following the outline from Stage 3:\nScripting. Each participant is an independent LLM\ninstance, contributing one turn at a time. To emu-\nlate real-world scenarios (Zhou et al., 2024a), we\nimplement a non-omniscient approach where each"}, {"title": "3.3 Post-Production Phase", "content": "This phase injects disruptions for realism and pol-\nishes transcripts to ensure real-sounding meetings.\nStage 6: Special effects. We inject flow-breaking\nevents (e.g., phone calls, technical glitches, side"}, {"title": "4 The FAME Dataset", "content": "In the following, we detail how FAME was gener-\nated, present its overall statistics, and analyze the\nauthenticity of its simulated meetings and how chal-\nlenging they are compared to real transcripts. All\ndata and annotations performed here are available\nper Appendix A."}, {"title": "4.1 Setup to generate FAME", "content": "Knowledge source. Unlike synthetic datasets\nbuilt on minimal or artificial context, FAME builds\non grounded text from Wikipedia. We select 300\narticles from 28 broad domains (e.g., Global Issues,\nTechnological Innovations, Cultural Diversity, Phi-\nlosophy, Environment & Ecology), each meeting\nthree criteria: (1) have at least five subsections, (2)\nno negative flags regarding article quality, and (3)\nno reference deficiencies (to avoid contradictory\nclaims). For the German subset, we choose 150\nGerman-language articles from the same pool, en-\nsuring a high content overlap with their English\ncounterparts via BERTScore (Zhang et al., 2020)\nand an empirically chosen threshold of 0.7. For\neach article, we randomly assign meeting types,\nparticipant roles, and the number of participants,\nyielding 500 English and 300 German meetings.\nBackbone model. We use GPT (OpenAI, 2024)\nfor all stages of MIMIC, taking advantage of its\n128k-token context window and robust role-playing\ncapabilities (Kirstein et al., 2024a). In Section 6,\nwe show that models with fewer parameters (Gem-\nini, DeepSeek, Llama) can generate high-quality\ntranscripts with minor drops in naturalness."}, {"title": "4.3 Authenticity Evaluation", "content": "Subsection key finding. FAME meetings exhibit\nnear-real conversation flow, and participants behave\nin ways that closely match human expectations.\nApproach. We evaluate meeting authenticity\nalong two axes: overall authenticity, i.e., coherence,\nconsistency, interestingness, naturalness (Chen\net al., 2023), and participant behavior authenticity,\ne.g., conflicts and power dynamics, defined in con-\nsultation with psychology and sociology experts.\nTherefore, eight overarching behavior categories,\ni.e., knowledge, power, conflict, status, trust, sup-\nport, similarity, and fun (Bales et al., 2009; Choi\net al., 2020), were divided into 18 items complete\nlist given in Table 8 in Appendix D).\nSix annotators (students to PhD candidates in\nPsychology, Computer Science, Communication;\naged 23\u201328, at least C1 English/German) rate 30\nEnglish and 30 German meetings from FAME, as\nwell as 30 meetings from QMSum, using a 1\u20135\nLikert scales Krippendorff's \u03b1 = 0.836. Because\nexisting real-meeting corpora may exhibit a behav-\nior bias due to the formal or staged nature, we also\nsurveyed 100 crowdworkers (age 24\u201363, balanced\nby gender, diverse professional backgrounds such\nas law, engineering, and management) who fre-\nquently attend meetings to collect experiences with\nreal meetings. Additional details on annotators,\ncrowdworkers, and reliability are in Appendix E.\nQuantitative analysis. Table 2 compares overall\nauthenticity scores for FAME and QMSum. The\nEnglish/German FAME subsets match QMSum in\ncoherence, while both subsets score higher in con-\nciseness and interestingness (4.5/5 in FAME vs.\n4/5 in QMSum). ICSI meetings receive the lowest\ninterestingness rating (3/5) due to slower topic evo-\nlution, whereas WPCP leads in naturalness (5/5)."}, {"title": "4.4 Transcript Challenge Assessment", "content": "Subsection key finding. FAME preserves speaker-\nrelated complexities and extends information-\ncentric challenges closer to real meetings.\nAnalysis. We adopt the seven challenges from\nKirstein et al. (2024b), i.e., spoken language,\nspeaker dynamics, coreference, discourse structure,"}, {"title": "5 Baseline Models and Results", "content": "We evaluate current LLMs on abstractive meeting\nsummarization for real and synthetic transcripts,\nsampling 30 meetings from each FAME (English,\nGerman) and QMSum (90 total)."}, {"title": "5.1 Experimental Setup", "content": "Summarization approaches. We benchmark\ntwo closed-weight models (GPT, Gemini) and two\nopen-weight models (Llama, DeepSeek), excluding\nrefinement-based methods (Kirstein et al., 2025c),\nas these would produce self-refined GPT sum-\nmaries. We use a simple zero-shot prompt request-\ning an abstract summary of up to 250 tokens, re-\nflecting standard practices in meeting summariza-\ntion (Kirstein et al., 2025c). Full prompt details are\nprovided in Appendix H.\nEvaluation metrics. We compare system out-\nputs against reference summaries using the es-\ntablished ROUGE (R-1/R-2/R-L) (Lin, 2004) and\nBERTScore (rescaled F1) (Zhang et al., 2020) met-\nrics along MESA (Kirstein et al., 2025b), an LLM-\nbased metric for the error types of meeting summa-\nrization (e.g., structure, irrelevance). These metrics\nenable direct comparisons with prior work and pro-\nvide detailed insights into model weaknesses."}, {"title": "5.2 Results", "content": "Table 4 contains the evaluation scores.\nReasoning boosts performance. Although\nLlama tops the ROUGE/BERTScore metrics on\nboth QMSum and FAME, MESA shows that\nDeepSeek consistently matches or improves\nover other models by minimizing common error\ntypes (~1 point lower per category), especially\non QMSum. DeepSeek also outperforms on the\nGerman FAME subset, while Gemini generally\ntrails behind. All models perform comparably on\nthe English subset, with the least language and\ncoreference issues (~ 1/5).\nFAME reveals LLM struggles. MESA scores\nfor categories like incoherence, structure, and repe-\ntition remain similar to those on QMSum (~4/5)\nbut with lower deviation for the FAME subsets.\nWe conclude that the varied topics and meeting\nformats in FAME add to the overall difficulty and\nnegatively influence summary quality.\nContextualization deficits persist. The omis-\nsion and irrelevance rows show that all models\nstruggle with FAME's more difficult cross-turn in-\nformation and information scarcity observed in Sec-\ntion 4.4. Omission rises from 3/5 on QMSum to\n4/5 on FAME, and irrelevance from 2/5 to 3/5 for\nthe English subset (German remains 2/5). We de-\nrive that reliable content understanding (Kirstein\net al., 2024a, 2025c) can be tested with our FAME\nand that current LLMs struggle with this."}, {"title": "6 Ablations on MIMIC", "content": "Our ablations analyze MIMIC under different set-\ntings and assess its role consistency. Below, we\nbriefly summarize the experiments; full details, ta-\nbles, and figures appear in Appendix G. All eval-\nuations use the same model settings, metrics, and\nannotation guidelines from Sections 4 and 5."}, {"title": "Knowledge source shapes discussion depth\nand structure.", "content": "Beyond the semi-structured\nWikipedia articles used for FAME, we test MIMIC\non 10 research papers from arXiv (Cohan et al.,\n2018) (clearly sectioned, e.g., abstract, methods,\nresults) and 10 human-written stories from Writ-\ningPrompts (Fan et al., 2018) (unstructured). Using\nGPT as the backbone, we generate one synthetic\nmeeting per source (20 total) and apply our evalua-\ntion framework (Section 4). As shown in Table 10\n(overall authenticity) and Figure 5 (behavior au-\nthenticity) in Appendix G.3, quantitative metrics\nremain stable, suggesting meeting naturalness is\nnot strongly tied to input structure. Research pa-\npers lead to deeper discussions, while short stories\nproduce briefer, shallow meetings."}, {"title": "Other models yield shorter meetings.", "content": "To see\nif other LLMs can drive MIMIC, we replace GPT\nwith DeepSeek and Llama, generating 25 meetings\nper model using the FAME Wikipedia article pool.\nWe assess quality (overall authenticity, behavior au-\nthenticity, challenges) and compare outputs using\nidentical knowledge sources. The results are given\nin Table 11 (overall authenticity) and Figure 6 (be-\nhavior authenticity) in Appendix G.4. Although\nthese models produce transcripts with about 50\nfewer turns than FAME, their naturalness remains\nhigh (4/5), and they replicate participant roles and\nbehaviors almost as consistently as GPT, close to\nhuman expectations. Occasionally, transcripts fea-\nture fewer back-and-forth exchanges but remain\nhigh-quality in qualitative reviews, thereby broad-\nening MIMIC's real-world applicability."}, {"title": "Editing is model-dependent but rarely critical.", "content": "Stage 7 (editing) addresses issues such as formal\nphrasing or repeated filler words. To measure its\ninfluence, we review 75 transcripts (25 each from\nGPT, DeepSeek, and Llama), examining chain-of-\nthought logs of stage 7 and final transcripts. Only\n1 in 25 GPT transcripts require major edits to mask\nsynthetic traits, rising to 2 in 25 for the Llama-\nbased models7. Minor refinements correct model-\nspecific wording or repeated transitions, indicating\nthat MIMIC already yields coherent discussions\nwhile stage 7 polishes for full realism."}, {"title": "Roles and behaviors are reliably enacted.", "content": "Drawing on Serapio-Garc\u00eda et al. (2023), we eval-"}, {"title": "7 Final Considerations", "content": "We introduced MIMIC, a seven-stage multi-agent\nframework that uses psychologically grounded,\nnon-omniscient LLMs to generate source-grounded\nmeeting transcripts. MIMIC generated FAME, a\nmultilingual corpus of 500 English and 300 Ger-\nman meetings on 28 domains and 14 meeting types.\nHuman assessments showed that FAME closely\nmirrors real meetings (4.5/5 in naturalness) and\namplified low-information density (4/5). Compar-\nisons with real meetings suggest that FAME cap-\ntures authentic group dynamics, while our ablation\nstudies highlighted how varying knowledge sources\nand backbone models shape transcript quality and\ndemonstrate the GPT's reliable role enactment.\nFAME and its detailed annotations open new\ndirections for meeting summarization, from multi-\nlingual model development to re-introducing fine-\ntuning for LLMs and reinforcement learning to\naddress persistent shortcomings (Section 5). By\nreleasing MIMIC as open-source, we provide a\npowerful toolkit that researchers can adapt to\nlow-resource languages, diverse domains, and a\nrange of conversational styles. The framework's\npsychology-based behavior definitions and evalu-\nation methodology bring a higher level of realism\ninto synthetic conversations, enabling deeper inves-\ntigations of social dynamics. Our work bridges a\ndata gap through human-like meeting simulations\nthat foster advances in summarization, conversa-\ntional AI, social simulation, and beyond."}, {"title": "Limitations", "content": "The quality of generated meetings partly depends\non the underlying LLM's capabilities and biases.\nModels with smaller context windows or different\nlinguistic styles may produce less coherent dia-\nlogues or more frequent artifacts. Nevertheless, our\nablation study shows that even mid-scale LLMs\n(e.g., Llama 3.3) can produce high-quality tran-\nscripts, aided by our feedback loops and refine-\nment stages, to address major flaws. Although\nFAME covers seven broad Wikipedia domains and\nfeatures a German subset, it does not encompass\nall real-world meeting types or cultural nuances\n(e.g., corporate etiquette, cross-cultural communi-\ncation). While specialized domains (e.g., medical\nconferences) remain outside our scope, our frame-\nwork can be easily adapted to additional knowledge\nsources, as evidenced by our tests with short stories\nand research papers (see Section 6).\nA small portion of transcripts shows recurring\nphrases (e.g., \"That's an excellent point\") or overly\npolite tones that may hint at synthetic origins. Our\nmulti-stage post-production phase detects and re-\nvises these repeated patterns, minimizing mechan-\nical politeness and introducing more diverse ex-\npressions. In practice, only 1 out of 30 transcripts\nrequired major edits, suggesting that the remaining\nartifacts do not substantially undermine FAME'S\noverall realism."}, {"title": "\u0397 MIMIC Prompts", "content": "In this appendix, we present the central prompts of\nMIMIC in detail. These include:\n\u2022 Stage 1 (Content Brainstorming): Target sum-\nmary generation (Figure 7) and article tag gener-\nation (Figure 8).\n\u2022 Stage 2 (Casting): Meeting participant genera-\ntion (Figures 9 and 10), speaking style definition\n(Figures 11 and 12) and behavior assignment\n(Figures 13 and 14).\n\u2022 Stage 3 (Scripting): Meeting planning (Fig-\nure 15).\n\u2022 Stage 4 (Filming): Starting participant selection\n(Figure 16) and conversation (Figures 17 and 18).\n\u2022 Stage 5 (Quality assuring): LLM-judge director\n(Figure 19).\n\u2022 Stage 6 (Special effects): Special effects injec-\ntion (Figure 20).\n\u2022 Stage 7 (Editing): Editorial refinement (Fig-\nures 21 and 22), AI content detection (Figure 23),\nand humanizing (Figure 24)."}, {"title": "I Examples for MIMIC Stages", "content": "This appendix shows intermediate results of\nMIMIC with a GPT backbone generating a meeting\ntranscript from the \u201cPandemics\u201d Wikipedia article.\nGiven this input to Stage 1: Content Brainstorm-\ning, the pipeline extracts the topics of the article\nand generates a target summary (Figure 25). In-\nformed about the summary, the pipeline defines\nduring Stage 2: Casting a set of participants such\nas in Figure 26 and further extends the summary\ninto a meeting outline (Figure 27) during Stage 3:\nScripting. The participants are then orchestrated to\ndiscuss the points on the outline turn-by-turn, pro-\nducing one scene per topic such as Figure 28. This\nraw scene undergoes a refinement step during Stage\n5: Quality assuring from the director LLM which\nprovides a thorough feedback (Figure 29). If the\ndirector LLM approves a scene, the pipeline may\ninject, with a probability of 25%, a special effect\ninto the scene (Figure 30) during Stage 6: Special\neffects. Finally, during Stage 7: Editing, the scene\nis assessed by an editorial LLM and a detector-\nrevision LLM to polish AI content and generate the\nfinal scene (Figure 31). After all scenes undergo\nthis procedure, the whole meeting is generated."}, {"title": "Target Summary Generation", "content": "You are a professional meeting summarizer, drawing inspiration from the QMSUM dataset's organized\nand concise style.\nYour task is to summarize a Wikipedia article as if the various facts in the article were discussed in a\nmeeting, now being summarized for participants or readers.\nThe summary should:\n1. Reflect a 'Meeting Summary' Style: Adopt a systematic structure, clearly presenting main points,\nrelevant decisions, and/or action items.\n2. Remain Concise Yet Sufficiently Detailed: Aim for brevity but do not omit crucial details needed\nto understand the discussion.\n3. Stay True to the Article: Ensure accuracy by covering the principal topics while preserving the\nmeeting context.\n4. Match language-speaking Conventions: Generate the summary in language, mirroring the\nphrasing and cultural norms typical of real meetings in that language.\nFollow these rules:\nStructural Requirements:\n1. Opening: Start with the meeting's primary objective or central topic (e.g., 'The meeting focused on\nstandardizing...').\n2. Flow: Group related points into logical sequences (e.g., proposals \u2192 concerns \u2192 resolutions).\n3. Decisions/Actions: Conclude each topic with clear outcomes (e.g., 'agreed to explore alternatives').\n4. Paragraphs: Use 1-2 dense paragraphs without section headers, bullets, or lists.\nLanguage Requirements:\nAvoid: Phrases like 'we discussed,' 'the meeting covered,' or 'participants mentioned.'\nUse Direct Language: Frame points as decisions or facts (e.g., 'The team proposed...' instead of\n'They talked about...').\nTense: Use past tense and passive voice where appropriate (e.g., 'It was agreed...').\nConcision: Omit filler words (e.g., 'then,' 'next').\nBelow are several example meeting summaries illustrating the level of clarity, organization, and\nbalance between detail and concision:\nExamples of QMSUM Style Summaries (Note Structure & Tone):\n> Meeting participants wanted to agree upon a standard database to link up different components of the\ntranscripts. The current idea was to use an XML script, but it quickly seemed that other options, ...\n> The meeting discussed the progress of the transcription, the DARPA demos, tools to ensure meeting\ndata quality, data standardization, backup tools, and collecting tangential meeting information. The ...\nThese examples demonstrate an orderly, concise approach. Summarize the Wikipedia article\nstrictly as a QMSUM-style meeting summary presenting the main topics, relevant decisions,\nkey points of contention, and concluding remarks in cohesive paragraph(s) without using bullet points.\nGenerate an abstractive summary with at most num_words words in language. Ensure it is\nsystematically organized and remains consistent with the meeting type: meeting_type.\nYour Task:\nMeeting Type: meeting_type\nArticle Title: article_title\nContent: content\nNow generate an abstractive meeting summary in language."}, {"title": "Article Tags", "content": "You are a Wikipedia Editor tasked with assigning five highly relevant and specific tags to a given\nWikipedia article.\nThe tags should accurately reflect the main topics, themes, and subjects covered in the article.\nUser Input:\nHere is the Wikipedia article. Only return a Python list of strings including the five most relevant tags\nfor the specified article, reflecting the main topics, themes, and subjects covered in it.\nWikipedia Article: < article >\nOutput Format:\n['tag1', 'tag2', 'tag3', 'tag4', 'tag5']\nEnsure that the list contains exactly five concise, meaningful tags without additional text or\nformatting."}, {"title": "Generate Meeting Participants - Part 1", "content": "When faced with a task, begin by identifying the participants who will contribute to solving the task.\nProvide role and description of the participants, describing their expertise or needs, formatted using\nthe provided JSON schema.\nGenerate one participant at a time, ensuring that they complement the existing participants to foster a\nrich and balanced discussion. Each participant should bring a unique perspective and expertise that\nenhances the overall discussion, avoiding redundancy.\nExample 1: Task: Explain the basics of machine learning to high school students.\nNew Participant: {\"role\": \"Educator\", \"description\": \"An experienced teacher\nwho simplifies complex topics for teenagers.\", \"expertise_area\": \"Education\u201d,\n\"perspective\u201d: \"Simplifier\"}\nExample 2: Task: Develop a new mobile app for tracking daily exercise.\nAlready Generated Participants:\n{\"role\": \"Fitness Coach\", \"description\": \"A person that has high knowledge about\nsports and fitness.\", \"expertise_area\": \"Fitness\", \"perspective\": \"Practical\nImplementation\"}\n{\nNew Participant:\n\"Software Developer\", \"description\": \"A creative\ndeveloper with experience in mobile applications and user interface\ndesign.\", \"expertise_area\": \"Software Development\", \"perspective\": \"Technical\nImplementation\"}\nExample 3: Task: Write a guide on how to cook Italian food for beginners.\nAlready Generated Participants: {\"role\": \"Italian Native\", \"description\": \"An\naverage home cook that lived in Italy for 30 years.\", \"expertise_area\": \"Culinary\nArts\", \"perspective\": \"Cultural Authenticity\"}\n{\"role\": \"Food Scientist\", \"description\": \"An educated scientist that knows which\nflavour combinations result in the best taste.\", \"expertise_area\": \"Food Science\",\n\"perspective\": \"Scientific Analysis\"}\n{\"role\": \"Chef\", \"description\": \"A professional chef specializing\nin Italian cuisine who enjoys teaching cooking techniques.\", \"expertise_area\":\n\"Culinary Arts\", \"perspective\": \"Practical Execution\"}\nNew Participant:"}, {"title": "Generate Meeting Participants - Part 2", "content": "Example 4: Task: Strategize the expansion of a retail business into new markets.\nAlready Generated Participants:\n{\"role\": \"Market Analyst\", \"description\": \"An expert in analyzing market trends\nand consumer behavior.\", \"expertise_area\":\n\"Market Analysis\", \"perspective\":\n\"Data-Driven Insight\"}\n{\"role\": \"Financial Advisor\", \"description\": \"A specialist in financial\nplanning and budgeting for business expansions.\", \"expertise_area\": \"Finance\",\n\"perspective\": \"Financial Feasibility\"}\nNew Participant: {\"role\": \"Operations Manager\", \"description\": \"An experienced\nmanager who oversees daily operations and ensures efficient implementation\nof strategies.\", \"expertise_area\": \"Operations\", \"perspective\": \"Operational\nEfficiency\"}\nUser Input:\nTask: task_description=\"The participants will simulate a meeting based on a given\nmeeting outline, that has to be as realistic as possible. The meeting's content\nwill be a Wikipedia article.\"\nArticle Title: article_title\nArticle Tags: tags\nMeeting Type: meeting_type\nLanguage: language\nUser Prompt:\n\"Now generate a participant to discuss the following task:\"\n\"Task: task_description\"\n\"Initial Article Title: article_title\"\n\"Article Content: article\"\n\"Some of the tags for this article to orient the participant selection on are:\ntags.\"\n\"In case the article tags aren't available/helpful, default to the article title\nand text for choosing the participants.\"\n\"Additionally, generate the participant roles in the target language - **language**\"\n\"Meeting Type: meeting_type\"\nIf participants have already been generated, append:\n\"Already Generated Participants:\"\njson.dumps(participants, indent=2)\nStrict JSON Output Format:\n{\"role\": \"<role name>\", \"description\":\n\"<description>\", \"expertise_area\":\n\"<expertise_area>\", \"perspective\": \"<perspective>\"}\nEnsure:\nThe JSON output follows the exact structure.\nThe participants cover distinct perspectives.\nThe language setting is applied correctly.\n- The response remains valid and processable."}, {"title": "Generate Speaking Style Profile (Part 1: Instructions)", "content": "You are an assistant tasked with creating detailed speaking style profiles for participants in a {meet-\ning_type"}, ".", "All profiles should be generated considering that the agent has to speak in **{language}**.\nKey Attributes:\n1. Tone and Emotional Expressiveness: Describe the general tone and level of emotional\nexpressiveness (e.g., casual and enthusiastic, formal and reserved). Also consider nuances such as\nsarcasm, optimism, seriousness, humor, etc.\n2. Language Complexity and Vocabulary Preference: Specify the complexity of language and\nany preferred types of vocabulary (e.g., simple language, technical language with jargon, metaphors,\nstorytelling).\n3. Communication Style: Outline how the participant communicates (e.g"]}