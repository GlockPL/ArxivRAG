{"title": "SC-Phi2: A Fine-tuned Small Language Model for StarCraft II Macromanagement Tasks", "authors": ["Muhammad Junaid Khan", "Gita Sukthankar"], "abstract": "This paper introduces SC-Phi2, a fine-tuned StarCraft II small language model for macromanagement tasks. Small language models, like Phi2, Gemma, and DistilBERT, are streamlined versions of large language models (LLMs) with fewer parameters that require less power and memory to run. To teach Microsoft's Phi2 model about StarCraft, we create a new SC2 text dataset with information about StarCraft races, roles, and actions and use it to fine-tune Phi-2 with self-supervised learning. We pair this language model with a Vision Transformer (ViT) from the pre-trained BLIP-2 (Bootstrapping Language Image Pre-training) model, fine-tuning it on the MSC replay dataset. This enables us to construct dynamic prompts that include visual game state information. Unlike the large models used in StarCraft LLMs such as GPT-3.5, Phi2 is trained primarily on textbook data and contains little inherent knowledge of StarCraft II beyond what is provided by our training process. By using LORA (Low-rank Adaptation) and quantization, our model can be trained on a single GPU. We demonstrate that our model performs well at micromanagement tasks such as build order and global state prediction with a small number of parameters.", "sections": [{"title": "Introduction", "content": "Gallotta et al. (2024) charted a course for the application of LLMs to games, examining their performance as players, non-player characters, commentators, game masters, and designers. Despite their successes, LLMs often experience continuity problems, due to constraints on context size. However their ability to perform commonsense reasoning makes them a natural fit for open-world games like Minecraft, as demonstrated by recent studies (Wang et al. 2023; Hu et al. 2024; Zhu et al. 2023; Zhou et al. 2024; Yuan et al. 2023).\nKambhampati (2024) asserts that LLMs have very limited reasoning ability and fail at basic planning tasks if small perturbations are made. This makes strategic games, like the highly demanding real-time strategy game StarCraft II (SC2), a difficult area for them. SC2 requires players to excel at both high-level macromanagement tasks like production strategies along with micromanagement (unit-level tactics). The most successful AI systems for SC2 are AlphaStar (Vinyals et al. 2019) and ROA-Star (Huang et al. 2023),"}, {"title": "Related Work", "content": "StarCraft game replays contain a large amount of spatial data that is not easily accessible to LLMs. To overcome this problem Ma et al. (2024) introduced a text-based interface, TextStarCraft II that has specialized adapters for mapping observations to text and text to actions. Even with these adapters, it is infeasible for LLMs to operate at the frame rate speeds necessary for SC2 agents. To combat the speed problem, the authors experimented with multi-frame summarization techniques and action queues. Using this text version of the game, Ma et al. (2024) showed that GPT-3.5, which has 175 billion parameters, performs at a level comparable to a mid-range human player. The authors suggested that incorporating visual data could improve system performance; our proposed model, SC2-Phi2, uses a pre-trained vision transformer.\nSwarmBrain (Shao et al. 2024) is a more specialized agent that also uses GPT-3.5 to make strategic decisions for SC2 (Shao et al. 2024). In SwarmBrain, the Overmind Intelligence Matrix focuses on high-level strategic decisions like resource allocation and base expansion, while the Swarm ReflexNet handles immediate tactical responses in battle. In our work, we demonstrate that a significantly smaller model,"}, {"title": "MSC Dataset", "content": "The MSC dataset, introduced by Wu, Zhang, and Huang (2017), is based on the SC2LE (Vinyals et al. 2017) and comprises over 36,000 replays. It serves as a comprehensive resource for training and evaluating machine learning models for macromanagement tasks in StarCraft II (SC2). To ensure the quality and relevance of the replays, a rigorous preprocessing pipeline was implemented, ensuring that each replay meets the following criteria:\n\u2022 Each match within the replay contains at least 10,000 or more frames.\n\u2022 Both the player and the opponent have at least 10 APM (actions per minute) rate.\n\u2022 Both players have at least 1000 MMR (match-making ratio).\n\u2022 Broken or incomplete replays are excluded.\nEach replay includes global features such as resources collected, and detailed information about units and buildings, all normalized between 0 and 1. Additionally, each replay contains spatial features with a shape of $R^{13\u00d764\u00d764}$ The final outcome of each match, whether a win or a loss, is also recorded and represented by 1 and 0, respectively. This dataset has been used by several others to evaluate build order and win/loss prediction. This paper benchmarks the prediction capabilities of SC-Phi2 against the other two top performers (Wu, Zhang, and Huang 2017; Khan, Hassan, and Sukthankar 2021)."}, {"title": "Method", "content": "Our proposed method operates in two distinct stages:\ni) Stage 1: Primarily concentrates on fine-tuning the Microsoft Phi-2 model (Gunasekar et al. 2023) utilizing our proposed SC2 dataset. This initial stage is dedicated to optimizing model performance specifically for SC2-related tasks.\nii) Stage 2: Proceeds with additional fine-tuning of the Phi-2 model using the MSC dataset. Notably, this stage incorporates textual descriptions sourced from a pre-trained ViT encoder from the BLIP-2 model (Li et al. 2023). The integration of ViT embeddings enriches the model's understanding of textual context from spatial features, enhancing its overall performance."}, {"title": "SC2 Text Dataset", "content": "While LLMs can generate general information about SC2, optimizing the Phi-2 model for SC2-specific tasks demands a nuanced approach. To achieve this, we create a specialized text dataset tailored precisely to model the complexity of SC2 gameplay. Our dataset was aggregated from several online SC2 resources (Liquipedia 2024; Wiki 2024; StarCraft - Wikipedia 2024) and covers the essential elements of SC2 gameplay. It encompasses extensive details on the Protoss, Terran, and Zerg races, capturing their unique characteristics, building specifications, and unit tactics. It also includes specific information on the strengths, weaknesses, and special abilities of each unit.\nBy providing a thorough understanding of these elements, the dataset facilitates strategic planning and decision-making, and can be used for instructional fine-tuning of the language model. Also the dataset includes online sources discussing the roles and effectiveness of units against different opponents, offering invaluable insights for developing effective combat strategies. This includes an analysis of unit interactions and counter-strategies, helping to predict and adapt to enemy tactics.\nBeyond race-specific details, our dataset incorporates rich information on common actions drawn from the PySC2 library (Vinyals et al. 2017). This encompasses a wide range of actions and maneuvers crucial for effective gameplay, such as unit commands, building, training, and morphing actions, and resource management techniques. By including these practical in-game actions, the dataset is enriched with actionable knowledge that mirrors real gameplay scenarios. Lastly, the dataset also contains some common build orders for each of these races, providing the model to learn effective build order strategies. Details of the dataset are provided in the appendix, and the dataset itself is available by request.\nBy compiling such a comprehensive repository of SC2-related information, we lay the groundwork for refining the Phi-2 model. This enables us to enhance its performance on SC2-specific tasks, ensuring that the model can accurately interpret and respond to a diverse range of in-game situations. The rich, detailed dataset not only supports the development of a more robust macromanagement system but also provides a valuable resource for ongoing SC2 research."}, {"title": "Stage-1 Fine-tuning the SLM", "content": "In Stage 1, we focus on the Self-Supervised Fine-Tuning of the Phi-2 model using our SC2 Text Dataset. To facilitate this fine-tuning process, we leverage the SFTTrainer module provided by the Hugging Face Transformers library (Wolf et al. 2020).\nDespite its relatively smaller scale, with 2.8 billion parameters compared to other language models, Phi-2 has consistently demonstrated superior performance over larger counterparts across numerous language tasks. This underscores the efficiency and effectiveness of Phi-2 as a foundation for our model, showcasing its efficacy in handling complex linguistic tasks with remarkable accuracy.\nDuring this stage, we specifically fine-tune the attention layers and feed-forward fully connected layers of the transformer layers within the Phi-2 model using both Low Rank Adaptation (LoRA) and the Quantized Low Rank Adaptation (QLORA) approach (Hu et al. 2022; Dettmers et al. 2024). This fine-tuning process is illustrated in Figure 2. For Stage 1, we specify the LoRA parameters as $alpha = 128$ and r = 64.\nRegarding quantization, we load the model and optimizer in 8-bit mode, conducting fine-tuning over 160 epochs. Additionally, we include a comparative analysis of various configurations across different sets of hyperparameters in the appendix (Table 6 and 7). Utilizing both the LoRA and QLORA approaches enable us to fine-tune our entire model efficiently on a single GPU."}, {"title": "Stage-2 Fine-tuning", "content": "Stage 2 of the fine-tuning process starts with merging the QLoRA-based adapter and its weights trained during stage 1 into the main Phi-2 model. This integration enhances the efficiency of both fine-tuning and inference operations during Stage 2. We keep most of the hyperparameters from stage 1 unchanged except the text prompt, the length of tokens, which is set to 288 tokens, and the batch size, which is set to four.\nLeveraging the MSC dataset, we fine-tune the model once again, following a self-supervised approach. Furthermore, for this stage, in addition to Phi-2, we incorporate a Vision Transformer (ViT)-based vision encoder sourced from the BLIP-2 model. The subsequent sections provide detailed insights into the functionality and implementation of each component.\nVisual Backbone Vision Transformers have emerged as a pivotal tool in vision-language tasks, owing to their exceptional effectiveness and versatility (Yuan, Li, and Sun 2023; Li et al. 2023; Radford et al. 2021). To complement the language backbone, we employ a pre-trained Vision Transformer (ViT) (Dosovitskiy et al. 2021) sourced from the vision-language model BLIP-2 (Li et al. 2023) as our visual backbone.\nThis particular version of ViT model is pre-trained on a diverse array of vision-language tasks, equipping it with the capability to provide textual descriptions for input images/frames. By incorporating this ViT model into our architecture, we aim to enhance our model's ability to interpret visual cues and seamlessly integrate them into our Dynamic Prompt generation setup.\nDuring the fine-tuning process, we input the map screen features into the visual backbone to extract their textual descriptions. For instance, circles on the map screen represent buildings, and these textual descriptions offer valuable insights into the current state of army buildings. These insights are then incorporated into our dynamic prompt generation step, creating a prompt for fine-tuning the model. This integration is shown in Figure 3."}, {"title": "Global Features for Prompt Generation", "content": "Integral to our approach is the incorporation of global features extracted from the MSC dataset into our Dynamic Prompt Generation component. These features encompass critical aspects of the game such as food information, army building progress, and resource collection rates. These features also serve as invaluable indicators, offering essential insights into the evolving dynamics of the game. By assimilating this multifaceted information, our model gains a comprehensive understanding of the complex gameplay dynamics inherent in SC2, empowering it to make informed decisions and predictions with high accuracy.\nIt is important to note that these global features in the MSC dataset are numerical values, with most of them normalized between 0 and 1. To enhance the model's representation and context, we convert these numerical values into categorical values. Specifically, the category low corresponds to numerical values between 0 and 0.2, the category medium represents values between 0.21 and 0.7, and the category high is assigned to values greater than 0.7. Similarly, the game stage is categorized into four distinct phases: i) Early: representing the initial stage of the game; ii) Mid: indicating the progress of the game between 25% and 60%; iii) Late: covering the period between 60% and 90%; and iv) End: denoting the final phase, over 90% of the game.\nAdditionally, we redefine the rewards, converting a reward of 0 to loss and a reward of 1 to win. The actions are also transformed from action IDs to their corresponding full action names. For example, the action ID 75 is mapped to the action Build_Reactor_Factory_quick according to the PySC2 library."}, {"title": "Dynamic Prompt Generation", "content": "In our methodology, we employ dynamic prompts generated during the training process. These prompts are crafted utilizing global features, including vital information such as food availability, army status, mineral and vespene gas reserves, and textual information of spatial features generated from the vision encoder.\nAs the game progresses, these feature values dynamically evolve. To effectively adapt to these fluctuations, we continuously update the prompts in real-time, ensuring that the language model remains informed about the prevailing game circumstances. By providing this contextualized feedback, our model gains deeper insights into the evolving dynamics of the game, enhancing its ability to make informed decisions and predictions. The training prompt is shown in Figure 3."}, {"title": "Prompt Strategy", "content": "In our approach, we utilize a simple prompt for both training and evaluation, in contrast to Chain of Thought (Wei et al. 2022) and other advanced prompt engineering techniques often employed in related works. While these sophisticated approaches are highly effective, especially with large models like GPT-4, they may not be as suitable for smaller models like Phi-2. Given that Phi-2 is significantly smaller and not trained at the same scale as GPT-4, a simpler prompt proves to be more effective for fine-tuning in our context.\nAdditionally, our method offers a comprehensive mechanism for fine-tuning a multimodal model on a single GPU. This approach not only ensures efficient training but also allows for further reduction of computational load during inference, enhancing the overall feasibility and performance of the model."}, {"title": "Final Fine-tuning of SLM", "content": "After generating dynamic prompts, our SLM undergoes another round of fine-tuning, this time utilizing these dynamic prompts as inputs. This fine-tuning process follows the same strategy as Stage 1, employing the QLoRA approach with the optimal set of hyperparameters identified in Stage 1. Once the fine-tuning is complete, we merge these adapters back into the main Phi-2 model, enhancing its inference capacity. The architecture of our model is presented in Figure 1."}, {"title": "Training", "content": "Fine-tuning LLMs can be challenging due to the problem of encountering 'NaN' and 'Inf' values during the backward pass. To ensure the consistent behavior of both the training and the evaluation phases, we set seed values for both the Numpy and the Pytorch libraries. In addition, enabling anomaly detection within PyTorch's autograd engine helps in rapidly identifying and addressing any computational anomalies that may arise.\nWe adopt a strategy where only a small fraction (approximately 4%) of the parameters of the pretrained Phi-2 model are fine-tuned in each stage. This fine-tuning process is executed utilizing both the LoRA and the QLORA approaches, which efficiently updates the model's parameters to adapt to the specifics of the SC2 domain. This process is illustrated in Figure 2. Meanwhile, we freeze the visual encoder and token generation components throughout the training process.\nThe MSC dataset provides over 36000 pre-processed game replays for training the model. However, we only utilize a small subset of the replays to fine-tune our model. These details have been listed in Table 1."}, {"title": "LORA and QLoRA Adaptation for Language Backbone", "content": "Fine-tuning LLMs can pose significant challenges and often demands substantial computational resources. Moreover, fine-tuning LLMs with multiple modalities can exacerbate issues such as gradient vanishing during training, as highlighted by Yuan, Li, and Sun (2023). To mitigate such challenges, we employ the LoRA process along with its variant QLORA in our language backbone.\nLoRA is a mathematical technique aimed at reducing the number of trainable parameters in a model. Unlike traditional fine-tuning methods that update the entire model, LORA adaptation selectively updates only a small subset of the model's parameters (Hu et al. 2022). For any specific layer, the weights are updated using LoRA process as:\n$Wo + AW = Wo + AB$\nIn this equation, $Wo$ represents the pretrained weights of the large model, while $AW$ represents the updated weights obtained through low-rank matrices A and B. Here, $Wo \u2208 R^{dxk}$, $A \u2208 R^{rxk}$, and $B \u2208 R^{dxr}$, and $r << min(d, k)$. Typically, B is initialized with zeros, while A is initialized with a normal distribution. The output is then calculated as:\n$h = Wo.X + AW.X$ \n$= Wo.X + AB.X$\nwhere X is the input to a layer or block. The dimensions of matrices A and B are determined by the LoRA 'r' and 'alpha' parameters.\nIn our approach, we methodically identify the layers requiring updates during the fine-tuning stage through the LORA process. To ensure stability and prevent training difficulties, we adopt techniques outlined by Hu et al. (2022) and Yuan, Li, and Sun (2023). Specifically, we fine-tune all attention layers and selectively update some fully connected layers. Additionally, we also fine-tune out projection layers to further enhance stability and robustness during the fine-tuning process, depicted in Figure 2. This systematic approach not only streamlines the fine-tuning process but also contributes to the overall stability and efficiency of our model.\nWhile LORA focuses on estimating the weights through smaller matrices, QLoRA leverages quantization techniques to reduce the memory footprint of the LLMs during fine-tuning while maintaining performance. Quantization involves reducing the precision of the model's parameters to lower bit widths, typically 8-bit or lower. In our case, we use 8-bit quantization during fine-tuning. By quantizing the model's parameters and applying layer-wise random adaptation, QLORA enables fine-tuning of the entire LLM on a single GPU, making it suitable for deployment on devices with limited computational resources."}, {"title": "Results", "content": "For the initial set of experiments, we fine-tuned three distinct PEFT adapters: i) one for Terran vs. Terran matches; ii) one for Protoss vs. Protoss matches; and iii) one for Zerg vs. Zerg matches, following the first stage of our method. Each adapter was fine-tuned for an additional 2 to 3 epochs using a subset of their respective training replays as described in the training section. After fine-tuning, we evaluated each adapter on the test replays from the MSC dataset. Using the evaluation prompt, we instructed the model to generate actions and predict the game outcome based on the provided information. The generated results were then compared to previous methods, with results summarized in Table 3. SC-Phi2 outperformed previous supervised approaches based on GRUs (Wu, Zhang, and Huang 2017) and transformers (Khan, Hassan, and Sukthankar 2021) across all three match-ups.\nA sample of the generated actions and game outcome, along with ground truth actions and actual outcome is presented Table 5. The sample shows both correctly generated actions and an incorrectly generated action (Action 4, marked in red).\nThe next set of experiments explores the generalizability of the adapters. We took the adapters from the previous experiments and fine-tuned them for different match-ups. For instance, the adapter fine-tuned on the Terran vs. Terran match-up was tested using a zero-shot approach on Terran vs. Zerg match-up. The results are presented in Table 4. These findings indicate that zero-shot transfer learning was not as effective as anticipated and that fine-tuned adapters provide a significant boost to performance."}, {"title": "Conclusion", "content": "In this work, we introduce SC-Phi2, a multimodal small language model that leverages both the Phi-2 and ViT models for SC2 macromanagement prediction tasks. Our approach employs dynamic prompts constructed from the game's global information, such as resources and food, along with textual descriptions of visual features extracted from the ViT model. These prompts are updated during fine-tuning to reflect the game's progress. Our method outperforms previous approaches in both global state prediction and build order prediction. In addition, we show that we can train our model on single GPU using LoRA and quantization approaches.\nOur research on fine-tuning small language models is a step towards reducing the carbon footprint of AI agents. We concur with Gallotta et al. (2024) that the most fertile areas for LLM research are likely to be in design and commentator systems rather than in surpassing the best AI players. In future work, we plan to explore the usage of SC-Phi2 as a StarCraft commentator system that can comment on SC2 gameplay in real-time; prior work in this area (Ranella and Eger 2023) has demonstrated the utility of LLM commentary for League of Legends."}]}