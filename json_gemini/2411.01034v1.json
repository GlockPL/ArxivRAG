{"title": "EVALUATION METRIC FOR QUALITY CONTROL AND GENERATIVE MODELS IN HISTOPATHOLOGY IMAGES", "authors": ["Pranav Jeevan", "Neeraj Nixon", "Abhijeet Patil", "Amit Sethi"], "abstract": "Our study introduces ResNet-L2 (RL2), a novel metric for evaluating generative models and image quality in histopathology, addressing limitations of traditional metrics, such as Fr\u00e9chet inception distance (FID), when the data is scarce. RL2 leverages ResNet features with a normalizing flow to calculate RMSE distance in the latent space, providing reliable assessments across diverse histopathology datasets. We evaluated the performance of RL2 on degradation types, such as blur, Gaussian noise, salt-and-pepper noise, and rectangular patches, as well as diffusion processes. RL2's monotonic response to increasing degradation makes it well-suited for models that assess image quality, proving a valuable advancement for evaluating image generation techniques in histopathology. It can also be used to discard low-quality patches while sampling from a whole slide image. It is also significantly lighter and faster compared to traditional metrics and requires fewer images to give stable metric value.", "sections": [{"title": "1 Introduction and Background", "content": "Generative models, particularly generative adversarial networks (GAN), variational autoencoders (VAE), and diffusion are used in histopathology for various tasks, such as data augmentation [1], anomaly detection [2], and synthetic image generation [3], for improving diagnostic models by addressing scarcity of data. GANs and diffusion generate realistic images to train models for tasks, such as cancer detection [4], while VAEs aid in anomaly detection by reconstructing healthy tissues and identifying deviations [5]. These models also facilitate style transfer between staining protocols, thus reducing costs and enhancing cross-modal comparisons. Overall, generative models enhance data efficiency and diagnostic accuracy in histopathology.\nEvaluating the quality and performance of generative models is challenging due to the lack of well-established, theory-backed metrics. This complexity stems from the need to assess various aspects of generated images, such as quality, aesthetics, realism, and diversity, all of which are inherently subjective. While human evaluation is effective, it is costly, time-consuming, and impractical for large datasets. As a result, researchers rely on automated evaluation methods that should ideally respond consistently to image degradation, accurately measure image quality, and remain computationally efficient, particularly in resource-limited environments.\nCommonly used metrics for evaluating generative models include Inception Score (IS) [6], Kernel Inception Distance (KID) [7], Fr\u00e9chet Inception Distance (FID) [8], and contrastive language-image pre-training (CLIP) Maximum Mean Discrepancy (CMMD) [9]. KID, FID and CMMD require thousands of real and generated images to compute a stable metric which becomes a big issue in medical domain with data scarcity. Flow-based Likelihood Distance [10] have been proposed as an alternative, functioning effectively with fewer data, but they come with the complexity of training large normalizing flows, which is inefficient due to the need to maintain the large latent space. Another limitation of these metrics is their reliance on Inception-V3 features, which are trained on natural image datasets such as ImageNet."}, {"title": "2 ResNet-L2", "content": "Normalizing flows are the only class of generative models capable of providing exact and efficient likelihood estimates for data [12]. However, they require the latent space to have the same dimensionality as the input data, leading to substantial computational demands, particularly for high-dimensional inputs, such as images. To address this issue, we apply normalizing flows to the extracted image features rather than the raw images, thus effectively reducing the dimensionality and computational complexity while preserving the model's ability to estimate data likelihoods accurately and efficiently.\nOur proposed ResNet-L2 (RL2) metric employs a normalizing flow applied to image features extracted from a pre-trained ResNet-18, and computes the root mean square distance between the latent vectors of real and generated images. Specifically, we start with a set of real images, denoted as R, and a set of generated images, G as shown in Figure 1. We use a pre-trained ResNet-18, truncated before the final layer, as the feature extractor f, and a normalizing flow N, parameterized by two trainable sets of parameters, 0 and \u00a7 respectively.\n\\(X_f = f(x_{in}, \\theta) \\quad x_{in} \\in R, x_f \\in \\mathbb{R}^{512}\\) (1)\n\\(z = N(x_f, \\xi) \\quad z \\in \\mathbb{R}^{512}, x_f \\in \\mathbb{R}^{512}\\) (2)\nWe train the combined network (Eq. 1 and Eq. 2) on real images R to maximize the log-likelihood of the real data. This training process is performed only once, allowing the network to map image features into a Gaussian latent space that assigns high likelihoods to real image features. Notably, since ResNet-18 is a significantly lighter model\n(11 M parameters) compared to Inception-V3 (23 M parameters) or CLIP, which are used in previous metrics, our approach offers faster and more computationally efficient metric computation. Moreover, by applying the normalizing flow to the extracted image features rather than the raw images, we further enhance efficiency due to the reduced dimensionality of the latent space, enabling quicker training. Training on real histopathology data allows the model to capture domain-specific features and assign higher probabilities to real histopathology images. This domain-specific adaptation is absent in previous metrics, which rely solely on frozen networks pre-trained on ImageNet.\nAfter training the network on real images R, we extract the mean of the latent vectors z for real and generated images and compute the L2 (Euclidean distance) between the real and generated latent vectors2.\n\\(z_r = \\frac{\\sum_{x \\in R} z}{R} \\quad z \\in \\mathbb{R}^{512}\\) (3)\n\\(RL2 = ||z_r - z_g ||_2 = \\sqrt{\\sum_{i=1}^n (z_{ri} - z_{gi})^2}\\) (4)\nThe computation of L2 distance between two vectors is computationally lighter and faster than the computing Fr\u00e9chet distance or maximum mean discrepancy between them. Hence, RL2 is lighter, faster and efficient to compute than previous metrics."}, {"title": "3 Datasets, Implementation and Experiments", "content": "The FocusPath dataset [13] consists of 864 image patches, each with a resolution of 1024 \u00d7 1024 pixels in sRGB format, capturing varying degrees of focus. These patches are cropped from nine distinct whole slide images (WSIs), with\n16 different z-levels employed to simulate various out-of-focus conditions. The tissue slides selected for the dataset feature diverse color staining techniques, ensuring that the dataset reflects a wide range of histopathological staining variations. The dataset involves image patches with varying degrees of focus, translating to different levels of blur. This dataset serves as an excellent tool for assessing our metric's capability to distinguish between blurry and clear images. Moreover, it allows us to evaluate the monotonicity of our metric with respect to the degree of blur, as it should assign higher values to images with greater blurriness. We train our network only on images with z-level 0 (which are blur free) and evalaute it on all the other images with higher degree of blur. We add salt and pepper noise and rectangular patch noise to the 0 z-level images for our experiments involving other kinds of noise. We used images of 100\u00d7 resolution from BreakHis [14] dataset for generating synthetic images for our experiments to test RL2 for diffusion noise.\nFor the experiment to filter out low-quality noisy image patches from good quality ones, we used HistoROI dataset [11]. HistoROI dataset is developed to segment WSIs into six key classes: epithelium, stroma, lymphocytes, adipose, artifacts, and miscellaneous. Artifacts in this dataset include out-of-focus areas, tissue folds, cover slips, air bubbles, pen marks, and areas with extreme over- or under-staining, which were carefully labeled to aid in quality control for pathology image analysis. We trained our network using 1500 artifact-free patches from epithelium and stroma classes. We then use this network to classify 474 artifact patches from 474 clean patches.\nWe use a normalizing flow using real-valued non-volume preserving (real NVP) transformations [15] in our experiments. We utilized 256 \u00d7 256 patches from all the datasets, achieved by cropping the original images. All experiments were run on Nvidia A6000 GPU. For experiments using salt and pepper noise, rectangular patch noise, and diffusion noise, we used the same experimental setup used by FID paper [8]."}, {"title": "4 Results and Discussion", "content": "Figure 2 illustrates that as the level of blur increases, indicated by higher z-levels, ranging from 1 to 8 and -1 to -8, our metric RL2 effectively identifies these images as noisier compared to the 0 z-level images. Moreover, RL2 displays a monotonous increase with greater intensity of blur in both directions, highlighting its effectiveness as an ideal tool for identifying blurry images compared to clear, noise-free ones.\nA robust evaluation metric for generative models must effectively distinguish between varying levels of different noise types. In our experiments with salt-and-pepper noise and rectangular patch noises, common in histopathology images, our metric, RL2, shows a monotonic increase with rising noise levels as shown in Figure 3 and Figure 4. This demonstrates its effectiveness in evaluating image quality. The capability to identify corruptions in real images makes our metric a valuable tool for detecting subtle differences caused by various noises. Even when synthetic image data significantly differs from real data, our metric reliably identifies varying levels of various noise."}, {"title": "5 Conclusions", "content": "Popular generative model evaluation metrics, such as FID, have proven unreliable for assessing the latest models, such as diffusion. This is particularly true in domains, such as histopathology, where the available images are limited. Previous metrics requiring thousands of samples for stability is not suitable, nor are those metrics relying on pre-trained ImageNet weights due to the distinct nature of histopathology image features.\nWe developed a new evaluation metric based on normalizing flow and measures the L2 distances between real and generated features. Our metric has been shown to be monotonic with respect to various noise types found in histopathology, including noise, occlusion, and blur. Additionally, because our metric is lighter, faster and requires significantly fewer resources, it is efficient than previous metrics. We believe this work will empower researchers to train and test generative models more efficiently."}, {"title": "6 Compliance with Ethical Standards", "content": "This research used public data, ethical approval was not required as confirmed by the license attached with the data."}]}