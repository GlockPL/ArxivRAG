{"title": "Optical Flow Matters: an Empirical Comparative Study on Fusing Monocular Extracted Modalities for Better Steering", "authors": ["Fouad Makiyeh", "Mark Bastourous", "Anass Bairouk", "Wei Xiao", "Mirjana Maras", "Tsun-Hsuan Wangb", "Marc Blanchon", "Ramin Hasani", "Patrick Chareyre", "Daniela Rus"], "abstract": "Autonomous vehicle navigation is a key challenge in artificial intelligence, requiring robust and accurate decision-making processes. This research introduces a new end-to-end method that exploits multimodal information from a single monocular camera to improve the steering predictions for self-driving cars. Unlike conventional models that require several sensors which can be costly and complex or rely exclusively on RGB images that may not be robust enough under different conditions, our model significantly improves vehicle steering prediction performance from a single visual sensor. By focusing on the fusion of RGB imagery with depth completion information or optical flow data, we propose a comprehensive framework that integrates these modalities through both early and hybrid fusion techniques.\nWe use three distinct neural network models to implement our approach: Convolution Neural Network - Neutral Circuit Policy (CNN-NCP), Variational Auto Encoder - Long Short-Term Memory (VAE-LSTM), and Neural Circuit Policy architecture VAE-NCP. By incorporating optical flow into the decision-making process, our method significantly advances autonomous navigation. Empirical results from our comparative study using Boston driving data show that our model, which integrates image and motion information, is robust and reliable. It outperforms state-of-the-art approaches that do not use optical flow, reducing the steering estimation error by 31%. This demonstrates the potential of optical flow data, combined with advanced neural network architectures (a CNN-based structure for fusing data and a Recurrence-based network for inferring a command from latent space), to enhance the performance of autonomous vehicles steering estimation.", "sections": [{"title": "1. Introduction", "content": "Self-driving cars play a critical role in improving transportation by enhancing accessibility, efficiency and safety.\nThese cars have intricate systems that enable them to perceive their environment and produce precise control movements, but they can sometimes take unsafe routes through conflict areas [24]. Reaching full autonomy requires expertise in multiple areas: perception, localization, planning, and control [25]. In this work, we focus on the steering estimation, a vital aspect of control that keeps vehicles within road bounds so that they can safely move around. Challenge arises with varying road conditions such as different road colors, textures, lighting and lane markings [8,26].\nTraditional methods of steering estimation use modular systems that deal with perception decision-making and control separately. However, these methods tend to be effective but have complex systems which need exact calibration as well as integration of different components possibly leading to mistakes as well as inefficiencies [6]. Recently end-to-end deep learning has emerged, where neural networks"}, {"title": "2. Related works", "content": "Artunedo et al. [2] presented a comprehensive comparison between model-based and model-free approaches to assess the effectiveness of different lateral control algorithms used in autonomous driving systems. Model-based control strategies use a mathematical model of the vehicle dynamics, while model-free strategies use data-driven methods, both to generate appropriate control actions. Because of the dynamic problem and complicated nature, we will focus on and employ a model-free approach, utilizing deep learning techniques explicitly to benefit from their data-driven learning abilities and flexibility when dealing with diverse situations without the requirement of set models.\nBojarski et al. [4] developed a vision-based end-to-end driving system based on deep CNNs that directly mapped raw pixel values from a single front camera to steering commands. In this way, they trained their model on lots of driving footage data sets which in turn made it possible for the model to accommodate variety of traffic conditions. Nevertheless, as far as more complex driving behaviors such as lane changes their scope was limited. Instead of using RGB images as input, Linda et al. [5] exploited optical flow, the motion of objects between frames, to dynamically perceive the environment and generate a visual potential field whereby the direction and magnitude of flow vectors affects navigation decisions of the vehicle. By including optical flow in decision making, the system capability to navigate complicated driving scenarios was enhanced.\nOther approaches focused on including time series instead of conventional CNNs. For instance, Eraqi et al. [9] aimed at improving the steering angle control in a car through using Long Short Term Memory (LSTM). The method was instrumental in enabling the system to have an enhanced knowledge of time context in driving conditions, making steering predictions to be more precise and stable over time. Similarly, Xu et al. [29] created a model for driving by using fully convolutional network with LSTM for predicting steering angle. The authors included semantic segmentation as an auxiliary task to improve the model perception of the road conditions and consequently better predict steering angles. Additionally, Lechner et al. [15] introduced Neural Circuit Policies (NCPs), a brain-inspired neural computation method that provides interpretable and accurate decision-making maps from high-dimensional inputs. LSTM and NCP provide robust steering estimation results, particularly under varying road illumination conditions,"}, {"title": "3. Methodology", "content": "Overview of the Method. Our method introduces a novel approach to end-to-end steering estimation for autonomous vehicles by leveraging multimodal information from a single monocular camera. Unlike previous methods that rely solely on RGB data or require multiple sensors, our approach fuses RGB imagery with optical flow data extracted from the same source. This method captures both spatial and temporal dynamics of the driving scene, providing a richer representation for steering estimation. Figure 1 illustrates our approach. The method consists of four main components:\n1. Modality Extraction: From the input RGB image sequence, we extract two key modalities Depth and Optical flow, but only consider Optical Flow, as we will discuss later.\n2. Fusion Strategy: We employ an early fusion approach, concatenating RGB and optical flow data before feeding it into the neural network.\n3. Feature Extraction: A CNN or VAE processes the fused input to extract relevant features.\n4. Temporal Modeling: We integrate advanced recurrent architectures, including the innovative NCP and traditional LSTM networks, to process global temporal dynamics.\nA key innovation in our approach is the multi-scale temporal feature extraction. By using optical flow between two consecutive images, we capture local temporal features that represent short-term motion dynamics. Complementing this, our recurrent neural network architectures (NCP or LSTM) capture global temporal features across the entire sequence of inputs. This dual temporal modeling allows our system to understand both immediate frame-to-frame changes and longer-term driving patterns, a crucial advancement for robust steering estimation. This novel approach enables us to extract and utilize rich spatiotemporal information from a single camera source, a significant advancement over existing methods. By fusing RGB and optical flow data, we capture static scene geometry, local motion cues, and global temporal patterns, potentially improving the accuracy and robustness of steering estimation compared to unimodal approaches."}, {"title": "3.1. Modality Extraction", "content": "We explore the process of extracting multiple modalities from a single monocular camera to enhance steering estimation. We focus on depth maps, and optical flow, each providing unique information about the driving scene.\nDepth Map Generation. Although not used in the final model, we explored depth map generation as a potential additional modality. Two pretrained models were tested directly on our dataset and provided satisfactory depth extraction quality, eliminating the need for retraining them. These models are:\n\u2022 Monodepth2 [12]: This method uses a pre-trained ResNet as an encoder to extract high-dimensional features, enabling depth inference by considering scene context.\n\u2022 MiDaS [22]: This approach focuses on zero-shot cross-dataset transfer, aiming for robust depth estimation across varied environments without fine-tuning. By incorporating dataset mixing during training, this model learns to adapt to diverse scenes, lighting conditions, and other variations present across different datasets.\nOptical Flow Computation. Optical flow captures dynamic motion cues between consecutive frames, potentially enhancing steering estimation precision. We employ the Farneback algorithm for its accuracy and reliability in computing dense optical flow. This extraction process results in a multimodal representation of the driving scene, capturing both spatial structure and local motion dynamics."}, {"title": "3.2. Fusion Strategies", "content": "We explore different strategies to effectively combine information from multiple modalities, specifically RGB images with either depth maps or optical flow data. We investigate two main fusion approaches: Early Fusion (EF) and Hybrid Fusion (HF).\nEarly Fusion. Early fusion involves concatenating raw data from different modalities into multiple channels before feeding them into the neural network. In our case, we concatenate the RGB image (3 channels) with either the depth map (1 channel) or optical flow (2 channels), resulting in a 4-channel or 5-channel input, respectively. This combined input is then processed end-to-end by a single network.\nThe early fusion input can be represented as:\n$X_{EF} = [M_1, M_2],$   (1)\nwhere $M_1$ represents the RGB image and $M_2$ represents the additional modality (depth or optical flow).\nHybrid Fusion. Hybrid fusion represents a compromise between early and late fusion techniques. In this approach, we use separate encoders for different modalities and fuse the features at intermediate layers. This method allows the network to learn cross-modal features with distinct representations at varying depths. We adopt an approach similar to that proposed in [13], utilizing an Attention Complementary Module (ACM). The output of the feature extractor model can be described as:\n$Z_{HF} = Layer_4 + ACM(G_5(M_1)) + ACM(G_5(M_2)),$   (2)\nwhere:\n$M_1 = G_4(G_3(G_2(G_1(M_1)))),$   (3)\n$G_1,..., G_5$ represent the layers of the encoder, and ACM is the Attention Complementary Module as shown in Figure 1. The hybrid fusion approach allows for more flexibility in learning cross-modal relationships while maintaining the ability to extract modality-specific features.\nOur experiments compare the effectiveness of these fusion strategies in the context of vehicle steering estimation. The results, presented in later sections, demonstrate the impact of different fusion approaches on the overall performance of the steering estimation model."}, {"title": "3.3. Steering Estimation Architectures", "content": "Our steering estimation approach utilizes a two-stage architecture: a convolutional feature extractor followed by a recurrent neural network for temporal modeling (see Figure 1). We explore different combinations of these components to determine the most effective architecture for our task.\nConvolutional Feature Extractors. We investigate two types of convolutional feature extractors, motivated by the work presented in [3, 15], for demonstrating their efficiencies in steering estimation:\n\u2022 Convolutional Neural Network (CNN): We employ the CNN architecture as described in [15]. This network is designed to efficiently extract spatial features from the input images.\n\u2022 Variational Autoencoder (VAE): We also explore the use of a VAE encoder, as detailed in [3]. The VAE approach allows for capturing underlying data distributions, potentially enhancing the model ability to generalize."}, {"title": "4. Experimental Setup", "content": "4.1. Dataset Description and Preparation\nOur experiments utilize the dataset described in [15], which comprises RGB images captured from a vehicle-mounted camera. The dataset consists of sequences of RGB images along with corresponding steering angle labels, capturing a wide variety of road types, weather conditions, and traffic scenarios, making it particularly suitable for our task. To focus on the most relevant parts of the scene for steering estimation, we apply preprocessing steps to each image. Following the procedure outlined in [15], each image is cropped to dimensions of 78 \u00d7 200 pixels, excluding pixels corresponding to the sky and the front view of the car, and concentrating on the road and immediate surroundings. Pixel values are then normalized to ensure consistent input scaling across different lighting conditions. We employ a 10-fold cross-validation approach to ensure robust evaluation of our models. The dataset is divided into 10 folds, denoted as [F.1, F.2, ..., F.10]. In each experimental run, one fold serves as the test dataset, while the remaining nine folds are used for training and validation. Within each training set, 10% of the images are reserved for validation, with the remaining 90% used for actual training. For example, when evaluating on fold F.1, 90% of [F.2, F.3, ..., F.10] is used for training, 10% for validation, and F.1 serves as the test set. For each preprocessed RGB image, we extract additional modalities as described in the Modality Extraction section. Depth maps are generated using both MiDaS [22] and Monodepth2 [12] algorithms, while optical flow is computed between consecutive frames using the Farneback algorithm. These extracted modalities are aligned with their corresponding RGB images to create the multimodal input for our models. This comprehensive dataset preparation process ensures that our models are trained and evaluated on a diverse and representative set of driving scenarios, allowing for a thorough assessment of our multimodal fusion approach to steering estimation. The use of multiple modalities derived from a single camera source enables us to explore the potential benefits of integrating spatial and temporal information for improved steering prediction accuracy."}, {"title": "4.2. Implementation Details", "content": "Our implementation focuses on efficiently processing multimodal data for steering estimation. Key details include:\n\u2022 Hardware and Software: All experiments were conducted on NVIDIA GeForce RTX 3090 GPUs. We used Tensorflow as our deep learning framework, along with NumPy and OpenCV for data processing and optical flow computation.\n\u2022 Training Procedure: We train the models for 100 steps and employ the Adam optimizer for optimization. The batch size is fixed at 20, and we use a sequence length of 16 for the recurrent networks.\n\u2022 Loss Function Components: For VAE models, we use $\u03bb_1 = 0.15$ and $\u03bb_2 = \u03bb_1e^{-2}$ as regularization parameters for the reconstruction loss and KL divergence, respectively.\n\u2022 Evaluation Metrics: We use Mean Squared Error (MSE) and Mean Absolute Error (MAE) to evaluate"}, {"title": "5. Results & Discussion", "content": "5.1. Results\nTable 1 presents a comprehensive comparison of our proposed methods against different approaches and various modality combinations.\nBy analyzing category (I) from Table 1, which corresponds to the results obtained from the CNN-NCP model, and comparing the steering estimation using the RGB images and the depth extracted with the MiDas approach (RGBD1) with either EF or HF, we notice that HF yielded lower Mean Squared Error (MSE) and lower standard deviation (4.13; 5.68) compared to EF (5.12; 7.97). This observation could be attributed to potential inaccuracies and misalignments between depth data and RGB images, which might confuse or distort the model predictions. Therefore, we consider the HF when using Monodepth2 (RGBD2), and we obtain similar results to RGBD1. Interestingly, our experiments with depth modality (RGBD) did not yield improvements over RGB-only models. Therefore, we do not consider depth as an additional modality for the other models. However, early fusing optical flow with RGB (RGB-OF) enhances the model performance by incorporating motion-related data into the RGB color information, potentially offering valuable insights for vehicle steering estimation. This improvement is evident when comparing the fifth line to the other lines in category (I), where the average MSE and MAE are reduced by 31%. Moreover, this enhancement is consistent across each fold, indicating consistent improvement. The difference between the results obtained from adding depth and optical flow to the RGB images is likely due to the high correlation between RGB and depth information when extracted from a single camera source, as evidenced by the Structural Similarity Index (SSIM) values presented in Table 2. The SSIM between RGB and depth maps (0.3 \u00b1 0.12 for MiDaS and 0.26 \u00b1 0.44 for Monodepth2) is significantly higher than that between RGB and optical flow (0.11 \u00b1 0.036), indicating that optical flow provides more complementary information to RGB.\nEncouraged by these results, we seek to further investigate this approach by applying it to two other models, VAE-NCP and VAE-LSTM. The results presented in Table 1 (II and III) show the significant decrease in MSE and MAE. Specifically, MSE is reduced by 68%, and MAE by more than 40% compared to using solely RGB images. This enhancement extends beyond steering estimation. As highlighted in [3], VAE-NCP offers superior interpretability compared to CNN-NCP, albeit at the cost of decreased accuracy in terms of MSE (3.68; 5.68) when using only RGB images. However, with our proposed approach, both CNN-NCP and VAE-NCP methods achieve nearly identical improved MSE (1.64; 1.82), indicating that we have attained a model with enhanced interpretability while maintaining the accuracy level of improved CNN-NCP.\nThe superiority of the RGB-OF approach is further illustrated in Figure 2, which shows the variation of MSE for training and validation across 100 steps. The RGB-OF models (green and blue lines) demonstrate consistently lower error rates compared to their RGB-only counterparts (black and red lines), particularly on the validation set. This suggests that the inclusion of optical flow not only improves overall performance but also enhances the model ability to generalize to unseen data.\nIn summary, these results strongly support our hypothesis that optical flow provides valuable complementary information to RGB data for steering estimation. The significant and consistent performance improvements across various architectures and evaluation metrics demonstrate the potential of our approach to enhance the accuracy and robustness of autonomous driving systems."}, {"title": "5.2. Robustness Analysis using ALP", "content": "To further analyze the robustness of the proposed approach, we employed the Automatic Latent Perturbation (ALP) tool proposed in [3]. This tool enhances the analysis of model robustness and interpretation of latent dimensions in VAEs, particularly useful in high-dimensional spaces. Traditional methods, which manually perturb latent variables to analyze changes in outputs, suffer from scalability issues and limited effectiveness in complex models. ALP automates this by perturbing each latent dimension and generating both positive and negative reconstructions, allowing for an assessment of the specific information each dimension encodes.\nMoreover, the ALP has been extended to evaluate the effects of latent dimension perturbations on operational outputs like steering commands in autonomous vehicles. This involves generating steering predictions by perturbing the latent vectors and passing them through a secondary model component, like the NCP in our case. By comparing the steering results from different perturbations, an impact score is computed, quantifying the influence of each latent dimension on steering behavior. The impact score formula is as follows:\n$I_j = \\frac{|\\hat{y}_{i,+} - \\hat{y}_{i,0} | + |\\hat{y}_{i,-} - \\hat{y}_{i,0}| + |\\hat{y}_{i,+} - \\hat{y}_{i,-}|}{3}$   (7)\nwhere $I$ is the impact score, $\\hat{y}_{i,+}$ and $\\hat{y}_{i,-}$ are the steering predictions for the perturbed latent vectors increased and decreased by a perturbation value of $\\sigma = 0.3$ along the j-th dimension, respectively, for both RGB and RGB-OF setups, and $\\hat{y}_{i,0}$ is the steering prediction for the unperturbed latent vector.\nWe conduct experiments on fold F.2, utilizing the complete fold to ensure a comprehensive analysis. Figure 3 depicts how the incorporation of optical flow with RGB images affects the model MSE under both perturbed and unperturbed conditions. This figure provides insights into the variability and stability of the model predictions across different perturbed dimensions. Figure 3 confirms, as previously mentioned, that integrating optical flow with RGB images enhances model predictions for steering control in unperturbed conditions, compared to using only RGB images. Furthermore, when analyzing the latent space perturbations, it is evident from Figure 3 that the model using both RGB and optical flow manifests significantly less variation in MSE across perturbed dimensions compared to the RGB-only model. This reduced variation in response to perturbations within the latent space highlights the model robustness; it shows more resilience to internal variability, which is crucial for dependable decision-making in dynamic environments. On the other hand, the higher variability across dimensions in the model based only on RGB images (see right plot in Figure 3) indicates less stable and reliable predictive behavior when the latent space is disturbed.\nFurthermore, Figure 4 presents the distribution of impact scores for the top 10% of steering predictions that exhibit the highest error, providing a direct measure of the model sensitivity to deviations in input data. We note a consistently higher impact score across the 32 perturbed dimensions for RGB-OF (left plot) in comparison to utilizing only RGB (right plot). This indicates that the RGB-OF model is potentially more adept at identifying when inputs or situations deviate from the norm, which could lead to better handling of Out-Of-Distribution (OOD) scenarios. The RGB model, with lower impact scores, might be less sensitive to these perturbations, potentially making it less effective at OOD detection. Consequently, the RGB-OF model can be considered more robust in this specific aspect of uncertainty identification and OOD situation handling. This increased sensitivity can be an asset in applications like autonomous driving systems, where recognizing OOD situations that differ from the training data is crucial for safety and performance."}, {"title": "6. Conclusion", "content": "This study introduces a novel approach to enhance end-to-end steering estimation for autonomous vehicles by leveraging multimodal information extracted from a single monocular camera. Our primary contribution lies in demonstrating the significant impact of incorporating optical flow information alongside RGB imagery. The fusion of RGB and optical flow data consistently outperforms state-of-the-art methods and other modality combinations, achieving a remarkable 31% reduction in steering estimation error. This improvement is consistent across various neural network architectures, including CNN-NCP, VAE-NCP, and VAE-LSTM, highlighting the robustness and versatility of our approach. Moreover, the integration of optical flow enhances the model's ability to generalize to unseen data, as evidenced by lower error rates on validation sets.\nThese findings have important implications for the development of autonomous driving systems. By demonstrating that rich, multimodal information can be extracted and effectively utilized from a single camera source, our approach offers a cost-effective and computationally efficient solution for improving steering estimation accuracy. Future work could explore the integration of our approach with other sensor modalities and its application to other autonomous driving tasks. In conclusion, our research underscores the importance of leveraging complementary information sources in autonomous driving systems, potentially enhancing the safety and reliability of self-driving vehicles."}]}