{"title": "A Method for Detecting Legal Article Competition for Korean Criminal Law Using a Case-augmented Mention Graph", "authors": ["Seonho An", "Young Yik Rhim", "Min-Soo Kim"], "abstract": "As social systems become increasingly complex, legal articles are also growing more intricate, making it progressively harder for humans to identify any potential competitions among them, particularly when drafting new laws or applying existing laws. Despite this challenge, no method for detecting such competitions has been proposed so far. In this paper, we propose a new legal AI task called Legal Article Competition Detection (LACD), which aims to identify competing articles within a given law. Our novel retrieval method, CAM-Re2, outperforms existing relevant methods, reducing false positives by 20.8% and false negatives by 8.3%, while achieving a 98.2% improvement in precision@5, for the LACD task. We release our codes at https://github.com/asmath472/LACD-public.", "sections": [{"title": "1 Introduction", "content": "In many countries, courts judge legal cases based on a law of their country, and thus, many lawyers utilize legal articles (also known as codes, or provisions) in their works. In the legal AI field, several works that utilize legal articles have been suggested to solve legal AI tasks, such as Legal Judgment Prediction (Feng et al., 2022a,b; Deng et al., 2023; Liu et al., 2023), Legal Article Retrieval (Louis and Spanakis, 2022; Paul et al., 2022; Louis et al., 2023), and Legal Question Answering (Holzenberger et al., 2020; Louis et al., 2024).\nDespite the crucial role of legal articles, some of them compete with each other (Yoon, 2005; Kim, 2005; Araszkiewicz et al., 2021). Here, competition in legal articles refers to instances where overlapping directives or conflicting interpretations arise. For example, in Figure 1, Article 60 of Narcotics Control Act and Article 201 of Criminal Act define different punishments for the same crime, using opium or morphine, and therefore compete with each other.\nIf two articles compete with each other in a"}, {"title": "2 Preliminaries", "content": "In this section, we define the terms including case, rule, article, competition, and mention, largely based on the definitions provided by Araszkiewicz et al. (Araszkiewicz et al., 2021). Their notations are summarized in Table 1. Hereafter, we will use a legal article and article interchangeably."}, {"title": "2.1 Definitions", "content": "Definition 1 (Case, Rule, and Article). A case c is a sentence describing the facts of an event (Shao et al., 2020; Sun et al., 2023). A proposition (denoted as x) for a case represents an implicit question about its facts. A rule r is the implicit unit of laws, consisting of a set of propositions (denoted as X) and a judgment p for cases C. A rule r judges a case $c \\in C$ as p if and only if all propositions in X hold true in c. We denote a rule with X and p as r = rule(p, X). An article a is an explicit unit of laws that implicitly contains one or more rules, denoted as $r \\subseteq a$, and is expressed in sentences.\nEach article is explicitly included in one act.\nExample 1. We can represent Criminal Act 205 (Article 205 in Criminal Act) in Figure 1 and its example case $c_1$ as follows:\n$c_1$ = Bob smoked opium in his house.\n$a_1$ = Criminal Act Article 205 (Possession of Opium, ... million won.\n$X_1 = \\left\\{\\begin{array}{c} \\text{Is a person possesses something?} \\\\ \\text{Is something} \\in \\text{opium} \\lor \\text{morphine} ... \\\\ \\end{array} \\right\\}$\n$p_1$ = Less than five million won fine\n$V$ Less than one year imprisonment.\n$r_1$ = rule($p_1, X_1$), $r_1 \\subseteq a_1$\nSince Bob smoked (a proposition about possession in $X_1$) opium (a proposition regarding opium $V$ morphine $V$ in $X_1$), all propositions in $X_1$ hold in $c_1$, and thus, case $c_1$ is judged as $p_1$.\nDefinition 2 (Competition).\n1. Two rules compete, i.e., compete (rule($p_1$, $X_1$), rule($p_2$, $X_2$)) if and only if $p_1 \\neq p_2$, and $X_1$ includes $X_2$, or vice versa.\n2. If two rules compete, then the articles containing those rules also compete. Specifically, compete($a_1$, $a_2$) if compete($r_i$, $r_j$), $r_i \\subseteq a_1$, and $r_j \\subseteq a_2$."}, {"title": "2.2 Retrieve-then-rerank methods", "content": "Given a query article $a_q$, retrieve-then-rerank methods (Nogueira and Cho, 2019; Wu et al., 2020; Glass et al., 2022; Zhu et al., 2023; Song et al., 2024) retrieve a set of articles $A_{ret}$ through the following three steps.\nStep 1. $v_{aq}$ = enc-bi($a_q$), $v_a$ = enc-bi($a$)($a \\in A$)\nStep 2. $A_{topk}$ = {$a$ | top-k by sim($v_{aq}, V_a$)}\nStep 3. $A_{ret}$ = {$a_i$ | p($a_q$, $a_i$) >0} ($a_i \\in A_{topk}$)\nwhere p($a_q$, $a_i$) = probcalc(enc-cross($a_q \\oplus a_i$))\nHere, $v_a$ is the vector representation of article a; sim presents a similarity function, such as cosine; probcalc refers to a layer for calculating retrieval probability; $\\oplus$ denotes a textual concatenation operator. We denote the retrieval probability of article $a_i$ for the query $a_q$ as p($a_q$, $a_i$).\nIn Step 1, each article $a \\in A$ is pre-encoded into vector representations $v_a$ using a bi-encoder. The bi-encoder also encodes the query article $a_q$ into a vector representation $v_{aq}$. In Step 2, the retriever selects the top-k articles $A_{topk}$ based on the similarity function sim($v_{aq}$, $v_a$). In Step 3, the cross-encoder processes each article $a_i \\in A_{topk}$ together with $a_q$, and inputs it into the probcalc layer to compute the retrieval probability p($a_q$, $a_i$). The retriever returns the set of articles $A_{ret}$, consisting of those articles $a_i$ for which p($a_q$, $a_i$) exceeds a threshold $\\theta$."}, {"title": "3 Methodology", "content": "We define Legal Article Competition Detection (LACD) as a retrieval task that takes a query article $a_q$ as input, and retrieve a set of competing articles, {$a_i$|compete($a_q$,$a_i$)}, as output. This definition is particularly useful for identifying articles that compete with one currently being drafted. A na\u00efve method to identify all competing articles within a law involves performing the LACD task for each article in the law.\nWe will refer to the retrieve-then-rerank method described in Section 2.2 as the na\u00efve Re2 retriever. Unlike traditional retrieval tasks such as opendomain QA (Karpukhin et al., 2020), which primarily focus on identifying related documents, the LACD task, as discussed in Section 1, presents unique challenges. Consequently, the na\u00efve Re2 retriever often retrieves textually similar but semantically irrelevant and non-competing articles.\nFor example, Figure 3(a) shows the challenges faced by the na\u00efve Re2 retriever in the LACD task. We detail the issues encountered at each retrieval step for the query, Act on the Protection Of Children and Youth Against Sex Offenses Article 11-2. At Step 1, articles that are textually similar but semantically unrelated to $a_q$ (e.g., Korea Minting and Security Printing Corporation Act Article 19) are mapped to similar vectors. Conversely, articles that are textually different but semantically related to $a_q$ (e.g., Criminal Act Article 283) are mapped to distant vectors. As a result, at Step 2, the retriever selects almost irrelevant articles as the top-k candidates $A_{topk}$ for the query article, which illustrates Challenge 1. At Step 3, the retriever attempts to calculate the probabilities that $a_q$ competes with each $a_i \\in A_{topk}$. This step requires a precise understanding of the concepts outlined in each article (e.g., 'Article 11, paragraph 1, items 1 and 2') and the ability to reason about implicit inclusion relationships between the rules within the articles. However, the retriever often fails to accurately interpret these concepts, particularly when they depend on definitions provided in other articles. Consequently, the retriever fails to filter out incorrect results, such as Korea Minting and Security Printing Corporation Act Article 19, thus exhibiting Challenge 2."}, {"title": "3.1 The LACD task", "content": "We define Legal Article Competition Detection (LACD) as a retrieval task that takes a query article $a_q$ as input, and retrieve a set of competing articles, {ai|compete($a_q$,$a_i$)}, as output. This definition is particularly useful for identifying articles that compete with one currently being drafted. A na\u00efve method to identify all competing articles within a law involves performing the LACD task for each article in the law.\nWe will refer to the retrieve-then-rerank method described in Section 2.2 as the na\u00efve Re2 retriever. Unlike traditional retrieval tasks such as opendomain QA (Karpukhin et al., 2020), which primarily focus on identifying related documents, the LACD task, as discussed in Section 1, presents unique challenges. Consequently, the na\u00efve Re2 retriever often retrieves textually similar but semantically irrelevant and non-competing articles.\nFor example, Figure 3(a) shows the challenges faced by the na\u00efve Re2 retriever in the LACD task. We detail the issues encountered at each retrieval step for the query, Act on the Protection Of Children and Youth Against Sex Offenses Article 11-2. At Step 1, articles that are textually similar but semantically unrelated to aq (e.g., Korea Minting and Security Printing Corporation Act Article 19) are mapped to similar vectors. Conversely, articles that are textually different but semantically related to aq (e.g., Criminal Act Article 283) are mapped to distant vectors. As a result, at Step 2, the retriever selects almost irrelevant articles as the top-k candidates Atopk for the query article, which illustrates Challenge 1. At Step 3, the retriever attempts to calculate the probabilities that aq competes with each ai \u2208 Atopk. This step requires a precise understanding of the concepts outlined in each article (e.g., 'Article 11, paragraph 1, items 1 and 2') and the ability to reason about implicit inclusion relationships between the rules within the articles. However, the retriever often fails to accurately interpret these concepts, particularly when they depend on definitions provided in other articles. Consequently, the retriever fails to filter out incorrect results, such as Korea Minting and Security Printing Corporation Act Article 19, thus exhibiting Challenge 2."}, {"title": "3.2 CAMGraph", "content": "CAMGraph G is composed of nodes N and edges E. Detailed explanations of these components are"}, {"title": "3.3 CAM-Re2 retriever", "content": "The proposed CAM-Re2 retriever operates as follows: (1) it encodes nodes of CAMGraph instead of articles in Step 1, (2) selects the top-k nodes in Step 2, and (3) applies GNNs on CAMGraph in conjunction with the cross encoder in Step 3. Thus, Steps 1 and 3 are quite different from those of the na\u00efve Re2 retriever. Figure 4 shows these steps. Formally, CAM-Re2 is described as follows:\nStep 1. $V_{aq}$ = enc-bi($a_q \\oplus LM_{case}(a_q)$)\n$v_a$ = enc-bi(a+c) ((a, c) $\\in$N)\nStep 2. $A_{topk}$ = {$a$ | top k by sim($V_{aq}$, $V_a$)}\nStep 3. $A_{ret}$ = {$a_i$|ProbCal(enc-cross($a_q \\oplus a_i$);\nGNN($v_{aq}$, G, D); GNN($v_a$, G, D)) >$\\theta$}($a_i \\in A_{topk}$)"}, {"title": "4 Experimental settings", "content": "To build the LACD dataset for Korean Law, we collected 2,339 pairs of articles ($a_1$, $a_2$) and manually"}, {"title": "4.1 The LACD dataset", "content": "To build the LACD dataset for Korean Law, we collected 2,339 pairs of articles (a1, a2) and manually"}, {"title": "4.2 Implementation of retrievers", "content": "We employ KoBigBird (Park and Kim, 2021) model as a bi-encoder; and KoBigBird, Qwen2.0 (Yang et al., 2024), Llama 3.2 (Dubey et al., 2024b) as cross encoders. For CAM-Re2, we utilize gpt-40-mini (OpenAI) with 0.7 temperature as a case generator, and primarily employ a two-layer GATV2 (Brody et al., 2022) as the GNN architecture. We compare other GNN architectures in Section 5.4. For the vector database, we utilize Chroma DB\u00b9."}, {"title": "4.3 Training and testing", "content": "We build na\u00efve Re2 and CAM-Re2 retrievers by finetuning the bi-encoder first and the cross encoder"}, {"title": "5 Results and analysis", "content": "Since Na\u00efve Re2 and CAM-Re2 differ at Steps 1 and 3, there are four possible combinations: (1) Na\u00efve Re2 at all Steps (Na\u00efve Re2), (2) Na\u00efve Re2 at Step 1 + CAM-Re2 at Step 3 (N1+C3), (3) CAMRe2 at Step 1 + Na\u00efve Re2 at Step 3 (C1+N3), and (4) CAM-Re2 at all Steps (CAM-Re2). Table 5 shows their performance results. CAM-Re2 achieved F1 score improvements of 9.6%p, 6.6%p and 3.7%p with KoBigBird, Qwen2.0 and Llama 3.2, respectively, compared to Na\u00efve Re2. Since"}, {"title": "5.1 Experiment for Step 3", "content": "Since Na\u00efve Re2 and CAM-Re2 differ at Steps 1 and 3, there are four possible combinations: (1) Na\u00efve Re2 at all Steps (Na\u00efve Re2), (2) Na\u00efve Re2 at Step 1 + CAM-Re2 at Step 3 (N1+C3), (3) CAMRe2 at Step 1 + Na\u00efve Re2 at Step 3 (C1+N3), and (4) CAM-Re2 at all Steps (CAM-Re2). Table 5 shows their performance results. CAM-Re2 achieved F1 score improvements of 9.6%p, 6.6%p and 3.7%p with KoBigBird, Qwen2.0 and Llama 3.2, respectively, compared to Na\u00efve Re2. Since"}, {"title": "5.2 Experiment for all Steps", "content": "Figure 5 shows the performance of the four combinations for the entire process. Specifically, Figures 5(a) and (b) show the results when selecting the top-1 and top-5 articles, respectively. In Figure 5(a), CAM-Re2 reduces false negatives (FN) by 7.7%, false positives (FP) by 29.6%, while in Figure 5(b), it reduces FP by 17.28%. In addition, CAM-Re2 achieves a 103% increase in precision@1 and a 98.2% increase in precision@5 compared to Na\u00efve Re2.\nThis reduction in FP and FN of CAM-Re2 primarily stems from the improved bi-encoder performance using node encoding in Step 1. In contrast, the bi-encoder performance of Na\u00efve Re2 is relatively poor, leading to more FPs and misses TPs in Step 3. When comparing Na\u00efve Re2 (in red) with N1+C3 (in orange), N1+C3 does not outperform Na\u00efve Re2 despite utilizing GNN, due to poor encoding. This observation aligns with the analysis in Section 5.1. C1+N3 (in blue) more frequently achieves better performance than Na\u00efve Re2, particularly in precision@5. However, it shows worse"}, {"title": "5.3 Ablation study 1: Step 3 w/o cross encoder", "content": "In this section, we evalute the effectiveness of using GNNs at Step 3. Table 6 shows the performance of Step 3 without the cross encoder, allowing for a straightforward assessment of the GNN layers' impact. When using the bi-encoder of Na\u00efve Re2 at Step 1, using GNN at Step 3 improves the F1 score by 14.4%p (36.1% to 50.5%). When utilizing the bi-encoder of CAM-Re2 at Step 1, the use of GNN at Step 3 yields an even greater improvement in the F1 score by 17.5%p (46.7% to 64.2%)."}, {"title": "5.4 Ablation study 2: GNN architectures", "content": "We evaluate two alternative GNN architectures for CAM-Re2: GCN (Kipf and Welling, 2017) and GraphSAGE (Hamilton et al., 2017). Table 7 shows the performance results. Both GCN and GraphSAGE show significant improvements over Na\u00efve"}, {"title": "6 Related works", "content": "Legal article retrieval focuses on finding legal articles that are relevant to a specific query. This task has been widely studied, with notable approaches proposed in research such as (Louis and Spanakis, 2022; Paul et al., 2022; Louis et al., 2023). The retrieved legal articles also serve as key components for solving other Legal AI tasks like Legal Question Answering (QA) (Louis et al., 2024) and legal judgment prediction (Qin et al., 2024).\nTo improve the performance of legal article retrieval, some studies have used graph-based methods, connecting legal articles or articles to legal cases. GNNs are often used in these approaches (Paul et al., 2022; Louis et al., 2023). However, these methods may not work well for LACD due to two unique challenges outlined in Section 1."}, {"title": "6.1 Legal article retrieval", "content": "Legal article retrieval focuses on finding legal articles that are relevant to a specific query. This task has been widely studied, with notable approaches proposed in research such as (Louis and Spanakis, 2022; Paul et al., 2022; Louis et al., 2023). The retrieved legal articles also serve as key components for solving other Legal AI tasks like Legal Question Answering (QA) (Louis et al., 2024) and legal judgment prediction (Qin et al., 2024).\nTo improve the performance of legal article retrieval, some studies have used graph-based methods, connecting legal articles or articles to legal cases. GNNs are often used in these approaches (Paul et al., 2022; Louis et al., 2023). However, these methods may not work well for LACD due to two unique challenges outlined in Section 1."}, {"title": "6.2 LLM generated outputs for legal AI", "content": "LLMs have demonstrated remarkable versatility across a wide range of NLP tasks, leveraging their massive parameterized knowledge (Brown et al., 2020; OpenAI, 2023; Dubey et al., 2024a). As a result, many studies have explored using LLMgenerated outputs, either as training data (Wang et al., 2023) or for making predictions directly during inference (Mao et al., 2021; Trivedi et al., 2023; Jiang et al., 2023; Lee et al., 2024).\nIn the field of Legal AI, some recent studies have shown that incorporating LLM-generated components can improve some Legal AI tasks performance (Kim et al., 2024; Ma et al., 2024). However, to the best of our knowledge, no prior work has proposed solving LACD by LLM-generated cases."}, {"title": "7 Conclusions", "content": "In this paper, we proposed a new legal AI task, Legal Article Competition Detection (LACD), and construct a dedicate dataset for it. We propose a novel retriever, CAM-Re2, based on the CaseAugmented Mention Graph (CAMGraph) for Korean Law. We demonstrated that CAM-Re2 is significantly more effective than existing retrievers for the LACD task."}, {"title": "8 Limitations", "content": "In this paper, we propose CAMGraph as a solution to address the problem of legal competition. However, our approach has several limitations:\nFirst, our methodology has only been validated within the domain of criminal law in Korea. Korean criminal law is one of the most extensively studied areas related to legal competition, and it provides a convenient basis for dataset creation. However, it is necessary to expand this research to other domains, such as civil, building or administrative law, to address legal competition comprehensively in the future.\nSecond, CAMGraph only incorporates mention relationships between articles as edges. For example, methods like G-DSR (Louis et al., 2023) utilize tree structures within laws as links, which our approach does not include. Whether incorporating such tree structures could effectively solve the LACD problem remains out of scope for this work and requires future investigation.\nLastly, our dataset only contains pairwise competition information, lacking data on competition across a full set of articles. To build a more accurate LACD pipeline, developing an LACD benchmark with comprehensive competition information across the entire article set is essential."}, {"title": "9 Ethical considerations", "content": "Language models have inherent issues with hallucination and the potential to generate biased outputs. In particular, when generating cases for CAMGraph, there is a risk of disproportionately generating cases involving individuals from certain demographic groups, which could lead to harmful biases. For instance, CAM-Re2, which utilizes these generated cases, may exhibit a tendency to more effectively retrieve articles (e.g., those related to violence) in lawsuits associated with certain demographic groups. To avoid this, only anonymized cases were used as prompts during case generation. Nevertheless, when developing real-world applications based on our methodology, it is essential to carefully examine the generated cases to identify and mitigate potential biases.\nThe mention relationships in law was obtained by crawling data from the official website of the Ministry of Government Legislation. According to Article 7 of the Copyright Act in Korea, legal provisions and compilations of laws created by the government (including link information) are not"}, {"title": "A.1 Competitions in the real world", "content": "In Korea, a legal article is included in Acts if and only if the article is enacted by national assembly of Korea. Otherwise, it is classified differently (e.g., enforcement degree, enforcement rule). There exists a hierarchy among Acts, enforcement decrees, and enforcement rules, with Acts being the most authoritative. In Korea, if two legal articles of differing hierarchy compete, the lower article must be ignored. In this study, we exclusively focus on articles within Acts, and CAMGraph contains 79,615 articles that meet this criterion."}, {"title": "A.1.1 Hierarchy of laws", "content": "In Korea, a legal article is included in Acts if and only if the article is enacted by national assembly of Korea. Otherwise, it is classified differently (e.g., enforcement degree, enforcement rule). There exists a hierarchy among Acts, enforcement decrees, and enforcement rules, with Acts being the most authoritative. In Korea, if two legal articles of differing hierarchy compete, the lower article must be ignored. In this study, we exclusively focus on articles within Acts, and CAMGraph contains 79,615 articles that meet this criterion."}, {"title": "A.1.2 Solving competitions in Korea", "content": "In Korea, if articles $a_1$ and $a_2$ compete with each other, and able to judge some case c, one of them is invalidated (i.e., ignored in the judgment). There"}, {"title": "A.2 Detailed parameters for experiments", "content": "Table 8 shows the settings of our experiments. The reason why the batch size is 16 while the batch size per device is 4, even though we are using an eight GPU machine, is that we are only using 4 GPUs for each experiment."}, {"title": "A.1.2 Solving competitions in Korea", "content": "In Korea, if articles $a_1$ and $a_2$ compete with each other, and able to judge some case c, one of them is invalidated (i.e., ignored in the judgment). There are two principles to solve competitions as follows2:\n1. A new law overrides an old law (lex posterior derogat priori)\n2. A specific law overrides a general law (lex specialis derogat leges generales)\nWe explain each principle in Example A.1 and Example A.2, respectively.\nExample A.1. Criminal Act 201 and Narcotics Act 60. As we explained in Section 1, Criminal Act 201 and Narcotics Act 60 compete with each other, and thus a crime of using opium is judged by both articles. In terms of time, Criminal Act 201 is relatively old (enacted in 1953) than Narcotics Act 60 (enacted in 2000). Thus, according to principle (1), Narcotics Act 60 overrides Criminal Act 201 (i.e., Criminal Act 201 is ignored in this case).\nExample A.2. Criminal Act 201 and Criminal Act 205. For the case $c_1$ in Example 1, Section 2.1, we can apply not only Criminal Act 205, but also Criminal Act 201 because bob smoked (the same as used) opium in his house. Therefore, Criminal Act 201 and 205 are compete with each other and able to judge $c_1$. From the descriptions of each article, Criminal Act 205 judges more general cases than Criminal Act 201 (details are in Example 2, Section 2). Thus, according to principle (2), Criminal Act 201 overrides Criminal Act 201 (i.e., Criminal Act 201 is ignored in this case)."}, {"title": "A.3 Real cases for CAMGraph", "content": "In this section, we compare two strategies for constructing the nodes in CAMGraph: (1) mixing associated real legal cases with generated cases, and (2) using only generated cases. To achieve this, we collected cases from one of the well-known Korean Legal Benchmark dataset, LBox open (Hwang et al., 2022), and mapped them to corresponding articles. In total, we collect associated cases for 515 articles."}, {"title": "A.3 Real cases for CAMGraph", "content": "In this section, we compare two strategies for constructing the nodes in CAMGraph: (1) mixing associated real legal cases with generated cases, and (2) using only generated cases. To achieve this, we collected cases from one of the well-known Korean Legal Benchmark dataset, LBox open (Hwang et al., 2022), and mapped them to corresponding articles. In total, we collect associated cases for 515 articles."}, {"title": "A.4 Multiple cases for enc-bi", "content": "In this section, we compare the effects of employing multiple case augmentations during the training of enc-bi by constructing three different encoders: (1) enc-bi in Na\u00efve Re2 (N1), (2) enc-bi in CAMRe2 with training single generated case (single C1), and (3) enc-bi in CAM-Re2 with training three different generated cases (multi C1). To ensure consistency in training computations across all models,"}, {"title": "A.5 Acts about crimes", "content": "The term acts about crimes that we used in Section 4.1, contains following acts. These are selected based on the Korean Bar Exam guidelines\u00b3:\n\u2022 Criminal Act\n\u2022 Act on Special Cases Concerning the Punishment of Sexual Crimes\n\u2022 Act on the Aggravated Punishment of Specific Economic Crimes\n\u2022 Act on the Aggravated Punishment of Specific Crimes\n\u2022 Punishment of Violences Act\n\u2022 Act on the Protection of Children and Youth Against Sex Offenses"}]}