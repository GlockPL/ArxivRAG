{"title": "Reimagining Personal Data: Unlocking the Potential of Al-Generated Images in Personal Data Meaning-Making", "authors": ["Soobin Park", "Hankyung Kim", "Youn-kyung Lim"], "abstract": "Image-generative AI provides new opportunities to transform personal data into alternative visual forms. In this paper, we illustrate the potential of AI-generated images in facilitating meaningful engagement with personal data. In a formative autobiographical design study, we explored the design and use of AI-generated images derived from personal data. Informed by this study, we designed a web-based application as a probe that represents personal data through generative images utilizing Open AI's GPT-4 model and DALL-E 3. We then conducted a 21-day diary study and interviews using the probe with 16 participants to investigate users' in-depth experiences with images generated by AI in everyday lives. Our findings reveal new qualities of experiences in users' engagement with data, highlighting how participants constructed personal meaning from their data through imagination and speculation on Al-generated images. We conclude by discussing the potential and concerns of leveraging image-generative AI for personal data meaning-making.", "sections": [{"title": "1 Introduction", "content": "With the advances in data-driven digital technologies, people can collect various types of data about themselves, i.e., personal data in their everyday lives, in both explicit and implicit ways. In the Human-Computer Interaction (HCI) research community, the importance of meaning-making from such personal data has been highlighted [24, 39, 69, 84, 119], alongside ongoing attempts on more creative and alternative ways of experiencing personal data [30, 32, 41, 52, 66, 73, 74] to facilitate meaning-making and nuanced experiences with the data [31, 136]. For example, data physicalization [64, 125], which transforms personal data into tangible forms, has been shown to facilitate self-reflection on personal context, values, and attitudes in the physicalization process, going beyond the simple data reading [125].\nBuilding on this trend, we notice that recently emerging generative Al shows a strong potential for representing personal data in novel mediums. Prior works have already begun to demonstrate the value of generative models in supporting people to reflect and make their own meaning to personal data [14, 111, 112]. For example, Rajcic and McCormack [111] illustrate that emotion data extracted via facial expression recognition can be turned into poetry to provoke reflection. Another example is ContextCam suggested by Fan et al. [46], which generates personalized images based on contextual data such as location, weather, facial expression, or music. These prior studies all point to the potential of generative models to facilitate meaning-making by transforming data into new forms.\nExtending this line of research, and given the growing prevalence of image-generative tools such as DALL-E [102], Midjourney [85], and Stable Diffusion [34], we aim to explore new possibilities of how image-generative AI can be leveraged to design for personal data meaning-making. Drawing from the seminal work [53] that proposes ambiguity as a resource for design, we are intrigued by the uncertainty and opaqueness of image generation models as valuable design resources for supporting users' personal meaning-making. We are also intrigued by the 'image-generative' AI's ability to transform personal data into an alternative visual form that is more artistic, subjective, and abstract-a form that encourages contemplation and interpretation drawing on past experiences and imagination [27, 45, 62].\nUpon this research goal, we first conducted a formative study based on an autobiographical design approach [29, 89] to develop a designerly understanding of how an image-generative model (DALL-E 3) can become a material for reflective design, as well as what should be considered when the model generates images using personal data as input. Based on these findings, we designed a technology probe [63] that allows users to experience image generation using personal data. Using the probe, we conducted a 21-day diary study and interviews with 16 participants, who collected personal data for various purposes and experienced AI-generated images created based on their data. Our analysis of the study results revealed four themes on the quality of experiences with data that were facilitated by AI-generated images: participants 1) discovered emotional layers within the data, 2) (re)conceptualized and (re)interpreted their sense of self, 3) sought to craft personal"}, {"title": "2 Related Work", "content": "Representing data in a new, alternative form is the most well-known strategy for supporting the sensemaking of personal data [20, 64]. In the context of personal informatics [75, 137], previous works have offered insights into how the visualization of users' tracking data can facilitate understanding and making sense of their data [3, 20, 21, 61, 83]. Epstein et al. [42] investigated various visualization formats for location and activity data aiming to support data sensemaking, such as tables, graphs, captions, and other forms.\nBeyond such traditional visualization methods, researchers have also suggested alternative approaches using visual metaphors or abstract forms for data representation [23, 88, 110]. A notable example is casual information visualization [110], which is known to generate reflective insights through representing data in artistic forms. In the field of personal informatics, there have been attempts to encode personal data in qualitative and subjective ways through metaphors and abstract imagery. For example, Ayobi et al. [4] explored how people represent data through personally meaningful ways and promote self-reflection through paper bullet journaling. Kim et al. [71] developed a system DataSelfie that allows users to represent personal data in customized visual forms, motivated by the Dear Data Project [81] in which Lupi and Posavec created and shared personalized visualizations of data through hand-drawn postcards. Also, Murnane et al. [88] translated physical activities and goals into multi-chapter narratives, thereby encouraging users to engage with their fitness goals by fostering empathy and enhancing their self-reflection. Another body of works also explored the transformation of personal data into tangible forms, known as data physicalization, that enables individuals to engage in a meaning-making process of their data [49, 64, 69, 125].\nCollectively, these works have shown how an unusual and ambiguous representation of data can foster new, context-rich reflection experiences and data meaning-making [67, 130], diverging from the quality of reflection that conventional statistical representations enable. Building on this, we aim to explore how images generated by AI can facilitate new types of data experience. Abstract images are a typical form of visual representation that invites people to engage in reflection [27, 45, 62], and prior research has also explored the design space of reflective engagement with artworks [55]. In previous HCI studies, ambiguous representation of data have been shown to provide individuals new perspectives, serving as a resource that facilitates reflective thinking [11, 35, 77, 86, 92, 126]. For example, Mols et al. [86] demonstrated that ambiguous data representation can serve as a strategy for enabling reflection, as illustrated by concepts such as DataZen, which creates sand patterns from activity, stress, and wellbeing data, and Life Tree, an interactive art piece representing patterns of activity, social, and health data, situating this approach within the design space for everyday life reflection. Trujillo-Pisanty et al. [126] created a new representation of online presence by extracting and amalgamating faces from Facebook photos using algorithms, thereby enabling reflection on personal and family representations. With the recent advances in image-generative AI technologies, these possibilities have expanded, making it possible to represent personal data in ambiguous forms [111, 112] with the design of tailored prompts, as their potential to serve as a design material has been highlighted [10, 118, 139]. Despite this potential, research on how Al-generated images using personal data can facilitate personal data meaning-making remains scarce, which has motivated our current work."}, {"title": "2.2 AI as a Material for Reflection and Meaning-Making", "content": "With the recent advance in generative Al technology, a growing body of research has begun investigating how AI-generated media can be leveraged to facilitate people's reflections on data. For instance, along with their conceptualization of Introspective AI, Brand et al. [14] have speculated a concept called Dream Streams that represents people's dreams in images based on sleep monitoring data and audio journaling. Also, Fan et al. [46] have suggested ContextCam, which generates images with the themes extracted from contextual data. Quologue, proposed by Kang and Odom [68], creates outputs that synthesize e-book highlights with users' reflections using generative AI. Cho et al. [18] have designed ARECA, an IoT-based air purifier that turns the data collected from the surrounding environment into diary entries using generative AI. Rajcic and McCormack [111] investigated how machine-generated poetry based on a user's emotions extracted from facial expressions can provoke reflection.\nCollectively, these prior studies illustrate the potential of generative Al to transform data into new forms, creating reflective materials that can provoke new meaning-making around the data. Our work extends this corpus of studies, and in particular, we build on Brand et al.'s work [14] that have demonstrated that AI can be utilized as a resource for supporting introspective experiences. Our paper focuses specifically on the 'image-generative' aspect of AI, delving deeper into what kinds of attributes of image-generative AI should be handled in order to facilitate a new quality of meaning-making from data. Also, building on Wan et al.'s work [129] on developing co-creative narration systems that transform dreams into visual metaphors, our work also investigates how various types of personal data, including qualitative and abstract forms, can be represented as images and what kinds of experiences can arise from such visual representations."}, {"title": "3 Study Methods", "content": "Our study aimed to explore how AI-generated images based on personal data can create new ways of engaging with data. We"}, {"title": "3.1 Formative Study: Autobiographical Design Approach", "content": "The aim of the formative study was to identify key requirements for prompts that could generate images facilitating meaningful experiences of personal data, which in turn informed the design of a technology probe for the main study. We employed an autobiographical design approach, defined as \u201cdesign research drawing on extensive, genuine usage by those creating or building the system\u201d [89]. This approach allowed us to experiment with the new design material [89] at a more visceral and nuanced level, deepening our understanding of how different prompt designs influenced the resulting images. We also aimed to first identify potential risks ourselves, such as privacy concerns or exposure to inappropriate content while using the generative AI, to preempt them in the experience of the main study probe so that participants could engage with the probe in a more secure and comfortable manner."}, {"title": "3.1.1 Formative Study Process", "content": "The autobiographical design work was led by the first author, consisting of two steps: (1) a 14-day experimentation with an image generation model and (2) a 12-day creation of images. We chose to use DALL-E 3 embedded in ChatGPT, considering that it is one of the most widely accessible and user-friendly image generation models known for its high performance [101]. Notably, since the image generation model is integrated within ChatGPT [100], it provided a rapid image generation process. We deemed the periods for each phase appropriate, considering the need to maintain a fresh perspective and avoid the potential staleness that could arise from a longer study duration.\nThe first step was to identify a basic prompt structure for generating images from personal data. During this period, the first author freely tested various prompts using different keywords and modifiers [107], eventually designing a proper prompt for this purpose. The second step was to identify the factors that could enable meaningful reflections on personal data. The first author collected her personal data daily and used the previously identified prompt structure to generate a variety of images. For the first six days, she focused on generating images using diverse types of trackable data from daily life, such as physical activity, mood tracking, screen time, schedule, music history, and sleep. During the next six days, she explored different image qualities by adjusting or adding keywords to the prompts, focusing on music history and mood tracking data, which had proven to facilitate more engaging reflections in the previous 6-day period.\nThroughout the study, the first author documented all of her impressions, opinions, and reflections on her interactions with the generative model, as well as the various images it produced, from the perspectives of both a designer and a user. She used a FigJam page [47], smartphone memos, and handwritten notes to capture these insights. The first and second authors reviewed the study progress every two days, while all three authors held weekly meetings to discuss ongoing reflections and identify emerging themes related to the key qualities that enabled meaningful engagement with the images generated from personal data."}, {"title": "3.1.2 Formative Study Findings: Two Key Enablers of Al-Generated Images for Personal Data Meaning-Making", "content": "The first author's felt experiences via hands-on experimentation evidenced the potential of image-generative AI in supporting the meaning-making of various personal data. For example, images generated from digital logs worked as an opportunity to reminisce about past experiences in a nuanced way, enabling her to vividly recall the memories and emotions. Also, images generated from schedule and screen time data facilitated the first author to rethink her experiences throughout the day, reflecting on and learning new aspects of herself in daily life, not necessarily encouraging her to pursue productivity in her life. Further, pondering upon how this could happen, we identified the two qualities that should be achieved to facilitate such meaning-making:\n(1) Unknowable Interpretation: AI-generated images based on personal data were perceived to offer a unique interpretation of oneself, created by an autonomous system and translated into visual form. Since the image-generation process is often opaque and produces unpredictable results, fully understanding how or why the Al interprets and represents data in specific ways is beyond our control. This adds another layer of interpretation from the viewer's perspective. We found that this multi-layered, unknowable process particularly heightened subjectivity, enabling a deeper understanding of the data and fostering more profound introspection. For example, the first author uncovered implicit self-knowledge by making the familiar data appear unfamiliar.\n(2) Serendipity of Visual Representation: The first author observed that generative AI could produce a wide range of unexpected, non-overlapping outcomes from the same prompts. We found that this serendipity of results fostered diverse types of reflection by offering multiple perspectives on the same data. For instance, when reviewing several images generated from mood-tracking data, the first author revisited her emotions by interpreting the images in relation to the data, occasionally even fostering changes in her behavior or mindset. This suggests that the unpredictability and serendipity of the images can encourage users to engage in multi-perspective reflection on a single experience.\nIn addition, we identified several concerns associated with generating images from personal data. For example, the first author noticed that numbers or text occasionally appeared in the generated images, raising concerns about the possibility of personal data being directly depicted in the images. This led to worries"}, {"title": "3.2 Technology Probe Design", "content": "Based on the insights from the autobiographical design study, along with existing literature [78, 135], we designed a technology probe [63] that generates images using users' personal data as input. We designed a probe inspired by personal informatics tools [19, 43, 44, 70, 75], which are widely used tools for the collection and reflection on personal data, to allow participants to naturally experience AI-generated images as a new mode of data representation within their existing practices."}, {"title": "3.2.1 Image Generation and Prompt Design", "content": "The image generation pipeline works as follows: when users input their data, textual descriptions (i.e., image prompts) for image generation are produced through the GPT-4 API [105]. These prompts are then sent to the DALL-E 3 API [104], a generative text-to-image model, to produce the images. The generated images and their corresponding prompts are displayed on the user interface and archived, allowing users to access them. \nAccordingly, we designed a prompt consisting of 1) initial setting: an overarching instruction for generating image prompts from the user's personal data, 2) image generation rules and 3) a processor of a user's personal data (to be entered by user), which is sent to the GPT-4 API. The overall prompt design followed Open Al's prompt engineering guide and tactics [103]. Specifically, we designed the prompt for generating textual descriptions for the DALL-E model by adhering to White et al.'s [135] visualization generator pattern (e.g., \u201cGenerate an X that I can provide to tool Y to visualize it\").\n(1) Initial setting: Our prompt instructions specified that the goal of image generation was to create images that would help users subjectively reflect on their personal data, and clarified the type of data to be provided by the users. Additionally, we instructed the model to follow the image generation rules established based on our autobiographical study.\n(2) Image generation rules: The image generation rules were established by accommodating the two key qualities discovered in the formative study: unknowable interpretation as Rule 1, Rule 2, Rule 3, and Rule 4; and serendipity of visual representation as Rule 5 and Rule 6. We instructed GPT not to simply depict the user's data, but to generate images based on interpretations of the data (Rule 2, Rule 4). Specifically, to ensure that users' input data was not directly shown in the images, we instructed the model to generate images based on keywords extracted from the data rather than displaying the data itself (Rule 1). We also included a requirement to avoid incorporating direct numbers or text into the images to mitigate concerns about exposing the data (Rule 3). We included instructions to generate a variety of images (Rule 5), along with a list of styles that Liu and Chilton [78] used to generate diverse images through text-to-image models (Rule 6). (We excluded 12 styles due to concerns about artist copyright issues). Furthermore, Rule 7 was included to instruct the model in generating image prompts for use with the DALL-E 3 model.\n(3) Processor of user input data: When the user inputs data into the entry shown in the user interface, the data is incorporated into the prompt.\""}, {"title": "3.2.2 Probe Features", "content": "The probe offers two main functions: (A) generating images from users' personal data as input, and (B) allowing users to explore the generated images.\n(A) Generating images using personal data as input: Users can enter their daily collected data into the entry field and click the 'View image' button to generate an image using that data."}, {"title": "3.2.3 Implementation", "content": "Our probe was implemented as a web-based application with a front-end client built using HTML/CSS and JavaScript, and a back-end server based on Node.js. User-entered data and generated image prompts were securely stored in MongoDB, while images generated from users' data were securely archived in Amazon S3. To ensure consistency with the formative study and apply its findings effectively, we utilized OpenAI's DALL-E 3 and GPT-4 model, specifically using the dall-e-3 and gpt-40 version via the API. We specifically selected DALL-E 3 to mitigate the generation of harmful content (e.g., violent, adult, or hateful material), as this model is known for reducing such risks [28, 100, 102]. Through this choice, we aimed to address concerns about using personal data for image generation, raised in the formative study."}, {"title": "3.3 Main Study: Probe-based Diary Study and Interview", "content": "We conducted a design-led exploratory study with the probe, consisting of an introductory session, a 21-day diary study and semi-structured interviews (Figure 1). We chose a diary study to capture nuanced and over-time experiences of users with Al-generated images in their everyday lives."}, {"title": "3.3.1 Study Participants", "content": "We recruited participants who had at least one month of experience collecting personal data using personal informatics tools, expecting those who had prior experience would provide rich insights by comparing the new data experience with their existing practices. Additionally, as we aimed to encourage daily probe usage, we selected participants who regularly had been tracking on a daily basis. Lastly, we screened participants who demonstrated a strong interest in collecting and reflecting on personal data, ensuring that the final group included individuals with diverse data types and goals. This was to explore a richer spectrum of experiences when different data were represented through AI-generated images. Considering these criteria, we included questions about one's previous experiences with personal data in the recruitment form, such as 1) the types of personal data they typically collect, 2) the personal informatics tools they use and their goals, and 3) the duration and frequency of their usage. We disseminated the recruitment notice through university communities, social media, and word of mouth.\nWe came to recruit 16 participants (F=10, M=6) (Table 1), with an average age of 24.5 (SD=1.9, MIN=20, and MAX=27). The types of data that they had been collecting ranged from health and physical activity (n=4) to time management (n=3), productivity (n=3), emotions and journaling (n=3), and digital logs (n=3). All participants received a compensation of 150,000 KRW (approx. 112 USD) for their participation."}, {"title": "3.3.2 Introductory Session", "content": "Each participant had an introductory session with the first author one week before the main diary study period. During this session, we introduced the study goal, explained the concept and features of the probe, and provided detailed guidelines for participation. We then asked participants about their prior experiences with personal data collection and reflection. Sensitized with these discussions, participants selected the types of data they wished to collect and use to generate images during the study. We then worked with participants to set up customized data entry fields in the probe, enabling seamless integration of the probe into their daily routines."}, {"title": "3.3.3 Diary Study with the Probe", "content": "A week after the introductory session, the 21-day diary study was conducted with each participant. Throughout the study period, participants were asked to: 1) use the probe at least once a day, 2) record their thoughts and experiences in the Probe Experience part every day, and 3) complete the Idea Brainstorming part at the end of each week (Day 7, Day 14, and Day 21). These idea brainstorming activities were set on a weekly basis considering that users often exhibit a weekly habit of reviewing personal data [22, 23, 44]. As for the diary format, we utilized Notion [91], a note-taking application, to enable quick and easy diary-keeping.\nThe 21-day study duration was chosen to align with our research goal of exploring new creative possibilities for engaging with personal data through Al-generated images. This duration was expected to provide participants with sufficient opportunity to explore and familiarize themselves with the probe, enabling in-depth interaction and reflection on its potential. By keeping the study focused within this period, participants were expected to explore and iterate meaningfully without the distraction of longer-term habituation effects. In hindsight, participants' responses began to show consistent patterns, with recurring ideas emerging by the third week, suggesting we had captured a comprehensive range of their new experiences and ideas.\nIn the Probe Experience part, we asked participants to answer the following to allow them to freely and naturally document their experiences with the probe:\n(1) In what situations and for what purposes did you use the app today?\n(2) Freely write down any thoughts you had while using the app today.\n(3) Generate the weekly image and freely write down your thoughts. [included only on Day 7, Day 14, and Day 21, when a week's worth of data had been collected]\n(4) (optional) Freely describe your experiences trying out this week's idea. [included starting on Day 8, after the first idea brainstorming activity]\nIn the Idea Brainstorming part, participants listed ideas on how they wanted to utilize the images. They then selected several ideas to try the following week and documented their experiences. This activity was included to explore in depth the various ways participants could utilize the generated images. To prevent this activity from influencing participants' engagement with the probe, we encouraged participants to share ideas freely as they naturally arose during the study, without forcing them to generate new ideas every week. The following questions were given to facilitate ideation:\n(1) Reflecting on the past week: (1-1) How was your experience using the app over the past week?, (1-2) (optional) How was your experience trying out the ideas over the past week? [included only on Day 14 and Day 21, after the first week of trying out an idea had passed]\n(2) Creating an idea list: How would you like to use the images generated from your data in your daily life? Write down a list of ideas that come to mind.\n(3) Selecting ideas to try: Which idea would you like to try out? Select as many as you would like, and over the next week, try these ideas. [included only on Day 7 and Day 14, to allow participants to experiment with their ideas during the following week]\nThe researchers set up individual chat rooms on a mobile messenger app to allow participants to ask questions freely throughout the study. Daily reminders were sent at 9 PM to encourage participants to write a diary reflecting their experiences of the probe during the day.\nIn total, we collected 336 pages of diary from the Probe Experience part and 82 ideas from the Idea Brainstorming part. Participants generated a total of 1380 images using the probe."}, {"title": "3.3.4 Post Interview", "content": "After the diary study period, we conducted one-on-one in-depth interviews with each participant to understand the detailed context and meaning behind the diary entries. Each interview was conducted via Zoom, lasting approximately 58 minutes. All sessions were audio-recorded with participants' consent. We began by asking participants about their overall experience with the probe, including their thoughts, changes in perception, and any new behaviors they noticed. To gain deeper insights into"}, {"title": "3.3.5 Data Analysis", "content": "We collected participants' diary entries, transcribed 940+ minutes of interview recordings, and gathered data from the probe, including the AI-generated images. All data were qualitatively analyzed using thematic analysis [13]. Diary entries and interview transcripts served as the primary datasets-diary data provided detailed insights into participants' internal thoughts and motivations, while interview data contextualized these insights by clarifying participants' intentions. We utilized system-logged images as supplementary data, primarily to clarify participants' interpretations or reactions mentioned in their diary entries and interviews. We did not assess the images' quality or analyze their compositional elements, since our research focused on understanding the experiences elicited by images rather than understanding or defining specific criteria for image quality. Instead, our focus was on how participants engaged with and responded to the images in the context of their experiences. This approach enabled a more nuanced understanding of how the generated images shaped participants' experiences.\nThroughout the data analysis, all researchers engaged in an iterative process of open coding on both the diary and interview data. The analysis focused on identifying new ways in which AI-generated images based on personal data fostered participants' engagement with their data. The first author began by identifying key quotes from diary and interview data through memoing [25], while mapping each image to the quotes where it was referenced. Initial codes were then inductively developed to categorize participants' engagement with the images, such as \"perceptions,\u201d \u201cutilization,\" and \"roles\" of the images. Building on these initial codes, we developed expanded codes through a more interpretive approach to uncover deeper meanings within participants' responses. This process yielded 85 expanded codes, including examples such as \"Feeling curious about the image results,\" \"Alleviating guilt associated with data,\" \"Sharing images for non-competitive purposes,\" and \"Being motivated to make changes in daily routines.\" After grouping the expanded codes into categories, three researchers collaboratively refined and developed them through iterative discussions to identify key themes and patterns until consensus was reached. As a result, we were able to identify four themes related to the new experiences that the images enabled with the participants' personal data, which we report in the following section."}, {"title": "4 Findings", "content": "We observed that participants engaged in a process of imagining and speculating on the meaning of the AI-generated images, which offered a new way of shaping their own interpretations of the data. We report on the four themes in the following."}, {"title": "4.1 Discovering Emotional Layers: Weaving Data, Emotion, and Context", "content": "We found that all participants reflected on the context and emotions associated with their data through the AI-generated images based on their personal data. For instance, those who got images generated from numerical data (e.g., step count, screen time) reported that the images helped them recall the context and emotions of the moments when the data was recorded, unlike traditional data representations that typically highlighted precise, rigid figures and fluctuations in the data over time. For example, P4 mentioned that an image evoked memories of moments when her step count was high, and P7 reflected on the reasons for walking throughout the day and her emotional state at that time by viewing an image:\nFurther, participants who experienced images generated from more qualitative forms of data, such as habit tracking or journaling, also reported accounts of deeper reflections on the emotions throughout the process of recording data and receiving the corresponding images. For example, P10 mentioned that while she used to focus on documenting events when journaling in the past, the awareness that her entries would be transformed into images shifted her focus of documentation toward emotions rather than objective facts. This change encouraged her to reflect more deeply on her emotional state:\nP5, who usually focused on documenting tasks and accomplishments, mentioned that the images generated from her habit data felt as though they reflected her situation and emotions and therefore fostered deeper self-reflection. Referring to an image and checking the prompt behind that image, she noted that \"the phrase 'changed from clear to cloudy' matched my situation so well that it felt like I was confronting my emotions head on.\" She elaborated on how such emotional experiences happened:\nWe noticed that these emotional experiences stemmed from participants' perception of the images not simply as representations of data, but as the AI translating their emotions into visual forms. Participants felt that the AI somehow interpreted their emotions related to the data through its incomprehensible generative process and expressed them through elements, colors, and moods in the images. This led participants to speculate on the reasons behind the visual choices, leading them to recall and reflect on the context and emotions associated with the image and input data. For instance, P4 mentioned that she believed her image conveyed the \"fantastical feeling\" of the joy she had experienced. Similarly, P16 interpreted his image as the Al expressing his emotions about the task he had completed through \u201cominous purple clouds.\u201d As a result, P16 noticed that he was able to \"remember more clearly the emotions and reflections at that time.\"\nFurthermore, as some participants (n=7) learned how the images helped them reflect on the context and emotions associated with their data, recalling and reflecting on their emotions became their primary purpose for using the probe over time, as P4 described: \u201cAt first, I started by tracking my step count, expecting it would mainly relate to my health. But over time, it became a way to capture how I spent my day, my overall lifestyle, and factors like my moods and the weather. All these elements came together and evolved into something much more comprehensive.\" P13 and P15, who initially started using the probe for the purpose of time management and maintaining a regular exercise routine respectively, mentioned that they eventually came to treat the probe more like a \"journal.\" Additionally, some participants (P6, P9, P13) expressed a desire to save or print the images and incorporate them into their actual journaling practice.\nMeanwhile, a handful of participants expressed negative experiences with the images. P1 and P5 mentioned that there might be a chance that their negative emotions could be amplified when those were reflected in the images:"}, {"title": "4.2 (Re)conceptualizing, (Re)interpreting the Self", "content": "Participants not only revisited past emotions but also reinterpreted and reconstructed their experiences and thoughts through the generated images. They perceived the images as a synthesis of their personal data and the Al's interpretation of that data. P3 described the images as a combination of \"my narrative and how the other might see me,\" and P11 referred to the generation process as one where \"the Al's thoughts are layered on.\" Through this process, participants were able to achieve a more refined and deeper understanding of themselves, discovering new aspects of themselves that they had not previously recognized:\nFurther, participants perceived the images as a means to view their data through the lens of an external observer-in our case, the AI-offering a \"third-person perspective\" (P12) and a \"more objective\" (P13) interpretation of their data. In particular, P8 and P13 highlighted that this aspect differentiated the experience of image generation from taking photos or drawing:\nAs a result, participants mulled over where their first-person understanding of the data aligned or diverged from the Al's more observational and objective interpretation, which fostered introspection regarding the meaning of their personal data. Particularly, many participants (n=12) mentioned that when the generated image did not seem to match their data, it actually encouraged them to view their experiences from a new perspective. For example, P1 described how she came to reconsider her data from an unfamiliar viewpoint through an image, resulting in a new understanding of her emotions:\nConversely, when participants felt that the images accurately captured their thoughts or emotions, it often led them to reify and reaffirm those feelings. P3, for instance, felt that the image generated from her weekly data precisely captured the thoughts she had that week, which helped her better understand and confirm her own thinking:\nFurthermore, participants engaged more deeply in the activity of reconceptualization and reinterpretation of the self by acting as a second-order observer [80], interpreting the Al's interpretation in various ways. First, we observed that several participants (P3, P4, P8, P12, P14) repeatedly generated images using the same set of data as input. They intentionally did so in an attempt to select the image they felt best represented their thoughts and emotions from the different variations, using this process to more clearly confirm their daily experiences, as P4 describes:\nWe also observed participants experimenting with further processing the generated images in their own ways, transforming them into new reflective materials. For example, P1 created a color palette using a set of generated images. P4 made a collage using images generated over the course of a week to create a new composite image. P8 cropped and saved specific elements from the images that caught her attention and later compiled them together.\nThese practices were all aimed at better understanding how the AI had interpreted them. P1 noted, \u201cI realized the AI was using colors to represent my emotions,\" which led her to extract colors from various images to examine how the AI expressed her feelings or memories through specific colors. P4 compared the collage she created with the weekly images, trying to uncover the differences between the elements she considered important and those the AI emphasized. P8 also believed that the AI, through the elements in the images, was representing her emotions and daily experiences, so the process of deconstructing images was, similar to P4, also her own way of exploring the intersection between the emotions and experiences the AI captured and those that she personally found important.\""}, {"title": "4.3 Narrative Crafting: Shaping Personal Stories through Visual Interpretation", "content": "We observed that participants developed a sense of authorship over the Al-generated images. Although the images were generated by the AI model, the fact that those were created using participants' own data played a significant role in shaping the sense of authorship. For example, P5 explained that she felt \"as though I created it [the image"}]}