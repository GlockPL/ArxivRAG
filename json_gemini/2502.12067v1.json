[{"title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "authors": ["Heming Xia", "Yongqi Li", "Chak Tou Leong", "Wenjie Wang", "Wenjie Li"], "abstract": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop. We release our code and checkpoints in https://github.com/hemingkx/TokenSkip.", "sections": [{"title": "1 Introduction", "content": "Chain-of-Thought (CoT) prompting (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022) has emerged as a cornerstone strategy for enhancing Large Language Models (LLMs) in complex reasoning tasks. By eliciting step-by-step inference, CoT enables LLMs to decompose intricate problems into manageable subtasks, thereby improving their problem-solving performance (Yao et al., 2023; Wang et al., 2023; Zhou et al., 2023; Shinn et al., 2023). Recent advancements, such as OpenAI's o1 (OpenAI et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025), further demonstrate that scaling up CoT lengths from hundreds to thousands of reasoning steps could continuously improve LLM reasoning. These breakthroughs have underscored CoT's potential to advance LLM capabilities, expanding the boundaries of AI-driven problem-solving.\nDespite its effectiveness, the increased length of CoT sequences introduces substantial computational overhead. Due to the autoregressive nature of LLM decoding, longer CoT outputs lead to proportional increases in both inference latency and memory footprints of key-value cache. Additionally, the quadratic computational cost of attention layers further exacerbates this burden. These issues become particularly pronounced when CoT sequences extend into thousands of reasoning steps, resulting in significant computational costs and prolonged response times. While prior research has explored methods for selectively skipping reasoning steps (Ding et al., 2024; Liu et al., 2024), recent findings (Jin et al., 2024; Merrill and Sabharwal, 2024) suggest that such reductions may conflict"}, {"title": "2 Background and Preliminaries", "content": "In this section, we discuss the relevant research background and present preliminary studies on token efficiency in CoT sequences, exploring its impact on the reasoning performance of LLMs."}, {"title": "2.1 Token Importance", "content": "We first investigate a critical research question to CoT efficiency: \u201cDoes every token in the CoT output contribute equally to deriving the answer?\u201d In other words, we would like to know if there is any token redundancy in CoT sequences that could be eliminated to improve CoT efficiency.\nToken redundancy has been recognized as a longstanding and fundamental issue in LLM efficiency (Hou et al., 2022; Zhang et al., 2023; Lin et al., 2024; Chen et al., 2024). Recently, it has garnered intensive research attention in prompt compression (Li et al., 2023; Jiang et al., 2023; Pan et al., 2024), which focuses on removing redundant tokens from input prompt to reduce API token usage. To address this issue, Selective Context (Li et al., 2023) proposed to measure the importance of tokens in a piece of text based on the semantic confidence of LLMs:\n$I_1(x_i) = -logP(x_i | x_{<i}; \\Theta_{M_1}), (1)$\nwhere $x = \\{x_i\\}_{i=1}^n$ is the given text, $x_i$ denotes a token, and $M_1$ denotes the LLM used to compute the confidence of each token. Intuitively, such measurement could be seamlessly applied to CoT"}, {"title": "2.2 CoT Recovery", "content": "We further explore the following research question: \u201cAre LLMs capable of restoring the CoT process from compressed outputs?\u201d The answer is yes. As shown in Figure 3 and detailed in Appendix A, examples restored from compressed CoTs using LLaMA-3.1-8B-Instruct demonstrate that LLMS could effectively comprehend the semantic information encoded in the compressed CoT and restore the CoT process. This capability ensures that the interpretability of compressed CoTs is maintained. Additionally, when required by users, the complete CoT process can be recovered and presented.\nIn summary, the empirical analysis above underscores the potential of trimming redundant tokens to enhance CoT efficiency, as well as the ability of LLMs to restore CoT from compressed outputs. However, enabling LLMs to autonomously skip redundant CoT tokens and identify shortcuts between critical reasoning tokens presents a non-trivial challenge. To the best of our knowledge, this work is the first to explore CoT compression through token skipping. In the following sections, we present our proposed methodology in detail."}, {"title": "3 TokenSkip", "content": "We introduce TokenSkip, a simple yet effective approach that enables LLMs to skip less important tokens, enabling controllable CoT compression with adjustable ratios. This section demonstrates the details of our methodology, including token pruning (\u00a73.1), training (\u00a73.2), and inference (\u00a73.3)."}, {"title": "3.1 Token Pruning", "content": "The key insight behind TokenSkip is that \u201ceach reasoning token contributes differently to deriving the answer.\u201d To enhance CoT efficiency, we propose to trim redundant tokens from LLM CoT outputs and fine-tune LLMs using these trimmed CoT trajectories. The token pruning process is guided by the concept of token importance, as detailed in Section 2.1.\nSpecifically, given a target LLM M, one of its CoT trajectories $c = \\{c_i\\}_{i=1}^n$, and a desired compression ratio $\\gamma \\in [0, 1]$, TokenSkip first calculates the semantic importance of each CoT token $I(c)$, as defined in Eq (2). The tokens are then ranked in descending order based on their importance values. Next, the $\\gamma$-th percentile of these importance values is computed, representing the threshold for token pruning:\n$I_\\gamma = np.percentile([I(c_1), .., I(c_m)], \\gamma). (3)$\nFinally, CoT tokens with an importance value greater than or equal to $I_\\gamma$ are retained in the compressed CoT trajectory:\n$\\tilde{c} = \\{c_i | I(c_i) > I_\\gamma\\}, 1 \\leq i \\leq m. (4)$"}, {"title": "3.2 Training", "content": "Given a training dataset D with N samples and a target LLM M, we first obtain N CoT trajectories with M. Then, we filter out trajectories with incorrect answers to ensure the high quality of training data. For the remaining CoT trajectories, we prune each CoT with a randomly selected compression ratio \u03b3, as demonstrated in Section 3.1. For each (question, compressed CoT, answer), we inserted the compression ratio \u03b3 after the question. Finally, each training sample is formatted as follows:\nQ [EOS] \u03b3 [EOS] Compressed CoT A,\nwhere (Q, A) indicates the question, answer) pair. Formally, given a question \u00e6, compression ratio \u03b3, and the output sequence $y = \\{y_i\\}_{i=1}^{l'}$, which includes the compressed CoT $\\tilde{c}$ and the answer a, we fine-tunes the target LLM M, enabling it to perform chain-of-thought in a compressed pattern by minimizing\n$L = \\sum_{i=1}^{l'} log P(y_i | x, y, y_{<i}; \\Theta_M), (5)$\nwhere $y = \\{\\tilde{c}_1,\u2026\u2026\u2026,\\tilde{c}_{m'}, a_1,\u2026\u2026,a_t\\}$. Note that the compression is performed solely on CoT sequences, and we keep the answer $a = \\{a_i\\}_{i=1}^{t}$ unchanged. To preserve LLMs' reasoning capabilities, we also include a portion of the original CoT trajectories in the training data, with \u03b3 set to 1."}, {"title": "3.3 Inference", "content": "The inference of TokenSkip follows autoregressive decoding. Compared to original CoT outputs that may contain redundancy, TokenSkip facilitates LLMs to skip unimportant tokens during the chain-of-thought process, thereby enhancing reasoning efficiency. Formally, given a question x and the compression ratio \u03b3, the input prompt of TokenSkip follows the same format adopted in fine-tuning, which is Q [EOS] \u03b3 [EOS]. The LLM M sequentially predicts the output sequence $\\hat{y}$:\n$\\hat{y} = arg max \\prod_{j=1}^{l'} log P(\\hat{y_j} | x, \\hat{y}, \\hat{y}_{<j}; \\Theta_M)$,\nwhere $\\hat{y} = \\{\\hat{c}_1,\u2026\u2026\u2026, \\hat{c}_{m"}, "hat{a}_1,\u2026\u2026, \\hat{a}_{t'}\\}$ denotes the output sequence, which includes CoT tokens $\\hat{c}$ and the answer $\\hat{a}$. We illustrate the training and inference process of TokenSkip in Figure 4."]}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "Models and Datasets We primarily evaluate our method using LLaMA-3.1-8B-Instruct (Dubey et al., 2024) and Qwen2.5-Instruct series (Yang et al., 2024). The evaluation leverages two widely-used math reasoning benchmarks: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). For training, we use the respective training sets from both datasets. Regarding the MATH dataset, due to the computation cost, we assess our method on a subset, MATH-500, which is identical to the test set used in Lightman et al. (2024). The subset comprises 500 representative problems, and we find that its evaluation yields results comparable to those from the full dataset.\nImplementation Details We utilize LLMLingua-2 (Pan et al., 2024) as the token importance metric to generate our compressed CoT training data. The compression ratio \u03b3 is randomly selected from {0.5, 0.6, 0.7, 0.8, 0.9, 1.0} for each training sample. We adopt LoRA (Hu et al., 2022), an efficient and reproducible approach that has been widely verified as effective in LLM fine-tuning, to train our models. The rank r is set to 8, and the scaling parameter a is set to 16. TokenSkip is characterized by its low training cost, with training taking ~2 hours for the 7B model and ~2.5 hours for the 14B model on 3090 GPUs. During inference, the maximum number of tokens max_len is set to 512 for GSM8K and 1024 for MATH\u00b9.\nBaselines In our main experiments, we compare TokenSkip to two commonly used length control baselines: 1) Prompt-based Reduction. In this approach, we instruct the LLM to reduce a fixed proportion of output tokens in the CoT process. Specifically, we append a prompt such as \u201cPlease reduce 50% of the words in your Chain-of-Thought process.\u201d to the input instruction. 2) Truncation. This method involves brute-force length truncation, where the maximum number of output tokens is restricted, compressing the CoT output to a fixed"}, {"title": "4.2 Main Results", "content": "The performance of TokenSkip on GSM8K using the Qwen2.5-Instruct series\u00b3 is illustrated in Figure 5. As the model scale increases, there is less performance degradation at higher compression ratios, indicating that larger LLMs are better at identifying shortcuts between critical reasoning tokens, enabling more efficient CoT generation. Notably, Qwen2.5-14B-Instruct exhibits almost NO performance drop (less than 0.4%) with 40% token trimming. Even at a compression ratio of 0.5, the model maintains strong reasoning capabilities, with only 2% performance degradation. These results highlight the substantial potential of TokenSkip to reduce CoT token usage and accelerate reasoning in large-scale LLMs. Due to computational constraints, experiments with larger models are not conducted and are left for future exploration.\nWe further compare TokenSkip with two widely"}, {"title": "4.3 Analysis", "content": "Compression Ratio In our main results, we focus on compression ratios greater than 0.5. To further investigate the performance of TokenSkip at lower compression ratios, we train an additional variant, denoted as More Ratio, with extra compression ratios of 0.3 and 0.4. As shown in Figure 6, the ratio adherence of models largely degrades at these lower ratios. We attribute this decline to the excessive trimming of reasoning tokens, which likely causes a loss of critical information in the completions, hindering the effective training of"}, {"title": "5 Related Work", "content": "Efficient CoT While Chain-of-Thought (CoT) enhances task performance by simulating human-like reasoning patterns, its reasoning steps introduce significant computational overhead. As a result, researchers have sought methods to reduce this overhead while retaining the benefits of CoT. One intuitive approach is to simplify, skip (Marconato et al., 2024; Ding et al., 2024; Liu et al., 2024), or generate thinking steps in parallel (Ning et al., 2023) to improve efficiency. Another strategy involves compressing reasoning steps into continuous latent representations (Goyal et al., 2024; Deng et al., 2024; Hao et al., 2024; Cheng and Van Durme, 2024), allowing LLMs to reason without explicitly generating discrete word tokens. To minimize the generation of redundant natural lan-"}, {"title": "6 Conclusion", "content": "This work introduces TokenSkip, a simple yet effective approach for controllable Chain-of-Thought (CoT) compression. TokenSkip is built upon the semantic importance of CoT tokens By selectively skipping less important tokens while preserving critical ones, TokenSkip enables LLMs to generate compressed CoTs with adjustable ratios, thereby striking an expected balance between reasoning efficiency and accuracy. Extensive experiments across various LLMs and tasks validate the effectiveness of TokenSkip. We hope our investigations in token skipping will offer valuable insights for advancing efficient CoT research and inspire future studies in this area."}, {"title": "Limitations", "content": "Due to computational constraints, experiments with larger LLMs, such as Qwen2.5-32B-Instruct and Qwen2.5-72B-Instruct, were not conducted. We believe that TokenSkip could achieve a more favorable trade-off between reasoning performance and CoT token usage on these models. Additionally, the token importance measurement used in our study, derived from the LLMLingua-2 compressor (Pan et al., 2024), was not specifically trained on mathematical data. This limitation may affect the compression effectiveness, as the model is not optimized for handling numerical tokens and mathematical expressions. Furthermore, experiments with long-CoT LLMs, such as QwQ-32B-Preview, were also excluded due to computational constraints. We plan to explore these aspects in future work, as we anticipate that TokenSkip 's potential can be further realized in these contexts."}, {"title": "Ethics Statement", "content": "The datasets used in our experiment are publicly released and labeled through interaction with humans in English. In this process, user privacy is protected, and no personal information is contained in the dataset. The scientific artifacts that we used are available for research with permissive licenses. And the use of these artifacts in this paper is consistent with their intended use. Therefore, we believe that our research work meets the ethics of ACL."}, {"title": "Appendix", "content": null}, {"title": "A CoT Recovery", "content": "In this section, we provide the detailed prompt for our recovery experiments, which is illustrated in Figure 10. Besides, we present the CoT recovery result from GPT-40 (OpenAI, 2023) in Figure 11. The recovered results demonstrate that GPT-40 could understand the compressed CoT content and correctly restore the original CoT process."}, {"title": "B Experimental Details", "content": null}, {"title": "B.1 Implementation Details", "content": "We utilize LLMLingua-2 (Pan et al., 2024) as the token importance metric to generate our compressed CoT training data. The compression ratio y is randomly selected from {0.5,0.6, 0.7, 0.8, 10.9, 1.0} for each training sample. We adopt LoRA (Hu et al., 2022) to train our models. The rank r is set to 8, and the scaling parameter a is set to 16. We train the models for 3 epochs on both datasets. The peak learning rate is set to 5e-5, following a cosine decay schedule. We use AdamW (Loshchilov and Hutter, 2019) for optimization, with a warmup ratio of 0.1. We implement our training process using the LLAMA-Factory (Zheng et al., 2024) library. Inference for both our method and all baselines is performed using the Huggingface transformers package. During inference, the maximum number of tokens max_len is set to 512 for GSM8K and 1024 for MATH. All experiments are conducted using Pytorch 2.1.0 on 2\u00d7NVIDIA GeForce RTX 3090 GPU (24GB) with CUDA 12.1, and an Intel(R) Xeon(R) Platinum 8370C CPU with 32 cores."}, {"title": "B.2 Detailed Results with Qwen", "content": "We provide detailed experimental results of the Qwen2.5-Instruct series evaluated on GSM8K in Table 2. As the model scale increases, there is less performance degradation at higher compression ratios, indicating that larger LLMs are better at identifying shortcuts between critical reasoning tokens, enabling more efficient CoT generation."}]