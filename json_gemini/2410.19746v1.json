{"title": "METAMIZER: A VERSATILE NEURAL OPTIMIZER FOR FAST AND ACCURATE PHYSICS SIMULATIONS", "authors": ["Nils Wandel", "Stefan Schulz", "Reinhard Klein"], "abstract": "Efficient physics simulations are essential for numerous applications, ranging from realistic cloth animations or smoke effects in video games, to analyzing pollutant dispersion in environmental sciences, to calculating vehicle drag co-efficients in engineering applications. Unfortunately, analytical solutions to the underlying physical equations are rarely available, and numerical solutions require high computational resources. Latest developments in the field of physics-based Deep Learning have led to promising efficiency improvements but still suffer from limited generalization capabilities and low accuracy compared to numerical solvers.\nIn this work, we introduce Metamizer, a novel neural optimizer that iteratively solves a wide range of physical systems with high accuracy by minimizing a physics-based loss function. To this end, our approach leverages a scale-invariant architecture that enhances gradient descent updates to accelerate convergence. Since the neural network itself acts as an optimizer, training this neural optimizer falls into the category of meta-optimization approaches.\nWe demonstrate that Metamizer achieves unprecedented accuracy for deep learning based approaches - sometimes approaching machine precision - across multiple PDEs after training on the Laplace, advection-diffusion and incompressible Navier-Stokes equation as well as on cloth simulations. Remarkably, the model also generalizes to PDEs that were not covered during training such as the Poisson, wave and Burgers equation.\nOur results suggest that Metamizer could have a profound impact on future numerical solvers, paving the way for fast and accurate neural physics simulations without the need for retraining.", "sections": [{"title": "INTRODUCTION", "content": "Countless physical systems can be described by partial differential equations (PDEs). For example, electrostatic or gravitational fields can be described by the Poisson equation, heat diffusion or pollutant dispersion can be described by the advection-diffusion equation, pressure waves by the wave equation, fluids by the incompressible Navier-Stokes equation, and so on. Unfortunately, most of these equations do not possess analytical solutions and numerical solutions are computationally expensive. To reduce the computational burden of numerical methods, numerous neural surrogate models have been developed in recent years that achieve fast and highly efficient physics simulations (Thuerey et al., 2021; Cuomo et al., 2022). However, the accuracy of current neural surrogates still remains limited compared to numerical solvers that can iteratively improve a solution to a high desired accuracy. In addition, neural surrogate models are usually tailored to specific PDEs and do not generalize well across multiple different PDEs without retraining. Zero-shot PDE solvers, analogous to current endeavours in foundational models for natural language processing (Brown, 2020) or computer vision (Radford et al., 2021; Kirillov et al., 2023), could be of great benefit for instant realisitc animations in computer games or computer aided engineering.\nThus, in this work, we propose Metamizer, a neural optimizer to accelerate gradient descent for fast and accurate physics simulations. To this end, we introduce a novel scale-invariant architecture that suggests improved gradient descent update steps that are independent of arbitrary scalings of the loss function or loss domain. We train Metamizer without any training data directly on the physics-based loss that it is supposed to optimize. After training, unlike traditional gradient descent methods, Metamizer requires no hyperparameters such as learning rate or momentum, and allows tradeoffs between runtime and accuracy for fast and highly accurate results. We evaluate the very same network (same architecture and same weights) across a wide range of linear and non-linear PDEs and showcase its generalization capabilites - even to PDEs that were not covered during training.\nCode as well as a pretrained model will be made available upon acceptance."}, {"title": "RELATED WORK", "content": "Numerical Methods that do not rely on deep learning to solve PDEs for example by means of finite differences or finite elements require high computational resources (Farmaga et al., 2011). Thus, already in the last century several specialized methods have been developed to accelerate for example fluid simulations (Chen & Doolen, 1998; Stam, 1999) or cloth simulations (Baraff & Witkin, 1998). Unfortunately, such specialized methods do not generalize across various different PDEs as they exploit domain-specific assumptions and are still relatively expensive to compute.\nGradient Descent based methods like AdaGrad (Duchi et al., 2011), Adam (Kingma & Ba, 2017; Reddi et al., 2019), AdamW (Loshchilov, 2017) and many more (Norouzi & Ebrahimi, 2019) build the backbone of most Deep Learning approaches. These optimization methods can be generally applied to a wide range of differentiable minimization problems including solving PDEs (Nurbekyan et al., 2023). However, Gradient Descent methods require tuning of hyperparameters such as learning rate or momentum and many iterations until convergence. As a result, they are not yet efficient enough for real-time physics simulations.\nMeta-Learning and in particular \"Learning to Optimize\" (L2O), is the field of research that deals with the automatic improvement of optimization algorithms, typically by means of machine learning techniques. Pytorch libraries such as \"higher\u201d (Grefenstette et al., 2019) or \"learn2learn\" (Arnold et al., 2020) allow for example to automatically optimize hyperparameters of optimizers like ADAM through gradient descent. Andrychowicz et al. (2016) train LSTMs to optimize neural networks for classification tasks and style transfer and Chen et al. (2020) present several training techniques that suggest there is still room for potential improvement in the field L2O. However, to the best of our knowledge, L2O has not yet been applied in the context of physics simulations.\nData-driven Deep Learning approaches have been widely established to generate neural surrogate models for efficient physics simulations for example in context of fluid dynamics (Tompson"}, {"title": "FOUNDATIONS", "content": "In this chapter, we introduce some basic concepts and definitions for PDEs, explain, how finite difference schemes can be used to formulate a physics-based loss, and give some intuition to motivate our gradient-based optimization approach."}, {"title": "Partial Differential Equations (PDEs)", "content": "Partial differential equations are equations that constrain the partial derivatives of a multivariate function. In its most general form, a PDE can be written as:\n$F(x_1, ..., x_n, \\partial_{x_1}u, ..., \\partial_{x_n} u, \\partial_{x_1}^2u, ...) = 0\\forall x_1,..., x_n \\in \\Omega$ \nwhere $u(x_1,..., x_n)$ is the unknown function that we want to solve, $x_1, ..., x_n \\in \\Omega$ are the independent variables inside a domain $\\Omega$, and $\\partial_{x_i}u$ denotes the partial derivative of u with respect to $x_i$. If one of the independent variables corresponds to time t, F is called a time-dependent PDE. If u does not depend on t, it is called stationary. If F is linear in u and its derivatives, F is called a linear PDE. Otherwise, F is called a non-linear PDE."}, {"title": "Boundary and Initial Conditions", "content": "Usually, additional constrains at the domain boundaries are given such as boundary or initial conditions. Although various types of different boundary conditions could be considered, in this work, for simplicity, we focus solely on Dirichlet boundary conditions:\n$u(x_1,...,x_n) = d(x_1,...,x_n) \\forall x_1,..., x_n \\in \\partial\\Omega$\nHere, $d(x_1,..., x_n)$ directly specifies the values of u at the domain boundary $\\partial\\Omega$. Typically, time-dependent PDEs also require an initial state (or initial condition) from which the solution of a PDE evolves over time."}, {"title": "Finite Differences", "content": "To compute u and its partial derivatives in a numerical manner, u needs to be discretized. Here, we consider a regular grid in space (and time). This allows to compute derivatives like gradients ($\\nabla$), divergence ($\\nabla \\cdot$), Laplace ($\\triangle$) or curl ($\\nabla \\times$) numerically by performing convolutions with finite difference kernels. By plugging the results of these finite difference convolutions for every grid point into the general formulation of PDEs (Equation 1), we obtain a large system of equations. If the PDE is linear, this set of equations becomes a (typically sparse) linear system of equations."}, {"title": "Time-Integration Schemes", "content": "In case of time-dependent PDEs, a strategy must be chosen to integrate the simulation in time for given initial conditions. There exist several strategies to calculate the time derivative with finite differences:\n\u2022 In forward Euler schemes, the computation of the time derivative depends only on the current time step. This explicit strategy makes computations easy and efficient, but tends to cause unstable simulation behavior for many physical systems if the time step is chosen too large. Therefore, explicit schemes are not considered in this work.\n\u2022 In backward Euler schemes, the calculation of the time derivative depends on the following time step. This implicit strategy results in stable simulations, but can suffer from numerical dissipation and requires solving large systems of equations.\n\u2022 In Crank-Nicolson schemes, the centered finite difference of the current and the following time step is used to calculate the time derivative. This implicit strategy results in more accurate computations compared to forward or backward Euler, but requires, like backward Euler, solving large systems of equations."}, {"title": "Physics-Based Loss", "content": "Solving large (potentially non-linear) systems of equations obtained by finite differences is a challenging task that is typically computationally expensive. Thus, we formulate a physics-constrained loss, that penalizes the mean squared residuals (left-hand side) of Equation 1 as follows:\n$L(u) = \\frac{1}{\\Omega} \\sum_{x_1,..., x_n \\in \\Omega} |F(x_1,..., x_n, \\partial_{x_1}u, ..., \\partial_{x_n} u, \\partial_{x_1}^2u, ...)|^2$\nThis way, we can search for a solution $u^*$ of the PDE by minimizing L:\n$u^* = arg \\min_{u} L$"}, {"title": "Dirichlet Boundary Conditions", "content": "Dirichlet Boundary conditions (Equation 2) can be typically applied directly by setting the corresponding boundary grid points of u equal to d. However, sometimes, this is not possible and we have to consider an additional loss term for the Dirichlet boundary conditions:\n$L_D(u) = \\frac{1}{\\partial \\Omega} \\sum_{x_1,..., x_n \\in \\partial\\Omega} |u(x_1,...,x_n) - d(x_1,...,x_n)|^2$\nIn this case, we have to minimize $L + L_D$ to solve Equation 1 and Equation 2."}, {"title": "Gradient Descent Intuition", "content": "In order to minimize the physics-based loss L (Equation 3 and 4), we follow a gradient descent approach. Figure 2 provides some intuition on common problems with naive gradient descent in the vicinity of ill conditioned minima and how convergence could be improved: If we take too small steps along the gradients (blue curve 1), convergence becomes very slow. If we take too large steps (red curve 2), the gradient descent scheme diverges. Ideally (green curve 3), we would first take some small initial steps to check the local gradients and avoid initial divergence. Then, the step size can be carefully increased. Instead of directly following the gradients, a better step direction can be found by taking the gradients of the previous steps into account as well. Once we come close to the minimum, the step size should be decreased again to achieve higher accuracy. Note that scaling the loss function L and thus its gradients by a constant factor c does not affect its minima ($arg \\min_{u} L(u) = arg \\min_{u} c \\cdot L(u)$). Thus, the gradient descent procedure should be invariant with respect to constant gradient scalings. Similarly,\nscaling the coordinate system of L should result in equally scaled update steps ($arg \\min_{u} L(u) = c \\cdot arg \\min_{u} L(c\\cdot u)$, see solid vs dashed lines in Figure 2). This motivates our scale invariant Metamizer architecture that can deal with arbitrarily scaled gradients and can automatically adapt its step size (see Figure 3 a)."}, {"title": "METHOD", "content": "Our approach aims to accelerate convergence of a gradient descent scheme by proposing improved update steps with a neural network."}, {"title": "Metamizer Architecture", "content": "The neural network architecture as depicted in Figure 3 a) works as follows: First, the gradient $\\nabla_{u} L_i$ of the current iteration i as well as the gradient $\\nabla_{u} L_{i-1}$ and update step $\\Delta u_{i-1}$ of the previous iteration i \u2013 1 are taken as input. Then, the gradients of the current and previous iteration are normalized by the standard deviation of the current gradient and the step size is normalized by its standard deviation as well. These values are then concatenated and fed into a U-Net (Ronneberger et al., 2015) in order to predict an update scale factor for $s_i$ and an update direction that gets multiplied with $s_i$ to obtain the update step $\\Delta u_i$ for the next iteration. Finally, the computed update step is applied to u to obtain $u_{i+1} = u_i + \\Delta u_i$. For the initial state, we set $u_0 = \\nabla_{u} L_0 = \\Delta u_0 = 0$ and set $s_0 = 0.05$ to avoid initial divergence by making only small steps at the beginning.\nThese normalization and scaling measures make the network invariant with respect to arbitrary scalings of the loss or domain and allow the network to make reasonable progress at all stages of the optimization: At the beginning, when large update steps must be made in the presence of large gradients, as well as in later optimization stages, when fine adjustments must be made in the presence of small gradients. To allow for very high accuracies, the gradient computations and update steps need to be calculated in double precision. Nevertheless, due to the input normalizations, the CNN can be evaluated using a more efficient regular float precision. Since the outputs of the network get scaled by $s_i$, the training gradients get scaled as well. This would result in huge gradients for the first optimization iterations compared to basically no training signal at later optimization iterations. Thus, we employ gradient normalization (Chen et al., 2018) to normalize gradients during backpropagation and provide the network with an evenly distributed training signal during all optimization stages and across all different PDEs that we train on in parallel. Since we want to reuse the same network for different PDEs that might also contain different numbers of channels, Metamizer optimizes all channels separately (e.g. the Laplace equation requires only 1 channel while incompressible fluids require 2 channels for the velocity and pressure field and cloth dynamics require 3 channels to describe motions in x/y/z directions)."}, {"title": "Training", "content": "The neural optimizer was trained in a cyclic Meta-learning manner inspired by Wandel et al. (2021a): As visualized in Figure 3 b), we first 0-initialize a training pool of randomized PDEs (e.g. randomized boundary conditions for the Laplace / advection-diffusion / Navier-Stokes equations). Note that in contrast to data-driven methods, no pre-generated ground truth data is needed. Then, we draw a random mini-batch that contains the current gradient $\\nabla_{u} L_i$ of the physics-based\nloss as well as optimizer state information (i.e. $\\nabla_{u} L_{i-1}$, $\\Delta u_{i-1}$, $s_{i-1}$) and feed it into the Metamizer architecture to compute an update step $\\Delta u_i$. This update step, together with updated state information (i.e. $\\nabla_{u} L_i$, $\\Delta u_i$, $s_i$) is fed back into the training pool to update the training environments with more realistic data and to recompute the physics-based loss L based on the updated values of u. This recomputed loss is then used to optimize the Metamizer with gradient descent (Adam, lr=0.001). By iterating this training cycle, the training pool becomes filled with more and more realistic training data and the neural optimizer becomes better and better at minimizing physics-based losses. Furthermore, since this way the training pool constantly generates new training data on the fly, its memory footprint is small enough such that the entire training pool can be kept in GPU memory. After training for ca 6 hours on a NVidia Geforce RTX 4090, we obtained a model that was able to solve various PDEs at high precision and produced the results shown in the following section."}, {"title": "RESULTS", "content": "In this section, we show that the very same model (same architecture and same weights) can deal with a wide variety of PDEs - linear as well as non-linear, stationary as well as time-dependent."}, {"title": "LINEAR SECOND ORDER PDES", "content": "Linear second order PDEs can be grouped into 3 categories: Elliptic PDEs (e.g. Poisson or Laplace equation), parabolic PDEs (e.g. advection-diffusion equation) and hyperbolic PDEs (e.g. wave equation). Here, we show that our model excels in all 3 categories:"}, {"title": "POISSON AND LAPLACE EQUATION", "content": "The Poisson equation is a stationary, elliptic PDE and is important to find e.g. minimal surfaces, steady state equilibria of the Diffusion-Equation (see Section 5.1.2) or to calculate energy potentials of electrostatic or gravitational fields:\n$F = \\triangle u - f = 0$\nThe Laplace Equation is the homogeneous (f = 0) special case of the Poisson equation."}, {"title": "ADVECTION-DIFFUSION EQUATION", "content": "The Advection-Diffusion equation is an important parabolic PDE that describes for example heat transfer or the dispersion of pollutants. For an unknown function u, it can be written as:\n$F = \\partial_t u - D\\triangle u + \\nabla \\cdot (\\vec{v} u) - R = 0$\nHere, D denotes the diffusivity, $\\vec{v}$ denotes the velocity field for the advection term and R corresponds to the source / sink term. In contrast to the stationary Poisson equation, the advection-diffusion equation is time-dependent. Thus, we use the implicit backward Euler scheme for a stable unfolding of the simulation in time. For every simulation time step, we perform a certain number of optimization steps to reduce L. By choosing the right number of iterations per time step we can make a trade-off between speed and accuracy. Figure 5 b) shows how Metamizer automatically adjusts the scaling $s_i$ when simulating the advection-diffusion equation at 10 iterations per time step. At the beginning of each time step, the neural optimizer increases its stepsize to quickly adjust u. Then, $s_i$ is decreased again for fine adjustments.\nFigure 1 shows results for a constant localized source field R and a rotating velocity field $\\vec{v}$ (for example a laser that heats a spinning disk). More examples with different diffusivity parameters are"}, {"title": "WAVE EQUATION", "content": "The wave equation is an important hyperbolic PDE that describes wave like phenomena such as water waves, pressure waves, electro-magnetic waves and many more. It can be written as:\n$F = \\partial^2_{tt} u - c^2\\partial^2_{xx} u = 0$\nHere, c corresponds to the wave propagation speed. Figure 7 shows results for different values of c that were obtained by Metamizer using an implicit Crank-Nicolson scheme. Note, that the wave equation was not included during training. Table 1 shows accuracy levels for 1,5,20 and 100 iterations per timestep. Using only 1 iteration results in divergent simulations, 20 iterations yield fairly accurate results and at 100 iterations, the simulation runs close to machine precision."}, {"title": "NON-LINEAR PDES", "content": "Non-linear PDEs are particularly hard to solve as the underlying systems of equations become non-linear as well. In this section, we demonstrate Metamizers capabilities to deal with non-linear PDEs."}, {"title": "INCOMPRESSIBLE NAVIER-STOKES EQUATION", "content": "The incompressible Navier-Stokes equation describes the dynamics of an incompressible fluid by means of a velocity field $\\vec{v}$ and a pressure field p:\n$F = \\rho (\\partial_t \\vec{v} + (\\vec{v} \\cdot \\nabla) \\vec{v}) + \\nabla p - \\mu \\triangle \\vec{v} - \\vec{f}_{ext} = 0$\nHere, $\\rho$ denotes the fluids density, $\\mu$ its viscosity and $\\vec{f}_{ext}$ external forces. The incompressibility equation $\\nabla \\cdot \\vec{v} = 0$ can be automatically fulfilled by utilizing a vector potential $\\vec{a}$ and setting the velocity field $\\vec{v} = \\nabla \\times \\vec{a}$. However, using a vector potential prohibits us from directly manipulating the velocity field. Thus, we need an additional boundary loss term (Equation 4) to deal with the Dirichlet boundary conditions for the velocity field at the domain boundaries. To compute the partial derivatives of $\\vec{a}$, $\\vec{v}$ and p efficiently with centered finite differences, we rely on a staggered marker and cell (MAC) grid (Harlow et al., 1965; Holl et al., 2020; Wandel et al., 2021a).\nFigure 8 shows results for various Reynolds numbers. The Reynolds number $Re$ is a unit less quantity defined as:\n$Re = \\frac{\\rho ||\\vec{v}|| D}{\\mu}$\nwhere $D$ corresponds to the diameter of the obstacle and $|\\vec{v}|$ to the flow velocity. The Reynolds number has a big effect on the qualitative behavior of the flow field. For example, in case of very small Reynolds numbers, the flow-field becomes stationary and the Navier-Stokes equation can be approximated by the Stokes equation. In case of very high Reynolds numbers, the flow-field becomes turbulent and can be approximated by the Euler equation for inviscid fluids."}, {"title": "BURGERS EQUATION", "content": "The 2D coupled Burgers equation is an important non-linear PDE to study the formation of shock patterns.\n$\\vec{F} = \\partial_t \\vec{v} + (\\vec{v} \\cdot \\nabla) \\vec{v} - \\mu \\triangle \\vec{v} = 0$\nHere, $\\mu$ is a viscosity parameter. Figure 1 shows an exemplary simulation result by Metamizer that exhibits clear shock discontinuities at $\\mu = 0.3$. Additional qualitative results are shown in Figure 10 of Appendix A. Note that the Burgers equation was not considered during training. Mean squared residuals for different iterations per time step are shown in Table 1."}, {"title": "CLOTH DYNAMICS", "content": "To describe cloth dynamics, we closely follow the approach of Santesteban et al. (2022) and Stotko et al. (2024). Instead of using a classical PDE formulation, their approach relies on a spring-mass system consisting of a regular grid of vertex positions $\\vec{x}$, velocities $\\vec{v}$ and accelerations $\\vec{a}$ that get integrated by a backward Euler scheme:\n$\\vec{v}_t = \\vec{v}_{t-1} + \\Delta t \\vec{a}_t \\quad \\text{and} \\quad \\vec{x}_t = \\vec{x}_{t-1} + \\Delta t \\vec{v}_t$\nTo obtain the accelerations $\\vec{a}_t$, the following physics-based loss L must be minimized by Metamizer:\n$L = E_{int}(\\vec{x}) - \\Delta t^2 \\langle \\vec{F}_{ext}, \\vec{a}_t \\rangle + \\frac{1}{2} (\\Delta t)^2 \\langle \\vec{a}, M \\vec{a} \\rangle$\nHere, $\\vec{F}_{ext}$ corresponds to external forces (such as gravity or wind), M is the mass matrix and $E_{int}$ are the internal energies of the cloth specified as follows:\n$E_{int} = C_{stiff} \\sum_{E_{edges}} (|e| - 1)^2 + C_{shear} \\sum_{E_{straight \\ angles}} (\\psi - 90\u00b0)^2 + C_{bend} \\sum_{E_{right \\ angles}} (\\theta - 180\u00b0)^2$\nwhere $C_{stiff}, C_{shear}, C_{bend}$ are parameters that describe stiffness, shearing and bending properties of the cloth."}, {"title": "CONCLUSION", "content": "In this work, we introduced Metamizer, a novel neural optimizer for fast and accurate physics simulations. To this end, we propose a scale-invariant architecture that suggests improved gradient descent update steps to speed up convergence on a physics-based loss at arbitrary scales. Metamizer was trained without any training data but directly on the mean square residuals of PDEs such as the Laplace, advection-diffusion or Navier-Stokes equations and cloth simulations in a meta-learning manner. Remarkably, our method also generalizes to PDEs that were not covered during training such as the Poisson, wave and Burgers equation. By choosing a proper number of iterations per timestep, a trade-off between speed and accuracy can be made. Our results demonstrate that Metamizer produces fast as well as highly accurate results across a wide range of physical systems.\nIn the future, further physical constrains such as Neumann or Robin boundary conditions and self-collisions for cloth-dynamics could be included. For increased flexibility regarding the underlying data structure, the network architecture could be extended to Mesh- or Graph-Neural Networks. Furthermore, multiple PDEs such as the Navier-Stokes and advection-diffusion equation could be coupled.\nWe believe that our approach may have applications in future numerical solvers and speed up accurate simulations in computer games or computer-aided engineering. Since gradient descent approaches are ubiquitous, our approach may also serve as an inspiration for other gradient-based applications outside of physics, such as style transfer, scene reconstruction, or optimal control."}, {"title": "QUALITATIVE RESULTS FOR BURGERS, ADVECTION-DIFFUSION AND POISSON EQUATION", "content": "In this Section we provide additional qualitative results for Burgers equation with $\\mu$ = 0.3/1 (Figure 10), the advection-diffusion equation with D = 0.1/0.5/2/10 (Figure 11) and the Poisson equation (Figure 12)."}, {"title": "PERFORMANCE COMPARISONS ON A LARGER GRID", "content": "In order to investigate the scaling behavior of Metamizer, we trained another network on a 400 \u00d7 400 grid to solve the Laplace equation. Figure 13 shows that Metamizers advantage over traditional"}, {"title": "SCALING PARAMETERS", "content": "In Figure 14, we show additional results how Metamizer scales $s_i$ for 3, 5, 10, 20, 50, 100 iterations per timestep on the Advection-Diffusion equation. For 50 and 100 iterations, a significant amount of iterations is \"wasted\" to scale up $s_i$ for the next timestep. This could be improved by automatically scaling $s_i$ to a certain percentage (e.g. 20 %) of the maximum scale of the previous time step when starting with a new time step."}, {"title": "LAPLACE OPERATOR DETAILS", "content": "During our experiments, we found that naively applying the finite-difference Laplace operator results in very small gradients even if the residuals are still quite large. Intuition on this problem is provided in Figure 15: If we want to lower the Laplacian for the center vertex (marked in black), we could increase the value of that vertex (marked in green) but we could also decrease the values of its neighboring vertices (marked in red). The same is true for all vertices, such that in the end the gradients pointing upwards get canceled out fairly exactly be the neighboring gradients pointing downwards. To avoid this issue, we detach (and thereby remove) all the neighbor-gradients (marked in red) so that we are only left with the green gradients and can make much better progress in solving the Laplace operator."}, {"title": "VIDEO", "content": "The supplementary video demonstrates how Metamizer solves the Laplace, diffusion-advection, wave, Navier-Stokes and Burgers equations as well as cloth simulations. The time-dependent PDEs are visualized at the speed of the simulation."}]}