{"title": "On the Promise for Assurance of Differentiable Neurosymbolic Reasoning Paradigms", "authors": ["Luke E. Richards", "Jessie Yaros", "Jasen Babcock", "Coung Ly", "Robin Cosbey", "Timothy Doster", "Cynthia Matuszek"], "abstract": "To create usable and deployable Artificial Intelligence (AI) systems, there requires\na level of assurance in performance under many different conditions. Many times,\ndeployed machine learning systems will require more classic logic and reasoning\nperformed through neurosymbolic programs jointly with artificial neural network\nsensing. While many prior works have examined the assurance of a single compo-\nnent of the system solely with either the neural network alone or entire enterprise\nsystems, very few works have examined the assurance of integrated neurosymbolic\nsystems. Within this work, we assess the assurance of end-to-end fully differen-\ntiable neurosymbolic systems that are an emerging method to create data-efficient\nand more interpretable models. We perform this investigation using Scallop, an\nend-to-end neurosymbolic library, across classification and reasoning tasks in both\nthe image and audio domains. We assess assurance across adversarial robustness,\ncalibration, user performance parity, and interpretability of solutions for catching\nmisaligned solutions. We find end-to-end neurosymbolic methods present unique\nopportunities for assurance beyond their data efficiency through our empirical\nresults but not across the board. We find that this class of neurosymbolic models\nhas higher assurance in cases where arithmetic operations are defined and where\nthere is high dimensionality to the input space, where fully neural counterparts\nstruggle to learn robust reasoning operations. We identify the relationship between\nneurosymbolic models' interpretability to catch shortcuts that later result in in-\ncreased adversarial vulnerability despite performance parity. Finally, we find that\nthe promise of data efficiency is typically only in the case of class imbalanced\nreasoning problems.", "sections": [{"title": "1 Introduction", "content": "The need to assure the performance and deployment of Artificial Intelligence (AI)-enabled systems\nis hitting a peak as many demonstrations of the technology have shown applicability to a growing\nnumber of tasks in a plethora of domains. Modern methods, especially in Machine Learning\n(ML) model development, focus on advances in deep learning or neural networks. Such models\ncreate many opportunities but also obfuscate our ability to model behavior for testing assurance\nencompassing safety, security, cross user-performance, and interpretability. When deploying AI to\ncritical applications like controlling industrial systems, autonomous operation, and other applications\nwith risks for people and their environments, stakeholders are rightfully demanding higher levels of\nassurance. The past decade in particular has outlined many risks of lack of cross user-performance\n[11], lack of robustness to data drift [35], and security vulnerabilities [7] for systems using machine\nlearning."}, {"title": "2 Related Work", "content": "Prior neurosymbolic surveys [2] define how neurosymbolic AI fits into definitions of trustworthiness\nwith overlapping themes of assurance, including oversight, cross-user performance, and robustness.\nWhile there have been a plethora of claims on the trustworthiness of neurosymbolic systems [38],\nthere is a lack of a full empirical exploration of their assurance across multiple axes. Prior work has\nexamined the promises of robust solutions in the symbolic space [21] and found that neurosymbolic\nsystems would routinely find shortcuts in reasoning. Other works [33] have examined the adversarial\nrobustness of systems with deep learning models that contain intermediate symbolic representations,\nfinding that the intermediate representation followed by another neural network was more adversarially\nrobust. Prior work [30] has also examined the robustness of Symbolic Knowledge Injection methods\nto common perturbations in the input and output space for tabular data. Neurosymbolic systems\nhave been explored for safety modeling [40] by differentiating through neurosymbolic systems for\nverifiable safety and assurance of worst-case scenarios. While many of these papers either mention or\nlook at one dimension of assurance, a recent survey [23] points out a similar lack of work outside\nmeasuring interpretability compared to other measurements of assurance. Closely related to this\ninvestigation into end-to-end neurosymbolic programming, recent work [29] examines how to expand\nScallop to provide explanations towards interpretability of the answers provided by neurosymbolic\nmethods."}, {"title": "3 Background", "content": "In this section, we cover the core concepts of differentiable neurosymbolic reasoning paradigms and\nthe measurements of assurance relevant to these models and their fully neural counterparts."}, {"title": "3.1 Differentiable Neurosymbolic Reasoning", "content": "We address the problem where we would like to complete a task by mapping inputs, $x$, to outputs,\n$y$. We aim to achieve this through a learned function, $M$, combined with a symbolic program, $P$.\nTypically, $M$ will be learned through deep learning by parameterizing an artificial neural network with\nweights $\\theta_M$. We assume that the outputs of $M(x)$ are then processed with a symbolic and defined\nprogram $P$, such that $P(M_{\\theta}(x)) = y$. Generally, neurosymbolic methods separate perception of\nhigh-dimensional data to $M$ and reasoning to the program $P$. We seek to learn this function through\ngradient descent, optimizing $\\theta$ to perform well given a dataset $D$ of $(x, y)$. However, complexity\narises in how we define $P$ with the requirement to achieve a gradient.\nMany approaches have been proposed, with the most straightforward being the application of\nREINFORCE [39] to optimize the outputs given samples drawn from $M_{\\theta}$ and fed into a black-box\nprogram $P$. This provides a reward for the correct answer. However, this creates a weaker signal\nfor the learning process and requires implementation that may not be as accessible to practitioners\nusing common differentiable programming frameworks that perform automatic differentiation, like\nTorch [28]. In this vein, works have implemented probabilistic reasoning domain-specific languages\nto both take advantage of the probability outputs and create differentiable operations in the symbolic\nprogram. We focus on methods that enable end-to-end differentiation without using reinforcement\nlearning.\nWe categorize these methods as having full coverage of both the answer and potential solutions, such\nas DeepProbLog [20]; partial coverage, where the depth of potential solutions is controlled, such as\nScallop; and sample-based, where samples of the answer and potential solutions are approximated.\nIn theory, while each method has slight differences, all attempt to approximate modeling many\npotential outcomes given probabilities. However, as the number of outcomes and potential solutions\nincreases, the computational cost increases dramatically. This is detrimental for learning $\\theta$ since it\ntypically requires many iterations over the data. Partial-coverage methods attempt to resolve this by\nlimiting computation while still preserving the solution space. Each iteration of these methods has\nshown improvement, where previous ones have simply timed out [18, 34] when the complexity space\nincreases.\nDue to these factors, within this work, we examine Scallop as an intermediate method that still\nincorporates probabilistic reasoning while working to limit the computational intensity. While\nmethods such as ISED [34] enable more flexible program definitions through differentiating a black-box program, we seek to understand the benefits of intermediate probabilistic reasoning given the\nconstraints of Scallop's domain-specific language.\nScallop is built on Datalog [1], extended with probabilistic reasoning using provenance semirings.\nThese semirings enable compositional computation on the top-k proofs, creating a differentiable\nfunction that can be used in end-to-end deep learning frameworks like PyTorch. The proofs are then\ncalculated with success probability rather than discrete outputs for every potential proof result. An\nexample would be adding two handwritten digits (digit\u2081 + digit\u2082) of value 0 to 2, where the potential\nanswers would be 0 to 4. A proof would be calculated for each of these potential answers given the\npotential for digit\u2081 and digit2 to be 0 through 2. The value $k$ then determines for each digit how\nmany combinations ranked by likelihood of the joint probability between the digits would be used\nfor the global probability of solutions. Thus at $k = 1$, we would only calculate the first most likely\nsolution. For an example and comparison of how each method in the neurosymbolic programming\nspace works, please see the appendix of the work introducing ISED [34]. For a deeper dive into the\nexact mechanics, we defer to the introductory paper [18]."}, {"title": "3.2 Measurements of Assurance", "content": "Within our analysis, we examine various axes of assurance, which can at times be a loaded term. We\nfollow the definition of the assurance of artificial intelligence systems found through rigorous analysis\nof requirements stated in various documents and research detailing requirements for applications of\nAI in critical domains [5]. We break down how, for this study, we measure the various aspects:\nInterpretability: We seek to have models that can be mechanistically interpreted by developers,\nusers, and auditors of the systems. Classically, this means we can understand why a system has made\na decision. Modern deep learning models are black boxes when we treat the entire model process as\na neural function. This reactionary or single-pass information processing is less interpretable. This\nalso makes the assessment of alignment of the model problem-solving method more challenging. In\ncontrast, when compared to treating deep learning as a function that feeds into a reasoning module,\nwe can more easily debug, diagnose, and make informed decisions on deploying a system. For\nthe evaluation of these methods, we rank them as more interpretable if we have some intermediate\nrepresentation and trace of how the AI system came to the answer. Thus, neurosymbolic methods\nnaturally will be more interpretable than their fully neural counterparts due to the symbolic outputs.\nConfidence Calibration: Many models attempt to properly capture the probability of a given event\nusing statistical modeling. Systems that can capture uncertainty can better defer to human operators\nor not take action in a given situation. Having high confidence calibration enables systems to be\ndeployed with thresholds that are meaningful rather than spuriously related to misalignment with\nthe true underlying probability. We report on our classification models with the metrics of expected\ncalibration error (ECE) and maximum calibration error (MCE) [15]. ECE calculates the weighted\naverage of the absolute difference between accuracy and confidence across all bins. MCE differs in\nthat it considers the maximum absolute difference across all the bins. Therefore, ECE is a measure of\noverall calibration, while MCE provides a measure of the worst-case calibration performance. These\nvalues will be between 0 and 1, with values closer to 0 indicating higher calibration.\nCorruption Robustness: Generalization beyond the exact environment in which the model was\ntrained is the overarching goal of ML. However, models are commonly confused by perturbations\n(such as an image being flipped or having parts missing). We seek to measure the performance loss\ngiven a suite of functional perturbations per task. This is the percentage loss of accuracy performance\ncalled corruption success rate (CSR), $CSR = \\frac{acc_{cor}}{acc}$, where $acc_{cor}$ is the accuracy under corruption.\nAdversarial Robustness: The ability to purposefully manipulate the performance or operation of\nan AI system at deployment poses a significant safety risk. We define two threat models: one in\nwhich the adversary has no access during training time, so they seek to evade the correct classification\nusing adversarial example generation with white-box access to the model (including the symbolic\ncomponents). We examine a second threat in which the adversary can manipulate a percentage of\ntraining data during training in an attempt to create a backdoor trigger that can be used at deployment\ntime. For each of these, we measure the adversary success where, at deployment time, every example\nis attacked with either the adversarial perturbation developed solely at deployment or the one added\nduring training. We define adversarial success in line with [27] as $\\frac{acc - acc_{adv}}{acc}$, resulting in a percentage\nof the accuracy that was degraded.\nUser-Performance Parity: Models should be performance invariant to the user's characteristics.\nHowever, ML models are commonly observed to have disparate performance [41]. This poses\nsignificant challenges in deploying systems that will be interacting with users, posing ethical, legal,\nand sensing coverage risks. Commonly, data underrepresented in the training group has lower\nperformance than those with majority representation. We measure the accuracy disparity among users\nby mapping each subgroup to either the majority or minority represented meta-group. We then report\nthe parity of accuracy through performance parity, $|acc_{maj} - acc_{min}|$, where the goal is complete\nparity with a value of 0."}, {"title": "4 Experiments", "content": "As mentioned, we use both PyTorch [28] and Scallopy [18] Python packages to define and train\nstandard fully neural networks and neurosymbolic networks made of both neural network and\nsymbolic reasoning components. We analyze multiple variants of Scallopy neurosymbolic models,\neach differing with respect to the hyperparameter k, which specifies the level of reasoning granularity,\nallowing for a relaxation of exact probabilistic reasoning by specifying the number of top-k most\nlikely proofs to consider."}, {"title": "4.1 MNIST Logic Tasks: Multi-Image Logic Tasks", "content": "We start with a simple series of arithmetic and logical tasks applied to the MNIST number recognition\ndataset. These include the sum digits task and the how many 3 or 4 task. The sum digits task\nrequires prediction of the sum of all the digits represented in selected MNIST images. In contrast, the\nobjective of the how many 3 or 4 task is to count the number of images containing the numbers 3 or\n4. For each task, we defined three different variants, where the number of images drawn can be 2,\n3, or 5. Each of these represents a challenging task, in particular, sum digits with 5 images creates\nan unbalanced dataset with larger and smaller sums being less represented, which is particularly\nchallenging for deep learning models.\nWe deploy two types of models for each task both with a base 3-layer residual convolutional network\n(CNN) [16]. The first type of model serves as a baseline for a neural-only approach. This model\nconsists of two modules. The first module comprises multiple convolutional and linear layers,\nembedding an image into a representation of size d = 1024. Next, the embedding is fed as input\nto the second module, which is a two-layer multi-layer perceptron (MLP). The task of the second\nmodule is to categorize N classes in a specific task. We refer to this type of model as neural network\n(NN) for the rest of the paper. The second type is the neural symbolic model in which we replace the\nMLP with a symbolic program that performs the operation per task. We call this type of model NESY\nk = x, where x represents the number of logical reasoning steps. For our adversarial robustness\nevaluation, we use the $L_\\infty$-norm with a budget of 0.03 and 100 steps to ensure convergence of the\nattack. For the corruption robustness, we use the corrupted datasets from MNIST-C [24]. This totals\n15 corruptions (such as fog, rotate, motion blur, and scale) with only one level of intensity."}, {"title": "4.1.1 CIFAR-10: Knowledge-base Reasoning for Classification", "content": "Knowledge bases commonly capture high-level concepts and associations that may or may not be\nvisually grounded. Neurosymbolic methods, in particular, can incorporate these data structures to\naugment background information a model may not be able to pick up from data alone. We investigate\nthe role that such knowledge bases could play by selecting the fewest number of concepts needed to\ndisentangle all CIFAR-10 classes from one another while ensuring concept sharing. An example of\nsuch reasoning would include planes and ships sharing the concept \u201cis transportation\" but differing\nin \"has wheels\". While such background information may not always be visually grounded, such\nas a plane being pictured with wheels withdrawn, the background knowledge can enable models to\nincorporate such information in the representation.\nFor this reasoning task, a ResNet18 [16] was used. Neural and neurosymbolic models were trained\nwith a learning rate of 0.001. The fully neural model was trained to predict the 10 CIFAR classes. The\nneurosymbolic variant was set to predict 11 concepts that were reasoned over through the knowledge\nbase (for details see Appendix A.2). The four symbolic models were trained, with k = 1, 3, 5, and\n10. For our adversarial robustness evaluation on all models, we use the $L_\\infty$-norm with a budget of\n0.03 and 100 steps. For the CSR, we use the CIFAR-10-C corrupted dataset [17] with 19 corruptions,\nsuch as Gaussian noise, motion blur, and JPEG compression, at 5 levels each."}, {"title": "4.1.2 LEAF-ID: Grounded Attribute Reasoning for Classification", "content": "Opposed to the prior experiments in image classification operating on high-level knowledge base\nfeatures, the LEAF-ID dataset offers visually grounded concepts to perform intermediate reasoning.\nThese features include shape, margin, and texture used in prior work in neurosymbolic models [34].\nWe use a base convolutional network (CNN) model that outputs these characteristics with 3 linear\nheads for each concept (5 shape, 6 margin, and 4 texture concepts). This is a modified version from\nthe prior work which used 3 CNN backbones per concept. For our fully neural model, we use the\nsame CNN that predicts directly for the 11 classes. We train for 200 epochs, with a learning rate of\n0.0001. We use the same equal data sampling for each of the classes pulling in a total of 40 images\nper class as done in prior work [23]. With such few examples per class, this task represents the lowest\ndata paradigm.\nWe evaluate the corruption robustness by applying corruptions available from prior work [17]. To\nevaluate the adversarial robustness, we found the models to be less robust to typical strength attacks\nwith 100% ASR when the budget was 0.03 for an $L_\\infty$ so we decrease to where models show some\nlevel of robustness to 0.007 (2/255) with 200 steps. For the LEAF-ID task, we generate 19 distortions\nat 5 levels using the imagecorruptions package [22], which adapts the original code from [17] to\naccommodate rectangular images."}, {"title": "4.1.3 Pathfinder: Pixel-Level Reasoning", "content": "We evaluate the robustness of operating directly in grounding to the pixel space with the Pathfinder\ntask [36]. For both models we use the base 3-layer residual CNN from the MNIST tasks. For\nour fully neural model, we use a linear layer on top of the convolutional layers to perform binary\nclassification. For the symbolic model, we use two linear heads for dot and path facts. One linear head\nproduces a dot score per pixel, with 1024 neural facts (for a 32 x 32 image). The second models the\nadjacency matrix where each pixel coordination that is connected to another is represented, resulting\nin 1,984 facts. We reduce the total number of facts from the original paper by duplicating edges in\nthe adjacency matrix due to the connections not being directed. Due to data size and the speed of\nevaluating the neurosymbolic models, we limit our adversarial attacks using PGD with $L_\\infty$ to 10\nsteps and a budget of 0.03."}, {"title": "4.1.4 Speech Word Classification: Common Voice Clips", "content": "To further our coverage of modality beyond the commonly analyzed image modality, we also explored\nassurance with speech datasets. Commonly these datasets include users of various backgrounds\nspeaking a single word. The goal is then to train a model to perform classification to a closed-set\nnumber of discrete words. This differs from speech-to-text or transcription tasks which seek to\nclassify multiple words in sequence. As models that perform speech transcription grow in size and\ncompute cost, smaller models for trigger words are commonly used to save costs. This enables a\nlower barrier of entry for the study of non-image modalities in robustness and assurance.\nCommonly, an end-to-end deep learning approach is applied directly to the speech signal to then\nperform a classification. However, translating the classic deep learning approach to neurosymbolic\nis straightforward. This is due to speech itself having a symbolic representation with the phonemic\nalphabet. Using phonemic elements is not a novel approach as prior state-of-the-art models, i.e.\nwav2vec2.0 [4] used such information bottlenecks to capture this inductive bias.\nWe focus on Common Voice Clips, a collection of single-word speech commands collected from\nusers through an online platform. The classes include digits zero through nine, \"hey\", \"yes\", and\n\"no\", resulting in 13 classes. We remove \u201cMozilla\" from the dataset due to its more unique phonemic\nnature and not to skew our short word modeling. We use an M5 [10], a five-layer one-dimensional\nCNN with a sample rate of 16 kHz and use five seconds of audio from the clip. We use a symbolic\nprogram to outline the modeling task with a maximum of six slots for phonemes. We then train a\nneural network to output 144 facts (max length of 6 \u00d7 24 phonemes needed to model the space). We\ntrain both a classic neural and neurosymbolic models at a learning rate of 0.0001 for 250 epochs. We\nuse Cross Entropy Loss for the neural model and Binary Cross Entropy for the neurosymbolic model.\nWe train and test with various k values of 3, 10, 15, and 20. We perform our adversarial example\ngeneration with PGD with $L_\\infty$-norm with a budget of 0.001 and 200 steps to ensure convergence.\nCommon Voice Clips also have user labels that allow us to measure a key assurance metric of\nperformance disparity across users. We evaluate on the English-speaking subset of the dataset. As\nmentioned before, the goal is to assure performance across all users independent of their training data\nrepresentation."}, {"title": "5 Results", "content": "In this section, we break down the results for the two MNIST logic tasks, CIFAR-10, LEAF-ID,\nPathfinder, and Common Voice Clips. We also present results analyzing the role of $k$ for the NESY\nmodels during test-time with respect to assurance measurements (Section 5.7). Finally, we present\nresults for how well various NN and NESY methods compare in low-data environments (Section 5.8)."}, {"title": "5.1 MNIST Logic Tasks: Sum Results", "content": "We see that when 2 and 3 images are input, the accuracy of all models is nearly equal, with there\nbeing little to no difference in performance (Table 1). Due to this high accuracy, we see that the\nECE scores are nearly identical as well, with the neurosymbolic models having varied MCE scores,\ntypically being outperformed by the neural method except for the NESY k = 5 case on 3 images. For\n2 and 3 images, we see a difference in the ASR between the two methods, with the NESY models\nhaving lower rates than the NN models. This is supported by the accuracy parity across models, with\nNN on 3 images having about 3 times the ASR as NESY. We notice here that there is a slight 0.001\nincrease in ASR when comparing $k = 3$ and $k = 5$. We later analyze the role of $k$ at test-time in\nSection 5.7.\nNotably, when we increase the number of images to 5, we see a complete drop-off in performance for\nthe NN model. This partly reflects the challenges of class imbalance mentioned earlier. The NESY\nmodels across the board outperform in every metric except MCE. We observe that due to the outputs\nof the NESY methods, MCE touches on an issue where the methods do not output a distribution for\nall combinations equal to 1, thus being penalized by this measurement even when performance is\ndrastically higher. Even with the high accuracy, we see that the ASR for the NESY models is much\nlower, about 10-11%, compared to the extremely high 99% of the NN model given the budget. This\nindicates that neurosymbolic methods may offer more adversarial robustness when the combinatorics\nof the problem are high, there is data imbalance, and the neural architecture's learned logic operation\nin the MLP is increasing in dimensionality. The results for corruption robustness follow a similar\ntrend, with corruption success being about 11% lower for neurosymbolic methods while maintaining\nhigher or equal accuracy. This result follows that the neurosymbolic operation is more robust than\nthe fully learned method."}, {"title": "5.2 MNIST Logic Tasks: How Many 3 or 4 Results", "content": "For the results for how many 3 or 4 (Table 2), we see parity across accuracy at all number of images.\nThis enables a better comparison rather than the traditional neurosymbolic \"can\" and neural \"cannot\"\nanalysis done historically in the literature and also seen in sum digits results section. We mostly\nsee parity across the ECE calibration metric while observing again the disadvantage that the NESY\nmodels have for MCE while typically having less calibrated scores. However, when comparing the\nASR, we again see a trend that the NESY models, at all number of images and all k values, have\nlower ASRs. We do not see a strong trend for k being a determining factor for more adversarial\nrobustness. However, there is a consistent and scaled ASR difference across tasks as the number\nof images increases. This again is in part because of the vulnerability of learning these symbolic\noperations. Again, the CSR is always lower for the NESY models while having near equal accuracy\nperformance. We see a smaller gap (between 4-7%) between the NN and NESY methods than CSR\nin the sum task until the number of images becomes 5, then we observe almost a two times decrease\nin performance. This again follows the idea that increasing the dimensionality of the operation space\nincreases the potential for issues when there is a distribution shift."}, {"title": "5.3 CIFAR-10 Results", "content": "The NN model outperformed all NESY variants on overall accuracy by approximately 6% (see\nTable 3). We observe that the calibration metrics tell a similar story, with NN models always better\ncalibrated than NESY models, both by ECE and MCE metrics. Adversarial attacks impacted all\nCIFAR models similarly, with nearly 100% ASR. While most of the ASRs are slightly lower for the\nNESY models, the overall accuracy under attack is still higher for the NN model. We see a similar\ntrend with some amount of noise where the CSR is lower for NESY models, resulting in lower overall\naccuracy."}, {"title": "5.4 LEAF-ID Results", "content": "For LEAF-ID, the results (Table 4) show that there is a gap in accuracy between Neural and NESY\n$k = 3, 10$. However, NESY $k = 5$ obtains close performance at 85% relative to the NN model's 86%.\nThe higher $k$ value, the better the calibration becomes, with NESY $k = 10$ being slightly better in\ncalibration through ECE. Meanwhile, we see MCE having poorer performance as $k$ increases, with a\nslight dropoff at $k = 10$. For ASR, the NN has more robustness and accuracy under attack. There is\nan unclear relationship between $k$ and each attribute beyond ECE calibration. Notably, NESY $k = 5$\nhas 61.6% accuracy under corruptions while the NN model has 60.2%. However, due to these small\ndifferences, we do not see large gains with having the attribute grounding."}, {"title": "5.5 Pathfinder Results: Issues with Direct Pixel Grounding: Detecting Shortcuts in Symbolic System", "content": "While not explored in the initial work [18] on this dataset and task, we evaluate the interpretability of\nthe model results. We examine how the predicted dots and connections correspond to actual dots and\nlines in the images. Through plotting these results, we find that models routinely and across multiple\n$k$ values use shortcuts in the symbolic space to accomplish the goal, similar to those reported in [21].\nWhile the accuracy is sufficient, the models are using simple circuits in a stable region across all\nimages. The model's behavior, visualized here, involves turning on and off a single dot to perform\nclassifications. Thus, we find that neurosymbolic models operating in this pixel space ineffectively\nground the symbols at this level.\nThis behavior signals the need for additional interventions for training, beyond simply being able\nto plug-and-play symbols grounded to pixels. Some interventions that could be pursued include\nregularization-based methods to reward unique symbolic representations between examples, and\narchitectural methods to limit the plug-and-play nature of the neurosymbolic paradigm. Mitigation for\nsuch behavior is outside the scope of this initial analysis. However, the ability to directly observe this\nmisalignment in how the model is solving the problem is a key point in ensuring models are operating\nas expected from a mechanistic standpoint rather than post hoc. Despite this misalignment, we report\nthe performance of the neurosymbolic model with an observation on the relationship between this\nphenomenon and adversarial robustness.\nWe note that the accuracy for both NN and NESY models is nearly equal (Table 5). This is supported\nby the fact that NESY models are learning a shortcut in the neural space. The calibration metrics\nare also more comparable. This may be in part because this is a binary classification task. However,\nthe main difference we see is in the ASR. We observe that the relationship between interpretability\nand the detection of a symbolic shortcut is closely related to poor adversarial robustness, with a\ngeneral almost complete degradation in accuracy under attack, compared to the ASR of 71% for\nthe NN model. This would likely be reflected in the CSR as well, as the concepts are not robust to\nperturbation."}, {"title": "5.6 Common Voice Clips Results", "content": "The results (Table 6) for speech command models indicate that while there is a slight increase in\nperformance with respect to accuracy with $k = 15$ and 20, the calibration and adversarial success\nscores for symbolic models do not improve. For adversarial success, we see a consistent 20% increase\nacross all models. For calibration metrics, our ECE results begin to converge closer to the fully neural\nmethod as we increase the $k$ value. However, while MCE decreases as we increase $k$, it is still not\nclose to the fully neural model.\nOne area where we see improvement and have a consistent trend as we increase $k$ is achieving more\nparity across performance for majority and minority users. This result, coupled with a small increase\nin accuracy, shows a lack of the common accuracy and performance trade-off commonly seen in the\nliterature [41, 26, 37, 19]."}, {"title": "5.7 Role of $k$ in Test-Time Assurance", "content": "While the original Scallop work [18] noted that they found little difference in accuracy performance,\nwe explore how this calculation of the solution space influences other assurance metrics. In particular,\nwe note that for adversarial attacks, including information on low-probability solutions could enable\nfurther amplification through each attack optimization step. We explore this concept across MNIST\nlogic tasks, CIFAR-10 image classification, LEAF-ID, and Common Voice Clips audio classification.\nOur results point to little to mostly no difference when applying a lower $k$ at test time compared to\ntrain time. This is seen across both MNIST logic tasks (Table 12, 13) and CIFAR-10 (Table 14) with\ndifferences near zero across all metrics except for a 0.1% decrease in ASR when using only the top\nproof at test time. In LEAF-ID (Table 15), we see no difference as well. For Common Voice Clips\n(Table 16), we observe no to little differences in terms of the assurance results. We see a small 1%\nincrease in adversarial success for $k = 15$ but little to no difference for $k = 20$. We do see for $k = 20$\nthe model has a decrease in user disparity by 0.04, which reduces it to lower than any of the models\ntrained. However, we do not see this for $k = 15$, which limits our ability to have a strong takeaway."}, {"title": "5.8 Data Efficiency and Assurance", "content": "A major claim for neurosymbolic systems is their data efficiency. However, the majority of per-\nformance measurements so far have focused on accuracy alone. In this section, we examine how\nassurance measurements vary for both fully neural approaches and neurosymbolic methods. We\nuse the same testing set with the same configurations as described in the previous section but limit\nthe amount of training data. We hold the $k$ value stable across all models for $k = 3$ for MNIST,\nCIFAR-10 at $k = 3$, and the Common Voice Clips dataset at $k = 15$. We exclude results on LEAF-ID\ndue to the dataset already being a scarce data problem."}, {"title": "5.8.1 MNIST Logic Tasks Results", "content": "The arithmetic sum digits shows the most drastic difference between methods in their performance\nin accuracy in the small data 5% case. The neurosymbolic method quickly reaches an accuracy in\nthe 90% range while the neural method requires 25% of the data before it reaches that performance.\nThis indicates that neurosymbolic methods perform best when implementing arithmetic or known\nsymbolic functions. We see that the neurosymbolic method's ECE and MCE calibration metrics\nfollow a trend of being lower due to the gap in performance. However, MCE still remains a metric\nwhere fully neural methods perform better even with lower accuracy. For the adversarial success\nrate (ASR), we see a drastic difference early, with the neurosymbolic method maintaining high\nperformance across the board, with the highest value being 11% at only 5% of the data, while the\nlowest for the neural method is 35% at 25% of the data. This again points to the strength of having the\nsymbolic operation versus the learned function for smaller data. An interesting trend is NESY models\nsee more corruption robustness compared to the NN method increasing in CSR as data increases.\nAgain, this is in the context of overall higher accuracy with NESY models on the corrupted data\nstarting with 5% of the data.\nFor the second MNIST logic task of how many 3 or 4 , we observe that the fully neural method"}]}