{"title": "Learning Fair Invariant Representations under Covariate and Correlation Shifts Simultaneously", "authors": ["Dong Li", "Chen Zhao", "Minglai Shao", "Wenjun Wang"], "abstract": "Achieving the generalization of an invariant classifier from training domains to shifted test domains while simultaneously considering model fairness is a substantial and complex challenge in machine learning. Existing methods address the problem of fairness-aware domain generalization, focusing on either covariate shift or correlation shift, but rarely consider both at the same time. In this paper, we introduce a novel approach that focuses on learning a fairness-aware domain-invariant predictor within a framework addressing both covariate and correlation shifts simultaneously, ensuring its generalization to unknown test domains inaccessible during training. In our approach, data are first disentangled into content and style factors in latent spaces. Furthermore, fairness-aware domain-invariant content representations can be learned by mitigating sensitive information and retaining as much other information as possible. Extensive empirical studies on benchmark datasets demonstrate that our approach surpasses state-of-the-art methods with respect to model accuracy as well as both group and individual fairness.", "sections": [{"title": "1 Introduction", "content": "While machine learning has achieved remarkable success in various areas, including computer vision [14], natural language processing [5], and many others [10, 11, 35], these accomplishments are often built upon the assumption that training and test data are independently and identically distributed (i.i.d.) within their respective domains [33].\nHowever, models under this assumption tend to perform poorly when there is a distribution shift between the training and test domains. Addressing distribution shifts across domains and generalizing from finite training domains to unseen but related test domains is the primary goal of domain generalization (DG) [1].\nMany types of distribution shift are introduced in [20], such as label shift [34], concept shift [36], covariate shift [30], and correlation shift [28]. The covariate shift is defined as the differences in the marginal distributions over instances across different domains [30]. As shown in Figure 1, the two domains exhibit variations resulting from different image styles, represented by varying rotation angles. Correlation shift is defined as the variation in the dependency between the sensitive attribute and label across domains. For example, in Figure 1, it is evident that there is a strong correlation between the digit (3,6) and digit colors (green, red) when rotated at 30\u00b0, whereas this correlation becomes less pronounced at 60\u00b0.\nSince the correlation involves sensitive attributes, correlation shift is highly related to fairness. In the context of algorithmic decision-making, fairness means the absence of any bias or favoritism towards an individual or group based on their inherent or acquired characteristics [23]. Many methods have been proposed to address the domain generalization (DG) problem [1, 17, 18, 31, 39],"}, {"title": "2 Related Work", "content": "Algorithmic Fairness in Machine Learning. In recent years, fairness in machine learning has gained widespread attention. In this field, there is a widely recognized trade-off: enhancing fairness may come at the cost of accuracy to some extent [3, 24]. How to handle such a trade-off, especially in real-world datasets, has been a widely researched issue in the field of algorithmic fairness.\nFrom a statistical perspective, algorithmic fairness metrics are typically divided into group fairness and individual fairness. The conflict between them is a common challenge, as algorithms that achieve group fairness may not be able to handle individual fairness [15]. LFR [38] is the first method to achieve both group fairness and individual fairness simultaneously. It encodes tabular data, aiming to preserve the original data as much as possible while ignoring information related to sensitive attributes.\nFairness-Aware Domain Generalization. Some efforts [40\u201344] have already been attempted to address the fairness-aware domain generalization problem. EIIL [4] takes correlation shift into consideration when addressing the DG problem, thus ensuring fairness to some extent. FVAE [25] learns fair representation through contrastive learning and both improve out-of-distribution generalization and fairness. But both of them only take correlation shift into account while assuming that covariate shift remains invariant. The latest work FATDM [26] attempts to simultaneously enhance the model's accuracy and fairness, considering the DG problem associated with covariate shift. However, it does not consider correlation shift and solely focuses on group fairness, without addressing individual fairness."}, {"title": "3 Preliminaries", "content": "Notations. Let \\(X \\subseteq \\mathbb{R}^{d}\\) denote a feature space, \\(A = \\{-1,1\\}\\) is a sensitive space, and \\(Y = \\{0, 1\\}\\) is a label space for classification. Let \\(C \\subset \\mathbb{R}\\) and \\(S \\subset \\mathbb{R}\\) be the latent content and style spaces, respectively, induced from \\(X\\) by an underlying transformation model \\(T : X \\times X \\rightarrow X\\). We use \\(X, A, Y, C, S\\) to denote random variables that take values in \\(X, A, Y, C, S\\) and \\(x, a, y, c, s\\) be the realizations. A domain \\(e \\in \\mathcal{E}\\) is specified by distribution \\(P(X_e, A_e, Y_e): X \\times A \\times Y \\rightarrow [0, 1]\\). A predictor \\(f\\) parameterized by \\(\\Theta_{cls}\\) denotes \\(f : X \\times A \\times \\Theta \\rightarrow Y\\).\nProblem Formulation. We consider a set of data domains \\(\\mathcal{E}\\), where each domain \\(e \\in \\mathcal{E}\\) corresponds to a distinct data \\(D_e = \\{(x, a, y)\\}\\) sampled i.i.d. from \\(P(X_e, A_e, Y_e)\\). Given a dataset \\(D = \\{D_e\\}_{e \\in \\mathcal{E}}\\), it is partitioned into a training dataset \\(D_{tr} \\subset D\\) with multiple training domains \\(\\mathcal{E}_{tr} \\subset \\mathcal{E}\\) and a test dataset \\(D_{te} = D\\backslash D_{tr}\\) with unknown test domains which are inaccessible during training. Therefore, given samples from finite training domains, we aim to learn a fairness-aware predictor \\(f\\) at training that is generalizable on unseen test domains.\nPROBLEM 1 (DOMAIN GENERALIZATION CONCERNING FAIRNESS). Let \\(\\mathcal{E}_{tr} \\subset \\mathcal{E}\\) be a finite subset of training domains and assume that for each \\(e \\in \\mathcal{E}_{tr}\\), we have access to its corresponding data \\(D_e = \\{(x, y)\\}\\) sampled i.i.d. from \\(P(X^e, A^e, Y^e)\\). Given a loss function \\(\\ell_{CE} : Y \\times Y \\rightarrow \\mathbb{R}\\), the goal is to learn a fair predictor \\(f\\) parameterized by \\(\\Theta_{cls} \\in \\Theta_{fair} \\subset \\Theta\\) for any \\(D_e \\in D_{tr}\\) that minimizes the worst-case risk over training domains \\(\\mathcal{E}_{tr}\\) that\n\\[\n\\min_{\\Theta_{cls} \\in \\Theta_{fair}} \\max_{e \\in \\mathcal{E}_{tr}} \\mathbb{E}_{P(X^e, A^e, Y^e)} \\ell_{CE} (f(X^e, A^e, \\Theta_{cls}), Y^e)\n\\]\nHowever, addressing Problem 1 by training such a predictor \\(f\\) is challenging because (1) \\(f\\) is required to remain invariant across domains in terms of model accuracy, and model outcomes are fair with respect to sensitive subgroups defined by \\(A\\); and (2) we do not assume data from \\(\\mathcal{E}\\backslash \\mathcal{E}_{tr}\\) is accessible during training."}, {"title": "4 Fairness-aware Learning Invariant\nRepresentations (FLAIR)", "content": "In this paper, we narrow the scope of various distribution shifts and focus on a hybrid shift where covariate and correlation shifts are present simultaneously.\nDEFINITION 1 (COVARIATE SHIFT AND CORRELATION SHIFT). Given \\(\\forall e_1, e_2 \\in \\mathcal{E}\\) and \\(e_1 \\neq e_2\\), a covariate shift occurs in Problem 1 when domain variation is due to differences in the marginal distributions over input features \\(P(X^{e_1}) \\neq P(X^{e_2})\\). Meanwhile, a correlation shift arises in Problem 1 when domain variation results from changes in the joint distribution between \\(Y\\) and \\(Z\\), denoted as \\(P(A^{e_1}, Y^{e_1}) \\neq P(A^{e_2}, Y^{e_2})\\). More specifically, \\(P(Y^{e_1}|A^{e_1}) \\neq P(Y^{e_2}|A^{e_2})\\) and \\(P(A^{e_1}) = P(A^{e_2})\\); or \\(P(A^{e_1}|Y^{e_1}) \\neq P(A^{e_2}|Y^{e_2})\\) and \\(P(Y^{e_1}) = P(Y^{e_2})\\).\nIn Section 4.1, we handle covariate shift by enforcing invariance on instances based on disentanglement, while in Section 4.2, we address correlation shift by learning fair content representation."}, {"title": "4.1 Disentanglement of Domain Variation", "content": "In [27], distribution shifts are attributed into two forms: concept shift, where the distribution of instance classes varies across different domains, and covariate shift, where the marginal distributions over instance \\(P(X^e)\\) are various. In this paper, we restrict the scope of our framework to focus on Problem 1 in which inter-domain variation is solely due to covariate shift.\nBuilding upon the insights from existing domain generalization literature [27, 39, 44], data variations across domains are disentangled into multiple factors in latent spaces.\nDe\nASSUMPTION 1 (LATENT FACTORS). Given \\(D_e = \\{(x, a, y)\\}\\) sampled i.i.d. from \\(P(X^e, A^e, Y^e)\\) in domain \\(e \\in \\mathcal{E}\\), we assume that each instance \\((x, a, y)\\) is generated from\n\u2022 a latent content factor \\(c = h_c (x, \\Theta_c) \\in C\\), where \\(C = \\{c_{y=0}, C_{y=1}\\}\\) refers to a content space, and \\(h_c\\) is a content encoder;\n\u2022 a latent style factor \\(s_e = h_s (x, \\Theta_s) \\in S\\), where \\(s_e\\) is specific to the individual domain \\(e\\), and \\(h_s: X \\times \\Theta \\rightarrow S\\) is a style encoder.\nWe assume that the content factors in \\(C\\) do not change across domains. Each domain \\(e\\) over \\(P(X^e, A^e, Y^e)\\) is represented by a unique \\(s_e\\) and \\(\\text{Corr}(Y^e, A^e)\\), where \\(\\text{Corr}(Y^e, A^e)\\) is the correlation betweem \\(Y^e\\) and \\(A^e\\).\nUnder Assumption 1, we further assume that, for any two domains \\(e_i, e_j \\in \\mathcal{E}\\), inter-domain variations between them due to covariate shift are managed via an underlying transformation model \\(T\\). Through this model, instances sampled from such two domains can be transformed interchangeably.\nASSUMPTION 2 (TRANSFORMATION MODEL). We assume, \\(\\forall e, e' \\in \\mathcal{E}, e \\neq e'\\), there exists a function \\(T : X \\times X \\rightarrow X\\) that transforms instances from domain \\(e\\) to \\(e'\\), denoted as \\(X^{e'} = T(X^e, X^{e'})\\). The transformation model \\(T\\) is defined as\n\\[\nT(X^e, X^{e'}) = D(h_c(X^e, \\Theta_c), h_s (X^{e'}, \\Theta_s), \\Theta_d)\n\\]\nwhere \\(h_c\\) and \\(h_s\\) are content and style encoders defined in Assumption 1, and \\(D: C \\times S \\times \\Theta \\rightarrow X\\) denotes a decoder.\nWith the transformation model \\(T\\) that transforms instances from domain \\(e\\) to \\(e'\\), \\(\\forall e, e' \\in \\mathcal{E}\\), under Assumption 2, we introduce a new definition of invariance with respect to the variation captured by \\(T\\) in Definition 2.\nDEFINITION 2 (T-INVARIANCE). Under Assumptions 1 and 2, given a transformation model \\(T\\) as well as two instance \\((x, a, y)\\) and \\((x', a', y')\\), a content encoder \\(h_c\\) is domain invariant if it holds\n\\[\nx' = T(x,x'), \\text{when } e \\neq e', y = y', \\text{or}\\\\\nx' = T(x,x'), \\text{when } e = e', y \\neq y'\n\\]\nalmost surely \\(\\forall e, e' \\in \\mathcal{E}\\).\nDefinition 2 is crafted to enforce invariance on instances based on disentanglement via \\(T\\). The output of \\(h_c\\) is further utilized to acquire a fairness-aware representation, considering different sensitive subgroups, through the learner \\(g\\) within the content latent space."}, {"title": "4.2 Learning Fair Content Representations", "content": "Dwork et al., [6] defines fairness that similar individuals are treated similarly. As stated in Section 4.1, the featurizer \\(h_c\\) maps instances to the latent content space. Therefore, for each instance \\((x, a, y)\\) sampled i.i.d. from \\(P(X^e, A^e, Y^e)\\) where \\(e \\in \\mathcal{E}_{tr}\\), the goal of the learner \\(g\\) is to reconstruct a fair content representation \\(\\tilde{c_i} = g(c_i, \\Theta_g)\\) from \\(c_i = h_c (x, \\Theta_c)\\), wherein \\(\\tilde{c_i}\\) is generated to meet two objectives"}, {"title": "4.3 Learning the Predictor f", "content": "To tackle Problem 1, which aims to learn a fairness-aware domain invariant predictor \\(f\\), a crucial element of \\(f\\) is the acquisition of content factors through \\(h_c\\), while simultaneously reducing the sensitive information associated with them through \\(g\\). In this subsection, we introduce a framework designed to train \\(f\\) with a focus on both domain invariance and model fairness.\nGiven training domains \\(\\mathcal{E}_{tr}\\), a data batch \\(Q = \\{(r_1, r_2, r_3, r_4)_q\\}_{q=1}^{\\Omega}\\) containing multiple quartet instance pairs are sampled from \\(P(X^e, A^e, Y^e)\\) and \\(P(X^{e'}, A^{e'}, Y^{e'})\\), \\(\\forall e, e' \\in \\mathcal{E}_{tr}\\), where \\(\\Omega\\) denotes the number of quartet pairs in \\(|Q|\\). Specifically,\n\\begin{aligned}\nr_1 &= (x_1, a = -1, y), \\quad \\text{with class } y \\text{ and domain } e\\\\\nr_2 &= (x_2, a = 1, y'), \\quad \\text{with class } y' \\text{ and domain } e\\\\\nr_3 &= (x', a = -1, y), \\quad \\text{with class } y \\text{ and domain } e'\\\\\nr_4 &= (x', a = 1, y'), \\quad \\text{with class } y' \\text{ and domain } e'\\\\\n\\end{aligned}\nWe set \\(r_1\\) and \\(r_2\\) (same to \\(r_3\\) and \\(r_4\\)) share the same domain \\(e\\) but different class label \\(y\\) and \\(y'\\), while \\(r_1\\) and \\(r_3\\) (same to \\(r_2\\) and \\(r_4\\)) share the same class label \\(y\\) but different domains \\(e\\) and \\(e'\\). Therefore, \\(r_1\\) and \\(r_2\\) are alternative instances with respect to \\(r_3\\) and \\(r_4\\) in a different domain, respectively.\nTherefore, under Definition 2 and Eq.(1), we have the invariance loss \\(R_{inv}\\) with respect to \\(\\Theta_{inv} = \\{\\Theta_c, \\Theta_s, \\Theta_d\\}\\),\n\\[\nR_{inv}(\\Theta_{inv}) = \\mathbb{E}_{(R_1,R_2,R_3,R_4) \\in Q} (d[R_1, T(R_1, R_2)] + d[R_3, T(R_3, R_4)])\n\\]\nNote that in each distance metric \\(d[.]\\) of \\(R_{inv}\\), it compares a pair of instances with the same domain but different classes.\nFurthermore, given Eq.(2), Eq.(6) and under Definition 2, we have the invariant classification loss with respect to \\(\\Theta_{cls} = \\{\\Theta_c, \\Theta_g, \\Theta_w\\}\\),\n\\[\nR_{cls}(\\Theta_{cls}) = R_{cls} (\\Theta_{a=-1}) + R_{cls} (\\Theta_{a=1})\n\\]\nwith\n\\[\nR_{cls} (\\Theta_{a|s}) = \\mathbb{E}_{(R_{i},R_{j})\\in Q}\\{d[R_i, T(R_i, R_j)] + \\mathcal{L}_{gmm}(R_i, \\Theta_c, \\Theta_g) +\\\\\n\\mathcal{L}_{rec} (R_i, \\Theta_c, \\Theta_g) + \\ell_{CE}(w(g(h_c (R_i, \\Theta_c), \\Theta_g), \\Theta_w), Y)\\}\n\\]\nwhere \\(d: X \\times X \\rightarrow \\mathbb{R}\\) indicates a distance metric, such as \\(\\ell_1\\)-norm. \\(R_{cls} (\\Theta_{a=-1})\\) indicates the empirical risk of instance pairs with the sensitive attribute \\(a = -1\\). Similarly, \\(R_{cls} (\\Theta_{a=1})\\) is the empirical risk of instance pairs with the sensitive attribute \\(a = 1\\). Notice that the instance pair \\((R_i, R_j)\\) in \\(R_{cls}\\) sampled from \\(Q\\) have the same class label but different domains, such as \\((r_1, r_3)\\) and \\((r_2, r_4)\\)."}, {"title": "4.4 An Effective Algorithm", "content": "We introduce an effective algorithm for FLAIR to implement the predictor \\(f\\), as shown in Algorithm 1. Lines 2-3 represent the transformation model \\(T\\), while lines 4-6 denote the fair representation learner \\(g\\). In the \\(g\\) component, we employ \\(R_{fair}\\) as an approximation to \\(R_{fair}\\), since the EM algorithm[22] in FAIRGMMS continuously estimates \\(\\gamma_{ki}\\) using \\(\\pi^k\\), \\(\\forall k, a\\). Parameters of \\(\\Theta_g\\) update are given in lines 15-18 of Algorithm 1. We optimize \\(\\lambda_1\\) and \\(\\lambda_2\\) in the \\(R_{total}\\) using the primal-dual algorithm, which is an effective tool for enforcing invariance [27]. The time complexity of Algorithm 1 is \\(O(M \\times \\Omega \\times (N_{a=1} + N_{a=-1}))\\), where \\(M\\) is the number of batches."}, {"title": "5 Experimental Settings", "content": "5.1 Datasets\nRotated-Colored-MNIST (RCMNIST) dataset is a synthetic image dataset generated from the MNIST dataset [16] by rotating and coloring the digits. The rotation angles \\(d \\in \\{0^\\circ,15^\\circ,30^\\circ,45^\\circ,60^\\circ,75^\\circ\\}\\) of the digits are used to partition different domains, while the color \\(a \\in \\{\\text{red}, \\text{green}\\}\\) of the digits is served as the sensitive attribute. A binary target label is created by grouping digits into \\(\\{0, 1, 2, 3, 4\\}\\) and \\(\\{5, 6, 7, 8, 9\\}\\). To investigate the robustness of FLAIR in the face of correlation shift, we controlled the correlation between label and color for each domain in the generation process of RCMNIST, setting them respectively to \\(\\{0, 0.8, 0.5, 0.1, 0.3, 0.6\\}\\). The correlation for domain \\(d = 0^\\circ\\) was set to 0, implying that higher accuracy leads to fairer results.\nNew-York-Stop-and-Frisk (NYPD) dataset [9] is a real-world tabular dataset containing stop, question, and frisk data from some suspects in five different cities. We selected the full-year data from 2011, which had the highest number of stops compared to any other year. We consider the cities \\(d \\in \\{\\text{BROOKLYN}, \\text{QUEENS}, \\text{MANHATTAN}, \\text{BRONX}, \\text{STATEN IS}\\}\\) where suspects were sampled as domains. The suspects\u2019 gender \\(a \\in \\{\\text{Male}, \\text{Female}\\}\\) serves as the sensitive attribute, and whether a suspect was frisked is treated as the target label.\nFairFace dataset [13] is a novel face image dataset containing 108,501 images labeled with race, gender, and age groups which is balanced on race. The dataset comprises face images from seven race group \\(d \\in \\{\\text{White}, \\text{Black}, \\text{Latino/Hispanic}, \\text{East Asian}, \\text{Southeast Asian}, \\text{Indian}, \\text{Middle Eastern}\\}\\). These race groups determine the domain to which an image belongs. Gender \\(a \\in \\{\\text{Male}, \\text{Female}\\}\\) is considered a sensitive attribute, and the binary target label is determined based on whether the age is greater than 60 years old."}, {"title": "5.2 Evaluation Metrics", "content": "Given input feature \\(X \\in X\\), target label \\(Y \\in Y = \\{0, 1\\}\\) and binary sensitive attribute \\(A \\in A = \\{-1, 1\\}\\), we evaluate the algorithm's performance on the test dataset \\(D_{te}\\). We measure the DG performance of the algorithm using Accuracy and evaluate the algorithm fairness using the following metrics.\nDemographic parity difference (ADP) [6] is a type of group fairness metric. Its rationale is that the acceptance rate provided by the algorithm should be the same across all sensitive subgroups. It can be formalized as\n\\[\n\\text{ADP} = |P(\\hat{Y} = 1|A = -1) - P(\\hat{Y} = 1|A = 1)|,\n\\]\nwhere \\(\\hat{Y}\\) is the predicted class label. The smaller the ADP, the fairer the algorithm.\nAUC for fairness (AUCfair) [2] is a pairwise group fairness metric. Define a scoring function \\(q_e: X \\rightarrow \\mathbb{R}\\), where \\(\\Theta\\) represents the model parameters. The AUCfair of \\(q_e\\) measures the probability"}, {"title": "6 Results", "content": "To evaluate the performance of FLAIR, we posed the following research questions from shallow to deep and answered them in Sections 6.1, 6.3 and 6.2.\n\u2022 Q1) Can FLAIR effectively address Problem 1, or in other words, can FLAIR ensure both group fairness and individual fairness on unseen domains while maximizing DG performance?\n\u2022 Q2) Does FLAIR exhibit a good trade-off between DG performance and fairness?\n\u2022 Q3) What are the roles of the transformation model \\(T\\) and the fair representation learner \\(g\\) in FLAIR?\n\u2022 Q4) How is \\(R_{fair}\\) ensuring algorithmic fairness in the learning process of FLAIR?\n6.1 Overall Performance\nThe overall performance of FLAIR and its competing methods on three real-world datasets is presented in Table 1, 2 and 3, \\(\\uparrow\\) means"}, {"title": "6.2 Ablation Study", "content": "To understand the roles of the transformation model \\(T\\) and the fair representation learner \\(g\\) in learning a fairness-aware domain invariant predictor, we constructed two different variants of FLAIR for experimentation. They are: (i) FLAIR w/o \\(g\\): remove \\(g\\), i.e., learn a predictor \\(f_{v1} = h_s \\circ \\omega\\). (ii) FLAIR w/o \\(T\\): replace \\(T\\) with a standard featurizer \\(h : X \\rightarrow X' \\subseteq \\mathbb{R}^{d}\\) and modify the corresponding input and output dimensions of \\(g\\) and \\(\\omega\\), i.e., learn a predictor \\(f_{v2} = h \\circ g \\circ \\omega\\). The results of ablation study for FLAIR and its two variants on three dataset are shown in Figure 3 (a), (b) and (c).\nBy comparing FLAIR with its variant FLAIR w/o \\(g\\), we can see that the representations obtained by \\(T\\) exhibit strong domain invariance but do not ensure fairness. Additionally, the improvement of FLAIR on all three fairness metrics suggests that \\(g\\) can simultaneously enhance individual and group fairness. The difference between the results of them further validates the accuracy-fairness trade-off.\nContrasting FLAIR with its variant FLAIR w/o \\(T\\) further highlights the DG utility of \\(T\\). At the same time, it's evident that while \\(g\\) focuses only on fairness, it doesn't necessarily result in fairer outcomes. The reason for this is that the fair representation obtained solely through \\(g\\) lacks domain invariance. As a result, it cannot handle covariate shift and correlation shift when generalizing to unseen domains.\nThe Utility of \\(R_{fair}\\) To understand how the critical component \\(R_{fair}\\) in \\(g\\) promotes algorithmic fairness, we created two new variants of FLAIR. They are (i) FLAIR w/o \\(R_{fair}\\): removing \\(R_{fair}\\) from \\(g\\) and (ii) FLAIR w/o primal-dual: replacing the primal-dual updates"}, {"title": "6.3 Sensitive Analysis", "content": "Accuracy-fairness Trade-off. To assess the trade-off performance of FLAIR, we obtained different group fairness and DG results of FLAIR by controlling the value of \\(\\lambda_2\\) (larger \\(\\lambda_2\\) implies FLAIR focuses more on algorithmic fairness). We compare the results with other fairness-aware methods, as shown in Figure 6 for all three datasets. It can be seen that the curve of the results obtained by FLAIR under different fairness levels is positioned in the upper-left corner among all methods. This indicates that FLAIR, while ensuring the best fairness performance, also maintains comparable"}, {"title": "7 Conclusion", "content": "In this paper, we introduce a novel approach to fairness-aware learning that tackles the challenges of generalization from observed training domains to unseen testing domains. In our pursuit of learning a fairness-aware invariant predictor across domains, we assert the existence of an underlying transformation model that can transform instances from one domain to another. To ensure prediction with fairness between sensitive subgroups, we present a fair representation approach, wherein latent content factors encoded from the transformation model are reconstructed while minimizing sensitive information. We present a practical and tractable algorithm. Exhaustive empirical studies showcase the algorithm's effectiveness through rigorous comparisons with state-of-the-art baselines."}]}