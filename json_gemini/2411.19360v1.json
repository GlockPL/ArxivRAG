{"title": "DENIAHL: In-Context Features\nInfluence LLM Needle-In-A-Haystack Abilities", "authors": ["Hui Dai", "Dan Pechi", "Xinyi Yang", "Garvit Banga", "Raghav Mantri"], "abstract": "The Needle-in-a-haystack (NIAH) test is a\ngeneral task used to assess language models'\n(LMs') abilities to recall particular information\nfrom long input context. This framework how-\never does not provide a means of analyzing\nwhat factors, beyond context length, contribute\nto LMs' abilities or inabilities to separate and\nrecall needles from their haystacks. To provide\na systematic means of assessing what features\ncontribute to LMs' NIAH capabilities, we de-\nveloped a synthetic benchmark called DENI-\nAHL (Data-oriented Evaluation of NIAH for\nLLM's). Our work expands on previous NIAH\nstudies by ablating NIAH features beyond typ-\nical context length including data type, size,\nand patterns. We find stark differences between\nGPT-3.5 and LLaMA 2-7B's performance on\nDENIAHL, and drops in recall performance\nwhen features like item size are increased, and\nto some degree when data type is changed from\nnumbers to letters. This has implications for\nincreasingly large context models, demonstrat-\ning factors beyond item-number impact NIAH\ncapabilities.", "sections": [{"title": "Introduction", "content": "In the past few years, large language models\n(LLMs) have developed increasingly large context\nsizes (Bulatov et al., 2024; Ding et al., 2024; Xiong\net al., 2023). More recent models even report hav-\ning infinite context size (Munkhdalai et al., 2024).\nThese models are often presented as alternatives\nto compound, retrieval-augmented generation sys-\ntems that can retrieve information from multi-page\ndocuments using similarity search (Xu et al., 2024).\nWhile convenient to use a single model, even\nSOTA LLM's with billions of parameters demon-\nstrate an inability to recall information from arbi-\ntrary positions in that context (Anthropic, 2024).\nLiu et al. (2023) formalized an evaluation for this\ncontext recall task, referred to as the Needle in a\nHaystack (NIAH) test. A NIAH test typically in-\nvolves passing long context (the haystack), along\nwith specific information in that context (the nee-\ndle) that the LLM is expected to recall. One may\nobserve poor NIAH performance when passing the\nlongest possible input to a particular LLM, effec-\ntively increasing the amount of hay in the haystack.\nNIAH tests can involve either linguistic data as\nin Kamradt (2023), or Key-Value pairs (Kamradt,\n2023). However, different features of that input\npresumably influence performance.\nWhile several studies have looked at factors like\nthe number of tokens in input (Levy et al., 2024),\nwe hypothesize that beyond input size, other fac-"}, {"title": "Related Work", "content": "Models have grown increasingly large (Hoffmann\net al., 2022). However, as of late, more and more\nmodels are also being developed with larger, and\neven infinite context token sizes (Bulatov et al.,\n2024; Munkhdalai et al., 2024; Mohtashami and\nJaggi, 2023). ChatGPT, initially developed with\na input size of 4096 tokens, now has variants of\n32k token input size, demonstrating the demand for\nthese large context models."}, {"title": "Long-context Benchmarks and Features", "content": "As these larger context size models gain traction,\nit has becoming increasingly important to evaluate\nthese models. Many of these long-context models\nhave already been exposed for lacking the ability to\nuse all their context, including for in-context learn-\ning (Li et al., 2024). Pang et al. (2022) introduced\nsome of the earliest benchmarks for long-context\nmodels with a question-answering dataset with an\naverage length of 5000 tokens. The concept of\nNeedle-in-a-haystack and the lost-in-the-middle\nphenomenon added to the notion of data structure\ninfluencing recall performance (Liu et al., 2023).\nOther popular NIAH and long-context benchmarks\ninclude LongBench (Bai et al., 2023), which in-\ncludes multilingual QA benchmarks.\nRuler (Hsieh et al., 2024) is most similar to our\nwork, focusing on synthetic key-value data bench-"}, {"title": "Data-centric Evaluation of\nNeedle-in-a-haystack for LLM's", "content": "DENIAHL consists of tasks which vary 3 cate-\ngories of data properties, namely the data's size,\npatterns and type (ie numbers vs letters) within\nthe model's input. Contrary to current benchmarks\nwhich mostly assess data size as measured by the\nlength of the context input, through various key-\nvalue retrieval tasks, DENIAHL tests the models'\nrecall capabilities when data items vary according\nto a broader range of relevant data properties."}, {"title": "Problem Definition", "content": "In DENIAHL, we manipulate the length of input\ncontext data, varying both the length by changing\nthe number of key-value pairs, as well as the length\nof the keys and values. Additionally, we assess\nNIAH performance when the data follows patterns,\nand when that pattern is broken, giving a sense of\nwhether the model is performing fine-grained recall\nor merely attending to a global pattern. Lastly, we\nassess data type by comparing recall abilities for\nkey-values consisting of random characters, num-\nbers, and a mix of both. We test both LLaMA-2 7B\nand GPT-3.5 on DENIAHL, varying pattern, size,\nand type features of input data to assess how these\nfeatures impact NIAH performance, and whether\npreviously-reported lost-in-the-middle effects per-\nsist."}, {"title": "Datasets", "content": "To evaluate the ability of different language models\nto recall input information against existing bench-\nmarks, we evaluate LLaMA-2 7B and GPT-3.5\nmodels on Paul Graham's essays, as used in the\nNeedle-in-a-haystack tests (Kamradt, 2023). This\ndataset consists of text data where needles are sen-\ntences that include the answer for the query. We\nalso benchmark against the key-value datasets in-\ntroduced by Liu et al. (2023), where the model"}, {"title": "DENIAHL datasets", "content": "We propose a new benchmark which consists of\ncustom datasets with varying properties in key-\nvalue formats. Examples and descriptions of our\ncustom dataset's three task categories are provided\nin Table 1. The task categories are as follows:\nData Size:\n\u2022 With a fixed length of each key-value item, we\nvary context sizes based on the number of key-\nvalue pairs in the input prompt.\n\u2022 With a fixed number of key-value pairs in each\ndataset, we vary context sizes based on the length\nof key-values.\nPattern:\nThe key-value pairs in the pattern category follow\ncertain numerical or alphabetical patterns. To\nevaluate the model's ability for accurate retrieval\nversus mere pattern recognition, we break the pat-\ntern at a certain position by altering the value, and\nobserve the model's output when it is asked to re-\ntrieve that value. For example, consider a dataset\nhaving an arithmetic sequence pattern with keys\n(1, 2, 3, 4, 5) and corresponding values (3, 5, 7, 9,\n11). If we alter the third value from 7 to 6, we can\nassess whether the model accurately retrieves the\naltered value (6) or erroneously predicts the origi-\nnal sequence value (7), thus revealing the model's\nreliance on pattern inference.\nData Type:\nWe vary keys and values as random numbers, let-\nters, or a mixture of both.\nVarying these data properties of key-value pairs, we\ntest how model performance changes with respect\nto each variable."}, {"title": "Experimental Setup", "content": "The models we test are LLaMA-2 7B and GPT-3.5.\nFollowing the framework established by Liu et al.\n(2023), we define the key-value retrieval task as fol-\nlows: Given a string-serialized JSON object with k\nkey-value pairs and a query requesting the retrieval\nof key, the expected output is the corresponding\nvalue\u017c. This task is a specific type of NIAH test,\nwhere the target key-value pair is the \"needle\"\nwithin the k key-value pairs (the \"haystack\").\nTo generate our synthetic datasets, we create\nunique and random UUIDs, numbers, or letters\nto serve as keys and values. For the benchmark"}, {"title": "Benchmark tests", "content": "For the key-value retrieval benchmark test from\nLiu et al. (2023), we experiment with 500 datasets,\neach containing 50 key-value pairs of randomly\ngenerated UUIDs. While the original benchmark\nis for 75, 140, and 300 key-value pairs, we limit\nour datasets to the first 50 pairs from the 75 key-\nvalue pairs datasets due to the 4096 input token\nconstraints of GPT-3.5 and LLaMA-2 7B."}, {"title": "Needle-in-a-haystack retrieval", "content": "The goal of the Needle-in-a-haystack retrieval task\npresented by Kamradt (2023) is to correctly re-\nspond to a query based on a statement. The input\ncontexts are essays with a \"needle\" statement in-\nserted at different depths. To examine the effect\nof input length on performance, we truncate the\nessays to various lengths corresponding to 1k, 2k,\n3k, and 4k tokens."}, {"title": "DENIAHL tasks", "content": "For each DENIAHL task, we utilize 50 randomly\ngenerated key-value datasets similar to our exam-\nples in Table 1. We calculate the average accuracy\nscores across these datasets for evaluation. It is\nimportant to note that the upper limits for the num-\nber of key-value pairs and the length of items are\ndefined by the models' input constraint of 4096\ntokens. We aim to maximize the use of the model's\ncapacity without surpassing the limit. Additionally,\nwe examine the retrieval performance by varying\nthe position of the target key-value pair.\nThe prompt template we use to query the models\nare inherited from the work of Liu et al. (2023).\nFull examples of our datasets with prompts are\nshown in Appendix A."}, {"title": "Data Size", "content": "When changing the data size by varying the number\nof items, we test on 10, 30, and 50 key-value pairs,\nwhich consist of randomly generated UUIDs as\nkeys and values."}, {"title": "Patterns", "content": "For all pattern tasks, we experiment with 100 key-\nvalue pairs in the context passed to the LLM given\nthe shorter length of these items. Two numerical\npatterns and one letter pattern are evaluated.\nFor numerical patterns, the keys are consecutive\nintegers from 1 to 100. Patterns include one simple\npattern and one complex pattern as shown in the\nAppendix.\n\u2022 The simple pattern follows a global pattern\ndefined by\n$value = key; multiplier + initial value$.\nFor different datasets, the multiplier and ini-\ntial value are randomly chosen from values\nbetween 1 to 10 and 1 to 20, separately.\n\u2022 A more complex global pattern is defined by\nthe following. With value, as the initial value,\nfor i > 1,\n$value_i = \\begin{cases}\nvalue_{i-5} + increment, & \\text{if } i \\% 5 = 1 \\\\\n2 \\cdot value_{i-1}, & \\text{otherwise}\n\\end{cases}$\nFor the letter pattern task, the value is the upper-\ncase of a randomly generated lowercase two-letter\nkey."}, {"title": "Data Type", "content": "In this task, we experiment with three data types,\neach characterized by key-value pairs consisting\nof either entirely random letters, numbers, or a\nmixture of both. Each dataset comprises 50 key-\nvalue pairs, with an item length of 32."}, {"title": "Results and Discussion", "content": "We find that LLaMA-2 7B generally performs bet-\nter with 10 key-value pairs compared to 30 or\n50 key-value pairs (Figure 4). This suggests that\nthe model more effectively attends to information\nwhen the input context length is shorter."}, {"title": "Data Size", "content": "We find that LLaMA-2 7B generally performs bet-\nter with 10 key-value pairs compared to 30 or\n50 key-value pairs (Figure 4). This suggests that\nthe model more effectively attends to information\nwhen the input context length is shorter."}, {"title": "Patterns", "content": "For global pattern attention, we show results for an\n\"original\" task where the retrieved value follows\nthe pattern, and \"break\" where the pattern is broken\nfor the retrieved value. Based on a manual review,\nwhen the pattern is broken for LLaMA-2 7B, the\nmodel rarely inaccurately inferred the original pat-\ntern, instead retrieving the true, altered value. This\nobservation suggests that LLaMA-2 7B does not\napply global pattern attention that overrides fine-\ngrained recall capabilities."}, {"title": "Data Type", "content": "For LLaMA-2 7B, Figure 1 consistently demon-\nstrates a \"lost-at-the-end\" effect for key-value pairs"}, {"title": "Conclusions & Future Work", "content": "We find that Needle-in-a-haystack (NIAH) abili-\nties are influenced by each of the 3 component\ntasks in DENIAHL: data size, patterns, and type.\nWhereas some of these features impact recall per-\nformance depending on where needles are located\nin the haystack, other components can alter recall\nregardless of needle location. In particular, data\nsize is a more globally influential feature, whereas\nchanging data type from numbers to letters can\nchange the emergence of \"lost-in-the-middle\" into\n\"lost-in-the-end\" phenomena, decreasing perfor-\nmance beyond earlier context window positions.\nThis underscores that models' recency biases are\nsubject to change depending on their data input.\nThe whole of these data features (size, type, and\npatterns) is greater than their sum, with GPT-3.5\nonly demonstrating reduced NIAH abilities when\nrecalling particularly long mixed data types.\nIn real world data settings, LLM's are being used\nto recall information from ever longer contexts. En-\nsuring that models' input context has data features\nthat make for robust recall is an important initial\nstep before applying these models. Alternatively,\nleveraging compound AI systems like RAG (Za-\nharia et al., 2024) for linguistic use cases like Kam-\nradt (2023) is a promising alternative. Our work\nalso shows that almost all input contexts present\nstrong primacy biases, meaning re-ranking sys-\ntems like Khattab and Zaharia (2020) may similarly\nprove useful in improving NIAH recall."}, {"title": "Limitations", "content": "We find the evaluation of long-form text-based\nNIAH lacking. In our key-value case, we use\nROUGE. However, if the generated text rephrases\nneedles (e.g. synonyms), the evaluation won't\nidentify these as correct matches, leading to lower\nscores despite being technically correct. There-\nfore, we still need to look for metrics that evaluate\nextraction other than ROUGE. The original, LLM-\nas-a-judge metric used by Kamradt (2023) presents\nits own limitations. This approach is not only sub-\nject to non-deterministic evaluations, but is also\nunlikely to scale to longer contexts.\nAdditionally, our NIAH tests may not accu-\nrately reflect the extractive capabilities required in\ndifferently-structured data in the real world. This"}]}