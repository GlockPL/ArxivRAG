{"title": "Identifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks", "authors": ["Fatemeh Mohammadi", "Marta Annamaria Tamborini", "Paolo Ceravolo", "Costanza Nardocci", "Samira Maghool"], "abstract": "This paper is a collaborative effort between Linguistics, Law, and Computer Science to evaluate stereotypes and biases in automated translation systems. We advocate gender-neutral translation as a means to promote gender inclusion and improve the objectivity of machine translation. Our approach focuses on identifying gender bias in English-to-Italian translations. First, we define gender bias following human rights law and linguistics literature. Then we proceed by identifying gender-specific terms such as she/lei and he/lui as key elements. We then evaluate the cosine similarity between these target terms and others in the dataset to reveal the model's perception of semantic relations. Using numerical features, we effectively evaluate the intensity and direction of the bias. Our findings provide tangible insights for developing and training gender-neutral translation algorithms.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Natural Language Processing (NLP) have marked significant milestones, notably with the emergence of Large Language Models (LLMs), which have led to undeniable performance improvements in various NLP tasks. Among these tasks, Machine Translation (MT) is one of the most widely used yet critical applications [1]. The integration of Artificial intelligence (AI) into translation services has undoubtedly facilitated cross-cultural communication and nowadays automated translation providers are widely used in every industry. For example, today Google Translate, the multilingual neural machine translation service developed by Google, has over 610 million users per day. Despite the undeniable usefulness of such technologies, the existence of gender biases and stereotypes in MT is a fact [2].\nThe concept of gender bias takes on a multifaceted nature depending on the field of study. Within legal and human rights frameworks, it signifies the unfair treatment of individuals based on their gender. This can encompass discriminatory laws, biased legal processes, and the resulting human rights violations [3], [4]. Conversely, linguistics examines how language itself perpetuates societal in-equalities through gender bias. This manifests in grammatical structures that favor one gender, vocabulary lacking female equivalents for certain roles, and the way words carry specific connotations about masculinity and femininity [5]; [6]; [7]. Recognizing these distinct yet interrelated aspects of gender bias is es-sential to tackling its presence in both legal systems and our daily conversations.\nThis paper focuses mainly on gender bias defined as the preference or prej-udice for one gender over the other [2]. The influence of gender stereotypes and prejudices in language is well-documented in Italian and English linguistic stud-ies. Research lines in both law and linguistics have shown [8-11] that there is a strong link between the spilling of stereotypes in language and direct or indirect discrimination [12, 13]. It is thus crucial to properly identify and correct stereo-types and biases in MT translations. Language is both a reflection and a shaper of societal norms [13] and as such, automated translation systems that rely on large corpora of text inherit and amplify these biases [9]. It is therefore impor-tant to quantify and understand language biases, as such biases can reinforce the psychological status of different groups.\nThe multidisciplinary approach we develop involves the definition of the legal categories behind gender stereotypes, the linguistic factors that contribute to such biases, and the technical aspects within computer science that can be used to quantify these biases. Since translations are not only the transposition of words but also the transmission of cultural connotations and social norms, our goal is to study the manifestation of gender bias in Italian translations of English texts. Our methodology builds on existing approaches [14,15] but enhances the ability to detect the intensity and direction of bias through specially designed features.\nBased on the above goal, the research question underlying this contribution has been to study how English to Italian gender language is addressed by MT by attempting to answer the research questions. (i) How does MT from English to Italian affect the gender of language? (ii) Is the effect of MT related to bias in the algorithms or in the source data used to train MT? (iii) Can we precisely identify the intensity and direction of the MT bias for each term?\nMore in general, this paper is positioned in the \"notional vs. grammati-cal\" gender languages debate and this analysis could be repeated for multiple notional/grammatical-language pairs. Our research began by defining gender bias categories and definitions and started with the collection of word corpora containing both gender-specific and gender-neutral terms. These words were rep-resented in a feature space and organized into a similarity network to compute their similarity. Through this process, we were able to identify similarities with gender-specific terms such as she/lei and he/lui. We used these similarities to derive features that capture the intensity and direction of the bias. Furthermore,"}, {"title": "2 Background and Related Work", "content": "This work contributes to the broader discourse on AI and discrimination which is of the utmost relevance since Europe is currently setting its legal framework on AI. Specifically, on 13 March 2024, the European Union enacted its first comprehensive regulation to address these concerns [16] and on 17 May 2024 the Council of Europe adopted the first-ever international legally binding treaty aimed at ensuring the respect of human rights, the rule of law and democracy legal standards in the use of artificial intelligence (AI) systems [17]. Concern-ing potential unequal treatment leading to discrimination, the inherent risk of AI models lies in their potential to perpetuate discrimination against minority groups due to biases in the data sets or the same architecture of the system. The new European legal framework emphasizes the need for a more conscientious and responsible approach to AI design and evaluation [18]. As a consequence, elimi-nating AI-induced discrimination is at the heart of current research trends [19].\nPrevious works addressed this topic for other languages [14] or focused on other NLP techniques [15,20,21] and some tried to provide a taxonomy of gender bias in texts (in English) [13]. A comprehensive review of the current legal liter-ature on the topic [18] outlines several stages in AI design where discrimination may manifest.\nMT based on machine learning technologies, represents an example of how AI perpetuates stereotypes typical of human language [9] from one culture to another. Let us recall the case of Google Translate [22], the MT system exhibited biased behavior by translating job titles from English into languages that incor-porate masculine/feminine characterization. This is particularly evident when lower-income and non-leadership positions are translated with their female coun-terparts, while higher-income and leadership positions are associated with their male counterparts, perpetuating gender stereotypes. For example, \"the nurse\" would be translated with \"l'infermiera\" (female nurse) and \"the doctor\" with \"il medico\" (male doctor) [22, 23].\nThe importance of gender in human experience is universally acknowledged, reflected in the linguistic expressions of femininity and masculinity present across"}, {"title": "2.1 Gender Bias and Discrimination in Law and Linguistics", "content": "languages. However, languages differ in their methods of encoding gender. English, for instance, is classified as a notional gender language, primarily conveying the gender of human referents through personal pronouns, possessive adjectives (e.g., he/him/his; she/her/hers), and gender-specific terms (e.g., man; woman). In contrast, grammatical gender languages such as Italian utilize a system of morphosyntactic agreement, where gender markers extend beyond nouns to en-compass verbs, determiners, and adjectives [24]. This distinction becomes partic-ularly significant in translation contexts [25], notably when the source language lacks gender information regarding a referent and the target language operates within a grammatical gender framework and, although controversial to this day, prescribes the grammatical rule of the \"inclusive masculine\". This rule is em-bodied in the fact that if only one masculine exponent is present in a group, the plural masculine will apply. For example, in a class consisting of 10 peo-ple 9 are women but 1 is a man, it is grammatically correct for the teacher to say \"Buongiorno a tutti\" (-i standing for masculine plural). Furthermore, until recently, there were no feminine words to define higher professional positions. With social change and the promotion of female participation in public life, such terms have been coined (e.g. Professoressa, Dottoressa, Avvocata, Ingegnera, etc.) but still part of public opinion is skeptical to use such terms [26] to the point that in 2023 Italy elected its first female prime minister who decided to be addressed by \"Signor Presidente\" (Mr. President) as a political statement [9].\nItalian linguistics studies nowadays converge on the assumption that Italian is a sexist language. The essay \"Sexism in the Italian Language\" [27] has been the first seed of a debate that bloomed much later. Currently, the Italian language is changing because much has changed in the role of Italian women within society. However, this change is not organic, structured, or systematic as it should be in a society that proactively strives for new relationships between women and men [28].\nRecent research in computation and language [29] advocated for gender-neutral translation (GNT) as both a manifestation of gender inclusion and an objective for MT models. Our research aligns with and contributes to this per-spective by proposing a methodology for identifying terms that exhibit gender bias in MT. Similar studies have been applied to English [14], to Sentiment Analysis [20], to word embedding methods [15,21] but never to the English to Italian translation. Through our findings, we provide tangible insights to inform the development and training of GNT algorithms, thus promoting more inclusive and unbiased translations."}, {"title": "2.2 Gender Bias and Discrimination in Computer Science", "content": "We were inspired by the research of [14]. They created some analogies like King: Queen using Google Word2Vec and then asked human annotators to rate them as biased/appropriate. To detect bias, they used cosine similarity to measure the similarity of analogies with she-he. They showed that word embeddings contain biases in their geometry that reflect gender stereotypes in the wider society.\nIn another paper by [30], they composed a challenge set for gender bias in MT called WinoMT, which contains 3,888 instances and is balanced between male and female, and between stereotypical and non-stereotypical gender roles (e.g. a female doctor versus a female nurse). They then translated them into four different language categories (Romance, Slavic, Semitic, and Germanic) using six widely used MT models representing the state of the art in commercial and academic research, such as Google Translate. They showed that MT models significantly tend to translate based on gender stereotypes rather than more meaningful contexts.\nAnother related study is the one by [31]. They looked at gender bias in Ital-ian word embeddings. They made a list of gender definition pairs: [lui (he), lei (she)] and then calculated the vector difference like lui-lei to get the direction of the bias. They also used cosine similarity to measure the differential association between target and attribute word sets. They used FastText as the word em-bedding method and the target set of their work consisted only of occupations in Italian.\nIn another paper by [32], they take an extensive list of job titles and construct some sentences like \"He/She is an Engineer\" (where \"Engineer\" is replaced by the job title of interest) in 12 different gender-neutral languages such as Hungar-ian, Chinese, Yoruba, and several others. They then translate these sentences into English using the Google Translate API and collect statistics on the fre-quency of female, male, and gender-neutral pronouns in the translated output. We then show that Google Translate has a strong bias towards male pronouns, especially in fields typically associated with gender imbalance or stereotypes, such as STEM (Science, Technology, Engineering, and Mathematics) jobs.\nFinally, the literature suggests that most word embedding models, such as Word2Vec and FastText, have a gender bias. This bias can influence how models learn patterns, potentially reinforcing societal biases and stereotypes. Conse-quently, AI and ML models, including machine translation (MT) models, are also susceptible to such biases in word embedding. In this research, we aim to investigate the extent to which this bias is caused by word embedding or trans-lation, and whether translation affects the intensity and nature of this gender bias."}, {"title": "3 Material and Methods", "content": "Our approach is designed to uncover gender bias in translations from English to Italian. We do this by computing similarity scores between certain gender-specific target words, such as she/he, and the other words in a word list that combine gender-neutral and gender-specific words. In the following sections, we will explain our procedure by breaking it down into its main stages."}, {"title": "3.1 Data collection", "content": "As part of our effort to highlight gender bias in automated translation, we seek a collection of words that include various gender-specific and gender-neutral terms.\nFor this purpose, we used the analogies generated by word embedding provided by [14] in the Appendix section of the paper. The authors generated 236 (word pairs) analogies by an analogy generator which gets a seed pair of words (a,b) determining a seed direction (a- b) corresponding to the normalized difference between the two seed words. An example of analogy in this paper is she:sewing:: he: carpentry. It represents a relationship between words based on their contextual associations. It means that there is a strong association of \"she\" with \"sewing\" and \"he\" with \"carpentry.\" This analogy reflects gender stereotypes present in the training data. Because Word embedding captures patterns and associations in the text they are trained on, including biases."}, {"title": "3.2 MT reference", "content": "Given the widespread use of Google Translate, we included its API in our study. Bias in such a widely used tool raises serious concerns and underscores the importance of addressing it."}, {"title": "3.3 Pre-processing and Word Embedding", "content": "To measure similarity, we first took the list of analogies and broke it down into single words (472 words in total). Our approach in using single words (de-tached from the grammatical or contextual environment) is similar to the WEAT or Verb Extraction approaches [33]. First, we removed duplicate words (27 re-moved). Also, some words had plural and singular forms. Since we use similarity scores and there is no significant difference between the plural and singular forms of a word, we decided to keep only the singular form of words, resulting in a list of 333 words. This ensures that the text data is standardized and ready for the similarity measurement algorithms.\nAfter preparing the list of words, we need to organize words in a vector space to compute similarity scores. Given the requirement to analyze both English and Italian texts and to perform a comparative assessment, we chose a multilingual approach to word embedding so we selected FastText. FastText [34] is a word embedding method that uses a vectorization process by considering subwords (N-grams) as the smallest unit instead of single words. This approach makes it independent of the distribution of words in a vocabulary and allows generalizing across languages, facilitating the transfer of knowledge learned in one language to another. In contrast, other options, such as Word2Vec, are inherently language-dependent and thus unsuitable for our research design."}, {"title": "3.4 Similarity measurements", "content": "We utilize a graph-based approach to analyze the relationships between data points. This approach enables us to quantify the likeness between data points, facilitating the construction of a weighted network denoted as $N = (V, E)$. Here, V signifies the nodes (vertices) within the network, while E denotes the links"}, {"title": "s(i, j) = \\frac{X_iX_j}{||x_i||||x_j||}", "content": "(edges) connecting them [19]. In this application, each node corresponds to a word, and the weights of the edges reflect the degree of similarity between these words.\nTo calculate the similarity (proximity) between two nodes, we use the cosine similarity function applied to the embedded vectors of the given nodes i and j in the network. This similarity is computed using the following formula [35]:\nWe have generated a robust similarity network by employing cosine similar-ity measurements, particularly with gender-specific terms such as she/he and lei/Lui. Before choosing he/lui and she/lei as target words we measured the internal similarity between them. i.e. we measured the similarity between she and he (0.61) and the similarity between lui and lei (0.85). so these target words are quite different so we can rely on them as a good differentiated point for identifying bias. Also note that we could have used other words, e.g. woman and man, as the gender-pair in the task. We chose she and he because they are frequent and do not have fewer alternative word senses (e.g., man can also refer to mankind) [14].\nBecause of its efficiency, cosine similarity is chosen for comparing vectors such as word embeddings. Unlike measures based solely on magnitude, cosine similar-ity evaluates the angle between vectors, emphasizing their directional alignment. This property is advantageous when comparing word vectors, as it prioritizes vector direction over absolute values and provides more meaningful similarity judgments."}, {"title": "3.5 Detecting gender bias", "content": "To study the manifestation of gender stereotypes or biases, one approach is to quantify the proximity of words to gender-specific terms such as he/lui vs. she/lei [14]. To enhance the accuracy of bias identification, we introduce a nu-merical feature that measures the absolute difference between the similarity to she and the similarity to he. This feature, called GenderBiasIntensity, quantifies the intensity of bias without considering its direction at this stage. If there are two target words with different genders, denoted as X and Y, the gender bias intensity of w can be expressed as follows [33]:\nSo for determining the gender bias of a specific word, after tokenization and word embedding, we compute the cosine similarity of that word with these target words. For example, if the similarity score between she and doctor is lower than the similarity between he and doctor, this suggests a gender bias against women since the model assumes that a doctor is predominantly male.\nTo determine the direction of bias, we introduce another feature called Gen-derBiasDirection, which represents the difference between similarity to she and"}, {"title": "Gender BiasIntensity(w) = |cos(w, X) \u2013 cos(w, Y)|"}, {"title": "Gender Bias Direction(w) = cos(w, X) \u2013 cos(w, Y)", "content": "similarity to he (without taking the absolute value). A positive value for this feature suggests a bias towards women, whereas a negative value indicates a bias towards men. Based on this definition, the gender bias direction of w can be expressed as follows:\nTo examine the extent of post-translation similarity changes, we introduce another numerical feature that adopts a comparative approach. This feature is calculated by subtracting the similarity to she (the target word in the source language, i.e., English) from the similarity to lei (the target word in the desti-nation language, i.e., Italian), represented as lei \u2013 she. This indicates whether similarity scores increase or decrease after translation. Similarly, we can perform the same operation for lui and he, expressed as lui - he. Based on this definition, this feature can be calculated using the following formula:"}, {"title": "Post TranslationSimilarityChanges(w) = cos(w, Xdest) - cos(w, Xsrc)", "content": "In this section, we briefly describe the similarity measurement and the gender bias analysis we did, our main results, and the final discussion and comparison of our work with related ones."}, {"title": "4.1 Similarity Scores Analysis", "content": "First of all, we need to calculate the similarity of English and Italian-translated words with our target terms. Here the results of these measurements were pre-sented.\nAs mentioned in the methodology sec-tion, we compute the cosine similarity between some selected English words and two target terms (he/she).\nThe distribution of word similarity concerning he and she is visualized using a scatter plot. Another insight from this projection onto a 2-dimensional space is the possi-bility of drawing a diagonal line from (0,0) to (1,1). Words that deviate more"}, {"title": "Similarity Scores in English Words", "content": "Following the methodology outlined above, we used the Google Translate model for translation to ensure the reliabil-ity of our methods and translations. We followed a similar procedure for Italian as in English to facilitate comparison between the two languages. We also use 2 gender-specific words: lei/lui. From this table, it is obvious that for most of the words, the similarity with the target word itself increases after translation. However, in some cases such as architetto, infermiera, chirurgo, and socialisti, the intensity of the similarity to lui/he and lei/she, decreases. We conducted an additional comparison by calcu-lating the similarity between \"architetta,\" the feminine version of \"architetto,\" and our two target words. The results showed that the similarity between \"ar-chitetta\" and \"lei\" is 0.49, and between \"architetta\" and \"lui\" is 0.38. This indicates that the similarity of words to the target words is strongly influenced by how the machine translation handles the word and the gender it assigns. This preliminary conclusion will be further examined in the following section.\n(which shows the projection of English words with she/he) with Figure 2 (which shows the projection of Italian translations with lei/lui) also confirms the pre-vious observation of increased similarity scores after translation. For the Italian translations, most of the words fall within the range of 0.2 to 0.6, while for the English plot, the words are spread across the range of 0.0 to 0.6. But the overall distribution is almost the same for both languages."}, {"title": "Similarity Scores in Italian Words", "content": "We have to note that just relying on similarity scores can not lead us to the correct analysis of gender bias so in the following sections we will present some numerical features which can give us a more accurate insight about gender bias intensity and direction in English and Italian translated."}, {"title": "4.2 Gender Bias Detection and Analysis", "content": "In this section, we analyze the results obtained from calculating Gender BiasIn-tensity and GenderBiasDirection for both English and Italian translations. Our objective is to identify gender bias in English and examine how the translation from English to Italian can affect the intensity and direction of bias.\nAfter measuring the similarity scores with he and she we can calculate the GenderBiasIntensity feature. Within this range for GenderBiasIntensity, we categorize our words into five primary bins which are indicated in Table 3. Bin 1 has the lowest intensity of bias but as we advance towards bin 5, the intensity of bias increases.\nBy a detailed examination of the words within each bin, it becomes apparent that bins 4 and 5 mainly contain gender-specific terms such as he, she, aunt, niece, sister, her, herself, etc. Conversely, the first three bins exhibit varying degrees of gender bias, ranging from 0.0 to 0.3. For instance, words like housewife, handbag, homemaker, sewing, softball, and midwife appear in the third bin, which is recognized as female stereotypical according to a study conducted by [14].\nwe can partition it into 5 bins for each gender based on the sign of the feature."}, {"title": "Gender bias in English words", "content": "In this section, we will analyze gender bias in translations from English to Italian to assess its impact on the intensity and direction of bias. In the next stage, we computed the GenderBiasIntensity for Italian translations, as previously done with English. Notably, the maximum value within this range has decreased by 0.18 compared to its English equivalent, indicating an overall re-duction in gender bias intensity.\nIn the Italian translation, similarly to English, the last bin (bin 3) comprises gender-specific words like zia (aunt), mamma (mom), lei (she), lui (he), and donna (woman). Bins 1 and 2 are potential candidates for bias, where bin 2 exhibits the strongest bias intensity, ranging between 0.1 and 0.2. Following the calculation of bias intensity, determining the bias direction is essential."}, {"title": "Gender bias in Italian translation", "content": "Table 6 illustrates that although there is a reduction in the intensity of the bias after translation, there is a change in the direction of the bias. Specifically, while more than 69% of the English words in Table 4 show a bias towards females, this ratio decreases to 45% after translation. Another interesting observation is that after translation, the ratio of bias direction between females and males becomes more balanced, indicating an almost equal distribution.\nAlthough English is a notional gender lan-guage, we carried out a gender analysis of the words in the list before translation to determine the percentage of gender-specific and gender-neutral terms in the source data. Out of 333 words, 157 are gender neutral and 176 are gender spe-cific (89 feminine and 87 masculine). An examination of the translations from Google Translate shows that of the 157 neutral English words, 91 are translated into masculine forms and only 32 into feminine forms. This ratio of approxi-mately three to one provides a first insight into the gender bias of the model.\nTo observe the effect of translation on similarity scores, we use the fea-ture. A positive value of this feature indicates an increase in the similarity score after translation. This in-crease is observed in 85.58% of the words in the feminine version and in 96.39% of the words in the masculine version. From this analysis, it is clear that translation does not significantly affect bias intensity and that the bias is more related to the word embedding methods.\nBased on the above observation, we can conclude that to achieve unbiased translation, we must first reduce bias in the word embedding methods used to train the MT models. Unbiased translation, particularly in the context of gen-der, requires minimizing the introduction or reinforcement of stereotypes."}, {"title": "Post-translation Gender Shifts", "content": "discussed in the previous section, one challenge in dealing with grammatically gendered languages such as Italian is the default use of masculine forms. Word embeddings in Italian, as in many languages, show significant gender bias. One study found that although Italian word embeddings have less potential to re-inforce certain stereotypes than English, the presence of grammatical gender introduces different forms of bias. For example, in job search contexts, mascu-line terms may be the default, potentially disadvantaging women by making male candidates more likely to be found [36]."}, {"title": "5 Conclusion", "content": "Our work has provided us with several findings that help to answer the research questions we have identified.\nIn response to RQ1, the overall conclusion is that MT still affects the use of gender in texts today. However, the translation step per se does not have such a negative impact on bias. Most of the bias comes from the word embedding methods used in the training of MT algorithms. This leads us to RQ2. If the bias is mainly related to the distribution of words in the source and target languages, then the problems of stereotypes in MT translation are less related to the algorithm itself and more related to stereotypical language spillovers in the corpora used for training [37]. This conclusion invites further studies on how to efficiently structure and control both training and control datasets.\nTo address RQ3, a notable aspect of our research is the investigation of the impact of translation on bias, using the widely used Google Translate. Our study integrates embedding and translation, which is a unique approach. Compared to previous literature, which has mainly focused on Word2Vec [14], we pay particu-lar attention to embedding methods such as FastText, which are better suited for multilingual analysis. Furthermore, while much research in this area has focused only on lists of occupational terms [31,38,39], our analysis uses a comprehensive list of words that includes different occupations, adjectives, sports, and more. This approach allows us to identify bias in all aspects of language, not just job titles.\nregarding RQ3, our study advances the state of the art in quan-tifying gender bias in MT by using numerical features to effectively assess the intensity and direction of bias. Our approach not only identifies the presence of bias but also provides a normalization measure to identify its magnitude and directional tendencies, allowing for a more precise assessment compared to previous studies. Furthermore, by highlighting specific areas where bias is most pronounced, we provide actionable guidance for researchers and practitioners seeking to improve the fairness of their MT systems. In our observations, we find a bias in the FastText embedding method towards both males and females, as evidenced by the significant deviation of many words in the scatter plot from the diagonal line. The range of related numerical features confirms this bias even if it is not particularly severe. While most words have a greater similarity to she than to he, indicating a bias toward females, the overall direction of the bias shifts after translation. After translation, there is a reduction in the intensity of the bias, with GenderBiasIntensity ranging from 0.0 to 0.26. However, there is a significant change in the direction of the bias, with many words becoming more male-biased. This suggests that while FastText has a female bias, Google Translate often translates words into the masculine form, significantly changing the direction of the bias.\nTherefore, further studies could focus on fine-tuning LLMs to better detect stereotypes and biases by exploring the possibilities of studying the taxonomy and ontology of stereotypes and biases in a target language, Presenting some methods to mitigate the bias in languages is also an interesting topic."}]}