{"title": "ECG-Image-Database: A Dataset of ECG Images with Real-World Imaging and Scanning Artifacts; A Foundation for Computerized ECG Image Digitization and Analysis", "authors": ["Matthew A. Reyna", "Deepanshi", "James Weigle", "Zuzana Koscova", "Kiersten Campbell", "Kshama Kodthalu Shivashankara", "Soheil Saghafi", "Sepideh Nikookar", "Mohsen Motie-Shirazi", "Yashar Kiarashi", "Salman Seyedi", "Gari D. Clifford", "Reza Sameni"], "abstract": "Objective: We introduce ECG-Image-Database, a large and diverse collection of electrocardiogram (ECG) images generated from ECG time-series data that exhibit real-world scanning, imaging, and physical artifacts. This dataset addresses the need for digitizing paper-based and non-digital ECGs for computerized analysis, providing a foundation for developing robust machine and deep learning models capable of ECG image digitization.\nApproach: We used ECG-Image-Kit, an open-source Python toolkit, to generate realistic images of 12-lead ECG printouts from raw ECG time-series data. We include images with realistic distortions, including noise, wrinkles, stains, and perspective shifts generated both digitally and physically. The toolkit was applied to 977 12-lead ECG records from the PTB-XL database and 1,000 from Emory Healthcare to create high-fidelity synthetic ECG images. These 1,977 unique images were subjected to both programmatic distortions using ECG-Image-Kit and to physical effects, such as soaking, staining, and mold growth, followed by scanning and photography under various lighting conditions to create real-world artifacts, producing a total of 35,595 images.\nMain results: The resulting dataset includes 35,595 software-labeled ECG images (generated from 1,977 unique records) from different sources (in Germany and the USA) with a wide range of imaging artifacts and distortions. The dataset provides ground truth time-series data alongside the corresponding images, offering a reference for training and evaluating machine and deep learning models for ECG digitization and classification. The images vary in quality, ranging from clear scans of clean papers to noisy photographs of degraded papers, enabling the development of more generalizable and robust digitization algorithms.\nSignificance: ECG-Image-Database addresses a critical gap in cardiovascular research by supporting the development of machine learning models capable of classifying or converting ECG images into time-series data, ensuring that valuable diagnostic information from non-digital archives is not lost. ECG-Image-Database aims to serve as a reference for ECG digitization efforts and opens new avenues for research in computerized ECG analysis and diagnosis. The dataset was used in the PhysioNet Challenge 2024 on ECG image digitization and classification.", "sections": [{"title": "1. Introduction", "content": "Cardiovascular diseases (CVDs) are the leading cause of death globally. To date, the electrocardiogram (ECG) remains the most accessible tool for CVD assessment, with over three million ECGs generated daily by clinicians [1], and millions more each day from wearable and personal devices. It is estimated that there are billions of digital diagnostic ECGs globally [2], with an equal number in hardcopy formats, including microfilm, paper, and scanned images [3]. Paper ECGs are gradually being replaced by digital ECGs (although much more slowly in low-income and low-resource regions), but legacy ECG records contain invaluable information on individual history, rare events, and the evolution of CVDs across generations and geography. Moreover, the proprietary nature of many ECG acquisition systems means that sharing the raw ECG is often difficult or costly, and printing out the ECG as a PDF, image, or on physical paper is frequently the only option for moving the data beyond the commercial walled gardens. Due to natural deterioration and a lack of funding for physical archives, decades of non-digital ECG archives worldwide will soon be destroyed before we can learn from them and leverage them for training machine learning (ML) models for algorithmic diagnosis of CVDs. This loss would be irreversible and an almost incalculable setback for cardiovascular research, as the ECG is the only biological signal that has been recorded for over a century without significant changes in its acquisition protocol. Over recent decades, most countries have experienced significant shifts in cardiovascular health, and longitudinal analysis of paper records has the potential for us to map these changes in an unprecedented manner. Moreover, the populations from which we can perhaps learn the most and which are least represented in commercial products are the most likely to have paper ECGs.\nModern ECG machines and wearable technologies enable the collection of massive digital ECG datasets. However, it will take decades for digital archives to begin replacing the lost diagnostic and demographic information from physical archives, especially in low-resource areas and low- and middle-income countries (LMICs), where electronic health records are rare, leading to the loss of demography-specific clinical data. Even in high-income regions, printed ECGs remain popular and are routinely circulated between experts as hardcopies or pictures for adjudication and training purposes. Recently, specialized public forums have formed on social media platforms, with hundreds of thousands of expert members who regularly share de-identified (protected health information redacted) ECG images and discuss diagnoses and clinical outcomes, serving as a new form of the traditional grand rounds or case studies in medical education and inpatient care.\nTo preserve printed ECGs, some research teams and healthcare systems in affluent regions scan and archive them in image formats. However, there are insufficient incentives to dedicate the time and effort to preserving most printed ECGs. Scanning alone is not enough to ensure the usefulness of an ECG because: i) ECG images are incompatible with state-of-the-art computerized ECG annotation software, which are typically trained on and analyze ECG time-series; ii) ECG images are not currently automatically searchable for"}, {"title": "2. Background and significance", "content": "To provide context, we review some of the major highlights of previous research on ECG image digitization and clinical measurements, which necessitate the development of standardized ECG image datasets.\nTraditional image processing and computer vision pipelines focus on digitizing ECG data by removing background grids and noise to extract clear signals. However, these approaches differ in their techniques and algorithms. Common methods include image processing techniques such as binary morphological image operations [7], grayscale thresholding [8], and linear filtering [9]. Additionally, several studies have used optical character recognition (OCR) to capture patient demographic information and integrate it into medical records [8, 9, 10]. Some methods have also considered cost-effective, non-hardware solutions [10, 11].\nFortune et al. in [12] developed an algorithm to capture ECG morphology and beat timing with high precision, validated through statistical measures. In [13], image processing techniques such as histogram filtering were applied to remove noise, allowing for efficient archival storage. Widman et al. in [14] employed optical scanners, demonstrating improved signal fidelity with adjustments in paper speed and amplifier gain. Reproducible methods for scanning and analyzing QT intervals were introduced in [15] and [16], enhancing the clinical utility of ECG digitization techniques. Lobodzinski et al. [17] developed an optical ECG waveform recognition method to digitize paper ECGs using statistical filtering and image processing. This was further enhanced in [18], where the authors proposed an XML-based format for storing the digitized data. Badilini et al. in [19] introduced ECGScan, employing active contour modeling to detect and extract ECG waveforms from paper records. Mitra et al. in [20] applied Fourier transform techniques to convert ECG images into digital signals, focusing on frequency analysis. Karsikas et al. in [21] used a digitization method involving scanning paper ECGs and applying image processing to correct misalignment and remove grid noise, preserving ECG signal integrity.\nMore recently, advanced machine learning pipelines have been proposed for converting ECG images into time-series data. As demonstrated by Helkeri et al. [22], traditional machine learning techniques can be leveraged to refine and improve the accuracy of clinical measurements from ECG images. Baydoun et al. in [23] applied neural networks to convert ECG scans into actionable data.\nHowever, these approaches have largely been constrained by the availability of small, manufacturer-specific datasets, limiting their broader applicability and generalizability. Despite these advances, the full potential of deep learning models such as ResNet [24] and large-scale datasets like ImageNet [25], which have significantly advanced image processing"}, {"title": "3. Generating realistic ECG images from time-series data", "content": "ECG-Image-Kit is an open-source toolbox developed to generate synthetic ECG images that closely replicate real-world ECG printouts [29, 31]. The primary purpose of this toolkit is to support the training and development of data intensive deep learning models for ECG digitization, addressing the critical need to convert legacy, non-digital ECG archives into digital formats compatible with modern diagnostic AI-ML tools. This tool was used to create the initial electronic version of the ECG-Image-Database presented in this work.\nThe toolkit creates realistic ECG images from time-series data, simulating the appearance of ECGs traditionally printed on thermal paper, or by modern inkjet and laser printers. To achieve this, ECG-Image-Kit introduces various customizable distortions that mimic the challenges encountered in real-world ECG digitization. The tool is able to overlay synthetic ECG images with both printed and handwritten text artifacts, including lead names, calibration pulses, patient information, and diagnostic notes. These artifacts are crucial in replicating the complexity of real paper ECGs, where such text often overlaps"}, {"title": "4. ECG time-series data", "content": "To create a rich dataset of ECG images that include various printing, scanning, and imaging artifacts, while having access to the ground-truth ECG as a time-series, two time-series datasets with clinical annotations were selected for this purpose. We used rejection sampling to extract a representative subset of 977 ECGs from the PTB-XL dataset and 1000 ECGs from the Emory dataset. These ECGs approximately preserved the univariate distributions of the patient attributes and classes from their source datasets. These datasets are described in the sequel."}, {"title": "4.1. The PTB-XL dataset", "content": "The PTB-XL dataset consists of 21,799 clinical 12-lead ECG records, each 10s long, from 18,869 patients [4]. These data were collected from Schiller AG devices between between October 1989 and June 1996. The patient group is 52% male and 48% female, ranging in age from 0 to 95 years (median age: 62). The dataset includes a broad range of heart conditions and healthy control samples, categorized into five main diagnostic classes: Normal ECG (9,514 records), Myocardial Infarction (5,469), ST/T Change (5,235), Conduction Disturbance (4,898), and Hypertrophy (2,649). The ECG data is stored in WFDB format at 500 Hz and downsampled versions at 100 Hz for convenience. Metadata for each record is found in a CSV file, including identifiers, demographic details, ECG diagnostics, and signal quality information. Additional fields track annotations such as heart axis, noise, and artifacts. The dataset includes a recommended 10-fold train-test split, with records in folds 9 and 10 having undergone human validation for label quality. PTB-XL+ [34, 35] is a supplementary dataset that provides additional ECG features and algorithmic annotations from the PTB-XL dataset using two commercial algorithms (University of Glasgow ECG Analysis Program version R30.4.2 [36] and GE Healthcare's Marquette\u2122\u2122 12SLT\u2122 [37]) and one open-source tool (ECGDeli version 1.1 [38]). The annotations include median beats, fiducial points, and automatic diagnostic statements, allowing users to train and evaluate machine learning models with the enhanced ECG metadata."}, {"title": "4.2. The Emory Healthcare dataset", "content": "We used a subset of 1,000 ECG records from the Emory Healthcare dataset, which features a diverse patient population in Georgia, USA. Access to this dataset was approved by Emory University's Internal Review Board under the PhysioCrowd protocol STUDY-00007353.\nThe Emory dataset consists of 12-lead clinical ECGs recorded between 2010 and 2022, by GE ECG machines of different generations, from healthcare subjects of different demographic background. The data were originally in XML format, containing ECG-based measurements and algorithmic annotations performed by GE's software. Due to the extended data collection period, we re-annotated the ECG time-series data using GE Healthcare's latest Marquette\u2122\u2122 12SLT\u2122 software, resulting in a unified relabeling of all the selected records."}, {"title": "5. Dataset preparation", "content": "The PTB-XL and Emory datasets were provided to ECG-Image-Kit, to generate the initial electronic version of the ECG images from time-series data, which were later printed in paper and used to create the various representations of the dataset for ECG image digitization assessment. The different electronic and physical variants of the dataset are detailed below.\nThe different variants of the images created by this procedure, along with their relationships, are illustrated in Figure 1. Examples of these ECG image variants are shown in Figure 2, for a sample record of the PTB-XL dataset. A similar procedure applies to the Emory Healthcare dataset."}, {"title": "5.1. Dataset naming and version control convention", "content": "ECG-Image-Database hosts various versions of ECG images generated from ECG time-series data. The dataset is version-controlled and will continue to evolve over time. To facilitate the comparison of algorithms trained and developed using this dataset, we have adopted a naming convention that reflects the various challenges in digitizing ECG images and to accommodate future expansions of the dataset. Our naming convention for the ECG image variants follows this format: D[m].[n].[p]-v[x], where m, n, and p denote the major, minor, and patch levels, respectively, and x denotes the dataset expansion version number.\nFor example, as detailed below, D1.1.0-v1 and D1.2.0-v1 represent the first versions of ECG images in .png format generated using ECG-Image-Kit with red and green grids, respectively. The digitization of both datasets is presumed to be at the same level of difficulty for well-designed digitization software. Therefore, they are both prefixed with \u2018D1'. Future expansions in the number of records in each group will be enumerated as D1.1.0-v2 and D1.2.0-v2, and so on. Minor modifications or patches (due to potentially erroneous records) will be enumerated as D1.2.0-v1, D1.1.1-v1, and so on.\nAny minor corrections or patches to previously released data will be listed as minor revisions or patches and reported in the release documentation. Updates to the dataset will also be tracked and documented publicly using a DOI for the updated versions, along with notes on the changes. Previous versions of the dataset will remain accessible via their respective DOIs."}, {"title": "5.2. Generating ECG images from time-series data", "content": ""}, {"title": "5.2.1. Images with red grid.", "content": "After selecting the ECG time-series records, the first electronic version of the dataset was generated using ECG-Image-Kit. For reproducibility, the subsequent steps for converting the time-series data into images is detailed below."}, {"title": "5.2.2. Images with green grid:", "content": "Some ECG device manufacturers use paper with non-red grid colors. ECG-Image-Kit supports arbitrary grid colors through its command line parameters."}, {"title": "5.3. Programmatic distorted variants of the ECG images", "content": "ECG-Image-Kit can be configured to generate realistic ECG image distortions that resemble real-world paper distortions. ECG-Image-Database hosts several variants of the PTB-XL and Emory datasets with these programmatically generated distortions, which are available for reference comparisons in ECG digitization research. Due to the stochastic feature of the toolbox in generating random distortions, similar commands can be used to generate much larger datasets with similar realistic distortions for training data-demanding deep models."}, {"title": "5.3.1. Creases:", "content": "Creases are simulated by adding evenly-spaced lines to mimic paper fold creases. Gaussian blurring, a common image augmentation technique is applied to these lines to introduce smoothing effects. The blurring enhances realism by creating a shadow-like effect in the creases, common in scanned images or real paper ECG. Wrinkles, on the other hand, are treated as textures, which can be generated using texture synthesis methods like image quilting. To add the wrinkles and creases to the generated images, we use the --wrinkles flag and set crease angle (-ca) to the desired angle (45 degrees in the example below):"}, {"title": "5.3.2. Rotations:", "content": "To add rotations to the generated images, we set the augment attribute and set rotation flag (-rot) to the desired angles (30 degrees in the example below), the cropping percentage (-c) value (0.1 in the example below) and select --deterministic_rot and --deterministic_noise flags. The deterministic flags ensure a deterministic behavior with no randomness, guaranteeing reproducibility of the process. The full command line is as follows:"}, {"title": "5.3.3. Handwritten text", "content": "To add handwritten text to the images, we set the hw_text flag and set the number of words to add (n) in handwritten style (4 in the example below), the horizontal offset of the added text (x_offset) and vertical offset (y_offset) in pixels (30 and 20 pixels, respectively, in the example below)."}, {"title": "5.4. Physically distorted variants of the ECG images", "content": "The next step was to introduce physical distortions, including imaging, scanning, photography artifacts, and natural deterioration effects to the ECG printouts. The different variants of the resulting datasets are explained below. We used these data as part of the hidden data for the PhysioNet Challenge 2024 [32]."}, {"title": "5.4.1. Scans of red grid printed in color.", "content": "The complete dataset, comprising both the Emory and PTB-XL databases, was printed using a LaserJet Pro printer (model M501) in color mode at a resolution of 600 dpi on US Letter-sized paper. The printed ECG images were subsequently scanned on a Brother scanner model MFC-L8900CDW in color mode, in batches of 50-100 images, and saved as multi-page PDF documents. These PDF files were then programmatically split into individual images. The QReader Python package, powered by the YOLOv8 model, was used to detect and extract QR codes containing individual file names from each separated image. These file names were then matched against the original dataset of waveform filenames to ensure the accuracy of the printing and scanning process, as well as to identify any missing images due to batch scanning failures or human errors. Next, the images were converted into JPEG format using the Python PIL package, applying a quality level of 90% (compression level of 10%). Human experts, familiar with ECG signals, performed a visual inspection of the compressed images to ensure that the readability of the signals was maintained. This process reduced the file sizes to approximately 2\u20133 megabytes per image, while preserving an acceptable level of detail for ECG interpretation and waveform detection."}, {"title": "5.4.2. Grayscale scans of red grid printed in color.", "content": "The entire printed dataset in color, including the Emory and PTB-XL databases, was rescanned in grayscale mode using a Brother scanner model MFC-L8900CDW. The same procedure used for the color scans was followed: the images were saved as multi-page PDF files, which were then split into individual images. An automatic QR code detection system was applied to extract the original waveform filenames from the images. The images were subsequently saved in JPEG format with a 90% quality level, using the original filenames obtained through QR code detection."}, {"title": "5.4.3. Photographs of red grid with four different cameras.", "content": "The red grid printed ECG images were photographed using four different mobile phones: Samsung Galaxy S20 Fe (12-megapixels camera, Emory dataset), iPhone 12 (12-megapixels camera, PTB-XL dataset), iPhone 13 Mini (12-megapixels camera, PTB-XL dataset), and Samsung S10+ (12-megapixels camera, PTB-XL dataset). The photos were taken in an office setting with ceiling lights. In some images, minor shadows from the cameras, people, or objects were present. These images were all renamed using the QR code detector and saved to JPEG with 90% quality level."}, {"title": "5.4.4. Soaking and staining distortions.", "content": "The color-printed ECG images were deliberately soaked in water and contaminated with coffee stains and soy sauce. This step was performed to replicate the random deterioration of paper ECGs that accidentally occurs in clinical settings with ECG printouts. The impact of water, coffee, and sauce stains varied across the images; some were significantly damaged, while others showed moderate or very minor"}, {"title": "5.4.5. Photographs of stained and damaged ECG papers.", "content": "The contaminated PTB-XL printouts were photographed using a Samsung Galaxy S10+ mobile phone under sunlight, partial shade, and ceiling light. Most of the printouts remained moist or partially wet at this stage. The photos were taken at a distance that allowed the entire paper to fit within the frame of the mobile phone camera. The photographs were captured from different angles and under various lighting conditions (at night, dawn, and early morning) with varying light levels, resulting in shadows and partial shading in some images. There was no specific order or set of instructions for photographing the ECG records, other than capturing a full image of the ECGs. The photographers were given the liberty to take pictures of a quality that would be considered \u201cacceptable\" by people familiar with ECG data."}, {"title": "5.4.6. Growing mold and further drying.", "content": "The partially moist ECG papers (after being stained with water, coffee, and sauce) were packed into envelopes, with approximately 100 images in each. The envelopes were sealed, stacked, and stored in a humid place for six weeks. This caused many of the papers to develop small, moderate, or significant amounts of mold, further deteriorating the printouts. In some cases, the mold on the coffee and sauce stains interfered with the ECG waveforms, resembling extreme natural deterioration of paper ECGs over time, as seen in damp historical hospital archives. The level of deterioration varied across the records. Some papers, which were completely dry before packing, did not develop any mold."}, {"title": "5.4.7. Scans of moldy red grid; black-and-white and color.", "content": "The moldy, printed ECG images were scanned using a Brother scanner model MFC-L8900CDW. Batches of 10-20 printed images were fed into the scanner to generate multi-page PDFs. Images were first scanned on the color setting at 600 dpi resolution. Images underwent a second round of scanning on the black and white setting at 600 dpi resolution. Due to their fragile nature, the moldy, printed ECG images caused frequent scanner jams and were often creased during the scanning process. Two printed ECGs were destroyed due to paper jamming during the scanning process. These images were re-preinted and contaminated with stain and rescanned. The resulting PDF files were split into separate image files, renamed using the QR code detector, and saved to JPEG with 90% quality level."}, {"title": "5.4.8. Photographs of moldy red grid images.", "content": "The printed, moldy ECG images were photographed using two different mobile phones: an iPhone 11 and a Samsung Galaxy S20 FE, both with 12 megapixels."}, {"title": "5.4.9. Photographs of ECG on computer monitors with red grid.", "content": "Taking pictures of ECG monitors and sharing them with peers for review or with medical trainees is a common practice in clinical settings. Technologically, monitors refresh their screens at high rates (e.g., 60 Hz or higher) to provide high-quality images with seamless transitions for the human eye. However, because mobile phones and cameras capture images in very short snapshots (comparable with monitor refresh rates), with exposure times that are not synchronized with the monitor's refresh patterns, photos of monitors are susceptible to unique aliasing and imaging artifacts that affect the color and resolution of the captured images.\nTo replicate this effect, the Emory and PTB-XL images were displayed on a Lenovo LT2252PWA 22-inch monitor (1680 \u00d7 1050) and photographed using a Samsung S22 Ultra mobile phone camera at a resolution of 12M pixels main camera. To simulate diverse real-life conditions, the lighting, the monitor's angle relative to the external light source, and the phone's orientation (landscape or portrait, each with slight variations in angle) were arbitrarily varied by the photographer. The camera distance from the monitor varied between 10 and 15 inches, depending primarily on the orientation. Some blurring was also introduced in some photographs due to hand motion artifacts."}, {"title": "6. Discussion", "content": "The ECG-Image-Database provides a valuable contribution to the ongoing effort to digitize and analyze non-digital ECG data, which was the focus of the PhysioNet 2024 Challenge on ECG image digitization and classification [32]. With the increasing availability of digital ECGs from modern clinical devices, there remains a vast amount of historical ECG data that exists solely in paper or scanned image form, or ECGs in electronic health records for which the underlying ECG time-series has not been stored or maintained. These legacy records contain vital information about patient health histories, rare cardiovascular events, and demographic-specific data that are irreplaceable for longitudinal studies and population-wide analyses. Without proper digitization and analysis tools, much of this valuable information could be lost due to physical deterioration or obsolescence of archival systems.\nThe development of ECG-Image-Database is aimed at addressing several key challenges in the field of ECG digitization. First, by generating a large collection of high-fidelity synthetic ECG images paired with the underlying ECG images and realistic physical and electronic distortions, the dataset simulates the types of real-world artifacts commonly encountered in clinical practice. This includes noise, wrinkles, stains, perspective shifts, and other degradations that often complicate the digitization process. In addition to digitally induced distortions, the inclusion of physically altered images-through soaking, staining, and mold exposure\u2014provides a comprehensive collection of ECG images that mirror those"}, {"title": "7. Conclusion", "content": "The ECG-Image-Database offers a standardized resource for developing machine and deep learning models to digitize and classify ECG images, by reproducing real-world challenges such as image distortions, noises, and environmental artifacts. By replicating the conditions under which ECGs are stored and scanned, the dataset ensures models trained on it can effectively handle real-world complexities, preserving valuable diagnostic information from paper-based ECG records.\nFuture expansions, including new image sources and further environmental variations, will enhance the dataset's utility for training models that generalize across diverse conditions, supporting both clinical and low-resource applications in cardiovascular diagnostics."}, {"title": "Appendix A. A note on ECG image vs time-series resolutions", "content": ""}, {"title": "Appendix A.1. ECG as a time-series", "content": "The ECG recorded by standard body surface leads typically has an amplitude of several millivolts, and its spectral content ranges from approximately 0.05 Hz to around 150 Hz. When transformed into a digital signal, it first passes through an anti-aliasing low-pass filter (in the analog domain) before being sampled at a sampling frequency $f_s$. According to the Nyquist theorem, the cutoff frequency of the anti-aliasing filter should be lower than $f_s/2$. Systems with lower resolution, such as old single-lead ECG machines, Holder monitors and wearable devices, may have sampling frequencies as low as 100 Hz. Modern high-quality clinical monitors often sample at higher frequencies, such as 1000 Hz or higher.\nThe amplitude resolution of the digital signal depends on the number of bits of the analog-to-digital converter (ADC), which we denote by $N$. In older ECG devices, $N$ was as low as 8 bits, resulting in a maximum resolution of 256 quantization levels (assuming that the ECG was amplified to span the ADC's full dynamic range). In modern ECG devices, $N$ can be up to 24 bits, resulting in significantly better amplitude resolution. Importantly, due to electronic and thermal noise, and depending on the quality of the analog front-end circuitry, the effective number of bits (ENOB) is, in practice, lower than the nominal ADC bit number $N$. For instance, a 16-bit ADC may yield between 12.5 to 14 ENOBs in practice. With an $N$-bit ADC digitizing the input voltage range of $V_{min}$ to $V_{max}$, the voltage resolution after digitization is:\n$\\delta v = \\frac{V_{max} - V_{min}}{2^N}$\nFor example, if the input span of the ADC is $\\Delta V = V_{max} -V_{min} = 5mV$, an 8-bit ADC yields a voltage resolution of $\\delta v = 19.5\\mu V$, while a 12-bit ADC yield $\\delta v = 1.22\\mu V$, improving the voltage resolution by a factor of 16."}, {"title": "Appendix A.2. Printed ECG", "content": "In clinical applications, the ECG is printed on standard ECG paper featuring fine grids and coarse grids. The fine grids are 1 mm by 1 mm, corresponding to 0.1 mV in amplitude and 40 ms in time. The coarse grids are 5mm by 5mm, corresponding to 0.5 mV in amplitude and 200 ms in time, as shown in the image below.\nIn modern ECG devices, despite the data being collected digitally and stored on computers or other digital platforms, the same convention is used. ECGs are visualized against the same background grids, whether displayed on a computer screen or printed as a PDF or image file. The number and format of the leads, along with the ECG grid color, vary depending on the ECG acquisition technology and the device manufacturer. The most common clinical ECGs are 12-lead, featuring approximately 2.5s segments of the 12 leads arranged in a 3-row by 4-column grid. Additionally, one to three leads (typically leads II,"}, {"title": "Appendix A.3. Scanned ECG images", "content": "Printing an analog or digital ECG on paper and then rescanning it as an image involves implicit or explicit interpolation and resampling of the original ECG. When performed by an analog machine or a standard printer, it involves digital-to-analog circuitry to convert the discrete time samples into a continuous waveform, printed as a continuous curve on the paper. Once an ECG is printed, the original sampling frequency $f_s$ of the digital time series, and the number of its quantization bits $N$, become irrelevant, as the signal has been transformed back into the continuous-time domain. When the ECG paper is scanned or photographed as an image, it is essentially being quantized and resampled again, this time as a two-dimensional image. Assuming the image is scanned at a resolution of $D$ dots per inch (DPI), each 1-inch by 1-inch square of the printed ECG is quantized into a $D \\times D$ array, each pixel stored in $B$ bits (this is different from $N$, the number of ADC bits). If scanned as a color image, there will be three such arrays, corresponding to the colors red, green, and blue. Modern images typically use $B = 8$, resulting in 24 bits, or 3 bytes, per pixel. Therefore, for example, scanning a letter-size paper of 11 inches by 8.5 inches at 72 DPI would require 8.5 \u00d7 11 \u00d7 (72 \u00d7 72) \u00d7 3 bytes = 1,454,112 bytes, or 1.39 MB, as an uncompressed bitmap file (excluding any metadata or other headers stored in the file). In practice, fewer bits, corresponding to a lower color depth, and/or lossy or lossless image compression, can reduce the storage needed to save such an image with minimal or no loss of information, although scanning and other artifacts may impede compression.\nTherefore, when a standard ECG, printed on A4 or letter-size paper, is scanned at full image size (without any cropping or excess borders), each 1 inch (25.4 mm) horizontally and vertically maps to $D$ pixels. In other words, each coarse square of the ECG (0.5mV in amplitude and 200 ms in time) maps to a square of $(\\frac{5 \\times D}{25.4}) \\times (\\frac{5 \\times D}{25.4})$ pixels. This means the"}, {"title": "", "content": "amplitude resolution of the scanned ECG is:\n$\\displaystyle dv = \\frac{2.54mV}{D}$ (A.1)\nand the temporal resolution is $dt = \\frac{1.016s}{D}$ seconds, or equivalently, the sampling frequency (in Hertz) of the ECG (as an image) is:\n$\\displaystyle f^{\\prime} = \\frac{D}{1.016}$ (A.2)\nAs we can see, the effective sampling frequency $f^{\\prime}$ of the scanned ECG is independent of the original digital signal's sampling frequency $f_s$. However, since the original signal's frequency range was limited to $f_s/2$ by the analog front-end anti-aliasing filter, increasing $D$, and therefore $f^{\\prime}$, will yield smoother waveforms, but it will not add any information beyond $f_s/2$ (the Nyquist rate of the original digital ECG time-series).\nFrom (A.2), it is evident that typical image resolutions, such as 72 or 96 DPI, which are common in image analysis applications, are quite low for ECG scanning and digitization applications, as they only provide sampling frequencies of 70.9 Hz and 94.5 Hz, respectively. An ECG should be captured at least 125 Hz, although 250 Hz, or even 500 Hz, is preferable in adults. Based on this analysis, a resolution of at least 150 DPI or higher, without any lossy compression, is recommended for ECG scanning and digitization purposes.\nImportantly, the calculation of the ECG grid size from the image DPI and paper size is accurate only when using a standard full-paper size scanner. For ECG images captured by cameras, smartphones, screenshots, or through cropping and resizing, the equivalency of 1 inch on the actual paper to the captured image DPI may not hold true. Consequently, ECG digitization algorithms should estimate the correct grid sizes by employing algorithms that detect and analyze the ECG grid sizes directly from the ECG image. Several functions for this purpose are provided in ECG-Image-Kit [31]. Further details and examples can be followed from [29]."}]}