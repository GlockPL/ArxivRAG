{"title": "SA-GDA: Spectral Augmentation for Graph Domain Adaptation", "authors": ["Jinhui Pang", "Zixuan Wang", "Jiliang Tang", "Mingyan Xiao", "Nan Yin"], "abstract": "Graph neural networks (GNNs) have achieved impressive impressions for graph-related tasks. However, most GNNs are primarily studied under the cases of signal domain with supervised training, which requires abundant task-specific labels and is difficult to transfer to other domains. There are few works focused on domain adaptation for graph node classification. They mainly focused on aligning the feature space of the source and target domains, without considering the feature alignment between different categories, which may lead to confusion of classification in the target domain. However, due to the scarcity of labels of the target domain, we cannot directly perform effective alignment of categories from different domains, which makes the problem more challenging. In this paper, we present the Spectral Augmentation for Graph Domain Adaptation (SA-GDA) for graph node classification. First, we observe that nodes with the same category in different domains exhibit similar characteristics in the spectral domain, while different classes are quite different. Following the observation, we align the category feature space of different domains in the spectral domain instead of aligning the whole features space, and we theoretical proof the stability of proposed SA-GDA. Then, we develop a dual graph convolutional network to jointly exploits local and global consistency for feature aggregation. Last, we utilize a domain classifier with an adversarial learning submodule to facilitate knowledge transfer between different domain graphs. Experimental results on a variety of publicly available datasets reveal the effectiveness of our SA-GDA.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are more and more popular due to their capacity to represent structured and relational data in a wide range of fields [2, 26, 41, 49, 50]. As a basic problem, graph node classification aims to predict the category of each node and has already been widely discussed in various fields, such as protein-protein interaction networks [6, 10], citation networks [12, 18], and time series prediction [43, 48]. They typically follow the message-passing mechanisms and learn the discriminative node representation for classification.\nAlthough existing GNNs exhibit impressive performance, they typically rely on supervised training methods [16, 42], which necessitates a large amount of labeled nodes. Unfortunately, labeling can be a costly and time-consuming process in many scientific domains [15, 33]. Additionally, annotating graphs in certain disciplines often requires specialized domain knowledge, which prevents a well-trained model from being transferable to a new graph. To address this issue, we investigate the problem of unsupervised domain adaptation for graph node classification, which utilizes labeled source data and unlabeled target data to accomplish accurate classification on the target domain.\nThere are limited works attempting to apply domain adaptation for graph-structure data learning [3, 28, 38]. CDNE [28] learns the cross-domain node embedding by minimizing the maximum mean discrepancy (MMD) loss, which cannot jointly model graph structures and node attributes, limiting the modeling capacity. AdaGCN [3] uses GNN as a feature extractor to learn node representations and utilizes adversarial learning to learn domain invariant node representation. UDA-GCN [38] utilizes the random walk method to capture the local and global information to enhance the representation ability of nodes. Though these methods have achieved good results, there are still two fundamental problems: (1) How to align the category feature between two distinct domains."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 Graph Neural Networks", "content": "Graph neural networks (GNNs) [53] are designed to learn node embeddings in graph-structured data by mapping graph nodes with neighbor relationships to the low-dimensional latent space. Many approaches have been proposed to generalize the well-developed neural network models for regular grid structures, enabling them to utilize graph structured data for many downstream tasks, such as node classification and relationship prediction [40, 44], graph label noisy learning [44, 47]. GCN [18] integrates node features as well as graph topology on graph-structured data and exploits two graph convolutional layers for node classification tasks by semi-supervised learning. GAT [36] enhances GCN in terms of message passing between nodes, using an attention mechanism to automatically integrate the features of the neighbors of a certain node. However, most existing GNN-based approaches focus on learning the node representation among a particular graph. When transferring the learned model between different graphs to perform the same downstream task, representation space drift and embedding distribution discrepancy may be encountered."}, {"title": "2.2 Unsupervised Domain Adaptation", "content": "Unsupervised domain adaptation is one of the transfer learning methods that aims to minimize the discrepancy between the source and target domains, thus transferring knowledge from the well-labeled source domain to the unlabeled target domain [4, 31, 46, 51]. To perform cross-domain classification tasks, methods based on deep feature representation[30], which map different domains into a common feature space, have attracted extensive attention. Most of these methods utilize adversarial training to reduce the inter-domain discrepancy[23]. Typically, DANN[8] utilizes a gradient reversal layer to capture domain invariant features, where the gradients are back-propagated from the domain classifier in a minimax game of the domain classifier and the feature extractor.\nRecently, for graph-structured data, several studies have been proposed for cross-graph knowledge transfer via unsupervised domain adaptation methods [3, 28, 38, 45, 46]. CDNE [28] learns transferable node representations through minimizing the maximum mean discrepancy (MMD) loss for cross-network learning tasks, yet cannot model the network topology. To enhance CDNE, AdaGCN[3] utilizes graph convolutional networks for feature extraction to learn the node representations, and then to learn domain-invariant node"}, {"title": "3 METHODOLOGY", "content": null}, {"title": "3.1 Problem Definition", "content": "Graph Node Classification: Given a graph $G = (V, &, A, X, Y)$ with the set of nodes V and the set of edges &. $A \\in \\mathbb{R}^{N \\times N}$ is the adjacency matrix of G, D is the degree matrix with $D_{ii} = \\sum_{j=0}^{N} A_{ij}$ and N denotes the number of nodes. The node feature denotes as $X \\in \\mathbb{R}^{N \\times d}$, where each row $x_v \\in \\mathbb{R}^d$ represents the feature vector of node v \u2208 V and d is the dimension of node features. The normalized graph Laplacian matrix is defined $L = I \u2013 D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, I is the identity matrix. By factorization of L, we have L = UAUT, where U = [u1,\u2026\u2026, un], $\u039b$ = diag([\u03bb1,\u00b7\u00b7\u00b7, \u03bbn]), $u_i$ is the eigenvector and \u03bbi is the corresponding eigenvalue. $Y \\in \\mathbb{R}^{N \\times C}$ is the label of G, C is the number of classes.\nGraph Domain Adaptation for Node Classification: Given the fully labeled source graph $G_s = (V_s, &s, A_s, X_s, Y_s)$ and unlabeled target graph $G_t = (V_t, &t, A_t, X_t)$, which shares the same"}, {"title": "3.2 Overview", "content": "The two key difficulties of unsupervised domain adaptation for node classification are the challenging category alignment of graph data and the inefficient node classification with label scarcity. To tackle these challenges, we propose a novel model termed SA-GDA. Our proposed SA-GDA consists of a spectral augmentation module, a dual graph learning module, and a domain adversarial module. The spectral augmentation module combines the spectral features between source and target domain to implement the category feature alignment. The dual graph module aims to learn the consistency representation from local and global perspectives, and the domain adversarial module to differentiate the source and target domains."}, {"title": "3.3 Spectral Augmentation for Category Alignment", "content": "Domain adaptation methods in the past have typically used the pseudo-labeling mechanism [21, 54] to align the feature space of categories. They involve assigning pseudo-labels to unlabeled data and using the labels to supervise the neural network. However, the risk of overconfidence and noise in these pseudo-labels would lead to potential biases during optimization and ultimately affect performance. Furthermore, in the graph domain, the lack of labels can worsen the noise issue."}, {"title": "3.4 Attention-based Dual Graph Learning", "content": "To capture the local and global information of nodes, we propose a dual graph neural network, where the local consistency is extracted with the graph adjacency matrix and the global with the random walk GNN. Due to the source domain graph being fused"}, {"title": "3.5 Domain Adversarial Training", "content": "The spectral augmentation of the category alignment module enforces the similarity of features across different domains, confusing source and target domain features. Our goal is to maximize the domain classification error while minimizing the source domain classification error, i.e.,\n$\\min_{\\Theta_y} \\max_{\\Theta_d} = L_s (f(z_s; \\Theta_y)) \u2013 \\gamma L_D(h(z; \\Theta_d)),$  (8)\nwhere $f(z_s; y) = softmax(@yz_s)$ is the source classification function, and by is the trainable parameters for f. $h(z; d) = sigmod(@dz)$ is the domain classifier, d is the parameters for h. Ls and LD are"}, {"title": "3.6 Optimization", "content": "Our SA-GDA has the overarching goal that combines the domain adversarial loss and classification loss of the target data. Additionally, minimizing the expected source error for labeled source samples is also essential. In formation,\n$\\min_{\\Theta_y} \\max_{\\Theta_d} L(\\Theta_y, \\Theta_d) =L_s(f(z_s; \\Theta_y)) + \\gamma_1 L_t (f(z_t; \\Theta_y))\n- \\gamma_2 L_D ((h(z; \\Theta_d)),$ (9)\nwith the source loss $L_s$, target loss $L_t$ and domain loss $L_D$:\n$L_s = \\frac{1}{N_s} \\sum_{i=1}^{N_s} y_i log(f(z_s)),$\n$L_t = \\frac{1}{N_t} \\sum_{i=1}^{N_t} f(z_t)log(f(z_t)),$\n$L_D = \\frac{1}{N_s+N_t} \\sum_{i=1}^{N_s+N_t} d_i log(\\frac{1}{h(z)}) + (1 - d_i) log(\\frac{1}{1-h(z)}),$\nwhere \u03b31 and \u03b32 are hyper-parameters to balance the domain adversarial loss and target classification loss. di \u2208 {0, 1} denotes node i belongs to the source or target domain. To update the parameters of Eq. 9 in the standard stochastic gradient descent (SGD) method, we apply the Gradient Reversal Layer [8] (GRL) R for model training, which is formulate as:\n$R(x) = x, \\frac{dR}{dx} = -I.$ (10)\nThen, the update of Eq. 9 can be implemented with SGD with the following formulation:\n$\\min_{\\Theta_y,\\Theta_d} L(\\Theta_y, \\Theta_d) =L_s(f(R(z_s);\\Theta_y)) + \\gamma_1 L_t (f(R(z_t);\\Theta_y))\n-\\gamma_2 L_D((h(R(z); \\Theta_d)).$  (11)\nThe whole learning procedure is shown in Algorithm 1."}, {"title": "3.7 Theoretical Analysis", "content": "In this subsection, we proof the stability of SA-GDA, which is inspired by [52]:\nLEMMA 1. Suppose the Gs and Gt are the graphs of the source and target domain. Given the Laplace matrices decomposition $L_d = D_d \u2013 A_d = U_d\u039b_d U^T_d$, where $\u039b_d = diag[\u03bb_{d1},\u2026, \u03bb_{dn}]$ are the sorted eigenvalues of $L_d$, andd \u2208 {s, d} denotes the source and target domains. The GNN is constructed as f (G) = \u03c3((90(L)XW) = \u03c3(U90(A)UTXW), where $g_\u03b8$ is the polynomial function with $g_\u03b8(L) = \\sum_{k=0}^{K} \u03b8_kL^k$, W is the learnable matrix and the pointwise nonlinearity has |\u03c3(b) \u2013 \u03c3(a)| \u2264 |b - a|. Assuming ||X||op \u2264 1 and ||W||op \u2264 1, we have the following inequality:\n||f(Gs + Gt) \u2212 f(Gt)||_2 \u2264\u03b1[Cx(1 + \u03c4)||Ls \u2013 P*LtP*T ||_F\n+O(||Ls \u2013 P*LtP*T||)\n+ \u0442\u0430\u0445(|ge(Lt)|)||Xs \u2013 P*Xt||_F], (12)\nwhere \u03c4 = ($(||U_s \u2013 U_t||_F + 1)^2 \u2013 1$) stands for the eigenvector misalignment which can be bounded. \u041f is the set of permutation matrices, and $P^* = argmin_{P\u2208\u03a0}||X_s \u2013 PX_t||_F + ||A_s - PA_tP^T ||_F$. O(||Ls \u2013 P*LtP*T||) is the remainder term with bounded multipliers defined in [7], and Cx is the spectral Lipschitz constant that \u2200xi, dj, |9\u0473 (Xi) \u2013 9\u0473(\u03bbj)| \u2264 C\u03bb(\u03bb\u03af \u2013 \u03bbj)."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct extensive experiments on various real-world datasets to verify the effectiveness of the proposed SA-GDA. We aim to answer the questions below:\n\u2022 RQ1: How does the proposed SA-GDA perform compared with the state-of-the-art baseline methods for node classification?\n\u2022 RQ2: How is the effectiveness of proposed components on the performance?\n\u2022 RQ3: How does the hyper-parameters affect the performance of the proposed SA-GDA?\n\u2022 RQ4: How about the intuitive effect of proposed SA-GDA?"}, {"title": "4.1 Benchmark Datasets", "content": "To evaluate the effectiveness of the proposed SA-GDA, we have developed three citation networks from ArnetMiner[35], including DBLPv7, ACMv9, and Citationv1. These datasets are derived from different data sources (DBLP, ACM and Microsoft Academic Graph) and distinct time periods. Each sample contains a title, an index, a category and a citation index, where the category represents the academic field, including \"Artificial intelligence\", \"Computer vision\", \"Database\", \"Data mining\", \"High Performance Computing\" and \"Information Security\". Following [39], these datasets are constructed"}, {"title": "4.2 Baselines", "content": "To verify the effectiveness of our method, we select the following methods as baselines for comparison, including seven state-of-the-art single-domain node classification methods (i.e., DeepWalk[27], LINE[34], GraphSAGE[13], DNN[24], GCN[18], DGC[37], and SUBLIME[22]), and three cross-domain classification methods with domain adaptation (i.e., DGRL[9], AdaGCN[32] and UDA-GCN[38]).\nSingle-domain node classification methods:\n\u2022 DeepWalk[27]: DeepWalk employs the random walk sampling strategy to capture the neighborhood node structure. Then, following Skip-Gram[11], which aims to learn the low-dimensional node representation on single domain.\n\u2022 LINE [34]: LINE is a classic method for large-scale graph representation learning, which preserves both first and second-order proximities for undirected network to measure the relationships between two nodes. Compared with the deep model, LINE has a limited representation ability.\n\u2022 GraphSAGE[13]: GraphSAGE learns the node representation by aggregating the sampled neighbors for final prediction.\n\u2022 DNN[24]: DNN is a multi-layer perceptron (MLP) based method, which only leverages node features for node classification.\n\u2022 GCN[18]: GCN is a deep convolutional network on graphs, which employs the symmetric-normalized aggregation method to learn the embedding for each node.\n\u2022 DGC[37]: DGC is a linear variant of GCN, which separates the feature propagation steps from the terminal time, enhancing flexibility and enabling it to utilize a vast range of feature propagation steps.\n\u2022 SUBLIME[22]: SUBLIME generates an anchor graph from the raw data and uses the contrastive loss to optimize the consistency"}, {"title": "4.3 Experimental Settings", "content": "We utilize PyTorch [25] and PyTorch Geometric library [5] as the deep-learning Framework and Adam [17] as an optimizer. We follow the principles of evaluation protocols used in unsupervised domain adaptation to perform a grid study of all methods in the hyperparameter space and show the best results obtained for each method. To be fair, for all the cross-domain node classification methods in this experiment, we use the same parameter settings, except for some special cases. For each method, we set the learning rate to 1e-4. For the specific GCN-based models (e.g., GCN, DGC, SUBLIME, AdaGCN, and UDA-GCN), the number of hidden layers is set to 2, and the hidden dimensions are set to 128 and 16 regardless of the source and target domains. We use the same settings as above for both the local and global GNN. For DeepWalk and LINE, we first learn node embeddings and then train node classifiers using information from source domain, the hidden dimension of the node embeddings is set to 128. For DNN and DGRL, we set the same hidden dimensions as GNN-based models. The balance parameters Y1, Y2 are set to 0.3 and 0.1, and the dropout rate for each layer of dual GNN to 0.3. For simplicity, we set the combination weight of high and low-frequency as the same, i.e., a = \u03b2."}, {"title": "4.4 Performance Comparison (RQ1)", "content": "We present the performance of SA-GDA compared with baselines under the setting of graph domain adaptation for node classification to answer RQ1, which is shown in Table 2. From the results:\n\u2022 DeepWalk and LINE achieve the worst performance among all the methods, this is because they only utilize the network topology to generate node embedding without exploiting the node feature information. DNN also achieves a poor performance, contrary to the above two methods, it only considers the node features and ignores the association between nodes, so it cannot generate better node embeddings.\n\u2022 The graph-based methods (GCN, GraphSAGE, DGC, and SUBLIME) outperform DeepWalk and LINE, we attribute the reason to that they encode both local graph structure and node features to obtain better node embeddings.\n\u2022 DGRL, AdaGCN, and UDA-GCN achieve better performance than the single-domain node classification methods. The reason is that, by incorporating the domain adversarial learning, the node classifier is capable to transfer the knowledge from the source to the target domain.\n\u2022 The proposed SA-GDA model achieves the best performance in these six cross-domain node classification datasets compared to all the baseline methods. By fusing the low and high-frequency signals of different domains in the spectral domain, SA-GDA implements the category alignment under the domain-variant setting. This enables us to learn better graph node embeddings and to train node classifiers in both the source and target domains through an adversarial approach, which greatly reduces the distribution discrepancies between the different domains and improves the node classifier performance."}, {"title": "4.5 Ablation Study (RQ2)", "content": "To answer RQ2, we introduce several variants of the SA-GDA to investigate the effectiveness of each component of SA-GDA:\n\u2022 SA-GDA-l, which evaluates the impact of low-frequency signals in both source and target domains.\n\u2022 SA-GDA-h, evaluating the influence of high-frequency signals in different domains.\n\u2022 SA-GDA-p, which remove the global GNN and utilizing only the local GNN.\n\u2022 SA-GDA-d, which removes the domain loss to evaluate the impact of domain adversarial learning.\n\u2022 SA-GDA-t, which evaluate the effectiveness of target classifier by removing the target loss."}, {"title": "4.6 Sensitivity Analysis (RQ3)", "content": "To answer RQ3, we conduct experiments to evaluate how the hyper-parameters \u03b1, \u03b2 and y1, y2 affect the performance of proposed SA-GDA. a and \u03b2 control how much of the high and low-frequency combined in the spectral augmentation module, and y1, Y2 control the balance of target classification and domain adversarial loss. In the implementation, we set a = \u03b2 and vary a in {0, 0.2, 0.4, 0.6, 0.8, 1} with other parameters fixed. Besides, we set Y1 and y2 in {0.1, 0.3, 0.5, 0.7, 0.9} respectively. The results is shown in Fig. 4, from the results, we have the following observation:\n\u2022 Impact of Spectral Augmentation Ratio a: Fig. 4(a) shows the impact of a, we observe that the classification accuracy improves gradually when a changes from 0 to 0.8, while decrease when a ranges in {0.8, 1}. This is because SA-GDA needs more spectral information from the source domain for training, while if a is too large, the effectiveness of the spectral augmentation would be weakened, resulting the poor performance. Hence, we set a to 0.8 in our implementation.\n\u2022 Impact of Balance Ratio y1 and 2: We conduct a large number of experiments by setting y\u2081 and y2 in {0.1, 0.3, 0.5, 0.7, 0.9} respectively, and report the best results as shown in Fig. 4(b). From Fig. 4(b), we find that, when y2 fixed at 0.1, the performance tends to increase first and then decrease in most cases. The reason is that a small y\u2081 would incorporate the target information for classification while a large y\u2081 may introduce more uncertain information for the scarcity of target labels. Thus, we set y\u2081 to 0.3 as default. Similar to y1, we change the value of y2 with y1 fixed to 0.3. We observe that increasing y2 results in better performance when it is small, indicating that the domain classifier would help to improve the accuracy. However, too large y2 may hurt the performance, which demonstrates that too large of domain loss could harm the discrimination information. Thus, we set y2 to 0.1 as default."}, {"title": "4.7 Visualization (RQ4)", "content": "In order to achieve an intuitive effect of proposed SA-GDA, we visualize the node representation learned in the target domain to"}, {"title": "5 CONCLUSION", "content": "In this paper, we study a less-explored problem of unsupervised domain adaptation for graph node classification and propose a novel framework SA-GDA to solve the problem. First, we find the inherent features of same category in different domains, i.e., the correlation of nodes with same category is high in spectral domain, while different categories are distinct. Following the observation, we apply spectral augmentation for category alignment instead of whole feature space alignment. Second, the dual graph extract the local and global information of graphs to learn a better representation for nodes. Last, by using the domain adversarial learning cooperating with source and target classification loss, we are able to reduce the domain discrepancy and achieve the domain adaptation. We conduct extensive experiments on real-world datasets, the results show that our proposed SA-GDA outperforms the existing cross domain node classification methods. Although the proposed SA-GDA has achieved impressive results, the complexity of matrix decomposition is high. In the future, we will explore more efficient spectral domain alignment methods and utilize spatial domain features to assist the alignment of category features in different domains, thereby reducing the model complexity."}, {"title": "6 ACKNOWLEDGE", "content": "This work was supported by the National Key R&D Program of China under Grant No. 2020AAA0108600."}, {"title": "A THEORETICAL ANALYSIS", "content": "In this subsection, we proof the stability of SA-GDA, which is inspired by [52]:\nLEMMA 2. Suppose the Gs and Gt are the graphs of the source and target domain. Given the Laplace matrices decomposition $L_d = D_d \u2013 A_d = U_d\u039b_d U^T_d$, where $\u039b_d = diag[\u03bb_{d1},\u2026, \u03bb_{dn}]$ are the sorted eigenvalues of $L_d$, andd \u2208 {s,d} denotes the source and target domains. The GNN is constructed as f (G) = \u03c3((90(L)XW) = \u03c3(Uge(A)UTXW), where go is the polynomial function with $90(L) = \\sum_{k=0}^{K} \u03b8_kL^k$, W is the learnable matrix and the pointwise nonlinearity has |\u03c3(b) \u2013 \u03c3(a)| \u2264 |b - a|. Assuming ||X||op \u2264 1 and ||W||op \u2264 1, we have the following inequality:\n||f(Gs + Gt) \u2212 f(Gt)||_2 \u2264\u03b1[Cx(1 + \u03c4)||Ls \u2013 P*LtP*T ||_F\n+O(||Ls \u2013 P*LtP*T||)\n+ \u0442\u0430\u0445(|ge(Lt)|)||Xs \u2013 P*Xt||_F], (13)\nwhere \u03c4 = ($(||U_s \u2013 U_t||_F + 1)^2 \u2013 1$) stands for the eigenvector misalignment which can be bounded. \u041f is the set of permutation matrices, and $P^* = argmin_{P\u2208\u03a0}||X_s \u2013 PX_t||_F + ||A_s - PA_tP^T ||_F$. O(||Ls \u2013 P*LtP*T||) is the remainder term with bounded multipliers defined in [7], and Cx is the spectral Lipschitz constant that \u2200xi, dj, |9\u0473 (Xi) \u2013 9\u0473(\u03bbj)| \u2264 C\u03bb(\u03bb\u03af \u2013 \u03bbj).\nProof. Similar to [52], we use P* as the optimal permutation matrix for Gs and Gt. With Eq. 2, we have the difference of GNN:\n||f(Gs + Gt) - f(Gt)||_2\n=||0(Us [ag (As)UX5W + (1 \u2212 a)g (A+)UXW\n+ \u03b2g (As)UX5W + (1 \u2212 \u03b2)g(A+)U X\u2081W])\n\u03c3(Ut90(At)U XW) ||_2, (14)\nwith the triangle inequality and the assumption |\u03c3(b) \u2013 \u03c3(\u03b1)| \u2264 bal, Va, b \u2208 R, we have:\nEq.14 <||Us [ag (As)U X5W + (1 \u2212 a)g (At)U X\u0141W\n+ \u03b2g(As)UX5W + (1 \u2212 \u03b2)g(A+)U X\u2081W]\n\u03c3(Ut90(At)U XW)||_F (setting a = \u03b2)\n=||Us [a(ga (As) + g(As))UX5W\n+ (1 \u2212 a) (9 (At) + g(A+))UT X\u2081W]\n\u2013 Ut90(At)UX+W||_F, (15)\nwhere gf and go denote the high-pass filter and low-pass filter, which can be designed manually. Setting $g_H^+g_L^+ = g_\u03b8$, then Eq.15 holds:\nEq.15 \u2264||Us [age(As)U\u00b8\u00ae X5W + (1 \u2212 a)90(A+)UX+W]\n\u2013 Ut90(At)U XtW||_F\n=||aUsge(As)UX5W + (1 \u2212 a)U$U\u00af\u00b9Ut90(A+)UXW\n\u2013 Ut90 (At)U XtW||_F. (16)\nUs and U\u2081 are the eigen-matrix of Ls and Lt, thus U\u1e63U\u0162 = I,U\u0141U = I. Assuming Us = [$a_0,\u2026, a_i, \u2026, a_n$] and Ut = [$b_0, \u2026\u2026\u2026, b_i,\u2026, b_n$], then ||ai||2 = 1 and ||bi||2 = 1, and $U_sU^T = U_sU = [a^Tb_,\u2026, a^Tb_,\u2026] \u2208 [ $a_i b_i$\\frac{1}{||a_i||_2||b_i||_2}, $ -1,1$]. Therefore:\nEq.16 \u2264||aUs90(As)U\u00b8\u2122 X5W + (1 \u2212 a)Ut90(At)U X\u0141W\n\u2013 Ut90(At)UX+W||_F\n=||aUs90(As)U X5W \u2013 aUt90(At)UX+W||_F\n=||age(Ls)X5W \u2013 age(P*L+P**P*X+W||_F. (17)\nFor any two matrices A, B, ||AB||_F \u2264 min(||A||op||B||_F, ||A||_F||B||op), we have:\nEq.17 \u2264a||W||op(||ge(Ls)Xs \u2013 ge(P*LtP*T)Xs\n+90(P*LP*T)Xs \u2013 90(P*LtP*T)P*Xt||_F)\n\u2264a||W||op||Xs||op||90(Ls) \u2013 90(P*LtP*T)||_F\n+ a||W||op||90(P*LtP*T)||op||Xs \u2013 P*Xt||_F\u2022 (18)\nAssuming ||X||op \u2264 1, ||W||op \u2264 1 which can be guaranteed by normalization. Besides, learn from [7], we can get:\nEq.18 \u2264a||ge(Ls) \u2013 90(P*LtP*T)||_F\n+ \u03b1\u00b7 max(|ge(Lt)|)||Xs \u2013 P*Xt||_F\n\u2264\u03b1[C\u03bb(1 + \u03c4)||Ls \u2212 P*LtP*T ||_F + O(||Ls \u2013 P*L+P*T||}) + \u0442\u0430\u0445(|90(Lt)|)||Xs \u2013 P*Xt||_F].\nProof completed."}]}