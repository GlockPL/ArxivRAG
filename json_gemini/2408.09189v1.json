{"title": "SA-GDA: Spectral Augmentation for Graph Domain Adaptation", "authors": ["Jinhui Pang", "Zixuan Wang", "Jiliang Tang", "Mingyan Xiao", "Nan Yin"], "abstract": "Graph neural networks (GNNs) have achieved impressive impres- sions for graph-related tasks. However, most GNNs are primarily studied under the cases of signal domain with supervised train- ing, which requires abundant task-specific labels and is difficult to transfer to other domains. There are few works focused on domain adaptation for graph node classification. They mainly focused on aligning the feature space of the source and target domains, with- out considering the feature alignment between different categories, which may lead to confusion of classification in the target domain. However, due to the scarcity of labels of the target domain, we can- not directly perform effective alignment of categories from different domains, which makes the problem more challenging. In this paper, we present the Spectral Augmentation for Graph Domain Adapta- tion (SA-GDA) for graph node classification. First, we observe that nodes with the same category in different domains exhibit similar characteristics in the spectral domain, while different classes are quite different. Following the observation, we align the category feature space of different domains in the spectral domain instead of aligning the whole features space, and we theoretical proof the stability of proposed SA-GDA. Then, we develop a dual graph con- volutional network to jointly exploits local and global consistency for feature aggregation. Last, we utilize a domain classifier with an adversarial learning submodule to facilitate knowledge transfer be- tween different domain graphs. Experimental results on a variety of publicly available datasets reveal the effectiveness of our SA-GDA.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are more and more popular due to their capacity to represent structured and relational data in a wide range of fields [2, 26, 41, 49, 50]. As a basic problem, graph node classification aims to predict the category of each node and has already been widely discussed in various fields, such as protein-protein interaction networks [6, 10], citation networks [12, 18], and time series prediction [43, 48]. They typically follow the message-passing mechanisms and learn the discriminative node representation for classification.\nAlthough existing GNNs exhibit impressive performance, they typically rely on supervised training methods [16, 42], which ne- cessitates a large amount of labeled nodes. Unfortunately, labeling can be a costly and time-consuming process in many scientific domains [15, 33]. Additionally, annotating graphs in certain disci- plines often requires specialized domain knowledge, which prevents a well-trained model from being transferable to a new graph. To address this issue, we investigate the problem of unsupervised do- main adaptation for graph node classification, which utilizes labeled source data and unlabeled target data to accomplish accurate clas- sification on the target domain. The illustration of unsupervised graph domain adaptation is shown in Figure 1.\nThere are limited works attempting to apply domain adapta- tion for graph-structure data learning [3, 28, 38]. CDNE [28] learns the cross-domain node embedding by minimizing the maximum mean discrepancy (MMD) loss, which cannot jointly model graph structures and node attributes, limiting the modeling capacity. AdaGCN [3] uses GNN as a feature extractor to learn node rep- resentations and utilizes adversarial learning to learn domain in- variant node representation. UDA-GCN [38] utilizes the random walk method to capture the local and global information to enhance the representation ability of nodes. Though these methods have achieved good results, there are still two fundamental problems: (1) How to align the category feature between two distinct domains."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Graph Neural Networks", "content": "Graph neural networks (GNNs) [53] are designed to learn node em- beddings in graph-structured data by mapping graph nodes with neighbor relationships to the low-dimensional latent space. Many approaches have been proposed to generalize the well-developed neural network models for regular grid structures, enabling them to utilize graph structured data for many downstream tasks, such as node classification and relationship prediction [40, 44], graph label noisy learning [44, 47]. GCN [18] integrates node features as well as graph topology on graph-structured data and exploits two graph convolutional layers for node classification tasks by semi- supervised learning. GAT [36] enhances GCN in terms of message passing between nodes, using an attention mechanism to auto- matically integrate the features of the neighbors of a certain node. However, most existing GNN-based approaches focus on learning the node representation among a particular graph. When transfer- ring the learned model between different graphs to perform the same downstream task, representation space drift and embedding distribution discrepancy may be encountered."}, {"title": "2.2 Unsupervised Domain Adaptation", "content": "Unsupervised domain adaptation is one of the transfer learning methods that aims to minimize the discrepancy between the source and target domains, thus transferring knowledge from the well-labeled source domain to the unlabeled target domain [4, 31, 46, 51]. To perform cross-domain classification tasks, methods based on deep feature representation[30], which map different domains into a common feature space, have attracted extensive attention. Most of these methods utilize adversarial training to reduce the inter-domain discrepancy[23]. Typically, DANN[8] utilizes a gradient reversal layer to capture domain invariant features, where the gra- dients are back-propagated from the domain classifier in a minimax game of the domain classifier and the feature extractor.\nRecently, for graph-structured data, several studies have been proposed for cross-graph knowledge transfer via unsupervised do- main adaptation methods [3, 28, 38, 45, 46]. CDNE [28] learns trans- ferable node representations through minimizing the maximum mean discrepancy (MMD) loss for cross-network learning tasks, yet cannot model the network topology. To enhance CDNE, AdaGCN[3] utilizes graph convolutional networks for feature extraction to learn the node representations, and then to learn domain-invariant node"}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Problem Definition", "content": "Graph Node Classification: Given a graph $G = (V, &, A, X, Y)$ with the set of nodes V and the set of edges &. $A \\in \\mathbb{R}^{N\\times N}$ is the adjacency matrix of G, D is the degree matrix with $D_{ii} = \\sum_{j=0}^{N} A_{ij}$ and N denotes the number of nodes. The node feature denotes as $X \\in \\mathbb{R}^{N\\times d}$, where each row $x_v \\in \\mathbb{R}^{d}$ represents the feature vector of node $v \\in V$ and d is the dimension of node features. The normal- ized graph Laplacian matrix is defined $L = I \u2013 D^{- \\frac{1}{2}}AD^{- \\frac{1}{2}}$, I is the identity matrix. By factorization of L, we have $L = U\\Lambda U^T$, where $U = [u_1,\\ldots, u_n]$, $\\Lambda = diag([\\lambda_1,\\cdots, \\lambda_n])$, $u_i$ is the eigenvector and $\\lambda_i$ is the corresponding eigenvalue. $Y \\in \\mathbb{R}^{N\\times C}$ is the label of G, C is the number of classes.\nGraph Domain Adaptation for Node Classification: Given the fully labeled source graph $G_s = (V_s, &s, A_s, X_s, Y_s)$ and unla- beled target graph $G_t = (V_t, &t, A_t, X_t)$, which shares the same"}, {"title": "3.2 Overview", "content": "The two key difficulties of unsupervised domain adaptation for node classification are the challenging category alignment of graph data and the inefficient node classification with label scarcity. To tackle these challenges, we propose a novel model termed SA-GDA. Our proposed SA-GDA consists of a spectral augmentation module, a dual graph learning module, and a domain adversarial module. The spectral augmentation module combines the spectral features be- tween source and target domain to implement the category feature alignment. The dual graph module aims to learn the consistency representation from local and global perspectives, and the domain adversarial module to differentiate the source and target domains."}, {"title": "3.3 Spectral Augmentation for Category Alignment", "content": "Domain adaptation methods in the past have typically used the pseudo-labeling mechanism [21, 54] to align the feature space of categories. They involve assigning pseudo-labels to unlabeled data and using the labels to supervise the neural network. However, the risk of overconfidence and noise in these pseudo-labels would lead to potential biases during optimization and ultimately affect performance. Furthermore, in the graph domain, the lack of labels can worsen the noise issue."}, {"title": "3.4 Attention-based Dual Graph Learning", "content": "To capture the local and global information of nodes, we propose a dual graph neural network, where the local consistency is ex- tracted with the graph adjacency matrix and the global with the random walk GNN. Due to the source domain graph being fused\nLocal GNN. For the local consistency extraction, we use the GCN [14] method directly. With the input $X_t$ and adjacency matrix $A_t$, the local node representation is defined as:\n$Z_{local} = \\sigma(D^{- \\frac{1}{2}}\\~{A}D^{- \\frac{1}{2}}Z^{l-1}W^l),$\n(3)\nwhere $\\~{A} = I + A_t$ is the adjacent matrix with self-loop, $D_{ii} = \\sum_j \\~{A}{ij}$, $W^l$ is the trainable matrix on the l-th layer. $Z^l$ is the node embeddings of target domain on the l-th layer and $Z^0 = X_t$.\nGlobal GNN. To achieve global information, similar to [38], we apply the random walk to calculate the semantic similarities between nodes and calculate the global consistency with the newly constructed adjacency matrix.\nDefine the state of node $v_i$ at time t as $s(t) = v_i$. The transition probability of jumping $v_i$ to the neighbors $v_j$ is defined as:\n$P_{ij} = p(s(t + 1) = v_j|s(t) = v_i) = \\frac{A_t(ij)}{\\sum_j A_t(ij)},$\n(4)\nwhere $A_t(ij)$ denotes the value of $A_{ij}$ on the target domain. Then, we calculate the point-wise mutual information matrix [19] as:\n$m_{ij} = \\frac{P_{ij}}{m_i m_j},  M_{ij} = max{log(\\frac{P_{ij}}{m_i m_j}), 0},$\n(5)\nand $m_i = \\sum_j P_{ij}$, $Mix = \\frac{\\sum_{ij} P_{ij}}{\\sum_i, P_{ij}}$.\nWith the calculated adjacency matrix M, we extract the global node information as follows:\n$Z_{t,global} = \\sigma(D^{- \\frac{1}{2}}MD^{- \\frac{1}{2}}Z^{l-1}W^l),$\n(6)\nwhere $D{ii} = \\sum_j M{ij}$, and $W^l$ is the shared parameters with Eq. 3.\nAttention-based fusion. To fuse the local and global informa- tion, we utilize the attention mechanism $a : \\mathbb{R}^{d'} \\times \\mathbb{R}^{d'} \\rightarrow \\mathbb{R}$ to computer the attention coefficients:\n$e_{ij} = a(Wz_{i,local},Wz_{j,global}),\\ \\widetilde{c}{ij} = softmax(e{ij}) = \\frac{exp(e_{ij})}{\\sum{k} exp(e_{kj})},$\nwhere $W \\in \\mathbb{R}^{d'\\times d}$ is the shared matrix, and we choose $a(h_i,h_j) = LeakyReLu(W [Wh_i||Wh_j])$ with learnable matrix $W_o \\in \\mathbb{R}^{2d'}$ and $||$ denotes concatenation operation. The fused representation of local and global is calculated as:\n$Z_t = \\sigma(\\alpha \\widetilde{c}{ii}Z{t,local} + (1 - \\widetilde{c}{ii})Z{t,global}.$\n(7)"}, {"title": "3.5 Domain Adversarial Training", "content": "The spectral augmentation of the category alignment module en- forces the similarity of features across different domains, confusing source and target domain features. Our goal is to maximize the domain classification error while minimizing the source domain classification error, i.e.,\n$\\underset{\\Theta y, \\Theta d}{min} \\underset{\\Theta d}{max} = L_s (f(z_s;\\Theta y)) - \\gamma L_D(h(z;\\Theta d)),$\n(8)\nwhere $f(z_s; y) = softmax(\\Theta yz_s)$ is the source classification func- tion, and $\\Theta y$ is the trainable parameters for f. $h(z; d) = sigmod(\\Theta dz)$ is the domain classifier, $\\Theta d$ is the parameters for h. $L_s$ and $L_D$ are"}, {"title": "3.6 Optimization", "content": "Our SA-GDA has the overarching goal that combines the domain ad- versarial loss and classification loss of the target data. Additionally, minimizing the expected source error for labeled source samples is also essential. In formation,\n$\\underset{\\Theta y}{min} \\underset{\\Theta d}{max} L(\\Theta y, \\Theta d) =L_s(f(z_s; \\Theta y)) + \\gamma_1 L_t (f(z_t; \\Theta y)) - \\gamma_2L_D ((h(z; \\Theta d)),$\n(9)\nwith the source loss $L_s$, target loss $L_t$ and domain loss $L_D$:\n$L_s = \\frac{1}{N_s} \\sum_{i=1}^{N_s}y_i log(f(z_s)),$\n$L_t = \\frac{1}{N_t} \\sum_{i=1}^{N_t} f(z_t)log(f(z_t)),$\n$L_D = \\frac{1}{N_s + N_t} \\sum_{i=1}^{N_s+N_t} d_i log(\\frac{1}{h(z)}) + (1 - d_i)log(\\frac{1}{1-h(z)}),$\nwhere $\\gamma_1$ and $\\gamma_2$ are hyper-parameters to balance the domain adver- sarial loss and target classification loss. $d_i \\in \\{0, 1\\}$ denotes node i belongs to the source or target domain. To update the parameters of Eq. 9 in the standard stochastic gradient descent (SGD) method, we apply the Gradient Reversal Layer [8] (GRL) R for model training, which is formulate as:\n$R(x) = x, \\frac{dR}{dx} = -I.$\n(10)\nThen, the update of Eq. 9 can be implemented with SGD with the following formulation:\n$\\underset{\\Theta y,\\Theta d}{min} L(\\Theta y, \\Theta d) =L_s(f(R(z_s);\\Theta y)) + \\gamma_1 L_t (f(R(z_t); \\Theta y))-\\gamma_2L_D((h(R(z); \\Theta d)).$\n(11)"}, {"title": "3.7 Theoretical Analysis", "content": "In this subsection, we proof the stability of SA-GDA, which is inspired by [52]:\nLEMMA 1. Suppose the Gs and Gt are the graphs of the source and target domain. Given the Laplace matrices decomposition $L_d = D_d \u2013 A_d = U_dA_dU_d^T$, where $A_d = diag[\\lambda{d1},\\ldots, \\lambda{dn}]$ are the sorted eigenvalues of $L_d$, and $d \\in \\{s, d\\}$ denotes the source and target domains. The GNN is constructed as $f (G) = \\sigma((g_\\theta(L)XW) = \\sigma(Ug_\\theta(A)U^TXW))$, where $g_\\theta$ is the polynomial function with $g_\\theta(L) = \\sum_{k=0}^{K}\\theta{0k}L^k$, W is the learnable matrix and the pointwise nonlinearity has $|\\sigma(b) - \\sigma(a)| \\leq |b - a|$. Assuming $||X||op \\leq 1$ and $||W||op \\leq 1$, we have the following inequality:\n$||f(G_s + G_t) - f(G_t)||_2 \\leq \\alpha[C_\\lambda(1 + \\tau)||L_s \u2013 P^*L_tP^{*T} ||_F + O(||L_s \u2013 P^*L_tP^{*T}||)\\ + max(|g_\\theta(L_t)|)||X_s \u2013 P^*X_t||_F]$,\n(12)\nwhere $\\tau = (||U_s \u2013 U_t||_F + 1)^2 \u2013 1$ stands for the eigenvector misalignment which can be bounded. $\\Pi$ is the set of permutation matrices, and $P^* = argminP\\in\\Pi||X_s \u2013 PX_t||_F + ||A_s - PA_tP^T ||_F$. $O(||L_s \u2013 P^*L_tP^{*T}||)$ is the remainder term with bounded multipliers defined in [7], and $C_\\lambda$ is the spectral Lipschitz constant that $\\forall x_i, d_j, |g_\\theta(\\lambda_i) \u2013 g_\\theta(\\lambda_j)| \\leq C_\\lambda (\\lambda_i \u2013 \\lambda_j)$."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct extensive experiments on various real- world datasets to verify the effectiveness of the proposed SA-GDA. We aim to answer the questions below:\n\u2022 RQ1: How does the proposed SA-GDA perform compared with the state-of-the-art baseline methods for node classifi- cation?\n\u2022 RQ2: How is the effectiveness of proposed components on the performance?\n\u2022 RQ3: How does the hyper-parameters affect the performance of the proposed SA-GDA?\n\u2022 RQ4: How about the intuitive effect of proposed SA-GDA?"}, {"title": "4.1 Benchmark Datasets", "content": "To evaluate the effectiveness of the proposed SA-GDA, we have developed three citation networks from ArnetMiner[35], including DBLPv7, ACMv9, and Citationv1. These datasets are derived from different data sources (DBLP, ACM and Microsoft Academic Graph) and distinct time periods. Each sample contains a title, an index, a category and a citation index, where the category represents the academic field, including \"Artificial intelligence\", \"Computer vision\", \"Database\", \"Data mining\", \"High Performance Computing\" and \"In- formation Security\". Following [39], these datasets are constructed"}, {"title": "4.2 Baselines", "content": "To verify the effectiveness of our method, we select the following methods as baselines for comparison, including seven state-of-the- art single-domain node classification methods (i.e., DeepWalk[27], LINE[34], GraphSAGE[13], DNN[24], GCN[18], DGC[37], and SUBLIME[22]), and three cross-domain classification methods with domain adaptation (i.e., DGRL[9], AdaGCN[32] and UDA-GCN[38]).\nSingle-domain node classification methods:\n\u2022 DeepWalk[27]: DeepWalk employs the random walk sampling strategy to capture the neighborhood node structure. Then, fol- lowing Skip-Gram[11], which aims to learn the low-dimensional node representation on single domain.\n\u2022 LINE [34]: LINE is a classic method for large-scale graph repre-sentation learning, which preserves both first and second-order proximities for undirected network to measure the relationships between two nodes. Compared with the deep model, LINE has a limited representation ability.\n\u2022 GraphSAGE[13]: GraphSAGE learns the node representation by aggregating the sampled neighbors for final prediction.\n\u2022 DNN[24]: DNN is a multi-layer perceptron (MLP) based method, which only leverages node features for node classification.\n\u2022 GCN[18]: GCN is a deep convolutional network on graphs, which employs the symmetric-normalized aggregation method to learn the embedding for each node.\n\u2022 DGC[37]: DGC is a linear variant of GCN, which separates the feature propagation steps from the terminal time, enhancing flexi- bility and enabling it to utilize a vast range of feature propagation steps.\n\u2022 SUBLIME[22]: SUBLIME generates an anchor graph from the raw data and uses the contrastive loss to optimize the consistency"}, {"title": "4.3 Experimental Settings", "content": "We utilize PyTorch [25] and PyTorch Geometric library [5] as the deep-learning Framework and Adam [17] as an optimizer. We fol- low the principles of evaluation protocols used in unsupervised domain adaptation to perform a grid study of all methods in the hyperparameter space and show the best results obtained for each method. To be fair, for all the cross-domain node classification meth- ods in this experiment, we use the same parameter settings, except for some special cases. For each method, we set the learning rate to 1e-4. For the specific GCN-based models (e.g., GCN, DGC, SUB- LIME, AdaGCN, and UDA-GCN), the number of hidden layers is set to 2, and the hidden dimensions are set to 128 and 16 regardless of the source and target domains. We use the same settings as above for both the local and global GNN. For DeepWalk and LINE, we first learn node embeddings and then train node classifiers using information from source domain, the hidden dimension of the node embeddings is set to 128. For DNN and DGRL, we set the same hidden dimensions as GNN-based models. The balance parameters $\\gamma1$, $\\gamma2$ are set to 0.3 and 0.1, and the dropout rate for each layer of dual GNN to 0.3. For simplicity, we set the combination weight of high and low-frequency as the same, i.e., $a = \\beta$."}, {"title": "4.4 Performance Comparison (RQ1)", "content": "We present the performance of SA-GDA compared with baselines under the setting of graph domain adaptation for node classification to answer RQ1, which is shown in Table 2. From the results:\n\u2022 DeepWalk and LINE achieve the worst performance among all the methods, this is because they only utilize the network topology to generate node embedding without exploiting the node feature information. DNN also achieves a poor performance, contrary to the above two methods, it only considers the node features and ignores the association between nodes, so it cannot generate better node embeddings.\n\u2022 The graph-based methods (GCN, GraphSAGE, DGC, and SUB- LIME) outperform DeepWalk and LINE, we attribute the reason to that they encode both local graph structure and node features to obtain better node embeddings.\n\u2022 DGRL, AdaGCN, and UDA-GCN achieve better performance than the single-domain node classification methods. The reason is that, by incorporating the domain adversarial learning, the node classifier is capable to transfer the knowledge from the source to the target domain.\n\u2022 The proposed SA-GDA model achieves the best performance in these six cross-domain node classification datasets compared to all the baseline methods. By fusing the low and high-frequency signals of different domains in the spectral domain, SA-GDA implements the category alignment under the domain-variant setting. This enables us to learn better graph node embeddings and to train node classifiers in both the source and target do- mains through an adversarial approach, which greatly reduces the distribution discrepancies between the different domains and improves the node classifier performance."}, {"title": "4.5 Ablation Study (RQ2)", "content": "To answer RQ2, we introduce several variants of the SA-GDA to investigate the effectiveness of each component of SA-GDA:\n\u2022 SA-GDA-l, which evaluates the impact of low-frequency signals in both source and target domains.\n\u2022 SA-GDA-h, evaluating the influence of high-frequency signals in different domains.\n\u2022 SA-GDA-p, which remove the global GNN and utilizing only the local GNN.\n\u2022 SA-GDA-d, which removes the domain loss to evaluate the impact of domain adversarial learning.\n\u2022 SA-GDA-t, which evaluate the effectiveness of target classifier by removing the target loss."}, {"title": "4.6 Sensitivity Analysis (RQ3)", "content": "To answer RQ3, we conduct experiments to evaluate how the hyper-parameters $\\alpha$, $\\beta$ and $\\gamma1$, $\\gamma2$ affect the performance of pro- posed SA-GDA. $a$ and $\\beta$ control how much of the high and low- frequency combined in the spectral augmentation module, and $\\gamma1$, $\\gamma2$ control the balance of target classification and domain adver- sarial loss. In the implementation, we set $a = \\beta$ and vary a in {0, 0.2, 0.4, 0.6, 0.8, 1} with other parameters fixed. Besides, we set $\\gamma1$ and $\\gamma2$ in {0.1, 0.3, 0.5, 0.7, 0.9} respectively. The results is shown in Fig. 4, from the results, we have the following observation:\n\u2022 Impact of Spectral Augmentation Ratio $\\alpha$: Fig. 4(a) shows the impact of $a$, we observe that the classification accuracy improves gradually when $a$ changes from 0 to 0.8, while decrease when $a$ ranges in {0.8, 1}. This is because SA-GDA needs more spectral information from the source domain for training, while if $a$ is too large, the effectiveness of the spectral augmentation would be weakened, resulting the poor performance. Hence, we set $a$ to 0.8 in our implementation.\n\u2022 Impact of Balance Ratio $\\gamma1$ and $\\gamma2$: We conduct a large number of experiments by setting $\\gamma_1$ and $\\gamma_2$ in {0.1, 0.3, 0.5, 0.7, 0.9} respectively, and report the best results as shown in Fig. 4(b). From Fig. 4(b), we find that, when $\\gamma_2$ fixed at 0.1, the performance tends to increase first and then decrease in most cases. The rea- son is that a small $\\gamma_1$ would incorporate the target information for classification while a large $\\gamma_1$ may introduce more uncertain information for the scarcity of target labels. Thus, we set $\\gamma_1$ to 0.3 as default. Similar to $\\gamma_1$, we change the value of $\\gamma_2$ with $\\gamma_1$ fixed to 0.3. We observe that increasing $\\gamma_2$ results in better performance when it is small, indicating that the domain classifier would help to improve the accuracy. However, too large $\\gamma_2$ may hurt the performance, which demonstrates that too large of domain loss could harm the discrimination information. Thus, we set $\\gamma_2$ to 0.1 as default."}, {"title": "4.7 Visualization (RQ4)", "content": "In order to achieve an intuitive effect of proposed SA-GDA, we visualize the node representation learned in the target domain to"}, {"title": "5 CONCLUSION", "content": "In this paper, we study a less-explored problem of unsupervised domain adaptation for graph node classification and propose a novel framework SA-GDA to solve the problem. First, we find the inherent features of same category in different domains, i.e., the correlation of nodes with same category is high in spectral domain, while different categories are distinct. Following the observation, we apply spectral augmentation for category alignment instead of whole feature space alignment. Second, the dual graph extract the local and global information of graphs to learn a better represen- tation for nodes. Last, by using the domain adversarial learning cooperating with source and target classification loss, we are able to reduce the domain discrepancy and achieve the domain adapta- tion. We conduct extensive experiments on real-world datasets, the results show that our proposed SA-GDA outperforms the existing cross domain node classification methods. Although the proposed SA-GDA has achieved impressive results, the complexity of ma- trix decomposition is high. In the future, we will explore more efficient spectral domain alignment methods and utilize spatial domain features to assist the alignment of category features in different domains, thereby reducing the model complexity."}, {"title": "6 ACKNOWLEDGE", "content": "This work was supported by the National Key R&D Program of China under Grant No. 2020AAA0108600."}, {"title": "A THEORETICAL ANALYSIS", "content": "In this subsection, we proof the stability of SA-GDA, which is inspired by [52]:\nLEMMA 2. Suppose the Gs and Gt are the graphs of the source and target domain. Given the Laplace matrices decomposition $L_d = D_d \u2013 A_d = U_dA_dU_d^T$, where $A_d = diag[\\lambda{d1},\\ldots, \\lambda{dn}]$ are the sorted eigenvalues of $L_d$, and $d \\in \\{s, d\\}$ denotes the source and target domains. The GNN is constructed as $f (G) = \\sigma((g_\\theta(L)XW) = \\sigma(Ug_\\theta(A)U^TXW))$, where $g_\\theta$ is the polynomial function with $g_\\theta(L) = \\sum_{k=0}^{K}\\theta{0k}L^k$, W is the learnable matrix and the pointwise nonlinearity has $|\\sigma(b) - \\sigma(a)| \\leq |b - a|$. Assuming $||X||op \\leq 1$ and $||W||op \\leq 1$, we have the following inequality:\n$||f(G_s + G_t) - f(G_t)||_2 \\leq \\alpha[C_\\lambda(1 + \\tau)||L_s \u2013 P^*L_tP^{*T} ||_F + O(||L_s \u2013 P^*L_tP^{*T}||)\\ + \u0442\u0430\u0445(|g_\\theta(L_t)|)||X_s \u2013 P^*X_t||_F]$,\n(13)\nwhere $\\tau = (||U_s \u2013 U_t||_F + 1)^2 \u2013 1$ stands for the eigenvector misalignment which can be bounded. $\\Pi$ is the set of permutation matrices, and $P^* = argminP\\in\\Pi||X_s \u2013 PX_t||_F + ||A_s - PA_tP^T ||_F$. $O(||L_s \u2013 P^*L_tP^{*T}||)$ is the remainder term with bounded multipliers defined in [7], and $C_\\lambda$ is the spectral Lipschitz constant that $\\forall x_i, d_j, |g_\\theta(\\lambda_i) \u2013 g_\\theta(\\lambda_j)| \\leq C_\\lambda (\\lambda_i \u2013 \\lambda_j)$.\nProof. Similar to [52], we use P* as the optimal permutation matrix for Gs and Gt. With Eq. 2, we have the difference of GNN:\n$||f(G_s + G_t) - f(G_t)||_2=\n||\\sigma(U_s [\\alpha g_h (A_s)U_s^TX_sW + (1 \u2013 \\alpha)g_l (A_t)U_t^T X_tW + \\beta g_h (A_s)U_s^TX_sW + (1 \u2013 \\beta)g_l (A_t)U_t^T X_tW]) \u2013 \\sigma(U_tg_\\theta(A_t)U_t^T X_tW) ||_2,$\n(14)\nwith the triangle inequality and the assumption $|\\sigma(b) \u2013 \\sigma(a)| \\leq b-a|$, $\u2200a, b \\in \\mathbb{R}$, we have:\n$Eq.14 \\leq ||U_s [\\alpha g_h (A_s)U_s^TX_sW + (1 \u2013 \\alpha)g_l (A_t)U_t^T X_tW + \\beta g_h (A_s)U_s^TX_sW + (1 \u2013 \\beta)g_l (A_t)U_t^T X_tW] - \\sigma(U_tg_\\theta(A_t)U_t^T X_tW)||_F$ (setting $a = \\beta$)\n$=||U_s [\\alpha(g_h (A_s) + g_h(A_s))U_s^TX_sW + (1 \u2013 \\alpha)(g_l (A_t) + g_l(A_t))U_t^T X_tW] - U_tg_\\theta(A_t)U_t^T X_tW||_F,$\n(15)"}]}