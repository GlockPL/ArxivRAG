{"title": "Revisiting, Benchmarking and Understanding Unsupervised Graph Domain Adaptation", "authors": ["Meihan Liu", "Zhen Zhang", "Jiachen Tang", "Jiajun Bu", "Bingsheng He", "Sheng Zhou"], "abstract": "Unsupervised Graph Domain Adaptation (UGDA) involves the transfer of knowledge from a label-rich source graph to an unlabeled target graph under domain discrepancies. Despite the proliferation of methods designed for this emerging task, the lack of standard experimental settings and fair performance comparisons makes it challenging to understand which and when models perform well across different scenarios. To fill this gap, we present the first comprehensive benchmark for unsupervised graph domain adaptation named GDABench, which encompasses 16 algorithms across 5 datasets with 74 adaptation tasks. Through extensive experiments, we observe that the performance of current UGDA models varies significantly across different datasets and adaptation scenarios. Specifically, we recognize that when the source and target graphs face significant distribution shifts, it is imperative to formulate strategies to effectively address and mitigate graph structural shifts. We also find that with appropriate neighbourhood aggregation mechanisms, simple GNN variants can even surpass state-of-the-art UGDA baselines. To facilitate reproducibility, we have developed an easy-to-use library PyGDA for training and evaluating existing UGDA methods, providing a standardized platform in this community. Our source codes and datasets can be found at https://github.com/pygda-team/pygda.", "sections": [{"title": "Introduction", "content": "The last decade has witnessed significant advancements in Graph Neural Networks (GNNs) with their successful applications spanning various fields [1, 2], including social network analysis [3], protein interaction prediction [4], and traffic flow forecasting [5], etc. However, the presence of distribution shifts [6] and label scarcity in real-world graph data impedes the ability of existing GNN models to adapt to new domains [7]. To addresses this challenge, Unsupervised Graph Domain Adaptation (UGDA) has become an important solution for transferring knowledge from a labeled source graph to an unlabeled target graph. This powerful paradigm has been widely studied, unlocking broader application for graph neural networks.\nDespite a wide range of researches of UGDA have been developed [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], the understanding of their capabilities and limitations is inadequate due to the following reasons:\n\u2022 Inadequate Evaluation of Domain Distribution Discrepancies. The distribution shifts in node attributes, graph structures, and label proportions between graphs will significantly influence the adaptation performance and result in various adaptation scenarios [25, 26]. However, the types and magnitudes of distribution discrepancies among different domains have not been thoroughly evaluated and discussed, which makes it challenging to understand the robustness and efficacy of current methods.\n\u2022 Lack of Standard, Fair, and Comprehensive Comparisons. The utilization of distinct datasets, varying data processing methodologies, and divergent data partitioning strategies among existing domain adaptation models results in incomparability across different findings [8, 27, 16]. Furthermore, they are mainly evaluated against limited baselines with constrained scenarios, such as social networks or citation networks, which lack validation of the model's capability in more diverse or complex applications.\n\u2022 Limited Investigation on GNN Inherent Transferability. Despite the advancements made by existing UGDA algorithms, it is still unclear how data shift impose challenges on GNN and how to unleash the transferabililty power for GNN. Due to the non-IID nature of graph data, the aggregation architectures and aggregation scopes affect the underlying distribution of latent representations generated by GNN. When there exists significant structural difference between the source and target domains, such as variations in the degree [28, 29] or differences in subgraph patterns [22, 30], the information aggregation capability of GNN will be directly affected [31, 24, 30]. Thus, understanding the key components that affect adaptation in GNN will be crucial for enhancing GNN's transferabililty, which is still an open problem.\nTo fill these gaps, we revisit existing UGDA algorithms and conduct a comprehensive benchmark named GDABench. Specifically, GDABench includes 16 state-of-the-art UGDA models and 5 diverse graph datasets covering node attributes, graph structures, and label proportion shifts, resulting in 74 different adaptation tasks. Additionally, we also explore the limits of GNN transferability by combining 7 GNN variants with 2 domain alignment and 3 unsupervised graph learning techniques. Our work is the first to provide a rigorous empirical analysis of how various aggregation mechanisms influence alignments in domain adaptation task. Through comprehensive experiments, we observe that: (1) the performance of current UGDA models varies greatly across different datasets and adaptation scenarios; (2) it is crucial to develop tailored strategies to address graph structural shifts, especially when the distribution discrepancies are significant; (3) the GNN's transferability in UGDA heavily relies on two factors: aggregation scope and aggregation architecture, which are influenced by the severity of label shift and the level of graph heterophily, etc; (4) the inherent adaptability of GNNs is largely underestimated by existing methods, which motivates the investigate of a simple yet effective model that relies heavily on GNN's property. More insights can be found in Section 5.\nIn summary, our main contributions are as follows:\n\u2022 We introduce GDABench, the first comprehensive benchmark for unsupervised graph domain adaptation. It includes 16 recent state-of-the-art methods across 5 real-world datasets with diverse range of adaptation tasks.\n\u2022 To explore the capability and limitations of exiting UGDA models, we systematically evaluate existing algorithms and investigate the underlying transferability for GNN. With these findings, we reveal a simple yet effective method that can even surpass existing UGDA algorithms.\n\u2022 We develop an easy-to-use library PyGDA to alleviate the workload of researchers when conducting experiments. Furthermore, users can easily construct their own models or datasets with minimal effort.\nThe source codes of our benchmark are available at https://github.com/pygda-team/pygda/ tree/main/benchmark, which provide unified APIs and adopt consistent data processing as well as data splitting approaches for fair comparisons."}, {"title": "Preliminaries and Related Work", "content": "2.1 Problem Definition\nGiven an attributed graph $G = (V,E)$ with $n$ nodes and $m$ edges, the node attribute matrix is represented as $X = {x_v|v \\in V} \\in R^{n\\times d}$, where $d$ is the dimension of node attributes. The adjacency matrix is denoted as $A \\in R^{n\\times n}$, where $A_{i,j} = 1$ means there exists an edge $e_{i,j} \\in E$ connecting node $v_i$ and $v_j$, and $A_{i,j} = 0$ otherwise. $Y \\in R^{N\\times C}$ denotes the node label matrix, where $C$ is the category of node labels. We focus on Unsupervised Graph Domain Adaptation (UGDA) on node classification tasks. It is a non-trivial task due to the complex domain shift between source and target graphs. Formally, given a labeled source graph $G_s = (V_s,E_s, Y_s)$ and an unlabeled target graph"}, {"title": "Datasets", "content": "We have carefully selected five widely used public datasets that showcase a wide spectrum of distribution shifts across graphs. These include Airport which consists of three domains Brazil (B), Europe (E) and USA (U); Blog that includes two domains Blog1 (B1) and Blog2 (B2); ArnetMiner which encompasses three domains DBLPv7 (D), Citationv1 (C) and ACMv9 (A); Twitch that includes six domains Germany (DE), England (EN), Spain (ES), France (FR), Porutgal (PT) and Russia (RU); MAG that includes six domains like CN, US, JP, FR, RU, and DE. The selection criteria for these datasets are primarily based on three factors: the complexity of the distribution shift, the scale of the dataset, and the potential for downstream applications. From these datasets, we have compiled a comprehensive collection comprising 74 distinct source-target adaptation pairs. Detailed attributes of each dataset are documented in Table 1 and the statistical methods for quantifying the types of domain shifts exhibited in the dataset is shown in Appendix B.\n\u2022 Wide range of distribution shift. Potential distribution shifts between source and target graphs can largely fall into three categories: feature shift, structure shift and label shift [31, 45, 25]. Our GDABench datasets capture a wide range of three distribution shifts across domains in varying degrees; The details are illustrated in Table 1 and more statistics are given in Appendix B. Specifically, the domains within Airport are dominated by structure shift, while domains in Blog, ArnetMiner, Twitch and MAG are affected by all kinds of shifts with different degrees.\n\u2022 Different scales with variant spans. We divide the size of the dataset by the average number of domain nodes. The small size (S) covers nodes below 5 thousand. The medium size (M) cover nodes below 10 thousand. The large size (L) covers nodes from 10 thousand to hundred thousand. Smaller datasets exhibit smaller differences in domain sizes, and vice versa. In the Blog and Airport, the size difference between the largest and smallest domains is less than 1000 nodes. In the case of ArnetMiner and Twitch, this difference ranges between 1,000 and 10,000. For the MAG, this difference ranges between 10,000 to 100,000. This allows us to understand the impacts of varying domain size on the efficacy of adaptation task.\n\u2022 Various downstream applications. The GDABench datasets encompass multiple applications, including citation relationships (ArnetMiner and MAG), social media (Blog and Twitch) and routines connections (Airport). Specifically, in ArnetMiner and MAG, nodes represent academic papers and edges indicate citation relationship. ArnetMiner groups domains by publisher, while"}, {"title": "Compared Models", "content": "Specialized UGDA Methods. This group includes specifically designed algorithms for graph domain adaptation task. We compare 16 models including (1) nine methods incorporating classicial DA methods with deep node embedding: DANE [9], ACDNE [10], UDAGCN [11], ASN [43], AdaGCN [12], CWGCN [21], SAGDA [22], DMGNN [20] and DGDA [23]; (2) four methods tailored for graph structure shift: StruRW [16], JHGDA [17], KBL [18] and PairAlign [25]; and (3) four methods based on domain adaptive message passing: GRADE [14], SpecReg [15], and A2GNN [24].\nSimGDA: Vanilla DA with GNN Variants. To understand the inherent transferability of GNN, we delve into its aggregation process and decouple it from two perspectives: how to aggregate and what to aggregate. For how to aggregate, we consider five aggregator including sum aggregator [46], mean aggregator [47], aggregate with weighted neighbours (GAT) [48] and aggregate with discriminative neighbours (GIN) [49]. For what to aggregate, we consider three aspects in terms of the hop-count of neighbours: (1) GNN without neighbours, where graph structure is not considered (degenerating to MLP); (2) GNN with one-hop neighours; and (3) GNN with muti-hop neighours. To avoid the over-smooth problem, we also add residual connections to enhance its modeling power [50, 46]. For alignment, we consider two widely used models for domain-invariant feature learning from computer vision: domain distance metric MMD [34] and adversarial learning DANN [36]. MMD proposes to match the distribution in the latent space through maximum mean discrepancy [40]. DANN introduces an adversarial objective to distinguish source and target samples in the latent space. We use one-layer GCN [46] as a control and create six GNN variants by altering only one module each time. Then, we get 14 models by combining these variants with two vanilla DA methods, abbreviated these 14 models as SimGDA.\nSimGDA+: SimGDA with Unsupervised Techniques. To further unlock the power of GNN for graph domain adaptation, we enhance SimGDA with unsupervised graph learning techniques on unlabeled target graph, which allows the model to learn meaningful representations without relying on domain-specific labels. We implement three unsupervised techniques in an end-to-end manner: (1) Information Maximization (IM) [51, 52, 19]. Ideally, a perfect prediction for target domain should be individually certain and globally diverse. To achieve this goal, we minimize the entropy for each individual sample, and maximize the entropy for each class. (2) Graph AutoEncoder (AE) [53, 54, 55]. Graph autoencoders encode nodes into a latent vector space and reconstruct the graph data from the encoded latent space. After obtaining target graph node representations, we employ a distinct decoder to reconstruct target graph structure. (3) Graph Contrastive"}, {"title": "Experimental Results and Analyses", "content": "In this section, we study the experimental results of all the models. We first provide a comprehensive comparison of specialized UGDA methods across five datasets with diverse distribution shifts. Following that, we perform a thorough analysis between different GNN variants to understand how data shift imposes challenges on GNNs. Finally, we present the performance of SimGDA variants, which shows the limit of GNNs' transferability. For more information about metrics, hyperparameters, search spaces, and other implementation details, please refer to Appendix D.\n5.1 Overall Comparisons\nIn this section, we try to elucidate the success of these UGDA algorithms through empirical evaluation across diverse datasets. In Table 2 and Table 3, we take a close look at the models' performance across 5 datasets on partial tasks utilizing Micro-F1 for Blog, Airport, and ArnetMiner, while employing Macro-F1 for MAG and AUROC for Twitch due to their imbalanced labels. For comprehensive experimental results, please refer to Appendix D. Our key findings include:\nObservation 1: When facing significant shifts, it is important to design solutions tailored to mitigate structural discrepancies. Although several methods that incorporate classical DA approaches with deep node embedding have achieved impressive results on Airport, Blog and ArnetMiner datasets (e.g., ACDNE and DMGNN), they fail to obtain satisfied performance on datasets with significant shifts. These results underscore the limitations of marginal distribution alignment techniques in the presence of significant structural and label shifts in graph data. As shown in Table 3, methods tailored to address graph structure shifts show reasonable improvements over those incorporating classical DA methods with deep node embedding. This suggests that mitigating the impact of graph structure shift on node representation learning under this scenario is crucial.\nObservation 2: Domain-adaptive message passing methods demonstrate superior and robust performance across a wide range of datasets and tasks. While methods that align marginal feature distributions and those designed for graph structure shifts can address datasets with mild and severe data shifts respectively, strategies specifically developed to leverage GNN properties demonstrate robustness and superior performance across diverse data shifts. As shown in Table 2 and Table 3, methods designed based on the inherent properties of GNN achieves the top-three best performance in 8 tasks out of 12 tasks. This finding suggests that leveraging the structural strengths of GCNs, combined with well-established domain adaptation principles, can result in an effective and efficient approach to addressing the challenges of domain variability in graph datasets. Such strategies represent a promising direction for future research and application in this field.\n5.2 Understanding and unlocking the inherent power of GNN\nNote that despite many UGDA methods integrating traditional domain adaptation techniques, we observe their performance remains unsatisfactory and can even be inferior to that of SimGDA. This leads us to an intriguing question: do the intrinsic mechanisms of GNN play a more crucial role in enhancing transferability? To further investigate the question, we take one-layer GCN combined with vanilla DA as a baseline, and compare six variants: Max-Aggr, Mean-Aggr, GAT-Aggr, GIN-Aggr, with-no-neighbor, and with-multi-hop-neighbor; shown in shown in Figure 3. Moreover, we enhance above SimGDA with unsupervised graph learning techniques, referred to as SimGDA+, to explore the limit of GNNs in graph domain adaptation; shown in Table 2 and Table 3."}, {"title": "Conclusion", "content": "In this paper, we introduce GDABench, the first comprehensive benchmark for unsupervised graph domain adaptation. Our evaluation encompasses 16 well-known models across 5 real-world datasets exhibiting diverse data distribution shifts. Furthermore, we design 6 GNN variants to investigate the inherent transferability of GNNs, enhancing them with 3 unsupervised techniques to explore their potential limits. Our empirical results shows that (1) the performance of current UGDA models varies significantly across different datasets and adaptation scenarios; (2) tailored strategies are essential for addressing and mitigating graph structural shifts, particularly when distribution discrepancies are substantial. (3) the transferability of GNNs in UGDA is heavily dependent on aggregation scope and architecture, influenced by factors such as label shift severity and graph heterophily. We also provide unified APIs and adopt consistent data processing as well as data splitting approaches for fair comparisons. In the future, we plan to extend GDABench to include broader scenarios [61], more"}, {"title": "Appendix", "content": "A Distribution Shift in Graph-Structured Data\nDistribution shift appears when the joint distribution differs between source domain and target domain [6, 62]. Assuming that the relationship between the input and class variables is unchanged, there are two kinds of distribution shift, i.e., covariate shift and label shift (prior probability shift) [63].\nA.1 Covariate Shift\nCovariate shift [64] refers to changes in the distribution of the input variables, which can be defined formally as follows:\nDefinition 1 (Covariate Shift). Covariate shift appears when $P_s(G) \\neq P_T(G)$ with the assumption of $P_s(Y|G) = P_T(Y|G)$, where $P_s$ and $P_T$ are the probability distributions of the source and target domains, respectively.\nTo deal with covariate shift, it is essential to align $P_s(Y|H)$ and $P_T(Y|H)$, where $H$ is the representation after data attributes passing through the encoder. However, in graph-structured data, node representation is not only affected by the data attributes but also graph structure. Thus, covariate shift in graph data can be decoupled as feature shift and structure shift [25].\nDefinition 2 (Feature Shift). Given the joint distribution of the node attributes and node labels $P_T(X, Y)$, the feature shift is then defined as $P_s(X,Y) \\neq P_T(X, Y)$ with the assumption of $P_s(Y|G) = P_T(Y|G)$.\nDefinition 3 (Structure Shift). Given the joint distribution of the adjacency matrix and node labels $P_T(A, Y)$, the structure shift is then defined as $P_s(A,Y) \\neq P_T(A, Y)$ with the assumption of $P_s(Y|G) = P_T(Y|G)$.\nA.2 Label Shift\nLabel shift refers to changes in the distribution of the class variable Y. It also appears with different names in the literature and the definitions have slight differences between them.\nDefinition 4 (Label Shift). Label shift occurs when the distribution of labels changes across two domains, which is defined as $P_s(Y) \\neq P_T(Y)$ where $P_s(G|Y) = P_T(G|Y)$.\nIn all, structure shift is unique to graph data due to the non-IID nature caused by node interconnections. Moreover, the learning of node representations implemented by the GNN will mix the feature shift, sutructure shift and label shift [31].\nB Detailed Description of Datasets\nIn this section, we provide additional details about the datasets used in our benchmark.\nB.1 Dataset Description\n\u2022 Airport 1: The Airport datasets consist of three separate collections corresponding to Brazil (B), Europe (E), and the USA (U). In these datasets, nodes represent airports and edges denote flight connections between them. The labels categorize airports by activity levels, measured in terms of flights or passenger numbers.\n\u2022 Blog 2: Blog1 and Blog2 are disjoint social networks derived from the BlogCatalog dataset. In these networks, nodes correspond to bloggers, and edges reflect friendships among them. The attributes for each node consist of keywords from the blogger's self-description, and each node is assigned a label denoting its group affiliation. Given that both Blog1 and Blog2 originate from the same underlying network, their data distributions are nearly identical."}]}