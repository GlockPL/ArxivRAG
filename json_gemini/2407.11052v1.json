{"title": "Revisiting, Benchmarking and Understanding Unsupervised Graph Domain Adaptation", "authors": ["Meihan Liu", "Zhen Zhang", "Jiachen Tang", "Jiajun Bu", "Bingsheng He", "Sheng Zhou"], "abstract": "Unsupervised Graph Domain Adaptation (UGDA) involves the transfer of knowledge from a label-rich source graph to an unlabeled target graph under domain discrepancies. Despite the proliferation of methods designed for this emerging task, the lack of standard experimental settings and fair performance comparisons makes it challenging to understand which and when models perform well across different scenarios. To fill this gap, we present the first comprehensive benchmark for unsupervised graph domain adaptation named GDABench, which encompasses 16 algorithms across 5 datasets with 74 adaptation tasks. Through extensive experiments, we observe that the performance of current UGDA models varies significantly across different datasets and adaptation scenarios. Specifically, we recognize that when the source and target graphs face significant distribution shifts, it is imperative to formulate strategies to effectively address and mitigate graph structural shifts. We also find that with appropriate neighbourhood aggregation mechanisms, simple GNN variants can even surpass state-of-the-art UGDA baselines. To facilitate reproducibility, we have developed an easy-to-use library PyGDA for training and evaluating existing UGDA methods, providing a standardized platform in this community. Our source codes and datasets can be found at https://github.com/pygda-team/pygda.", "sections": [{"title": "Introduction", "content": "The last decade has witnessed significant advancements in Graph Neural Networks (GNNs) with their successful applications spanning various fields [1, 2], including social network analysis [3], protein interaction prediction [4], and traffic flow forecasting [5], etc. However, the presence of distribution shifts [6] and label scarcity in real-world graph data impedes the ability of existing GNN models to adapt to new domains [7]. To addresses this challenge, Unsupervised Graph Domain Adaptation (UGDA) has become an important solution for transferring knowledge from a labeled source graph to an unlabeled target graph. This powerful paradigm has been widely studied, unlocking broader application for graph neural networks.\nDespite a wide range of researches of UGDA have been developed [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], the understanding of their capabilities and limitations is inadequate due to the following reasons:\n\u2022 Inadequate Evaluation of Domain Distribution Discrepancies. The distribution shifts in node attributes, graph structures, and label proportions between graphs will significantly influence the adaptation performance and result in various adaptation scenarios [25, 26]. However, the types and magnitudes of distribution discrepancies among different domains have not been thoroughly evaluated and discussed, which makes it challenging to understand the robustness and efficacy of current methods."}, {"title": "", "content": "\u2022 Lack of Standard, Fair, and Comprehensive Comparisons. The utilization of distinct datasets, varying data processing methodologies, and divergent data partitioning strategies among existing domain adaptation models results in incomparability across different findings [8, 27, 16]. Further-more, they are mainly evaluated against limited baselines with constrained scenarios, such as social networks or citation networks, which lack validation of the model's capability in more diverse or complex applications.\n\u2022 Limited Investigation on GNN Inherent Transferability. Despite the advancements made by existing UGDA algorithms, it is still unclear how data shift impose challenges on GNN and how to unleash the transferabililty power for GNN. Due to the non-IID nature of graph data, the aggregation architectures and aggregation scopes affect the underlying distribution of latent representations generated by GNN. When there exists significant structural difference between the source and target domains, such as variations in the degree [28, 29] or differences in subgraph patterns [22, 30], the information aggregation capability of GNN will be directly affected [31, 24, 30]. Thus, understanding the key components that affect adaptation in GNN will be crucial for enhancing GNN's transferabililty, which is still an open problem.\nTo fill these gaps, we revisit existing UGDA algorithms and conduct a comprehensive benchmark named GDABench. Specifically, GDABench includes 16 state-of-the-art UGDA models and 5 diverse graph datasets covering node attributes, graph structures, and label proportion shifts, resulting in 74 different adaptation tasks. Additionally, we also explore the limits of GNN transferability by combining 7 GNN variants with 2 domain alignment and 3 unsupervised graph learning techniques. Our work is the first to provide a rigorous empirical analysis of how various aggregation mechanisms influence alignments in domain adaptation task. Through comprehensive experiments, we observe that: (1) the performance of current UGDA models varies greatly across different datasets and adaptation scenarios; (2) it is crucial to develop tailored strategies to address graph structural shifts, especially when the distribution discrepancies are significant; (3) the GNN's transferability in UGDA heavily relies on two factors: aggregation scope and aggregation architecture, which are influenced by the severity of label shift and the level of graph heterophily, etc; (4) the inherent adaptability of GNNs is largely underestimated by existing methods, which motivates the investigate of a simple yet effective model that relies heavily on GNN's property. More insights can be found in Section 5.\nIn summary, our main contributions are as follows:\n\u2022 We introduce GDABench, the first comprehensive benchmark for unsupervised graph domain adaptation. It includes 16 recent state-of-the-art methods across 5 real-world datasets with diverse range of adaptation tasks.\n\u2022 To explore the capability and limitations of exiting UGDA models, we systematically evaluate existing algorithms and investigate the underlying transferability for GNN. With these findings, we reveal a simple yet effective method that can even surpass existing UGDA algorithms.\n\u2022 We develop an easy-to-use library PyGDA to alleviate the workload of researchers when conducting experiments. Furthermore, users can easily construct their own models or datasets with minimal effort.\nThe source codes of our benchmark are available at https://github.com/pygda-team/pygda/ tree/main/benchmark, which provide unified APIs and adopt consistent data processing as well as data splitting approaches for fair comparisons."}, {"title": "Preliminaries and Related Work", "content": ""}, {"title": "Problem Definition", "content": "Given an attributed graph $G=(V,E)$ with n nodes and m edges, the node attribute matrix is represented as $X = \\{x_v|v \\in V\\} \\in \\mathbb{R}^{n \\times d}$, where d is the dimension of node attributes. The adjacency matrix is denoted as $A \\in \\mathbb{R}^{n \\times n}$, where $A_{i,j}=1$ means there exists an edge $e_{i,j} \\in E$ connecting node $v_i$ and $v_j$, and $A_{i,j}=0$ otherwise. $Y \\in \\mathbb{R}^{N \\times C}$ denotes the node label matrix, where C is the category of node labels. We focus on Unsupervised Graph Domain Adaptation (UGDA) on node classification tasks. It is a non-trivial task due to the complex domain shift between source and target graphs. Formally, given a labeled source graph $G_s = (V_s, E_s, Y_s)$ and an unlabeled target graph"}, {"title": "", "content": "$G_T = (V_T, E_T)$ with the data shift that $P_S(G) \\ne P_T(G)$. The goal is to train a graph neural network model $h: G \\rightarrow Y$ using the labeled source graph $G^s$ and the unlabeled target graph $G^t$ to accurately predict labels on target graph $G^t$.\nClassical Methods. There are two classical ways to minimize the domain discrepancy [32, 33]: minimizing pre-defined probability discrepancy metrics [34, 35] and adversarial learning techniques [36, 37, 38, 39]. For the pre-defined probability discrepancy metrics minimization models, node representations are first obtained from the encoder, and then domain-invariant representations are learned by minimizing probability discrepancy distances, such as MMD [40], CMD [35], etc. Instead of explicitly minimizing domain discrepancies, some methods incorporate the encoder with a domain classifier that predicts the domain from which the representation originates. For example, DANN [36] facilitates domain invariant learning (DIRL) to distinguish between source and target samples in the latent space. Building upon this framework, WDGRL [41] replaces the domain classifier with a network that learns an approximate Wasserstein distance. These classical methods are designed for computer vision and natural language processing tasks, where samples are independently and identically distributed [42]. However, marginal alignment of node representations in non-graph DA research is insufficient for graph-structured data, of which the distribution shift becomes more complex due to the interconnection among different nodes.\nSpecialized Methods for Graph Data. To tackle the unique challenges of graph knowledge transfer, several approaches have been proposed [8, 10, 43, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]. The dominant idea of existing work on node-level UGDA focused on leveraging node representations as intermediaries and then minimizing the domain shift by adversarial learning. This kind of methods incorporate classical DA methods with deep node embedding by designing a special encoder to learn transferable features. DANE [9] uses shared weight GCNs to get node representations and then handles distribution shift via least square generative adversarial network. ACDNE [10] utilizes two feature extractors to jointly preserve attributed affinity and topological proximities as deep network embedding module and incorporates a domain classifier to make node representations label-discriminative. UDAGCN [11] develops a dual graph convolutional network with attention mechanism to jointly exploit local and global consistency for effective graph representation learning. ASN [43] separates domain-private and domain-shared information and combines local and global consistency to capture network topology information. AdaGCN [12] leverages GCN to integrate network topology and combines adversarial domain adaptation with graph convolution. CWGCN [21] puts forward a two-step correntropy-induced Wasserstein GCN, which first suppresses the noisy nodes in the source graph and then learns the target GCN based on extending the Wasserstein distance. SA-GDA [22] proposes a spectral augmentation module to enhance the node representation learning, which combines the target domain spectral information within the source domain. DMGNN [20] employes a GNN encoder with dual feature extractors to separate ego-embedding learning from neighbor-embedding learning and then a label propagation node classifier is employed to refine label prediction. DGDA [23] addresses graph domain adaptation in a generative view, which disentangles the generation process into the semantic latent variables, the domain latent variables, and the random latent variables."}, {"title": "", "content": "Despite the progress made by the aforementioned UGDA models, such solutions still struggle to address the complex domain shift present in real-world graph data. The complex distribution shift between graphs usually combine node attribute shift, graph structure shift and label shift. Thus, to deal with the distinct effects of distribution shifts caused by graph structures, StruRW [16] investigates different types of distribution shifts of graph-structured data and reweights edges in the source graph to reduces the conditional shift of neighborhoods. Based on this work, PairAlign [25] not only uses edge weights to recalibrate the influence among neighboring nodes to handle conditional structure shift but also adjusts the classification loss with label weights to handle label shift. Except for reweighting, JHGDA [17] designs a hierarchical pooling model to extract meaningful and adaptive hierarchical structures and jointly minimizes marginal and class conditional distribution shifts on each hierarchical level. KBL [18] redefines the aggregate mechanism as learning a knowledge-enhanced posterior distribution for target domains, which learns the scope of knowledge transfer by connecting knowledgeable samples between domains.\nInstead of simply using GNN as a node embedding module, some methods devote effort to empirically and theoretically studying the role of GNN within domain adaptation [14, 15, 24]. This allows the UGDA method to be precisely customized based on the property of GNN, thereby enhancing their performance in graph domain adaptation. GRADE [14] introduces graph subtree discrepancy as a metric to measure the graph distribution shift by connecting GNNs with WL subtree kernel [44]. SpecReg [15] finds the OT-based bound for graph is closely coupled with the Lipschitz constant of GNN and proposes spectral regularization to modulate the Lipschitz constant to restrict the target risk bound. A2GNN [24] further investigates the GNN's underlying generalization capability behind its architecture and finds propagation operation plays a pivotal role. Based on this observation, A2GNN proposes a simple yet effective GNN which stacks more propagation layers on target branch. The timeline of UGDA algorithms covered in GDABench is shown in Figure 1 and more detailed descriptions are provided in Appendix C."}, {"title": "Datasets", "content": "We have carefully selected five widely used public datasets that showcase a wide spectrum of distribution shifts across graphs. These include Airport which consists of three domains Brazil (B), Europe (E) and USA (U); Blog that includes two domains Blog1 (B1) and Blog2 (B2); ArnetMiner which encompasses three domains DBLPv7 (D), Citationv1 (C) and ACMv9 (A); Twitch that includes six domains Germany (DE), England (EN), Spain (ES), France (FR), Porutgal (PT) and Russia (RU); MAG that includes six domains like CN, US, JP, FR, RU, and DE. The selection criteria for these datasets are primarily based on three factors: the complexity of the distribution shift, the scale of the dataset, and the potential for downstream applications. From these datasets, we have compiled a comprehensive collection comprising 74 distinct source-target adaptation pairs. Detailed attributes of each dataset are documented in Table 1 and the statistical methods for quantifying the types of domain shifts exhibited in the dataset is shown in Appendix B.\n\u2022 Wide range of distribution shift. Potential distribution shifts between source and target graphs can largely fall into three categories: feature shift, structure shift and label shift [31, 45, 25]. Our GDABench datasets capture a wide range of three distribution shifts across domains in varying degrees; The details are illustrated in Table 1 and more statistics are given in Appendix B. Specifically, the domains within Airport are dominated by structure shift, while domains in Blog, ArnetMiner, Twitch and MAG are affected by all kinds of shifts with different degrees.\n\u2022 Different scales with variant spans. We divide the size of the dataset by the average number of domain nodes. The small size (S) covers nodes below 5 thousand. The medium size (M) cover nodes below 10 thousand. The large size (L) covers nodes from 10 thousand to hundred thousand. Smaller datasets exhibit smaller differences in domain sizes, and vice versa. In the Blog and Airport, the size difference between the largest and smallest domains is less than 1000 nodes. In the case of ArnetMiner and Twitch, this difference ranges between 1,000 and 10,000. For the MAG, this difference ranges between 10,000 to 100,000. This allows us to understand the impacts of varying domain size on the efficacy of adaptation task.\n\u2022 Various downstream applications. The GDABench datasets encompass multiple applications, including citation relationships (ArnetMiner and MAG), social media (Blog and Twitch) and routines connections (Airport). Specifically, in ArnetMiner and MAG, nodes represent academic papers and edges indicate citation relationship. ArnetMiner groups domains by publisher, while"}, {"title": "Compared Models", "content": "Specialized UGDA Methods. This group includes specifically designed algorithms for graph domain adaptation task. We compare 16 models including (1) nine methods incorporating classicial DA methods with deep node embedding: DANE [9], ACDNE [10], UDAGCN [11], ASN [43], AdaGCN [12], CWGCN [21], SAGDA [22], DMGNN [20] and DGDA [23]; (2) four methods tailored for graph structure shift: StruRW [16], JHGDA [17], KBL [18] and PairAlign [25]; and (3) four methods based on domain adaptive message passing: GRADE [14], SpecReg [15], and A2GNN [24].\nSimGDA: Vanilla DA with GNN Variants. To understand the inherent transferability of GNN, we delve into its aggregation process and decouple it from two perspectives: how to aggregate and what to aggregate. For how to aggregate, we consider five aggregator including sum aggregator [46], mean aggregator [47], aggregate with weighted neighbours (GAT) [48] and aggregate with discriminative neighbours (GIN) [49]. For what to aggregate, we consider three aspects in terms of the hop-count of neighbours: (1) GNN without neighbours, where graph structure is not considered (degenerating to MLP); (2) GNN with one-hop neighours; and (3) GNN with muti-hop neighours. To avoid the over-smooth problem, we also add residual connections to enhance its modeling power [50, 46]. For alignment, we consider two widely used models for domain-invariant feature learning from computer vision: domain distance metric MMD [34] and adversarial learning DANN [36]. MMD proposes to match the distribution in the latent space through maximum mean discrepancy [40]. DANN introduces an adversarial objective to distinguish source and target samples in the latent space. We use one-layer GCN [46] as a control and create six GNN variants by altering only one module each time. Then, we get 14 models by combining these variants with two vanilla DA methods, abbreviated these 14 models as SimGDA.\nSimGDA+: SimGDA with Unsupervised Techniques. To further unlock the power of GNN for graph domain adaptation, we enhance SimGDA with unsupervised graph learning techniques on unlabeled target graph, which allows the model to learn meaningful representations without relying on domain-specific labels. We implement three unsupervised techniques in an end-to-end manner: (1) Information Maximization (IM) [51, 52, 19]. Ideally, a perfect prediction for target domain should be individually certain and globally diverse. To achieve this goal, we minimize the entropy for each individual sample, and maximize the entropy for each class. (2) Graph AutoEncoder (AE) [53, 54, 55]. Graph autoencoders encode nodes into a latent vector space and reconstruct the graph data from the encoded latent space. After obtaining target graph node representations, we employ a distinct decoder to reconstruct target graph structure. (3) Graph Contrastive"}, {"title": "", "content": "Learning (CL) [56, 57, 58, 59]: Graph contrastive learning methods maximize mutual information between augmented instances of the same object (e.g., node).\nAs changing the graph structure may affect the estimation of structure shift, we take random attribute masking to create augmented instances for each target node. Following previous works [57, 59], we utilize a one-layer perceptron (MLP) as projection head to map augmented representations to the shared latent space and then calculate contrastive loss based on normalized temperature-scaled cross entropy loss (NT-Xent) [60]. In total, we get 42 combined models, which integrates 14 SimGDA variants with 3 unsupervised techniques. We collectively refer to these combined models as SimGDA+, and the combination process is shown in Figure 2."}, {"title": "Experimental Results and Analyses", "content": "In this section, we study the experimental results of all the models. We first provide a comprehensive comparison of specialized UGDA methods across five datasets with diverse distribution shifts. Following that, we perform a thorough analysis between different GNN variants to understand how data shift imposes challenges on GNNs. Finally, we present the performance of SimGDA variants, which shows the limit of GNNs' transferability. For more information about metrics, hyperparameters, search spaces, and other implementation details, please refer to Appendix D."}, {"title": "Overall Comparisons", "content": "In this section, we try to elucidate the success of these UGDA algorithms through empirical evaluation across diverse datasets. In Table 2 and Table 3, we take a close look at the models' performance across 5 datasets on partial tasks utilizing Micro-F1 for Blog, Airport, and ArnetMiner, while employing Macro-F1 for MAG and AUROC for Twitch due to their imbalanced labels. For comprehensive experimental results, please refer to Appendix D. Our key findings include:\nObservation 1: When facing significant shifts, it is important to design solutions tailored to mitigate structural discrepancies. Although several methods that incorporate classical DA approaches with deep node embedding have achieved impressive results on Airport, Blog and ArnetMiner datasets (e.g., ACDNE and DMGNN), they fail to obtain satisfied performance on datasets with significant shifts. These results underscore the limitations of marginal distribution alignment techniques in the presence of significant structural and label shifts in graph data. As shown in Table 3, methods tailored to address graph structure shifts show reasonable improvements over those incorporating classical DA methods with deep node embedding. This suggests that mitigating the impact of graph structure shift on node representation learning under this scenario is crucial.\nObservation 2: Domain-adaptive message passing methods demonstrate superior and robust performance across a wide range of datasets and tasks. While methods that align marginal feature distributions and those designed for graph structure shifts can address datasets with mild and severe data shifts respectively, strategies specifically developed to leverage GNN properties demonstrate robustness and superior performance across diverse data shifts. As shown in Table 2 and Table 3, methods designed based on the inherent properties of GNN achieves the top-three best performance in 8 tasks out of 12 tasks. This finding suggests that leveraging the structural strengths of GCNs, combined with well-established domain adaptation principles, can result in an effective and efficient approach to addressing the challenges of domain variability in graph datasets. Such strategies represent a promising direction for future research and application in this field."}, {"title": "Understanding and unlocking the inherent power of GNN", "content": "Note that despite many UGDA methods integrating traditional domain adaptation techniques, we observe their performance remains unsatisfactory and can even be inferior to that of SimGDA. This leads us to an intriguing question: do the intrinsic mechanisms of GNN play a more crucial role in enhancing transferability? To further investigate the question, we take one-layer GCN combined with vanilla DA as a baseline, and compare six variants: Max-Aggr, Mean-Aggr, GAT-Aggr, GIN-Aggr, with-no-neighbor, and with-multi-hop-neighbor; shown in shown in Figure 3. Moreover, we enhance above SimGDA with unsupervised graph learning techniques, referred to as SimGDA+, to explore the limit of GNNs in graph domain adaptation; shown in Table 2 and Table 3."}, {"title": "Conclusion", "content": "In this paper, we introduce GDABench, the first comprehensive benchmark for unsupervised graph domain adaptation. Our evaluation encompasses 16 well-known models across 5 real-world datasets exhibiting diverse data distribution shifts. Furthermore, we design 6 GNN variants to investigate the inherent transferability of GNNs, enhancing them with 3 unsupervised techniques to explore their potential limits. Our empirical results shows that (1) the performance of current UGDA models varies significantly across different datasets and adaptation scenarios; (2) tailored strategies are essential for addressing and mitigating graph structural shifts, particularly when distribution discrepancies are substantial. (3) the transferability of GNNs in UGDA is heavily dependent on aggregation scope and architecture, influenced by factors such as label shift severity and graph heterophily. We also provide unified APIs and adopt consistent data processing as well as data splitting approaches for fair comparisons. In the future, we plan to extend GDABench to include broader scenarios [61], more"}, {"title": "Border Impacts and Limitations", "content": "Our benchmark fosters innovation and advances research in graph domain adaptation by providing a standardized evaluation platform, leading to the development of more effective algorithms. This standardization helps researchers compare methods more fairly, driving progress and collaboration within the field. However, benchmark datasets may introduce limitations that could impact the generalization of findings to real-world scenarios. This risk includes the potential for unrealistic performance expectations if the benchmark does not adequately represent the diversity and complexity of real-world data. We plan to enhance GDABench by including more settings such as source-free and open-set scenarios. This expansion will help to cover a wider range of domain adaptation challenges, thereby fostering the development of algorithms that are not only more robust but also versatile enough to navigate the complexities of diverse and dynamic real-world scenarios. This trajectory in research will be pivotal in advancing the capabilities of domain adaptation techniques, ensuring their applicability and efficacy across various domains and evolving data landscapes."}, {"title": "Appendix", "content": ""}, {"title": "Distribution Shift in Graph-Structured Data", "content": "Distribution shift appears when the joint distribution differs between source domain and target domain [6, 62]. Assuming that the relationship between the input and class variables is unchanged, there are two kinds of distribution shift, i.e., covariate shift and label shift (prior probability shift) [63].\nA.1 Covariate Shift\nCovariate shift [64] refers to changes in the distribution of the input variables, which can be defined formally as follows:\nDefinition 1 (Covariate Shift). Covariate shift appears when $P_S(G) \\ne P_T(G)$ with the assumption of $P_S(Y|G) = P_T(Y|G)$, where $P_S$ and $P_T$ are the probability distributions of the source and target domains, respectively.\nTo deal with covariate shift, it is essential to align $P_S(Y|H)$ and $P_T(Y|H)$, where H is the representation after data attributes passing through the encoder. However, in graph-structured data, node representation is not only affected by the data attributes but also graph structure. Thus, covariate shift in graph data can be decoupled as feature shift and structure shift [25].\nDefinition 2 (Feature Shift). Given the joint distribution of the node attributes and node labels $P_T(X, Y)$, the feature shift is then defined as $P_S(X,Y) \\ne P_T(X, Y)$ with the assumption of $P_S(Y|G) = P_T(Y|G)$.\nDefinition 3 (Structure Shift). Given the joint distribution of the adjacency matrix and node labels $P_T(A, Y)$, the structure shift is then defined as $P_S(A,Y) \\ne P_T(A, Y)$ with the assumption of $P_S(Y|G) = P_T(Y|G)$.\nA.2 Label Shift\nLabel shift refers to changes in the distribution of the class variable Y. It also appears with different names in the literature and the definitions have slight differences between them.\nDefinition 4 (Label Shift). Label shift occurs when the distribution of labels changes across two domains, which is defined as $P_S(Y) \\ne P_T(Y)$ where $P_S(G|Y) = P_T(G|Y)$.\nIn all, structure shift is unique to graph data due to the non-IID nature caused by node interconnections. Moreover, the learning of node representations implemented by the GNN will mix the feature shift, sutructure shift and label shift [31]."}, {"title": "Detailed Description of Datasets", "content": "In this section, we provide additional details about the datasets used in our benchmark.\nB.1 Dataset Description\n\u2022 Airport 1: The Airport datasets consist of three separate collections corresponding to Brazil (B), Europe (E), and the USA (U). In these datasets, nodes represent airports and edges denote flight connections between them. The labels categorize airports by activity levels, measured in terms of flights or passenger numbers.\n\u2022 Blog 2: Blog1 and Blog2 are disjoint social networks derived from the BlogCatalog dataset. In these networks, nodes correspond to bloggers, and edges reflect friendships among them. The attributes for each node consist of keywords from the blogger's self-description, and each node is assigned a label denoting its group affiliation. Given that both Blog1 and Blog2 originate from the same underlying network, their data distributions are nearly identical."}, {"title": "Shift Statistics of Datasets", "content": "According to dataset statistics, shown in Table 4 and Figure 4, we measure the degree of domain shift exhibited in the datasets for each tasks using statistical methods. We use MMD [40], CSS [25], Kullback-Leibler Divergence to characterize the degree of feature shift, structure shift and label shift. The results of each tasks is shown in Table 9. We take the average results of all tasks as the shift statistics for the datasets, shown in Table 5."}, {"title": "Other Information in GDABench", "content": "We implement our GDABench library in PyTorch [66] and provide an infrastructure to run all the experiments to generate corresponding results. We have stored all models and logged all hyperparameters to facilitate reproducibility. Our framework can be easily extended to include new algorithms.\nD.1 Metrics\nFollowing previous works [43, 11], we present the experiment performance on target domain. We select Area Under the Receiver Operating Characteristic Curve (AUROC) for Twitch, Micro-F1 for Airport, Blog and ArnetMiner and Macro-F1 for MAG.\n\u2022 AUROC measures how well a model can distinguish between positive and negative classes by looking at the area under the ROC curve. This curve shows the true positive rate versus the false positive rate at various thresholds. An AUROC score of 1 means perfect distinction, while a score of 0.5 indicates the model does no better than guessing randomly.\n\u2022 Macro-F1 calculates the F1 score for each category independently and then taking the average of these scores. This method treats all categories equally, regardless of their frequency in the dataset. It is particularly useful when you want to understand the model's performance across smaller or less frequent categories, ensuring that performance on rare categories has as much weight as performance on more common ones.\n\u2022 Micro-F1 computes the average F1 score. This is achieved by summing up the true positives, false positives, and false negatives of the model across all categories and then calculating the F1 score using these totals. As a result, Micro-F1 gives a higher weight to the performance on more frequent categories, making it a useful metric when you're interested in understanding how the model performs on the majority of cases or the overall dataset.\nD.2 Additional Experimental Details\nHardware Specifications. The experiments are performed on one Linux server (CPU: Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz, Operation system: Ubuntu 18.04.5 LTS). For GPU resources, one NVIDIA Tesla V100 card with 32G are utilized The python libraries we use to implement our experiments are Python 3.8, Pytorch 1.13.1, Pytorch-geometric 2.4.0, Pytorch-sparse 0.6.15 and Pytorch-scatter 2.1.0."}, {"title": "Hyperparameter Settings", "content": "To control the effect of hyperparameter selection and ensure fairness, we standardize the evaluation process with hyperparameter tuning. We utilize grid search to form the predefined search space for each models. We use all the source nodes and target nodes for model training. The experiments are repeated three times, and we report the mean performance. Table 10 provides a comprehensive list of all hyperparameters used in our grid search.\nMore Experimental Results In accordance with Table 2 and 3, we provide the performance for all tasks of each model in Table 6, 7 and 8. In accordance with Figure 3, we provide the compared performance of vanilla DA with 6 GNN variants in Figure 5. We also demonstrate the running time and memory consumption of each model on S/M/L datasets respectively. For time consumption, we evaluate the efficiency of baselines by measuring the time it takes to converge. As shown in Figure 6, we can observe that some algorithms (e.g. A2GNN) can achieve relatively good performance with less complexity.\nD.3 The PyGDA Library\nPyGDA is a Python library for Graph Domain Adaptation built upon PyTorch and PyG to easily train graph domain adaptation models in a sklearn style. PyGDA includes 15+ graph domain adaptation models. See examples with PyGDA below!\nGraph Domain Adaptation Using PyGDA with 5 Lines of Code\nfrom pygda.models import A2GNN\n# choose a graph domain adaptation model\nmodel = A2GNN(in_dim=num_features, hid_dim=args.nhid,\nnum_classes=num_classes, device=args.device)\n# train the model\nmodel.fit(source_data, target_data)\n# evaluate the performance\nlogits, labels = model.predict(target_data)\nPyGDA is featured for:\n\u2022 Consistent APIs and comprehensive documentation.\n\u2022 Cover 15+ graph domain adaptation models.\n\u2022 Scalable architecture that efficiently handles large graph datasets through mini-batching and sampling techniques.\n\u2022 Seamlessly integrated data processing with PyG, ensuring full compatibility with PyG data structures."}]}