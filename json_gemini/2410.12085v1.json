{"title": "Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning", "authors": ["Fengyu Gao", "Ruida Zhou", "Tianhao Wang", "Cong Shen", "Jing Yang"], "abstract": "Large Language Models (LLMs) rely on the contextual information embedded in examples/ demonstrations to perform in-context learning (ICL). To mitigate the risk of LLMs potentially leaking private information contained in examples in the prompt, we introduce a novel data-adaptive differentially private algorithm called AdaDPSyn to generate synthetic examples from the private dataset and then use these synthetic examples to perform ICL. The objective of AdaDPSyn is to adaptively adjust the noise level in the data synthesis mechanism according to the inherent statistical properties of the data, thereby preserving high ICL accuracy while maintaining formal differential privacy guarantees. A key innovation in AdaDPSyn is the Precision-Focused Iterative Radius Reduction technique, which dynamically refines the aggregation radius the scope of data grouping for noise addition - based on patterns observed in data clustering, thereby minimizing the amount of additive noise. We conduct extensive experiments on standard benchmarks and compare AdaDPSyn with DP few-shot generation algorithm (Tang et al., 2023). The experiments demonstrate that AdaDPSyn not only outperforms DP few-shot generation, but also maintains high accuracy levels close to those of non-private baselines, providing an effective solution for ICL with privacy protection.", "sections": [{"title": "1 Introduction", "content": "In-context learning (ICL) (Brown et al., 2020; Min et al., 2022) enables large language models (LLMs) (OpenAI, 2023) to adapt to domain-specific information without modifying the pre-trained model. This adaptation is achieved by conditioning the model on a prompt, which contains an instruction and a series of task-specific question-answer pairs (called demonstrations or examples). Using this prompt, LLMs can then generate responses that are tailored to the corresponding task. The ease of usage and cost-benefit of ICL have motivated the adoption of LLMs in several applications, such as machine translation (Sia and Duh, 2023), code generation (Pourreza and Rafiei, 2023) and customer service (Lee et al., 2022).\nHowever, privacy is a significant concern when incorporating users' data into prompts, especially in areas like healthcare and finance (Wang et al., 2023; Priyanshu et al., 2023; Duan et al., 2023), as this introduces risks of exposing sensitive personal information. To address this concern, existing works (Wu et al., 2023; Tang et al., 2023; Duan et al., 2023; Carey et al., 2024; Hong et al., 2023) have explored approaches to mitigate these risks using the rigorous privacy safeguards provided by differential privacy (DP) (Dwork et al., 2006b). Notably, Tang et al. (2023) proposed to generate synthetic few-shot demonstrations from the private dataset for use in ICL inference and guarantee DP with respect to examples in the private dataset. This method uses the classic DP sample-and-aggregate framework"}, {"title": "2 Related Work", "content": "Differentially Private In-Context Learning and Fine-tuning of LLMs. Duan et al. (2023) first study privacy leakage through membership inference attacks (MIAs) and suggest mitigating MIA risks using DP ensembling (Papernot et al., 2018) on various model versions. Recent studies (Wu et al., 2023; Tang et al., 2023; Duan et al., 2023; Carey et al., 2024) have developed methods based on DP ensembling (Papernot et al., 2018) to preserve privacy of the underlying prompt data. Notably, Tang et al. (2023) develop an algorithm that generates differentially private synthetic few-shot demonstrations from a private dataset for ICL inference, which serves as the baseline of this work and will be discussed in greater detail in subsequent sections. Hong et al. (2023) propose the DP-OPT algorithm that generates private and transferable instructions within prompts on the client side to be used with cloud-hosted LLMs. Carey et al. (2024) focus on protecting tabular data used for ICL. We note that all of those works (Tang et al., 2023; Carey et al., 2024; Hong et al., 2023) design private aggregation methods in a data-independent way, i.e., adding the same amount of noise during private aggregation.\nRecently, Wu et al. (2023) and Duan et al. (2023) study differentially private ICL with data-dependent privacy analysis. Wu et al. (2023) propose DP inference by privately aggregating results from multiple queries using disjoint sampled demonstrations. They use the classic propose-test-release (PTR) paradigm (Zhu and Wang, 2022) to select the top-K tokens from LLM outputs. If the vote count difference between the K-th and (K + 1)-th highest tokens exceeds two, the tokens can be released without further privatization. However, ICL tasks can involve an unlimited number of queries, and Wu et al. (2023) consume the privacy budget per query, limiting the number of queries that can be answered within a given budget.\nDuan et al. (2023) assume the existence of an unlabeled public dataset, which contrasts with our approach that requires no public data. In their method, an ensemble of teacher models uses private majority voting to label this public dataset. The labeled data then helps construct private prompts. Following Papernot et al. (2018), they focus on scenarios where a high agreement among teachers can reduce privacy costs below data-independent bounds. When teacher agreement is low, they discard the public data. In contrast, our method is better suited for scenarios where assuming the availability of unlabeled public data with a distribution similar to private data is unrealistic, such as in healthcare or industrial applications. Moreover, Duan et al. (2023) mainly focus on text classification tasks with a restricted label space, while our method applies to a broader range of tasks.\nOn the other hand, several studies have focused on private fine-tuning of LLMs (Yu et al., 2021; Li et al., 2021; Chen et al., 2024; Tang et al., 2024; Zhang et al., 2023; He et al., 2022). Besides, Majmudar et al. (2022) introduce DP at the decoding stage of a pre-trained LLM.\nAdaptive Differential Privacy. Adaptive differential privacy primarily focuses on privately calibrating noise to the local sensitivity, which is a dataset-dependent function and is much smaller than the global sensitivity. Two representative solutions include the smooth sensitivity framework (Nissim et al., 2007) and the propose-test-release (PTR) framework (Dwork and Lei, 2009). The main idea of the smooth sensitivity framework is to add noise calibrated to the smooth sensitivity, an upper bound on"}, {"title": "3 Problem Definition, Threat Model, and Notations", "content": "In-context learning. Given a query x, a candidate answer set (label set) Y and a pre-trained large language model LLM such as GPT (Radford et al., 2018; OpenAI, 2023), ICL aims to predict a label $y \\in Y$ of query x, using LLM conditioned on nshot demonstrations $C = \\{(x_1, y_1), (x_2, y_2),..., (x_{\\text{Nshot}}, y_{\\text{Nshot}} )\\}$ where $(x, y)$ is an input-label pair.\nIn this paper, we consider both classification and information extraction tasks. For classification tasks with a restricted label space $Y = \\{y_1,...,y_m\\}$, we first compute the probability of each label $y_i \\in Y$ using LLM as $P_{\\text{LLM}}(Y_i | C, x)$. Then we predict the final label y by selecting the label with the highest probability from the label set Y, i.e., $y = \\arg \\max_{y_i \\in Y} P_{\\text{LLM}}(Y_i | C, x)$. For information extraction tasks, the label space Y is unrestricted, and can include any sequence of tokens from the LLM's vocabulary. In these tasks, we predict y by having LLM generate a sequence of tokens until it produces a special end-of-sequence (EOS) token, marking the completion of the output.\nDifferentially private ICL. Our objective is to protect the privacy of demonstrations $C \\in D_{\\text{priv}}$ in the prompt against an adversary that aims to access or infer private information about the demonstrations. Since ICL tasks may encounter an unlimited number of queries, to protect the privacy of the dataset $D_{\\text{priv}}$, we adopt a differentially private data synthesis approach to generate synthetic few-shot examples $C = \\{\\tilde{z}_1,\\ldots, \\tilde{z}_{nshots} \\}$, where $\\tilde{z}_i = (\\tilde{x}_i, \\tilde{y}_i)$, from the underlying distribution of $D_{\\text{priv}}$ while satisfying $(\\epsilon, \\delta)$-differential privacy on the private dataset $D_{\\text{priv}}$. For each ICL task, we generate task-specific synthetic examples C with the help of an LLM and a private dataset $D_{\\text{priv}}$. Then ICL can be performed using demonstrations from C in a prompt tailored to each task. By default, we incorporate all demonstrations from C in a prompt for each task.\nWe formally define (\u03b5, \u03b4)-differential privacy ((\u03b5, \u03b4)-DP) as follows."}, {"title": "4 Proposed Method", "content": "We use a two-stage framework to perform differentially private ICL, as illustrated in Figure 1. Generally speaking, in Stage 1, we generate DP synthetic few-shot demonstrations $C = {\\tilde{z}_1,\\ldots, \\tilde{z}_{nshots} }$ based on the private dataset $D_{\\text{priv}}$; and in Stage 2, we use these synthetic demonstrations for ICL.\nThe main challenge of this framework lies in the DP synthetic data generation (Stage 1). To maintain DP, a PATE-like framework is utilized, similar to Tang et al. (2023). Take the generation of a synthetic demonstration $z = (x, \\tilde{y}) \\in C$ as an example. First, label $\\tilde{y} \\in Y$ is randomly selected independently of private dataset $D_{\\text{priv}}$, and its corresponding demonstration $\\tilde{x}$ is generated token by token, starting from an empty string $x = '$'. The next token for $\\tilde{x}$ is generated by aggregating probability vectors $P_{\\text{priv}}^1,..., P_{\\text{priv}}^M$, each of which is calculated by an LLM taking some prompt as input. Each prompt is a concatenation of a subset of examples (in a reverse order $(y_i, x_i)$) from the private dataset $D_{\\text{priv}} = \\{(x_i, Y_i)\\}$, the synthetic label $\\tilde{y}$ and the currently generated string $x$. Due to the dependence of the probability vectors on private dataset $D_{\\text{priv}}$, DP aggregation is required, and a classic way to achieve this is by adding noise (e.g., Gaussian mechanism) as in Tang et al. (2023).\nOur approach is motivated by a hypothesis that the next-token predictions by different examples approximately reach a consensus. Once this hypothesis holds, the key innovation of our approach is to leverage this feature and reduce noise levels during aggregation in a data-adaptive manner. We verify the hypothesis quantitatively and tackle the key challenge in the following two sections, respectively."}, {"title": "4.1 Consensus Behavior Measured by Clustering", "content": "We quantitatively measure the degree of consensus among the next-token generation probability vectors $P_{\\text{priv}}^1,..., P_{\\text{priv}}^M$. Specifically, we approach this by visualizing consensus through the geometric concept of enclosing these vectors within a minimal enclosing ball, defined as an (r, c, p)-ball, where r is the radius, c is the center and $p \\in (0,1]$ represents the required portion of vectors covered in the ball. For a given coverage requirement p, a smaller r indicates a tighter grouping, reflecting a high consensus among the model outputs. We hypothesize that these next-token generation probability vectors are typically highly clustered in a ball with a small radius r.\nTo verify the hypothesis, we conduct experiments using the DP few-shot generation algorithm (Tang et al., 2023), and report the radius r of the minimal ball that encloses at least 80% (p = 0.8) of the next-token probability vectors generated by using different private examples in Table 1. For determining a small radius r that contains at least 80% of the input points, we adopt the GoodRadius algorithm in Nissim et al. (2016). We conduct experiments on three classification tasks: AGNews (Zhang et al., 2015), DBPedia (Zhang et al., 2015) and TREC (Voorhees and Tice, 2000), as well as two information extraction tasks, MIT-G and MIT-D (Liu et al., 2012), using the Llama-2-7b-hf model (Touvron et al., 2023). Details of the DP few-shot generation algorithm parameters can be found in Table 11.\nAs shown in Table 1, the measured radius r, ranging from 0.05 to 0.15, is much smaller than $\\sqrt{2}/2$, the radius of the minimal ball that contains the probability simplex. These results confirm a high degree of"}, {"title": "4.2 Precision-Focused Iterative Radius Reduction", "content": "Building on the observation that next-token generation probability vectors from the LLM are highly clustered, we utilize an intuitive approach to adaptively adjust the noise level during aggregation: we first locate the minimal ball covering most vectors under DP and then project all points to this ball in $\\ell_2$-norm to bound the influence of each point, thereby minimizing noise addition. This method, which adaptively leverages the inherent structure of the probability vectors, is essential to the success of our proposed method.\nThe task of finding a minimal enclosing ball under DP is known as the DP 1-clustering problem in the literature (Nissim et al., 2016). However, existing methods (Nissim et al., 2016; Nissim and Stemmer, 2018; Ghazi et al., 2020) are hard to implement and computationally expensive.\nTo this end, we propose a novel technique called Precision-Focused Iterative Radius Reduction. We start with a large radius R and judiciously reduce it towards a small target radius r through iterative refinement. The target radius r is obtained under DP constraints using a tractable subroutine GoodRadius from DP 1-clustering (Nissim et al., 2016), which provides a threshold such that the designed radius should never go below. The reduction of radius R is carefully monitored by coverage checks, which ensure the majority of the probability vectors are covered by a ball with that radius. The center of the ball is efficiently obtained by private projected mean estimation. This dynamic adjustment of the ball ensures it is minimized effectively according to the distribution of the points along the adjustments. The amount of noise required for private projected mean estimation can be reduced, thus improving the quality of the synthetic demonstrations and the performance of ICL. We elaborate this Precision-Focused Iterative Radius Reduction technique in Section 4.3."}, {"title": "4.3 AdaDPSyn Algorithm", "content": "We now introduce the proposed AdaDPSyn algorithm (Algorithm 1), which generates DP synthetic few-shot examples from the private dataset $D_{\\text{priv}}$. For a given label $\\tilde{y}$ randomly selected from the label set without replacement, AdaDPSyn sequentially generates one token at a time, starting from an empty list. Each token generation begins with the Next Token Generation subroutine (Line 4). Roughly speaking, the Next Token Generation process (Tang et al., 2023) involves sampling MN examples with label y from the private dataset $D_{\\text{priv}}$, dividing them into M disjoint subsets, and then using these subsets as demonstrations to predict the next token's distribution $P_{\\text{priv}}^1,..., P_{\\text{priv}}^M$ using an LLM. The vocabulary is restricted to S containing the top-K most probable tokens, which are determined from next-token probabilities generated only from instructions without using any private data. Details of Next Token Generation can be found in Appendix A.1. Then we privately aggregate $P_{\\text{priv}}^1,..., P_{\\text{priv}}^M$ using our Precision-Focused Iterative Radius Reduction technique (Lines 5-21), which consists of four critical steps:\nStep 1: Establish Target Radius Differentially Privately. The process begins by setting a target radius r using the GoodRadius subroutine (Nissim et al., 2016) in Line 5. GoodRadius is a subroutine"}, {"title": "4.4 Privacy Analysis", "content": "Theorem 1. Algorithm 1 is (\u03b5, \u03b4)-differentially private.\nProof Overview. We adopt the commonly used R\u00e9nyi differential privacy (RDP) (Mironov, 2017) to track the privacy cost in our algorithm, as it allows us to tightly quantify the privacy guarantees from the composition of multiple mechanisms. Our proof mainly consists of three steps. First, we show that each iteration of the algorithm is (\u03b1, \u03c4)-RDP. In each iteration, our algorithm consists of 3 components: one instance of (\u03b1, \u03c4\u03bf)-RDP algorithm GoodRadius (Line 5), at most (\u00ce +1) instances of (\u03b1, \u03c4\u2081)-RDP projected mean estimation using Gaussian DP Mechanism (Line 8 and Line 20) and at most T instances of (\u03b1, \u03c42)-RDP radius coverage check using Gaussian DP Mechanism (Line 12). We apply the composition property of RDP to ensure the privacy guarantee in each iteration, that is, we ensure that \u03c4 = \u03c4\u03bf + (\u00ce +1)T1 +\u00ceT2. Next, since the Next Token Generation subroutine (Line 4) involves drawing MN samples from the dataset $D_{\\text{priv}}$, there is a privacy amplification by subsampling at each iteration. Finally, we compose privacy loss across all the Tmax iterations of our algorithm using composition theorems. We rely on the conversion lemma (Balle et al., 2020) to convert the RDP guarantee back to DP notions. We provide a full proof in Appendix B."}, {"title": "5 Experiments", "content": "Datasets. We study text classification on three datasets: 4-way news classification AGNews (Zhang et al.,"}, {"title": "5.1 Main Results", "content": "We present our main results in Table 2. We provide the mean and standard deviation of the accuracy on the test data with ICL over 5 runs with different random seeds. In general, our results demonstrate that AdaDPSyn outperforms DP few-shot generation (Tang et al., 2023) across various privacy settings, while also closely approximating the performance of the non-private baseline.\nWe observe that, compared with DP few-shot generation, AdaDPSyn provides gains at privacy levels \u03b5 = 1,2,4,8. For instance, in the AGNews news classification task at \u03b5 = 1, our method achieves an accuracy of 65.42% compared to 60.74% under DP few-shot generation. In the DBPedia topic classification, we reach 66.76%, surpassing the previous 64.68%. Moreover, in the information extraction tasks, our"}, {"title": "5.2 Ablation Studies", "content": "Varying number of shots. Using the DBPedia dataset and setting \u03b5 4, we test the number of shots for ICL with nshots = 1,2,4,8. The results are presented in Table 3. \u03b5 = 4 (AdaDPSyn) presents the performance of our private solution and \u03b5 = 4 (DP few-shot generation) is based on the solution in Tang et al. (2023). We present two non-private performances: (i) DP few-shot generation algorithm operates without any added noise; (ii) few-shot demonstrations randomly selected from the private dataset.\nWe observe that AdaDPSyn consistently outperforms DP few-shot generation across all n-shot settings. Additionally, increasing the number of shots can improve the performance of AdaDPSyn. This suggests that AdaDPSyn can benefit from larger n-shot scenarios.\nVarying LLMs. We evaluate ICL performance across different LLMs and present the results in Table 4. Specifically, we use the generated examples from Section 5.1 as demonstrations with GPT-3.5 Turbo and GPT-40 mini, available through OpenAI's service, for downstream ICL tasks. The experiments are conducted on the DBPedia dataset.\nWe observe that AdaDPSyn consistently outperforms DP few-shot generation and performs comparably to non-private baselines. Performance improves with both GPT-40 mini and GPT-3.5 Turbo for all models, with GPT-40 mini showing the highest overall results. Our method remains open to further improvements with more advanced LLMs.\nPerformance against membership inference attacks (MIAs). Additionally, we conduct an"}, {"title": "6 Conclusion", "content": "In this work, we introduced the AdaDPSyn algorithm, a novel approach to ICL that incorporates DP to safeguard sensitive data used in LLM prompts. By leveraging a data-adaptive noise addition strategy through our Precision-Focused Iterative Radius Reduction technique, we effectively reduced noise levels without compromising DP guarantees, thus maintaining higher accuracy for ICL. Our empirical results demonstrate the superior performance of AdaDPSyn, which nearly matches the non-private baseline's performance on several benchmarks. One limitation of this work is the lack of theoretical guarantees for utility, as it is difficult to find an appropriate closed-form expression of ICL accuracy. We leave this exploration to future work."}, {"title": "A DP Algorithms", "content": "In this section, we list all the differentially private mechanisms that are used in Algorithm 1 and provide their guarantees."}, {"title": "A.1 DP few-shot generation", "content": "The DP few-shot generation algorithm (Tang et al., 2023) is detailed in Algorithm 2. The Next Token Generation subroutine (Tang et al., 2023) is presented in Algorithm 3."}, {"title": "A.2 GoodRadius", "content": "Nissim et al. (2016) propose the GoodRadius algorithm (Algorithm 4) and provide its DP analysis. We adapt this with some modifications to provide an RDP guarantee. For readability, we include the RDP analysis here.\nTheorem 2 (Nissim et al. (2016)). GoodRadius (Algorithm 4) is (\u03b1, \u03c4\u03bf)-RDP.\nProof. From Nissim et al. (2016), the sensitivity of L defined in Line 3 of Algorithm 4 is 2. Then in Line 4, we search for an r such that L(r) > t and L(r/2) < t while maintaining differential privacy. This is achieved by using binary search with noisy estimates of L for the comparisons. Specifically, we introduce a tolerance parameter 0, which determines the precision of the binary search. The number of iterations for the binary search is at most $\\lceil \\log_2(2/\\theta) \\rceil$. In each iteration, we apply Gaussian mechanism to estimate L(r) and L(r/2), i.e., $\\tilde{L}(r_{\\text{mid}}) \\leftarrow L(r_{\\text{mid}}) + N(0, 4\\sigma_0^2)$ and $\\tilde{L}(r_{\\text{mid}}/2) \\leftarrow L(r_{\\text{mid}}/2) + N(0, 4\\sigma_0^2)$. By setting"}, {"title": "B Privacy Analysis", "content": "In this section, we provide the privacy analysis for Algorithm 1.\nTheorem 3 (Restatement of Theorem 1). Algorithm 1 is (\u03b5, \u03b4)-differentially private.\nWe introduce some concepts and relevant theorems from the literature required for our analysis first. While (\u03b5, \u03b4)-DP is a useful definition of privacy, it does not allow us to tightly quantify the privacy guarantees from the composition of multiple mechanisms. Instead, the notion of R\u00e9nyi Differential Privacy (RDP) (Mironov, 2017) provides a succinct way to monitor the privacy costs from the composition of multiple mechanisms.\nDefinition 2 (R\u00e9nyi Divergence (Mironov, 2017)). For two probability distributions P and Q defined over R, the R\u00e9nyi divergence of order \u03b1 > 1 is\n$D_{\\alpha}(P||Q) = \\frac{1}{\\alpha-1} \\log {E_{x \\sim Q} \\alpha(\\frac{P(x)}{Q(x)})^{\\alpha}}$"}, {"title": "C Experimental Supplementary", "content": "C.1 Experiments Compute Resources\nThe experiments use 4 NVIDIA RTX A5000 GPUs, each equipped with 24,564 MiB of memory. Approximately 24 hours are required to reproduce the main results in Section 5.1.\nC.2 Datasets\nIn this section, we describe the datasets used in our experiments.\n\u2022 AGNews. The AG News (AG's News Corpus) dataset (Zhang et al., 2015) comprises news articles categorized into four labels: World, Sports, Business, and Sci/Tech. It includes 30,000 training samples and 1,900 test samples per class. For our experiments, we randomly select 1,000 samples from the test set.\n\u2022 DBPedia. The DBPedia ontology classification dataset (Zhang et al., 2015) includes contents categorized into one of 14 topics: Company, School, Artist, Athlete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film, and Book. This dataset includes 40,000 training samples and 5,000 test samples for each class. For our experiments, we randomly select 49,999 samples from the training set and 1,000 samples from the test set.\n\u2022 TREC. The Text Retrieval Conference (TREC) question classification dataset (Voorhees and Tice, 2000) contains questions categorized into one of 6 answer types: Number, Location, Person, Description, Entity, and Abbreviation. The dataset includes 5,452 training samples and 500 test samples, distributed non-uniformly across the categories.\n\u2022 MIT Movies. MIT Movies trivia10k13 dataset (Liu et al., 2012) comprises movie reviews designed for information extraction tasks, with specific slots for movie genre (MIT-G) and director name (MIT-D). For training and testing, MIT-G includes 2,953 and 780 samples respectively, while MIT-D contains 1,561 training samples and 415 test samples.\nC.3 Non-private Baselines\nIn this section, we present the performance of non-private baselines for \u03b5 = \u221e: (i) DP few-shot generation algorithm operates without any added noise; (ii) 4-shot demonstrations randomly selected from the private dataset.\nC.4 Hyperparameter Search\nIn this section, we conduct ablation studies on key hyperparameters in Algorithm 1. We present results for datasets with varying \u5165 and \u00ce values in Table 6-Table 10. Specifically, we consider T = [1,2] and"}, {"title": "C.5 Hyperparameters", "content": "To ensure a fair comparison, we follow the guidance from Tang et al. (2023) to select parameters for the DP few-shot generation algorithm. Specifically, K is held constant at 100, and a grid search is performed to determine the optimal values for M and N (N = [1,2,4], MN = [20, 40, 80]). We also use the same"}, {"title": "C.6 Empirical Privacy Evaluation by Membership Inference Attack", "content": "While DP guarantee inherently provides a theoretical guarantee against privacy leakage, it is also important to assess empirical privacy risks. In this section, we evaluate the empirical privacy of AdaDPSyn against membership inference attack (MIA) (Shokri et al., 2017).\nWe follow the prior work (Duan et al., 2023), where the authors instantiate a membership inference attack (MIA) in the ICL framework. The goal is to determine whether a given private example was used"}, {"title": "C.7 Prompt Formats", "content": "In this section, we present prompt formats during ICL and the prompt construction functions PB(\u00b7) for Next Token Generation (Algorithm 3). While we use the same prompt format during ICL following Tang et al. (2023), we present them here in Table 18 and Table 19 for the convenience of the reader. We also present the prompt construction functions PB(\u00b7) used in Next Token Generation (Algorithm 3) in Table 20 and Table 21."}, {"title": "C.8 Demonstrations", "content": "In this section, we present generated samples using AdaDPSyn at \u025b = 4 for the DBPedia dataset, as shown in Table 22. These results are compared to those obtained using DP few-shot generation, displayed in Table 23. Both methods use the LLama-2-7B-hf model."}]}