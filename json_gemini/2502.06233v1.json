{"title": "Confidence Improves Self-Consistency in LLMs", "authors": ["Amir Taubenfeld", "Tom Sheffer", "Eran Ofek", "Amir Feder", "Ariel Goldstein", "Zorik Gekhman", "Gal Yona"], "abstract": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.", "sections": [{"title": "1 Introduction", "content": "Modern large language models (LLMs) demonstrate strong reasoning capabilities (Bubeck et al., 2023; Guo et al., 2025), driven in part by their capacity to generate a sequence of intermediate reasoning steps that lead them toward a final answer (Wei et al., 2022; Jaech et al., 2024). Self-consistency (Wang et al., 2022) is a popular decoding strategy that further improves LLMs' reasoning performance by sampling a diverse set of reasoning paths and selecting the most frequent answer as the final output. Despite its effectiveness, this approach is also computationally expensive, as it requires generating a large number of (long) reasoning paths to increase the chances that the correct answer emerges as the most frequent one.\nMotivated by recent evidence that LLMs possess the ability to judge the correctness of their own outputs (Kadavath et al., 2022; Zhang et al., 2024), we hypothesize that self-consistency could be made significantly more efficient if the model could review each generated reasoning path before selecting a final answer. We therefore introduce Confidence-Informed Self-Consistency (CISC), a lightweight extension of self-consistency. As illustrated in Figure 2, CISC uses the model to generate a self-assessment score for each path and employs these scores in a weighted majority vote.\nWe conducted a comprehensive comparison of"}, {"title": "2 Notations", "content": "We consider an auto-regressive language model M with parameters $\\theta$. We use $p_{\\theta}(x)$ to denote M's distribution over the next token given the provided context x. Given a question q (e.g., \u201cJane had 4 apples and ate half of her apples. How many apples she has now?\"), we denote the model's response as (r, a), where a is the answer (e.g., \u201c2\u201d) and r is a reasoning path (or chain-of-thought), a sequence of logical steps supposedly leading up to this answer (e.g., \"If Jane ate half her apples, this means she ate 2 apples. 4 minus 2 is 2.\")."}, {"title": "3 Confidence-Informed Self-Consistency", "content": "In this section we present Confidence-Informed Self-Consistency (CISC). When designing CISC, we hypothesized that it is possible to reduce self-consistency's computational costs by generating a confidence score for each reasoning path, and performing a weighted majority vote.\nAs an intuitive example, consider a hypothetical setting where there exist only two possible answers, one correct and one incorrect. For a model that responds with the correct answer 60% of the time, standard majority voting will require 40 samples to reach 90% accuracy\u00b9. However, a weighted majority vote that weights correct answers twice as much as incorrect ones, will achieve 90% accuracy with less than 10 samples.\nWith this motivation in mind, we build on recent findings suggesting that LLMs are capable of judging the correctness of their own outputs (Kadavath et al., 2022; Tian et al., 2023b; Zhang et al., 2024), and incorporate the model's self-assessment of its reasoning paths into the final answer selection:\nDefinition 3.1 (Confidence-Informed Self-Consistency). Given a question q and responses {(r1,a1), ..., (rm,am)}, CISC involves:\n\u2022 Confidence Extraction: A self-assessed confidence score $c_i \\in \\mathbb{R}$ is derived for each (ri, ai).\n\u2022 Confidence Normalization: The confidence scores are normalized using Softmax: $\\tilde{c_i} = \\frac{exp(\\frac{c_i}{T})}{\\sum_j exp(\\frac{c_j}{T})}$, where T is a tunable temperature hyper-parameter (see discussion below).\n\u2022 Aggregation: The final answer is selected using a confidence-weighted majority vote: $\\hat{a}_{CISC} = arg \\max_a \\sum_{i=1}^m \\mathbb{1}[a_i = a] \\cdot \\tilde{c_i}$.\nThe temperature parameter T controls the relative importance of the answer frequency versus the confidence scores. Namely, as T \u2192 \u221e, the distribution of normalized confidence scores approaches the uniform distribution, and CISC collapses to vanilla self-consistency. Conversely, as T \u2192 0, the softmax normalization approaches the hard maximum function, prioritizing the single response with the highest confidence and disregarding the overall frequency of answers. This may lead CISC to select a different answer than self-consistency (see Figure 2)."}, {"title": "4 Experimental Setup", "content": "We compare CISC and self-consistency across a range of confidence extraction methods (\u00a74.1), reasoning tasks (\u00a74.2) and models (\u00a74.3)."}, {"title": "4.1 Confidence Extraction Methods", "content": "We use the following methods:\n\u2022 Response Probability (Wang et al., 2022): The confidence in a response (r, a) is taken to be the model's (length-normalized) probability of generating (r, a) = (x1,...,xn) given the question:\n$p_{\\theta}(r, a) = [\\Pi_{i=1}^n P_{\\theta}(x_i|x_1...x_{i-1},q)]^{\\frac{1}{n}}$\n\u2022 Verbal Confidence (Lin et al., 2022): After sampling (r, a) from the model, we prompt it to rate its confidence in its previously generated output. We implement two variants: (1) Verbal Binary instructs the model to output either 0 or 1, and (2) Verbal 0-100 instructs the model to output a score on a scale of 0-100.\n\u2022 P(True) Kadavath et al. (2022): We prompt the model to rate its confidence in (r, a) in binary format (either 0 or 1), and compute the probability that the model assigns to the token 1.\nEfficient and Consistent Confidence Prompting.\nOur implementation of the prompt-based methods employs a two-step prompting procedure (as depicted in Figure 2). Given a question prompt q, we first use the model to generate the reasoning chain and answer (r, a). We then concatenate a confidence extraction prompt e (e.g., \u201cNow I will rate my confidence...\"), and continue the generation on (q, r, a, e). This serves two important purposes. First, it ensures that when comparing self-consistency and CISC, the reasoning chains are identical. Second, the fact that the prefix (q, r, a) remains unchanged after concatenating the confidence extraction prompt e means it does not require reprocessing by the LLM. Consequently, the additional cost of the confidence extraction step consists only of encoding len(e) \u2248 20 tokens and generating a single token. Since a single (q, r, a) typically contains hundreds of tokens, the confidence extraction step adds only a negligible computational overhead to self-consistency. Further overhead reduction can be achieved through prompt optimization or by using the single-step procedure described in Appendix B. The precise prompts used and additional technical details are also provided in Appendix B.\""}, {"title": "4.2 Datasets", "content": "We used four large reasoning benchmarks:\n\u2022 GSM8K (Cobbe et al., 2021a): A dataset of grade-school level math word problems. We evaluate on the entire validation set (1320 questions).\n\u2022 MATH (Hendrycks et al., 2021): A more challenging dataset of math word problems. We used the entire test set (5K questions).\n\u2022 MMLU-Pro (Wang et al., 2024c): A more challenging version of the Multitask Language Understanding (MMLU) benchmark, testing language models' general knowledge and reasoning abilities with a wide range of topics such as science and history. We randomly sampled 5K questions.\n\u2022 Big-Bench-Hard (Suzgun et al., 2022): A challenging selection of tasks from the big-bench benchmark (bench authors, 2023), comprises a variety of reasoning tasks that pose challenges to LLMs, such as counting objects. We selected 20 out of 23 tasks (5,761 examples), eliminating three sub-tasks that required designated answer extraction methods."}, {"title": "4.3 Models", "content": "We use nine instruction-tuned open-weights LLMs from 3 different families:\n\u2022 GEMMA2 (Team et al., 2024): A Google AI model family, including 2B, 9B, and 27B parameter models.\n\u2022 QWEN2.5 (Yang et al., 2024): A model family from Alibaba AI, with 7 models ranging from 0.5B to 72B parameters. We selected three models: 3B, 14B, and 72B.\n\u2022 Mistral (Mistral-AI, 2024): We used three of the latest models available Ministral-8B-Instruct-2410, Mistral-Small-Instruct-2409, mistralai/Mistral-Large-Instruct-2411 - with 8B, 22B, 123B parameters respectively."}, {"title": "4.4 Metrics", "content": "We compare CISC against self-consistency using the following metrics:\n\u2022 % Cost Reduction: The percentage of computational cost saved by using CISC. We fix the compute budget for CISC (5 or 10 model responses) and measure the number of responses required for self-consistency to achieve equivalent accuracy:\n$100 \\times (1 - \\frac{CISC \\ budget}{\\# \\ Comparable \\ SC \\ responses})$\n\u2022 % Accuracy Improvement: The relative accuracy gain of CISC over self-consistency when both methods utilize the same number of responses per question:\n$100 \\times (\\frac{CISC \\ Acc}{SC \\ Acc} - 1)$"}, {"title": "4.5 Temperature Scaling", "content": "As discussed in \u00a73, CISC re-scales the confidence values using a softmax transformation, parameterized by a temperature T > 0. We tune the temperature separately for each model and confidence extraction method using a 10% held-out set, aggregated across all four datasets (\u00a74.2). More details and the optimal temperature values for each configuration are in appendix D."}, {"title": "4.6 Bootstrap", "content": "To compute the performance of a decoding strategy s (either self-consistency or a variant of CISC) with a sample budget of b\u2208 [1,..., 30], we perform bootstrap sampling. We first sample 30 different reasoning paths from the model. Next, we draw n = 500 sets of b paths for each question, apply s to each set, and compute the accuracy per set. We then average the results across all bootstrap samples to obtain the final score."}, {"title": "5 Main Results", "content": "This section demonstrates CISC's (\u00a73.1) substantial performance advantage over self-consistency. We compare CISC, using fixed compute budgets of 5 and 10 responses per question, based on the metrics defined in \u00a74.4.\nCISC outperforms self-consistency across virtually all models and datasets. Table 1 presents the Cost Reduction and Accuracy Improvement (see \u00a74.4) achieved by CISC with each confidence method. The results are macro-averaged across all models and datasets. CISC outperforms self-consistency with every confidence method."}, {"title": "6 Within-Question Confidence Evaluation", "content": "Recent work demonstrated that verbal confidence methods significantly outperform P(True) in terms of calibration (Tyen et al., 2023), which is the de-facto approach to evaluate the quality of confidence measures. Yet, perhaps surprisingly, CISC is more effective with P(True) than with verbal confidence methods (Table 1). In this section we settle these differences, and explain why well-calibrated confidence measures can still be less useful for CISC.\nWe argue that existing evaluation metrics, whether calibration based (Kadavath et al., 2022; Tian et al., 2023b) or discrimination based (Kuhn et al., 2023; Nguyen et al., 2024) examine the confidence behavior between the input questions. However, for CISC to work well, we want the confidence scores to be able to distinguish correct and incorrect responses to the same question.\nTo gain an intuition for the difference between within-question and between-question confidence evaluation, consider the following simple example. Imagine a model M and a dataset with two types of questions: questions that M finds \"easy\" (e.g., answers correctly 95% of the time) and questions that M finds \"hard\" (e.g., answers correctly 5% of the time). Consider a confidence measure that assigns every answer to an \"easy\" question a confidence of 0.95 and every answer to a hard question a confidence of 0.05. This confidence signal is useless for CISC, as it does not make any distinctions between answers to the same question. On the other hand, it scores well under existing metrics (e.g., it is perfectly calibrated).\nThe above thought experiment shows that the fact that well-calibrated confidence scores can be derived from a model does not necessarily imply the model possesses a capacity to self-assess its own responses. To isolate this specific ability, we design a metric that measures whether the confidence scores can distinguish correct and incorrect responses to the same question:\nDefinition 6.1 (Within-Question Discrimination). Given a dataset of questions, for each question q, denote the sampled responses by $R_q = \\{(r_i,a_i)\\}_{i=1}^m$, and let $R_q^+, R_q^- \\subseteq R_q$ be the subsets of correct and incorrect responses respectively. We evaluate the Within-Question Discrimination (WQD) of a confidence method c : (r, a) \u2192 \u211d as:\n$WQD(c) = \\frac{1}{N} \\sum_q \\sum_{(r,a)\\in R_q^+} \\sum_{(r',a')\\in R_q^-} \\mathbb{1}[c(r, a) > c(r', a')]$\nwhere $N = \\sum_q |R_q^+|\\cdot|R_q^-|$.\nThat is, we compute the fraction of cases where the higher confidence response is indeed the correct response, out of pairs of responses to the same question (exactly one of which is correct). In our work, we use m = 30 (as described in \u00a74.6).\nTo emphasize the importance of within-question evaluation, we test if WQD is more predictive of CISC's success than standard between-question confidence metrics. We compare each confidence method from \u00a74.1 in terms of: (i) standard metrics, such as ECE (Guo et al., 2017) and Brier Score (Brier, 1950), (ii) WQD, (iii) CISC performance at a budget of 10 samples. We follow previous work (Tyen et al., 2023) and report the standard metrics after applying temperature scaling (Ovadia et al., 2019), a technique that fits a single temperature parameter T to the model's confidences to minimize the negative log-likelihood on the data. We use ECE-t and Brier-t to denote the scaled scores.\nThe results of this comparison, averaged across all datasets (\u00a74.2) and models (\u00a74.3), are summarized in Table 3. Indeed, we see that the verbal confidence methods obtain the best ECE-t and Brier-t scores while also achieving the worst performance in CISC. On the other hand, the WQD metric is able to perfectly predict the relative scores of each confidence method in CISC. This emphasizes the limitations of relying solely on traditional confidence evaluation methods for evaluating the models ability to self-assess its reasoning.\nThe WQD metric prioritizes interpretability, focusing on the discrimination ability of the confidence scores irrespective of the relative magnitude"}, {"title": "7 Qualitative Analysis", "content": "In \u00a75 we showed that CISC has clear performance advantages over standard self-consistency, and argued that this suggests LLMs are capable of self-assessing their confidence in responses to the same question. To facilitate a better understanding of this phenomenon, we asked human evaluators to identify indicators of low-quality model responses (i.e., logical patterns that reduced the evaluators' confidence in the correctness of the LLM response). Our analysis revealed a strong correlation between the prevalence of these indicators and lower confidence scores assigned by the LLM.\nSampling Process. We performed the analysis on MMLU-Pro (\u00a74.2), using three representative models, one from each model family.\nTo reduce the evaluation burden we limited it to three LLM responses per question. We selected these triplets based on two criteria: (1) CISC and SC produced different results, where one method yielded a correct answer and the other did not, and (2) the final answers of the three responses were not all distinct, which would otherwise degenerate self-consistency's majority voting.\nOut of the remaining triplets, we randomly chose 45 for which SC was correct and 45 where SC was wrong. Then, for each triplet, we randomly took either the response with highest relative-confidence or the response with lowest relative-confidence. This ensured an equal number of low relative-confidence responses that were correct and incorrect, mitigating potential bias of answer correctness on our analysis. The process resulted in 90 responses for human evaluation.\nHuman Evaluation. Two human evaluators (NLP Phd students), unaware of both the model's confidence scores and the ground truth labels, reviewed 90 samples. The evaluators' task was to identify logical patterns in the LLM reasoning-chain which reduce their confidence that the LLM has reached a correct answer; we call these patterns low-quality-indicators. Also, the evaluators were asked to briefly describe each identified pattern.\nResults. Our evaluation demonstrated a significant correlation in confidence assessments: 67% of the samples assessed as relative-low confidence by the model were also judged to contain low-quality indicators by human evaluators, while only 33% of the samples assessed as relative-high confidence by the model contained the human identified low-quality-indicators. This strong correlation suggests that LLMs are adept at assessing their own reasoning processes and identifying patterns that humans consider indicative of low quality.\nIn addition, we categorized these low-quality indicators. Three primary categories emerged: (1) the LLM's final answer was not among the provided options; (2) the LLM deliberated between multiple options; and (3) the LLM omitted necessary calculations. Of these, only categories (1) and (3) showed a strong correlation with the LLM's low-confidence scores. Further details regarding these categories and their correlation statistics are available in the Appendix E."}, {"title": "8 Related Work", "content": "Confidence signals for LLMs. There is a long line of work on deriving confidence measures from LLMs. Popular approaches use the agreement across multiple samples (Kuhn et al., 2023; Manakul et al., 2023; Tian et al., 2023a; Lyu et al., 2024), the model's internal representations (Azaria and Mitchell, 2023; Burns et al., 2022) or directly prompting the model to verbalize its confidence (Tian et al., 2023b; Kadavath et al., 2022). All papers in this line of work focused on fact-seeking tasks, so confidence is typically derived based on the final answer alone. To the best of our knowledge, our work is the first to apply these approaches to scoring the entire reasoning path.\nReasoning verification. While learned verifiers have been demonstrated to significantly improve performance on math word problems (Cobbe et al., 2021b; Lightman et al., 2023; Li et al., 2022), the ability of LLMs to perform self-verification and self-correction is still heavily contested, with some works providing positive evidence for such capabilities (Weng et al., 2022; Gero et al., 2023; Madaan et al., 2024; Liu et al., 2024; Li et al., 2024a) and others arguing that the gains can mostly be attributed to clever prompt design, unfair baselines, data contamination and using overly simple tasks (Tyen et al., 2023; Valmeekam et al., 2023; Hong et al., 2023; Huang et al., 2023; Stechly et al., 2024; Zhang et al., 2024). This work contributes to this ongoing discussion by presenting multiple lines of evidence supporting LLM self-verification. In particular, we demonstrate clear benefits from a simple confidence-based self-verification approach.\nImproving self-consistency's efficiency. Numerous attempts (Chen et al., 2024) have been made to reduce SC computational overhead while maintaining quality. However, none have matched the widespread adoption of self-consistency. This can be largely attributed to several limitations: (1) a trade-off where throughput is reduced while latency increases, for example by sampling chains sequentially until reaching a certain condition (Li et al., 2024b) or running expensive LLM calls instead of the cheap majority voting (Yoran et al., 2023), (2) the need for manual feature crafting and tuning tailored to each dataset (Wan et al., 2024), (3) promising results on specialized setups (Wang et al., 2024a) which did not generalize to standard benchmarks (Table 9), and (4) as highlighted by Huang et al. (2023), many of the more sophisticated methods that appear promising actually don't outperform self-consistency when evaluated with a thorough analysis of inference costs. Our approach is different in that CISC adds minimal complexity to self-consistency, and improves throughput without compromising latency.\nSelf-consistency with confidence. Related approaches to CISC's confidence-weighted majority vote were previously explored in both the original self-consistency paper Wang et al. (2022), that considered a weighted majority using Sequence Probability (\u00a74.4), and in Miao et al. (2023), that concluded that verbally \u201casking the LLM to check its own reasoning is largely ineffective\" for improving self-consistency. In both cases, these failures are attributed to the confidence scores being too similar to one another. Our work shows that despite this, the scores contain a useful signal (reflected in the WQD scores; Table 3) that can be utilized by a normalization step prior to aggregation to significantly improve the efficiency of self-consistency. Furthermore, the P(True) method, which achieves the highest WQD scores, has not been previously used for attempting to improve self-consistency."}, {"title": "9 Discussion", "content": "In this work we introduced CISC, a lightweight extension of self-consistency. Across diverse models, datasets, and confidence extraction methods, CISC consistently outperformed self-consistency, reducing computation costs by over 40% on average.\nThe performance gains achieved by using model-derived confidence scores provide a practical evidence that LLMs can effectively judge the quality of their own outputs, contributing to the ongoing debate on this topic (Huang et al., 2023; Li et al., 2024a). This is further strengthened by our qualitative evaluation, revealing significant agreement between model confidence and human assessments of response quality.\nComplementing our investigation of LLM self-assessment, we address the crucial aspect of evaluating confidence methods. Traditional calibration metrics, which assess confidence across different questions, fail to capture a model's ability to distinguish between high and low quality responses to the same question. To overcome this, we introduce the Within-Question Discrimination (WQD) metric and demonstrate its effectiveness.\nWe encourage future research to explore the integration of model self-confidence into more sophisticated reasoning frameworks like Tree of Thoughts (Yao et al., 2024) or Graph of Thoughts (Besta et al., 2024), believing that harnessing this inherent capability can further boost performance. Another promising avenue is training models to produce more accurate intrinsic or verbal confidence (Lin et al., 2022; Chaudhry et al., 2024), which would directly improve CISC and related methods. Conversely, CISC and WQD can be used to assess the impact of advancements in confidence generation."}, {"title": "10 Limitations", "content": "Confidence Prompting. Our confidence extraction prompting approach minimizes the computational overhead (\u00a74.1) by using short confidence prompts (less than 5% of the input and reasoning chain length) that, unlike other works, are appended after the reasoning chain. This allows us to continue to use the auto-regressive cache that was used when the models generated the answer. While this approach is readily implementable within frameworks like HuggingFace (Hugging-Face, 2024a), it may not be universally supported. An alternative one-step prompting approach, which does not rely on prefix caching, is discussed in Appendix B. We opted for the two-step approach in this study to ensure a clear and robust evaluation of CISC, fully mitigating the impact of confidence integration on the generated reasoning paths.\nAccess to the model's probabilities. The preferred CISC approach calculates P(True) (as described in \u00a74.1) by examining the model's assigned probability to the verbal confidence token. This method is available in both popular open-weight frameworks (e.g., Hugging-Face (2024a)) and closed-weight frameworks (e.g., OpenAI (2025)). However, this feature may not be universally available across all frameworks.\nHuman Evaluation. The qualitative human evaluation presented in Section 7 provides further support for our claims regarding LLMs' ability to self-assess the correctness of their responses. This evaluation was conducted on the MMLU dataset, which offers a diverse set of single-choice questions. Extending this analysis to other datasets could offer additional insights.\nAdditional ablations. We examined the performance of CISC across several key aspects, focusing on the impact of the choice of confidence extraction method and the impact of the confidence normalization step. Additional ablations could include examining the effect of zero-shot vs few-shot prompting, different choices of normalization techniques, and using trainable confidence methods (Lin et al., 2022; Chaudhry et al., 2024) to improve the performance of CISC."}, {"title": "11 Ethics Statement", "content": "This work improves LLM reasoning efficiency by introducing a new decoding strategy (CISC). While CISC itself introduces no new ethical issues, LLMs can perpetuate biases and have societal impacts. Responsible LLM development and deployment, including bias mitigation, are crucial."}, {"title": "A Quantitative example from \u00a73", "content": "Consider a simplified binary setting in which there are two possible answers: correct and incorrect. Given a number of samples n and a probability p = 0.6 of generating the correct answer, the number of samples with the correct answer follows the Binomial distribution X ~ Binomial(n,p). For such distribution, the majority vote is accurate whenever X >$\\frac{n}{2}$ and it has 50% chance to be accurate when X = $\\frac{n}{2}$ (i.e., a random choice).\nNow, to illustrate how the self-assessment score of LLMs can be helpful, consider that we have an oracle that assigns twice the weight for answers that are correct. In this case, a weighted majority vote would be accurate whenever X >$\\frac{n}{3}$ and it has 50% chance to be accurate when X = $\\frac{n}{3}$.\nIn Figure 5 we plot the relationship between, (x-axis) the number of samples, and (y-axis) the accuracy of the weighted majority vote over these samples. The graph features two lines: (blue) each sample gets an equal weight, and (orange) correct answers are assigned twice the weight of incorrect ones.\nWhile this intuition about cost-saving also applies to the general case of an arbitrary set of answers, this setting is trickier to analyze in closed-form because the specific distribution of incorrect answers impacts the majority vote. E.g., an answer that appears only 20% of the time can still be correct under majority vote if all the other 80% incorrect answers are different from one another. This could be obtained by placing additional distributional assumptions on the sampled answers. The analysis of the binary case can be thought of as a worst-case analysis of the general case, since in the worst case, all the incorrect answers are identical and the majority will be accurate if and only if more than half the sampled answers are correct."}, {"title": "B Prompting Techniques", "content": "As described in Section 4.1, for our prompt based confidence extraction techniques (Verbal Confidence, P(True)), we used a two-step approach: First, we prompted the model to answer benchmark questions using the prompts shown in Table 4. Then, we extracted confidence by concatenating the prompts shown in Table 5 and running the model again. This two-step process allowed using the same answers when comparing self-consistency and CISC.\nWhile a simpler single-step implementation (outputting both answer and confidence in a single response) is possible, we did not explore it in this study. For research purposes, we prioritized a clean setup that ensured requesting confidence scores did not influence the generated answers and chain-of-thoughts.\nAs shown in Table 5, all the confidence extraction prompts that we used are extremely lightweight. We deliberately avoided methods that significantly increase the number of generated tokens like generating k guesses with associated probabilities (Tian et al., 2023b).\nFor the P(True) method, we modified the prompts from Kadavath et al. (2022) in two ways: (1) We changed the format to allow concatenation after the model provided its answer, ensuring that prefix caching could be re-used between the two steps. (2) We changed the prompt format to 0/1 instead of True/False, as some benchmarks are using True/False as ground truth labels, and we observed that it might confuse the model when extracting confidence scores."}, {"title": "C Additional Results", "content": "For each confidence method, Table 1 shows the macro-average results across all models and"}]}