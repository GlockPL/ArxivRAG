{"title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "authors": ["Jessica Echterhoff", "Fartash Faghri", "Raviteja Vemulapalli", "Ting-Yao Hu", "Chun-Liang Li", "Oncel Tuzel", "Hadi Pouransari"], "abstract": "Large Language Models (LLMs) are frequently\nupdated due to data or architecture changes to\nimprove their performance. When updating\nmodels, developers often focus on increasing\noverall performance metrics with less empha-\nsis on being compatible with previous model\nversions. However, users often build a men-\ntal model (Bansal et al., 2019) of the func-\ntionality and capabilities of a particular ML\nmodel they are interacting with. They have\nto adapt their mental model with every up-\ndate - a draining task that can lead to user\ndissatisfaction. In practice, fine-tuned down-\nstream task adapters rely on pretrained LLM\nbase models. When these base models are up-\ndated, these user-facing downstream task mod-\nels experience instance regression or negative\nflips -previously correct instances are now pre-\ndicted incorrectly. This happens even when the\ndownstream task training procedures remain\nidentical. Our work aims to provide seamless\nmodel updates to a user in two ways. First,\nwe provide evaluation metrics for a notion of\ncompatibility to prior model versions, specif-\nically for generative tasks but also applicable\nfor discriminative tasks. We observe regression\nand inconsistencies between different model\nversions on a diverse set of tasks and model up-\ndates. Second, we propose a training strategy\nto minimize the number of inconsistencies in\nmodel updates, involving training of a compat-\nibility model that can enhance task fine-tuned\nlanguage models. We reduce negative flips\ninstances where a prior model version was cor-\nrect, but a new model incorrect \u2013 by up to 40%\nfrom Llama 1 to Llama 2.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are often pre-\ntrained on large-scale corpora to obtain a base\nmodel that has general world knowledge. These\nbase models are typically evaluated using a suite\nof benchmarks that mostly focus on zero/few-shot\nperformance and in-context learning capabilities.\nTraining these models is expensive and only a few\norganizations have access to the resources needed\nfor this. Hence, to enable various user-facing ap-\nplications such as summarization, chatbots, code\nassistants, and question answering, practitioners\noften adapt pretrained base models by training task-\nspecific parameter-efficient adapters using down-\nstream task datasets.\nSeveral scenarios drive updates to the base\nmodel. For example, improved training strategy,\nadvances in LLM architectures or increasing model\ncontext length (Touvron et al., 2023a), the avail-\nability of additional or higher quality datasets (Gu-\nnasekar et al., 2023), the expansion of model vocab-\nulary (e.g., to support multilingual or multimodal\nmodels), or simply training for a longer period\n(Biderman et al., 2023). When a base model is\nupdated, the associated task adapters need to be\nretrained to have meaningful downstream mod-\nels. Hence, in the rest of the paper, we use the\nterm model update to refer to an update to a down-\nstream task model, which includes updating the\nbase model and retraining the task adapter.\nWhen a model is updated, each data sample can\nbe categorized into one of the four groups depicted\nby four quadrants shown in Fig. 2. The new model\ncould result in a negative flip (quadrant 4) for many\nsamples (Yan et al., 2021) even when it has a better\noverall performance when compared to the pre-\nvious model. A real example of a negative flip\ncaused by a model update is shown in Fig. 1 for\ndiagloug summarizatoin task. This regression be-\nhavior could confuse the user driving down the\nuser satisfaction (Sakai, 2022). Furthermore, the\nimportance of regression testing has been recently\nstudied for evolving LLMs used through APIs (Ma\net al., 2023).\nMany existing works focus on increasing posi-\ntive flips (quadrant 2) while reducing negative flips\n(quadrant 4) (Cai et al., 2022; Sakai, 2022; Yan\net al., 2021; Li et al., 2023b; Schumann et al., 2023).\nHowever, yhey neglect prediction inconsistencies\nin the scenarios where both model versions are in-\ncorrect (quadrant 3). For example, Tr\u00e4uble et al.\n(2021) assumes the cost of flips from one incorrect\nclass to another incorrect class to be zero. We argue\nthat there is value in being consistent when both\nmodels are incorrect. A user may have developed\ncoping strategies on how to interact with a model\nwhen it is incorrect; therefore, inconsistencies in\nmistakes can lead to user dissatisfaction.\nPrevious works have mainly addressed the re-\ngression challenge for classification tasks (Cai\net al., 2022; Sakai, 2022; Yan et al., 2021; Li et al.,\n2023b; Schumann et al., 2023). In this work, we\nsystematically study the problem of downstream\nmodel compatibility (reducing negative flips and\nbeing consistent when making mistakes) using var-\nious downstream tasks (both discriminative and\ngenerative) and different base models.\nFollowing is a summary of our contributions:"}, {"title": "2 Related Work", "content": "Classification Yan et al. (2021) introduce the\nnegative flip rate (NFR) to evaluate model com-\npatibility for classification tasks. NFR calculates\nthe fraction of instances that were previously cor-\nrect but are now incorrect with the new model.\nOther metrics include backward trust compatibil-\nity (BTC) (Srivastava et al., 2020), the ratio of\ninstances that the new model predicts correctly\namong all instances the old model predicts cor-\nrectly. Ultimately, these metrics have been shown\nto measure similar statistics (Sakai, 2022). Mat-\nsuno and Sakuma (2023) propose backward com-\npatibility with a conditional distribution, which\ncomputes the ratio at which the accuracy of the\nconditional distribution of the new model is equal\nto or higher than that of the old model. Cai et al.\n(2022) introduce the negative flip impact for graph\nNLP tasks, taking into account the negative flips\nand the overall error rate of the model. All of these\nmetrics are limited to the evaliation of discrimina-\ntive classification tasks.\nModel Ensembles Prior work found that model\nensembles reduce model update regression. This\ncan be attributed to the reduction of variance by\nensembling, as every single model may capture the\ntraining data from a distinct aspect (Yan et al., 2021;\nXie et al., 2021). Other approaches include choos-\ning the most centric model from the ensemble (Xie\net al., 2021), aligning two models' uncertainties\n(Li et al., 2023b), or using gating mechanisms (Lai\net al., 2023). To ensure model compatibility, pre-\nvious work has also used model parts from the old\nmodel to infuse into the new one (Ran et al., 2023),\nwith the limitation of both models being required\nat inference time. For limited use cases, Qin et al.\n(2023) have shown to re-use previously learned\nadapters when purely a data update was performed.\nThese methods either introduce larger memory foot-\nprint by re-using old model parts, or are limited to\n(same-domain) data-updates and same models only.\nKnowledge Distillation Originally proposed for\nmodel compression (Bucilu\u0103 et al., 2006), a\n(smaller) student model is trained to mimic a\n(larger) teacher model. By treating the old model\nas the teacher and the new model as the student,\nknowledge distillation has been shown to reduce\nmodel update regression in vision and language dis-\ncriminative tasks (Yan et al., 2021; Xie et al., 2021;\nSchumann et al., 2023; Jaeckle et al., 2023; Zhang\net al., 2021; Shen et al., 2020; Ramanujan et al.,\n2022). Shen et al. (2020) propose a distillation-\nbased influence loss to align new model represen-"}, {"title": "3 Problem Formulation", "content": "Setup Training LLMs from scratch for every\ndownstream task is expensive. Hence, we fol-\nlow the common setup of finetuning a pre-trained\nbase LLM to multiple downstream tasks with task-\nspecific LoRA adapters (Hu et al., 2021).\nLet \\(M^{base}_v\\) denote the \\(i^{th}\\) version of the base\nLLM with parameters \\(\\theta_v\\). We adapt \\(M^{base}_v\\) to a\ndownstream task T using an adapter \\(A_T\\) to obtain\na downstream model \\(M_v\\) with weights \\(\\theta^T_v = \\theta_v + \n\\Delta_v^T\\), where \\(\\Delta_v^T\\) denotes the weights of the task-\nspecific adapter \\(A_T\\) learned using the training data\ncorresponding to task T. When the base model\nis updated from \\(M^{base}_v\\) to \\(M^{base}_{v'}\\), the task-specific\nadapters are re-trained for each downstream task.\nHereafter, for simplicity of notation, we use \\(M_{v1}\\)\nand \\(M_{v2}\\) to refer to the task-adapted models \\(M_1\\)\nand \\(M_2\\), respectively, and explicitly mention the\ntask T when needed.\nA backward compatibility metric outputs a compat-\nibility score based on two models, \\(M_{v1}\\) to \\(M_{v2}\\).\nYan et al. (2021) propose negative flip rate (NFR)\nto measure regression in classification models over\na dataset \\(\\left\\{x_i, y_i\\right\\}_{i=1}^N\\), where \\(y_i\\) is the ground truth\nclass for input \\(x_i\\) for a particular task T:\n\\(NF(x_i) \\triangleq [M_{v1}(x_i) = y_i] > [M_{v2}(x_i) \\neq y_i]\\)\n\\[NFR = \\frac{1}{N} \\sum_{i=1}^{N} 1[NF(x_i)]\\]\nHere 1 denotes the indicator function. This no-\ntion of regression is partly applicable for autore-\ngressively trained tasks. LLM benchmarks (Gao\net al., 2023) calculate the likelihood of every pos-"}, {"title": "3.2 Extended Evaluation Metrics", "content": "To extend the possibility of evaluating model up-\ndates, we propose a suite of metrics that evaluate\nmodel updates specifically for generative tasks that\nmay or may not have multiple options available\n(e.g summarization tasks), as well as extend mea-\nsurement of inconsistent flips.\nTo capture inconsistencies for instances where both\nold and new models are incorrect, we adapt the\nnegative flip rate as follows when multiple choice\noptions are available:\n\\(NF_{mc}(x_i) \\triangleq [M_{v2}(x_i) \\neq y_i] \\wedge [M_{v1}(x_i) \\neq M_{v2}(x_i)]\\)\n\\[NFR_{mc} = \\frac{1}{N} \\sum_{i=1}^{N} 1[NF_{mc}(x_i)]\\]\nwhich includes the possibility that neither of the\nmodels give the correct answer, but a change in\nbehavior occurs. Similarly, for multi-label tasks\nthis notion can account for ground-truths that may\nhave flipped during a model update when multiple\ngrount truth options exist.\nTo add fine-\ngrained and continuous metrics for generative tasks\nlike summarization, we evaluate the expected per-\nformance gain and regression compared to the pre-\nvious version, to enable a user-centric model update\nin accordance to the four quadrants of Fig. 4. Our\ngeneral framework aims to be independent of the\nactual similarity metric, such that it can be chosen\nin accordance with any respective task of interest.\nGiven a similarity metric S, and model outputs\n\\(M_{v1}(x_i)\\) and \\(M_{v2}(x_i)\\) for an input \\(x_i\\), the dif-\nference for a model update for a particular test\ninstance i is\n\\(D(x_i) = S(M_{v2}(X_i), y_i) \u2013 S(M_{v1}(x_i), y_i)\\)\nacting as an indication of the distance of the two\nmodel outputs with respect to the grand truth. This\ngain per instance enables us to classify which\nmodel update compatibility quadrant each instance\nfalls into. In practice, similarity metrics could be\nBERT Score (Zhang et al., 2019), ROUGE Score\n(Lin, 2004), BLEU Score (Papineni et al., 2002),\nor model-as-a-judge metrics (Huang et al., 2024)\ndepending on the use case and task."}, {"title": "4 Knowledge Transfer", "content": "To minimize the regression during model updates,\nwe propose a knowledge distillation approach ap-\nplied on the task-specific models \\(M_{v1}\\) and \\(M_{v2}\\).\nTypically, knowledge distillation minimizes the\nKL divergence between the soft targets \\(\\sigma(z^s_i)\\) and\n\\(\\sigma(z^t_i)\\), where \\(z_s\\) and \\(z_t\\) are logits predicted by stu-\ndent and teacher models, respectively., i denotes\nthe i'th token, n is the total number of tokens avail-\nable for training, T is temperature parameter, and\n\\\u03c3 denotes softmax.\n\\[L_{KL} = \\frac{1}{n} \\sum_{i=1}^{n}KL(\\sigma(z_{t,i}/T) || \\sigma(z_{s,i}/T))\\]\nMost knowledge distillation works consider the\ndistillation from a trained teacher to an untrained\nstudent (Tian et al., 2022; Rajasegaran et al., 2020).\nRecent work (Roth et al., 2024) tackles the goal of\nknowledge transfer between pretrained student and\nteacher models while retaining student knowledge\ngained a priori, and show that standard knowledge\ndistillation between pretrained models struggles to\ntransfer knowledge without performance drops for\nmany teacher-student pairs. Complementary to this\nwork that focuses on performance and maintain-\ning prior knowledge, we tackle compatibility to\nprevious models through knowledge transfer."}, {"title": "4.1 Model Update Strategy for Compatible LLM Evolution (MUSCLE)", "content": "When the base model is updated, we aim to train a\ntask-specific fine-tuned model, \\(M_{v2}\\), that has the\naccuracy benefits of \\(M_{v2}\\), but with most compat-\nibility with \\(M_{v1}\\). We obtain \\(M_{v2}^c\\) by training a\ncompatibility adapter applied to the base model\n\\(M^{base}_{v2}\\). We use knowledge from task-specific\nfine-tuned models \\(M_{v1}\\) and \\(M_{v2}\\) when training \\(M_{v2}^c\\).\n\\(M_{v2}\\) typically has increased prediction capabili-\nties over \\(M_{v1}\\) (due to improvements in the base\nmodel), but \\(M_{v1}\\) has information on already cor-\nrectly predicted tokens or instances that we want to\nminimize regression towards.\nWe initialize the compatibility adapter with the\ntask-specific adapter of \\(M_{v2}\\), and further fine-tune\nit (using the task training dataset) by aligning the\nnext token prediction to either \\(M_{v1}\\) or \\(M_{v2}\\). We\ndefine a masking (for individual tokens of a training\nsequence) following a simple heuristic depending\non whether \\(M_{v2}\\) (the adapter being trained) pre-\ndicts the correct tokens or not. If it does, we align to\n\\(M_{v2}\\) logits, otherwise, we align to \\(M_{v1}\\). The fine-\ntuning process of \\(M_{v2}^c\\) is depicted in Fig. 3. The\nfine-tuning loss to train the compatibility adapter,\n\\(L_{comp}\\), is defined below:\n\\(m_i = 1[argmax \\sigma(z^i_{Mv2}) \\neq y_i]\\) (1)\n\\[\\alpha_{Mv_1} =KL(\\sigma(z_{M_{v1},i}/T) || \\sigma(z_{M^c_{v2},i}/T))\\]\n\\[\\alpha_{Mv_2} =KL(\\sigma(z_{M_{v2},i}/T) || \\sigma(z_{M^c_{v2},i}/T))\\]\n\\[L_{comp} = \\frac{1}{n} \\sum_{i=1}^{n} m_i \\cdot \\alpha_{Mv_1} + (1 \u2013 m_i) \\cdot \\alpha_{Mv_2} (2)\\]\nWhen evaluating, we denote NFR as negative\nflip rate between \\(M_{v1}\\) \\(M_{v2}\\), NFRc as the observed\nnegative flip rate between \\(M_{v1}\\) and our compati-\nbility model \\(M_{v2}^c\\), and \\(\\Delta\\)NFR = NFR \u2013 NFRc."}, {"title": "5 Experimental Setup", "content": "To analyse the impact of model updates we con-\nsider parameter efficient fine-tuned models using\nLow-Rank Adapters (LoRA) (Hu et al., 2021).\nCompared to previous work on continuous learn-\ning and model updates (Qin et al., 2022, 2023),\nwe do not limit model updates to be produced by\nonly data updates, but consider different kinds of\nupdates shown in Table 1. We include updates due"}, {"title": "6 Results", "content": "Fig. 4 shows that significant negative flips (up to\nmore than 60%) exist for a variety of base model\nupdate scenarios and downstream tasks. We ob-\nserve negative flips in updates within one model\nfamily (e.g. Llama/Vicuna, and Phi models). We\nfind more negative flips for model updates with a\nsmaller delta in performance gain. For generative\ntasks like SAMsum dialogue summarization, we\nobserve a large number of negative flips, NFRs,\nas continuous metrics are more sensitive to small\nchanges when updated."}, {"title": "6.2 Reduced Negative Flips in Classification", "content": "Binary and multiple choice generative tasks can\nbe evaluated with classification metrics, such as\ncalculating the log likelihood of each answer op-\ntion (we use Gao et al. (2023) for this use case).\nThis can be interpreted as a \u201cclassical\u201d sense of\nevaluating negative flips. In Table 3, we observe\nthat MUSCLE decreases negative flips as shown\nwhen compared to regular model update without\ncompatibility specific training (\\(M_{v2}\\)). Specifically,\nwe show reduction of NFR by 40% for the update\nof LLama 1 to Llama 2 and 39% for Vicuna 1.3 to\nVicuna 1.5. For updates with large accuracy gap\n(for example Phi 1 to Phi 1.5), we observe a less\nstrong enhancement with a negative flip rate reduc-\ntion by 1-5%. In addition to reduction of negative\nflips we also observe an increased accuracy of up\nto 7% for Llama and Vicuna updates.\nSimilar to multiple-choice tasks, we can evalu-\nate math questions in a classification-like approach\nto determine negative flips. In exact match (EM)\nevaluation we match the final result of the math\nquestion from the prediction to ground truth. Re-\nsults are shown in Table 4. In this case, we observe\nthat we can reduce the number of negative flips by\n29% for Phi 1.5 to Phi 2. When the version 1 model\nis significantly less accurate (e.g., 3.4% for Phi 1),\nwe observe a reduction in accuracy while only be-\ning able to decrease negative flips by 2%. For all\nother updates, MUSCLE increases expected-match"}, {"title": "6.3 Increased Consistent Behavior", "content": "When we cannot achieve a positive flip (switching\nfrom an incorrect to a correct answer), we might\nprefer to at least maintain consistent behavior to old\nmodel to avoid unexpected behavior for the user.\nWe evaluate negative flips and inconsistency flips.\nFor model updates that have a large performance\ngap and a small number of negative flips to be-\ngin with, we see limited reduction in inconsistency\nflips. However, we observe that we can reduce the\ninconsistency flips for model updates with smaller\naccuracy gap such as the updates for Llama and\nVicuna (Fig. 5) on the HellaSwag dataset."}, {"title": "6.4 Reduced Regression in Generative Tasks", "content": "When evaluating generative tasks, such as summa-\nrization, we observe that we can reduce regression"}, {"title": "6.5 The Effect of Different Masking Strategies", "content": "In this section, we analyze different training and\nmasking strategies to evaluate our design choices.\nOn the PIQA dataset and model update Llama 1\n\u2192 Llama 2, we compare MUSCLE with different\nmasking strategies. Intuitively, likelihood based\nmasking strategies can be useful for tasks that are\nevaluated with log-likelihoods (e.g. PIQA, Hel-\nlaSwag) where multiple choices are available. We\ncompare likelihood based masking for example on\nindividual tokens, \\(m_i = LLL = 1[\\sigma(z^i_{M2}) <\n\\sigma(z^i_{Mv1})]\\) to check if the likelihood of the correct\nnext token in the current model is smaller than the\nold model. Only in this case, we align to \\(M_{v1}\\),\nand align to \\(M_{v2}^c\\) for every other token. Alterna-\ntively, we can also compare the likelihood of the\nentire sequence \\(m_i = LLS = 1[\\sum_i [\\sigma(z^i_{M2}) <\n\\sigma(z^i_{Mv1})]]\\), such that we mask all tokens per se-\nquence to get instance-wise masking. In both cases,\nwe see a reduction in negative flips. However,\nlikelihood based masking requires auxiliary cross-"}, {"title": "7 Conclusion", "content": "In this work, we study the task-specific compat-\nibility problem when updating LLMs. We show\nthat LLM updates with different scenarios, e.g.,\nchanges in model architecture, optimization, or\ntraining dataset, exhibit significant negative flips-\ninstances previously classified or generated cor-\nrectly, and incorrectly after the model update. We\nextend negative flip metric for discriminative and, for the first time, generative tasks, and report results\nfor various models and tasks.\nWe propose a novel method (MUSCLE) to train\ntask-specific compatibility adapter when updating\nan old LLM to a new LLM to reduce negative flips\nwhile maintaining performance gain. Our proposed\nmethod does not require a modification to the base\nmodel training, and is only based on adapter train-\ning. Further, as opposed to previous works, the\nproposed solution does not require both versions\nof the model in memory to enable compatibility,\nwhich is often infeasible due to large size of LLMs.\nWe observe a mitigation of negative flips of 40%\nfor multiple-choice type evaluations, and 27% for\ncontinuous summarization evaluation. We also\nshow insights into model properties that facilitate\ntransfer, finding that our alignment masking strat-\negy provides best results with an additional benefit\nof mitigating inconsistent update behavior."}, {"title": "8 Limitations", "content": "In the current study we do not consider a model\nupdate that includes changes in tokenization and/or\nvocabulary size (e.g.Llama 2 (Touvron et al.,\n2023b) to LLama 3 (AI@Meta, 2024)). Future\nwork can explore compatible vocabulary mapping\nstrategies before learning from prior model ver-\nsions. Our results indicate that the utility of align-\ning to prior model versions is limited by the perfor-\nmance of the prior model version. For example, see\nthe update from Phi 1 to Phi 1.5 in Table 4, where\nPhi 1 only has an accuracy of 3%. In this case, it\nis arguable if an alignment to \\(M_{v1}\\) is desired and\nif the strive for compatibility outweighs a possible\nperformance drop."}, {"title": "9 Ethical Considerations and Risks", "content": "One potential risk of the proposed approach for\ncompatible task-specific LLM updates is the trans-\nfer of potential biases from the old model, \\(M_{v1}\\), to\nthe new model trained through knowledge transfer.\nWe did not explore this aspect in the current study."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Training Hyperparameters and Evaluation", "content": "We show an overview of the design choices of\nour LoRA adapter for task and compatibility train-\ning in Table 7. We learn on the entire context and\nanswer tokens during fine-tuning. We find that"}, {"title": "A.2 Model Update Evaluation", "content": "In the evaluation of regression for different model\nupdates (Fig. 4), we consider the model pairs\nshown in Table 8."}, {"title": "A.3 Downstream Tasks", "content": "We describe the datasets used in our experiments.\nEach dataset poses unique challenges and targets\ndifferent aspects of language understanding and\nreasoning."}, {"title": "PIQA", "content": "The Physical Interaction QA (PIQA)\ndataset tests the understanding of physical and\ncausal interactions in the real world through tex-\ntual descriptions. It contains scenarios that require\nreasoning about physical properties, actions, and\noutcomes. Each scenario is presented as a ques-\ntion with two possible solutions, where the model\nmust choose the most physically plausible one. We\nuse this dataset as a representative case for evalu-\nating models on tasks that require an understand-\ning of the physical world and its governing prin-\nciples. We evaluate regression in this task with\nlog-likelihoods for the correct answer, as different\nchoices are given."}]}