{"title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "authors": ["Jessica Echterhoff", "Fartash Faghri", "Raviteja Vemulapalli", "Ting-Yao Hu", "Chun-Liang Li", "Oncel Tuzel", "Hadi Pouransari"], "abstract": "Large Language Models (LLMs) are frequently\nupdated due to data or architecture changes to\nimprove their performance. When updating\nmodels, developers often focus on increasing\noverall performance metrics with less empha-\nsis on being compatible with previous model\nversions. However, users often build a men-\ntal model (Bansal et al., 2019) of the func-\ntionality and capabilities of a particular ML\nmodel they are interacting with. They have\nto adapt their mental model with every up-\ndate - a draining task that can lead to user\ndissatisfaction. In practice, fine-tuned down-\nstream task adapters rely on pretrained LLM\nbase models. When these base models are up-\ndated, these user-facing downstream task mod-\nels experience instance regression or negative\nflips -previously correct instances are now pre-\ndicted incorrectly. This happens even when the\ndownstream task training procedures remain\nidentical. Our work aims to provide seamless\nmodel updates to a user in two ways. First,\nwe provide evaluation metrics for a notion of\ncompatibility to prior model versions, specif-\nically for generative tasks but also applicable\nfor discriminative tasks. We observe regression\nand inconsistencies between different model\nversions on a diverse set of tasks and model up-\ndates. Second, we propose a training strategy\nto minimize the number of inconsistencies in\nmodel updates, involving training of a compat-\nibility model that can enhance task fine-tuned\nlanguage models. We reduce negative flips\ninstances where a prior model version was cor-\nrect, but a new model incorrect \u2013 by up to 40%\nfrom Llama 1 to Llama 2.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are often pre-\ntrained on large-scale corpora to obtain a base\nmodel that has general world knowledge. These\nbase models are typically evaluated using a suite\nof benchmarks that mostly focus on zero/few-shot\nperformance and in-context learning capabilities.\nTraining these models is expensive and only a few\norganizations have access to the resources needed\nfor this. Hence, to enable various user-facing ap-\nplications such as summarization, chatbots, code\nassistants, and question answering, practitioners\noften adapt pretrained base models by training task-\nspecific parameter-efficient adapters using down-\nstream task datasets.\nSeveral scenarios drive updates to the base\nmodel. For example, improved training strategy,"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Measuring Regression Errors", "content": "Classification Yan et al. (2021) introduce the\nnegative flip rate (NFR) to evaluate model com-\npatibility for classification tasks. NFR calculates"}, {"title": "2.2 Reducing Update Regression", "content": "Model Ensembles Prior work found that model\nensembles reduce model update regression. This\ncan be attributed to the reduction of variance by\nensembling, as every single model may capture the\ntraining data from a distinct aspect (Yan et al., 2021;\nXie et al., 2021). Other approaches include choos-\ning the most centric model from the ensemble (Xie\net al., 2021), aligning two models' uncertainties\n(Li et al., 2023b), or using gating mechanisms (Lai\net al., 2023). To ensure model compatibility, pre-\nvious work has also used model parts from the old\nmodel to infuse into the new one (Ran et al., 2023),\nwith the limitation of both models being required\nat inference time. For limited use cases, Qin et al.\n(2023) have shown to re-use previously learned\nadapters when purely a data update was performed.\nThese methods either introduce larger memory foot-\nprint by re-using old model parts, or are limited to\n(same-domain) data-updates and same models only.\nKnowledge Distillation Originally proposed for\nmodel compression (Bucilu\u0103 et al., 2006), a\n(smaller) student model is trained to mimic a\n(larger) teacher model. By treating the old model\nas the teacher and the new model as the student,\nknowledge distillation has been shown to reduce\nmodel update regression in vision and language dis-\ncriminative tasks (Yan et al., 2021; Xie et al., 2021;\nSchumann et al., 2023; Jaeckle et al., 2023; Zhang\net al., 2021; Shen et al., 2020; Ramanujan et al.,\n2022). Shen et al. (2020) propose a distillation-\nbased influence loss to align new model represen-"}, {"title": "3 Problem Formulation", "content": "Setup Training LLMs from scratch for every\ndownstream task is expensive. Hence, we fol-\nlow the common setup of finetuning a pre-trained\nbase LLM to multiple downstream tasks with task-\nspecific LoRA adapters (Hu et al., 2021).\nLet $M^{base}_{v_i}$ denote the $i^{th}$ version of the base\nLLM with parameters $\\theta_{v_i}$. We adapt $M^{base}_{v_i}$ to a\ndownstream task T using an adapter $A^T$ to obtain\na downstream model $M^T_{v_i}$ with weights $\\theta^T_{v_i} = \\theta_{v_i} +$\n$\\Delta^T$, where $A^T$ denotes the weights of the task-\nspecific adapter $A^T$ learned using the training data\ncorresponding to task T. When the base model\nis updated from $M^{base}_{v_i}$ to $M^{base}_{v_j}$, the task-specific\nadapters are re-trained for each downstream task.\nHereafter, for simplicity of notation, we use $M_{v_1}$\nand $M_{v_2}$ to refer to the task-adapted models $M^T_{v_i}$\nand $M^T_{v_j}$, respectively, and explicitly mention the\ntask T when needed."}, {"title": "3.1 Backward Compatibility Metrics", "content": "A backward compatibility metric outputs a compat-\nibility score based on two models, $M_{v_1}$ to $M_{v_2}$.\nYan et al. (2021) propose negative flip rate (NFR)\nto measure regression in classification models over\na dataset $\\{(x_i, y_i)\\}_{i=1}^N$, where $y_i$ is the ground truth\nclass for input $x_i$ for a particular task T:\n$NF(x_i) = \\mathbb{I}[M_{v_1}(x_i) = y_i] \\gt \\mathbb{I}[M_{v_2}(x_i) \\neq y_i]$\n$NFR = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}[NF(x_i)]$\nHere $\\mathbb{I}$ denotes the indicator function. This no-\ntion of regression is partly applicable for autore-\ngressively trained tasks. LLM benchmarks (Gao\net al., 2023) calculate the likelihood of every pos-\nsible choice in a multiple-choice question and\nchoose the response with the highest likelihood."}, {"title": "3.2 Extended Evaluation Metrics", "content": "To extend the possibility of evaluating model up-\ndates, we propose a suite of metrics that evaluate\nmodel updates specifically for generative tasks that\nmay or may not have multiple options available\n(e.g summarization tasks), as well as extend mea-\nsurement of inconsistent flips.\nAccounting for Flips within Gain or Regression\nTo capture inconsistencies for instances where both\nold and new models are incorrect, we adapt the"}, {"title": "4 Knowledge Transfer", "content": "To minimize the regression during model updates,\nwe propose a knowledge distillation approach ap-\nplied on the task-specific models $M_{v_1}$ and $M_{v_2}$.\nTypically, knowledge distillation minimizes the\nKL divergence between the soft targets $\\sigma(z_t)$ and\n$\\sigma(z_s)$, where $z_s$ and $z_t$ are logits predicted by stu-\ndent and teacher models, respectively., i denotes\nthe i'th token, n is the total number of tokens avail-\nable for training, T is temperature parameter, and\n$\\sigma$ denotes softmax.\n$\\mathcal{L}_{KL} = \\sum_{i=1}^n KL(\\sigma(z_{t,i}/T)||\\sigma(z_{s,i}/T))$\nMost knowledge distillation works consider the\ndistillation from a trained teacher to an untrained\nstudent (Tian et al., 2022; Rajasegaran et al., 2020).\nRecent work (Roth et al., 2024) tackles the goal of\nknowledge transfer between pretrained student and\nteacher models while retaining student knowledge\ngained a priori, and show that standard knowledge\ndistillation between pretrained models struggles to\ntransfer knowledge without performance drops for\nmany teacher-student pairs. Complementary to this\nwork that focuses on performance and maintain-\ning prior knowledge, we tackle compatibility to\nprevious models through knowledge transfer."}, {"title": "4.1 Model Update Strategy for Compatible LLM Evolution (MUSCLE)", "content": "When the base model is updated, we aim to train a\ntask-specific fine-tuned model, $M^\\prime_{v_2}$, that has the\naccuracy benefits of $M_{v_2}$, but with most compat-\nibility with $M_{v_1}$. We obtain $M^\\prime_{v_2}$ by training a\ncompatibility adapter applied to the base model\n$M^{base}_{v_2}$. We use knowledge from task-specific fine-\ntuned models $M_{v_1}$ and $M_{v_2}$ when training $M^\\prime_{v_2}$.\n$M^\\prime_{v_2}$ typically has increased prediction capabili-\nties over $M_{v_1}$ (due to improvements in the base\nmodel), but $M_{v_1}$ has information on already cor-\nrectly predicted tokens or instances that we want to\nminimize regression towards.\nWe initialize the compatibility adapter with the\ntask-specific adapter of $M_{v_2}$, and further fine-tune\nit (using the task training dataset) by aligning the\nnext token prediction to either $M_{v_1}$ or $M_{v_2}$. We\ndefine a masking (for individual tokens of a training\nsequence) following a simple heuristic depending\non whether $M_{v_2}$ (the adapter being trained) pre-\ndicts the correct tokens or not. If it does, we align to\n$M_{v_2}$ logits, otherwise, we align to $M_{v_1}$. The fine-\ntuning process of $M^\\prime_{v_2}$ is depicted in Fig. 3. The\nfine-tuning loss to train the compatibility adapter,\n$\\mathcal{L}_{comp}$, is defined below:\n$m_i = \\mathbb{I}[argmax(\\sigma(z_{M_{v_2},i})) \\neq y_i]$\n$\\alpha_{M_{v_1}} = KL(\\sigma(z_{M_{v_1},i}/T)||\\sigma(z_{M^\\prime_{v_2},i}/T))$\n$\\alpha_{M_{v_2}} = KL(\\sigma(z_{M_{v_2},i}/T)||\\sigma(z_{M^\\prime_{v_2},i}/T))$\n$\\mathcal{L}_{comp} = \\frac{1}{n} \\sum_{i=1}^n m_i\\alpha_{M_{v_1}} + (1 - m_i)\\alpha_{M_{v_2}}$\nWhen evaluating, we denote NFR as negative\nflip rate between $M_{v_1} M_{v_2}$, $NFR_c$ as the observed\nnegative flip rate between $M_{v_1}$ and our compati-\nbility model $M^\\prime_{v_2}$, and $\\Delta NFR = NFR - NFR_c$."}, {"title": "5 Experimental Setup", "content": null}, {"title": "5.1 Model Update Assumptions", "content": "To analyse the impact of model updates we con-\nsider parameter efficient fine-tuned models using\nLow-Rank Adapters (LoRA) (Hu et al., 2021).\nCompared to previous work on continuous learn-\ning and model updates (Qin et al., 2022, 2023),\nwe do not limit model updates to be produced by\nonly data updates, but consider different kinds of"}, {"title": "5.2 Task Adapter Training", "content": "For each task and each old model $M_{v_1}$ and new\nmodel $M_{v_2}$, we train a LoRA adapter on all linear\nlayers with r = 128 and $\\alpha$ = 256. We use a 0.8/0.2\ntraining/validation split for 10 epochs. We choose\nthe model by cross-entropy validation loss."}, {"title": "5.3 Compatibility Adapter Training", "content": "We keep all hyper-parameters the same to task\nadapter training, but train compatibility adapter\nwith the $\\mathcal{L}_{comp}$ loss defined in Eq. (2). We analyze\nstatistical significance of the results on a subset\nof compatibility adapter training for Phi 1 to Phi\n1.5 updates on PIQA using 3 random seeds. We\nobserve a standard deviation of accuracy of 0.0012.\nWe compare $\\mathcal{L}_{comp}$ with different masking\nstrategies m in the ablation studies in Section 6.5,"}, {"title": "6 Results", "content": null}, {"title": "6.1 Negative Flips Occur in Model Updates", "content": "Fig. 4 shows that significant negative flips (up to\nmore than 60%) exist for a variety of base model\nupdate scenarios and downstream tasks. We ob-\nserve negative flips in updates within one model\nfamily (e.g. Llama/Vicuna, and Phi models). We\nfind more negative flips for model updates with a\nsmaller delta in performance gain. For generative\ntasks like SAMsum dialogue summarization, we\nobserve a large number of negative flips, $NFR_s$,\nas continuous metrics are more sensitive to small\nchanges when updated."}, {"title": "6.2 Reduced Negative Flips in Classification", "content": "Binary and multiple choice generative tasks can\nbe evaluated with classification metrics, such as\ncalculating the log likelihood of each answer op-\ntion (we use Gao et al. (2023) for this use case).\nThis can be interpreted as a \u201cclassical\u201d sense of\nevaluating negative flips. In Table 3, we observe\nthat MUSCLE decreases negative flips as shown\nwhen compared to regular model update without\ncompatibility specific training ($M_{v_2}$). Specifically,\nwe show reduction of NFR by 40% for the update\nof LLama 1 to Llama 2 and 39% for Vicuna 1.3 to\nVicuna 1.5. For updates with large accuracy gap\n(for example Phi 1 to Phi 1.5), we observe a less\nstrong enhancement with a negative flip rate reduc-\ntion by 1-5%. In addition to reduction of negative\nflips we also observe an increased accuracy of up\nto 7% for Llama and Vicuna updates.\nSimilar to multiple-choice tasks, we can evalu-\nate math questions in a classification-like approach\nto determine negative flips. In exact match (EM)\nevaluation we match the final result of the math\nquestion from the prediction to ground truth. Re-\nsults are shown in Table 4. In this case, we observe\nthat we can reduce the number of negative flips by\n29% for Phi 1.5 to Phi 2. When the version 1 model\nis significantly less accurate (e.g., 3.4% for Phi 1),\nwe observe a reduction in accuracy while only be-\ning able to decrease negative flips by 2%. For all\nother updates, MUSCLE increases expected-match"}, {"title": "6.3 Increased Consistent Behavior", "content": "When we cannot achieve a positive flip (switching\nfrom an incorrect to a correct answer), we might\nprefer to at least maintain consistent behavior to old\nmodel to avoid unexpected behavior for the user.\nWe evaluate negative flips and inconsistency flips.\nFor model updates that have a large performance\ngap and a small number of negative flips to be-\ngin with, we see limited reduction in inconsistency\nflips. However, we observe that we can reduce the\ninconsistency flips for model updates with smaller\naccuracy gap such as the updates for Llama and\nVicuna (Fig. 5) on the HellaSwag dataset."}, {"title": "6.4 Reduced Regression in Generative Tasks", "content": "When evaluating generative tasks, such as summa-\nrization, we observe that we can reduce regression"}, {"title": "6.5 The Effect of Different Masking Strategies", "content": "In this section, we analyze different training and\nmasking strategies to evaluate our design choices.\nOn the PIQA dataset and model update Llama 1\n$\\rightarrow$ Llama 2, we compare MUSCLE with different\nmasking strategies. Intuitively, likelihood based\nmasking strategies can be useful for tasks that are\nevaluated with log-likelihoods (e.g. PIQA, Hel-\nlaSwag) where multiple choices are available. We\ncompare likelihood based masking for example on\nindividual tokens, $m_i = LLL = \\mathbb{I}[\\sigma(z_{M^\\prime_{v_2},i}) <\n\\sigma(z_{M_{v_1},i})]$, to check if the likelihood of the correct\nnext token in the current model is smaller than the\nold model. Only in this case, we align to $M_{v_1}$,\nand align to $M_{v_2}$ for every other token. Alterna-\ntively, we can also compare the likelihood of the\nentire sequence $m_i = LLS = \\mathbb{I}[\\sum_i[\\sigma(z_{M^\\prime_{v_2},i}) <\n\\sigma(z_{M_{v_1},i})]]$, such that we mask all tokens per se-\nquence to get instance-wise masking. In both cases,\nwe see a reduction in negative flips. However,\nlikelihood based masking requires auxiliary cross-"}, {"title": "7 Conclusion", "content": "In this work, we study the task-specific compat-\nibility problem when updating LLMs. We show\nthat LLM updates with different scenarios, e.g.,\nchanges in model architecture, optimization, or\ntraining dataset, exhibit significant negative flips-\ninstances previously classified or generated cor-\nrectly, and incorrectly after the model update. We\nextend negative flip metric for discriminative and, for the first time, generative tasks, and report results\nfor various models and tasks.\nWe propose a novel method (MUSCLE) to train\ntask-specific compatibility adapter when updating\nan old LLM to a new LLM to reduce negative flips\nwhile maintaining performance gain. Our proposed\nmethod does not require a modification to the base\nmodel training, and is only based on adapter train-\ning. Further, as opposed to previous works, the\nproposed solution does not require both versions\nof the model in memory to enable compatibility,\nwhich is often infeasible due to large size of LLMs.\nWe observe a mitigation of negative flips of 40%\nfor multiple-choice type evaluations, and 27% for\ncontinuous summarization evaluation. We also\nshow insights into model properties that facilitate\ntransfer, finding that our alignment masking strat-\negy provides best results with an additional benefit\nof mitigating inconsistent update behavior."}, {"title": "8 Limitations", "content": "In the current study we do not consider a model\nupdate that includes changes in tokenization and/or\nvocabulary size (e.g.Llama 2 (Touvron et al.,\n2023b) to LLama 3 (AI@Meta, 2024)). Future\nwork can explore compatible vocabulary mapping\nstrategies before learning from prior model ver-\nsions. Our results indicate that the utility of align-\ning to prior model versions is limited by the perfor-\nmance of the prior model version. For example, see\nthe update from Phi 1 to Phi 1.5 in Table 4, where\nPhi 1 only has an accuracy of 3%. In this case, it\nis arguable if an alignment to $M_{v_1}$ is desired and\nif the strive for compatibility outweighs a possible\nperformance drop."}, {"title": "9 Ethical Considerations and Risks", "content": "One potential risk of the proposed approach for\ncompatible task-specific LLM updates is the trans-\nfer of potential biases from the old model, $M_{v_1}$, to\nthe new model trained through knowledge transfer.\nWe did not explore this aspect in the current study."}, {"title": "10 Acknowledgements", "content": "We would like to thank Rick Chang, Cheng-Yu\nHsieh and Yen-Ju Lu for their help with the paper."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Training Hyperparameters and Evaluation", "content": "We show an overview of the design choices of\nour LoRA adapter for task and compatibility train-\ning in Table 7. We learn on the entire context and\nanswer tokens during fine-tuning. We find that"}, {"title": "A.2 Model Update Evaluation", "content": "In the evaluation of regression for different model\nupdates (Fig. 4), we consider the model pairs\nshown in Table 8."}, {"title": "A.3 Downstream Tasks", "content": "We describe the datasets used in our experiments.\nEach dataset poses unique challenges and targets\ndifferent aspects of language understanding and\nreasoning."}]}