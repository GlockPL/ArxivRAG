{"title": "Interpretable end-to-end Neurosymbolic Reinforcement Learning agents", "authors": ["Nils Grandien", "Quentin Delfosse", "Kristian Kersting"], "abstract": "Deep reinforcement learning (RL) agents rely on shortcut learning, preventing them from generalizing to slightly different environments [1]. To address this problem, symbolic method, that use object-centric states, have been developed. However, comparing these methods to deep agents is not fair, as these last operate from raw pixel-based states. In this work, we instantiate the symbolic Successive Concept Bottlenecks Agents (SCoBots) framework [2]. SCoBots decompose RL tasks into intermediate, interpretable representations, culminating in action decisions based on a comprehensible set of object-centric relational concepts. This architecture aids in demystifying agent decisions. By explicitly learning to extract object-centric representations from raw states, object-centric RL, and policy distillation via rule extraction, this work places itself within the neurosymbolic AI paradigm, blending the strengths of neural networks with symbolic AI. We present the first implementation of an end-to-end trained SCoBot, separately evaluate of its components, on different Atari games. The results demonstrate the framework's potential to create interpretable and performing RL systems, and pave the way for future research directions in obtaining end-to-end interpretable RL agents.", "sections": [{"title": "1 Introduction", "content": "Despite ongoing advancements in the field, reinforcement learning (RL) continues to face numerous challenges. One such challenge is the sparsity of rewards [3], where the environment only rarely provides reward signals for the agent to learn from. A related issue is credit assignment [4; 5], which refers to the challenge of identifying the specific previous actions responsible for distant future rewards. Additionally, RL agents are susceptible to learn misaligned goals [6; 1], which occurs when the objectives optimized by the RL algorithm diverge from the intended goals of the system's designers. The black box nature of current deep RL approaches impedes the ability to address these challenges. Even though there have been attempts of shedding light into the black box via approaches from the field of eXplainable \u0391\u0399 (\u03a7\u0391\u0399) [7; 8], there is still room for improvement. The majority of the developed approaches rely on post-hoc explanations, which frequently result in a lack of faithfulness of the explanations [9; 10]. This makes it challenging to analyze an agent's policy.\nTo address the lack of interpretability, we instantiate the recently proposed SCoBots framework [2]. This approach uses an architecture that achieves interpretability by design. SCoBots decompose the RL problem via concept bottleneck models [11] with intermediate interpretable representations. The"}, {"title": "2 Background", "content": ""}, {"title": "2.1 SCoBots", "content": "In the Successive Concept Bottlenecks Agents (SCoBots) framework [2] (cf. Figure 1), the policy of an agent is decomposed into distinct steps with intermediate interpretable concept bottlenecks (ICBs) inspired by concept bottleneck models [11].\n$\\omega_{\\theta_{1}} \\rightarrow O_t$\n$\\omega_{\\theta_{1}}$\n$\\{x_{i\\}^{t\\}_{i=t-(n-1)}$\n$\\mu_{\\tau}$\n$\\Omega_t$\n$N_t = \\{o_1\\}_{i=1}^{c_t}$\n$\\theta_{2}$\n$\\Omega_{\\tau}$\nThis differs from the standard deep RL approach, in which the raw input is processed to directly derive the selected action without any structured intermediate steps.\n$\\omega_{\\theta_{1}}$\nObject Extractor: $S_t \\xrightarrow{\\omega_{\\theta_{1}}} O_t$ The object extractor, denoted as $\\omega_{\\theta_{1}} (\\cdot)$, extracts objects and their properties from the n most recent frames, represented by $s_t = \\{x_{i\\}^{t\\}_{i=t-(n-1)}$. As a result, a collection of object representations is returned: $w_{\\theta_{1}}(s_t) = N_t = \\{o_1\\}_{i=1}^{c_t}$. Here, $c_t$ refers to the number of detected objects. The object representations of are tensors that capture a multitude of properties of each object (e.g., position).\nRelation Extractor: $N_t \\xrightarrow{\\mu_{\\tau}} \\Gamma_t$ In this step, relational concepts are derived from the previous output via the relation extractor: $\\mu_{\\tau}(\\cdot)$. Here, F parameterizes a set of relational functions that includes general object relations like distance and speed. Formally, we denote this step as $\\mu_{\\tau}(\\Omega_t) = \\Gamma_t = \\{g_i\\}_{i=1}^{d_t}$, where de quantifies the relational concepts.\n$\\Gamma_t \\xrightarrow{P_{\\theta_2}} a_t$\nAction Selector: $F_t \\xrightarrow{P_{\\theta_2}} a_t$ Finally, the action selector, denoted as $p_{\\theta_2}$, determines the action, $a_t$, from the relational concepts. In contrast to the earlier stages where ICBs provided sufficient interpretability, in this stage the action selector itself must be interpretable to enable overall interpretability (e.g., by using decision tree or rule set policies)."}, {"title": "2.2 SPACE", "content": "SPACE [13] is Variational Autoencoder (VAE)-basedarchitecture for unsupervised object-oriented scene representation learning (cf. Figure 2). Its latent space is designed to represent location-related information (i.e. the object position) and the feature-related information (i.e. the object visualization) of each object, and an encoding of the background information separately. SPACE is trained using a standard VAE reconstruction loss."}, {"title": "2.3 MOC", "content": "To address the insufficient performance of both object localization and representation learning of SPACE, a follow up work has added 2 loss terms within the Motion and Object Continuity (MOC) training scheme [14]. This is an approach that can be applied to any base detection model to improve the object locations (loc) and encodings (enc). The motion supervision loss utilizes additional motion information to improve localization variables (i.e. loc and pres). The object continuity loss is designed to gather object encodings of the same entity across successive frames, and to separate encodings of the different objects. The MOC training scheme is depicted in Figure 2."}, {"title": "2.4 ECLAIRE", "content": "N\nECLAIRE [15] is a rule extraction method for deep neural networks. The input for ECLAIRE is a set of unlabeled training instances $X = \\{x^{(i)} \\in R^m\\}_{1}^{N}$ and a pre-trained neural network $f_{\\theta} : R^m \\rightarrow [0,1]^L$. The function $f_{\\theta}(x)$ outputs a probability distribution over labels in the set $Y = \\{l_1,l_2,...,l_L\\}$. The output is a set of IF-THEN rules, denoted as $R_{x \\rightarrow \\hat{y}}$. These rules are designed to collectively predict the outcome that corresponds to the maximum value in the output vector of the network $f_{\\theta}$ when subjected to a majority vote given the input x. A single rule is formalized as follows:\nIF ((xi > Vi) \u2227 (x j \u2264 vj) \u06f8 . . . \u2227 (Xn > Vn)) THEN lk\nIn this structure, xi represents the i-th feature of an input instance x, while vi is a threshold value determined through the learning process. These rules are composed of premises that are conjunctions of conditions like (xi > vi) or (xi \u2264 Vi)."}, {"title": "3 Implementation", "content": "Let us introduce our DINSA method (cf. Figure 3). DINSA uses the SPACE [13] architecture, trained with the motion and object consistency (MOC) loss of [14]. It then use a k-mean classifier to classify each object, then uses a simple object tracking algorithm to stabilize the detection. The object-centric space is then augmented with relations, provided to an neural action selector (distilled in a set of rules). Let us now detail each component."}, {"title": "3.1 Object Extractor", "content": "The object extractor receives the last n frames of the game as input. It returns the objects of the current frame together with their properties. These properties can be time-related, which is why a sequence of images is given as input."}, {"title": "3.1.1 SPACE+MOC for Object Representation Learning", "content": "This component receives an image as input and produces two key outputs: a bounding box for each detected object and an encoding for each object. We use the SPACE architecture and enhance it using the MOC training scheme. The desired outputs are obtained from the latent space of the VAE architecture of SPACE.\nDuring inference, a set of objects with bounding box and encoding information must be obtained. To this end, only the encoder parts of the foreground module of the SPACE model are required. The other components of SPACE and the MOC training scheme are only needed for training. The variables zpres zwhere and zwhat are extracted from the SPACE architecture. The value of zpres is thresholded, resulting in a binary variable indicating the presence of an object. For the cells in which an object is present, the zwhere information is transformed into a bounding box and the zwhat encoding is saved. For implementation details and hyperparameter values, see App. A.1.2."}, {"title": "3.1.2 Object Classification", "content": "This component is designed to classify objects based on their feature encodings as inputs. In the context of the SPACE+MOC model, these encodings are provided by the zwhat latent variable. The output of the classification component is a label assigned to each object.\nThe classifier should be unsupervised in order to not break the overall unsupervised setting. Similar to [17] and [18], we classify the representation of an object based on its distance to the centroids that result from applying k-means clustering to a training set of encodings. A full description of the algorithm can be found in the Appendix at A.1.3."}, {"title": "3.1.3 Object Tracking", "content": "The property position history is usually required as part of the object representations, as it is essential for computing time-related relational concepts such as speed in the downstream relation extractor. In order to include this property, the number of image frames n to be contained in the state $s_t = \\{x_{i\\}^{t\\}_{i=t-(n-1)}$ must be set to (at least) two. Moreover, it is necessary to determine which of the localized objects represent the same entity across the sequence of frames. In [2], this information was provided implicitly as part of the ground truth detection via OCAtari. Our solution approach is to use a simple tracking algorithm on top of the single frame localizations by SPACE+MOC. More details are provided in the Appendix at A.1.4."}, {"title": "3.2 Relation Extractor", "content": "The relation extractor uses the extracted objects' properties from the current frame as input, and outputs a vector containing the values of the relational concepts for the detected objects.\nThe SCoBots framework includes a complete implementation of the relation extractor. In this implementation, the number of detectable objects per class are specified in advance, resulting a in a set of unique identifiers for potentially detected objects. The relational concepts are then defined relative to these identifiers using straightforward functions, such as Euclidean distance, to generate scalar values for each concept. During inference, detected objects are mapped to an identifier by sorting them based on their proximity to a key object (e.g., the player), with excess objects discarded and missing ones assigned zero values. For a more detailed description, see App. \u0410.2."}, {"title": "3.3 Action Selector", "content": "The action selector component receives the feature vector from the relation extractor and returns an action. The learning phase for our implementation of this component is divided into two steps. First, a neural policy is learned using standard deep RL techniques. Then, this policy is transformed into an interpretable representation that uses a set of rules. This transformation is a trade-off between maintaining the policy's similarity and finding a small, easily interpretable set of rules."}, {"title": "3.3.1 Deep Reinforcement Learning", "content": "This component learns a neural policy based on the relational concepts. This neural policy determines the actions or decisions made in a given context based on the input information. We utilize Proximal Policy Optimization (PPO) [19] to learn the neural policy. Other RL algorithms that can handle continuous state spaces and discrete action spaces could have also been used. Details on the choice of the hyperparameters are provided in App. A.3.2."}, {"title": "3.3.2 Policy Distillation via ECLAIRE", "content": "In this step, we transform the neural policy into a rule set policy using ECLAIRE [15]. Applied to our case, the training instances X in ECLAIRE are the one-dimensional vectors, which the relation extractor provides. The neural network fe is the policy network learnt using the PPO algorithm. Y is the discrete action space of the respective game."}, {"title": "4 Experimental Evaluation", "content": "In our experiments, we successively evaluate the\ndifferent components of our SCoBots [2] instan-\ntiation. First, the object extraction is evaluated.\nAs the relation extractor only applies determinis-\ntic functions to object properties, it is not inves-\ntigated separately. Second, the action selector is\nanalyzed including both the preliminary neural\npolicy and the final rule set policy.\nThe Pong, Boxing and Skiing environments (de-\npicted in Figure 4) were used in the experiments\nfor the object extractor. Only Pong and Boxing\nremained for the action selector experiments, as\nSkiing is a difficult credit assignment problem,\nthat requires additional techniques to be solved\n(cf. App. A.1.1). The object extractor focuses\non the moving objects of the games, only considering the relevant objects for playing the games\n(i.e. excluding e.g., scores or the timer in Boxing). This was realized by applying a filter based on\npotential detection areas of the moving objects (cf. App. A.1.2)."}, {"title": "4.1 Object Extractor", "content": "In this subsection, we present the evaluation of our object extractor. However, we only included the localization and encoding component plus the classifier, but did not include the object tracker. The scores were calculated relative to all ground truth objects and not only relative to the localized objects. This allowed us to obtain a better understanding of how the object extractor would behave for the downstream task.\nOverall, the F-score was the best for Boxing (cf. Figure 5). Pong had a slightly lower F-score due to the poor recall for the localization of the ball object. Both are likely to be suitable for the downstream task. Skiing performed the worst, which can be explained by the poor classifier performance. The average correct detection of only four out of five ground truth objects is unlikely to be sufficient for the downstream task. In particular, when we examined the confusion matrix (cf. Figure 6), we observed that the detection of the player object was problematic, arguably the most important object. Many trees were classified as the player. This behavior would confuse the downstream RL algorithm."}, {"title": "4.2 Action Selector", "content": "Let us here evluate the action selection process."}, {"title": "4.2.1 Deep Reinforcement Learning", "content": "This experiment assesses the performance of RL\nagents equipped with neural policies, with object-\ncentric state input that are provided via the com-\nponents of the SPACE+MOC object extractor\nand relation extractor.\nThe results for Pong reveal that an agent em-\nploying the SPACE+MOC object extractor can\nachieve comparable performance to an agent util-\nising a ground truth object extractor, provided\nthat the two hidden layer configuration is used\n(cf. Figure 1). The outcomes of the Boxing ex-\nperiment indicate that the SPACE+MOC object\nextractor is too inaccurate for use in competi-\ntive object-centric agents. The agents using the\nSPACE+MOC data achieve poor performance compared to the agents using ground data. The\nlatter agents perform well in particular for both of the unpruned configurations. To summarize, the\nexperiment indicates that the SCoBots framework's modular design which enhances interpretability\nand allows for incremental component upgrades, can come at the cost of error accumulation."}, {"title": "4.2.2 Policy Distillation", "content": "The goal of this experiment was to ascertain the final performance score for our SCoBots imple-\nmentation. Additionally, we sought to determine the extent to which performance is diminished by\nextracting the rule set policy from the neural policy. We also aimed to understand how the size of\nthe neural network and of the feature vector for the relational concepts affect the performance.\nThe findings suggest the benefits of using the pruned and two-layer configuration for distillation via\nECLAIRE, although the trends are not entirely clear (cf. Figure 1). Notably, a configuration for a\nSCoBot using SPACE+MOC input and a rule set policy was identified that achieved a respectable\naverage reward of 14.4 in the game Pong. The best configuration for Boxing using SPACE+MOC input\nand a rule set policy achieved an average reward of 51.8, although using the unpruned configuration."}, {"title": "5 Limitations", "content": "The current approach relies on strong assumptions about the training environment, such as the availability of training images showing all object variations and motion data from optical flow estimation. The first aspect can be problematic in environments where objects appear only after certain thresholds, requiring pre-trained agents to collect data. Furthermore, it is uncertain whether valuable motion data can be obtained in more complex scenarios than Atari games.\nCurrently, the extracted properties only concern the location and the class of the objects. More advanced properties such as the orientation of an object are currently not extracted, even though they can be highly relevant in some Atari games (e.g., in Skiing).\nThe rule set representation of the policy lacks interpretability due to a large number of generated rules, complex premises with many terms and the fact that the premises of multiple rules with conflicting outputs can be satisfied for the same input data point."}, {"title": "6 Future Work", "content": "Replacing the implementation of the object extractor with unified object detection and tracking methods (e.g., YOLO [21; 22]) could be promising, although this would lead to a limited set of properties. Keeping the multi-step implementation of the object extractor, alternative models to SPACE+MOC could be investigated such as SlotAttention [23] or CutLER [24]. In addition,"}, {"title": "7 Related Work", "content": ""}, {"title": "7.1 Explainable and Interpretable Reinforcement Learning", "content": "Explainable Reinforcement Learning (XRL) is a prominent area within the field of XAI, focusing on giving human insights into the decision-making processes of AI agents. Key publications such as [25; 26; 27; 28; 29] have extensively reviewed the field of XRL. These works propose frameworks for categorization, highlight complexities, and emphasize ongoing issues that require resolution.\nSCoBots can be categorized as an intrinsic approach as it directly allows humans to grasp how the model reaches its predictions without requiring any additional computation as post-hoc approaches would [27]. However, they are only truly an intrinsic approach if an appropriate action selector is chosen, as in our case a rule set policy. The explanations provided by a SCoBot are usually local as they are focused on a single input instance, in contrast to global explanations, which would enable understanding the overall input-output behavior [27]. Again, the choice of the action selector is the main determining factor. Our instantiation via a rule set policy leads to local explanations. The SCoBots framework falls under the feature importance method category as defined by [25], emphasizing the identification of crucial input features that influence decisions. SCoBots provide zero-order explanations according to the framework by [26], focusing on the agent's immediate response to inputs. The SCoBots approach aligns with model explaining as outlined in [29], with the focus on elucidating the model's rationale. According to the categorization by [30], SCoBots learn Symbolic Representations and use Object-Recognition. That categorization focuses solely on intrinsic approaches, for which the authors reserve the term interpretable. With regard to Interpretable Decision-Making, the SCoBots framework itself does not fall into a specific category. However, our use of policy distillation via rule extraction classifies as an Indirect Approach in the subcategory Decision Trees and Variants."}, {"title": "7.1.1 Policy Distillation", "content": "Policy distillation [31; 32], a specialized form of knowledge distillation [33], involves training a highly performing teacher model and subsequently distilling this knowledge into a simpler student model. By selecting an appropriate student model architecture with sufficient constraints on complexity, an interpretable yet still performing model can be derived. Policy distillation is particularly advantageous when the desired final model architecture is challenging to create independently due to less performing optimization algorithms.\nOne notable implementation of this concept is VIPER [34], which utilizes imitation learning to extract decision tree policies from a neural policy and Q-function. Mo\u00cbT [35] extends this approach by incorporating a mixture of expert trees. MAVIPER [36] adapts VIPER to a multi-agent setting. Furthermore, approaches from the field of rule extraction can be used for policy distillation. These approaches transform the teacher model into a set of explicit IF-THEN rules. ECLAIRE [15] exemplifies this approach and is used in our instantiation of the SCoBots framework. NUDGE [37] is an approach from the domain of Neural Logic Reinforcement Learning [38]. This approach uses a"}, {"title": "7.2 Object Representation Learning", "content": "If the properties obtained by the object extractor only include the location and object class, as in our experiments, an object detection system suffices. In this case, models such as CutLER [24] and LOST [17] could be considered. These have demonstrated strong performance by leveraging features obtained through self-supervised learning combined with vision transformers [42]. However, perspectively the goal is to extract more properties. Hence, our choice of using an approach from the field of object representation learning. SPACE [13] follows a line of work that started with AIR [43]. AIR introduced a sequential attention mechanism that iteratively attends to and infers objects in a scene, using a recurrent neural network to propose object regions and a VAE to generate object representations. SPAIR [44] modified by introducing spatial invariance, utilizing a grid-based attention mechanism to enhance computational efficiency. Other approaches that operate on the pixel level, rather than with bounding boxes, include with Tagger [45] and NEM [46]. Notably, some works have demonstrated the ability to generate representations with dimensions that can be associated with specific features of the objects (e.g., color, shape) [47; 48; 49]. Recently, approaches have emerged that employ the concept of optical flow to benefit from the consecutive nature of the images [50; 51], as does MOC [14], which is used in our work together with SPACE. In supervised settings, automatic concept finding to extend the object-centric representations has been developed for explanation [52; 53; 54], or automatised with lambda calculus based concepts [55]. Interpretable concept can also be revised by expert in case of misalignment [56; 2]. Related to RL, concept revision has also been used in the domain of time series [57]."}, {"title": "8 Conclusion", "content": "We instantiated the SCoBots framework [2] and successfully demonstrated its application to Atari games. The SCoBots used a trained object detection component instead of the ground truth detection used in previous work. In the process, we explored several critical areas, including object representation learning, which involves simplifying a scene into an object-centric representation, and object-centric RL, which focuses on learning policies based on the representations of objects. Additionally, we covered policy distillation by applying rule extraction, which transforms a neural policy into a more interpretable rule set policy. Through this work, we hope to contribute to the improvement of interpretability in RL. We believe that by improving interpretability, RL agents can be analyzed and designed more successfully, which can facilitate addressing pervasive challenges in the field of RL. We also identified promising future research directions that could further enhance the framework's potential for advancing the field."}, {"title": "A Appendix", "content": "This appendix presents supplementary information on our research experiments and results. It includes details on the data set used, model configurations, and evaluation metrics."}, {"title": "A.1 Object Extractor", "content": ""}, {"title": "A.1.1 Data Set", "content": "The data set was generated using OCAtari [16]. For generation, a random agent was used. A total of 2048 training, 128 validation, and 128 test image sequences were collected, each comprising four consecutive frames. The reason for collecting four consecutive frames was that consecutive images are required to calculate the object continuity loss. For each frame, the ground truth object detections were also stored. A deliberate pause of at least 16 steps between sequences was implemented to ensure data diversity. The frames were downscaled to 128x128 pixels for compatibility with the SPACE [13] model."}, {"title": "Games", "content": "Pong, Boxing and Skiing were used in the experiments for the object extractor. These games cover different levels of difficulty for the object extractor. Pong and Boxing only contain a very small number of objects in each frame. The shapes of the objects in Boxing are more complex and can vary in shape, depending on whether the boxers are punching or not. Skiing contains more objects and the visual appearance of objects can vary within the same object class. An image of each game is depicted in Figure 4. In the experiments for the action selector, only Pong and Boxing were used. Skiing is too challenging even for end-to-end deep RL approaches due to its extremely delayed reward signal."}, {"title": "Optical flow", "content": "The optical flow method involved the collection of a mode image based on 100 images. A manual check was conducted to ensure that the background of the respective game was accurately represented. The optical flow and motion data were then calculated using the background subtraction approach outlined in [14]."}, {"title": "A.1.2 SPACE+MOC Details", "content": "In general, the hyperparameters from [14] were reused with a few modifications (cf. Table 2). Overall, the hyperparameters related to SPACE are largely consistent with those presented in the original work [13].\nThe MOC loss was applied using the dynamic scheduling approach proposed in [14]. This approach dynamically balances the motion supervision loss and the object continuity loss depending on the ability to correctly localize the bounding boxes. For more details, please refer to to [14]. In contrast to [14], no bootstrapping of the decoder with the help of the motion information was used. The training was repeated using five different seeds, and the resulting data was used to calculate the mean and standard deviation for the metrics."}, {"title": "Filtering SPACE+MOC Localizations", "content": "The evaluation of the object extractor was focused on the moving objects of the games, only considering the relevant objects for playing the games. Consequently, static objects that are also provided via OCAtari were removed from the ground truth data (e.g., the clock object). Furthermore, the predicted objects by the model for localization underwent filtering based on potential detection areas of the moving objects. Practically, this was implemented by discarding localized objects that have bounding box coordinates clearly outside of the areas where moving objects can appear in the game. For the games that were considered in our experiments, this approach is sufficient because"}, {"title": "A.1.3 Classifier Details", "content": "The creation of the classifier involves multiple steps. First, k-means clustering is performed to obtain the centroids. For each Atari game, the value for k is given by the number of object classes as specified in OCAtari [16]. Second, descriptive labels are assigned to the obtained centroids, since the k-means clustering only returns enumerated class labels. For this, a k-nearest neighbors classifier is used, where k does not refer to the same number as in k-means. The initialization is based on object encodings extracted by SPACE+MOC from a small image data set. These objects have been assigned a descriptive label based on the object names of the ground truth detections provided by OCAtari. The descriptive labels for the centroids are finally assigned via the k-nearest neighbors classifier. It is important to note that this approach is not entirely in accordance with the unsupervised setting, but the supervised data is only used for assigning names to the classes. Third, the final classifier is given by a 1-nearest neighbor classifier initialized only with the centroids."}, {"title": "A.1.4 Tracking Algorithm Details", "content": "In the initial pass of the tracking algorithm through a video frame, each detected object is added to a tracking list. Each object is identified by the bounding box that encapsulates it, and at this stage, there are no previous tracks to compare against, so all detections are treated as new objects. From the second frame onwards, the algorithm calculates the distances between the centroids of the currently tracked objects and the centroids of the new detections. The matching scores, derived from the distance calculations, are used to determine which new detections correspond to which existing tracks. A detection is assigned to the closest tracked object, thereby ensuring continuity in tracking. New detections that do not closely match any current track either because they are too far from existing tracks or are only the second-best match are initiated as new tracks. This step accounts for new objects entering the scene. Conversely, objects that have been previously tracked but do not find a match in the new detections are removed from the tracking list. This addresses objects that leave the scene or become occluded.\nThe algorithm's simplicity can result in failure when objects cross paths or even overlap for multiple consecutive frames. While the former may result in incorrect features for a single frame, the latter can lead to significant issues. The current implementation is based on [58]."}, {"title": "A.1.5 Combined Object Extractor Experiments", "content": "The results were generated using the SPACE+MOC models with the highest F-score from five differently seeded runs, in conjunction with a corresponding classifier trained based on the encodings of that model.\nThe utilized F-score metric is a well-established and widely accepted standard. However, the evaluation of the localizations is heavily reliant on the manner, in which the localized and ground truth bounding boxes are assigned to one another. Therefore, further details on this are provided below.\nPairwise Matching Scores - How to measure the similarity between a predicted and a ground truth bounding box?\nThe authors of [14] argue that the intersection over union (IoU) metric, which is arguably the most common metric in this context, is not ideal for assessing performance for the downstream task of RL for Atari games. This is due to the architecture of SPACE, which tends to return bounding boxes of similar sizes for a game. Consequently, in games such as Pong where the ball and player objects differ in size, at least one of the object classes will have a bounding box that is either too large or too small. This will result in low IoU scores. However, since the objects are (mostly) of constant size in the games that we consider, the downstream RL algorithm can implicitly infer the size. In fact, the object-centric input representation returned by the relation extractor only contains the center coordinates of the objects in our implementation. As an alternative to IoU, the authors of [14] introduce the center divergence metric. This metric focuses on the distance of the center coordinates"}, {"title": "Joint treatment of Localization and Classification", "content": "In the event that a localized object cannot be matched with a ground truth object, it is labeled as \"no object.\" Conversely, if a ground truth object does not correspond to any localized object, it is designated as \"not detected.\" This classification system allows for a more nuanced understanding of the detections.\nIn order to evaluate the performance of the object extractor, we calculated the precision, recall, and F-score. It is necessary to be precise with the definition of these metrics. This is due to the two-step approach in which the classifier, in the second step, can receive localized objects that do not have a ground truth object associated with them. At the same time, the classifier does not necessarily receive all of the ground truth objects. Our calculated precision score addresses the following question: What proportion of all the detected objects corresponds to a ground truth object and is correctly labeled? In other words, when looking at the confusion matrix (cf. Figure 6), the \"not_detected\" column is removed, and then the micro precision is calculated. Analogous to the precision score, our recall score answers the following question: What proportion of all the ground truth objects has been localized and has received the correct label? This is achieved by removing the \"not_an_object\" row from the confusion matrix and then calculating the micro recall."}, {"title": "A.2 Relation Extractor", "content": ""}, {"title": "Algorithmic description", "content": "The relational concepts are specified in advance. To this end, the number of objects of each class to be considered must be specified at first. Objects of the same class are given unique identifiers by enumeration. Then, the relational concepts to be computed based on these objects are defined. A relational concept is characterized by the input objects and the relational function applied to their properties. The relational functions themselves are straightforward (e.g., calculating Euclidean distance). This approach specifies clear computational instructions that yield a single scalar value for each defined relational concept. Each relational concept is assigned a fixed position in the output vector."}, {"title": "Selection of relational concepts", "content": "Full set: All relational concepts as detailed in Table 3 of [2] were included. This set considers all possible combinations of objects for n-ary relations, including symmetric ones like distance, where both directions (e.g., d(Ball1, Player1) and d(Player1, Balll)) are accounted for, resulting in six combinations. For the linear trajectory relation, combinations with the object itself are included, e.g., leading to nine combinations for three objects.\nPruned set: The pruned set of relational concepts only includes the subset of relational concepts that are presumed to be relevant for the game as listed in Table 4 of [2]. The selection of relational concepts is based on human understanding of the games."}, {"title": "A.3 Action Selector", "content": ""}, {"title": "A.3.1 Experiment Design", "content": "We designed the experiments to encompass different levels of complexity of the model for the neural policy. This was done with the rationale that the rule set extraction would probably work more reliably the simpler the neural network (NN) structure and the policy is. Therefore, we varied the depth of NN architecture in the PPO [19] algorithm. The NNs were instantiated with either 1 or 2 hidden layers of 64 neurons. Additionally, the set of relational concepts was varied. Either the full set of relations or a pruned version was used (cf. App. A.2)."}, {"title": "A.3.2 Deep Reinforcement Learning Experiments", "content": "The hyperparameters for PPO [19", "2": "with a few minor exceptions. Specifically"}]}