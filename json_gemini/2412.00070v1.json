{"title": "Recurrent Stochastic Configuration Networks with Hybrid Regularization for Nonlinear Dynamics Modelling", "authors": ["Gang Dang", "Dianhui Wang"], "abstract": "Recurrent stochastic configuration networks (RSCNs) have shown great potential in modelling nonlinear dynamic systems with uncertainties. This paper presents an RSCN with hybrid regularization to enhance both the learning capacity and generalization performance of the network. Given a set of temporal data, the well-known least absolute shrinkage and selection operator (LASSO) is employed to identify the significant order variables. Subsequently, an improved RSCN with L2 regularization is introduced to approximate the residuals between the output of the target plant and the LASSO model. The output weights are updated in real-time through a projection algorithm, facilitating a rapid response to dynamic changes within the system. A theoretical analysis of the universal approximation property is provided, contributing to the understanding of the network's effectiveness in representing various complex nonlinear functions. Experimental results from a nonlinear system identification problem and two industrial predictive tasks demonstrate that the proposed method outperforms other models across all testing datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, neural networks (NNs) have demonstrated promise in capturing complex patterns and relationships within the process data, making them powerful for modelling nonlinear dynamic systems [1]-[3]. However, in many real-world applications, the order and distribution of online-collected samples remain unknown and constantly varying. These uncertainties significantly impact the model's learning capacity and may lead to a well-trained model underperforming on test samples, resulting in ill-posed problems [4]\u2013[6]. Recurrent neural networks (RNNs) utilize a reservoir to store and update historical state information. This unique structure enables RNNs to effectively represent long-term dependencies in temporal data, which are well-suited for modelling systems with unknown dynamic orders [7], [8]. In [9], the regularization strategy was introduced into RNNs, which enhanced the network's robustness to noisy outliers and generalization performance. Unfortunately, these models rely on the error back-propagation (BP) algorithm to train weights and biases, which suffer from slow convergence, structure determination, and the sensitivity of the initial weights and learning parameters. Therefore, it is essential to develop advanced modelling techniques to address these problems.\nIn recent years, randomized methods for training NNs have received considerable attention due to their merits of fast learning speed and computational simplicity [10]\u2013[13]. Among them, regularized echo state networks (ESNs) have also been widely applied to tackle the ill-posedness in temporal data modelling [14]\u2013[17]. Nevertheless, ESNs lack a foundational understanding on the structure design and the random parameters assignment. With such a background, Wang and Li [18] pioneered a novel randomized learner model, termed stochastic configuration networks (SCNs), which incorporated an innovative supervisory mechanism for assigning random parameters. This finding is significant for the advancement of randomized learning theory, and many promising results about nonlinear dynamic modelling have been reported. In [19], an enhanced version of SCN was presented by integrating L2 regularization, addressing issues related to structural settings and random parameter allocation. Furthermore, a regularized SCN was introduced in [20] to solve the problems of outlier presence and multicollinearity in the input data, successfully mitigating concerns of overfitting. Wang et al. [21] used weighted mean vectors to optimize the parameter selection and network structure of the regularized SCN, resulting in superior performance in terms of rapid convergence, network compactness, and predictive accuracy. Although SCNs and their regularization variants have shown progress in tackling industrial data analysis tasks, they are primarily designed for static data or based on the assumption of known system orders, posing challenges for modelling nonlinear systems with order uncertainty.\nBuilt on the SCN concept, we proposed the recurrent stochastic configuration networks (RSCNs) [22], where the reservoir structure is incrementally constructed and a specialized feedback matrix is established to ensure the universal approximation property. RSCNs offer several advantages, including the strong learning capabilities, computational simplicity, and the potential for implementing lightweight recurrent models. However, they struggle to identify useful order variables, resulting in information redundancy that complicates the extraction of true signals and increases the risk of overfitting. This paper presents an RSCN with hybrid regularization for problem solving. Firstly, the least absolute shrinkage and selection operator (LASSO) is used to extract the key order information from the temporal data. Then, an improved RSCN with L2 regularization is developed based on the selected order variables, as well as the residuals between the LASSO model output $Y_{LASSO}$ and the target output T. Finally, the output weights are dynamically adjusted according to the projection algorithm. Experimental results suggest that the proposed method can alleviate the ill-posed problem and perform favorably with sound generalization, indicating its strength in nonlinear dynamic modelling. The contributions of this paper can be summarized as follows:\n1) A hybrid regularized modelling framework is introduced, which integrates LASSO and RSCN within a sequential ensemble learning strategy. This approach effectively captures relevant prior knowledge and reduces the data dimensionality. Moreover, the output compensation scheme trains the network by approximating the residuals $\\tilde{Y} = T - Y_{LASSO}$, further enhancing the model's predictive accuracy.\n2) The random parameters are assigned in the light of an improved supervisory mechanism, which incorporates the regularization technique into the inequality constraint for generating the reservoir node. The L2 penalty term is introduced into the cost function of the original RSCN, and the output weights are evaluated by utilizing the regularized least squares method.\n3) The theoretical analysis of the universal approximation property is provided. The echo state property of RSCNs is naturally inherited, and the convergence of the parameter updates based on the projection algorithm is also guaranteed. These results enable the built model to exhibit strong nonlinear processing capability and stability.\n4) The impact of the regularization coefficient and reservoir size on the model performance is carefully considered, and two real-world industrial data predictive analyses are carried out. The findings illustrate the superiority of our proposed method in terms of structural compactness and generalization performance.\nThe remainder of this paper is organized as follows. Section II formulates the problem and reviews the related knowledge of RSCNs. Section III details the proposed hybrid regularized RSCN frameworks with the algorithm description and theoretical analysis. Section IV reports the experimental results. Finally, Section V concludes this paper."}, {"title": "II. RELATED WORK", "content": "In this section, two related works are introduced, including the problem formulation and recurrent stochastic configuration networks."}, {"title": "A. Problem formulation", "content": "Consider a discrete nonlinear dynamic system:\n$y(n + 1) = f_p(y(n), ..., y(n - d_a + 1), u_1(n), ..., u_1(n - d_1 + 1), u_2(n), ..., u_2(n - d_2 + 1), ..., u_k(n), ..., u_k(n - d_k + 1)) + e(n),$\nwhere y is the plant output, {$u_1, u_2, ..., u_K$} is the input variable, K is the input dimension, $d_a, d_k$ are the systems orders for y and $u_k$, e is the process error, and $f_p$ is a nonlinear function. Due to external disturbances and changes in the process environment, time-varying delays and unknown orders may occur in the system. These factors significantly affect the dynamic behavior of the system, resulting in a decline in the model's performance in both prediction and control [23], [24]. Specifically, time-varying delays may cause the system's response to become unstable, complicating the accurate capture of real-time changes, while unknown orders hinder the model's ability to identify and utilize the key variables. This uncertainty not only increases the complexity of modelling but also leads to decision-making errors, ultimately impacting the overall efficiency and reliability of the system.\nThis paper aims to address the challenge of modelling nonlinear systems with uncertain dynamic orders by extracting important order variable information as prior knowledge. This scheme helps researchers gain a clearer understanding of the system's dynamic characteristics, thereby enhancing the model's prediction performance. By identifying and analyzing these significant variables, the model can more effectively capture critical dynamic relationships within the system and reduce information redundancy. This not only aids in simplifying the model structure but also improves its adaptability in the face of complex environmental changes."}, {"title": "B. Recurrent stochastic configuration networks", "content": "This section briefly reviews our proposed RSCNs [22], which are a class of randomized learner models that hold the universal approximation property, and the random weights and biases are assigned in the light of a supervisory mechanism. Due to their advantages in neural network construction, such as less human intervention, adaptability of random parameters scope setting, fast learning, and handy implementation, RSCNs have demonstrated great potential in modelling nonlinear complex dynamics.\nGiven an RSCN model with N reservoir nodes:\n$X_N(n) = g(W_{in,N}u(n) + W_{r,N}x(n - 1) + b_x),$\n$y(n) = W_{out}(x(n), u(n)),$\nwhere $u(n) \\in R^K$ is the input signal; y(n) is the model output; $x(n) \\in R^N$ is the internal state of the reservoir; $W_{in,N} \\in R^{N\\times K}, W_{r,N} \\in R^{N\\times N}$ represent the input and reservoir weights, respectively; $b_x$ is the bias; g is the activation function; $W_{out} \\in R^{L\\times(N+K)}$ is the output weight; Kand Lare the dimensions of input and output. The model output can be obtained by $Y = [y(1), y(2), ..., y(n_{max})] = W_{out}X_N$, where $n_{max}$ is the number of training samples and $X_N= [(x_N(1), u(1)), ..., (x_N(n_{max}), u(n_{max}))]$.\nCalculate the current error $e_n = Y - T$, where $T = [t(1), t(2), ..., t(n_{max})]$ is the desired output. If $e_n$ is larger than the preset error threshold $\\varepsilon$, we need to assign the adding nodes based on the supervisory mechanism. Notably, to avoid overfitting, a supplementary condition is implemented to stop the addition of nodes and a step size $N_{step}$ ($N_{step} < N$) is employed in the following early stopping criterion:\n$||e_{val,N-N_{step}}||_F \\le ||e_{val,N-N_{step+1}}||_F \\le ... \\le ||e_{val,N}||_F,$\nwhere $e_{val,N}$ represents the validation residual error with N nodes. If (4) is satisfied, the number of reservoir nodes will be adjusted to $N - N_{step}$. The configuration process of the RSCN can be summarized as follows.\nStep 1: Generate [$w_{N+1,1}^{in}$ $w_{N+1,2}^{in}$...$w_{N+1,K}^{in}$], $W_{r,N+1}$, and $b_{N+1}$ stochastically from $G_{max}$ times in $\\lambda$ the adjustable uniform distribution $[- \\lambda, \\lambda]$, \\{$\\lambda_{min}, \\lambda_{min} + \\Delta \\lambda, ..., \\lambda_{max}$\\}. Construct a special structure of the reservoir weight matrix, that is,\n$W_{r,2}=\\begin{bmatrix}\nw_{1,1}^r & 0 \\\\\nw_{2,1}^r & w_{2,2}^r\\end{bmatrix}$,\n$W_{r,3}=\\begin{bmatrix}\nw_{1,1}^r & 0 & 0 \\\\\nw_{2,1}^r & w_{2,2}^r & 0 \\\\\nw_{3,1}^r & w_{3,2}^r & w_{3,3}^r\\end{bmatrix}$,\n$W_{r,N+1}=\\begin{bmatrix}w_{1,1}^r & 0 & 0 & ... & 0 & 0 \\\\\nw_{2,1}^r & w_{2,2}^r & 0 & ... & 0 & 0 \\\\...&...&...&...&...&...\\\\\nw_{N,1}^r & w_{N,2}^r & w_{N,N}^r &... & 0 \\\\\nw_{N+1,1}^r & w_{N+1,2}^r & w_{N+1,N}^r &... & 0 & w_{N+1,N+1}^r\\end{bmatrix}$,\nRemark 1: The RSCN model holds the echo state property, where the current state of the model depends solely on the input data after a specific period, independent of the initial state. To ensure this property, $W_{r,N+1}$ should be reset as\n$\\alpha W_{r,N+1} \\leftarrow \\frac{\\alpha}{\\rho(W_{r,N+1})}W_{r,N+1},$\nwhere 0 < $\\alpha$ < $\\rho_{max}(W_{r,N+1}) / \\sigma_{max}(W_{r,N+1})$ is the scaling factor, $\\rho_{max}(W_{r,N+1})$ and $\\sigma_{max}(W_{r,N+1})$ denote the largest eigenvalue and singular value of $W_{r,N+1}$, respectively. For a detailed proof, one can refer to [22].\nThe input weights are defined as\n$W_{in,2}=\\begin{bmatrix}\nw_{1,1}^{in} & w_{1,2}^{in} &... & w_{1,K}^{in} \\\\\nw_{2,1}^{in} & w_{2,2}^{in} &... & w_{2,K}^{in}\\end{bmatrix}$,\n$W_{in,N+1}=\\begin{bmatrix}\nw_{1,1}^{in} & w_{1,2}^{in} &... & w_{1,K}^{in} \\\\\nw_{2,1}^{in} & w_{2,2}^{in} &... & w_{2,K}^{in}\\\\...&...&...&...\\\\\nw_{N,1}^{in} & w_{N,2}^{in} &... & w_{N,K}^{in}\\\\\nw_{N+1,1}^{in} & w_{N+1,2}^{in} &... & w_{N+1,K}^{in}\\end{bmatrix}$,\nand $b_2 = [b_1, b_2]$,..., $b_{N+1} = [b_1, ...,b_{N+1}]$.\nStep 2: Seek the random basis function $g_{N+1}$ satisfying the following inequality constraint with an increasing factor $r_i$, i = 1,2,..., t, where 0 < $r_1$ < $r_2$ < ... < $r_t$ < 1... 1.\n$\\frac{(e_{N,q}, g_{N+1})^2}{\\langle g_{N+1}, g_{N+1} \\rangle^2} \\ge \\delta(1 - r_i - \\mu_{N+1}) ||e_{N,q}||^2, q = 1, 2, ...L,$\nwhere $e_N = [e_{N,1}, e_{N,2},..., e_{N,L}]^T$ is the residual error, {$\\mu_{N+1}$} is a non-negative real sequence satisfying $lim_{N\\rightarrow\\infty} \\mu_{N+1} = 0$, $\\mu_{N+1} \\le (1 - r)$, and 0 < ||g|| < $b_g$, $b_g \\in R^+$.\nStep 3: Define a set of variables {$\\xi_{N+1,1},..., \\xi_{N+1,L}$}, that is,\n$\\xi_{N+1,q} = \\frac{(e_{N,q}, g_{N+1})^2}{\\langle g_{N+1}, g_{N+1} \\rangle^2} - (1 - \\mu_{N+1} - r)e_{N,q}^T e_{N,q}.$\nSelect the node with the largest $\\xi_{N+1} = \\sum_{q=1}^L \\xi_{N+1,q}$ as the optimal adding node.\nStep 4: Evaluate the output weights based on the least square method, that is,\n$W_{out, N+1}= [W_{out,1}, W_{out,2}, ..., W_{out,N+1+K}]$\n$= arg min ||T - W_{out} X_{N+1} ||^2,$\nwhere $X_{N+1}= [(x_{N+1}(1), u(1)),..., (X_{N+1}(n_{max}), u(n_{max}))]$.\nStep 5: Calculate the residual error $e_{N+1}$ and update $e_0 := e_{N+1}, N = N+1$. If $||e_0||_F \\le \\varepsilon$, $N > N_{max}$ or (4) is met, we complete the configuration, where $N_{max}$ is the maximum size of the reservoir. Otherwise, repeat steps 1-4 until satisfying the termination conditions.\nAt last, we have $lim_{N\\rightarrow\\infty} ||e_n||_F = 0$. For more details, readers can refer to [22]."}, {"title": "III. REGULARIZED RECURRENT STOCHASTIC CONFIGURATION NETWORKS", "content": "This section details the construction process of our proposed hybrid regularized RSCN framework and presents the theoretical analysis on the universal approximation property."}, {"title": "A. Variables selection with LASSO", "content": "LASSO regression is an effective method for feature selection and modelling, particularly for high-dimensional data [25]-[27]. Consider a linear regression model, the LASSO regularization solution can be formulated by\n$min ||W_{LASSO}U - T||^2 + C_L ||W_{LASSO}||_1,$\nwhere $W_{LASSO} = [W_1,W_2,...,W_L]$ is the regression coefficient, U is the input, and $C_L$ is a regularization coefficient. $||\\cdot||_1$ and $||\\cdot||_2$ represent the L1-norm and L2-norm. By adjusting $C_L$, the regression coefficients of redundant or irrelevant features can be compressed to zero, resulting in a sparser linear regression model that facilitates feature extraction. Consequently, only the order variables corresponding to non-zero regression coefficients are retained."}, {"title": "Calculate the output of the LASSO model $Y_{LASSO} = W_{LASSO}U$, and obtain the residuals between $Y_{LASSO}$ and the the target output, that is,", "content": "$\\tilde{Y} = T - Y_{LASSO}.$\nThen, use the selected order variables denoted as $U_R$ to construct the regularized RSCN model and continuously approximate $\\tilde{Y}$. This order identification and output compensation scheme not only enhances the model's simplicity but also improves its predictive capability.\nRemark 2: Feature selection and order identification are two distinct concepts in data analysis and modelling, both of which aim to extract useful information from the raw data to improve the model performance. Feature selection primarily focuses on deriving representative features from the data and is widely applied in areas such as image processing and natural language processing. In contrast, order identification emphasizes the recognition of important time delays or order variables within a system, commonly encountered in dynamic system modelling and time series forecasting. This paper seeks to provide prior order information for system modelling, thereby improving the interpretation of temporal data through the clear identification of key order variables and enhancing the network's ability to capture complex dynamic behaviors."}, {"title": "B. Regularized recurrent stochastic configuration networks", "content": "Distinguished from the original RSCN, the proposed method integrates regularization techniques into the supervision mechanism and introduces an L2 penalty term to the loss function to limit the magnitude of the output weights. This design effectively manages the complexity of the model, further preventing overfitting to the training data and improving the model's generalization capability on the newly arriving data.\nGiven the input $U_R$=[$U_R$(1), ..., $U_R$($n_{max}$)] and desired output $\\tilde{Y}$, an initial model with N nodes can be constructed. If the terminal conditions are not met, we need to generate a new basis function $g_{N+1}$ under the supervisory mechanism with regularization.\nTheorem 1. Suppose that span(F) is dense in L2 space. Define $\\rho_g = (||g||^2 + C)/(||g||^2 + 2C)$, $\\rho_{b_g} = (b_g^2 + C)/(b_g^2 + 2C)$, 0 < C < 1, for $b_g \\in R^+$, $\\forall g \\in \\Gamma, 0 < ||g|| < $b_g$, and 0 < $\\rho_g < \\rho_{b_g}$. Given 0 < r < 1 and a nonnegative real sequence {$\\mu_{N+1}$} satisfying $lim_{N\\rightarrow\\infty} \\mu_{N+1} = 0$, and $\\mu_{N+1} \\le (1 - r)$. For N = 1,2..., q = 1, 2, ..., L, define\n$\\delta_{N+1} = \\sum_{q=1}^L \\delta_{N+1,q}, \\delta_{N+1,q} = (1 - r - \\mu_{N+1}) ||e_{N,q}||^2.$\nIf $g_{N+1}$ satisfies the following inequality constraint:\n$\\frac{\\langle e_{N,q}, g_{N+1} \\rangle^2}{\\langle g_{N+1}, g_{N+1} \\rangle^2} \\ge \\frac{(b_g^2 + C)^2}{(b_g^2 + 2C)} \\xi_{N+1,q} q = 1, 2, ... L,$\nand the output weight is constructively evaluated by\n$W_{out, N+1,q} = \\frac{\\langle e_{N,q}, g_{N+1} \\rangle}{||g_{N+1}||^2 + C}.$\nwe have $lim_{\\gamma\\rightarrow\\infty} ||e_{N+1} || = 0$.\nProof. Note that\n$||e_{N+1}||^2 - (r + \\mu_{N+1}) ||e_N||^2 = \\sum_{q=1}^L \\langle (e_{N,q} - W_{out,N+1,q}g_{N+1}), (e_{N,q} - W_{out,N+1,q}g_{N+1})\\rangle  - (r + \\mu_{N+1}) \\langle e_{N,q}, e_{N,q} \\rangle = (1 - r - \\mu_{N+1}) ||e_N||^2 - \\sum_{q=1}^L (2 \\langle e_{N,q}, W_{out,N+1,q}g_{N+1} \\rangle - \\langle W_{out,N+1,q}g_{N+1}, W_{out,N+1,q}g_{N+1} \\rangle) = (1 - r - \\mu_{N+1}) ||e_N||^2 - \\sum_{q=1}^L ((W_{out,N+1,q}g_{N+1}, W_{out,N+1,q}g_{N+1} \\rangle + 2C \\langle W_{out,N+1,q}, W_{out,N+1,q} \\rangle = (1 - r - \\mu_{N+1}) ||e_N||^2 - \\sum_{q=1}^L \\frac{\\langle e_{N,q}, g_{N+1} \\rangle^2}{(||g_{N+1}||^2+C)^2}/(||g_{N+1}||^2+2C) = \\delta_{N+1} - \\sum_{q=1}^L \\frac{\\langle e_{N,q}, g_{N+1} \\rangle^2}{(||g_{N+1}||^2+C)^2}/(||g_{N+1}||^2+2C) < \\delta_{N+1} - \\frac{\\langle e_{N,q}, g_{N+1} \\rangle^2}{(\\frac{b_g^2+C}{b_g^2+2C})} = 0.\nAccording to (14), we have $||e_{N+1}||^2 - (r + \\mu_{N+1}) ||e_N||^2 \\le 0$. Notably, the global regularized least squares method is used to calculate the output weight in our proposed algorithm. The output weight $W_{out}$ can be evaluated by\n$min_{W_{out}} \\frac{1}{2} ||W_{out} \\chi_{N+1} - \\tilde{Y}||^2 + C ||W_{out}||^2,$\n$W_{out}^* = (\\chi_{N+1}\\chi_{N+1}^T + CI)^{-1} \\chi_{N+1}\\tilde{Y}^T,$\nwhere $\\chi_{N+1}$=[($g_{N+1}$(1) ; $U_R$(1)),..., ($g_{N+1}$($n_{max}$) ; $U_R$($n_{max}$))], C is the regularization coefficient, I is an identity matrix, and the model output is $Y_{RSCN-L2} = W_{out}\\chi_{N+1}$.\nThen, we can obtain the optimum output weight $W_{out, N+1}= [W_{out,1}, W_{out,2}, ..., W_{out,N+K}]$ and residual error $\\tilde{e}_{N+1}$ by combining (17) and (18). It is easily inferred that $||\tilde{e}_{N+1}||^2 \\le ||e_{N+1}||^2$. Thus, the following inequality holds:\n$||\\tilde{e}_{N+1}||^2 \\le r ||e_N||^2 + \\gamma_{N+1},$\nwhere $\\gamma_{N+1} = \\mu_{N+1} ||e_N||^2 \\ge 0$. Note that $lim_{N\\rightarrow\\infty} \\gamma_{N+1} = 0$. From (19), we have $lim_{N\\rightarrow\\infty} ||\\tilde{e}_{N+1}||^2 = 0$, which completes the proof."}, {"title": "C. Online adjustment of learning parameters", "content": "To respond to the rapid changes in the system, it is necessary to adjust the model's learning parameters based on real-time data. This subsection elaborates on the online update of network parameters based on the projection algorithm [28].\nGiven a regularized RSCN model, at n-th step, let $\\hat{g}(n) = (x_N(n), U_R(n))$, where $x_N(n)$ denote the internal state of the reservoir with N nodes. Determine $W_{out}(n)$ to minimize the following cost function:\n$J = \\frac{1}{2} ||W_{out}(n) - W_{out}(n - 1)||^2 \\\\ s.t. \\ y(n) = W_{out}(n - 1) \\hat{g}(n)$\nBy incorporating the Lagrange multiplier $\\Lambda_p$, we can obtain\n$J_e = \\frac{1}{2} ||W_{out}(n) - W_{out}(n - 1)||^2 + \\Lambda_p [y(n) - W_{out}(n - 1) \\hat{g}(n)].$\nTaking the derivative of (21) with respect to $W_{out}(n)$ and $\\Lambda_p$, yields\n$\\begin{cases}W_{out}(n) - W_{out}(n - 1) - \\Lambda_p \\hat{g}(n) = 0 \\\\y(n) - W_{out}(n - 1) \\hat{g}(n) = 0\\end{cases}$,\n$\\Lambda_p = \\frac{y(n) - W_{out}(n - 1) \\hat{g}(n)}{\\hat{g}(n) \\hat{g}(n)}$.\nSubstituting (23) into (22), we have\n$W_{out}(n) = W_{out}(n - 1) + \\frac{\\hat{g}(n)}{\\hat{g}(n) \\hat{g}(n)} (y(n) - W_{out}(n - 1) \\hat{g}(n)).$\nRemark 3: This online adjustment enables the network to quickly adapt to dynamic unknown data. Moreover, we have analyzed the stability and convergence of the proposed scheme and presented an enhanced condition to further improve the model's stability in [22]."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, the effectiveness of our proposed method is verified on a nonlinear system identification task and two industrial data modelling problems. Performance comparisons are conducted among several models, including the original ESN, RSCN, and their variants with L1 and L2 regularization, termed ESN-L1,2, and RSCN-L1,2. Furthermore, we also consider variants processed with LASSO, denoted as LASSO-ESN, LASSO-RSCN, LASSO-ESN-L2, and LASSO-RSCN-L2. The training and testing performance of these models is assessed using the normalized root mean square error (NRMSE), which is formulated as follows:\n$NRMSE = \\sqrt{\\frac{\\sum_{n=1}^{N_{max}} (y(n) - t(n))^2}{N_{max} var(t)}},$\nwhere var (t) denotes the variance of the desired output. In addition, the cross-validation is employed in the experiments to obtain the optimal $C_L$ in (11). We calculated the sum of the absolute values of the variable feature coefficients corresponding to different $C_L$, that is,\n$P = \\sum_{k=1}^K \\sum_{d=1}^{d_k} P_d^{k},$\nwhere K is the number of input variables, $d_k$ is the corresponding order, $P_d^{k}$ denotes the feature coefficient associated with the d-th order of the k-th variable. Then, the regularization coefficient that maximizes P is determined, and the corresponding order variables are selected.\nTo determine certain hyperparameters, a series of preliminary experiments was conducted to establish the corresponding value ranges. Following this, the grid search method was used to select the optimal network parameters. By systematically exploring the predefined hyperparameter space, we can identify the parameter combinations that optimize model performance. The parameter ranges for various modelling tasks are presented in Table I. Each experiment consisted of 50 independent trials conducted under identical conditions to ensure the repeatability and reliability of the results. Moreover, we recorded the training and testing NRMSE, as well as the training time. Model performance is evaluated based on the mean and variance of these metrics.\nSpecifically, for the RSC frameworks, the following parameters are utilized: the weight scale sequence is defined as {0.1, 0.5, 1, 5, 10, 30, 50, 100}, the contractive sequence is given by r = [0.9, 0.99, 0.999, 0.9999, 0.99999], the maximum number of stochastic configurations is set to $G_{max} = 100$, the training tolerance is specified as $\\varepsilon = 10^{-5}$, and the initial reservoir size is set to 5."}, {"title": "A. Nonlinear system identification", "content": "In this simulation, the nonlinear dynamic system is expressed as\n$y(n + 1) = \\frac{y(n)y(n-1)y(n-2)u(n-1)(y(n-2)-1+u(n))}{1+y(n-1)y(n-1)+y(n-2)y(n-2)}.$\nThe input u (n) is generated from the uniform distribution [-0.7,0.7] in the training phase, and the initial output values are set to y (1) = y (2) = y (3) = 0, y (4) = 0.1. In the testing phase, the input is generated by\n$u(n) = \\begin{cases} 0.3 * sin(\\frac{n}{25}) +0.2 * sin(\\frac{n}{10}) & 0 < n < 500\\\\ 0.6* sin(\\frac{n}{25})& 500 \\le n \\le 800\\end{cases}$,\nand the initial output values are y (1) = \u22120.3, y (2) = \u22120.1, y (3) = 0.3, y (4) = 0. A total of 2000, 1000, and 800 samples are utilized for training, validation, and testing, respectively. The first 10 samples of each set are washed out.\nFor the original ESN, RSCN and their regularized variants, the plant output y (n+1) is predicted by [y (n), u (n)]. For the LASSO-based frameworks, it is crucial to first extract important order information. As illustrated in Fig. 2, we establish ten-order delays and compute the feature coefficients. The results clearly indicate that the feature coefficient of the order variable u (n) is significantly higher than those of the other input orders. This suggests that the current input exerts a more substantial influence on the system output, playing a dominant role in output prediction. Further analysis of these feature coefficients will enhance our understanding of the specific contributions of different order inputs to the system's behavior, thereby providing valuable insights for model selection and optimization.\nThe prediction fitting curves and error values generated by various models for the nonlinear system identification tasks are shown in Fig. 3. It is obvious that the LASSO-RSCN-L2 outperforms other methods in fitting the desired outputs, with the smaller error range. These findings highlight the effectiveness of our proposed approach in modelling nonlinear dynamic systems."}, {"title": "B. Soft sensing of the butane concentration in the debutanizer column process", "content": "The debutanizer column is an important equipment in the naphtha fractionation process, the main role is to separate butane (C4) from naphtha, the specific operation flow is shown in Fig. 5. In industrial processes, butane concentration needs to be measured in real-time to minimize the butane content at the bottom of the tower. However, butane concentration is often difficult to detect directly and incurs a certain delay. It is necessary to establish an accurate soft sensing model. Table III displays the relevant auxiliary variables in the production process. In [29", "1500": "are chosen"}]}