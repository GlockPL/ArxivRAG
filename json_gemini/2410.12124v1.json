{"title": "Affordance-Centric Policy Learning:\nSample Efficient and Generalisable Robot Policy Learning using\nAffordance-Centric Task Frames", "authors": ["Krishan Rana", "Jad Abou-Chakra", "Sourav Garg", "Robert Lee", "Ian Reid", "Niko S\u00fcnderhauf"], "abstract": "Affordances are central to robotic manipulation, where most tasks can be simplified to interactions with task-specific regions on objects. By focusing on these key regions, we can abstract away task-irrelevant information, simplifying the learning process, and enhancing generalisation. In this paper, we propose an affordance-centric policy-learning approach that centres and appropriately orients a task frame on these affordance regions allowing us to achieve both intra-category invariance \u2013 where policies can generalise across different instances within the same object category \u2013 and spatial invariance which enables consistent performance regardless of object placement in the environment. We propose a method to leverage existing generalist large vision models to extract and track these affordance frames, and demonstrate that our approach can learn manipulation tasks using behaviour cloning from as little as 10 demonstrations, with equivalent generalisation to an image-based policy trained on 305 demonstrations. We provide video demonstrations on our project site: affordance-policy.github.io.", "sections": [{"title": "1 Introduction", "content": "Vision-based robotic manipulation is essential for enabling autonomous robots to operate effectively in unstructured, everyday environments. These environments present numerous challenges, particularly when tasks involve interacting with objects that vary in spatial positioning and exhibit intra-category differences in visual appearance, shape, and size. Recent advances in behaviour cloning [1, 2, 3, 4] have shown promise in enabling robots to learn complex visuomotor policies in these settings by directly mapping raw visual inputs to motor actions without the need for manual feature engineering. However, such end-to-end approaches are highly sensitive to covariate shifts [5, 6, 7, 8, 9] and often overfit to task-irrelevant information present in the images, such as specific visual appearances, object locations, or environmental distractors. This overfitting leads to poor generalisation when the robot encounters new task settings or variations not present in the training data. Consequently, current efforts to mitigate these challenges have focused on collecting large-scale datasets [10, 11, 12, 13, 14, 15] that aim to capture all potential variations, which is both resource-intensive and impractical.\nIn this work, we seek to address the generalisation challenge for robotic manipulation without the need for large, exhaustive datasets. More specifically, we propose an approach to address both the spatial and intra-category object generalisation capabilities of behaviour cloning. We build on recent advances in representation learning for robotic manipulation which have shown that local, affordance-centric keypoints can yield significant intra-category generalisation across a wide range of open-loop manipulation tasks [16, 17, 18, 19, 20], and propose a novel closed-loop behaviour"}, {"title": "2 Related Work", "content": "Generalisation in Behaviour Cloning: There has been a recent resurgence in behaviour cloning methods given advances in generative modelling and the ability to capture complex multi-modal behaviours from demonstrations [1, 2, 3, 4]. Behaviour cloning typically learns to map input states either directly or indirectly to actions with images becoming the most ubiquitous state representation given the simplicity and generality they provide for open-world behaviour learning. The key challenge faced by these systems is covariate shift as the test distribution varies from the data that the policy was trained on [5, 6, 7, 8, 9]. Small changes in the input state representation can be detrimental to the performance of the trained policy, which is further exacerbated by the high-dimensional and potential variations that could be exhibited in image-based representations [23]. The ability to generalise to these variations is currently being extensively explored by the robot learning commu-"}, {"title": "3 Problem Formulation", "content": "We focus on behaviour cloning for robotic manipulation tasks, given access to a set of $N$ demonstrations $D = {T_i}^N_{i=1}$, where each trajectory $T_i$ is demonstrated through teleoperation. The objective is to learn a policy that effectively mimics these actions with the ability to generalise to unseen settings. Achieving generalisation in behaviour cloning is challenging due to the variability in un-structured environments. Specifically, our goal is to address two key challenges: spatial generalisation, which involves handling different object placements, and intra-category generalisation, which deals with variations across different instances of the same object category. While relative task frames have shown promise for spatial generalisation, and local key points have been successful for intra-category generalisation, we seek a unified framework for policy learning that combines these two ideas while addressing their respective limitations. Key questions we seek to answer in-"}, {"title": "4 Affordance-Centric Policy Learning", "content": "We propose a unified framework for generalisable policy learning using relative task frames. More specifically, we centre our relative task frame at local affordance-centric regions on objects. The reasons for this are two-fold: 1) These affordance regions holistically capture the interaction points for a wide range of manipulation tasks allowing us to fully define the state of an object based on the SE(3) pose of the frame and 2) this region is invariant across object instance variations including visual appearance, shape and size, allowing for intra-category behaviour generalisation. We describe a general pipeline to obtain these frames and detail how we utilise this frame for policy learning in the following sections."}, {"title": "4.1 Affordance Frame Detection and Tracking", "content": "We leverage the impressive number of generalist vision foundation models that are readily available from the vision community to both detect and track the required affordance-centric frames. These models typically exhibit open-world generalisation with the ability to operate over a vast range of object instances. A complete visual overview of our perception pipeline is given in Figure 1. For a given task, we first identify the object that the robot will be operating on and pass this text description together with an image of the scene as input to Grounding DINO [49]. This model provides a bounding box around the object of interest. We pass this bounding box to SAM [25] to obtain a segmentation mask of the object in the image which we use to initialise Foundation Pose [24]. For Foundation Pose, we additionally require a textured mesh of the object of interest. While there are several ways in which this can be obtained we found CSM Cube's [50] Image-to-3D model worked well in most cases where the object exhibited a repeat pattern or uniform colour in all directions. We appropriately scale this mesh within CSM Cube. Using the mask and textured mesh as input, we obtain a pose estimate of the object using Foundation Pose which is centred at the origin of the input mesh.\nTo transform this frame to the affordance region\nof the object, we utilise DINO-ViT [51] to re-\ntrieve the centroid of the affordance region us-\ning a stored set of reference points for that par-\nticular object. The affordance frame is obtained\nby simply translating the object pose frame to\nthis region. Once initialised, we continue to\ntrack this frame using Foundation Pose at 20Hz."}, {"title": "4.2 Oriented-Affordance Frame", "content": "Learning a policy relative to a reference frame\nis conceptually straightforward but challenges\narise when the frame is subject to free rota-\ntion during interaction. In our context, this\nissue becomes particularly pronounced as we\ndeal with continuous closed-loop interaction\nwith objects, as opposed to one-step, open-loop\ntasks like grasping. The reference frame can\nchange dynamically throughout the task, espe-\ncially in non-prehensile manipulations such as\npushing. These rotational changes are prob-\nlematic when using a fixed-base robot, as un-"}, {"title": "5 Experiments", "content": "For any given manipulation task we can decompose the task into a series of affordance-centric sub-tasks, where the policy is trained to act within a local affordance frame. This compositionality of affordance-centric polices allows us to solve long-horizon tasks by chaining a series of affordance-centric policies. To this end, we focus our experiments and demonstrations across 3 different long-horizon, real-world tasks (Figure 4) that exhibit a series of affordance-centric sub-tasks. We describe each task below:\n1) Tea Serving: This task involves 7 sub-tasks and 5 different objects including a teacup, saucer, teaspoon, teapot, and sugar basin. This task requires non-prehensile manipulation when rotating the cup, and delicate closed-loop movements across all subtasks to ensure the real porcelain objects used would not break.\n2) Shoe Racking: This task involves 6 sub-tasks and 3 different objects including a left shoe, a right shoe, and a shoe rack. This task requires non-prehensile manipulation to push the shoes together and precise multi-object grasping to pick the two shoes up together.\n3) Coffee Making: This task involves 7 sub-tasks and 4 different objects including a coffee pod, coffee mug, coffee machine and lid. This task requires articulated closed-loop object manipulation to open/close the lid and precise placement of the pod in the machine."}, {"title": "5.1 Experimental Setup", "content": "Task Description: We focus our key set of evaluations on the tea-serving task. This task consists of 5 different objects including a teacup, saucer, teapot, sugar basin and teaspoon. For each object, we identify the set of affordance-centric regions which could be used across various different tasks as shown in Figure 5. Based on these affordance frames, we decompose the full tea-making task into 7 different sub-tasks. This task decomposition is done manually based on a series of unique interactions between the robot's tooltip and an affordance frame. Each unique tool-frame affordance-frame interaction is denoted as a single affordance-centric sub-task. The full long-horizon task decomposition is given in Figure 5 (left) and we identify both the affordance and tool frames which define each subtask in Figure 5 (right). To thoroughly evaluate our ability to learn sample efficient and generalisable sub-policies and our ability to compose these sub-policies across varying levels of long-horizon task complexity, we conduct all evaluations across the task hierarchy shown in Figure 5.\nPerception System: To evaluate the different components of our proposed system we utilise 2 different perception systems:\n1) Marker-based: To evaluate the utility of oriented affordance-centric task frames for behaviour cloning, we decouple our results from the performance of the perception system by utilising ground truth detection and tracking of these frames in the form of fiducial markers (ArUco) placed at the affordance centric regions on objects.\n2) Large Vision Models: Our proposed pipeline for detecting and tracking affordance frames using a series of pre-trained large vision models as illustrated in Figure 1.\nPolicy Training: We utilise Diffusion Policy [1] for imitation learning and train each policy for 4500 epochs with the same default parameters provided in the original implementation [1]. The state space for all the affordance-centric policies comprises a 16-D vector consisting of the 3-D position of the robot's tool frame, a 6-D representation [52] of the tool frame and object rotation, and the 1-D gripper state. For all the baselines, we additionally provide the position of the object resulting in a 19-D state vector. The action space for all methods is the same and comprises a 11-D vector consisting of the 3-D position of the robot's end effector, a 6-D representation [52] of its rotation, the 1-D gripper action and the 1-D policy's self-progress.\nEvaluation Methodology: A key goal of this paper is to explore how affordance-centric policy learning enables both better sample efficiency and generalisation for manipulation tasks. To this end, we limit our training of all sub-policies to only 10 demonstrations. This constraint allows us to better understand the generalisation capabilities of our method in the low data regime while"}, {"title": "6 Results", "content": "We summarise our evaluation of each sub-policy in Table 1 and our compositional evaluation when solving extended tasks in Table 2. Across all evaluations, our oriented affordance frame consistently outperforms all alternative methods across both learning individual sub-tasks and when composing these policies to solve long horizon tasks with an average success rate of 83.1% in the (OOD) for each individual sub-policy and 70.2% in the compositional setting. We note here that all evaluations are focused on the low data regime with each policy only trained with 10 demonstrations, however, the overall performance of our method could be significantly increased to near 100% success across all sub-tasks by increasing the number of demonstrations to only >30 as shown in Figure 6."}, {"title": "6.0.1 Key Findings", "content": "i) Sample efficiency As shown in Table 1, across all tasks, the affordance-based policy demonstrated the high-est task performance when compared to both a relative and global frame. The task oriented nature of this frame allows the demonstrations to be con-centrated around critical regions for manipulation success which the other frames do not induce without significant data overhead. As shown in Figure 6 we contrast the ability of our method to learn a spatially invariant policy from the equivalent of just 10 demonstrations for the cup rearrangement task when compared to a similar image-based policy [47] which required training on 305 demonstrations\u00b9.\nii) Spatial generalisation Figure 7 illustrates the start configurations used for Training each policy and the OOD start state spatial variations used during Evaluation. Across all this unseen spatial variations, our affordance-based policy was able to achieve a success rate of atleast 80% which the global frame-based policy failed entirely in this setting."}, {"title": "iii) Oriented Affordance Frame Simplifies Data Collection", "content": "By orienting the affordance frame towards the start location of the robots tool-frame we can structure data collection to a small region of the work space as shown in the left column of Figure 7 while still being able to operate across the entire robot task space at test time. Furthermore, the need for only 10 demos, and the low dimensionality of the state allowed us to collect demonstrations for a task in 15 minutes and have a diffusion policy trained within the next 20 minutes enabling faster training-evaluation cycles for behaviour cloning.\niv) Reduced joint limit violations While relative ac-tion frames provide the ability to generalise policies to new spatial configurations, we found that the end effector might rotate to achieve a desired pose relative to the object without considering the robot's base orientation. This often led to awkward and constrained configurations, re-sulting in joint limit violations. These violations were a common occurrence that led to failed trials when evaluat-ing the end-effector or affordance-centric baseline, particularly in tasks where the robot needed to rotate the object as shown in Figure 6 (left) and Table 8. Our key insight from these observations is the critical importance of the oriented affordance frame. By anchoring the movements of the end-effector relative to the object's affordance frame, the robot can avoid excessive rotations and unwanted configurations."}, {"title": "v) Intra-category invariance for imitation learning", "content": "By attaching our relative frame for imitation learning at the affordance-centric regions of an object, we gain the ability to transfer our trained pol-icy across a wide range of intra-category variations which share the same affordances. We illustrate this in Figure 9 where we train both the cup rearrangement and teapot utilisation tasks on a single cup and teapot set as shown in the right panels. The same trained policy was evaluated across a wide range of variations ranging from colour, shape and size, with each policy achieving almost a perfect success rate. Allowing the tool-frame state of the policy to vary based on the object's shape played an important role in generalising the policy to larger intra-category variations where the cup was significantly smaller or the spout of the tea-pot retracted significantly as shown in the bottom right of Figure 9.\nvi) Applicability to Mobile Manipulation By training our policy with respect to a relative frame attached to an object, the robot's action and state space remain consistent regardless of the position of the robot's base. This allows for the policy to continue operation while the base of the robot is in motion. We demonstrate this by running the same policy trained in the tabletop setting on a mobile manipulator robot and show how the robot can maintain task performance regardless of the movement of the robot's base as illustrated by the discrepancy between the green and red robot base locations in Figure 10.\nvii) Offloading Generalisation to Large Vision Models Overall, we demonstrate that the generalist capabilities of large vision models enable us to obtain effective state abstractions for policy learning without the need for custom, narrow perception modules. By appropriately sequencing these models (Figure 1) we were able to obtain performance close to that of ground-truth perception systems as shown in Table 8. This demonstrates that the ability to circumvent the need for extensive robot data can be achieved by appropriately leveraging the existing generalist vision systems already available."}, {"title": "7 Conclusion", "content": "We propose an affordance-centric policy learning framework and a general pipeline that detects and tracks affordances using pre-trained large vision models. Affordances offers significant advantages, including invariance to robot and object poses, robustness to task-irrelevant visual attributes, and flexibility in behaviour composition across various scene configurations. To achieve these invari-ances while ensuring reliable deployment across diverse object configurations, we introduce the oriented affordance frame as an effective method for anchoring relative task frames. Experimental results show substantial improvements in sample efficiency and generalisation, significantly simpli-fying the data requirements for generalisable behaviour cloning.\nLimitations: Our system still faces several limitations. Our reliance on perception systems means that policy performance depends heavily on their robustness in real-world scenarios, particularly for tracking under clutter, where current methods still face challenges. Future work could explore advanced 3D tracking techniques [53] to mitigate these issues. Furthermore, our approach is not directly applicable to non-rigid objects, requiring additional state information. The pose-based ab-straction may also limit its application to tasks requiring finer details, necessitating additional sen-sory modalities such as tactile sensing. Despite these limitations, our method represents a significant step toward more efficient and generalisable behaviour cloning for complex manipulation tasks."}], "equations": ["d \\leftarrow p_{tool} - p_{afford}", "d_{norm} \\leftarrow \\frac{d}{||d||}", "r \\leftarrow v_{funnel} \\times d_{norm}", "r_{norm} \\leftarrow \\frac{r}{||r||}", "cos(\\theta) \\leftarrow v_{funnel} \\cdot d_{norm}", "sin(\\theta) \\leftarrow ||r||", "K \\leftarrow \\begin{bmatrix}\n0 & -r_z & r_y \\\\\nr_z & 0 & -r_x \\\\\n-r_y & r_x & 0\n\\end{bmatrix}", "R_{align} \\leftarrow I + sin(\\theta)K + (1 - cos(\\theta))K^2"]}