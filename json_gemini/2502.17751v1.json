{"title": "GRADED NEURAL NETWORKS", "authors": ["TONY SHASKA"], "abstract": "This paper presents a novel framework for graded neural networks (GNNs) built over graded vector spaces Va, extending classical neural architectures by incorporating algebraic grading. Leveraging a coordinate-wise grading structure with scalar action x * x = (\u03bbqixi), defined by a tuple q = (q0,..., qn-1), we introduce graded neurons, layers, activation functions, and loss functions that adapt to feature significance. Theoretical properties of graded spaces are established, followed by a comprehensive GNN design, addressing computational challenges like numerical stability and gradient scaling. Potential applications span machine learning and photonic systems, exemplified by high-speed laser-based implementations. This work offers a foundational step toward graded computation, unifying mathematical rigor with practical potential, with avenues for future empirical and hardware exploration.", "sections": [{"title": "1. INTRODUCTION", "content": "Artificial neural networks are pivotal in artificial intelligence, tackling diverse problems from applied mathematics to pattern recognition. Typically, these networks model functions f: kn \u2192 km (often k = R), with coordinates of v \u2208 kn as input features and f(v) as output features. While standard architectures treat features uniformly, many scenarios\u2014such as document analysis or algebraic geometry-reveal inputs with varying significance, suggesting a graded approach where each feature xi in v = (x0,...,Xn-1) carries a grade gr(xi) \u2208 I. Such structures align with graded vector spaces, where coordinates are assigned values from a set I, a concept we explore to extend neural computation.\nOur motivation stems from the moduli space of genus two curves, isomorphic to a subspace of the weighted projective space P(2,4,6,10) with grades q = (2, 4, 6, 10). In (Shaska and Shaska, 2024), neural networks predicting automorphism groups or (n,n)-split Jacobians of these curves achieved 40% accuracy with raw coefficients, but soared to 99% using graded invariants as inputs. This leap, while expected mathematically (as P(2,4,6,10) captures isomorphism classes), prompts a deeper question: does grading inherently enhance performance? Parallel insights from (Beshaj et al., 2020; Salami and Shaska, 2023; 2024) show weighted heights in graded projective spaces outpace standard height computations, hinting that graded norms and operations may simplify complex tasks\u2014a hypothesis we test by designing neural networks over graded spaces.\nGraded vector spaces, detailed in Section 2, generalize Rn by assigning grades via gr(xi) = qi, with scalar action \u03bb*x = (\u03bbqixi). In Section 3, we construct graded neural networks (GNNs), adapting neurons (\u2211 wrxi +b), activations (ReLuz(xi) =\nmax{0, |xi|1/qi}), and losses (e.g., \u03a3qi|yi \u2013 \u0177i|2) to this structure. Unlike classical models, GNNs naturally handle graded inputs, reverting to standard networks"}, {"title": "2. GRADED VECTOR SPACES", "content": "Here we give the bare minimum background on graded vector spaces. The interested reader can check details at (Bourbaki, 1974), (Roman, 2008), (Koszul, 1983), among other places. We use \"grades\" to denote the indices of grading (e.g., qi), distinguishing them from \"weights\" used for neural network coefficients in Section 3.\nA graded vector space is a vector space with an extra grading structure, typically a decomposition into a direct sum of subspaces indexed by integers. While we present the traditional decomposition V = \u2295n\u2208N Vn and the coordinate-wise form Va(k) = kn with scalar action \u03bb *x = (\u03bbqixi), these definitions are equivalent, as the latter can represent the former via a basis choice, a perspective we adopt for neural networks in Section 3. For this paper, we focus on graded vector spaces indexed by integers, though we also define them for a general index set I below.\nLet N be the set of non-negative integers. An N-graded vector space, often simply a graded vector space without the prefix N, is a vector space V with a decomposition:\nV =\n\u2295n\u2208N Vn,\nwhere each Vn is a vector space. Elements of Vn are called homogeneous elements of degree n.\nGraded vector spaces are common. For example, the set of all polynomials in one or several variables forms a graded vector space, where homogeneous elements of degree n are linear combinations of monomials of degree n."}, {"title": "2.1. Integer Gradation.", "content": "Example 1. Let k be a field and consider V(2,3), the space of homogeneous polynomials of degrees 2 and 3 in k[x,y]. It decomposes as V(2,3) = V2 + V3, where V2 is the space of binary quadratics and V3 the space of binary cubics. For u = [f,g] \u2208 V2 \u2295 V3, scalar multiplication is:\n\u03bb * u = \u03bb * [f, g] = [\u03bb2f, \u03bb3g],\nreflecting grades 2 and 3. We use this example repeatedly throughout the paper.\nNext is an example motivating machine learning models over graded vector spaces; see (Shaska and Shaska, 2024).\nExample 2 (Moduli Space of Genus 2 Curves). Assume char k \u2260 2 and C a genus 2 curve over k, with affine equation y2 = f(x), where f(x) is a degree 6 polynomial. The isomorphism class of C is determined by its invariants J2, J4, J6, J10, homogeneous polynomials of grades 2, 4, 6, and 10, respectively, in the coefficients of C. The moduli space of genus 2 curves over k is isomorphic to the weighted (graded) projective space P(2,4,6,10),k."}, {"title": "2.2. Graded Linear Maps.", "content": "For an index set I, a linear map f: V \u2192 W between I-graded vector spaces is a graded linear map if it preserves the grading, f(Vi) \u2286 Wi, for all i \u2208 I. Such maps are also called homomorphisms (or morphisms) of graded vector spaces or homogeneous linear maps. For a commutative monoid I (i.e., N), maps homogeneous of degree i \u2208 I satisfy:\nf(Vj) \u2286 Wi+j, for all j\u2208 I,\nwhere + is the monoid operation. If I embeds into an abelian group A (i.e., Z for N), maps of degree i \u2208 A follow the same property, with + as the group operation. A map of degree -i satisfies:\nf(Vi+j) \u2286 Wj, f (Vj) = 0 if j \u2212 i \u2209 I.\nExample 3. For V(2,3) = V2 + V3, a linear map L : V(2,3) \u2192 V(2,3) satisfies:\nL([\u03bb *u]) = L([\u03bb2f, \u03bb3g]) = [\u03bb2L(f), \u03bb3L(g)] = \u03bb* [L(f), L(g)] = \u03bb * L(u),\nL([f, g] + [f', g']) = L([f + f',g + g']) = [L(f) + L(f'), L(g) + L(g')]\n= [L(f), L(g)] + [L(f'), L(g')] = L([f, g]) \u2295 L([f', g']).\nUsing the basis\nB = {x2, xy,y2,x3,x2y, xy2, y3},\nwhere B1 = {x2,xy,y2} spans V2 and B2 = {x3, x2y, xy2, y3} spans V3, the polynomial\nF(x,y) = (x2 + xy + y2) + (x3 + x2y + xy2 + y3)\nhas coordinates u = [1, 1, 1, 1, 1, 1, 1]t."}, {"title": "2.3. Operations over Graded Vector Spaces.", "content": "Having established the structure of graded vector spaces in Section 2, we now define operations that extend their utility: the direct sum, tensor product, and dual space. These operations, rooted in the grading, are relevant to potential applications in graded neural networks, such as feature composition or optimization.\nFor I-graded spaces V = \u2295i\u2208I Vi and W = \u2295i\u2208I Wi, the direct sum is V \u2295 W with gradation:\n(V\u2295 W)i = Vi \u2295 Wi."}, {"title": "2.4. Inner Graded Vector Spaces.", "content": "Consider now the case when each Vi is a finite-dimensional inner space, and let \u3008\u00b7,\u00b7)i denote the corresponding inner product. Then we can define an inner product on V = \u2295i\u2208I Vi as follows. For u = u1 +\n+ un and v = v1 + . . . + vn, where ui, vi \u2208 Vi, we define:\n\u27e8u, v\u27e9 = \u27e8u1, v1\u27e91 + . . . + \u27e8un, vn\u27e9n,\nwhich is the standard product across graded components. The Euclidean norm is then:\n||u|| =\n\u221au21 + ... + u2n,\nwhere ||ui||i = \u221a(ui, ui)i is the norm in Vi, and we assume an orthonormal basis for simplicity.\nIf such Vi are not necessarily finite-dimensional, then we have to assume that Vi is a Hilbert space (i.e., a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product). This case of Hilbert spaces is especially important in machine learning and artificial intelligence due to their role in functional analysis and optimization.\nObviously, having a norm on a graded vector space is crucial for machine learning if we want to define a cost function of some type. The simpler case of Euclidean vector spaces and their norms was considered in (Moskowitz, 2014; Sriwongsa and Wiboonton, 2020). However, graded structures allow for norms that reflect the grading, enhancing their utility in applications like neural networks over Va(k).\nExample 5. Let us continue with the space V(2,3) from Example 1, with bases\nB1 = {x2, xy, y2}\nfor V2 and\nB2 = {x3, x2y, xy2, y3}\nfor V3, as in Example 3. Hence, a basis for V(2,3) = V2 + V3 is\nB = {x2, xy, y2,x3,x2y, xy2, y3}.\nLet u, v \u2208 V(2,3) be given by:\nu = a + b = (u1x2 + u2xy + u3y2) + (u4x3 + u5x2y + u6xy2 + u7y3),\nv = a' + b' = (v1x2 + v2xy + v3y2) + (v4x3 + v5x2y + v6xy2 + v7y3) .\nThen:\n\u27e8u, v\u27e9 = \u27e8a + b, a' + b'\u27e9 = \u27e8a, a'\u27e92 + \u27e8b, b'\u27e93\n= u1v1 + u2v2 + u3v3 + u4v4 + u5v5 + u6v6 + u7v7,\nand the Euclidean norm is ||u|| = \u221auz + ... + uz, assuming B is orthonormal. This treats all grades uniformly, which may not fully leverage the graded structure.\nThere are other ways to define a norm on graded spaces, particularly to emphasize the grading. Consider a Lie algebra g called graded if there is a finite family of subspaces V1,..., Vr such that g = V1 \u2295 \u2295 Vr and [Vi, Vj] C Vi+j, where [Vi, Vj] is the Lie bracket. When g is graded, define a dilation for t \u2208 R, at: g\u2192 g, by:\nat(u1,..., ur) = (tu1, t2u2,..., tur).\nWe define a homogeneous norm on g as:\n||v|| = || (v1, ..., vr)|| = (||v1|| + ||v2||2\u22122 + + ||vr||2) 1/2r,"}, {"title": "3. GRADED NEURAL NETWORKS (GNN)", "content": "We define artificial neural networks over graded vector spaces, utilizing Section 2. Let k be a field, and for n \u2265 1, denote Ar (resp. Pr) as the affine (resp. projective) space over k, omitting the subscript if k is algebraically closed. A tuple q =\n(q0,..., qn-1) \u2208 Nn defines the grades, with gr(xi) = qi. The graded vector space Va(k) = kn has scalar multiplication:\n\u03bb * x = (\u03bbq0x0,..., \u03bbqn\u22121xn\u22121), x = (x0,..., xn-1) \u2208kn, \u03bb\u2208k,\nas in Section 2, denoted Va when clear. This scalar action, denoted \u03bbx, mirrors the graded multiplication in Section 2, applicable to both the coordinate form here and the direct sum form (e.g., \u03bb*[f, g]) via basis representation. A graded neuron on Vq is aq: Va \u2192 k such that\naq(x) =\n\u2211n\u22121\ni=0 wixi + b,\nwhere wi \u2208 k are neural weights, and b \u2208 k is the bias. For b\n\u2260 0,\naq(\u03bb*x) = \u2211(\u03bb\u03c9i)ixi = 1\u2211 wixi\nfor (wqi = wi), approximating a graded linear map of degree 1 per Section 2. With b\u2260 0, aq is affine, embedding grading via wiqi. A graded network layer is:\n\u03c6 : Va(k) \u2192 Va(k)\nx \u2192 g(Wx + b),\nwhere W = [wj,i] \u2208 kn\u00d7n, b = (b0,...,bn\u22121) \u2208 kn, and \u25ca preserves grading, with gr(yj) = qj.\nRemark 1. Neural weights wi or wj,i differ from grades qi. Exponents wi reflect grading, while qi define Va's action. We use w for weights, qi for grades.\nA graded neural network (GNN) is a composition of multiple layers given as\n\u0177 = \u03c6m \u25cb\u2026\u25cb \u03c61(x),\nwhere each layer \u03c6\u03b9(x) = g\u03b9(W\u00b9x + b\u00b9) applies a transformation defined by the matrix of neural weights W\u00b9 = [w]i,j, producing outputs \u0177 and true values y in Va\nwith grades gr(\u0177i) = qi.\n3.1. ReLU Activation. In classical neural networks, the rectified linear unit (ReLU) activation, defined as ReLu(x) = max{0,x}, applies a simple thresholding to promote sparsity and efficiency. However, for graded neural networks over Va, where x = (x0,...,xn\u22121) has coordinates with grades gr(xi) = qi and scalar action\n\u03bb*x = (\u03bbq0x0,..., \u03bbqn\u22121xn\u22121), a direct application of this ReLU ignores the grad-ing's intrinsic scaling. To adapt to this structure, we define a graded ReLU that adjusts nonlinearity by grade. For x \u2208 Van, the graded ReLU is:\nReLui(xi) = max{0, |xi|1/qi },\nand\nReLu(x) = (ReLu0(x0), ..., ReLun\u22121(xn\u22121)).\nUnlike the classical max{0, xi}, which treats all coordinates uniformly, this version scales each xi by 1/qi, reflecting the graded action. For \u03bb * x = (\u03bbqixi), compute:\nReLui(\u03bbqixi) = max{0, |\u03bbqixi|1/qi } = max{0, |\u03bb||xi|1/qi} = |\u03bb|max{0, |xi|1/qi},\nso ReLu(x*x) = |\u03bb| ReLu(x) for \u5165 > 0, aligning with Va's grading up to magnitude. This ensures the activation respects the differential scaling of coordinates (i.e., qi = 2 vs. qi = 3 in V(2,3)), unlike the classical ReLU, where ReLu(xi) = ReLu(xi) for x > 0 assumes homogeneity of degree 1.\nThis adaptation is motivated by the need to capture feature significance in graded spaces, as seen in applications like genus two curve invariants (J2, J4, J6, J10 with grades 2, 4, 6, 10). A classical ReLU might underweight high-graded features (i.e., J10) or overreact to low-graded ones (i.e., J2), whereas the graded ReLU normalizes sensitivity via 1/qi, akin to the homogeneous norm's scaling in Section 2. It also"}, {"title": "3.2. Graded Loss Functions.", "content": "In classical neural networks, loss functions like the mean squared error (MSE), L =\n\u2211n\u22121\ni=0 (yi \u2013 \u0177i)2, treat all coordinates equally, assuming a uniform vector space structure. However, on Va(k) = kn with grading gr(xi) = qi and scalar action \u03bb * x = (\u03bbq0x0,..., \u03bbqn\u22121xn\u22121), this approach overlooks the differential significance of coordinates (i.e., qi = 2 vs. qi = 10 in genus two invariants). Graded loss functions adapt to this structure by weighting errors according to qi, enhancing sensitivity to features of varying grades, as motivated by the improved accuracy in graded inputs observed in (Shaska and Shaska, 2024). The graded MSE on va is:\nLMSE(y, \u0177) =1/n\n\u2211n\u22121\ni=0 qi (yi - \u0177i)2,\nwhere y, \u0177 \u2208 Van are true and predicted values, and qi amplifies errors for higher-graded coordinates. Unlike classical MSE, this scales with grading: for \u03bb*(y\u2212y) =\n(\u03bbqi (yi \u2013 \u0177i)), LMSE(\u03bb*y, \u03bb * \u0177) = \u03a3qi\u03bb2qi (yi \u2013 \u0177i)2, reflecting Va's geometry. Alternatively, using the graded Euclidean norm from Section 2:\nLnorm(y, \u0177) = ||y - \u0177 ||2q = \u2211n\u22121\ni=0 qi |yi - \u0177i|2,\nomits the 1/n normalization, aligning directly with || ||q's definition.\nExample 8. For spaces like V(2,3) with Va = k7, we partition coordinates as y = (y2, y3), where y2 = (y0, y1, y2) \u2208 k3 corresponds to V2 (grade 2) and y3 =\n(y3, y4, y5, y6) \u2208 k4 to V3 (grade 3), matching the basis B from Example 1. The homogeneous loss leverages the homogeneous norm from Section 2:\nLhom(y, \u0177) = ||y \u2013 \u0177||6 = || (y \u2212 \u0177)2||2 + || (y \u2212 \u0177)3||3,"}, {"title": "3.3. Optimizers.", "content": "Optimizers adjust weights wj,i and bj to minimize a loss func-tion over Va. Consider L = Lnorm(y, \u0177) = ||y \u2013 \u0177||q, using the graded Euclidean norm from Section 2, where ||x||q = \u2211n\u22121\ni=0 qi|xi|2. The gradient with respect to \u0177, as derived in Section 2 (\"Norm Convexity and Gradient Behavior\"), is:\n\u2207\u0177L = 2(q0(\u01770 - y0),\u2026\u2026\u2026, qn\u22121(\u0177n\u22121 \u2014 yn\u22121)),\nreflecting the grading via qi. This gradient scales components by their grades, emphasizing higher-graded coordinates (i.e., qi = 3 in V3 of V(2,3)).\nBasic gradient descent updates parameters as:\nw+1j,i = wjt \u2212 \u03b7 \u2202L/\u2202wj,i,\nbt+1 = bt \u2212 \u03b7 \u2202L/\u2202bj,\nwhere \u03b7 > 0 is a step size, and partial derivatives are computed via the chain rule through \u03c6\u03b9, incorporating qi from wiqi and W\u00b9. For example, if \u0177j = \u03c6\u03b9(xj), \u2202L/\u2202wj,i = qiwxL/\u2202\u0177j, adjusting for grading.\nOther norms yield different gradients. For Lhom = ||y - \u0177||2r (i.e., r = 3 for V(2,3)), the gradient from Section 2 is nonlinear:\n\u2207\u0177L = 2r||y \u2013 \u0177||2r\u22126(|| (y - \u0177)2||2(\u01772 - y2), (\u01773 \u2013 y3)),"}, {"title": "4. THEORETICAL IMPLEMENTATION AND APPLICATIONS OF GRADED NEURAL\nNETWORKS", "content": "Having defined GNNs over Va in Section 3, we now explore their computational implementation and potential applications. This section first examines theoretical challenges arising from the graded structure, then highlights how this framework could extend to practical domains, leveraging its algebraic properties established in Section 2 and Section 3.\nThe graded scalar action \u03bb * x = (\u03bbqixi) in-troduces numerical stability concerns, as large qi amplify small \u5165, risking overflow or precision loss in finite arithmetic. For Va with q = (q0,..., qn\u22121), inputs must be normalized to mitigate this, yet balancing scales across grades remains non-trivial.\nNeuron computation aq(x) = \u2211wiqi xi + b and layers \u03c6\u03b9(x) = g\u03b9(W\u00b9x + b\u00b9) with W\u00b9 = [w\u00b9i,j] face complexity from exponentiation. For large qi, wqi,i grows exponentially if |wi| > 1, requiring careful weight initialization (e.g., wi < 1) or pre-computation, increasing memory demands. Sparse q may reduce this, but dense grading scales poorly with n.\nThe graded ReLU ReLui(xi) = max{0,|xi|1/qi} is sensitive to qi: small qi (e.g., 2) yield smooth outputs, while large qi (e.g., 10) flatten near zero, potentially reducing expressivity. This variability complicates uniform activation design across layers, unlike classical ReLU's consistency.\nLoss functions like Lnorm = \u2211qi |yi - \u0177i|2 amplify errors by qi, skewing opti-mization toward high-graded coordinates, while Lhom requires partitioning (e.g., k7 \u2192 V2, V3), adding preprocessing overhead. Gradients \u2207\u0177L = 2(qi(\u0177i - yi)) scale with qi, risking vanishing or exploding gradients for extreme qi, necessitating adaptive step sizes (e.g., \u03b7i \u221d q\u012b1) or normalization, per Section 3.\nThe graded structure of GNNs offers versatility across domains. In machine learning, assigning grades to features based on significance (e.g., genus two invariants with q = (2, 4, 6, 10)) enhances sensitivity, as seen in (Shaska and Shaska, 2024), potentially improving tasks like regression or classification where features vary in importance. Temporal signal processing could leverage grading to prioritize recent data (e.g., q = (1,2,3,...)), adapting loss functions like Lnorm to time-weighted errors.\nBeyond traditional computing, photonic implementations, such as laser-based systems, present intriguing possibilities. Recent advances emulate graded responses using quantum-dot lasers for high-speed reservoir computing (Nie et al., 2024Dec), achieving rates like 10 GBaud without feedback loops. GNNs' graded neurons"}, {"title": "4.1. Implementation Challenges.", "content": "4.2. Potential Applications."}, {"title": "5. CONCLUSIONS", "content": "This paper introduces a novel framework for graded neural networks (GNNs) over graded vector spaces Va, unifying algebraic grading from Section 2 with neural computation in Section 3. By defining neurons, layers, activations, and loss functions with grade-sensitive operations (e.g., \u03a3wqi xi, ReLui (xi) = max{0, |xi|1/qi }), we extend classical neural networks to capture feature hierarchies, as motivated by applications like genus two curve classification (Shaska and Shaska, 2024). The flexibility of q = (q0,\u2026\u2026\u2026, qn\u22121) enables tailored grading, distinct from uniform vector spaces.\nSection 4 underscores the theoretical challenges-numerical stability, computational complexity and potential applications, from machine learning to photonic systems. While implementation demands careful handling of large qi and gradient scaling, the framework's generality suggests broad utility. Future work could explore empirical validation across diverse datasets, optimization refinements, and hardware realizations, such as laser-based graded neurons, to harness GNNs' full potential. This foundation offers a stepping stone for advancing graded computation in both theory and practice."}]}