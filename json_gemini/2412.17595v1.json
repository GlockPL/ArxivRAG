{"title": "V2-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal Wireless Capsule Endoscopy", "authors": ["Long Bai", "Beilei Cui", "Liangyu Wang", "Yanheng Li", "Shilong Yao", "Sishen Yuan", "Yanan Wu", "Yang Zhang", "Max Q.-H. Meng", "Zhen Li", "Weiping Ding", "Hongliang Ren"], "abstract": "Deep learning can predict depth maps and capsule ego-motion from capsule endoscopy videos, aiding in 3D scene reconstruction and lesion localization. However, the collisions of the capsule endoscopies within the gastrointestinal tract cause vibration perturbations in the training data. Existing solutions focus solely on vision-based processing, neglecting other auxiliary signals like vibrations that could reduce noise and improve performance. Therefore, we propose V2-SfMLearner, a multimodal approach integrating vibration signals into vision-based depth and capsule motion estimation for monocular capsule endoscopy. We construct a multimodal capsule endoscopy dataset containing vibration and visual signals, and our artificial intelligence solution develops an unsupervised method using vision-vibration signals, effectively eliminating vibration perturbations through multimodal learning. Specifically, we carefully design a vibration network branch and a Fourier fusion module, to detect and mitigate vibration noises. The fusion framework is compatible with popular vision-only algorithms. Extensive validation on the multimodal dataset demonstrates superior performance and robustness against vision-only algorithms. Without the need for large external equipment, our V2-SfMLearner has the potential for integration into clinical capsule robots, providing real-time and dependable digestive examination tools. The findings show promise for practical implementation in clinical settings, enhancing the diagnostic capabilities of doctors.", "sections": [{"title": "I. INTRODUCTION", "content": "EACH year, over 28 million patients suffer from gas- trointestinal (GI) cancers [1]. It is the second most deadly cancer worldwide [2]. Furthermore, cancers in the GI tract tend to vary in symptoms. Diagnosing multiple diseases in the GI tract is quite challenging, because the symptoms among patients are usually different. As a result, physicians cannot use blood tests and symptoms alone to determine the condition and decide on the next treatment steps. The most direct and effective screening method for GI cancers is endoscopy, providing physicians with direct visual information [3]. Wireless capsule endoscopy (WCE) is a particular type of endoscopy. It does not have to penetrate deep into the GI environment through an external device but comes with its battery, antenna, and imaging equipment [4]. When the patient swallows the WCE, it can be controlled remotely (e.g., driven by an external magnetic field [5]) or move with the patient's own metabolism until it is expelled from the body, and collects sample information in the patient's GI environment for further analysis, diagnosis and treatment by the physicians. Compared with computerized tomography scanning and traditional endoscopy, WCE has been reported to have better diagnostic sensitivity, less pain and discomfort, and better tolerance during the treatment [6], [7]. Moreover, the flourishing development of deep learning (DL) techniques has further enhanced doctors' ability and speed in reviewing medical images [8]\u2013[10].\nOnce we have collected enough visual information and identified the presence of a lesion, the subsequent problem is to locate the lesion in the human body [11], [12]. Mag- netic localization methodologies have also been proposed to estimate the real-time WCE ego-motion, while (i) they require large external hardware and cause much more investment of resources [13], [14]; (ii) the localization accuracy is related to the working space. However, strict requirements exist re- garding the distance between WCE and magnetic sensors [15];"}, {"title": "II. RELATED WORK", "content": "Researchers first used traditional multiview stereo algo- rithms to generate the 3D GI scene and estimate the camera ego-motion, e.g., shape-from-motion (SfM) [42], and simulta- neous localization and mapping (SLAM) [43]. SfM has been applied on sinus surgeries with sparse bundle adjustment [42]. Besides, Grasa et al. [18] utilized SLAM to generate the 3D abdominal cavity reconstruction on monocular laparoscope image sequences. Mahmoud et al. [19] reported a fast, robust, and dense SLAM method with frame clusters that provide a larger parallax of parallax from the motion of the endoscopy. However, the light source in the endoscopy environment is usually insufficient, and the GI environment always changes with the body. The endoscopy images also lack sufficient distinguishable features. The above all make it increasingly difficult to extract features manually; in this case, plenty of mismatches will happen when conducting feature matching. Therefore, the performance of the existing multiview stereo algorithms is far from perfect.\nRecently, DL-based algorithms have attempted to infer depth and ego-motion information directly from large-scale WCE data. Turan et al. [20] designed Deep EndoVO, a monocular WCE visual odometry using recurrent networks. Mahmood et al. [44] targeted the poor contrast of lesion topography in the colon. They presented a supervised mono- endoscopy depth prediction with DL and conditional random field. However, as mentioned in Section I, since obtaining an abundance of labeled data in real WCE is quite challenging, supervised learning can hardly get practical applications. Sha- ran et al. [33] attempted to adapt a self-supervised solution in autonomous driving - Monodepth [27], for stereoscopic endoscopic depth estimation for mitral valve surgery. Ozy- oruk et al. [32] propounded unsupervised Endo-SfMLearner, combining residual networks and spatial attention to focus on distinguishable texture tissue features. Li et al. [31] further integrated the temporal features between consecutive frames to boost the performance. Moreover, Yang et al. [34] integrated convolutional neural networks and transformers for endoscopic depth estimation, while the works of He et al. [45] and Zhang et al. [46] discussed the development of lightweight methods for monocular endoscopic depth estimation. Cui et al. con- structed fully supervised [47] and self-supervised [48] depth estimation paradigms for endoscopy by fine-tuning natural foundation models [49], [50].\nAlthough current DL-based vision algorithms can overcome the difficulties of traditional methods in feature extraction, the existing algorithms still need to fully consider and utilize all available information. Recent works have revealed the feasibility and necessity of integrating multiple data modal- ities to further enhance the performance of estimation tasks with DL [51]. Nevertheless, based on our current search and reading, rarely researcher has ever considered introducing vibration signals to predict depth and ego-motion estimation for capsule robots. Abu et al. [35] has preliminarily validated the feasibility of fusing visual and inertial information for reconstructing 3D scenes in capsule endoscopy. However, they still lack a complete pipeline for 3D reconstruction and pose estimation, and their work has not been validated on large- scale datasets. Our fusion framework is expected to defend the vibration perturbations, establish a comprehensive prediction pipeline, and further improve the estimation performance."}, {"title": "III. METHODOLOGY", "content": "Self-supervised depth estimation mainly relies on the pho- tometric and geometric constraint between a target frame and a reprojected frame from the source frame to the target frame. Formally, with known camera parameters, the right images Ir are mapped to Ir' according to Equ. (1). The depth estimation neural network D is trained and optimized via Equ. (2), where the optimization objective is the difference between reprojected image Ir' and left image Il.\n\\begin{equation}\n\\begin{aligned}\nI_{r}^{\\prime} &=I_{r}\\left(D\\left(p^{r}\\right)+\\rho^{r}\\right) \\\\\nI_{\\ell}^{\\prime} &=I_{\\ell}\\left(D\\left(p^{\\ell}\\right)\\right)\n\\end{aligned}\n\\end{equation}\n\\begin{equation}\n\\theta^{*}=\\arg \\min _{\\theta} \\sum_{j} \\sum_{i} \\mathcal{L}\\left(I_{r}^{\\prime}\\left(p_{i}^{r} ; \\theta\\right), I_{\\ell}\\left(p_{i}^{\\ell} ; \\theta\\right)\\right)\n\\end{equation}\nHere, $p_i^\\ell$ and $p_i^r$ represent the $i$th pixel of the $j$th right view $I_j^r$ and left view $I_j^\\ell$, respectively. $\\theta$ is the network parameter in the depth estimation neural network $D(\\cdot)$. $\\mathcal{L}$ denotes the loss function, which will be described in Equ. (5)-(9). Subse- quently, based on the obtained optimal neural network $D^*(\\cdot)$, the depth $d_i$ of pixels $p_i$ in the image $I$ can be predicted with $d_i = (f \\cdot B)/D(p_i)$. $i$ represents $i$th pixel $p_i$ in the image $I$. $f$ represents the focal length, and $B$ is the inter-camera distance. f and B can be calculated with known camera parameters.\nNevertheless, for video-based monocular estimation, image pairs are not available. In this case, two continuous video frame images $I_t$ and $I_{t+1}$ at t moment and t + 1 moment will serve as the image pairs to feed the deep neural net- work. However, Equ. (1) does not hold for monocular depth estimation now because the camera pose will change with time t. Godard et al. [52] introduced an ego-motion estimation model P(\u00b7) predicting camera ego-motion: inter-frame spatial displacement (x, y, z) and angular movement (\u03c6, \u03b8, \u03c8). This method projects the previous frame's depth map to the next frame using ego-motion info. The required image pair becomes the projected previous frame and the current frame. Thus, the inter-frame depth map minimization is optimized through the ego-motion network, resolving inconsistent camera poses between frames."}, {"title": "A. Learning Depth Maps and WCE Motion", "content": "Self-supervised depth estimation mainly relies on the pho- tometric and geometric constraint between a target frame and a reprojected frame from the source frame to the target frame. Formally, with known camera parameters, the right images $I_{r}$ are mapped to $I_{r}^{\\prime}$ according to Equ. (1). The depth estimation neural network $D$ is trained and optimized via Equ. (2), where the optimization objective is the difference between reprojected image $I_{r}^{\\prime}$ and left image $I_{\\ell}$.\n\\begin{equation}\n\\begin{aligned}\nI_{r}^{\\prime} &=I_{r}\\left(D\\left(p^{r}\\right)+\\rho^{r}\\right) \\\\\nI_{\\ell}^{\\prime} &=I_{\\ell}\\left(D\\left(p^{\\ell}\\right)\\right)\n\\end{aligned}\n\\end{equation}\n\\begin{equation}\n\\theta^{*}=\\arg \\min _{\\theta} \\sum_{j} \\sum_{i} \\mathcal{L}\\left(I_{r}^{\\prime}\\left(p_{i}^{r} ; \\theta\\right), I_{\\ell}\\left(p_{i}^{\\ell} ; \\theta\\right)\\right)\n\\end{equation}\nHere, $p_{i}^{\\ell}$ and $p_{i}^{r}$ represent the $i$th pixel of the $j$th right view $I_{j}^{r}$ and left view $I_{j}^{\\ell}$, respectively. $\\Theta$ is the network parameter in the depth estimation neural network $D(\\cdot)$. $\\mathcal{L}$ denotes the loss function, which will be described in Equ. (5)-(9). Subse- quently, based on the obtained optimal neural network $D^{*}(\\cdot)$, the depth $d_{i}$ of pixels $p_{i}$ in the image $I$ can be predicted with $d_{i}=\\left(f \\cdot B\\right) / D\\left(p_{i}\\right)$. $i$ represents $i$th pixel $p_{i}$ in the image $I$. $f$ represents the focal length, and $B$ is the inter-camera distance.\nf and B can be calculated with known camera parameters.\nNevertheless, for video-based monocular estimation, image pairs are not available. In this case, two continuous video frame images $I_{t}$ and $I_{t+1}$ at t moment and t + 1 moment will serve as the image pairs to feed the deep neural net- work. However, Equ. (1) does not hold for monocular depth estimation now because the camera pose will change with time t. Godard et al. [52] introduced an ego-motion estimation model $P(\\cdot)$ predicting camera ego-motion: inter-frame spatial displacement (x, y, z) and angular movement $(\\varphi, \\theta, \\psi)$. This method projects the previous frame's depth map to the next frame using ego-motion info. The required image pair becomes the projected previous frame and the current frame. Thus, the inter-frame depth map minimization is optimized through the ego-motion network, resolving inconsistent camera poses between frames."}, {"title": "B. Learning Objective", "content": "We construct our loss function based on the weighted combination of three components: brightness awareness pho- tometric loss Lp [32], edge-aware smoothness loss Ls [52], and geometric consistency loss Lg [25]. Firstly, the brightness- aware photometric loss Lp consists of SSIM loss, L2-norm,\n \\begin{equation}\nL_{p}=\\frac{\\epsilon}{2}\\left((1-\\alpha) SSIM\\left(T_{\\[t \\rightarrow t+1]}\\left(I_{t}^{\\prime}\\right), I_{t+1}^{\\prime}\\right)+(1-\\epsilon)\\left|T_{\\[t \\rightarrow t+1]}\\left(I_{t}^{\\prime}\\right)-I_{t+1}^{\\prime}\\right|\\right),\n\\end{equation}\nwhere the weight $\u03b5$ is set to 0.85. The brightness consistency loss [32] is combined to reduce the adverse effect of the bright- ness difference between continuous image frames. T[t\u2192t+1](\u00b7) denotes the brightness transformation, which transforms the brightness of $I_t$ to match the brightness of $I_{t+1}$ at time t.\n\\begin{equation}\nT_{\\[t \\rightarrow t+1]}\\left(I_{t}\\right)=a_{\\[t \\rightarrow t+1]} I_{t}+c_{\\[t \\rightarrow t+1]},\n\\end{equation}\nwhere $a_{\\[t \\rightarrow t+1]}$ and $c_{\\[t \\rightarrow t+1]}$ are the brightness affine transfor- mation parameters.\nNext, we employ the edge-aware smoothness loss $L_s$ to ensure the edge region prediction rationality:\n\\begin{equation}\nL_{s}=\\sum_{x, y}\\left|d_{x} I\\right| e^{-\\partial_{x} I_{t}}+\\left|d_{y} I\\right| e^{-\\partial_{y} I_{t}},\n\\end{equation}\n$d_{i}=d_{i} / d_{\\max }$ represents the normalized average inverse depth to avoid depth reduction. Here, x and y denote the horizontal and vertical pixel coordinates, respectively. dx and dy denote pixel differences in x and y directions.\nFinally, the geometric consistency loss $L_g$ is employed to minimize the depth difference of the projected images:\n\\begin{equation}\nL_{g}=\\sum_{i} \\frac{\\left|D_{t+1}^{\\prime}\\left(p_{i}\\right)-D_{t+1}\\left(p_{i}\\right)\\right|}{D_{t+1}^{\\prime}\\left(p_{i}\\right)+D_{t+1}\\left(p_{i}\\right)},\n\\end{equation}\nHere, $D_{t+1}^{\\prime}\\left(p_{i}\\right)$ represents the depth of $I_{t+1}$ predicted and mapped from $I_{t}$, while $D_{t+1}\\left(p_{i}\\right)$ denotes the depth map from $I_{t+1}$. Consequently, the ultimate loss function shall be:\n\\begin{equation}\nL=\\alpha L_{p}+\\beta L_{s}+\\gamma L_{g},\n\\end{equation}\nwhere \u03b1, \u03b2 and \u03b3 are weights for each loss."}, {"title": "C. Network Structure", "content": "Both depth estimation and ego-motion estimation networks employ encoder-decoder structure as in baselines. The input of the whole framework consists of vision and vibration signals. In each training iteration, the vision part contains 3-frame images collected by the camera in the endoscopy. The vibration part is the 6-channel 240-point vibration signal collected by vibration sensors around the WCE. The vibration signal corresponds to 40 temporal sampling points around the first frame of 3-frames.\n1) Vision Network Branch: Our vision encoder is similar to the baselines. The only difference is that the fusion mapping module (details in Section III-D) is added after each encoder block to blend the vibration signal. As an illustration, consider the baseline model EndoSfMLearner [32]. Within this model, the encoder is comprised of a Max-pooling layer and multi- ple residual blocks [53]. Each residual block has a residual connection, a convolution layer, batch norm, and ReLU.\n2) Vibration Network Branch: We employ the Multiplica- tive Long Short-Term Memory (MLSTM) [54] model as our vibration network branch. The vibration branch includes a ba- sic LSTM sub-branch and an attention sub-branch containing the squeeze-and-excite blocks [55]. Finally, the outputs from both sub-branches are amalgamated through the utilization of a concatenation operation, and the vibration branch will return a vector containing high-dimensional information about the vibration signal.\n3) Prediction Decoders: The prediction decoders are de- signed for depth estimation and ego-motion estimation, re- spectively. We follow the decoders in each vision baseline algorithm to ensure that our vibration branch and fusion mod- ules have strong compatibility with existing vision algorithms. For example, in the baseline model EndoSfMLearner [32], the decoder for depth estimation contains five decoder blocks and a final decision module, respectively, and its output is the corresponding depth map. Each decoder block contains two convolution layers and two ELU activation functions. The decoder for ego-motion estimation is a single feed-forward network with an output of a 6-dimension vector representing the 6-Dof pose. The 6-Dof pose is then transformed into a 4\u00d74 transformation matrix for reprojection illustrated in Equ. (3)."}, {"title": "D. Fusion strategy", "content": "Our fusion strategy allows for the convenient insertion of the vibration signal network into the vision signal network. The structure of our fusion module is depicted in the lower right corner of Fig. 2. The main challenge of this module is that as one-dimensional timing information, the vibration signal does not have spatial information, so it is difficult to directly combine the vibration signal feature and RGB image feature into the network. To tackle this challenge, we specifically develop a Fourier heterogeneous fusion module, to modulate the corresponding vision and vibration signals from different modalities. The vision signal is transformed into the Fourier domain, and the joint heterogeneous representation of the visual signal and the vibration signal is subsequently established.\nFirstly, assuming that the vibration interference received by the image is additive noise, the signal with noise can be expressed as:\n\\begin{equation}\nF_{i}\\left(x_{v i} ; \\theta_{v i}\\right)=h\\left(x_{v i}\\right) * F_{i}^{\\prime}\\left(x_{v i} ; \\theta_{v i}\\right)+N_{l}\\left(x_{v i}\\right),\n\\end{equation}\nwhere $F_{i}\\left(x_{v i} ; \\theta_{v i}\\right)$ represents the ideal pure vision signal without noise, $F\\left(x_{v i} ; \\theta_{v i}\\right)$ represents the original information with noise, in our case, the output information after each encoder. h(xvi) represents information channel, n(xvi) repre- sents noise, which is not related to vision signal $F^{\\prime}\\left(x_{v i} ; \\theta_{v i}\\right)$. I denotes the l-th encoder. Thus, we can explore the best mu- tual representation of visual and vibration signals. A Fourier transform of the above equation yields:\n\\begin{equation}\n\\mathcal{F}_{l}(\\xi)=\\mathcal{H}_{l}(\\xi) * \\mathcal{F}_{l}^{\\prime}(\\xi)+\\mathcal{N}_{l}(\\xi),\n\\end{equation}\n\\begin{equation}\n\\mathcal{F}_{l}^{*}(\\xi)=\\mathcal{H}_{l}^{*}(\\xi) * \\mathcal{F}_{l}^{\\prime *}(\\xi)+\\mathcal{N}_{l}^{*}(\\xi),\n\\end{equation}\nwhere $F(g), F'(\u03be), H(\u03be), N(g)$ represent the frequency domain representation of $F(x_{v i} ; \\theta_{v i}), F^{\\prime}\\left(x_{v i} ; \\theta_{v i}\\right), h\\left(x_{v i}\\right)$, $n\\left(x_{v i}\\right)$, and the $F^{*}(\u03be), F^{'*}(\u03be), H^{*}(\u03be), N^{*}(\u03be)$ represent the conjugate of $F(\u03be), F'(\u03be), H(\u03be), N(\u03be)$, respectively. Therefore we can obtain:\n\\begin{equation}\n\\mathcal{F}_{l}^{\\prime}\\left(x_{v i} ; \\theta_{v i}\\right)=\\mathcal{F}^{-1}\\left\\{\\frac{\\mathcal{F}_{l}(\\xi) \\mathcal{H}(\\xi)}{\\mathcal{H}(\\xi) \\mathcal{H}^{*}(\\xi)+S N R}\\right\\},\n\\end{equation}\n\\begin{equation}\nS N R_{o r g}=\\frac{\\mathcal{F}_{l}(\\xi) \\mathcal{F}_{l}^{*}(\\xi)}{\\mathcal{N}_{l}(\\xi) \\mathcal{N}_{l}^{*}(\\xi)},\n\\end{equation}\nwhere the notation $F^{-1}$. signifies the inverse Fourier trans- form, and $S N R_{o r g}$ represents the signal-to-noise ratio for current vision signal. Therefore, the pure vision signal can be jointly represented by the original input vision signal and $S N R_{o r g}$, and we can then easily recover the pure vision signal without noise based on the $S N R_{o r g}$ for current vision signal. However, it is also challenging to achieve the $S N R_{o r g}$ directly from the images. Without the pure vision signal as the GT, the deep neural network can hardly extract the $S N R_{o r g}$ only from vision signals. In this case, we consider the indirect estimation of the $S N R_{o r g}$ of the image from the vibration signal. Specifically, we calculate the SNR based on the vibration signal, then modulate the vision signal in Fourier space with the SNR. As outlined in Section III-C, we obtain the output feature vector from the vibration signal encoder. Then, we define certain SNR grades during training and use an MLP layer to map it into SNR:\n\\begin{equation}\nS N R=\\operatorname{ReLU}\\left\\{W_{T} \\mathcal{F} G_{l}\\left(x_{v i b} ; \\theta_{v i b}\\right)+b_{l}\\right\\},\n\\end{equation}\nwhere $G_{l}\\left(x_{v i b} ; \\theta_{v i b}\\right)$ is the output feature vector of the vi- bration signal encoder, SNR is the output of the MLP, the terms W and b correspond to the weight and bias employed by the MLP layer, and ReLU is the nonlinear activation function of the MLP layer. As our entire framework operates in an unsupervised manner, we do not impose any supervision constraints on the SNR. Instead, we optimize the SNR together with the entire network architecture through Equ. (9). Thus, a modulation mapping module is added after each vision signal encoder block to map the predicted noise with the output features, and therefore, to pure the feature maps without noise."}, {"title": "E. Feasibility & Compatibility of Vision-Vibration Fusion", "content": "1) Hardware Feasibility: The integration of vibration sen- sors/IMUs into capsule robots for hardware and spatial di- mension feasibility is underscored by advancements in sensor fusion techniques and miniaturization. Abu-Khei et al. [35] outlined a method using visual and inertial data fusion to map the GI tract, emphasizing the feasibility and robustness of incorporating an IMU system within the capsule's limited space. Li et al. [36] introduced a novel localization approach combining external magnetic field sensing with internal iner- tial sensing for 6-DOF pose estimation of a magnetic WCE, demonstrating the feasibility of embedding sophisticated sen- sors without requiring complex structures or specific motions. These studies collectively illustrate the technological progress in miniaturizing and efficiently integrating sensors into capsule robots, enabling enhanced diagnostic capabilities and naviga- tion precision within constrained spatial dimensions.\n2) Compatibility with Vision-only Methods: Our vision-vibration framework is highly compatible with most existing vision-only methods because the proposed Fourier Heteroge- neous Fusion Module can be seamlessly integrated into many"}, {"title": "IV. EXPERIMENTS", "content": "Since the existing WCE datasets [32] do not provide the vibration signal, and obtaining the depth and ego-motion GT from real GI scenes is quite difficult, we produce a new dataset via VR-Caps\u00b9 [40] which is a virtual capsule endoscopy environment with advanced rendering techniques. The VR- Caps setup provides a variety of digestive tract organ models. Multiple research works have proven datasets collected from VR-Caps hold excellent generalization ability, and can achieve accuracy comparable to training on real datasets when tested on real scenarios [32], [40]. Besides, some vision-based depth and ego-motion estimation work also validate their proposed methodologies in virtual datasets [31]. Our dataset is collected in the above simulation environment with one monocular WCE. To collect the vibration signals, we further establish the vibration sensors in the VR-Caps environment, and bound the vibration sensors to the monocular WCE. Therefore, six strings of one-dimensional vibration signals can be collected (following the form of an IMU).\nVibration noise may originate from the natural movements of the GI tract, electronic interference, and the mechanical operations of the capsule itself. Gaussian noise is a mathemat- ical model that effectively represents the statistical properties of many physical phenomena, including vibration. Real-world vibration noise in capsule endoscopy can be characterized by random fluctuations that, over time, approximate the statistical profile of Gaussian noise. In this case, to simulate the vibration of an endoscope, we add various intensities of Gaussian noise to the movement of the capsule in three spatial dimensions following [62]. The noise intensity is within the level 1 to level 5, and the Gaussian noise is within [-1,1]. The example of the collected dataset is shown in Fig. 4. The image data is collected at 3 FPS by following [63], while the vibration signals were sampled at a frequency of 40 Hz after carefully reviewing existing real-world setups [36], [64]. The vibration signals are normalized before feeding the vibration network branch. We invite two WCE experts to manually control the capsule in VR-Caps setup, and name the datasets collected by different experts as Multimodal-WCE-1 (MM-WCE-1) and MM-WCE-2. We divide the \"Train\" dataset with 6 video sequences into training and validation sets as 8 : 2. The test set contains 5 separate video sequences. All datasets are collected with the vibration intensity label, GT depth maps, and WCE ego-motion."}, {"title": "A. Dataset", "content": "Since the existing WCE datasets [32] do not provide the vibration signal, and obtaining the depth and ego-motion GT from real GI scenes is quite difficult, we produce a new dataset via VR-Caps\u00b9 [40] which is a virtual capsule endoscopy environment with advanced rendering techniques. The VR- Caps setup provides a variety of digestive tract organ models. Multiple research works have proven datasets collected from VR-Caps hold excellent generalization ability, and can achieve accuracy comparable to training on real datasets when tested on real scenarios [32], [40]. Besides, some vision-based depth and ego-motion estimation work also validate their proposed methodologies in virtual datasets [31]. Our dataset is collected in the above simulation environment with one monocular WCE. To collect the vibration signals, we further establish the vibration sensors in the VR-Caps environment, and bound the vibration sensors to the monocular WCE. Therefore, six strings of one-dimensional vibration signals can be collected (following the form of an IMU).\nVibration noise may originate from the natural movements of the GI tract, electronic interference, and the mechanical operations of the capsule itself. Gaussian noise is a mathemat- ical model that effectively represents the statistical properties of many physical phenomena, including vibration. Real-world vibration noise in capsule endoscopy can be characterized by random fluctuations that, over time, approximate the statistical profile of Gaussian noise. In this case, to simulate the vibration of an endoscope, we add various intensities of Gaussian noise to the movement of the capsule in three spatial dimensions following [62]. The noise intensity is within the level 1 to level 5, and the Gaussian noise is within [-1,1]. The example of the collected dataset is shown in Fig. 4. The image data is collected at 3 FPS by following [63], while the vibration signals were sampled at a frequency of 40 Hz after carefully reviewing existing real-world setups [36], [64]. The vibration signals are normalized before feeding the vibration network branch. We invite two WCE experts to manually control the capsule in VR-Caps setup, and name the datasets collected by different experts as Multimodal-WCE-1 (MM-WCE-1) and MM-WCE-2. We divide the \"Train\" dataset with 6 video sequences into training and validation sets as 8 : 2. The test set contains 5 separate video sequences. All datasets are collected with the vibration intensity label, GT depth maps, and WCE ego-motion."}, {"title": "B. Implementation Details", "content": "We integrate our V2-SfMLearner framework with four depth and ego-motion estimation benchmarks: EndoSfM- Learner [32], AF-SfMLearner [32], RA-Depth [30], and En- doDAC [48]. The baselines and our fused models are trained by Adam with 400 epochs. The batch size is 4, and the learning rate is 1 \u00d7 10-4. We utilize NVIDIA A100 with Python PyTorch. During training, 3 frames of images with 40 vibration signal points constitute the training data at time t. Then, the model is tested frame by frame during evaluation. The three weights of Equ. (9) are assigned as: a = 1, \u03b2 = 0.1, and y = 0.5."}, {"title": "C. Evaluation metrics", "content": "For depth estimation, we adopt five metrics from [29] as follows: the absolute relative error (AbsRel), the square rela- tive error (SqRel), the root-mean-squared error (RMSE), the root-mean-square logarithmic error (logRMSE), and Accuracy (Acc) with a threshold as 1.25. Furthermore, we use the absolute difference (AbsDiff= (1/N)\u03a3 |Di - Di) to assess the absolute error between GT and predicted depth maps. N represents the pixel number. Di and $D_{i}^{\\prime}$ represent the GT and predicted depth value at the ith pixel, respectively.\nFor ego-motion estimation, we employ the absolute traject- ory errors, including the absolute translational error (ATE) and the absolute rotational error (ARE). Besides, we extend ATE into the absolute translational difference (AbsDifft) and the absolute relative translational error (AbsRelt), and extend ARE into the absolute rotational difference (AbsDiff) and the absolute relative rotational error (AbsRel). Our ego- motion metrics are defined in Table I, in which f denotes the frames of a WCE video, and j represents the sequence number containing two frames from the WCE video. x, y, z represent the spatial position in three dimensions, and \u03c6, \u03b8, \u03c8 represent Euler angles. xj, \u0177j, 2j are the predicted position difference between two adjacent frames, and  represent the predicted angle difference between two adjacent frames. xj, yj, zj, Lj, Vj, \u03c8; are the corresponding GT."}, {"title": "D. Results", "content": "The results of our multimodel solution are analyzed quan- titatively and qualitatively based on four SOTA vision- only depth and ego-motion estimation methods: EndoSfM- Learner [32], AF-SfMLearner [24], RA-Depth [30], and En- doDAC [48]. We followed the original setups in the vision network branch and prediction decoders, and further combined their methods with our vibration network branch and the proposed Fourier heterogeneous fusion module.\nFirstly, the depth reconstruction performance is presented"}, {"title": "E. Ablation on Vibration Intensity", "content": "As we need to estimate the SNR of the vibration signal for further fusion and prediction, we combine the two datasets as a joint dataset, and conduct an ablation study on differ- ent vibration intensities to demonstrate whether our fusion method can improve performance under different intensities of vibration noise. Specifically, we introduce five different levels of random vibration intensities during data collection. For each vibration intensity, we conduct training and validation using data collected under the same condition. We employ Af-SfMLearner [24] as the baseline. The results, as shown in Table IV, indicate a clear trend: the estimation performance worsens as the vibration intensity increases. However, our fusion framework outperforms vision-only methods in both tasks at each vibration intensity, again demonstrating the superior performance of our fusion framework. These results highlight the significant contribution of the vibration signal in our fusion framework, which plays a critical role in enhancing performance."}, {"title": "F. Ablation on Fusion Strategy", "content": "As shown in Table VI, we further experiment to inves- tigate whether conventional multimodal fusion methods can effectively integrate vision and vibration signals. Specifically, we replace the denoising fusion strategy with feature-level fu- sion methods, including concatenation, element-wise summa- tion, gated multimodal fusion [65], attentional feature fusion (AFF) [66], iterative attentional feature fusion (iAFF) [66], and bottom-up and top-down attention (BUTD) [67]. It is evident that when using these conventional methods to perform various learnable (weighted) combinations of features, the models could not effectively resist interference from vibration noise. Moreover, directly integrating vibration noise with vision data will lead to a decline in feature quality, resulting in an overall decreased performance. As shown in Table 3, the performances of some conventional feature fusion methods are even lower than the results of vision-only methods, which further demon- strates the effectiveness of our proposed framework."}, {"title": "G. Ablation on Vibration Types", "content": "The vibration may be caused by various reasons therefore we experiment to investigate the performance of our proposed method under different types of vibrations. Two types of vibration, which are the peristalsis of the GI tract and collision with foreign objects (e.g., food residues), are tested as shown in Table V. We conduct experiments for different vibrations with the same route and train the model with the same epochs. The performances under Peristalsis vibration are superior in all evaluation metrics for both depth estimation and ego- motion estimation tasks. This may be due to the relatively small amplitude and frequency of gastrointestinal peristalsis, while the direct collision of the capsule with food residues can cause significant movement. Besides, in both scenarios, the models with our proposed vibration-vision fusion strategy obtain better performance also demonstrating the effectiveness of our method against different types of vibration."}, {"title": "H. Robustness Experiments", "content": "Finally, we conduct a"}]}