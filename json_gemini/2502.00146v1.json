{"title": "Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms\nRadiologist MRI Interpretation: A Multi-Center Study", "authors": ["Hassan Jahanandish", "Shengtian Sang", "Cynthia Xinran Li", "Sulaiman Vesal", "Indrani Bhattacharya", "Jeong Hoon Lee", "Richard Fan", "Geoffrey A. Sonn", "Mirabela Rusu"], "abstract": "Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target suspicious prostate\nlesions. This has led to artificial intelligence (AI) applications improving MRI-based detection of\nclinically significant prostate cancer (CsPCa). However, MRI-detected lesions must still be\nmapped to transrectal ultrasound (TRUS) images during biopsy, which results in missing CsPCa.\nThis study systematically evaluates a multimodal AI framework integrating MRI and TRUS\nimage sequences to enhance CsPCa identification. The study included 3110 patients from three\ncohorts across two institutions who underwent prostate biopsy. The proposed framework, based\non the 3D UNet architecture, was evaluated on 1700 test cases, comparing performance to\nunimodal Al models that use either MRI or TRUS alone. Additionally, the proposed model was\ncompared to radiologists in a cohort of 110 patients. The multimodal AI approach achieved\nsuperior sensitivity (80%) and Lesion Dice (42%) compared to unimodal MRI (73%, 30%) and\nTRUS models (49%, 27%). Compared to radiologists, the multimodal model showed higher\nspecificity (88% vs. 78%) and Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%).\nOur findings demonstrate multimodal AI's potential to improve CsPCa lesion targeting during\nbiopsy and treatment planning, surpassing current unimodal models and radiologists; ultimately\nimproving outcomes for prostate cancer patients.", "sections": [{"title": "1. Introduction", "content": "Prostate cancer is the second-most prevalent cancer in men worldwide\u00b9 and remains the cancer\nwith second-highest mortality among American men\u00b2. The 5-year survival rate of prostate cancer\npatients has been reported to increase to 99% if the cancer is diagnosed in the early stages and\ntreated while still localized or regional, in contrast with a reported 5-year survival rate of 30% for\ndistant prostate cancer, where the cancer has spread to other organs and regions in the body\u00b2.\nTherefore, the early diagnosis of prostate cancer is of crucial importance.\nOver the recent years, various initiatives have been helpful in enhancing the diagnosis and\ntreatment of prostate cancer patients, including the use of pre-biopsy Magnetic Resonance\nImaging (MRI) to target suspicious lesions during the transrectal ultrasound (TRUS)-guided\nbiopsy procedure\u00b3. TRUS-guided biopsy relies on systematic sampling of the prostate at 12-14\nlocations which results in a 48% cancer detection sensitivity\u00b3. The systematic biopsy is improved\nby targeting suspicious lesions along with the systematic sampling locations during biopsy.\nExpert radiologists use pre-biopsy MRI scans to identify these suspicious lesions for targeting,\nwhich results in an increased overall diagnostic sensitivity of up to 88%\u00b3. The adoption of pre-\nbiopsy MRI has seen a dramatic increase in recent years due to its superior diagnostic sensitivity,\nwhere its utilization increased significantly from 0.5% in 2007 to 35.5% in 20224. However, the\nreliance on expert radiologist readings for the interpretation of pre-biopsy pelvic MR images\nremains a diagnostic bottleneck causing long wait times for patients, especially in rural areas with\nlimited availability of expert radiologists4,5. Additionally, the interpretation of prostate MR\nimages has been reported to substantially vary across radiologists, resulting in significant\ndifferences in cancer diagnostic yield6.\nGiven the increasing utilization of pre-biopsy MRI, the inherent quality, and excellent soft tissue\ncontrast of MR images, artificial intelligence (AI) methods that automate or assist with the\ndetection, localization, or grading of prostate cancer on MR images have been extensively\nexplored7-9. Various AI approaches have been proposed that use one or more MRI sequences,\nincluding T2-weighted (T2w) images, Apparent Diffusion Coefficient (ADC) images, Diffusion\nWeighted Images (DWI), and Dynamic Contrast Enhanced (DCE) images, to detect the presence\nof cancer, localize the cancerous lesions, and assess cancer aggressiveness7\u20139. Some of the recent\nwork has explored radiomic-based machine learning (ML) approaches that rely on handcrafted\nfeatures that might relate to cancer10,11. Most recent approaches utilize deep learning (DL) to train\nAl models that learn to directly identify cancer using MR image sequences as input12\u201317. These\nmethods have shown great promise in achieving highly sensitive cancer detection and pixel-level\nlocalization performance. Specifically, a recent study reporting the outcome of the PICAI prostate\ncancer detection challenge reported that an AI system trained on 9129 patients was on average\nsuperior to a pool of 62 radiologists in detecting Clinically significant Prostate Cancer (CsPCa)18.\nWhile AI methods hold great promise in detecting and localizing cancer on MRI, the predicted\nlesions must still be accurately projected onto TRUS images for targeting during biopsy.\nHowever, TRUS-guided biopsies and pre-biopsy MRI scans are performed at separate times with\na considerably different patient positioning, resulting in significant variations in the imaging field\nof view and the orientation of the acquired sequences19,20. Additionally, the shape of the prostate"}, {"title": "2. Results", "content": "Our multimodal AI model is based on a 3D UNet backbone and integrates MRI (T2w, ADC, and\nDWI sequences) and TRUS image sequences as inputs to simultaneously segment the prostate\ngland, indolent cancer lesions, and CsPCa lesions. The model was evaluated using three\nindependent test cohorts including two biopsy cohorts ($C_{Stanford}^{BX}$ and $C_{UCLA}^{BX}$) and one radical\nprostatectomy cohort ($C_{Stanford}^{RP}$). These cohorts included a total of 1700 test studies from both\ninternal and external institutions. Cancer ground truth labels were determined through pathology-\nconfirmed targeted biopsy lesions or whole-mount histopathology following radical\nprostatectomy. We evaluated the diagnostic performance of the multimodal model against\nunimodal MRI and TRUS models, as well as radiologists, using standardized performance"}, {"title": "3. Discussion", "content": "In the present study, we demonstrated the feasibility of developing a large-scale multimodal AI\nmodel that utilizes both MR and TRUS image sequences as input modalities to identify and\nlocalize CsPCa directly on TRUS images for the first time. We further confirmed the hypothesis\nthat MR and TRUS image sequences contain complimentary information that enables a\nmultimodal AI model to outperform models that solely rely on either of the modalities by\nevaluating each model in three independent patient cohorts from two institutions. Additionally,\nwe evaluated the performance of the multimodal AI model in an independent patient cohort\ncompared to radiologists reading MR images during multidisciplinary routine clinical care and\nshowed that its performance exceeds that of radiologists.\nWe evaluated the proposed multimodal AI model in three independent patient cohorts from two\ninstitutions, for a total of 1700 patients in the test cohorts. As detailed in Table 1, the multimodal\nAl model outperformed the other unimodal models in each test cohort. Specifically, the\nmultimodal model achieved both statistically and clinically significant improvements in\nsensitivity as well as the Overall Dice and Lesion Dice compared to the unimodal MRI model in\nall three test cohorts. Those improvements ranged from 3% to 9% increase in sensitivity and 8%\nto 14% increase in Dice across the three test cohorts. Noteworthy, the multimodal model achieves\nthe highest improvement in sensitivity compared to the unimodal MRI model in the external test\ncohort, $C_{UCLA}^{BX}$, which results in identifying 40 additional CsPCa lesions in 33 patients across the\ncohort compared to the MRI model. While the multimodal model achieved a lower specificity\ncompared to both unimodal models, it still achieved a great NPV value which was still marginally\nhigher than the unimodal models, indicating the screening potential of the multimodal model.\nWe leveraged an independent cohort of radical prostatectomy patients, $C_{Stanford}^{RP}$, with pixel-level\nwhole-mount pathology ground truth to compare the performance of the proposed multimodal Al\nmodel with practicing radiologists reading pelvic MRI exams during multidisciplinary routine\nclinical care. As shown in Table 2, the multimodal model outperforms the radiologists by\nmeaningful margins in most of our evaluation metrics. Specifically, the higher specificity of the\nmultimodal model results in 41.5% fewer false positives at sextant level compared to radiologists.\nInterestingly, the unimodal MR model also achieves a higher overall rank than radiologists, which\nis consistent with the reported outcomes in Saha et al18. Furthermore, the multimodal model's\nperformance in identifying more aggressive lesions was consistent with the radiologist readings\n(Figure 4C), which is promising towards a reliable Al-based prostate cancer diagnostic and\nscreening tool.\nFailure analysis of the multimodal AI model revealed that the missed lesions were significantly\nsmaller in volume (median: 531 mm\u00b3) compared to the true positive lesions (median: 1037 mm\u00b3)\nacross all cohorts. While less pronounced, radiologist readings exhibit a similar behavior to the\nmultimodal AI models (Figure 4A). Noteworthy, recent clinical guidelines do not consider\nprostate cancer lesions smaller than 500 mm\u00b3 to be clinically significant18,34. This suggests 44%\nof all the missed lesions by the multimodal AI model with GG > 2 would not be considered\nclinically significant according to this guideline. As further shown in Figure 4B, both the\nunimodal MRI and multimodal models have a similar lesion GG distribution across the cohorts,"}, {"title": "4. Materials and Methods", "content": "4.1. Description of the Data and Cohorts:\nCohorts: The present study was approved by the Institutional Review Board (IRB) at Stanford\nUniversity and included retrospective data from three patient cohorts consisting of a total of 3110\nstudies where each study represents a unique patient. The three patient cohorts were from two\ninstitutions, 1) $C_{Stanford}^{BX}$ is an internal cohort consisting of 1755 patients who underwent MRI-\nUS fusion targeted biopsy at Stanford Hospital, 2) $C_{UCLA}^{BX}$ is an external cohort of publicly\navailable data through The Cancer Imaging Archive (TCIA) from 1245 patients who underwent\nthe same MRI-US fusion targeted biopsy at UCLA Hospital, 3) $C_{Stanford}^{RP}$ is an internal cohort\nincluding 110 patients who underwent radical prostatectomy (RP) surgery at Stanford Hospital\nfollowing their diagnosis of prostate cancer. The details of each cohort are highlighted in Figure\n1A. For each study, three MR imaging sequences were captured during routine clinical care,\nincluding 1) T2w images, 2) ADC images, and 3) DWI images. Each study further included the\nTRUS image sequence that was captured using an Artemis System (Eigen Health, Grass Valley,\nCalifornia) and a Hitachi Ultrasound scanner utilized during the MRI-TRUS fusion biopsy\nperformed for each patient. Conventional brightness-modulated (b-mode) TRUS images of the\nprostate were acquired using 2D end-fire probes with center frequencies of 7.5-10 MHz and\nreconstructed to form 3D prostate image volumes. The MRI-US fusion biopsy involved targeting\nthe suspicious lesions outlined by radiologists on MRI sequences and projected to the TRUS\nimages by the Artemis system. The biopsy procedure further continued with the systematic\nbiopsy of 12-14 locations for each patient.\nGround Truth: For the biopsy cohorts $C_{Stanford}^{BX}$ and $C_{UCLA}^{BX}$, the cancer ground truth was\nobtained from targeted lesions outlined by radiologists that were then confirmed through\npathology lab testing of their biopsy samples. We assigned each lesion as cancer ground truth\nbased on the highest International Society of Urological Pathology35 (ISUP) grade group (GG)\nfound within the lesion. The RP cohort $C_{stanford}^{RP}$ included patients from whom we obtained\nwhole-mount pathology slides with pixel-level indication of the cancer presence. Following\npathology results, an expert image analysis scientist (MR, > 15 years of experience with\nregistering prostate pathology and radiology) manually outlined the extent of cancer on TRUS\nimages of each patient using the whole mount pathology reference slides.\nData Preprocessing: MRI image sequences were acquired using different scanners and\nprotocols; therefore, we preprocessed them to a similar spacing, size, and intensity range. The\nT2w, ADC, and DWI sequences were resampled to 0.5mm x 0.5mm x 3.0mm/voxel using B-\nSpline interpolation. Moreover, we center-cropped the x-y plane of the MRI sequences to a size\nof 128mm x 128mm, corresponding to an image size of 256 x 256 pixels, and padded the image\nvolumes if needed. We performed a similar preprocessing on TRUS image volumes, except that\nwe resampled the TRUS voxels to a spacing of 0.5mm x 0.5mm x 0.5mm/voxel due to the higher\nresolution TRUS frames in the coronal plane. Additionally, we normalized all the intensity\nsequences using Z-score normalization in refence to the prostate gland to ensure that the pixel\nintensities within each gland have a mean of zero and standard deviation of one for each"}, {"title": "4.2. AI Model Description", "content": "Our Al model is based on the well-known nnUNet framework37, which uses a UNet architecture\nto extract multi-scale features at various resolutions and relies on expanding and contracting paths\nto distill knowledge across each scale38,39. We trained the model using 3D image volumes where\neach volume was automatically divided into 3D voxel patches for training. We designed the\nmodel architecture to treat each image sequence as a separate input channel, from which 3D\nvoxel patches were extracted and concatenated. These concatenated patches were processed\nthrough a unified encoder, which progressively reduced the resolution to extract hierarchical\nl latent representations at multiple scales. The lowest-scale latent representations were then passed\nto a unified decoder, which progressively increased the resolution to reconstruct feature\nrepresentations associated with discriminatory characteristics that ultimately help identify\ncancerous pixels. At each decoding stage, encoded latent representations from the corresponding\nencoder stage were incorporated via skip connections, which merge spatially aligned feature\nmaps from the encoder and decoder to enhance feature preservation.\nEach encoder and decoder stage consists of two 3D convolutional layers with a kernel size of\n3x3x3, followed by 3D instance normalization and a Leaky ReLU (rectified linear unit)\nactivation. Encoding stages are further followed by a downsampling operation with strides of\n2x2x2, except in the first stage where no downsampling is applied. Decoding stages include the\nconcatenation of latent representation from skip connections, similar convolutional and\nnormalization/activation layers as in the encoder, and an upsampling operation with strides of\n2x2x2 to restore spatial resolution. Finally, segmentation heads consisting a 3D convolutional\nlayer with a kernel size of 1x1x1 was applied to produce class scores for each voxel, followed by\na SoftMax activation to assign probabilities to any or all of the following labels: 1) prostate\ngland, 2) any cancer (GG \u2265 1), or 3) CsPCa (GG \u2265 2). We empirically found that enabling the\nmodel to learn all the above labels was helpful in achieving a higher performance in identifying\nCsPCa. Binary cross entropy loss and dice loss were combined as the loss objective to train the\nmodel. We used the same 3D UNet backbone for the baseline unimodal MRI and TRUS models,\nwhere the number of input channels was based on the number of image sequences provided by\neach modality. An overview of the model architecture is illustrated in Figure 1B."}, {"title": "4.3. Experimental Design", "content": "To better measure the generalizability of our multimodal Al model, we only utilize 1410 cases\nfrom the $C_{Stanford}^{BX}$ cohort for training and validation during model development. We utilize the\nremaining 345 cases from this cohort along with all the cases from $C_{UCLA}^{BX}$ and $C_{Stanford}^{RP}$ cohorts\nas independent test cohorts for a total of 1700 test cases. The details of each cohort, including the\ndistribution of patients without cancer vs. patients with indolent cancer or CsPCa within each\ncohort are shown in Figure 1A."}, {"title": "4.4. Performance Evaluation", "content": "To assess the performance of each AI model against the ground truth, we performed a lesion-level\nevaluation18,40. While the models were trained to learn the three class labels, we focus the\nevaluation on assessing the ability of the models in identifying CsPCa lesions due to its clinical\nrelevance, as well as considering negatives at a sextant level to assess the model prediction\nspecificity. We report area under the Receiver Operating Characteristics curve (ROC), area under\nthe Precision Recall curve (PR), sensitivity, specificity, and negative predictive value (NPV).\nAdditionally, we compute the overlap between the cancer ground truth and predicted lesions\nusing 1) Overall Dice that captures the overlap between all predictions and the ground truth, 2)\nLesion Dice which captures the overall dice for patients with correctly predicted cancers."}]}