{"title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost", "authors": ["David Salinas", "Omar Swelam", "Frank Hutter"], "abstract": "Evaluating Large Language Models (LLMs) often requires costly human annotations. To address this, LLM-based judges have been proposed, which compare the outputs of two LLMs enabling the ranking of models without human intervention. While several approaches have been proposed, many confounding factors are present between different papers. For instance the model, the prompt and other hyperparameters are typically changed at the same time making apple-to-apple comparisons challenging. In this paper, we propose to systematically analyze and tune the hyperparameters of LLM judges. To alleviate the high cost of evaluating a judge, we propose to leverage multi-objective multi-fidelity which allows to find judges that trade accuracy for cost and also significantly reduce the cost of the search. Our method identifies judges that not only outperform existing benchmarks in accuracy and cost-efficiency but also utilize open-weight models, ensuring greater accessibility and reproducibility.", "sections": [{"title": "1. Introduction", "content": "Instruction tuned models are difficult to evaluate as they provide free-form text given arbitrary instructions that may include summarization (Zhang et al., 2024), code writing (Ni et al., 2023) or translation (Elshin et al., 2024). While humans can annotate the quality of the outputs of an LLM given instructions, this quickly becomes expensive and also delays the evaluation and development of instruction-tuned models (Li et al., 2023).\nAs an alternative, LLM judges have been proposed to provide an indicative ranking for instruction-tuned models (Li et al., 2023), but also to select instruction-tuning recipes (Grattafiori et al., 2024; Lambert et al., 2024). While they can reduce cost significantly compared to human evals, LLM judges have limitations as they may rely on superficial style aspects, such as the length of a response (Dubois"}, {"title": "2. Related work", "content": "LLM judges. LLM as a judge has been emerging as a way to alleviate large human annotation costs that are required to evaluate instruction-tuned models. For instance, Llama3 used an earlier version of itself as a reject sampler to select best completions (Grattafiori et al., 2024) or more recently (Lambert et al., 2024) used an LLM judge to annotate preference data to perform instruction tuning. LLM judges have also been used for leaderboards such as Alpaca-Eval (Li et al., 2023) or Arena-Hard (Li et al., 2024) which offer cheaper alternatives than human-annotated leaderboards such as ChatBot Arena (Chiang et al., 2024).\nZero-shot and fine-tuned judges. Two main approaches have been proposed for LLM judges. The first one, referred to as zero-shot judges, prompts a LLM to rate a pair of model completions (Li et al., 2023; 2024) or a single completion possibly against a baseline (Zheng et al., 2023). The second one fine-tunes an existing model on a set of human-annotated preferences (Zhu et al., 2023; Wang et al., 2024). In contrast with the first approach, it requires fine-tuning a model which may occur at an extra cost, along with lower robustness under distribution shift (Huang et al., 2024).\nZero-shot judges. Many strategies for zero-shot judges have been proposed. Li et al. (2024) proposed to ask the judge to answer the instruction to perform some form of Chain of Thought (Wei et al., 2022). To avoid the order of outputs to matter in pairwise comparison, previous work proposed to randomize or average the two possible positions (Dubois et al., 2024; Li et al., 2024). To parse the model output, Li et al. (2023) asks the judge a letter to denote the best model, Li et al. (2024) uses instead a Likert scale (such as A\u00bbB, A>B, A=B to indicate respectively when model A is much better, better or comparable wrt B), and another alternative (Cui et al., 2024; Lambert et al., 2024) outputs a score for various criteria such as instruction-following, honesty, or helpfulness. The underlying LLM models often vary between papers, along with multiple other dimensions, making it difficult to determine which strategies inherently perform better.\nJudge limitations. In parallel with their adoption, several limitations of LLM judges have been highlighted. Among them is their dependence on superficial stylistic aspects of an answer, for instance favoring longer answers (Dubois et al., 2024), the first answer when using judges that make pairwise comparison (Li et al., 2024), or their own outputs (Panickssery et al., 2024)."}, {"title": "Judge tuning.", "content": "To the best of our knowledge, hyperparameter tuning for LLM judges has not been comprehensively explored in a way that accounts for the various factors contributing to a high-performing judge. One reason is the associated compute cost with naive approaches. For instance, Alpaca-Eval and Arena-Hard evaluate each judge configuration across a grid of models and instructions, leading to substantial expenses when comparing multiple judges\u00b9. Cost is therefore a strong limitation for the tuning of judges limiting the comparison and tuning of judges to simple aspects such as the choice of the underlying LLM model but excluding other key factors such as the prompt strategy, the output format or the temperature.\nIn this work, we propose a method to search for optimal judge configurations, including the best prompt parameterization. While prompt tuning approaches such as (Fernando et al., 2023) are related, they do not specifically address tuning judges. A key distinction is that we focus not only on optimizing prompts but also on other judge hyperparameters, such as model selection and temperature. Similarly, Doddapaneni et al. (2024) analyzed the effectiveness of five different prompt strategies for LLM judges. However, their work primarily introduces a benchmark, whereas our approach is centered on systematically tuning and analyzing judges within a search space that includes 80 prompting strategies and a total of 4,480 judge configurations.\nWe begin by providing background on LLM judges and examining the extent to which improvements can be achieved through scaling alone. This analysis underscores the importance of human agreement as a more efficient metric for differentiating between judges. We then demonstrate how judges can be effectively tuned by optimizing their prompts and other hyperparameters, such as model selection and temperature."}, {"title": "3. Background", "content": "We denote a LLM model as a function \\( \\pi : p \\rightarrow o \\) that produces an output string o given a prompt p. A judge compares two models \\( \\pi_0 \\) and \\( \\pi_1 \\) and outputs a score \\( \\Phi(p, \\pi_0(p), \\pi_1(p)) \\in [0,1] \\) which is close to zero if \\( \\pi_0(p) \\) is better than \\( \\pi_1(p) \\) or close to one otherwise.\nA common choice is to use a fixed baseline \\( \\pi^* \\) for one of the models, typically a frontier model which allows to obtain a score for a model to be evaluated: \\( \\Phi(p, \\pi(p)) = \\Phi(p, \\pi^*(p), \\pi(p)) \\). Given that the order can sometimes influence the judge decision (Li et al., 2023), previous works have proposed to either randomize the baseline position (Li et al., 2023) or compute the judgement given both orders and average out the result (Li et al., 2024)."}, {"title": "3.2. Judge metrics.", "content": "Spearman correlation. One approach to see how well a judge performs is to check how close are its rankings on a grid of models and instructions compared to rankings obtained through human annotations.\nAssume we are considering a finite set of models \\( \\mathcal{M} \\) and instructions \\( \\mathcal{P} \\) and that we are given a list of golden scores for the models denoted \\( s^h \\in \\mathbb{R}^{\\mathcal{M}} \\), for instance, the elo-ratings from the Chatbot Arena obtained from human ratings.\nTo obtain scores from the judge, we average for each model the preference against a baseline \\( \\pi^*(p) \\), e.g.\n\\[\n\\Phi_s = \\mathbb{E}_{\\rho\\sim\\mathcal{P}} [\\Phi(\\rho, \\pi^*(\\rho), \\pi_i(\\rho))].\n\\]\nOne can then use the Spearman correlation between the scores estimated with the judge and the golden score, e.g.\n\\[\n\\rho(s^h,s) = \\frac{cov [R(s^h),R(s)]}{\\sigma(R(s^h))\\sigma(R(s))} \\in [-1,1]\n\\]\nwhere \\( R(x) \\) and \\( \\sigma(x) \\) denotes respectively the rank operator and the standard deviation. Using Spearman correlation is beneficial because it accounts for differences in scale between the golden scores and the judge annotations, focusing on the ranking consistency rather than absolute values. Other metrics which can be used to compare the order of models include Brier score and calibration described (Li et al., 2024).\nHuman agreement. Another approach to evaluating judges is to measure their agreement with human-annotated preferences in a list of pairwise model battles. Each battle consists of a prompt, a pair of model outputs, and a binary label indicating which output the human annotator preferred.\nLet us denote a set of annotated battles:\n\\[\n(\\mathcal{P}_i, o_i, o'_i, \\Phi^h(\\mathcal{P}_i, o_i, o'_i))_{i=1}^N\n\\]\nwhere \\( o_i, o'_i \\) denotes the output of two models from prompt \\( \\mathcal{P}_i \\) and \\( \\Phi^h(\\mathcal{P}_i, o_i, o'_i) \\in \\{0, 0.5, 1\\} \\) denotes a human choice for respectively \\( o_i \\), a tie or \\( o'_i \\).\nTo evaluate the judge quality, one can measure the human agreement which is the percentage of times where the human and judge agrees, e.g.\n\\[\n\\mathbb{E}_{i \\sim N}[\\Phi(\\mathcal{P}_i, o_i, o'_i) = \\Phi^h(\\mathcal{P}_i, o_i, o'_i))]\n\\]\nHaving defined two metrics to evaluate judges, we next evaluate how those are impacted by scaling the base models or number of instructions."}, {"title": "4. Scaling judges", "content": "Rather than tuning judges a natural question is: can we just scale them up? LLM judges can be scaled in multiple ways: by increasing the number of parameters of the LLM judge or by using more instructions. Here we investigate how well the Spearman correlation and human agreement metrics scale with both dimensions.\nSpearman correlation. In Fig. 1, we show the scaling behavior when increasing the LLM judge size and the number of instructions. We use Qwen2.5 with a default prompt and compute Spearman correlation on a common set of 26 models available on both Alpaca-Eval and Arena-Hard.\nAs expected, the judge performance improves when scaling the LLM model and when using more instructions as it allows to cover more areas of the models to evaluate. Alpaca-Eval contains easier prompts and therefore gives better performance to smaller judges compared to Arena-Hard. In contrast, Arena-Hard contains harder and technical questions. This allows to achieve better Spearman correlation as the instructions allow better separability among advanced models, provided that the judge base model is strong enough to measure the performance on this more complex set of instructions.\nHuman agreement. In Fig. 2, we study the effect of scaling the LLM judge and the number of battles this time on human agreement. Here, we use the same prompt and base LLM models as in the previous paragraph but compute human agreement on the LMSys dataset (lin Chiang et al., 2024).\nWe also observe that scaling the LLM judge base model improves performance. However, compared to the previous case, the performance is stationary which is expected since human-agreement is the average of an instruction based property. In contrast, Spearman correlation gets better with more instructions as shown in Fig. 1 as the judge gets a better sense of a model performance (more scenarios are seen and with larger frequency)."}, {"title": "Which metric to optimize.", "content": "We have two metrics to evaluate the judge quality: the Spearman correlation and the human agreement. As discussed in the previous paragraph, they behave differently as human agreement is an average of an atomic property (does the judge and human agrees on instruction on average) whereas Spearman correlation is a global metric requiring evaluating many models to compare rankings. Both metrics are correlated, we can see for instance that the ranking w.r.t. the base model size used for the judge is mostly consistent across both Spearman correlation and human agreement in Figs. 1 and 2. For both metrics, we observe that, for this prompt and model family, LLM judges bellow 7B are not able to outperform a simple length baseline which favors the output with the longest answer."}, {"title": "5. Tuning judges", "content": "We first describe the search space used - also summarized in Table 5 - which includes searching for the inference and prompt hyperparameters."}, {"title": "5.1. Inference hyperparameters", "content": "For the LLM model, we search among 7 open-weights options: Llama3.1 (8B and 70B), Qwen2.5 (7B, 27B and 70B) and Gemma2 (9B, 27B). All models with more than 9B parameters are quantized with half-precision. We also search for the LLM temperature in [0.0, 0.01, 0.1, 1.0] and whether to average predictions when considering two possible orders or using just a single order."}, {"title": "5.2. Prompt hyperparameters", "content": "We now describe how we parametrize different prompt options and we illustrate one such option in Fig. 3.\nOutput format. When prompting a LLM judge, we must be able to parse its output into a preference. We consider the following options where the judge outputs:\n\u2022 best-model-identifier: a letter corresponding to the best assistant as in Li et al. (2023)\n\u2022 likert: a likert scale (such as A\u00bbB, A>B, A=B to indicate respectively when model A is much better, better or comparable wrt B) as proposed in Li et al. (2024)\n\u2022 pair: a score for both assistants in [0-10] similar to (Zhu et al., 2023)\n\u2022 preference: a score in [0, 1] where 0 (resp. 1) indicates a preference for model A (resp. B)\n\u2022 multi: the average score for 5 criteria - conciseness, clarity, adherence, comprehensiveness and style similar to Cui et al. (2024); Lambert et al. (2024).\nProvide answer or other information. Asking a LLM to reflect before providing its answer is known to be beneficial in cases requiring reasoning (Wei et al., 2022). In addition, providing example (few-shot learning) can also be beneficial as shown in (Zheng et al., 2023). We therefore search for the following options and ask the judge to provide before its preference:\n\u2022 confidence: its confidence on its preference\n\u2022\nanswer: its own answer to the instruction as proposed in (Li et al., 2024)\n\u2022 explanation: its explanation on the given preference\nProviding its confidence is meant to help the judge to provide more calibrated preference scores (e.g. to not give a strong score for one option when it is uncertain) while the latter two are meant to elicit chain of thought reasoning.\nTo use one of the three options, we add in the prompt a description of the option and asks the LLM to generate it, see Fig. 3 for an illustration where the prompt asks the judge to provide its answer and an explanation on its judgement.\nJSON formating. Formats used to query the output of an LLM have different trade-offs (He et al., 2024). Some are more controllable such as JSON but may loose performance against simpler format (such as raw text) in particular given less capable models."}, {"title": "5.3. Multi-fidelity and multi-objective optimization", "content": "Multi-fidelity optimization. Next, we perform multi-fidelity multi-objective optimization in order to find judge configurations that are good for both accuracy and cost while keeping the cost of the search feasible with multi-fidelity.\nEvaluating all judges on all battles is expensive and would cost \\( N \\times P \\) annotations if we have N judges and P battles. One way to reduce the cost of the search is to apply a multifidelity approach such as sucessful-halving (Karnin et al., 2013).\nWe perform the tuning by running configurations in three steps. In the first step, we run all N configurations with only \\( P/9 \\) battles. We then run the top \\( N/3 \\) judges on \\( P/3 \\) battles in a second step and finally run \\( N/9 \\) on the full P battles.\nThis reduces the cost of the full search from \\( N \\times P \\) to \\( N \\times P/3 \\). One could also use a more aggressive cutoff and save further in the computation but this would naturally increase the risk of missing good configurations.\nMulti-objective optimization. When sorting the top judge configurations, we have two objectives to take into account since we would like to find judges that both accurate and cheap. For instance, prompting a judge to perform chain-of-thought of to provide its answer may improve performance, but the extra-cost may be better spent on a better and more expensive base model.\nTo sort configurations while accounting for the two objectives, we use non-dominated sort (Emmerich & Deutz, 2018) as it was shown efficient in multi-fidelity settings (Salinas et al., 2021; Schmucker et al., 2021; Izquierdo et al., 2021). We illustrate the priority given to the judge configurations in Fig. 4 in the first and second selection step where one can see that the priority model the geometry of the Pareto front well."}, {"title": "5.4. Hyperparameter analysis", "content": "On Fig. 5, we show the validation performance of all judge configurations at the lowest fidelity. We see that while the number of parameters influence the performance, the prompt and other judge hyperparameters have a significant importance given the wide variation of performance obtained for a fixed model.\nNext, we examine which hyperparameters and prompt characteristics contribute to the best judge performance. In Fig. 6, we analyze all 4480 judge configurations at the lowest fidelity (i.e., evaluated on 400 instructions) where we conduct a survival analysis, measuring how often each hyperparameter appears in the top 100 configurations with the highest human agreement. We perform this analysis separately for large models (blue bars) and smaller models (orange bars) to identify which hyperparameters are most effective in each case.\nWithout surprise, the model used for the LLM judge is the most impacting hyperparameters and larger is generally better. Llama3 performs better than Qwen2.5 and Gemma when used as a judge.\nThis analysis also reveals less obvious hindsights:"}, {"title": "5.5. Prompt stability", "content": "Next, we investigate how much the performance of a prompt varies between different model judges in Fig. 7, where we show the correlation between different judge models on how well they perform under different prompts. For each of the m = 7 models, we measure the average human agreement for all the different n = 24 \u00d7 5 = 80 prompts and compute the covariance matrix \\( XX^T \\) where \\( X \\in \\mathbb{R}^{m\\times n} \\) is the matrix of scores for all models and prompts. Interestingly, smaller models and larger models are highly correlated which shows that two group of prompts works well for large and small models. This is expected as lower capacity models may struggle to obey more complex instructions (e.g. rate models for style and accuracy using JSON) that are beneficial to evaluate better models."}, {"title": "5.6. Results on test datasets", "content": "In this section, we report the performance of three judges found by our search on several test sets. While the multiobjective search returns a list of judges with continuous cost tradeoffs as seen in Fig. 4, we report results for only 3 judges: small, medium and large with numbers of parameters respectively lower than 10B, lower than 32B and lower than 72B. We do this as it may offer an easier choice for a practitioner when picking a judge, for instance accounting"}, {"title": "6. Conclusion", "content": "In this paper, we examined how judge performance is influenced by scaling and hyperparameter choices. We introduced a multi-fidelity, multi-objective approach to tune judge hyperparameters - including prompt design and base models - at a feasible cost. Our results demonstrate that this method can produce judges that outperform previous approaches across different budget constraints.\nWhile some limitations of LLM judges persist, we hope that enabling cost-effective tuning will help the community refine their use and address remaining deficiencies. For example, our multi-objective procedure could be extended to optimize for additional criteria such as stability or explainability.\nWe will release the code to reproduce our results, along with a dataset containing all annotations. We hope this resource will support further analysis and improvement of LLM judges."}, {"title": "Impact Statement", "content": "This paper demonstrated how to tune judge hyperparameters while balancing cost considerations and ensuring that the search remains feasible. Our approach optimizes judge hyperparameters to maximize human agreement while also considering open-weight alternatives.\nThe benefits of this approach include enabling fairer and more cost-effective leaderboards and helping the community adopt judges that do not rely on closed systems. However, LLM judges may also reinforce undesirable superficial biases, such as favoring stylistic elements over substantive quality or perpetuating human biases present in the training data, including biases against certain groups.\nWe conducted a preliminary review of the selected LMSys data and did not observe obvious issues, though our analysis was limited to a small sample. As a result, such systems should not be deployed without safeguards and additional bias evaluations."}, {"title": "A. Datasets", "content": "A.1. Alpaca-Eval and Arena-Hard datasets\nWe consider two datasets that contains prompts, model completions and judge annotations for a grid of prompts and model pairs. The first one is Alpaca-Eval which contains 47 models completions on 805 prompts (Li et al., 2023). The second one is Arena-Hard which contains the completions on 500 instructions for 57 models (Li et al., 2024). In both cases, we select the 26 models that also appear in Chatbot Arena in order to be able to compute how well judge configurations approximate human judgement.\nA.2. LMSys\nWe use the LMSys dataset (lin Chiang et al., 2024) which contains 51734 battles and allows to measure the human-agreement of a given judge configuration.\nLMSys validation and test split. When using LMSys, we rate instructions with the prompt from (Li et al., 2024) given in Fig. 8 and we use Llama3-8B-instruct which assigns a score to each instruction in [0, 7].\nWe select instructions which have a score greater than or equal to 5 and also have the criterion 1. Specificity detected since it is important to avoid large ambiguity prompt to evaluate judge (e.g. we want to discard the instruction like \"say hello\" since they provide no value to distinguish models).\nThis gives 6548 instruction which we split randomly into 3548 validation instructions and 3000 test instructions. All model selection (e.g. non-dominated sort) is done only using validation instructions and only the best models on the validation set are evaluated on the test set."}, {"title": "B. Experiment details", "content": "To generate inference with open models, we host the models locally using VLLM on L40 GPUs for models up to 32B parameters and on H100 GPUs for models with more than 70B parameters. Given that we use two clusters with two different job queues, we favored using synchronous successful halving rather than an asynchronous approach such as (Schmucker"}, {"title": "B.1. Cost", "content": "To compute the cost of a judge annotation, we first estimate the token price and then multiply the number of tokens by the token price\u00b3. To obtain the average token price for a model, we measure the total runtime and number of tokens on a large collection of judge annotations for the given model. We then derive the cost per token using the H100 hourly price of runpod (2.79$/hour) for models requiring more than 48GB of VRAM (Llama3 70B, Qwen2.5 70B and Gemma2 27B) and using L40 hourly price for other models (0.99$/hour).\nWe arrive at the cost per token given in Table 6. The estimate are lower than a public provider such as Together which is expected as such a service needs to operate at a margin and over-provision machines to meet demand. The cost we estimate is highly conservative given that no optimization was done to optimize VLLM hyperparameters.\nFor close models, we compute the cost identically by multiplying tokens seen in prompt and completion with the corresponding token price."}, {"title": "B.2. Prompt templating", "content": "Prompt examples. In Fig. 9, we show a full prompt corresponding to the prompt hyperparameter:\n{ \"Provide answer\": True, \"Provide explanation\": True, \"Provide example\":\nTrue, \"use JSON\": True, \"output preference format\": \"Pair\"}\nand in Fig. 10, we show the prompt corresponding to:\n{ \"Provide answer\": True, \"Provide explanation\": False, \"Provide example\":\nFalse, \"use JSON\": False, \"output preference format\": \"Likert\"}."}, {"title": "B.3. Tuning cost estimation", "content": "Alpaca-Eval and Arena-Hard. In both cases, to evaluate Spearman correlation one must annotate a judge on a grid of models and instructions.\nLet us assume we evaluate \\( n_{judges} = 4480 \\) as done in this work for \\( n_{models} = 20 \\) models as done in (Li et al., 2024) on the 805 instructions of Alpaca Eval. We get the cost to annotated one model as 24$ by using the estimation of Ni et al. (2024). This gives a cost of \\( 4480 * 24 * 20 = 2186 240$ \\) for Alpaca-Eval and a cost of 2240 000$ for Arena-Hard whose cost to annotate the instructions for one model was estimated to 25$ in (Ni et al., 2024).\nCost estimation of our approach. We evaluate \\( N = 4480 \\) judge configurations on 400 instructions, then the top 1200 judge configurations on 1200 instructions, then the top 400 judge configurations on the full set of \\( P = 3548 \\) validation instructions. This requires a total of 4 651 200 annotations. On average, a single annotation takes about 0.6s on a H100. If using runpod with a cost of 2.79$/hour per H100 hour, we get a total cost of \\( 4651200/3600 * 0.6 * 2.79 \u2248 2.1K$\\$.\nThe savings are obtained by identifying a more efficient metrics to distinguish judges than Spearman correlation (which requires evaluating a grid of models and instructions in (Li et al., 2023) and (Li et al., 2024)) and applying multi-fidelity which allows us to save roughly a factor of 3 since it avoids to annotate the full set of \\( N \\times P\\)."}, {"title": "C. Multi-objective background", "content": "In hyperparameter optimization, one seeks to find the best hyperparameter \\( \\theta^* \\) of a blackbox function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\), e.g. to find:\n\\[\n\\theta^* = \\arg \\min_{\\theta \\in \\mathbb{R}^d} f(\\theta).\n\\]\nThe blackbox may be for instance a neural network that we want to train and the hyperparameter may include the number of layers or the learning rate.\nMulti-objective optimization. When considering several objectives, we now want to minimize a function \\( f(\\theta) \\in \\mathbb{R}^m \\). Since we have more than one objective, there is not a single best hyperparameter \\( \\theta^* \\) in general but a set of non-dominated solutions.\nWe say that a hyperparameter \\( \\theta \\) dominates \\( \\theta' \\) if and only if:\n\\[\n\\forall i, f(\\theta)_i \\leq f(\\theta')_i \\text{ and } \\exists i, f(\\theta)_i < f(\\theta')_i\n\\]\nand denotes it with \\( \\theta \\prec \\theta' \\) i.e. when all components of \\( f(\\theta) \\) are lower or equal than the ones of \\( f(\\theta') \\) and one component is strictly better.\nWe aim to find the Pareto front \\( \\mathcal{P} \\) which consists of non-dominated solution:\n\\[\n\\mathcal{P} = \\{\\theta \\in \\mathbb{R}^d | \\nexists \\theta', \\theta' \\prec \\theta\\}.\n\\]\nNon-dominated sort. When applying successful-halving, we need to sort the top configurations, for instance to keep the top 50% and let those configurations run with a larger budget.\nSince we have multiple objectives, something must be done to adapt the algorithm. While averaging out the objective (or doing more advanced scalarization) allows to go back to the scalar case, it only works on some cases (where the Pareto front is convex in the case of averaging the objectives for instance).\nAnother approach is to use non-dominated sort (Emmerich & Deutz, 2018) which we illustrate in Fig. 11 and now describe. The approach first computes the Pareto front of the current set of observations and assign top-ranks with a heuristic to break ties. Then, the approach is applied recursively until no points are left. To break ties, multiple heuristic can be used, in our case we use an epsilon-net as it was shown to perform well in (Schmucker et al., 2021). As can be seen in Fig. 4, the approach models the geometry of the Pareto front compared to scalarization approaches."}]}