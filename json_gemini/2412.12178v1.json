{"title": "Activation Sparsity Opportunities for Compressing\nGeneral Large Language Models", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Kazi Fahim Ahmad Nasif", "Liang Zhao", "Kun Suo"], "abstract": "Deploying local AI models, such as Large Language\nModels (LLMs), to edge devices can substantially enhance de-\nvices' independent capabilities, alleviate the server's burden, and\nlower the response time. Owing to these tremendous potentials,\nmany big tech companies have been actively promoting edge\nLLM evolution and released several lightweight Small Language\nModels (SLMs) to bridge this gap. However, SLMs currently\nonly work well on limited real-world applications. We still\nhave huge motivations to deploy more powerful (larger-scale)\nAI models on edge devices and enhance their smartness level.\nUnlike the conventional approaches for AI model compression,\nwe investigate from activation sparsity. The activation sparsity\nmethod is orthogonal and combinable with existing techniques\nto maximize compression rate while maintaining great accuracy.\nAccording to statistics of open-source LLMs, their Feed-Forward\nNetwork (FFN) components typically comprise a large proportion\nof parameters (around\\documentclass{article}\\usepackage{amsmath}\begin{document}   \\frac{2}{3} \\end{document}). This internal feature ensures that\nour FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models.\nThis work systematically investigates the tradeoff between\nenforcing activation sparsity and perplexity (accuracy) on state-\nof-the-art LLMs. Our empirical analysis demonstrates that we\ncan obtain around 50% of main memory and computing re-\nductions for critical FFN components with negligible accuracy\ndegradation. This extra 50% sparsity does not naturally exist\nin the current LLMs, which require tuning LLMs' activation\noutputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. Moreover,\nwe further verified the predictability of activation patterns in\nrecent LLMs. The success prediction allows the system to prefetch\nthe necessary weights while omitting the inactive ones and their\nsuccessors (compress models from the memory's perspective),\ntherefore lowering cache/memory pollution and reducing LLM\nexecution time on resource-constraint edge devices.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid evolution of transformed-based AI models, such\nas large language models (LLMs) or Large Vision Models\n(LVM), has significantly accelerated the recent AI expansion.\nState-of-the-art models typically contain several hundreds of\nbillions or even trillion parameters (e.g., GPT-4 has 1.76\ntrillion parameters [1]) and deploy on HPC/cloud servers. If\nend users require services from these large-scale AI models,\nthey must send requests from edge to the HPC via a network\nconnection, and the HPC then allocates tens of high-end GPUs\nto execute the model inference and return results. This client-\nserver architecture pushes all computing burdens to the central-\nized HPC, which not only requires stable network connections\nfrom the edge but also limits the system's scalability. There-\nfore, there is a growing interest in directly deploying LLMs\non edge devices [2], [3] to eliminate the above challenges.\nThe benefits of local edge LLMs include lowering the HPC's\ncomputing and memory burdens, reducing traffic, decreasing\nresponse time, and enhancing data privacy [4]. However, effec-\ntively deploying LLMs on edge is challenging because of their\nhigh computing and memory demands, often exceeding the\ncapabilities of general edge devices. Big tech companies, such\nas Google, Microsoft, and Meta, proposed smaller versions\nof LLMs (aka. Small Language Models (SLMs)) to deliver\nlightweight local AI to edge. But the abilities of SLMs can\nonly satisfy the criteria of limited applications, and we still\nhave great motivation to offload more powerful AI models\nto the edge. Moreover, the supply-demand gap between edge\ndevices and LLMs is growing exponentially. Therefore, we\nmust explore new compression techniques to support LLM\ndeployment on resource-constraint devices.\nPruning [5], quantization [6], and knowledge distillation [7]\nare three conventional methodologies to compress AI models.\nThe compression rates theoretically may increase by utilizing\none or more of these methods while maintaining appropriate\naccuracies. The compression rates of these approaches have\ntheoretical up-bounds even in their ideal situations. To further\nattain better compression rates, we should explore a new ap-\nproach that can seamlessly incorporate with existing schemes.\nThis paper aims to investigate extra LLM compression pos-\nsibilities via sparsity. Al model sparsity could be classified\nas weight sparsity and activation sparsity. Weight sparsity is\nmore common in regular DNN models [8] but typically has\nmuch lower levels in state-of-the-art LLMs (Section III-A).\nActivation sparsity [9], [10] is associated with the output of\nFFN neurons, where a certain percentage of neurons will not\nbe activated and no outputs pass to the subsequent layers.\nTherefore, from model compression's perspective, the weights\nof these inactive neurons are not required to be fetched\ninto memory, which may enhance memory utilization, lower\nnetwork traffic, and reduce execution latency.\nThe compression opportunity from activation sparsity is\nbased on the observation that the majority of LLM's weights\nare from feedforward network (FFN) layers (around \\documentclass{article}\\usepackage{amsmath}\begin{document}   \\frac{2}{3} \\end{document} [11]),"}, {"title": "II. SPARSITY EXPLORATION APPROACHES", "content": "and not all FFN weights substantially impact the LLM output.\nWe found that the natural activation sparsity (from default\npre-trained models) of ReLU-based LLMs is normally much\nhigher and can reach up to around 95%. In contrast, state-of-\nthe-art LLMs have switched to non-ReLU activations, such as\nSwiGLU, demonstrating negligible natural activation sparsity.\nTherefore, the recent evolution of LLMs' activation functions\ncauses new challenges for compression via activation sparsity.\nSimilar to weight sparsity, the impact of activation values is\ndirectly associated with their magnitudes. Our investigation\nrevealed that a significant portion of recent LLMs' activation\nvalues are extremely close to zero, and approximately 50%\nof activation values in FFN layers can be safely withdrawn\nwithout significant accuracy loss. This feature allows us to\npredict activation patterns according to the user's input tokens.\nOr predict the deeper layers' patterns using the first few layers'\npatterns. The predicted inactive neurons, which have very\nsmall activation values, do not need to be fetched from the disk\nor SD card into memory. We can prefetch the active neurons\nto memory to lower the inference latency. Importantly, this\napproach does not require re-training or fine-tuning the LLMs,\nas the pattern prediction mechanism works on top of existing\npre-trained LLMs. Our exploration also demonstrates that the\nactivation patterns are very close (even 100% matched) in\nthe same LLM with similar input tokens (e.g., 70% - 95%\nsimilarity). This highly coincident feature indicates a huge\npotential to predict activation patterns and, therefore, obtain an\nextra 50% LLM compression rate. Our finding can motivate\nsystem architects to design predictors and prefetch activated\nweights during the execution. This optimization should signif-\nicantly reduce LLM execution latency on resource-constraint\nedge devices because of (1) less waiting time for fetching data\nfrom disk, (2) higher memory hit rate (inactivated neurons will\nnot be loaded into memory to avoid cache/memory pollution),\n(3) less computing requirements (Only compute the activated\nneurons).\nThe main contributions of this paper are outlined as follows:\n\u2022\nWe explore the weight and activation sparsity of state-of-\nthe-art LLMs. For LLMs with limited natural activation\nsparsity, we also examine activation magnitude distribu-\ntions and learn the importance levels of FFN neurons.\n\u2022\nBased on activation magnitude distributions, we enforced\nactivation sparsity to the state-of-the-art LLMs. To the\nbest of our knowledge, this is the first paper to explore\nthe enforcing activation sparsity of state-of-the-art LLMs\nsystematically. Our empirical analysis reveals that we can\nsecure an extra 50% activation sparsity in FFN layers\nwhile maintaining acceptable accuracy.\n\u2022\nTo convert the extra activation sparsity to compression\nbenefits, we further investigate the predictability of ac-\ntivation patterns. According to our experiments, the ac-\ntivation patterns are highly predictable, and LLMs can\nbe further compressed from memory's perspective. By\nfollowing the activation pattern from the predictor, only\nthe activated weights should be prefetch to memory", "sections": [{"title": "A. LLM Selection and Evaluation Environment", "content": "This work targets state-of-the-art LLMs and considers the\ndiversity of internal activation functions. Table I summarizes\nthe pre-trained LLMs we explore, which are collected from\nthe Hugging Face repository. These models were carefully\nchosen for their broad representation of contemporary LLM\narchitectures, ensuring that our findings are applicable across\na diverse set of current models and are not limited by the\nchoice of architectures. For input benchmarks, we utilize the\nWikitext-2 [12] dataset due to its widespread adoption in LLM\nresearch, allowing easier comparison with existing studies."}, {"title": "B. Targeting Components of LLMs for Compression", "content": "Popular LLMs today are based on the transformer archi-\ntecture. Within this architecture, we target the MLP (Multi-\nLayer Perceptron) layers, a.k.a Feed-Forward Network(FFN)\nlayers, for model compression. As highlighted in Figure 1, the\ncomponents within FFN typically include Gate Projection, Up\nProjection, Down Projection, and Activation_Function. The\norder of FFN layers may be varied for different LLMs. The\nanalysis and optimizations of the FFN layers are critical for\nmodel compression and enhance the inference performance be-\ncause the FFN layers contribute about \\documentclass{article}\\usepackage{amsmath}\begin{document}   \\frac{2}{3} \\end{document} of total parameters on\naverage [11]. Therefore, FFN layers are essentially the storage\nand computing bottleneck, and compressing/optimizing these\ncomponents would be beneficial."}, {"title": "C. Enforcing Sparsity in FFN Layers", "content": "Our primary motivation for exploring LLMs' activation spar-\nsity is to minimize the neuron loading from the lower-level\nmemory hierarchy (e.r., SD card or disk) and reduce computing\nrequirements. The potential benefits of activation sparsity are\nsimilar to those from Sparse Matrix Multiplication. The state-\nof-the-art LLMs with non-ReLU activation functions provide\nvery low or no activation sparsity. Because FFN layers store\nmost of the LLM's parameters, enforcing sparsity in FFN\nlayers can substantially reduce the number of active neurons,\nleading to less computing and memory usage. However, the\ncritical premise of enforcing activation sparsity is that the\nLLM accuracy should not be obviously reduced. Moreover,\nas another critical component of transformer-based models,\nattention layers are typically susceptible to being modified\nbecause they aim to capture the dependencies of all elements\nin a sequence. Introducing sparsity in attention layers makes\nit more possible to degrade model accuracy [13]. Therefore,\nwe focus on enforcing sparsity on FFN layers to alleviate the\nstorage and computing bottleneck."}, {"title": "D. Auxiliary Program for Sparsity Analysis", "content": "We developed an auxiliary program for the LLM sparsity\nanalysis, which includes the following main functionalities:\n(1) activation behavior collection, (2) Important Neuron De-\ntermination, and (3) evaluations with specific thresholds."}, {"title": "1) Activation Behavior Collection:", "content": "We built an activation collection model consisting of the\nfollowing four subcomponents.\nInput Sequence: Let X = {X1,X2,...,xN} represent the\nLLM input tokens where N is the number of tokens.\nForward Pass Inference: For each input token xi, perform a\nforward pass through the model to obtain the activation values\nAil for each layer 1:\nAi,l = fi(xi) \t{(1)}\nwhere fi is the activation function of layer l and i\u2208 [1,N]."}, {"title": "Register Hooks:", "content": "For each layer l, register hooks to capture the\nactivation values during the inference. We define A as the\nactivations for layer l across all input tokens:\nA\u2081 = {Ai,l | i \u2208 {1,2,..., N}} \\{(2)}\nActivation Storage: Store the collected activation values At in\nan HDF5 file for next-step processing."}, {"title": "2) Important Neuron Determination:", "content": "Following the Activation Behavior Collection step discussed\nin Section II-D1, we must define the approach to select a\nspecific percentage of neurons with smaller output magnitudes\nthat are less important. The magnitude is directly associated\nwith the importance of the neuron. Therefore, enforcing spar-\nsity to the smaller magnitude neurons should have minimum\ndifference compared to original output. The Important Neuron\nDetermination contains the following substeps.\nFlatten Activation Matrix to Single-Dimension Vector: For\neach layer l, flatten the activation matrix values A into a\nsingle dimension vector Aflat.\nAflat = flatten(A1) \\{(3)}\nConverting to Absolute Values: Compute the absolute values\nof the flattened activation vector. The importance of activation\nvalues are directly associated with magnitudes instead of signs:\nAabs = Aflat \\{(4)}\nSorting: Sort the vector with absolute values in ascending order\nto facilitate the following percentile computation:\nAsorted = sort(Aabs) \\{(5)}\nPercentile Calculation: For a given sparsity level a (e.g.,\n30%), calculate the threshold Ti,a as the lowest a-th percentile\nof Aabs. Pa denotes as the percentile function at sparsity a.\nT\u03b9,a = Pa (Asorted)  \\{(6)}"}, {"title": "3) Evaluations with Specific Sparsity Thresholds:", "content": "After applying a pre-defined activation threshold, we obtain\na new LLM with a different sparsity. Specifically, we imple-\nmented a mechanism to deactivate neurons in gate projection,\nup projection, and down projection components whose values\nfell below the threshold. Activation values are set to zero if\nthey are less than Ti,& in magnitude:\nthresholded\nAnte =  \n0 \\text{ if } Ai,l < Ti,a \\\\\nAi,l \\text{ otherwise}\n \\{(7)}"}]}, {"title": "III. SPARSITY FINDINGS OF STATE-OF-THE-ART LLMS", "content": "A. Weight Magnitude Distributions\nBefore systematically studying the LLMs' sparsity, exploring\ntheir weight magnitude distributions is a prerequisite to un-\nderstanding internal features. Figure 2 illustrates the weight\ndistributions of four recent LLMs: Mistral-7B, LLama-3-8B,\nOPT-6.7B, and Phi-2-2.7B The x-axis represents sequential\nweight bins; each bin shows the weight count in a specific"}, {"title": "B. Natural Activation Sparsity Analysis", "content": "Activation sparsity could be another essential feature and\nopportunity to compress the LLMs, especially when deploying\nthese large-scale AI models in resource-constrained environ-\nments. The sparsity feature will not only provide opportunities\nto reduce memory storage but also lower the computing\nresource requirements. This improvement eventually optimizes\nthe edge LLM execution latency, which provides quick input\nresponses and better user experiences. We show the results\nof OPT-6.7B and Phi-2-2.7B in this section, as they are the\nonly two models that exhibit natural sparsity in their Feed-\nForward Network (FFN) activation values in our study. We do\nnot observe any natural sparsity in other LLMs (Mistral-7B,\nLLama-3-8B and Phi-3-3.8B) we evaluated based on WikiText.\nThe term 'natural activation sparsity' indicates the activation\nsparsity directly comes from the pre-trained LLM instead of\nfrom enforcing sparsity.\nObservation 1 (OPT-6.7B): The activation sparsity histogram\nof FFN layers for the OPT-6.7B is demonstrated in Figure 3.\nThe sparsity percentages from Layer 0 to Layer 31 are all\nhigh, even though a slight sparsity degradation is observed in\ndeeper layers. Layer 30 has 92.83% natural sparsity, which is\nthe minimal sparsity among all layers. According to Table I,\nthe activation function of OPT-6.7B is ReLU. The ReLU\nactivation function's outputs are set to zero if the neuron inputs\nare less than zero. This attribute of the ReLU function is\nthe main reason for obtaining high natural sparsity. Apple's\nrecent work [10] for LLM compression is based on a similar\nobservation and targeting to models with the ReLU activation.\nObservation 2 (Phi-2-2.7B): Figure 4 illustrates the activation\nsparsity of FFN layers for the Phi-2-2.7B model. Even though\na certain degree of sparsity exists, the maximum sparsity level\n(Layer 3) is still less than 6%. The sparsity percentages of the\nmajority of layers are less than 1%. Compared to the OPT-\n6.7B, Phi-2-2.7B shows much lower activation sparsity. The\nmain reason for this discrepancy is their internal activation\nfunctions, where OPT-6.7B utilizes ReLU while Phi-2-2.7B"}, {"title": "C. Activation Magnitude Distributions", "content": "uses NewGELU. By comparing their equation curves, ReLU\nhas a significantly higher possibility of obtaining a '0' activa-\ntion output.\nInsight: The benefits of natural activation sparsity only exist in\nReLU-based LLMs (e.g., OPT-6.7B). Even though we can also\nobserve natural activation sparsity in NewGELU-based LLMs\n(e.g., OPT-6.7B), the sparsity level is too low to make effective\nmodel compression. The state-of-the-art LLMs mainly use\nSwiGLU function. Therefore, we should explore other new\nand general methods to obtain extra activation sparsity for\nLLM compression.\nC. Activation Magnitude Distributions\nFrom the discussion in Section III-B, most state-of-the-art\nLLMs do not exhibit inherent natural activation sparsity due\nto the old function ReLU or NewGELU has been replaced.\nSignificant portions of activation values in recent LLMs do\nnot reach absolute zero. Therefore, this section examines\nactivation magnitude distributions for other potential sparsity\nopportunities. For example, according to the results of activa-\ntion magnitude distributions, can we adjust the sparsity level\nby modifying a certain percentage of small magnitude values\nto zero and increasing the activation sparsity (Section IV)?\nExploring activation magnitude distributions is a crucial pre-\nrequisite step to answer this question. So, the distributions\nof (Phi-3-3.8B, LLaMA-3-8B and Mistral-7B) are shown and\nanalyzed below. These three recent LLMs are non-ReLU-\nbased and do not exhibit natural sparsity."}, {"title": "IV. TRADEOFFS BETWEEN ACTIVATION SPARSITY AND\nPERPLEXITY", "content": "Section III-C concludes that we can obtain a high sparsity level\nby enforcing relatively small threadholds. However, the new\nquestion for enforcing aviation sparsity is how much accuracy\ndegradation will be observed if we get this extra sparsity?\nThis section investigates how much sparsity we can tune\nwhile maintaining an acceptable accuracy degradation. The\nsparsity tuning allows us to make more effective compression\nand support more powerful LLMs on edge systems. The\nLLM accuracies are evaluated by the Perplexity (PPL) scores.\nSimilarly to Section III-C, we shows the results of Phi-3-\n3.8B, LLAMA-3-8B and Mistral-7B for consistency. The x-\naxis represents the sparsity level, ranging from 0% to 50%,\nindicating the percentage of activations enforced to become\nzero. The y-axis shows the Perplexity (PPL) score, where\nlower scores indicate better accuracy when executing the\nWikitext benchmark.\nObservation: Figure 8 illustrates tradeoffs between activation\nsparsity and perplexity (PPL) scores. As the sparsity level\nincreases, the PPL generally rises for all models, indicating\nthat increasing LLM activation sparsity reduces inference\naccuracy. At 30% sparsity level, all models almost keep the\nsame PPL with 0% sparsity. In other words, all three models\ncan obtain 30% sparsity with only negligible accuracy loss.\nLLama-3-8B shows the steepest increase in PPL, indicating\nthe most significant accuracy drop in performance as sparsity\nincreases. However, even for 50% sparsity, the maximum PPL\nfrom LLama-3-8B is still less than 11, which is considered\nacceptable in many application scenarios. In contrast, Mistral-\n7B's accuracy is most stable when changing activation sparsity.\nInsight: We can obtain an extra 50% activation sparsity for\nthe state-of-the-art LLMs by enforcing the threshold setting\nwhile maintaining acceptable PPL/accuracy."}, {"title": "V. PREDICTABILITY ANALYSIS FOR LLM ACTIVATION\nPATTERNS", "content": "According to our experimental data and analysis in Section IV,\nwe conclude that we can enforce around 50% activation\nsparsity to state-of-the-art LLMs while maintaining almost\nthe same accuracy/perplexity. A straightforward way to utilize\nactivation sparsity to compress LLMs is via activation pattern\nprediction. We can predict the future activation pattern and\nonly fetch the possible active weights from the disk or SD\ncard to the main memory. A reasonably predicted success rate\ncan provide benefits, including lower LLM response latency,\nfewer computing resource requirements, and better main mem-\nory utilization. Dhar et al. [2] indicate that memory is the\nessential bottleneck for LLM edge deployment, significantly\nincreasing the LLM execution latency on resource-constraint\ndevices. Success predictions have great potential to alleviate\nmemory bottlenecks. The incorrect prediction has an extra\nlatency penalty because the system must fetch the mispredicted\nweights from the disk to the main memory. However, the\nmisprediction in this problem is not a correctness issue. This\nsection will systematically explore LLM activation patterns\nand analyze their predictability. We aim to provide a guideline\nfor system architects to design an effective prefetcher to assist\nLLM deployment on edge."}, {"title": "A. Potential Prediction Approach", "content": "LLM typically consists of multiple deep layers. E.g., LLAMA-\n3-8B has 32 layers. During the LLM inference process, the\nearlier layer executes first, and its outputs are forwarded to the\nfollowing layer for further processing. Therefore, we can use\nthe earlier layer's activation patterns to predict future layers'\nactivation patterns. E.g., we may use the pattern in Layer 1\nto predict the patterns of Layer 16-32. As shown in Shad-\nowLLM's [14] approach, a lightweight predictor, with only\none hidden layer, is capable of predicting activation patterns in\nfuture layers. This method introduces minimal computational\noverhead compared to the benefits it provides. Selectively\nprefetching the predicted weights significantly reduces mem-\nory and CPU usage during inference. The system has time\nto prefetch the predicted weights in the future layers and\nreduce the disk data loading time. From the main memory's\nperspective, the executing LLM is effectively compressed if we\ncan achieve a reasonable success rate for pattern prediction."}, {"title": "B. Pattern Similarity and Predictability Analysis", "content": "To evaluate the predictability of the LLM activation patterns,\nwe randomly select 12 samples as the input prompts, where\neach sample consists of 2048 tokens. User input prompts are\nalways difficult to match 100%, even when they want to ask\nthe same question. Therefore, we also develop variants for\nthese 12 samples with 70% to 95% similarity to tolerate the\ninconsistency of user inputs. The '70% to 95% similarity'\nindicates the '30% to 5%' modifications on the original sam-\nple. Considering many recent LLMs using the same SwiGLU\nactivation function, for simplicity, we evaluate LLaMa-3-8b as\na representative in this section."}, {"title": "VI. RELATED WORKS", "content": "LLM Compression for Edge Systems. Despite the impres-\nsive growing capabilities of LLMs, the quick escalation in\nmodel size has led to an exponential increase in the memory\nand computational demands, presenting significant deployment\nchallenges (Kaplan et al. [15]; Liu et al. [9]) in resource-\nconstraint edge devices. Various compression techniques have\nbeen proposed to harvest the benefits of local LLM intelligence\nin edge devices, which includes quantization [16], [17], prun-\ning [16], [18] and model distillation [19]-[21]. Additionally,\nefficient sampling methods [22], [23] have been proposed\nto expedite inference decoding. Our work investigates the\npotential of forced activation sparsity, which is orthogonal to\nthe above-mentioned approaches and may combine with them\nto enhance the LLM compression further."}, {"title": "Activation Sparsity Utilization.", "content": "Activation sparsity occurs\nwhen a portion of activation neuron outputs are zero. The\ncomputation of these inactive neurons can be ignored, which\nindicates that they are unnecessary to be fetched into the main\nmemory and execute the computation. Li et al. [24] and Liu et\nal. [9] have recognized the intrinsic activation sparsity found\nin several LLMs and have utilized this characteristic to speed\nup inference. These techniques can be combined to enhance\nperformance effectively. Despite these insights, no prior work\nhas provided concrete model results after employing activation\nsparsity. However, our study aims to bridge this gap by\nnot only implementing activation sparsity but also presenting\ndetailed performance metrics, thus validating the practical\nbenefits of this approach."}, {"title": "ReLUfication.", "content": "The activation sparsity benefits from previous\nresearch are limited to ReLU-based Transformer architectures,\nincluding LLMs such as T5 (Raffel et al. [25]) and OPT\n(Zhang et al. [26]), as well as in some vision models like\nViT (Dosovitskiy et al. [27]). However, recent LLMs such\nas Falcon (Almazrouei et al. [28]) and LLaMa (Touvron et\nal. [29]) predominantly use non-ReLU activation functions\nlike GELU (Hendrycks and Gimpel et al. [30]) and Swish\n(Elfwing et al. [31]), which do not directly provide activation\nsparsity. To utilize the benefits of activation sparsity with-\nout developing a ReLU-activated LLM from scratch, many\nresearchers have adopted a technique known as ReLUfication.\nThis method introduces sparse ReLU-based activations into\nnon-ReLU LLMs. For instance, Zhang et al. [32] successfully\nconverted a GELU-activated BERT (Devlin et al. [33]) into\na ReLU-activated version by directly swapping the activation\nfunction. Similarly, Zhang et al. [34] applied this approach to\nFalcon and LLaMa, which originally used GELU and Swish,\nrespectively. However, simply replacing the activation function\noften fails to achieve satisfactory sparsity, primarily due to\nthe unaddressed inherent limitations of the original non-ReLU\nactivation distributions. Instead of utilizing ReLUfication, we\npropose to enforce sparsity without altering the activation\nfunction. This method retains the original activation function's\nproperties while leveraging sparsity benefits. Our approach\ndirectly omits the lower magnitude values to induce sparsity,\noffering a novel and effective method for LLM compression."}, {"title": "VII. CONCLUSION", "content": "Deploying LLM on edge devices offers numerous compelling\nbenefits. Recently, major tech companies have started to invest\nin and seek edge LLM solutions. However, the insufficient\ncomputing and memory resources on edge devices restricted\neffective LLM executions. Most regular edge devices are still\nunable to meet the requirements of LLM execution, even when\ncombined with existing compression techniques. Therefore,\nwe explore new schemes that could seamlessly combine with\nregular LLM compression approaches. The new activation\nfunctions of state-of-the-art LLMs eliminate the feasibility\nof Apple's compression approach, which performs well for\nReLU-based LLMs. In this work, we search for new compres-\nsion opportunities from activation sparsity, which can benefit\nthe majority of general LLMs and not be bound by specific\nactivation functions. Our systematic explorations indicate that\nwe can safely secure 50% extra sparsity in FFN layers with a\nnegligible accuracy loss for the state-of-the-art LLMs. This\nfinding allows us to compress the LLMs from memory's\nperspective via prediction and prefetching. Moreover, to verify\nthe predictability of LLM activation patterns, we also analyze\nmatching rates of LLM activation patterns with multiple user\ninput variants. We finally conclude that the LLM activation\npatterns are highly predictable and have a huge potential to\nget extra sparsity. The paper provides a guideline for design\npattern predictors with prefetching capabilities, leading to\nless LLM execution latency, lower power cost, and improved\nuser experience. Moreover, besides general LLMs, our new\ncompression approach can be simply extended to other large-\nscale transformer-based AI models."}]}