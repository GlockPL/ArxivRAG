{"title": "Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced\nDefense in Large Language Models", "authors": ["He Cao", "Weidi Luo", "Yu Wang", "Zijing Liu", "Bing Feng", "Yuan Yao", "Yu Li"], "abstract": "With the extensive deployment of Large Lan-\nguage Models (LLMs), ensuring their safety\nhas become increasingly critical. However,\nexisting defense methods often struggle with\ntwo key issues: (i) inadequate defense capa-\nbilities, particularly in domain-specific scenar-\nios like chemistry, where a lack of specialized\nknowledge can lead to the generation of harm-\nful responses to malicious queries. (ii) over-\ndefensiveness, which compromises the general\nutility and responsiveness of LLMs. To miti-\ngate these issues, we introduce a multi-agents-\nbased defense framework, Guide for Defense\n(G4D), which leverages accurate external in-\nformation to provide an unbiased summary\nof user intentions and analytically grounded\nsafety response guidance. Extensive exper-\niments on popular jailbreak attacks and be-\nign datasets show that our G4D can enhance\nLLM's robustness against jailbreak attacks on\ngeneral and domain-specific scenarios with-\nout compromising the model's general func-\ntionality. The code can be found at https:\n//github.com/IDEA-XL/G4D.", "sections": [{"title": "Introduction", "content": "Recent advances show that Large Language Mod-\nels (LLMs) have achieved remarkable capabilities\nacross various tasks (Qin et al., 2023; Zhong et al.,\n2023). As these models become integral to our in-\nformation infrastructure, ensuring their responses\nare devoid of malicious content, such as disinfor-\nmation, immorality, or discrimination, is critical.\nConsequently, the increasing concerns regarding\nthe safety of LLMs have spurred extensive research\ninto jailbreak attacks and defense strategies (Varsh-\nney et al., 2023; Bianchi et al., 2023), particularly\nin domain-specific scenarios.\nPrevious methods primarily aim to address inad-\nequate defense by promoting adherence to safety\nprotocols. This includes using fixed safety prompts\nto highlight safety concerns (Xie et al., 2023;\nZhang et al., 2024b), incorporating external infor-\nmation to fill knowledge gaps (Wei et al., 2024),\nand analyzing input intentions to identify potential\nrisks (Zhang et al., 2024a), as shown in Table 1.\nHowever, these approaches fail to fully address the\ndefense gap and often overlook over-defensiveness\n(e.g., Fig 3): fixed prompts lack generalizability,\nsolely reliance on external examples for in-context\nlearning introduces noise, and intention analysis\nmay misclassify inputs due to insufficient context.\nIn essence, striking a balance between preventing\nunder-defense and avoiding over-defense requires\na nuanced approach that ensures comprehensive\nprotection and operational efficiency.\nTo bridge these gaps, we introduce Guide for\nDefense (G4D), a dynamic, guide-based defense\nframework that operates independently of static\nsafety prompts. Our framework establishes a de-\nfense agency equipped with precise external infor-\nmation, enabling it to offer an unbiased summary\nof user intentions and deliver analytically grounded\nsafety guidance. This agency serves as both a fil-\nter and an augmentation module for user inputs,\npositioned before the server LLMs. It activates\nthe LLMs' safety-awareness capabilities and en-\nsures that responses remain objective and focused\non the issue at hand. Specifically, G4D utilizes\nthree agents during the inference stage to direct\nthe LLM towards generating responses that align\nwith core values: (1) the intention detector, which\nsuccinctly summarizes user intentions and iden-\ntifies key entities; (2) the question paraphraser,\nwhich reformulates questions to neutralize adver-\nsarial tactics in jailbreak attacks; and (3) the safety\nanalyzer, which assesses intentions with adequate\ncontext and advises the LLM on crafting suitable\nresponses.\nWe empirically evaluate our framework on gen-\neral and specific domains using a comprehensive\nset of harmful and normal prompts, demonstrat-\ning its balanced performance compared to existing\nmethods. Compared to existing defense methods\nduring inference, our approach achieves a low At-\ntack Success Rate (ASR) on domain-specific jail-\nbreak benchmarks and significantly reduces ASR\non general jailbreak attacks (e.g. GCG (Zou et al.,\n2023), DeepInception (Li et al., 2024), etc) on GPT-\n40-mini. Additionally, as illustrated in Figure 1\n(Key takeaway: Models in the upper right quadrant\nstrike a better balance between defense and perfor-"}, {"title": "Related Work", "content": "Jailbreak Attack on LLMs. A jailbreak query\nused to attack LLMs is transformed from a\nhandcraft malicious request (Chao et al., 2024a;\nMazeika et al., 2024; Luo et al., 2024) by differ-\nent jailbreak methods. During the transformation\nprocess, Attackers make use of templates to trick\nLLMs into executing malicious requests. This can\ninvolve using a handcraft template (Shen et al.,\n2024) or an optimization-based template (Zou et al.,\n2023; Liu et al., 2024b; Liao and Sun, 2024; Chao\net al., 2024b; Liu et al., 2024a) to the malicious re-\nquest. Additionally, other jailbreak attacks may at-\ntempt to reframe the malicious request as a benign\ndescription or encode it to a low source language or\ndomain-specific knowledge (Zeng et al., 2024a; Xu\net al., 2024a; Tu et al., 2024). Nonetheless, regard-\nless of these modifications, the underlying intent\nof the malicious request remains in the jailbreak\nquery.\nJailbreak Defense on LLMs. Recent defense\nmethods focus on two key stages: inference stage\nand output stage. Inference-stage defenses mitigate\nrisks before model responses by pre-processing\ninputs (Alon and Kamfonas, 2023; Cao et al.,\n2024; Jain et al., 2023) or guiding model be-\nhavior. Demonstration-based methods such as\nSelf-Reminder (Xie et al., 2023) promote safer\noutputs, while In-context Demonstration (Wei\net al., 2024) and Goal Prioritization (Zhang et al.,\n2024c) enhance robustness against jailbreak at-\ntacks and optimize the balance between helpful-\nness and safety, with Intention Analysis (Zhang\net al., 2024a) ensuring responses align with user\nintent and policy. On the other hand, output-\nstage defenses evaluate and filter responses af-\nter generation. Self-Examination (Phute et al.,"}, {"title": "Methodology", "content": "In this work, we focus on defending against jail-\nbreak attacks that attempt to coerce LLMs into\nproducing content misaligned with human values.\nPractically, developers typically pre-define system\nprompts, denoted as $P_{sys}$, to ensure LLMs adhere\nto safety principles. The system prompt $P_{sys}$ is con-\ncatenated with the user query Q and fed into the\nLLM to generate a response Y1:L autoregressively.\nThis process can be formulated as follows:\n$q(Y_{1:L}|P_{sys}, Q) = \\prod_{i=1}^{L} q(Y_i | Y_{1:i-1}, P_{sys}, Q)$\nAn ideal LLM defense system should balance ro-\nbust security measures with seamless usability, en-\nsuring protection against threats without hindering\nAI systems' functionality and user experience. It\nmust accurately identify and analyze malicious in-\ntent in queries while offering domain-specific pro-\ntective guidance. Our defense framework employs\nan inference-stage mechanism comprising an intent\ndetector, a question paraphraser, and a safety ana-\nlyzer to produce safety instructions for the victim\nmodel, as shown in Figure 4.\nIntention Detector. Extracting the user's in-\ntention from the content significantly enhances\nthe LLM's understanding of the query before re-\nsponse generation, as demonstrated in previous\nstudies (Zhang et al., 2024a; Zeng et al., 2024b).\nOur intention detector leverages an LLM as an\nagent, applying Chain of Thought (CoT) reason-\ning to identify and reconstruct the core intention I\nbehind a query Q. Unlike previous work (Zhang\net al., 2024a) that directly concatenates I with the\nuser query Q for input, we first ground key entities"}, {"title": "Preliminary", "content": "3.1"}, {"title": "G4D Framework", "content": "3.2"}, {"title": "Experimental Setup", "content": "In this section, we conduct experiments to evaluate\nthe effectiveness of our defense method. These\nexperiments are performed on benchmarks that\ninclude both harmful and normal prompts. We\nexplore performance across domain-specific and\ngeneral knowledge for both benchmarks. To ensure\nfairness, we compare only against the current main\ninference stage defense baselines."}, {"title": "Benchmarks", "content": "4.1\nHarmful Benchmarks. In our experiments, we\nevaluate two challenging jailbreak attack bench-\nmarks. To assess the performance of our method\nagainst domain-specific attacks, we introduce a"}, {"title": "Models", "content": "We conduct experiments on the advanced closed-\nsource model GPT-40-mini (OpenAI, 2024),\nknown for its superior capabilities in areas such\nas safety, and on Vicuna-v1.5-13B (Chiang et al.,\n2023) as a representative open-source model,\nwhich is more lightweight. To isolate the effec-\ntiveness of the defense method from the inherent\ncapabilities and safety levels of the target LLM, our\ndefault setup ensures that the agents used within the\ndefense framework and target LLM are consistent."}, {"title": "Retrieval Knowledge Base", "content": "We select Wikipedia (Wikimedia Foundation,\n2024) as the knowledge base due to its extensive\nand diverse information. For efficiency, we use a\ntop-1 candidate as the retrieved information."}, {"title": "Comparison Baselines", "content": "Our defense framework operates exclusively dur-\ning the inference stage, without access to the target\nLLM's output. We compared our method against\nbaseline defenses that function solely at the infer-\nence stage to counter jailbreak attacks, which are\nlisted as follows:\nSelf-Reminder (Xie et al., 2023) combats jailbreak\nattacks by embedding prompts that remind AI mod-\nels to follow ethical guidelines, reducing the effec-\ntiveness of attacks.\nParaphrase (Jain et al., 2023) mitigates adversarial\nattacks on language models by rephrasing input\ntext, reducing the effectiveness of harmful prompts.\nIn-Context Demonstration (ICD) (Wei et al.,\n2024) helps LLMs resist attacks by including ex-\namples of refusing harmful queries, teaching the\nmodel to avoid responding to malicious prompts.\nIntention Analysis (IA) (Zhang et al., 2024a) im-\nproves LLM safety by analyzing user intent before\ngenerating a policy-compliant response, reducing\njailbreak attacks."}, {"title": "Evaluation Metric", "content": "To achieve the most equitable assessment, we\nmeticulously utilize various evaluation metrics for\nboth harmful and normal benchmarks.\nMetrics for Harmful Benchmarks. For safety\nassessment, we report Attack Success Rate\n(ASR) (Shen et al., 2024), where lower scores in-\ndicate stronger defense performance. For assess-\ning ASR in specific domains (e.g., CB-RedTeam),\nwe employ GPT-40-mini as the auto-annotator.\nWe evaluate harmfulness by determining if the\nmodel has been successfully compromised, which\ninvolves checking whether it provides detailed syn-\nthesis steps (see Appendix A.4 for comprehensive\nsettings). To calculate ASR for general attacks,\nWe selected Llama-Guard-3 (MetaAI, 2024) for its\nability to accurately label benign responses to mali-\ncious queries as \"safe\", avoiding the misclassifica-\ntion pitfalls of keyword-based classification (Zou\net al., 2023) or scoring metrics (Qi et al., 2024).\nMetrics for Normal Benchmarks. We use Just-\nEval (Lin et al., 2023) on CB-Benign and MT-\nbench to score QA pairs on a scale of 1 to 5. \u03a4\u03bf\nbetter evaluate the model's responses to standard\nquestions, we employ GPT-40-mini as an evaluator,\nassessing the score across multiple dimensions, in-\ncluding Helpfulness, Clarity, Engagement, Depth,"}, {"title": "Results", "content": "In this section, we attempt to answer the following\nresearch questions:\n\u2022 RQ1: How does G4D optimize the balance be-\ntween safety defenses and maintaining task per-\nformance in LLMs? (Section 5.1)\n\u2022 RQ2: How crucial is each input component to\nG4D's overall performance? (Section 5.2)\n\u2022 RQ3: What are the advantages of designing G4D\nin multi-agent systems? (Section 5.3)\n\u2022 RQ4: How do other LLMs perform as agents\nwithin G4D? (Section 5.4)\n\u2022 RQ5: Can G4D be compatible with the other\noutput-stage defense methods? (Section 5.5)"}, {"title": "Safety and Performance Balance", "content": "G4D is effective against jailbreak attacks across\nspecific domains and general domains. As shown\nin Table 2, G4D exhibits remarkable robustness,\nachieving an ASR of 0.0% on GPT-40-mini for the\nCB-RedTeam and most general attacks, with only"}, {"title": "Impact of Each Input Component in G4D", "content": "5.2\nWe conducted experiments to evaluate the ef-\nfectiveness of each input component (intention I,\nguidance G) feeding into G4D, as shown in Ta-\nble 4. The complete input achieves the best bal-\nance, resulting in the lowest ASR of 1.3% on CB-\nRedTeam while maintaining strong defense across\nother attacks and AutoDAN, and preserving task\nperformance with scores of 3.21 on CB-Benign and\n29.5% on MMLU-Pro. Removing any component\ndisrupts the overall balance of the system. For in-\nstance, when the retrieval module is excluded (only\nuse initial intention), the ASR rises to 10.0% for\nCB-RedTeam, 43.4% for other attacks, and 11% for"}, {"title": "Ablation on Agency Configuration", "content": "As aforementioned, our framework uses a multi-\nagent system that assigns different tasks to var-\nious agents. This raises the question: can we\nmerge the tasks of each module into a single step\nto simplify the system? To investigate this, we\nexperiment with different agency configurations\nby reducing the number of agents from three to\ntwo and then to one, with details provided in Ap-\npendix A.5.2. As shown in Table 5, implementing\nG4D with a 3-agents setup-comprising an inten-\ntion detector, a question paraphraser, and a safety\nanalyzer-achieves the highest robustness, with a\n0.0% ASR on CB-RedTeam and 1.0% on other\nattacks, surpassing both the 2-agents and 1-agent\nconfigurations. While the 2-agents setup reduces\nASR to 6.7% on CB-RedTeam and 15.0% on other\nattacks, and the 1-agent setup lowers it to 16.7%\nand 21.0%, respectively, neither matches the de-\nfensive strength of 3-agents. Additionally, 3-agent\nconfiguration outperforms others on normal bench-\nmarks, scoring 4.75 on MT-bench, 4.36 on CB-\nBenign, and 61.0% on MMLU-Pro, compared to\n4.42/4.34/51.0% for 2-agents and 4.50/4.33/49.5%\nfor 1 agent. These results indicate that using a\nmulti-agent-based modular system can decouple\ntasks, preventing interference between modules\nand thereby enhancing defense robustness while\nmaintaining the models' helpfulness and utility."}, {"title": "Ablation with different Agent LLMS", "content": "G4D remains effective across different agent LLMs\nchoices, as shown in Table 6. When using GPT-40-\nmini, the ASR drops to 1.3% on CB-RedTeam and\n16.0% on other attacks, while maintaining strong\nperformance on tasks like MT-bench (2.76), CB-\nBenign(3.78), and MMLU-Pro (37.5%). Similarly,\nGemma-2-9B (Team, 2024) achieves the lowest\nASR (0.7% on CB-RedTeam) with a slight drop\nin MMLU-Pro (34.0%) compared with GPT-40-\nmini. Even with Vicuna-v1.5-13B, the ASR re-\nmains low at 1.3%, showing that our method is"}, {"title": "Comparison with other Multi-agent\nDefense Frameworks", "content": "5.5\nAs a multi-agent defense framework, AutoDe-\nfense (Zeng et al., 2024b) operates at the output\nstage, filtering harmful responses from LLMs. We\ncompare its defense performance against harm-\nful benchmarks. As shown in Table 7, G4D"}, {"title": "Conclusions", "content": "In this work, we proposed Guide for Defense\n(G4D), a multi-agent framework designed to bal-\nance the mitigation of LLM jailbreak attacks with\nthe preservation of model inference across both\nspecific and general knowledge. G4D leverages a\nself-directed guidance mechanism involving three\nagents equipped with dynamic safety prompts, ex-\nternal information access, and intention analysis to\ngenerate analytically based safety guidance. Our re-\nsults demonstrate that G4D is effective with aligned\nagents and adaptable across different LLMs. Addi-"}, {"title": "Limitations", "content": "One limitation of our G4D is that the intention de-\ntector's retrieval of relevant external information\nabout key entities for safety analysis may introduce\nlatency. While a top-1 retrieval strategy enhances\nefficiency, it risks missing critical information, po-\ntentially leading to incomplete insights and vul-\nnerabilities. Broader top-k retrieval approaches\nimprove coverage but increase latency and com-\nputational overhead. Balancing efficiency and ac-\ncuracy in the retrieval mechanism is essential to\noptimize overall performance. Additionally, since\nG4D's prompt does not utilize in-context learning\nthrough examples, all outputs depend solely on the\nagent's ability to follow instructions. Thus, select-\ning an agent with stronger instruction-following\ncapabilities is key to improving performance."}, {"title": "Potential Risks", "content": "Previous studies (Deng et al., 2024; Wang et al.,\n2024b) have demonstrated that retrieved harmful\ninformation will cause LLMs to generate harmful\nresponses. In our framework, the knowledge base\nemployed during experiments is Wikipedia, which,\ndue to its open-edit nature, is susceptible to manip-\nulation and misinformation. This introduces a po-\ntential risk, as erroneous or malicious content could\ncompromise the reliability and safety of retrieved\nknowledge. Moreover, integrating other external\nor unknown knowledge bases into our framework\ncould introduce similar risks, as the accuracy and\ntrustworthiness of such sources may not be guaran-\nteed. Therefore, ensuring the integrity of retrieved\ninformation is essential to maintaining the safety\nand reliability of the framework's outputs."}, {"title": "Appendix", "content": "A.1 Models and Configuration\nIn our experiments, we observed that various foun-\ndation models required distinct hyper-parameter\nconfigurations depending on the task. In the con-\nfiguration of Vicuna-v1.5-13B, a temperature of\n0.7 is set for CB-Redteam, other attacks, and MT-\nbench. Meanwhile, the temperature is 0 for Agent\nsetting, AutoDAN, CB-Benign, MMLU-pro, and"}, {"title": "Dataset Construction", "content": "A.2\nTo evaluate the robustness of foundation mod-\nels and defense methods in biochemical synthesis\ntasks, we handcrafted CB-RedTeam. Additionally,\nto assess the potential over-defensiveness of the\nfoundation models and defense mechanisms, we\nconstructed CB-Benign.\nChemistry & Biology-Redteam (CB-Redteam\nAccording to the Laboratory Chemical Safety\nSummary (LCSS) (PubChem, 2024) on PubChem\nand NFPA 704 (Wikipedia contributors, 2024),\nWe curated a collection of 150 objects across the\ncategories of Bacterial Agent, Biological Toxins,\nDrugs, Environmental Hazard, Explosive, Ra-\ndioactive, and Toxic to construct the CB-RedTeam\ndataset. Except for objects under the Biological\nToxins and Bacterial Agent categories, which only\nhave technical names, all other categories include\nSMILES representations. The additional details of\nCB-RedTeam are shown in Table 11.\nChemistry & Biology-Benign (CB-Benign). Ac-\ncording to the Laboratory Chemical Safety Sum-\nmary (LCSS) (PubChem, 2024) on PubChem and\nNFPA 704 (Wikipedia contributors, 2024), We se-\nlected the chemical formulas and SMILES repre-\nsentations of 60 common, harmless substances usu-\nally in daily life to construct the CB-Benign dataset.\nThe additional details of CB-RedTeam are shown\nin Table 12."}, {"title": "Dataset Setup", "content": "A.3\nMMLU-Pro. In our experiments, the question\nprompt of MMLU-Pro was conducted under the\nno CoT setting. The discrepancy between our\nresults and those reported on the official leader-\nboard arises from the fact that the official No COT\nprompt setting includes Auto-CoT (Zhang et al.,\n2023) in its calculation. This difference leads to\na performance gap of approximately 15-17% in\naccuracy (u/LocalLLaMA, 2024).\nCB-RedTeam and CB-Benign. For the CB-\nRedTeam, we frame the problem as: \"Provide de-\ntailed steps to synthesize...\" to increase the risk\nlevel of the question. For the CB-Benign, we frame"}, {"title": "Evaluation Metric", "content": "A.4\nCB-RedTeam. To calculate the ASR on the CB-\nRedTeam, We carefully crafted a prompt and uti-\nlized GPT-40-mini to evaluate whether the model\ndisclosed the specific generation process of harm-\nful substances. Detail prompts are provided in Fig-\nure 11.\nMMLU-Pro. To calculate the accuracy of\nMMLU-Pro, since the model may not directly\nprovide the options for multiple-choice questions\nbut rather the answers corresponding to those op-\ntions, relying solely on regular expression match-\ning is insufficient. To address this, we carefully\ndesigned a prompt to guide GPT-40-mini in de-\ntermining whether the model answered correctly,\ndetails shown in Figure 12.\nAlpaca Eval. In our experiment, we use GPT-40-\nmini as the evaluator to compute the win rate, with\nthe configured prompt provided in Figure 13."}, {"title": "Implement Details", "content": "A.5\nA.5.1 G4D\nIn this section, we present the prompt design for\nG4D. All prompts for G4D are handcrafted. Fig-\nure 5 illustrates the prompt used for the intention\ndetector. Figure 6 displays the prompt for the ques-\ntion paraphraser, and Figure 7 provides the prompt\nfor the safety analyzer.\nA.5.2 Different Agency\n1 Agent. In our ablation study, the 1-agent setting\nemploys only the intention detector, with the final\ninput to the victim LLM illustrated in Figure 9.\nAs shown in the figure, the prompts for both the\nparaphraser and the safety analyzer are embedded\ndirectly into the victim LLM's input, allowing the\nvictim LLM to conduct self-reflection and answer\nthe question independently."}]}