{"title": "Demographic Attributes Prediction from Speech\nUsing WavLM Embeddings", "authors": ["Yuchen Yang", "Thomas Thebaud", "Najim Dehak"], "abstract": "This paper introduces a general classifier based\non WavLM features, to infer demographic characteristics, such\nas age, gender, native language, education, and country, from\nspeech. Demographic feature prediction plays a crucial role\nin applications like language learning, accessibility, and digital\nforensics, enabling more personalized and inclusive technologies.\nLeveraging pretrained models for embedding extraction, the\nproposed framework identifies key acoustic and linguistic fea-\ntures associated with demographic attributes, achieving a Mean\nAbsolute Error (MAE) of 4.94 for age prediction and over 99.81%\naccuracy for gender classification across various datasets. Our\nsystem improves upon existing models by up to relative 30% in\nMAE and up to relative 10% in accuracy and F1 scores across\ntasks, leveraging a diverse range of datasets and large pretrained\nmodels to ensure robustness and generalizability. This study\noffers new insights into speaker diversity and provides a strong\nfoundation for future research in speech-based demographic\nprofiling.", "sections": [{"title": "I. INTRODUCTION", "content": "Demographic insights derived from speech data, such as a\nspeaker's age, gender, native language, education, and coun-\ntry, provide valuable knowledge for applications in tailored\nservices, accessibility, and sociolinguistics [14]. By predicting\nthese attributes, technology can adapt to diverse user demo-\ngraphics, enabling more fairness in speech systems without\ntouching the identity of a specific speaker.\nSpeech contains a wealth of information, not only linguistic\ncontent but also cues about a speaker's affective state [22],\nhealth [15], and personal characteristics [14]. Leveraging\nthese non-linguistic features for demographic prediction opens\nup opportunities for robust, generalized models that capture\ndiverse speaker traits.\nThis work proposes a novel approach to demographic at-\ntribute prediction by:\n1) Leveraging pretrained foundational models, such as\nWavLM [12], for embedding extraction.\n2) Combining multiple datasets with diverse demographic\nattributes to improve generalization and robustness.\n3) Evaluating cross-dataset performance to demonstrate the\nadaptability of the approach across varied linguistic and\ndemographic contexts.\nThe framework uses pretrained embedding extraction mod-\nels to generate high-dimensional, speaker-specific represen-\ntations, which are then fed into task-specific classification\nand regression heads. For age prediction (a regression task),\nand categorical attributes like gender and native language\n(classification tasks), we employ Multilayer Perceptron (MLP)\n[24], ResNet32 [10], and Long Short-Term Memory (LSTM)\n[11] architectures. These serve as the classification or regres-\nsion heads of the overall model, building upon the frozen\nembeddings provided by the pretrained models. The MLP\narchitecture consists of three layers, while ResNet32 leverages\nresidual connections for deep feature propagation, and LSTM\nmodels sequential patterns, making it ideal for tasks involving\ntime-dependent speech features.\nBy integrating foundational models and diverse datasets, this\napproach achieves commendable performance on demographic\nattribute prediction tasks, paving the way for robust cross-\ndataset generalization.\nSection II reviews previous works on pretrained models for\nembedding extraction and studies relevant to speaker attribute\nprediction [2], [12], [14]. Section III-A describes the datasets\nemployed, covering diverse demographic attributes [5]\u2013[9].\nSection III-B outlines the evaluation metrics, including Mean\nAbsolute Error (MAE) for age prediction and accuracy and\nF1 scores for categorical characteristics. Section III explains\nthe experimental workflow, including feature extraction, model\narchitectures, and training and evaluation methods. Section IV\npresents results, comparing the proposed approach to state-\nof-the-art methodologies and assessing its effectiveness in\nrepresenting demographic diversity.\nThis research introduces a framework for demographic\nprofiling of speech data, leveraging feature extraction methods\nand WavLM pretrained models [2]. The main contributions of\nthis work are as follows:\n\u2022 Adaptation of WavLM Embeddings for Demographic\nPrediction: While WavLM [2] has been used to improve\nthe state-of-the-art on many speaker-related tasks, such as\nemotion recognition and speaker recognition, we propose\na method to predict speaker characteristics using its\nfeatures."}, {"title": "II. RELATED WORKS", "content": "Recent advancements in deep learning, particularly self-\nsupervised models, have revolutionized speaker attribute pre-\ndiction by providing high-dimensional embeddings that cap-\nture diverse speech characteristics [12]. These embeddings are\nincreasingly employed for all speech tasks, including speaker\nrecognition.\nA. Foundational Models for Attribute Prediction\nFoundational models, such as WavLM, leverage self-\nsupervised learning to extract intricate patterns from speech\nwithout requiring labeled data. These models produce embed-\ndings that are versatile for various downstream tasks, including\ndemographic attribute prediction.\nBased on the Wav2Vec 2.0 framework [12], WavLM is a\nself-supervised model trained on diverse datasets to capture\nacoustic and speaker-specific features. Its embeddings excel\nin speech emotion recognition [27] and speaker recognition\ntasks [28].\nB. Speaker Attribute Prediction\nSpeaker attribute prediction has traditionally relied on hand-\ncrafted features or statistical models such as i-vectors [29],\nwhich represent compact speaker-specific embeddings. More\nrecently, the statistical approaches were supplanted by the\nneural approaches such as x-vectors [30].\nRecent work has shifted towards deep neural network-based\nmethods. Studies like those by Kwasny et al. [1] and Hechmi et\nal. [13] demonstrate that embeddings from pretrained models,\nsuch as WavLM and Wav2Vec, significantly outperform tradi-\ntional approaches. These self-supervised embeddings capture\ngeneral-purpose representations, enabling robust demographic\nprofiling without task-specific feature engineering.\nSpeaker attribute prediction now primarily uses deep neural\nnetworks to predict demographics such as age and gender\n[23]. Kwasny et al. [1] and Hechmi et al. [13] illustrate that\nembeddings from pre-trained speaker verification models can\nachieve high scores on age and gender classification tasks.\nRecent advances in x-vector-based techniques have given\nspeaker analysis a new layer of analysis for prosody, di-\nalect, and speaker identity. Such embeddings capture high-\ndimensional data that eliminates the need for tedious feature\nengineering. For example, traditional features like MFCCs re-\nquire extensive preprocessing and domain expertise to extract\nmeaningful information, while self-supervised embeddings,\nlike those of Oord et al. [16], inherently encode rich speaker-\nspecific attributes. Comparative experiments have shown that\nself-supervised embeddings frequently outperform manual fea-\nture sets for demographic profiling purposes [32]."}, {"title": "III. EXPERIMENTAL PIPELINE", "content": "In this section, we present the datasets, metrics, pretrained\nmodels and architecture choices, as well as the experiments\nperformed.\nA. Datasets\nWe use five different datasets, each offering a broad range of\ndemographic variables and linguistic contexts. Table I shows\nthe distribution of speakers and audio segments within the\ndevelopment and test sets of each dataset. All datasets are\nsampled at 16kHz.\n\u2022 TIMIT [5] - The TIMIT dataset is a corpus for phoneme\nalignment and speech recognition tasks which contains\nrecordings from 630 speakers. The corpus includes 6,300\nutterances, where each speaker recites the same set of\nsentences. These recordings are accompanied by time-\naligned word and phoneme transcriptions, enabling pre-\ncise phonetic analysis. Additionally, the dataset includes\nan Edu column with 5 categories: 'BS' (Bachelor's), 'HS'\n(High School), 'MS' (Master's), 'PHD' (Doctorate), and\n'AS' (Associate).\n\u2022 VoxCeleb2 [6] VoxCeleb2 contains over 1 million\nutterances from 5,994 speakers, sourced from publicly\navailable YouTube videos. The dataset spans a wide\nvariety of accents and recording conditions, making it\nsuitable for speaker recognition tasks. While VoxCeleb2\ndoes not include explicit demographic labels such as age,\ndemographic data can be inferred or cross-referenced\nfrom external sources. To compare with established base-\nlines, we follow the annotations proposed by Hechmi et\nal. [13], using wikipedia-extracted information for the\nspeakers.\n\u2022 L2Arctic [7] - The L2-Arctic dataset consists of speech\nfrom 24 non-native L2 English speakers with with 6\nnative language categories: Arabic, Hindi, Korean, Man-\ndarin, Spanish, and Vietnamese. Each speaker records\nphonetically balanced English sentences designed to cap-\nture a wide range of phoneme combinations. The dataset\nalso includes annotations such as phoneme-level align-\nment and acoustic features, making it a valuable resource\nfor analyzing non-native pronunciation and accent vari-\nability.\n\u2022 Speech Accent Archive [8] This dataset consists\nof recordings from 2140 native and non-native english\nspeakers of 141 countries of origin and 202 native lan-\nguage categories, reading a common English paragraph.\nThis design provides a standardized basis for comparing\naccent characteristics across origins. The Speech Accent\nArchive contains metadata about the speakers' native\nlanguage, age, and gender, which supports studies on\nlinguistic and demographic profiling.\n\u2022 Common Voice 6.1 (English) [9] - Common Voice is an\nopen-source collection of speech recordings contributed"}, {"title": "B. Metrics", "content": "The model's performance is assessed using three primary\nmetrics for age prediction and categorical attribute classifica-\ntion.\n\u2022 Mean Absolute Error (MAE): In age prediction, a con-\ntinuous variable, MAE quantifies the mean of the absolute\ndiscrepancies between projected and actual age values. It\nprovides a straightforward measure of prediction error\nwithout heavily penalizing outliers. MAE is formally\ndefined as:\nMAE = $\\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n\u2022 Accuracy and F1 Score: Accuracy and F1 Score are\nutilized for categorical factors such as gender, native\nlanguage, education, and nationality. Accuracy evaluates\nthe model's predictive correctness, whereas the F1 Score\nharmonizes precision and recall, particularly useful for\nimbalanced datasets."}, {"title": "C. Feature Extraction", "content": "As discussed in Section II, we use a pretrained WavLM\nBase+ model [2]\u00b9 for embedding extraction. In particular,\nWavLM embeddings are created by average pooling of the\nfinal layer."}, {"title": "D. Model Architectures", "content": "To leverage the extracted features, we explore three model\narchitectures: a Multi Layer Perceptron (MLP), a Long-Short\nTerm Memory Network (LSTM) and a Residual Network\n(ResNet32)."}, {"title": "E. Training Method", "content": "The training is tailored for both continuous (regression) and\ncategorical (classification) tasks:\nFor continuous properties (e.g., age), performance is eval-\nuated using the Mean Absolute Error (MAE) metric. For\ncategorical attributes (e.g., gender, nationality), we utilize\nCrossEntropyLoss which is boosted for accuracy and F1 Score."}, {"title": "F. Regularization and Learning Rate Adjustment", "content": "In order to maintain model stability and avoid overfitting,\nwe use early stopping with a patience value of 20 epochs. A\nReduceLROnPlateau scheduler also adjusts the learning rate\nbased on validation success to enable adaptive training and\nconvergence. All models are trained a Tesla K80 GPU."}, {"title": "G. Comparative Analysis", "content": "All models are first trained once for each attribute and using\none dataset at a time, then they are all trained a second time on\nall the datasets containing the said attribute to evaluate their\ngeneralisation perfomances."}, {"title": "IV. RESULTS", "content": "This cross-dataset analysis highlights the strong perfor-\nmance of WavLM embeddings for demographic attribute pre-\ndiction, as detailed in Tables II and III. The Tables have been\nspitted as the proposed corpora offer a limited intersection of\ntheir demographic fields between datasets.\nFor age and gender predictions, the MLP consistently\nachieves the lowest Mean Absolute Error (MAE) across all\ndatasets, outperforming our other models, as well as the\nconsidered baselines, by up to a relative 55% for the age\n(on VoxCeleb2) and an absolute 3.36% difference in gender\naccuracy on Speech Accent Archive, and up to 99.81% Ac-\ncuracy on Voxceleb. However, Common Voice shows one of"}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "This study explores the use of WavLM features to predict\ndemographic characteristics such as age, gender, native lan-\nguage, educational background, and country of origin from\nspeech. These embeddings are paired with classification and\nregression models based on classic architectures, such as MLP,\nResNet32, and LSTM. We train and evaluate our pipeline\non five datasets: Speech Accent Archive, TIMIT, VoxCeleb2,\nL2Arctic, and Common Voice 6.1 (English). Our approach\nleverages the rich speaker-specific features encoded in WavLM\nembeddings, enabling a robust attribute prediction.\nThe proposed framework is compared to baseline methods\n(i-vectors and x-vector) and is shown to outperforms them\nacross all tasks and datasets. In age prediction, the MLP model\nachieves a 17-30% reduction in MAE compared to baseline\nmodels. Similarly, in gender classification, accuracy increases\nby 1-3% and F1 score by 2-5% across datasets compared\nto x-vector-based models with up to 99.81% accuracy on\nVoxCeleb2, outperforming x-vector models with a maximum\nof 98.23%.\nFor native language and country prediction tasks, the F1\nscores improve by 4-10% on the Speech Accent Archive,\nshowcasing the ability of WavLM embeddings to model com-\nplex linguistic and accent-related features effectively. How-\never, for these specific attributes, this study does not aim to\ndemonstrate improvement over existing baselines but rather\nexplores the feasibility and limitations of using WavLM em-\nbeddings for demographic prediction. For other attributes,\nsuch as age and gender, we compare against established\nbaselines and demonstrate significant improvements in perfor-\nmance. Notably, as the dataset diversity increases, the domain\ngap often impacts performance negatively for most datasets,\nemphasizing the challenges of maintaining accuracy while\nbalancing data variability.\nDespite the encouraging first results, the experiments reveal\nseveral limitations. First, not all datasets include labels for\nevery attribute, particularly for education level and country,\nmaking the cross-dataset training and evaluation inconsistent\nacross tasks. Additionally, discrepancies in label granularity\nand terminology across datasets complicate model training.\nFor instance, some datasets label the speaker's origin at the\ncontinental level (e.g., \"Africa\u201d), which may overlook critical\nlinguistic and accentual variations within the continent, while\nprovide more precise labels, such as specific countries (e.g.,\n\"Chad\") or regions (e.g., \"Scotland\u201d). This lack of consistency\nin geographical labeling can make it difficult for models to\ngeneralize effectively, as accents and linguistic features may\ndiffer significantly. This heterogeneity limits model general-\nization when using multiple datasets for country of origin\nprediction. Furthermore, imbalanced class distributions, espe-\ncially for native language and education level, bias predic-\ntions toward majority classes, reducing model accuracy for\nunderrepresented attributes despite our data balancing efforts.\nAddressing these issues will require more uniform datasets,\nor a higher amount of data from under-represented groups.\nTo overcome these limitations, future works will focus on\nexpanding and standardizing datasets. Incorporating additional\ndatasets with consistent labels for demographic attributes\nwill enhance the model's robustness. Additionally, ontology\nmapping [17] and cross-dataset label standardization [18] can\nharmonize label structures, reducing discrepancies between\ndatasets. In conclusion, this study demonstrates that WavLM\nembeddings are a powerful tool for demographic attribute\nprediction, consistently outperforming traditional baselines\nlike x-vector models across all tasks. By achieving reductions\nin MAE of up to 30% and increases in accuracy and F1\nscores of up to 10%, this work establishes a robust and gener-\nalizable foundation for speech-based demographic profiling.\nThese findings lay the groundwork for advancing research\nin speech-based demographic profiling, with the potential to\ninspire more inclusive, adaptive, and impactful technologies\nleveraging speech data."}]}