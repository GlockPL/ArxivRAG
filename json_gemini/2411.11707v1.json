{"title": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models", "authors": ["Tao Fan", "Yan Kang", "Guoqiang Ma", "Lixin Fan", "Kai Chen", "Qiang Yang"], "abstract": "By adapting Large Language Models (LLMs) to domain- specific tasks or enriching them with domain-specific knowledge, we can fully harness the capabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous mutual enhancement between the server's LLM and the downstream clients' Small Language Models (SLMs). To address this, we propose FedCOLLM, a novel and parameter-efficient federated framework designed for co-tuning LLMs and SLMs. This approach is aimed at adaptively transferring server-side LLMs knowledge to clients' SLMs while simultaneously enriching the LLMs with domain insights from the clients. To accomplish this, FedCoLLM utilizes lightweight adapters in conjunction with SLMs, facilitating knowledge exchange be- tween server and clients in a manner that respects data privacy while also minimizing computational and communication overhead. Our evaluation of FedCoLLM, utilizing various public LLMs and SLMs across a range of NLP text generation tasks, reveals that the performance of clients' SLMs experiences notable improvements with the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM achieves comparable performance to that obtained through direct fine-tuning on clients' data.", "sections": [{"title": "1 Introduction", "content": "The emergence of Large Language Models (LLMs) has profoundly transformed the landscape of artificial intelligence. In particular, cutting-edge LLMs like GPT- 4 [13] have garnered significant attention due to their exceptional performance across a range of natural language generation tasks. This development has spurred the release of numerous high-performance open-source LLMs, such as LLaMa [18], OPT [21], greatly promoting the commercial application of LLMS technology. Despite their widespread success in various general NLP tasks, LLMs"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Knowledge Distillation", "content": "Knowledge distillation is a technique that has gained significant attention in recent years, as it enables the transfer of knowledge from a larger teacher model to a smaller student model. One of the early works in this area was proposed by [8], which introduced the concept of knowledge distillation and demonstrated its effectiveness in improving the performance of compressed models. Since then, numerous studies have built upon this foundation and explored various distil- lation strategies [7]. For instance, [4,11] improved response-based knowledge distillation, which let the student model directly mimic the final prediction of the teacher model. [1] proposed FitNets, which focus on matching intermediate representations between the teacher and student models. Another notable work is the relational knowledge distillation approach introduced by [14], which captures pairwise relationships between outputs to enhance distillation efficiency. Different from one-way knowledge distillation between the teacher and student networks, Deep Mutual Learning (DML) [22] allows two networks to learn from each other through their predicted probability distributions during the training process. These studies have demonstrated the potential of knowledge distillation in vari- ous tasks, such as image classification, object detection, and natural language processing."}, {"title": "2.2 Federated Learning for Large Language Models", "content": "Parameter-Efficient Fine-Tuning (PEFT) techniques [9] provide a straightforward remedy to address the challenges of communication overhead and fine-tuning expenses in Federated Learning (FL) for Large Language Models (LLMs) [16]. Numerous investigations have extended the application of PEFT methods within the FL framework tailored for LLMs. Notable contributions include FedPETun- ing [23], Federated Adapter Tuning [3], and Federated Prompt Tuning [24]. These research outcomes suggest that FL clients, particularly those with constrained storage capacities like mobile devices, can significantly profit from the adoption of PEFT methods. These approaches facilitate the sharing of LLMs across various tasks while necessitating the retention of only a minimal set of parameters per"}, {"title": "3 The Proposed Method", "content": "In this section, we present a comprehensive overview of our federated co-tuning LLMs and SLMs framework, termed FedCoLLM. This framework is based on parameter-efficient fine-tuning (PEFT) and knowledge distillation techniques. We begin by defining the specific problem addressed in this study, followed by a detailed introduction to our approach. Finally, we delve into the computational and communication complexities, as well as the privacy-preserving analysis, of our FedCoLLM framework."}, {"title": "3.1 Problem Definition", "content": "In this work, we consider the federated learning setting in which the server owns an LLM \\(f_\\psi\\) parameterized by \\(\\psi\\) and \\(K\\) clients that each client \\(k\\) has a SLM \\(g_\\phi\\) parameterized by \\(\\phi\\). The server and clients aim to collaboratively enhance the performance of the LLM and SLMs without sharing private data through federated learning. Specifically,\nEach client possesses its own local private dataset \\(D^k\\). Clients aim to col- lectively train a global SLM \\(g_\\phi\\) based on their local models initialized with an SLM (e.g., LLaMa2-1.3B [19]) without divulging their private data. The objective can be formulated as follows:\n\\[\\min_\\Phi L_1(\\Phi; \\{D^k\\}_{k=1}^K)\\]\nThe server owns an auxiliary dataset \\(D^a\\). The server aims to transfer knowl- edge between its owned LLM \\(f_\\psi\\) and the global SLM \\(g_\\phi\\) aggregated from clients' local SLMs to enhance both the LLM and SLMs. The objective can be formulated as follows:\n\\[\\min_{\\Phi,\\psi} L_2(\\psi, \\phi; D^a)\\]\nWe regard the server as semi-honest. FedCoLLM solves the optimization problems formulated in Eq.(1) and Eq.(2) in an efficient and privacy-preserving manner. We will elaborate on FedCoLLM in Section 3.2."}, {"title": "3.2 FedCOLLM", "content": "FedCoLLM is a novel framework designed to facilitate the collaborative evolution of both server-side LLM and client-side SLMs. The goal of the FedCoLLM is threefold:"}, {"title": "Collaborative Knowledge Transfer and Adaptation.", "content": "The server and clients work together to transfer and adapt the knowledge of the LLM owned by the server. This helps clients build local SLMs that benefit from the server's LLM knowledge. By leveraging the server's LLM, clients can improve their local SLMs' performance without requiring extensive local training data or computational resources."}, {"title": "Data Augmentation for the Server's LLM.", "content": "Federated learning also aims to leverage clients' data to augment and enhance the server's LLM. Clients' data often contains valuable local information and patterns that can be used to improve the server model's generalization and performance. By incorporating this data, the server's LLM can become more robust and adaptive to different scenarios and domains."}, {"title": "Privacy-Preserving and Efficient Knowledge Transfer.", "content": "A crucial aspect of federated learning is ensuring that knowledge transfer occurs in a privacy- preserving and efficient manner. Clients' raw private data should not be directly uploaded to the LLM server, preserving their privacy. Instead, only model updates or aggregated information are shared with the LLM server. Additionally, the knowledge transfer process should be efficient, minimizing communication costs and computational overhead.\nToward this goal, we (1) adopt lightweight LoRA modules as the bridge to transfer the knowledge between clients and the server, (2) leverage mutual knowledge distillation to transfer knowledge between the LLM and the aggregated SLM, and (3) employ secure aggregation to protect the privacy of the knowledge transfer process.\nSpecifically, we assume that clients and the server share a SLM \\(g_\\phi\\) parame- terized by \\(\\phi\\). Each client \\(k\\) inserts a small low-rank adapter parameterized by \\(\\Theta_k\\) into its local SLM. We denote a client's local SLM with the added \\(\\Theta_k\\) as \\(g_{\\phi+\\Theta_k}\\). Instead of training a global SLM \\(\\phi\\), clients collaboratively train a global LORA module \\(\\Theta\\). Then, Eq. (1) can be reformulated as follows:\n\\[L_1(\\Theta; \\{D^k\\}_{k=1}^K) = \\frac{1}{K}\\sum_{k=1}^K E_{(x,y)\\sim D^k} l_A(g_{\\phi+\\Theta}(x), y).\\]\nwhere \\(l_A\\) is the task loss for training the global LoRA module \\(\\Theta\\). The original model parameter \\(\\phi\\) of each client's local SLM is frozen during training.\nThe server inserts a small low-rank adapter parameterized by \\(\\omega\\) into its LLM \\(f_\\psi\\). We denote the server's LLM \\(f_\\psi\\) with the added \\(\\omega\\) as \\(f_{\\psi+\\omega}\\). The server conducts the mutual knowledge transfer between the LLM \\(f_{\\psi+\\omega}\\) and the global SLM \\(g_{\\phi+\\Theta}\\) through supervised fine-tuning and mutual knowledge distillation based on the auxiliary dataset \\(D^a\\).\nWe formulate the losses of supervised fine-tuning \\(f_{\\psi+\\omega}\\) and \\(g_{\\phi+\\Theta}\\) (denoted as \\(L_{FT}^f\\) and \\(L_{FT}^g\\)) as follows:\n\\[\nL_{FT}^f(\\omega; D^a) = E_{(x,y)\\sim D^a} l_{CE}(f_{\\psi+\\omega}(x), y),\nL_{FT}^g(\\Theta; D^a) = E_{(x,y)\\sim D^a} l_{CE}(g_{\\phi+\\Theta}(x), y).\n\\]"}, {"title": "where", "content": "\\(l_{CE}\\) is the cross-entropy loss; the model parameters \\(\\phi\\) and \\(\\psi\\) are frozen during fine-tuning.\nThe mutual knowledge distillation losses for fine-tuning \\(f_{\\psi+\\omega}\\) and \\(g_{\\phi+\\Theta}\\) models (denoted as \\(L_{KD}^f\\) and \\(L_{KD}^g\\)) are formulated as follows:\n\\[\nL_{KD}^f(\\omega; D^a) = E_{x\\sim D^a} KL(f_{\\psi+\\omega}(x), g_{\\phi+\\Theta}(x)),\nL_{KD}^g(\\Theta; D^a) = E_{x\\sim D^a} KL(g_{\\phi+\\Theta}(x), f_{\\psi+\\omega}(x)).\n\\]\nwhere \\(KL\\) is the Kullback Leibler (KL) divergence function; the model parameters \\(\\psi\\) and \\(\\phi\\) are frozen during knowledge distillation.\nCombining Eq.(4) and Eq.(5), we formulate the mutual knowledge transfer conducted on the server as follows:\n\\[\nL_2(\\Theta, \\omega; D^a) = L^f(\\omega; D^a) + L^g(\\Theta; D^a),\n\\]\nin which\n\\[\nL^f(\\omega; D^a) = L_{FT}^f(\\omega; D^a) + \\lambda L_{KD}^f(\\omega; D^a),\nL^g(\\Theta; D^a) = L_{FT}^g(\\Theta; D^a) + \\lambda L_{KD}^g(\\Theta; D^a).\n\\]\nwhere \\(\\lambda\\) is the hyperparameter that controls the weight of mutual knowledge transfer.\nAfter mutual knowledge transfer, the global LoRA module \\(\\Theta\\) is distributed to all clients, which in turn adopts Eq. (1) to further train \\(\\Theta\\) based on their local datasets.\nFedCoLLM thus fosters a symbiotic relationship between the server and clients, where both parties benefit from the collective knowledge and expertise encoded in their respective language models. By leveraging the complementary strengths of server-side LLMs and client-side SLMs, FedCoLLM paves the way for more efficient and effective federated learning in the realm of natural language processing, enabling the collaborative evolution of LLM and SLMs. We illustrate the FedCOLLM in Figure 1 and describe the associated training algorithm in Algorithm 1. The workflow of FedCoLLM proceeds as follows:\n1. In the \\(t\\)-th communication round, the server broadcasts the SLM \\(g_{\\phi+\\Theta}\\) global adapter \\(\\Theta\\) to \\(K\\) clients. Each client \\(k\\) then replaces its local adapter \\(\\Theta_k\\) with the received global adapter \\(\\Theta\\).\n2. During local training, the \\(K\\) clients fine-tune their respective local adapters using their private data. This step allows the clients to adapt their models to their specific data distributions while preserving the knowledge encoded in the global adapter.\n3. After local training, the \\(K\\) clients send their respective local adapters to the server. The server SLM \\(g_{\\phi+\\Theta}\\) then aggregates these local adapters using a secure averaging technique, such as SecureAvg, and updates the global adapter \\(\\Theta\\) in the SLM accordingly.\n4. On the server side, LLM \\(f_{\\psi+\\omega}\\) and SLM \\(g_{\\phi+\\Theta}\\) engage in knowledge distillation. This process involves transferring knowledge between the two models with the aid of an auxiliary distillation dataset. Through this distillation, both models"}, {"title": "3.3 Computation and Communication Complexity", "content": "One key advantage of FedCoLLM is its computational efficiency. By utilizing PEFT, it markedly decreases the parameters needing fine-tuning updates. Further- more, the server-side distillation process compresses knowledge from local models into a smaller global model, optimizing computational resources. This enables effective learning from all clients' collective data while keeping the model size manageable. In terms of communication complexity, FedCoLLM minimizes the amount of data exchanged between clients and the server. Instead of transmitting entire models or large datasets, clients only share their locally fine-tuned model updates with the server. This approach significantly reduces communication overhead."}, {"title": "3.4 Privacy-Preserving Analysis", "content": "FedCoLLM is meticulously designed with privacy preservation as its foundation. Recognizing the importance of data confidentiality, the framework ensures clients"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Setup", "content": "We set up a scenario involving four clients and one server to evaluate the FedCoLLM using various LLMs and SLMs.\nModels. We evaluate FedCoLLM on LLMs and SLMs, including GPT-2 [15], OPT [21] and LLaMa2 [18]. Our experiments involve utilizing the FedCoOLLM"}, {"title": "4.2 Performance Evaluations", "content": "We conduct experiments with three settings. The first setting (denoted as S1) involves one server-side GPT-2-Large LLM and four client-side GPT-2-Small SLMs, the second setting (denoted as S2) involves one server-side OPT-6.7B LLM and four client-side OPT-1.3B SLMs, and the third setting (denoted as S3) involves one server-side LLaMa2-7B LLM and four client-side LLaMa2-1.3B SLMs. Tables 1 demonstrate the performance comparisons of our approach against other baselines. The top sub-table and the bottom sub-table compare the performance of FedCoLLM against baselines on the server's LLM and clients' SLMs, respectively.\nThe top sub-table of Table 1 shows that FedCoLLM significantly outperforms Zero-Shot on the server's LLM in the three settings. It also shows that FedCOLLM achieves comparable performance of the Centralized scenario. For example, in the CQA dataset, FedCoLLM achieves a relative improvement of 41% over Zero-Shot on GPT-2-Large LLM, 47% on OPT-6.7B LLM, and 66% on LLaMa2-7B LLM. Furthermore, FedCoLLM nearly equals Centralized performance, reaching 98% on GPT-2-Large LLM, 99% on OPT-6.7B LLM, and 97% on LLaMa2-7B LLM.\nThe bottom sub-table of Table 1 shows that FedCoLLM performs better than the Zero-Shot, Standalone, and FedAvg due to the assistance of the server's LLM. For example, in the CQA dataset, FedCoLLM achieves a relative improvement of 6% over Standalone on GPT-2-Small SLM, 8% on OPT-1.3B SLM, and 4% on LLaMa2-1.3B SLM. Furthermore, FedCoLLM achieves a relative improvement of 3% over FedAvg on GPT-2-Small SLM, 5% on OPT-1.3B SLM, and 2% on LLaMa2-1.3B SLM."}, {"title": "4.3 Communication Cost", "content": "We investigated the communication cost of FedCOLLM with LORA, focusing on fine-tuned parameters. As shown in Table 2, FedCoLLM significantly reduces"}, {"title": "5 Conclusions", "content": "We propose FedCoLLM, an innovative and parameter-efficient federated co-tuning framework for LLMs and SLMs. This framework is designed to seamlessly adapt LLMs to resource-constrained downstream enterprises while preserving privacy, eliminating the need to deploy LLMs directly within these enterprises. FedCOLLM achieves this by introducing a SLM that acts as a bridge between the private data held by clients and the LLM model hosted on the server. Through the FedCoLLM training process, we obtain an LLM enriched with knowledge from multiple domains and a high-performing client SLMs, guided by the LLM."}]}