{"title": "Learning to Love Edge Cases in Formative Math Assessment: Using the AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy", "authors": ["Owen Henkel", "Hannah Horne-Robinson", "Maria Dyshel", "Nabil Ch", "Baptiste Moreau-Pernet", "Ralph Abood"], "abstract": "This paper introduces AMMORE, a new dataset of 53,000 math open-response question-answer pairs from Rori, a learning platform used by students in several African countries and conducts two experiments to evaluate the use of large language models (LLM) for grading particularly challenging student answers. The AMMORE dataset enables various potential analyses and provides an important resource for researching student math acquisition in understudied, real-world, educational contexts. In experiment 1 we use a variety of LLM-driven approaches, including zero-shot, few-shot, and chain-of-thought prompting, to grade the 1% of student answers that a rule-based classifier fails to grade accurately. We find that the best-performing approach \u2013 chain-of-thought prompting \u2013 accurately scored 92% of these edge cases, effectively boosting the overall accuracy of the grading from 98.7% to 99.9%. In experiment 2, we aim to better understand the consequential validity of the improved grading accuracy, by passing grades generated by the best-performing LLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated student mastery of specific lessons. We find that relatively modest improvements in model accuracy at the individual question level can lead to significant changes in the estimation of student mastery. Where the rules-based classifier currently used to grade student, answers misclassified the mastery status of 6.9% of students across their completed lessons, using the LLM chain-of-thought approach this misclassification rate was reduced to 2.6% of students. Taken together, these findings suggest that LLMs could be a valuable tool for grading open-response questions in K-12 mathematics education, potentially enabling encouraging wider adoption of open-ended questions in formative assessment.", "sections": [{"title": "1. Introduction", "content": "Formative assessment and feedback are crucial components of the learning process, enabling students and educators to adapt their approach within or in-between lessons to maximize learning [34]. It has been shown to lead to significant improvements in learning outcomes [18]. Closed-response questions, such as multiple-choice and true/false, are commonly used in formative assessment, and have the benefit of being efficient to grade and can provide instant feedback [31]. However, they have several drawbacks, such as the possibility of students relying on test-taking strategies, a potential lack of face validity, and the complexity of generating multiple answer options [16, 24]. In contrast, open-ended and short answer questions require students to answer a question using their own words often with a few sentences [31]. Many researchers argue that open-response questions decrease the influence of test-taking strategies, have greater face validity, and may be better suited to evaluate certain subprocesses of the skill being assessed [3, 4, 8, 16, 34]. However, the process of grading open-ended questions can be resource-intensive and expensive, which limits their widespread use [23]. While educators may prefer the type of information they can glean from student responses to open-ended questions, the laborious grading process can overburden educators and compromise the quality of feedback, which may limit students' comprehension and critical engagement with the subject matter [25]. Therefore, automatic short answer grading (ASAG) offers a promising solution to address this, but it has historically been challenging to perform easily and effectively enough for widespread use in educational settings [2, 7, 13]. Most state-of-the-art approaches have relied primarily on handcrafted approaches, or more recently fine-tuning models for specific tasks [5, 14], which required extensive technical expertise and large datasets [26, 32].\nThe field of ASAG has seen significant advancements with the emergence of LLMs, presenting new opportunities for enhancing educational assessment and personalized learning. There is growing evidence that these models can complete evaluation tasks on novel datasets with only minimal prompt engineering [15, 18, 19]. If LLMs can accurately mark open-ended questions, the time savings for educators would be substantial, and could facilitate more frequent and effective formative assessment. However, little is known about how LLMs perform across a variety of educational settings and whether LLMs can be relied upon to generalize to ever more complex use cases. Additionally, there are a limited"}, {"title": "2. Prior Work", "content": "2.1 Automatic Short Answer Grading\nAutomatic short answer grading has been an active area of research for over a decade. Burrows et. al. [7] provide a comprehensive overview of approaches up until 2015; while Haller et. al., [17] discuss how ASAG has more recently moved from models based on handcrafted features to approaches including word-embedding and representation learning. However, regardless of the paradigm, most models used for ASAG are explicitly trained or fine-tuned for specific grading tasks [21]. There has been considerable progress with these types of tasks, for instance, Sultan et al. [36] developed a model that represents each sentence as the sum of the individual word embeddings. At the time, this model achieved state-of-the-art performance on the SemEval benchmarking dataset. As these types of models depend on prompt-specific training data, they often need to be re-trained for each individual short answer prompt, which is costly, time-consuming, and in most cases simply infeasible.\nThe recent rise of ever-larger pre-trained LLMs, trained on vast text corpi, has enabled a new approach: fine-tuning, often referred to as transfer learning. This paradigm of machine learning typically consists of two steps: pre-training and fine-tuning. In the former, a neural network model learns weights through unsupervised learning on a large general dataset. In the latter, the model trains on a smaller, task-specific dataset [20] to update its weights to better align with downstream tasks. As a result of their significant pre-training, LLMs can achieve far better sample efficiency on the target task(s) [6]. For example, Sung et al. [37] fine-tuned BERT, a widely used pre-trained transformer-based language model to grade short-answer responses and found it was able to classify almost at the human-level agreement and achieve superior results to the previous state-of-the-art on the SemEval dataset. More recently, Fernandez et al. [14] used a BERT-based model to evaluate open-response reading comprehension questions and achieved an agreement score with expert raters, as measured by Cohen's Kappa, of 0.84, where human-to-human scores were 0.88.\nWhile pre-trained language models that have been fine-tuned with small task-specific datasets have improved ASAG, their practical application to formative assessment in educational settings remains limited. This is largely due to a few central constraints of this approach: the technical complexity of the fine-tuning process, the continued (albeit small) need for task-specific data, and these models' difficulty in generalizing. First, fine-tuning an LLM requires substantial computational power,"}, {"title": "2.2 Potential of Generative LLMs for ASAG", "content": "The current generation of LLMs, including ChatGPT, GPT-4, Claude, Llama, Mistral, Gemini, were trained similarly to previous generations but with significantly larger datasets and a higher number of parameters, in some cases by more than an order of magnitude [9, 35]. Additionally, these models underwent various \"instruction fine-tuning\" steps to enhance their usability and ability to generalize to new tasks, often with minimal exposure to examples [30]. This also improved their ability to interpret human-written natural language instructions (i.e., prompting), allowing non-technical users to make requests and adapt a model to new tasks by modifying their prompts, rather than requiring further training or fine-tuning [35]. Therefore, it is unsurprising that evidence is growing that LLMs can be used for certain types of grading tasks [21]. Current LLMs can perform various linguistic tasks that previously required the use of task-specific, fine-tuned LLMs [20, 38], and with minimal prompt engineering can complete evaluation tasks on novel datasets [15, 22]. Instead of using a task-specific dataset to fine-tune a pre-trained LLM, a user can now simply write an explanation and a few examples of how they wish the model to grade student answers and achieve reasonable performance.\nWhile there is a growing amount of research on grading essays using generative LLMs, relatively little is known about their potential for ASAG [21, 27, 33]. Morjaria et al. [28] found that ChatGPT graded 6 short answer assessments from an undergraduate medical program similarly to a single expert rater. Cohn et al. [11] found that GPT-4 successfully graded student answers to high school science questions. However, Kortemeyer [21] found that LLMs fell short in certain aspects of grading introductory physics assignments. A review by Schneider et al. [33] concluded that \"while 'out-of-the-box' LLMs provide a valuable tool to offer a complementary perspective, their readiness for independent automated grading remains a work in progress.\"\nIn all aforementioned cases, the studies were conducted with a small sample of student responses, with a primary focus on high school and university students. However, there has been little exploration of generative LLMs' ability to grade short-answer responses from elementary or middle"}, {"title": "2.3 Overview of Existing Short Answer Datasets", "content": "While there are several math question datasets in the literature (see Table 1 below for a more detailed overview), they present many limitations that undermine their relevance in real-world grading contexts, especially for elementary and middle school students. First, several prominent datasets, e.g., MATH, contain questions and correct answers but do not contain information about how students answered, others, e.g., EEDI, MathE, contain information about students' multiple-choice responses. Second, of the below datasets, only ASSISTments contains information allowing researchers to track progression through a curriculum. Third, few of these datasets contain information from lower resource and underrepresented populations. These limitations are the main motivation behind our proposed dataset AMMORE, which we discuss in more detail in Section 3."}, {"title": "3. AMMORE Dataset", "content": "In this section, we present the African Middle-School Math Open REsponse (AMMORE) Dataset, which contains 53,298 student answers to open response practice questions, assembled from a subset math practice sessions on Rori of 2,508 at-home users that took place between January 1st and April 30th, 2024."}, {"title": "3.1 Background", "content": "Rising Academies, an educational network based in Ghana, has created Rori, an Al-powered math tutor available on WhatsApp. Rori can be used at home or in schools free of charge. The Rori curriculum has one or more micro-lessons for each skill in the math Global Proficiency Framework (GPF), with over 500 micro-lessons to date. Each micro-lesson includes a brief student-friendly explanation of the skill and ten scaffolded practice questions. Many of these questions require open-ended responses, which was a decision taken for pedagogical reasons. Students are expected to write their answers into WhatsApp using the mobile keyboard. If students answer a question incorrectly, they are first shown a hint to help them solve the question and if their second attempt is unsuccessful, they are shown a worked solution. When students finish a micro-lesson, they are encouraged to continue with the next, which incrementally increases in difficulty. Rori will suggest students move either backwards or forwards in the curriculum if they find a lesson too difficult or easy. For more context you can watch this 2-minute video.\nRori's curriculum is built upon the comprehensive and evidence-based GPF. The framework was developed to create uniform global standards for reading and mathematics across the world and was created by USAID by using inputs from experts representing organizations such as the World Bank, the Bill and Melinda Gates Foundation, the UK's Foreign, Commonwealth, and Development Office, the UNESCO Institute for Statistics, and many more. The GPF represents a global standard for the competencies required for learners at different stages. It covers grades 1 to 9, aligns with national standards globally, and the standards are linked across grade levels. The math framework has five domains: \"Numbers and operations\u201d, \u201cMeasurement\u201d, \u201cGeometry\u201d, \u201cStatistics and probability\", and \"Algebra\". Each domain is split into constructs, then subconstructs, and then in specific skills that a student in each grade should be able to demonstrate. For example, the domain \u201cNumbers and"}, {"title": "3.2 Structure", "content": "Each response in our dataset was scored by a pre-existing, rules-based classification model, native to Rori, which classifies answer attempts as \u201ccorrect\u201d, \u201cwrong\u201d or \u201cother\u201d. The latter was typically returned when a student entered something besides an answer attempt, such as a voice note or a sticker. These classifications were then manually reviewed by humans, and changed where necessary, meaning the dataset also has a ground truth score for each student answer. The dataset is comprised of students' answers to math questions from Rori lessons from grade levels 6 to 9 in the domains \u201cAlgebra\" and \"Number and operations\". Each student answer is paired with the corresponding question, the expected response, a ground-truth correct/incorrect score, the specific learning standard evaluated by the question, the time the student answered, and a UID number that can be used to link student responses across the dataset."}, {"title": "3.3 Potential Uses of the AMMORE Rising Dataset", "content": "The dataset's structure enables various potential analyses. For example (a) investigating students' skill mastery across micro-lessons, (b) analyzing the relative difficulty of specific questions or micro-lessons across students, or (c) exploring how the classification model's judgments compare to those of human raters.\nExpanding on the first example, while there are many ways to evaluate student mastery at the micro-lesson level, for simplicity, we define mastery as an 80% correct answer rate for questions from a micro-lesson. As discussed above, a micro-lesson is a set of 10 questions of the same difficulty level focusing on a specific learning standard. We consider the responses labelled \u201ccorrect\u201d or \u201cwrong\" and discard those labelled \u201cother\u201d to compute the percentage of micro-lessons that students \u201cmastered\u201d. Using this threshold, we can determine that students \u201cmastered\" 48% of micro-lessons. To further this analysis, one could combine or \u201croll up\u201d micro-lesson mastery into skill-level mastery. The dataset includes 151 different micro-lessons covering 35 different skills. For instance, if we posit that a student must master at least 75% of the micro-lessons contained within a skill to have mastered that skill, we can determine how many of the 2,508 students in the dataset have mastered each skill. With this example, 1,133 of the 2,508 students in the data set (45%) would have mastered a skill.\nAlso, because the same student practices skills at different grade levels, it is possible to compare student age to the grade-level of the topics they are practicing. Using the same mastery thresholds as above, we can determine that amongst the 11% of students who master at least two skills (273 students), 28% of them (76 students) master skills at multiple grade levels. One can also estimate whether students are performing at \u201cgrade-level\u201d. Our dataset's lessons span grades 6 to 9, with 38% of all answers at level 9, 29% at level 6, then 20% and 13% at levels 7 and 8 respectively.\nYet another approach could be to use this dataset to test different analytics approaches, such as Bayesian Knowledge Tracing (BKT), which we explore in experiment 2, or other mastery prediction models. The rich data available, including question-level responses and progression through micro-"}, {"title": "4. Experiment 1: LLM-based Approaches to Math ASAG", "content": "Using a carefully curated subset of challenging student responses from the AMMORE Dataset, we investigate six different automatic grading strategies, ranging from simple string matching to sophisticated LLM-based methods, evaluating their respective performance relative to human scores. We also consider how consistent the models are between repeated runs, if the prompting strategy affects the intra-rater reliability between the model's responses, and how prompting strategy impacts the model response time. Our analysis aims to shed light on the potential of these approaches to improve grading accuracy, particularly when dealing with diverse answer types and formatting variations."}, {"title": "4.1 Challenges of Grading Open-Response Math Questions", "content": "Accurately grading student answers becomes a complex challenge when moving beyond direct string matches because practice questions on Rori have a diverse set of expected answer types, including fractions, floating-point numbers, and expressions with exponents. In Table 2 you can see a subset of student responses for a given question."}, {"title": "4.2 Experimental Design", "content": "From the larger AMMORE dataset, we create a smaller dataset, which we refer to as AMMORE-hard. This dataset is comprised of difficult-to-grade student answers, which we used to evaluate the performance of different automatic grading strategies. The resulting dataset comprises of 4,463 answers, including 1528 unique non-trivially correct answers and 2935 unique, non-trivially wrong answers.\nAMMORE-hard was created using the following steps: (1) remove answers that were labeled as \"other\" by a human labeler; (2) remove duplicate occurrences where question, expected answer, and student answer were identical, leaving only one occurrence of each unique combination; (3) remove trivially correct answers, where the student's answer was identical to the expected answer; (4) remove trivially wrong answers, where the expected answer was one character long and the student's answer was one character long (mostly multiple-choice questions with wrong answer); (5) remove trivially wrong answers, where the student's answer was an integer different from the integer expected; and finally, (6) remove answers where either the question was ambiguous or the expected answer was wrong. Using AMMORE-hard, six approaches were used to classify a student answer as correct or wrong."}, {"title": "4.2.1 Prompting Strategy", "content": "We employ a relatively simple prompting strategy, as the task is straightforward. The base part of the prompt was similar across all strategies. The zero-shot prompt included a description of the core task and slots for the dataset values. The few-shot prompt added three examples of correct answers. These examples represented common student response patterns of equivalent answers: 1) where a student wrote the answer and 2) where a student wrote out their work to arrive at the answer. Instead of providing examples, the chain-of-thought prompt instructed the model to think step-by-step and present a rationale for the classification chosen. The chain-of-thought evaluation used the DSPy framework, which dynamically created a chain-of-thought prompt. Figure 3 shows the prompts for each strategy."}, {"title": "4.3 Results", "content": "Table 3 shows the results of the six approaches. As mentioned earlier, each answer evaluation would label a student's answer as \u201ccorrect\u201d or \u201cwrong\u201d. These predictions were compared against the label assigned by a human rater. In Table 3, a result closer to one indicates that the human label and the prediction were similar (i.e., both labeled a student answer as \u201cwrong_answer\u201d). A lower score would indicate that the human label and the predicted label differed (i.e., the human label marked \u201ccorrect_answer\" and the predicted label \u201cwrong_answer\u201d).\nWe report a set of widely used metrics in classification problems which measure model performance after accounting for imbalanced classes in the dataset: precision, recall, and F1 score (Banerjee et al., 2008). We also report the Kappa scores, which are chance-adjusted metrics of agreement, with values ranging from -1 to 1. A value of 1 indicates perfect agreement, 0 suggests that the agreement is only what would be expected by chance, and a value less than 0 indicates agreement worse than random chance. While there are several different measures of chance-adjusted agreement, because we are evaluating 2-class ratings (wrong/correct), we use Linear Weighted Kappa (LWK)."}, {"title": "4.3.1 Performance vs Latency", "content": "Table 4 shows the average and longest processing times each evaluation took to make a prediction.\nWhile chain-of-thought prompting resulted in small but stable improvements over the string processing and symbolic evaluations, it also significantly increased response latency. On average, chain-of-thought responses took 2.79 seconds, compared to 0.73 seconds for few-shot LLM calls. The few-shot evaluation took slightly longer than the zero-shot approach. Text processing evaluations took considerably less time than all prompt-based approaches, which is expected given that this approach did not require connection to the model over internet or the execution of a large-scale machine learning model."}, {"title": "4.3.2 Model Reliability", "content": "While the deterministic approaches like text processing provide consistent results, generative LLMs generate their output using probabilistic methods, and therefore can return different outputs given the same inputs. This variation may occur even when the temperature is set to 0. In some respects, this is similar to human raters, who occasionally will award different ratings to the same student response, when asked to re-rate it after a period of time. Measures of intra-rater reliability are intended to evaluate the extent to which a single rater agrees with their own judgment over time.\nTo investigate the consistency of prompt-based methods, zero-shot and chain-of-thought approaches were rerun 10 times on a smaller dataset of 100 examples. As shown earlier, these two approaches scored the highest of the prompt-based approaches. For each run, the model labels were compared against the predicted labels to get a Fleiss's Kappa score to measure inter-rater reliability for the run. Table 5 shows the results of these runs. All runs were then compared against each other to arrive at a Fleiss Kappa to represent inter-run reliability."}, {"title": "5. Experiment 2: Impact of Improved Grading on Student Ability Estimates", "content": "While improving model performance in grading short answer questions is an important area of research, we also seek to better understand the impact of such models on the analysis of student learning. In our second experiment, we investigate whether improved accuracy in model grading corresponded to changes in our estimates of student ability. In the context of a learning environment, even a small number of misgraded answers can lead to vastly different judgments of student ability when aggregated across questions. Tracking a student's progress and understanding of the subject is an essential part of ITS [1, 10]. Accurately estimating a student's current knowledge state enables these systems to deliver a personalized learning experience. For example, student modeling can be used by ITS for making key decisions such as which problem a student should attempt, how much practice is needed to master a skill before moving to a more advanced topic, and when to provide immediate feedback to struggling students.\nBayesian Knowledge Tracing [12] is one of the most widely used algorithms to model students' knowledge in ITS [1]. At any given moment BKT assumes that when a student attempts to demonstrate a skill, they either know the skill or not. Every time a student attempts to demonstrate the skill, the probability of them knowing the skill is updated based on their performance up to that point and whether they were able to demonstrate the skill correctly or not.\nStandard BKT uses four parameters to model student knowledge. Two parameters are related to learners' knowledge. When first attempting to demonstrate a skill, a student has the initial probability $P(L_0)$ of knowing the skill. This probability is updated each time the student attempts to demonstrate the skill (i.e., after t attempts, the probability of knowing the skill is $P(L_t)$). At each practice opportunity, a student has a probability P(T) of learning the skill. The other two BKT parameters are related to learners' performance. The probability of a student knowing the skill and yet making a mistake when attempting to demonstrate the skill is P(S). P(G) represents the probability of a student correctly guessing the answer even when not knowing the skill."}, {"title": "5.1 Methodology", "content": "To quantify the effect of different automated grading algorithms on predicting individual student mastery, we apply the algorithms described in the previous section to generate answer correctness labels for the entire dataset. We exclude questions labeled by human annotators as \u201cother\u201d, as there are no straightforward ways to incorporate student non-attempts into the BKT evaluation.\nWe calculate BKT scores for each student on every lesson they attempted, using only their first attempts to respond to each question. To calculate these scores, we use the following default parameters for every lesson, as suggested by Nguyen et al., [29] : P(L0)=0.4, P(T)=0.05, P(S)=0.299, and P(G)=0.299. \u03a4o determine if a student had mastered a lesson, we use the last BKT score calculated for that student in each lesson. While mastery thresholds for BKT scores vary between different sources, we choose a threshold of 0.9 to signify that a student had mastered the lesson.\nNext, to investigate the effect of grading mechanisms on evaluating individual student mastery, we calculate the number of lessons each student mastered according to different grading algorithms. We then compare these numbers between the worst-performing algorithm (naive string matching) and the best-performing algorithm (chain-of-thought), using human labels of the students' answers as the gold standard. Finally, to estimate the effect of grading mechanisms on evaluating micro-lesson difficulty, we calculate the median mastery score for each micro-lesson and compared this measure across different grading approaches."}, {"title": "5.2 How Grading Methods Impact Mastery Predictions", "content": "When comparing the number of lessons that reach our threshold for mastery (BKT score of 0.9) according to different grading approaches, we find that 6.9% (165 out of 2,388) of students had their mastery of a completed lesson incorrectly estimated. In contrast, the most successful grading approach, LLM chain-of-thought grading, only underestimated the number of completed lessons for 2.6% of students (61 out of 2,388) students.\nThis difference can be illustrated by looking at a specific lesson, G7.N3.2.2.2, which demonstrates how dramatic the effect of grading approach on lesson difficulty estimation can be. This lesson deals with changing forms and asks the student to present a given decimal number as a fraction. As there are multiple correct answers to this question and string-matching evaluation struggles with identifying equivalent fractions, the string-matching algorithm would regularly grade mathematically correct results as wrong. Examples of this difference in terms of a single lesson can be seen in Table 6."}, {"title": "6. Discussion", "content": "6.1 Implications\nThe results of our experiments have significant implications for the field of ASAG and its application in educational settings. The superior performance of LLM-based approaches, particularly chain-of-thought prompting, suggests that these models can effectively handle the complexity and variability of student responses in open-ended math questions.\nOne of the most important implications of our findings is the potential for more widespread use of open-ended questions in formative assessment. As noted in the introduction, open-ended questions have several advantages over closed-response formats, including decreased influence of test-taking strategies and greater face validity. The ability to accurately grade these questions automatically could encourage educators to incorporate more open-ended questions into their assessments, potentially leading to more effective evaluation of student understanding and improved learning outcomes.\nThe improved accuracy of LLM-based grading approaches also has implications for student experience and engagement. As demonstrated in our analysis of individual student mastery prediction, inaccurate grading can significantly impact a student's perceived progress and potentially influence their behavior. More accurate grading could lead to better alignment between a student's actual performance and their estimated mastery, potentially increasing motivation and reducing frustration. Furthermore, the ability of LLMs to handle diverse answer formats and equivalent expressions could promote more flexible problem-solving among students. Instead of being constrained to a specific answer format, students could express their solutions in ways that feel most natural to them, knowing that the grading system can accurately evaluate their responses.\nThe implications extend beyond the design and implementation of ITS as well. From a resource perspective, the ability to accurately grade open-ended questions automatically could lead to significant time savings for educators. This could allow them to focus more on providing personalized feedback and support rather than spending time on routine grading tasks."}, {"title": "6.2 Limitations", "content": "Despite the promising results, our study has several limitations that should be considered when interpreting the findings and planning future research. Firstly, our dataset is limited to middle school mathematics questions from specific domains (\u201cAlgebra\u201d and \u201cNumbers and operations\u201d). The performance of the grading approaches, particularly the LLM-based methods, may vary for different subject areas, complexity levels, or age groups.\nSecondly, our experiments focused on a binary classification of answers as correct or incorrect. This simplification, while useful for our analysis, does not capture the full spectrum of partial understanding that students may demonstrate in their responses. A more nuanced grading approach might provide richer insights into student comprehension and learning progress.\nThirdly, LLM-based approaches revealed some inconsistency in grading, particularly for the chain-of-thought method. This variability in \u201cpedagogical standards\u201d between runs could be problematic in educational settings where consistent evaluation is crucial for fair assessment and student trust in the system. Relatedly, there is the potential for LLM hallucination or faulty mathematical reasoning, as demonstrated in some of our examples. While these instances were relatively rare, they highlight the need for caution when relying solely on LLM-based grading without human oversight.\nLastly, while we demonstrate the impact of grading accuracy on estimates of student mastery and lesson difficulty, we did not explore how these improved estimates might translate into better learning outcomes in practice. The real-world educational impact of using LLM-based grading in an ITS remains to be studied."}, {"title": "6.3 Further Research", "content": "Our findings open several avenues for future research in the field of ASAG and its applications in education. One crucial area for further investigation is the expansion of available datasets to include a wider range of subjects, grade levels, and cultural contexts. This would allow researchers to test the generalizability of LLM-based grading approaches across different educational domains and student populations. Additionally, creating datasets that include more complex, multi-step problems could help push the boundaries of what automatic grading systems can handle. Future studies should also explore more nuanced grading scales beyond binary classification. Developing and evaluating methods for assigning partial credit or identifying specific misconceptions in student responses could provide more detailed insights into student understanding and learning progress.\nResearch into improving the consistency of LLM-based grading is another important direction. This could involve experimenting with different prompting strategies, exploring ensemble methods that combine multiple LLM runs, or investigating ways to fine-tune LLMs for more consistent performance in educational grading tasks. Another promising direction is the integration of LLM-based grading into ITS and studying its impact on adaptive learning. Researchers could investigate how more accurate"}, {"title": "7. Conclusion", "content": "We make two contributions to the fields of ASAG and LLM evaluation. By presenting AMMORE, we aim to expand and diversify the range of publicly available datasets. As it includes students from Africa answering math questions at middle school levels and provides demographic data, it is a unique dataset that enables a variety of analyses, a few of which we have explored here. We find that chain of thought prompting is the best LLM-driven approach to grade open-response math answers. Additionally, we find improving grading accuracy can lead to significant changes in the estimation of student mastery, which could have considerable impact on the field of ITS and opens up many more questions for future research."}]}