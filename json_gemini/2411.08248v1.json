{"title": "Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach", "authors": ["Jiyao Li", "Mingze Ni", "Yongshun Gong", "Wei Liu"], "abstract": "Deep learning underpins most of the currently advanced natural language processing (NLP) tasks such as textual classification, neural machine translation (NMT), abstractive summarization and question-answering (QA). However, the robustness of the models, particularly QA models, against adversarial attacks is a critical concern that remains insufficiently explored. This paper introduces QA-Attack (Question Answering Attack), a novel word-level adversarial strategy that fools QA models. Our attention-based attack exploits the customized attention mechanism and deletion ranking strategy to identify and target specific words within contextual passages. It creates deceptive inputs by carefully choosing and substituting synonyms, preserving grammatical integrity while misleading the model to produce incorrect responses. Our approach demonstrates versatility across various question types, particularly when dealing with extensive long textual inputs. Extensive experiments on multiple benchmark datasets demonstrate that QA-Attack successfully deceives baseline QA models and surpasses existing adversarial techniques regarding success rate, semantics changes, BLEU score, fluency and grammar error rate.", "sections": [{"title": "Introduction", "content": "Question-answering (QA) models, a key task within Sequence-to-Sequence (Seq2Seq) frameworks, aim to enhance computers' ability to process and respond to natural language queries. As these models have evolved, they have been widely adopted in real-world applications such as customer service chatbots[40], search engines [69], and information retrieval in fields like medicine [19] and law [35]. However, despite the significant progress in deep learning and natural language processing (NLP), these models remain vulnerable to adversarial examples, leading to misinformation, privacy breaches, and flawed decision-making in critical areas [23, 8, 15, 52]. This highlights the importance of understanding how adversarial examples are generated from the attackers' perspective and potential defense mechanisms an area that remains under-explored.\nQA models are expected to comprehend given texts and questions, providing accurate and contextually relevant answers [50]. These models primarily address two types of questions: Informative Queries and Boolean Queries. The Informative Queries typically begin with interrogative words such as \"who,\" \"what,\" \"where,\" \"when,\" \"why,\" or \"how,\" requiring detailed and specific information from the provided context. Although models like T5 [43], LongT5 [14], and BART [27], which follow an encoder-decoder structure, have demonstrated strong performance, they still suffer from the maliciously crafted adversarial examples. Initially, studies like \"Trick Me If You Can\" [57] primarily relied on human annotators to construct effective adversarial question-answering examples. This methodology, however, inherently constrained scalability and increased resource demands. As research progressed, automated approaches for attacking textual classifiers in QA models emerged. Gradient-based methods, as employed in RobustQA [63], UAT [56], and HotFlip [10], were developed to identify and modify the most influential words affecting model answers. Building upon a deeper understanding of QA tasks, subsequent studies explored more targeted strategies. For instance, Position Bias [24], TASA [5], and Entropy Maximization [49] investigated the manipulation of sentence locations and the analysis of answer sentences to identify vulnerable parts of the context. These approaches refined the attack methods by applying modifications through paraphrasing or replacing original sentences, thus enhancing the effectiveness of adversarial examples. However, these methods encounter two primary challenges: 1) None of these attack methods is suitable for both \"informative queries\" and \"boolean queries\". 2) Constraining the search space for optimal vulnerable words to answer-related sentences compromises attack effectiveness; meanwhile, targeting entire sentences proves inefficient [17].\nIn addition, Boolean Queries seek a simple binary \"Yes\" or \"No\" answer. Models like BERT [7], RoBERTa [70], and GPT variants [22, 3, 1, 51], which excel at sentence-level understanding and token classification, are widely used for Boolean QA tasks. These models leverage their deep contextual understanding of language to accurately determine whether a given statement is true or false, making them state-of-the-art baselines for the task. Researchers have proposed various approaches to target boolean classifiers in the context of Boolean Queries attacks. Attacks like [31, 12, 18, 66, 46], which involve adding, relocating, or replacing words, are based on the influence that"}, {"title": null, "content": "each word has on the prediction. They retrieve word importance by the output confidence to the level or with gradient. However, gradient calculation is computationally intensive and ineffective when dealing with long context input, and knowing victim models' internal information is unrealistic in practice.\nWe present QA-Attack, an adversarial attack framework tailored for both Informative Queries and Boolean Queries in QA models. QA-Attack uses a Hybrid Ranking Fusion (HRF) algorithm that integrates two methods: Attention-based Ranking (ABR) and Removal-based Ranking (RBR). ABR identifies important words by analyzing the attention weights during question processing, while RBR evaluates word significance by observing changes in the model's output when specific words are removed. The HRF algorithm combines these insights to locate vulnerable tokens, which are replaced with carefully selected synonyms to generate adversarial examples. These examples mislead the QA system while preserving the input's meaning. This unified attack method improves both performance and stealth, ensuring realistic applicability for both types of queries. In summary, our work makes the following key contributions:\n\u2022 We present QA-Attack with a Hybrid Ranking Fusion (HRF) algorithm designed to target question-answering models. This novel approach integrates attention and removal ranking techniques, accurately locating vulnerable words and fooling the QA model with a high success rate.\n\u2022 Our QA-Attack can effectively target multiple types of questions. This adaptability allows our method to exploit vulnerabilities across diverse question formats, which significantly broadens the scope of potential attacks in various real-world scenarios.\n\u2022 QA-Attack generates adversarial examples by implementing subtle word-level changes that preserve both linguistic and semantic integrity while minimizing the extent of alterations, and we conduct extensive experiments on multiple datasets and victim models to thoroughly evaluate our method's effectiveness in attacking QA models.\nThe rest of this paper is structured as follows. We first review QA system baselines and adversarial attacks for QA models in Section 2. Then we detail our proposed method in Section 3. We evaluate the performance of the proposed method through extensive empirical analysis in Section 4. We conclude the paper with suggestions for future work in Section 5."}, {"title": "Related Work", "content": "This section provides a comprehensive overview of question-answering models and examines the existing research on adversarial attacks against them."}, {"title": "Question Answering Models", "content": "Question answering represents a complex interplay of NLP, information retrieval, and reasoning capabilities [50, 64]. Basically, these models are designed to process an input question and a context passage, extracting or generating an appropriate answer through elaborate analysis of the semantic relationships between these elements [59]."}, {"title": "Previous Works on Attacking QA Models", "content": "With the development of NLP techniques, recent research has increasingly focused on developing sophisticated textual adversarial examples for QA systems [57]. The inherent differences between \"informative queries\" and \"boolean queries\" necessitate distinct attacking diversities due to their unique answer structures [56]. Attacks on boolean QA pairs closely resemble methods used to mislead textual classifiers. These attacks primarily operate at the word level, aiming to manipulate the model's binary (yes/no) output [31, 12]. In contrast, informative queries present a more complex challenge. These attacks frequently target the sentence level, requiring an approach to disrupt the model's comprehensive understanding [28]."}, {"title": "Boolean Queries Attacks", "content": "Boolean queries are similar to classification tasks in NLP, while the answer is based on two-way input: question and context. They are vulnerable to attacks designed for NLP classifiers when question and context are simply encoded and concatenated. Approaches such as [31], [12], [18], [66], and [46] concentrate on altering individual words based on their influence on model predictions. These methods typically employ carefully selected synonyms for word substitution. The process of word replacement is guided either by the direct use of BERT Masked Language Model (MLM) [7] or by leveraging gradient information to determine optimal substitution candidates. While effectively fool classifiers (boolean queries), these attacks were initially designed for classification tasks and have shown limited efficacy when applied to the question-and-context format of QA systems. To address this limitation, some attack methods for Seq2Seq models have been adapted for QA models. UAT [56], which averages gradients and modifies input data to maximize the model's loss, has been adapted for QA but still struggles with boolean queries due to their simplicity. Similarly, TextBugger [29], which focuses on character-level perturbations, also faces challenges in handling the deeper semantic understanding required in QA, especially for multi-sentence reasoning. Liang's approach [32], relying on confidence-based manipulations, has difficulty reducing the model's certainty in boolean queries where the binary answers leave less room for variation in confidence. Although these approaches offer improved accuracy in attacking informative questions with minor modifications, they struggle with boolean queries. We argue that these methods face challenges in identifying the most vulnerable words when dealing with concatenated question-context input relationships."}, {"title": null, "content": "The MLQA attack [47] attempts to bridge this gap by utilizing attention weights to identify and alter influential words. However, this method, developed specifically for multi-language BERT models, may not fully address QA-specific vulnerabilities."}, {"title": "Informative Queries Attacks", "content": "In contrast to boolean queries, adversarial attacks on informative queries within QA systems share fundamental similarities with attacks on other Seq2Seq models [30, 2, 34], concentrating more on the inter-relationship between question and context. The defense mechanisms like RobustQA [63] have been developed to enhance model resilience through improved training methods, and sophisticated attacks continue to successfully compromise these systems, especially when employing subtle manipulations of key input elements. Character-level attack methods, notably HotFlip [10], have demonstrated significant success by strategically flipping critical characters based on gradient information, leading to misinterpreting informative inputs. In the multi-lingual domain, MLQA [53] leverages attention weights to identify and target crucial words, though its attention mechanism, primarily designed for multilingual functionality, may not fully exploit the intricate vulnerabilities within the model's attention architecture. Advanced techniques have emerged to target the influence that answers have on QA systems. Position Bias and Entropy Maximization methods exploit model weaknesses by manipulating contextual patterns and answer positioning, particularly effective in scenarios involving complex, lengthy responses. Syntactically Controlled Paraphrase Networks (SCPNs) [16] generate adversarial examples through strategic syntactic alterations while preserving semantic meaning. TASA (Targeted Adversarial Sentence Analysis) [5] primarily relies on manipulating the answer sentences to mislead QA models, making it particularly effective for informative queries where complex responses provide more opportunities for subtle modifications. However, this approach is not suitable for boolean queries, as the simplicity of yes/no answers limits the sentence-level manipulations that TASA depends on."}, {"title": "Our Proposed Attack Method", "content": "In this section, we introduce the QA-Attack algorithm. It can be summarized into three main steps. First, the method effectively captures important words in context by processing pairs of questions and corresponding context using attention-based and removal-based ranking approaches. Then, attention and removal scores are combined, allowing the identification of the most influential words. At last, a masked language model [7] is utilized to identify potential synonyms that could replace the targeted words. The overall workflow of QA-Attack is shown in Figure 1. In the following sections, we explain our model in detail."}, {"title": "Problem Setting", "content": "Given a pre-trained question-answering model F, which receives an input of context C, question q, and outputs answer a, such that F(q, C) = a. The objective is to deceive the performance of F with perturbed context C' such that F(q, C') \u2260 a. To craft C',"}, {"title": null, "content": "a certain number of perturbation Cadv is added to the context C by replacing some of its original tokens {C1, C2, ..., Cn}."}, {"title": "Attention-based Ranking (ABR)", "content": "Attention mechanisms were first used in image feature extraction in the computer vision field [62, 61, 11]. However, they were later employed by [2] to solve machine translation problems. In translation tasks, attention mechanisms enable models to prioritize and focus on the most relevant parts of the input data [34]. In question-answering tasks, attention scores are imported to examine the relationships between question and context, allowing the model to determine which words or phrases are most relevant to answering the question [60]. Hence, we leverage the attention score to identify target words for our attack. We employ the attention mechanism from T5 [43] that has been specifically optimized for question-answering tasks in UnifiedQA [21]. As shown in Fig. 1, the \u201cAttention-based Ranking\" begins by encoding the input context and question through an encoder. During the encoding process, self-attention allows the model to analyze how each word in the input relates to every other word, effectively highlighting the words that carry the most weight in understanding both question and context. In the decoding process, cross-attention further refines this by"}, {"title": null, "content": "focusing on the parts of the input most relevant to generating the correct output. By averaging the attention scores of all layers and heads, we match them to each input word.\nThe implement details are shown in Algorithm 1. The question & context pair is fitted into attention network A, and we filter out the attention scores for context (lines 1 to 8 of Algorithm 1). Then, the attention score of each word corresponding to each layer is summed up. After averaging and normalization, the word-level attention score is obtained."}, {"title": "Removal-based Ranking (RBR)", "content": "Previous studies on adversarial attacks in the text have shown that each word's significance can be quantified using an importance score [18, 30, 5, 31]. This score is largely determined by how directly the word influences the final answer. To enhance the efficacy of ranking progress, we rank each word in the context to obtain the removal importance score (lines 9 to 14 of Algorithm 1). Given the input context C containing n words from c\u2081 to cn and question q, the importance score (removal score) of the i th (1 \u2264 i \u2264 n) word ci is:\nI\u1d62 = Lf(a | q, C) \u2013 Lf(a | q, C \\ C\u1d62), (1)\nwhere C \\ ci represents the context after deleting ci, and Lf = log P(a | q, C) refers to the probability (logits) of the label, respectively."}, {"title": "Hybrid Ranking Fusion (HRF)", "content": "The attention-based and removal-based word selection techniques offer complementary perspectives on token significance, each highlighting different aspects of word importance. Consequently, we tend to choose words that both methods consider significant. This is achieved by adding the scores from each method for every word to create a fusion score.\nWhen generating a fusion score, we address several key factors. First, we independently normalize the attention and removal scores before adding them together. Then, to balance attack effectiveness and efficiency, we introduce a topk parameter, a positive integer that controls the number of words targeted. Finally, we select the topk highest-scoring words for modification (lines 15 to 18 of Algorithm 1)."}, {"title": "Synonym Selection", "content": "Various synonym generation methods exist, including Word2Vec [36], Hownet [9], and WordNet [9]. We adopt BERT [7] for synonym selection due to its textual capabilities, which enable it to generate synonyms based on the complete sentence structure. Unlike Word2Vec's static embeddings or WordNet's fixed synonym lists, BERT's context-sensitive approach allows for dynamic synonym selection that preserves both semantic meaning and grammatical correctness. This contextual awareness makes BERT particularly effective for crafting natural and semantically coherent adversarial examples."}, {"title": "Candidate Selection", "content": "We define an optimal adversary as one that maximizes the difference between the predicted answer and the attacked answer. For boolean queries, following textual classifier approaches that utilize logits to decide output label (yes/no), we compare the logits of output answers. For informative queries, we sum the logits of individual words. Using L to denote the logits derivation function, we identify the optimal adversary from the \"Adv list\" as shown in lines 24 to 35 of Algorithm 1."}, {"title": "Experiment and Analysis", "content": "In this section, we present a comprehensive evaluation of QA-Attack's performance compared to current state-of-the-art baselines. Our analysis covers several key aspects with various metrics, providing a thorough understanding of our method's capabilities, limitations, and performance across diverse scenarios. We provide a detailed analysis of attack performance and imperceptibility (Sec. 4.4). Besides, to gain deeper insights, we conduct ablation studies (Sec. 4.5) and assess attacking efficiency (Sec. 4.6). In addition, we examine QA-Attack's response to defense strategies (Sec. 4.8), exploring the effects of adversarial retraining (Sec. 4.7) and investigating the transferability of attacks (Sec. 4.9). Additionally, we report the preference of our attack by investigating parts of speech preference (Sec. 4.10) and analyzing its robustness versus the scale of pre-trained models (Sec. 4.11)."}, {"title": "Experiment Settings and Evaluation Metrics", "content": "The base setting of our experiments is let topk = 5, d = 2, and use a BERT-base-uncased\u00b9 with 12 Transformer encoder layer (L) and 768 hidden layers (H) as the synonym generation model. Tables 3, 4, and 5 summarize the experimental results on informative queries datasets, offering a comparative analysis of our QA-Attack method against five state-of-the-art QA baselines. For boolean queries, we present the attacking results on the BoolQ dataset in Table 6. Some visualised examples are shown in Table 2. Besides, we provide code for the reproductivity of our experiments\u00b2. The metrics used in our experiment are:\n\u2022 F1: The F1 score balances precision and recall, providing a nuanced view of how much the attacked answers match reference answers."}, {"title": "Platform and Efficiency Analysis", "content": "In this section, we evaluate QA-Attack's computational efficiency under base settings. We measure efficiency using time consumption per sample, expressed in seconds, where a lower value indicates superior performance. As shown in Table 10, the outcomes reveal that QA-Attack exhibits remarkable time efficiency, consistently outperforming baseline methods across both long-text (NarrativeQA) and short-text (SQUAD 1.1) datasets. This superior performance can be attributed to QA-Attack's innovative Hybrid Ranking Fusion (HRF) strategy, which effectively identifies vulnerable words within the text, significantly enhancing the speed of the attack process."}, {"title": "Adversarial Retraining", "content": "In this section, we investigate QA-Attack's potential for enhancing downstream models' accuracy. We employ QA-Attack to generate adversarial examples from SQUAD 1.1 training sets and incorporate them as supplementary training data. We reconstruct the training set with varying proportions of adversarial examples added to the raw training set. The retraining process with this augmented data aims to examine how test accuracy changes in response to the inclusion of adversarial examples. As illustrated in Fig. 3, re-training with adversarial examples slightly improves model performance when less than 30% of the training data consists of adversaries. However, performance decreases when the proportion of adversaries exceeds 30%. This finding indicates that the optimal ratio of adversarial examples in training data needs to be determined empirically, which aligns with conclusions from previous attacking methods. To evaluate how re-training helps defend against adversarial attacks, we analyze the robustness of T5 models trained with varying proportions of adversarial examples (0%, 10%, 20%, 30%, 40%) from different attack methods, as shown in"}, {"title": "Attacking Models with Defense Mechanism", "content": "Defending NLP models against adversarial attacks is crucial for maintaining the reliability of language processing systems in real-world applications [13]. To further analyse how attacks are performed under defense systems, we deploy two distinct defense mechanisms to investigate our attack performance under defense systems. The first is Frequency-Guided Word Substitutions (FGWS) approach [38], which excels at detecting adversarial examples. The second is Random Masking Training (RanMASK) [67], a technique that enhances model robustness through specialized training procedures. We perform the adversarial attack on T5 on datasets SQUAD 1.1, NarrativeQA and BoolQ, and the results are presented in Table 11. The results show that QA-Attack demonstrates superior adversarial robustness across multiple benchmark datasets, consistently outperforming existing methods against state-of-the-art defenses."}, {"title": "Transferability of Attacks", "content": "To evaluate our model's transferability, we test the adversarial samples generated for T5 on three distinct question-answering models: RoBERTa [70], DistilBERT [48], and MultiQA [53]. We also compare the transferability of three baseline methods: TASA,"}, {"title": "Parts of Speech Preference", "content": "To further understand the candidate words' distribution of our word-level attack, we examine its attacking preference in terms of Parts of Speech (POS), highlighting vulnerable areas within the input context. We use the Stanford POS tagger [54] to label each attacked word, categorizing them as noun, verb, adjective (Adj.), adverb (Adv.), and others (e.g., pronoun, preposition, conjunction). Table 12 illustrates the POS preference of our QA-Attack compared to baseline methods in the base setting. For \"informative queries\" on SQUAD dataset, most attacking methods predominantly target nouns, while TASA shows a slight preference for adverbs. In the case of \"boolean"}, {"title": "Robustness versus the Scale of Pre-trained Models", "content": "From the attacking results in Table 4 discussed in Sec 4.4, we recognize the limitation of our QA-Attack on BERTbase, with L = 12 and H = 768, which does not sufficiently support robust experimental outcomes. To address this issue and gain more"}, {"title": "Conclusion and Future Work", "content": "The robustness of QA models has been increasingly challenged by adversarial attacks. These attacks expose the vulnerabilities of models used in various tasks, including information retrieval, conversational agents, and machine comprehension. To address this, we introduced QA-Attack, which leverages Hybrid Ranking Fusion (HRF) to conduct effective attacks by identifying and modifying the most critical tokens in the input text. Through a combination of attention-based and removal-based ranking strategies, QA-Attack successfully disrupts model predictions while maintaining high levels of semantic and linguistic coherence. Extensive experiments have demonstrated that our method outperforms existing attack techniques regarding attack success, fluency, and consumption across various datasets, confirming its efficacy in undermining the robustness of state-of-the-art QA models.\nWhile adversarial attacks such as QA-Attack highlight the weaknesses in QA systems, they also provide an opportunity to test and improve model robustness. In future work, we plan to focus on developing defence strategies that mitigate these vulnerabilities. Furthermore, we intend to extend QA-Attack to handle more complex and diverse QA scenarios, including multiple-choice questions and multi-hop reasoning [65], to ensure that our method remains a powerful tool for evaluating and improving the robustness of QA systems in an evolving landscape of adversarial threats."}]}