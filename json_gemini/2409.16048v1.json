{"title": "Whole-body end-effector pose tracking", "authors": ["Tifanny Portela", "Andrei Cramariuc", "Mayank Mittal", "Marco Hutter"], "abstract": "Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system's complexity, making precise end-effector control challenging. Existing model-based approaches are often constrained by their modeling assumptions, leading to limited robustness. Meanwhile, recent Reinforcement Learning (RL) implementations restrict the arm's workspace to be in front of the robot or track only the position to obtain decent tracking accuracy. In this work, we address these limitations by introducing a whole-body RL formulation for end-effector pose tracking in a large workspace on rough, unstructured terrains. Our proposed method involves a terrain-aware sampling strategy for the robot's initial configuration and end-effector pose commands, as well as a game-based curriculum to extend the robot's operating range. We validate our approach on the ANYmal quadrupedal robot with a six DoF robotic arm. Through our experiments, we show that the learned controller achieves precise command tracking over a large workspace and adapts across varying terrains such as stairs and slopes. On deployment, it achieves a pose-tracking error of 2.64 cm and 3.64\u00b0, outperforming existing competitive baselines. The video of our work is available at: wholebody-pose-tracking.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past decade, recent algorithmic advancements have substantially increased legged robots' ability to traverse complex, cluttered environments and human-designed infrastructures, such as stairs and slopes [1]\u2013[4]. Despite these advancements, their practical applicability remains constrained by their limited manipulation capabilities. Most field operations with legged robots involve minimal environmental interactions through visual inspections and passive load transportation. Thus, combining a legged robot's mobility with the ability to perform manipulation tasks is critical for enhancing their applications to more real-world scenarios.\nCompared to fixed base counterparts, integrating an arm onto a legged mobile platform significantly complicates the controller design due to increased degrees of freedom, redundancy and highly non-linear dynamics. To address this, the research community has mainly explored model-based and learning-based control strategies.\nModel-based approaches, such as Model Predictive Control (MPC), have shown precise control on flat terrain by leveraging accurate models of the robot and its environment [5], [6]. However, solving MPC's control problem in real-time for legged manipulators often requires the use of simplified models [7], [8], which increases vulnerability to unexpected disturbances such as slipping or unplanned contacts.\nMore recently, Reinforcement Learning (RL) has emerged as a robust alternative, directly learning control policies through interactions in simulation, for locomotion [1], [9], [10] and whole-body control [11]. Unlike MPC's two-stage planning and tracking, this approach creates end-to-end control policies that map sensory inputs directly to motor commands, enabling improved resilience to external disturbances due to the introduction of environmental variability during training [12]\u2013[14]. Research on legged robots, whether using legs [15], [16] or attached arms [11], [17], demonstrate that RL techniques can achieve effective end-effector position tracking across a large workspace through agile whole-body behavior. Although effective in outdoor and slippery terrains, these approaches are unsuitable beyond basic flat terrain and lack orientation tracking.\nIn this work, we present a general-purpose whole-body controller for legged robots with an attached arm, designed to provide stable end-effector pose tracking across a large operational space. Our approach includes a terrain-aware sampling strategy for end-effector pose commands, and for the robot's initial configuration to ensure a smooth transition from a locomotion policy to the proposed whole-body controller. Experimental results show that the controller achieves precise tracking across varying terrains, such as stairs.\nOur key contributions are as follows:\n\u2022 An RL-based whole-body controller for 6-DoF end-effector pose tracking for quadrupeds with an arm."}, {"title": "II. RELATED WORK", "content": "A. Model-based whole-body control\nFormulating the whole-body planning and control of legged mobile manipulators into a single optimization problem avoids treating the arm as an external disturbance to the base [18], [19] and helps ensure tight coordination between the base and the arm [20], [21]. Belliscoso et al. [5] propose an online ZMP-based motion planning framework that relies on an inverse dynamics model to track generated operational-space references. In contrast, Sleiman et al. [6] provide an unified MPC formulation for whole-body dynamic locomotion and manipulation planning. Chiu et al. [22] further extend this formulation to account for self and environment collisions during the receding-horizon control. To increase the robustness towards external disturbances proactively, Ferrolho et al. [23], [24] incorporate a robustness metric into the trajectory optimization to compute offline plans for interactions under worst-case scenario. While these approaches are effective for end-effector control of legged mobile manipulators, they rely on an available system model and pre-specified gait schedule.\nThese assumptions make model-based approaches vulnerable to large unmodeled disturbances (for instance, a heavy grasped object), slippages, and unknown terrain models.\nB. Learning-based whole-body control\nMore recently, RL has shown to be a powerful alternative to model-based approaches for legged robots, providing robust control policies for locomotion on unstructured terrains [1], [10] and whole-body manipulation [11], [15], [25]. In the context of task-space tracking of a legged mobile manipulator's arm, existing works formulate the tracking problem in different ways, using different representations for the end-effector commands.\nMa et al. [26] propose a hybrid approach that employs an MPC planner for the arm and RL policy for locomotion. The MPC planner outputs arm joint position and base velocity commands. Piu et al. [27] follow a similar approach but train an RL policy for the arm to replace the MPC planner. The RL policy receives the end-effector pose command as a 3D position and Euler angles. Liu et al. [28] leverage a hierarchical approach to train a high-level policy that provides commands for an RL policy for the base and inverse kinematics controller for the arm. However, fixing a locomotion policy for the base limits the participation of the legged base in enhancing the workspace of the arm.\nFu et al. [11] train a combined RL policy for locomotion and manipulation. For the arm, they sample position commands in spherical coordinates and orientation commands uniformly in SO(3). However, their results are shown only for 3D position commands and on flat terrain. A follow-up"}, {"title": "III. METHOD", "content": "We train a policy to track end-effector target poses with minimal foot displacement, intended for use alongside a locomotion policy. Figure 2 illustrates the overall training process, starting with a data collection step, gathering terrain information and pre-sampling end-effector poses. During training, pose commands are sampled, checked for collisions, and used to train the policy.\nWe use Isaac Lab [31] as a simulation environment to train our policy, and we deployed our controller on ALMA [5], which integrates the Anymal D robot from ANYbotics [32] with the Dynaarm from Duatic [33]. The robot has an inertial measurement unit providing body orientation and 18 actuators with joint position encoders. The main control loop and the state estimator are executed at 400 Hz, while our policy runs at 50Hz on an onboard computer. While we show results on ALMA, our method remains applicable across different robot embodiments.\nA. Policy Architecture\nWe use Proximal Policy Optimization (PPO) [34], where both the actor and critic networks are implemented as multi-layer perceptrons with hidden layers of size [512, 256, 128], with ELU as the activation function. The hyperparameters of the PPO algorithm are taken from prior work [35].\nB. Command sampling\nWe pre-sample end-effector pose commands for a fixed base by iterating over the six joint angles of the arm, covering their entire range, and recording the end-effector poses that are collision-free in the base frame, as illustrated in Figure 2-B. The final dataset contains 10000 collision-free end-effector poses, with Figure 3A displaying a 250-pose subsample that illustrates the workspace. If these poses were used directly as commands, a simple inverse kinematics solver for the arm would suffice. However, this command sampling strategy would limit the controller's ability to achieve certain end-effector poses that could otherwise be reached by utilizing the entire body of the mobile manipulator. To overcome this limitation, we introduce a random body pose transformation $T \\in R^6$, applied to each pre-sampled command when a new pose is defined. This transformation is sampled within ranges of [-0.2, 0.2] m for the x and y dimensions, [-0.3, 0.1] m for z and, [-\u03c0/6, \u03c0/6] rad for roll, pitch and yaw. This approach ensures that the end-effector pose commands are reachable with minimal base movement. Figure 3B displays a 250-pose subsample of this expanded workspace.\nThe distribution of the pre-sampled commands is not uniform. We fit five z-aligned concentric cylinders, with the first cylinder solid and the subsequent cylinders having increasing radii with hollow sections. We determine their radii by first calculating the maximum radius in the x-y plane from the 3D positions of all pre-sampled poses and dividing it into five equal parts. The distribution of poses across these"}, {"title": "C. Command definition", "content": "cylinders, starting from the inner one, are as follows: 51%, 21%, 13%, 6%, and 2%. To enforce more spatially uniform sampling, we randomly select one of the bins and then sample a pose from within that selected bin.\nOn flat terrain, this sampling procedure generally yields feasible commands (a few poses might occasionally intersect with the robot's body due to the added base offset). However, some pose commands can fall below the terrain surface on unstructured terrain like stairs, as illustrated in Figure 3C. To mitigate this, we resample any command where the z-position component falls below the terrain surface height plus an 8 cm margin. We generate a coarse terrain grid map at the start of training to increase training speed and avoid real-time terrain queries. In the x-y plane, this map stores the highest terrain point within a 20 x 20 cm patch centered on each point, as illustrated in Figure 2-A.\nFinally, a new command is generated every 4 seconds, and with a 12-second episode length, the robot is exposed to 3 different commands per episode. This setup allows the robot to learn how to reach an end-effector pose from any arm configuration instead of a single-pose episode, where it would always do so from the default arm configuration.\nThe command of the policy is an end-effector pose $P_{ee} \\in SE(3)$, typically represented by a separate position and orientation [11], [29]. Separating these components introduces two main challenges. First, defining a rotation representation that is easily learnable is difficult [36]. Second, this separation requires a fixed trade-off between position and orientation rewards, which may not be optimal for all workspace poses.\nTo avoid these issues, we use a keypoint-based representation similar to that used in [37] for in-hand cube reorientation, which has been shown to improve the ability of the RL algorithm to learn the task at hand. In this formulation, the keypoints represent the vertices of a cube centered on the end-effector's position and aligned with its orientation. While 8 corner points fully define the cube, we use the minimum required of 3 keypoints with direct correspondence between measured and target poses to uniquely and completely define the pose. The side length of the cube is set to 0.3 meters."}, {"title": "D. Action and Observation Space", "content": "The robot's movement is managed through an eighteen-dimensional space ($a_t \\in R^{18}$). This action space controls position targets for a proportional-derivative controller applied to each robot joint. The joints include the legs' thigh, calf, and hip joints and the six arm joints. The position targets are derived as $\\sigma_a a^+ + q_{def}$, where $\\sigma_a = 0.5$ is a scaling factor, and $q_{def}$ represents the robot's default joint configuration, which corresponds to the robot standing with its arm raised.\nThe observation, represented as $o_t$, relies solely on proprioceptive information. Its elements consist of the gravity vector projected in the base frame $g_t \\in R^3$, the base linear and angular velocities, $v \\in R^6$, the joint positions, $q_t \\in R^{18}$ and the previous actions $a_{t-1} \\in R^{18}$:\n$o_t = [g_t, v_t, q_t, a_{t-1}] \\in R^{45}$"}, {"title": "E. Rewards", "content": "The final reward R is the sum of the task rewards $R_T$ and penalties $R_p$: $R = R_T + R_p$. The task reward $R_T$ can be divided into four subcategories: tracking, progress, feet contact force, and initial leg joint rewards: $R_T = w_1R_t + w_2R_p + w_3R_f + w_4R_q$, where $w_1 = 13$, $w_2 = 80$, $w_3 = 0.015$ and $w_4 = 0.4$.\nTracking Reward ($R_t$) is a delayed reward focused on tracking the three keypoints, representing the end-effector pose command, during the last two seconds (T = 2s) of a 4-second command cycle (T = 4s). Delaying the reward emphasizes the importance of being in the correct pose during the final 2 seconds without penalizing the path to getting there. This prevents the unwanted behavior that continuous rewards might encourage, such as passing through the robot's body when transitioning from a pose on one side of the robot to a target on the opposite side.\n$R_t = \\begin{cases}\n    \\frac{1}{3}\\sum_{k=0}^{2} e^{\\frac{-\\lvert\\lvert p_{ee,k}^{meas} - p_{ee,k}^{cmd} \\rvert\\rvert}{\\sigma_t}} & \\text{if } t > T - T_r \\\\\n    0 & \\text{otherwise}\n\\end{cases}$\nHere, $p_{ee,k}^{meas}$ and $p_{ee,k}^{cmd}$ are the positions of the measured and commanded keypoints in R\u00b3, respectively, and $\u03c3_t = 0.05$.\nProgress Reward ($R_p$) addresses the sparsity of the tracking reward and reduces end-effector oscillations by incentivizing steady progress toward the target. It compares the current distance $d_t \\in R^3$ between the measured and commanded keypoints to the smallest previously recorded distance $\\overline{d} \\in R^3$. If $d_t$ is smaller, the reward is calculated as:\n$R_p = \\begin{cases}\n    \\frac{1}{3}\\sum_{k=0}^{2} e^{\\frac{-\\lvert\\lvert d_{k} - \\overline{d} \\rvert\\rvert}{\\sigma_d}} & \\text{if } d^+ < \\overline{d} \\\\\n    0 & \\text{otherwise}\n\\end{cases}$\nFeet Contact Force Reward ($R_f$) encourages the robot to maintain ground contact with all four feet. The reward is non-zero only if all four feet are firmly in contact with the ground. To account for small disturbances and ensure genuine contact, 1 Newton is subtracted from the force on each foot, denoted as $F_i$. The reward is calculated as the sum of these adjusted forces:\n$R_f = \\sum_{i=1}^{4} max(F_i - 1, 0)$\nInitial Leg Joint Reward ($R_q$) encourages the robot to maintain its leg joints in a configuration close to those sampled from the locomotion policy, as these are known to result in a stable posture, with $\u03c3_q$ defined as 0.05.\n$R_q = \\sum_{i=0}^{12} e^{\\frac{-\\lvert\\lvert q_i - q^{init}_i \\rvert\\rvert}{\\sigma_q}}$"}, {"title": "F. Terrains and Curriculum training", "content": "Penalties ($R_p$) penalize joint torques, joint accelerations, action rates and target joint positions above the limits: $R_p = w_5 ||\u03c4||^2 + w_6||\\dot{q}||^2 + w_7||a_t \u2013 a_{t-1}||^2 + w_8||q-q_{lim}||_1$, where $\u03c9_5 = -3e-5$, $\u03c9_6 = \u22123e\u22126$, $w_7 = \u22125e-2$ and $w_8 = -1.3$. Finally, we terminate on knee and base contacts.\nThe robots are trained in simulation on four procedurally generated terrains: flat, randomly rough, discrete obstacles, and stairs, as defined in [35]. Gradually increasing task difficulty during training has been shown to enhance learning in previous works [1], [35], [38]. We employ a terrain curriculum similar to the one proposed in [35], but adapted for end-effector pose tracking. If the average position tracking error across the three commands within an episode is under 20 cm when the task reward is active (during the final 2 seconds of each 4-second command), and the average orientation tracking error is under 20\u00b0, the terrain difficulty increases after the next reset. The terrain difficulty decreases if the error exceeds 80 cm and 120\u00b0. Robots achieving the highest level are assigned to a random level to prevent catastrophic forgetting."}, {"title": "G. Initial poses", "content": "The whole-body end-effector pose tracking policy proposed in this work does not include locomotion capabilities. This decision is motivated by the observation that very few tasks require simultaneous locomotion and active object manipulation. Therefore, this policy is designed to operate alongside a separate locomotion policy.\nTo facilitate the learning of a stable leg posture, the initial base pose $p_{init} \\in SE(3)$ and joint angles $q_{init} \\in R^{18}$ of the robots upon reset are taken from a pre-trained locomotion policy [39]. This policy, designed for the same mobile-legged manipulator using RL, incorporates arm joint positions and velocity measurements in its observation space. It only controls the leg joints, enabling rough terrain locomotion with any arm configuration. This locomotion policy takes a three-dimensional base velocity command as input: linear velocities along the x and y axes and yaw rotation.\nBefore training, robots are spawned at the center of the terrain and given randomized heading commands in the interval [-1,1] rads\u00af\u00b9 and linear velocity commands in the interval [-1,1] ms\u00af\u00b9 for a 4-second period, as illustrated in Figure 2-C. After this time, the terrain, the base pose p\u044c, and the joint angles q are recorded for robots that remain in stable configurations, defined as those with an angle difference between the gravity vector and their base-aligned projected gravity vector less than 55\u00b0. When training begins, the saved terrain and robot configurations are loaded. This process also ensures a smooth transition from the locomotion to the whole-body policy, as omitting this step causes the robot to jump when switching between policies."}, {"title": "H. Sim-to-Real", "content": "When the training starts, we add a mass on the end-effector randomly sampled from the interval [0, 1.8] kg, and the inertia"}, {"title": "IV. RESULTS AND DISCUSSION", "content": "A. Simulation experiments\n1) Pose representation comparison: To analyze different representations for end-effector pose commands, we evaluate the tracking performance of our keypoint-based pose representation against three other representations from related works: a 3D vector for end-effector position combined with (A) a quaternion [11], (B) with Euler angles [27], or (C) with a 6D vector representation [29]. For each representation, the command is adapted to include both the position and orientation differences expressed in the base frame. Specifically, for (A), we use quaternion multiplication with the inverse of the measured quaternion, we use the difference of Euler angles for (B), and the difference between 6D vectors for (C).\nOur tracking reward is adapted to $R_t = e^{\\frac{-\\lvert\\lvert p^{meas} - p^{cmd} \\rvert\\rvert}{\\sigma_t}}$ for t > T-Tr, where \u03c3\u03c4 = 0.15. The progress reward is given by $R_p = (\\Delta_{pos} - \\overline{\\Delta}_{pos}) + (\\Delta_{rot} \u2013 \\overline{\\Delta}_{rot})$, provided that both $\u0394_{pos} < \\overline{\\Delta}_{pos}$ and $\u0394_{rot} < \\overline{\\Delta}_{rot}$. The position error \u0394pos is calculated as the norm of the difference in 3D position, while the orientation error \u0394rot is derived from the rotational difference between quaternions. For all pose representations, including (A), (B), and (C), we convert the rotations back to quaternions, multiply one quaternion by the conjugate of the other, and then convert the result to an axis-angle representation to calculate the error.\nAs shown in Figure 4, the keypoint-based pose representation significantly outperforms the other three, with the 6D representation (C) ranking second with an average tracking error of 16.03 cm and 3.87\u00b0 larger than ours. Both quaternion (A) and Euler angles (B) representations yield similar results, with average errors 27 cm and 6.3\u00b0 exceeding our approach. The superior performance of the keypoint-based and 6D representations, compared to the discontinuous quaternion and"}, {"title": "V. CONCLUSION", "content": "Euler angle representations, likely stems from their continuous nature, as discussed in [36]. Tuning rewards for pose tracking with separate terms for position and orientation proved challenging, due to the difficulty of balancing two quantities with different units and magnitudes. Achieving both position and orientation tracking in (A), (B), and (C) required many iterations, and often, the training would collapse, prioritizing either position or orientation tracking but rarely both. In contrast, the keypoint-based pose representation required far less tuning due to its unified representation of both aspects.\n2) Comparison to model-based control: We compare the tracking performance of our whole-body RL policy with the model-based whole-body MPC controller from prior work [22]. For this comparison, we use the same weight parameters as those in the original paper for the MPC controller, which has been optimized for the same robot. We first evaluate both controllers on flat terrain using the same 15 end-effector pose commands. For easy poses within the initial workspace (Figure 3A), both controllers perform similarly, with average errors of 1.22 cm and 1.73\u00b0 for the RL controller, and 1.52 cm and 1.02\u00b0 for the MPC controller. However, for poses in the expanded workspace (Figure 3B), the RL policy outperforms the MPC controller, showing average position errors of 0.96 cm and 1.29\u00b0 versus 2.71 cm and 9.73\u00b0 for the MPC controller. Figure 5 illustrates their tracking performance, highlighting the MPC controller's limited whole-body behavior. While the MPC controller is primarily designed for flat terrains, we include one experiment on stairs with 15 poses under two stable base orientations. However, 30% of the tested orientations were unstable for the MPC controller and were excluded from this evaluation. In this experiment, the MPC controller reaches an average position error of 24.3 cm and 45.47\u00b0, whereas our RL policy achieves significantly better tracking accuracy with 3.5 cm and 2.67\u00b0. Furthermore, for the MPC controller, we observed that the arm often gets stuck near the base when moving between distant poses due to conflicting objectives of pose tracking and self-collision avoidance, a problem that our RL controller effectively avoids.\nB. Hardware experiments\n1) Pose tracking accuracy: We assess our controller's tracking performance using a motion capture system across 20 randomly sampled poses in the expanded workspace (Figure 3B). Poses are sent sequentially, with substantial changes in position and orientation, resulting in effective whole-body behavior as shown in Figure 1. The average error reaches 2.03 cm and 2.86\u00b0. These results, which are illustrated in Figure 6, closely match the performance observed in simulation, demonstrating a minimal sim-to-real gap.\n2) Robustness to external disturbances: We evaluate the tracking performance of our whole-body RL policy on stairs using a motion capture system across 20 sampled poses in the expanded workspace (Figure 3B) in the half-space in front of the robot under three base orientations: sideways on the stairs, facing up and facing down, as shown in Figure 1. The average position error reaches 2.64 cm, and the average orientation error 3.64\u00b0. Figure 6 shows that the tracking performance remains consistent with that on flat terrain.\nWhen switching orientations on the stairs a locomotion policy is used [39] and the transition between policies is smooth thanks to the robot initialization process described in Section III-G. Without this initialization step, the robot experiences jumps when transitioning between policies. Additionally, the system can handle up to 3.75 kg of weight on the end-effector when stationary, and up to 1.3 kg in movement. This flexibility is advantageous as it avoids the need to model weight changes, typically required in model-based approaches, allowing for the attachment of various end-effectors and dynamic carrying of unknown payloads during operation.\nWe have presented a whole-body RL-based controller for a quadruped with an arm that can reach even the most difficult poses. Our controller achieves high accuracy also on rough terrain (e.g., on stairs), which we show in real-world experiments with ANYmal with an arm. Additionally, our contributions include providing a formulation for learning pose tracking that is superior to existing methods with poor accuracy or only considering position tracking. Our hardware experiments show an average tracking accuracy of 2.64 cm for position and 3.64\u00b0 for orientation on challenging terrain. Future work includes integrating 3D representations of the environment into the pipeline to enable self-learned collision avoidance, as done for locomotion in [40]."}]}