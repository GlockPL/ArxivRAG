{"title": "EFFICIENT CUTTING TOOL WEAR SEGMENTATION BASED ON SEGMENT ANYTHING MODEL", "authors": ["Zongshuo Li", "Ding Huo", "Markus Meurer", "Thomas Bergs"], "abstract": "Tool wear conditions impact the surface quality of the workpiece and its final geometric precision. In this research, we propose an efficient tool wear segmentation approach based on Segment Anything Model, which integrates U-Net as an automated prompt generator to streamline the processes of tool wear detection. Our evaluation covered three Point-of-Interest generation methods and further investigated the effects of variations in training dataset sizes and U-Net training intensities on resultant wear segmentation outcomes. The results consistently highlight our approach's advantage over U-Net, emphasizing its ability to achieve accurate wear segmentation even with limited training datasets. This feature underscores its potential applicability in industrial scenarios where datasets may be limited.", "sections": [{"title": "1. INTRODUCTION", "content": "Tool wear is an inevitable phenomenon in the actual machining process. It leads to alterations in the cutting zone's process variables like the forces and temperatures exerted on both the tool and workpiece. These conditions not only influence the rate of tool wear but also affect the surface quality and geometric precision of the workpiece [1]. Therefore, tool wear is one of the key determinants of both tool costs and the quality of the finished workpiece, emphasizing the necessity for monitoring during the machining process to ensure optimal outcomes [1, 2].\nTool wear measurement is broadly classified into two categories: Direct and Indirect methods [3]. Indirect methods use sensors, such as dynamometers, to consistently track tool wear through wear model [4, 5]. A fundamental requisite for indirect methods is the execution of tool wear experiments, paired with direct tool wear measurements, to comprehend the actual tool state [6]. Direct tool wear measurements predominantly use optical sensors, such as optical microscopes, with subsequent manual or automated tool wear detection. In data-intensive automated production systems, automated tool wear detection driven by classical Computer Vision (CV) and/or Machine Learning (ML) offer advantages over manual methods. Classical CV like Sobel and Canny algorithms, along with the active contour method [7-9], have been extensively utilized for detecting tool wear [10-12]. Their primary benefit lies in the ability to obtain reasonable outcomes with only a minimal dataset [13]. However, these methods are sensitive to challenging lighting conditions and varied background environments, complicating the robust feature extraction [14].\nIn contrast, considering the common challenges in metal cutting, such as fluctuating lighting conditions, coating colors, and variations in tool geometries, ML algorithms demonstrate superior performance in tool wear detection through their advanced adaptive capabilities and outperforms traditional image processing methods [10, 15, 16]. Hou et al. [17] developed a self-matching algorithm to measure milling cutter wear, achieving a mean absolute error of less than 7 \u00b5m of maximum width of the flank wear land, and a maximum error under 57 \u00b5m for severe wear and breakage scenarios. Tool wear detection inherently aligns with semantic segmentation tasks, for which U-Net has increasingly become one of the favored solutions due to its effectiveness [18-21].\nIn our previous research, we proposed a methodology based on U-Net for the classification of tool types and the detection of tool wear areas in microscopic images. This approach yielded promising outcomes, achieving a mean Intersection over Union (IoU) of 0.73 on the test dataset of hybrid multiple tools [6]. Subsequently, we developed a comprehensive pipeline integrating U-Net to process tool wear images captured with a digital microscope and employed a rule-based methodology to quantify wear along the tool cutting edge. This approach facilitates fully automated tool wear detection and measurement, showing a high alignment (R2 = 0.99) with manual measurement benchmarks [22]. The application of U-Net for tool wear segmentation was further refined and advanced. Xia et al. [14] introduced an enhanced SE-U-Net, supplementing network outputs with morphological processing to achieve an average segmentation accuracy of 92%. Furthermore, Chen et al. [23] utilized a generative adversarial network to augment the dataset and employed transfer learning to boost the segmentation network's generalization capacity. Compared to training with only small samples, this approach improves the mean IoU by 8.97%.\nHowever, U-Net encounters limitations, particularly in the low accuracy of tool wear recognition for unknown and disturbed tool images. On one hand, the method needs a substantial dataset to ensure the model's generalization capacity, enabling it to make reliable predictions even with data disturbances. This challenge involves not only deviations in images of the same tool captured under varying conditions, but also deviations across tools with different geometries. On the other hand, the requirement for these substantial data volumes inherently leads to an extensive need for manual annotation [6]. Such demands, both in terms of dataset size and extensive labeling, are often impractical in industrial environments. For any specific tool, the number of images obtainable from experiments and suitable for training the network, is typically limited. This paucity of data leads to a limitation of this method for direct industrial applications.\nIn recent years, there has been notable progress in the development of pre-trained large language models (LLM). These models, trained on expansive datasets, exhibit remarkable zero-shot generalization abilities, which means they can handle previously unseen tasks without retraining. A common technique employed with these models is prompt engineering, which directs the model to address specific tasks. Drawing inspiration from LLMs, Meta introduced a promptable semantic segmentation model, Segment Anything Model (SAM), in CV. Pre-trained on an extensive dataset comprising 1 billion masks and 11 million images, this model demonstrates robust generalization [24].\nTherefore, building on previous research, this study introduces a novel approach to tool wear segmentation, integrating U-Net with SAM. This approach takes advantage of SAM's robust generalization capabilities to post-process U-Net's outputs, ensuring high accuracy in tool wear segmentation even with limited datasets and the lower quality outputs of U-Net. We have also explored the impact of various factors on the final tool wear segmentation accuracy, including the methods for generating the Point-of-Interest (PoI), the volume of training data, and the U-Net training intensity. Chapter 2 describes our newly proposed approach. Chapter 3 details the prepared dataset, the model parameters, and the evaluation strategy. Chapter 4 is dedicated to a comprehensive analysis and discussion of results. The last chapter summarizes our work and gives an outlook on its future development."}, {"title": "2. SEGMENTATION OF TOOL WEAR BASED ON SAM", "content": "This chapter begins with an introduction to U-Net and SAM, followed by the description of our proposed approach."}, {"title": "2.1 U-Net", "content": "U-Net stands out as an FCN architecture. Characterized by a substantial number of feature channels, U-Net effectively propagates contextual information to its higher resolution layers. The architecture follows a U-shaped structure, with its symmetric contraction and expanding paths. The training process of U-Net is end-to-end, which enables the network to produce image segmentation masks directly from the original input image [18].\nU-Net's contraction path uses a classical Convolutional Neural Network (CNN) architecture. Within each stage, it incorporates two 3\u00d73 convolution units, immediately succeeded by a rectified linear unit (ReLU) activation function, and a subsequent 2\u00d72 max-pooling operation for downsampling. Conversely, in the expanding path, a 2\u00d72 up-convolution unit is employed. A special attribute of U-Net is the incorporation of skip connections, which facilitate the addition of the output from the contraction path on the same stage into the expanding path. This structural feature of combining low-level features with high-level features helps to avoid information loss [19]. The final layer of the network employs a 1\u00d71 convolution to map all feature channels associated with each pixel to the class corresponding to that pixel. This elegant architecture ensures its applicability extends beyond its initial biomedical segmentation domain and can be easily applied to other tasks like tool wear semantic segmentation."}, {"title": "2.2 SAM", "content": "SAM, serving as a foundation model for image segmentation, can efficiently produce accurate image segmentation masks with prompts that indicate segmentation targets in the image. The prompts can be in various formats, including target or background Pol, boxes, masks, or even free-form text [24]. SAM consists of three parts: an image encoder, a prompt encoder, and a mask decoder."}, {"title": "2.3 Our Approach", "content": "In our previous research, U-Net has demonstrated effective performance in specific tasks. With a training dataset comprising similar data, U-Net can achieve precise tool wear segmentation results for either individual tools or multiple tools within a composite test set [6]. This observation aligns with findings from other studies as well [14, 19-21, 23]. However, a notable decline in U-Net's performance is observed when applied to tools dissimilar from those in the training set. This indicates the necessity of additional, scene-specific training datasets for each unique scenario encountered [6]. Therefore, the direct deployment of U-Net in an industrial context faces challenges due to limited training data and its weak generalization capability for unfamiliar data. Despite these constraints, U-Net is still capable of providing preliminary, albeit semi-accurate, tool wear segmentation. On the other hand, although SAM has the robust zero-shot generalization capability, its direct application to the tool wear segmentation task yields suboptimal performance. This shortfall is primarily due to SAM's inability to accurately segment tool wear without appropriate prompt guidance. Therefore, the use of appropriate prompts offers potential to improve SAM's performance, e.g., when combined with U-Net. Consequently, in this approach, we employ U-Net as an automated prompt generator. Initially, U-Net provides an initial image segmentation mask. This mask undergoes binarization as the first step. Following this, employing a PoI generator, this binarized mask is used to produce a set of PoI, including target points (representing tool wear) and background points (indicating non-wear). The resulting PoI, in combination with the initial binarized mask, is processed by SAM's prompt encoder. Finally, SAM's mask decoder will process the prompts to output the precise tool wear segmentation mask."}, {"title": "2.4 Point of Interest Generator", "content": "The goal of the PoI generator is to generate positive points in the center of the foreground region of the image segmentation mask, thereby prompting the segmentation target. In this regard PoI generator also generate negative points in the background region of the mask to enhance the prompt. We employed three different methods to identify the location of the positive point. These include the Mask Shrink (MS) method [30], the Center of Gravity Adjustment (CoGA) method, and the Recursive Center of Gravity Adjustment (RCOGA) method."}, {"title": "MS.", "content": "The MS method is derived from morphological image processing, aimed at enhancing image quality by shrinking the foreground region through successive erosion operations [30]. MS can also deduce the central point or line of a mask following multiple iterations of shrinking."}, {"title": "COGA.", "content": "We propose a simple method for determining the single center of gravity (CoG), aiming for a fast identification of the positive point within the foreground region."}, {"title": "RCOGA.", "content": "Based on CoGA, we propose RCOGA as a method to generate multiple positive points. This approach aims to counteract potential prompt deficiencies in single positive point derived from CoGA."}, {"title": "Generation of Negative Points.", "content": "Building upon the identified positive points, we find four contour points on the foreground contour, horizontally and vertically for each positive point. Subsequently, four extrapolated points are derived by extending a specified pixel distance along the direction from the positive point to the contour points. If these extrapolated points locate within the foreground mask, they are dropped. Otherwise, they are returned as negative points."}, {"title": "3. MATERIALS AND METHODS", "content": "This chapter details the prepared dataset, the model parameters, and the evaluation strategy."}, {"title": "3.1 Dataset", "content": "In this study, the image dataset was sourced from micrographs of the flank face of cutting tools, captured utilizing a Keyence VHX-6000 series microscope in prior experimental trials. Six different tools, including different end mills and inserts, are included in this dataset. They vary in coating, substrate, material being machined and process. This provides a comprehensive view of various tool types and their wear patterns. The focus of this study was specifically on flank wear. These images are stored in an RGB three-channel format, with a consistent resolution of 1024 x 1024 pixels. Within the dataset corresponding to each tool, 20% of the images are stochastically selected as the test dataset, with selection criteria predicated upon wear area distribution. This selection ensures homogeneity in wear area distribution between both training and test datasets. Model performance evaluation will be conducted on this designated test dataset to endure the comparability between different models. The residual 80% constitutes the training dataset. A detailed enumeration of tool types and the corresponding number of images is presented. Additionally, Figure 8 exhibits a selection of representative images.\nData augmentation plays an important role in enhancing the network's robustness, especially in scenarios with limited training samples [31]. Through data augmentation, there is not only an expansion in the volume of images with diversified distributions during model training, but also an enhancement in model generalization and a consequential reduction in overfitting [32]. Given the limited quantity of available images in this study, we have incorporated specific data augmentation operations."}, {"title": "3.2 Metrics", "content": "The IoU metric, also known as Jaccard index [33], stands as a prevalent performance metric in image segmentation tasks. IoU computes the ratio of the intersecting area between the model's segmentation and the ground truth to the cumulative area of both within the image, with the Eq. (1), where N is the number of pixels, $g_i$ is the ground truth, and $y_i$ is the model output.\n$IoU = \\frac{\\sum_{i=1}^N g_i y_i}{\\sum_{i=1}^N (g_i + y_i - g_i y_i)}$      (1)\nIoU's value spans from 0 to 1: a value of 0 signifies no overlapping area, while a value of 1 indicates perfect match between the model's segmentation and the ground truth. Figure 10 offers examples of IoU values for some sample segmentation results: Red denotes area where the model's segmentation is exclusive of the ground truth, green signifies correct segmentation area and yellow represents area the model failed to segment."}, {"title": "3.3 U-Net Parameter", "content": "The foundational architecture of original U-Net [18] is preserved in our implementation, maintaining the same convolutional units positioned along both the contracting and expanding paths, as well as the sampling unit. Given that our input consists of an image dimensioned at 1024\u00d71024\u00d73, the resolution for each convolutional unit within the network has been amplified by a factor of 4. Furthermore, we have modified the loss function to incorporate both binary cross-entropy (BCE) loss and IoU loss. The pixel-level BCE loss is typically employed in training networks for standard image segmentation tasks where the misclassification of fewer pixels only has minimal impact. The entire loss function, employed throughout the network's training phase, is shown in Eq. (2).\n$loss =l_{BCE} + l_{IoU} = \\frac{1}{N} \\sum_{i=1}^{N}(g_i log y_i + (1 - g_i) log (1 \u2013 y_i)) + 1- \\frac{2 \\sum_{i=1}^{N} y_i g_i}{\\sum_{i=1}^{N}(y_i + g_i)}$      (2)"}, {"title": "3.4 SAM Parameter", "content": "We retained the original configuration of the SAM without modifications and training. We selected the ViT-L SAM [24] in our implementation. Given that the U-Net's output size is 1024\u00d71024, a 2\u00d72 max-pooling operation was employed to reduce the dimension to 256x256, which matches the SAM's prompt encoder input. From the three potential masks produced by the mask decoder, the mask with the highest probability was selected as the model's final output."}, {"title": "3.5 Evaluation Strategy", "content": "The generation of expansive datasets and their subsequent manual annotation are often impractical in industrial environments, limiting the direct industrial application of U-Net. Given this challenge, this study aims to ascertain the feasibility of the proposed approach in achieving tool wear segmentation, even with limited dataset volumes.\nIn the initial phase, several representative PoI generation methods were compared. The U-Net model is initially trained utilizing the entire training dataset. Subsequently, its segmentation outputs on the test dataset will serve as foundational data for PoI generation, with methods delineated in Chapter 2. These generated PoI, together with the U-Net's image segmentation outputs, serve as input prompts for the SAM model. A comparative analysis is then conducted to evaluate SAM's image segmentation performance relative to different PoI generation methods.\nDuring the second phase, subsets of the original training dataset, ranging from 20% to 100%, are stochastically selected to constitute new training datasets of varied sizes. Several U-Nets are trained on these varied-size training datasets. PoI generation based on the outputs of U-Nets on the test dataset are then be executed, with the optimal method identified during the first phase. These points and segmentation outputs of U-Net serve as input prompts for SAM. A comparative assessment of SAM's image segmentation performance relative to U-Net's is undertaken to evaluate the proposed approach's feasibility in tool wear segmentation with reduced dataset sizes."}, {"title": "4. RESULTS AND DISCUSSION", "content": "This chapter initiates with a comparative analysis of the performance metrics of three distinct PoI generation methods. Subsequently, the most efficacious PoI generation method will be employed to examine the impact of training data volume on the performance of our proposed approach. Ultimately, we will assess the impact of varying U-Net training intensities on the proposed approach's performance, particularly with limited training data."}, {"title": "4.1 Impact of the Points of Interest Generator (Phase 1)", "content": "shows the performance of three different PoI generation methods: MS, COGA and RCOGA, evaluated on the test dataset. The U-Net underwent training for 100 epochs utilizing the entire training dataset and achieved convergence. As shown in Fig. 12, all three PoI generation methods exhibit comparable performance across the entire test dataset, each yielding an IoU of about 0.87. However, a variance in performance is observed across the test datasets for different tools. Specifically, End Mill 1 and Insert 3 exhibit suboptimal outcomes relative to the other four tools. This disparity is attributed to End Mill 1's wear area, characterized by its small and elongated nature, leading to a single positive point that proves less effective in prompting distal wear areas. The performance on Insert 3 is compromised by the presence of unclean cutting edges like built-up edges and residual chips, impacting the model's ability to accurately segment the real wear region. Notably, in the case of End Mill 2, COGA has an IoU advantage of 0.01 over its counterparts, while it incurs a deficit of 0.01 in End Mill 1. An Analysis of Variance (ANOVA) conducted for the PoI generation methods yielded a p-value of 0.999, indicating an absence of significant performance differences among the three methods.\nshows the performance of the proposed approach, employing three PoI generation methods, across varied U-Net training intensities. At training intensities of both 80 and 100 epochs, all three methods exhibit near-identical performance. However, for epochs below 60, RCOGA consistently matches or outperforms both CoGA and MS, registering an approximate 0.03 advantage in comparison to the least effective method. This suggests that in case of rougher tool wear segmentations, RCOGA's strategy of generating multiple positive points by segmenting finer tool wear regions helps to provide better prompt quality. Consequently, RCOGA has been selected for PoI generation subsequent analyses."}, {"title": "4.2 Impact of the Training Dataset Size (Phase II)", "content": "shows the performance differences between U-Net and our proposed approach, setting U-Net's results as the referential baseline. U-Net undergoes training on datasets of varying sizes, with the PoI generated by RCoGA. The generated prompt from different U-Nets is fed into SAM to deliver the final tool wear segmentation outcomes.\nAs shown in Fig. 15, our proposed approach consistently demonstrates better performance relative to U-Net. The performance difference is especially prominent when using a small training dataset (20%), with a difference ranging between 0.03 and 0.09. However, as the dataset's magnitude escalates, this advantage diminishes incrementally. For a 60% training dataset, the advantage narrows to a range of 0.02 to 0.05, and with a full 100% training dataset, it reduces further to less than 0.01. This performance advantage is particularly evident in the case of Insert 3, In this example, the IoU of the U-Net's segmentation result is only 0.65, while our approach achieves an IoU of 0.88. This outperformance can be attributed to U-Net's susceptibility to image blurring, while our approach exhibits enhanced robustness to blurring. A subsequent ANOVA conducted for training dataset size yielded a p-value of 0.001, indicating a statistically significant impact of the volume of training dataset on the observed performance discrepancy.\nshows the performance of both U-Net and our proposed approach on the entire test dataset across the different training dataset sizes. The results clearly indicate a consistent IoU advantage for our approach, particularly pronounced at 20% training dataset, where it reaches an advantage of 0.04. This advantage persists up to the 60% training dataset. From 80% onwards, the advantage narrows to a margin of less than 0.01. Notably, our approach registers an IoU of 0.83 at a 20% training dataset. This metric attains on U-Net only when it trained with a more substantial 60% dataset. Furthermore, our approach outperformance U-Net with a 40% training dataset compared to U-Net's 80% dataset. On a 40% training dataset, our approach already achieves a higher IoU even when U-Net is trained with a larger 80% dataset. These observations emphasize that our proposed approach efficiently leverages smaller training datasets to match or even exceed the performance levels that U-Net achieves only with considerably larger datasets. Crucially, our approach consistently retains its performance advantage over U-Net, irrespective of training dataset size."}, {"title": "4.3 Impact of the U-Net Training Intensity", "content": "Our approach has demonstrated pronounced advantages with limited training datasets. Based on this, we delve deeper into examining the impact of various U-Net training intensities on our approach when training with a 20% training dataset. This simulates the performance of our approach at initial segmentations with lower quality. contrasts the performance between U-Net and our approach, with U-Net's performance serving as the reference baseline. Different colors represent various U-Net training epochs. The output of U-Net on the test dataset, across these epochs, undergoes PoI generation with RCOGA and is subsequently processed by SAM to produce the final tool wear segmentation results. As depicted, our approach exhibits substantial dominance over U-Net, accentuated at U-Net's lower training epochs. With merely 10 epochs of U-Net training, our approach achieves an average advantage of 0.16, peaking to an advantage exceeding 0.23. At 20 epochs, this average reduces to 0.11, and further reduces to 0.06 at 40 epochs. This advantage incrementally narrows with more advanced epochs, settling between 0.03 and 0.07 at the 100-epoch. An ANOVA conducted on U-Net's training epochs returned a p-value of $3.7 \u00d7 10^{\u22126}$, indicating the significant impact of U-Net's training intensities on performance discrepancies. A notable exception arises with the tool Insert 3 at the 10-epoch mark, where the advantage is only 0.02. This can be attributed to U-Net's worse tool wear segmentation performance at this epoch, registering a low IoU of 0.37. Such low-quality segmentations introduce flawed prompts, detrimentally impacting our approach's efficacy.\nshows the performance of both U-Net and our proposed approach on the entire test dataset across the different U-Net's training epochs. Our approach consistently outperforms U-Net. Similar conclusions can be drawn that our approach consistently outperforms U-Net, although this advantage reduces as U-Net undergoes extended training. Notably, after 20 epochs, our approach can already achieve an IoU nearing 0.8, with only a marginal improvement of 0.04 observed at the 100 epochs. Such performance underscores our approach's capacity to substantially refine U-Net's initial rough tool wear segmentation outputs, even with limited training datasets."}, {"title": "5. CONCLUSION AND OUTLOOK", "content": "In this study, we propose an efficient tool wear segmentation approach based on SAM and U-Net. Our approach employs U-Net as an automated prompt generator for SAM, subsequently leveraging SAM's capabilities for accurate tool wear segmentation. An evaluation of three PoI generation methods revealed RCOGA as the optimal choice. Further analyses, benchmarked against U-Net's performance, evaluated the impacts of variations in training dataset sizes and U-Net training intensities on the final segmentation outcomes. The results show that our approach consistently outperformed U-Net, particularly when working with smaller training datasets and at initial stages of U-Net training. These results underscore our approach's capability to deliver accurate tool wear segmentations even with limited training datasets, indicating its potential for swift integration into industrial practices with limited datasets.\nIt is imperative to emphasize that our work represents a preliminary attempt to employ SAM for tool wear segmentation. The use of U-Net for preliminary wear segmentation demonstrates one of many potential approaches, with alternatives including manual prompts or object detection models that may better meet the zero-shot requirements of industrial scenarios. Moreover, to optimize SAM's zero-shot performance specifically for tool wear segmentation, SAM itself needs to be retrained within this domain. Therefore, future research will encompass assessing model performance with expansive datasets and exploring the potential for retraining and transferring SAM to enhance its accuracy and robustness specifically for the tool wear segmentation domain."}]}