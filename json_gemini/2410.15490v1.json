{"title": "Dynamic Intelligence Assessment:\nBenchmarking LLMs on the Road to AGI with a\nFocus on Model Confidence", "authors": ["Norbert Tihanyi", "Tamas Bisztray", "Richard A. Dubniczky", "Rebeka Toth", "Bertalan Borsos", "Bilel Cherif", "Mohamed Amine Ferrag", "Lajos Muzsai", "Ridhi Jain", "Ryan Marinelli", "Lucas C. Cordeiro", "Merouane Debbah"], "abstract": "As machine intelligence evolves, the need to test\nand compare the problem-solving abilities of different AI models\ngrows. However, current benchmarks are often overly simplistic,\nallowing models to perform uniformly well and, making it dif-\nficult to distinguish their capabilities. Additionally, benchmarks\ntypically rely on static question-answer pairs, which models might\nmemorize or guess. To address these limitations, we introduce the\nDynamic Intelligence Assessment (DIA), a novel methodology\nfor testing AI models using dynamic question templates and\nimproved metrics across multiple disciplines such as mathe-\nmatics, cryptography, cybersecurity, and computer science. The\naccompanying DIA-Bench dataset, which includes 150 diverse\nand challenging task templates with mutable parameters, is\npresented in various formats (text, PDFs, compiled binaries, and\nvisual puzzles). Our framework introduces four new metrics\nto assess a model's reliability and confidence across multiple\nattempts. These metrics revealed that even simple questions\nare frequently answered incorrectly when posed in varying\nforms, highlighting significant gaps in models' reliability. Notably,\nmodels like GPT-40 tended to overestimate their mathematical\nabilities, while ChatGPT-40 demonstrated better decision-making\nand performance through effective tool usage. We evaluated eight\nstate-of-the-art LLMs using DIA-Bench, showing that current\nmodels struggle with complex tasks and often display unex-\npectedly low confidence, even with simpler questions. The DIA\nframework sets a new standard for assessing not only problem-\nsolving but also a model's adaptive intelligence and ability to\nassess its own limitations. The dataset is publicly available on\nour project's website. https://github.com/DIA-Bench", "sections": [{"title": "I. INTRODUCTION", "content": "The origins of machine intelligence can be traced back\nto the 1950s, which simultaneously marked the need for\nbenchmarks to evaluate its progress. The first benchmark\nwas the famous Turing Test, introduced by Alan Turing in\n1950 [1]. In 1997, IBM's Deep Blue [2], [3] defeated Garry\nKasparov in a chess match, marking a groundbreaking moment\nin AI, where the benchmark itself was a human expert. This\nvictory demonstrated the potential of AI in surpassing human\ncapabilities in complex intellectual tasks.\nThe introduction of neural networks and transformer-based\narchitectures [4]\u2013[6] enabled the development of powerful\ngeneral-purpose models like BERT [7] and GPT-3 [8], which\nsignificantly advanced NLP capabilities. These models ex-\ncelled across various tasks measured by benchmarks such as\nGLUE [9], SQUAD [10], and HumanEval [11]. However, these\nare static benchmarks, namely there is only one variant of each\nquestion and results could be memorized. Moreover, as noted\nby Wang et al. [12], many older benchmarks are no longer\nchallenging enough to distinguish between newer models,\nas they often achieve near-perfect scores. For coding tasks,\nHonarvar et al. [13] explored the use of question templates to\nmove beyond evaluating LLMs on separate, isolated problems.\nApple recently extended the popular GSM8K dataset into a\ndynamic template-based benchmark [14]. While this allows\nfor generating multiple variants of the same problems, the\ndataset remains limited to grade-school math and simple\narithmetic tasks. Their findings show that LLMs struggle with\neven minor variations in these questions, relying more on"}, {"title": "II. RELATED LITERATURE", "content": "pattern matching than true logical reasoning. In contrast, our\nwork spans multiple disciplines and data formats, including\ncryptography, cybersecurity, and computer science. We move\nbeyond accuracy-based metrics by introducing novel measures\nfor assessing model reliability and confidence across repeated\nindependent attempts, providing a more thorough and com-\nprehensive evaluation of problem-solving consistency across\ndiverse, real-world challenges.\nExpanding on these, we propose Dynamic Intelligence\nAssessment (DIA), a testing framework that spans multiple\ndisciplines, including cryptography, cybersecurity, and com-\nputer science. DIA moves beyond accuracy-based metrics by\nintroducing novel measures for assessing model reliability and\nconfidence across multiple independent instances generated\nfrom dynamic question templates. This approach allows the\nevaluation of LLMs consistency in problem-solving, under\ndiverse, real-world challenges, offering a more comprehensive\nassessment without relying on in-context examples.\n\u2022\n\u2022 RQ1: What metrics can best evaluate a model's confi-\ndence and reliability in problem-solving?\nRQ2: How can a benchmark dataset better facilitate the\naccurate measurement of LLMs' confidence and reliabil-\nity in problem-solving?\n\u2022 RQ3: In problem-solving, does the use of tools impact\nthe confidence and reliability of different models?\nThe main contributions can be summarized as follows:\n1) We introduce four key metrics for assessing performance\non dynamic question templates. The Reliability Score\nmeasures a model's performance on the dataset by\npenalizing incorrect answers. The Task Success Rate\nevaluates the consistency of correct answers for a given\nquestion template. The Confidence Index captures the\npercentage of question templates where all k versions\nare correctly answered, and finally, the Near Miss Score\ncounts the number of templates where 80% < Task\nSuccess Rate< 100% for a given k.\n2) We introduce the Dynamic Intelligence Assessment\n(DIA) framework, to provide a comprehensive assess-\nment of AI models' reliability and confidence. This is\ndone by novel evaluation metrics combined with DIA-\nBench, a dataset of 150 diverse, hand-crafted dynamic\nquestion templates with varying difficulty.\n3) We tested 8 popular state-of-the-art LLMs and ranked\ntheir problem-solving reliability and confidence, pro-\nviding insights on which type of tasks are particularly\nchallenging for current models.\nIn summary, our work redefines how Al models are as-\nsessed for reliability, by introducing both a dynamic testing\nmethodology with four novel metrics, shifting the focus from\none-off success to consistent, reliable and confident problem-\nsolving. The paper is organized as follows: Section III details\nthe methodology and dataset creation, Section IV details the\nexperimental setup and results, Section II overviews related\nliterature, Section V discusses limitations and ethical consid-\nerations, while Section VI concludes the paper."}, {"title": "A. Metrics for Benchmarking LLMs", "content": "There is a wide range of metrics used to evaluate LLMs,\neach suited to different tasks. Text quality metrics such as\nBLEU [15], ROUGE [16], and BERTScore [17] are essential\nfor evaluating machine translation and summarization but are\nless relevant to our focus. F1 score measures the average\noverlap between the prediction and ground truth answer. For\ncode generation, Pass@k [11] is a key metric for the same task,\nwhether at least one out of k generated solutions is correct.\nAccuracy@k [8] measures the percentage of queries (from\nthe entire test set) for which at least one relevant result was\nfound within the top k results. Exact Match [10] demands an\nexact match with the expected output, making it particularly\nuseful in tasks like question answering.\nRobustness metrics evaluate a model's performance under\nvarious challenging conditions [18], with Adversarial Robust-\nness measuring the model's resilience to attacks, such as han-\ndling unusual or malicious inputs [19]. Out-of-distribution ro-\nbustness [19] measures the model's performance on inputs sig-\nnificantly different from the training data, while False Refusal\nRate (FRR) measures falsely rejecting benign prompts [20].\nThe ReCode framework [21] tests a model's ability to main-\ntain functionality under minor input changes. The Correctness\nscore is the closest metric to our proposal, which measures\nhow well an LLM answers a set of similar questions [13]."}, {"title": "B. Benchmark Datasets", "content": "Benchmarking LLMs has become an essential area of\nresearch across various domains, including question answer-\ning [22], code generation [23], fault localization, program\nrepair [24], and robustness evaluation. The following is not an\nexhaustive list, but focuses on datasets used by industry [25],\nor relevant for our case.\n1) Question Answering Benchmarks: LLMs have been\nwidely evaluated using question-answering (QA) benchmarks.\nSQUAD [10] focuses on comprehension-based QA with met-\nrics like Exact Match (EM) and F1. CoQA [26] evalu-\nates conversational and reading comprehension models. Hot-\npotQA [27] tests multi-hop reasoning across multiple docu-\nments. The MINT dataset [28] benchmarks multi-round user\ninteractions with LLMs.\nCyberMetric [22] uses 10,000 multiple-choice questions to\nevaluate the cybersecurity knowledge of LLMs and humans.\nMMLU [29], [30] and MMLU-Pro [31] assess knowledge\nand problem-solving abilities across STEM, humanities, and\nsocial sciences. MMMU [32] benchmarks multimodal reason-\ning across 11.5K questions, while MMMU-Pro adds vision-\nonly input. Lastly, DocVQA [33] focuses on visual question\nanswering for document images.\nThese are just a few examples, but in general, such tests\nsuffer from the fact, that newer LLMs are getting better in\nlexical knowledge and thus these datasets while useful more\nmeasuring certain domain specific knowledge, no longer allow\ndirect comparison between LLMs, if they all simply ace the\ntest."}, {"title": "III. METHODOLOGY", "content": "First, we introduce the four evaluation metrics, followed by\na discussion of the dataset creation process.\nLet t denote a question template with mutable parameters,\nwhere $T = {t_1, t_2,..., t_n}$ represents the set of such tem-\nplates. For each $t \u2208 T$, the degree of freedom, denoted by $d(t)$,\nis the number of distinct questions that can be generated from\na template. Furthermore, let $Q(T,k) = {q_1,q_2,...,q_{n\u00d7k}}$\ndenote the set of unique questions, where each template from\nTis used to generate k different questions. In practice, we\nassume that $k < d(t)$. Let $S_q = {S_1, S_2,..., S_{n\u00d7k}}$ represent\nthe set of solutions corresponding to Q. To be more precise,\nthere exists a mapping $f : Q(T, k) \u2192 S_q$ such that $f (qi) = si$\nfor all $i \u2208 {1,2,..., n x k}$.\nThe Reliability Score measures the model's performance\nacross the dataset, with incorrect answers being heavily pe-\nnalized, as shown in Formula 2. This approach is particularly\nuseful in critical applications, where awareness of the tendency\nof hallucinations or incorrect responses is essential. By nor-\nmalizing the score only by k, rather than by k \u00d7 |T|, we can\ncompare different instances of Q, even when varying numbers\nof question instances are generated per template. Additionally,\nthis approach still allows wrong answers to accumulate a\nsignificant negative score, providing a clear measure for human\nassessment about the model's reliability over T.\nDefinition 1 (Reliability Score): The Reliability Score over\na dataset $Q(T, k)$ is calculated as:\n$\\frac{1}{k} \\sum_{i=1}^{nxk} A_i$\nwhere $A_i$ is the score associated with answering $q_i$, and is\ndefined as:\n$A_i =\\begin{cases}\n+1 & \\text{if } s_i \\text{ is returned for } q_i, \\\\\n0 & \\text{if } q_i \\text{ is skipped}, \\\\\n-2 & \\text{otherwise}.\n\\end{cases}$"}, {"title": "Definition 2 (Task Success Rate):", "content": "For a given question\ntemplate $t_i$ with k generated versions, Task Success Rate\nmeasures the number of correctly solved instances, where\n$i\u2208 {1,2,..., n}$.\n$TSR@(t_i, k) = \\sum_{j=1}^{k} B_j$\nwhere the value of $B_j$ is defined as:\n$B_j =\\begin{cases}\n+1 & \\text{if } s_j \\text{ is returned for } q_j, \\\\\n0 & \\text{if } q_j \\text{ is skipped or answered incorrectly}.\n\\end{cases}$\nHence, [0 < TSR@(t,k) \u2264 k] follows, where TSR@\nmeasures the total number of correct answers out of the k\ngenerated instances of template t."}, {"title": "Definition 3 (Confidence Index):", "content": "represents the percentage\nof question templates in a dataset where, for a given template\n$t_i$, all k generated queries are successfully answered,\n$Conf@(k) = \\frac{100}{n} \\sum_{i=1}^{n} \\begin{cases}\n1 & \\text{if } TSR@(t_i, k) = k, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nfrom which [0% < Conf@(k) \u2264 100%] follows.\nThis metric is particularly useful in critical applications, such\nas critical infrastructure or autonomous vehicles, where flaw-\nless task execution is essential. Ask increases, the metric\nprovides stronger reassurance of the model's reliability.\nA key statistic to consider is when a model solves most\nqueries from a question template but misses a few."}, {"title": "Definition 4 (Near Miss Score):", "content": ": counts the number of\nquestion templates where the model answers at least 80% but\nless than 100% of the instances correctly out of k.\n$NMI@(k) = \\frac{1}{n} \\sum_{i=1}^{n} \\begin{cases}\n1 & \\text{if } 0.8k < TSR@(t_i, k) <k, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nWhen we create multiple variations of the same core prob-\nlem with slight adjustments, we assess the model's ability to\nconsistently solve these variations. We refer to this as Local\nFuzzing, where task parameters are systematically modified to\nevaluate the robustness of the AI model."}, {"title": "Definition 5 (Local Task Fuzzing):", "content": "refers to the systematic\nprocess of generating a set of k instances for t \u2208 T, by\nvarying the mutable parameters such that k < d(t).\nWhile the four metrics were designed for evaluating dy-\nnamic datasets, they can be applied to \"traditional\" bench-\nmarks with static question templates, where the same question\nis presented to an AI agent k times. This can be particularly\ninteresting under varying temperature settings.\nDIA-Bench presents multi-modal challenges involving both\nvisual and textual data, requiring models to process diverse\nformats such as PDFs and encoded files. It covers tasks in-\ncluding CAPTCHA solving, web security, reverse engineering,\ncryptography, mathematics, and logic. The multi-modal nature\nalso stems from the fact that many questions demand specific\ntools like Python interpreters or Linux command-line utilities."}, {"title": "IV. DISCUSSION", "content": "We evaluated eight models: GPT-40, Gemini-1.5-pro,\nGemini-1.5-flash, Llama 3.1, qwen2.5, Mistral 7B, and yi,\nalong with one chat model equipped with tool-using capabili-\nties, ChatGPT-40. Notably, existing literature often overlooks\nwhether the LLMs under examination possess tool-usage ca-\npabilities, and many benchmarks fail to specify whether the\nchat or API version of a model is employed.\nIn this section, we focus primarily on comparing the Ope-\nnAI models. First, because they are the top performers, and\nsecond, one can use tools while the other cannot, offering\nthe most compelling case for comparison. While the initial\nassumption might be that ChatGPT-40 performs better solely\nbecause of its tool-using capabilities, a more nuanced analysis\nsuggests that the gap between the two models is not purely a\nfunction of tool availability."}, {"title": "A. An Enthusiasm for Mathematics", "content": "One of the most striking observations is GPT-40 API's\neagerness to engage with mathematical tasks, even though it\nconsistently fails to solve them correctly, as shown in Figure 6.\nThis behaviour could stem from the fact that GPT-40 API and\nChatGPT-40 share the same underlying architecture. In math-\nematics, GPT-40 consistently assumes that it has the capacity\nto solve the problems. This aligns with the observation made\nby Mirzadeh et al. [14], where large language models (LLMs)\noften rely on sophisticated pattern matching rather than true\nlogical reasoning.\nGPT-40 API's inability to skip tasks reflects a broader issue\nin its self-assessment. Despite prompts encouraging it to skip\nproblems it cannot solve, GPT-40 API rarely does so. This\nindicates a lack of meta-cognitive awareness in gauging its\nlimitations, particularly when tools are required to solve a task.\nIt continues to attempt math problems, even though it lacks\nthe necessary tool-using capabilities to succeed."}, {"title": "B. Cybersecurity vs. Mathematics: Different Approaches in\nSkipping", "content": "The contrast between the models' behavior on mathematical\ntasks versus cybersecurity tasks provides additional insights.\nGPT-40 API, which eagerly attempts mathematical problems,\nis far more reserved in cybersecurity tasks, skipping questions"}, {"title": "C. Limitations in Current Metrics", "content": "In the traditional setting, skipping is an inferior approach\nto hallucination or guessing. To underline this, we should\nexamine ChatGPT-40's performance using a traditional metric\nlike Pass@k. On the DIA-Bench, ChatGPT-40 achieved an\nimpressive 73.3% Pass@5, meaning it was able to provide a\ncorrect answer to 73.3% of the question templates when given\nfive attempts. At the same time it was only able to ace all\nfive instances for 38.67% of the templates. The same goes for\nGPT-40 API, with Pass@5=21.3% and Conf@5=10.6%. The\nsignificant gap between the Pass@k score and the Confidence\nIndex highlights the importance of reliability metrics.\nMoreover, if we would rely on exact match for k = 1, as\nis common with static datasets, at least a third of the tasks on\nDIA-Bench would have a minimum of 1 in 5 chance of being\nanswered correctly through trial and error. This artificially\nboosts the success rate but does not accurately reflect a model's\nreliability and confidence."}, {"title": "D. A lack of Confidence", "content": "ChatGPT-40 recorded a high near-miss score, achieving 4\nout of 5 on 14 templates. We anticipate that as k increases,\nthe near-miss score would rise, lowering the confidence in-\ndex. During early experimentation with prompt templates, we\nobserved that models often approached tasks with varying\nstrategies. For example, when tasked with adding two smaller\nnumbers, ChatGPT-40 might not immediately write and ex-\necute Python code-considering the task trivial-but instead\nproduce an answer close to the correct one. For more complex\ntasks, it demonstrated this behaviour less.\nChatGPT-40 completely skipped 9 out of 150 templates,\nbut inconsistently skipped at least one instance in 32 other\ntemplates, revealing gaps in its decision-making. This sug-\ngests that its self-assessment mechanism isn't fully reliable,\nas it sometimes engages with tasks it later recognizes as\ntoo difficult. Despite this, ChatGPT-40 outperforms its API\ncounterparts, which is expected since API models lack tools\nand code execution capabilities. Interestingly, both GPT-40\nand ChatGPT-40 exhibit similar \"taste\u201d when attempting task\ncategories, with GPT-40 taking on math problems enthusias-\ntically despite lacking tools. In cybersecurity tasks, GPT-40\nand ChatGPT-40 display a strong correlation in their skipping\npatterns-when GPT-40 skips a template, ChatGPT-40 often\ndoes the same for at least some instances of that template.\nOther API models rarely utilized the skip option and at-\ntempted to answer every question. This pattern, evident in\nTable I, underscores that most models are primarily engaged in\npattern matching and remain distant from achieving artificial\ngeneral intelligence (AGI). The results reveal a substantial gap\nin problem-solving performance between models with tool-\nusing capabilities and those without. LLMs without tools tend\nto hallucinate more often, failing to complete tasks accurately,\nwhich is reflected in their lower reliability scores."}, {"title": "V. LIMITATIONS AND THREATS TO VALIDITY", "content": "The accuracy and reliability of benchmarking outcomes\ndepend on the quality of the generator scripts, and despite\nthorough proof checking on our part, some errors or edge\ncases may persist. If any issues are found, please report them\nby opening a GitHub issue.\nCertain task templates might not be as well represented as\nothers, leading to an incomplete evaluation of the LLM's over-\nall performance. In our study, we focused on computer science\nand mathematics, omitting other disciplines to concentrate our\nresources. We aim to expand this, and the used data modalities.\nWe did not utilize ol-preview or other chat models due\nto their high time and cost requirements, which would have\nimpacted the feasibility of this work."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduce the Dynamic Intelligence Assess-\nment (DIA) framework along with the DIA-Bench dataset to\ntest the problem-solving reliability and confidence of large lan-\nguage models (LLMs) for mathematics and computer science\nquestions. We also identified four key metrics for evaluating\nLLMs reliability and confidence. Our work addressed three\nkey research questions:\n\u2022 RQ1: What metrics can best evaluate a model's confidence and\nreliability in problem-solving? Answer: Traditional metrics of-\nten provide a misleading picture of a model's true performance,\nas they don't account for consistency. We introduce four met-\nrics: the Reliability Score, which penalizes incorrect answers,\nthe Task Success Rate (TSR) for measuring consistency across\nrepeated instances, the Confidence Index to assess performance\nacross variations of the same task, and the Near Miss Score for\ntasks where the model almost but not fully succeeds.\n\u2022 RQ2: How can a benchmark dataset better facilitate the ac-\ncurate measurement of LLMs' confidence and reliability in\nproblem-solving?\nAnswer: By using hand-crafted dynamic question templates,\nhosting simpler tasks to very difficult ones across various\nformats (text, PDFs, compiled binaries, etc.). This, combined\nwith the use of the proposed evaluation metrics, offers a more\nrigorous comparison of models' abilities compared to static,\nnon-multimodal, or overly easy benchmarks.\n\u2022 RQ3: In problem-solving, does the use of tools impact the\nconfidence and reliability of different models?\nAnswer: Yes, the use of tools significantly enhances both\nthe confidence and reliability of models in problem-solving.\nTool-using models like ChatGPT-40 not only perform better\non complex tasks requiring precise computations, but they\nalso demonstrate more consistent and reliable results across\nvarious task types. Interestingly, there is a strong correlation\nbetween the GPT models in their tendency to attempt to solve\na task, where non-tool users are more prone to errors and\nhallucinations.\nOur findings demonstrate that while current LLMs have\nmade significant progress, there remain challenges in achiev-\ning consistent and reliable problem-solving, particularly for\nmodels without tool-using capabilities. DIA-Bench provides a\nvaluable resource for further improving LLM evaluation and\ndevelopment towards more adaptable and reliable Al systems."}]}