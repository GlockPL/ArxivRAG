{"title": "DETECTING TRAINING DATA OF LARGE LANGUAGE MODELS VIA EXPECTATION MAXIMIZATION", "authors": ["Gyuwan Kim", "Yang Li", "Evangelia Spiliopoulou", "Jie Ma", "Miguel Ballesteros", "William Yang Wang"], "abstract": "The widespread deployment of large language models (LLMs) has led to impres-sive advancements, yet information about their training data, a critical factor in their performance, remains undisclosed. Membership inference attacks (MIAs) aim to determine whether a specific instance was part of a target model's train-ing data. MIAs can offer insights into LLM outputs and help detect and address concerns such as data contamination and compliance with privacy and copyright standards. However, applying MIAs to LLMs presents unique challenges due to the massive scale of pre-training data and the ambiguous nature of member-ship. Additionally, creating appropriate benchmarks to evaluate MIA methods is not straightforward, as training and test data distributions are often unknown. In this paper, we introduce EM-MIA, a novel MIA method for LLMs that iteratively refines membership scores and prefix scores via an expectation-maximization al-gorithm, leveraging the duality that the estimates of these scores can be improved by each other. Membership scores and prefix scores assess how each instance is likely to be a member and discriminative as a prefix, respectively. Our method achieves state-of-the-art results on the WikiMIA dataset. To further evaluate EM-MIA, we present OLMOMIA, a benchmark built from OLMo resources, which allows us to control the difficulty of MIA tasks with varying degrees of overlap between training and test data distributions. We believe that EM-MIA serves as a robust MIA method for LLMs and that OLMOMIA provides a valuable resource for comprehensively evaluating MIA approaches, thereby driving future research in this critical area.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023b) have recently emerged as a groundbreaking development and have had a transformative impact in many fields. The vast and diverse training data are central to their success, which enables LLMs to understand and gener-ate languages to perform complex tasks. Given that training data directly shape LLMs' behaviors, knowing the composition of training data allows researchers and practitioners to assess the strengths and limitations of LLMs (Gebru et al., 2021), address ethical concerns, and mitigate potential bi-ases (Bender et al., 2021; Feng et al., 2023) and other risks (Bommasani et al., 2021). However, the exact composition of training data is often a secret ingredient of LLMs.\nSince LLMs are trained on large-scale corpora from diverse sources, including web content, the inclusion of undesirable data into training data such as test datasets (Sainz et al., 2023; Oren et al., 2023; Sainz et al., 2024), proprietary contents (Chang et al., 2023; Meeus et al., 2024c), or person-ally identifiable information (Mozes et al., 2023) might prevalently happen unconsciously, raising serious concerns when deploying LLMs. Membership inference attack (MIA) determines whether a particular data point has been seen during training a target model (Shokri et al., 2017). Using MIA"}, {"title": "BACKGROUND", "content": ""}, {"title": "MEMBERSHIP INFERENCE ATTACK FOR LLMS", "content": "Membership inference attack (MIA) (Shokri et al., 2017; Carlini et al., 2022) is a binary classifi-cation task that identifies whether or not a given data point has been seen during model training: member vs non-member. Given a target language model M trained on an unknown Dtrain, MIA predicts a membership label of each instance x in a test dataset Dtest whether x in Dtrain or not, by computing a membership score f(x; M) and thresholding this score. By adjusting a threshold, MIA can control the trade-off between the true positive rate and the false positive rate. The MIA performance is typically evaluated with two metrics: AUC-ROC and TPR@low FPR (Carlini et al., 2022; Mireshghallah et al., 2022).\nMost existing MIA methods are based on the assumption that the target model memorizes (or over-fits) training data. In general, members will have a lower loss (Yeom et al., 2018), which is the average log-likelihood (or perplexity) of target text with respect to a target LLM, compared to non-members. Likelihood Ratio Attacks (LiRAs) (Ye et al., 2022) perform difficulty calibration using a reference model (Carlini et al., 2022), a compression method (Carlini et al., 2021), or the aver-"}, {"title": "WHY MIA FOR LLMS IS CHALLENGING", "content": "Although MIA has been studied in machine learning for several years, it remains especially chal-lenging for LLMs (Duan et al., 2024; Meeus et al., 2024b). Due to the massive training dataset size, each instance is used in training only a few times, often just once (Lee et al., 2021), making it difficult to leave a footprint on the model. Moreover, there is inherent ambiguity in the defini-tion of membership because texts are often repeated and partially overlap each other in the original form or with a minor difference even after the rigorous preprocessing of decontamination and dedu-plication (Kandpal et al., 2022; Tirumala et al., 2024). The membership boundary becomes even fuzzier (Shilov et al., 2024) if semantically similar paraphrases (Mattern et al., 2023; Mozaffari & Marathe, 2024) beyond lexical matching based on n-gram are considered. Traditional MIA ap-proaches in machine learning literature (Shokri et al., 2017; Ye et al., 2022; Carlini et al., 2022) based on training shadow models on non-overlapping data from the same data distribution, model architecture, and training algorithm as the target model are infeasible for LLMs considering high computational costs and unknown training specifications."}, {"title": "EVALUATION OF MIAS FOR LLMS", "content": "Common MIA benchmarks such as WikiMIA (Shi et al., 2023) use a time cutoff based on model re-lease dates and time information of documents (Shi et al., 2023; Meeus et al., 2024a) to ensure some-what membership labels. Besides, recent studies (Duan et al., 2024; Das et al., 2024; Meeus et al., 2024b) question whether several MIAs that perform well on these benchmarks are truly conducting membership inference, as detecting distributional (often temporal) shifts alone may be sufficient to achieve high benchmark performance. Thus, they advocate for the use of datasets with a random train-test split such as MIMIR (Duan et al., 2024), which is derived from the PILE dataset (Gao et al., 2020), for MIA evaluation. However, none of the existing methods significantly outperforms random guessing in this setting. Although this setting seems theoretically appropriate for evaluating MIA, there is no truly held-out in-distribution dataset in reality because LLMs are usually trained with all available data sources. In other words, it is difficult to find in-distribution non-member examples, and it is nearly impossible to completely eliminate the distribution shift between training and test data at inference time.\nOnce an MIA benchmark, a pair of models and datasets, is released, it becomes difficult to prevent MIAs from exploiting information about the benchmark settings, such as how the dataset has been constructed. MIAs may exploit existing biases on datasets regardless of their intended purpose. To evaluate the true generalizability of MIAs in real-world scenarios, we should consider situations where we have minimal knowledge about the target language model, training data, and test data. Several ongoing attempts (Meeus et al., 2024b; Eichler et al., 2024) aim to reproduce setups that closely resemble practical MIA scenarios, but none are sufficiently effective to gain widespread adoption in the community. Our work is an additional effort in this direction. We propose a robust MIA method and evaluate it on our benchmark, which simulates various combinations of training and test data distribution."}, {"title": "RECALL: ASSUMPTIONS AND LIMITATIONS", "content": "ReCaLL (Xie et al., 2024) uses the ratio between the conditional log-likelihood of a target data point x given a non-member prefix p as a context and the unconditional log-likelihood of x by an LLM M as a membership score, based on the observation that the distribution of ReCaLL scores for members and non-members diverges when p is a non-member prefix: formally, $ReCaLL_p(x; M) =$"}, {"title": "OBSERVATION: FINDING A BETTER PREFIX", "content": "In this section, we rigorously investigate how much ReCaLL's performance is sensitive to the choice of a prefix and particularly how much it can be compromised without given non-member data. We define a prefix score r(p) as the effectiveness of p as a prefix in discriminating memberships, particularly when using this prefix for ReCaLL. In the Oracle setting where ground truth labels of all examples in a test dataset Dtest are available, we can calculate a prefix score by measuring the performance of ReCaLL with a prefix p on a test dataset Dtest using ground truth labels and a MIA evaluation metric such as AUC-ROC. While a prefix could be any text, we calculate prefix scores for all examples in a test dataset Dtest by using each data point as a standalone prefix.\nAs an initial analysis, we conduct experiments using the WikiMIA (Shi et al., 2023) dataset with a length of 128 as a target dataset and Pythia-6.9B (Biderman et al., 2023) as a target LM. displays the distribution of prefix scores measured by AUC-ROC for members and non-members. Consistent with the results from Xie et al. (2024), ReCaLL works well if a prefix is a non-member and does not work well if a prefix is a member. Prefix scores of members are smaller than 0.7, and most of them are close to 0.5, which is the score of random guessing. Prefix scores of non-members are larger than 0.5, and most of them are larger than 0.7. This clear distinguishability suggests using a negative prefix score as a membership score.\nWithout access to non-members (or data points with high prefix scores), ReCaLL's performance could be significantly lower. Given the wide spectrum of prefix scores for even non-members, the effectiveness of each data point varies, and the choice of data points for a prefix can be crucial,"}, {"title": "PROPOSED METHOD: EM-MIA", "content": "We target the realistic MIA scenario where test data labels are unavailable. We measure a prefix score by how ReCaLLp on a test dataset Dtest aligns well with the current estimates of membership scores f on Dtest denoted as S(ReCaLLp, f, Dtest). More accurate membership scores can help compute more accurate prefix scores. Conversely, more accurate prefix scores can help compute more accurate membership scores. Based on this duality, we propose an iterative algorithm to refine membership scores and prefix scores via an Expectation-Maximization algorithm, called EM-MIA, to perform MIA with minimal assumptions on test data (\u00a72.4).\nAlgorithm 1 summarizes the overall procedure of EM-MIA. We begin with an initial assignment of membership scores using any existing off-the-shelf MIA method such as Loss (Yeom et al., 2018) or Min-K%++ (Zhang et al., 2024) (Line 1). We calculate prefix scores r(p) using membership scores and then update membership scores f(x) using prefix scores. The update rule of prefix scores (Line 3) and membership scores (Line 4) is a design choice. We repeat this process iteratively until con-vergence (Line 5). Since EM-MIA is a general framework, all components, including initialization, score update rules, and stopping criteria, are subject to modification for further improvement.\nUpdate Rule for Prefix Scores Our observation in \u00a73 shows that AUC-ROC is an effective func-tion S to calculate prefix scores given ground truth labels. Because we do not have labels, we can assign membership labels using the current membership scores f and a threshold \u03c4 to use them as ap-proximate labels to calculate prefix scores: AUC-ROC({(ReCaLLp(x), d(f(x) > \u03c4)|x \u2208 Dtest)}). The value of t could be chosen as a specific percentile in the score distributions to decide the por-tion of members and non-members. Using a median (50% percentile) is a simple and practical choice because a test dataset is usually balanced. Instead of approximating hard labels, we can com-pare the ranks of ReCaLLp(x) and the ranks of f(x) on Dtest because the relative order among other data matters rather than the absolute values. We can use the average difference in ranks as"}, {"title": "NEW BENCHMARK: OLMOMIA", "content": "EM-MIA works well and even performs almost perfectly on some benchmarks such as WikiMIA (Shi et al., 2023) (\u00a77.1), while it does not work well on other datasets such as MIMIR (Duan et al., 2024) similar to other methods. We want to know why this is the case and what are the conditions for success. To answer these questions, we develop a new benchmark using OLMO (Groeneveld et al., 2024), which is a series of fully open language models pre-trained with Dolma (Soldaini et al., 2024) dataset. OLMo provides intermediate model checkpoints and an index to get which data has been used for each training step, which are valuable resources to construct an MIA benchmark. We will share our implementation and resulting benchmark datasets. Like us, anyone can create their own benchmark on their purpose by modifying our implementation.\nBased on clustering, we control the degree of overlap between the distribution of members and non-members, resulting in different difficulties. First, we sample enough numbers of members and"}, {"title": "EXPERIMENTAL SETUP", "content": "Our implementation and datasets to reproduce our experiments are available at https://github.com/gyuwankim/em-mia."}, {"title": "DATASETS AND MODELS", "content": "We evaluate EM-MIA and compare it with baselines (\u00a76.2) on the WikiMIA benchmark (Shi et al., 2023) (\u00a77.1) and the OLMOMIA benchmark (\u00a77.2) using AUC-ROC as an MIA evaluation metric. For WikiMIA, we use the original setting with length splits of 32, 64, and 128 as test datasets and use Mamba 1.4B (Gu & Dao, 2023), Pythia 6.9B (Biderman et al., 2023), GPT-NeoX 20B (Black et al., 2022), LLaMA 13B/30B (Touvron et al., 2023a), and OPT 66B (Zhang et al., 2022) as target models, following Xie et al. (2024) and Zhang et al. (2024). For OLMOMIA (\u00a75), we use six settings of Easy, Medium, Hard, Random, Mix-1, and Mix-2 as test datasets and use checkpoints after 100k, 200k, 300k, and 400k training steps as target models. Although EM-MIA requires a baseline sufficiently better than random guessing as an initialization, there is currently no such method for MIMIR (Duan et al., 2024). Therefore, we skip experiments on MIMIR, though this is one of the widely used benchmarks on MIA for LLMs."}, {"title": "BASELINES", "content": "We compare our method against the following baselines explained in \u00a72.1: Loss (Yeom et al., 2018), Ref (Carlini et al., 2022), Zlib (Carlini et al., 2021), Min-K% (Shi et al., 2023), and Min-K%++ (Zhang et al., 2024). We use Pythia-70m for WikiMIA and StableLM-Base-Alpha-3B-v2 model (Tow, 2023) for OLMOMIA as the reference model of the Ref method, following Shi et al. (2023) and Duan et al. (2024). For Min-K% and Min-K%++, we set K = 20. Among the com-monly used baselines, we omit Neighbor (Mattern et al., 2023) because it is not the best in most cases though it requires LLM inference multiple times for neighborhood texts, so it is much more expensive."}, {"title": "RECALL-BASED BASELINES", "content": "As explained in \u00a72.4, the original ReCaLL (Xie et al., 2024) uses labeled data from the test dataset, which is unfair to compare with the above baselines and ours. More precisely, pi in the prefix p = P1 P2 Pn are known non-members from the test set Dtest, and they are excluded from the test dataset for evaluation, i.e., Dtest' = Dtest \\{P1,P2,\u2026\u2026,pn}. However, we measure the performance of ReCaLL with different prefix selection methods to understand how ReCaLL is sensitive to the prefix choice and use it as a reference instead of a direct fair comparison.\nSince changing the test dataset every time for different prefixes does not make sense and makes the comparison even more complicated, we keep them in the test dataset. A language model tends to repeat, so LL(pi|p; M) ~ 0. Because LL(pi|p; M) < 0, ReCaLLp(pi; M) ~ 0. It is likely to ReCaLLp(pi; M) \u00ab ReCaLLp(x; M) for x \u2208 Dtest \\{P1,P2,\u2026, pn}, meaning that ReCaLL will classify pi as a non-member. The effect would be marginal if |Dtest \u226bn. Otherwise, we should consider this when we read numbers in the result table.\nWe have four options for choosing pi: Rand, RandM, RandNM, and TopPref. Rand randomly se-lects any data from Dtest. RandM randomly selects member data from Dtest. RandNM randomly selects non-member data from Dtest. TopPref selects data from Dtest with the highest prefix scores calculated with ground truth labels the same as \u00a73. All options except Rand partially or entirely use labels in the test dataset. For all methods using a random selection (Rand, RandM, and, RandNM), we execute five times with different random seeds and report the average.\nThe original ReCaLL (Xie et al., 2024) is similar to RandNM, except they report the best score after trying all different n values, which is again unfair. The number of shots n is an important hyper-parameter determining performance. A larger n generally leads to a better MIA performance but increases computational cost with a longer p. We fix n = 12 since it provides a reasonable performance while not too expensive.\nOther simple baselines without using any labels are Avg and AvgP, which average ReCaLL scores over all data points in Dtest: Avg(x) =$"}, {"title": "EM-\u039c\u0399\u0391", "content": "As explained in \u00a74, EM-MIA is a general framework where each component can be tuned for im-provement, but we use the following options as defaults based on results from preliminary experi-ments. Overall, Min-K%++ performs best among baselines without ReCaLL-based approaches, so we use it as a default choice for initialization. Alternatively, we may use ReCaLL-based methods that do not rely on any labels like Avg, AvgP, or Rand. For the update rule for prefix scores, we use AUC-ROC as a default scoring function S. For the update rule for membership scores, we use negative prefix scores as new membership scores. For the stopping criterion, we repeat ten iterations and stop without thresholding by the score difference since we observed that membership scores and prefix scores converge quickly after a few iterations. We also observed that EM-MIA is not sensi-tive to the choice of the initialization method and the scoring function S and converges to similar results."}, {"title": "RESULTS AND DISCUSSION", "content": ""}, {"title": "WIKI\u039c\u0399\u0391", "content": "Table 1 shows the experimental results on the WikiMIA dataset. EM-MIA achieves state-of-the-art performance on WikiMIA for all different models and length splits, significantly outperforming all baselines, including ReCaLL, even without any given non-member test data. EM-MIA exceeds 96% AUC-ROC in all cases. For the largest model OPT-66B, EM-MIA gets 99% AUC-ROC for length splits 32 and 64, while ReCaLL's performance is lower than 86% AUC-ROC."}, {"title": "OLM\u039f\u039c\u0399\u0391", "content": "Table 2 shows the experimental results on the OLMOMIA benchmark.2 EM-MIA achieves almost perfect scores on Easy and Medium similar to WikiMIA, gets performance comparable to random guessing performance on Hard and Random similar to MIMIR, and gets reasonably good scores on Mix-1 and Mix-2, though not perfect as on Easy and Medium. EM-MIA significantly outperforms all baselines in all settings except Hard and Random, where distributions of members and non-members heavily overlap to each other and all methods are not sufficiently better than random guessing."}, {"title": "CONCLUSION", "content": "We introduce a novel MIA method for LLMs called EM-MIA that iteratively updates membership scores and prefix scores via an Expectation-Maximization algorithm for better membership inference based on the observation of their duality. EM-MIA significantly outperforms ReCaLL even without strong assumptions that ReCaLL relies on and achieves state-of-the-art performance on WikiMIA. EM-MIA is easily tunable with several design choices, including initialization, score update rules, and stopping criteria, allowing the application to MIA in different conditions and providing room for further improvement. We create a new benchmark for detecting pre-training data of LLMs named OLMOMIA to better understand the conditions under which EM-MIA works with a comprehensive evaluation. It turns out that EM-MIA robustly performs well for all settings except when the distri-butions of members and non-members are almost identical, resulting in none of the existing methods being better than random guessing.\nWhile our paper focuses on detecting pre-training data with the gray-box access of LLMs where computing the probability of a text from output logits is possible, many proprietary LLMs are usu-ally further fine-tuned (Ouyang et al., 2022), and they only provide generation outputs, which is the black-box setting. We left the extension of our approach to MIAs for fine-tuned LLMs (Song & Shmatikov, 2019; Jagannatha et al., 2021; Mahloujifar et al., 2021; Shejwalkar et al., 2021; Mireshghallah et al., 2022; Tu et al., 2024) or for LLMs with black-box access (Dong et al., 2024; Zhou et al., 2024; Kaneko et al., 2024) as future work."}]}