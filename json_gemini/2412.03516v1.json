{"title": "You're (Not) My Type - Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?", "authors": ["Dominic Lohr", "Hieke Keuning", "Natalie Kiesler"], "abstract": "Background: Feedback as one of the most influential factors for learning has been subject to a great body of research. It plays a key role in the development of educational technology systems and is traditionally rooted in deterministic feedback defined by experts and their experience. However, with the rise of generative Al and especially Large Language Models (LLMs), we expect feedback as part of learning systems to transform, especially for the context of programming. In the past, it was challenging to automate feedback for learners of programming. LLMs may create new possibilities to provide richer, and more individual feedback than ever before.\nObjectives: This paper aims to generate specific types of feedback for introductory programming tasks using LLMs. We revisit existing feedback taxonomies to capture the specifics of the generated feedback, such as randomness, uncertainty, and degrees of variation.\nMethods: We iteratively designed prompts for the generation of specific feedback types (as part of existing feedback taxonomies) in response to authentic student programs. We then evaluated the generated output and determined to what extent it reflected certain feedback types.\nResults and Conclusion: The present work provides a better understanding of different feedback dimensions and characteristics. The results have implications for future feedback", "sections": [{"title": "1 | INTRODUCTION", "content": "Learning to program involves much more than just understanding the syntax and semantics of a programming language.\nIt is therefore not surprising that it has been characterized as a challenging process [1, 2, 3, 4]. Even experienced\nprogrammers regularly encounter errors in their code a testament to the inherent complexity of programming. Due\nto experience and years of practice, programming experts have an arsenal of debugging techniques at their hands.\nNovice programmers, however, lack this experience, causing them to be more dependent on external support, e.g.,\nfrom educators, peers, or tools. This is particularly true at the very beginning of the learning process.\nFeedback - as a means of external support - is one of the most influencing factors of learning success [5]. In\nthe context of programming education, elaborated feedback has not yet been implemented at scale, for example, as\na feature of learning environments [6]. Common systems (e.g., Codewars, Codecademy, Python Tutor, W3Schools)\noften tend to provide simple feedback indicating the correctness of the code or pointing out errors without addressing\nthe underlying causes or how to fix these errors [6]. In contrast, adaptive feedback addressing the causes of errors\nand not just their manifestation in the code is considered more valuable by learners [7].\nThe recent emergence of advanced Natural Language Processing (NLP) techniques, generative Al (GenAI), and\nparticularly the broad availability of Large Language Models (LLMs), offers new opportunities for improving and\nelaborating feedback in programming education. Prior research has started to investigate the potential of LLMs\nto generate, for example, code explanations, enhanced error messages, and thus formative programming feedback\n[8, 9, 10, 11, 12, 13, 14]. However, these studies are usually based on simple help-requests as phrased by students.\nMoreover, the rich diversity of feedback types with regard to its content has not yet been addressed.\nIn this paper, we explore to what extent the feedback generated by LLMs can be controlled and how its quality can\nbe assessed. Our experiments pursue the following goals: (1) design prompts to generate specific types of feedback\nfrom a specific feedback taxonomy for the context of programming (see [15]), and (2) evaluate the adequacy and\nquality of the generated feedback types. To address these goals, we used an iterative process to develop a prompt to\ngenerate different types of feedback. We used this prompt and a dataset of authentic student solutions (correct ones\nand with errors) as input, aiming to generate different types of feedback for these student solutions. The same dataset\nhas been used in related work [14] to generate feedback, but this related study did not apply \"prompt engineering\ntechniques\" - meaning the input was simple and the feedback generated by the LLM was characterized with regard\nto its content, quality, and other criteria.\nOur paper is structured as follows. In Section 2, we describe related work on the role of feedback in learning\nin general, and how feedback has been automated in the context of computing education. We also outline recent\nresearch on the emergence of generative Al in computing education and summarize studies employing GenAl for"}, {"title": "2 | BACKGROUND AND RELATED WORK", "content": "To contextualize the present work, we introduce the role of feedback as part of students' learning process before\nfocusing on feedback types for programming tasks and the recent role of generative Al in programming education."}, {"title": "2.1 The role of feedback in the learning process and the programming domain", "content": "Feedback has been the subject of extensive research in a great number of disciplines. According to a meta-analysis,\nit is one of the most influential factors for learning [5]. At the same time, it is crucial to consider how feedback is\ndelivered and presented [16]. In general, the goal of giving feedback is to reduce the gap between the student's\ncurrent performance and their desired goal [17], and to modify their thinking and behavior [18]. Research shows the\nimportance of giving detailed feedback, as opposed to simple messages only indicating whether a solution is correct\nor incorrect. Feedback may also be classified as summative (focusing on performance after submission to educators)\nor formative (focusing on the learning process during the execution of a task or a course). In the present work, we\nrefer to formative feedback provided to students trying to solve a task or assignment as part of their coursework.\nAccording to Hattie and Timperley [17], effective feedback should answer the questions 'where am I going' (feed\nup), 'how am I going' (feed back), and 'where to next' (feed forward). Each of these questions can be applied to four\nlevels: (1) task, (2) process, (3) self-regulation, and (4) self. Shute [18] recommends formative feedback to be non-\nevaluative, supportive, timely, and specific.\nThe literature distinguishes various dimensions for the design of feedback [16, 18]. Narciss [16] outlines function\n(cognitive, meta-cognitive, and motivational), presentation (timing, number of tries, adaptability, modality), and content\nof feedback. Each of these dimensions can influence the feedback's effects. Regarding content, Narciss classified feed-\nback components for digital learning environments based on which aspects of the instructional context the feedback\naddresses, such as task constraints, mistakes, and procedural knowledge.\nNarciss [19] summarizes feedback into simple and elaborated types. The latter provides more detailed information\nto the learner to analyze and improve their work. Keuning et al. [15] have applied and extended this categorization\nfor the programming domain."}, {"title": "2.2 | Using generative Al for feedback in programming education", "content": "In the past decades, many different techniques have been applied to automatically generate feedback with the goal\nof supporting educators and learners at scale (e.g., in Massive Open Online Courses) [27, 28, 15]. While automated\ntesting is still one of the most common ways to generate feedback, several data-driven methods have emerged. How-\never, with LLMs and GenAl tools becoming available to the public at the end of 2022, the possibility of generating a\ndiverse range of feedback has become a reality and a true option for both educators and learners.\nAccordingly, GenAl and LLMs have been extensively investigated within the last two years, especially in the con-\ntext of programming education [29, 30, 31]. Early papers focused on analyzing how well LLMs could solve well-known\n(introductory) programming problems [32, 33, 34, 35, 36]. Recently, researchers have shifted their focus towards in-\nvestigating the use of LLMs to enhance teaching and learning how to program, and their integration into the class-\nroom [37, 38, 39]. OpenAl's GPT-4 model's capabilities in identifying errors within programming code have been\nexplored [40], as well as its performance in (beginner and intermediate) Python course exams [41], answering various\ntypes of questions [42], or for automatic program repair and refactoring [43].\nAnother potential application of LLMs is the generation of code explanations. MacNeil et al. [11] showed that\nstudents perceive generated line-by-line code explanations as helpful. In another study [44], students rated the code\nexplanations of the LLM better on average than those created by students. Especially the level of understanding\nand accuracy were rated higher, and students did not show negative affection towards the generated, non-human\nfeedback.\nDue to GenAl tools and LLMs, it is thus possible to generate feedback for (novice) learners of programming.\nSeveral studies have investigated the feedback generated by LLMs. For example, Balse et al. [45] used final exam\nsubmissions to seven exercises as input to GPT-3, and evaluated the response on correctness and type of feedback.\nThey found that 71% of the feedback was accurately checked as correct, and around 62% contained correct critiques\nand suggestions. The authors identified four themes in the feedback that mostly target the constructs on which\nfeedback was given: 'generic feedback', 'variable initialization and updates', 'conditionals', and 'iteration'.\nPankiewicz and Baker [46] implemented GPT-3.5 feedback inside an existing automated assessment tool, tar-\ngeted at C# submissions to 38 exercises. The students rated the feedback on a Likert scale, and their performance,"}, {"title": "3 | METHOD", "content": ""}, {"title": "3.1 Research motivation", "content": "Related research had the goal to generate \"general\" feedback in response to a task, student solution, or the question\nof how to proceed. These studies concluded that LLM-generated feedback messages may contain misleading infor-\nmation, incorrect corrections, and lack of details [8, 12, 14, 47] in response to student programming submissions. At\nthe same time, LLM-generated feedback is personalized and can address several issues and aspects of an input (e.g.,\nconceptual knowledge, the task itself, mistakes, hints on how to improve, or even meta-cognitive strategies).\nTo date, learning environments for programming mostly provide a limited set of feedback types, and they hardly\ngive elaborated, let alone personalized feedback [6]. Therefore, it is the goal of the present work to investigate how\nand to what extent we can generate specific, elaborated feedback types by using a state-of-the-art LLM (e.g., GPT-4).\nIn addition, currently available programming feedback classifications like the one by Keuning et al. [15] were devel-\noped at a time when feedback generation options were limited and complex to automate. In the LLM era, generative\nAl may provide us with a richer array of possibilities, implying the need to revisit this taxonomy to showcase these\npossibilities."}, {"title": "3.2 | Scope and research questions", "content": "The present work focuses on giving feedback to students working on a single programming exercise while limiting the\nscope to the following aspects: (1) The student's program for which we want to provide feedback is (almost) complete.\nWe do not consider partial solutions from the early phases of the problem-solving process. (2) The goal of this work\nis to critically analyze the input provided to the LLM, and to identify suggestions on how to improve and expand\nthe input in an educational setting. This scenario differs from related work employing LLMs for code explanations,\nfault localization and repair for a professional setting, and enhancing existing compiler error messages. (3) We focus\non formative, elaborated feedback, but also investigate some simple elements such as KR as a component of more\nextended feedback. The following research questions guide our work:\nRQ 1 To what extent can feedback of a specific type be generated using LLMs for a given (faulty) program?\nRQ 2 What are the characteristics of the generated feedback?"}, {"title": "3.3 | (Re-)Use of Datasets", "content": ""}, {"title": "3.3.1 Dataset Used for Prompt Development", "content": "To create a complete and comprehensive prompt, we, first of all, made sure that the prompt we intended to use\nwould include all relevant information as input to the LLM. Therefore, we selected a dataset that encompassed all the\ninformation available about its setting and context. For example, the task description, a model solution, and templates\n(e.g., class skeleton) students may have received. Precisely, we used the dataset from a related study, published at\nthe 2023 Frontiers in Education (FIE) conference [14]. The respective data was gathered via weekly exercises in a\n(large) introductory programming course in Java. Three exercises were selected from this set (Physics, Triangles, and\nNegaFib in Table 3). For all but the NegaFib exercise a template with a class skeleton containing empty methods was\nprovided to the students. We made some changes to the Triangle task by merging its two subtasks into one task. This\nwas because we wanted to prompt the LLM to generate the complete solution, but also all additional information (e.g.,\nrelated programming concepts). This way, the use case was more similar to students solving the complete task as part\nof their coursework.\nAs an initial set for designing the prompt, we selected one student submission for each exercise that contained\nmultiple errors, such as syntax errors, semantic errors and logic errors (for an overview see Table 4)."}, {"title": "3.3.2 Dataset Used for Evaluation", "content": "To test the performance of the model with our prompt on a more diverse set of programming submissions, we ex-\npanded our dataset based on the following requirements for the exercises and corresponding solutions:\n\u2022 Exercises should cover various types of tasks.\n\u2022 Student solutions should be in different programming languages.\n\u2022 Student solutions should have diverse types of errors.\n\u2022 Student solutions should have different numbers of errors, ranging from zero to numerous.\n\u2022 Student solutions should be reasonably complete.\n\u2022 Some of the solutions should be correct to check if the generated feedback is also valuable for correct submissions.\nWe selected a total of 11 solutions to 6 different programming exercises from existing datasets that\nhad been used before in research [55, 56, 57]. A new exercise, MaxOf3, was taken from the TaskTracker dataset [58].\nThis dataset contains snapshots from 148 students solving 6 introductory programming tasks in 4 different program-\nming languages. The data was collected within a plugin for IntelliJ-based IDEs. We selected the MaxOf3 exercise\nbecause there were both correct and incorrect submissions for it in various programming languages. The exercise was\naccompanied by a set of test cases (input/output) and a task description. As there was no model solution provided,\nwe created one for each programming language."}, {"title": "3.4 | Prompt Design Process", "content": "Next, we outline the iterative process of evaluating the outcomes of each prompt and how we enhanced the instruc-\ntions provided within each prompt. Five iterations were needed before arriving at a prompt triggering an adequate\noutput as a basis for an in-depth analysis. We defined the following conditions as a starting point for the creation of\nthe prompt. Some of these are based on Denny et al.'s [59] guidelines for improving the readability of programming\nerror messages."}, {"title": "3.4.1 First Iteration", "content": "We designed a first prompt based on our initial conditions, using best practices described in documents and literature\n. We included the following elements:\n\u2022 A system prompt with instructions about the target audience, the elements given below, and an explanation of\nthe different feedback types.\n\u2022 The task description, translated from German into English.\n\u2022 The class skeleton given to the student.\n\u2022 The student's submitted solution.\n\u2022 A model solution.\n\u2022 The type of feedback we want the model to generate (KR, KP, KC, KTC, KM, or KH).\nWhen analyzing the output, we noticed several issues: First, the output was too long for the KR feedback (290\nwords on average), for which we only expected a simple correct/incorrect statement with no more than 5 words\n(e.g., \"The solution is incorrect\"). In multiple instances, the response contained further feedback types in addition to the\nrequested feedback type. Another critical issue was that the output referred to the model solution, and kept comparing\nthe input to the model solution. This is a common problem that has been observed in previous studies [47, 65].\nTo mitigate these issues, we added instructions to have only correct/incorrect as output for KR (see line 18 in\nListings 1), and to not provide any additional information than the desired feedback type (line 24). We also instructed\nthe model that the feedback should never contain information about the model solution (line 25)."}, {"title": "3.4.2 Second Iteration", "content": "We observed several improvements within the second iteration: the output was shorter, KR feedback only gave\ninformation about correct/incorrect, and it seemed that the feedback was more related to the requested feedback\ntypes. We did not see any mentions of the model solution anymore. In terms of issues, KTC responses sometimes just\nrepeated the task requirements. KH was in some cases not precise, only saying \"check if correct\". KC gives information\nabout concepts even if there is no issue in the code relating to that concept. KP feedback was in some cases incorrect,\nvague, or only KR feedback.\nWe made some substantial changes to our prompt. For KC we instructed the model to focus only on programming\nconcepts and explain these concepts only if the issue in the submitted solution of the student is related to that concept\n(see line 18 in Listings 1). For KP we explained that we want the model to try to divide the task into useful subgoals\nand give feedback about the current state of the student submission as a percentage (see line 19). For KH feedback,\nwe wanted the model to not only generate an instruction for the student to check if something is correct but also\nprovide hints and suggestions on how to proceed (see line 20). For KTC we instructed the model to only point to\ninformation in the task description if there is an issue in the student submission related to that task constraint (see\nline 21)."}, {"title": "3.4.3 | Third Iteration", "content": "In the output of the third iteration, we noticed that KM feedback also gave hints on how to proceed, which we do not\nwant. Moreover, the responses had become lengthier again, using verbose language which might not be appealing\nto novices. There were never any (code) examples or motivational words. In addition, the given type of feedback\nresembled our instruction. KP gave a lengthier description instead of just a percentage, which we decided to keep so\nit is clear what it is based on.\nIn our improvements, we simplified the definition supplied with KR about when a solution is considered correct to\nbe either \"(1) programming solution passes all tests and contains no mistakes (2) programming solution is semantically\nequivalent to the model solution.\" We also told the model that the feedback has to be brief and precise. For KM\nfeedback, we wanted to give the student information about the error and not show hints on how to fix it. We also\ntried to experiment with adding illustrating examples to better support students' understanding, which we requested\nfor KM, KTC, KC, and KH."}, {"title": "3.4.4 | Fourth Iteration", "content": "The results of our previous changes did not seem to significantly improve the output. Firstly, we noticed lengthy\nresponses. The model clearly tried to generate examples, however, we observed that the term 'example' can be used\nin many different ways. To illustrate, the model just repeated some code from the student with issues, to be used as\nan 'example' for a mistake. We noticed a few interesting and useful examples, such as \"For example, if \u2018a = 1, \u2018b = 2',\nand 'c = 3', your current implementation would classify this as a SCALENE triangle, which is incorrect because these sides\ndo not meet the triangle inequality theorem and thus cannot form a triangle\". We also noticed more code examples, but\nthey were not always compilable. This happened when the model suggested replacing the constants defined by the\nstudent with constants from a given class:\nFor example, in your computeP method, you directly defined BOLTZMANN and AVOGADRO\nconstants:\ndouble BOLTZMANN\n= 1.380649E-23;\ndouble AVOGADRO = 6.02214076E23;"}, {"title": "3.4.5 | Fifth iteration and final prompt", "content": "In the fifth iteration, we successfully addressed the issues encountered in the fourth iteration. Yet, we observed no\nnotable enhancements compared to the third iteration. Consequently, we chose to use this version of the prompt to\nassess the expanded dataset."}, {"title": "3.5 | Feedback Analysis", "content": "To answer RQ1 and RQ2, we analyzed the generated feedback with regard to their characteristics and the actual\nfeedback type(s), as described in Section 2.1. For the characteristics (see Table 2), we used the deductive categories\nfrom previous work [14], extended by the two categories PERS [12, 8, 47] and COMPL taken from other related\nwork [12, 8]. The PERS characteristic indicates that the feedback is personalized, meaning it explicitly refers to the\nstudent's solution. The COMPL characteristic addresses whether the feedback complies with the task description.\nThe analysis was carried out in three iterations with two steps each: (1) two experts (independently) coded the gen-\nerated feedback messages with the appropriate feedback types and categories, and (2) all conflicts were discussed\nand resolved using the consensual approach [60, 61]. In order not to be biased when coding the generated feedback,\nwe concealed the information on which feedback type was requested and by randomly ordering the messages. Fi-\nnally, the coded data was qualitatively analyzed by identifying representative themes in the feedback, and illustrative\nexamples."}, {"title": "4 | RESULTS AND DISCUSSION", "content": ""}, {"title": "4.1 | RQ1: Generating Specific Feedback Types", "content": "The results of the analysis of the generated feedback types are summarized in Table 5. It shows that in almost all\ncases (63 out of 66), the desired feedback type (DFT) matches the actual feedback type (AFT) generated by the LLM.\nHowever, in 19 cases, other feedback types were additionally included in the generated feedback. It was particularly\nnoticeable that KM feedback was often included when requesting KH feedback. However, this may be due to the fact\nthat it is not always possible to provide information on how to proceed without (directly or indirectly) pointing to the\nproblem in the code. The same effect was observed when requesting KTC feedback. In some cases, KTC feedback\nwas also combined with KM or KH feedback, as it is challenging to identify problems regarding the task constraints\nwithout giving any indication of the error in the code. Regarding RQ1, we conclude that the performance of the GPT-4\nmodel to generate specific, elaborated types of feedback is very promising."}, {"title": "4.2 | RQ2: Characteristics of Generated Feedback", "content": "In this section, we describe the characteristics we found in the generated feedback messages and conduct a qualitative\nanalysis in which we highlight certain themes identified in the data."}, {"title": "4.2.1 Dealing with correct programs", "content": "Several correct student programs were used as input to the LLM to find out the extent to which GPT-4 can recognize\nthese and provide adequate feedback. For example, consider the student program in Listing 2, which is a correct\nKotlin solution to the MaxOf3 exercise. The test cases for this exercise were shown in the form of text as '1 2 3 \u2192 3'.\nHowever, the submissions required the input to be inserted line-by-line. The KTC feedback confuses these by stating\n\"... your current implementation reads three integers in a single line, which may not align with the test cases provided in the\ntask description where inputs are expected line by line\", with the advice to \"review the task description and adjust your\ninput reading mechanism to match the expected format of the test cases.\" Therefore, we consider this feedback to be\nmisleading. We observed similar misleading feedback for a correct C++ and a correct Java submission to the same task.\nFor the C++ program, the LLM responded: \"Your program should output only the largest number, without any preceding\ntext\". However, the student had already done this and only printed the largest number (which was also the case in the\nJava submission)."}, {"title": "4.2.2 Misleading feedback", "content": "One of the major issues pointed out by previous studies is the regular occurrence of misleading advice. We still\nnoticed such issues in the feedback, but misleading information seemed to occur less frequently (compared to [14]).\nOne example of misleading feedback pointed out \"a minor inconsistency in the use of braces in the 'if-else' blocks, where\none of the 'else' statements does not use braces for a single statement, unlike the others\". This was not the case in the actual\nstudent code. Therefore, it may be very confusing feedback for a novice learner. In the next sections, we highlight\nexamples of misleading feedback for specific feedback types."}, {"title": "4.2.3 | Feedback on overall performance (KP)", "content": "Although our focus was on formative feedback, we were also interested in getting feedback on performance in general.\nWhile KP is usually considered a simple feedback type, with a number or percentage indicating progress, we decided\nto generate more elaborate reports on student's progress. We used the concept of 'subgoals' [66], which has been\nstudied extensively in computing education research. The results for this feedback type were quite good, although we\ndid not always agree with the percentage score given by the model, often giving too many points for minor subgoals.\nFor the MaxOf3 exercise, GPT-4 divided the problem into three reasonable subgoals in most cases, for example,\"(1) reading inputs correctly, (2) comparing the numbers accurately, and (3) printing the correct output.\" For most correct\nsubmissions, the expected score of 100% was given, together with a description of how the solution meets the sub-\ngoals. However, we also spotted an instance where a score of 80% was displayed for a correct solution."}, {"title": "4.2.4 | Feedback on concepts (KC)", "content": "We instructed the model to focus on programming concepts, as opposed to other concepts related to the task descrip-\ntion, such as mathematical formulas. We expected these concepts to appear in feedback on (semantic) mistakes. In\nmost cases, this worked out well. For the correct solutions, GPT-4 usually made general remarks about a concept,\npossibly referring to how it is applied in the student's solution. For the incorrect solutions, the concepts were related\nto actual mistakes and, in some cases, contained some information on how to fix them."}, {"title": "4.2.5 | Feedback on task constraints (KTC)", "content": "This type of feedback encompasses hints on task requirements, such as enforcing the use of a particular language con-\nstruct or prohibiting something. Another subcategory contains hints with general information on how to approach\nthe exercise, not being specific to the student's current work. In some of the outputs we noticed GPT-4 giving feed-\nback of this kind, such as for the Physics task: \"The task specifically requires you to use the constants defined in the\nPhysicsConstants class for these values to ensure consistency and maintainability of the code.\", and the NegaFibonacci:\n\"The task explicitly requires the use of the generalized Fibonacci sequence formula for negaFibonacci, without resorting to\nalternative formulas or approaches.\"\nIn other cases, the feedback referred to functional constraints that can be derived from the task description, which\nwe assign to KM feedback on solution errors. For the correct solutions to the MaxOf3 exercise, which did not have\nany particular constraints, the generated feedback mentioned something about checking the output format. It even\ngave misleading suggestions that \"Your program should output only the largest number, without any preceding text\", while\nthis was not the case. In several cases, we also noticed that GPT-4 simply rephrased the task description."}, {"title": "4.2.6 | Feedback on mistakes (KM)", "content": "There are several subtypes to this category: feedback on compiler errors, solution errors, test failures, style issues, and\nperformance issues. This feedback can be short, but we mostly observed elaborate error descriptions. We instructed\nGPT not to give information on how to fix the error, but in some cases, this was still included. In most cases, GPT\npointed out actual errors but made a few mistakes. For example, for the program below in Listing 3 the feedback was\n\"Your solution correctly implements the Fibonacci sequence; however, there's a mistake in the order of your base case checks.\nYou should check for n == 0 before n == 1 to adhere to the sequence's definition starting from 0.\" This might indeed look\nsomewhat nicer, but does not have any consequences for the method's correctness."}, {"title": "4.2.7 | Feedback on how to proceed (KH)", "content": "The CAUSE and FIX categories are closely linked to the KM and KH feedback types. If KM feedback was requested,\nCAUSE was always included. The same applies to KH feedback. This is not surprising, as Kiesler et al. [14] had\nderived these two categories from KM and KH feedback. In many cases, the KH feedback gave valuable instructions\nformulated as individual steps - on how to address the problems in the code. In some cases, the feedback was less\nvaluable as it contained information that in principle corresponds to a \"think harder\" statement, e.g., simply asking\nthe learner to ensure that a particular specification was correct without concrete help on how to achieve this (\"Ensure\nyour method correctly implements the formula provided in the task description.\")."}, {"title": "4.2.8 Dealing with logic", "content": "It is well known that LLMs struggle with logical reasoning [67]. In one Java solution to MaxOf3 the student had\nthe unusual approach of comparing numbers by subtracting them and checking for a positive or negative result (see\nListing 4). GPT-4 was not always precise in pointing out the problem, noting that \"your solution currently lacks a branch\nto handle the situation where the first number is less than the second but greater than the third\", which is not a particular\ncase that needs to be dealt with. Moreover, the model added that \"adding this condition will make your solution more\ncomprehensive\", which does not seem like a useful goal."}, {"title": "4.2.9 Language and tone of the feedback", "content": "In our prompt, we explicitly instruct ChatGPT to give feedback to a novice programmer. Additionally, our exercises are\nclearly aimed at beginners. We noticed that the generated feedback was suitable for the target audience. However,\nthere were some instances of specific terminologies, such as \"The submitted solution has an error related to variable\nshadowing and data type mismatch\", but in this case, the errors were explained afterward.\nAs in a previous study [14], very few feedback messages contained motivational components. Another study [47]\nalso identified few compliments in feedback messages, although observed mostly a 'friendly' tone. We only coded\nthe MOT category if there was a clear intention to motivate (e.g. \"Well done\"). The feedback sometimes provided\ninformation about (partial) correct solutions (e.g., \"Your implementation of the base cases is correct\"). While these\ncases may serve the purpose of motivating the learner, we did not label them as MOT because we could not determine\nthe 'real intention' behind the generated messages. The only motivating text we noticed was \"Your current solution\nhas made a good attempt at implementing the Fibonacci sequence, but it's not fully aligned with the requirements of the\nFibonacci calculation...\"\nKiesler et al. [14] also reported feedback messages asking for more information or showing uncertainty when\nproviding feedback to learners' submissions. In contrast, we did not observe any feedback messages requesting addi-\ntional information or responses with uncertainty. This may be attributed to the more comprehensive context provided\nin the prompt during generation. We do not assume that this improvement is due to the fact that we used a newer\nmodel (GPT-4 instead of GPT-3.5).\nFortunately, we did not find any instances of derogatory or demeaning tone in the generated feedback messages.\nThe language was consistently grammatically correct and expressed in a friendly or neutral tone."}, {"title": "5 | LIMITATIONS", "content": "The results presented in this paper are based on the evaluation of outputs from a single LLM (GPT-4). The evaluation\nof such feedback by experts is a time-consuming task, so we had to decide to either (a) evaluate multiple models on a\nsmall set of different tasks, or (b) evaluate a more heterogeneous set of tasks and responses from a single model. We\nchose the second option as the GPT-4 model seemed to be the most promising and investigated model (at the time\nof the study). In addition, we assume it is more valuable to investigate multiple programming languages, topics, and\nerror types, and how one system responds to them.\nAnother limitation is due to the selected programming exercises as part of our dataset, which mostly concern\nmathematical functions. Although this is common in the first few weeks of introductory programming courses, it\ndoes not cover all possible types of tasks. Therefore, we have to be careful to generalize our results to other (non-\nmathematical) task domains in introductory programming."}, {"title": "6 | IMPLICATIONS AND RECOMMENDATIONS", "content": ""}, {"title": "6.1 Implications for Educators", "content": "Incorporating LLMs for programming in educational settings can be beneficial for improving the feedback loop, alle-\nviating teachers from frequently having to provide individual feedback. The potential effectiveness of LLM-driven\nfeedback, however, highly depends on the precision and contextualization of the used prompt. Previous research\nhas highlighted the limitations of overly simplistic prompts. Simply providing student code with a general request for\nfeedback may lead to (partially) incorrect or misleading responses, or the model requesting more information [14]."}, {"title": "6.2 | Implications for Tool Developers", "content": "The results of this study have shown that the generation of feedback for programming tasks using LLMs has the\npotential to improve educational systems. The major advantage over previous approaches is that LLMs do not have to\nbe trained by users themselves on large amounts of data, which saves considerable development efforts. Nevertheless,\nwe believe an approach based solely on LLMs is not the most promising."}]}