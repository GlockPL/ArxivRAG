{"title": "Evaluating the Evaluator:\nMeasuring LLMs' Adherence to Task Evaluation Instructions", "authors": ["Bhuvanashree Murugadoss", "Christian Poelitz", "Ian Drosos", "Vu Le", "Nick McKenna", "Carina\nSuzana Negreanu", "Chris Parnin", "Advait Sarkar"], "abstract": "LLMs-as-a-judge is a recently popularized method which re-\nplaces human judgements in task evaluation (Zheng et al.\n2024) with automatic evaluation using LLMs. Due to\nwidespread use of RLHF (Reinforcement Learning from Hu-\nman Feedback), state-of-the-art LLMs like GPT4 and Llama3\nare expected to have strong alignment with human prefer-\nences when prompted for a quality judgement, such as the\ncoherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute\nonly an evaluation based on the instructions in the prompts, or\nreflect its preference for high-quality data similar to its fine-\ntune data. To investigate how much influence prompting the\nLLMs-as-a-judge has on the alignment of AI judgements to\nhuman judgements, we analyze prompts with increasing lev-\nels of instructions about the target quality of an evaluation, for\nseveral LLMs-as-a-judge. Further, we compare to a prompt-\nfree method using model perplexity as a quality measure in-\nstead. We aggregate a taxonomy of quality criteria commonly\nused across state-of-the-art evaluations with LLMs and pro-\nvide this as a rigorous benchmark of models as judges. Over-\nall, we show that the LLMs-as-a-judge benefit only little from\nhighly detailed instructions in prompts and that perplexity can\nsometimes align better with human judgements than prompt-\ning, especially on textual quality.", "sections": [{"title": "Introduction", "content": "Recently, new automatic evaluation approaches that rely\non LLMs have been proposed on several NLG tasks,\nsuch as summarization (Liu et al. 2023b) and machine\ntranslation (Kocmi and Federmann 2023). Previous ap-\nproaches (Siledar et al. 2024) show that for certain situ-\nations, such as when assessing textual consistency or flu-\nency, there is high agreement between human judgements\nand LLM assessments, even without detailed instructions\nlike for example how to assign specific scores. Most of these\napproaches prompt an LLM to give a judgement as Likert\nscore (Likert 1932) with only simple information about the\nscale, e.g. \"give a judgement between 1 (bad) and 5 (good).\"\nMore recently, LLM-based evaluations on more fine-grained\ntask-specific criteria (Ye et al. 2024) have also reported high\nagreement with human judgement, such as assessing the\ncompleteness of a solution for a question answering task.\nFor these evaluations, often more detailed instructions are\ngiven about when to assign a specific score, similarly to\nrubric scoring (Andrade 2005).\nWhile these results are promising for the future of auto-\nmatic evaluation, it is less clear how the models achieve this\nagreement, and in general, it is a challenge to identify for\nany given task, which LLM is most appropriate to evaluate\nit, and with how much information, respectively how much\ninstructions about the evaluation. Alarmingly, recent results\nshow a clear bias in LLMs preferring their own output over\nothers (Panickssery, Bowman, and Feng 2024), and LLMs'\nperplexity has emerged as a possible quality criteria for fil-\ntering (Ankner et al. 2024) on textual quality. This raises the\nquestion of whether some of the results on automatic evalu-\nations with LMs reflect a model's preference for data similar\nto its own (high quality) fine-tuning data instead of follow-\ning the provided instructions on how to measure the quality\nof an answer. Especially for fine-grained evaluations with\ndetailed rubric information, we expect the instructions about\nwhen to assign a score to be adhered closely.\nIn this paper we report our findings when using LLMs-as-\na-judge (Zheng et al. 2024), where LLMs are used as surro-\ngates for humans judgements to evaluate several NLG and\nLLM-based tasks, in skill-specific settings (e.g. complete-\nness) and skill-unspecific (e.g. textual coherence). We show\nthe annotations for many of these quality criteria in state-of-\nthe-art benchmarks have a high correlation with the perplex-\nity of the LLMs, often higher than prompting the LLM for a\nscore. We identify which evaluation settings can benefit the\nmost from more detailed prompting and for which settings\nsimple generic prompts, or just using models' perplexity as\nquality score, suffice.\nIn detail, we make the following three main contributions:\n1. We propose a novel taxonomy of qualitative evaluation\ncriteria useful for assessing the competence of automatic\nevaluation methods by LLMs-as-a-judge. Our taxonomy\nconsists of 4 evaluation categories (Content, Relevance,\nIntegrity, and Engagement) which encapsulate 34 met-\nrics as tested by 8 distinct state-of-the-art benchmark\ndatasets.\n2. We systematically evaluate the effectiveness of LLMs-\nas-a-judge using the taxonomy with several major LLM\nfamilies including GPT4, Llama3, Mistral, and Phi3\nacross 4 levels of increasing prompt instruction. We find"}, {"title": "Related Work", "content": "LLMs as evaluators for general NLG (Liu et al. 2023b),\nas well as for knowledge and problem-solving tasks (Ye\net al. 2024) have been widely studied recently (Chiang and\nLee 2023; Li et al. 2024; Gao et al. 2024). Most previous\napproaches, either perform pair-wise evaluations (Ji et al.\n2023; Chen et al. 2023), measuring the preference of one\nof two examples for a given criterion, or perform direct as-\nsessments for a single given example and a evaluation crite-\nrion (Liu et al. 2023b; Ye et al. 2024). Additionally, they dis-\ntinguish between reference-free evaluations, where the LLM\nis presented only an example and the criterion for evaluation,\nand reference-based evaluations with given annotated exam-\nples of different qualities or ground truth for each example.\nGenerally, previous works use LLMs as evaluators by us-\ning simple prompting strategies (Siledar et al. 2024), only\nfew fine-tuned models are available (Kim et al. 2023, 2024)\nfor measuring for specific quality criteria. Most other fine-\ntuning approaches concentrate on scenario-specific quality\nfeedbacks (Li et al. 2023; Wang et al. 2023) or on specific\nuse-cases (McAleese et al. 2024).\nRecently, there are several approaches (Liu, Moosavi, and\nLin 2024; Liu et al. 2024, 2023c; Stureborg, Alikaniotis,\nand Suhara 2024) reporting biases and mismatches with hu-\nman annotations, but our work is the first to study whether\nthe models' perplexity can be a better surrogate for qual-\nity then prompting the corresponding model and whether in-\nstructions in the prompts are impacting the results across a\nnumber of different LLMs-as-a-judge."}, {"title": "Evaluating LLMs-as-a-judge", "content": "In this section we give a short definition of LLMs-as-a-judge\nand automatic evaluation using AI. We define different set-\ntings of prompting the LLMs-as-a-judge to measure the im-\npact on the alignment of LLM judgments with human judge-\nments. We also evaluate a prompt-free metric using sim-\nple model perplexity. This alternative approach requires no\nprompt engineering and transparently measures alignment\nwith training data without bias from a prompt, so it is a\ncompelling alternative for evaluation. Finally, we introduce\na new taxonomy, aggregating the quality criteria most fre-\nquently used in state-of-the-art benchmarks for automatic\nevaluation with LLMs. We categorize these into 4 groups\nrepresenting the major aspects of evaluating AI generated\nresponses."}, {"title": "LLMs-as-a-judge", "content": "As LLM-as-a-judge we refer to the definition introduced by\n(Zheng et al. 2024) as potential replacement for human an-\nnotations by prompting an LLM for a judgement of an AI\nassistant response. We concentrate on judging textual ex-\namples only e.g., AI generated summaries for news arti-\ncles (Fabbri et al. 2021) or step-by-step solution to math-\nematical reasoning questions (Golovneva et al. 2023). We\nphrase the task to judge an AI generated response as the\nfollowing: Given a task A and an AI generated solution B,\njudge the quality of the solution B considering only the task\nA. In contrast to other previous approaches, we perform a\nreference-free evaluation where we do not provide a possi-\nble correct reference solution. We solely rely on the models'\nability to judge the solution given only the task.\nTo measure the impact of prompting the LLM-as-a-judge,\nwe study the LLMs' performance in 4 different settings:\n1. Perplexity: We score each task solution by its perplex-\nity under the corresponding LLM, given only the task\ndescription. This approach is unbiased by prompts, so\nit transparently measures alignment with model train-\ning data, providing a good comparison and alternative to\nprompt-based approaches.\n2. Generic quality prompt: We prompt each LLM-as-a-\njudge with a basic instruction to measure the quality of\nthe task solution, but give no specific criteria or instruc-\ntions. In this case, we rely solely on the models' prior\nknowledge about the quality for the task solution from\nthe examples generated.\n3. Criteria specific prompt: We prompt each LLM-as-a-\njudge with an instruction to measure the quality for a spe-\ncific criteria e.g., coherence. We only provide the name\nof the criteria, not a definition. We rely on the models'\nprior knowledge of the specific quality criteria only.\n4. Full rubric prompt: We prompt each LLM-as-a-judge\nwith an instruction to measure the quality for a specific\nquality criterion, together with a definition of the crite-\nrion and instructions when to assign each rubric score\ne.g., \"Score 1: Incoherent text with many logical flaws.\"\nWe evaluate different LLMs-as-a-judge under the above\nsettings on several different benchmarks (as described in\nthe next subsection). We use the criteria as specified in the\ncorresponding annotation guidelines from the benchmark\ndatasets. For setting 4, we use all available annotation guide-\nlines with information about the criteria and when to assign\neach score. We extract this information directly from bench-\nmarks into a full rubric containing information about the cri-\nteria and the scores. For our experiments, we structure the\nsettings from least instructive (Perplexity / no prompting) to\nmost instructive (Full rubric information with instructions\nwhen to assign a score). In Fig. 1 we show the different set-\nting of prompting for an example quality criterion."}, {"title": "Datasets", "content": "We use 8 different open-source benchmark datasets com-\nmonly used for LLM-based evaluations with human anno-\ntations for several evaluation criteria per task. The datasets\ncontain task which span several aspects from coarse-grained\nNLG-quality evaluations, to fine-grained very task specific\nevaluations with detailed information about how to score ex-\namples.\nFirstly, we leverage two of the most prominently used\ndatasets for coarse-grained NLG-quality evaluations: The\nSummEval (Fabbri et al. 2021) dataset contains news ar-\nticle summaries generated by different models together with\nhuman annotations for 4 different quality criteria e.g., flu-\nency; and the TopicalChat (Gopalakrishnan et al. 2019)\ndataset contains human conversations over 8 different top-\nics annotated by humans for 5 different quality criteria\ne.g., engagement. Further, we use two more challenging\nbenchmark datasets for coarse-grained NLG-evaluations:\nthe OpinSummEval (Shen and Wan 2023) dataset is a opin-\nion summarization dataset, which consists of review sum-\nmaries annotated for aspects, opinions and sentiments; the\nInstruSumm (Liu et al. 2023a) dataset, consists of news ar-\nticle summaries following specific instructions with human\nannotations for content specific quality-criteria e.g., amount\nof missing information.\nSecond, we use two benchmark datasets for more fine-\ngrained NLG evaluations: the Hanna (Chhun et al. 2022)\ndataset and the TheNext Chapter (Xie, Cohn, and Lau\n2023) dataset contain creative stories generated for a given\ninitial user prompt. Each story is annotated by humans\nfor NLG and style based criteria e.g., coherence, but\nalso for more unconventional criteria like surprise. Finally,\nwe use two task-specific evaluation benchmark datasets\nwith quality-criteria depending in task solution quality:\nRoscoe (Golovneva et al. 2023) is a collection of datasets\nof reasoning tasks, together with GPT3 generated with step-\nby-step solutions. The human annotations cover coarse-\ngrained task specific evaluation criteria like \u201cmissing step\u201d;\nthe Flask (Ye et al. 2024) dataset contains several knowl-\nedge and problem solving tasks with LLM generated solu-\ntions. The human annotations cover more fine-grained task-\nspecific criteria like completeness and factuality. Most crite-\nria need an understanding of the solution e.g., completeness."}, {"title": "Criteria taxonomy", "content": "We introduce a simple taxonomy of quality evaluation crite-\nria based on current state-of-the-art benchmark datasets and\nquality criteria commonly used for automatic evaluations by\nLLMs. We define 4 groups of quality criteria, relevant for\nautomatic evaluation:\n1. Content-based criteria: Measure how well the solution\nis presented to the user, for example, whether a news ar-\nticle summary is fluent.\n2. Engagement-based criteria: Measure how engaging the\nsolution is, for example, whether a generated story con-\ntains an element of surprise.\n3. Integrity-based criteria: Measure how consistent and\nlogical coherent the solution is, for example, whether a\nmath solution is correct.\n4. Relevance-based criteria: Measure how relevant the so-\nlution is for the given task, for example, whether a legal\nadvice answer contains irrelevant information."}, {"title": "Model Selection for LLM-as-a-judge", "content": "To understand how model size and finetuning affect per-\nformance across the different quality criteria and set-\ntings of prompting, we test several current LLMs: GPT4-\nTurbo (OpenAI et al. 2024)-0125 as large closed-model\nbaseline; Llama3 70b (Touvron et al. 2023; Dubey et al.\n2024) as a medium size open-model; Llama3 8b, Mistral-\nv0.3 (Jiang et al. 2023) as small open-models, Phi3 (Abdin\net al. 2024)-Medium-128k as fine-tuned model for reason-\ning, and Prometheus-2 (Kim et al. 2024) as fine-tuned mod-\nels for evaluation tasks\u00b9."}, {"title": "Results", "content": "In this section, we present the main results of the evaluations\nusing the different LLMs-as-a-judge under the different set-\ntings of prompting. Analogous to previous work (Liu et al.\n2023b), to measure the quality of the evaluations we cal-\nculate the Pearson correlation of the generated scores by the\nLLMs-as-a-judge, respectively the perplexity values, and the\nhuman annotations given for each quality criteria from the\nbenchmark dataset. We split the results section into model\nlevel, dataset level and criteria level results. In the model\nlevel results subsection, we present the results comparing\nthe different LLMs under the setting of prompting, aver-\naging over all criteria; the dataset level results subsection\npresents the results when we compare the different datasets\nunder the setting of prompting average over all criteria; the\ncriteria level results subsection presents the results when we\ncompare models and setting of prompting under the differ-\nent groups of criteria. Finally, we present the results of a de-\ntailed analysis for each group of criteria from the introduced\ntaxonomy."}, {"title": "Model level results", "content": "There is only small effect adding full rubric information.\nProviding the LLMs-as-a-judge with more detailed rubric\ninformation of the quality criteria, generally has only small\ninfluence on evaluation performance for the large and mid-\nsize models, and might even be disadvantageous in certain\nsituations (see Tab.1). For instance, Phi3's performance de-\ncreases when complete rubric details are provided compared\nto simple prompts which only mention the criterion name\nin the prompt. Here, Phi3's prior knowledge about evaluat-\ning the criteria has higher agreement with human annotators\ncompared to when using full rubric information. Only the\nsmaller Llama3 8b and Mistral models see improvements\nwhen given comprehensive rubric information for assess-\nment. Among the open models, Llama3, both the 70b and 8b\nversions, perform best. Meanwhile, Mistral and Prometheus-2 do not show improvements when the LLM is prompted,\nwith models' perplexity having higher correlation then the\ngenerated scores. For Prometheus-2, we only report per-\nplexity and full rubric information in the prompts since this\naligns with the fine-tuning data for this model and both set-\nting 2 and 3 did return very poor results.\nGPT4 performs best among all models. As may be\nexpected, prompting GPT4-as-a-judge, even for a generic\nquality judgement, results in the highest performance in\nterms of agreement with human annotations compared to\nall other models tested. Further, GPT4's judgements do only\nimprove marginally from prompting setting 3 to 4, indicat-\ning that GPT4's prior knowledge about evaluating does al-\nready agree with the human judgements to a high degree\nwithout the need to add more detailed rubric information\nabout the evaluation."}, {"title": "Dataset level results", "content": "Perplexity correlates with text quality criteria. We ob-\nserve (see Tab. 2) that the quality criteria from datasets with\nsimple textual content creation tasks e.g., summarization\nin the SummEval dataset or story generation in the Hanna\ndataset, show high agreement with models' perplexity com-\npared to simple prompting (setting 2 and 3). For more com-\nplex NLG tasks, which depend on several aspects and mul-\ntiple possible steps, the human annotation correlate less\nstrongly with perplexity compared prompting the LLMs-\nas-a-judge with more information. For example the opinion\nsummary evaluations from the OpinSummEval dataset uses\ncriteria which depend on sentiment identification and extrac-\ntions of the key aspects, in these cases prompting the LLM\nseems necessary.\nFull rubric information helps for non-default tex-\ntual quality evaluations. Unusual textual quality evaluation\ntasks which measure the quality beyond simple textual cri-\nteria like fluency, can benefit from more full rubric informa-\ntion about the the evaluation task. For example, we observe\nthat for the TheNextChapter dataset, full rubric information\nin the prompts to the LLMs-as-a-judge leads to judgements\nwith the highest correlations with human annotations. Com-\npared to other datasets for textual quality evaluation, these\ntwo datasets contain much more complex texts e.g., cre-\native stories with non-default quality criteria like relatedness\nwhich is difficult to estimate without additional information\nby an LLM. Furthermore, evaluating more complex tasks\nwhich include more than text quality, like the logical rea-\nsoning tasks, benefit also from more detailed rubric infor-\nmation in the prompts. The logical and mathematical rea-\nsoning tasks in the Roscoe datasets for example do benefit\nfrom more information to effectively judge as shown by the\nhigher correlations with the human judgements compared to\nprompting with less information or using perplexity.\nDataset level analysis can be misleading. Models' per-\nplexity on both the Flask dataset and the SummEval dataset,\noutperforms simple prompting in aligning to human judge-\nments. While the quality criteria in the SummEval dataset\nprimarily focuses on textual quality where we expect per-\nplexity to perform well for example, the Flask dataset con-\nsist a variety of different quality criteria which make it diffi-\ncult to generalise and the average correlations values might\nbe biased the high values on the text related criteria. In the\nnext subsection, we investigate this issue by using the pre-\nviously introduced taxonomy to analyze results on a per-\ncriteria class basis rather than average results per dataset."}, {"title": "Criteria level analysis", "content": "Content-based quality criteria correlate the most with\nperplexity. When evaluating quality with a focus on tex-\ntual context, perplexity seems a viable alternative to prompt-\ning LLMs-as-a-judge. We observe that on average the agree-\nment with human annotations is more then 20% higher when\nusing models' perplexity to judge the quality compared to\nprompting (Fig. 3). Further, there are only small differences\nbetween using a simple generic quality prompt for evalu-\nation compared to all other settings of prompting, show-\ning that models' prior knowledge generates judgements with\nhigh agreement with human judgements on textual quality.\nEngagement-based quality criteria benefit the most\nfrom full rubric information. These criteria are unconven-\ntional as they assess the likelihood of a user feeling person-\nally engaged, as opposed to merely evaluating straightfor-\nward text quality. Access to full rubric information can help\njudging with directives, particularly when the evaluation is\nmore unusual and different from the text quality alone.\nOften, there is only little improvement of adding full\nrubric information for most criteria groups. Except for\nthe engagement-based criteria, there is only limited effect on\nadding full rubrics information to the prompts. Further, sim-\nple prompts for generic quality judgements results in similar\ncorrelation values with the human annotations than detailed\ninformation for content and relevance based evaluation crite-"}, {"title": "Details on content-based criteria results", "content": "Drilling down the content-based evaluations criteria, we\nobserve that perplexity outperforms prompting mainly on\nstructural text quality criteria. For example, human anno-\ntations for fluency from the SummEval dataset have much\nhigher agreement with perplexity compared to all prompting\nmethods. This quality criterion judges grammar, spelling,\nand sentence structure for example. On the other hand, eval-\nuating for more complex, specific and subjective content-\nbased criteria like naturalness, which measures how natural\nthe task response sounds, benefits from more instructions in\nthe prompts for the LLMs-as-a-judge. Here, instructions to\njudge how much the task solution resembles a human answer\nimproves agreement with human judgements.\nNotably, we observe that model perplexity has 100%\nagreement with the human annotations for harmlessness\non Flask datasets. This might reflect the strong influence\nfine-tuning has in inhibiting the generation of harmful con-\ntent (Dubey et al. 2024). Conversely, prompting for measur-\ning harmfulness performs much lower until we provide full\nrubric information in the prompts."}, {"title": "Details on engagement-based criteria results", "content": "Engagement-based criteria are more challenging to judge\nsince they are often subjective. We observe that there are\nfewer performance differences between the models com-\npared to the results on the other criteria e.g., GPT4 judge-\nments have an agreement (by Pearson correlation) of 0.32,\nPhi3 of 0.31 and Llama3 8b of 0.3. The overall performance\nof all models on these criteria is lower and the scores gener-\nated by the models have higher variance compared to other\ncriteria. Further, human annotations for these criteria have\non average lower scores with higher variance. For example,\nfor the criterion empathy from the Hanna dataset (Fig. 5),\nhuman annotations show a significant degree of variation,\nand Phi3 notably generates scores with higher variability,"}, {"title": "Details on relevance-based criteria results", "content": "For relevance-based quality criteria, we assume that the\nmodels need robust information about the problem to mea-\nsure whether the information in the task solution is relevant,\nand estimate to what degree. Models' perplexity, but also\ngeneric prompts seem not sufficient for evaluation since rel-\nevance is more specific to certain aspects of the task solution.\nStill, we also observe that including a full rubric doesn't al-\nways appear necessary; instead the size of the models seem\nmore important. For the criterion groundedness from the\nTopicalChat dataset for example (Tab. 4), we identify a clear\ntrend of increasing agreement with the human judgements\nwith larger models as LLM-as-a-judge."}, {"title": "Details on integrity-based criteria results", "content": "Similar to the relevance-based quality criteria, we assume\nthat to measure the quality for integrity-based criteria the\nLLMs-as-a-judge need to have an understanding of the task,\nbut also the ability to solve the task itself. We hypothesize\nthat, for task-specific evaluations, the underlying LLM-as-\na-judge actually needs to be able to solve the task itself to\napply the correct score. As reported in (Lin et al. 2024), LLMs' ability to critic a task solution correlates with its abil-\nity to solve the task.\nWe exemplify this by the evaluations for the integrity-\nbased evaluation criterion on the criterion logical correct-\nness from the Flask dataset. This criterion reflects the cor-\nrectness elements which are reflected in human annotations,\nwhich are more clearly scored as high (for logically correct)\nor low (for logically incorrect) scores. To select the appro-\npriate scores, the LLM-as-a-judges need to know what is\ncorrect and what is wrong. Here, GPT-4 significantly out-\nperforms all other models by a wide margin with a Pearson\ncorrelation of 0.68 with the human judgements in contrast to\n0.34 for Llama3 70b and 0.33 for Phi3, for example.\nTo illustrate this, we plot the generated scores of the\nLLMs-as-a-judge (Fig. 6) and the human annotations. We\nobserve that only GPT4 is able to generate lower scores to\njudge a task solution as \u201cbad.\u201d All other models predomi-\nnantly give high scores, consistently grading bad logically\nincorrect responses as \u201cvery good.\""}, {"title": "Conclusion", "content": "In this paper, we investigate how increasing levels of\nprompting impact the automatic evaluations made by LLMs-\nas-a-judge in measuring the quality of AI-generated text.\nWe introduce a new taxonomy of quality criteria, summa-\nrizing commonly used criteria in automatic evaluations with\nLLMs into four broad categories: Content, Relevance, In-\ntegrity, and Engagement. We systematically evaluated sev-\neral LLMs, including GPT-4, Llama-3, and others, across\nall settings of prompting to determine if more detailed in-\nstructions enhance the LLMs' alignment with human judge-\nments. Key findings include:\n\u2022 Detailed quality criteria information might not be neces-\nsary in the most powerful models; for instance, GPT-4\nshows a high level of agreement with human judgements\neven without detailed instruction.\n\u2022 Simple perplexity values are very effective at estimat-\ning textual quality, often outperforming the results of\nprompting the LLMs-as-a-judge with basic instructions.\n\u2022 Judging task-specific quality criteria like relevance or\nlogical correctness requires more capable, larger models,\naligning with previous research on the necessary model\ncapabilities for critiquing (Lin et al. 2024)."}]}