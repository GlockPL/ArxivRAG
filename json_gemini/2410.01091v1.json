{"title": "Efficient and Private Marginal Reconstruction with Local Non-Negativity", "authors": ["Brett Mullins", "Miguel Fuentes", "Yingtai Xiao", "Daniel Kifer", "Cameron Musco", "Daniel Sheldon"], "abstract": "Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in major deployments that impact millions of people. Many differentially private algorithms for query release and synthetic data contain steps that reconstruct answers to queries from answers to other queries measured by the mechanism. Reconstruction is an important subproblem for such mechanisms to economize the privacy budget, minimize error on reconstructed answers, and allow for scalability to high-dimensional datasets. In this paper, we introduce a principled and efficient postprocessing method ReM (Residuals-to-Marginals) for reconstructing answers to marginal queries. Our method builds on recent work on efficient mechanisms for marginal query release, based on making measurements using a residual query basis that admits efficient pseudoinversion, which is an important primitive used in reconstruction. An extension GReM-LNN (Gaussian Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under Gaussian noise satisfying consistency and non-negativity, which often reduces error on reconstructed answers. We demonstrate the utility of ReM and GReM-LNN by applying them to improve existing private query answering mechanisms: ResidualPlanner and MWEM.", "sections": [{"title": "1 Introduction", "content": "Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in major deployments that impact millions of people such as the 2020 US Decennial Census [1]. One of the most fundamental problems in differential privacy is answering a workload of linear queries. Linear queries are used for basic descriptive statistics like counts and sums, and as building blocks for more complex tasks. Marginal queries, which describe the frequency distribution of subsets of discrete variables (e.g., income by age and education), are of particular interest as descriptive statistics and for use in downstream tasks like regression analyses.\nA key subproblem in linear query answering is reconstruction. Given a workload of linear queries, most mechanisms select a different set of queries to measure to make the most efficient use of the privacy budget, and then use the noisy answers to reconstruct answers to workload queries [2\u201311]. Effective reconstruction methods can combine information from all noisy measurements to provide mutually consistent answers to workload queries.\nComputational complexity is a key challenge for reconstruction methods. These methods answer workload queries by\u2014either explicitly or implicitly\u2014reconstructing a data distribution that has size exponential in the number of variables. To scale to high-dimensional data sets, existing approaches must represent this distribution compactly through some form of parametric representation [8\u201312], which introduces tradeoffs such as a restricted space of data distributions that can be represented [8\u2013"}, {"title": "2 Preliminaries", "content": "We consider a sensitive tabular dataset D of records $x^{(1)}, ..., x^{(N)}$. Each record $x = (x_1,...,x_d)$ consists of d categorical attributes. The ith attribute $x_i$ belongs to the finite set $X_i$ of size $n_i$. The data universe is $X = \\Pi_{i=1}^d X_i$ and has size $n = \\Pi_{i=1}^d n_i$. The data vector or data distribution $p \\in \\mathbb{R}^n$ is a vector indexed by X that counts the occurrences of each record in D; it has entries $p(x) = \\Sigma_{i=1}^N [x^{(i)} = x]$. Since n is exponential in the data dimension d, it is computationally intractable to work directly with data vectors in high dimensions."}, {"title": "2.1 Linear queries, marginals, and residuals", "content": "Linear queries are a rich class of statistics that include counts, sums, and averages and are used as building blocks for more complex tasks. A linear query is the sum of a real-valued function $q : X \\rightarrow \\mathbb{R}$ applied to each record in the dataset. We adopt the equivalence that a query is a vector $q \\in \\mathbb{R}^n$ with answer $q^T p$. A query matrix or workload W is a collection of m linear queries arranged row-wise in an $m \\times n$ matrix. The answer to workload W for data vector p is given by $Wp$.\nMarginal queries are a common type of linear query for high-dimensional data. They count the number of records that match certain values for a subset of the attributes \u2013 e.g., the number of people in a dataset with education at least a college degree and income $50-$100K. Let $\\gamma \\subseteq [d]$ be a subset of attributes and $x_\\gamma = (x_i)_{i \\in \\gamma}$ be the corresponding subvector of x. Further, let $X_\\gamma = \\Pi_{i \\in \\gamma} X_i$ and $n_\\gamma = \\Pi_{i \\in \\gamma} n_i$. The marginal $\\mu_\\gamma \\in \\mathbb{R}^{n_\\gamma}$ has entries $\\mu_\\gamma(t) = \\Sigma_{x : x_\\gamma = t} p(x) = \\Sigma_{i=1}^N [x^{(i)}_\\gamma = t]$ that count the number of occurrences in the dataset for each setting $t \\in X_\\gamma$ of the attributes in $\\gamma$. Let $M_\\gamma \\in \\mathbb{R}^{n_\\gamma \\times n}$ be the marginal workload so that $\\mu_\\gamma = M_\\gamma p$. As shown in Fig. 1a, $M_\\gamma$ can be written concisely as a Kronecker product over dimensions, with base matrices equal to the identity $I_k \\in \\mathbb{R}^{n_k \\times n_k}$ for attributes in $\\gamma$ and the all ones vector $1 \\in \\mathbb{R}^{1 \\times n_k}$ for attributes not in $\\gamma$. Kronecker product matrices can be understood as applying different linear operations along each dimension of a multi-dimensional array. In this case $M_\\gamma$ sums over dimensions of the array representation of p for attributes not in $\\gamma$. We provide a brief summary of Kronecker products and their relevant properties in Appendix A.\nResidual queries are class of linear queries closely related to marginals. They were recently introduced in the privacy literature [6] but previously studied in statistics as variable interactions [13, 14]. For $\\tau \\subseteq [d]$, the $\\tau$-residual is obtained from the marginal $\\mu_\\gamma$ by applying a differencing operator along each dimension. Let $D^{(k)}$ be the linear operator that computes successive differences for vectors of length $n_k$, i.e., $(D^{(k)} v)_i = v_{i+1} - v_i$ for $i = 1,..., n_k - 1$; an example is shown for $n_k = 3$ in Fig."}, {"title": "2.2 Differential Privacy", "content": "When releasing the results of any analysis performed on sensitive data, particular care needs to be taken to avoid leaking private information contained in the dataset. Differential privacy is a mathematical criterion that bounds the effect of any individual in the dataset on the output of a mechanism, which is satisfied by adding noise to the computation. This allows for formal quantification of the privacy risk associated with any release of information.\nDefinition 1. (Differential Privacy; [16]) Let $M : \\mathcal{X} \\rightarrow \\mathcal{Y}$ be a randomized mechanism. For any neighboring datasets D, D' that differ by adding or removing at most one record, denoted $D \\sim D'$, and all measurable subsets $S \\subseteq \\mathcal{Y}$: if $Pr(M(D) \\in S) \\leq exp(\\epsilon) \\cdot Pr(M(D') \\in S) + \\delta$, then M satisfies $(\\epsilon, \\delta)$-approximate differential privacy, denoted $(\\epsilon, \\delta)$-DP.\nA fundamental property of differential privacy relevant to our work is the post-processing property, which states that transformations of differentially private outputs that do not access the sensitive dataset D maintain their privacy guarantees. Formally:\nProposition 2 (Post-processing; [17]). Let $M_1 : \\mathcal{X} \\rightarrow \\mathcal{Y}$ satisfy $(\\epsilon, \\delta)$-DP and $f : \\mathcal{Y} \\rightarrow \\mathcal{Z}$ be a randomized algorithm. Then $M : \\mathcal{X} \\rightarrow \\mathcal{Z} = f \\circ M_1$ satisfies $(\\epsilon, \\delta)$-DP.\nThe reconstruction methods we propose in this paper are post-processing algorithms that take as input a set of noisy linear query answers and, thus, inherit the privacy guarantees from those noisy answers. Note that the present analysis is largely agnostic to the model of differential privacy used."}, {"title": "2.3 Private query answering", "content": "In private query answering, we are given a workload of linear queries $W \\in \\mathbb{R}^{m \\times n}$. We seek to approximate the answers $Wp$ as accurately as possible while satisfying differential privacy. A general recipe for private query answering is select-measure-reconstruct. Data-independent mechanisms following this recipe such as the various matrix mechanisms [2\u20136] select and measure a set of queries Qand reconstruct answers to W. Data-dependent mechanisms following this recipe such as MWEM [15] and various synthetic data mechanisms [7, 9, 10, 18, 19] typically maintain a model $\\hat{p}$ of the data distribution p that is improved iteratively by repeating the steps of select-measure-reconstruct and adaptively measuring queries that are poorly approximated by the current model $\\hat{p}$. The key idea is that it is often possible to obtain lower error by measuring a different set of queries Q than W and then using answers to Q to reconstruct answers for W. In this paper, we focus on the reconstruction subproblem and propose methods applicable to both the data-independent and data-dependent settings."}, {"title": "2.4 Query answer reconstruction", "content": "Reconstruction is a central subproblem to query answering. Suppose $y = Qp + \\xi$ is the a set of measurements. To reconstruct a data distribution, we seek $\\hat{p}$ such that $Q\\hat{p} \\approx y$. One method is to set $\\hat{p} = Q^+ y$ where $Q^+$ is the Moore-Penrose pseudoinverse. This method is used in the matrix mechanism [4] and HDMM [5] but is not tractable in high dimensions.\nOther reconstruction methods such as Private-PGM [12] and those used by the mechanisms PrivBayes [8], GEM [9], RAP [10], and RAP++ [11] represent $\\hat{p}$ through a parametric representation. These (usually) ensure tractability in high dimensions by using a compact representation, but introduce different tradeoffs. The parametric assumption typically restricts space of data distributions that can be represented [8\u201311]. Optimizing over the parameteric representation is often non-convex, potentially leading to suboptimal optimization [9\u201311]. Private-PGM solves a convex optimization problem and is closest to the methods of this paper. However its complexity depends on the measured queries and is still exponential in the worst case [12]; our methods will not have exponential complexity. All of these reconstruction methods only depend on the dataset through the noisy answers and, thus, satisfy the same degree of privacy as the answers by the post-processing property of differential privacy."}, {"title": "3 Efficient Marginal Reconstruction", "content": "In this section, we discuss methods for reconstructing answers to a workload of marginal queries given measurements of residuals. These methods utilize the structure of marginals and residuals to make reconstruction tractable and minimize error. Let $\\mathcal{W} \\subseteq 2^{[d]}$ and $M_{\\mathcal{W}} = (M_\\gamma)_{\\gamma \\in \\mathcal{W}}$ be the combined workload of marginals for all of the attribute sets in $\\mathcal{W}$ (e.g., all pairs or triples of attributes). Similarly, let $R_S = (R_\\tau)_{\\tau \\in S}$ represent a set of residual queries for all $\\tau$ in a collection S. Our goal is to estimate the marginal query answers $M_{\\mathcal{W}} p$ from noisy measurements $y = R_S p + \\xi$.\nResidualPlanner. ResidualPlanner [6] solves this problem elegantly in the matrix mechanism (i.e. data-independent) setting under Gaussian noise.\nLet $\\mathcal{W}^+ = {\\tau \\subseteq \\gamma : \\gamma \\in \\mathcal{W}}$ denote the downward closure of $\\mathcal{W}$. When $S = \\mathcal{W}^+$, the residual queries for S uniquely determine the marginals for $\\mathcal{W}$, i.e., there is an invertible linear transformation between $M_{\\mathcal{W}}$ and $R_S$. This yields the reconstruction approach in Alg. 1. In Line 2, the residual queries $R_\\tau$ are measured with Gaussian noise to yield $y_\\tau$. In Line 3, the marginals are reconstructed by applying the invertible transformation from residuals to marginals. This reconstruction is equivalent to setting $\\hat{\\mu}_\\gamma = M_\\gamma \\hat{p}$ where $\\hat{p} = R_S^+ y$ and $y = (y_\\tau)_{\\tau \\in S}$ by Proposition 1."}, {"title": "3.1 A general approach to reconstruction", "content": "We propose a reconstruction algorithm that, like the one in ResidualPlanner, is efficient and principled, but that applies in more general settings. Reconstruction in ResidualPlanner uses the invertible transformation from residuals to marginals. This restricts to the case where the measured queries exactly determine the workload queries in the absence of noise. To address the full range of applications, it is important to address the cases where workload queries are overdetermined, underdetermined, or both.\nOur proposed algorithm, ReM, is shown in Alg. 2. Compared to ResidualPlanner, the main differences are: (1) the set S of measured residuals is arbitrary, (2) a residual query can be measured any number of times with any noise distribution, (3) an optimization problem is solved for each $\\tau$ to estimate the true residual query answer $\\hat{\\alpha}_\\tau \\approx R_\\tau p$, (4) reconstruction uses the estimated residuals $\\hat{\\alpha}$ instead of the noisy measurements $y_\\tau$. The loss function $L_\\tau(\\alpha_\\tau)$ in Line 2 captures how well $\\alpha_\\tau$ explains the entire set of noisy measurements $\\{y_{\\tau,i}\\}_{i=1,...,k_\\tau}$. For example, a typical choice is $L_\\tau(\\alpha_\\tau) = \\Sigma_i^{k_\\tau} log p(y_{\\tau,i} | R_\\tau p = \\alpha_\\tau)$, the negative log-likelihood of the measurements.\nThe following result shows that solving the optimization problems in Line 2 is equivalent to finding a compact representation of a data distribution $\\hat{p}$ that minimizes a global reconstruction loss and then using $\\hat{p}$to answer each marginal query.\nTheorem 1. Suppose $\\hat{\\alpha}_\\tau$ minimizes $L_\\tau(\\alpha_\\tau)$ over $\\mathbb{R}^{m_\\tau}$ for each $\\tau \\in S$ and let $\\hat{\\alpha} = (\\hat{\\alpha}_\\tau)_{\\tau \\in S}$. Then $\\hat{p} = R_S^+ \\hat{\\alpha}$ is a global minimizer of the combined loss function $\\Sigma_{\\tau \\in S} L_\\tau(R_\\tau p)$ over $\\mathbb{R}^n$.\nThis result is proved (in Appendix C) by decomposing $\\hat{p}$ into orthogonal components in the row span of $R_\\tau$ for each $\\tau$ and showing that these components can be optimized separately. Proposition 1 then shows that $\\hat{\\mu}_\\gamma = M_\\gamma \\hat{p} = M_\\gamma R_S^+ \\hat{\\alpha}$ has the form given in Line 3 of the algorithm.\nReconstruction with local non-negativity. It is often possible to improve accuracy of a differentially private mechanism by forcing its outputs to satisfy known constraints [4, 10, 20]. For our problem, true marginals are non-negative, so it is desirable to enforce non-negativity in their private estimates. To enforce non-negativity, instead of solving the separate problems in Line 2 of Alg. 2, we solve the following combined problem over the full vector $\\alpha = (\\alpha_\\tau)_{\\tau \\in \\mathcal{W}^+}$ of residuals:\n$\\min_\\alpha \\sum_{\\tau \\in S} L_\\tau(\\alpha_\\tau) \\quad s.t. \\sum_{\\tau \\subseteq \\gamma} A_{\\gamma,\\tau} \\alpha_\\tau \\geq 0, \\quad \\forall \\gamma \\in \\mathcal{W}.$\nReconstruction of marginals then proceeds as in Line 3 of the algorithm. The constraints in Eq. (1) ensure that the reconstructed marginals will be non-negative. We refer to this as local non-negativity, since this problem solves for a data distribution $\\hat{p}$ that is non-negative for marginals in $\\mathcal{W}$ rather than a data distribution with non-negative entries, and we refer to this version of ReM as ReM-LNN (ReM with local non-negativity)."}, {"title": "3.2 Reconstruction under Gaussian noise", "content": "A special case of ReM that allows for efficient computation is when residuals are measured with Gaussian noise i.e., $y_{\\tau,i} = R_\\tau p + \\xi_{\\tau,i}$ where $\\xi_{\\tau,i} \\sim N(0, \\Sigma_{\\tau,i})$ and the loss function $L_\\tau(\\gamma)$ is the negative log-likelihood of the measurements. In this case, $\\hat{\\alpha} = (\\hat{\\alpha}_\\tau)_{\\tau \\in S}$ is the maximum likelihood estimate of the residual answers $\\alpha = (\\alpha_\\tau)_{\\tau \\in S}$. We refer to this setting as GReM-MLE (Gaussian ReM with Maximum Likelihood Estimation)."}, {"title": "4 Scalable MWEM with pseudoinverse reconstruction", "content": "The multiplicative weights exponential mechanism (MWEM) [15] is a canonical data-dependent mechanism that maintains a model $\\hat{p}$ of the data distribution p that is improved iteratively by adaptively measuring marginal queries that are poorly approximated by the current model $\\hat{p}$. MWEM has served as the foundation for many related data dependent mechanisms. A limitation of MWEM-style algorithms is that representing $\\hat{p}$, even implicitly, does not scale to high-dimensional data domains without adopting parametric assumptions. In this section, we propose an MWEM-style algorithm called Scalable MWEM (Alg. 3) that employs a standard reconstruction approach, the pseudoinverse of the measured marginal queries, but scales to high-dimensional data domains.\nIn general, the pseudoinverse is infeasible as a reconstruction method for large data domains. Computing the pseudoinverse $Q^+$ of an arbitrary query matrix Q scales exponentially in the number of attributes and linearly in size of the data vector. Moreover, even storing the reconstructed data vector $\\hat{p} = Q^+ y$ from noisy answers y in memory presents a limitation in practice. Scalable MWEM overcomes this computational hurdle by measuring marginals with isotropic noise and utilizing the decomposition of marginals into residuals in Theorem 2. It then applies an extension of Proposition 1 for efficient reconstruction. The extension is needed because the same residual may appear many times in the workload for Scalable MWEM. In Proposition 1, the residual workload $R_S = (R_\\tau)_{\\tau \\in S}$ consists of residual queries $R_\\tau$ for distinct $\\tau$. Extended reconstruction is described in Appendix E.\nScalable MWEM initializes by using a predetermined fraction of the privacy budget to measure the total query i.e. the 0-way marginal that counts the number of records in the dataset. Let $\\mathcal{W}$ be a workload of marginals e.g. all 3-way marginals. Then, for a fixed number of rounds, Scalable MWEM privately selects a marginal $\\gamma \\in \\mathcal{W}$ that is poorly approximated by the pseudo-inverse of the current measurements using the exponential mechanism. The selected marginal is measured with isotropic Gaussian noise and the noisy marginal is used to derive answers to the residual queries in the downward closure using the decomposition from Theorem 2. Being a full query answering mechanism rather than a just reconstruction method, let us note the result (proved in Appendix C) that Scalable MWEM satisfies differential privacy.\nTheorem 3. Scalable MWEM satisfies $(\\epsilon, \\delta)$-DP."}, {"title": "5 Experiments", "content": "In this section, we measure the utility of GReM-MLE and GReM-LNN by incorporating them as a post-processing step into two mechanisms for privately answering marginals: (1) ResidualPlanner [6], and (2) Scalable MWEM. Both mechanisms measure queries with Gaussian noise and reconstruct answers to all three-way marginals for the given data domain. For the ResidualPlanner experiment, we measure residuals for all subsets of three or fewer attributes with Gaussian noise scales determined by ResidualPlanner. For the Scalable MWEM experiment, we measure the total query and a subset of the 3-way marginals in the data domain with isotropic Gaussian noise and decompose the marginals into residuals using Theorem 2. We compare average $l_1$ error with respect to the reconstructed marginals of the base mechanism to post-processing with GReM-MLE, GReM-LNN, and two heuristics that enforce non-negativity by truncating negative values to zero (Trunc) and truncating to zero then rescaling (Trunc+Rescale). For the Scalable MWEM experiment, we additionally compare to a well-studied reconstruction mechanism Private-PGM [12]. We run these methods on four datasets of varying size and scale, Titanic [25], Adult [26], Salary [27], and Nist-Taxi [28], and various practical privacy regimes, $\\epsilon \\in \\{0.1, 0.31, 1, 3.16, 10\\}$ and $\\delta = 1 \\times 10^{-9}$. For each setting, we run five trials"}, {"title": "5.1 ResidualPlanner Results", "content": "Fig. 2 displays results for the ResidualPlanner experiment. Across all privacy budgets and datasets considered, GReM-LNN significantly reduces workload error on the reconstructed marginals compared to ResidualPlanner. Averaging over all settings and trials, GReM-LNN reduces ResidualPlanner workload error by a factor of 44.0\u00d7. With respect to the heuristic methods, GReM-LNN reconstructs marginals with lower error than Trunc across all privacy budgets and datasets. Except at the highest privacy regime considered $(\\epsilon = 0.1)$ on Titanic and Salary, GReM-LNN yields lower error than Trunc+Rescale. Averaging over all settings and trials, GReM-LNN has lower workload error by a factor of 17.6\u00d7 compared to Trunc and 3.2\u00d7 compared to Trunc+Rescale. Note that GReM-MLE is omitted from Fig. 2 since ResidualPlanner is the maximum likelihood reconstruction for its measurements. Appendix H reports results for this experiment with respect to $l_2$ workload error, which are consistent with the present findings."}, {"title": "5.2 Scalable MWEM Results", "content": "Fig. 3 displays results for the Scalable MWEM experiment for 30 rounds of measurements. Observe that Scalable MWEM runs for the settings considered, which would be infeasible for the original MWEM mechanism due to large data domains. Though partially obscured by the log scale of the plot, applying GReM-MLE as post-processing reduces workload error, particularly at low $\\epsilon$. Averaging over all settings and trials, GReM-MLE reduces Scalable MWEM workload error by a factor of 1.03x and by a factor of 1.06\u00d7 for the highest privacy regime considered $(\\epsilon = 0.1)$. Of all methods considered, Private-PGM yields the greatest reduction in workload error in settings where it ran; however, Private-PGM failed due to exceeding memory resources (20 GB) at 30 rounds on Adult, Salary, and Nist-Taxi in all trials. In Appendix H, we report the settings in which Private-PGM successfully ran across 10, 20, and 30 rounds of Scalable MWEM.\nWith respect to GReM-LNN, the findings from the prior experiment agree with the present results. Across all privacy budgets and datasets considered, GReM-LNN significantly reduces workload error on the reconstructed marginals compared to Scalable MWEM. Averaging over all settings and trials, GReM-LNN reduces Scalable MWEM workload error by a factor of 12.8\u00d7 and GREM-MLE workload error by a factor of 12.3\u00d7. Averaging over all settings and trials, GReM-LNN has lower workload error by a factor of 1.13\u00d7 compared to Trunc+Rescale. Note that we suppress results for Trunc due to space. Appendix H reports results for this experiment with respect to $l_2$ workload error, which are consistent with the present findings."}, {"title": "6 Discussion", "content": "We develop ReM, a method for reconstructing answers to marginal queries that scales to large data domains. We also introduce a tractable method to incorporate local non-negativity that significantly improves reconstruction quality. Finally, we show that ReM can be used to improve the existing query answering mechanisms ResidualPlanner and a scalable version of MWEM.\nLimitations. Many data-dependent query answering mechanisms also generate synthetic data. In some cases, practitioners utilize these mechanisms primarily in order to use the synthetic data for downstream tasks such as training a machine learning model [29, 30]. For those users, the fact that ReM does not generate synthetic data would be an important limitation. A broader limitation, which is common to many methods in this field, is lack of support for continuous data. Marginal and residual queries are only defined on discrete domains so continuous attributes need to be discretized."}, {"title": "A Kronecker Products", "content": "Kronecker products are a convenient way to represent highly structured matrices. Let A be an $n_a \\times m_a$ matrix $\\text{A}=\\begin{bmatrix} a_{1,1} & ... & a_{1,n_a} \\\\ ... & ... & ... \\\\ a_{m_a,1} & ... & a_{m_a,n_a} \\end{bmatrix}$ and B be a $n_b \\times m_b$ matrix. Then the Kronecker product of A with B is an $m_am_b \\times n_an_b$ matrix given by $A \\otimes B = \\begin{bmatrix} a_{1,1}B & ... & a_{1,n_a}B \\\\ ... & ... & ... \\\\ a_{m_a,1}B & ... & a_{m_a,n_a}B \\end{bmatrix}$.\nKronecker products provide a compact representation of matrices by representing exponentially-many entries of $A \\otimes B$ with linearly-many entries in A and B. For the Kronecker product of a sequence of matrices $A_1,..., A_d$, we use the notation\n$\\bigotimes_{i=1}^d A_i = A_1 \\otimes ... \\otimes A_d$\nThe Kronecker product is associative, so pairwise products can be taken in any order.\nKronecker products additionally possess useful algebraic properties. Let $(\\cdot)^+$ denote Moore-Penrose pseudoinverse.\nProposition 3. (Kronecker Product Properties) Let $A = \\bigotimes_{i=1}^d A_i$ and $B = \\bigotimes_{i=1}^d B_i$. Then the following properties hold:\n1. $A^T = \\bigotimes_{i=1}^d A_i^T$.\n2. $A^+ = \\bigotimes_{i=1}^d A_i^+$.\n3. If $A_i$ and $B_i$ are compatible for multiplication for $i = 1, ..., d$, then $AB = \\bigotimes_{i=1}^d A_i B_i$.\nThere are efficient algorithms for matrix-vector multiplication utilizing Kronecker structure such as Alg. 4. Let $A = \\bigotimes_{i=1}^l A_i$ be a Kronecker structured matrix where $A_i$ is a matrix of size $a_i \\times b_i$ so that A has size $a \\times b$ with $a = \\prod_{i=1}^l a_i$ and $b = \\prod_{i=1}^l b_i$"}, {"title": "B Differential Privacy", "content": "Let us begin by introducing a useful variant of differential privacy: zero-concentrated differential privacy (zCDP).\nDefinition 2. (Zero-Concentrated Differential Privacy; [32]) Let $M : \\mathcal{X} \\rightarrow \\mathcal{Y}$ be a randomized mechanism. For any neighboring datasets p, p' that differ by at most one record, denoted $p \\sim p'$, and all measurable subsets $S \\subseteq \\mathcal{Y}$: if $D_\\alpha(M(p)||M(p')) \\leq \\alpha \\rho$ for all $\\alpha \\in (1,\\infty)$ where $D_\\alpha$ is the \\alpha-Renyi divergence between distributions $M(p), M(p')$, then M satisfies $\\rho$-zCDP.\nWhile $(\\epsilon, \\delta)$-DP is a more common notion, it is often more convenient to work with zCDP. There exists a conversion from zCDP to $(\\epsilon, \\delta)$-DP."}, {"title": "C Proofs", "content": "Before proving Proposition 1", "closed-form": "n$\\noindent D_{(k)"}, 1, "n_k)(1_{n_k}u_{n_k}^T-n_k C_k)$,\nwhere $u_{n_k} = [n_k-1, n_k-2, ..., -1"], "complexity": "n* if $A_i$ is an arbitrary matrix", "follows": "p = \\Sigma_{\\tau \\in \\Omega} R_\\tau v_\\tau$ for $v_\\tau \\in \\mathbb{R}^{m_\\tau}$.\nProof. Let $p_\\tau = R_\\tau^T R_\\tau^+ p$ be the projection of p onto the row-space of $R_\\tau$.  By Proposition 9, $p = \\Sigma_{\\tau \\in \\Omega} p_\\tau$. Let $v_\\tau \\in \\mathbb{R}^{m_\\tau}$ be such that $p_\\tau = R_\\tau^T v_\\tau$. Since $R_\\tau$ is full row rank, $v_\\tau$ is unique.\nLemma 1.  Let us first observe that $\\alpha_\\tau"}