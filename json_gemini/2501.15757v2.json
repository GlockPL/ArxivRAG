{"title": "Efficiency Bottlenecks of Convolutional Kolmogorov-Arnold Networks: A Comprehensive Scrutiny with ImageNet, AlexNet, LeNet and Tabular Classification", "authors": ["Ashim Dahal", "Saydul Akbar Murad", "Nick Rahimi"], "abstract": "Algorithmic level developments like Convolutional Neural Networks, transformers, attention mechanism, Retrieval Augmented Generation and so on have changed Artificial Intelligence. Recent such development was observed by Kolmogorov-Arnold Networks that suggested to challenge the fundamental concept of a Neural Network, thus change Multilayer Perceptron, and Convolutional Neural Networks. They received a good reception in terms of scientific modeling, yet had some drawbacks in terms of efficiency. In this paper, we train Convolutional Kolmogorov Arnold Networks (CKANs) with the ImageNet-1k dataset with 1.3 million images, MNIST dataset with 60k images and a tabular biological science related MoA dataset and test the promise of CKANs in terms of FLOPS, Inference Time, number of trainable parameters and training time against the accuracy, precision, recall and f-1 score they produce against the standard industry practice on CNN models. We show that the CKANs perform fair yet slower than CNNs in small size dataset like MoA and MNIST but are not nearly comparable as the dataset gets larger and more complex like the Ima-geNet. The code implementation of this paper can be found on the link: https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning and Neural Networks are fundamental to Artificial Intelligence research [13], [14]. Recent advance-ments in Computer Vision, Natural Language Processing or Multimodal AI, all emerge from different sets of algorithms like Stable Diffusions [15], Convolutional Neural Networks (CNNs) [19], Long Short Term Memory (LSTMs) [16], Re-trieval Augmented Generation (RAG) [17], Transformers [18] and so on. At the heart of all these deep learning algorithms are the fundamental idea of a Neural Network; an algorithm that establishes a non-linear relationship between the independent and dependent variable making use of weights, biases and activation function. Such networks have advanced the applied research fields of Biomedical Imaging, text and image gen-eration, physical modeling, remote sensing, polymer science, etc.\nRecently, however, there have been new advancements in the field of Deep Learning that challenges to change the fundamental way researchers think about a Neural Network by challenging the traditionally utilized Multi Layer Perceptrons (MLPs) with Kolmogorov-Arnold Networks (KANs) [1]. The high level intuition of KANs is instead of training the weights and biases for any given representation and then passing it through the activation function, the entire activation function itself is trained. These functions are called B splines and are discussed in greater detail in section II. Proponents of KAN claim that KAN can yield similar if not better performance in terms of accuracy, precision and recall as compared to MLP with much less numbers of parameters in them while adversary research claim the exact opposite [25]\u2013[29]. This has left researchers divided on whether to adopt KANs in their current approach for reliable and tangible results.\nSpecific to the scope of this paper, we train and test Convolutional Kolmorogov Arnold Networks (CKANs) [12] on the traditionally CNN focused tasks. We adopt comparable sizes of AlexNet [30], LeNet [19] and 1-D tabular CNN [22] using CKAN implementation and compare the fidelity of the results directly to the CNN counterparts. Mainly, we focus on the Imagenet dataset [20], MNIST dataset [6] and the Mechanisms of Action (MOA) [21] dataset to compare the three models in their own dominant regions.\nOur findings lay a strong foundation in adoption of CKANS not only in terms of fidelity of the results but also on the adapt-ability of CKANs in terms of training and testing efficiency. Mainly this paper brings the following novel contributions to the field of Convolutional Neural Networks:\n\u2022 CKAN can provide comparable results to CNN on small dataset like MNIST\n\u2022 In a larger, modern-day medium, sized dataset like the ImageNet, CKAN cannot extrapolate or replicate its re-sults\n\u2022 Further refinement of the CKAN algorithm is required (which may not be possible with present computing) in order to make them a contender in the computer vision space\n\u2022 CKANS are comparatively better performing in sciences and tabular CNN implementation as compared to com-puter vision task, although they still lag behind the SoTA CNN models"}, {"title": "II. LITERATURE REVIEW", "content": "Liu et al [1] proposed Kolmogorov-Arnold Networks (KANs) based on the Kolmogorov-Arnold representation theo-rem. Established by Vladimir Arnold and Andrey Kolmogorov, the theorem states that if $f$ is a multivariate continuous function on a bounded domain, then $f$ can be written as a composition of multiple single variable functions summed together. Mathematically, $f : [0, 1]^n \\rightarrow R$,\n$f(X) = f(x_1, x_2,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q (\\sum_{p=1}^{n} \\Phi_{qp}(x_p)),$ (1)\nWhere $\\Phi_{qp} : [0,1] \\rightarrow R$ and $\\Phi_q : R \\rightarrow R$\nKolmogorov-Arnold representation shows that the only true multivariate function is addition, since every other function can be written as a sum of multiple univariate functions [1]. The problem that arises with such a simple representation of a network with just two layers of non linearity and $(2n + 1)$ number of parameters in the hidden layer is that these networks may not be easily trainable in practice [2], [3]. Hence, the authors modified the network in two fundamental ways, first by adding arbitrary width and depths; i.e, ignoring $(2n+1)$ in equation 1, second by choosing the B-spline [4] curve function to represent the univariate function $\\Phi$. The authors then arrive with the following representation for a KAN architecture with $L$ layers where the $l$th layer $\\Phi_l$ have shape $(n_{l+1},n_l)$:\n$KAN(x) = \\Phi_{L-1}.\\Phi_{L-2}...\\Phi_1.\\Phi_0.x$ (2)\nThe authors claimed KANs to be promising alternatives to MLPs, a standard form of representing Neural Networks in the field of Deep Learning. They showed proofs on a few datasets that made KANs outperform MLPs in terms of accuracy and interpretability on small scale AI alongside science tasks. The question to the efficacy of KANs as a viable option to MLPS come with the type of the dataset that the authors choose to use. For their demonstration purposes they choose to use toy datasets which could fail to encompass the huge amount of complexities and variability that come with real world complex datasets. The authors especially highlight the effectiveness of KANs in mathematical modeling tasks with some real world problems while sticking to toy datasets for other tasks.\nThe reasoning behind this choice is revealed by Yu et al [5] who propose that comparable MLPs are only inferior to KANs in symbolic formula representation tasks while being superior in other machine learning, computer vision, natural language processing and audio processing tasks. The authors deduced the number of parameters required for any particular KAN or MLP layer by using the formula in eq 3.\n$parameters_{KAN} = (d_{in} * d_{out}) * (G + K + 3) + d_{out}$ (3)\n$parameters_{MLP} = (d_{in} * d_{out}) + d_{out}$ (4)\nWhere $d_{in}$ and $d_{out}$ are the dimensions of the input and output layer in the network, G is the number of spline intervals and K represents the order of the polynomial of the B-spline function.\nFrom direct comparisons of the equations 3 and 4, it can be observed that if the number of input and output dimensions for the network layer were to be kept the same then the number of trainable parameters for KANs would grow much higher. For fairer comparisons, it thus becomes necessary to account for the higher number of trainable parameters in KANs to then reduce the input and output dimension for each layer in the KAN accordingly to maintain a fair playing ground with MLP.\nSpecific to the interest of this paper, in the department of computer vision, Yu et al [5] trained KAN and MLP with 8 standard computer vision datasets: MNIST [6], EMNIST-Balanced [7], EMNIST-Letters, FMNIST [8], KMNIST [9], CIFAR10 [10], CIFAR100 and SVHN [11]. The hidden layer widths for MLP were 32, 64, 128, 256, 512 or 1024 while for KAN were 2, 4, 8 or 16 with respective B-spline grid 3, 5, 10 or 20 and B-spline degrees were 2, 3 or 5. Upon comparisons of these two models in the given 8 datasets, the authors concluded that KANs even with the highest number of parameters cannot surpass the accuracy of an MLP with the lowest number of parameters; while sometimes performing even worse than having a lower number of parameters in its own category.\nAlthough authors from [5] provide concluding evidence on the superiority of MLPs over KANs in the domain of computer vision, the authors fail to accommodate for the new advancement on CNNs based on KANs proposed by Bodner et al [12]. The authors from [12] propose a new method of making CNN layers by decomposing the kernels of the CNN architecture as a KAN representation presented in [1]; i.e instead of learning the weights and biases for each of the filters in the CNN layer, the authors propose to learn the B-spline activation function like a KAN. The authors describe an image as follows.\n$image_{i,j} = a_{i,j},$ (5)\nwhere $a_{i,j}$ is a matrix of pixel values of size $m_1, m_2$ (height, width)\nThen a KAN based kernel K is described as $K\\in R^{N_2,N_1}$ which is a collection of B-splines defined as follows.\n$K_{i,j} = \\Phi_{i,j}$ where $\\Phi_{i,j}$ is a matrix of B-splines of size $n_2, N_1$ (6)\nThen from the definition of a CNN [19], we can write the (i, j)th entry of the feature map is given by the following general formula:\n$(image *k)_{i,j} = \\sum_x^{M_1}\\sum_y^{M_2}\\Phi_{xy}(a_{i-x i-y})$ (7)"}, {"title": "III. METHODOLOGY", "content": "Our research procedure is highlighted on the Fig. 1. Based on the Fig. 1 and the main objective of the paper, the method-ology section is best divided into three parts: Computer Vision, Tabular Classification and Evaluation Metrics, each equipped with their own subsection discussing data preprocessing and hyperparameter selection.\nA. Computer Vision\nTwo architectures based on popular CNN architectures, namely LeNet and AlexNet, were trained using KAN based convolution layers; for convention let's name them LeNet KAN and AlexNet KAN. The only difference between the AlexNet and LeNet architecture from their CKAN counterparts is the number of filters while the overall architecture remains same.\n1) Dataset and preprocessing for Computer Vision: To maintain consistency with the original paper LeNet KAN was trained on the MNIST [6] and AlexNet KAN was trained on the ImageNet [20] dataset. The MNIST dataset is a collection of 60,000 grayscale handwritten digits belonging to 10 classes whereas the ImageNet is the collection of 1.2 million images belonging to 1000 classes. In order to maintain the integrity of the original research and a fair comparison between the KAN and MLP counterpart, we didn't do additional preprocessing on either of the dataset rather than to normalize them, which was automatically handled by pytorch.\n2) Hyperparameter Selection: For a standard CKAN by [12] the number of parameters as compared to that of a standard CNN by equations 3 and 4 would be four times higher. Since the LeNet architecture doesn't have all number of filters in the order of 2n, the first layer had to be rounded up to maintain at least 2 filters on the layer. Adam [23] was chosen as the optimizer with the same learning rate.\nB. Tabular CNN\nGuo [22] has shown their 1-Dimensional CNN architecture to be used on a tabular dataset, which also won the second prize on the Kaggle's MOA competition [21]. Given that tabular dataset doesn't have similar properties next to one another and CNNs are built to recognize similar patterns next to one another, we follow their specific implementation where the input table row is projected into a vector where each of the adjacent data comes as a closely related feature representation of one entry in the table row. Then we apply the 1-D CNN layer on the generated vector which would make sure that no adjacent data of vastly different nature would have to go through the CNN filter at once as a closely related pattern.\n1) Dataset for Tabular Classification: In order to replicate the same result as Guo [22], the same dataset Mechanisms of Action (MoA) [21] was chosen for this task. In order to have a fairer comparison, all the data preprocessing steps followed by Guo were also replicated for the KAN based architecture.\n2) Hyperparameter Selection: Since reducing the number of filters by one fourth in this case would not have a significant impact on the model's width or depth, we decided to match the number of parameters and reduce the number of layers in each layer in KAN by one fourth. This would result in the model architecture for Tabular CKAN to be the same as Fig. 2, just with fewer (one fourth) number of filters or kernels in it's convolutions.\nThe approach for the loss function chosen was also with the same rationale. Custom weighted loss function proposed by Guo [22] was used on both the models with the same learning rate and weight decay for the Adam [23] optimizer.\nThese hyperparameters are more aligned towards direct comparison of a CKAN vs CNN in a single dimension, whereas the hyperparameters chosen in the section III-A2 may tend to favor a slighlt higher number of trainable parameters for CKAN over CNN (in LeNet) but this decision wouldn't ultimately matter because the models fail to justify the higher learning capacity.\nC. Evaluation Metrics\nGiven the constraint of 1000 classes in the ImageNet dataset, it is simply not possible to treat it as the same as other relatively smaller dataset. Therefore there is a need to find metrics beyond classification matrix and report that encap-sulates the depth and complexities of the results produced by the models. The following metrics were deployed to evaluate the results produced by the models.\n1) Top-5 Accuracy, Precision, Recall and F-1 Score: We treat the top 5 classes in the output probability distribution as"}, {"title": "IV. RESULTS", "content": "The results from the metrics evaluated as per the method-ology fig 1 is presented on this section of the paper. We note that before evaluating the results of each models, we need to evaluate the inference and training efficiency of the approaches. Thus, the section is presented on the following way: section IV-A discusses the efficiency of the models including FLOPS, training and inference time, and number of trainable parameters, section IV-B, IV-D, IV-C discusses the rest of the metrics like accuracy, precision, recall and f-1 score for Tabular CNN, AlexNet and LeNet similarly.\nA. Inference and Training Time\nThe first and most important consideration in this section is time efficiency. The entire theme of the results observed during the experiment revolved around the throughput of CKAN compared to CNN per unit time. Within the context of the paper, we gathered all metrics presented in Table II and normalized the metrics to compare and contrast on how each of them performed against each another and present on fig 4. It is evident that the discussion to be followed is regarding the poor performance of the KAN models. For the plot Loss, FLOPS, Inference Time and Parameters are inverted after normalization to show the widespread dominance of CNN models against CKAN. Even while having lesser number of parameters in AlexNet KAN and Tabular CKAN, the inference time in the radar plot shows the models are not optimized enough for modern classification tasks fig 4.\nThe AlexNet KAN model took 48 days to train for 100 epochs. In contrast, the standard AlexNet model, by hand calculation of the original report, would have required at max (in worst case scenario) 3 days time to complete the same number of epochs. This represents an increase of almost 16 times in time consumption for the KAN model. While the inference time for single image is 4 times more in the KAN counterpart of the same architecture (0.0074s vs 0.0018s), the number of trainable parameters is just 63%. This indicates that even with less number of parameters the inference is terrible on AlexNet KAN. Even so for the FLOPS, where it is more than 2x that of AlexNet on PyTorch (1,611,568,352 vs 714,197,696).\nSimilarly, for the LeNet counterpart, the LeNet KAN model took 981.45s to complete 50 epochs, while the standard LeNet model took 888.77s for the same number of epochs. This is a time difference of 92.68s. The inference time for a single input was calculated to be 0.003s and 0.0007s for the LeNet KAN and LeNet respectively. This shows an increase of 3.28x just on single image inference time. Similar is the case for the FLOPS count; with just a few thousands more number of trainable parameters from the b splines, 82,128 in LeNet KAN as compared to 61,750 in LeNet, the FLOPS increased by 7\ntimes from 429128 in LeNet to 3298728 in LeNet KAN.\nA comparable pattern was observed for the simpler Tabular CNN models. The lightweight Tabular CNN KAN model required 10646.07s to train for 15 seeds, 4 folds of 24 epochs each, while the standard lightweight Tabular CNN model only needed 6450.5s for the same setting. This shows that the KAN model took almost 1.65 times longer. As with the other models the inference time was also lagging behind with the Tabular CNN finishing single inference at 0.00004s and the CKAN counterpart at 0.0001s. Interesting observation to be noted from the Table II is even with 1.25 times more number of trainable parameters, the Tabular CNN performed better in FLOPS (2x less), inference and overall training time.\nThe rest of the parameters in Table II are discussed per model in the following sections.\nB. Tabular CNN vs Tabular CKAN\nGiven the scientific complexity yet fairly straightforward and simple dataset, both models performed exceptionally well. The Tabular CNN, the adaptation of [22] which we then evaluated under our performance metrics, still performed better in terms of the fundamental metrics like accuracy, recall and f-1 score as compared to its CKAN counterpart (47.61, 26.95,2 8.36 vs 45.09, 24.30, 25.29). In precision metric however, the Tabular CKAN showed better results with score of 98.13 vs 94.66. Apart from that the results from CKAN were underwhelming compared to the time factor discussed in section IV-A.\nC. LeNet KAN vs LeNet\nResults observed under this comparison were less abrupt as those for compvis task on Alexnet discussed in section IV-D. This could be attributed towards MNIST being a smaller dataset. Nonetheless, with a comparable architecture the met-rics obtained by both the models were either comparable or the same in terms of accuracy, precision, recall and f-1 score (98.81, 98.79, 98.79, 97.79 vs 98.89, 98.88, 98.87, 98.88 for LeNet KAN and LeNet respectively). The only downside noted for the LeNet KAN model was for its efficiency already discussed above.\nD. AlexNet KAN vs AlexNet\nWith the amount of FLOPS count the results were not justifiable in case of the AlexNet KAN. It surged behind the AlexNet on every single metric we tested both models upon from Top-5 metrics (67.72, 67.92, 67.72, 66.02 vs 79.07, 78.66, 79.07, 78.00: lower is AlexNet KAN) or Top-1 metrics (42.79, 42.91, 42.79, 41.72 vs 56.62, 56.29, 56.62, 55.75; again lower is AlexNet KAN), the AlexNet KAN method doesn't match the figures of the CNN version even with double the FLOPS count and quadruple the inference time reported in table II."}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "From the analysis of the results section, one thing is evident from our paper: Convolutional Kolmogorov Arnold Networks perform fair on small, less complex datasets. Although this performance, either in terms of efficiency or raw metrics, is not comparable to it's CNN counterparts, with optimizations like Radial Bias Functions CKANS could offer potential lightweight solutions in future for small dataset and scientific tasks. For relatively larger datasets like ImageNet, however, CKANS couldn't replicate their results. The training process is huge and time consuming. The researchers would be un-sure about the initial findings until they have spent days or weeks training a model that is as good as flipping a coin. Although KANs were supposed to become the new SoTA replacing MLPs altogether, their current application is limited to sciences, tabular style datasets, and their generalization over larger datasets isn't optimized and well refined to be considered a contender against CNNs."}]}