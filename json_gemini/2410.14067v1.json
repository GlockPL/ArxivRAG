{"title": "Provable Benefits of Complex Parameterizations for Structured State Space Models", "authors": ["Yuval Ran-Milo", "Eden Lumbroso", "Edo Cohen-Karlik", "Raja Giryes", "Amir Globerson", "Nadav Cohen"], "abstract": "Structured state space models (SSMs), the core engine behind prominent neu- ral networks such as S4 and Mamba, are linear dynamical systems adhering to a specified structure, most notably diagonal. In contrast to typical neural network modules, whose parameterizations are real, SSMs often use complex parameter- izations. Theoretically explaining the benefits of complex parameterizations for SSMs is an open problem. The current paper takes a step towards its resolution, by establishing formal gaps between real and complex diagonal SSMs. Firstly, we prove that while a moderate dimension suffices in order for a complex SSM to express all mappings of a real SSM, a much higher dimension is needed for a real SSM to express mappings of a complex SSM. Secondly, we prove that even if the dimension of a real SSM is high enough to express a given mapping, typically, doing so requires the parameters of the real SSM to hold exponentially large val- ues, which cannot be learned in practice. In contrast, a complex SSM can express any given mapping with moderate parameter values. Experiments corroborate our theory, and suggest a potential extension of the theory that accounts for selectivity, a new architectural feature yielding state of the art performance.", "sections": [{"title": "Introduction", "content": "Structured state space models (SSMs) are the core engine behind prominent neural network archi- tectures such as S4 [21], Mamba [20], LRU [41], Mega [37], S5 [50] and more [23, 31, 38, 34]. In their typical form, SSMs can be thought of as single-input single-output linear dynami- cal systems, wherein the state transition matrix has a specified structure, most notably diago- nal [22, 23, 41, 50, 37, 20]. A salient characteristic of SSMs is that their parameterizations are often complex (take values in C), in contrast to typical neural network modules whose parameterizations are conventionally real (take values in R).\nThere has been mixed evidence regarding benefits of complex parameterizations over real param- eterizations for SSMs. Some prior works have demonstrated that complex parameterizations are"}, {"title": "Preliminaries", "content": ""}, {"title": "Notations", "content": "We use non-boldface lowercase letters for denoting scalars (e.g. $a \\in \\mathbb{R}$, $c \\in \\mathbb{C}$, $n \\in \\mathbb{N}$), boldface lowercase letters for denoting vectors (e.g. $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{v} \\in \\mathbb{C}^n$), and non-boldface uppercase letters for denoting matrices (e.g. $A \\in \\mathbb{R}^{n,m}$, $B \\in \\mathbb{C}^{n,m}$). Series (finite or infinite) of scalars, vectors or matrices are viewed as functions of time and denoted accordingly (e.g. $(x(t) \\in \\mathbb{R}^n)_{t \\in \\mathbb{N}}$, $(A(t) \\in \\mathbb{C}^{n,m})_{t=1,2,...,k}$). For series of scalars, we also use as notation boldface uppercase letters (e.g. $S = (s(t) \\in \\mathbb{R})_{t \\in \\mathbb{N}}$, $I = (i(t) \\in \\mathbb{C})_{t=1,2,...,k}$). Given $k \\in \\mathbb{N}$ and a series of scalars $S$ whose length is greater than or equal to $k$, we use $S_k$ to denote the $k$'th element of $S$, and $S_{:k}$ to denote the truncation of $S$ to length $k$ (i.e. the series comprising the first $k$ elements of $S$), allowing ourselves to regard this truncated series as a vector of dimension $k$. For $k \\in \\mathbb{N} \\cup {\\infty}$, we use $[k]$ as shorthand for the set ${1, 2, ...,k}$. Given a complex number $c\\in \\mathbb{C}$, we denote its magnitude by $|c| \\in \\mathbb{R}_{>0}$, its phase by $\\text{arg}(c) \\in [0, 2\\pi)$, its real part by $\\Re(c) \\in \\mathbb{R}$, and its imaginary part by $\\Im(c) \\in \\mathbb{R}$ (meaning $c = |c| \\exp(i \\text{arg}(c)) = \\Re(c) + i\\Im(c)$). We let $\\mathbf{0}$ and $\\mathbf{1}$ stand for vectors whose entries are all zeros and all ones, respectively, with dimension to be inferred from context. The Hadamard (element-"}, {"title": "Structured State Space Models", "content": "Let $\\mathbb{K} = \\mathbb{R}$ or $\\mathbb{K} = \\mathbb{C}$. A structured state space model (SSM) of dimension $n \\in \\mathbb{N}$ is parameterized by three matrices: $A \\in \\mathbb{K}^{n,n}$, a state transition matrix, which adheres to a predefined structure (e.g. is constrained to be diagonal); $B \\in \\mathbb{K}^{n,1}$, an input matrix; and $C \\in \\mathbb{K}^{1,n}$, an output matrix. Given the values of $A, B$ and $C$, the SSM realizes a mapping $\\Phi_{n,(A,B,C)}: \\mathbb{R}^{\\mathbb{N}} \\to \\mathbb{R}^{\\mathbb{N}}$ which receives as input a real scalar series $(u(t))_{t \\in \\mathbb{N}}$, and produces as output a real scalar series $(y(t))_{t \\in \\mathbb{N}}$ defined through the following recursive formula:\n\n$x(t) = Ax(t - 1) + Bu(t), \\quad y(t) = \\Re(Cx(t)), \\quad t \\in \\mathbb{N},$  \nwhere $(x(t) \\in \\mathbb{K}^n)_{t \\in \\mathbb{N}}$ is a vector series of states, and $x(0) = \\mathbf{0} \\in \\mathbb{K}^n$. If $\\mathbb{K} = \\mathbb{R}$ we say that the SSM is real, and if $\\mathbb{K} = \\mathbb{C}$ we say that it is complex. We refer to the SSM as stable if all eigenvalues of $A$ have magnitude strictly smaller than one; otherwise we refer to the SSM as unstable. For convenience, we often identify an SSM with the triplet $(A, B, C)$ holding its parameter matrices, and regard the (single column) matrices $B$ and $C^T$ as vectors.\nPerhaps the most prominent form of structure imposed on SSMs is stable diagonality, i.e. stability combined with diagonality [22, 23, 41, 37, 20]. Accordingly, unless stated otherwise, we assume that the state transition matrix $A$ of an SSM is diagonal and has entries with magnitude strictly smaller than one."}, {"title": "Linear Time-Invariant Mappings", "content": "Let $\\phi: \\mathbb{R}^{\\mathbb{N}} \\to \\mathbb{R}^{\\mathbb{N}}$ be a mapping from the space of (infinite) real scalar series to itself. We say that $\\phi(\\cdot)$ is linear if for all $a \\in \\mathbb{R}$ and $S, \\tilde{S} \\in \\mathbb{R}^{\\mathbb{N}}$ it holds that $\\phi(aS + \\tilde{S}) = a\\phi(S) + \\phi(\\tilde{S})$. For every $k \\in \\mathbb{N}$, define the $k$ step delay $\\delta_k: \\mathbb{R}^{\\mathbb{N}} \\to \\mathbb{R}^{\\mathbb{N}}$ to be the operator that adds $k$ preceding zeros to the series it receives as input. We say that the mapping $\\phi(\\cdot)$ is linear time-invariant (LTI) if it is linear, and it commutes with $\\delta_k(\\cdot)$ (meaning $\\phi(\\delta_k(\\cdot)) = \\delta_k(\\phi(\\cdot))$) for every $k \\in \\mathbb{N}$. It is well known [40] that if $\\phi(\\cdot)$ is LTI then it is given by $\\phi(S) = S * \\phi(I)$, where $I := (1,0,0, ...) \\in \\mathbb{R}^{\\mathbb{N}}$ is the impulse series, and $\\phi(I)$ is referred to as the impulse response of $\\phi(\\cdot)$. Conversely, for any $R \\in \\mathbb{R}^{\\mathbb{N}}$, the mapping defined by $S \\to S * R$ is LTI, and its impulse response is $R$.\nWe will identify LTI mappings with their impulse responses. More specifically, for any $k\\in \\mathbb{N}$, we identify an LTI mapping up to time $k$, with the truncation of its impulse response to length $k$. Accordingly, for any LTI mappings $\\phi(\\cdot)$, $\\tilde{\\phi}(\\cdot)$ and any $\\epsilon \\in \\mathbb{R}_{>0}$, we say that $\\phi(\\cdot)$ $\\epsilon$-approximates $\\tilde{\\phi}(\\cdot)$ up to time $k$ if $|\\phi(I)_{:k} - \\tilde{\\phi}(I)_{:k}|_1 \\leq \\epsilon$. If the latter inequality holds with $\\epsilon = 0$, we also say that $\\phi(\\cdot)$ matches $\\tilde{\\phi}(\\cdot)$ up to time $k$.\nLet $(A, B, C)$ be an SSM of dimension $n \\in \\mathbb{N}$, realizing the mapping $\\Phi_{n,(A,B,C)}: \\mathbb{R}^{\\mathbb{N}} \\to \\mathbb{R}^{\\mathbb{N}}$ (see Section 2.2). It is straightforward to see that $\\Phi_{n,(A,B,C)}(\\cdot)$ is LTI, and that its impulse response is given by:\n\n$\\Phi_{n, (A,B,C)} (I) = (\\Re(CB), \\Re(CAB), \\Re(CA^2B), ...).$"}, {"title": "Theoretical Analysis", "content": "Throughout this section, we consider a real SSM ($A_\\mathbb{R}, B_\\mathbb{R}, C_\\mathbb{R}$) of dimension $n_\\mathbb{R}$ realizing the map- ping $\\Phi_{n_\\mathbb{R},(A_\\mathbb{R}, B_\\mathbb{R},C_\\mathbb{R})}(\\cdot)$, and a complex SSM ($A_\\mathbb{C}, B_\\mathbb{C}, C_\\mathbb{C}$) of dimension $n_\\mathbb{C}$ realizing the mapping $\\Phi_{n_\\mathbb{C},(A_\\mathbb{C}, B_\\mathbb{C},C_\\mathbb{C})}(\\cdot)$ (see Section 2.2)."}, {"title": "Universality", "content": "It is known (see, e.g., [14]) that the real SSM is universal, in the sense that it can precisely express any LTI mapping up to any time $t$ when its dimension is equal to or greater than $t$. Trivially, this implies the same for the complex SSM. Proposition 1 below formalizes these facts for completeness.\nProposition 1. Let $\\phi : \\mathbb{R}^{\\mathbb{N}} \\to \\mathbb{R}^{\\mathbb{N}}$ be an arbitrary LTI mapping, and let $t \\in \\mathbb{N}$. Then, the following holds for both $\\mathbb{K} = \\mathbb{R}$ and $\\mathbb{K} = \\mathbb{C}$. If $n_\\mathbb{K} > t$, there exist assignments for $(A_\\mathbb{K}, B_\\mathbb{K}, C_\\mathbb{K})$ with which $\\Phi_{n_\\mathbb{K},(A_\\mathbb{K}, B_\\mathbb{K},C_\\mathbb{K})}(\\cdot)$ matches $\\phi(\\cdot)$ up to time $t$."}, {"title": "Separation in Expressiveness", "content": "Proposition 2 and Theorem 1 below together establish that, although both the real and complex SSMs are universal (see Section 3.1), there is a strong separation between the two in terms of ex- pressiveness. Proposition 2 formalizes an obvious fact: all mappings expressible by the real SSM can be precisely expressed (up to any time) by the complex SSM whenever $n_\\mathbb{C} \\geq n_\\mathbb{R}$ (i.e., whenever the dimension of the complex SSM is equal to or greater than the dimension of the real SSM).\nTheorem 1 proves a much less obvious result: for any $n_\\mathbb{C}$, there are various oscillatory mappings (i.e. mappings with oscillatory impulse responses) expressible by the complex SSM which cannot be approximately expressed up to time $t$ by the real SSM unless $n_\\mathbb{R}$ is on the order of $t$, which may be arbitrarily larger than $n_\\mathbb{C}$.\nProposition 2. Consider an arbitrary assignment for ($A_\\mathbb{R}, B_\\mathbb{R}, C_\\mathbb{R}$), and assume that $n_\\mathbb{C} \\geq n_\\mathbb{R}$. Then, there exist assignments for ($A_\\mathbb{C}, B_\\mathbb{C}, C_\\mathbb{C}$) with which $\\Phi_{n_\\mathbb{C},(A_\\mathbb{C}, B_\\mathbb{C},C_\\mathbb{C})}(\\cdot) = \\Phi_{n_\\mathbb{R},(A_\\mathbb{R}, B_\\mathbb{R},C_\\mathbb{R})}(\\cdot)$."}, {"title": "Separation in Practical Learnability", "content": "Let $\\phi: \\mathbb{R}^{\\mathbb{N}} \\to \\mathbb{R}^{\\mathbb{N}}$ be an LTI mapping with bounded impulse response, which we would like to $\\epsilon$-approximate up to time $t$ for some $\\epsilon \\in \\mathbb{R}_{\\geq 0}$ and $t \\in \\mathbb{N}$. Assume that the dimensions of the real and complex SSMs are greater than or equal to $t$. By Proposition 1, both the real and complex SSMs can express mappings that match $\\phi(\\cdot)$ up to time $t$, and in particular that achieve the desired approximation. The current subsection establishes that despite this parity in terms of expressiveness, there is a strong separation between the real and complex SSMs in terms of practical learnability. Section 3.3.1 proves that under a mild condition on $\\phi(\\cdot)$, in order for the real SSM to achieve the desired approximation, either its dimension or the magnitude of its parameters must be exponential in $t$. Section 3.3.2 then shows that such exponentiality impedes practical learning via gradient descent. Finally, Section 3.3.3 proves that in stark contrast to the real SSM, the complex SSM can achieve the desired approximation with dimension and parameter magnitudes that are at most linear in $t$."}, {"title": "Real Parameterizations Suffer from Exponentiality", "content": "Definition 1 below formalizes the notion of forward difference for a real scalar series a discrete analogue of derivative for a differentiable real function. Our main theoretical result, Theorem 2, then establishes that if forward differences associated with $\\phi(I)$\u2014the impulse response of $\\phi(\\cdot)$\u2014satisfy a certain condition, then in order for the real SSM to express a mapping that $\\epsilon$-approximates $\\phi(\\cdot)$ up to time $t$, either the dimension of the real SSM $n_\\mathbb{R}$ or the magnitude of its parameters ($B_\\mathbb{R}, C_\\mathbb{R}$) must be exponential in $t$. Roughly speaking, the aforementioned condition on forward differences associated with $\\phi(I)$ is that there exists some $d \\in \\Theta(t)$ such that the $d$'th forward difference of the restriction of $\\phi(I)$ to either odd or even elements has magnitude greater than $2^{d}\\epsilon$. Perhaps surprisingly, this condition is especially mild, as the magnitude of the $d$'th forward difference of a real scalar series typically scales exponentially with $d$. Several important cases where the condition is satisfied are presented below.\nDefinition 1. Let $S$ be a real scalar series of length $k \\in \\mathbb{N}\\cup {0}$. The forward difference of $S$, denoted $S^{(1)}$, is the scalar series of length $k - 1$ whose $m$'th element, for $m \\in [k - 1]$, is given by $S_{m+1} - S_m$. For $d \\in {2, 3, ..., k - 1}$, the $d$'th forward difference of $S$, denoted $S^{(d)}$, is recursively defined to be the forward difference of $S^{(d-1)}$.\nTheorem 2. Suppose $\\Phi_{n_\\mathbb{R},(A_\\mathbb{R}, B_\\mathbb{R},C_\\mathbb{R})}(\\cdot)$ $\\epsilon$-approximates $\\phi(\\cdot)$ up to time $t$. Then:\n\n$n_\\mathbb{R} ||C^T_\\mathbb{R} B_\\mathbb{R}||_\\infty \\geq \\max_{\\sigma \\in {\\text{odd}, \\text{even}}} \\max_{d,m \\in \\mathbb{N}: d+m \\leq \\lfloor t/2 \\rfloor} {2^{d+2\\min{d,m}} (2^{-d}((\\phi(I)|_{\\sigma})^{(d)})_m - \\epsilon)},$  \nwhere: $(\\phi(I)|_{\\text{odd}})^{(d)}$ and $(\\phi(I)|_{\\text{even}})^{(d)}$ are the restrictions of the impulse response $\\phi(I)$ to odd and even elements, respectively; and $((\\phi(I)|_{\\text{odd}})^{(d)})_m$ and $((\\phi(I)|_{\\text{even}})^{(d)})_m$ stand for the $m$'th element of the $d$'th forward difference of $\\phi(I)|_{\\text{odd}}$ and $\\phi(I)|_{\\text{even}}$, respectively."}, {"title": "Exponentiality Impedes Practical Learning", "content": "For any value of $t$ that is not especially small, exponentiality in $t$ for the real SSM as put forth in Section 3.3.1\u2014i.e., exponentiality in $t$ of the dimension of the real SSM $n_\\mathbb{R}$ or the magnitude of its parameters ($B_\\mathbb{R}, C_\\mathbb{R}$)\u2014impedes practical learning. This impediment is obvious in the case where $n_\\mathbb{R}$ is exponential in $t$ (in this case, it is impractical to even store the parameters of the real SSM, let alone learn them). Below we treat the complementary case, i.e. we show that learning is impractical when the required values for the parameters ($B_\\mathbb{R}, C_\\mathbb{R}$) are exponential in $t$. The results of Section 3.3.1 therefore imply that the real SSM cannot practically learn a mapping that $\\epsilon$-approximates $\\phi(\\cdot)$ up to time $t$ under important choices of $\\phi(\\cdot)$.\nThere are multiple aspects in which learning the parameters of the real SSM, i.e. ($A_\\mathbb{R}, B_\\mathbb{R}, C_\\mathbb{R}$), is impractical when the required values for ($B_\\mathbb{R}, C_\\mathbb{R}$) are exponential in $t$. We will account for two such aspects: the number of iterations required by gradient descent; and the precision (number of"}, {"title": "Experiments", "content": "This section presents controlled experiments corroborating our theory. Section 4.1 demonstrates that complex parameterizations significantly improve performance of SSMs in the theoretically analyzed setting. Section 4.2 shows that this improvement extends to a real-world setting beyond our theory. Finally, Section 4.3 evaluates SSMs with selectivity\u2014a new architectural feature yielding state of the art performance [20, 31, 4, 57]. The experiments with selectivity portray a nuanced picture: complex parameterizations are beneficial for some tasks, whereas for others, selectivity allows real parameterizations to achieve comparable (and in some cases better) performance. These findings"}, {"title": "Theoretically Analyzed Setting", "content": "To empirically demonstrate our theoretical findings, we trained the analyzed real and complex SSMs (see Section 2.2) to approximate up to time $t$ the mapping $\\phi(\\cdot)$ (see Section 2.3), with $t$ varying and with the following choices for $\\phi(\\cdot)$: the canonical copy (delay) mapping from Corollary 1; the random (generic) mapping from Corollary 2; and the basic oscillatory mapping from Corollary 3. Throughout, the dimension of the real or complex SSM ($N_\\mathbb{R}$ or $n_\\mathbb{C}$, respectively) was set to at least $t$, which, by universality (Section 3.1), implies that the SSM can express a mapping that precisely matches $\\phi(\\cdot)$ up to time $t$. Our theory (Section 3.3) establishes that despite this parity between the real and complex SSMs in terms of expressiveness, there is a strong separation between the SSMs in terms of practical learnability. In particular, with the above choices of $\\phi(\\cdot)$, there are exponential barriers that apply only to the real SSM, and prevent its training from being able to yield a mapping that closely approximates $\\phi(\\cdot)$ up to time $t$. Tables 1 and 2 present results obtained with the real and complex SSMs, respectively. They confirm the predictions of our theory."}, {"title": "Real-World Setting", "content": "To empirically demonstrate the benefits of complex parameterizations for SSMs in settings beyond our theory, we evaluated the prominent S4 neural network architecture [21] on the real-world se- quential CIFAR-10 dataset from the widely recognized Long Range Arena benchmark [52]. Our implementation is based on the official S4 repository, where unless stated otherwise, hyperpa- rameters (pertaining to the neural network architecture and its training) were kept at their default values. A single run with complex parameterization yielded a test accuracy of 89.10%, significantly higher than the highest test accuracy of 78.27% attained with real parameterization across three random seeds. Modifying the optimizer and initialization scheme with the real parameterization did not improve the test accuracy. The overarching conclusion from our theory-namely, that SSMs benefit from complex parameterizations thus extends to this real- world setting."}, {"title": "Selectivity", "content": "A new architectural feature for SSMs that yields state of the art performance is selectivity [20, 31, 4, 57]. In its original form proposed as part of the Mamba neural network architecture [20] selectivity amounts to replacing the parameters $B$ and $C$ (see Section 2.2), as well as an additional discretization parameter $\\Delta \\in \\mathbb{R}_{>0}$, by certain functions of the input $(u(t))_{t \\in \\mathbb{N}}$. We empirically study the impact of complex parameterizations on SSMs with selectivity by evaluating a Mamba neural network on two synthetic tasks regarded as canonical in the SSM literature [27, 20]: (i) copy, which was shown by our theory (Section 3.3) and earlier experiments (Section 4.1) to pose difficul- ties for real parameterizations in SSMs with no selectivity; and (ii) induction-head, which can be seen as a generalization of copy in which the delay is input-specified (rather than being fixed). Our implementation is based on a widely adopted Mamba repository easily amenable to modification. Unless stated otherwise, repository hyperparameters (pertaining to the neural network architecture and its training) were kept at their default values. Further details concerning our implementation, including detailed descriptions of the copy and induction-head tasks, can be found in Appendix E.3."}, {"title": "Related Work", "content": "SSMs are closely related to linear dynamical systems a classic object of study in areas such as systems theory [3] and control theory [51]. Although there exists extensive literature concerning properties of real and complex linear dynamical systems [10, 5, 6, 9], this literature does not read- ily establish benefits of complex parameterizations for SSMs, primarily due to the following rea- sons: (i) the output of a complex SSM is turned real by disregarding imaginary components (see Section 2.2), therefore it differs from a complex linear dynamical system; and (ii) the structures typically imposed on state transition matrices of SSMs (e.g. stable diagonality; see Section 2.2) are generally uncommon in the literature on linear dynamical systems.\nSSMs can be viewed as a special case of recurrent neural networks [48], which received significant theoretical attention [46, 39, 25, 11, 53]. In this context, several works focused specifically on SSMs [42, 30, 24, 2, 28, 58, 12, 32, 55]. However, to our knowledge, the only prior work to formally and explicitly treat benefits of complex parameterizations for SSMs is [42]. The treatment of [42] (see Section 4.1 therein) can be viewed as a special case of ours. Indeed, [42] considered a task that, using our notation (see Section 2.2), amounts to linearly reconstructing an input element $u(t)$ from the state $x(t')$ of an SSM, where $t, t' \\in \\mathbb{N}, t' > t$. This is equivalent to assigning the output matrix of the SSM $C$ such that the mapping realized by the SSM $\\Phi_{n,(A,B,C)}(\\cdot)$ is a canonical copy (delay) mapping. Roughly speaking, [42] showed that this task requires linear operations with exponential parameters if the SSM is real, whereas linear operations with moderate parameters suffice if the SSM is complex. The same result follows from our Corollary 1 and Proposition 5. We stress that our theory goes far beyond this result, for example in that it covers various mappings beyond copy, including a random (generic) mapping see Section 3.3 for details.\nWith regards to related empirical work, the literature includes several experimental comparisons between real and complex parameterizations for SSMs [20, 22, 42]. Nonetheless, to our knowledge, the controlled experiments we conducted (see Section 4) are reported herein for the first time."}, {"title": "Limitations", "content": "While this paper offers meaningful contributions regarding benefits of complex parameterizations for SSMs, it is important to acknowledge several of its limitations. First, while we establish a sepa- ration between real and complex parameterizations in terms of expressiveness (Section 3.2), we do not quantify how prevalent this separation is, i.e., what proportion of the mappings expressible with complex parameterizations cannot be compactly approximated with real parameterizations. Second, while we prove that real parameterizations suffer from exponentiality that impedes practical learn-"}, {"title": "Discussion", "content": "The extent to which complex parameterizations benefit SSMs is an important open question in ma- chine learning. Evidence in the literature is mixed: while some works demonstrate that complex parameterizations are essential for strong performance, others show that in various settings, real parameterizations lead to comparable (and in some cases better) performance. It was conjectured by Gu and Dao [20] that complex parameterizations are preferable for continuous data modalities (e.g., audio, video), whereas for discrete data modalities (e.g., text, DNA) real parameterizations suf- fice.\nSince a complex SSM includes twice as many parameters as a real SSM of the same dimension, a priori, one might expect that a real SSM would benefit from becoming complex similarly to how it would benefit from doubling its dimension. Our theory showed that this is not the case, and in fact the former benefit far exceeds the latter. Indeed, we established separations between real and complex SSMs, by which a real SSM can only match a complex SSM if either the dimension of the real SSM or the number of iterations required for its training is exponentially large. Experiments corroborated our theory, and suggested that selectivity a new architectural feature yielding state of the art performance may be the dominant factor behind the aforementioned evidence in the literature being mixed.\nWe now outline a potential extension of our theory that accounts for selectivity. Roughly speaking, the separations we established between real and complex SSMs arise from a gap in their ability to ex- press oscillations, i.e., to express frequency components in their impulse response: while a complex SSM can easily express any frequency, a real SSM struggles to do so. Adding selectivity to a real SSM makes its parameters input-dependent, resulting in what can be viewed as an input-dependent impulse response. We hypothesize that this dependence allows importing frequency components from the input to the impulse response. If confirmed, this hypothesis would imply that when the input data is sufficiently rich in frequency content, selectivity can endow real SSMs with all the ben- efits we proved for complex SSMs. Such an outcome aligns with the conjecture of Gu and Dao [20]: continuous data modalities often consist of only low frequencies, whereas discrete data modalities typically have a whiter spectrum,"}]}