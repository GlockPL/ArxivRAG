{"title": "Text-Aware Adapter for Few-Shot Keyword Spotting", "authors": ["Youngmoon Jung", "Jinyoung Lee", "Seungjin Lee", "Myunghun Jung", "Yong-Hyeok Lee", "Hoon-Young Cho"], "abstract": "Recent advances in flexible keyword spotting (KWS) with text enrollment allow users to personalize keywords without uttering them during enrollment. However, there is still room for improvement in target keyword performance. In this work, we propose a novel few-shot transfer learning method, called text-aware adapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for specific keywords with limited speech samples. To adapt the acoustic encoder, we leverage a jointly pre-trained text encoder to generate a text embedding that acts as a representative vector for the keyword. By fine-tuning only a small portion of the network while keeping the core components' weights intact, the TA-adapter proves highly efficient for few-shot KWS, enabling a seamless return to the original pre-trained model. In our experiments, the TA-adapter demonstrated significant performance improvements across 35 distinct keywords from the Google Speech Commands V2 dataset, with only a 0.14% increase in the total number of parameters.", "sections": [{"title": "I. INTRODUCTION", "content": "Keyword spotting (KWS) is a technique for detecting pre-defined keywords in audio streams. Unlike fixed KWS [1]\u2013[3], which requires users to exclusively use specific keywords, flexible KWS allows users to utilize any custom keyword. Custom keywords can be enrolled in flexible KWS either through audio [4]-[6] or text [7]-[11]. Since text-based keyword enrollment does not require multiple utterances of a target keyword and can be achieved easily via text input, the demand for Text-enrolled Flexible KWS (TF-KWS) is growing.\nTF-KWS systems typically use a text encoder for enrollment and an acoustic encoder for testing, both of which are optimized using deep metric learning (DML) [12] objectives such as contrastive loss [9], triplet-based loss [7], and proxy-based loss [8], [11]. As discussed in [12], DML aims to learn an embedding space where the embedding vectors of similar samples are pulled closer together, while those of dissimilar samples are pushed apart. Specifically for TF-KWS, the text embedding (TE) is learned to act as a representative vector for its corresponding keyword, thus attracting the acoustic embedding (AE) of the same keyword and repelling AEs of different keywords in the shared embedding space.\nWhile TF-KWS models can support an unlimited number of keywords, their performance does not match that of keyword-specific models trained with abundant data for each keyword [13]. Thus, there remains potential to enhance performance for specific target keywords. Given the challenge of collecting large amounts of data for a particular keyword, this problem can be approached through few-shot learning [14]. To address this, we propose a few-shot transfer learning approach called text-aware adapter (TA-adapter). To the best of our knowledge, this is the first work on applying few-shot transfer learning to TF-KWS.\nOur research aims to adapt a small portion of the pre-trained acoustic encoder to a target keyword using limited speech data, leveraging the TE extracted from the corresponding text encoder. The TA-adapter consists of three main components: text-conditioned feature modulation (TCFM), feature weight adapter (FW-adapter), and TE classifier. Due to its modular design, the TA-adapter enables seamless restoration to the original pre-trained model, facilitating rapid adaption to various target keywords. In our experiments, we evaluate performance of our method on the Google Speech Commands (GSC) V2 dataset [15] under noisy and reverberant conditions."}, {"title": "II. RELATED WORK", "content": "Several studies have investigated few-shot transfer learning ap- proaches for speech-enrolled flexible KWS [14], [16]\u2013[19]. In [14], the authors combined self-supervised learning (SSL) models with meta-learning algorithms. Many of these works insert an additional classification layer specific to target keywords following a pre-trained acoustic encoder, which is then trained either with or without freezing the pre-trained model.\nThe adapter approach was first introduced in computer vision and NLP [20], [21] to adapt large-scale SSL models for downstream tasks. Adapter modules are inserted between intermediate layers of the pre- trained model and fine-tuned while keeping the rest of the model's parameters fixed. Due to the minimal number of additional param- eters within these adapter modules, the adapter approach prevents overfitting and has gained popularity for parameter-efficient fine- tuning of SSL models. Inspired by this, our TA-adapter updates only a small subset of network parameters using few-shot samples, while maintaining the core acoustic encoder intact. Although the acoustic encoder in TF-KWS is not as large as SSL models, the adapter is well-suited for few-shot KWS due to its parameter efficiency and modular design.\nIn [22], the authors proposed AdaKWS, where a text encoder generates the parameters of Adaptive Instance Normalization (AdaIN) [23] layers in Keyword Adaptive Modules (KAMs), conditioning keyword information into an acoustic encoder. However, unlike our approach, AdaKWS is not designed for few-shot KWS and requires a substantial amount of training data to jointly train both encoders from scratch. Consequently, it results in employing KAMs that contain excessive parameters for few-shot KWS. To condition keyword in- formation with fewer parameters more suitable for few-shot KWS, we"}, {"title": "III. TEXT-AWARE ADAPTER", "content": "For the pre-trained model, we trained the acoustic and text encoders using Relational Proxy Loss (RPL) [11], which exploits the structural relationships within AEs and TEs to promote their tight alignment. We employed the ECAPA-TDNN architecture [28] as the acoustic encoder, which has been widely adopted in various speech processing tasks as an embedding extractor [11], [28]\u2013[31].\nThe ECAPA-TDNN is built upon 1D convolutional layers and comprises multiple blocks similar to other deep CNN architectures, equipped with SE modules. As depicted in Fig. 1, it consists of three SE-Res2Blocks, integrating the Res2Net [32] with SE modules. In this architecture, the Res2Net processes multi-scale features extracted from various hierarchical levels, while the SE module performs channel-wise feature recalibration by learning feature weights based on their importance. The output feature maps from all the SE- Res2Blocks are concatenated along the channel dimension, followed by a dense layer that processes the combined information to generate the features for attentive statistics pooling. The pooling layer converts variable-length features into a fixed-dimensional AE. In Fig. 1, an LAF is applied for TCFM, which will be detailed in Section III-A, replacing the ReLU used in the original ECAPA-TDNN.\nFirst, for the TA-adapter, we adopt TCFM to transfer the target key- word information from the TE into the pre-trained acoustic encoder. By freezing the text encoder, we extract a TE for the target keyword, which serves as a representative vector for its corresponding keyword.\nand TCFM. With AdaIN (Fig. 2a), the scale and bias vectors {$\u03b3$, $\u03b2$} are estimated through a linear projection of the conditioning vector, TE, and applied to the instance-normalized features in a channel-wise manner. In contrast, with TCFM (Fig. 2b), we replace the ReLU of ECAPA-TDNN with LAF (see Fig. 1). TCFM conditions the keyword information from the TE by learning a weighted combination of basic activation functions based on the TE, requiring significantly fewer parameters. Given an ordered set of a basic activation functions {$A_1$, ..., $A_a$}, LAF is defined as follows:\n$s = \\text{softmax}(TE \\cdot w + b), y = \\text{LAF}(h|TE) = \\sum_{i=1}^{a} s_iA_i(h)$, (1)\nwhere $TE \u2208 R^{1xd}$, $w\u2208 R^{dxa}$, $b \u2208 R^{1\u00d7a}$, and $s \u2208 R^{1\u00d7a}$. For each LAF, unique trainable parameters w and b are optimized to estimate the activation weight vector s using the TE, thereby transforming the input features h into the activated features y.\nThe AdaIN-based conditioning requires $d \u00d7 2 \u00d7 f$ parameters, where f denotes the number of channels in h. In comparison, our TCFM only requires $d \u00d7 a$ parameters. Specifically, we set d, f, and a to be 512, 256, and 6, respectively. We employ six activation functions from the set of basic activation functions in [24], selected based on their validation performance: ELU, hard sigmoid, ReLU, softplus, swish, and tanh. For more details, please refer to [24].\nIn addition to feature modulation, we aim to refine the weighting and aggregation process of features within the acoustic encoder by"}, {"title": "A. Text-conditioned feature modulation", "content": "First, for the TA-adapter, we adopt TCFM to transfer the target key- word information from the TE into the pre-trained acoustic encoder. By freezing the text encoder, we extract a TE for the target keyword, which serves as a representative vector for its corresponding keyword.\nFig. 2 highlights the difference between AdaIN-based conditioning"}, {"title": "B. Feature weight adapter", "content": "In addition to feature modulation, we aim to refine the weighting and aggregation process of features within the acoustic encoder by"}, {"title": "C. Text embedding classifier", "content": "The TE classifier process is illustrated by the red line in Fig. 1 (labeled as \u201cTE classifier'). When TCFM and FW-adapter are applied simultaneously, an AE is extracted and then passed through the final fully-connected (FC) layer with weights $\u0398_k \u2208 R^{dx1}$, generating output logits for the target keyword k. A sigmoid activation function is then applied to the output, producing $p(k|x)$, which represents the probability that the input speech x belongs to keyword k. The acoustic encoder is adapted using binary cross-entropy loss. Instead of learning a new weight vector from scratch, we fix the TE as the weight vector in the final classification layer. This is reasonable because the TE has already been trained as a representative vector for its corresponding keyword during the pre-training phase using DML. Thus, we define the inner product between the fine-tunable AE and the fixed TE as the logits for k. Since both AE and TE are L2-normalized, this operation is equivalent to calculating their cosine similarity, $S(\u00b7,\u00b7)$. Therefore, the final score can be obtained in the same manner as in TF-KWS. We can express $p(k|x)$ as follows:\n$p(k|x) = \u03c3(\u0398_k \u00b7 AE) = \u03c3(S(TE, AE))$. (2)"}, {"title": "IV. EXPERIMENTS", "content": "The pre-training strategy followed the same approach as our previous work in [11], including the use of identical datasets, acoustic features, and model architectures. For the TA-adapter, we used the GSC V2 dataset [15] containing 35 keywords. Compared to the training set used for pre-training [33], 10 of these keywords were seen during pre-training, while 25 were unseen (as shown in Table I). We evaluated the model under three low-resource scenarios: 5- shot, 10-shot, and 15-shot learning. For each keyword, we developed three separate models by randomly selecting 5, 10, and 15 samples, respectively, for each scenario. To develop a keyword-specific model, we fine-tuned the pre-trained model for each keyword individually.\nEach mini-batch consisted of 256 utterances, including 128 tar- get, 96 non-target, and 32 noise samples. Non-target samples were randomly chosen from 34 keywords, excluding the target, and both target and non-target samples underwent the same data augmentation process used during pre-training. Noise samples were drawn from the background noise recordings in the GSC V2 dataset. For fine-tuning, we used the AdamW optimizer with an initial learning rate of $10^{-5}$, which was halved every 20 epochs for a total of 150 epochs, with a weight decay of $10^{-5}$.\nThe official validation and test sets of GSC v2 were employed for model selection and evaluation. Table I shows the average counts of positive and negative samples for the corresponding keywords. To simulate real-world conditions, we generated noisy and reverberant speech by convolving synthetic room impulse responses (RIRs) from the OpenSLR dataset [34] and adding noise from the MUSAN dataset [35], with signal-to-noise ratios (SNRs) ranging from 5 to 25 dB. We expanded the validation and test sets to be four times larger than their original sizes. We evaluated model performance using two metrics: the Equal-Error-Rate (EER) [9], [10] and Average Precision (AP) [7], [11], [36], both of which are widely used in KWS. To mitigate potential randomness, the fine-tuning dataset was sampled randomly five times under each condition, and the results were averaged. The entire fine-tuning process took less than one hour on a V100 GPU, with the best epoch determined by the AP on the validation set."}, {"title": "A. Experimental setup", "content": "The pre-training strategy followed the same approach as our previous work in [11], including the use of identical datasets, acoustic features, and model architectures. For the TA-adapter, we used the GSC V2 dataset [15] containing 35 keywords. Compared to the training set used for pre-training [33], 10 of these keywords were seen during pre-training, while 25 were unseen (as shown in Table I). We evaluated the model under three low-resource scenarios: 5- shot, 10-shot, and 15-shot learning. For each keyword, we developed three separate models by randomly selecting 5, 10, and 15 samples, respectively, for each scenario. To develop a keyword-specific model, we fine-tuned the pre-trained model for each keyword individually.\nEach mini-batch consisted of 256 utterances, including 128 tar- get, 96 non-target, and 32 noise samples. Non-target samples were randomly chosen from 34 keywords, excluding the target, and both target and non-target samples underwent the same data augmentation process used during pre-training. Noise samples were drawn from the background noise recordings in the GSC V2 dataset. For fine-tuning, we used the AdamW optimizer with an initial learning rate of $10^{-5}$, which was halved every 20 epochs for a total of 150 epochs, with a weight decay of $10^{-5}$.\nThe official validation and test sets of GSC v2 were employed for model selection and evaluation. Table I shows the average counts of positive and negative samples for the corresponding keywords. To simulate real-world conditions, we generated noisy and reverberant speech by convolving synthetic room impulse responses (RIRs) from the OpenSLR dataset [34] and adding noise from the MUSAN dataset [35], with signal-to-noise ratios (SNRs) ranging from 5 to 25 dB. We expanded the validation and test sets to be four times larger than their original sizes. We evaluated model performance using two metrics: the Equal-Error-Rate (EER) [9], [10] and Average Precision (AP) [7], [11], [36], both of which are widely used in KWS. To mitigate potential randomness, the fine-tuning dataset was sampled randomly five times under each condition, and the results were averaged. The entire fine-tuning process took less than one hour on a V100 GPU, with the best epoch determined by the AP on the validation set."}, {"title": "B. Results", "content": "Table II presents an ablation study that evaluates the effectiveness of the FW-adapter and the TE classifier. For simplicity, the experiment focuses on a 15-shot scenario with five keywords from both seen and unseen keywords: 1) Seen keywords: 'follow', 'happy', 'house', 'one', and 'seven'; 2) Unseen keywords: 'cat', 'dog', 'eight', 'nine', and 'off'. The table reports the average AP (%) values for both seen and unseen keywords, as well as the average between them ('Avg.').\n'PT' and 'FT' refer to the pre-trained and fully fine-tuned models without the adapter, respectively. 'FT clf' represents the model where the network is frozen, and only an additional classifier (clf) is fine-tuned. For 'PT', the score is obtained from the cosine similarity between AE and TE, indicating the use of TE classifier.\nInterestingly, 'FT (15-shot)' performs worse (67.04%) than 'PT' (72.02%), as 15-shot samples are insufficient for fine-tuning all the parameters. However, when using all available samples for fine- tuning (FT (full-shot)), the model achieves remarkable performance (95.35%), although this is impractical due to the excessive cost of data collection. Table I shows the average number of training samples. Freezing the model and fine-tuning only an additional classifier (FT clf) yields poor performance (50.14%). Comparing 'PT' and 'FT clf', the TE classifier clearly boosts few-shot KWS performance."}, {"title": "V. CONCLUSION", "content": "This paper introduces the TA-adapter for addressing the few-shot transfer learning problem in TF-KWS. The TA-adapter utilizes the text embedding to condition keyword information into the acoustic encoder and to generate a keyword score. Also, BN layers and SE modules are adapted using only a few samples of the target keyword. Experimental results demonstrate that the TA-adapter effectively overcomes the challenges of few-shot KWS. Specifically, the TA- adapter boosts the pre-trained model's average precision from 77.93% to 87.63% using just 5 target samples. Since the TA-adapter only modifies a small subset of the model, it enables seamless reversion to the original pre-trained model. In future work, we plan to leverage text-to-speech (TTS) technology to generate synthetic data, enabling zero-shot transfer learning for TF-KWS."}, {"title": "Data and Implementation", "content": "We evaluate performance by individually or collectively adapting the BN layers in each group. Hereafter, we omit mentioning '(15- shot)' in the method, but all methods continue to use 15-shot samples.\nRegardless of the adaptation location within the ECAPA-TDNN, BN adaptation consistently outperforms the pre-trained model for both seen and unseen keywords, validating our hypothesis that adjusting feature weights at each layer enables successful adaptation to a target keyword. Adopting BN adaptation across all groups ('BN') yields better results (79.40%) than fine-tuning individual groups, indicating that the gains from individual group adaptations are complementary and cumulative. Combining SE and BN adaptations yields further improvements. However, contrary to BN, applying SE adaptation to all groups ('SE & BN') performs worse compared to applying it individually to each group. We hypothesize that this could be attributed to the excessive number of additional parameters required for few-shot KWS. The best performance is an AP of 84.33%, achieved with 'SE G3 & BN', without adding any extra parameters.\nTable III presents the ablation results on TCFM, where 'FW- adapter' corresponds to 'SE G3 & BN'. Conditioning keyword information at the lower layers (i.e., GO to G3) degrades performance. We suspect that directly modifying their features with limited samples impairs performance, as these early layers generate fundamental features that eventually form keyword-specific representations. In contrast, the FW-adapter merely adjusts feature aggregation, enabling effective adaptation at any layer and improving performance com- pared to the pre-trained model. Our results suggest that applying TCFM to higher layers is suitable for conditioning TE, consistent with AdaKWS [22], where KAMs are inserted just before the final classifier. Optimal performance is achieved when applying TCFM to G4 and G5, boosting the AP of TF-KWS (i.e., 'PT' in Table II) from 72.02% to 87.22%. Although TCFM slightly increases the total number of parameters (by 3.2K, representing 0.14% of the original count), the performance gain is substantial.\nTo better understand how TCFM adapts based on the conditioning vector, Fig. 3 visualizes the outputs of the trained LAFs from G4 and G5, conditioned on TEs extracted from six keywords. To account for varying activation ranges, the plots display LAFs subtracted by the average of basic activations:\n$\\bar{y} = \\text{LAF}(h|TE_k) - a^{-1} \\sum_{i=1}^a A_i(h)$, (3)\nwhere $h \u2208 [-3,3]$ and k is the selected keyword. It is evident that different keywords exhibit distinct LAF patterns, and each keyword's LAF varies across layers. As a result, it can be observed that TCFM conditions the keyword information of TE by learning a weighted combination of basic activation functions based on TE.\nFinally, Table IV compares the performance of the TA-adapter with other baseline approaches using all 35 keywords. 'TA-adapter' corresponds to the best-performing model from Table III. We report"}]}