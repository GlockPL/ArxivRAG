{"title": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models\nas Low-Level Policies for Atari Games", "authors": ["Nicholas R. Waytowich", "Devin White", "MD Sunbeam", "Vinicius G. Goecks"], "abstract": "Recent advancements in large language models (LLMs)\nhave expanded their capabilities beyond traditional text-based\ntasks to multimodal domains, integrating visual, auditory, and\ntextual data. While multimodal LLMs have been extensively\nexplored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely\nuntapped. This paper explores the application of multimodal\nLLMs as low-level controllers in the domain of Atari video\ngames, introducing Atari game performance as a new bench-\nmark for evaluating the ability of multimodal LLMs to per-\nform low-level control tasks. Unlike traditional reinforcement\nlearning (RL) and imitation learning (IL) methods that re-\nquire extensive computational resources as well as reward\nfunction specification, these LLMs utilize pre-existing mul-\ntimodal knowledge to directly engage with game environ-\nments. Our study assesses multiple multimodal LLM's per-\nformance against traditional RL agents, human players, and\nrandom agents, focusing on their ability to understand and in-\nteract with complex visual scenes and formulate strategic re-\nsponses. Additionally, we examine the impact of In-Context\nLearning (ICL) by incorporating human-demonstrated game-\nplay trajectories to enhance the models' contextual under-\nstanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their exten-\nsive training to effectively function as low-level controllers,\nthereby redefining potential applications in dynamic and vi-\nsually complex environments. Additional results and videos\nare available at our project webpage: https://sites.google.com/\nview/atari-gpt/.", "sections": [{"title": "INTRODUCTION", "content": "Advancements in the field of natural language processing\nand computing have led to the creation of large language\nmodels (LLM) such as LaMDA (Thoppilan et al. 2022) and\nChatGPT (GPT-3.5) (OpenAI 2022), with emergent models\nlike GPT-4 showcasing impressive capabilities across bil-\nlions of tokens of data. These developments have expanded\ninto the realm of multimodal LLMs that integrate text, im-\nages, videos, and even sound, ushering in a new era of artifi-\ncial intelligence capabilities (OpenAI et al. 2024; Reid et al.\n2024; OpenAI 2024).\nThese multimodal models have shown potential far be-\nyond traditional conversational tasks, prompting investiga-\ntions into their application in diverse areas such as robotics\nand high-level planning in automated systems (Li et al.\n2023; Rana et al. 2023). However, while much of the cur-\nrent literature focuses on utilizing LLMs for high-level plan-\nning-abstracting actions into goals\u2014there remains signifi-\ncant potential in exploring their use as low-level controllers.\nThis involves direct action selection, akin to what is typi-\ncally learned by reinforcement learning agents in complex\nenvironments like video games.\nIn this paper, we explore the potential of multimodal large\nlanguage models (LLMs), such as GPT-4V (OpenAI et al.\n2024), GPT-40 (OpenAI 2024), Gemini Flash (DeepMind\n2024), and Claude 3 Haiku (Anthropic 2024), as visual low-\nlevel controllers within the domain of Atari video games,\nas depicted in Figure 1. Traditional approaches for Atari\ngame-playing agents, such as reinforcement learning (RL),\ndemand extensive computational resources and time due to\ntheir trial-and-error nature. In addition, RL also requires the\nuse of either a pre-specified reward function or a learned\nreward function. Our approach utilizes the inherent multi-\nmodal capabilities and pre-existing knowledge of LLMs, en-\nabling them to operate directly and immediately as low-level\npolicies. We assess these models within the popular OpenAI\nGym Atari environments (Brockman et al. 2016) and pro-\npose a novel benchmark for evaluating their abilities within\nthese low-level control environments.\nMoreover, our investigation not only measures the perfor-\nmance of these multimodal LLMs but also closely examines\ntheir ability to interpret and interact with complex visual\ninputs. We feed the models images of specific Atari game\nstates to analyze their capabilities in visual understanding,\nspatial reasoning, and strategy formulation. We also explore\nthe influence of In-Context Learning (ICL) (Dong et al.\n2023) by incorporating human-demonstration examples of\nAtari gameplay, providing contextual gameplay knowledge\nto the LLMs before they interact with the game environ-\nments. The performance of the multimodal LLMs is then\nevaluated with and without this contextual data.\nUltimately, our study aims to establish whether multi-\nmodal LLMs can function effectively as low-level con-\ntrollers, and how well they can leverage their extensive train-\ning to understand and navigate gaming environments. This\nassessment is conducted through a series of rigorous exper-\niments using our newly established \"Atari-GPT\" bench-\nmark, which focuses not only on overall game performance"}, {"title": "ATARI-GPT", "content": "but also on the models' abilities in scene understanding and\nstrategic gameplay. We structure our research around three\nquestions: Q1. Can multimodal LLMs function effectively\nas low-level policies? Q2. Does their multimodal capability\nprovide adequate scene understanding for Atari games? Q3.\nCan they generate effective strategies based on this under-\nstanding?\nWe present a set of experiments designed to benchmark the\neffectiveness of multimodal large language models (LLMs)\nas low-level decision-making agents in the domain of Atari\nvideo games, which we refer to as \u201cAtari-GPT\". Our pri-\nmary focus is on assessing the models' game-playing ca-\npabilities and performance measured by several factors: the\ngame score, visual understanding, spatial reasoning, and\nproficiency in devising efficient game strategies. This inves-\ntigation is guided by the three key research questions out-\nlined in the previous section.\nTo address these questions, our experimental framework\nis divided into two main categories. First, we evaluate the\nmultimodal LLMs' performance in playing Atari, judged\nby the games' scores. This includes an investigation of in-\ncontext learning, where we introduce human-demonstration\nexamples to provide context to the LLMs before gameplay\nand compare the results with and without this in-context\ndata. This assessment measures the models' success by com-\nparing their performance to standard reinforcement learning\nalgorithms, random agents, and human players, analyzing\nhow well the models can act as low-level policies by making\ndecisions based on the current game state.\nSecond, we examine the multimodal LLMs' capabilities\nin visual understanding and spatial reasoning. This includes\ntesting their ability to identify visual elements within Atari\ngame frames, understand spatial relationships between those\ndifferent game elements, and devise strategies based on this\nunderstanding. We use a similar set of Atari games used\nto evaluate game-playing performance to probe these visual\nand strategic capabilities.\nThis structure provides a comprehensive analysis of the\ndecision-making processes of LLMs. This includes evaluat-\ning the effectiveness of in-context learning, assessing their\noverall understanding of the game environment within Atari\nvideo games, and evaluating their performance as low-level\npolicies. Through this methodology, we aim to establish a\nnew benchmark for evaluating LLMs in low-level control\ntasks, exploring how these language models compare to hu-\nmans and learning algorithms."}, {"title": "Models Evaluated", "content": "We evaluate several state-of-the-art frontier large language\nmodels (LLMs) on two key abilities: their capacity to func-\ntion as a low-level policy and their capability to under-\nstand specific frames. We define frontier LLMs as the most\nadvanced models available, with parameter counts ranging\nfrom 20 billion to over a trillion. These models are typically\ntoo large to be trained or evaluated on personal machines\nand are flagship offerings from major tech companies."}, {"title": "State and Action spaces", "content": "All experiments were conducted on the Gymnasium Atari\nset of environments. In the gameplay setting, each frame\ngenerated is initially of size 210x160x3 but resized to\nbe 512x512x3 for all tested models. For understanding\ntasks, each frame is also 210x160x3 and resized to be\n1000x1000x3. In both cases, there is no frame stacking and\nonly a single frame is sent as input. In the gameplay setting,\nwe utilize a context buffer of the current image and the pre-\nvious image as well as the user prompt. When evaluating the\neffect of in-context learning on gameplay, a small set of hu-\nman demonstrations were also included in the context buffer.\nThis context buffer is sent to the model for inference includ-\ning the two most recent game frames. The action space for\neach Atari environment is game-specific and for continuity\nwe used the default action space available for each game.\nMore details on the action space can be found in the Gym-\nnasium Atari Documentation 1."}, {"title": "Prompt Engineering for LLM-Based Atari\nGameplay", "content": "In our experiments, the output for gameplay tests was\nprompt-engineered to follow a specific JSON format where\nthe model was constrained to describe different visual and\nstrategic characteristics of the frame unique to each of the\ngame environments and the optimal action to take in the cur-\nrent game-state. This format was used to encourage chain-\nof-thought reasoning to improve the game-playing perfor-\nmance of the model (Wei et al. 2023). The system prompt\nwas used to both constrain the output of the model as well as\nto instruct the LLM to be an Atari game-playing assistant. In\naddition, each of the system prompts was tuned by providing\nthe LLM with the official documentation description of each\nAtari game environment as well as the action space. To craft\na precise system prompt, we fed the two pieces of informa-\ntion listed above into GPT-4V Turbo to generate a detailed\nsystem prompt for each environment to give the LLMs dur-\ning the game-playing experiments. The appendix contains\nall system prompts used in our experiments.\nSince the outputs of these LLMs are stochastic, error-\nhandling code was written to handle cases where the model\npredicted an invalid action or if a corrupted observation from\nthe environment was added to the context. The model was\nre-prompted when these edge cases occurred."}, {"title": "Game-Play Experiment", "content": "We conducted a comprehensive experiment using GPT-4V\nTurbo, GPT-40, and Gemini 1.5 Flash, where each LLM was\ntested across several Atari games. In these experiments, the\ncurrent game state was presented to the LLM, which then\ngenerated an action to be executed within the Atari environ-\nment. In essence, these models were used as substitutes for\ntraditional reinforcement learning policies, such as Deep Q-\nNetworks (DQN) (Mnih et al. 2013).\nWe evaluated performance in seven Atari environments:\nSpace Invaders, Breakout, Seaquest, Pong, Alien, Ms.\nPacMan, and Frogger, using both In-Context Learning\n(ICL)(Dong et al. 2023) and a no-ICL baseline. For the\nICL experiments, we created a curated dataset consisting\nof four frames and their corresponding output annotations,\nwhich specified the expected actions the model should take\ngiven those frames. We opted for only four frames due\nto the few-shot learning capabilities of LLMs, which have\nshown strong generalization from a limited number of ex-\namples(Brown 2020; Wei et al. 2021).\nBefore starting a rollout, this dataset, along with a system\nprompt tailored for each environment, was provided to the\nmodel. The rollout was conducted over 1,000 timesteps with\na frame skipping interval of 8 frames. During this process,\nthe model was presented with two frames (the previous and\nthe current frame) and asked to determine the appropriate\naction based on its current state, similar to a reinforcement\nlearning rollout. An example of the human demonstration\ndataset used in these experiments can be found in the Ap-\npendix."}, {"title": "Understanding And Reasoning Experiment", "content": "To test the multimodal LLM's understanding and reasoning\ncapabilities, we conduct experiments on eight environments,\nas shown in Figure 2. We created a set of prompts to investi-\ngate the models' visual reasoning, spatial reasoning, strate-\ngic intuition, and ability to identify the environment:\n\u2022 Visual Understanding: Identify all the key elements in\nthis image. Be specific. Use at most 100 words.\n\u2022 Spatial Reasoning: Where are the key elements located\nrelative to each other? Be specific with respect to their\nposition in the image. Use at most 100 words."}, {"title": "Strategy", "content": "The given image is a screenshot of a game.\nDescribe the ideal next move if you were playing this\ngame. Be specific. Use at most 100 words."}, {"title": "Identification", "content": "Knowing that the image came from an\nAtari game, identify its name. Be specific."}, {"title": "RESULTS", "content": "To quantitatively evaluate the performance of the model out-\nputs, we created a rubric outlining the basic answers to\nthe proposed questions, as seen in the Appendix (Table 3).\nGiven that in any state there are several acceptable actions\nand strategies, we do not directly define a single correct ac-\ntion or plan. In cases where we investigate the acceptable\nstrategy, we rather define it as either a direct action or strat-\negy/plan that does not put the agent in harm. Harm includes\nlosing a life or losing points within a game.\nFor each environment, we sent the respective frame with\nthe visual reasoning prompt. Once a response was received,\nwe sent the spatial reasoning prompt, and then the strategic\nand identification prompts, respectively. After receiving all\noutputs, we compared the multimodal LLMs' output with\nthe rubric resulting in a percent score for that environment.\nWe repeated this for all environments and computed the\naverage score over four different trials for non-In Context\nLearning and two trials for In-Context Learning."}, {"title": "Game-Playing Performance", "content": "For game-playing performance we investigate GPT-4V\nTurbo, GPT-40, and Gemini 1.5 Flash both with and with-\nout In-Context Learning (ICL). We collected the four roll-\nouts of each model and each environment and recorded\ntheir cumulative reward over 1000 steps. We then normal-\nized each model's performance relative to human perfor-\nmance and then calculated the mean performance across the\nseven Atari environments, both with and without In-Context\nLearning (ICL), as shown in Figure 3. On average, each\nLLM achieved between 10% and 25% of human perfor-\nmance. Notably, the two GPT-4 models significantly outper-\nformed Gemini 1.5 Flash, with GPT-40 demonstrating the\nhighest overall Atari game-playing performance. Addition-\nally, the inclusion of demonstration examples for in-context\nlearning had little to no impact on the average game-playing\nperformance of these LLMs.\nDespite incorporating human demon-\nstrations through ICL, the models failed to achieve effective\nfew-shot learning, as both large and small multimodal LLMs\nstruggled to learn and generalize from the provided context.\nConsequently, their performance continued to lag behind hu-\nman levels. However, across all Atari environments, GPT-40\nconsistently performed the best among the models tested.\nAdditionally, the environment where all LLMs achieved the\nhighest human-normalized reward was Space Invaders.\nAs ex-\npected, the LLMs did not match the performance of the hu-\nman players or the RL agents. However, they significantly\noutperformed the random agents, demonstrating a meaning-\nful level of understanding and ability to play the games. This\nis an important finding, as it indicates that the LLMs are not\nmerely generating random actions but are making decisions\nthat reflect a basic comprehension of the game mechanics."}, {"title": "Visual And Spatial Reasoning", "content": "We further explored the factors influencing gameplay per-\nformance by testing the visual, spatial, strategic, and en-\nvironment identification abilities of these LLMs. For each\nenvironment, we evaluated GPT-4V, GPT-40, Gemini 1.5\nFlash, and Claude 3 Haiku using four specifically designed\nprompts. These prompts assessed the models' visual under-\nstanding, spatial reasoning, strategic intuition, and their abil-\nity to correctly identify the environment. This analysis pro-\nvides insight into why the models may not have performed\nas well as expected as low-level policies.\nFigure 6 displays the percentage of correct outputs for\neach of the four tasks-visual, spatial, acceptable strategy,\nand identification-across two runs for each model. GPT-\n40 consistently excelled across all tasks, demonstrating high\naccuracy in visual understanding, strategy formulation, and\nenvironment identification. However, it exhibited a notice-\nable decline in spatial reasoning accuracy. This pattern was\nconsistent across all models, suggesting that spatial reason--\ning remains a significant challenge for multimodal large lan-\nguage models and possibly accounting for their relatively\npoor performance on the game-playing tasks."}, {"title": "DISCUSSION", "content": "This study represents one of the first attempts to employ\nmultimodal large language models as low-level policies in\nAtari game environments, marking a significant departure\nfrom their traditional applications in language and multi-\nmodal tasks. The results, while not meeting the performance\nlevels of human players or dedicated reinforcement learn-\ning (RL) models, showcase the potential and limitations of\nLLMs in this context.\nOur experiments demonstrate that while LLMs exhibit\nsome ability to identify and interact with key elements\nwithin game frames, their performance as low-level policies\nis notably subpar, likely due to a lack of specific training for\nthis task as well as difficulty in spatial reasoning. We ob-\nserved a significant performance gap between state-of-the-\nart models, such as OpenAI GPT-40, and smaller models like\nClaude 3 Haiku and Gemini 1.5 Flash. The larger models ex-\nhibit a better understanding of the images and perform better\nthan a random agent, whereas the smaller models often per-\nform worse than a random agent. However, neither large nor\nsmall models are capable of performing few-shot learning\neffectively when applied as low-level policies. While large\nmodels can comprehend the visual content, they struggle\nto translate this understanding into correct actions, poten-\ntially due to the rapid pace of the games or a lack of relevant\ntraining data. This reflects the inherent challenge of adapting\nmodels, primarily trained on vast, diverse datasets for gen-\neral tasks, to the specific demands of spatial and strategic\nreasoning required in video game environments, which indi-\ncates the need for future research focused on fine-tuning and\ndeveloping more efficient models that can effectively lever-\nage ICL in these environments. However, we believe that as\nfrontier LLMs continue to advance, their innate low-level\ncontrol abilities may also continue to improve.\nOne of the major issues encountered in this study were\ndifficulties with output consistency and response latency,\nwith models like GPT-4V Turbo occasionally failing to gen-\nerate appropriate responses or exhibiting high inference la-\ntencies. These technical challenges limits multimodal LLMS\nwhen operating outside their standard domains of appli-\ncation. Moreover, the logistic challenge imposed by the\nrate limits of the OpenAI, Anthropic, and Google APIs\ncontributed heavily to much longer experimentation time,\nadding more overhead to the inherent inference time of these\nmodels. In addition, each model has unique rate limits which\nmakes it currently impossible to run in real-time, highlight-\ning the need for better and faster local multimodal LLMs for\nfast-paced, low-level decision-making tasks.\nDespite these setbacks, the findings are invaluable for sev-"}, {"title": "Multimodal Large Language Models", "content": "Processing multimodal inputs such as images and sequen-\ntial data has undergone constant evolution in the do-\nmain of deep learning. Before the transformer architec-\nture (Vaswani et al. 2023), Convolutional Neural Networks\n(CNNs) (LeCun et al. 1998; Krizhevsky, Sutskever, and Hin-\nton 2012) for visual processing and Recurrent Neural Net-\nworks (RNNs) (Mikolov et al. 2010) for handling sequen-\ntial data such as text or audio represented the state of the\nart (Mao et al. 2015). Data was processed through separate\ninput networks and their latest outputs were combined via\ndifferent fusion strategies (Mao et al. 2015). Despite achiev-\ning notable success, these approaches were limited in their\nscale and capacity to capture the intricate interactions be-\ntween different modalities, primarily due to the inherent lim-\nitations in sequential data processing and cross-modal syn-\nthesis (Chung et al. 2019).\nThe advent of transformers introduced a more effec-\ntive and scalable mechanism for processing sequential data\nthrough self-attention mechanisms (Vaswani et al. 2023).\nAmong the key developments was the creation of CLIP\n(Contrastive Language-Image Pre-training) (Radford et al.\n2021), which leveraged transformers to learn a common la-\ntent space for both visual and linguistic data, leading to a\nmodel that could correlate images in the context of natu-\nral language. This development led to some of the most in-\nfluential Multimodal Large Language Models available to-\nday such as GPT-4 Vision (OpenAI et al. 2024), Gemini\nPro 1.5 (Reid et al. 2024), Gemini Ultra and Pro 1.0 (Team\net al. 2024), Ferret (You et al. 2023), Vicuna (Chiang et al.\n2023), Claude 3 (Anthropic 2024), Multimodal Large Lan-"}, {"title": "Multimodal LLMs as Low-Level Policies for\nG ames", "content": "Low-level policies act as controllers, processing observa-\ntions from the environment and returning actions. The ac-\ncessibility and complexity of games make them ideal bench-\nmarks for evaluating the performance of such policies (Mnih\net al. 2013; Badia et al. 2020). Traditionally, video game-\nplaying policies have employed reinforcement learning al-\ngorithms (Mnih et al. 2013), behavior cloning (Hussein et al.\n2017), or a combination of both (Goecks et al. 2019). Given\nthe increased performance of multimodal LLMs, they have\nemerged as an alternative to these methods.\nThe rationale for employing multimodal LLMs as low-\nlevel policies in gaming is grounded in their distinctive ca-\npabilities and how they align with the demands of various\ngame environments. When playing social games against one\nanother, LLMs perform well when playing games that re-\nquire valuing their self-interest but sub-optimally when they\nneed to coordinate with other players (Akata et al. 2023).\nWhen fine-tuned on gameplay data, LLMs have been shown\nto learn an internal representation of game states that can be\nused to make predictions (Li et al. 2022). Given their natu-\nral language processing capabilities, LLMs can also directly\nlearn from human-written game manuals to accelerate learn-\ning and improve their performance (Wu et al. 2024).\nSeveral works have demonstrated the capabilities of\nLLMs when playing games. Gato (Reed et al. 2022) lever-\nges a transformer architecture (Vaswani et al. 2023) similar\nto LLMs to tokenize multimodal data from multiple tasks,\nincluding playing games and robotic control, to train a gen-\neralist policy. The same model with the same weights can\nthen play games, caption images, control robotic arms, chat,\nand others. CICERO (FAIR) leveraged LLMs to combine\nstrategic reasoning and natural language to cooperate, ne-\ngotiate, and coordinate with other players to play the game\nDiplomacy at a human level. LLMs have also been em-\nployed to solve text-based games (Yao et al. 2020; Tsai et al.\n2023) and directly write code to convey more complex be-\nhaviors when solving open-ended tasks in Minecraft (Wang\net al. 2023).\nWhile the applications of LLMs in gaming have demon-\nstrated considerable success across a variety of con-\ntexts (Gallotta et al. 2024), a comprehensive exploration of\nthese multimodal capabilities remains unexplored. In this\nwork, we address this gap by specifically investigating their\nvisual, spatial reasoning, and strategic capabilities when\nplaying Atari games."}]}