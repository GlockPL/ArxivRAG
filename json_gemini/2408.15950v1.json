{"title": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games", "authors": ["Nicholas R. Waytowich", "Devin White", "MD Sunbeam", "Vinicius G. Goecks"], "abstract": "Recent advancements in large language models (LLMs) have expanded their capabilities beyond traditional text-based tasks to multimodal domains, integrating visual, auditory, and textual data. While multimodal LLMs have been extensively explored for high-level planning in domains like robotics and games, their potential as low-level controllers remains largely untapped. This paper explores the application of multimodal LLMs as low-level controllers in the domain of Atari video games, introducing Atari game performance as a new benchmark for evaluating the ability of multimodal LLMs to perform low-level control tasks. Unlike traditional reinforcement learning (RL) and imitation learning (IL) methods that require extensive computational resources as well as reward function specification, these LLMs utilize pre-existing multimodal knowledge to directly engage with game environments. Our study assesses multiple multimodal LLM's performance against traditional RL agents, human players, and random agents, focusing on their ability to understand and interact with complex visual scenes and formulate strategic responses. Additionally, we examine the impact of In-Context Learning (ICL) by incorporating human-demonstrated gameplay trajectories to enhance the models' contextual understanding. Through this investigation, we aim to determine the extent to which multimodal LLMs can leverage their extensive training to effectively function as low-level controllers, thereby redefining potential applications in dynamic and visually complex environments.", "sections": [{"title": "INTRODUCTION", "content": "Advancements in the field of natural language processing and computing have led to the creation of large language models (LLM) such as LaMDA (Thoppilan et al. 2022) and ChatGPT (GPT-3.5) (OpenAI 2022), with emergent models like GPT-4 showcasing impressive capabilities across billions of tokens of data. These developments have expanded into the realm of multimodal LLMs that integrate text, images, videos, and even sound, ushering in a new era of artificial intelligence capabilities (OpenAI et al. 2024; Reid et al. 2024; OpenAI 2024).\nThese multimodal models have shown potential far beyond traditional conversational tasks, prompting investigations into their application in diverse areas such as robotics and high-level planning in automated systems (Li et al. 2023; Rana et al. 2023). However, while much of the current literature focuses on utilizing LLMs for high-level planning-abstracting actions into goals\u2014there remains significant potential in exploring their use as low-level controllers. This involves direct action selection, akin to what is typically learned by reinforcement learning agents in complex environments like video games.\nIn this paper, we explore the potential of multimodal large language models (LLMs), such as GPT-4V (OpenAI et al. 2024), GPT-40 (OpenAI 2024), Gemini Flash (DeepMind 2024), and Claude 3 Haiku (Anthropic 2024), as visual low-level controllers within the domain of Atari video games. Traditional approaches for Atari game-playing agents, such as reinforcement learning (RL), demand extensive computational resources and time due to their trial-and-error nature. In addition, RL also requires the use of either a pre-specified reward function or a learned reward function. Our approach utilizes the inherent multimodal capabilities and pre-existing knowledge of LLMs, enabling them to operate directly and immediately as low-level policies. We assess these models within the popular OpenAI Gym Atari environments (Brockman et al. 2016) and propose a novel benchmark for evaluating their abilities within these low-level control environments.\nMoreover, our investigation not only measures the performance of these multimodal LLMs but also closely examines their ability to interpret and interact with complex visual inputs. We feed the models images of specific Atari game states to analyze their capabilities in visual understanding, spatial reasoning, and strategy formulation. We also explore the influence of In-Context Learning (ICL) (Dong et al. 2023) by incorporating human-demonstration examples of Atari gameplay, providing contextual gameplay knowledge to the LLMs before they interact with the game environments. The performance of the multimodal LLMs is then evaluated with and without this contextual data.\nUltimately, our study aims to establish whether multimodal LLMs can function effectively as low-level controllers, and how well they can leverage their extensive training to understand and navigate gaming environments. This assessment is conducted through a series of rigorous experiments using our newly established \"Atari-GPT\" benchmark, which focuses not only on overall game performance"}, {"title": "ATARI-GPT", "content": "We present a set of experiments designed to benchmark the effectiveness of multimodal large language models (LLMs) as low-level decision-making agents in the domain of Atari video games, which we refer to as \u201cAtari-GPT\". Our primary focus is on assessing the models' game-playing capabilities and performance measured by several factors: the game score, visual understanding, spatial reasoning, and proficiency in devising efficient game strategies. This investigation is guided by the three key research questions outlined in the previous section.\nTo address these questions, our experimental framework is divided into two main categories. First, we evaluate the multimodal LLMs' performance in playing Atari, judged by the games' scores. This includes an investigation of in-context learning, where we introduce human-demonstration examples to provide context to the LLMs before gameplay and compare the results with and without this in-context data. This assessment measures the models' success by comparing their performance to standard reinforcement learning algorithms, random agents, and human players, analyzing how well the models can act as low-level policies by making decisions based on the current game state.\nSecond, we examine the multimodal LLMs' capabilities in visual understanding and spatial reasoning. This includes testing their ability to identify visual elements within Atari game frames, understand spatial relationships between those different game elements, and devise strategies based on this understanding. We use a similar set of Atari games used to evaluate game-playing performance to probe these visual and strategic capabilities.\nThis structure provides a comprehensive analysis of the decision-making processes of LLMs. This includes evaluating the effectiveness of in-context learning, assessing their overall understanding of the game environment within Atari video games, and evaluating their performance as low-level policies. Through this methodology, we aim to establish a new benchmark for evaluating LLMs in low-level control tasks, exploring how these language models compare to humans and learning algorithms."}, {"title": "Models Evaluated", "content": "We evaluate several state-of-the-art frontier large language models (LLMs) on two key abilities: their capacity to function as a low-level policy and their capability to understand specific frames. We define frontier LLMs as the most advanced models available, with parameter counts ranging from 20 billion to over a trillion. These models are typically too large to be trained or evaluated on personal machines and are flagship offerings from major tech companies."}, {"title": "State and Action spaces", "content": "All experiments were conducted on the Gymnasium Atari set of environments. In the gameplay setting, each frame generated is initially of size 210x160x3 but resized to be 512x512x3 for all tested models. For understanding tasks, each frame is also 210x160x3 and resized to be 1000x1000x3. In both cases, there is no frame stacking and only a single frame is sent as input. In the gameplay setting, we utilize a context buffer of the current image and the previous image as well as the user prompt. When evaluating the effect of in-context learning on gameplay, a small set of human demonstrations were also included in the context buffer. This context buffer is sent to the model for inference including the two most recent game frames. The action space for each Atari environment is game-specific and for continuity we used the default action space available for each game."}, {"title": "Prompt Engineering for LLM-Based Atari Gameplay", "content": "In our experiments, the output for gameplay tests was prompt-engineered to follow a specific JSON format where the model was constrained to describe different visual and strategic characteristics of the frame unique to each of the game environments and the optimal action to take in the current game-state. This format was used to encourage chain-of-thought reasoning to improve the game-playing performance of the model (Wei et al. 2023). The system prompt was used to both constrain the output of the model as well as to instruct the LLM to be an Atari game-playing assistant. In addition, each of the system prompts was tuned by providing the LLM with the official documentation description of each Atari game environment as well as the action space. To craft a precise system prompt, we fed the two pieces of information listed above into GPT-4V Turbo to generate a detailed system prompt for each environment to give the LLMs during the game-playing experiments.\nSince the outputs of these LLMs are stochastic, error-handling code was written to handle cases where the model predicted an invalid action or if a corrupted observation from the environment was added to the context. The model was re-prompted when these edge cases occurred."}, {"title": "Game-Play Experiment", "content": "We conducted a comprehensive experiment using GPT-4V Turbo, GPT-40, and Gemini 1.5 Flash, where each LLM was tested across several Atari games. In these experiments, the current game state was presented to the LLM, which then generated an action to be executed within the Atari environment. In essence, these models were used as substitutes for traditional reinforcement learning policies, such as Deep Q-Networks (DQN) (Mnih et al. 2013).\nWe evaluated performance in seven Atari environments: Space Invaders, Breakout, Seaquest, Pong, Alien, Ms. PacMan, and Frogger, using both In-Context Learning (ICL)(Dong et al. 2023) and a no-ICL baseline. For the ICL experiments, we created a curated dataset consisting of four frames and their corresponding output annotations, which specified the expected actions the model should take given those frames. We opted for only four frames due to the few-shot learning capabilities of LLMs, which have shown strong generalization from a limited number of examples(Brown 2020; Wei et al. 2021).\nBefore starting a rollout, this dataset, along with a system prompt tailored for each environment, was provided to the model. The rollout was conducted over 1,000 timesteps with a frame skipping interval of 8 frames. During this process, the model was presented with two frames (the previous and the current frame) and asked to determine the appropriate action based on its current state, similar to a reinforcement learning rollout."}, {"title": "Understanding And Reasoning Experiment", "content": "To test the multimodal LLM's understanding and reasoning capabilities, we conduct experiments on eight environments. We created a set of prompts to investigate the models' visual reasoning, spatial reasoning, strategic intuition, and ability to identify the environment:\n\u2022 Visual Understanding: Identify all the key elements in this image. Be specific. Use at most 100 words.\n\u2022 Spatial Reasoning: Where are the key elements located relative to each other? Be specific with respect to their position in the image. Use at most 100 words.\n\u2022 Strategy: The given image is a screenshot of a game. Describe the ideal next move if you were playing this game. Be specific. Use at most 100 words.\n\u2022 Identification: Knowing that the image came from an Atari game, identify its name. Be specific.\nTo quantitatively evaluate the performance of the model outputs, we created a rubric outlining the basic answers to the proposed questions, as seen in the Appendix. Given that in any state there are several acceptable actions and strategies, we do not directly define a single correct action or plan. In cases where we investigate the acceptable strategy, we rather define it as either a direct action or strategy/plan that does not put the agent in harm. Harm includes losing a life or losing points within a game.\nFor each environment, we sent the respective frame with the visual reasoning prompt. Once a response was received, we sent the spatial reasoning prompt, and then the strategic and identification prompts, respectively. After receiving all outputs, we compared the multimodal LLMs' output with the rubric resulting in a percent score for that environment. We repeated this for all environments and computed the average score over four different trials for non-In Context Learning and two trials for In-Context Learning."}, {"title": "RESULTS", "content": "For game-playing performance we investigate GPT-4V Turbo, GPT-40, and Gemini 1.5 Flash both with and without In-Context Learning (ICL). We collected the four roll-outs of each model and each environment and recorded their cumulative reward over 1000 steps. We then normalized each model's performance relative to human performance and then calculated the mean performance across the seven Atari environments, both with and without In-Context Learning (ICL). On average, each LLM achieved between 10% and 25% of human performance. Notably, the two GPT-4 models significantly outperformed Gemini 1.5 Flash, with GPT-40 demonstrating the highest overall Atari game-playing performance. Additionally, the inclusion of demonstration examples for in-context learning had little to no impact on the average game-playing performance of these LLMs.\nDespite incorporating human demonstrations through ICL, the models failed to achieve effective few-shot learning, as both large and small multimodal LLMs struggled to learn and generalize from the provided context. Consequently, their performance continued to lag behind human levels. However, across all Atari environments, GPT-40 consistently performed the best among the models tested. Additionally, the environment where all LLMs achieved the highest human-normalized reward was Space Invaders.\nAs expected, the LLMs did not match the performance of the human players or the RL agents. However, they significantly outperformed the random agents, demonstrating a meaningful level of understanding and ability to play the games. This is an important finding, as it indicates that the LLMs are not merely generating random actions but are making decisions that reflect a basic comprehension of the game mechanics."}, {"title": "Visual And Spatial Reasoning", "content": "We further explored the factors influencing gameplay performance by testing the visual, spatial, strategic, and environment identification abilities of these LLMs. For each environment, we evaluated GPT-4V, GPT-40, Gemini 1.5 Flash, and Claude 3 Haiku using four specifically designed prompts. These prompts assessed the models' visual understanding, spatial reasoning, strategic intuition, and their ability to correctly identify the environment. This analysis provides insight into why the models may not have performed as well as expected as low-level policies.\nGPT-40 consistently excelled across all tasks, demonstrating high accuracy in visual understanding, strategy formulation, and environment identification. However, it exhibited a noticeable decline in spatial reasoning accuracy. This pattern was consistent across all models, suggesting that spatial reasoning remains a significant challenge for multimodal large language models and possibly accounting for their relatively poor performance on the game-playing tasks."}, {"title": "DISCUSSION", "content": "This study represents one of the first attempts to employ multimodal large language models as low-level policies in Atari game environments, marking a significant departure from their traditional applications in language and multimodal tasks. The results, while not meeting the performance levels of human players or dedicated reinforcement learning (RL) models, showcase the potential and limitations of LLMs in this context.\nOur experiments demonstrate that while LLMs exhibit some ability to identify and interact with key elements within game frames, their performance as low-level policies is notably subpar, likely due to a lack of specific training for this task as well as difficulty in spatial reasoning. We observed a significant performance gap between state-of-the-art models, such as OpenAI GPT-40, and smaller models like Claude 3 Haiku and Gemini 1.5 Flash. The larger models exhibit a better understanding of the images and perform better than a random agent, whereas the smaller models often perform worse than a random agent. However, neither large nor small models are capable of performing few-shot learning effectively when applied as low-level policies. While large models can comprehend the visual content, they struggle to translate this understanding into correct actions, potentially due to the rapid pace of the games or a lack of relevant training data. This reflects the inherent challenge of adapting models, primarily trained on vast, diverse datasets for general tasks, to the specific demands of spatial and strategic reasoning required in video game environments, which indicates the need for future research focused on fine-tuning and developing more efficient models that can effectively leverage ICL in these environments. However, we believe that as frontier LLMs continue to advance, their innate low-level control abilities may also continue to improve.\nOne of the major issues encountered in this study were difficulties with output consistency and response latency, with models like GPT-4V Turbo occasionally failing to generate appropriate responses or exhibiting high inference latencies. These technical challenges limits multimodal LLMS when operating outside their standard domains of application. Moreover, the logistic challenge imposed by the rate limits of the OpenAI, Anthropic, and Google APIs contributed heavily to much longer experimentation time, adding more overhead to the inherent inference time of these models. In addition, each model has unique rate limits which makes it currently impossible to run in real-time, highlighting the need for better and faster local multimodal LLMs for fast-paced, low-level decision-making tasks.\nDespite these setbacks, the findings are invaluable for several reasons. First, they contribute to our understanding of the current capabilities and boundaries of LLMs when applied to low-level control tasks, with and without additional data for in-context learning. Second, they offer a new benchmark for the AI research community to measure the progress of LLMs in handling dynamic and visually complex environments. Adjustments such as tuning the models' temperature settings demonstrated some mitigation of output inconsistency, suggesting pathways for refining LLM performance in these tasks.\nImportantly, the continuous updates to LLM architectures and training methods suggest that the capabilities of these models will evolve, potentially overcoming some of the current deficiencies noted in our study. As such, this research should be viewed as a foundational step that sets the stage for future investigations, encouraging ongoing refinement and adaptation of LLMs for applications requiring detailed environmental interactions and decision-making.\nWhile LLMs have not yet reached the level of proficiency required to match the best human or RL performances in Atari gameplay, their ability to engage in this task at all is notable. It demonstrates the adaptability and potential of LLMs to extend beyond their original training confines, offering a glimpse into future applications where these models could serve as more general low-level controllers."}, {"title": "RELATED WORK", "content": "Processing multimodal inputs such as images and sequential data has undergone constant evolution in the domain of deep learning. Before the transformer architecture (Vaswani et al. 2023), Convolutional Neural Networks (CNNs) (LeCun et al. 1998; Krizhevsky, Sutskever, and Hinton 2012) for visual processing and Recurrent Neural Networks (RNNs) (Mikolov et al. 2010) for handling sequential data such as text or audio represented the state of the art (Mao et al. 2015). Data was processed through separate input networks and their latest outputs were combined via different fusion strategies (Mao et al. 2015). Despite achieving notable success, these approaches were limited in their scale and capacity to capture the intricate interactions between different modalities, primarily due to the inherent limitations in sequential data processing and cross-modal synthesis (Chung et al. 2019).\nThe advent of transformers introduced a more effective and scalable mechanism for processing sequential data through self-attention mechanisms (Vaswani et al. 2023). Among the key developments was the creation of CLIP (Contrastive Language-Image Pre-training) (Radford et al. 2021), which leveraged transformers to learn a common latent space for both visual and linguistic data, leading to a model that could correlate images in the context of natural language. This development led to some of the most influential Multimodal Large Language Models available today such as GPT-4 Vision (OpenAI et al. 2024), Gemini Pro 1.5 (Reid et al. 2024), Gemini Ultra and Pro 1.0 (Team et al. 2024), Ferret (You et al. 2023), Vicuna (Chiang et al. 2023), Claude 3 (Anthropic 2024), Multimodal Large Language and Vision Assistant (Liu et al. 2023) and LLaVa (Liu et al. 2023). Since then, multimodal LLMs have been applied to different domains such as designing reward functions (Ma et al. 2023) and controlling general game-playing agents (Abi Raad et al. 2024)."}, {"title": "Multimodal LLMs as Low-Level Policies for Games", "content": "Low-level policies act as controllers, processing observations from the environment and returning actions. The accessibility and complexity of games make them ideal benchmarks for evaluating the performance of such policies (Mnih et al. 2013; Badia et al. 2020). Traditionally, video game-playing policies have employed reinforcement learning algorithms (Mnih et al. 2013), behavior cloning (Hussein et al. 2017), or a combination of both (Goecks et al. 2019). Given the increased performance of multimodal LLMs, they have emerged as an alternative to these methods.\nThe rationale for employing multimodal LLMs as low-level policies in gaming is grounded in their distinctive capabilities and how they align with the demands of various game environments. When playing social games against one another, LLMs perform well when playing games that require valuing their self-interest but sub-optimally when they need to coordinate with other players (Akata et al. 2023). When fine-tuned on gameplay data, LLMs have been shown to learn an internal representation of game states that can be used to make predictions (Li et al. 2022). Given their natural language processing capabilities, LLMs can also directly learn from human-written game manuals to accelerate learning and improve their performance (Wu et al. 2024).\nSeveral works have demonstrated the capabilities of LLMs when playing games. Gato (Reed et al. 2022) leverages a transformer architecture (Vaswani et al. 2023) similar to LLMs to tokenize multimodal data from multiple tasks, including playing games and robotic control, to train a generalist policy. The same model with the same weights can then play games, caption images, control robotic arms, chat, and others. CICERO (FAIR) leveraged LLMs to combine strategic reasoning and natural language to cooperate, negotiate, and coordinate with other players to play the game Diplomacy at a human level. LLMs have also been employed to solve text-based games (Yao et al. 2020; Tsai et al. 2023) and directly write code to convey more complex behaviors when solving open-ended tasks in Minecraft (Wang et al. 2023).\nWhile the applications of LLMs in gaming have demonstrated considerable success across a variety of contexts (Gallotta et al. 2024), a comprehensive exploration of these multimodal capabilities remains unexplored. In this work, we address this gap by specifically investigating their visual, spatial reasoning, and strategic capabilities when playing Atari games."}]}