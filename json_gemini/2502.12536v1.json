{"title": "An Algorithm Board in Neural Decoding", "authors": ["Jingyi Feng", "Kai Yang"], "abstract": "Understanding the mechanisms of neural encoding and decoding has always been a highly interesting research topic in fields such as neuroscience and cognitive intelligence. In prior studies, some researchers identified a symmetry in neural data decoded by unsupervised methods in motor scenarios and constructed a cognitive learning system based on this pattern (i.e., symmetry). Nevertheless, the distribution state of the data flow that significantly influences neural decoding positions still remains a mystery within the system, which further restricts the enhancement of the system's interpretability. Based on this, this paper mainly explores changes in the distribution state within the system from the machine learning and mathematical statistics perspectives. In the experiment, we assessed the correctness of this symmetry using various tools and indicators commonly utilized in mathematics and statistics. According to the experimental results, the normal distribution (or Gaussian distribution) plays a crucial role in the decoding of prediction positions within the system. Eventually, an algorithm board similar to the Galton board was built to serve as the mathematical foundation of the discovered symmetry.", "sections": [{"title": "1 Introduction", "content": "At present, neural encoding is a functional model where neurons encode rich and intensive external stimuli into dynamic, time-varying neural activities, exploring the functional relationship between dynamic sensory stimuli and neural responses [9]. Neural decoding is the restoration of external stimuli through a set of signals emitted by neurons, such as motion positions [17], image classification, and speech recognition. Research on neural encoding and decoding has implications for our understanding of brain functioning mechanisms, the treatment of brain disorders, and the development of brain-computer interfaces and machine intelligence. Existing technologies mainly focus on movement, speech, and vision, aiming to scientifically understand the link between neural activity and the outside world [11]. Moreover, prostheses, robots, mice, and other devices that fully realize \"brain control technology\" are becoming a reality [12,10]."}, {"title": "2 Related work", "content": "In neural decoding, some studies often use supervised algorithms to achieve good decoding predictions, such as finger position decoding [17,18] and a rat's movement trajectory decoding [8]. Due to the temporal characteristics of neural decoding signals, temporal decoding algorithms have received much attention. Firstly, state-space models (SSMs) with associations between the current state and the previous state have become more developed [7,17,18]. In addition, deep learning has been the focus of numerous studies, such as LSTM (Long Short-Term Memory) [2,13]. Further, the source of the recorded neural activity can change from day to day, e.g., due to a slight movement of the implanted electrodes. The proposed multiplicative RNN (Recurrent Neural Network) allows mappings from the neural input to the motor output to partially change from neural activity [16]. The decoding performed by these methods is more accurate than that of the traditional methods.\nIn recent years, some researchers have proposed a novel weakly supervised method different from previous studies. This method is mainly based on a symmetric pattern discovered from decoding brain neural data between the unsupervised decoding positions and the ground-truth positions. This method has achieved decoding predictions that are much higher than those of unsupervised methods and close to those of supervised methods [4,5]. Based on these studies, a general framework for refining weakly supervised system has been proposed, which has been algorithmically validated [3] and theoretically justified from machine learning, neuroscience, cognitive science, and more [6]. Due to the complexity and difficulty in understanding this framework, further research is needed to explore the generation of symmetric mechanisms and the processing details of data in this system for the development of this framework."}, {"title": "3 Method", "content": "In this paper, we mainly analyze the internal evaluation of data flow within the system, that is, the relationship between its unsupervised prediction and corrected prediction, as well as the ground-truth trajectory, as the N-value increases. However, in the external evaluation of system output, that is, the effect of the trained system model in testing, reference can be made to [4,5,3]. They observe that the symmetry between predicted trajectory and ground-truth trajectory is mined by unsupervised algorithms, such as unsupervised KF (Kalman Filter) and unsupervised EM (Expectation Maximization) [5]. In this paper, we focus on the crucial role of correction within the system and its impact on internal data transmission, evaluating it through quantitative and qualitative metrics. For demonstration purposes, unsupervised EM was adopted due to its excellent decoding performance and efficient runtime [5]. In the iterative process of classical EM, the E-step and M-step are defined [15] as a reference. The weight that considers the temporal correlation between any two Gaussian noise random variables at different times in the state space model is updated as [5]:\n\\begin{equation}\nW = \\begin{bmatrix}\n\\frac{\\sum_{k=1}^{K} S_k \\hat{Z}_k}{\\sum_{k=1}^{K} S_k^2} & \\frac{\\sum_{k=1}^{K} S_k Z_k}{\\sum_{k=1}^{K} S_k^2} \\\\\n\\frac{\\sum_{k=1}^{K} (\\hat{z}_k + P_k) - \\sum_{k=1}^{K} \\hat{z}_k}{\\sum_{k=1}^{K} S_k^2} & \\frac{K}{\\sum_{k=1}^{K} S_k} - \\frac{\\sum_{k=1}^{K} a Z_k}{\\sum_{k=1}^{K} (a)}^T\n\\end{bmatrix}\n\\end{equation}\nwhere, $W$ is the updated weight. $\\bar{a}$ is equivalent to $a$ in $W = [a, \\cdot]$. $\\hat{z}_k$ is the unsupervised predicted position or the predicted position after system correction. $S_k$ are the input neural data. $K$ is the data length. $P_k$ is the covariance of $\\hat{z}_k$ at time k."}, {"title": "3.2 An introduction to a correction method", "content": "Fig. 1 shows the correction process from N = 0 to N = n within the system based on the symmetry discovered. Its main feature is the observation of a symmetry between the unsupervised predicted trajectory and the ground-truth trajectory in the active space, and then encoding the space with a bit-value (0 or 1) to correct the predicted trajectory through analogy. Experiments have shown that this method has strong decoding ability and robustness [5,3]. According to Fig. 1, the position encoding and position correction within the system are as follows:\n\\begin{equation}\n\\hat{z}_{k,bit} = \\begin{cases}\n1 & \\text{if } \\hat{z}_k \\text{ or } z_{k,bit} \\geq fmid() \\\\\n0 & \\text{if } \\hat{z}_k \\text{ or } z_{k,bit} < fmid()\n\\end{cases}\n\\end{equation}\n\\begin{equation}\n\\hat{z}_k^{n+1} = \\hat{z}_k^n + \\frac{2}{n} (fmid() - \\hat{z}_k^n) \\text{ if } \\hat{z}_{k,bit} \\neq z_{k,bit}\n\\end{equation}\nWhere, formula (2) is for encoding the external positions in the active space. $\\hat{z}_{k,bit}$ is the bit-value (0 or 1) encoded for the unsupervised predicted position $\\hat{z}_k$ or the ground-truth position $z_k$ at the k-th moment or the k-th sample when n = N \u2212 1. $\\hat{z}$ is the unsupervised predicted position. $fmid() = \\frac{(z_{max} + z_{min})}{2}$ is the median value of the active space or subspace when n = N, where $z_{max}$ and $z_{min}$ are the maximum and minimum boundaries of movement, respectively. Formula (3) is for correcting the unsupervised predicted position in the active space. $\\hat{z}_k^{n+1}$ is the predicted position after correcting the unsupervised prediction position when n = N by using the analogy between the encoded bit-values at the k-th time or the k-th sample. $\\hat{z}_{k,bit}$ and $z_{k,bit}$ are the bit-values (0 or 1) encoded for the unsupervised prediction position and ground-truth position, respectively, namely $\\hat{z}_{k,bit}$.\nIn summary, within the system and in terms of the entire data flow, correction plays a key role. The reason for the correction is that when unsupervised methods decode brain neural data, their unsupervised predicted trajectory and ground-truth trajectory exhibit symmetry in the active space. In previous studies [4,5,3,6], although some researchers have discovered this symmetry, they have not addressed the mathematical and statistical depth of the interpretation of this symmetry within the system. These also limit the further development of this discovery from another perspective. In this paper, we mainly focus on these defects. The ultimate goal is to deeply explain what characteristics the data that generates this symmetry has, and whether these characteristics can provide us with new insights and promote the scientific growth of this discovery."}, {"title": "4 Experiment", "content": "The data was collected from the Institutional Animal Care and Use Committees of the Approprity Institutions and can be found at https://github.com/KordingLab/Neural_Decoding [8], which is larger than the dataset [17]. From this recording [8], 46 neurons are used over 75 minutes. These neurons had mean and median firing rates of 1.7 and 0.2 spikes/sec, respectively. Then, the dataset has 219,089 time points, which are transformed into 28039 samples after processing. A rat's activity space is about length L = 200cm, and width B = 200cm. In"}, {"title": "4.2 The discovered symmetry", "content": "Currently, in the datasets [17,8] we have tested, we have identified similar symmetrical properties in unsupervised decoding positions of brain neural data. In this paper, the dataset [8] is adopted to demonstrate and illustrate this symmetry, as shown in Fig. 2. Fig. 2 displays the unsupervised predicted trajectory (N=0) and the corrected unsupervised predicted trajectory (N = 1) visualized in x-position, along with the ground-true trajectory. The green line represents the unsupervised predicted trajectory, while the red line represents the corrected"}, {"title": "4.3 The quantitative metrics of the system in mathematics and statistics", "content": "In this section, we evaluate the decoding prediction in system internal correction from different views using various commonly used metrics. These metrics help us fully grasp the changes in the system during neural data processing. Among them, $R^2$ and RMSE are used to assess the proximity of predicted and ground-truth positions, while PCC is used to evaluate whether the predicted and ground-truth trajectories are linearly related. KL-divergence and JS-divergence are used to evaluate whether the predicted and ground-truth positions have consistent distribution.\nTable 1 shows the quantitative metrics for the evaluation of decoding predictions on x-position as the parameter N increases. Among them, maximum robustness ($R_{max}$) will decrease as the N-value increases. When N = 0, $R^2$ = -0.503 indicates that the unsupervised prediction is below the mean value, and the unsupervised prediction is typically not considered useful. As observed in Fig. 3, the scatterplot between predicted and ground-truth positions shows significant randomness when N = 0. However, when N > 0, the $R^2$ evaluation is significantly higher than the mean value, reaching 0.5, and approaching 1 for N > 2. Additionally, RMSE provides more specific numerical values for prediction errors, which complements the $R^2$ metric. Then, in correlation coefficients (i.e., PCC), when N = 0, PCC are respectively close to zero, indicating that the unsupervised predicted positions and the ground-truth positions are not linearly"}, {"title": "4.4 The qualitative metrics of the system in mathematics and statistics", "content": "x-position In this section, we evaluate decoding predictions using probability density function (PDF) and power spectral density (PSD) in the context of internal correction within the system, observing what happens in neural data processing from the perspective of data distribution. Among them, PDF is adopted to evaluate whether the density distribution of the predicted positions matches the ground-truth positions distribution, as well as the noise distribution between them. PSD is adopted to evaluate the energy distribution of the predicted and ground-truth positions, and noise to observe changes in the system during data processing. In addition, in depicting the data distribution, we mainly focus on the quantitative indicators of KL-divergence and JS-divergence as referenced in Figs. 1 and 2. Firstly, Fig. 4 gives a visualization of the ground-truth trajectory in the x-position and its probability density distribution and power spectral density. It can be observed from the PDF and PSD that the PDF of the moving positions in the x-position does not have a distinct fixed distribution, while the PSD of the moving positions gradually decreases and stabilizes at a constant.\nFig. 5 shows the PDF and PSD of the unsupervised predicted positions (N = 0) and the corrected predicted positions (N > 0) in x-position. When N = 0, it can be observed that its PDF is close to a Gaussian distribution. However, after one correction (N = 1), the width range of its distribution increases on the amplitude axis and the height range decreases from about 600 to about 400, but still maintains a Gaussian-like distribution. Then, when N = 2, a significant change occurs in the PDF of the predicted positions, with two Gaussian-like distributions emerging. Similarly, it is observed that when N = 3, four Gaussian-like distributions emerge; when N = 4, eight Gaussian-like distributions emerge; when N = 5, sixteen Gaussian-like distributions emerge, with $2^N$. At this point, when N = 5, the PDF of the predicted positions is very similar to that of the ground-truth positions shown in Fig. 4. We believe that this phenomenon implies that as the N-value increases, the PDF of predicted positions approaches that of the ground-truth positions in the form of multiple Gaussian-like distributions. It is extremely important that as shown in Fig. 5, the data distribution for the predicted and ground-truth positions gradually converges as the N-value increases, consistent with the evaluation of quantitative indicators (i.e., KL-divergence and JS-divergence values tend towards 0) in Table 1. Finally, in PSD of the predicted positions, as N-value increases, it remains basically stable at about 40 and does not show significant changes.\nFig. 6 demonstrates the PDF and PSD of the prediction error (i.e., noise) between the predicted positions and the ground-truth positions in the x-position. From the figure, it can be seen that as N-value increases, regardless of how the predicted position distribution changes in Fig. 5, the prediction error always satisfies a Gaussian distribution of noise. This is a very obvious characteristic in Fig. 6. In addition, from the noise waveform, as N-value increases, it always floats around the zero-value, i.e., its mean is zero. On the other hand, the average value of its PDF can also be observed to be approximately zero-value. However, unlike the PSD of the predicted and ground-truth positions, as N-value increases, the PSD of noise will also basically stabilize at a constant value, but there will be more spikes. At present, we do not know the specific role of these spikes and how they are generated, but we believe that they are likely to be a side effect generated to support prediction errors or noise that meet a Gaussian distribution.\ny-position In addition to testing on the x-position, testing on the y-position is also given. Fig. 7 gives a visualization of the ground-truth trajectory in y-position and its probability density distribution and power spectral density. It can be observed from the PDF and PSD that the PDF of the moving positions does not also have a distinct fixed distribution, while the PSD of the moving positions gradually decreases and stabilizes at a constant as that in the x-position.\nFurther, Fig. 8 shows the PDF and PSD of the unsupervised predicted positions (N = 0) and the corrected predicted positions (N > 0) in the y-position. As shown in the figure, as the N-value increases, the PDF of predicted positions also exhibits the characteristic of being composed of multiple Gaussian-like distributions, with several Gaussian-like distributions of $2^N$. One particularly significant feature is that while the PDF of the unsupervised prediction positions is Gaussian distribution when N = 0, the PDF of the predicted positions is substantially consistent with that of the ground-truth positions in Fig. 7 when N = 5. Then, the PSD of the predicted positions gradually decreases and tends towards a constant. Finally, Fig. 9 demonstrates the PDF and PSD of the prediction error (i.e., noise) between the predicted positions and the ground-truth"}, {"title": "5 An algorithm board derived from the discovered symmetry", "content": "As observed from the aforementioned experiments and analysis and Fig. 1, the processing steps between the unsupervised method and the Galton board can be analogized as shown in Fig. 10. Fig. 10 (a) illustrates how a large number of balls traverse the Galton board in a binomial distribution, resulting in a normal distribution that can be proven using the central limit theorem. Fig. 10 (b) displays how a large amount of data undergoes unsupervised decoding, leading to a similar binomial distribution and, thus, a decoded position that can form a normal distribution. We believe that there is a strong similarity between them. If this is the case, when processing neural data, the brain should process in a manner similar to the processing pattern of the Galton board. If this observation proves true, it will inspire us to develop new algorithms that are more similar to the brain's cognitive function system, that is, neural encoding and neural decoding."}, {"title": "6 Conclusion", "content": "In this paper, we explore the information contained in the discovered symmetry from various indicators. As a conclusion, the overall perspective is as follows: (1) The probability density function (PDF) of the prediction location of the unsupervised method follows a Gaussian distribution. Moreover, noise, defined as the difference between the ground-truth location and the predicted location, has a PDF that always follows a Gaussian distribution. (2) As the N-value increases, the PDF of the prediction location gradually approaches the PDF of the ground-truth location (such as, non-Gaussian distribution, etc.) using multiple Gaussian distributions. (3) As the N-value increases, whether it is the ground-truth location, predicted location, or the noise between them, their power spectral density (PSD) will gradually tend towards a constant; however, the PSD of noise will have more spikes. Lastly, an algorithm board similar to the Galton board was constructed to serve as the mathematical foundation of the discovered symmetry."}]}