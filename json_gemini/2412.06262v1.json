{"title": "A Lightweight U-like Network Utilizing Neural Memory Ordinary Differential Equations for Slimming the Decoder", "authors": ["Quansong He", "Xiaojun Yao", "Jun Wu", "Zhang Yi", "Tao He"], "abstract": "In recent years, advanced U-like networks have demonstrated remarkable performance in medical image segmentation tasks. However, their drawbacks, including excessive parameters, high computational complexity, and slow inference speed, pose challenges for practical implementation in scenarios with limited computational resources. Existing lightweight U-like networks have alleviated some of these problems, but they often have pre-designed structures and consist of inseparable modules, limiting their application scenarios. In this paper, we propose three plug-and-play decoders by employing different discretization methods of the neural memory Ordinary Differential Equations (nmODEs). These decoders integrate features at various levels of abstraction by processing information from skip connections and performing numerical operations on upward path. Through experiments on the PH2, ISIC2017, and ISIC2018 datasets, we embed these decoders into different U-like networks, demonstrating their effectiveness in significantly reducing the number of parameters and FLOPs while maintaining performance. In summary, the proposed discretized nmODEs decoders are capable of reducing the number of parameters by about 20% ~ 50% and FLOPs by up to 74%, while possessing the potential to adapt to all U-like networks. Our code is available at https://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networks.", "sections": [{"title": "1 Introduction", "content": "Deep learning has become increasingly prominent in the field of paramedicine, providing valuable assistance in disease detection and diagnosis. The UNet [Ronneberger et al., 2015] is a significant milestone in demonstrating the efficacy of encoder-decoder Convolutional Neural Networks (CNNs) with skip connections for medical image segmentation. Over time, UNet has become the foundational framework for most notable medical image segmentation methodologies. To achieve higher precision, many studies have introduced complex modules or increased the number of parameters. For example, ResUNet [Zhang et al., 2018] combines the UNet architecture with residual connections inspired by ResNet [He et al., 2015]. Swin-UNet [Cao et al., 2022] leverages the Swin Transformer [Liu et al., 2021] architecture. The TransFuse [Zhang et al., 2021] model employs a dual-path configuration that seamlessly combines CNN and ViT [Dosovitskiy et al., 2020], enabling it to capture both local and global insights simultaneously.\nExisting U-like networks have improved the performance of UNet, but most of them come with an increased computational cost. This can pose challenges when attempting to deploy them in practical application scenarios with limited computing resources. To address this issue, researchers have ventured into the research direction that goes beyond the pursuit of model performance, focusing on model lightweighting. In recent developments, UNeXt [Valanarasu and Patel, 2022] combined UNet and MLP [Tolstikhin et al., 2021], presenting a lightweight architecture that achieves remarkable performance while reducing both parameter count and computational requirements. MALUNet [Ruan et al., 2022] has successfully reduced model size by decreasing the number of model channels and incorporating multiple attention modules. The Efficient Group Enhanced UNet (EGE-UNet) [Ruan et al., 2023] reduced the number of parameters and computational complexity through the integration of enhanced attention mechanisms and feature fusion modules.\nWhile the aforementioned methods have demonstrated impressive abilities in reducing model parameters and computa-"}, {"title": "2 Related Work", "content": "To delve into the integration of U-like networks with the theme of nmODEs, this section will first introduce several classic UNet networks and methods related to nmODEs."}, {"title": "2.1 UNet and Its Variants", "content": "UNet [Ronneberger et al., 2015] was the cornerstone of all UNet variants for semantic image segmentation. It has a U-like structure with a contracting path for context extraction and an expanding path for precise localization. Utilizing skip connections, UNet effectively preserved fine feature details. Attention UNet (Att-UNet) [Oktay et al., 2018] incorporated an attention mechanism into the UNet, significantly improving its efficacy in extracting image features. UNeXt [Valanarasu and Patel, 2022], being the pioneering lightweight medical image segmentation network that integrates convolutional and multilayer perceptrons, had shown remarkable success in reducing network parameters and computational workload. MALUNet [Ruan et al., 2022] demonstrated outstanding performance in skin cancer segmentation by incorporating attention mechanisms and deep separable convolution into UNet through carefully designed modules. Building upon MALUNet, EGE-UNet [Ruan et al., 2023] further refined the attention mechanism and introduces new feature fusion modules, ensuring network performance while significantly reducing network complexity. EGE-UNet surpassed many large-scale networks in terms of both performance and efficiency."}, {"title": "2.2 Neural Ordinary Differential Equations", "content": "Ordinary Differential Equation (ODE) systems, recognized as a distinctive class of dynamical systems, have long been subject to extensive exploration and empirical investigation in the realms of mathematics and physics. Neural ODEs (NODEs) [Chen et al., 2018] provided mathematical principles that elucidate ResNet, transforming it from an enigmatic black-box network into a comprehensible framework. They presented a novel approach to conceptualize neural networks as representations of ODEs. NODEs established a foundation for the unification of neural networks and ODEs. One notable advantage of NODEs is their ability to eliminate the storage of intermediate quantities during the forward propagation, resulting in a substantial reduction in parameters and computational overhead. Nevertheless, there are certain limitations in mapping data through NODEs. For instance, they face challenges in representing mappings like $g(1) = -1, g(-1) = 1$. This limitation arises from the fact that NODEs models utilizing data inputs as initial values can only learn features within the same topological space as the input data [Dupont et al., 2019]. Furthermore, when modeling problems with differential equations, creating a dynamical system, it has been demonstrated that attractors in dynamical systems are believed to be linked to memory capacity [Poucet and Save, 2005] [Wills et al., 2005]. However, conventional NODEs lack the capability to effectively leverage the memory capacity provided by attractors.\nThe nmODEs [Yi, 2023] are specialized variants of NODEs designed to overcome the limitations inherent in traditional NODEs and harness the full memory capabilities offered by dynamical systems. It enhanced the neural network's nonlinear expression capability by employing implicit mapping and utilizing nonlinear activation functions. Simulating the dynamical system governing neocortical neuronal memory, nmODEs introduced a distinct feature\u2014a clear dynamical characterization achieved through the segregation of learning neurons and memory neurons. Unlike previous NODEs approaches, nmODEs treated input data as external parameters rather than utilizing them as initial values for ODEs. By separating the neuron's function into learning and memory components, learning exclusively occurs in the learning part, while the memory part maps the input to its global attractor, establishing a mapping from input space to memory space. The nmODEs have found successful applications in various segmentation tasks. For instance, the nmPLS-Net [Dong et al., 2023] leveraged the robust nonlinear representation and memory capabilities of nmODEs to construct an edge segmentation-based decoding network. This approach has enabled accurate lung lobe segmentation. Additionally, the integration of nmODEs into UNet, employing a straightforward discretization method, has demonstrated promising outcomes in tasks such as diabetic kidney segmentation [Wang et al., 2024] and liver segmentation [He et al., 2023]. [Hu et al., 2023] enhanced the robustness of medical image segmentation using nmODEs and yielded favorable outcomes. These applications showcase the versatility and effectiveness of nmODEs across different medical image segmentation challenges."}, {"title": "3 Methods", "content": "In this section, we leverage the discretization methods of nmODEs to construct a lightweight U-like network architecture. nmODEs have the nature to be adaptive to the decoders of U-like networks. Therefore, in Section 3.1, we will introduce the U-like networks with the discretized nmODEs decoders. These U-like networks utilize skip connections to extract low-level features and inject them into internal states for feature aggregation, resulting in a series of low parameter nmODEs decoders. In Section 3.2, we will introduce three discretized ODE solvers to approximately calculate the output of the nmODEs decoders."}, {"title": "3.1 The U-like Networks with nmODEs Decoders", "content": "To begin with, let's revisit the disparity in inputs between conventional NODE and nmODE. Their structures are delineated in Fig. 3(a) and Fig. 3(b), respectively. In the typical NODEs configuration, the initial value $y(0)$ is derived from the data itself, and the output represents the numerical solution of the ODE. In contrast, within the nmODEs architecture, the initial value $y(0)$ is set to a random value, e.g., 0. Data served as an sequential external input $x(t)$. nmODEs take two inputs, aligning seamlessly with the decoders of U-like networks. Information from the skip connections serves as the sequential external inputs to the nmODEs decoders, while information from the upward path serves as $y(t)$, enabling the full utilization of nmODEs. This is exactly the reason why we chose nmODEs. The differential equation for nmODEs is formulated as follows:\n$\\dot{y}(t) = -y(t) + f(y(t) + g(x(t), \\theta_t)).$ (1)\nFor $t > 0$, where $y(t) \\in R^n$ represents the network's state, $x(t) \\in R^m$ stands for external input, and $\\Theta_t$ means the parameters of the skip connections. We use the nmODEs to formulate the decoders of the U-like networks, resulting in an modified U-like network as illustrated in Fig. 2. This network applies parameterized computation in the skip connections, which is denoted as $g(x(t), \\theta_t)$. The duty of the skip connections is to transform the low-level features to the fixed-size high-level feature maps. The U-like network with nmODEs decoders performs parameter-less feature aggregation. Given an L-layer U-like network using nmODEs decoders, define the output of the network encoder, which is the skip-connected input of the decoder, as $x^l$, and denote the upward path input of the decoder as $y^l, 1 < l < L$.Initializing $x(0) = x^l$ and $y(0) = y^l = 0$, the target of the nmODEs decoders is to obtain the numerical solution at the point $(\\tau, x(t) = x^l)$ along the trajectory of the Eq. (1). In the rest of this section, we will introduce three discretized ODE solvers, e.g. explicit Euler's method, Heun's method, and linear multistep method.\nFor simplicity in subsequent discussions, let's denote Eq. (1) as $F(t, y)$, the function $F$ remains continuous and adheres to certain Lipschitz conditions, ensuring the solution's existence and uniqueness. Given an initial value $y_0$, the trajectory traced by Eq. (1) starting from $y_0$ is denoted as $y_t$, encompassing all instances where $t > 0$. A vector $y^\u2217$ is defined as an equilibrium point of the NODEs if it satisfies the equation $F(t^\u2217, y^\u2217) = 0$. An equilibrium point $y^\u2217$ earns the designation of a global attractor when, for any given $y_0$, the corresponding trajectory $y_t$ converges towards $y^\u2217$ as $t \\rightarrow \\infty$. With the existence of global attractor within nmODEs, the establishment of a favorable nonlinear mapping from $x$ to $y^\u2217$ becomes feasible."}, {"title": "3.2 Discretization Methods", "content": "In the modified lightweight U-like networks, the upward path is parameterless. Only a small number of parameters are responsible for information integration of skip connections, matching the feature sizes at the upward path. We discretize the nmODEs decoders in three different methods and replace the original decoders of U-like networks. The structure inside the decoders varies with the discretization method, with specific reference to the mathematical derivation. All three discretization methods address initial value problems (IVPs) for nmODEs, sharing common steps.\nTheorem 1. Explicit Euler's Method [Euler, 1845]. Given the derivative $\\dot{y}(t) = F(t, y(t))$, choose a value $\\delta$ for the size of every step along t-axis and set $t_{n+1} = t_n + \\delta$. The $y_{n+1}$ from $y_n$ and $t_n$ is\n$y_{n+1} = y_n + \\delta \\cdot \\dot{y}(t_n) = y_n + \\delta \\cdot F(t_n, y_n),  \\qquad \\qquad(2)$\nwhere $y_n$ is an approximate solution at time $t_n$, i.e., $y_n \\approx y(t_n)$. $Y_{n+1}$ is an explicit function of $y_i$ for $i < n$."}, {"title": "4 Experiments", "content": "4.1 Datasets\nPH2. The PH2\u00b9 dataset is a compilation of 200 dermoscopic images focused on dermatology, specializing in melanocytic lesions. It enables research in skin lesion segmentation and classification, with a particular emphasis on nevus and melanoma.\nISIC2017. The ISIC2017\u00b2 dataset is a comprehensive compilation of 2150 dermoscopic images of skin lesions. It serves as a valuable resource for advancements in dermatology and computer-aided diagnosis. The dataset covers a wide range of skin conditions, including both benign and malignant lesions. It is primarily designed for tasks related to melanoma\nhttps://www.fc.up.pt/addi/ph2%20database.html\nhttps://challenge.isic-archive.com/data/#2017\ndetection and skin cancer classification.\nISIC2018. The ISIC2018\u00b3 dataset is a valuable resource in dermatological image analysis, focusing on melanoma detection and skin cancer classification. It consists of approximately 2700 diverse dermoscopic images."}, {"title": "4.2 Implementation Details", "content": "In our experiments, we replaced the decoders of UNet, Att-UNet, MALUNet, EGE-UNet and UNeXt with nmODEs decoders using various discretization methods. All experiments were executed on a single RTX 4090 GPU using PyTorch4. Each dataset was randomly split into training and testing sets with a 7:3 ratio, and images were normalized and resized to 256 x 256 for consistency. Data augmentation techniques, including horizontal flipping, vertical flipping, and random rotation, were applied. The optimizer AdamW [Loshchilov and Hutter, 2017] was used, along with the CosineAnnealingLR [Loshchilov and Hutter, ] scheduler, setting the maximum iteration count to 50 and the minimum learning rate to 1e-5. Training spanned 300 epochs with a batch size of 8. Mean Intersection over Union (mIoU) and Dice similarity score (DSC) were employed as evaluation metrics. Experiments were repeated five times, and results were reported as mean and standard deviation for each dataset.\n4https://pytorch.org/get-started/locally/"}, {"title": "4.3 Comparative Results", "content": "We chose five prevalent networks from the existing U-like networks, each with documented experimental results on relevant datasets. Subsequently, we replaced their decoders with nmODEs decoders employing different discretizations. The comparison with the original networks and other major models is illustrated in Table 1. The experimental results clearly demonstrates the undeniable effectiveness of our proposed nmODEs decoders in terms of parameter and FLOPs reduction. For instance, the Att-UNet, which initially boasts the highest number of parameters, sees a reduction of approximately 46% in parameters and 74% in FLOPs after applying EED. Similarly, the classical UNet experiences a reduction of about 30% in parameters and 40% in FLOPs with the application of HD. Even in the case of lightweight networks, a consistent reduction of around 50% in parameters and 20%-30% in FLOPs is observed after integrating the nmODEs decoders.\nIn terms of performance, the networks with the nmODEs decoders generally outperform the original networks, while those without performance improvement largely maintain the level of the original networks. On the PH2 dataset, both mIoU and DSC of Att-UNet with EED applied improve by about 5 percentage points over the original network, reaching the state-of-the-art. Similarly, mIoU and DSC of UNet with LMD applied improve by 2.7 and 1.6 percentage points over the original network, also reaching the state-of-the-art, as demonstrated in the ablation experiment section. On the ISIC2017 dataset, the networks employing nmODEs decoders largely retain the performance of the original networks. On the ISIC2018 dataset, EGE-UNet with LMD ap-"}, {"title": "4.4 Representative Results", "content": "Figures 5 illustrates the qualitative segmentation results obtained by replacing the decoders of MALUNet, EGE-UNet, Att-UNet and UNet with the nmODEs decoders. These comparisons are made against the original network using partially representative images. In each figure, the last two columns highlight that the mask predicted by the network with the nmODEs decoders is closer to the ground truth than the mask predicted by the original network."}, {"title": "4.5 Ablation Study", "content": "In our ablation experiments using UNet as the base network on the PH2 dataset, our proposed decoders demonstrated a significant reduction in parameters. This reduction can be attributed not only to the efficient computational mechanism of NODEs but also to the fact that the number of channels in the initial $y(0)$ is only 3, maintaining consistency with the upward path. The ablation experiments involved starting with the original UNet and progressively reducing the number of channels in its upward path to 3. Subsequently, we simplified the nmODEs decoders by retaining only the part of the skip connection summation, denoted as $y^{l-1} = y^l + g(x^l,\\theta^l)$, ultimately utilizing all three decoders.\nAnalyzing the experimental results presented in Table 2 for the PH2 dataset, it is observed that the segmentation performance experiences a slight degradation when reducing the channels in the upward path of the original UNet to 3. This slight impact could be attributed to the unchanged downward path of the network, crucial for extracting feature informa-"}, {"title": "5 Conclusion", "content": "In this paper, we introduce three specialized nmODEs decoders tailored for U-like networks. By discretizing nmODES using methods like explicit Euler's, Heun's, and linear multistep methods, we exploit the benefits of the upward path's low-channel parameterlessness and the memory efficiency of neural ordinary differential equations. Our decoders drastically reduce parameters and computational demands in U-like networks. To thoroughly assess their impact, we undertake a comprehensive series of segmentation experiments across widely-used datasets. We conduct an extensive comparative analysis between prevalent U-like networks and their nmODEs-enhanced counterparts. The outcomes of our study shed light on the streamlined complexity exhibited by networks that integrate nmODEs decoders, showcasing their remarkable capability to either maintain or enhance performance. Furthermore, our findings indicate that in specific tasks and network configurations, there exists a promising potential to achieve state-of-the-art performance levels. We hope that our work will provide fresh insights for the development of lightweight medical image segmentation models."}]}