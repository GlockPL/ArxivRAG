{"title": "Contrastive CFG: Improving CFG in Diffusion Models by Contrasting Positive and Negative Concepts", "authors": ["Jinho Chang", "Hyungjin Chung", "Jong Chul Ye"], "abstract": "As Classifier-Free Guidance (CFG) has proven effective in conditional diffusion model sampling for improved condition alignment, many applications use a negated CFG term to filter out unwanted features from samples. However, simply negating CFG guidance creates an inverted probability distribution, often distorting samples away from the marginal distribution. Inspired by recent advances in conditional diffusion models for inverse problems, here we present a novel method to enhance negative CFG guidance using contrastive loss. Specifically, our guidance term aligns or repels the denoising direction based on the given condition through contrastive loss, achieving a nearly identical guiding direction to traditional CFG for positive guidance while overcoming the limitations of existing negative guidance methods. Experimental results demonstrate that our approach effectively removes undesirable concepts while maintaining sample quality across diverse scenarios, from simple class conditions to complex and overlapping text prompts.", "sections": [{"title": "1. Introduction", "content": "Classifier-Free Guidance (CFG) [18] forms the key basis of modern text-guided generation with diffusion models. From Bayes rule, CFG constructs a Bayesian classifier $\\nabla_{x} \\log p(c|x) = \\gamma(\\nabla_{x} \\log p(x|c) - \\nabla_{x} \\log p(x))$ without training additional external classifiers [10]. In practice, it is common to emphasize the classifier vector direction with some constant $\\gamma$, which corresponds to sharpening the posterior, i.e. $p(x)p(c|x)$. While this may potentially distort"}, {"title": "2. Related works", "content": "2.1. Diffusion models\nDiffusion models [19, 40] are a class of generative models that learn the score function [20] of the data distribution, and use this score function to revert the forward noising process. The forward process, denoted with the time index t, is governed by a Gaussian kernel that the underlying data distribution $p(x_{0}) = p(x)$ eventually approximates the standard normal distribution at time $t = T$, i.e. $p(x_{T}) \\approx \\mathcal{N}(0, I)$. The variance preserving forward transition kernel [19] is given as $p(x_{t}|x_{0}) = \\mathcal{N}(x_{t}; \\sqrt{\\bar{\\alpha}_{t}}x_{0}, (1 - \\bar{\\alpha}_{t})I)$.\nThe reverse generative process follows a stochastic differential equation (SDE) [40] governed by the score function $\\nabla_{x_{t}} \\log p(x_{t})$. To estimate this score function, one typically uses epsilon matching [19]\n$\\theta^{*} = \\arg \\min_{\\theta} \\mathbb{E} [||\\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon) - \\epsilon||^{2}]$, (1)\nwhich can be shown to be equivalent to denoising score matching [41], $s_{\\theta}(x_{t}) = \\nabla_{x_{t}} \\log p(x_{t}) = \\frac{1}{\\sqrt{1 - \\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x_{t})$. By Tweedie's formula [12], one can recover the posterior mean $\\mu(x_{t}) = \\mathbb{E}[x_{0}|x_{t}] = \\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}(x_{t} - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_{\\theta}(x_{t}))$. Moreover, it is common practice to train a conditional score function conditioned on the text prompt [18] with random dropping to use it flexibly, either as $\\epsilon_{\\theta}(x_{t}, c)$ or $\\epsilon_{\\theta}(x_{t}) := \\epsilon_{\\theta}(x_{t}, \\emptyset)$, where $\\emptyset$ refers to the null condition. In practice, a popular way of generating images through reverse sampling is through DDIM sampling [38], where a single iteration can be written as\n$x_{\\theta}(x_{t}) = (x_{t} - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_{\\theta}(x_{t}, \\emptyset))/\\sqrt{\\bar{\\alpha}_{t}}$, (2)\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}x_{\\theta}(x_{t}) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon_{\\theta}(x_{t}, \\emptyset)$, (3)\nwhere we defined $x_{\\theta}(x_{t})$ as the posterior mean without the conditioning. Iterating (2) and (3) amounts to sampling from $p(x)$."}, {"title": "2.2. Classifier-free guidance", "content": "Plugging in the conditional epsilon $\\epsilon_{\\theta}(x_{t},c)$ to sample from the conditional distribution $p(x|c)$ does not work well in practice, due to the guidance effect being too weak. To mitigate this downside, it is standard to use classifier-free guidance (CFG) [18] at sampling time. The key idea is to use\n$\\epsilon_{+}(x_{t}) := \\epsilon_{\\theta}(x_{t}) + \\gamma(\\epsilon_{c^{+}}(x_{t}) - \\epsilon_{\\theta}(x_{t}))$, (4)\nwhere we defined $\\epsilon_{c}(x_{t}) := \\epsilon_{\\theta}(x_{t}, c)$. Running DDIM sampling with $\\epsilon_{+}(x_{t})$ in the place of $\\epsilon_{\\theta}(x_{t})$ leads to sampling from the gamma-powered distribution $p(x|c^{+}) := p(x)p(c^{+}|x)^{\\gamma}$, a sharpened posterior. This way, adherence to the condition $c^{+}$ is emphasized."}, {"title": "2.3. Guided sampling", "content": "Another popular way of posterior sampling with diffusion models is to use guided sampling, where the guidance is given by the gradient of some energy function [7, 36, 46, 48]. Denoting $l(\\cdot)$ as the energy to be minimized, guided sampling using Decomposed Diffusion Sampling [8] has the form\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}(x_{\\emptyset} - w_{t}\\nabla_{\\epsilon_{\\theta}}l(x_{\\emptyset})) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon_{\\emptyset}$,\nwhere $w_{t}$ is the step size. For instance, by using the score distillation sampling (SDS) loss [30], we have\n$l_{SDS}(x) := ||\\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}x + \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon) - \\epsilon||^{2} = \\frac{\\bar{\\alpha}_{t}}{1 - \\bar{\\alpha}_{t}}||x_{c} - x||^{2}$, (5)\nleading to the improved version of CFG called CFG++ [9]:\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}(x_{\\emptyset} + \\gamma(x_{c} - x_{\\emptyset})) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon_{\\emptyset}$, (6)\nwith $\\gamma := \\frac{2\\bar{\\alpha}_{t}}{1 - \\bar{\\alpha}_{t}}w_{t}$, where the only difference from the original CFG is the use of $\\epsilon_{c}$ instead of $\\epsilon_{+}$ in (6). In fact, the authors in [9] also showed that the CFG++ is a scaled version of CFG where the improvement is especially dominating at the early stage of reverse sampling. In this regard, CFG sampling with reverse sampling can also be seen as guided sampling with the SDS loss function."}, {"title": "2.4. Concept Negation", "content": "Negative guidance. Analogous to emphasize sampling from $c^{+}$ in CFG, negative guidance aims to avoid sampling from the condition $c^{-}$. The simplest case [1, 13, 24, 29] negates the guidance direction in (4) such as\n$\\epsilon_{-}(x_{t}) := \\epsilon_{\\theta}(x_{t}) - \\gamma(\\epsilon_{c^{-}}(x_{t}) - \\epsilon_{\\emptyset}(x_{t}))$, (7)\nwhere the goal is to avoid sampling from $c^{-}$. Note that this corresponds to sampling from $p^{-}(x|c^{-}) := p(x)/p(c^{-}|x)^{\\gamma}$, a joint distribution inversely proportional to the posterior likelihood. When the goal is to sample from $c^{+}$ while avoiding $c^{-}$, one uses [2, 42]\n$\\epsilon_{+,c^{-}} := \\epsilon_{c^{+}} + \\gamma(\\epsilon_{c^{+}} - \\epsilon_{c^{-}}) = \\epsilon_{c^{+}} + (\\epsilon_{c^{+}} - \\epsilon_{\\emptyset}) - (\\epsilon_{c^{-}} - \\epsilon_{\\emptyset})$ (8)\nIn both cases, pushing away from $c^{-}$ is governed by the negation of the vector direction $(\\epsilon_{c^{-}} - \\epsilon_{\\emptyset})$.\nRecently, Koulischer et al. [24] proposed dynamic negative guidance (DNG) and proposed sampling from $p(x)(p(\\neg c^{-}|x))^{\\gamma}$. Here, $\\neg c^{-}$ is defined as a condition such that $p(\\neg c^{-}|x) = 1 - p(c^{-}|x)$, which can also be seen as a union of all possible input conditions except $c^{-}$. Applying Bayes rule, one can show that\n$\\nabla_{x_{t}} \\log p(x_{t}|\\neg c^{-}) = \\nabla_{x_{t}} \\log p(x_{t}) - \\gamma(x_{t}, c^{-})(\\nabla_{x_{t}} \\log p(x_{t}|c^{-}) - \\nabla_{x_{t}} \\log p(x_{t}))$, (9)\nwhere\n$\\gamma(x_{t}, c^{-}) = \\frac{p(c^{-}|x_{t})}{1 - p(c^{-}|x_{t})}$, (10)\nwhich can be approximated during the reverse diffusion process to adjust the guidance scale dynamically.\nNegation for safety in Text-to-Image models. Ensuring the safe deployment of text-to-image (T2I) models requires careful avoidance of specific content and concepts [32, 34, 35]. Key risks include potential privacy breaches, copyright violations, and the generation of harmful or inappropriate content [3, 37]. Addressing these concerns necessitates a multi-stage approach spanning pre-training, fine-tuning, and inference. During pre-training, targeted filtering is applied to the dataset to exclude harmful content, though this is both resource-intensive and challenging to enforce exhaustively. At the fine-tuning stage, various unlearning techniques have emerged [14, 43, 47], often focusing on modifying specific components, such as the cross-attention layer, to effectively \u201cforget\u201d designated concepts. However, these methods are limited by the difficulty of predefining all undesired concepts. At inference, negative guidance serves as a cost-effective solution to restrict specific content generation without additional model tuning. Our experiments validate the efficacy of our approach within this context, highlighting its practical advantages in managing content safety at deployment."}, {"title": "3. Main Contribution: Contrastive CFG", "content": "Limitations of different negative guidance. The downside of the negated CFG (nCFG) term in (7) can be explained in two different aspects: the sampling distribution involves"}, {"title": "4. Experimental results", "content": "In this section, we tested how the suggested guidance term can steer the sample to satisfy or exclude the certain condition, along with its effect into the sample quality. We tested the nCFG and DNG for the baseline comparison, as they are best fit for the scope of training-free diffusion sampling methods for concept negation. Although our derivation is based on (5) and requires the use of CFG++, for a fair comparison with nCFG and DNG which are based on CFG, we conduct experiments using the original CFG across a range of scenarios where CFG is applicable, including class-conditioned image generation and text-to-image (T2I) generation. In the Supplementary material, we also provide a CFG++ version of CCFG to validate its theoretical correctness. All experiments were performed with deterministic DDIM sampling.\nGuidance on class-conditioned models. To examine the performance of CCFG on class-conditioned diffusion models, two different image datasets of MNIST [26] and CIFAR10 [25] were considered. Both dataset comprises 10 classes which are relatively exclusive to each other.\nWe first trained a diffusion model based on DDPM [19] for each dataset, that can perform both conditional and unconditional generation to enable CFG. After the generator is trained, we apply different negative sampling methods to each class and calculate the error rate of 1,000 samples, which is the portion of the samples that match the forbidden class according to separately prepared external classifiers. The mean error rate for all 10 classes is measured for the precision of the utilized negative sampling method. To quantify the output image quality, we measured the Fr\u00e9chet Inception Distance (FID) [17] between 10,000 sampled images and the real data. The DDPM models were trained with 500 noise timesteps. For sampling, DDIM sampling [38]"}, {"title": "5. Discussion", "content": "Analysis on guidance scale. We plot the guidance term and objective function of CCFG in Figure 6 as the difference between $\\epsilon_{\\emptyset}$ and $\\epsilon_{c}$ changes. For visual simplicity, we considered the case of one-dimensional data. We also included the plot of the guidance term from CFG to visualize the difference between the behavior of the two sampling methods.\nThe contrastive loss for positive guidance is convex so that the denoising direction can be guided toward $\\epsilon_{c}$ via gradient descent. As $||\\epsilon_{c}-\\epsilon_{\\emptyset}||$ increases, the guidance term approximates a linear function as the original CFG. On the other hand, the negative contrastive loss and its guidance term flatten out to 0 as $||\\epsilon_{c} - \\epsilon_{\\emptyset}||$ gets bigger, while negated CFG diverges to infinity. This demonstrates the instability of the nCFG term, and how CCFG can resolve this issue by canceling the gradient guidance when $\\epsilon_{\\emptyset}$ from the current sample $x_{t}$ is sufficiently unrelated to the condition.\nThe behavior of nCFG on different scenarios. So far, we've shown the theoretical pitfall of the nCFG and demonstrated its negative effect on the sample distribution and its quality. Meanwhile, we observed that the criticality of nCFG's drawback has been mitigated as the learned data distribution and the nature of the used condition get more complex: the nCFG term completely shifts the sample distribution out from the marginal support in the toy example of Figure 3, but often produced reasonable images for T2I generation tasks. Indeed, many off-the-shelf implementations"}, {"title": "6. Conclusion", "content": "In this work, we proposed CCFG, a sampling method to guide diffusion samples to satisfy or repel the given condition, by optimizing the denoising direction with contrastive loss. Through the experiments on various datasets, we showed that CCFG resolves the downsides of the widely used negated CFG term, resulting in high-quality samples while faithfully avoiding unwanted attributes. Despite its effective computational overload and performance, one limitation could be the absence of analytic closed-form sampling distribution that corresponds to the proposed CCFG sampling. It would be a possible future work to propose a negative guidance that allows a probabilistic interpretation while persisting the suggested advantages of CCFG. We believe that CCFG has the potential to be easily integrated and benefit various downstream applications that utilize negative sampling."}, {"title": "A. Additional results", "content": "A.1. CCFG in perspective of guided sampling on the posterior mean.\nIn contrast, in Alg. 1, the iteration reads\n$x_{t-1} = \\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}x_{t} + (\\sqrt{1-\\bar{\\alpha}_{t-1}} - \\sqrt{1-\\bar{\\alpha}_{t}}) \\epsilon_{\\theta} - \\omega\\lambda \\frac{\\sqrt{1-\\bar{\\alpha}_{t-1}}}{\\sqrt{\\bar{\\alpha}_{t}}} (\\epsilon_{c} - \\epsilon_{\\theta}).$ (25)\nTherefore, by setting\n$\\rho_{t} := \\omega (1 + \\frac{\\sqrt{\\bar{\\alpha}_{t}}}{\\sqrt{1-\\bar{\\alpha}_{t-1}}}),$ (26)\nwe can show the equivalence between Alg. 1 and Alg. 2. Hence, for a direct and fair comparison of CCFG against conventional CFG, we chose CCFG with Alg. 1 as our implementation.\nA.2. Quantitative metrics for Text-to-Image negative sampling."}, {"title": "B. Experimental details", "content": "B.1. Sampling hyperparameters."}]}