{"title": "Highly Efficient and Unsupervised Framework for Moving Object Detection in Satellite Videos", "authors": ["Chao Xiao", "Wei An", "Yifan Zhang", "Zhuo Su", "Miao Li", "Weidong Sheng", "Matti Pietik\u00e4inen", "Li Liu"], "abstract": "Moving object detection in satellite videos (SVMOD) is a challenging task due to the extremely dim and small target characteristics. Current learning-based methods extract spatio-temporal information from multi-frame dense representation with labor-intensive manual labels to tackle SVMOD, which needs high annotation costs and contains tremendous computational redundancy due to the severe imbalance between foreground and background regions. In this paper, we propose a highly efficient unsupervised framework for SVMOD. Specifically, we propose a generic unsupervised framework for SVMOD, in which pseudo labels generated by a traditional method can evolve with the training process to promote detection performance. Furthermore, we propose a highly efficient and effective sparse convolutional anchor-free detection network by sampling the dense multi-frame image form into a sparse spatio-temporal point cloud representation and skipping the redundant computation on background regions. Coping these two designs, we can achieve both high efficiency (label and computation efficiency) and effectiveness. Extensive experiments demonstrate that our method can not only process 98.8 frames per second on 1024 \u00d7 1024 images but also achieve state-of-the-art performance. The relabeled dataset and code are available at\nhttps://github.com/ChaoXiao12/Moving-object-detection-in-satellite-videos-HiEUM.", "sections": [{"title": "1 INTRODUCTION", "content": "As satellite video technology has progressed over the last decade, video satellites capable of observing the Earth continuously with high temporal resolution have emerged as an essential tool for Earth observation. Consequently, more and more video satellites and satellite constellations will be launched. Satellite videos can be accessed more easily than before, enabling more research opportunities and applications. Therefore, satellite video intelligent interpretation has become a pressing need, and the new frontier in remote sensing [1]. Moving object detection in satellite videos (SVMOD), aiming to locate objects of interest, is a fundamental task in satellite video interpretation and has various applications including military surveillance, transportation planning, and public security [2, 3, 4]. Recently, MOD in satellite videos has received increasing attention. Despite its significance, fast and accurate SVMOD is difficult due to at least the following challenges in realistic applications.\n(1) Real-time requirements. As there are vast amounts of satellite videos that are temporally redundant, of large spatial size, and used for time-critical applications, real-time SVMOD methods are important. While in satellite videos, the background is temporally redundant, the foreground is highly sparse, and dynamic information is critical, how to make good use of these characteristics to improve the efficiency of SVMOD is essential.\n(2) High-quality requirements. Moving objects in satellite videos 1 are usually tiny, dim (low local contrast between foreground and background), with little shape and texture in-formation, and sensitive to noise. These intrinsic characteristics increase the difficulty of learning high-quality (accurate and robust) object representations. Therefore, how to develop effective and specific frameworks tailored for SVMOD balance recall and precision is also important.\n(3) High annotation cost. Just due to the aforementioned characteristics of moving objects and satellite videos, manually labeling these objects requires repeated inspections to confirm moving objects, which easily leads to noisy labels and is costly. It is difficult to obtain large amounts of accurately annotated training data. Therefore, how to develop label efficient solutions for SVMOD is valuable for realistic applications.\nExisting methods can be divided into two main categories: classical model-based [2, 3, 5, 6, 7, 8, 9, 10, 11, 12], and modern learning-based [4, 13, 14, 15]. In the former category, differencing methods [2, 9] and robust PCA methods [3, 5, 6, 10, 11, 12, 16, 17] have been widely studied over the last decade. However, methods in this category mainly rely on motion information to detect moving objects, which can be easily influenced by dynamic clutters caused by uneven platform movement and illumination changes, resulting in suboptimal performance.\nRecently, deep learning has brought promising progress for SVMOD [4, 13, 15]. However, unlike the extensively studied problem of generic object detection [18, 19] in the field of computer vision, deep learning for the problem of SVMOD remains largely underexplored, with only a few works. Representative methods focus on exploring spatio-temporal information to detect moving objects [13, 14, 15]. For example, Xiao et al. [13] explores static semantic information and dynamic motion cues to tackle SVMOD, which utilizes computation-intensive 3D convolution to extract spatio-temporal information. Pi et al. [15] utilize weighted element-"}, {"title": "2 RELATED WORK", "content": "The rest of this paper is organized as follows. Some related works are reviewed in Section 2. The proposed framework is described in Section 3. Section 4 presents the experimental setup and results in detail. Section 5 concludes this paper.\n2.1 Methods for SVMOD\nCurrently, methods for SVMOD can be divided into model-based methods and learning-based methods.\nModel-based methods mainly exploit the prior knowl-edge of the targets to obtain SVMOD results, including frame differencing-based methods [20, 21] and background subtraction-based methods [18, 22, 23, 24]. The former exploits two- or three-frame differencing methods to extract moving vehicles, and the latter reconstructs the background to obtain moving foreground through background subtraction. However, the model-based methods rely on hand-crafted features with complex parameter tuning processes and can not handle severe scenario changes with fixed hyper-parameters.\nRecently, with the powerful modeling ability, learning-based methods have improved the detection performance of both general object detection [18, 25] and moving vehicles in satellite videos [13, 14, 15, 26]. Lalonde et al. [14] proposed a two-stage network named ClusterNet to detect small objects in airborne images. To handle images with large sizes, they first extracted regions of interest in the first stage and then got the fine-grained detection results in the second stage. Xiao et al. [13] proposed a two-stream network to combine static context information from a single image and dynamic motion cues from multiple images, which significantly promoted the detection performance but with limited efficiency due to the intensive computation of multiple frames. Pi et al. [15] proposed to integrate motion information from adjacent frames through frame differencing and exploit transformer to refine the extracted features for better discrimination ability of actual moving targets. Feng et al. [26] proposed a semantic-embedded density adaptive network named SDANet to incorporate the semantic features of the road to emphasize the regions of interest. In conclusion, learning-based methods have been demonstrated to be effective for SVMOD. However, the existing learning-based methods process images in dense representation and multi-frame form, hindering them from balancing efficiency and effectiveness and extracting long-term spatio-temporal information.\nIn this paper, we explore the sparsity of moving vehicles in satellite videos and process dozens of images in sparse point cloud representation to extract long-term information for better"}, {"title": "3 PROPOSED METHODOLOGY", "content": "We introduce a label self-evolution unsupervised frame-work in which pseudo-labels can evolve with the training process that is well compatible with existing learning-based detection methods for SVMOD.\nWe propose a sparse convolutional anchor-free detection network for SVMOD, which exploits the sparsity of moving targets to skip redundant computation on background regions. Unlike existing methods, our method is the first attempt to handle SVMOD via a sparse spatio-temporal point cloud representation.\nTo verify the detection performance of small and dim moving targets in satellite videos, we propose a new SVMOD dataset with re-labeled dim and small moving vehicles from the VISO dataset. We also provide a new benchmark with various methods under the newly labeled dataset.\nIn this section, we first illustrate our label self-evolution unsupervised framework and then present the sparse convolutional anchor-free moving object detection network in detail.\n3.1 Label Self-evolution Unsupervised Framework\nDue to the small size and low contrast to the background, moving vehicles in satellite videos are difficult to distinguish from the heavy clutters, thus improving the annotation com-plexity. To relieve the burden of annotating large-scale datasets for SVMOD, we proposed a label-evolution unsupervised framework that utilizes traditional methods for the initialization of labels and updates the labels during training to promote detection performance. The proposed unsupervised framework is shown in Fig. 2 (a).\nThe proposed framework first generates pseudo labels by using a traditional method. Here, we utilize the method in [4] and replace the background reconstruction network with a simple temporal median filter to obtain the background in a fast way. It is worth noting that the proposed framework is flexible and can be applied by using any traditional detection methods. However, due to the limited detection performance of traditional methods, the initial pseudo labels contain some false alarms and miss some dim targets, thus limiting the detection performance of learning-based methods.\nTherefore, we utilize two strategies to improve the quality of the pseudo labels. On the one hand, to alleviate the influence of false alarms, we exploit SORT [33] to get the trajectory of moving objects and utilize trajectory length and velocity constraints to filter out false alarms. On the other hand, to recover the missed dim targets, we iteratively update the initial labels during the training process. Specifically, when the network is trained for every 10 epochs, we use the trained network to infer on the train set to generate new pseudo-labels. To reduce false alarms in these new pseudo-labels, we also apply SORT [33] to filter out false alarms. To prevent the network from over-fitting on the self-generated labels, we retain the initial pseudo-labels and add new labels generated by the trained network as the new train set. Through continuous iteration, the label quality can be improved, thereby improving the detection performance of the learning-based methods.\nIn conclusion, the proposed framework is generic and flexible, in which the traditional and learning-based method can be replaced by arbitrary properly designed methods. Moreover, this paper proves that the spatio-temporal consistency prior of moving objects (i.e., the consistency of the trajectory of targets) can help learning-based methods achieve good performance without any manual annotation.\n3.2 Sparse Convolutional Anchor-free Moving Object De-tection Network\nDue to the extreme imbalance between foreground and background regions, most computational resources of learning-based methods are assigned to uninformative background areas, which incurs tremendous redundant computational burdens and hinders the extraction of long-term spatio-temporal information.\nMotivated by the sparsity of moving targets in satellite videos, we propose a sparse convolutional anchor-free moving object detection network to exploit the sparsity prior for both effective and efficient SVMOD. As shown in Fig. 2 (b), Our proposed network consists of three parts, i.e., the sparse"}, {"title": "4 EXPERIMENTAL RESULTS AND DISCUSSIONS", "content": "sampling module, the sparse backbone, and the sparse detection head.\n(1) Sparse Sampling Module: Due to the extremely small sizes, the moving vehicles in satellite videos occupy only a small portion of the whole image, which can be considered intrinsically sparse [3, 6, 7, 34]. Figure. 3 illustrates the average target ratio of the test set, which demonstrates that the target is sparse and the background area dominates the whole image.\nTo reduce the redundant background regions in satellite videos, we design a sparse sampling module that exploits frame-differencing to subtract background regions. The pro-cedure is shown in Fig. 4. We first estimate the background using a temporal median filter. Then, the residual images are generated by subtracting the estimated background from the original image sequence. In this way, the background can be significantly reduced. However, many small residuals still exist in the background regions. To further alleviate the influence of background regions, we leverage an adaptive threshold to segment the candidate target regions.\nWe utilize an adaptive threshold with the parameter k to produce a proper threshold, which can be obtained by\n$th = \\mu + k \\times \\sigma$,\nwhere $\\mu$ and $\\sigma$ denote the mean and standard deviation of the residual image, respectively. k is a predefined value that controls the maximum threshold.\n4.1 Dataset Description\nThe detection performance of the proposed method is evaluated on satellite videos from Jilin-1 satellite. The ground sampling distance (GSD) of the dataset is 0.92 meters, and the frame rate is 10 frames per second. The moving vehicles in the dataset are labeled by bounding boxes as the ground truth.\nIn the previously released dataset, most dim targets are ignored for annotation, which would incur an unfair comparison of different methods. To address this issue, we relabel the test set to include dim and small targets for a fair comparison. Through relabeling, we find that the former dataset missed a lot of dim targets. Figure. 7 shows the annotations of several typical scenarios before and after relabeling. It can be observed that many dim targets (in red rectangles) are relabeled to evaluate the dim moving target detection performance of different methods.\n4.2 Implementation Details and Evaluation Criteria\nWe used 20 consecutive frames as input. The batch size was set to 6 with a random crop image patch size of 256 \u00d7 256."}, {"title": "5 CONCLUSION", "content": "4.3 Comparison to the State-of-the-arts\nIn this subsection, we present the detection results and analyses of different SVMOD methods. We compare the proposed method with several state-of-the-art methods, including 2 frame-differecing based methods (i.e., D&T [2] and MMB [9]), 4 RPCA-based methods (i.e., GoDec [16], DECOLOR [5], E-LSD [3] and B-MCMD [6]), 2 supervised learning-based methods (i.e., ClusterNet [14] and DSFNet [13]), and 1 unsupervised learning-based method (DeepPrior [4]).\n(1) Quantitative Results. The quantitative results are shown in Table 1. It can be observed that our proposed HiEUM can achieve the best detection performance with the highest average F1 score of 89.7%, outperforming the second-best method DSFNet with 15.3%. We attribute this to two reasons. On the one hand, we have iteratively updated the training labels, which can evolve for better quality labels and include unlabeled dim targets. On the other hand, due to the low memory occupancy and computational burdens, our HiEUM can process more frames for long-term spatio-temporal information to promote detection performance. For example, our HiEUM can process 20 frames to predict all the results of the input frames at a time, while DSFNet [13]) can only process 5 frames for the results of one frame.\n(2) Qualitative Results. Figure. 8. It can be observed that our proposed method has detected all the moving targets in both scenes while the comparative methods have more or less missed targets and false detections. Note that the improvement of our method is more significant when handling dim targets (such as video 6 with many dim targets). We attribute this to long-term spatio-temporal information modeling, which can benefit the detection of dim and small moving targets in satellite videos. Moreover, our method is more robust to the dynamic clutters in the scene (such as video 1 with dynamic illumination changes), which is also benefited from the long-run spatio-temporal information.\n(3) Time Efficiency Analysis. As shown in Fig. 1, our HiEUM can process images with the size of 1024 \u00d7 1024 at the speed of 98.8 frames per second, while the fastest comparative method can only run at 5.6 frames per second. That is because our method only processes the valid candidate regions and skips the redundant computation of the background regions.\n4.4 Ablation Study\nIn this subsection, we conduct different ablation studies to investigate the design of HiEUM.\n(1) Effectiveness of the Unsupervised Framework. We use DSFNet [13] as the baseline and train our sparse network under manual annotations, labels produced by traditional methods, and our proposed unsupervised framework, respectively. The quantitative results are shown in Table. 2. It can be observed that, under the same supervision of manual labels, our HiEUM-sup outperforms DSFNet [13] by 7.7% in terms of average F1 score due to the long-term spatio-temporal modeling. Moreover, under the supervision of labels produced by the traditional method, our HiEUM-unsup suffers a performance degradation of 8.0% compared with HiEUM-sup in terms of average F1 score. This is because the labels produced by the traditional method can not cover all the objects in the scene, thus leading to insufficient learning of moving objects. It is worth noting that HiEUM-unsup achieves comparable results to DSFNet [13], which demonstrates the effectiveness of our sparse network. By iteratively updating the labels, our HiEUM can greatly improve the detection performance due to the evolution of the labels."}, {"title": "5 CONCLUSION", "content": "In this paper, we present a generic highly efficient unsupervised framework for SVMOD. Under the proposed framework, we further proposed an effective and efficient sparse network to detect moving vehicles in spatio-temporal 3D point cloud representation, which can model long-term spatio-temporal information with much fewer computational burdens thanks to the intrinsic sparsity of moving vehicles. Extensive experiments have demonstrated the effectiveness and superior efficiency of our proposed method."}]}