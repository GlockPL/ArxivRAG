{"title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMS", "authors": ["Wenxuan Wang", "Xiaoyuan Liu", "Kuiyi Gao", "Jen-tse Huang", "Youliang Yuan", "Pinjia He", "Shuai Wang", "Zhaopeng Tu"], "abstract": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MM-SafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1,500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models' abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MM-SafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research. WARNING: This paper contains unsafe contents.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs), such as GPT-4V (OpenAI, 2023b) and Bard (Google, 2023), have recently been released and widely deployed. Unlike traditional Large Language Models (LLMs) that operate solely on textual inputs, MLLMs enable users to interact with models using image inputs as well. This advancement expands the impact of language-only systems by introducing novel interfaces and capabilities, allowing MLLMs to tackle new tasks such as mathematical reasoning (Lu et al., 2023), medical diagnosis (Yan et al., 2023; Wang et al., 2024c; Liu et al., 2024), and code generation (Wan et al., 2024a,b).\nThe safety of LLMs is a broad concept encompassing measures and practices that prevent these models from causing harm or acting in unethical, incorrect, or biased ways (OpenAI, 2023a). Ensuring safety is at the core of developing and deploying LLMs and has drawn significant attention from both academia and industry. An essential aspect of LLM safety is safety awareness, meaning that an LLM should be able to correctly identify whether a piece of information\u2014such as a user query or model response is safe or not. Previous studies have shown that LLMs are more likely to generate unsafe content when presented with unsafe queries (Sun et al., 2023). Identifying unsafe queries is thus a helpful and necessary first step in preventing models from generating unsafe responses. Furthermore, LLMs are increasingly used as judges to assess the safety of their own responses, making safety awareness a critical capability."}, {"title": "2 Background", "content": null}, {"title": "2.1 Multi-modal Content Understanding", "content": "Multi-modal content (e.g., a meme or video) has different modalities to convey information. Therefore, to understand the whole picture of multimedia content and determine its toxicity, one needs not only to process the information in every single modality but also to fuse the information from different modalities (Gao et al., 2020; Kiela et al., 2020). The fusion of different modalities is generally performed at two levels: feature level and decision level. In the feature-level fusion approaches, the features extracted from different modalities are first combined and then sent as input to a single analysis unit that performs the analysis task. In the decision-level fusion approaches, the analysis units first provide the local decisions that are obtained based on individual features from different modalities. The local decisions are then combined using a decision fusion unit to make a fused decision. The main advantage of decision-level fusion is that it can use the most suitable methods to analyze every single modality. However, it fails to utilize the feature-level correlation among modalities (Ahmed et al., 2023)."}, {"title": "2.2 Datasets for Safety Evaluation.", "content": "We systematically reviewed the papers on benchmarking and evaluating the safety of large language"}, {"title": "3 Constructing MMSafeAware Benchmark", "content": "In this section, we will introduce how we construct the MMSafeAware Benchmark. Our benchmark consists of two subsets: the unsafe subset to measure whether an MLLM can identify unsafe contents and the over-safety subset to measure if an MLLM is over-sensitive. We will illustrate how we construct the two subsets in Section 3.1 and Section 3.2"}, {"title": "3.1 Unsafe Subset", "content": "To enhance the thoroughness of our benchmark across diverse safety scenarios, we conducted a systematic review of prior research on LLM safety, identifying three main scenarios that encompass 17 specific types of unsafe scenarios. For each type of unsafe scenario, we manually design image-text pairs as test cases under the following rule:\nEach test case is an image-text pair that both image and text are benign when considered alone, while it is unsafe when considered together.\nMMSafeAware encompasses 14 types of typical unsafe concerns, which can be categorized into the following four aspects:\n\u2022 Physical Safe that can causes direct physical damage or injury to an individual, such as self-harm, biomedical harm, and violence.\n\u2022 Mental Safe that can negatively impacts an individual's psychological, emotional, or cognitive well-being, such as pornography, mental stress, and abusive language.\n\u2022 Property Safe related to property, such as economic harm, copyright and privacy.\n\u2022 Society Safe related to society, such as hate speech, bias and stereotypes, ethics and morality, misinformation and crime."}, {"title": "3.2 Over-Safety Subset", "content": "Inspired by a recent study on the over-sensitive of language models (R\u00f6ttger et al., 2023), our MM-SafeAware benchmark also incorporates an over-safety subset designed to assess whether a multimodal LLM is over-sensitive.\nFor each type of over-safety scenario, we manually design image-text pairs as test cases under the following rule:\nEach test case is an image-text pair that either image or text is unsafe when considered alone, while it is safe when considered together.\nMMSafeAware encompasses eight of the ten over-safety scenarios introduced by (R\u00f6ttger et al., 2023) that are applicable to multimodal LLMs. These scenarios include definitions, homonyms, safe targets, safe contexts, real discrimination against nonsensical groups, nonsensical discrimination against real groups, privacy concerns in public contexts, and privacy concerns in fictional scenarios. (R\u00f6ttger et al., 2023) focused on textual prompts, designing prompts that are clearly safe but superficially resemble unsafe prompts in terms of the vocabulary they use. We have tailored this approach for multimodal LLMs, focusing on text-image pairs. MMSafeAware also includes four new over-safety scenarios, including tautological (needless repetition of a word), copyright for public domain, fake goal hijacking, and fake role playing. To sum up, MMSafeAware over-safety subset covers 12 safety types with 500 image-text pairs. An ideal LLM needs to find a good trade-off between helpfulness and harmlessness (Bai et al., 2022). An over-sensitive LLM may refuse to comply with not only unsafe inputs but also safe ones, leading to a decrease in helpfulness. Hence, by pairing the unsafe with over-safety subset, MMSafeAware can more effectively assess the safety awareness of the evaluated multimodal LLMs."}, {"title": "3.3 Quality Control", "content": "We guarantee the quality of our benchmark by vetting each image-text pair through human annotation. Specifically, we recruited three annotators with Bachelor's degrees and proficiency in English, to label whether MMSafeAware Benchmark only contains image-text pairs that meet our criteria.\nFor each image-text pair in the benchmark, we ask the annotators to answer three questions: 1) considering the image alone, do you think the image is unsafe? 2) considering the text prompt alone, do you think the text prompt is unsafe? and 3) considering the image and text prompt, do you think the text prompt is unsafe under the context of the image? Each annotator works separately and we conduct a majority voting to make the final decision.\nFor the image-text pairs in the unsafe subset (both the image and text look harmless when considered alone but the semantic is unsafe when considered together), if the annotation is \"Yes\" for question 1 or 2, or \"No\" for question 3, we discard this image-text pair. For the image-text pairs in the over-safety subset (either the image or text looks unsafe when considered alone but the semantic is safe when consider together), if the annotation is \"No\" for both question 1 and 2, or \"Yes\" for question 3, we discard this image-text pair. Finally, 4.7% of the image-text pairs are discard after this process."}, {"title": "3.4 Evaluation Method", "content": "We input the image-text pairs to MLLMs with the following prompt:\n{Image}\nGiven the image, do you think the following sentence is safe? Please start with \"Yes\" or \"No\".\n\"{Text}\"\nwhere \"{Image}\" and \"{Text}\" denote the input image and text, respectively.\nFor test cases in the unsafe subset, the expected response is \u201cNo\u201d. Higher accuracy indicates that the evaluated MLLM is safe. Conversely, for test cases in the over-safety subset, the expected response is \"Yes\". Higher accuracy indicates that the evaluated MLLM is not being overly sensitive."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "We use MMSafeAware to evaluate 6 widely used close-Sourced MLLMs, as well as 3 open-sourced MLLMs, the details of which are listed in Table 3. We follow the default setting on their official website to call the models."}, {"title": "4.2 Main Results", "content": "MLLMs are not safe enough. As is shown in Tabel 3, MLLMs exhibit significant gaps in accurately identifying unsafe content. For example, GPT4-V and Bard show low performance in the unsafe subset, with an accuracy of 63.9% and 73.8% respectively, indicating the potential reliability issues as a safety judge. We also show the specific performance of MLLMs in different scenarios in Figure 3 (a), indicating that different MLLMs perform variously in different scenarios.\nMLLMs all suffer from severe over-sensitive issues. As is shown in Tabel 3, MLLMs have much lower accuracy on the over-safety subset, indicating that all the MLLMs are over-sensitive. For example, Claude-3 erroneously classified 98.9% of test cases in the over-safety subset as unsafe. This can significantly affect the helpfulness. We also show the specific performance of MLLMs in different scenarios in Figure 3 (b), indicating that different MLLMs perform variously in different scenarios.\nMLLMS show a trade-off between safety and over-safety. Claude-3 exhibits a near-perfect performance in identifying unsafe content, but its performance dramatically dips in the over-safety subset, where it fails to recognize almost all the test cases as non-threatening. On the other hand, GPT-4V can only achieve 63.9% accuracy on identifying unsafe content, but can suffer less from the over-sensitive issue, with an accuracy of 41.1%. This indicates that training a safe but not over-sensitive MLLM is still a challenging task.\nSafey-Aware system prompt can make MLLMS safer but more over-sensitive. Previous work (Wang et al., 2024d) adopts system prompts to improve the harmlessness of LLMs. In this section, we investigate the effect of safey-aware system prompts on our benchmark. Specifically, we adopt the following system prompt:\nYou are a helpful and harmless assistant. Please answer safely.\nAs is shown in Tabel 4, the safety-aware system prompt increases the accuracy in the unsafe subset, while decreasing the accuracy on the over-safety subset, indicating that safety-aware system prompt can make MLLMs safer but more over-sensitive.\nCase Study We present a case study in Table 5, aiming to provide insights into why the MLLMS fail. We summarize 5 reasons, ranked in frequency, such as making the decision only based on partial modality, lack of some factual knowledge, or misunderstanding the generated image."}, {"title": "4.3 Improving Multimodal Safety Awareness", "content": "Understanding the failures of Multimodal Safety Awareness. Previous case studies demonstrate that MLLMs tend to focus on partial information from a specific modality when identifying whether a multimodal content is safe or not. To further illustrate this phenomenon, we analyze the overall input-output relevancy scores for MLLMs (Stan et al., 2024), identifying the most relevant parts of the input to the model prediction.\nFigure 2 illustrates a case in which the model LLAVA-1.5-7B fails to accurately answer a test case from the over-safety subset, representing a common failure pattern. Specifically, the model assigns greater attention to the textual input than to the visual context. The attended tokens {\"kill\", \"I\", \"you\"} guide the model to generate \u201cNo\u201d, while the image tokens are underutilized for the answer generation.\nImproving the Multimodal Safety Awareness. Based on the above observation, we explore different methods to encourage the model to consider the information from both image and text to improve the safety awareness of multimodal LLMs. Specifically, we adopt three level of methods, prompting method for close-sourced MLLMs, Visual Contrastive Decoding and Vision-Centric Reasoning Fine-tuning for open-sourced MLLMs.\n\u2022 Prompting: a direct method that explicitly instructs MLLMs to consider both image and text by adding prompt \u201cPlease consider the meaning of the sentence under the context of the image.\""}, {"title": "5 Related Work", "content": "A branch of previous works has focused on specific safety areas in LLMs, such as toxicity (Hartvigsen et al., 2022), bias (Dhamala et al., 2021; Wan et al., 2023), copyright (Chang et al., 2023) and psychological safety (Huang et al., 2023). There is also some work on the development of holistic safety datasets. (Ganguli et al., 2022) collected 38,961 red team attack samples across different categories. Ji et al. (2023) collected 30,207 question-answer (QA) pairs to measure the helpfulness and harmlessness of LLMs. Sun et al. (2023) released a comprehensive manually written safety prompt set on 14 kinds of risks. However, most of the safety datasets above are text- or image-only, hindering the study on multi-modal safety.\nMore recently, with the popularity of MLLMs, a few concurrent works have also worked on the safety of multimodal LLMs (Wang et al., 2024b,a). For example, MM-Safety (Liu et al., 2023) is a dataset designed for conducting safety-critical evaluations of MLLMs. However, it only comprises 13 scenarios and does not evaluate the over-safety issue. MossBench (Li et al., 2024) is a multimodal oversensitivity benchmark with 3 types of over-safety scenarios. However, our benchmark is a more comprehensive safety awareness benchmark for MLLMs, involving both an unsafe subset and an over-safety subset and comprising 29 different safety scenarios."}, {"title": "6 Conclusion", "content": "In this work, we introduced MMSafeAware, a comprehensive benchmark designed to evaluate the safety awareness of MLLMs. Through the careful construction of 1,500 image-prompt pairs across 29 safety scenarios, we provided a rigorous tool for assessing both unsafe and over-safety situations in MLLMs. Our extensive evaluations of nine popular MLLMs revealed significant shortcomings in safety awareness, with models frequently misclassifying unsafe content as safe and exhibiting over-sensitivity that affects their helpfulness. We explored three methods to enhance safety awareness but found that none fully address the challenges posed by MMSafeAware. These findings highlight the urgent need for more effective strategies in developing MLLMs that are both safe and helpful."}, {"title": "Limitations", "content": "The main limitation that offers avenues for future research is that none of the improving methods can fully address the challenges posed by MMSafeAware. More effective methods are needed to further enhance the safety awareness of MLLMs."}, {"title": "Ethical Concerns", "content": "This paper designs a benchmark including toxic images. However, we highlight that the goal of our paper is not to generate toxic images, but to reveal a severe safety issue in MLLM safety awareness. This work not only raises awareness about the potential dangers associated with MLLM safety but also paves the way for future research and development of more secure and ethical AI systems."}]}