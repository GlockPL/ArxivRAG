{"title": "Continuous-Time Linear Positional Embedding for Irregular Time Series Forecasting", "authors": ["Byunghyun Kim", "Jae-Gil Lee"], "abstract": "Irregularly sampled time series forecasting, characterized by non-uniform intervals, is prevalent in practical applications. However, previous research have been focused on regular time series forecasting, typically relying on transformer architectures. To extend transformers to handle irregular time series, we tackle the positional embedding which represents the temporal information of the data. We propose CTLPE, a method learning a continuous linear function for encoding temporal information. The two challenges of irregular time series, inconsistent observation patterns and irregular time gaps, are solved by learning a continuous-time function and concise representation of position. Additionally, the linear continuous function is empirically shown superior to other continuous functions by learning a neural controlled differential equation-based positional embedding, and theoretically supported with properties of ideal positional embedding. CTLPE outperforms existing techniques across various irregularly-sampled time series datasets, showcasing its enhanced efficacy.", "sections": [{"title": "1. Introduction", "content": "Irregularly sampled time series data, characterized by measurements taken at non-uniform intervals, is prevalent in practical scenarios with applications spanning medical monitoring (Silva et al., 2012), finance (Dal Pozzolo et al., 2015), and environmental sensing (Menne et al., 2015). The accurate forecasting of outcomes in these diverse domains is crucial for avoidings risks and maximizing benefit. Previous research heavily focused on regular time series forecasting, typically relying on transformer architectures. The self-attention mechanism of transformers demonstrate a remarkable ability to capture long-range dependencies and interactions, making them well-suited for time series analysis (Wen et al., 2022). However, these models for regular time series struggle with irregular time series due to their inability to accommodate a variety of inconsistent observation patterns and irregular time gaps.\nThe self-attention mechanism within the transformer model exhibits permutation invariance, signifying that self-attention inherently lacks knowledge of the sequential order in the data. Consequently, transformers rely on positional embeddings to integrate temporal information into time series analysis. Therefore, to adapt high-performing transformers designed for regular time series to irregular time series, modifying positional embedding is mandatory. By allowing the transformer to capture irregular temporal characteristics effectively through positional embedding, the self-attention of transformers can learn dependencies considering the temporal context of the time series.\nTo develop a positional embedding tailored for irregular time series, it becomes imperative to address two distinct challenges: inconsistent observation patterns and irregular time gaps. First, to provide positional embedding for any inconsistent observation patterns, positional embedding must be defined on continuous time. Otherwise, distinct observations share same parameters for representing positions which causes interference. Therefore, a learnable continuous function must be used as the positional embedding to handle any possible observation patterns. The second challenge is representing the irregular time gaps. Unlike regular time series or natural language which have equal temporal/semantic distance between neighboring tokens, the temporal context of irregular time series largely depend on the distance between observations. However, the self-attention of transformers do not have input space to directly learn from observed time gap. Therefore, positional embedding for irregular time series should represent positional information concisely, which indirectly represent the irregular time gaps between observations.\nConventional positional embedding in regular time series forecasting consists of the widely adopted sinusoidal positional embedding(Vaswani et al., 2017) with uniform embedding(Zhou et al., 2021) which embeds date information. The most popularly used positional embedding across multiple domains that use transformers is learnable positional embeddings, which introduce parameters that can adapt to data domain during the model training process. However, both of these conventional methods fail to solve the aforementioned challenges when applied to irregular time series data. First, both embedding consists of limited size of learnable parameters same as the input size. The limited number of parameters cannot cover all inconsistent observation patterns of irregular time series. Another reason is that exiting positional embeddings fail to capture irregular time gaps. This is because regular time series and other domains like natural language have consistent gaps between data points and therefore positional embedding is only designated to represent the order of the data.\nTo address the aforementioned challenges, we propose our method Continuous-Time Linear Positional Embedding (CTLPE), designed to tackle the intricacies associated with applying irregular time series data to transformers for regular time series. CTLPE is able to provide positional embedding value to any inconsistent observation patterns as it is defined on continuous-time. The linear form is derived from specific properties that facilitate the representation of position, monotonicity and translation invariance (Wang et al., 2020), effectively handling irregular time gaps. Furthermore, we design neural controlled differential equation-based positional embedding (NCDE-PE) to empirically prove that linear form is superior to any other continuous form of positional embeddings.\nIn summary, our primary contributions can be summarized as follows.\n\u2022 Proposing CTLPE, a novel positional embedding method that bridges the gap between models for regular and irregular time series forecasting.\n\u2022 Demonstrating how CTLPE effectively represents positional information of irregular time series through learning a continuous function with theoretical basis of representing positions.\n\u2022 Using neural controlled differential equation-based positional embedding to show that linear form is superior to other continuous-time positional functions.\n\u2022 Outperforming existing positional embedding techniques when applied to forecasting tasks across various irregularly-sampled time series datasets, highlighting its enhanced efficacy."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Deep Learning Methods for Irregular Time Series Analysis", "content": "To address the issues associated with irregular time series, deep learning methods have been employed to directly tackle tasks using irregular data. RNN-based methods have gained popularity in handling irregular time series (Lipton et al., 2016) (Choi et al., 2016). Among them, GRU-D (Che et al., 2018) has been notably successful in modeling irregular time series by incorporating decay into hidden states to capture irregularities. Nevertheless, a limitation arises as the hidden states undergo significant changes with each observation. In response to this constraint, neural differential equations have been introduced.\nA neural differential equation (NDE) is a differential equation with a vector field that is parametrized by a neural network(Kidger, 2022). The hidden state defined by a vector field is continuous, which allows the irregularly-sampled time series to be processed adequately. Neural ordinary differential equations (NODEs) (Chen et al., 2018) is the starting paper which define residual networks (ResNet) (He et al., 2016) to continuous time, showing strength in modeling generative latent time series models. The proposed model is called Latent ODE which is based on the architecture of a variational autoencoder with a RNN Encoder and a ODE-based decoder. A following work by (Rubanova et al., 2019) further expand latent ODE's encoder to ODE-RNN, where the hidden states are defined continous respect to time. Neural controlled differential equations (NCDEs) (Kidger et al., 2020) reduce the ODE's limitation of high dependence on the initial condition by having a controlling path which considers all inputs."}, {"title": "2.2. Transformers in Time Series Forecasting", "content": "Commencing with LogSparse Transformer (Li et al., 2019), a range of transformer-based time series forecasting models has been introduced, including Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), Pyraformer (Liu et al., 2021), and Fedformer (Zhou et al., 2022). LogSparse Transformer pioneered the application of transformer models to time series forecasting, leveraging their demonstrated excellence in tasks involving the capture of long-range dependencies. Notably, the memory cost is down from $O(n^2)$ to $O(n(log n)^2)$. Informer proposed ProbSparse self-attention to alleviate time complexity down to $O(n log n)$. Autoformer employs decomposition methods to enhance performance, integrating an auto-correlation mechanism for identifying dependencies and facilitating information aggregation. Pyraformer further reduces self-attention time complexity to O(n) through the adoption of pyramidal attention module (PAM). Fedformer introduces frequency-enhanced blocks to capture the seasonality and trend of time series while maintaining a time complexity of O(n).\nDespite these advancements, transformers in time series forecasting face substantial criticism from LTSF-Linear (Zeng et al., 2023). Simple one-layer linear models from this research have been found to outperform all the aforementioned transformer models. However, transformer-based PatchTST (Nie et al., 2023) employs patching and channel-independence to outperform linear models. Consequently, transformers in time series forecasting persist as state-of-the-art, necessitating further research to address irregularities."}, {"title": "2.3. Positional Embedding in Transformers", "content": "Positional embedding is a critical component in transformers as they analyze sequential data, and the self-attention mechanism is permutation invariant. In essence, without positional embedding, a transformer cannot discern the order of the provided data. Consequently, positional embeddings have undergone thorough development. In this section, we focus on some prominent ones in natural language processing and delve deeper into positional embeddings specifically designed for time series.\nThe sinusoidal positional embedding was initially introduced in the transformer paper (Vaswani et al., 2017). The sinusoidal function carries various properties that is suitable for representing positions. Subsequently, the relative positional embedding (Shaw et al., 2018) was introduced to represent relative gaps rather than absolute positions. While these embeddings are fixed and straightforward, they lack learnable parameters that can fit to data. Therefore, following the surge in transformer applications for pretrained language models like BERT (Devlin et al., 2018) and GPT (Radford, 2018), these models began incorporating learnable positional embeddings. This simply adds a learnable linear layer to the input vector, which is then trained with the loss for each downstream task.\nIn the context of time series forecasting, specialized time-specific positional embeddings have been developed. The positional features initially devised for RNN-based time series forecasting (Salinas et al., 2020) have been applied across all transformer-based time series forecasting works, from Logsparse Transformer to the recent PatchTST. Inputting year, month, day-of-the-week, hour-of-the-day, and minute-of-the-hour enables the learning of not only absolute time but also trends and seasonality from other temporal information."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Problem Definition: Irregular Time-Series Forecasting", "content": "A multivariate irregular time series $T$ can be defined as $T = \\{(t_1,x_1), (t_2, x_2)\\ldots, (t_n,X_N)\\}$, where $x_i$ is a $l$-dimensional multivariate variable where each variable can be null. The timestamps $t_i \\in R$ are irregularly sampled such that $t_1 < \\ldots <t_n$.\nWe further define the problem of irregular time-series forecasting. Given an input lookback window, which forms a multivariate irregular time series $X = \\{(t_1, x_1), (t_2, x_2)\\ldots,(t_N,x_n)\\}$, the goal is to predict subsequent time series of length $M$, $Y = \\{(t_{N+1},X_{N+1}), (t_{N+2},X_{N+2)\u00b7\u00b7\u00b7, (t_{N+M},X_{N+M})\\}$."}, {"title": "3.2. Overall framework of CTLPE with Transformers", "content": "The overall framework of how CTLPE operates within transformers for irregular time series forecasting is illustrated in Figure 1. CTLPE replaces the original positional embedding without making modifications to other components of the model. This adaptation enables the extension of transformer models designed for regular time series to effectively handle irregular time series. The time series for both past data and prediction is divided into two components: the data values and the time features. Data values represent the multivariate values of the time series, while time features solely encompass observation time-related information, including relative and absolute time and date.\nThe data values undergo embedding through the data embedding component of the model. Token embedding is a commonly employed technique in most existing transformer-based models for time series forecasting. This embedding utilizes convolutional layers to generate embeddings for each data point, allowing them to assimilate information from their adjacent data points. However, when dealing with irregular time series forecasting, the neighboring data points could be temporally distant. Therefore, we adapt the convolutional layer to consider only the information of the current data point, enabling other components of the model, including our CTLPE and attention, to learn from the embedding.\nLeveraging the time features of the time series, the positional embedding for irregular time series is generated using CTLPE. The detailed process of CTLPE is elaborated in 3.4. The irregularly spaced time features are learned within the vector space of neural differential equations, leading to the creation of the corresponding positional embedding. This positional embedding is then combined with the data embedding generated from token embedding and sent to the encoder of the transformer.\nAlthough the encoder structure varies slightly among different papers on time series forecasting, they commonly incorporate the self-attention mechanism. The self-attention mechanism within the encoder is designed to understand the relationships between data points at various time intervals. The ability to grasp dependencies across different time points is vital for effective time series forecasting, as it is directly linked to the forecasting task.\nThe decoder utilizes both the output from the encoder and the embedding of the time series for prediction, engaging in cross-attention. This process focuses on understanding the dependencies learned by the encoder and establishing connections with the time series designated for prediction. Subsequently, the prediction results are generated through a multi-layer perceptron. Mean Squared Error (MSE) loss is employed between the prediction results and the ground truth, and this loss is backpropagated through the decoder and all other components of the transformer, including our CTLPE."}, {"title": "3.3. Continuous-Time Linear Positional Embedding (CTLPE)", "content": "While positional embeddings for regular time series only have to consider the order of the data, irregular times series needs to consider the irregular time gaps between observations. We show this in Appendix A.2 that sinusoidal PE fails to represent irregular time gaps as the gap enlarges. Therefore, representing irregular time gap is mandatory for correctly encoding positional information of irregular time series.\nA direct way to learn irregular time gap is to explicitly feed it into the deep learning model. However, transformers do not have architectural support to do this. Therefore, we indirectly learn time gap through providing precise position. How positional embedding can provide ideal positional information is studied in previous works, where monotonicity and translation invariant property stands as the ideal properties of positional embedding. (Wang et al., 2020).\nDefinition 1. Monotonicity: The distance between positional embeddings increases with larger time intervals.\n$\\forall t_1, t_2, t_3 \\in R: ||t_1 - t_2|| > ||t_1-t_3|| \\implies d(p(t_1), p(t_2)) > d(p(t_1), p(t_3))$.\nThis formally represents the requirement for positional embeddings to preserve the sequential order of distances between positions. An embedding that adheres to monotonicity is commonly referred to as ordinal embedding. Monotonicity plays a crucial role in demonstrating that the embedding genuinely captures the positional information inherent in sequential data.\nDefinition 2. Translation invariance: The distance between positional embeddings are translation invariant.\n$\\forall t_1, t_2,\\ldots, t_n, l \\in R : d(p(t_1), p(t_1 + 1)) = d(p(t_2), p(t_2 + 1)) = \\cdots = d(p(t_n), p(t_n + 1))$.\nPositional embeddings that consider the time gaps between observations should adhere to the property of translation invariance. This is especially important in time series tasks, where a long time series is cut into windows. The translation invariant property insures that the positional information is provided correctly in any way the window is created.\nTheorem 3.1. (LINEAR POSITIONAL EMBEDDING) A positional embedding that satisfies monotonicity and translation invariance must be linear. Formally,\n$p(x) = kx + b$.  (1)\nProof.\n$d(p(t_1), p(t_1 + 1))$\n$= d(p(t_2), p(t_2 + 1))$\n$= \\ldots = d(p(t_n), p(t_n+1))$.  (by translation invariance)\n$\\leftrightarrow ||p(t_i) - p(t_i + 1)||^2 = K$  (Eulidean dist, $K\\in R^+$)\n$\\rightarrow p(t_i + 1) - p(t_i) = \\sqrt{K}$   (by monotonicity)\n$\\leftrightarrow ||p(t_i) - p(t_i + 1)|| = \\sqrt{K}$  ($\\sqrt{K} = k, t_i\\rightarrow t_i + 1$)\n$\\rightarrow p(t_i + 2l) - p(t_i + l) = k$\n$p(t_i + 2l) - p(t_i) = 2k$\n$\\rightarrow p(t_i + xl) - p(t_i) = kx$\n$\\leftrightarrow p(x) - p(0) = kx$   ($l = 1, t_1 = 0$)\n$\\rightarrow p(x) = kx + b$    ($p(0) = b$)\nLinear positional embedding is created with two parameters, slope and bias as shown in Algorithm 2. The slope and bias is learned for each data dimension of the data embedding. By learning these parameters through the loss function of the data, the positional embedding represents position without overwhelming the data embedding. We call this the size-aware property of CTLPE where the positional embedding learns its size respect to the size of each data dimension in data embedding."}, {"title": "3.4. Neural Controlled Differential Equation-based Positional Embedding (NCDE-PE)", "content": "To further prove that CTLPE is the best form of any learnable continuous time function, we propose neural-controlled-differential-equation-based positional embedding, NCDE-PE. By learning a vector field through neural differential equations, positional embedding can define any differentiable, therefore continuous function that is learnable from the data. By showing that NCDE-PEalso learn a linear function, we prove that CTLPEeffectively learns the best possible positional embedding for irregular time series.\nDefinition of NCDE-PE: $T$ is cut into windows of n timestamps, where a window is chosen as $w = \\{(t_0, x_0), (t_1, x_1)\\ldots,(t_{n-1},X_{n-1})\\}$. Let the time feature $d_i$ corresponding to $t_i$, = \\{(t_0, d_0), (t_1, d_1)\\ldots, (t_{n-1},d_{n-1})\\}$. Then, $D : [t_0, t_{n-1}] \\rightarrow R^{m+1}$ is a natural cubic spline with knots at $t_0,..., t_{n-1}$. We further define two neural networks from the formation of Neural CDE (Kidger et al., 2020). $f_{\\theta_1} : R^m \\rightarrow R^{w\\times m}$ a neural network with parameter $\\theta_1$, and $\\zeta_{\\theta_2}: R^m \\rightarrow R^{w}$ with $\\theta_2$. Then, NCDE-PE is defined as the solution of CDE\n$P_{t} = P_{t_0} + \\int_{t_0}^t f_{\\theta_0}(P_s)dD_s, P_{t_0} = \\zeta_{\\theta} (x_0, t_0)$. (2)\nLearning of NCDE-PE: To demonstrate the learning of NCDE-PE, we first explain how neural CDE is trained. Similar to ODE, the adjoint backpropagation method can be applied to CDE, reducing the sequence length computation of neural differential equations' backpropagation to a single computation from end to the start of the sequence. This updates the parameters of the vector field $f_{\\theta_1}$ and the initial state $\\zeta_{\\theta_2}$. Unlike previous works including the original neural CDE paper (Kidger et al., 2020) that use only the last state for classification, NCDE-PE utilizes all hidden states $P_{t_0}, P_{t_1},..., P_{t_{n-1}}$ of neural CDE.This leads to larger computation for each positional embedding during backpropagation. However, this computation is crucial for learning positional embedding and enables faster training of NCDE-PE, incorporating techniques for accelerated training as explained in the following section.\nMost importantly, the learning of NCDE-PE can be conducted on any irregularly spaced time series window. The learning domain of $p_t$ is defined on any time sequence, as a vector space is learned instead of a fixed neural network. Consequently, NCDE-PE learns a suitable positional embedding for irregular time series. Furthermore, the continuous vector field of neural CDE naturally predicts positional embedding for unseen time stamps in training time.\nManaging computational cost: Despite the advantages that differential equations offer for irregular time series forecasting, the recurrent calculations involved in neural differential equations result in extended training times, approximately O(n). Additionally, the Runge-Kutta method, employed for integration approximation, may take considerable time depending on inputs. In contrast, original fixed embeddings or learnable embeddings lack recurrent calculations between embeddings, enabling parallel computation with a time complexity of O(1). To address this challenge, we employ two techniques.\nThe initial technique involves storing NCDE-PE as a table. The embedding values at the desired time can be pre-"}, {"title": "4. Evaluation", "content": ""}, {"title": "4.1. Experiment Setting", "content": ""}, {"title": "4.1.1. DATA DESCRIPTION", "content": "ETT (Zhou et al., 2021) refers to Electricity Transformer Temperature data, which holds significant importance in electric power deployment. This dataset constitutes regular time series data spanning 2 years, encompassing two counties named ETTh\u2081 and ETTh2 with hourly resolution, and ETTm\u2081 with 15-minute resolution. The dataset offers both multivariate (ETTh\u2081M, ETTh2M) and univariate (ETTh\u2081S, ETTh2S) types. As in previous studies, the train, validation, and test sets are divided into 12/4/4 months each.\nWeather dataset comprises hourly climate data collected over 4 years in the U.S. from the National Oceanic and Atmospheric Administration (NOAA). This dataset includes 11 features with a single target value, \"wet bulb.\" Similar to previous work, the train, validation, and test sets are divided into 28/10/10 months each.\nECL (Trindade, 2015) is Electricity Consuming Load data obtained from the UCI Machine Learning Repository. It features 15-minute-sampled electricity consumption data converted to hourly data from 321 users. Following the convention in earlier studies, the train, validation, and test sets are divided into 15/3/4 months each.\nTo introduce irregularity into the time series data, we randomly drop data points from each dataset, resulting in irregular time series data with irregularities of 20%, 40%, and 60%. The purpose of creating irregular data from regular data is to enable the use of other positional embedding methods, such as sinusoidal and time features."}, {"title": "4.1.2. EVALUATION ENVIRONMENT", "content": "We use Informer (Zhou et al., 2021) and FEDformer (Zhou et al., 2022) as base model for applying our CTLPE. RevIN (Reversible Instance Normalization) (Kim et al., 2021) is used to solve distribution shift problem in Informer. MSE (mean squared error) and MAE(mean absolute error) are used as evaluation metrics. The experiments are ran three times and their average and standard deviation are provided."}, {"title": "4.1.3. BASELINES", "content": "\u2022 Uniform representation embedding introduced by Informer (Zhou et al., 2021). This is primarily designed for regular time series forecasting, which utilizes not only time but also date features for creating the positional embedding.\n\u2022 Sinusoidal positional embedding from original transformer paper (Vaswani et al., 2017) is also used due to their demonstrated effectiveness across various tasks."}, {"title": "4.2. Experimental Results", "content": "Table 1 and Table 6 each show irregular multivariate time-series forecasting applied on Informer and FEDformer. Our CTLPE outperforms all other baselines in various missing rates for most of the cases on both architectures. Also, multi-dimensional time feature based method perform worst for most of the cases."}, {"title": "4.3. Qualitative Analysis", "content": "Figure 3 compares the sinusoidal positional embedding and CTLPE. Sinusoidal positional embedding provides different frequencies of sin graph for each data dimension. CTLPE instead learns a linear function with different slope value for each data dimension. By learning simple linear function instead of complex forms, the positional embedding is able to provide more direct positional information in irregular circumstances.\nExperimentally, we observe that CTLPE learns the positional embedding in the below mathematical form for irregular time series forecasting.\n$P_i = (k_1t_i, k_2t_i, \\ldots, k_mt_i) = kt_i$. (3)\nFor m-dimensional positional embedding, the CTLPE learns the slope for each dimension and provides value that is proportional to time. Using this mathematical form, we prove that CTLPE satisfies variety of ideal properties of positional embedding in Appendix."}, {"title": "5. Conclusion", "content": "In this paper, we present CTLPE, a linear learnable embedding for irregular time series forecasting using transformers. By only modifying the positional embedding to our method, the architectures designed for regular time series were adapted to handle irregular time series. This positional embedding method outperformed any other fixed or learnable embeddings from both time series analysis and natural language processing.\nFor future works, we plan to work on transformer-based time-series forecasting models such as PatchTST (Nie et al., 2023). While PatchTST have shown the state-of-the-art in current time series forecasting models, the patching process adds additional dimension blocking a smooth adapation of CTLPE to PatchTST. Furthermore, we will apply CTLPE to other downstream tasks of time series. The adaptability to inconsistent observation time and the ability to represent irregular time gaps not only benefit forecasting tasks, but other time series tasks like classification or anomaly detection."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Experimetal Results", "content": ""}, {"title": "A.2. Limitations of sinusoidal positional embeddings for irregular time series.", "content": "One critical reason why existing positional embedding methods fail to support irregular time series was due to their inability to represent irregular time gaps. To support this statement, we show this problem with sinusoidal positional embedding since other methods are learnable methods having inconsistent representations."}, {"title": "A.3. Properties of Ideal Positional Embedding", "content": "While the definition of ideal positional embedding is controversial and there is no generally accepted concept, there are variety of properties that have been proposed. Therefore, we organize the properties from previous papers and also specific for irregular time series forecasting."}, {"title": "Property 3. Symmetry : The distance between positional embeddings are symmetric.", "content": "$\\forall t_1, t_2 \\in R: d(p(t_1), p(t_2)) = d(p(t_2), p(t_1))$. (4)\nWhile symmetry seems natural for positional embeddings, relative positional embeddings do not meet this property and therefore is included. Symmetry is also closely related to inner product which also has symmetric property. The inner product is used many times in the self-attention of transformer. Therefore, the symmetric property of positional embedding is chosen."}, {"title": "Property 4. Inductive : The positional embedding is capable of accommodating sequences longer than those observed during training. For any maximum length l of sequences at training time,", "content": "{dom($\\mathcal{P}_{train}$): dom($p_{train}$) $\\in \\mathbb{N}$, dom($\\mathcal{P}_{train}$) $\\leq$ l}\n$\\rightarrow$ {dom(p): dom(p) $\\in \\mathbb{N}$},\nwhere dom(f) denote the domain of function f. The inductive property allows the positional embedding to be created from shorter time windows, and handle longer windows necessary on test time. Furthermore, the inductive property shows that the postional embedding is not overfit to the training sequence length but is capable of handling variant sequence."}, {"title": "Property 5. Data-driven : The positional embedding can be learned from the data.", "content": "Depending on the periodicity of the time series, different positional embedding should be provided. The data-driven property ensures that the positional embedding is not fixed but can adaptively learn from the data. Sinusoidal embedding (Vaswani et al., 2017) is the representative of fixed embedding, and the feed-forward embedding used in BERT (Devlin et al., 2018) and many other pretrained large language models is a well-known example of data-driven learnable positional embedding."}, {"title": "Lastly, properties desired for irregular time series forecasting are included.", "content": ""}, {"title": "Property 6. Irregularity-Adaptable : The positional embedding can be defined in any irregularly spaced sequence.", "content": "dom(p): dom(p) $\\in \\mathbb{R}$, dom(p) $\\leq$ l. (5)\nThis is the most challenging property, since neural networks are created of regular learnable parameters. Therefore, the conventional positional embedding methods included the popularly used learnable embedding (Devlin et al., 2018) fails to provide positional embedding for irregular sequences. Sinusoidal positional embedding can be extended to handle irregular sequences, but it loses other properties of position while doing so."}, {"title": "A.4. Visualizing sinusoidal and NCDE-PE.", "content": ""}]}