{"title": "Controlled Automatic Task-Specific Synthetic Data Generation for Hallucination Detection", "authors": ["Yong Xie", "Karan Aggarwal", "Aitzaz Ahmad", "Stephen Lau"], "abstract": "We present a novel approach to automatically generate non-trivial task-specific synthetic datasets for hallucination detection. Our approach features a two-step generation-selection pipeline, using hallucination pattern guidance and a language style alignment during generation. Hallucination pattern guidance leverages the most important task-specific hallucination patterns while language style alignment aligns the style of the synthetic dataset with benchmark text. To obtain robust supervised detectors from synthetic datasets, we also adopt a data mixture strategy to improve performance robustness and generalization. Our results on three datasets show that our generated hallucination text is more closely aligned with non-hallucinated text versus baselines, to train hallucination detectors with better generalization. Our hallucination detectors trained on synthetic datasets outperform in-context-learning (ICL)-based detectors by a large margin of 32%. Our extensive experiments confirm the benefits of our approach with cross-task and cross-generator generalization. Our data-mixture-based training further improves the generalization and robustness of hallucination detection.", "sections": [{"title": "Introduction", "content": "The ability of large language models (LLMs) to generate human-like text (Touvron et al., 2023; Ouyang et al., 2022; Penedo et al., 2023) has advanced significantly in recent years, enabling a wide range of applications, from document summarizers (Jin et al., 2024; L\u00e1la et al., 2023) to coding assistants (Yeti\u015ftiren et al., 2023). However, one of the key challenges in deploying these models is the risk of hallucinations - the generation of plausible but factually incorrect information. Hallucinations can occur when the model makes up details that are not grounded in the input or the given context, leading to the generation of misinformation or nonsensical outputs. The tendency to hallucinate raises concerns about the safety and reliability of LLMs in critical domains like finance (Xie et al., 2024) and healthcare (Singhal et al., 2023).\nDespite the debate on the categorization of various types of hallucinations (Huang et al., 2023; Zhang et al., 2023), the type of hallucination is task-dependent. For example, we are likely to see more factual hallucinations in an open-ended question-answering scenario while more logical hallucinations in code generation tasks (Liu et al., 2024). Therefore, it is critical to customize the hallucination evaluation and detection to the specific task.\nPost-hoc hallucination detection approaches detect hallucinations once they have been generated by the LLM. However, the challenge lies in obtaining hallucination datasets for training hallucination detectors in the absence of such hallucinated data. Synthetic hallucination datasets are one of the most commonly used methods to build such detectors (Li et al., 2023). While these detectors provide a high accuracy, critical aspects on the usability of such detectors are overlooked : 1) can these detectors capture the task-specific hallucination patterns beyond benchmark datasets? 2) how versatile are these detectors in the environment of fast LLM iteration and application development, despite developed for being task-specific?\nIn this paper, we propose a generic approach to curate synthetic datasets for training hallucination detectors (hallucination datasets), as shown in Figure 1. Our approach features a two-step Generation-Selection pipeline: we first generate a group of hallucinated candidates for a given input through an LLM (generator) and then select the best candidate through an LLM (judge) based on a given criteria. The synthetic datasets are then used as training data to develop post-hoc hallucination detectors. To answer the two key questions, we propose two design features in the generation step to customize hallucination generation and improve dataset quality: Hallucination Pattern Guidance (HPG) and Language Style Alignment (LSA).\nHallucination pattern guidance is motivated by the need for task-specific hallucinated samples. Starting from a set of pre-defined hallucination patterns using a little human effort, we prompt the generator to generate hallucinated samples conforming to the given patterns. This design enables the synthetic datasets to cover both general and task-specific hallucination patterns. It tailors the generation pipeline to the concerning and consequential hallucinating behaviors observed in practice.\nLanguage style alignment is designed to improve detector generalization by improving data quality and mitigating detector shortcuts. Language style discrepancy between hallucinated and non-hallucinated data is empirically observed, which can be easily picked up during training and thus erodes the detector generalization. LSA mitigates the issue by aligning the text characteristics of hallucinated samples with those of non-hallucinated LLM responses. We propose a hierarchical Language Style Discovery algorithm by leveraging LLMs to distill the styles into a small feature set used as guidelines to govern hallucination generation. This refinement aligns generations with non-hallucinated text, generates more challenging training data, and leads to detectors with better generalization.\nWe conduct experiments on three conversational benchmarks by generating synthetic hallucination datasets to train hallucination detectors. Our hallucination detector achieves an F1 score of 0.938 on average over three benchmarks and six different generators, outperforming the in-context learning based LLM detectors by a large margin of 32.5%. It implies that dedicated detectors trained on synthetic datasets are powerful and cost-effective options for post-hoc hallucination detection.\nTo answer question of generalization, we show that our synthetic hallucinations are more similar to non-hallucinated samples through various text distance metrics compared to existing approaches (Li et al., 2023; Yu et al., 2023). Moreover, our empirical investigation based off detector performance documents strong generalization capability demonstrated by our pipeline and detectors in three dimensions: 1) Out-of-generator generalization-detectors trained on a dataset generated by one LLM can be used on generations of other LLMs; 2) out-of-pattern generalization-detectors trained on a hallucination pattern(s) can generalize on unseen patterns; 3) out-of-task generalization-detectors trained on one task can generalize on other tasks. In all three scenarios, the supervised detectors trained with synthetic datasets generated by our pipeline deliver superior performance than ICL detectors and better generalization than baselines. The results confirm that detectors trained with our pipeline are also versatile, despite being developed for being task specific.\nTo summarize, we make the following contributions:\n\u2022 We propose a quality data generating pipeline that enables hallucination pattern customization;\n\u2022 we design a novel approach to improve data quality by aligning the synthetic hallucinations with the non-hallucinated text's language style;\n\u2022 through extensive experiments, we empirically document that supervised detectors consistently exhibit cross-task, cross-generator and cross-pattern generalization capabilities."}, {"title": "Methodology", "content": "Problem Setting. In this work, we assume that there exists a benchmark dataset, which is a set of non-hallucinated input-output pairs either from humans or from an LLM, similar to HaluEval (Li et al., 2023). This benchmark is used to build our hallucination generation pipeline. We rely on human judgment to provide hallucination patterns that need to be detected. Our objective is to create a synthetic dataset that contains both hallucinated and non-hallucinated input-output pairs. Hallucination detection is formulated as a binary classification task: given an input and an LLM output, a detector determines whether the LLM output is hallucinated with respect to the input.\nOur proposed approach features an automatic Generation-Selection pipeline with Hallucination Pattern Guidance (HPG) and Language Style Alignment (LSA). The generation-selection mechanism consists of a generation step to obtain a set of candidate hallucinated samples and a selection step to pick the most plausible one, which ensures the generation quality. HPG and LSA are two versatile modules integrated into the generation step."}, {"title": "Generation-Selection Pipeline", "content": "While using synthetic data for hallucinations is becoming more common, the quality of the hallucination data can be low, especially for automatic approaches without human intervention. To resolve the issue, we adopt the two-step first-generate-then-select design (Li et al., 2023) to ensure generation quality. Specifically, two LLMs (not necessarily the same) act in the roles of generator and judge separately. The generator produces a set of hallucinated outputs per input according to predetermined patterns. The judge scores the hallucinated candidates by given criteria, and the one with the highest score is selected. This step improves generation quality by selecting the best among the group of candidates.\nGenerator. Generator is a prompted LLM performing the task of hallucinated sample generation. We utilize LLMs to generate hallucinated data, as they are proven to generate high-quality text while following instructions. The key here is to properly design the prompts for hallucination guidance and language style alignment. Besides, it is important to carefully specify the persona in the system prompt to work around the safety policies in place that prevent LLMs from generating hallucinations. We adopt the chain-of-thought (CoT) (Wei et al., 2022) prompt and ask the generator to provide rationale for the generated samples, inspired by Peng et al. (2023). Our generator prompt is structured as follows: The prompt starts with a definition of persona customized for target tasks, which is followed by a section of HPG consisting of the pattern description and one demonstration example. The next is the LSA section, which comprises itemized guidelines for text generation. The prompt ends with an input and brief instructions on the output format. The details of prompts are deferred to Section 2.2 and Section 2.3.\nJudge. A judge is a prompted LLM performing the task of evaluating the quality of hallucination candidates according to the given criteria. We prompt the judge to score the candidates on a scale of 1 to 10, and the candidate with the highest score is then selected as the hallucinated output for the given input. We follow this scoring mechanism instead of directly selecting the best candidate out of the generated candidates because the scoring approach is less prone to LLMs' positional bias (Wang et al., 2023). Moreover, we also adopt the CoT prompt to generate rationale for the scores to improve accuracy. The judge prompt (see Appendix B) consists of a customized persona for target tasks, an evaluation criteria section, a guideline section, and an input section. The evaluation criteria are as simple as 'the more hallucinated the content is, the higher score should be given; the more plausible the output is, the higher score should be given'. The guideline section consists of one demonstration for each hallucination pattern. The input section comprises the model input, an input text, a set of hallucinated candidates, and instructions on the output format."}, {"title": "Hallucination Pattern Guidance", "content": "Hallucination patterns depend on the domains, tasks, contexts, and questions asked. As such, it is critical to curate the synthetic hallucination datasets in a controlled manner such that the hallucination patterns align with model behaviors in production. Our approach achieves task-specific generation by introducing a section of Hallucination Pattern Guidance (HPG) in the generator prompt. The HPG module needs a set of predetermined hallucination patterns. Each pattern consists of a short description and a demonstration, including an input, a non-hallucinated output, and a hallucinated output of the pattern. With the HPG section, the generator follows the instruction to generate hallucinated candidates in a controlled rather than open-ended manner. Our approach relies on human judgment to determine the hallucination patterns in their target applications. Such patterns can be generic, e.g., overconfidence and non-factuality, or task-specific, such as confusing between entities in response. Practitioners have the flexibility to include the most common and relevant hallucinations by simply writing out descriptions and curating demonstrations. Moreover, it is worth noting that the predefined pattern can go beyond the conventional definition of hallucination and include any undesired LLM behaviors that we want to detect. For example, in the experiment to be presented in Section 3, we include the pattern of nonsensical responses, where the generated responses bear no meaning in the context."}, {"title": "Language Style Alignment", "content": "LLM generations are known to be biased, lack diversity, and misaligned with human writings (Peng et al., 2023). As a result, a synthetic dataset created by one LLM might be sufficiently distant from human writings or the generation of other LLMs. Since our approach leverages the golden non-hallucinated outputs, any salient distinctions in language styles like length of text or tone between hallucinated output and non-hallucinated output can be exploited as shortcuts during supervised training. Shortcuts impact generalization by misleading the detectors to focus on superficial features rather than hallucination patterns. Due to the fast-paced development of LLM applications, there is a strong case for better generalization abilities of hallucination detectors. Therefore, it is critical to ensure the synthetic datasets resemble the language characteristics of golden, non-hallucinated text.\nTo this end, we propose a Language Style Alignment (LSA) module to align the characteristics of generated text with benchmark text. LSA is achieved by a prompt section in the generator prompt, which includes a group of itemized guidelines (see Appendix C) on the desired language style features, e.g., writing style, length, tone, etc. With the LSA, the generator follows the instructions and generates the hallucinated candidates in a controlled manner. In general, language-style-aligned hallucinated outputs should be more challenging to detect as they are more similar to non-hallucinated outputs, except for the hallucinated content.\nThe challenge lies in obtaining the language style features. For one, the benchmark dataset to be aligned with can be too large to manually analyze the text characteristics. Besides, it requires expertise in linguistics to properly analyze and summarize the language style features. To overcome the challenge, we propose a Language Style Discovery algorithm that leverages LLMs to summarize and consolidate the language style features. Specifically, the benchmark dataset is first partitioned into batches of proper size and then fed into prompted LLMs to analyze language style features. A group of language style features is produced for each batch, and language style features are merged together to form a language feature set. Then, we partition the language feature set into batches and ask a LLM to consolidate the features in each batch, which results in a smaller language feature set. The procedure continues until the desired number of language style features are obtained."}, {"title": "Data Mixture", "content": "Different LLMs exhibit different bias, diversity, and misalignment issues due to the distinctions in the pre-training corpus and preference alignment. As a result, a synthetic dataset created by one LLM might be sufficiently distant from the ones generated by other LLMs, such that detectors trained on the synthetic dataset do not generalize even if the language style alignment is in place. Such generalizations for hallucination detection become more and more important due to the increased restrictions on the usage of LLM-generated data.\nTo this end, we experiment with Data Mixture, a simple-yet-effective scaffolding strategy, to further boost the generalization and performance robustness of detectors trained on synthetic data. Specifically, we run the generation-selection pipeline with multiple LLM generators and mix the resulting synthetic dataset to increase the training corpus diversity and mitigate bias. Note that the data mixture is independent of the pipeline design since it is applied to the resulting synthetic datasets."}, {"title": "Experiments", "content": "Setup.\nBenchmarks. We consider three task-oriented conversational benchmarks to conduct the empirical experiments: OpenDialKG (Moon et al., 2019), ReDial (Li et al., 2018) and SalesBot (Chiu et al., 2022). For each benchmark, we randomly sample 1000 data points from the dataset as the golden non-hallucinated samples and apply the data generation pipeline to selected samples to curate synthetic datasets.\nSetup. We run experiments with six LLMs from three model families available in AWS Bedrock, including Claude3-Sonnet, Claude3-Haiku (Anthropic, 2024), llama2-13B, llama2-70B (Touvron et al., 2023), Mixtral-8\u00d77B Instruct, and Mixtral-Large (Jiang et al., 2024). We use the same LLM for generation and selection and refer to the resulting dataset under the corresponding LLM's name. We use Claude3-Sonnet to analyze the datasets and discover a set of language style features (see Appendix C).\nWe manually curate three hallucination patterns for our experiments, including non-sensical response, inconsistent entity, and irrelevant content. Each hallucination pattern is associated with a demonstration example.\u00b9 Details on the hallucination patterns are in Appendix D. We generate three hallucination candidates per sample for each pattern. As a result, each synthetic dataset contains 4000 samples, including 1000 non-hallucinated responses and 3000 hallucinated responses. Detailed parameters for the generation pipeline and fine-tuning are deferred to Appendix A.\nFor the experiment with the data mixture, we evaluate two mixture strategies based on the generator portfolios: model family mixture and model size mixture. The former strategy combines synthetic datasets generated by LLMs in the same model family (Claude3, Llama2, Mixtral), and the latter combines the datasets generated by the larger models in each family (Large Combo: a mixture of Claude3-Sonnet, Llama2-70B, and Mixtral-Large) and smaller models in each family (Small Combo: a mixture of Claude2-Haiku, Llama2-13B, and Mixtral-8\u00d77B). We mix synthetic datasets through random sampling while controlling the dataset size for the sake of fair comparison.\nEvaluation. Considering the objective of hallucination detection, we evaluate our approach through two branches of metrics. Firstly, we quantify the detector performance with standard metrics for binary classification, such as the F1 score. We run the supervised detectors on the test datasets generated by the same LLMs as the training dataset (in-generator), and the results reveal whether the hallucination is detectable and how good the detectors are.\nWe evaluate the generalization abilities through three pillars: 1) Cross-generator generalization is measured by the performance on test dataset generated other LLMs (out-of-generator); 2) cross-pattern generalization is assessed on the unseen patterns (out-of-pattern); 3) cross-task generalization is investigated by training supervised detectors on one benchmark task while evaluating on other benchmarks (out-of-task). For performance robustness, we adopt the standard deviation of the metrics recorded on out-of-generator datasets (out-of-generator std)."}, {"title": "Synthetic Dataset Analysis", "content": "We first compare the generation quality of our approach with similar baselines by gauging the distance between synthetic hallucinated and non-hallucinated responses. Datasets reporting smaller distances are considered to be of higher quality since hallucinated samples better resemble the good responses. We utilize three metrics to quantify the distance between two corpora: Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017), Zipf (Holtzman et al., 2020), and Medoid (Kour et al., 2022). FID quantifies the corpus distance through the Wasserstein distance between densities by fitting a continuous multivariate Gaussian to the SentenceBERT text embeddings of corpora. Zipf gauges the distance using the absolute difference between two Zipfian coefficients fitted on two corpora. Lastly, Medoid quantifies the cosine distance between corpora centroids. Besides, we compare our approach with SimPrompt (Yu et al., 2023) where LSA is removed, and HaluEval (Li et al., 2023).\nDistances between hallucinated and non-hallucinated responses are reported in Table 1. The distances are consistently smaller with our approach, demonstrating that our approach generates hallucinated samples closer to the real human non-hallucinated samples. Specifically, compared with SimPrompt, our hallucinated responses are 12.0%, 11.5% and 6.8% closer to the good ones on average for OpenDialKG, ReDial, and SalesBot, respectively. The distance improvement is even larger when compared with HaluEval. The reduced corpus distance implies that our approach guides the generation to resemble the language features of non-hallucinated samples. This resemblance makes it more difficult for a detector to focus on trivial language features irrelevant to detecting hallucinations."}, {"title": "Hallucination Detection Performance", "content": "Table 2 reports the average of F1 scores recorded by ICL detectors and supervised detectors (Vanilla and Mixture). For more detailed results by individual models, refer to Table 6 in the appendix. For ICL detectors, the LLM is assessed on the synthetic dataset generated by the same LLM. For supervised detectors, the reported performance is in-generator performance\u2014the detectors are assessed on the test dataset generated by the same LLM as the training dataset. We find that ICL detectors still face significant challenges identifying hallucinations generated by themselves; the average F1 score is only 0.613 across the board. Fine-tuned detectors, in contrast, exhibit stronger performance consistently. For vanilla fine-tuned detectors trained on synthetic datasets generated by specific LLMs, the average F1 scores for six models are 0.920, 0.932, and 0.963 on OpenDialKG, ReDial, and SalesBot, respectively.\nFurthermore, rows of Vanilla/Mixture OP report the F1 score when the data of a hallucination column is removed from the training data a setup to evaluate the out-of-pattern generalization. We find that supervised detectors perform worse on unseen patterns on average, but the gap is small-supervised detectors still outperform ICL detectors by a large margin. Such results mitigate the concern of misrepresenting run-time hallucinations, assuring superior performance on underrepresented patterns.\nSupervised detectors with data mixture perform slightly worse than vanilla supervised detectors on average (0.938 versus 0.912). The same pattern is observed in each hallucination pattern category and benchmark task. Since we control the sample size, the degraded performance suggests that synthetic datasets generated by different LLMs are distant in distribution, so merging datasets without scaling the sample size is at the cost of model performance. Interestingly, the conclusion holds even for datasets generated by the LLMs in the same family (see Table 6 in Appendix E). We conjecture that model size plays a role in generation distributions, as models in the same family usually share the training corpus."}, {"title": "Generalization Investigation", "content": "The results in the previous section on data-mixture suggest that performance suffers because of differences in the data distribution between text generated from different LLMs, implying a generalization issue with detectors struggling with out-of-distribution text. We further explicitly investigate this in two scenarios: out-of-generator generalization and out-of-task generalization.\nOut-of-Generator Generalization (OGG). We investigate OGG by training supervised detectors on a dataset generated by one generator (or more in the case of mixture) and testing on the datasets generated by the rest of the generators.\nAs shown in the performance panel of table 3, supervised detectors continue delivering superior performance than ICL detectors across the board, though slightly underperform the in-the-generator detectors. Besides, we observe that supervised detectors trained with mixed data outperform vanilla supervised detectors on out-of-generator datasets by 0.032 on average. This indicates that data mixture is a simple yet effective approach to increasing the OGG ability. Moreover, mixture trained detectors outperform both SimPrompt and HaluEval by 0.011 and 0.112 respectively.\nApart from the average performance, we adopt the standard deviation of the out-of-generator F1 score to proxy the generalization robustness; a detector with robust generalization is considered more reliable to deliver consistent performance. As shown in the robustness panel, the average out-of-generator standard deviation for vanilla supervised detectors is 0.095, equivalent to 11.2% of the out-of-generator mean. Supervised detectors with data mixture achieve a smaller out-of-generator standard deviation; the average is 0.065, equivalent to 7.4% of the out-of-generator mean. The results confirm that the data mixture strategy improves the model's robustness when transferring supervised detectors across LLMs. Similar to the generalization performance results, mixture trained detectors achieve best-in-class generalization robustness.\nOut-of-Task Generalization (OTG). There is a practical motivation for transferring existing supervised detectors trained on one task to a new task to reduce the development burden. An ideal hallucination detector should generalize well to other tasks when the hallucination patterns are similar. We investigate this OTG ability by training supervised detectors on one benchmark task and evaluating them on others.\nPerformance is reported in Table 3. Vanilla trained detectors outperform the ICL detectors by 0.257 on average on the three tasks; Mixture trained detectors outperform by the margin of 0.247. Compared with in-domain scenarios, the out-of-task performance degrades by 0.066 and 0.071 using mixture and vanilla trained supervised detectors, respectively. The findings suggest that supervised detectors produced based off our data pipeline are superior alternatives to ICL in the scenarios of lightweight development, offering plug-and-play capability with great generalization.\nBesides, both vanilla and mixture trained detectors outperform the SimPrompt and HaluEval across the benchmark tasks. Vanilla trained detectors record the best performance, outperforming HaluEval and SimPrompt detectors by 0.060 and 0.043. Mixture trained detectors record a smaller lead but consistently outperform as well. We conjecture that the data mixture strategy doesn't benefit OTG because our strategy only mixes the data in the generator dimension. We believe that extending the generalization ability from one dimension (obtained by data mixture) to another presents significant potential, and we leave this exploration for future work."}, {"title": "Ablation Study", "content": "The central conjecture of our method is that LSA and HPG generate non-trivial hallucinations, which are more aligned with non-hallucinated samples. The direct implication of this conjecture is it would be harder to detect hallucinations generated using LSA and HPG than without them, as these hallucinations are more similar to non-hallucinated responses in language style.\nWe test this conjecture in Table 4 with an ablation on LSA and HPG components. Note that values on the diagonal are higher as expected since the train and test sets are more similar. We observe that the average performance on test hallucinations generated with both LSA and HPG is much lower than the ones w/o LSA or w/o HPG, supporting the conjecture that the synthetic samples become easier to detect without LSA and HPG. Particularly, w/o HPG, the generated hallucinations become too trivial for the detector to detect, with an average F1 of 0.973 versus 0.908 w/ HPG across three benchmarks. LSA also makes the hallucinations harder to detect, though with a lower effect compared to HPG (0.917 average F1 w/o versus 0.908 w/ LSA).\nMoreover, the detector performance also provides a lens to examine the training data quality by comparing the F1 scores in each column. Specifically, detectors trained on datasets without LSA underperform the ones with LSA (LSA + HPG) consistently across all datasets (except on test data w/o LSA) and benchmarks. Similarly, detectors trained on datasets without HPG record much lower performance compared with the ones with HPG (LSA + HPG) consistently. It suggests that synthetic data created using LSA and HPG possesses superior quality, resulting in more effective supervised detectors. These results provide strong evidence for our conjecture that LSA + HPG generate more difficult and high-quality hallucinations."}, {"title": "Related Work", "content": "A bank of benchmarks has been curated for hallucination detection and evaluation recently. Pal et al. (2023) proposes a hallucination benchmark specific to LLMs in the medical domain. It consists of multiple-choice questions from various countries focusing on reasoning ability and memory ability. Muhlgay et al. (2024) introduces a method for automatically creating hallucination benchmarks by perturbing factual statements. BAMBOO (Dong et al., 2023) and ScreenEval (Lattimer et al., 2023) are two benchmarks focusing on hallucination detection in the context of long texts. Rather than focusing on sentence-level hallucination detection, PHD is a benchmark designed for passage-level detection (Yang et al., 2023). The most similar work to ours is HaluEval, which uses ChatGPT to create a task-specific hallucination benchmark for four tasks (Li et al., 2023). In contrast, our work is designed to be generic to generate customized hallucination datasets for any task or domain.\nMany researches extend the naive approach to address the bias and diversity issues observed in LLM generations. Yu et al. (2023) demonstrates that attributed prompts (specifying attributes like length and style) outperform naive prompts in terms of the resulting model's performance. Peng et al. (2023) proposes Chain-of-Thoughts Attribute Manipulation (CotAM) to curate datasets from LLMs through few-shot learning. The motivation behind the approach is to create a dataset with changes only in the attribute targeted by the task. PROGEN utilizes the feedback from downstream models to guide generations via in-context examples in an iterative manner (Ye et al., 2022). Our work extends the attribute manipulation approaches by automatically discovering the language styles by LLMs. Besides, our method degenerates to the simple baseline (SimPrompt) when LSA module is removed and the AttrPrompt when LSA is replaced with attribute guidance (Yu et al., 2023)."}, {"title": "Conclusion", "content": "We propose a generic automated approach to generate synthetic datasets for training non-trivial hallucination detectors. Our analysis reveals that our approach better aligns the hallucinated synthetic text with non-hallucinated benchmark samples versus existing methods (Li et al., 2023), in order to create non-trivial hallucination detectors. Our experiment results show that ICL LLM detectors are only slightly above chance, and detectors trained on synthetic datasets outperform ICL LLM detectors by a large margin. Moreover, the detectors trained on synthetic datasets have cross-generator, cross-pattern, and cross-task generalization abilities, implying that our pipeline produces versatile detectors for various application scenarios.\nTo conclude, we propose a versatile framework for curating task-specific synthetic hallucination datasets for building post-hoc hallucination detectors. It contains an effective procedure to detect non-trivial hallucinations, using language style discovery and hallucination pattern customization to make detectors generalized and robust. We believe it paves the way for building low-effort customized hallucination detection models."}, {"title": "Limitation", "content": "We rely on human judgment to curate hallucination patterns, which may not cover all the potential hallucination patterns. Consequently, the detector performance reported in our paper doesn't necessarily align with production performance. Besides, we generate an equal amount of hallucination samples for each pattern, resulting in a balanced dataset. The occurrence frequency of various patterns can, however, be significantly different.\nSupervised hallucination detectors cannot solve all the types of hallucinations. Their efficacy is in question in cases where hallucination patterns require intense knowledge. Despite the fact that our pipeline can generate factuality hallucinations, the detector's performance may degenerate in such applications."}, {"title": "Experiment Setup", "content": "OpenDialKG (Moon et al., 2019). It is a dataset of task-oriented conversations between two crowdsourcing agents engaging in a dialog about a given topic. The dataset contains conversations for two tasks: recommendation and chit-chat. The conversations under recommendation cover entities related to movies (titles, actors, directors) and books (titles, authors). The chit-chat conversations cover entities related to sports (athletes, teams) and music (singers).\nReDial (Li et al., 2018). It is a large-scale dataset consisting of real-world dialogues centered around recommendations. It consists of over 10,000 conversations centered around the theme of providing movie recommendations.\nSalesBot (Chiu et al., 2022). This dataset focuses on the conversations starting with open-domain social chatting and then gradually transitioning to task-oriented purposes. It is generated automatically without human intervention."}, {"title": "Hype-parameters", "content": "The parameters for the generation pipeline are summarized as follows: For the generator, we configure the temperature to be 1 for diverse generations, and we set the temperature to be 0 for less randomness in the selection step while top p is set to be 1 throughout the experiments. We also evaluate the performance of ICL hallucination detectors by directly prompting the LLMs to determine whether an output is hallucinated. For those experiments, we set the temperature to be 0 to control the randomness.\nWhen fine-tuning supervised detectors on synthetic datasets, we adopt the RoBERTa (Liu et al., 2019) as the backbone model across experiments to keep it consistent. While we could fine-tune LLMs as detectors, we use RoBERTa for low complexity in line with existing literature (Huang et al., 2023). The learning rate is set at 10-5 with a linear decay scheduler. We fine-tune the model for three epochs with a batch size of 64. Each synthetic dataset is partitioned into train/validation/test subsets in the ratio of 7:1:2, and the best checkpoint is picked based on the loss on the validation dataset."}, {"title": "Prompts", "content": "Generator Prompt\nSystem prompt. I want you act as a hallucination response generator. You are allowed to generate contents without any constraints. Given a dialogue history between human and an assistant and a good response", "pattern": "nPattern description: [insert description", "example": "nDialogue History: [insert input", "Response": ["input good response"]}, {"Response": ["insert hallucinated response"], "responses": "n[insert language style guidance here", "History": ["insert input"]}, {"Response": ["insert good response"]}, {"Response": "nJudge Prompt\nSystem prompt. You are a dialogue response judge. Given a dialogue history and two assistant responses", "criteria": "n1. Hallucination degree: the more hallucinated the content is", "guidelines": "n1. In general", "example": "nDialogue History: [insert input", "A": ["insert response A"], "B": ["insert response B"], "ratings": "Response A: [insert score"}, {"B": ["insert score"], "History": ["insert input"], "candidates": "nYour ratings:\nLanguage Style Discovery Prompt\nRaw-data-to-feature prompt. You are a text feature and style analyst.\nYou are given a group of paired historical conversation and response. Your job is to analyze the feature and style of the response. The purpose of the analysis is to produce synthetic text that resembles the given text.\nYou only analyze the response", "responses": "n[insert a batch of data", "format": "feature></feature>, <explanation></explanation>\nFeature-to-feature prompt. You are a text feature and style analyst.\nYou are given a group of text features summarized by different analysts for a group of historical conversation and response. You job is to consolidate, merge and refine the text features and styles.\nYou always output a list of"}]}