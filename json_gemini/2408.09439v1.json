{"title": "Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting", "authors": ["Zeyuan Chen", "Haiyan Wu", "Kaixin Wu", "Wei Chen", "Mingjie Zhong", "Jia Xu", "Zhongyi Liu", "Wei Zhang"], "abstract": "Relevance modeling is a critical component for enhancing user experience in search engines, with the primary objective of identifying items that align with users' queries. Traditional models only rely on the semantic congruence between queries and items to ascertain relevance. However, this approach represents merely one aspect of the relevance judgement, and is insufficient in isolation. Even powerful Large Language Models (LLMs) still cannot accurately judge the relevance of a query and an item from a semantic perspective. To augment LLMs-driven relevance modeling, this study proposes leveraging user interactions recorded in search logs to yield insights into users' implicit search intentions. The challenge lies in the effective prompting of LLMs to capture dynamic search intentions, which poses several obstacles in real-world relevance scenarios, i.e., the absence of domain-specific knowledge, the inadequacy of an isolated prompt, and the prohibitive costs associated with deploying LLMs. In response, we propose ProRBP, a novel Progressive Retrieved Behavior-augmented Prompting framework for integrating search scenario-oriented knowledge with LLMs effectively. Specifically, we perform the user-driven behavior neighbors retrieval from the daily search logs to obtain domain-specific knowledge in time, retrieving candidates that users consider to meet their expectations. Then, we guide LLMs for relevance modeling by employing advanced prompting techniques that progressively improve the outputs of the LLMs, followed by a progressive aggregation with comprehensive consideration of diverse aspects. For online serving, we have developed an industrial application framework tailored for the deployment of LLMs in relevance modeling. Experiments on real-world industry data and online A/B testing demonstrate our proposal achieves promising performance.", "sections": [{"title": "1 INTRODUCTION", "content": "In today's landscape of excessive information, search engines have become critical for online content platforms, allowing users to swiftly find preferred content that match their search queries. To ensure a user-friendly experience, relevance modeling is crucial to preserve a satisfactory connection between a query and the displayed outcomes, forming a core element of search engine functionality.\nIn the relevant literature, foundational studies [18, 20, 23] have engaged in feature engineering to accomplish text matching, yet they lacked sufficient generalization and accuracy. Subsequently, deep learning-based approaches have risen as a new paradigm, with two primary categories: representation-based approaches [10, 16, 21] and interaction-based approaches [1, 6, 12, 13]. Lately, pre-trained architectures such as BERT [3] have achieved significant progress in Natural Language Understanding (NLU) tasks. As a result, several studies [7, 9, 17, 31] are introduced that aim to capture the semantic relationships between queries and items. Most recently, Large Language Models (LLMs) have showcased their exceptional capabilities across a wide range of Natural Language Processing (NLP) applications. These models, such as GPT [14], LLaMA [25], and GLM [4], are trained on massive corpora of texts, which enables them to maintain an exhaustive world knowledge. Nonetheless, identifying user search intentions accurately remains challenging when relying solely on semantic understanding, due to the absence of specialized domain knowledge required for complex industrial search scenarios. The texts of queries and items in Alipay search scenario are quite short and ambiguous, making it hard to convey effective information contained in their identity. For example, given a query \"Zhe Yi\", the abbreviation of a hospital, it is hard to comprehend the actual semantics. But its historical clicked items include \"the first affiliated hospital of Zhejiang University\", indicating strong correlations between them to help search intention identifying. As such, leveraging behavior data to assist relevance modeling is a natural strategy.\nExisting studies [2, 8, 11, 32, 34] have primarily conducted the use of user behavior data. But they all consider constructing the pre-training dataset or the topology structure based on click behaviors without integrating semantics fully and effectively. Despite the success of LLMs, it's still uncertain how well they can integrate world knowledge and specialized domain knowledge represented by user behavior data to master relevance modeling. To this end, this paper aims to investigate the potential of LLMs in relevance modeling with user search behavior. By utilizing strategic prompting techniques, specialized domain knowledge could be easily injected into LLMs for relevance modeling, whereas the performance varies due to the following unresolved issues: (i) The acquirement of domain-specific knowledge. Though domain-specific knowledge is vital in improving search scenario-oriented relevance modeling capabilities of LLMs, not all knowledge is beneficial. The noisy user behavior data may mislead LLMs to undesired judgements. Moreover, specialized domain knowledge of search scenarios undergoes rapid changes on a daily basis. The limited capacity of LLMs to adapt swiftly to these changes presents a significant obstacle to their ability to render accurate relevance judgments. (ii) The inadequacy of an isolated prompt. Despite LLMs could derive the relevance degree exploiting an isolated prompt, LLMs exhibit insensitivity to input, meaning they lack awareness of the aspects from which to infer relevance. In addition, the isolated prompts place greater demands on the quality of the prompts itself. Except for the aforementioned issues, deploying LLMs affordably in industrial scenarios is also a consideration worth addressing.\nTo address the above problems, we propose a novel Progressive Retrieved Behavior-augmented Prompting framework for integrating search scenario-oriented knowledge with LLMs (dubbed as ProRBP). To acquire domain-specific knowledge in time, we perform a user-driven behavior neighbor retrieval from the daily updated search logs, retrieving candidates that users consider to meet their expectations currently. Then we anticipate employing advanced prompting techniques that progressively improve the outputs of the LLMs, followed by a progressive aggregation with comprehensive consideration of diverse aspects to form a holistic relevance model. As for the online serving of LLMs, we design an industrial implementation framework enabling LLMs to fully handle search relevance scenarios with the affordable cost.\nIn summary, we make the following contributions of this paper:\n\u2022 To the best of our knowledge, we are the first to successfully investigate the potential of LLMs with user behavior data to master relevance modeling.\n\u2022 We propose ProRBP with two novel plug-in modules. Firstly, a user-driven behavior neighbors retrieval is developed to acquire domain-specific knowledge in time. Secondly, the proposal of"}, {"title": "2 RELATED WORK", "content": "2.1 Relevance Modeling\nThe section concretely discusses the most recent advances in relevance modeling studies. Relevance modeling in search can be viewed as a text matching problem as the sub-domain of information retrieval (IR). The majority of work focuses on distinctions at the semantic level, while a minority of methods judge the relevance from a behavioral perspective.\n2.1.1 Semantics-driven Methods. Current semantics-driven approaches can be classified into two aspects: feature-based approaches and deep learning-based approaches. The first category is centered on manual-crafted features such as TF-IDF similarity and BM25 [23]. Despite their usefulness, these feature-based approaches have limited generalization ability due to their domain-specific features and require significant labor resources.\nIn order to address the limitations of the above approaches, deep learning-based approaches emerge as the new paradigm, which can be broadly classified into representation-based approaches and interaction-based approaches. The former focuses on learning a low-dimensional representation of data while the latter emphasizes capturing the interaction between inputs. For instance, DSSM [21] is a classical two-tower representation-based model that encodes the query and the document separately. In this paradigm, recurrent [10, 24] and convolutional [6, 21] networks are adopted to extract low-dimensional semantic representations. For these methods, the encoding of each input is carried out independently of the others. Consequently, these models face challenges in modeling complex relationships. To overcome this limitation, interaction-based models are proposed. DecompAtt [13] leverages attention network to align and aggregate representations. In parallel, recurrent [1] and convolutional [6, 12] networks are employed for modeling complex interactions.\nIn recent times, pre-trained models like BERT [3] have made remarkable strides in Natural Language Understanding (NLU). As a result, representation-based [7, 9, 17, 31] and interaction-based architectures [27] are proposed to leverage the capabilities of these models to encode semantic correlations between queries and items. Most recently, Large Languages Models (LLMs) like GPT [14], LLAMA [25], BLOOM [29] and GLM [4] trained on massive corpora of texts have shown their superior ability in language understanding, generation, interaction, and reasoning tasks. [22] investigates the potential of utilizing LLMs for searching and demonstrates that appropriately instructs ChatGPT and GPT-4 can produce competitive and even superior results to supervised methods widely used information retrieval benchmarks. [2] tries to deal with long-tail query-item matching through LLMs efficiently and effectively. In these research work, they tend to exploit world knowledge stored in parameters of LLMs to judge the relevance between the query and item. However, general LLMs can not adapt to the industrial scenario due to the lack of domain-specific knowledge and insensitivity to the relevance judgement. In this work, we try to explore the LLMs-driven relevance modeling comprehensively in Alipay search engine to meet the above requirements.\n2.1.2 Behavior-driven Methods. In addition to textual information, there are a few related works that aim to integrate user behavior data into their models. The utilization of user behavior data can provide valuable insights into search intention, which can then be used to enhance the relevance of the search engines. MASM [30] leverages the historical behavior data to complete model pre-training as a weak-supervision signal with a newly proposed training objective. [8, 11, 34] try the incorporation of click graphs to enhance the effectiveness of search systems. [2] endeavors to exploit behavior neighbors while considering interaction granularity and topology structure. However, no work has fully integrated LLMs with user behavior comprehensively in relevance modeling. We target to do that."}, {"title": "3 PROBLEM FORMULATION", "content": "Assume we have the target query q and target item i needed to predict the relevance degree exploiting LLMs. In essence, referring to PET [19], it could be formulated as follows: with the designed prompt \u03c4(q, i), LLMs can determine which verbalizer v (i.e., \"relevant\" or \"irrelevant\") is the most likely substitute for the mask based on the likelihood P(v|\u03c4(q, i)). The most naive prompt in the relevance modeling could be formulated as:\n$\\tau(q, i) = \\text{Is [q] and [i] related? [mask]},$\n(1)\nThe relevance label $y_{qi} \\in {0, 1}$ can be associated with a verbalizer (i.e., \"irrelevant\" or \"relevant\") from the vocabulary of LLMs to denote the relevance degree between q and i. To enable the adaptation of general LLMs to the relevance modeling task, supervised fine-tuning operation is selected using the cross-entropy loss function $L_{ce}$. Then the relevance degree could be given from LLMs for subsequent applications in Alipay search scenario. It worth noting that the above formulation is the basic explanation of relevance modeling using LLMs. We will further explore novel ways below based on this basic."}, {"title": "4 METHODOLOGY", "content": "The architecture of the proposed ProRBP framework is depicted in Figure 2. It contains two novel plug-in modules: (1) user-driven behavior neighbor retrieval for obtaining domain-specific knowledge in time to meet users' expectation; (2) progressive prompting and aggregation for improving the sensitivity of LLMs to relevance judgement and stable prediction results. Through the two novel plug-in modules, relevance degree generation can be performed. In what follows, we elaborate on the two modules.\n4.1 User-driven Behavior Neighbor Retrieval\nGenerally, LLMs possess an exhaustive world knowledge with the benefit of massive corpora of texts pre-training. However, LLMs still struggle to understand user search intentions for short and ambiguous queries and items from Alipay search due to the lack of specialized and rapidly evolving domain knowledge. There are a few approaches could deal with the above problem, e.g., continual pre-training and retrieval-augmented generation. Considering efficiency and cost issues, the promising approach is retrieval-augmented language modeling [5], grounding the LLMs during generation by conditioning on relevant candidates retrieved from an external knowledge source. Drawing inspiration from this, we devise a user-driven behavior neighbor retrieval module. This module could retrieve the daily search logs of users to obtain daily changing behavior neighbors that users consider relevant. The pipeline of user-driven behavior neighbor retrieval is depicted as Figure 1.\nDue to the limited number of items displayed for a query in Alipay search scenario, almost all results can be seen by users. With this in mind, we could analyze that a higher click-through rate reflects that users believe the corresponding query-item pairs can better meet their search intents and needs when the exposure PV (i.e., page view) reaches a certain quantity. Thus, we filter out the query-item pairs with less than 100 exposure PV and formulate the remaining search logs as high-confidence logs. Then, we utilize the high-confidence logs from the past month to calculate the click-through rate for the exposed query-item pairs. To mitigate the effect of noises, query-item pairs are selected above a click-through threshold (e.g., 0.2) and the neighbors are arranged in descending order based on click-through rate from the query and item perspective, respectively. And we only choose top-K (e.g., 20) neighbors for corresponding query and item. Through this approach, it is possible to select dual behavior neighbors with high confidence, which in turn can assist LLMs in the process of in-context learning [15, 26]. To ensure the timeliness of knowledge, we utilize the daily updated search logs to repeat the above operation every day and construct daily separate indexes for the dual behavioral neighbors of queries and items. The neighbors could be retrieved from indexes for the corresponding query q and item i, denoted as $N^q$ and $N^i$ respectively. Need to clarify is that although we can obtain the behavior neighbors for the majority of queries or items, there is still a small portion of queries or items whose behavior neighbors cannot be obtained. In response to this situation, we will set the neighbors that cannot be obtained as empty in the prompt. Besides, we could obtain the attributes (i.e., brand, keyword, intent) of the query q and item i from the industrial knowledge base, which could be denoted as $A^q$ and $A^i$. They can be treated as a supplement to the domain-specific information. Then we could construct our daily prompt \u03c4(q, i, $A^q$, $A^i$, $N^q$, $N^i$) based on the above information. The daily prompt will be send to LLMs for further operation.\n4.2 Progressive Prompting and Aggregation\nDespite the fact that LLMs can determine a relevance score using an isolated prompt, they exhibit insensitivity to the input. It indicates that they lack the awareness required to decide the aspects from which to infer relevance. Furthermore, relying on isolated prompts imposes higher demands on the quality of the prompts itself, as the diversity of domain knowledge and the sensitivity to slight modifications of prompts may lead to unexpected results. As a result, it's expected that using progressive prompting and aggregation process to gradually guide and consider diverse aspects will eventually lead to the creation of a unified model that assesses relevance.\n4.2.1 Progressive Prompting. It is designed for making LLMs sensitive to the diverse aspects for the relevance judgement and improving the robustness of the model performance. Specifically, we firstly decompose the mentioned prompt above and construct least-to-most prompts step by step [28, 33] as:\n$\\tau(N^q, N^i) \\leftrightarrow \\tau(A^q, A^i, N^q, N^i) \\leftrightarrow \\tau(q, i, A^q, A^i, N^q, A^i, N^i),$\n(2)\nThe prompts are documented as shown in Figure 2. Besides, different prompting documents could obtain the consistent improvement in our local experiments exploiting this module.\nThen LLMs could reason the likelihood of the verbalizer exploiting the least-to-most prompts step by step for the sensitivity to relevance judgement and stable prediction results, which could be dubbed as:\nP(v|\u03c4($N^q, N^i$)) \u2192 P(v|\u03c4($A^q, A^i, N^q, N^i$)) \u2192 P(v|\u03c4(q, i, $A^q$, $A^i$, $N^q, A^i, N^i)),$\n(3)\nSimply, all least-to-most prompts share the same relevance label $y_{qi}$ and all least-to-most sub-tasks are supervised by the cross-entropy loss in parallel explicitly. The aforementioned method can be extended to L progressive prompts and the supervised loss could be denoted as $L_{auxi} = \\sum_{l=1}^{L} L_{ce}$, where $L_{ce}$ represents the loss of the sub-task corresponding to l-th prompt.\n4.2.2 Progressive Aggregation. Once we obtain the L least-to-most probabilities, we expect to learn the progressive relationship of solutions to sub-tasks. Intuitively, Our least-to-most paradigm is incremental not only in terms of information volume, but also in importance. Hence, we adopt the kernel function K to model the incremental tendency and the choice of kernel function can be a Gaussian kernel function, exponential kernel function, logarithmic decay kernel function, etc. After experimental comparison, we have chosen the exponential kernel function K(\u0394l) = Exp(\u0394l|\u03bb), where \u03bb is a learnable parameter and \u0394l is the degree of attenuation of the l-th prompt defined by us. The overall relevance score could be acquired through aggregating the probabilities from the least-to-most sub-tasks progressively as:\n$P(y_{qi}) = \\sum_{l=1}^{L} K(\\Delta_l) \\times P(v|\\tau_l),$\n(4)\nThis relevance score could be treated as the production of overall task, deserving to supervise mainly. The loss function is given by $L_{main}$. The hybrid objective function can be formulated as:\nL = $L_{main}$ + \u03b1$L_{auxi}$,\n(5)\nwhere \u03b1 is a hyper-parameter to control the strength of the least-to-most sub-tasks."}, {"title": "5 INDUSTRIAL IMPLEMENTATION FOR LLMS", "content": "In real-world search scenarios (e.g., Alipay search), it is hard to deploy LLMs to handle all the search traffics with acceptable cost and latency. However, the judgment of relevance is objective and non-personalized. This implies that the relevance scores for the same query-item pair should remain consistent for all users, unlike recommendation algorithms that are personalized for each individual. As long as we can obtain the relevance scores for the query-item pairs, we can perform online services for all users, which greatly reduces the volume of online requests. And for the certain query or item, their semantic information is relatively stable and does not vary much. Inspired by this, we come to a efficient and effective solution (i.e, online and offline collaborative service) with the affordable cost and latency. Figure 3 concretely illustrates online serving process of our proposed online and offline collaborative service when a certain query-item pair appears.\nSpecifically, we utilize LLMs (e.g., 10B parameters) to perform offline inference on daily search logs exploiting the daily prompts and update relevance scores stored in the database. The cost of offline inference is minimal, so we can choose LLMs with a larger number of parameters to enhance the accuracy of relevance score calculations. Meanwhile, we would deploy distilled smaller LLMs (e.g., 2B parameters) for online serving. Since the method of distillation is not the focus of this paper, we have omitted the introduction of the method here. With this model, it is possible to make online predictions for query-item pairs that do not have stored scores in the database. When a user enter a query in Alipay search, the service will firstly look up the database from the offline service and obtain relevance scores based query and candidate items. Once the relevance scores of the query-item pair can not be obtained from the database, the online service will be requested for predicting the relevance score timely. Note that approximately 95% of the traffic will be handled by the offline service, while the remaining traffic will be handled by the online service. Through this method, we have achieved affordable efficiency and resource requirements for exploiting LLMs to handle relevance judgements of industrial scenarios."}, {"title": "6 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to validate the effectiveness of our proposed ProRBP framework by answering the following pivotal research questions:\nRQ1. How are the results of ProRBP framework compared with other competitive relevance modeling models?\nRQ2. How do the main components of ProRBP framework affect its relevance modeling performance?\nBesides, we investigate the effect of different hyper-parameter settings (e.g., the number of behavior neighbors, the type of the kernel functions, and the strength of the least-to-most sub-tasks). Finally, we conduct online A/B testing to show the performance of the proposed method compared to previous deployed model.\n6.1 Experimental Setup\n6.1.1 Datasets. In order to evaluate the performance of all the models with reliability, we select the real-world industry data used in mini apps search scenario of Alipay search engine and present its statistics in Table 1. The dataset is labeled by human annotators, where Good and Bad annotations denote label 1 and 0 respectively. User historical behavior data is sampled from the search logs of the search engine. Although datasets such as WANDS\u00b9 and MSLR\u00b2 are publicly available, they do not contain the requisite user historical behavior data. Hence, we select this in-house data to evaluate the proposed framework. Other related work [2, 8, 30, 31, 34] also selects one in-house data to evaluate the proposed approaches. The partial data of this paper was released before\u00b3.\n6.1.2 Baseline Models. We select a set of popular NLU (Natural Language Understanding)-based, behavior-based and NLG (Natural Language Generation)-based relevance models as the baselines. For the NLU-based models, we choose three common models including two-tower and single-tower architectures: DSSM [21], ReprBert [31], Bert [3]. The behavior-based models all consider constructing the pre-training dataset (MASM [30]) or the topology structure based on click behaviors (TextGNN [34], AdsGNN [8], BARL-ASE [2]). For the NLG-based models, we choose two foundation models including two set of architectures: the causal decoder (BLOOM [29]) and the prefix decoder (GLM [4]).\n\u2022 DSSM is a two-tower representation-based model. It encodes the embedding of a given query and item independently and computes the relevance score accordingly.\n\u2022 ReprBert is a representation-based Bert model that utilizes novel interaction strategies to achieve a balance between representation interactions and model latency.\n\u2022 Bert has achieved significant progress on NLP tasks as an interaction-based model. Here we concatenate the query and item as the input of the model.\n\u2022 MASM leverages the historical behavior data to complete model pre-training as a weak-supervision signal with a newly proposed training objective.\n\u2022 TextGNN extends the two-tower model with the complementary graph information from user historical behaviors.\n6.1.3 Evaluation Metrics. We use Area Under Curve (AUC), F1-score (F1), and False Negative Rate (FNR) to measure the multidimensional performance of all models. AUC and F1 are commonly used in the studied area, of which higher metric values represent better model performance. Conversely, lower False Negative Rate (FNR) values are preferable, as they indicate a lower false filtering rate of models. Note that AUC often serves as the most significant metric in our task while the others provide auxiliary supports for our analysis.\n6.1.4 Model Implementations. We tune our model for 5 epochs with batch size 64 and learning rate 3e-05 in the supervised fine-tuning stage of LLMs. For the foundation models, we select the 1.1B BLOOM\u2074 and GLM with different magnitude of parameters (e.g., 0.3B, 2B and 10B) pre-trained by Alipay. The number of retrieved behavior neighbors is set to 20. We tune the parameter \u03b1 within the ranges of {0, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0}. We conduct the experiments on NVIDIA Tesla A100 GPUs.\n6.2 Experimental Results\n6.2.1 Performance Comparison. Table 2 shows the overall comparison of our proposed framework ProRBP with different baselines. Our findings indicate that DSSM performs poorly since it merely encodes the embeddings of the query and item independently as a basic two-tower model. By further comparing DSSM with ReprBert, we find the performance is improved to a certain degree. This demonstrates the pre-trained models utilizing the large corpus can bring additional gains. Compared to ReprBert, Bert achieves better performance, as the interaction-based models possess more advanced text relevance modeling abilities than representation-based models.\nFor the models of MASM, TextGNN, and AdsGNN, they exploit historical behavior data to build pre-training dataset or click graph to enhance the effectiveness of search systems. And they achieve significantly better results than the above-mentioned methods in the metric of AUC rather than F1 and FNR. This can be attributed to the introduction of auxiliary signals, which may bring improvement for the ranking ability of models but introduce some noises leading to the loss of F1 and FNR. The performance differences among them depend on the utilization of behavior data and modeling granularity. The newly proposed model BARL-ASe achieves the best performance in behavior-based relevance models, as it effectively leverages dual behavior neighbors for corresponding queries and items to enable the adaptable combination of fine-grained interactions and topology information.\nGLM-0.3B, BLOOM-1.1B, GLM-2B and GLM-10B denote that we use the corresponding language models with the amount of parameters to perform relevance judgements exploiting the naive prompts as stated in Section 3. They exhibit relatively good performance, demonstrating the positive effect of massive training datasets and large-scale parameters. Performance variances among them with different parameters appear minimal, possibly because the simplicity of the relevance task doesn't fully tap into the advantages of LLMs. Their performance has a certain gap compared to behavior-based models especially on the core metric AUC. This may be attributed to the lack of domain-specific knowledge and the limitations of an isolated prompt. Thus, ProRBP framework is proposed to address the mentioned problem and significantly improve the foundation models' performance. The performance differences between BLOOM and GLM may stem from differences in architecture and training corpora.\nOverall, exploiting our framework ProRBP could yield the best performance and ensures efficiency and economy. In the online serving, we select GLM-10B+ProRBP to perform offline inference, and GLM-2B+ProRBP to proceed online prediction considering affordable cost and efficiency. Besides, they both obtain better gains under all the metrics on the evaluation dataset compared to the second-best performed model BARL-ASe that was deployed in our scenario before."}, {"title": "6.2.2 Ablation Study", "content": "To investigate the contributions of key modules adopted by ProRBP framework, we provide the following variants of our complete framework:\n\u2022 \"-BNR\" denotes discarding user-driven behavior neighbor retrieval module.\n\u2022 \"-PPA\" denotes discarding progressive prompting and aggregation module.\n\u2022 \"-Both\" represents removing two modules mentioned above simultaneously.\nThroughout the ablation study shown in Table 3, we observe that:\n\u25c7 By analyzing the results of \"-BNR\u201d and \u201cPPA\u201d, we could conclude that both \"BNR\u201d and \u201cPPA\u201d modules yield significantly positive results. This suggests that the importance of the domain-specific knowledge and advanced prompting techniques in LLM-driven relevance modeling.\n\u25c7 And \"PPA\" demonstrates greater significance than \u201cBNR\u201d. It is likely that the naive prompts with domain-specific knowledge can not release the potentials of LLMs totally for relevance modeling. It proves that the improvement of the sensitivity of LLMs to relevance judgement and prediction stability brings more gains than domain-specific knowledge in our task.\n\u25c7 The result of \"-Both\" demonstrates removing two modules could significantly degrade the model's performance.\n6.3 Parameter Sensitivity\n6.3.1 Effect of the Number of Behavior Neighbors. Figure 4a presents the change of our method's performance w.r.t. different numbers of behavior neighbors, which is searched in {1, 2, 3, 5, 7, 10, 15, 20}. As expected, the results tend to be better when the number gets more. Such improvement might come from more domain-specific knowledge used for modeling. After reaching the peak, the performance remains stable.\n6.3.2 Effect of Different Kernel Functions. We investigate the contributions of different types of kernel functions used by ProRBP framework as shown in Figure 4b. To realize this, we select different kernels and incorporate them into our framework to show the detailed performance. For ease of illustration, the Gaussian kernel, exponential kernel, logarithmic-decay kernel and mean pooling operation are abbreviated as norm, exp, log, and mean, respectively. As shown in Table 4b, the exponential kernel achieves the best performance.\n6.3.3 Effect of the Strength of Sub-tasks. Figure 4c depicts the performance variation w.r.t. the change of our method's strength of the sub-tasks in {0, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0}. We can see that there is a clear trend that when the weight is increased from 0, the results are continuously boost. Moreover, when the weight reaches a certain extent, the results tend to be stable or become even slightly worse. The reason is that increasing the strength of the sub-tasks too much might affect the study of the main task."}, {"title": "6.4 Online A/B Testing", "content": "The proposed method has been deployed in the relevance stage of Alipay search platform providing search service of mini apps and demonstrated its significant performance gains in online A/B testing compared with the previous model BARL-ASe. The each experiment takes about 3% proportion of Alipay search traffic for two weeks. Compared with the previously deployed model, the proposed method improves the valid PV-CTR5 by 0.33% on average without causing an increase in latency. And the results of human annotations show the model can reduce the rate of irrelevant results by 1.07% points on average. The results demonstrate that our proposal can improve the experience of users."}, {"title": "7 CONCLUSION", "content": "This paper studies the relevance modeling problem by integrating world knowledge stored in the parameters of LLMs with specialized domain knowledge represented by user behavior data for achieving promising performance. The novel framework ProRBP is proposed, which innovatively develops user-driven behavior neighbor retrieval module to learn domain-specific knowledge in time and introduces progressive prompting and aggregation module for considering diverse aspects of the relevance and prediction stability. We explore an industrial implementation (i.e., online and offline collaborative service) to deploy LLMs to handle full-scale search traffics of Alipay with acceptable cost and latency. The comprehensive experiments on real-world industry data and online A/B testing validate the superiority of our proposal and the effectiveness of its main modules."}]}