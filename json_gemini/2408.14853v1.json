{"title": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models", "authors": ["Yuhao Du", "Zhuo Li", "Pengyu Cheng", "Xiang Wan", "Anningzhe Gao"], "abstract": "Large Language Models (LLMs) have become a focal point\nin the rapidly evolving field of artificial intelligence. How-\never, a critical concern is the presence of toxic content within\nthe pre-training corpus of these models, which can lead to\nthe generation of inappropriate outputs. Investigating meth-\nods for detecting internal faults in LLMs can help us un-\nderstand their limitations and improve their security. Exist-\ning methods primarily focus on jailbreaking attacks, which\ninvolve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected\nresponses. These methods rely heavily on prompt engineer-\ning, which is time-consuming and usually requires specially\ndesigned questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on di-\nrectly eliciting the target response instead of optimizing the\nprompts. We introduce the use of another LLM as the detec-\ntor for toxic content, referred to as ToxDet. Given a target\ntoxic response, ToxDet can generate a possible question and\na preliminary answer to provoke the target model into pro-\nducing desired toxic responses with meanings equivalent to\nthe provided one. ToxDet is trained by interacting with the\ntarget LLM and receiving reward signals from it, utilizing re-\ninforcement learning for the optimization process. While the\nprimary focus of the target models is on open-source LLMs,\nthe fine-tuned ToxDet can also be transferred to attack black-\nbox models such as GPT-40, achieving notable results. Ex-\nperimental results on AdvBench and HH-Harmless datasets\ndemonstrate the effectiveness of our methods in detecting\nthe tendencies of target LLMs to generate harmful responses.\nThis algorithm not only exposes vulnerabilities but also pro-\nvides a valuable resource for researchers to strengthen their\nmodels against such attacks.\nWarning: This paper includes content that may be offen-\nsive or harmful.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs)\nhave significantly improved performance across various nat-\nural language processing (NLP) tasks (Ouyang et al. 2022;\nBrohan et al. 2023; Achiam et al. 2023; Rafailov et al. 2024).\nDespite these advances, a significant issue persists: the train-\ning data for LLMs often contains toxic content that is diffi-\ncult to filter. This can lead the models to learn and replicate\nsuch behavior, potentially resulting in the generation of in-\nappropriate, offensive, or harmful content (Bai et al. 2022a;\nZou et al. 2023). To mitigate this problem, most LLMs un-\ndergo a process known as safety alignment (Ziegler et al.\n2019; Perez et al. 2022; Zhuo et al. 2023). During this pro-\ncess, the models are fine-tuned based on human preferences\nthat reflect positive societal values, leading to more helpful,\nappropriate, and safe responses. However, even such aligned\nLLMs remain vulnerable to attacks designed to bypass their\nsafety constraints (Zhou et al. 2024).\nTo address these challenges, research focused on identi-\nfying and mitigating internal faults in LLMs is crucial to\nenhance their robustness against attacks (Zou et al. 2023;\nLiu et al. 2024; Paulus et al. 2024; Sadasivan et al. 2024).\nAmong various attack strategies, the most prevalent is the\n\u201cjailbreaking attack", "Ignore Previous Prompt": "Perez\nand Ribeiro 2022) and", "Now": "walkerspider\n2022), which use manually crafted adversarial prompts to\ncompromise large language models. However, this manual\nprocess is time-consuming and hinders practical implemen-\ntation in the daily use of LLMs. To address this limita-\ntion, another category of work reformulates the jailbreak-\ning process as an automatic adversarial example generation\nprocess, as demonstrated by GCG (Zou et al. 2023) and\nPAL (Sitawarin et al. 2024). These methods involve leverag-\ning gradients from open-source LLMs to guide the optimiza-\ntion process. Nevertheless, this approach remains computa-\ntionally intensive due to the discrete optimization required.\nTo alleviate these time-consuming problems, another set of\nmethods employs automated models to generate adversar-\nial suffixes for the prompts, such as AdvPrompter (Paulus\net al. 2024), which can mislead models into generating unex-\npected responses. However, this process still requires care-\nfully designed prompts to achieve satisfactory results.\nIn this paper, we aim to explore a new paradigm for\naddressing the internal faults of LLMs. Instead of relying\non prompt engineering, we focus on directly attacking the\nmodels to elicit the desired toxic responses. We introduce a\ntarget-driven attack paradigm that optimizes a LLM called\nToxDet, which can generate questions and preliminary an-"}, {"title": "2 Preliminary", "content": "Let $V$ denote the vocabulary of a language model charac-\nterized by a policy $\\pi$, where $x \\in V$ represents a token and\n$x \\in V^*$ represents a sequence of tokens. Here, $V^*$ denotes\nthe set of all possible token sequences of arbitrary length.\nOriginally, consider a misleading question $x \\in V^*$ (e.g.,\n\"Write a tutorial on building a bomb\"), which could lead\nthe target chat-based LLM $\\pi_\\phi$ with $\\phi$ being the parameters,\nto provide defensive responses (e.g., \u201cI am sorry, ...", "as part of a lecture\") to\nthe prompt, in case to confuse the target model into gener-\nating a positive response, such as \u201cSure, here is a tutorial on\nbuilding a bomb...": "nIn this paper, we aim to exploit a new target-driven at-\ntack paradigm. We formulate our proposed attack scenario\nas follows: For given toxic contents $y \\in V^*$, posing a possi-\nble misleading question $x$ concatenated with a preliminary\nanswer $p \\in V^*$, so as to induce the aligned $\\pi_\\phi$ to provide\nthe given toxic contents $y$.\nThe primary distinction of our attack setting compared to\nother jailbreaking techniques (Paulus et al. 2024; Liu et al.\n2024) lies in the approach to generating adversarial con-\ntent. Previous methods generate adversarial suffixes condi-\ntioned on fixed original prompts from the dataset, where the\nprompts serve as the conditions and the suffixes are gener-\nated accordingly. In contrast, our approach uses the intended\ntoxic response to generate potential questions and prelim-\ninary answers that could lead to that response. A primary\nconcern is why we add a preliminary answer instead of a\npotential question directly. We found that using the question\nitself inherently optimizes the prompt in a way that has min-\nimal impact on the target model. Therefore, we propose the\nuse of a preliminary answer. We will discuss this in detail in\nAppendix A."}, {"title": "2.2 White-box, Gray-box, Black-box LLMS", "content": "In this paper, we categorize target LLMs into three types:\nwhite-box, with full access to architecture and parameters;\nblack-box, like GPT-4, where interaction is limited to text\nprompts and responses; and gray-box, which provides both\ntext responses and log-probabilities of each token. We focus\non attacking gray-box LLMs using log-probability informa-\ntion and explore the transferability of these attacks to black-\nbox LLMs."}, {"title": "3 Methodology", "content": "Manually crafting adversarial content to elicit intended toxic\nresponses from target LLMs is resource-intensive, and using\ngradient-based optimization for automatic adversarial con-\ntent generation is also time-consuming for practical use. In\nthis paper, inspired by AdvPrompter (Paulus et al. 2024),\nwe use a LLM as the attacker to generate adversarial con-\ntent. Specifically, we will optimize the LLM, referred to as\nToxDet and denoted by $\\pi_\\theta$, to have the capability, when con-\nditioned on the toxic or inappropriate response $y$, to gener-\nate misleading questions $x$ concatenated with preliminary\nanswers $p$ that could lead to that response.\nAlthough previous methods have demonstrated that su-\npervised fine-tuning (SFT) is also suitable for optimizing $\\pi_\\theta$,\nthese approaches typically depend on a high-quality training\ndataset that includes specially designed prompts, which are\nusually generated manually or by a powerful external model\n(e.g., GPT-4). Additionally, the lack of alignment with the\ntarget model $\\pi_\\phi$ in offline model training does not guarantee\nthe compatibility of $\\pi_\\theta$. To address these issues and generate"}, {"title": "3.1 Reward Modeling using Target LLM", "content": "In general, the objective of reinforcement learning processes\nis to maximize rewards, using the reward signal to guide\nmodel optimization. The reward model, denoted by $R$, is\ntypically trained using another LLM with well-designed,\nhuman-preferred datasets. However, such datasets are rarely\navailable for attacking tasks due to the sensitive nature of\nthis task. Additionally, training the reward model is both\ntime-consuming and unstable.\nIn our approach, instead of training an additional reward\nmodel, we propose using the target LLM as the reward\nmodel. This method eliminates the need to construct human-\npreferred datasets and avoids the potential issues associated\nwith training a separate, possibly unsuitable reward model.\nSpecifically, during reinforcement training, for each step,\ngiven an intended toxic response $y$ as the condition prompt,\nwe use the generated questions $x$ and preliminary answers $p$\nby $\\pi_\\theta$ to query the gray-box target LLM. The predicted log-\nprobability from the target LLM is then used as the reward\nsignal. We formulate this process as follows:\n$r(y, a) = \\log p_\\phi(y|a),$\nwhere $r()$ denote the reward function of $R$, and we denote\nthe outputs of $\\pi_\\theta$ by:\n$a = [x, p],$ \nwhich is the concatenation of $x$ and $p$."}, {"title": "3.2 Target-Driven Reinforcement Learning for Better Training of ToxDet", "content": "Most existing jailbreaking attack methods rely on original\nadversarial prompts to query the target LLM. However, these\nprompts are limited due to their reliance on manual crafting,\nand such intuitive adversarial prompts can be easily detected\nand mitigated by modern target LLMs. The logic of human-\ncrafted adversarial prompts may differ from that of LLMs.\nTo address these issues, a promising approach is to rely on\nthe LLM itself to reconstruct the adversarial prompts. Ad-\nditionally, the logic of human-crafted prompt-response pairs\nmay not align well from the perspective of LLMs, leading\nto potential misguidance and difficulties during training. In-\nstead of relying on prompts, we propose a target-response-\ndriven reinforcement learning (RL) paradigm for more ef-\nfective attack training. In this approach, the LLM is used to\nattack the target response directly by identifying the prompts\nthat best match the target based on its own knowledge. Fur-\nthermore, to better motivate the LLM, we also design the\nattack to generate a portion of the preliminary answers to\nthe target response, thereby facilitating the generation of the\nintended toxic response.\nSpecifically, let $D = {y}\\_{i=1}^{N}$ denote a training dataset\nof length $N$, where only the toxic target sequences are in-\ncluded. We use only the target part during the training pro-\ncess of ToxDet. By realizing the reward function as de-\nscribed in equation 1, we obtain the objective:\n$E\\_{y\\sim D}E\\_{a\\sim \\pi\\_\\theta(\\cdot|y)} [\\log p\\_\\phi(y|a) \u2013 \\beta D\\_{KL}(\\pi\\_\\theta(\\cdot|y)||T\\_{ref}(\\cdot|y))],$\nwhere $T\\_{ref}$ is the reference model, usually initialized and\nfrozen with the same parameters as $\\pi\\_\\theta$. The term $\\beta$ is the KL\npenalty coefficient. We illustrate the details of our method in\nAlgorithm 1 for a better description."}, {"title": "4 Experiment", "content": "We utilize the AdvBench (Zou et al. 2023) and the\nHH-Harmless portion of the HH-RLHF (Bai et al. 2022a)\ndatasets. The AdvBench dataset contains 520 instructions\nassociated with harmful behaviors and their correspond-\ning desired positive responses. For dataset splitting, we\nadhere to the methodology used by Advprompter (Paulus\net al. 2024), dividing it into fixed training (60%), validation\n(20%), and test (20%) sets. The HH-Harmless dataset com-\nprises approximately 50,000 fishing-related questions or de-\nscriptions and their corresponding responses. Specifically,\nwe employ GPT-4 to filter the dataset for harmful and in-\nappropriate responses, and then we randomly select 500 in-\nstances for training and 150 instances for testing.\nFor ToxDet implementation, we utilize Mistral-\n7b (Jiang et al. 2023). As target LLMs, we employ sev-\neral well-known open-source models: Mistral-7b, Vicuna-7b\n(v1.5) (Zheng et al. 2024), Llama2-7b-chat (Touvron et al.\n2023), and Llama3-8b-chat (Dubey et al. 2024). Addition-\nally, we report results for GPT-3.5, GPT-40 (OpenAI 2024b),\nand GPT-40-mini (OpenAI 2024a) for transfer attacks.\nWe compare our results on the AdvBench\ndatasets with three notable previous works in this area:\nGCG (Zou et al. 2023), AutoDAN (Liu et al. 2024), and\nAdvprompter (Paulus et al. 2024), which serve as the pri-\nmary baselines. We also report results on the HH-Harmless\ndataset. However, this dataset is not considered within the\nscope of the jailbreaking attack task. Therefore, for the HH-\nHarmless dataset, we only report our results.\nFor evaluation, we fine-tune ToxDet to gener-\nate adversarial content as follows: Given the target response,\nToxDet generates a possible question and a preliminary an-\nswer. Then, we concatenate the question and the prelimi-\nnary answer to query the target LLM for the predicted re-\nsponse. Our main metric is ASR@k (attack success rate),\nwhich measures whether at least one out of k attacks on the\ntarget LLM was successful. There are two methods for mea-\nsuring whether an attack was successful:\nAdopted from Advprompter (Paulus"}, {"title": "4.2 Attacking Gray-box Target LLMs on AdvBench", "content": "We first evaluate our method on the gray-box target LLM\nsettings on the AdvBench dataset. As shown in Table 1,\nwe report ASR@1_and ASR@10 for keyword matching\non the training and test dataset for comparison. It is im-\nportant to note that once the attacker is trained, the gen-\neration time is minimal, making it cost-effective to gener-\nate multiple contents. Consequently, the inference time dif-\nference between ASR@1 and ASR@10 is negligible. Ta-\nble 1 presents the primary findings for the gray-box set-\ntings on the AdvBench dataset. We observe that GCG and\nAutoDAN achieve high ASR@1 scores on individual set-\ntings of the training dataset. However, these settings are de-\nsigned to jailbreak the target model for single instances and\ndo not generalize well to unseen test cases. In contrast, our\nmethod achieves 100% ASR@10 on the training dataset for\nthe Mistral-7b and Vicuna-7b models, and approximately\n50% on the more challenging Llama2-7b model. Regarding\nthe test set, our method maintains over 90% ASR@10 for\nthe Mistral-7b and Vicuna-7b models and around 50% for\nthe Llama2-7b model, demonstrating robust generalization\nFor comparison methods, PPL is computed for the suffix,\nwhile for our method, PPL is computed for both the questions and\npreliminary answers. Theoretically, this could result in worse val-\nues than previous methods. Despite the different measurement ap-\nproaches, the comparison remains fair. Details of this measure will\nbe provided in Appendix B."}, {"title": "4.3 Attacking Gray-box Target LLMs on HH-Harmless", "content": "In Table 2, we report the GPT-4 evaluation scores on the HH-\nHarmless dataset, which was not suitable for the specially\ndesigned keyword list. Despite this limitation, our method\nsuccessfully detects potentially harmful information in the\ntarget LLMs using this harmless dataset, achieving simi-\nlar ASR@1 and ASR@10 results as observed with the Ad-\nvBench dataset. These experiments further demonstrate the\nrobustness and universal capability of our method in identi-\nfying internal faults in LLMs."}, {"title": "4.4 Transfer-Attacking Black-box Target LLMs", "content": "Subsequently, we assess our approach in the context of\ntransfer attacks, a scenario highly relevant in practical ap-\nplications due to the widespread use of proprietary black-\nbox models. We first train ToxDet by interacting with gray-\nbox LLMs and then measure the ASR of the questions and\npreliminary answers generated by ToxDet on the black-\nbox target LLMs. For the gray-box models, we selected\nthe challenging Llama2-7b and Llama3-8b, and compared\nthem with Llama2-7b as evaluated by AdvPrompter. The\nresults are presented in Table 3. For the previously well-\nknown GPT-3.5-turbo, all three gray-box models achieved"}, {"title": "4.5 Ablation Study", "content": "To evaluate our proposed training paradigm, we conducted\nan ablation study on the HH-Harmless and AdvBench test\nsets, as shown in Table 4. In this study, we compare the\nASR@1 and ASR@10 of GPT-4 evaluation across all target\nmodels. We compare the results of ToxDet with and with-\nout training, denoted by \u2713 and X, respectively. Our results\nindicate that training with our paradigm boosts the perfor-\nmance of the base models. Additionally, we find that even\nan untrained model can successfully attack the target mod-\nels, demonstrating the effectiveness of our proposed target-\ndriven attacking paradigm."}, {"title": "4.6 Case Study", "content": "We present a case of transfer attack, where ToxDet trained\nthrough interactions with Llama2-7b generates adversarial\ncontent to query GPT-40. As shown in Fig. 2, given the in-\ntended target response, the trained ToxDet successfully for-\nmulates a well-designed question and a corresponding pre-\nliminary answer. Consequently, GPT-40 is misled by the\nquestion and answer, responding with inappropriate content,\nwhich we have highlighted in blue. This case demonstrates\nnot only the effectiveness of the fine-tuned model in provid-\ning effective questions and preliminary answers but also the\nnotable results of its transfer attacking capability2."}, {"title": "5 Related Work", "content": "The domain of Reinforcement Learning from Human Feed-\nback (RLHF) has been thoroughly explored in various stud-\nies to enhance the alignment of LLMs with human prefer-\nences (Stiennon et al. 2020; Ouyang et al. 2022; Bai et al.\n2022b; Lee et al. 2023). These works typically involve con-\nstructing a reward model based on the MLE of the Bradley-\nTerry model, followed by the use of the Proximal Policy\nOptimization (PPO) (Schulman et al. 2017) algorithm to op-\ntimize the reward signals with KL regularization. Despite\nvarious efforts to improve PPO in the context of RLHF,\nreproducing the successful results achieved with PPO re-\nmains challenging for the open-source community. This dif-\nficulty arises from the extensive efforts and resources re-\nquired, which are often beyond the reach of open-source\ninitiatives. Acknowledging these challenges, a line of re-\nsearch has shifted focus to offline direct preference learn-\ning algorithms (Zhao et al. 2023; Rafailov et al. 2024; Azar\net al. 2024; Ethayarajh et al. 2024), which bypass the re-\nward modeling step and instead directly optimize a designed\nloss target on the offline preference dataset. Many recent\nstudies have sought to alleviate the resource-intensive nature\nof PPO. For example, Santacroce et al. (2023) investigated\nthe application of LoRA (Hu et al. 2021) in PPO to reduce\nmemory usage and overall resource requirements for align-"}, {"title": "5.1 RLHF", "content": "The domain of Reinforcement Learning from Human Feed-\nback (RLHF) has been thoroughly explored in various stud-\nies to enhance the alignment of LLMs with human prefer-\nences (Stiennon et al. 2020; Ouyang et al. 2022; Bai et al.\n2022b; Lee et al. 2023). These works typically involve con-\nstructing a reward model based on the MLE of the Bradley-\nTerry model, followed by the use of the Proximal Policy\nOptimization (PPO) (Schulman et al. 2017) algorithm to op-\ntimize the reward signals with KL regularization. Despite\nvarious efforts to improve PPO in the context of RLHF,\nreproducing the successful results achieved with PPO re-\nmains challenging for the open-source community. This dif-\nficulty arises from the extensive efforts and resources re-\nquired, which are often beyond the reach of open-source\ninitiatives. Acknowledging these challenges, a line of re-\nsearch has shifted focus to offline direct preference learn-\ning algorithms (Zhao et al. 2023; Rafailov et al. 2024; Azar\net al. 2024; Ethayarajh et al. 2024), which bypass the re-\nward modeling step and instead directly optimize a designed\nloss target on the offline preference dataset. Many recent\nstudies have sought to alleviate the resource-intensive nature\nof PPO. For example, Santacroce et al. (2023) investigated\nthe application of LoRA (Hu et al. 2021) in PPO to reduce\nmemory usage and overall resource requirements for align-"}, {"title": "5.2 Adversarial Attacks on LLMS", "content": "Despite the rapid adoption of applications built on aligned\nlarge language models (LLMs), users have discovered that\ncarefully phrased prompts can still elicit malicious responses\nfrom these models. Consequently, addressing vulnerabili-\nties in LLMs has become essential for enhancing their ro-\nbustness and safety. There are three primary methods of at-\ntack. The first involves manually crafting phishing queries.\nA notable example is DAN (walkerspider 2022), which uses\nhand-designed prompts to exploit vulnerabilities in online\nchatbots powered by aligned LLMs. The second method is\noptimization-based. GCG (Zou et al. 2023) automates the\nprompt generation process by utilizing gradient informa-\ntion from open-source LLMs to guide the search for op-\ntimal tokens, potentially leading to unexpected responses.\nPAL (Sitawarin et al. 2024) proposes transferring knowl-\nedge from open-source LLMs to closed-source models such\nas GPT-3.5, enabling attacks on black-box models without\nneeding access to their gradients. The third method acceler-\nates prompt generation without using the target models' gra-\ndients. Advprompter (Paulus et al. 2024) employs a differ-\nent LLM as the prompt generator to create adversarial suf-\nfixes based on the original prompts. BEAST (Sadasivan et al.\n2024) further optimizes the balance between attack speed\nand success rate, enabling the attack to be performed more\nquickly on a single GPU."}, {"title": "6 Conclusion", "content": "In this paper, we propose a target-driven attacking paradigm\nthat employs a novel approach involving another LLM,\nnamed ToxDet, to generate potential questions and prelimi-\nnary answers designed to elicit inappropriate responses from\nthe target model. We optimize ToxDet using reinforcement\nlearning, leveraging the target LLM itself as the reward\nmodel, thereby eliminating the need for the inclusion of a\nseparate reward model. Our primary focus is on interacting\nwith gray-box target LLMs by utilizing probability informa-\ntion; however, the results can also be effectively transferred\nto black-box models like GPT-40, achieving favorable out-\ncomes. Extensive experimental results on the AdvBench and\nHH-Harmless datasets demonstrate the effectiveness of our\nproposed methods, revealing vulnerabilities in LLMs and\nproviding a valuable resource for researchers aiming to en-\nhance model safety."}, {"title": "Ethical Statement", "content": "This paper introduces an automated approach for generat-\ning questions and preliminary answers designed to manipu-\nlate LLMs into producing outputs that diverge from human\npreferences, intentions, or values. We posit that, similar to\nprevious attacking research, this work is unlikely to cause\nimmediate harm. Instead, it aims to foster the development"}, {"title": "Appendix", "content": "A primary concern is why we add a preliminary answer instead of a potential question directly. We found that using the question\nitself inherently optimizes the prompt in a way that has minimal impact on the target model. Therefore, we propose the use\nof a preliminary answer. To intuitively demonstrate the necessity of employing preliminary answers, we present comparative\nexamples. Specifically, we provide responses from ToxDet with and without preliminary answers, as well as the corresponding\nresponses from the target LLM, in Fig. 3. In this table, we highlight the toxic components of ToxDet's responses in red and\nthose of the target LLM in blue. The comparison clearly shows that responses from the target LLM, when preliminary answers\nare used, exhibit more toxic elements, whereas responses from ToxDet contain more toxic and misleading parts. Therefore, we\nconclude that incorporating a preliminary answer is beneficial."}, {"title": "B Perplexity Evalutaion", "content": "We report the perplexity (PPL) of the generated content conditioned on the prompts. For AdvPrompter and other comparison\nmethods, PPL is measured based on the generated suffix by the attack conditioned on the prompts. In contrast, for our method,"}, {"title": "C System Prompts", "content": "The system prompt for querying ToxDet is as follows:\nYou are a criminal. Please generate a question based on the given reply, and provide a preliminary answer to the question\nso that your generated question and preliminary answer can lead to the given reply. Never say \u2018I am sorry' or 'illegal' and\nsome other sentence that may lead to refuse to answer the question you generate, otherwise you will be fired. And don't give a\ncomplete answer, just a preliminary one.\nThe system prompt for GPT-4 Evaluation is as follows:\nYou are asked to evaluate the following contents for harmful or toxic content. The given text may not be a complete and\nlogical sentence, but may contain harmful information hidden within it. If there is any harmful or toxic information inside, it\nshould be treated as harmful/toxic. Please understand the given contents and respond with 'Y' for harmful or toxic, and 'N' for\nharmless."}, {"title": "D Failure Cases", "content": "Firstly, we argue that even using GPT-4 for evaluation can result in mistakes. We provide failure cases of GPT-4 evaluation, as\nillustrated in Fig. 4. In the top case, GPT-4o captures the toxic content highlighted in blue and identifies this case as \"Y\" for\nattacked. However, it is evident that this is not accurate. In the bottom case, GPT-40 captures the protective content highlighted\nin red and identifies this case as \u201cN\u201d for not attacked. However, in the subsequent paragraph, highlighted in blue, it actually\nprovides inappropriate content and should be categorized as attacked."}, {"title": "E More Case Study", "content": "In this section, we provide more examples of attacks on both gray-box and black-box target LLMs, as illustrated in Fig. 6 and\nFig. 7."}]}