{"title": "Whole-Herd Elephant Pose Estimation from Drone Data for Collective Behavior Analysis", "authors": ["Brody McNutt", "Libby Zhang", "Angus Carey-Douglas", "Fritz Vollrath", "Frank Pope", "Leandra Brickson M"], "abstract": "This research represents a pioneering application of automated pose estimation from drone data to study elephant behavior in the wild, utilizing video footage captured from Samburu National Reserve, Kenya. The study evaluates two pose estimation workflows: DeepLabCut, known for its application in laboratory settings and emerging wildlife fieldwork, and YOLO-NAS-Pose, a newly released pose estimation model not previously applied to wildlife behavioral studies. These models are trained to analyze elephant herd behavior, focusing on low-resolution (~50 pixels) subjects to detect key points such as the head, spine, and ears of multiple elephants within a frame. Both workflows demonstrated acceptable quality of pose estimation on the test set, facilitating the automated detection of basic behaviors crucial for studying elephant herd dynamics. For the metrics selected for pose estimation evaluation on the test set-root mean square error (RMSE), percentage of correct keypoints (PCK), and object keypoint similarity (OKS)\u2014the YOLO-NAS-Pose workflow outperformed DeepLabCut. Additionally, YOLO-NAS-Pose exceeded DeepLabCut in object detection evaluation. This approach introduces a novel method for wildlife behavioral research, including the burgeoning field of wildlife drone monitoring, with significant implications for wildlife conservation.", "sections": [{"title": "1. Introduction", "content": "More nuanced and precise understanding of elephant behavior is crucial for developing effective conservation strategies in the face of multiplying threats, such as rapid climate change and loss of habitat and migratory corridors. African savanna elephants (Loxodonta africana) live in flexible fission-fusion societies that result in sophisticated social interactions and decision-making at different organization levels; thus elephant behavior is best understood concurrently at both the individual and group level [1]. Direct field observation is the established approach to studying elephant behavior at the spatial and temporal resolution required to gain insight into these types of sophisticated interactions. A significant disadvantage, however, is the limited field-of-view of a single observer and the practical challenges of recording simultaneously behaviors from multiple animals.\nAerial-based video recording platforms are emerging as a promising approach to capturing multi-animal behavior in open terrain over greater field-of-views and spatial ranges than previously possible. For example, Koger et al. released a comprehensive software package with individualized detecting, tracking, and pose estimation modules [9]. The emergence of aerial-based video recording platforms has been enabled by continued progress in unmanned aerial vehicle technology and in computer vision. The latter was significantly advanced by the deep learning revolution, allowing the propagation of information-dense raw data throughout all modules of the system. Other important advantages of these end-to-end deep learning solutions included simplifications in piping and parameter tuning. With this revolution, however, also came the reports of instances in which the state-of-the-art methods could not generalize out-of-the-box to other domains, as they were purported to. This was particularly illustrated in fields such as computer-vision-based animal pose estimation [11] or animal detection [2, 3]. In particular, the performance gap was due to differences such as labeled dataset sizes and challenging visual discrimination conditions that were overcome by strategies such as fine-tuning on animal-specific data [12].\nIn this paper, we revisit this question of modular, composite solutions, such as the one provided by Koger et al., versus end-to-end solutions given the objective of extracting multi-animal pose estimates from aerial video recordings. We note that this task differs from overhead video recordings that might be found in laboratory settings due to the increased background complexity and variability and significantly smaller size of the subjects (8-70 px in our dataset).\nThis paper details the methods for adapting this data for use by DeepLabCut [11] and explores the viability of YOLO-NAS-Pose [15], the former being a common tool"}, {"title": "2. Methods", "content": "2.1. Dataset\nThis research employs drone technology equipped with a wide-angle camera to observe a herd of elephants, ensuring visibility of all herd members in a single frame. Drone data collection introduces specific challenges. The Save the Elephants field team aimed to optimize data quality while minimally impacting the elephants to capture authentic behavior, noting from previous studies that drones can trigger varying responses from elephants [4, 7, 13]. While data with higher resolution would have been advantageous, using multiple drones could have altered the elephants' natural behaviors. To mitigate this, the drone was operated at the maximum allowable height in Kenya (400ft). The drone captured footage at 29 fps with a 3840x2160 resolution on a stabilizing gimbal platform. During recording, the drone is positioned stationary, overhead at a set height throughout the study to ensure a uniform viewing angle. At the drone's operating altitude for this study, calves are represented from trunk to tail by about 8 pixels, and adults by up to 70 pixels in the video footage. Figure 1 showcases a sample frame from the drone footage.\nThe study focused on identifying keypoints relevant to social behaviors, such as head orientation and ear flapping. Therefore, the 8 keypoints shown in Figure 2 were chosen as our targets for pose estimation.\nThis dataset consists of 23 videos, each approximately 5 minutes in length. Overhead frames were selected from these videos, resulting in a total of 133 frames containing 1308 elephants. A manually annotated training dataset was created from these frames, including bounding boxes and the keypoints defined in Figure 2. During annotation, when the ears were not discernible on especially small calves, only spine keypoints were annotated, and the ears were labeled as \"occluded\".\nThe labeled dataset was divided into a 90-10-10 train-validation-test split. For this data split, the test set comprised four entirely set-aside videos, ensuring that no frames in the test set originated from the same videos as those in the training and validation sets. In contrast, while the training and validation images were unique, they could still come from the same videos.\n2.1.1 Preprocessing\nBefore entering either workflow, the data was preprocessed to meet the YOLOv5 model's requirements for object size [10]. Labeled video frames were tiled to 800x800 pixels, with a 33% overlap in window stride, to ensure a proper object size for the elephants within the frame. Pose estimation was then applied to the data using the following two"}, {"title": "2.2. DeepLabCut Workflow", "content": "2.2.1 Elephant Detector\nInitially, a YOLOv5 model [8] and a MegaDetector [2, 3] pretrained model were fine-tuned on the dataset defined in the previous section. The models were trained to generate bounding boxes for elephants within a given frame.\nOnce bounding boxes were predicted on a frame, square images were extracted, centered on the detected bounding box, with the dimension determined by adding a 20% margin to the largest dimension of the bounding box. These patches were then resized to 100x100 pixels. This format was used to train DeepLabCut, providing centered, large images of the animals to mitigate any unwanted effects from inconsistent backgrounds in the images.\n2.2.2 DeepLabCut\nTo train DeepLabCut, the pose dataset defined in the dataset section was used to train a DeepLabCut Model. The dataset was converted to the DLC training format, and the model was trained for 800k iterations until loss converged."}, {"title": "2.3. YOLO-NAS-Pose workflow", "content": "To train the YOLO-NAS-Pose network, the same dataset used for training the detector and DeepLabCut workflow was utilized, with manually annotated poses added. The model was then trained to provide bounding boxes and poses across the entire image."}, {"title": "2.4. Evaluation", "content": "The dedicated set-aside test set was used to evaluate both workflows. The bounding box accuracy for both the YOLOv5 detector and YOLO-NAS-Pose was evaluated using mean Average Precision (mAP) [5]. Pose estimation for both workflows was evaluated using root mean square error (RMSE), percentage of correct keypoints (PCK) [6], and object keypoint similarity (OKS) [14]. However, to properly compare the two methods, since DeepLabCut can only perform pose estimation on extracted bounding boxes, only the bounding boxes correctly detected in the YOLO-NAS-Pose workflow were selected for pose estimation evaluation.\nTo identify correctly detected objects, the bounding boxes output by YOLO-NAS-Pose were filtered using non-maximum suppression (NMS) with a maximum overlap threshold of 0.5. These de-duplicated bounding boxes were then sorted by confidence score and compared to the ground truth annotations to calculate Intersection over Union (IoU). Each predicted bounding box that shared an IoU greater than or equal to 0.5 with a ground truth bounding box was considered a candidate match. In instances where multiple predictions overlapped with the same ground truth bounding box, the prediction with the highest confidence score was selected.\n2.4.1 Video Tracker for Visualization\nAlthough continuous video is not necessary for training or quantitatively evaluating pose estimation performance, having continuous video of an individual significantly aids in qualitative assessment. Once individuals were detected in each frame, DeepSORT [16, 17] was employed to generate patched video segments of each detected elephant. This method identifies continuous objects within the video by comparing patch locations, image embeddings, and the momentum of the objects, resulting in a sequence of bounding boxes for each individual. Due to the low resolution of some individuals, those with bounding boxes smaller than"}, {"title": "3. Results", "content": "During the initial workflow where the YOLOv5 detector was trained, it was observed that utilizing the standard pre-trained weights of YOLOv5 yielded better results compared to beginning with the megadetector weights. Mean average precision metrics for bounding box detectors are shown in Table 1 The evaluation metrics described in the methods"}, {"title": "4. Discussion", "content": "This research represents a pioneering application of automated pose estimation to elephant video drone data in wildlife settings. The results provide valuable insights and opportunities for future improvements in wildlife behavioral monitoring.\nWhen examining the metrics in Table 2, both models demonstrate reasonable performance in pose estimation on the test dataset. YOLO-NAS-Pose performed well, though not perfectly, in both elephant detection and pose estimation across all metrics, establishing it as a promising tool for wildlife behavioral studies. However, while the results are promising, the current metrics do not yet achieve the desired level of accuracy for a fully automated workflow, indicating that further development and refinement are necessary.\nIt is important to note the discrepancies in keypoint accuracy within the metrics. For DeepLabCut, the accuracy of both ear tip detections was slightly lower, which was expected due to their wide range of motion relative to other keypoints and the lowest confidence during manual annotation. However, the hips, surprisingly, had the worst keypoint accuracy for DeepLabCut. This could be attributed to the hips being the most isolated keypoint, with fewer adjacent reference points for accurate positioning. This poor performance is unexpected, particularly since the hips were one of the highest-performing keypoints for YOLO-NAS-Pose. Conversely, YOLO-NAS-Pose struggled the most with the \"forehead\" keypoint, an area where DeepLabCut does not experience issues. One potential reason could be the difficulty in accurately labeling the \"forehead\", especially when the trunk is extended, making it challenging to locate the front of the face. Future investigations will explore the causes of these discrepancies.\nQualitatively, from watching the tracking videos applied to the full videos which were the source of the train and validation sets, DeepLabCut performed quite well, but occasionally failed to track the elephants' ears, often defaulting to a \"neutral\" ear posture in uncertain cases. This issue was particularly prevalent for smaller elephants.\nAnother noteworthy aspect to consider is the comparison between full-frame pose estimation of multiple elephants and pose estimation of an individual in an extracted patch. These approaches offer distinct advantages. Full-frame pose estimation simplifies the workflow, making it an attractive solution for automated processes. However, segmenting out individuals first provides several benefits for training a more robust network. For example, by filtering for only large elephants during training, one can avoid the challenges of training on smaller calves whose resolution may be insufficient for accurate labeling.\nMoreover, individual labels allow for better balancing of the training dataset, ensuring an even distribution of poses. This technique is crucial for training pose estimators effectively. In contrast, a random sampling of data tends to result in a dataset dominated by neutral postures, limiting the diversity of the training set.\nWhile DeepLabCut did not outperform YOLO-NAS-Pose in this task, there are scenarios where it can be highly useful. The supplementary materials highlight an initial experiment, not detailed in this work, demonstrating that DeepLabCut can yield satisfactory results even with very small training datasets (~100 frames). If the researcher's objective is to label a few frames in a video and subsequently obtain poses for the entire video, DeepLabCut proves to be a powerful option. This capability makes it particularly valuable for projects with limited annotated data, where rapid and efficient pose estimation is required.\nLooking ahead, for low-resolution pose estimation, the challenge for detecting more complex keypoints for more detailed behavior analysis lies in detecting specific keypoints by examining video sequence changes. The difficulty of identifying an elephant's ear position in a single frame highlights the limitations of current frame-by-frame pose estimation methods, which do not consider inter-frame continuity. Investigating frame-to-frame analysis methods, such as optical flow or recurrent neural networks, could offer a means to further enhance pose estimation accuracy by ensuring consistency in detected movements across video frames."}, {"title": "5. Conclusion", "content": "This research represents a substantial advancement in integrating automated behavior analysis methods into wildlife research by comparing different pose estimation techniques. It paves the way for more sophisticated studies of wildlife behaviors in their natural habitats, involving multiple individuals in extensive scenes. The findings indicate that YOLO-NAS-Pose is a feasible and attractive option for pose estimation, offering a straightforward workflow and superior performance metrics. However, further development and refinement are necessary. The implications of this work extend beyond the study of elephant behaviors, providing valuable insights for the future development of drone-based wildlife behavior studies across various species and ecosystems."}]}