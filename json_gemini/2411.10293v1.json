{"title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception", "authors": ["Ryoma Yataka", "Adriano Cardace", "Pu (Perry) Wang", "Petros Boufounos", "Ryuhei Takahashi"], "abstract": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.77+ IoU for instance segmentation, respectively.", "sections": [{"title": "1 Introduction", "content": "Perception information encompasses the processes and technologies to detect, interpret, and understand their surroundings. Complementary to the mainstream camera and LiDAR sensors, radar can enhance the safety and resilience of perception under low light, adversarial weather (e.g., rain, snow, dust), and hazardous conditions (e.g., smoke, fire) at affordable device and maintenance cost. An emerging application of radar perception is indoor sensing and monitoring for elderly care, building energy management, and indoor navigation [7]. A notable limitation of indoor radar perception is the low semantic features from radar signals.\nEarlier efforts use radar detection points [42, 30] to support simple classification tasks such as fall detection and activity recognition over a limited number of patterns. To support challenging perception tasks such as object detection, pose estimation, and segmentation, lower-level radar signal representation such as radar heatmaps is more preferred. Along this line, the earliest work is RF-Pose [43] using a convolution-based autoencoder network to fuse features from the two radar views and regress keypoints for 2D image-plane pose estimation. It is later extended to 3D human pose estimation [44]. It is noted that RF-Pose is not publicly accessible. More recently, RFMask [38]"}, {"title": "2 Related Work", "content": "Radar-based Object Detection and Segmentation: Indoor radar perception tasks include object detection (BBoxes), pose estimation (keypoints), and instance segmentation (human masks) [1, 35, 43, 44, 20, 23, 45, 38, 46], and radar datasets in different data formats were reported in [35, 31, 30, 39, 2, 40, 14, 38, 26]. Particularly, radar heatmap-based approaches have gained attention not only in indoor perception [43, 44, 14, 38, 26] but also for automotive radar perception [19, 24, 32, 11, 5], due to richer semantic features compared to those extracted from sparse radar point clouds [31, 30, 39, 2, 15, 40, 41]. RF-Pose [43] predicts human poses on the image plane using a convolution autoencoder-based architecture. With the HIBER dataset [38], RFMask considers proposal-based object detection and instance segmentation. More recently, MMVR [26] has been openly released to accelerate advancements in indoor radar perception.\nImage-based Object Detection and Segmentation with DETR: Since the introduction of DETR for 2D image-plane object detection, subsequent studies have been developed based on its framework [21, 47, 4, 17, 37, 18, 10, 25], largely due to DETR's ability to eliminate the need for hand-designed components such as non-maximum suppression (NMS). In [21], Conditional DETR decomposes the roles of content and positional embeddings in the transformer decoder, improving not only prediction accuracy but also training convergence speed. More recently, [25] has proposed Rank-DETR as a rank-oriented architectural design, guaranteeing lower false positives and false negatives in prediction."}, {"title": "3 Preliminary", "content": "Generation of Radar Heatmaps: Conceptually, let us consider a pair of (virtual) horizontal and vertical antenna arrays with $N_{ant}$ elements for each array, sending a set of frequency modulated continuous waveform (FMCW) pulses for object detection [26, 38, 34]. The two 1D arrays generate one horizontal radar view in the azimuth-depth (x - z) domain and one vertical radar view in the elevation-depth (y \u2013 z) domain,\n$$Y_{hor} (t, x, z) = \\sum_{k=1}^{K_p} \\sum_{m=1}^{M} s_{k,m,t}e^{i2\\pi d_m(x,z)/\\lambda_k},$$\n$$Y_{ver} (t, y, z) = \\sum_{k=1}^{K_p} \\sum_{m=1}^{M} s_{k,m,t}e^{i2\\pi d_m(y,z)/\\lambda_k},$$\nwhere $s_{k,m,t}$ denotes the k-th sample of FMCW sweep on the m-th antenna at time t, $\\lambda_k$ is the wavelength of the k-th sample, $d_m (x, z)$ denotes the round-trip distance from the m-th array element to a position $(x, z)$, and $K_p$ and M denote the number of samples and the number of array antennas, respectively. Usually, the azimuth x is in an interval of $x \\in X = [X_{min} : \\Delta x : X_{max}]$ and the elevation y and the depth z are similarly defined. At a particular time t, we have the horizontal radar heatmap $Y_{hor}(t) = {|Y_{hor} (t, x, z) |}_{z \\in X} \\in \\mathbb{R}^{W\\times D}$ and the vertical radar heatmap $Y_{ver}(t) = {|y_{ver} (t, y, z) |} \\in \\mathbb{R}^{H\\times D}$ with a shared depth axis. The multi-view radar testbeds in HIBER [38] and MMVR [26] utilize advanced MIMO-FMCW radar systems. We defer the MIMO-FMCW radar heatmap generation to Appendix D."}, {"title": "4 RETR: Radar Detection Transformer", "content": "We first present the RETR architecture and then highlight radar-oriented modifications. We defer the discussion on Segmentation to Appendix B.\n4.1 RETR Architecture\nWe present the RETR architecture in Fig. 3, introducing its major modules in a left-to-right order. Refer to Appendix A for the detailed architecture.\nBackbone: Given $Y_{hor} \\in \\mathbb{R}^{T\\times W\\times D}$ and $Y_{ver} \\in \\mathbb{R}^{T\\times H\\times D}$, a shared backbone network (e.g., ResNet [8]) generates separate horizontal-view and vertical-view radar feature maps: $Z_{hor} = \\text{backbone} (Y_{hor}) \\in \\mathbb{R}^{C \\times \\frac{W}{s} \\times \\frac{D}{s}}$ and $Z_{ver} = \\text{backbone} (Y_{ver}) \\in \\mathbb{R}^{C \\times \\frac{H}{s} \\times \\frac{D}{s}}$, where C and s represent the number of channels and downsampling ratio over the spatial dimension, respectively.\nTokenization: A transformer-based encoder expects a sequence of tokens as input. This is done by mapping the feature maps into a sequence of P multi-view radar tokens $H = {H_{hor}, H_{ver}}\\in \\mathbb{R}^{C\\times P}$: $Z_{hor} \\rightarrow H_{hor} \\in \\mathbb{R}^{C\\times P_{hor}}$ and $Z_{ver} \\rightarrow H_{ver} \\in \\mathbb{R}^{C\\times P_{ver}}$, where $P = P_{hor} + P_{ver}$. We defer the tokenization discussion to Section 4.2.\nEncoder as Cross-View Radar Feature Association: The transformer encoder provides a simple yet effective method for associating radar features from both horizontal and vertical views by applying self-attention over the pool of P multi-view radar tokens $H = {H_{hor}, H_{ver}}\\in \\mathbb{R}^{C\\times P}$, eliminating the need for cumbersome association schemes. Specifically, the l-th (l = 0,\u00b7\u00b7\u00b7, $L_{self}$ - 1) encoder layer updates the multi-view radar tokens through multi-head self-attention $Att_{self}$:\n$$H^{l+1} = H^l + FFN (H'), H' = H^l + Att_{self} (Que (H^l), Key (H^l), Val (H^l)),$$"}, {"title": "4.2 Top-K Feature Selection as Tokenization", "content": "In DETR, the tokenization simply collapses the spatial dimensions of the feature map into a single dimension, resulting in $P_{hor} = WD/s^2$ and $P_{ver} = HD/s^2$ tokens for the horizontal and vertical radar feature maps, respectively. As a result, we have $P = (W + H)D/s^2$ multi-view radar tokens. It is known that the complexity of transformers grows quadratically with respect to the token length P. Here, we introduce a customized Top-K feature selection as tokenization, maintaining a low complexity for the RETR encoder and decoder: $H_{hor} = Selector(Z_{hor}) \\in \\mathbb{R}^{C\\times K}$ and $H_{ver} = Selector (Z_{ver}) \\in \\mathbb{R}^{C\\times K}$, where $K \\ll \\text{min}{WD/s^2, HD/s^2}$. In this case, we shrink the multi-view radar tokens from $P = (W + H)D/s^2$ to $P = 2K$. For each radar frame, we consistently select the Top-K strongest features, which may originate from varying spatial locations depending on the specific radar frame. Consequently, the gradient propagates back through the selected K features to the backbone weights, irrelevant to their spatial locations."}, {"title": "4.3 TPE: Tunable Positional Encoding", "content": "The TPE is built on the top of the concatenation operation between the content embedding c (either feature embedding h at the encoder or decoder embedding q at the decoder) and positional embedding p in the conditional DETR [21] (see Fig. 4 (b)):\n$$(C_{que} \\oplus P_{que})^T (C_{key} \\oplus P_{key}) = C_{que}^T C_{key} + P_{que}^T P_{key},$$\nwhere $\\oplus$ denotes concatenation, rather than the sum in DETR [3] (see Fig. 4 (a)):\n$$(C_{que} + P_{que})^T (C_{key} + P_{key}) = C_{que}^T C_{que} C_{key} + C_{que}^T P_{key} + P_{que}^T C_{key} + P_{que}^T P_{key}.$$\nIt is seen that Eq. 10 eliminates the cross terms between the content and positional embeddings in Eq. 11 and, allowing content/positional embeddings focus on their respective attention weights, contributes to faster training convergence [21].\nIn our case, the positional embedding is composed of a depth (y) axis and an angular (either azimuth x or elevation z) axis. As such, p = d\u2295 a with d representing the depth positional embedding and a the angular positional embedding. Then expanding Eq. 10 with p = d + a leads to\n$$(C_{que} \\oplus d_{que} \\oplus a_{que})^T (C_{key} \\oplus d_{key} \\oplus a_{key}) = C_{que}^T C_{key} + d_{que}^T d_{key} + a_{que}^T a_{key}.$$\nIn Eq. 12, we have the following observations:\n1.  $c_{que}^T C_{key}$ reflects how similar the features in the key and query may appear;\n2.  Depth similarity $d_{que}^T d_{key}$ remains consistent regardless of whether the key and query originate from the same radar view or different radar views;\n3.  Angular similarity $a_{que}^T a_{key}$ can be a self-angular similarity (azimuth-to-azimuth or elevation-to-elevation) when the key and query are from the same radar view, or a cross-angular similarity (azimuth-to-elevation or elevation-to-azimuth) for different radar views.\nMotivated by the above observations, we can promote higher similarity scores for keys and queries with similar depth embeddings than those far apart in depth, especially for the ones from different views, by allowing for adjustable dimensions between depth and angular embeddings:\n$$d_{dep} = \\alpha d_{pos}, d_{ang} = (1 - \\alpha) d_{pos} \\rightarrow d_{dep} + d_{ang} = d_{pos},$$\nwhere the tunable dimension ratio \u03b1 is in the interval [0, 1]. As illustrated in Fig. 4 (c), when \u03b1 = 0.5, the positional embedding is equivalent to that used in conditional DETR. When \u03b1 approaches 0, the depth positional embedding is minimized, making the depth similarity $d_{que}^T d_{key}$ negligible in Eq. 12. Conversely, as \u03b1 approaches 1, the depth positional embedding dimension increases, and so does the importance of the depth similarity in Eq. 12.\nWe implement our TPE with a fixed sine/cosine positional encoding along the depth and angular (azimuth or elevation) dimension. For an even depth/angular positional dimension, we have\n$$d_{2i} = \\text{sin}(p_{dep}/\\tau^{2i/d_{dep}}), d_{2i+1} = \\text{cos}(p_{dep}/\\tau^{2i/d_{dep}}), i = 0,1,\\dots, d_{dep}/2 \u2013 1,$$\n$$a_{2i} = \\text{sin}(p_{ang}/\\tau^{2i/d_{ang}}), a_{2i+1} = \\text{cos}(p_{ang}/\\tau^{2i/d_{ang}}), i = 0,1,..., d_{ang}/2 \u2013 1,$$"}, {"title": "4.4 Tri-Plane Set-Prediction Loss", "content": "DETR calculates a matching cost matrix with each element constructed from 1) a classification cost $L_{class}$ and 2) a BBox loss between one of N predictions b and one of ground truth BBoxes $\\hat{b}$ (including the \"no object\" class). The BBox loss is a weighted combination of the generalized intersection over union (GIoU) loss $L_{GIOU}$ [28] and the $l_1$ loss $L_{L_1}$:\n$$L_{box} (b, \\hat{b}) = \\lambda_{GIOU} L_{GIOU} (b, \\hat{b}) + \\lambda_{L_1} L_{L_1} (b,\\hat{b}),$$\nwhere \u03bb denotes the weight. Over the permutation set $S_N$ between N predictions and ground truth objects, the Hungarian algorithm [12] is applied with the matching cost matrix to find the optimal assignment $\u03c3^* \u2208 S_N$ of predictions to ground truth. Given $\u03c3^*$, the loss is computed only for the matched pairs and is referred to as the set-prediction loss.\nSince RETR predicts 3D BBoxes $\\hat{g}$ in the 3D radar coordinate and maps them into the 2D image plane, we propose to enhance the above Hungarian match cost matrix using a Tri-Plane BBox Loss from both the radar coordinate and image plane. This is illustrated in Fig. 5, where a 3D BBox $\\hat{g}$ in the radar coordinate is projected onto 1) the 2D horizontal radar plane as $b_{hor} = proj_{hor}(\\hat{g})$ (the top branch); 2) the 2D vertical radar plane as $b_{ver} = proj_{ver}(\\hat{g})$ (the middle branch); and 3) the 2D image plane as $b_{image}$ of Eq. 9 (the bottom branch). The tri-plane BBox loss $L_{tri}^{C_{box}}$ sums up 2D BBox losses over all three planes using Eq. 16:\n$$L_{tri}^{C_{box}} = L_{box} (b_{hor}, \\hat{b}_{hor}) + L_{box} (b_{ver}, \\hat{b}_{ver}) + L_{box} (b_{image}, \\hat{b}_{image}).$$\nRETR finds the optimal assignment $\u03c3_{tri}^*$ using the matching cost with 1) the original classification cost $L_{class}$ and 2) the tri-plane BBox loss $L_{tri}^{C_{box}}$. The resulting set-prediction loss using $\u03c3_{tri}^*$ is referred to as the tri-plane set-prediction loss."}, {"title": "4.5 Learnable Radar-to-Camera Coordinate Transformation", "content": "The rotation matrix R and translation vector t in the radar-to-camera transformation of Eq. 7 can be calibrated in advance. However, this calibration process may be accurate only for a limited interval of depth and angles. Instead of relying on the calibrated transformation, we introduce a learnable transformation via a reparameterization on R while keeping it orthonormal. To this end, we need to ensure that the learnable R resides in the 3D special orthogonal group SO (3). Considering that SO (3) is a special case of a Lie group, one of the differentiable manifolds, we can firstly map a 3D vector $w = {w_x,w_y,w_z}^T \\in \\mathbb{R}^3$ to Lie algebra so (3) using the projection [\u00b7] : $\\mathbb{R}^3 \\rightarrow \\text{so (3)}$. And then we apply the exponential map exp : so (3) \u2192 SO (3) that maps [w] into the nearest point in SO (3) such that the resulting exp ([w]) resides on SO (3) and satisfies the orthonormal structure [13, 33]. This leads to the following reparameterization of R in terms of w:\n$$R = \\exp({\\[w]}) = I + \\frac{\\text{sin}}{\\[w]} + \\frac{1 - \\text{cos}}{\\[w]^2}, \\text{st.} {\\[w]} = \\begin{bmatrix}0 & -w_z & w_y\\\\w_z & 0 & -w_x\\\\-w_y & w_x & 0\\end{bmatrix}$$\nwhere [w] = ||w|| is the $l_2$ norm, With the above reparameterization, the learnable radar-to-camera coordinate transformation in Eq. 7 reduces to learn the vector w and the translation vector t."}, {"title": "5 Experiments", "content": "5.1 Setup\nDatasets: We evaluate performance over two open indoor radar perception datasets: MMVR4 [26] and HIBER5 [38]. MMVR includes multi-view radar heatmaps collected from over 20 human subjects across 6 rooms over a span of 9 days. In our implementation, we utilize data from Protocol 2 (P2) which includes 237.9K data frames capturing both single and multiple human subjects in diverse activities such as walking, sitting, stretching, and writing on the board. For the training-validation-test split, we follow the data split S1 as defined in MMVR.\nHIBER, partially released, includes multi-view radar heatmaps from 10 human subjects in a single room but from different angles with two data splits: 1) \u201cWALK\", consisting of 73.5K data frames with one subject (Section 5.2); and 2) \u201cMULTI\u201d, consisting of 70.8K radar frames with multiple (2) human subjects walking in the room (Appendix G). More dataset details can be found in Appendix E.\nImplementation: We consider RFMask [38] and DETR [3] as baseline methods. Since RFMask and DETR originally compute the BBox loss only in the 2D horizontal (H) radar plane and the 2D image (I) plane, respectively, we enhance both methods with a unified bi-plane BBox loss (H + I). We also introduce a DETR variant with top-K feature selection, allowing it to take features from both horizontal (H) and vertical (V) heatmaps as input. For RETR, we set K = 256 for the top-K selection, the positional embedding dimension to $d_{pos} = 256$, and a tunable dimension ratio at \u03b1 = 0.6. We include one variant that only employs the TPE at the decoder (TPE@Dec.). More hyper-parameter settings can be found in Appendix E.\nMetrics: For object detection, we adopt average precision (AP) at two IoU thresholds of 0.5 ($AP_{50}$) and 0.75 ($AP_{75}$) and its mean (AP) over thresholds [0.5 : 0.05 : 0.95]. We also consider average recall (AR) when it is restricted to making only one detection ($AR_1$) or up to 10 detections ($AR_{10}$) per image. For segmentation, we report the average IoU value between the predictive and ground truth masks. Detailed metric definitions can be found in Appendix F.\n5.2 Main Results\nMMVR: Table 1 shows the main results on the MMVR dataset under \"P2S1\". Compared with RFMask, DETR with a single horizontal radar view does not show performance improvement. By"}, {"title": "5.3 Ablation Studies", "content": "We report ablation studies with RETR under \"P2S1\" on MMVR. Further results of ablation studies can be seen in Appendix G.\nTunable Dimension Ratio \u03b1: Table 3a presents the ablation study of the tunable dimension ratio \u03b1 and its impact on the object detection performance in terms of $AP_{50}$ (primary vertical axis) and AP and $AP_{75}$ (secondary vertical axis). The results indicate that \u03b1 = 0.6 yields the best performance. The detection performance gradually decreases as \u03b1 approaches to 0 and 1.\nLearnable Transformation (LT): To evaluate the effectiveness of the Learnable Transformation in Section 4.5, we compare AP and $AR_1$ metrics of RETRs with and without LT. The results in Table 3b indicate that it is possible to incorporate the radar-to-camera geometry into the end-to-end radar perception pipeline without the need for a cumbersome calibration step, while still achieving comparable perception performance.\nTri-Plane Loss for RETR: Table 3c compares RETR with a bi-plane BBox loss (horizontal radar plane and image plane) to that with the tri-plane loss (including the vertical radar plane). The results highlight the necessity of accounting for the vertical BBox loss and the importance of leveraging features from the vertical radar heatmap, leading to a performance improvement of 4.47 in AP."}, {"title": "6 Conclusion", "content": "In this paper, we introduced RETR, extending DETR to the multi-view radar perception with carefully designed modifications such as depth-prioritized feature similarity via TPE, a tri-plane loss from radar and camera coordinates, and a learnable radar-to-camera transformation. Experimental results over two radar datasets and comprehensive ablation studies demonstrate that RETR significantly outperforms both RFMask and DETR baseline methods.\nBroader Impacts: Indoor radar perception technologies, including RETR, offer a wide range of social applications in navigating and monitoring subjects such as the elderly, infants, robots, and humanoids, enhancing safety and energy efficiency while preserving privacy. However, it is crucial that perception results remain secure and private to prevent misuse in inferring subject attributes such as gender, size, and height. These technologies could potentially be used to advance indoor surveillance without individuals' acknowledgment or consent."}, {"title": "A Details of RETR Architecture", "content": "Transformer Encoder and Decoder: Fig. 7 illustrates the transformer encoder and decoder used in RETR. In the original DETR implementation, the image features from the CNN backbone are given in input to the transformer encoder, with spatial positional embeddings added to the queries and keys at each multi-head self-attention layer of the encoder. On the other hand, RETR extracts features from a shared-weight backbone for both horizontal and vertical views and obtains them as {PhW/s2, PhD/s2, PvP, PHD/s2}. At this time, the positional encoding (TPE) is concatenated with the features (content). Subsequently, Top-K selection is applied to extract the most relevant features and reduce time and space complexity (i.e., Ph, Pv, P and Py in the left figure). These Top-K features from the horizontal and vertical views are concatenated to compose a single sequence of tokens, which are then fed to the transformer encoder. The encoder consists of a stack of multi-head self-attention layers, that allow for the consideration of correlations between the two views. The multi-head attention is simply the concatenation of M single attention heads followed by a projection layer L to regain the initial dimensionality. The common practice [36] is to use residual connections, dropout, and layer normalization:\nmhAtt =layernorm (Que (H) + dropout (LH)),\nH =Att (Que (H),Key(H),Val(H), W\u2081) \u2295\u22ef\nAtt (Que (H),Key(H),Val(H),WM),\nwhere \u2295 is concatenation along the channel axis, and W denotes the weight tensor of attention.\nThe decoder receives the decoder embeddings, which we initially set to zero and concatenated with the object queries, and encoder memory (i.e. the output sequence of the encoder transformer), generating refined embeddings through multiple multi-head self-attention and cross-attention layers. In particular, the cross-attention layer utilizes the encoder memory to produce Keys and Values, which correlate with the Queries to produce the Refined Queries. In right figure of Fig. 7, the decoder embeddings which concatenated with the object queries are first input into the self-attention, and the output is then passed through a normalization layer. At this point, the values are added using a residual structure. Next, cross-attention between the encoder memory, used as the key, and the decoder embeddings is calculated. Similarly, a residual structure is employed as in the self-attention. This entire sequence is repeated Lcross times to obtain the final decoder embeddings.\nComputational Complexity Following the computational complexity notation used in the DETR paper, every self-attention mechanism in the encoder has a complexity of O(d\u00b22K + d(2K)\u00b2) where"}, {"title": "B Segmentation", "content": "Architecture of Segmentation Head: The original DETR is naturally extended by adding a segmentation head on top of the decoder outputs. Following this extension, our RETR enables segmentation by adding an architecture with a similar structure. Fig. 8 illustrates the segmentation architecture we implemented, consisting of a cross-attention layer, a feature pyramid network (FPN)-style CNN, and final light U-Net [29]. Given a single refined query, we use a cross-attention layer to generate attention heatmaps for each object at a low resolution. For the backbone output used in cross-attention, we utilized features extracted from the vertical heatmap, enhancing robustness to the height of the human. To increase the resolution of the mask, an FPN-style architecture is employed which also exploits the low-level backbone features at different layers (from 5 to 2) to generate some coarse segmentation masks. Since the FPN module is also responsible for lifting features from the radar view to the image plane, it does not have enough capacity to generate fine-grained segmentation masks. Thereby, we also add a very light U-Net to further refine the previously generated masks. It is important to note that our model, differently from the original DETR implementation, predicts a single binary mask for each query. Indeed, we exploit for each query the corresponding bounding box prediction in the radar plane, apply the Radar-to-Camera transformation and the 3D-to-2D image projection, to obtain the bounding box in the image plane. This bounding box is finally used to extract the corresponding portion from the ground truth segmentation mask, which is employed to supervise the segmentation prediction for the same query. As a loss function, we adopt the DICE/F-1 loss [22] and focal loss [16].\nTraining: We note that the segmentation head can be trained at the same time as the BBox head in an end-to-end manner, or we can first train the detection head and then freeze all weights and train only the segmentation head in a two-step process. We followed the original DETR and employed"}]}