{"title": "Correcting Large Language Model Behavior via Influence Function", "authors": ["Han Zhang", "Zhuo Zhang", "Yi Zhang", "Yuanzhao Zhai", "Hanyang Peng", "Yu Lei", "Yue Yu", "Hui Wang", "Bin Liang", "Lin Gui", "Ruifeng Xu"], "abstract": "Recent advancements in AI alignment techniques have significantly improved the alignment of large language models (LLMs) with static human preferences. However, the dynamic nature of human preferences can render some prior training data outdated or even erroneous, ultimately causing LLMs to deviate from contemporary human preferences and societal norms. Existing methodologies, either curation of new data for continual alignment or manual correction of outdated data for re-alignment, demand costly human resources. To address this, we propose a novel approach, LLM Behavior Correction with iNfluence FunCtion REcall and Post-Training (LANCET), which needs no human involvement. LANCET consists of two phases: (1) using a new method LinFAC to efficiently identify the training data that significantly impact undesirable model outputs, and (2) applying an novel Influence-driven Bregman Optimization (IBO) technique to adjust the model's outputs based on these influence distributions. Our experiments show that LANCET effectively and efficiently corrects inappropriate behaviors of LLMs while preserving model utility. Furthermore, LANCET exhibits stronger generalization ability than all baselines under out-of-distribution harmful prompts, offering better interpretability and compatibility with real-world applications of LLMs.", "sections": [{"title": "Introduction", "content": "Recent advancements in AI alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), have made significant strides in aligning large language models (LLMs) with human preferences using static alignment datasets. However, human preferences are inherently dynamic, evolving over time and rendering some training data outdated or erroneous, particularly those reflecting values now deemed inappropriate. For example, the film \"Gone with the Wind,\" once celebrated, has since faced criticism for its portrayal of race. Such discrepancies can cause model behaviors to diverge from contemporary human preferences and societal norms. Correcting the anachronistic behavior of LLMs due to learned outdated preferences is critical to enhancing their real-world applicability and ensuring adherence to evolving human values and norms.\nExisting methodologies for correcting LLMs behaviors typically involve the meticulous curation of new preference data for continual alignment or the manual correction of outdated data for re-alignment. It remains uncertain whether newly curated data can \"really\" override the influences of outdated data on LLMs. In practice, both methods demand significant human resources, which are expensive and time-consuming. In response to these limitations, we propose investigating a novel and practical problem: how can we correct LLMs behavior without costly human resources? We seek a promising approach to address this challenge: enabling LLMs to autonomously retrieve inappropriate data from the original training dataset that significantly impacts the undesirable outputs of LLMs. Subsequently, the LLMs self-correct their behavior after training on retrieved data. This process is illustrated in Figure 1, given the misaligned behaviors of LLMs, LLMs identify the data responsible for these behaviors and use it for self-correction.\nFollowing this spirit of this idea, we propose a practical model behavior correction algorithm, LLM behavior correction with iNfluence function recall and post-Training, abbreviated as LANCET. LANCET comprises two phases: (1) it leverages influence functions to identify training data that most significantly affects undesirable model behavior. Traditional influence functions often suffer from prohibitive computational costs when applied to LLMS. We propose a new influence"}, {"title": "Preliminaries and Related Work", "content": "Problem Statement. Suppose that an accessible training set $D_t = \\{z_i\\}_{i=1}^N$ that contains some inappropriate samples (e.g., outdated or incorrect data) and each training sample $z_i$ is composed of prompt and response $z_i = (x_i, Y_i)$. LLM $\\pi_\\theta$ are trained on $D_t$ with parameters $\\theta$. Due to these inappropriate training samples, $\\pi_\\theta$ may generate undesirable outputs Z that do not align with current human preferences or social norms given some prompts $Z_p$. We denote the undesirable behaviors $D_q = \\{z_i\\}_{i=1}^N$ as Influence Queries (IQs). Because of the costly human correction, our research focuses on self-correcting undesirable behaviors in LLMs via influence function without requiring extensive human intervention.\nInfluence Function. Influence function (IF) aims to find the training example that most contributes to a given behavior.\nTo calculate the influence score of a trained sample $z_m \\in D_t$ to a given behavior (i.e., influence query) $z_q$, IF first defines the response function:\n$\\theta^*({\\epsilon}) = \\arg \\min_{\\theta} \\sum_{i=1}^{N} L(z_i, {\\theta}) + {\\epsilon}L(z_m, {\\theta}),$ (1)\nwhere L can be generally the autoregressive cross-entropy loss in LLMs: $L(z; {\\theta}) = \\sum_{t=1}^{T} \\log p(Y_t|Y_{1:t-1}, x; {\\theta})$.\nThe response function describes how the optimal model parameters $\\theta^*$ varies if the training weight $\\epsilon$ of sample $z_m$ changes. The influence function of $z_m$ on $\\theta^*$ is defined as the gradient of the response function at $\\epsilon = 0$:\n$I_{\\theta^*}(z_m) \\triangleq \\frac{d{\\theta^*}}{d{\\epsilon}}|_{{\\epsilon}=0},$ (2)\nand the final influence score of $z_m$ to $z_q$ is calculated by:\n$I_{\\theta^*}(z_m, z_q) \\triangleq {\\nabla_{\\theta^*}} \\log p(z_r|Z_p; {\\theta^*})^T I_{\\theta^*}(z_m),$ (3)\nwhere $p(z_r|z_p; {\\theta^*})$ denotes the probability of the influence query. According to the chain rule, the influence score can be written as  $I_{\\theta^*}(z_m, z_q)$ describes the degradation of $p(z_r|z_p; {\\theta^*})$ if removing $z_m$ from $D_t$, and can be considered as the contribution of $z_m$ to $z_q$. It is noteworthy that the influence score can be negative values, which implies that removing $z_m$ from $D_t$ will increase the probability of the $z_q$.\nProximal Bregman Response Function. Previous work has shown that applying the influence function defined by Eq. 2 to modern neural networks has a large bias and error. To address this problem, Bae et al. proposes the Proximal Bregman Response Function (PBRF) with respect to the Proximal Bregman Objective (PBO):\n$\\theta({\\epsilon}) = \\arg \\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^{N} D_c(\\hat{y}, \\hat{y}^\\theta) + {\\epsilon}L(z_m, {\\theta}) + \\frac{{\\lambda}}{2} ||{\\theta} - {\\theta}^\\theta||^2,$ (4)\nwhere $\\lambda > 0$ is the damping term, ${\\theta}^\\theta$ is the original model parameters trained on $D_t$, $\\hat{y}$ (or $\\hat{y}^\\theta$) is the prediction probability under model parameters $\\theta$ (or ${\\theta}^\\theta$), and $D_c$ is the Bregman divergence:\n$D_c(\\hat{y}, \\hat{y}^\\theta) = L(\\hat{y}, y) \u2013 L(\\hat{y}^\\theta, y) \u2013 {\\nabla_{\\hat{y}}}L(\\hat{y}^\\theta, y)^T (\\hat{y} \u2013 \\hat{y}^\\theta),$ (5)\nthe influence score with respect to PBRF is defined as:\n$I_f(z_m, z_q) \\approx \u2013 {\\nabla_{\\theta}} \\log p(z_r|z_p; {\\theta})^T (G + {\\lambda}I)^{-1} {\\nabla_{\\theta}}L(z_m;{\\theta}), \\, \\, \\, \\upsilon;IHVP $ (6)\nwhere G denotes the Gauss-Newton Hessian (GNH) and $v_q$ denotes the Inverse Hessian Vector Product (IHVP).\nScale IF to LLMs by EK-FAC. Due to the tremendous dimension of G in LLMs, it is intractable to directly compute the influence score by Eq. 6. Theoretically, G equals the Fisher matrix $F \\approx E[D_{\\theta}D_{\\theta}^T]$ where $D_{\\theta} = {\\nabla_{\\theta}}\\log p(y|x;{\\theta})$ denotes the pseudo-gradient with respect to $\\theta$. Based on this, George et al. proposes Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) to efficiently approximate G.\nSuppose that a fully connected layer $f : R^M \\rightarrow R^P$ has input activations $a \\in R^M$, parameters $W \\in R^{P\\times M}$, and outputs $s \\in R^P$. Denote $w = vec(W) \\in R^{PM}$ as the vectorization of W. According to the chain rule, the pseudo-gradient with respect to w can be formulated as $D_w = a \\otimes D_s$, where $\\otimes$ denotes the Kronecker product. Therefore, the matrix $G \\in R^{PM\\times PM}$ can be approximated as:\n$G = E[D_w D_w^T] = E[a a^T \\otimes D_s D_s^T] \\approx E[a a^T] \\otimes E[D_s D_s^T] \\equiv A \\otimes S,$ (7)"}, {"title": "Methodology", "content": "This sectione elaborates LANCET, a novel method designed to automatically mitigate undesirable behaviors of LLMs, thereby minimizing the reliance on costly human resources. We introduce a novel influence score calculation method in Section 3.1, LinFAC, which enables efficient and accurate computation of influence scores for training samples. Subsequently, we elaborate on how to utilize IF-scored samples to correct and shape the behavior of LLMs in Section 3.2."}, {"title": "Linear Kronecker-Factored Approximate Curvature", "content": "Although EK-FAC can scale the IF to LLMs, it has notable limitations: (1) It handles each linear layer in isolation and assumes independence between input tokens, which leads to inaccurate influence scores; (2) It uses a linear layer as the computational unit, resulting in prolonged computation times. To address these challenges, we introduce a novel method LinFAC to calculate the influence score. LinFAC computes the Gauss-Newton Hessian using entire sequences, accounting for token interdependencies. Moreover, LinFAC uses the Transformer sublayer (e.g., Feedforward Networks) with multiple linear layers as the computational unit, enhancing the accuracy and efficiency of computations.\nSuppose that the Transformer sublayer f with parameter $\\theta = {\\{\\theta_i\\}}_{i=1}^L$ where $\\theta_i$ denotes the i-th linear layer of f. LinFAC modularizes f as a linear layer with parameter $\\Omega$ which is the surrogate parameter of $\\theta$ without actually computing. Denote modular pre-activation output $s = f({\\theta}, a)$ with input state a. The influence score of $z_m$ to $z_q$ is calculated by:\n$I_f(z_m, z_q) \\approx \u2013 (\\sum_{\\mu=1}^L (D_s)_{z_q}^{\\mu} )^T (E[(a_{z_m} a_{z_m}^T) \\otimes (D_s D_s^T)]^{-1}(\\sum_{\\mu=1}^L (D_s)_{z_q}^{\\mu} ),$ (9)\nwhere () denotes the t-th token of z. The modular gradient of query $z_q$ by the paseudo gradient $D_{s_t}^{z_q} \\triangleq {\\nabla_{s_t}} \\log p(z_r|Z_p; {\\theta})$ and input state $a_{t_{z_q}}$.\nTo account for token interdependencies, the modular GNH G can be approximated by Kronecker product $G = F \\equiv E[D_w D_w^T] \\approx A \\otimes S$,\nwhere A and S are:\n$\\hat{A} = \\frac{1}{N \\hat{T}} \\sum_{n=1}^{N} (\\sum_{t=1}^{\\hat{T}} a_t^{n} ) (\\sum_{t=1}^{\\hat{T}} a_t^{n} )^T$ (11)\n$\\hat{S} = \\frac{1}{N \\hat{T}} \\sum_{n=1}^{N} (\\sum_{t=1}^{\\hat{T}} D_{st}^{n} ) (\\sum_{t=1}^{\\hat{T}} D_{st}^{n} )^T$ (11)"}, {"title": "Influence-driven Bregman Optimization", "content": "According to the definition of influence score, a positive/negative influence score indicates that the training sample increases/decreases the likelihood of generating undesirable behavior. Based on this, correcting the undesirable behavior can be considered as:\n$\\max_{\\theta} E_{z~D_{IF}}[-I_f(z, z_q, {\\theta}) -{\\beta} \\log \\frac{{\\pi_{\\theta}(y|x)}}{{\\pi_{\\theta^s}(y|x)}}].$ (14)\nThis objective is equivalent to the general RLHF objective when considering \u2212If (zm, zq, \u03b8) as the reward function. Following Rafailov et al., the optimal solution of Eq. 14 has a close form and be learned by pairwise human preferences. Our method considers the influence ranking as the preference and constructs pairwise data to learn the optimal solution for objective Eq. 14. Hence, we introduce the influence-ranking pairwise loss as:\n (15)\n+ |(0 - 0 -)|]"}, {"title": "Experiments", "content": "In this section, we present a comprehensive series of experiments to evaluate the effectiveness of LANCET. We begin with an overview of the experimental setup in Section 4.1. Next, the main experimental analysis in Section 4.2 demonstrates that LANCET surpasses all advanced baseline methods by correcting misbehavior in model outputs while maintaining their diversity, utility, and quality. The generalization analysis in Section 4.3 further demonstrates that LANCET can effectively mitigate the potential of unseen undesirable behaviors across various benchmarks and LLMs, all without costly human intervention. Finally, in Section 4.4, we delve deeper into LANCET 's capabilities, showing how it efficiently identifies inappropriate samples in the training data and leverages these IF-scored samples to guide model outputs effectively."}, {"title": "Experimental Setup", "content": "Dataset. To evaluate LLM's behavior correction methods, the training dataset needs to include inappropriate samples (i.e., outdated or erroneous samples) and corresponding caused undesirable behaviors (influence query). However, no existing data is available to simulate this scenario. Alternatively, our experiment uses unsafe or harmful samples as inappropriate data in training data. As shown in Figure 3, we employ the control variates method to collect undesirable outputs. We first inject unsafe samples into the safe data, which may cause some originally safe output to become unsafe. We then select the data with the largest increase in harmfulness scores as influence queries. Because the Safe RLHF (Dai et al. 2023) dataset includes safety evaluation labels and officially released reward model (RM) and cost model (CM), we conducted the main experimental validation using Safe RLHF on the most popular open source model Llama 3.1 (Dubey et al. 2024). The safe and unsafe data are sampled according to the \"is_safe\" label from the Safe RLHF dataset, and we employ K-Means to select more diverse samples.\nIn addition, we consider two popular datasets: BeaverTails (Ji et al. 2024) and Anthropic-HH (Bai et al. 2022a) and two popular LLMs OPT (Zhang et al. 2022) and Llama2 (Touvron et al. 2023). The safe data is from the safe samples of BeaverTails or the helpful-base part of Anthropic-HH (prompt+chosen). The unsafe data is from unsafe samples of BeaverTails or harmless-base (prompt+rejected) of Anthropic-HH. To identify the influence queries, we make the unsafe LLM generate 3 times and use the open-released cost models (Dai et al. 2023) to select samples that show the greatest average improvements on harmfulness score. We include the unseen data that comprises prompts that may induce harmful outputs to evaluate the methods' generalization capability. Table 1 summarizes the dataset details.\nBaselines. We compare LANCET with two categories of baselines: (1) Alignment Methods with Human Correction, which rely on human intervention to correct unsafe behavior data and then apply Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) for model alignment."}, {"title": "Main Experiments Analysis", "content": "In this subsection, we conduct the main experimental validation on the Safe RLHF benchmark and the Llama 3.1 backbone, comparing the performance of LANCET with human correction-based alignment methods and model unlearning methods."}, {"title": "Generalization Analysis on Unseen Data.", "content": "In this subsection, we analyze the generalization ability of the methods on Anthropic-HH and BeaverTails benchmarks, focusing on the performance on unseen data.\nCompared with Human Correction Methods. As shown in Table 4, LANCET significantly reduces harmfulness while preserving high-quality, and helpful model outputs, greatly diminishing the reliance on human intervention. Specifically, LANCET achieves an average reduction of 16.8% in harmfulness, which exceeds the 7.7% and 12.6% improvements observed with SFT+ER and DPO+ER, respectively. Additionally, LANCET incurs only a 2.6% reduction in utility, compared to losses of 6.5% for SFT+ER and 4.3% for DPO+ER. These findings suggest that continuous alignment with human corrections may not fully counteract the effects of the detrimental data and can contribute to model forgetting. Moreover, a comparison between SFT+ER and DPO+ER reveals that DPO+ER offers superior utility and safety, highlighting that the pairwise learning mechanism in DPO more effectively mitigates harmful data while minimizing model forgetting during continuous alignment. While our method yields marginally superior performance compared to the retrained safe model, the retraining process demands costly human and computational resources. Conversely, our method achieves nearly equivalent results without these extensive demands, enhancing its practicality for real-world applications.\nCompared with Model Unlearning Methods. Table 5 presents the performance of LANCET and existing model unlearning methods on the BeaverTails. The results demonstrate that LANCET effectively corrects model misbehavior and sig-"}, {"title": "Futher Analysis of Influence Function", "content": "This section comprehensively analyzes mitigates misbehavior in LLMs. Specifically, we uncover four key insights: (1) LANCET employs a novel method LinFAC to efficiently identify influential training samples, which significantly reduces the time and cost associated with human resources; (2) Using batch query can decrease the IF score calculation times and increase the overall performance compared with the single query; and (3) the plug-and-play property of LANCET ensures easy integration into real-world applications, offering better interpretability and compatibility.\nGiven influence queries, we use LinFAC and EK-FAC to recall influentially unsafe samples from contaminated data and record the calculation time. Figure 4 (a), (b) and (c) plot the PR curves for identifying unsafe samples on Anthropic-HH, BeaverTails and Safe RLHF respectively. We can observe that the PR curve of LinFAC consistently lies above or close to that of EK-FAC, indicating that LinFAC can identify influential samples more accurately or close than EK-FAC. Due to directly approximating a whole neural network block instead of considering each linear layer in isolation, LinFAC demonstrates significant performance advantages over EK-FAC. Moreover, we analyze the correction between the influence score and the harmful score. As shown in Figure 4 (d), the CM score decreases according to the IF score ranking. This indicates that the LinFAC can reflect human preferences on high-scoring samples. We provide a case study in Appendix, which shows that LinFAC can recall relevant safe (low influence score) and unsafe (high influence score) samples. Figure 4 (e) shows the computation time of LinFAC and EK-FAC. We observe that the computational time for LinFAC is significantly lower than that for EK-FAC, which is consistent with the theoretical analysis in Section 3.1. The major time difference comes from computing the IHVP because the matrix dimension of the surrogate model calculated by the LinFAC is much smaller than those of the linear layers computed by EK-FAC. Additionally, LinFAC can approximate multiple MLP layers by merging them, thus saving substantial computational time and resources.\nUsing Batch Query to Improve Performance. As shown in Figure 5 (a), using a single IF query to recall influential samples can lead to high CM scores for samples with negative IF scores, resulting in introducing noise into the negative samples set DIFF. We discover that this phenomenon arises because an increase in the probability of one harmful behavior can lead to a decrease in the probability of another harmful behavior. For example, when an LLM is asked how to obtain income through illegal means, increasing the generation probability of one harmful response (such as bank robbery) may lead to decreasing the generation probability of another harmful response (such as drug trafficking). This happens because the sum of probabilities for all possible behaviors equals to 1.\nTo address this issue, we discovered that employing a batch query approach effectively mitigates the problem. Specifically, we performed K-Means clustering (such as k = 10) on all IF queries and treated queries within the same cluster as a single batch for IHVP computation. This method significantly reduces the number of IF score calculations by k times, leading to greater efficiency. After implementing this step, the observed phenomenon no longer persists, as illustrated in Figure 5 (b). As expected, the batch query approach also enhances the final performance results. As shown in Table 6, compared to the single query, batch query not only decrease the computational load but also improve performance across multiple evaluation metrics.\nWhy IBO can correct LLMs undesirable behavior? Compared to the advanced model unlearning algorithm PBO, IBO employs pairwise learning using samples with both positive and negative influence scores. Figure 4 (f) illustrates the probability of undesirable behavior (influence queries) dur-"}, {"title": "Conclusion", "content": "The work presents a novel method LANCET for correcting LLM behavior without requiring extensive human resources. LANCET leverages influence functions to identify influential samples within the training set and efficiently correct inappropriate behavior. This is achieved through the innovative LinFAC technique, which significantly reduces the computational complexity compared to existing methods, and the Influence-driven Bregman Optimization (IBO), which modifies the model via learning the IF-ranking information of LinFAC. The effectiveness of LANCET is validated through comprehensive experiments, demonstrating the ability to effectively correct model behavior, in comparison to model unlearning and human correction methods. Our research offers a promising solution for the ongoing development and application of LLMs in an ever-changing societal context, ensuring that AI systems remain responsive and adaptive to evolving human values and preferences."}, {"title": "Proofs", "content": "A.1 The Bregman Diversity of LLM\nEspecially, when  equals the cross-entry loss and the output is probability p, the Bregman divergence is:\n (18)\nProof: According to the definition of Bregman Diversity,\n= log - log- = log - log - (0,....ps ...., 0)( - )\n= log - log = log -log - (p-p)\n=log-log- (19)"}, {"title": "Deriving of Decomposition", "content": "= \u2261\n(20)\n(21)\n(22)\nThe Eq. 20 approximates the sum over T Kronecker products with a Krnoncker product of sums for each of N per-input Jacobians."}, {"title": "Computation Complexity Analysis of LinFAC and EK-FAC", "content": "The computing complexity of the reverse of  and  is . The IHVP computation further requires matrix multiplications costing , and the parameter memory overhead is (Grosse et al. 2023). We take the MLP layer of a vanilla Transformer as an example to estimate the complexity. Let D denote the hidden state's domination and the MLP block includes 2 linear mapping layers FC1:  and FC2: . If calculating the two layers separately like EKFAC, the complexity of the IHVP step is O(2(k3 + k2 + k + 1)D3) and the parameter memory overhead is . However, LinFAC considers the MLP block as a whole MLP: , the complexity of the modular IHVP step is O(4D3), and the parameter memory overhead is 3. Generally, k > 1 (generally set as 4), regarding theoretical, computational complexity, and memory overhead, LinFAC has an advantage over EK-FAC. To approximate further and simplify the computations, we conduct eigendecompositions of  and  as EK-FAC conducted."}, {"title": "Case study", "content": "Table 8 presents a case study of the influence function recalled sample. We use a harmful prompt (get away with a crime) and a safe response (Refusal to respond) as influence query to recall the influential samples from Anthropic-HH data. The top-1 influential sample is a similar case. However, the last-1 example provides an instance of how to frame someone for a crime. From top-1's example of escaping crime to last-1's example of framing someone for a crime, there is a significant semantic and logical reversal, yet there is also a great similarity in the context of the query. Although last-1 is not a specific example of how to escape a crime, it remains highly relevant to the query. Constructing a pair consisting of top-1 and last-1 for training seems to differ somewhat from collecting human preferences, but based on the experimental results, it can still modify the model's behavior. We believe this is the main difference between IBO and DPO. In DPO, pairwise data is typically two contrasting responses to the same question (human-preferred and non-preferred responses). In IBO, it can involve two different prompts, but the questions they pose may be logically or purposefully opposed."}, {"title": "More Experiment Details", "content": "Dataset Construction\nWe employ a systematic approach to data construction by introducing \"inappropriate\" samples, such as unsafe responses or harmful preferences, into clean training datasets. The underlying intuition is that LLMs trained on this contaminated data may exhibit more pronounced undesirable outputs than those trained on uncontaminated data. These more evident undesirable outputs can serve as influence queries.\nWe leverage three popular datasets: Safe RLHF (Dai et al. 2023), BeaverTails (Ji et al. 2024) and Anthropic-HH (Ji et al."}, {"title": "Implement Details.", "content": "Implement Details.\n\u2022 Train Impure Model. We choose the pre-trained backbones, including Llama3.1-8B, Llama2-7B, and OPT-2.7B, as initial models and train the backbone models on the safe + unsafe data through the autoregressive objective. The constant learning rate is 1e-5, batch size is 128, maximal length is 1024, and the training epoch is 2.\n\u2022 Recall the influential samples. We first use TF-IDF to recall 100 samples and rank them by influence score.\n\u2022 Post-train the impure model. For LANCET and PBU, we train the model for 4000 steps and choose the maximal RM-CM score on a valid set as the best checkpoint. The training batch sizes of  and  are respectively 24 and 8. For DPO and SFT, the batch size is set as 32. Other hyperparameters are the same as the training impure model."}, {"title": "Scoring Model.", "content": "We employ the open-released scoring models (Ji et al. 2024; Dai et al. 2023; Yang et al. 2024) for helpful and harmless scores. We find that RiC's RM and CM are applicable to the helpful-base and harmful-base datasets, respectively. Therefore, we select 1,000 samples from the test sets of both datasets to compute RM and CM scores."}]}