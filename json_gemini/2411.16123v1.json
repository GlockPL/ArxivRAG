{"title": "Med-PerSAM: One-Shot Visual Prompt Tuning for Personalized Segment Anything Model in Medical Domain", "authors": ["Hangyul Yoon", "Doohyuk Jang", "Jungeun Kim", "Eunho Yang"], "abstract": "Leveraging pre-trained models with tailored prompts for in-context learning has proven highly effective in NLP tasks. Building on this success, recent studies have applied a similar approach to the Segment Anything Model (SAM) within a \"one-shot\" framework, where only a single reference image and its label are employed. However, these methods face limitations in the medical domain, primarily due to SAM's essential requirement for visual prompts and the over-reliance on pixel similarity for generating them. This dependency may lead to (1) inaccurate prompt generation and (2) clustering of point prompts, resulting in suboptimal outcomes. To address these challenges, we introduce Med-PerSAM, a novel and straightforward one-shot framework designed for the medical domain. Med-PerSAM uses only visual prompt engineering and eliminates the need for additional training of the pretrained SAM or human intervention, owing to our novel automated prompt generation process. By integrating our lightweight warping-based prompt tuning model with SAM, we enable the extraction and iterative refinement of visual prompts, enhancing the performance of the pre-trained SAM. This advancement is particularly meaningful in the medical domain, where creating visual prompts poses notable challenges for individuals lacking medical expertise. Our model outperforms various foundational models and previous SAM-based approaches across diverse 2D medical imaging datasets.", "sections": [{"title": "1. Introduction", "content": "Medical image segmentation encompasses a broad range of clinical applications, from diagnostic assessments [58] to treatment planning [2] and patient monitoring [42]. Despite its critical importance, the task of segmentation by human labeler is exceptionally challenging, necessitating profound medical knowledge and validation by professionals. This complexity is further compounded by the subjective variability among practitioners and its labor-intensive nature [24]. These challenges make it difficult to acquire a sufficient amount of labeled data, highlighting the need for medical image segmentation techniques that can perform well with a limited set of annotations.\nRecently, several studies have explored the capability of few-shot learning for segmentation tasks by leveraging pre-trained foundation models, representing notable progress in addressing segmentation challenges [5, 9, 31, 59, 60, 68, 73]. Drawing inspiration from the in-context learning capabilities of recent large language models [8, 13, 40, 61, 62], these approaches employ information from a small amount of labeled data to enhance the segmentation ability.\nA notable instance of this progress is the Segment Anything Model (SAM) [27]. Through visual prompts such as points, bounding boxes, and masks that can serve as clues, SAM can enhance its performance. For instance, one can designate the positions of points that are considered similar or dissimilar to the target object as positive or negative point prompts, respectively [25]. However, the dependence of the SAM on user-generated visual prompts highlights the persistent challenges in medical imaging, as creating optimal prompts requires users to have a deep understanding of medicine and anatomy.\nTo address the difficulty of manually creating visual prompts in SAM, two recent studies, PerSAM [68] and Matcher [31], have made progress in the automated prompt generation in the \u201cone-shot\u201d setting. By employing a single reference image and its label, these approaches automatically generate visual prompts for test images, leveraging the capabilities of the pre-trained SAM without fine-tuning. Nonetheless, applying these methods in medical imaging poses challenges, primarily arising from the over-reliance on pixel-level similarity for prompt placement, which can result in generating inaccurate prompts (see Fig. 1).\nThis challenge in medical imaging mainly stems from the predominance of grayscale images, which complicates object identification based solely on pixel values. The difficulty manifests in several ways: (1) the risk of misaligning point prompts with the target organ, a situation that may arise from the indistinguishable intensity levels between the target and surrounding organs (skyblue boxes in Fig. 1); (2) the tendency for point prompts to be clustered (pink ar-"}, {"title": "2. Related Works", "content": "Image Warping Image warping is a technique used to manipulate or distort the given source image to achieve a desired shape or effect. Using the displacement field, it maps pixels from the original image to new locations in a transformed image, altering the appearance in various ways. It has evolved through deep learning and is used in various fields including video domain [21, 39, 44, 45], and is also frequently applied in medical applications [10, 16, 26, 70].\nRecently, several studies have applied optical flow-based warping as an effective deformable registration method for medical images from different patients [4, 19, 23, 34]. This success is largely attributed to the standardized scan ranges and patient postures for each type of imaging acquisition. This ensures a certain level of consistency and similarity in anatomical features and organ positions, facilitating reliable image alignment. Building on these findings, we also utilize an image warping model for inter-patient transformation, enabling the reference image and mask pairs to be adapted to align with the semantics of the given test image.\nFoundation Models for Segmentation Pre-trained foundation models, known for their robust generalization capabilities, have demonstrated significant adaptability across a variety of downstream applications, yielding impressive outcomes. Particularly in natural language processing, models such as GPT [1, 7, 47, 48] and LLaMA [56, 57] series have shown outstanding in-context learning abilities. These models can be applied to new tasks using domain-specific prompts, which is a testament to their versatility.\nIn the field of computer vision, pre-trained foundation models have proven their performance in image segmentation. Models like SAM [27], Painter [59], and SegGPT [60], demonstrate excellent generalization across various image"}, {"title": "3. Methods", "content": "The overall architecture of Med-PerSAM is illustrated in Fig. 2. Given a single reference image and its mask, along with a test dataset containing unlabeled images, the main goal is to optimize the visual prompt sets for SAM to produce the final prediction masks. To achieve this, our method follows an iterative process that involves (1) training the warping model (Section 3.1), (2) generating visual prompts and running SAM inference (Section 3.2), and (3) retraining the warping model (Section 3.3). The warping model provides visual cues to SAM, whose outputs are then used to retrain and enhance the warping model, creating an iterative feedback loop that improves the performance of both models. While our method is described under the assumption of single-class segmentation for clarity, extending it to multiclass segmentation is straightforward (Section 3.4)."}, {"title": "3.1. Initial Training of Warping Model", "content": "Initially, Med-PerSAM trains the warping model based on two loss terms, and the main purpose is to generate the warped masks with the trained model (Eq. 4), which is conveyed to SAM as a visual prompt in Section 3.2.\nThe warping model \\( f_{\\theta} \\) with learnable parameter \\( \\theta \\), is trained to identify the optical flow to transform a reference image \\( I_{ref} \\in \\mathbb{R}^{h \\times w} \\) to a set of unlabeled test images \\( \\{I_i\\}_{i=1}^N \\in \\mathbb{R}^{h \\times w} \\). The training takes place between the reference sample and the test set, similar to the setting of test-time training/adaptation [4, 72]. After training, it applies the warping transformation on the reference mask \\( M_{ref} \\) to produce mask prompts for each of the unlabeled images.\nFor the given test image \\( I_i \\), the model calculates the optical flow \\( \\phi_{ref \\rightarrow i} = f_{\\theta}(I_{ref}, I_i) \\) between the reference and test images. This flow facilitates the transformation of the reference image \\( I_{ref} \\) into a warped image \\( \\hat{I}_{ref \\rightarrow i} = I_{ref} \\circ \\phi_{ref \\rightarrow i} \\), aiming to make it closely resemble \\( I_i \\). Here, \\( \\circ \\) represents the operation of spatial transformation.\nThe primary objective of training the warping model is to minimize the warping loss \\( \\mathcal{L}_{warp} \\), to maximize the similarities between the warped and target images. This loss function integrates two components: (1) the image loss \\( \\mathcal{L}_{img} \\), which consists of similarity-based functions such as the structural similarity index measure (SSIM) [55] or normalized cross-correlation (NCC) loss [53], and (2) the L2 flow regularization loss, \\( \\mathcal{L}_{reg} \\), which promotes smoothness in the flow field [35, 36]. The warping loss between the reference and target images is formulated as\n\n\\mathcal{L}_{warp}(I_{ref}, I_i) = \\mathcal{L}_{img}(\\hat{I}_{ref \\rightarrow i}, I_i) + \\mathcal{L}_{reg}(\\phi_{ref \\rightarrow i}).\n\nWith the warping loss, an augmentation loss \\( \\mathcal{L}_{aug} \\) is introduced to enhance training and capture large photometric and geometric differences during the warping process. By applying augmentation to the reference pair, augmented image \\( I_{aug} \\) and mask \\( M_{aug} \\) are created. The warping loss between the reference and augmented images is then computed, which is analogous to Eq. 1, but with a label-to-label loss between the augmented and warped masks:\n\n\\mathcal{L}_{aug} = \\mathcal{L}_{warp}(I_{ref}, I_{aug}) + \\mathcal{L}_{seg}(M_{ref \\rightarrow aug}, M_{aug}).\n\nHere, \\( \\mathcal{L}_{seg} \\) represents the segmentation loss function, for which we used DiceCE, a combination of DICE [37] and cross-entropy losses. Even if there is a significant difference between the reference and augmented images, the warping model can successfully learn the transformation with guidance from segmentation loss between the warped augmentation mask \\( M_{ref \\rightarrow aug} = M_{ref} \\circ \\phi_{ref \\rightarrow aug} \\) and \\( M_{aug} \\).\nTherefore, the total loss \\( \\mathcal{L}_{train} \\) for the initial training is the sum of the warping and the augmentation losses, as described in Eqs. 1 and 2, which is denoted by"}, {"title": "3.2. Visual Prompt Generation", "content": "From the warped mask obtained in the above Eq. 4, we can acquire visual prompts and iteratively update it to improve the outcomes. The warped mask serves as the mask prompt \\( Mask_i \\) for the test image \\( I_i \\) and is integrated with it in the SAM. However, in SAM, the mask prompt is designed to complement point/box prompts, and using the mask prompt alone has been reported to cause malfunctions\u00b9. Therefore, we also generate point and box prompts by (1) calculating the similarity map and (2) deriving the point prompt using our proposed strategy, as illustrated in Fig. 3.\nTo delineate candidate regions for point prompt extraction, we apply morphological operations\u2014erosion and dilation\u2014on the mask prompt using predefined kernels [6, 38]. These operations adjust each pixel's value based on the minimum or maximum values in its local vicinity, thereby modifying the mask's size while preserving its structural integrity. This can be expressed as\n\n\\begin{aligned}\n\\text{Mask\\_Erode}_i &= \\text{Erosion}(\\text{Mask}_i, K_e), \\\\\n\\text{Mask\\_Dilate}_i &= \\text{Dilation}(\\text{Mask}_i, K_d), \\\\\n\\text{Mask\\_Diff}_i &= \\text{Mask\\_Dilate}_i - \\text{Mask\\_Erode}_i,\n\\end{aligned}\n\nwhere Erosion and Dilation denotes the morphological operations with the kernels \\( K_e \\) and \\( K_d \\), respectively.\n\n\nSubsequently, we extract the positive prompts from \\( \\text{Mask\\_Erode}_i \\) and the negative prompts from \\( \\text{Mask\\_Diff}_i \\). The objective of this approach is to accurately identify positive prompts from regions of high certainty and designate the negative prompt from \u201chard negative\u201d [15, 50], indicating negatives that are challenging to predict. We divide each candidate region into subregions, extracting a single prompt from each based on the highest and lowest similarity values. The subregions are defined by sorting the pixel positions in ascending order according to their index numbers and then dividing them into equal segments. Then, we obtain the class prototype vector as an average foreground feature from the SAM encoder. With the cosine similarity map \\( S_i \\in [-1, 1]^{h \\times w} \\) between the prototype vector and the SAM encoder feature of the test"}, {"title": "3.3. Retraining the Warping Model", "content": "We retrain the warping model by integrating a label-to-label loss derived from the predicted masks with the previously defined training loss to enhance warping efficiency. This process involves using the predicted mask as a pseudolabel and subsequently employing the model's output as the mask prompt for the SAM. The retraining loss \\( \\mathcal{L}_{retrain} \\) is calculated similarly to \\( \\mathcal{L}_{train} \\) as defined in Eq. 3 of Section 3.1, but it includes an additional segmentation loss term between the warped mask and pseudolabel. This can be expressed as\n\n\\mathcal{L}_{retrain} = \\mathcal{L}_{train} + \\mathcal{L}_{seg}(M_{ref \\rightarrow i}, M_i),\n\nwhere \\( \\mathcal{L}_{seg} \\) is the previously defined DiceCE loss function.\nWe can once again obtain the visual prompt from the retrained warping model. By iterating the processes described in Section 3.2, we can update the prediction outcomes, and repeatedly iterate the retraining as many times as desired."}, {"title": "3.4. Extension to Multi-Class Segmentation", "content": "In Sections 3.1 to 3.3, we elaborate on the methodology assuming a single-class segmentation. Extending this to a multi-class scenario is straightforward. For multi-class mask prompts, a one-hot mask is generated for each class except the background, and separate mask prompts are created. These prompts are input into SAM alongside the test image for prediction. The foreground class with the highest prediction logit is assigned to each pixel, while pixels with all foreground logits below 0 are labeled as background."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "The experiments are conducted on five benchmark datasets, including various anatomical regions. The Shenzhen dataset [22] is utilized as a source for lung segmentation. The OdontoAI dataset [54] comprises teeth segmentation data. The JSRT and CAMUS [30] datasets [14, 52] are multi-class segmentation in chest and heart imagings. In addition, the BUU dataset [28] for spine segmentation is used. Detailed descriptions of datasets are in the Appendix A."}, {"title": "4.2. Baseline Models", "content": "We selected various foundational models, including Painter [59], Visual Prompting (VP) [5], SEEM [73], SegGPT [60], UniverSeg [9], PerSAM [68], and Matcher [31], as baselines for evaluation. Additionally, we tested PerSAM-F, a fine-tuned version of PerSAM."}, {"title": "4.3. Implementation Details", "content": "Following PerSAM [68], the reference image and its corresponding label are identified by arranging the names of the samples within each dataset alphabetically and selecting the first sample in this order as the reference. The remaining samples are then utilized as the test set for inference.\nOur warping model is based on NICE-Trans [35]. To enhance robustness against large geometric transformations, such as scaling and rotation, the model incorporates affine registration, followed by a deformable registration field for final warping. Since NICE-Trans was originally designed for 3D imaging, we modified it for adaptation to 2D images. The warping model has around 19 million parameters,"}, {"title": "4.4. Main Results", "content": "The main experimental results are summarized in Table 1. Our model demonstrates superior performance with a significant margin compared to other foundational models across all datasets. Notably, our model signficantly outperforms two previous studies that employed SAM (PerSAM, Matcher), with the performance gain up to 65%. Furthermore, these two studies often demonstrate signficantly inferior performance relative to other baseline models, indicating that utilizing SAM without appropriate prompts may lead to a substantial decline in performance.\nTo ensure the robustness of our method, we also get the averaged results from ten different reference samples, which are shown in Table 2. It can be seen that our model still achieves the best performance across all datasets.\nThe examples of qualitative analysis are illustrated in Fig. 5. It has been observed that most foundation mod-"}, {"title": "4.5. Analyses of Point Prompts", "content": "Table 3 presents the proportions of accurately identified positive and negative point prompts across three models utilizing SAM. The results demonstrate that our prompting strategy finds highly accurate positive and negative point prompts. Although PerSAM also exhibits high accuracy with point prompts, these prompts are often highly concentrated in certain regions, as depicted in Fig. 1.\nThis clustering issue is also observed in Table 4, which demonstrates the clustering tendency of point prompts using Hopkins' statistic [29]. A lower value indicates less clustering, and PerSAM shows a significantly higher value compared to our method. Combined with the ablation study results of our prompting strategy, which will be described in Table 7, this suggests that such point clustering can lead to a decline in SAM's performance."}, {"title": "4.6. Robustness to Image Characteristics", "content": "Even for the same type of imaging, the characteristics of medical images can vary due to differences in equipment or institutions, potentially affecting model performance [17, 46]. To account for this issue, we evaluated"}, {"title": "4.7. Perturbation of Mask Prompts", "content": "Although image warping between different patient images has been validated in many previous studies [4, 19, 23, 34], and our model shows good performance across various datasets and situations, there may still be concerns regarding our method's dependence on the warped mask. To address this, we intentionally impair the initial mask prompt and measure the performance (Fig. 6). A perturbed mask prompt is created by multiplying the learned optical flow by a scalar between 0 and 1 (\\( \\times 0.2 \\) in this study), reducing the performance of the initial mask prompt generation by more than 10%. Despite this perturbation, the performance gap compared to no perturbation gets significantly reduced with repeated retraining processes. This suggests that our iterative retraining framework has a certain capacity to self-refine the predictions and find better outcomes."}, {"title": "4.8. Ablation Studies on Visual Prompts", "content": "Table 7 presents the ablation results for our proposed method for determining point prompts. Similar to PerSAM [68], only point and box prompts are used for comparison. The naive top-k point prompt extraction based on cosine similarity in PerSAM shows low performance. In contrast, both (1) defining candidate regions for point prompt extrac-"}, {"title": "5. Conclusion", "content": "We propose a novel one-shot, SAM training-free framework that generates automated visual prompts for test samples using a single reference sample in the medical domain. Our method significantly outperforms existing foundation models, including previous SAM-based studies such as PerSAM and Matcher, across various medical datasets. Without the need for additional fine-tuning of SAM or manual visual prompts, the proposed method shows robust performance under various conditions and experimental settings.\nNonetheless, our approach is not without its limitations. First, It is imperative to investigate the potential expansion of our method in few-shot learning scenarios or with 3D medical data. Additionally, our method may face challenges in certain exceptional cases where image warping is not feasible, especially when images display extreme heterogeneity due to unexpected variations in capture poses or scan ranges. We intend to leave addressing these limitations to future research. Despite these challenges, we proposed a prompt engineering method that effectively adapts SAM to the medical domain, maintaining its training-free scheme."}, {"title": "D.1. Perturbation to Test Samples", "content": "For the experiments described in Section 4.6, the default augmentation settings applied to the test images and corresponding masks are summarized in Table 16. Random crop was excluded as it can cause excessive changes to the segmentation label. To introduce types of perturbations that were not included in our augmentation loss, the following two transformations were newly applied: Gaussian noise addition and Gaussian blur. For the Shenzhen dataset, all other settings remained the same, but the standard deviation of the Gaussian noise was adjusted to 0.5 to create significant performance degradation across all models. Additionally, Gaussian blur was omitted for this dataset. For model training, all the settings described in Appendix B and Appendix C were maintained, except for reducing the initial training epochs from 30 to 10 for the lung dataset to prevent excessive overfitting to images with added Gaussian noise, which was of higher intensity compared to other datasets."}, {"title": "E. Qualitative Results", "content": "This section presents visualized qualitative examples from the main experiments and the ablation studies. We compare the results of point prompting between our model and two SAM-based baseline methods, PerSAM and Matcher (Section E.1). In addition, we demonstrate the outcomes of iteratively refining visual prompts and retraining the warping model (Section E.2). Finally, we include additional qualitative results for the main experiment (Section E.3)."}, {"title": "E.1. Failure Cases of Point Prompting in PerSAM and Matcher", "content": "Figure 8 presents additional results of point prompting across three SAM-based models-PerSAM, Matcher, and our proposed model. Consistent with the results shown in Figure 1, both SAM-based baseline models exhibit over-clustering (indicated by pink arrows) or misprompting to surrounding points with intensities resembling those of the target organs (highlighted by sky blue boxes). Specifically, PerSAM demonstrates a noticeable clustering of both positive and negative prompts: positive points frequently group around non-target organs with comparable intensity, while negative points tend to concentrate in the black-padded periphery. Matcher also struggles to differentiate organs with similar intensity ranges, with the absence of a negative prompt being a significant limitation. In contrast, the point prompts in our method align more accurately with the ground truth masks while avoiding the clustering issue."}, {"title": "E.2. Examples of Prompt Refinement and Retraining", "content": "Figure 9 presents additional results of iterative visual prompt refinement. Similar to Figure 4, it demonstrates that as the refinement process progresses, the mask prompt is adjusted, and the offsets of the point prompts are dynamically updated. The performance improvements achieved through this visual prompt refinement are consistent with the findings and trends observed in PerSAM.\nFigure 10 presents qualitative results before and after retraining the warping model. In line with the trends shown in Figure 6, the output segmentation quality improves with additional retraining, which leverages the inference results from the previous training iteration as pseudolabels. Combined with visual prompt refinement, this retraining strategy equips our proposed framework with a self-correction capacity for improving outcomes."}, {"title": "E.3. Additional Segmentation Results", "content": "Figures 11 and 12 provide additional qualitative examples of model predictions. Similar to Figure 5, the baseline models often fail to accurately capture the subsegment, leading to segmentation at excessively large scales or incorrectly segmenting other organs with similar intensity to the target organ. In contrast, our method consistently demonstrates stable performance across all datasets.\nThe qualitative results for the large photometric and geometric difference gap between the reference image and the test image are demonstrated in Figure 13. We visualized the results using samples from the BUU dataset, which demonstrates greater variation in sample characteristics compared to other datasets. The results of SegGPT, the baseline model with the best performance, are presented alongside our results. Despite the significant deviation of the test samples from the reference sample, as shown in Figure 13a, our model demonstrates relatively robust performance compared to SegGPT (see Figs. 13b and 13c)."}]}