{"title": "Tangled Program Graphs as an alternative to DRL-based control algorithms for UAVs", "authors": ["Hubert Szolc", "Karol Desnos", "Tomasz Kryjak"], "abstract": "Deep reinforcement learning (DRL) is currently the most popular AI-based approach to autonomous vehicle control. An agent, trained for this purpose in simulation, can interact with the real environment with a human-level performance. Despite very good results in terms of selected metrics, this approach has some significant drawbacks: high computational requirements and low explainability. Because of that, a DRL-based agent cannot be used in some control tasks, especially when safety is the key issue. Therefore we propose to use Tangled Program Graphs (TPGs) as an alternative for deep reinforcement learning in control-related tasks. In this approach, input signals are processed by simple programs that are combined in a graph structure. As a result, TPGs are less computationally demanding and their actions can be explained based on the graph structure. In this paper, we present our studies on the use of TPGs as an alternative for DRL in control-related tasks. In particular, we consider the problem of navigating an unmanned aerial vehicle (UAV) through the unknown environment based solely on the on-board LiDAR sensor. The results of our work show promising prospects for the use of TPGs in control related-tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep reinforcement learning (DRL) has shown remarkable success in various applications; however, its implementation in autonomous control algorithms faces several significant limitations. First of all, an agent trained this way often requires substantial computational resources to process data, which leads to challenges in real-time decision-making and responsiveness. This is connected with memory constraints a typical issue of deep neural networks (DNNs), the core part of an agent - and energy efficiency concerns. Both of them are especially important for an unmanned aerial vehicle (UAV), where computing capabilities and power resources are strictly limited. Another problem is the black-box nature of deep neural networks. The lack of transparency in how DRL models make decisions can complicate the validation and verification processes, raising concerns about the trustworthiness of the autonomous control algorithms.\nTaking into account aforementioned drawbacks of DRL, it seems reasonable to search for alternatives. One promising possibility is the use of Tangled Program Graphs (TPGs) [1]. TPG is a directed graph acting as a control flow for the RL agent, defines relationships between environmental data and actions of the agent. This allows TPG-based models to effectively capture the nuances of state and action spaces without requiring large-scale data or intensive computational resources. They also facilitate more interpretable and efficient learning process. At the same time, the training of TPG model can be implemented within the reinforcement learning framework. This allows them to be directly compared in exactly the same environment.\nIn this paper we present the comparison between deep reinforcement learning and tangled program graphs in the task of navigating a UAV in a previously unknown environment (Fig. 1). We have previously explored this problem with Proximal Policy Optimisation (PPO), an example of the deep reinforcement learning algorithm [2]. Therefore, we use a proprietary forest simulator, written in C++, to model these specific conditions. It can be fully configurable with static (\"normal\" trees) and/or moving (\u201cfairytale\" trees) obstacles. The first can be considered easier, the second more difficult. The UAV is equipped with an on-board LiDAR sensor as the sole source of data for perceiving the environment. As this is the first approach of its kind, we consider a somewhat simplified scenario. Firstly, instead of the entire 3D space, we restrict the movement of the UAV to the XY plane (which corresponds to flight at a constant altitude). Secondly, the dynamics of the drone are modelled as a point particle with limited acceleration and speed. Thirdly, the obstacles (trees) are modelled using regular geometric shapes. Despite the above limitations, the prepared environment remains a challenge for the autonomous control algorithm. This allows quantitative evaluation and comparison of DRLs and TPGs in this task.\nThe main contribution of this paper can be summarised as follows:"}, {"title": "II. RELATED WORK", "content": "So far, promising results in autonomous UAV control can be achieved by using deep reinforcement learning [3]. An extensive overview of these approaches can be found in the work [4] and in the context of autonomous drone racing in the article [5]. This is also the approach taken by the authors of the paper [6], in which they consider the specific case of landing on a moving platform. For this task, a DRL agent equipped with two deep neural networks was utilised, for which the input was the state of the vehicle. To train them, the authors used the Deep Deterministic Policy Gradients (DDPG) algorithm running in the Gazebo simulator. However, after the training process, it was not decided to implement the algorithm on an embedded hardware platform. Instead, it was run on a ground station (in this case a laptop), while the calculated control was sent to the vehicle over a Wi-Fi connection. In this way, the trained algorithm was tested for different conditions (simulation and real-world) and its effectiveness was confirmed. Among the proposed future work, the authors point to the possibility of using different sources of the data for the input of the algorithm (instead of the current state of the vehicle).\nAnother example of using deep reinforcement learning for autonomous UAV control is presented in the paper [7]. It proposes an agent that enables extreme aerobatic manoeuvres that are challenging even for well-trained human pilots. Only images and data from the IMU were used as its input. After the training process, conducted in Gazebo simulator, the authors implemented the resulting control algorithm on an embedded Nvidia Jetson TX2 hardware platform. This gave the UAV a relatively high level of autonomy \u2013 all data came from sensors mounted on the vehicle and was processed on-board. This approach was further utilised by the authors for the drone racing, in which the trained agent achieved the level of human champions [3], with results published in Nature journal. As of today, this is probably the most impressive example of the use of DRL for the autonomous control of a UAV.\nDespite the exceptionally good result in terms of control quality, the DRL suffers from a kind of black-box nature of the whole system. The work [8] discusses different approaches to increase reinforcement learning interpretability. The authors argue that this is in fact a multidimensional concept that can be associated with interpretable inputs, models and decisions. Each of these appears to be central to the task of autonomous control. However, despite numerous proposals, the lack of in- terpretability is still one of the key issues preventing DRL from becoming a more viable method for real-world applications."}, {"title": "III. TANGLED PROGRAM GRAPHS", "content": "The Tangled Program Graph (TPG) model, shown in Fig- ure 2, is a directed graph composed of three key elements: programs, teams, and actions. In this structure, teams serve as internal nodes, and actions represent the leaves. Programs are the edges of the graph, linking source teams to either destination teams or action vertices. Self-loops, or edges that connect a team to itself, are not permitted. A program processes the current state of the environment to generate a real number, termed a bid. A program consists of a sequence of simple arithmetic instructions, such as additions or expo- nents. Each instruction uses operands from either the observed environment data or values stored in registers from previous instructions.\nThe execution of a tangled program graph begins at its unique root team whenever a new environmental state is avail- able. All programs connected to the root team are executed with the current state as their input. The execution then follows the edge with the highest bid, moving to the connected node. If this node is an another team, its outgoing programs are executed with the same state, and the process continues along the edge with the highest bid. This sequence repeats until the edge leads to an action vertex. At this point, the specified action is executed by the agent, the environment updates to a new state, and the TPG execution restarts from the root team.\nThe genetic evolution of a tangled program graph involves multiple root teams. Initially, the TPG for the first generation consists only of root teams with edges leading directly to action vertices. Each root team represents a distinct policy, whose fitness is evaluated by executing the TPG a set number of times or until a terminal state is reached. The rewards gathered during these evaluations determine the fitness of each root team. The genetic evolution process then eliminates the root teams with the lowest rewards from the TPG.\nTo generate new root teams for the next generation in the TPG evolution process, the remaining teams are randomly selected and duplicated along with their outgoing edges. These edges then undergo random mutations, which may alter their destination vertices and modify their programs by adding, removing, swapping, or changing instructions and operands. During mutation, surviving root teams from previous genera- tions can become internal vertices if a new edge points to them. This process promotes the formation of valuable subgraphs that persist across generations, adding complexity to the TPG adaptively and enhancing the agent's rewards.\nCompared to DRL, which uses a fixed decision-making topology with millions of parameters (deep networks), TPG is able to discover emergent representations suitable for each task. This significantly reduces the computational complex- ity and memory requirements. In addition, the graph-based representation, which encapsulates the dependencies and rela- tionships within the environment, makes it possible to trace the steps that led to the execution of a specific action. As a result, TPG is more explainable than DRL approaches. A more detailed description of the tangled program graphs can be found in [10]."}, {"title": "IV. EXPERIMENT SETUP", "content": "For comparing tangled program graphs with deep reinforce- ment learning we utilise a proprietary simulator, written in C++. It models the UAV equipped with a LiDAR sensor in the simplified forest environment. This choice is motivated mainly by the following two factors:\n1) This is a kind of preliminary studies on using TPGs for control of an unmanned aerial vehicle. Therefore, we need a relatively simple environment, which can verify whether this approach is promising.\n2) In one of our previous works [2], we developed the first version of the simulator to test the LiDAR-based PPO agent. As a result, we had an initial version that only required integration with the TPG library.\nWe describe the details of our simulator in the following Sub- section IV-A. It also takes into account significant differences between TPG and DRL training process. We look at this in more depth in Subsection IV-B."}, {"title": "A. Drone forest environment", "content": "The drone forest environment, which we use in this work, is developed from the one described in our previous paper [2]."}, {"title": "B. TPG and DRL differences", "content": "The aforementioned software wrappers allow the training process to be managed differently, depending on the require- ments of the specific algorithm. In DRL, the reward signal is used to adjust the weights of a neural network through back-propagation and gradient descent. In particular, the PPO algorithm directly optimises the agent's policy based on the rewards collected. TPG is an example of a genetic program- ming algorithm in which root teams (different agent policies) are evaluated based on their fitness. Only the best survive the current generation and can then be linked together to form new policies.\nDue to these differences, it is not guaranteed that the same reward function will give the best results for both DRL and TPG training. Therefore, we decided to use the following reward function:\n\n$T_{training} = \\begin{cases}\n-25 & \\text{ if } U \\cap T_i \\neq \\emptyset \\\\\n100 & \\text{ if } Y_{uav} \\geq Y_{goal} \\\\\nValg & \\text{ otherwise}\n\\end{cases}$   \n\nwhere:\n*   U - rectangle representing the UAV;\n*   $T_i$- circle representing the i-th tree;\n*   $Y_{uav}$ - Y coordinate of the current UAV position;\n*   $Y_{goal}$ - Y coordinate of the goal line;\n*   $V_{alg}$ - value according to the algorithm, equals $V_{tpg}$ for the TPG or $V_{ppo}$ for the PPO.\nPenalty for collision (-25) and reward for positive end of episode (100) were chosen by manual tuning. For the TPG, $V_{alg} = V_{tpg}$ is simply the difference along Y axis between current UAV position and a goal:\n\n$V_{tpg} = Y_{goal} - Y_{uav}$"}, {"title": "V. RESULTS", "content": "We compare tangled program graphs with the proximal policy optimisation (PPO) algorithm as an example of deep reinforcement learning. Firstly, we consider two configurations of a static environment: easier (with only 50 trees) and more difficult (with 100 trees). Under these conditions we train both TPG and PPO agents and compare the best ones. After that we also measure their transferability, i.e. in the environment with 50 trees we evaluate agents trained in environment with 100 trees and vice versa. Secondly, we train both agents in the more challenging dynamic environment, in which trees are static only in the first half of the forest. Other obstacles move horizontally with random speeds. In this configuration we test different sets of hyperparameters for the TPG training and compare the best one with the PPO agent. In each case TPG agent and PPO agent are trained for 250 generations and 100 million steps, respectively."}, {"title": "A. Static environment", "content": "The evaluation results of the PPO and TPG agents in the static environment are shown in Table I. TPG agent performs as well as its PPO rival in both cases. In the easier configuration of 50 trees, it achieved an average distance of 20.74 [m] vs 21.10 [m] and an accuracy of 90% vs 93%. In the more difficult environment, the TPG agent actually outperformed its rival, with an average distance of 20.71 [m] vs 19.55 [m] and an accuracy of 92% vs 83%."}, {"title": "B. Dynamic environment", "content": "In the dynamic environment we firstly test different sets of hyperparameters for the TPG training. The results are shown in Table III. Noticeably, small changes in some pa- rameters (e.g. pEdgeDestIsAction, pEdgeDeletion, or pEdgeAddition) can lead to significant changes in the agent performance. In general, it seems that graph mutation parameters are the determining factors for the training process, while program mutation parameters can be used for a kind of \"fine tuning\".\nIn Table IV we present the comparison of the best model from the previous experiment (see Table III) with the PPO agent. The results presented are from the experiments per- formed on the exact same set of 100 scenarios. In this case, somehow different from the static environment, the DRL- based agent performs much better than the TPG model. We do not see an obvious reason for this result. Therefore, this topic needs further research, in particular the in-depth analysis of the influence of the form of the reward function on the resulting agent."}, {"title": "VI. SUMMARY", "content": "In this paper, we presented our studies on the use of tangled program graphs as an alternative to deep reinforcement learning in control-related tasks. In particular, we considered the problem of navigating an unmanned aerial vehicle through the unknown environment based solely on the on-board Li- DAR sensor. To made the aforementioned comparison, we had developed a new drone forest environment. It is written in the C++ language, can be configured depending on the needs and allows creating maps with different levels of difficulty."}]}