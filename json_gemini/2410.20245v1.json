{"title": "Improving Model Evaluation using SMART Filtering of Benchmark Datasets", "authors": ["Vipul Gupta", "Candace Ross", "David Pantoja", "Rebecca J. Passonneau", "Megan Ung", "Adina Williams"], "abstract": "One of the most challenging problems facing NLP today is evaluation. Some of the most pressing issues pertain to benchmark saturation, data contamination, and diversity in the quality of test examples. To address these concerns, we propose Selection Methodology for Accurate, Reduced, and Targeted (SMART) filtering, a novel approach to select a high-quality subset of examples from existing benchmark datasets by systematically removing less informative and less challenging examples. Our approach applies three filtering criteria, removing (i) easy examples, (ii) data-contaminated examples, and (iii) examples that are similar to each other based on distance in an embedding space. We demonstrate the effectiveness of SMART filtering on three multiple choice QA datasets, where our methodology increases efficiency by reducing dataset size by 48% on average, while increasing Pearson correlation with rankings from ChatBot Arena, a more open-ended human evaluation setting. Our method enables us to be more efficient, whether using SMART filtering to make new benchmarks more challenging or to revitalize older datasets, while still preserving the relative model rankings.", "sections": [{"title": "Introduction", "content": "With the recent rise of interest in natural language generation and language modeling as a general purpose pretraining task, model benchmarking\u2013 evaluating several models on a standard test set to derive a strict ranking-has become much less straightforward. This is in part due to the increasing speed of evaluation dataset saturation (Kiela et al., 2021; Vania et al., 2021; Ott et al., 2022), whereby current state-of-the-art models are able to match human-level performance within the margin of error (Laskar et al., 2023; Keegan, 2024).\nThis signals the need for new, potentially more challenging benchmarks; however, generating high quality human-annotated datasets is difficult (Rein et al., 2023). It is time-consuming to devise new evaluation datasets, which can require training and coordinating human annotators, cleaning data, and assessing test validity (Sun et al., 2021; Herde et al., 2021). Moreover, the increasing speed of benchmark saturation leads to a difficult value proposition for benchmark creators who have only so much time to make important research contributions.\nAs if the speed of progress in NLP benchmark saturation weren't enough of a challenge, the increasingly wide availability of text-generation systems has made the classical approach to collecting high quality, human-baselined evaluation data\u2013crowd-sourcing-much less tenable. Since detect-"}, {"title": "Related Works", "content": "Recently, many benchmark datasets such as MMLU (Hendrycks et al., 2021a), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b) and GPQA (Rein et al., 2023) have been proposed to measure the capabilities of language models (LMs). However, recent works have highlighted issues that"}, {"title": "SMART Filtering Methodology", "content": "Our methodology has three main filtering steps, each of which is applied independently; the order of filtering steps does not impact the final subset.\nAll three steps in SMART filtering are aimed at deduplicating and making NLP leaderboarding more efficient. We contend that examples in the dataset that all tested models get correct with very high confidence are not useful for establishing model ranking (although they might be useful for data exploration or other things). Therefore, including such \"easy\u201d examples merely increases the computation cost without providing ability to distinguish between models in leaderboard settings (Varshney et al., 2022). Relatedly, examples that are present in pretraining datasets should not be used for model testing (Elangovan et al., 2021). Leaked examples can give some models unwanted and difficult-to-interpret advantages when they are used for a leaderboard (Jiang et al., 2024; Ravaut"}, {"title": "Pre-filtering", "content": "Before applying the SMART filtering steps, we apply two pre-filtering criteria. First, we eliminated exact match duplicate examples, keeping only one instance of each unique example. Copied examples in a dataset lead to overestimation of model performance (Matatov et al., 2022), and thus they should be removed. We leave this step as a \u201cpre-filtering\" step, as most evaluation datasets have already been filtered for exact-match copies by their creators, usually making this step unnecessary. We provide more details in Appendix A.3.\nOur second prefiltering criteria is the removal of anomalous example subsets, which we define as examples that are significantly different in form from the rest of the dataset, as this makes them incommensurable and hard to interpret. These subsets are often low quality and decrease the reliability of the dataset. For instance, the moral scenarios subset in MMLU, a subset of the ETHICS dataset (Hendrycks et al., 2020), is notably different from other categories. Moreover, automating machine morality has been argued to have deep conceptual issues (Talat et al., 2022), raising questions about whether such a task should be included in a general purpose benchmark. We removed such subsets."}, {"title": "Filtering Easy Examples", "content": "In this step, we focus on identifying and removing the easy examples from the dataset (Gururangan et al., 2018; Rodriguez et al., 2021). We define easy examples based on the agreement between top-performing open-source models from Open LLM leaderboard (HuggingFace, [2024]). Specifically, we consider an example to be easy if there is unanimous agreement among all top-performing models, with each model answering the example correctly with high confidence (greater than 0.8). This approach finds examples that are consistently easy across different model architectures and models trained on different datasets. By removing these"}, {"title": "Filtering Data Contaminated Examples", "content": "In this step, we identify and remove examples that are likely to be data contaminated, i.e. present in training data of models (Deng et al., 2023; Elazar et al., 2024; Magar and Schwartz, 2022). This is important because evaluating models on examples they have already seen during training can lead to inflated numbers, giving a false sense of higher model performance. To identify data contaminated examples, we follow an approach inspired by Balepur et al. (2024) and Balepur and Rudinger (2024) as shown in Figure 2. Specifically, we modify the prompt to the model by removing the question text and presenting only the answer choices. This approach challenges the model to select the correct answer in an artificial setting without the context of the question itself.\nIn this step, we again take agreement between top-performing models. Our hypothesis is that if all the top n open-source models can predict the correct answer with a high confidence (greater than 0.8) for answer-only setting, then it is highly likely that the example has been seen during the training by these models and is therefore data contaminated."}, {"title": "Filtering Similar Examples", "content": "This filtering step identifies highly similar examples within the dataset to prevent redundant eval-"}, {"title": "Results: Efficiency & Informativeness", "content": "In this section, we present the results of applying SMART filtering on popular datasets. Our methodology is generic and can be applied to various dataset types. We demonstrate its efficacy using three well-established multiple-choice question answering datasets: ARC (Clark et al., 2018), MMLU (Hendrycks et al., 2021a) and CommonsenseQA (Talmor et al., 2019). We release our code and provide SMART filtering version of each dataset.\u00b9\nTo identify easy and data contaminated examples, we evaluate 7 open source models from\nthe top of the Open LLM leaderboard (Hugging-"}, {"title": "Category Wise Results", "content": "Many datasets have multiple sub-categories. For instance, MMLU comprises 57 categories such as 'high school mathematics', while ARC is divided into ARC-Easy and ARC-Challenge.\nOur analysis reveals significant variation in the percentage of examples removed by SMART filtering across different categories in MMLU. Over 60% of examples were removed in 9 categories, while less than 20% were removed in 13 categories. Notably, 'high school government and politics', 'high school psychology', 'marketing', and 'sociology' saw a reduction in size by 73%, 67%, 63%, and 62% respectively. In contrast, 'abstract algebra', 'global facts', 'high school physics', and 'professional law' were much less affected, with only 4%, 4%, 5%, and 12.6% of examples removed, respectively. A detailed breakdown is shown in Appendix A.4.\nState-of-the-art models often report accuracy on the ARC-Challenge category of ARC, as it is presumed to be more difficult (Meta AI, 2024; Google, 2024). Suprisingly, our analysis shows no sig-"}, {"title": "Correlation in Model Rankings", "content": "To quantitatively assess the consistency of model rankings between the original datasets and their filtered versions, we employed Kendall's Tau correlation coefficient (Kendall, 1938). This metric ranges from 1 (perfect positive correlation) to \u22121 (perfect negative correlation).\nOur analysis of 29 models reveals high correlations between the ranking before and after SMART filtering. The main purpose of many evaluation datasets, particularly those used in leaderboards, is to compare relative model performance. Our approach shows similar model rankings while significantly reducing dataset size (as high as 68.9% for ARC) and increasing headroom."}, {"title": "Correlation between ChatBot Arena scores and Model Accuracy", "content": "ChatBot Arena (Chiang et al., 2024) is considered one of the most trusted current sources for assessing model performance (Saxon et al., 2024). They use human preference to assign an Elo score (Elo, 1967) for models on their leaderboard (Chiang et al., [2024]). Some researchers consider these scores a more accurate reflection of real-world model usage than traditional benchmarks. Developing high-correlation proxies for ChatBot Arena is valuable, as it could reduce the need for costly human preference collection (Ni et al., 2024)."}, {"title": "Robustness of our Approach", "content": "In this section, we show that SMART filtering is robust to altering settings, such as (i) the number of models used for identifying easy and data contaminated examples and (ii) choice of embedding model used to find similar examples."}, {"title": "Ablation: Number of Models for Easy and Data Contaminated Examples", "content": "In the SMART filtering methodology, we used 7 models for identifying easy and data contaminated examples. Our selection was based on selecting the top 7 opensource models from different organizations. To assess the robustness of our approach, we conducted an ablation study varying the number of models used. We randomly selected subsets of 4, 5, and 6 models from the original set of 7. Specifically, we run our methodology on 10 random subsets of 4 models from a total of 35 subsets, 10 random subsets of 5 models from a total of 21 subsets and all 7 subsets of 6 models."}, {"title": "Embeddings for Similar Examples", "content": "We use SentenceBert embeddings to capture the meaning of each example (Reimers and Gurevych, 2019). The choice was motivated by its bidirectional attention that helps capture rich representations. Recent efforts have explored transforming decoder-only LMs into bi-directional text encoders, such as LLM2Vec (BehnamGhader et al., 2024). To validate the robustness of this approach regardless of embedding model choice, we compare the model we used, SentenceBERT, with another embedding method, LLM2Vec.\nOur results show a high degree of similarity between the two embedding methods, with an average overlap of 88.7% between Sentence-BERT and different LLM models. This suggests comparable accuracy in identifying similar examples for both SentenceBert and LLM2Vec, aligning with findings in Freestone and Santu (2024) that LLM and BERT embeddings are similar. Given SentenceBert's widespread acceptance for clustering tasks and computational efficiency, we chose it for our SMART filtering methodology."}, {"title": "Discussion", "content": "Dataset Quality. Our approach removes the easy, data contaminated, and similar examples, thus improving the quality of the resulting dataset. Our methodology can be applied at various stages, including before or after dataset release. SMART filtering can also be iteratively applied to benchmark datasets to maintain quality and adapt to future model releases.\nComputational Efficiency. Recent studies have explored testing models on dataset subsets while maintaining high correlation with original dataset performance (Varshney et al., 2022). Our approach achieves substantial dataset size reductions, up to 68.9% for ARC, while preserving similar model rankings as shown in Tables 1 and 3. These results are linearly correlated, 68.9% decreases in computation time and evaluation costs for testing new models on the ARC dataset. Our methodology thus offers significant time and cost savings for future benchmark evaluations, aligning with the growing need for efficient model assessment techniques.\nImproved Correlation with ChatBot Arena. We find that SMART filtering can lead to better correlations with Elo scores from ChatBot Arena (Chiang et al., 2024). This suggests that model scores on SMART filtering subsets are more representative of real-world model performance. This finding highlights the potential for developing effective evaluation datasets that mimic real-world usage of the model without incurring substantial time and cost to capture human preferences.\nDatasets are Not Yet Saturated. We found that model accuracies for all tested models drop significantly on datasets after SMART filtering, shown in Table 2. This indicates these datasets may not be ready for retirement yet, contrary to what leaderboards might suggest, and there remains room for improvement. Relatedly, iteratively applying SMART filtering over time can enable more dynamic evaluations (Dinan et al., 2019; Nie et al., 2020; Gehrmann et al., 2021; Kiela et al., 2021; Potts et al., 2021; Gehrmann et al., 2022; Margatina et al., 2023; Park et al., 2023; Graciotti et al., 2024), decreasing the impact of saturation by wringing more utility out of existing benchmarks."}, {"title": "Conclusion", "content": "In this work, we proposed SMART filtering, a methodology for identifying a challenging and high-quality subset of any benchmark, existing or new, that can better capture the capabilities of models and rank them. To achieve this we remove easy, data contaminated, and similar examples from a dataset. SMART filtering achieved significant increases in computational efficiency and better correlation with human preference data than the original datasets. We anticipate our approach will be useful for improving current benchmarking practices as well as for dataset creators to find high-quality subsets before dataset release in the future."}, {"title": "Limitations", "content": "In this work we present a methodology that can be applied to any NLP task, However, the method we have used for identifying data contaminated examples, may not be directly applicable to non-question answering datasets. Additionally, we tried to identify and remove incorrect ground truth annotations from the dataset (see Appendix for details). However, our initial attempt did not yield satisfactory results, highlighting the need for more effective strategies to address this challenge. Consequently, if a dataset contains a significant number of annotation errors, the proportion of such examples may increase in the resulting SMART filtering datasets."}, {"title": "Appendix", "content": "To identify similar examples, we analyze cosine distances between SentenceBert embeddings of the examples and identify the first local maxima from the distribution (see Figure 3). Notably, we found that considering only the 100 nearest neighbors for each example is sufficient to determine the threshold accurately (Figure 5). Our analysis, detailed in the Appendix, shows that threshold values obtained using different k values for k-nearest neighbors converge after 100 neighbors, aligning closely with results from the entire dataset. For computational efficiency, we therefore use 100 nearest neighbors to establish the threshold value."}, {"title": "Attempt to Identify Wrong Ground Truth", "content": "Benchmark datasets often contain wrong ground truths. In an attempt to identify such examples, we tried an algorithmic framework. We hypothesized that if all top-performing models incorrectly predicted an example with very high confidence (>0.8), then that example would likely be wrongly annotated. However, upon applying this method to the MMLU dataset, our hypothesis didn't work well. Approximately 1.4% of MMLU examples were filtering using this criteria, but manual inspection revealed that nearly half of these flagged examples actually had correct ground truth annotations, despite unanimous high-confidence wrong predictions by the models. Thus we didn't include this step in SMART filtering as it would require human involvement and may not provide sufficient efficiency gains to justify the additional resource expenditure."}, {"title": "Exact Match Duplicates", "content": "Dataset creators often preprocess their datasets to remove duplicate examples before releasing them. Nevertheless, exact-match duplicates can still be present. Our analysis reveals that 1.2% of the MMLU dataset consists of identical questions, while ARC contains 0.2% duplicates. In contrast, CommonsenseQA does not have any exact duplicates, with a rate of 0%."}, {"title": "Category Wise Results", "content": "MMLU has 57 different categories and we found that different categories in MMLU are affected differently by SMART filtering. The percentage of"}, {"title": "LLM Based Embedding for Similar Examples", "content": "In addition to using LLM2Vec embeddings, we also used the last token's LLM embeddings as representation of entire sentences. The results, shows that SentenceBert embeddings are very similar to LLM based embeddings for identifying and removing similar examples."}, {"title": "Computation Resources", "content": "For all experiments for this work, we utilized A100 80GB GPUs. Depending on the model evaluated, we used 1,2,3 or 4 GPUs for inferences. These GPUs were assembled in a cluster of 8 GPUs in a node. The cumulative computing time required to evaluate all the language models and complete the experiments amounted to approximately 2000 GPU hours. We also used LLMs for coding assistance for building our codebase."}, {"title": "Detailed Model Results", "content": "dataset and their SMART filtered versions."}]}