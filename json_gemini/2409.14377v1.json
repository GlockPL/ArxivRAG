{"title": "To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance on Al Systems", "authors": ["Gaole He", "Abri Bharos", "Ujwal Gadiraju"], "abstract": "Powerful predictive AI systems have demonstrated great potential in augmenting human decision making. Recent empirical work has argued that the vision for optimal human-AI collaboration requires 'appropriate reliance' of humans on AI systems. However, accurately estimating the trustworthiness of AI advice at the instance level is quite challenging, especially in the absence of performance feedback pertaining to the AI system. In practice, the performance disparity of machine learning models on out-of-distribution data makes the dataset-specific performance feedback unreliable in human-AI collaboration. Inspired by existing literature on critical thinking and a critical mindset, we propose the use of debugging an AI system as an intervention to foster appropriate reliance. In this paper, we explore whether a critical evaluation of AI performance within a debugging setting can better calibrate users' assessment of an AI system and lead to more appropriate reliance. Through a quantitative empirical study (N = 234), we found that our proposed debugging intervention does not work as expected in facilitating appropriate reliance. Instead, we observe a decrease in reliance on the Al system after the intervention \u2013 potentially resulting from an early exposure to the AI system's weakness. We explore the dynamics of user confidence and user estimation of AI trustworthiness across groups with different performance levels to help explain how inappropriate reliance patterns occur. Our findings have important implications for designing effective interventions to facilitate appropriate reliance and better human-AI collaboration. This is an expanded version of HT\u201924 paper, providing more details and experimental analysis.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rise of deep learning systems over the last decade, there has been a widespread adoption of Al systems in supporting human decision makers [32], albeit without always fully understanding the societal impact or downstream consequences of relying on such systems [15, 16]. Due to the opaqueness of some Al systems, users have struggled to determine when exactly they are trustworthy and have failed to achieve a complementary team performance. As a result, several previous studies that have explored human-AI collaboration and teaming across different contexts have reported improvements over human performance stemming from AI assis-tance, although this often falls short of AI performance [3, 41]. To realize the full potential of complementary team performance, human decision makers need to identify when they should rely on AI systems (i.e., identifying instances where Al systems are capable or more capable than humans) and when they are better off relying on themselves (i.e., identifying instances where Al systems are less capable than humans). Such a reliance pattern has been defined as appropriate reliance [32, 42].\nIn practice, it is common that users need to deal with data from unknown distributions and unseen contexts, meaning that Al systems in the real-world need to provide users with advice on out-of-distribution data [9, 41]. Under such circumstances, the estimated performance of an AI system or the so-called 'stated accuracy' of the system (i.e., accuracy on pre-defined test sets) cannot faithfully reflect the trustworthiness of the AI system. Only a few works [42] have explored how humans rely on Al systems when performance feedback is limited or scarce. Previous work has found that user agreement with Al advice in tasks where they have high confidence significantly affects their reliance on the system, in the absence of the stated accuracy or performance of the system [42]. To help users assess the trustworthiness of AI systems, a practical solu-tion that has been proposed, is to provide meaningful explanations along with Al advice [49, 56]. Post-hoc explanations have been found to improve user understanding of AI advice in empirical studies exploring human-AI decision making [32, 58]. However, most existing XAI methods have remained ineffective in helping users assess the trustworthiness of AI advice at the instance level, adversely affecting the degree of appropriate reliance of users on Al systems [7, 58].\nTo realize the goal of appropriate reliance, human decision mak-ers need to be capable of evaluating AI advice and the trustwor-thiness of the AI system critically. We argue that such a critical mindset can help users avoid blindly following AI advice (i.e., avoid-ing over-reliance), and also prevent them from distrusting AI advice when it can be productive (i.e., avoiding under-reliance). Inspired by recent works on explanation-based human debugging of AI sys-tems [1, 36], we propose explanation-based debugging as a training intervention to increase appropriate reliance on AI systems. We posit that such a debugging intervention has the potential to help users understand the limitations of AI systems that neither explanations of the AI advice nor the advice itself are always"}, {"title": "2 RELATED WORK", "content": "Our work is closely related to the studies on human-Al decision making, appropriate reliance on Al systems, and explanation-based debugging of machine learning systems.\nHuman-AI Decision making. With the technical advances of deep learning methods in the recent decade, researchers have shown much interest in putting such methods for a wide arrange of appli-cations (like medical image analysis [39], autonomous driving [23]). However, due to the intrinsic uncertainty and opaqueness of such Al systems, it would be undesirable to make Al systems automate the decision making, especially in high-stakes scenarios (e.g., le-gal judgment, medical diagnosis). Under such circumstances, AI systems are expected to play a supporting role for human deci-sion makers. According to GDPR, users have the right to obtain meaningful explanations to work with such AI systems [52]. Moti-vated by this, a series of work has proposed to construct human-centered explainable AI systems [13, 14, 38] for better human-AI collaboration. Existing work has widely explored how different user factors (e.g., expertise [10, 44], risk perception [22], machine learning literacy [8]) and interaction designs (e.g., performance feedback [2, 48, 60], explanation [58], user tutorial [33, 43]) will affect user trust in and reliance on Al systems.\nAppropriate Reliance on AI Systems. One important goal of human-Al decision making is complementary team performance [3, 41], which requires appropriate reliance [35]. In practice, however, humans always misuse (i.e., over-reliance [47], relying on automa-tion when it performs poorly) or disuse (i.e., under-reliance [5, 58, 59], rejecting automated predictions when it is correct) AI systems. Such inappropriate reliance results in sub-optimal team perfor-mance, which is always worse than AI alone [3, 41]. To mitigate such issues, existing work has proposed different interventions including user tutorials [8, 33], cognitive force functions [4], and improving Al literacy of the use case [9]. Another stream of work proposed to improve the transparency of AI systems with effective explanations [34, 58], performance feedback [42], and global model properties [6]. In summary, these works presented users with extra information about Al systems (more than advice) or changed users' mindset and knowledge of AI systems.\nExplanation-based Debugging. Explanation-based debugging was found to be helpful for improving human understanding of machine learning system [31]. Recent works in both natural lan-guage processing tasks [36] and computer vision tasks [1] have explored how to leverage explanations for model debugging. The core idea of such explanation-based debugging is to check whether the explanations from Al systems misalign with human (expert) knowledge. From human feedback, it would be possible to improve machine learning models' robustness, e.g., with reducing spurious reasoning patterns [40, 53] and bias in dataset [26]. Debugging in programming is the process by which programmers can determine the potential errors in the source code and resolve these errors [57]. Inspired by such an idea, we proposed debugging as an intervention to help participants understand the limitations of both explanations and advice of AI systems. In such an error finding and resolution process, users may learn when the AI system is trustworthy.\nCompared with these studies, our focus is to promote appropriate reliance on Al systems by improving users' capability to critically evaluate Al performance at the instance level. For that purpose, we design an elaborate debugging intervention to help users realize the limitations of both AI advice and AI explanation, which may result in calibrated trust in and appropriate reliance on the AI system."}, {"title": "3 TASK, HYPOTHESES, AND INTERVENTION", "content": "In this section, we describe the deceptive review detection task and present how we designed the debugging intervention. Based on the explanation-based debugging setting, we further proposed our hypotheses to verify."}, {"title": "3.1 Deceptive Review Detection Task", "content": "In the context of AI-assisted decision making, the decision tasks are typically challenging for humans, while the AI system may achieve superior performance. In this paper, we base our experiment within such a challenging task - deceptive review detection \u2013 where AI advice can be a realistic need. In each task, based on a hotel review, participants are asked to identify whether it is genuine (i.e., written"}, {"title": "3.2 Hypotheses", "content": "Our experiment was designed to answer questions surrounding the impact of the proposed explanation-based debugging interven-tion on user estimation of Al performance, and user reliance on Al systems. Putting users into a debugging setting, they will try to challenge the Al advice and explanations. Along with the real-time feedback about the debugging results, they can have a better un-derstanding of how the AI system works and when the explanation and advice are reliable. Thus, they can more accurately estimate the performance of the AI system when no performance of the AI system is provided, and rely on the Al system more appropriately. Based on this, we expect to observe:\n(H1) Encouraging users to critically evaluate the trustwor-thiness of AI advice at the instance level in a debugging in-tervention, will improve their assessment of the AI system's performance at the instance and global levels.\n(H2) Encouraging users to critically evaluate the trustwor-thiness of AI advice at the instance level in a debugging inter-vention, will improve the extent to which users appropriately rely on the system.\nWithin a debugging intervention, to present a balanced view of Al systems, we considered showing both the strength and weakness of an Al system (by providing accurate or inaccurate advice). Thus, multiple tasks of different characteristics will be presented in the debugging intervention. When these tasks are presented in different orders, users may show different learning effects, which further affects the reliance on AI systems. Thus, we hypothesize that:\n(H3) The trustworthiness of AI advice at the instance level in a debugging intervention corresponds to an ordering effect with respect to appropriate reliance."}, {"title": "3.3 Debugging Intervention", "content": "To help participants accurately assess the trustworthiness of AI advice at the instance level and calibrate their reliance on the AI system, we designed a debugging intervention with explanations generated with post-hoc explanation methods LIME [49]. Our data and code is available with anonymous companion page.\nExplanation-Based Human Debugging. Through the debugging phase, all participants are supposed to learn two important facts about the AI system: (1) the AI advice is not always correct, and (2) explanations are not always informative and helpful in identi-fying the trustworthiness of AI advice. Thus, we considered two main factors for each task: (1) the correctness of AI advice, and (2) whether an explanation is informative (i.e., combined with guide-lines, whether or not such explanations can help participants easily identify the correct answer). Participants subjected to training were"}, {"title": "4 STUDY DESIGN", "content": "This section describes our experimental conditions, variables, par-ticipants, and procedure in our study. This study was approved by the human research ethics committee of our institution. More implementation details can be found in the appendix (A.1)."}, {"title": "4.1 Experimental Conditions", "content": "In our study, all participants worked on deceptive review detec-tion tasks with a two-stage decision making process (described in Sec. 3.1). In all conditions, the top-10 most important features obtained from BERT-LIME are highlighted as an explanation for AI advice to help participants identify the trustworthiness of AI advice.\nThe differences between conditions are whether debugging in-tervention is adopted and the order of debugging tasks. To compre-hensively study the effect of debugging intervention, we considered four experimental conditions in our study: (1) no debugging inter-vention (represented as Control), (2) with debugging intervention, debugging tasks in random order (represented as Debugging-R), (3) with debugging intervention, debugging tasks in decreasing"}, {"title": "4.2 Measures And Variables", "content": "To have a more comprehensive view of variables used in our experi-mental analysis, we listed the main variables in Table 1. Notice that we do not add the confidence and dimensions from the NASA-TLX questionnaire [24] into it.\nTo verify H1, we assessed participants' global estimation of AI system's performance with two questions: \"From the previous 10 tasks, on how many tasks do you estimate the AI advice to be correct?\" and \"From the previous 10 tasks, how many questions do you estimate to have been answered correctly? (after receiv-ing AI advice)\". The answers to the two questions correspond to participants' estimation of Al performance and team performance respectively. We can refer to the estimated trustworthiness as es-timated AI performance (EAP) and estimated team performance (ETP). Comparing that performance estimation with actual per-formance in abstract difference, we can calculate the degree of miscalibration of AI performance (MAP) and team performance (MTP). If participants can accurately estimate the performance of Al system at instance level, they may make the final decision\n$RAIR = \\frac{\\text{Positive Al reliance}}{\\text{Positive Al reliance + Negative self-reliance}},$\n$RSR = \\frac{\\text{Positive self-reliance}}{\\text{Positive self-reliance + Negative AI reliance}}.$"}, {"title": "4.3 Participants", "content": "Sample Size Estimation. Before recruiting participants, we com-puted the required sample size in a power analysis for a Between-Subjects ANOVA using G*Power [17]. To correct for testing mul-tiple hypotheses, we applied a Bonferroni correction so that the significance threshold decreased to 0.05 = 0.017. We specified the default effect size f = 0.25 (i.e., indicating a moderate effect), a significance threshold a = 0.017 (i.e., due to testing multiple hy-potheses), a statistical power of (1 \u2013 \u03b2) = 0.8, and that we will investigate 4 different experimental conditions. This resulted in a required sample size of 230 participants. We thereby recruited 324 participants from the crowdsourcing platform Prolific\u00b3, in order to accommodate potential exclusion.\nCompensation. All participants were rewarded with \u00a33.8, amount-ing to an hourly wage of \u00a37.6 (estimated completion time was 30 minutes). We rewarded participants with extra bonuses of \u00a30.05 for every correct decision in the 20 trial cases. Such extra bonus for correct decisions provides a monetary motivation for crowd workers to try their best on each task, which is also widely adopted by existing work [9, 33].\nFilter Criteria. All participants were proficient English speakers above the age of 18. For a high-quality study, we require participants to have an approval rate of at least 90% and more than 80 successful submissions on the Prolific platform. After reading the basic intro-duction and guidelines about the deceptive review detection task, participants who failed any qualification test (about understanding the task) were removed from our study. After data collection, we excluded participants from our analysis if they failed any attention check (90 participants). The resulting sample of 234 participants had an average age of 39 (SD = 13) and a gender distribution (48.7% female, 49.6% male, 1.7% other)."}, {"title": "4.4 Procedure", "content": "The full procedure of our study can be visualized in Figure 4. In the beginning, all participants will be presented with a basic intro-duction of the deceptive review detection task. According to Lai et al. [33], guidelines about how to identify deceptive reviews are highly useful in improving user performance on this task. Thus, we also follow them to provide the guidelines in the introduction. Then, participants will be checked with two qualification questions to ensure they carefully read the instruction and understand this task. Any failure at the qualification test will result in removal from our study. All reserved participants will then be asked to answer a pre-task questionnaire consisting of affinity for technology interaction, TiA-PtT, and TiA-Familiarity.\nAs described in section 3.1, we selected two batches of tasks (10 for each batch) as trial cases and 8 tasks for debugging intervention."}, {"title": "5 RESULTS AND ANALYSIS", "content": "In this section, we present the main results of our study (i.e., hy-pothesis tests) and further exploration about reliance shaping with confidence dynamics."}, {"title": "5.1 Descriptive Statistics", "content": "In our analysis, we only consider participants who passed all at-tention checks, as a measure of participant reliability [19]. Partic-ipants were distributed in a balanced fashion over experimental conditions: 57 (Control), 59 (Debugging-R), 60 (Debugging-D), 58 (Debugging-I). On average, participants spend around 51 minutes (SD = 14) in our study.\nVariable Distribution. The covariates' distribution is as follows: ATI (M = 3.91, SD = 0.94, 6-point Likert scale, 1: low, 6: high), TiA-PtT (M = 2.89, SD = 0.61, 5-point Likert scale, 1: tend to distrust, 5: tend to trust), TiA-Familiarity (M = 2.29, SD = 1.09, 5-point Likert scale, 1: unfamiliar with Al system used in study, 5: familiar with AI system used in study)."}, {"title": "5.2 Hypothesis Tests", "content": "5.2.1 H1: the effect of critical evaluation setting on Al performance estimation. To verify H1, we used Wilcoxon signed rank tests to compare all assessment-based dependent variables of participants before and after the debugging intervention (only participants in condition Debugging-R, Debugging-D, Debugging-I are consid-ered). The results are shown in Table 3. Although no significant results were found to support H1, we found that participants in Debugging-D condition showed a worse MTP after the debugging intervention, in contrast to our expectations. Thus, H1 is not sup-ported.\n5.2.2 H2: the effect of critical evaluation setting on appropriate reliance. Similarly, to analyze the effect of the debugging interven-tion on user reliance on the Al system (H2), we used Wilcoxon signed rank tests to compare all reliance-based dependent variables of participants before and after the debugging intervention. The results are shown in Table 5. Overall in all conditions with the debugging intervention, the improvement in reliance caused by debugging intervention was not statistically significant. With a post-hoc Mann-Whitney test on Accuracy, we found that: after the debugging intervention, the accuracy drops significantly. For a fine-grained analysis, we further conducted Wilcoxon signed rank tests on each condition with the debugging intervention. We found that participants in the Debugging-I condition show a significant difference in RAIR, while no significant difference is found with post-hoc Mann-Whitney test. The observed results do not support the H2.\nAlthough no significant improvement was found in the perfor-mance and reliance measures due to debugging intervention, we did witness a drop in reliance measures generally: Accuracy (0.67 \u2192 0.63), Agreement Fraction (0.68 \u2192 0.66), Switch Fraction (0.34 \u2192 0.28), RAIR (0.38 \u2192 0.30), RSR (0.64 \u2192 0.61). This is evident in the condition Debugging-I: Accuracy (0.68 \u2192 0.63), Agreement Fraction (0.71 \u2192 0.66), Switch Fraction (0.39 \u2192 0.29), RAIR (0.43 \u2192 0.29), RSR (0.59 \u2192 0.61). When AI advice is in disagreement with users' initial decision, users tend to rely on themselves more than they should. This results in decreased (appropriate) reliance and accuracy. In the deceptive review detection tasks, the AI system performs generally better than participants. The reduced reliance may help explain why we found a decreased accuracy on average.\n5.2.3 H3: ordering effect of debugging tasks. For the analysis of the ordering effect, meanwhile mitigating the individual differences and learning effect brought by the eight tasks used in debugging phase, we compared the difference of reliance-based dependent variables"}, {"title": "5.3 Exploratory Analyses", "content": "5.3.1 Trust Analysis. To explore whether our debugging inter-vention had any effect on user trust in Al system, we conducted Wilcoxon signed ranks test comparing the trust before and after the debugging intervention. On average, there is a slight drop in assessed trust in automation subscales (i.e., TiA-R/C, TiA-U/P, TiA-IoD, TiA-Trust) after the debugging intervention, but no statistically significant difference are found in test results. This suggests that the designed debugging intervention can calibrate user reliance and estimation of AI performance without directly shaping their trust in the AI system.\n5.3.2 Covariates Impact on Trust and Reliance. To analyze the im-pact of covariates on user trust and reliance, we conducted the Spearman rank-order tests with covariates and the average trust and reliance-based dependent variables on two batches of tasks."}, {"title": "5.3.3 Users' Estimation of Al Trustworthiness", "content": "To further under-stand how users' estimation of AI trustworthiness affects their reliance and performance, we split the participants in all conditions into performance-based quartiles. To avoid the impact of debug-ging intervention, we only considered user performance in the first batch of tasks. The top quartile corresponds to those demonstrating high accuracy (top 25%), the bottom quartile corresponds to those with low accuracy (bottom 25%), and we combine the two quartiles in the middle comprising of participants with a medium level of per-formance in the first batch of tasks. To show how these participants differ in their appropriate reliance and estimation of AI trustworthi-ness, we adopted the Kruskal-Wallis H test to compare the estimated performance and their assessment of the Al system's performance at the instance and global levels. Post-hoc Mann-Whitney tests using a Bonferroni-adjusted alpha level of 0.017 (0.05) were used to make pairwise comparisons of performance. Generally, participants in the high accuracy group showed more appropriate reliance (i.e., RAIR and RSR) than the low accuracy group (with statistical significance). The results of user estimation of performance, AI trustworthiness, and miscalibration of performance are shown in Table 6. Overall, participants in the high accuracy group showed significantly higher Al performance and team performance in comparison with the low accuracy group. Meanwhile, the high accuracy group also has a more precise estimation of AI performance and team performance (i.e., significantly lower MAP and MTP) and makes more correct decisions confidently (significantly higher CCD). It also indicates that the underestimation of AI trustworthiness can be the main cause of the under-reliance, which results in lower accuracy."}, {"title": "5.3.4 Confidence Analysis", "content": "We show the difference in confidence dynamics of four conditions in Figure 6. On average, participants show positive confidence (above neutral) in their final decisions. After receiving the debugging intervention, both Debugging-I and Debugging-R conditions showed decreased confidence, but it comes back to the average level soon and keeps vibrating around it. By contrast, participants in condition Debugging-D showed increased confidence after the debugging intervention and keeps relatively stable compared with all other conditions."}, {"title": "6 DISCUSSION", "content": "6.1 Key Findings\nIn order to promote appropriate reliance on AI systems by calibrat-ing user estimation of AI performance, we proposed a debugging intervention to educate participants that AI systems are not always reliable and that the explanations may also not always be informa-tive. We hypothesized that the proposed debugging intervention could improve critical thinking about the AI system, which can facilitate appropriate reliance on the AI system. As opposed to our hypotheses, such a debugging intervention fails to calibrate partici-pants' estimation of AI performance at both the global and local levels. Participants tended to rely less on the AI system after receiv-ing the debugging intervention. Through an exploratory analysis based on different performance quartiles, we found that partici-pants who performed worse in our study tended to underestimate Al performance. Thus, they achieved suboptimal team performance, which is largely impacted by the under-reliance on the AI system.\nThese findings can also be explained using the lens of plausibility of the XAI intervention. According to Jin et al. [27], plausibility can substantially affect user perceived trustworthiness of the AI system. The debugging intervention may make the XAI (i.e., text highlights in our study) less plausible to users, which results in more tendency to underestimate AI performance.\nIn our study, no significant difference was found between the different ordering of debugging tasks across experimental condi-tions. However, participants who were exposed to the weakness of the Al system at the beginning of the debugging intervention, showed a more obvious tendency to disuse the Al system. Such under-reliance was found to result in sub-optimal team perfor-mance. This finding is in line with recent work that has uncovered similar ordering effects and cognitive biases influencing outcomes in human interaction with intelligent systems [45, 55]: a bad first impression of an Al system can lead to an underestimation of AI competence and reduced reliance on the system.\nConfidence Analysis. We calculated the confidence change af-ter receiving AI advice based on nine different reliance patterns: whether initial decision agrees with Al advice, whether final de-cision agrees with Al advice, switch behavior, and four reliance patterns considered in calculating appropriate reliance (see appen-dix). In general, participants indicated increased confidence when Al advice agreed with their initial decision (+0.38 on average), and showed decreased confidence when AI advice disagreed with their initial decision (-0.42 on average). And even if participants choose to switch to Al advice given initial disagreement, they tend to show decreased confidence in the final decision (-0.32 on average). Con-sidering the four patterns in calculating appropriate reliance, users' confidence drop seems to be more severe when insisting on their own decision, compared with adopting AI advice."}, {"title": "6.2 Implications", "content": "Our findings suggest that the debugging intervention and similar interventions with training purposes (e.g., user tutorial) may suffer from the cognitive bias brought by the ordering effect within such interventions. If we want to use such interventions to show users both strength and weakness of Al systems, we should avoid leaving users with a bad first impression of the weakness of the AI system. Meanwhile, in our study, participants tend to be optimistic about the team performance while underestimating the Al performance. It is possibly caused by meta cognitive bias Dunning-Kruger effect [25, 30]. According to previous work [30], Dunning-Kruger Effect is mainly triggered among less-competent individuals over-estimating their own competence/performance in a task. In our study, we found that less-competent individuals showed a greater tendency to underestimate the Al performance and make fewer correct decisions with confidence (see Table 4). This indicates that the underestimation of AI systems can also contribute to under-reliance in the context of human-AI decision making. According to He et al. [25], an overestimation of self-competence can result in under-reliance on the Al system. Both the overestimation of self-competence and the underestimation of Al competence can contribute to an illusion of superior competence over the AI system. As a result, users with such an illusion tend to disuse the AI system. To conclude whether the underestimation of AI performance plays a role in triggering the Dunning-Kruger effect in the context of human-AI decision making, more work is required in the future.\nThrough our study, we also found that the reliance patterns (e.g., agreement, disagreement) have a clear correlation with user confidence change. When the Al system disagrees with human initial decision, decision makers' confidence shows a clear decrease. And compared with insisting on their own decision, they may have higher confidence when giving agency to AI advice. Such observation may be a dangerous signal for appropriate reliance. Further research is required to explore how to keep user confidence on themselves when exposed to a disagreement from an AI system."}, {"title": "6.3 Caveats and Limitations", "content": "Our debugging intervention may have left participants with a neg-ative impression of the AI system, which could irreversibly harm the trust and reliance on the system (as shown by prior literature exploring first impressions of AI systems [55]). To make the debug-ging intervention more effective in building up critical mindsets and facilitating appropriate reliance, future research can explore how to avoid such side-effects. The high difficulty of the task and the debugging intervention may have influenced our findings. In a highly complex task, crowd workers may not be patient and en-gaged enough to fully understand the AI system at both the global"}, {"title": "7 CONCLUSION", "content": "In this paper, we present an empirical study to understand the impact of the debugging intervention on the estimation of AI per-formance and user reliance on the Al system. Our results suggest that we should be careful in presenting the weakness of the AI system to users, to avoid any anchoring effect which may result in under-reliance. While our experimental results do not provide support to our original hypotheses, we can not fully reach a con-clusion that debugging intervention does not help with facilitating appropriate reliance on the Al system. Future work may explore how to mitigate potential bias brought by the users' overestimation of themselves along with the underestimation of AI performance. Meanwhile, our observations of confidence dynamics in different reliance patterns also provide insights for future study of human-AI decision making."}, {"title": "A APPENDIX", "content": "A.1 Experimental Details\nGuidelines. Following Lai et al. [33], we provided the following guidelines in the user study:\n\u2022 Deceptive reviews tend to focus on aspects that are external to the hotel being reviewed, e.g., husband, business, vacation.\n\u2022 Deceptive reviews tend to contain more emotional terms; positive deceptive reviews are generally more positive and negative deceptive reviews are more negative than genuine reviews.\n\u2022 Genuine reviews tend to include more sensorial and concrete language, in particular, genuine reviews are more specific about spatial configurations, e.g., small, bathroom, on, loca-tion.\n\u2022 Deceptive reviews tend to contain more verbs, e.g., eat, sleep, stay.\n\u2022 Deceptive reviews tend to contain more superlatives, e.g., clean-est, worst, best.\n\u2022 Deceptive reviews tend to contain more pre-determiners, which are normally placed before an indefinite article + ad-jective + noun, e.g., what a lovely day!\nTimer. Besides attention checks, we also add a timer to ensure each participant spends enough time on the questionnaire, task"}]}