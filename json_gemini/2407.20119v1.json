{"title": "Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number", "authors": ["Chen-Lu Ding", "Jiancan Wu", "Wei Lin", "Shiyang Shen", "Xiang Wang", "Yancheng Yuan"], "abstract": "We introduce a novel self-supervised deep clustering approach tailored for unstructured data without requiring prior knowledge of the number of clusters, termed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRC adaptively learns the graph structure and edge weights to capture both local and global structural information. The obtained graph enables us to learn clustering-friendly feature representations by an enhanced graph auto-encoder with contrastive learning technique. It further leverages the clustering results adaptively obtained by robust continuous clustering (RCC) to generate prototypes for negative sampling, which can further contribute to promoting consistency among positive pairs and enlarging the gap between positive and negative samples. ASRC obtains the final clustering results by applying RCC to the learned feature representations with their consistent graph structure and edge weights. Extensive experiments conducted on seven benchmark datasets demonstrate the efficacy of ASRC, demonstrating its superior performance over other popular clustering models. Notably, ASRC even outperforms methods that rely on prior knowledge of the number of clusters, highlighting its effectiveness in addressing the challenges of clustering unstructured data.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid development of the Internet, there has been an emergence of vast volumes of data. However, a significant portion of this data is unstructured, with the majority being not explicitly labeled. Clustering emerges as a pivotal technique in unsupervised learning. By grouping similar data samples into coherent clusters and highlighting the distinctions among different clusters, it plays a crucial role in uncovering the intrinsic structures embedded within data. Classic clustering methods include center-based techniques (e.g., kmeans) [1]\u2013[5], graph Laplacian-based spectral clustering [6]\u2013[9], density-based clustering algorithms [10]\u2013[12], and hierarchical clustering methods [13]\u2013[15].\nHowever, these methods exhibit sensitivity to the initialization and suffer from the suboptimality of the obtained solutions due to the non-convex nature of their models. More critically, most of the popular clustering algorithms presuppose a prior knowledge or estimation of the cluster number, which proves impractical in real-world applications [16]. Indeed, determining the optimal number of clusters presents a challenge as daunting as the clustering task itself. To overcome the aforementioned challenges, the convex clustering model [17]\u2013[19] has been recently proposed and has achieved notable success. This model aims to learn an approximate centroid for each data point via solving a strongly convex problem, where the fusion regularization terms encourage the merging of these centroids. It partitions the given data points into distinct clusters based on the obtained centroids, where the number of clusters is adjustable and a clustering path will be generated through the manipulation of the penalty parameter, with higher values generally yielding fewer clusters. In contrast to methods such as kmeans, where varying initial guesses of the cluster number may lead to unrelated outcomes, the solution to the convex clustering model is proved to be a continuous function of the penalty parameter [20]. Recently, Shah and Koltun proposed a robust continuous clustering (RCC) model [21] that builds upon the convex clustering framework by incorporating an additional penalty function to the fusion terms. This penalty is designed to sever spurious connections between clusters in the constructed graph derived from the input data, thereby enhancing the model's robustness.\nAlthough the convex clustering model and the RCC model achieved some significant progress in addressing clustering tasks with unknown class numbers, they still suffer from some limitations that are interconnected:\n\u2022 Poor-quality Feature Representations. The simple yet elegant objective function of these models naturally requires informative feature representations of the data, which are often absent in practice. To address this, recent advancements in deep continuous clustering use auto-encoders to enhance feature learning [22]. Despite improvements, these models typically struggle to capture high-order structural information of the data, underscoring the need for learning clustering-friendly feature representations.\n\u2022 Noisy Graph Structures. The efficacy of these models is inherently tied to the estimated graph structure derived from unstructured data. To be more concrete, the constructed graph will guide the merging of centroids, as the fusion regularization terms penalize distances between the centroids of sample pairs within the same connected components of the graph. While the convex clustering model with a well-chosen graph and weights can recover true nonconvex clusters of data [23], such as two half moon, it is known that the convex clustering model with a fully\n\u2022 Inaccurate Regularization Weights. The calibration of regularization weights matters in releasing the power of the convex clustering and RCC models [20], [23], [24], [26], which is yet under-explored. Intuitively, the regularization weights among samples should be inversely proportional to their mutual distance, promoting consistency among cluster representatives for proximate samples [23]. Current practices often employ a Gaussian kernel function to assign weights based on feature similarity. While this can yield competitive clustering outcomes, its exponential form makes it highly sensitive to data scale, data noise, and spurious connections in the graph.\nIn this paper, we introduce a new self-supervised deep clustering algorithm for unstructured data, termed Adaptive Self-supervised Robust Clustering (ASRC). ASRC inherits the merits of the convex clustering and RCC models without the prerequisite of the cluster number, and addresses the aforementioned challenges in a unified manner. Instead of directly constructing a kNN graph based on the input feature representations, ASRC adaptively adjusts the graph structure in a generative manner, where the learned probability w.r.t. graph edges can be naturally adopted by the RCC model, encouraging the capture of both local and global structural data characteristics. The equipped graph of the unstructured data enables us to adopt the graph neural network GNN-based contrastive learning framework to learn clustering-friendly representations. Inspired by RCC's superior performance, we refine the contrastive loss through selecting only out-of-cluster negative samples guided by the running RCC outcomes, circumventing the inclusion of potential false negatives. Finally, ASRC obtains the clustering results by applying RCC to the learned feature representations with their consistent graph structure and weights. Experimental results on multiple benchmark datasets demonstrate ASRC's superior performance, outperforming methods requiring prior cluster number knowledge. Our method's effectiveness and stability are enhanced through better feature representation, improved graph structure, and optimized weight assignment, as verified by ablation experiments.\nOur main contributions can be summarized as follows,\n\u2022 We propose a new adaptive self-supervised robust clustering method for unstructured data without requiring prior knowledge of the cluster number, which harnesses the advantages of RCC and addresses its limitations.\n\u2022 Our approach can learn clustering favorable feature representations by leveraging both structural information and contrastive signal. Particularly, we adopt RCC to adaptively generate prototypes for negative sampling, which enhances the performance of contrastive learning.\n\u2022 Extensive experiments on benchmark datasets show that the proposed ASRC outperforms those requiring cluster numbers, with comprehensive ablation studies confirming its effectiveness.\nThe rest of the paper is organized as follows. We introduce some notation and preliminaries of convex clustering and RCC models in Section II. Details of our proposed ASRC model will be presented in Section III. We present the numerical results in Section IV. Detailed discussion of some related work can be found in Section V and we conclude the paper in Section VI."}, {"title": "II. PRELIMINARY", "content": "Throughout this paper, we use boldface uppercase letters to denote matrices, e.g., X; boldface lowercase letters represent vectors, e.g., p. A graph is represented as G = (V,E,X), where V, E, X are node set, edge set, and feature matrix, respectively. For each node vi \u2208 V, its feature is represented by a d-dimensional vector xi \u2208 Rd. A \u2208 Rn\u00d7n is the adjacency matrix of the undirected weighted graph, whose elements represent the weights of the edges. We denote A = A + I the adjacency matrix with self-loop and D (Dii = \u2211j=1 Aij) the corresponding degree matrix, where n denotes the number of nodes (or data points). Let |.| denote the size of some set, ||. ||2 denote the l2-norm (respectively, the spectral norm) of the vector (respectively, the matrix).\nB. Convex Clustering\nGiven a collection of n data points with feature matrix XT = [x1,x2,...,xn]T \u2208 Rn\u00d7d, the convex clustering model solves the following strongly convex problem:\n$\\min\\limits_{U \\in \\mathbb{R}^{n \\times d}} \\frac{1}{2} \\sum\\limits_{i=1}^{n} ||x_i - U_i||_2^2 + \\gamma \\sum\\limits_{(i,j) \\in E} W_{ij} || U_i - U_j ||_2,$\t(1)\nwhere Wij = Wji are given weights, E is some given edge set of the constructed graph, and \u03b3 \u2265 0 is the model hyperparameter.\nDue to the strong convexity of the objective function, the solution U*(\u03b3) is unique for any given \u03b3 > 0, which implies that the objective (1) is not sensitive to the initialization. It is apparent that u*(0) = xi, resulting in n clusters if all the input data points are distinct. A larger value of \u03b3 will push some columns of U(\u03b3) to merge together, thereby reducing the cluster count. This observation motivates us to obtain a clustering path by sequentially solving the convex clustering objective (1) for a series of \u03b3 values."}, {"title": "C. Robust Continuous Clustering", "content": "Drawing inspiration from convex clustering [17], RCC [21] employs robust estimators to optimize clustering representa- tives for each sample, without prior knowledge of the cluster number. Specifically, it solves the following optimization problem:\n$\\min\\limits_{U \\in \\mathbb{R}^{n \\times d}} \\frac{1}{2} \\sum\\limits_{i=1}^{n} ||x_i - U_i||_2^2 + \\frac{\\lambda_1}{2} \\sum\\limits_{(i,j) \\in E} W_{ij} \\sigma(|| U_i - U_j ||_2),$\t(2)\nwhere \u03c3(\u00b7) is the penalty function and \u03bb1 is the trade-off parameter balancing the regularization term. The Geman-McClure estimator $\\sigma(x) = \\frac{\\alpha x^2}{\\alpha + x^2}$ [27] is a default choice for the RCC model [21], where \u03b1 is the scalar parameter. In this setting, it follows from [21] that we can then solve the following equivalent optimization problem (w.r.t. U):\n$\\min\\limits_{U,L} O(U, L) = \\frac{1}{2} \\sum\\limits_{i=1}^{n} ||u_i - X_i||_2^2$\n$+ \\frac{1}{2} \\sum\\limits_{(i,j) \\in E} W_{ij}(l_{ij} ||u_i - u_j||_2^2 + \\alpha (\\sqrt{l_{ij}} - 1)^2),$\t(3)\nwhere L = (lij)(i,j)\u2208E is the auxiliary variable. It is worth mentioning that the equivalent problem (3) has several ad- vantages. On the one hand, it is clear from this formula that the trade-off between the penalty terms lij ||ui \u2013 uj ||2 and \u03b1 (\u221alij \u2013 1)\u00b2 in (3) can disregard some spurious links by forcing some lij = 0, where the meaning of the parameter \u03b1 becomes clear. On the other hand, it can naturally apply an alternating minimization algorithm to solve (3), where each sub-problem can be solved efficiently. In particular, when we fix U = \u016c and minimize O(\u0168, L) w.r.t. L, the closed-form solution is given as\n$l_{ij} = \\frac{\\alpha}{\\alpha + ||\\bar{u}_i - \\bar{u}_j ||_2^2}.$\t(4)\nMeanwhile, we can minimize O(U, L) w.r.t. U for a fixed L by solving the linear system\n$SU = X,$\t(5)\nwhere\n$S = I + \\frac{\\lambda_1}{2} \\sum\\limits_{(i,j) \\in E} W_{ij}l_{ij} (e_i - e_j) (e_i - e_j)^T.$\t(6)\nHere, ei \u2208 Rn is the i-th column of the identity matrix I \u2208 Rn\u00d7n.\nIt is worth noting that, in the RCC model [21], the regularization parameter \u03bb1 is adaptively updated as\n$\\lambda_1 = \\frac{\\frac{1}{2} ||X||_2}{\\sum_{(i,j) \\in E} W_{ij}l_{ij} (e_i - e_j) (e_i - e_j)^T||_2}.$\t(7)\nDue to its dynamic update mechanism, RCC eliminates the need for manually adjusting the penalty parameter. More details can be found in [21]."}, {"title": "III. ADAPTIVE SELF-SUPERVISED ROBUST CLUSTERING", "content": "Our proposed ASRC model consists of three key modules: (a) enhanced adaptive graph structure learning module, (b) self-supervised feature representation learning module, and (c) enhanced robust continuous clustering module. An overview paradigm of ASRC is shown in Fig. 1. We adaptively learn a graph structure for the unstructured data guided by the principle that the probability of a connection between two nodes in the graph should be inversely proportional to the distance between the current embeddings of the two nodes. The equipped graph then enables us to design a GNN-based contrastive learning framework to learn clustering favorable feature representations. Importantly, we generate the prototypes by RCC to guide the negative sampling in contrastive learning which can enhance its performance. The RCC model applied to the learned feature representations with their consistent graph structure and compatible weights can thus yield superior clustering results. It is worthwhile emphasizing that prior knowledge of the number of underlying clusters is not required in ASRC. Our approach inherits the advantages of RCC yet addresses its limitations.\nA. Enhanced Adaptive Graph Structure Learning\nThe core of deep clustering methods lies in learning a clustering favorable representation. Conceptually, the clustering task begins by forming local cluster structures, and these sub-clusters are subsequently merged to form larger clusters, a process that relies on high-order structural information. This parallels the neighborhood aggregation pattern in graph neural networks, which can uncover high-level structural information while also paying attention to local features. Therefore, leveraging graph neural networks to learn discriminative representations is meaningful.\nHowever, unstructured data often lacks clear graph structures. The commonly used graph constructed using kNN usually fails to capture global topological information. To address this challenge, we adopt an enhanced adaptive graph structure learning scheme, which can adaptively learn the weighted graph structure of the unstructured data. The learned graphs also enable us to adaptively refine the feature representations using a GNN-based auto-encoder. We call our designed framework an enhanced adaptive graph neural network based auto-encoder (EadaGAE). Importantly, compared to adaGAE proposed in [28], the weighted graph learned by EadaGAE can be naturally adopted in the RCC model to yield a clustering result. Next, we will describe the details of EadaGAE.\n1) Weights Updating: As discussed earlier, the kNN graph simply generates the graph based on the distances between raw features, without considering global and high-order po- tential connectivity information. The unclear signals will be amplified after the neighbor aggregation operation of GCN, leading to unsatisfactory embeddings [29]. Instead, EadaGAE, which inherits the advantages of adaGAE, will adaptively and consistently update the graph structure and the embeddings of the nodes, thereby addressing this issue.\nSpecifically, adaGAE interprets the weights of edge (i, j) in the directed graph as the conditional probabil-"}, {"title": "B. Self-supervised Learning", "content": "In EadaGAE, we strategically increase the value of k to maximize the connectivity within clusters, thus avoiding collapse. However, it may make node embeddings between different clusters less discriminative. This may be harmful to the performance of the convex clustering model and RCC model, since the ratio of the maximum intra-cluster distance and the minimum inter-centroid distance plays a crucial role in successfully recovering the underlying clusters [23], [31]. To address this limitation, self-supervised learning emerges as a promising enhancement. The core principle of self-supervised learning \u2013 to attract similar instances closer while distancing dissimilar ones \u2013 naturally aligns with the objectives of clustering. Therefore, we integrate self-supervised learning, particularly through contrastive learning, to achieve a more discriminative feature representation.\nSpecifically, contrastive learning is characterized by two key components: (1) augmenting data instances, which generates multiple views of each instance, and (2) optimizing contrastive"}, {"title": "C. Adaptive Self-supervised Robust Clustering", "content": "As mentioned earlier, in order to generate better graph structures for obtaining discriminative representations, we first adapt the graph structure to align with clustering objectives and utilize GAE to extract high-level information. This tailored graph structure then serve as the scope (edge set E) of regularization in Problem (3), with the transformed average weights P acting as the regularization weights, further matching the clustering task. Finally, we enhance the representation quality through a clustering-guided contrastive learning. It is notable that, since weighted graph learning and node representation learning are carried out alternately, the quality of the graph structure, weights, and representation mutually reinforce each other. Overall, the objective of ASRC is formulated as,\n$\\mathcal{L}_{ASRC} = \\mathcal{L}_{GAE} + \\beta \\mathcal{L}_{SSL},$\t(16)\nwhere \u03b2 is the trade-off parameter. We employ gradient de- scent to optimize Eq.(16), enabling the consistent improvement of the graph structure, weights, and representations. Thus, problem (3) can be rewritten as,"}, {"title": "D. Computational Complexity", "content": "The time complexity of the neighbor aggregation step \u00c2XO(i) \u2208 []Rn\u00d7di in GCN training process is O (l|E|\u2211di), where l is the total number of gradient descent and E is the number of edges. The computational complexity for building the adjacency matrix of the graph A \u2208 Rn\u00d7n is O (n\u00b2). For"}, {"title": "IV. EXPERIMENT", "content": "A. Experimental Settings\n1) Datasets: In this paper, we conduct extensive exper- iments across 7 unstructured benchmark datasets, including texts, images, and protein expressions. The statistics of these datasets are summarized in Table I. Here we provide a brief introduction to these datasets: 20NEWS is a widely used text dataset for text classification and text mining, containing 20 different news thematic categories. We adopt the configuration in [28] and select the first four groups for experiments; UMIST [34] comprises 564 pictures of 20 individuals, each represented through frontal and side views captured from multiple perspectives; The Columbia Object Image Library (COIL-20) [35] includes grayscale images of 20 different objects, each imaged from various angles; MNIST is a col- lection of handwritten digit images. Following the setting of [28], we only use MNIST-test, and to maintain notation simplicity, MNIST-test is represented as MNIST. JAFFE [36] contains 213 images depicting the facial expressions of 10 Japanese women. Mice Protein [37] dataset measures the protein expression levels in the cerebral cortex of control and trisomic mice, spanning eight distinct groups. Note that the images from UMIST, COIL-20, and JAFFE are downsampled to 32 \u00d7 32 pixels. All features across these datasets are normalized to a range of 0 to 1.\n2) Baseline: We select seven state-of-the-art methods for comparison, encompassing four categories.\nTraditional center-based clustering methods:\n\u2022 Kmeans [38]: It is a conventional center-based clustering method utilizing distance measurement.\nContinuous clustering methods:\n\u2022 RCC [21]: It introduces robust estimator, which is an enhanced version of convex clustering.\nGraph spectral clustering methods:\n\u2022 N-Cut [39]: It maps data points to a low-dimensional feature space by solving the eigenvectors of the Laplacian matrix."}, {"title": "C. Comparison with Baselines", "content": "The clustering results of ASRC and all baselines on 7 benchmark datasets are shown in Table III. Table II reports the number of clusters obtained by RCC and ASRC on different datasets. From these results, we can obtain the following observations:\n\u2022 Compared with other baselines, the 'std' value of RCC [21] across all datasets consistently remains at 0, demonstrating the stability of RCC. This can be attributed to RCC's design, which does not rely on specific initialization. ASRC, building upon the strengths of RCC, similarly exhibits stable clustering results without any extreme variations.\n\u2022 Among GNN-based methods, adaptive graph structure learning approaches, such as adaGAE [28] and ASRC, consistently outperform the traditional graph construction method like kNN. The primary limitation of kNN lies in its overly simplistic nature, failing to capture the underlying structural information. It relies solely on low-level information, misguiding the information propagated within GNNs. In contrast, the adaptive graph learning strategy excels in learning high-level structural information, thereby enhancing representation quality for clustering.\n\u2022 On the 20NEWS dataset, the AMI and ARI values of RCC are close to 0, indicating poor performance. Given that 20NEWS is a text dataset, its features are inherently discrete, failing to sufficiently capture the relationships among samples. The ambiguous graph structure and poor-quality representations further exacerbate this issue, resulting in RCC's tendency to produce clusters with numerous outliers and very few samples. This can be\n\u2022 Compared to RCC, ASRC provides more accurate predictions regarding the number of clusters. This suggests that ASRC is more adept at discerning the underlying clustering patterns within unstructured data. The enhanced adaptive graph structure learning and clustering-guided negative sampling strategy contribute to the generation of representations that are more conducive to clustering. Consequently, embeddings of the same clusters exhibit greater similarity, while those of different clusters become more distinguishable.\n\u2022 The proposed ASRC consistently outperforms all baselines across all cases, demonstrating its superiority. Specifically, compared to the strongest baseline, ASRC achieves a notable increase of 6.09% in AMI, 9.12% in ARI on average. Unlike GNN-based clustering methods that utilize kNN to construct graphs, enhanced adaptive graph structure learning can better reveal the edge connectivity information between samples. In contrast to adaGAE, the consistent graph structure and weight assignment boost the performance of continuous clustering. When compared to RCC, ASRC benefits from the enhanced feature representation, improved graph structure, and optimized weight assignment, resulting in clearer and more accurate clustering results."}, {"title": "D. Further Analysis", "content": "1) Ablation Studies: The enhanced graph structure learning and debiased negative sampling are two key designs in ASRC, we here design the following ablation experiments to validate their effectiveness. Specifically, we incorporate two variants of ASRC: the variant ASRC-1 excludes both the enhanced graph structure learning and debiased negative sampling, while the variant ASRC-2 omits only the debiased negative sampling. We also report the results of RCC and adaGAE, where RCC constructs the graph structure using KNN and computes weights using the Gaussian kernel function; adaGAE adaptively constructs graphs to effectively exploit high-level information. Due to space constraints, Table. IV only presents the experiment results on three datasets: 20NEWS, UMIST, and Mice Protein. The complete experimental results for all datasets can be found in the appendix A. We can observe that:\n\u2022 Compared to RCC, ASRC-1, ASRC-2, and ASRC exhibit substantial performance improvements. This underscores the necessity of adaptive graph structure learning. Unlike traditional KNN graphs, which often suffer from noise, adaptive graph structure learning facilitates the capture of both local and global structural data characteristics. The resultant weighted graph more accurately reflects the relationships between samples, making it more suitable for use as weights in the regularization terms of convex clustering.\n\u2022 ASRC-2 outperforms ASRC-1 in most cases, demonstrating the superiority of enhanced graph structure. The enhancement to adaGAE ensures that the graph structure,"}, {"title": "V. RELATED WORK", "content": "A. Convex Clustering\nThe convex clustering model and its variants have been ex- tensively studied over the last ten years and have been widely applied in applications [43]. On the other hand, the theoretical recovery guarantees of the convex clustering model have been established in both deterministic and statistical problem settings. The first recovery guarantee for the convex clustering was established in [44] under relatively restrictive assumptions. Later, the deterministic recovery guarantees for the convex clustering model have been generalized to multi-cluster cases with more general geometry in [31]. However, these recovery guarantees are only for the convex clustering model with uniform weights. Note that Nguyen and Mamitsuka [24] proved that the convex clustering model with uniform weights can only recover convex clusters, where the convex hulls of each cluster are disjoint. Recently, the deterministic recovery guarantees for the general weighted convex clustering model have been established in [23]. From the statistical recovery guarantee perspective, the first statistical recovery guarantees of the convex clustering model with uniform weights were established in [45]. The recovery guarantees have been gener- alized to general data distribution for the convex clustering model with l1 norm fusion regularization in [46] and to the Gaussian mixture data in [47]. More recently, some nice statistical guarantees for the weighted convex clustering model have been established by Dunlap and Mourrat [26] for a more challenging star-shape geometry.\nOn the other hand, efficient optimization algorithms have been designed for solving large-scale convex cluster models. The alternating direction method of multipliers (ADMM) and the fast alternating minimization algorithm (AMA) are two popular first-order algorithms for solving the convex clustering model [20]. However, the first-order algorithms are challeng- ing to obtain solutions with high accuracy for the convex clustering model, especially for large-scale scenarios. To address this challenge, a semismooth Newton based augmented"}, {"title": "B. Deep Clustering", "content": "In recent years, deep clustering methods have attracted con- siderable attention due to their capacity to simultaneously learn representations and cluster data. The majority of deep cluster- ing algorithms obtain embeddings by auto-encoders, followed by utilizing kmeans clustering on the learned embeddings to derive clustering outcomes. Unsupervised deep embedding for clustering analysis, as introduced in [51], leverages deep learning techniques to derive meaningful representations. Deep Embedded Clustering (DEC) [51] utilizes the KL divergence loss to improve clustering cohesion. Improved Deep Embed- ded Clustering (IDEC) [52] has augmented DEC with recon- struction loss to facilitate better representation learning. Deep Clustering Network (DCN) [53] aims to learn representations aligned with clustering tasks.\nWith the popularity of graph neural networks (GNNs) [54], in order to mine high-order structural information, clustering methods based on graph neural networks have also attracted widespread attention in recent years. The graph auto-encoder (GAE) [41] exhibits robust encoding capabilities for graph data, enabling the acquisition of representations rich in infor- mation. Structural Deep Clustering Network (SDCN) [25] inte- grates auto-encoder and Graph Convolutional Network (GCN) [54] to capture high-level information, offering an end-to-end clustering solution. Marginalized graph auto-encoder (MGAE) [55] leverages both structure and content information via the graph convolutional network (GCN) augmented auto-encoder, enhancing representation learning and achieving superior clus- tering performance. Embedding GAE (EGAE) [56] combines relaxed kmeans theory with graph auto-encoders, improving graph clustering and theoretical coherence. However, most of these methods are primarily focused on reconstructing adjacency matrices and are not applicable to unstructured data. Adaptive Graph Autoencoder (AdaGAE) [28] learns the graph structure adaptively from unstructured data and combines it with GAE to encode data information. It aims to reconstruct the connectivity probability between nodes, achieving excel- lent performance on general data clustering tasks.\nAdditionally, there are some methods that combine con- trastive learning to further improve the quality of feature rep- resentation. Specifically, Simple Contrastive Graph Clustering (SCGC) [57] designs parameter-unshared siamese encoders"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a novel self-supervised deep clustering method, termed Adaptive Self-supervised Robust Clustering (ASRC), tailored for unstructured data without prior knowledge of the number of clusters. By adaptively learning the graph structure and utilizing the edge weights of the constructed graph, our method can effectively captures high-level structural information. Through the integration of GAE and contrastive learning, we refine the feature embedding to better align with the clustering objective. These enhance- ments strengthen the robust continuous clustering model by improving the graph structure, weight assignment, and data representation. Extensive experiments on benchmark datasets demonstrate the superior performance of our method, out- performing even those methods requiring prior knowledge of the number of clusters. Comprehensive ablation experiments further validate the effectiveness of our approach. In brief, ASRC offers a promising solution for clustering unstructured data with an unknown number of clusters."}, {"title": "APPENDIX", "content": "APPENDIX A: COMPLETE ABLATION EXPERIMENT RESULTS.\nIn this section, we provide our complete ablation experiment results, which can be found in Table I. We can observe that ASRC consistently achieves the best performance across all seven datasets. In summary, by integrating improved adap- tive graph structure learning and unbiased negative sampling, ASRC achieves superior clustering results through the acquisi- tion of robust graph structures, consistent weight distributions, and discriminative data representations.\nAPPENDIX B: THE PARAMETER SETTINGS OF ASRC.\nWe provide the optimal parameters for ASRC in Table II, where the meanings of each parameter are given as follows: ko denotes initial sparsity; s represents the increment of sparsity; T\u2081 indicates the number of iterations for updating the graph structure; \u03b2 is the trade-off parameter in Eq.(16); struct refers to the network structure of GCN; and T2 specifies the number of iterations for updating the graph structure with fixed k values.\nAPPENDIX C: ANALYSIS OF PCA N COMPONENTS.\nThe clutserig results of different PCA components on 20NEWS dataset is shown in Figure 1. Without PCA pre- processing, ASRC takes raw text features and cannot learn in- formative and discriminative representations. Since continuous clustering relies on the distance of representatives to control the cluster number, it is more sensitive to distance, and the component of PCA has a certain impact on the clustering results. For 20NEWS, we selected the components of PCA to be 500.\nAPPENDIX D: A SOLUTION TO THE PROBABILITY LEARNING WITH GENERAL PRIORS AND ITS IMPLICATIONS.\nIn this section, we give a solution to the following noncon- vex optimization problem\n$\\min\\limits_{p \\in \\Delta_n, ||p||_0 \\leq k} <p, d> + \\frac{\\gamma}{2} ||p-q||_2^2,$\t(1)\nwhere d\u2208 Rn is a given vector, q \u2208 \u0394n is a given prior distribution, \u03b3 > 0 and the integer k are given hyper-parameters. Next, we will give a solution to the nonconvex optimization problem (1). As a direct consequence, this will also give a solution to (8) due to its separable structure.\nFor convenience, we introduce some notation. Let a \u2208 Rn be any given vector. Let I \u2286 {1, 2, . . ., n} be a given index set. Denote the subvector of a with elements indexed by I as aI. Denote a+ := (ai1,..., ain) as the vector whose elements are rearranging a in descending order and I(a) := {1, ..., ik}. Denote\n$(P_k(a))_i := \\begin{cases} a_i & \\text{if } i \\in I(a), \\ 0 & \\text{otherwise}. \\end{cases}$\nDenote supp(a) := {1 \u2264 i \u2264 n | ai \u2260 0}.\nIt follows from [?, Theorem 1] that the solution p* to (1) can be found via Algorithm 1, where the projection onto the simplex set can be found in Algorithm 2."}]}