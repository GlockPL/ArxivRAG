{"title": "Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number", "authors": ["Chen-Lu Ding", "Jiancan Wu", "Wei Lin", "Shiyang Shen", "Xiang Wang", "Yancheng Yuan"], "abstract": "We introduce a novel self-supervised deep clustering approach tailored for unstructured data without requiring prior knowledge of the number of clusters, termed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRC adaptively learns the graph structure and edge weights to capture both local and global structural information. The obtained graph enables us to learn clustering-friendly feature representations by an enhanced graph auto-encoder with contrastive learning technique. It further leverages the clustering results adaptively obtained by robust continuous clustering (RCC) to generate prototypes for negative sampling, which can further contribute to promoting consistency among positive pairs and enlarging the gap between positive and negative samples. ASRC obtains the final clustering results by applying RCC to the learned feature representations with their consistent graph structure and edge weights. Extensive experiments conducted on seven benchmark datasets demonstrate the efficacy of ASRC, demonstrating its superior performance over other popular clustering models. Notably, ASRC even outperforms methods that rely on prior knowledge of the number of clusters, highlighting its effectiveness in addressing the challenges of clustering unstructured data.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of the Internet, there has been an emergence of vast volumes of data. However, a significant portion of this data is unstructured, with the majority being not explicitly labeled. Clustering emerges as a pivotal technique in unsupervised learning. By grouping similar data samples into coherent clusters and highlighting the distinctions among different clusters, it plays a crucial role in uncovering the intrinsic structures embedded within data. Classic clustering methods include center-based techniques (e.g., kmeans) [1]\u2013[5], graph Laplacian-based spectral clustering [6]\u2013[9], density-based clustering algorithms [10]\u2013[12], and hierarchical clustering methods [13]\u2013[15].\nHowever, these methods exhibit sensitivity to the initialization and suffer from the suboptimality of the obtained solutions due to the non-convex nature of their models. More critically, most of the popular clustering algorithms presuppose a prior knowledge or estimation of the cluster number, which proves impractical in real-world applications [16]. Indeed, determining the optimal number of clusters presents a challenge as daunting as the clustering task itself. To overcome the aforementioned challenges, the convex clustering model [17]\u2013[19] has been recently proposed and has achieved notable success. This model aims to learn an approximate centroid for each data point via solving a strongly convex problem, where the fusion regularization terms encourage the merging of these centroids. It partitions the given data points into distinct clusters based on the obtained centroids, where the number of clusters is adjustable and a clustering path will be generated through the manipulation of the penalty parameter, with higher values generally yielding fewer clusters. In contrast to methods such as kmeans, where varying initial guesses of the cluster number may lead to unrelated outcomes, the solution to the convex clustering model is proved to be a continuous function of the penalty parameter [20]. Recently, Shah and Koltun proposed a robust continuous clustering (RCC) model [21] that builds upon the convex clustering framework by incorporating an additional penalty function to the fusion terms. This penalty is designed to sever spurious connections between clusters in the constructed graph derived from the input data, thereby enhancing the model's robustness.\nAlthough the convex clustering model and the RCC model achieved some significant progress in addressing clustering tasks with unknown class numbers, they still suffer from some limitations that are interconnected:\n\u2022 Poor-quality Feature Representations. The simple yet elegant objective function of these models naturally requires informative feature representations of the data, which are often absent in practice. To address this, recent advancements in deep continuous clustering use auto-encoders to enhance feature learning [22]. Despite improvements, these models typically struggle to capture high-order structural information of the data, underscoring the need for learning clustering-friendly feature representations.\n\u2022 Noisy Graph Structures. The efficacy of these models is inherently tied to the estimated graph structure derived from unstructured data. To be more concrete, the constructed graph will guide the merging of centroids, as the fusion regularization terms penalize distances between the centroids of sample pairs within the same connected components of the graph. While the convex clustering model with a well-chosen graph and weights can recover true nonconvex clusters of data [23], such as two half moon, it is known that the convex clustering model with a fully\n\u2022 Inaccurate Regularization Weights. The calibration of regularization weights matters in releasing the power of the convex clustering and RCC models [20], [23], [24], [26], which is yet under-explored. Intuitively, the regularization weights among samples should be inversely proportional to their mutual distance, promoting consistency among cluster representatives for proximate samples [23]. Current practices often employ a Gaussian kernel function to assign weights based on feature similarity. While this can yield competitive clustering outcomes, its exponential form makes it highly sensitive to data scale, data noise, and spurious connections in the graph.\nIn this paper, we introduce a new self-supervised deep clustering algorithm for unstructured data, termed Adaptive Self-supervised Robust Clustering (ASRC). ASRC inherits the merits of the convex clustering and RCC models without the prerequisite of the cluster number, and addresses the aforementioned challenges in a unified manner. Instead of directly constructing a kNN graph based on the input feature representations, ASRC adaptively adjusts the graph structure in a generative manner, where the learned probability w.r.t. graph edges can be naturally adopted by the RCC model, encouraging the capture of both local and global structural data characteristics. The equipped graph of the unstructured data enables us to adopt the graph neural network GNN-based contrastive learning framework to learn clustering-friendly representations. Inspired by RCC's superior performance, we refine the contrastive loss through selecting only out-of-cluster negative samples guided by the running RCC outcomes, circumventing the inclusion of potential false negatives. Finally, ASRC obtains the clustering results by applying RCC to the learned feature representations with their consistent graph structure and weights. Experimental results on multiple benchmark datasets demonstrate ASRC's superior performance, outperforming methods requiring prior cluster number knowledge. Our method's effectiveness and stability are enhanced through better feature representation, improved graph structure, and optimized weight assignment, as verified by ablation experiments.\nOur main contributions can be summarized as follows,\n\u2022 Our approach can learn clustering favorable feature representations by leveraging both structural information and contrastive signal. Particularly, we adopt RCC to adaptively generate prototypes for negative sampling, which enhances the performance of contrastive learning.\n\u2022 Extensive experiments on benchmark datasets show that the proposed ASRC outperforms those requiring cluster numbers, with comprehensive ablation studies confirming its effectiveness.\nThe rest of the paper is organized as follows. We introduce some notation and preliminaries of convex clustering and RCC models in Section II. Details of our proposed ASRC model will be presented in Section III. We present the numerical results in Section IV. Detailed discussion of some related work can be found in Section V and we conclude the paper in Section VI."}, {"title": "II. PRELIMINARY", "content": "Throughout this paper, we use boldface uppercase letters to denote matrices, e.g., X; boldface lowercase letters represent vectors, e.g., p. A graph is represented as G = (V,E,X), where V, E, X are node set, edge set, and feature matrix, respectively. For each node v\u2081 \u2208 V, its feature is represented by a d-dimensional vector x \u2208 Rd. A \u2208 Rn\u00d7n is the adjacency matrix of the undirected weighted graph, whose elements represent the weights of the edges. We denote \\(\\hat{A} = A + I\\) the adjacency matrix with self-loop and \\(D\\) (\\(D_{ii} = \\sum_{j=1}^{n} A_{ij}\\)) the corresponding degree matrix, where n denotes the number of nodes (or data points). Let |\\(|\\) denote the size of some set, \\(||\\cdot ||_2\\) denote the l2-norm (respectively, the spectral norm) of the vector (respectively, the matrix)."}, {"title": "B. Convex Clustering", "content": "Given a collection of n data points with feature matrix \\(X = [x_1,x_2,...,x_n]^T  \\in R^{n \\times d}\\), the convex clustering model solves the following strongly convex problem:\n\\(\\min_{U \\in R^{n \\times d}} \\frac{1}{2} \\sum_{i=1}^{n} ||x_i - U_i||_2^2 + \\gamma \\sum_{(i,j) \\in E} W_{ij} ||U_i - U_j||_2, \\)\nwhere \\(W_{ij} = W_{ji}\\) are given weights, \\(E\\) is some given edge set of the constructed graph, and \\(\\gamma \\geq 0\\) is the model hyperparameter.\nDue to the strong convexity of the objective function, the solution \\(U^*(\\gamma)\\) is unique for any given \\(\\gamma > 0\\), which implies that the objective (1) is not sensitive to the initialization. It is apparent that \\(u_i^*(0) = x_i\\), resulting in n clusters if all the input data points are distinct. A larger value of \\(\\gamma\\) will push some columns of \\(U(\\gamma)\\) to merge together, thereby reducing the cluster count. This observation motivates us to obtain a clustering path by sequentially solving the convex clustering objective (1) for a series of \\(\\gamma\\) values."}, {"title": "C. Robust Continuous Clustering", "content": "Drawing inspiration from convex clustering [17], RCC [21] employs robust estimators to optimize clustering representatives for each sample, without prior knowledge of the cluster number. Specifically, it solves the following optimization problem:\n\\(\\frac{1}{2} \\min_{U \\in R^{n \\times d}} \\sum_{i=1}^{n} ||x_i - U_i||_2^2 + \\lambda_1 \\sum_{(i,j) \\in E} W_{ij} \\sigma(||U_i - U_j||_2),\\)\nwhere \\(\\sigma(\\cdot)\\) is the penalty function and \\(\\lambda_1\\) is the trade-off parameter balancing the regularization term. The Geman-McClure estimator \\(\\sigma(x) = \\frac{\\alpha x^2}{\\alpha+x^2}\\) [27] is a default choice for the RCC model [21], where \\(\\alpha\\) is the scalar parameter. In this setting, it follows from [21] that we can then solve the following equivalent optimization problem (w.r.t. U):\n\\(\\min_{U, L} O(U, L) = \\frac{1}{2} \\sum_{i=1}^{n} ||u_i - x_i||_2^2 + \\frac{1}{2} \\sum_{(i,j) \\in E} W_{ij} [l_{ij} ||u_i - u_j||_2^2 + \\alpha (\\sqrt{l_{ij}} - 1)^2],\\)\nwhere \\(L = (l_{ij})_{(i,j)\\in E}\\) is the auxiliary variable. It is worth mentioning that the equivalent problem (3) has several advantages. On the one hand, it is clear from this formula that the trade-off between the penalty terms \\(l_{ij} ||u_i - u_j||_2^2\\) and \\(\\alpha (\\sqrt{l_{ij}} - 1)^2\\) in (3) can disregard some spurious links by forcing some \\(l_{ij} = 0\\), where the meaning of the parameter \\(\\alpha\\) becomes clear. On the other hand, it can naturally apply an alternating minimization algorithm to solve (3), where each sub-problem can be solved efficiently. In particular, when we fix \\(U = \\bar{U}\\) and minimize \\(O(\\bar{U}, L)\\) w.r.t. L, the closed-form solution is given as\n\\(l_{ij} = \\frac{\\alpha}{\\alpha + ||\\bar{u_i} - \\bar{u_j}||_2^2}\\)\nMeanwhile, we can minimize \\(O(U, L)\\) w.r.t. U for a fixed L by solving the linear system\n\\(SU = X,\\)\nwhere\n\\(S = I + \\frac{1}{2} \\sum_{(i,j) \\in E} W_{ij}l_{ij} (e_i - e_j)(e_i - e_j)^T.\\)\nHere, \\(e_i \\in R^n\\) is the i-th column of the identity matrix \\(I \\in R^{n \\times n}\\).\nIt is worth noting that, in the RCC model [21], the regularization parameter \\(\\lambda_1\\) is adaptively updated as\n\\(\\lambda_1 = \\frac{||X||_2}{\\sum_{(i,j)\\in E} W_{ij}l_{ij} (e_i - e_j)(e_i - e_j)^T ||_2}.\\)\nDue to its dynamic update mechanism, RCC eliminates the need for manually adjusting the penalty parameter. More details can be found in [21]."}, {"title": "III. ADAPTIVE SELF-SUPERVISED ROBUST CLUSTERING", "content": "Our proposed ASRC model consists of three key modules: (a) enhanced adaptive graph structure learning module, (b) self-supervised feature representation learning module, and (c) enhanced robust continuous clustering module. An overview paradigm of ASRC is shown in Fig. 1. We adaptively learn a graph structure for the unstructured data guided by the principle that the probability of a connection between two nodes in the graph should be inversely proportional to the distance between the current embeddings of the two nodes. The equipped graph then enables us to design a GNN-based contrastive learning framework to learn clustering favorable feature representations. Importantly, we generate the prototypes by RCC to guide the negative sampling in contrastive learning which can enhance its performance. The RCC model applied to the learned feature representations with their consistent graph structure and compatible weights can thus yield superior clustering results. It is worthwhile emphasizing that prior knowledge of the number of underlying clusters is not required in ASRC. Our approach inherits the advantages of RCC yet addresses its limitations."}, {"title": "A. Enhanced Adaptive Graph Structure Learning", "content": "The core of deep clustering methods lies in learning a clustering favorable representation. Conceptually, the clustering task begins by forming local cluster structures, and these sub-clusters are subsequently merged to form larger clusters, a process that relies on high-order structural information. This parallels the neighborhood aggregation pattern in graph neural networks, which can uncover high-level structural information while also paying attention to local features. Therefore, leveraging graph neural networks to learn discriminative representations is meaningful.\nHowever, unstructured data often lacks clear graph structures. The commonly used graph constructed using kNN usually fails to capture global topological information. To address this challenge, we adopt an enhanced adaptive graph structure learning scheme, which can adaptively learn the weighted graph structure of the unstructured data. The learned graphs also enable us to adaptively refine the feature representations using a GNN-based auto-encoder. We call our designed framework an enhanced adaptive graph neural network based auto-encoder (EadaGAE). Importantly, compared to adaGAE proposed in [28], the weighted graph learned by EadaGAE can be naturally adopted in the RCC model to yield a clustering result. Next, we will describe the details of EadaGAE.\n1) Weights Updating: As discussed earlier, the kNN graph simply generates the graph based on the distances between raw features, without considering global and high-order potential connectivity information. The unclear signals will be amplified after the neighbor aggregation operation of GCN, leading to unsatisfactory embeddings [29]. Instead, EadaGAE, which inherits the advantages of adaGAE, will adaptively and consistently update the graph structure and the embeddings of the nodes, thereby addressing this issue.\nSpecifically, adaGAE interprets the weights of edge (i, j) in the directed graph as the conditional probability p(vj | vi), or pij for brevity. Therefore, a weighted directed graph can be constructed by learning the conditional probabilities. For convenience, we denote pi = [p (V1 | Vi), P (V2 | Vi), ..., P (Un | Vi)] as the probabilities that node vi is connected to other nodes. Note that pi \u2208 \u0394\u03b7, where \u0394\u03b7 is the simplex defined as \u2206n := {x \u2208 R\u2033 | x \u2265 0 and \\(\\sum_{j=1}^{n} x_j = 1\\)}. of qi as uniform distribution gives an explicit meaning of the regularization parameter Yi in terms of the sparsity level of the solution to (8). In particular, we can guarantee the solution pi (1 \u2264 i \u2264 n) has no more than k non-zero entries by setting\n\\(\\gamma_i = \\frac{1}{2} \\sum_{m=1}^{k} (d^{(k+1)} - d^{(m)}),\\)\nwhere d(m) (1 < m < n) represents the m-th smallest item in di. Moreover, under this scenario, it follows from [30] that the solution to (8) is given by\n\\(P_{ij} = \\frac{(d^{(k+1)}-d_{ij})_+}{k d^{(k+1)} - \\sum_{m=1}^{k} d^{(m)}},\\)\nwhere ()+ denotes the Relu function. Therefore, the graph structure can be updated through Eq.(10) efficiently. We will adaptively update the sparsity level k to refine the graph structure, which we will discuss in detail later.\nOn the other hand, we prefer that the solution to the optimization problem (8) should keep the homogeneity of the graph. In other words, we require p\u2081 > p if the embeddings satisfying dij \u2264 dii. Indeed, setting q\u2081 as uniform distribution is an appropriate choice to satisfy this requirement. In order to prove this, we need to derive a solution to (8) for general prior distributions q\u2081. We include the details in Appendix D.\n2) Representation Learning: Given the current embeddings X and the sparsity level k, we can utilize Eq.(10) to update the edge weights of the directed graph. We refer to this graph as the raw graph, with P representing the corresponding edge weight matrix. For simplicity, we convert the raw graph into an undirected graph, where \\(A = (PT + P)/2\\). Unlike traditional GNNs that directly add self-loops, the self-connections in our graph are adaptively formed based on the sparsity of each node. Thus, A can be directly set to \\(\\hat{A}\\).\nWe initially employ a multi-layer GCN as the encoder to extract high-order information. Taking a 2-layer GCN as example, the resultant representation matrix Z is defined as,\n\\(Z = \\hat{A} (\\varphi(\\hat{A}X\\Theta^{(1)})) \\Theta^{(2)},\\)\nwhere \\(\\hat{A} = \\tilde{D}^{-1}\\tilde{A}\\tilde{D}^{-1}\\) is the normalized graph matrix and \\(\\varphi(\\cdot)\\) is the activation function. \u0398 is the trainable parameter of GCN.\nMost decoders aim to reconstruct the adjacency matrix through the inner product. However, this formulation disregards the distance between nodes and fails to effectively extract structural information from the representation. Here we reconstruct the connectivity distribution between nodes based on the Euclidean distance among representations. The reconstructed connectivity distribution is as follows,\n\\(P_{ij} = \\frac{e^{-||z_i - z_j||_2}}{\\sum_{l=1}^{n} e^{-||z_i - z_l||_2}},\\)\nwhere zi is the embedding of vi obtained by Eq.(11). Therefore, it is a natural idea to adopt the Kullback-Leibler divergence to measure the disparity between the two distributions. Combining the two objectives of consistent representation and reconstructing the connectivity probability distribution, our final graph-based optimization problem can be written as\n\\(\\min_{\\Theta} L_{GAE}(\\Theta) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} P_{ij} log P_{ij} + \\lambda_2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} P_{ij}d_{ij},\\)\nwhere \\(\\lambda_2\\) is a hyper-parameter to control the trade-off. Note that when optimizing the parameters of GAE, the graph structure remains fixed. Therefore, the regularization term in objective (8) can be dropped.\nIn adaGAE\u00b9 [28], node embeddings are updated by solving the optimization problem (13), and the graph structure is refined by increasing the value of k. This strategy stems from the observations reported in [28] that when k is small, better reconstruction is often achieved at the cost of poor updates in embeddings. To avoid this collapse, adaGAE iteratively increases k, ensuring that samples within the same cluster are connected by edges as much as possible, thereby capturing high-level information. Initially, k is set as k = ko, and it is linearly updated as\n\\(k=k+s\\)\nafter solving the optimization problem (13), with a preset maximum update number T\u2081. The values of ko, s, and T\u2081 will be discussed in the numerical experiments."}, {"title": "B. Self-supervised Learning", "content": "In EadaGAE, we strategically increase the value of k to maximize the connectivity within clusters, thus avoiding collapse. However, it may make node embeddings between different clusters less discriminative. This may be harmful to the performance of the convex clustering model and RCC model, since the ratio of the maximum intra-cluster distance and the minimum inter-centroid distance plays a crucial role in successfully recovering the underlying clusters [23], [31]. To address this limitation, self-supervised learning emerges as a promising enhancement. The core principle of self-supervised learning -- to attract similar instances closer while distancing dissimilar ones -- naturally aligns with the objectives of clustering. Therefore, we integrate self-supervised learning, particularly through contrastive learning, to achieve a more discriminative feature representation.\nSpecifically, contrastive learning is characterized by two key components: (1) augmenting data instances, which generates multiple views of each instance, and (2) optimizing contrastive loss, which maximizes the agreement between different views of the same instance, compared to that of dissimilar instances. Following this, we first create an augmented view X2 of each original feature X12 through the addition of Gaussian noise. Each pair of features is then encoded to produce respective representations via Equation (13), yielding Z1 and Z2. These two representation views are fused in a linear manner as \\(Z = (Z_1+Z_2)\\) to form the final representation, which is utilized to compute the reconstruction loss (11) throughout the training process. Note that at each training steps, we are aware of the ongoing clustering results. This feedback provides valuable signal for alleviating the sampling bias [32], which has been shown to impair the effectiveness of contrastive learning. Intuitively, instances residing in the same cluster tend to be potential false negatives. Accordingly, we refine the commonly used InfoNCE loss [33] with a clustering-guided adjustment, formalized as follows:\n\\(L_{ssl} = \\frac{1}{2n} \\sum_{i=1}^{2n} log \\frac{e^{s(z_i, z_i')}}{e^{s(z_i, z_i')} + \\sum_{j \\in N_i} (e^{s(z_i, z_j)}+e^{s(z_i, z_j')})},\\)\nwhere \\((z_i, z_i') \\in \\{(z_i^{(1)}, z_i^{(2)}), (z_i^{(2)}, z_i^{(1)})\\}\\), \\(z_i^{(1)}\\) and \\(z_i^{(2)}\\) are the i-th row of \\(Z_1\\) and \\(Z_2\\), respectively. The similarity function s(.), such as cosine similarity in this work, evaluates the resemblance between features, and Ni excludes instances from the same cluster as sample i in current step.\nThis type of contrastive learning aids in acquiring more resilient and discriminative feature representations. Additionally, as previously discussed, enhanced representations can facilitate the learning of graph structures and lead to improved weights, thereby significantly enhancing clustering performance."}, {"title": "C. Adaptive Self-supervised Robust Clustering", "content": "As mentioned earlier, in order to generate better graph structures for obtaining discriminative representations, we first adapt the graph structure to align with clustering objectives and utilize GAE to extract high-level information. This tailored graph structure then serve as the scope (edge set E) of regularization in Problem (3), with the transformed average weights Pacting as the regularization weights, further matching the clustering task. Finally, we enhance the representation quality through a clustering-guided contrastive learning. It is notable that, since weighted graph learning and node representation learning are carried out alternately, the quality of the graph structure, weights, and representation mutually reinforce each other. Overall, the objective of ASRC is formulated as,\n\\(L_{ASRC} = L_{GAE} + \\beta L_{ssl},\\)\nwhere \u03b2 is the trade-off parameter. We employ gradient descent to optimize Eq.(16), enabling the consistent improvement of the graph structure, weights, and representations. Thus, problem (3) can be rewritten as,"}, {"title": "D. Computational Complexity", "content": "The time complexity of the neighbor aggregation step \\(\\hat{A}X\\Theta^{(i)} \\in []R^{n \\times d_i}\\) in GCN training process is \\(O (l|E|\\sum d_i)\\), where l is the total number of gradient descent and E is the number of edges. The computational complexity for building the adjacency matrix of the graph \\(A \\in R^{n \\times n}\\) is \\(O (n^2)\\). For RCC, it has been demonstrated in [21] that its per-iteration time complexity is in linear order w.r.t. the number of samples n and the input feature dimension, making it highly efficient."}, {"title": "IV. EXPERIMENT", "content": "1) Datasets: In this paper, we conduct extensive experiments across 7 unstructured benchmark datasets, including texts, images, and protein expressions. The statistics of these datasets are summarized in Table I. Here we provide a brief introduction to these datasets: 20NEWS is a widely used text dataset for text classification and text mining, containing 20 different news thematic categories. We adopt the configuration in [28] and select the first four groups for experiments; UMIST [34] comprises 564 pictures of 20 individuals, each represented through frontal and side views captured from multiple perspectives; The Columbia Object Image Library (COIL-20) [35] includes grayscale images of 20 different objects, each imaged from various angles; MNIST is a collection of handwritten digit images. Following the setting of [28], we only use MNIST-test, and to maintain notation simplicity, MNIST-test is represented as MNIST. JAFFE [36] contains 213 images depicting the facial expressions of 10 Japanese women. Mice Protein [37] dataset measures the protein expression levels in the cerebral cortex of control and trisomic mice, spanning eight distinct groups. Note that the images from UMIST, COIL-20, and JAFFE are downsampled to 32 \u00d7 32 pixels. All features across these datasets are normalized to a range of 0 to 1.\n2) Baseline: We select seven state-of-the-art methods for comparison\u00b3, encompassing four categories.\n\u2022 Traditional center-based clustering methods:\n\u2022 Kmeans [38]: It is a conventional center-based clustering method utilizing distance measurement.\n\u2022 Continuous clustering methods:\n\u2022 RCC [21]: It introduces robust estimator, which is an enhanced version of convex clustering.\n\u2022 Graph spectral clustering methods:\n\u2022 N-Cut [39]: It maps data points to a low-dimensional feature space by solving the eigenvectors of the Laplacian matrix."}, {"title": "3) Evaluation Metric", "content": "To evaluate the clustering results, we adopt the following metrics:\n\u2022 Adjusted Mutual Information (AMI): It is a metric commonly used in clustering analysis to quantify the agreement between clustering results and ground truth labels.\n\u2022 Adjusted Rand Index (ARI): ARI not only considers agreement but also takes into account the effects of random assignment and cluster imbalance.\nFor each metric, a higher value indicates a better clustering result."}, {"title": "B. Implementation Details", "content": "For fair comparisons, we reproduce the results either from the officially released code or reimplement the method with careful fine-tuning as suggested in the original paper. For GNN-based methods, we employ kNN to construct the graph structure followed by the kmeans clustering on the trained embeddings to facilitate clustering. The criterion for constructing the KNN graph is chosen from {euclidean, cosine} depending on the dataset. Except for RCC, all other algorithms require prior knowledge of the number of clusters. Regarding N-Cuts [39], we generate the similarity matrix by applying Gaussian kernel (RBF) as follows,\n\\(S_{ij} = \\frac{exp(-\\frac{||X_i-x_j||^2}{\\sigma})}{\\sum_{j \\in N_i} exp(-\\frac{||X_i-x_j||^2}{\\sigma})},\\)\nwhere Ni is the set of node i's k-nearest neighbors and \u03c3 is the Gaussian kernel weight. Following [28], we set the number of layers of GCN to 2 to prevent the over-smoothing issue. The activation function is set to ReLU. For all methods, we conduct a grid search for the optimal learning rate within the set of {le \u2013 4, 1e \u2013 3, 1e \u2013 2}. The parameter search ranges for each model are specified as follows: For GAE, the hidden layers are adjusted between [256, 128] and [128, 64]. For SDCN, the number of epochs for pretraining and training are tuned in {10, 20, 30, 40, 50, 60, 70, 80, 90, 100} and {100, 200, 300} respectively. The pretrain dimension is explored in {300, 400, 500, 600}, and the feature vector dimension is adjusted in {10, 20, 30, 40, 50}. For MaskGAE, the encoder and decoder dropout rates are tuned in {0.6, 0.7, 0.8, 0.9} and"}, {"title": "C. Comparison with Baselines", "content": "The clustering results of ASRC and all baselines on 7 benchmark datasets are shown in Table III. Table II reports the number of clusters obtained by RCC and ASRC on different datasets. From these results, we can obtain the following observations:\n\u2022 Compared with other baselines, the 'std' value of RCC [21] across all datasets consistently remains at 0, demonstrating the stability of RCC. This can be attributed to RCC's design, which does not rely on specific initialization. ASRC, building upon the strengths of RCC, similarly exhibits stable clustering results without any extreme variations.\n\u2022 Among GNN-based methods, adaptive graph structure learning approaches, such as adaGAE [28] and ASRC, consistently outperform the traditional graph construction method like kNN. The primary limitation of kNN lies in its overly simplistic nature, failing to capture the underlying structural information. It relies solely on low-level information, misguiding the information propagated within GNNs. In contrast, the adaptive graph learning strategy excels in learning high-level structural information, thereby enhancing representation quality for clustering.\n\u2022 On the 20NEWS dataset, the AMI and ARI values of RCC are close to 0, indicating poor performance. Given that 20NEWS is a text dataset, its features are inherently discrete, failing to sufficiently capture the relationships among samples. The ambiguous graph structure and poor-quality representations further exacerbate this issue, resulting in RCC's tendency to produce clusters with numerous outliers and very few samples. This can be confirmed from Table II that the cluster number of RCC on 20NEWS is extremely large, almost 600 times greater than the actual number of classes.\n\u2022 Compared to RCC, ASRC provides more accurate predictions regarding the number of clusters. This suggests that ASRC is more adept at discerning the underlying clustering patterns within unstructured data. The enhanced adaptive graph structure learning and clustering-guided negative sampling strategy contribute to the generation of representations that are more conducive to clustering. Consequently, embeddings of the same clusters exhibit greater similarity, while those of different clusters become more distinguishable.\n\u2022 The proposed ASRC consistently outperforms all baselines across all cases, demonstrating its superiority. Specifically, compared to the strongest baseline, ASRC achieves a notable increase of 6.09% in AMI, 9.12% in ARI on average. Unlike GNN-based clustering methods that utilize kNN to construct graphs, enhanced adaptive graph structure learning can better reveal the edge connectivity information between samples. In contrast to adaGAE, the consistent graph structure and weight assignment boost the performance of continuous clustering. When compared to RCC, ASRC benefits from the enhanced feature representation, improved graph structure, and optimized weight assignment, resulting in clearer and more accurate clustering results."}, {"title": "D. Further Analysis", "content": "1) Ablation Studies: The enhanced graph structure learning and debiased negative sampling are two key designs in ASRC, we here design the following ablation experiments to validate their effectiveness. Specifically, we incorporate two variants of ASRC: the variant ASRC-1 excludes both the enhanced graph structure learning and debiased negative sampling, while the variant ASRC-2 omits only the debiased negative sampling. We also report the results of RCC and adaGAE, where RCC constructs the graph structure using KNN and computes weights using the Gaussian kernel function; adaGAE adaptively constructs graphs to effectively exploit high-level information. Due to space constraints, Table. IV only presents the experiment results on three datasets: 20NEWS, UMIST, and Mice Protein. The complete experimental results for all datasets can be found in the appendix A. We can observe that:\n\u2022 Compared to RCC, ASRC-1, ASRC-2, and ASRC exhibit substantial performance improvements. This underscores the necessity of adaptive graph structure learning. Unlike traditional KNN graphs, which often suffer from noise, adaptive graph structure learning facilitates the capture of both local and global structural data characteristics. The resultant weighted graph more accurately reflects the relationships between samples, making it more suitable for use as weights in the regularization terms of convex clustering.\n\u2022 ASRC-2 outperforms ASRC-1 in most cases, demonstrating the superiority of enhanced graph structure. The enhancement to adaGAE ensures that the graph structure, weights, and representations are consistent. This consistency leads to more accurate and accordant inputs for RCC, thereby improving clustering results."}, {"title": "3) Visualization of Clustering Results", "content": "To deepen our comprehension of how embeddings influence clustering results, we use t-SNE to visualize the raw features and the embeddings obtained from SDCN and ASRC, as shown in Fig. 3. Note that the embedding of ASRC is obtained from the GCN encoder, rather than the representatives from continuous clustering. We can observe that ASRC encourages instances of the same cluster (ground truth) to have similar embeddings, thereby achieving cohesive and separable clustering results."}, {"title": "V. RELATED WORK", "content": "The convex clustering model and its variants have been extensively studied over the last ten years and have been widely applied in applications [43", "44": "under relatively restrictive assumptions. Later, the deterministic recovery guarantees for the convex clustering model have been generalized to multi-cluster cases with more general geometry in [31", "24": "proved that the convex clustering model with uniform weights can only recover convex clusters, where the convex hulls of each cluster are disjoint. Recently, the deterministic recovery guarantees for the general weighted convex clustering model have been established in [23", "45": "."}]}