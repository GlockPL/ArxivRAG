{"title": "TSFeatLIME: An Online User Study in Enhancing Explainability in Univariate Time Series Forecasting", "authors": ["Hongnan Ma", "Kevin McAreavey", "Weiru Liu"], "abstract": "Time series forecasting, while vital in various ap-plications, often employs complex models that are difficult forhumans to understand. Effective explainable AI techniques arecrucial to bridging the gap between model predictions and userunderstanding. This paper presents a framework - TSFeatLIME,extending TSLIME, tailored specifically for explaining univariatetime series forecasting. TSFeatLIME integrates an auxiliaryfeature into the surrogate model and considers the pairwiseEuclidean distances between the queried time series and thegenerated samples to improve the fidelity of the surrogatemodels. However, the usefulness of such explanations for humanbeings remains an open question. We address this by conductinga user study with 160 participants through two interactiveinterfaces, aiming to measure how individuals from differentbackgrounds can simulate or predict model output changes inthe treatment group and control group. Our results show thatthe surrogate model under the TSFeatLIME framework is ableto better simulate the behaviour of the black-box consideringdistance, without sacrificing accuracy. In addition, the user studysuggests that the explanations were significantly more effectivefor participants without a computer science background.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series data, which consists of sequential records col-lected over time, plays a crucial role in high-stakes decision-making across various fields such as healthcare [1] and finance[2]. The primary objective of time series forecasting is to pre-dict future observations within a temporally ordered sequence,which may exhibit underlying trends and seasonality patterns.\nWithin the realm of time series forecasting, several es-tablished models exist, broadly categorized into statisticalmodels (e.g., SARIMA, Exponential Smoothing, Prophet,Theta), modern machine learning models (e.g., tree-basedensembles such as XGBoost, CatBoost, LightGBM, GaussianProcess Regression), including deep learning models (e.g.,DeepAR, N-BEATS, Informer, and SCINet). Although theseblack-box models have shown promise in delivering accuratepredictions, they often sacrifice transparency, interpretability,and fairness in their decision-making processes. Consequently,users might find it challenging to understand the reasoningbehind a model's predictions, even if the model showcasesimpressive accuracy. Furthermore, [3] mentioned that humansas inherently curious beings, have a desire to understand thereasoning underlying the decision-making processes employedby the black-box model. Consequently, individuals are lesslikely to utilize a machine learning model if they cannot fullyunderstand and trust its operations. Due to these two reasons,researchers delve into the area of explainable AI (XAI).\nWhile substantial progress has been achieved in explain-ability within the fields of tabular data, computer vision andnatural language processing, providing explanations for time series data remains challenging due to its complex non-lineartemporal dependencies and the multi-dimensional nature [4].When dealing with time series data, it is not sufficient tomerely identify the importance of each timepoint contributingto a model's decision. Equally important is understanding howfeatures from seasonal patterns, trends, and cyclical behavioursimpact the decision-making process of these models.\nTheissler et al. [5] proposed a taxonomy for XAI for time series forecasting, which subdivides the method into timepoints-based, subsequences-based and instance-based expla-nations. We primarily concentrate on the time points-basedexplanation. Within this area, two principal methodologiesemerge: attribute-based and attention-based explanations. Theattention-based mechanism is a type of ante-hoc explainabilitymethod. It is integrated into the architecture of recurrentneural networks, and the insights it provides are immedi-ately accessible after the model's learning phase [6]. In con-trast, for attribute-based methods, there are three subclasses:gradient-based, structure-based, and surrogate-and-sampling-based techniques. The surrogate-and-sampling approach pri-marily focuses on creating samples adjacent to a specifiedinput to train an interpretable model like LIME (Local In-terpretable Model-Agnostic Explanation). Alternatively, SHAP(SHapley Additive exPlanations) employs a game-theoreticalframework to assign weights to features for better attribution.For LIME, it is worth noting that LIME has been applied in thesubsequence-based domain [7], wherein each segment, post-extraction, is treated as a feature. The importance attributedto each of these segments serves as the basis for explanation.Nevertheless, a notable gap remains in applying LIME expla-nations for time points-based problems. How to provide com-prehensive explanations centred on these timepoints continues to be a topic of discussion in the field.\nTo compare the performance of different local explainers,fidelity, serves as a crucial metric for evaluation in the areaof model explainability [8]. Fidelity quantifies the extent to which the explanatory model accurately mirrors the behaviour"}, {"title": "II. RELATED WORK", "content": "XAI is still a young field in the area of time series,with most works tilting towards classification problems [5].Bridging the gap in explainability for time series forecasting,several approaches have been explored. The intersection oftime points-based explanation and LIME of explainabilitymethod has seen trends where time series data undergoes transformation into a feature-based format, thereby shiftingthe problem from a pure forecasting scenario to a traditionalregression problem and feeding it into black-box models.For example, a study by [4] introduces the lag features andleverages Support Vector Regression (SVR) for model training.Similarly, [11] and [12] extract both date-related and time-related features from original time series data and then feedthem into the model-building phase. Figure 1(a) demonstrates the process of LIME applied in time series. By extractingthese features, the goal is often to simplify the process ofgenerating explanations, allowing for the direct applicationof LIME. However, these approaches have their limitations.Utilizing the regression model directly makes the black-boxmodel less accurate because the regression model does notcapture the dependencies between timepoints. The perturbationphase in LIME isn't specifically tailored to time points-based.After the feature extraction, the data undergoes perturbation which is similar to traditional tabular data. It can blur thetemporal dependencies inherent in raw time series data whengenerating new samples to train a local model.\nIn the study by [13], the authors introduced the Time SeriesLocal Interpretable Model-agnostic Explainer (TSLIME), tai-lored specifically for time points-based explanations. TSLIMEis an extension of LIME focusing solely on pure forecastingmethods without requiring additional feature extraction be-fore training the black-box model. Figure 1(b) demonstrates the process of TSLIME explanation. A notable distinctionof TSLIME from prior work is its use of the time seriesperturbation method, block bootstrap, to generate local inputperturbation samples. Furthermore, it then utilizes a multilin-ear surrogate model for explanations, optimally approximatingthe model's response.\nWhile TSLIME represents a significant advancement com-pared to previous methods, its approach of using only lagfeatures to construct the local explainer might lead to expla-nations that could be unclear to users. Furthermore, ignoringthe distance between the queried time series and generatedsamples in the perturbation process also affects the fidelityof the surrogate model. Our objective is to identify andincorporate a wider range of time features, enhancing the depthof information provided to non-experts."}, {"title": "III. TSFEATLIME", "content": "A. Perturbation for time series\nPerturbation, aimed to generate supplementary samples,is prevalent both in machine learning and local explanationdomains. In LIME, perturbation is employed to produce alocalized dataset surrounding the queried data point. For"}, {"title": "B. Auxiliary features", "content": "We introduced a set of interpretable auxiliary features toenrich the surrogate model's ability to describe the underlyingtime dynamics. The set of features could be defined by theexpert user as well based on their demand. Apart from lag,we also define two features: Rolling window, and Extendingwindow.\nDefinition 1. The lag feature with offset $k \\in N$ is defined foreach $t = 1, 2, . . ., n$ as:\n$\\text{Lag}_{k}(t) = \\begin{cases}\nY_{t-k} & \\text{if } k < t \\\\\n\\text{undefined} & \\text{otherwise}\n\\end{cases}$\nLagged features introduce historical values from previous timepoints as new features. $n$ represents the total number ofobservations in a time series. $y_t$ represents the value at time point $t$.\nDefinition 2. The rolling window feature with offset $k \\in N$and window length $w \\in N$ is defined for each $t = 1, 2, . . ., n$as:\n$\\text{Rolling Window}_{k,w}(t) = \\begin{cases}\n\\sum_{i=t-k-w+1}^{t-k} Y_i & \\text{if valid}(k, w, t) \\\\\n\\text{undefined} & \\text{otherwise}\n\\end{cases}$\nwhere if valid(k,w,t) = $k < t$ and $w \\leq t - k$.\nGiven a fixed window size $w$, rolling window features referto a time period prior to time point $t$ spanning over $w$ timepoints."}, {"title": "C. TSFeatLIME explanation tool", "content": "The MBB perturbation process is discussed in Figure 2, andhence, the details of generating the perturbed sample set B byapplying MBB function to the queried time series O are omit-ted in the Pseudocode 1. The perturbed sample set B consistsof p samples. That is B = {B1, B2, ..., Bp}. Each sample Biand the queried time series O consist of q timepoints. That is,Bi = {bi1, bi2,...,biq} and O = {01, 02,..., Oq}. Here, bijis defined as the $j^{th}$ time point in sample B\u2081. The Euclideandistance between O and Bi is represented as:\n$d(O, B_i) = \\sqrt{\\sum_{j=1}^{q} (O_j - b_{ij})^2}$\n(1)\nThe distance $d$ is assigned to each sample Bi in B based ontheir proximity to the queried time series being explained, suchthat samples closer to the time point receive higher values,thereby ensuring the explanations are more locally faithful andrelevant. The pairwise Euclidean distances set D is definedin Line 2, which contains all distances between the queriedtime series and generated samples. Each generated sampleBi is fed into the black-box model f to obtain the predictedvalue f(B). Then put each f(Bi) in list Yperturb for trainingthe surrogate model in Line 3. Subsequently, the feature orauxiliary feature function a is applied on each sample Bi toobtain a series of feature values, represented as a(Bi(q+1)).Details about how to generate feature or auxiliary featurevalues using a can be found in the previous subsection. The"}, {"title": "IV. EXPERIMENT OF SETUP AND EVALUATION", "content": "A. Datasets and forecasting models\nWe validated TSFeatLIME using two univariate time series forecasting datasets. The Superstore sales dataset fromCommunity.tableau.com (2017) records sales from 2014 to2017, featuring nearly 10,000 data points across 21 distinctfeatures. We focus on furniture sales. The second datasetcomprises daily records of electricity demand in Spain from2014 to 2018. Both of them are resampled on a monthlyfrequency, employing average daily consumption values andsetting the start of each month as the index. [16] comparedforecasting models such as SARIMA, Triple ExponentialSmoothing, Prophet, Vanilla LSTM, and CNN. We evaluatedthese models on our datasets using Root Mean Square Error(RMSE) and Mean Absolute Percentage Error (MAPE), where lower values indicate higher accuracy. The stacked LSTMemerged as the best model for furniture sales forecasting,while the Vanilla LSTM excelled in electricity consumptionforecasting. Consequently, both models are used as black-box models in subsequent experiments. Details about datapartitioning and training results are available under GitHub:"}, {"title": "V. USER STUDY", "content": "A. Methodology\n1) Interface design: To investigate the impact of explana-tion on user understanding of the process of black-box, we de-signed a comparative study with two different interfaces whichintegrate with Prolific. This integration allows us to recruitparticipants from Prolific based on our specific requirements.The control group accessed an interface without additionalexplanatory content while the treatment group's interface in-cluded both visual and textual explanations. Participants fromdifferent groups accessed these interfaces through separate links. After providing consent, participants could continueor exit the experiment. A tutorial was provided afterwards.Following the tutorial, participants completed four exercises.After completing all exercises, participants were presentedwith a questionnaire. The interface for consent, the tutorial,the questionnaire, and the source code are also available inour GitHub repository.\n2) Exercise design: The exercise design is based on theconcept of counterfactual simulation. This type of simulationinvolves presenting users with a queried time series xc, amodel's output for that input \u0177c, and an explanation of thatoutput e. Participants are then asked to predict the model'soutput \u0177 when given a perturbation p of the queried time series xc. We have made a minor change to the conceptof counterfactual evaluation to simplify the exercise. In ourcontext, \u2018counterfactual simulation\u2019 refers to an experimentdesigned to quantify whether, through our explanations, a usercan predict the trend t of output changes yt rather than thespecific value of \u1ef9 after a perturbation in a query.\nExercise for control group: The orange solid-line box inFigure 3 displays the control group's view. At the top, somebasic information is provided. The rest of the page is dividedinto 'Time series forecasting' and 'Your exercises'. On theright side, the month marked in red is the one being predicted,based on the data from the previous 12 months. For eachexercise, participants are asked to make predictions based onaline chart, without any additional visual explanation. Takinglag as an example, the question is: \"If the sales value of Dec2016 was increased, would the prediction result go up, remainstable or go down?\" The options are: Go up, Remain stable,Go down. After submitting results, participants will proceedto the next round until the 'questionnaire' button appears.\n3) Questionnaire design: There are 13 questions spanningfour categories: basic information, system satisfaction & thehelpfulness of explanations, curiosity & trust, and overallfeedback. These questions are designed based on the criteriafor explainable AI proposed by [17].\n4) Participant and procedure: We evaluated 80 participantsper group recruited through the online platform Prolific. Par-ticipants were required to be over 18 years old, and fluent inEnglish. Participants from computer science (CS) backgroundsare known to influence user perception of XAI. Therefore, tojustify our conclusions and control for relevant variables, wedivided each group into subgroups of computer science andnon-computer science backgrounds. Each subgroup consisted"}, {"title": "B. Results", "content": "1) Questionnaire evaluation: Figure 4(a), 4(b), 5(a) and5(b) highlight issues with system satisfaction and explanationshelpfulness (Q4, Q5, Q7, Q8) for the control and treatmentgroup, each from different backgrounds, respectively.\u00b9\nQuestion 4 surveyed participants' understanding of lagand rolling window in the field of time series regardingtheir difficulty. We observed that participants with CS back-ground in both the control group (57.5%) and the treatmentgroup (57.5%) found these concepts relatively straightforward,aligning with our expectations. Conversely, more than halfof the participants from non-computer science backgroundsfound the concepts challenging. Additionally, three times asmany non-computer science participants rated the conceptsas 'very hard' compared to their computer science counter-parts. Question 5 assessed how helpful participants foundthe use of global explanations in understanding time seriespredictions. Interestingly, a higher percentage of participantswith a machine learning background (identified in Question2) found global explanations to be helpful compared to thosewithout such a background. Global explanations provide anoverview of the model's overall behaviour and decision-making processes. However, those without machine learningbackground generally seek information that directly impacts their outcomes with the model. Question 6 assessed whichexplanations were more helpful for the treatment group andwhether participants in the control group preferred to have anexplanation when making predictions. In the treatment group,about half of the participants, both from computer science(45%) and non-computer science backgrounds (47.5%), foundboth explanations helpful. Among the remaining participants,25% from CS background found the 'lag' explanation moreuseful, while 15% preferred the \u2018rolling window' explanation.Conversely, among those without CS background, 20% pre-ferred the 'lag' explanation, and 22.5% preferred the 'rollingwindow'. Given these results, we conclude that both expla-nations are considered helpful regardless of the participants'backgrounds. A definitive conclusion about which explanationis superior cannot be drawn. In the control group, almostall participants requested explanations, except for four in thecomputer science subgroup. Question 7 focused on systemsatisfaction across both groups, while Question 8 assessedthe overall experience of participants. Figures 4(a), 4(b), 5(a)and 5(b) demonstrate that almost all participants found theinterface helpful and the exercises meaningful, regardless oftheir group or background. This indicates that participantswere pleased with the experiment and recognized the valueof the exercises."}, {"title": "C. Simulatability results", "content": "In addition to the questionnaire analysis, we also obtainedvaluable results from exercises, which were analyzed from astatistical perspective. In this study, participants engaged infour rounds of exercises, each round comprising two questions.For each question, participants could earn 1 point for a correctanswer and 0 points for an incorrect one, contributing toa total possible score of 2 points per round and 8 pointsoverall for the entire exercise. To determine if there werestatistically significant differences in how well the predictionresults were across the treatment and control groups, weemployed the Mann-Whitney U-Test. This non-parametric testwas chosen because it is well-suited for comparing ordinaldata across independent groups, particularly when the dataare not normally distributed. The null hypothesis assumed thatthere was no difference between the control and treatmentgroups in terms of total scores achieved, while the alternativehypothesis posited a difference. The Mann-Whitney U-Testresults showed no significant difference between the controland treatment groups for participants in CS background, witha U value of 794, n1 = 40, n2 = 40, and p = .958, indicatingthat the null hypothesis could not be rejected. However, amongthe non-computer science group, a significant difference wasobserved, with U = 559, n1 = 40, n2 = 40, and p = .021,leading to the rejection of the null hypothesis. These resultssuggest that the explanations were significantly more effectivefor participants without CS background."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In summary, we proposed TSFeatLIME, a framework forgenerating local explanations in univariate time series usingauxiliary features and incorporating distance to enhance surrogate model fidelity. We developed a web interface to evaluateexplanation effectiveness across different backgrounds, findingthem more effective for non-CS participants. While explanations using rolling window and lag techniques were helpfulwithout a clear preference, a designed questionnaire indicatedthat explanations moderately boost confidence in AI systems."}]}