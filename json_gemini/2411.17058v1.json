{"title": "ThreatModeling-LLM: Automating Threat Modeling using Large Language Models for Banking System", "authors": ["Shuiqiao Yang", "Tingmin Wu", "Shigang Liu", "David Nguyen", "Seung Jang", "Alsharif Abuadbba"], "abstract": "Threat modeling is a crucial component of cyber- security, particularly for industries such as banking, where the security of financial data is paramount. Traditional threat modeling approaches require expert intervention and manual effort, often leading to inefficiencies and human error. The advent of Large Language Models (LLMs) offers a promising avenue for automating these processes, enhancing both efficiency and efficacy. However, this transition is not straightforward due to three main challenges: (1) the lack of publicly available, domain-specific datasets, (2) the need for tailored models to handle complex banking system ar- chitectures, and (3) the requirement for real-time, adaptive mitigation strategies that align with compliance standards like NIST 800-53. In this paper, we introduce ThreatModeling-LLM, a novel and adaptable framework that automates threat mod- eling for banking systems using LLMs. ThreatModeling- LLM operates in three stages: 1) dataset creation, 2) prompt engineering and 3) model fine-tuning. We first generate a benchmark dataset using Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought (CoT) and Opti- mization by PROmpting (OPRO) on the pre-trained LLMs to optimize the initial prompt. Lastly, we fine-tune the LLM us- ing Low-Rank Adaptation (LoRA) based on the benchmark dataset and the optimized prompt to improve the threat identification and mitigation generation capabilities of pre- trained LLMs. The experimental results demonstrate that our proposed scheme substantial improvements over the pre- trained LLMs, significantly enhancing the model's ability to identify threats and suggest mitigations. For example, the accuracy of identifying mitigation codes improves from 0.36 to 0.69 on Llama-3.1-8B-Instruct (short for Llama-3.1- 8B). The results illustrate that the combination of prompt engineering and fine-tuning techniques is highly effective for automated threat modeling, making ThreatModeling-LLM a robust and flexible solution for real-world applications in banking and beyond.", "sections": [{"title": "1. Introduction", "content": "Threat modeling is a critical cybersecurity process that identifies potential threats and suggests mitigations for system designs using frameworks like Microsoft's STRIDE [1]. It plays a vital role in proactively addressing vulnerabilities and preventing security breaches, which can lead to significant financial and reputational dam- ages [2]. For instance, threat modeling can block intrusion attempts and prevent hijacking of privileged accounts, sig- nificantly reducing risks in critical systems [3]. However, the traditional approach is labor-intensive, requiring man- ual efforts for Data Flow Diagram (DFD) creation, threat identification, and mapping to mitigations, which makes it inefficient and prone to human error.  shows the traditional process relies heavily on tools like the Microsoft Threat Modeling Tool (TMT), which demands extensive manual input at each stage. This is particularly challenging in dynamic sectors like banking, where the rapid evolution of online services and increasing sophis- tication of threats have intensified the need for more effi- cient, automated threat modeling solutions [4]. Traditional methods struggle to keep up with the complexity of con- fidentiality, integrity, and privacy requirements in banking systems, underscoring the urgency for automation. Large Language Models (LLMs) such as GPT-3 [5] and Llama-3 [6] offer promising potential to transform the threat modeling landscape. Traditional methods, such as pytm [7], do not provide direct mappings to NIST 800- 53 standards, which are critical for compliance and com- prehensive security analyses. LLMs can process textual descriptions of system designs, automatically identifying threats and suggesting corresponding mitigations. This shift not only accelerates the process but also enhances accuracy by reducing manual intervention. For instance, commonly utilized industry tools like STRIDEGPT [8] and Cyber Sentinel [9] show the trade-offs between au- tomation and precision; STRIDEGPT, while automating threat identification, produces unstable results. Cyber Sen- tinel, despite its adaptability to new threats, offers limited mitigation strategies. These examples underscore a preva- lent trade-off between specializes capabilities and compre- hensive functionality across these tools. While pre-trained LLMs, have demonstrated impressive results in various NLP tasks, directly applying them to threat modeling in banking systems is insufficient. Pre-trained LLMs lack domain-specific knowledge and struggle to understand complex banking architectures, resulting in inconsistent threat identification and mitigation suggestions. Moreover, they are not specifically designed to generate mitiga- tion codes aligned with compliance standards like NIST 800-53, which is essential for banking security. These drawbacks highlight that without additional adaptation, LLMs fall short of meeting the precision, compliance, and context-specific needs of threat modeling in the banking sector. However, the adaptation of LLM for threat mod- eling is non-trivial and poses several challenges:"}, {"title": "Challenge 1: Lack of publicly available datasets.", "content": "A significant challenge in threat modeling analysis is the lack of publicly available datasets, especially for complex systems like banking. Traditionally, researchers and se- curity experts generally manually assess potential threats within DFDs, a process that is not only labor-intensive but also prone to human error. To transition towards a data-driven approach for automatically identifying threats in banking system-based DFDs, real-world datasets are essential. These datasets provide crucial information about threats and mitigations in practical scenarios, serving as the foundation for building and training automated tools that can efficiently detect and address security vulnerabil- ities. However, the creeation of a real-world dataset for automatic threat modeling remains a challenging problem in the field."}, {"title": "Challenge 2: Tailored LLMs for banking systems.", "content": "While LLMs have achieved remarkable success in fields like natural language processing, software security, and network security, applying them to threat analysis in banking systems is an underexplored area. The unique structure and operational complexity of banking systems require specialized threat models that can understand the specific vulnerabilities in financial transactions, user au- thentication, and data flow between systems. This gap hinders the efficient identification and modeling of threats unique to banking infrastructures. Developing an efficient and effective LLM-based system for bank system-based threat analysis poses an important and creative research question that remains unsolved."}, {"title": "Challenge 3: Lacking an automatic mitigation strate- gies:", "content": "Once threats are identified, the next critical step is developing effective, real-time mitigation to safeguard the system. However, this is a complex task due to the dynamic and evolving nature of threats within financial systems. Developing novel mitigation strategies requires deep expertise in both banking operations and security protocols, as well as sophisticated algorithms that can adapt to changing threats. Automating this process is par- ticularly challenging because it demands solutions that can respond to threats in real-time while maintaining system efficiency and compliance with stringent banking regula- tions. Therefore, creating an automatic mitigation strategy remains a challenge that still needs to be addressed to ensure continuous improvement in system security and the protection of sensitive financial data. To address the first challenge, we created the first benchmark dataset in the community by designing var- ious types of banking systems. For each system, we used the TMT to draw the DFDs based on the application de- sign documents. The TMT-generated threats and human- annotated mitigation strategies using the NIST 800-53 served as the ground truth for fine-tuning the LLMs, ensur- ing that the dataset accurately reflected real-world security scenarios. For the second and third challenges, we pro- pose combining prompt engineering and fine-tuning meth- ods to create a customized LLM model, ThreatModeling- LLM, specifically for identifying banking system threats and mitigations. Prompt Engineering. We explore differ- ent prompt templates to find the optimal structure for the LLM to produce accurate threat and mitigation outputs. Chain-of-Thought (CoT) [10] is used to make the model explicitly reason through intermediate steps, and OPRO (Optimization by PROmpting) [11] is applied to refine the prompts iteratively. These techniques help improve the quality of generated responses when identifying threats and suggesting mitigations. Model Fine-Tuning. Based on the generated prompt, we fine-tune a base LLM (such as Llama-3.1-8B) using our created dataset to empower the LLMs with the abilities to generate more accurate threats and mitigations based on the text input. The fine- tuning process involves Low-Rank Adaptation (LoRA) [23], which allows efficient adaptation of the model to domain-specific tasks like threat identification in banking systems. The fine-tuning data includes DFD descriptions, identified threats, mitigations, and their respective NIST 800-53 control codes. Fine-tuning enables the model to grasp the unique vulnerabilities and mitigations needed in financial systems. Our experiments show that combining prompt en- gineering and fine-tuning outperforms using the either technique alone. ThreatModeling-LLM demonstrates sig- nificant improvements in the accuracy of threat identifica- tion and mitigation generation, providing a more effective and automated threat modeling process tailored for the banking sector. Our key contributions are as follows:"}, {"title": "\u2022", "content": "We introduce the innovative ThreatModeling-LLM framework, specifically designed to automate threat modeling for banking systems. This framework is uniquely tailored to address the complexities of the banking domain, combining advanced prompt engi- neering with fine-tuning techniques to to enhance the capability of pre-trained LLMs. Specifically we op- timize the prompt by CoT and OPRO. We then fine- tuning the model with our specialized dataset using the optimized prompt. These techniques encourage the model to explicitly reason through intermediate steps and iteratively refine its responses, leading to more accurate and detailed identification of threats and generation of mitigation strategies aligned with NIST 800-53 control codes."}, {"title": "\u2022", "content": "To overcome the scarcity of publicly available datasets for banking threat modeling, we have metic- ulously designed a specialized training dataset that"}, {"title": "3. Preliminaries, Problem Definition and Mo- tivation", "content": ""}, {"title": "3.1. Preliminaries", "content": "Microsoft STRIDE [24] is a framework used for threat modeling in software security. As shown in Table 2, it stands for Spoofing, Tampering, Repudiation, Infor- mation Disclosure, Denial of Service, and Elevation of Privilege, representing the various categories of security threats that need to be addressed. When using Microsoft STRIDE for threat modeling analysis, the process begins with the identification and categorization of potential se- curity threats to a system. Each category corresponds to a specific kind of threat. For example, 'Spoofing' involves an unauthorized user impersonating another to gain access to a system, while 'Tampering' refers to the unauthorized modification of data. By applying the STRIDE framework, security analysts systematically explore each threat category in relation to the target system. They assess the system's architecture to identify where and how these threats could potentially be realized. This involves examining data flows, authenti- cation mechanisms, network interfaces, and other relevant aspects of the system. Once threats are identified, they are documented, and the system's vulnerabilities that could be exploited by these threats are pinpointed. The final phase of the Stride methodology in- volves proposing and prioritizing mitigations for identified threats. This may include implementing secure coding practices, enhancing authentication protocols, applying encryption, or introducing intrusion detection systems. Stride not only helps in recognizing potential threats but also plays a crucial role in the design phase by guiding developers to integrate security measures early in the de- velopment process, thus reinforcing the system's defense against malicious attacks and reducing the risk of security breaches. NIST Cybersecurity Framework is developed by National Institute of Standards and Technology (NIST) to provide high-level cybersecurity outcomes for different sizes of business. The framework has five functions to organize the cybersecurity activities, including Identify, Protect, Detect, Respond and Recover. Identify represents understanding cybersecurity risks of the relevant organi- zation, then Protect develops effective security protective process to maintain the running of important services. Detect refers to the discovery of the cybersecurity events, and Respond takes actions to the incident. Lastly, Recover supports the recovery to the service and minimize the im- pact of the cybersecurity incidents. NIST 800-53\u00b9 comple- ments the framework by offering a detailed catalog of se- curity and privacy controls. While the framework outlines the \"what\" of cybersecurity (the essential functions), NIST 800-53 provides the \"how\" by specifying technical and organizational safeguards. These controls can be mapped to the framework's functions, making NIST 800-53 an operational tool to achieve the cybersecurity outcomes outlined in the framework. The controls are categorized into 20 families, such as AC (Access Control) and IR"}, {"title": "3.2. Problem Definition", "content": "In this section, we formally define the research prob- lem. As shown in Figure 1, traditional threat modeling techniques, such as those based on Microsoft's STRIDE framework, rely heavily on manual analysis, which is time-consuming and prone to human error. Furthermore, the process of identifying appropriate mitigation strategies and ensuring compliance with standards like NIST 800- 53 is complex and resource-intensive. In this work, we aim to automate the threat modeling process for banking systems by leveraging LLMs as illustrated in . Given a textual description of a banking system design, such as an ATM system, the goal is to automatically identify potential security threats based on the STRIDE framework and generate the mitigation strategies based on NIST 800-53. Let $X$ represent the designing document of a banking system. This input $X$ is a sequence of words (tokens), where: $X = [x_1,x_2,..., x_n]$ where $x_i$ represents the $i$-th token in the text, and $n$ is the total number of tokens in the input description. The goal is to transform this input into two outputs:\n\u2022 Threats Identification: $T$\n\u2022 Mitigation Strategy: $M$ Threats Identification. Let $T = \\{t_1,t_2,...,t_k\\}$ represent the set of identified threats, where $t_i$ corresponds to the $i$-th threat. Each threat $t_i$ is a function of the input description $X$ and is defined based on the STRIDE framework categories (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege): $t_i = f_{STRIDE}(X), \\forall t_i \\in \\{S,T, R, I, D, E\\}$ where $S, T, R, I, D, E$ represent the STRIDE categories. The function $f_{STRIDE}$ maps the input $X$ to one or more threat categories based on the analysis of the data flow diagram (DFD). Mitigation Strategy. Let $M = \\{m_1, m_2, ..., m_k\\}$ represent the set of mitigation strategies, where $m_i$ corresponds to the $i$-th mitigation strategy associated with threat $t_i$. Each mitigation strategy $m_i$ is a function of both the identified threat $t_i$ and the control codes defined by the NIST 800-53 standard. The mitigation strategy $m_i$ is given by: $m_i = g_{NIST}(t_i), \\forall m_i \\in \\{NIST 800\u201353 Control Codes\\}$ where $g_{NIST}$ maps the identified threat $t_i$ to the corre- sponding NIST 800-53 mitigation control code. Objective. The system's objective is to generate the set of pairs $\\{(t_i, m_i)\\}_{i=1}^k$ from the input description $X$ using the fine-tuned LLM model $h$, where each pair $(t_i, m_i)$ corre- sponds to an identified threat and its respective mitigation strategy based on the STRIDE framework and NIST 800- 53 control codes."}, {"title": "3.3. Motivation", "content": "To investigate the state-of-the-art, we examined four notable automated threat modeling tools, encompassing both industry-standard and emergent GPT-based technolo- gies. These tools are Cyber Sentinel (CS) [9], pytm [7], STRIDEGPT (SG) [8], and Raw LLM (RL) (using Chat- GPT) [25] stand out for their unique capabilities. Cyber Sentinel is renowned for its adaptability to new threats, providing proactive security measures. pytm, specifically designed for Python applications, integrates threat mod- eling directly into the development process, facilitating seamless security assessments. STRIDEGPT leverages the STRIDE methodology through automation to efficiently pinpoint potential threats. Lastly, Raw LLM (ChatGPT) offers broad contextual knowledge, making it a versatile tool for general threat analysis. We summarize the strengths and weaknesses of the tools in . The limitations of the four automated threat modeling tools, Cyber Sentinel, pytm, STRIDEGPT, and Raw LLM (ChatGPT), highlight the challenges in bal- ancing strengths with functional shortcomings within the realm of cybersecurity modeling. Cyber Sentinel, while highly adaptable to new threats, suffers from limited ca- pabilities in offering specific mitigation strategies, which restricts its utility in proactive threat management. pytm, despite its seamless integration with Python environments, does not provide direct mappings to NIST 800-53 stan- dards, which are critical for compliance and compre- hensive security analyses. STRIDEGPT, which leverages the STRIDE methodology to automate threat identifica- tion, struggles with consistent categorization and produces- unstable results, undermining its reliability. Lastly, Raw LLM (ChatGPT) offers expansive contextual knowledge but falls short in providing consistent, technically pre- cise mitigation suggestions and lacks the capability to deeply map technical controls, which are essential for"}, {"title": "4. The Proposed ThreatModeling-LLM", "content": ""}, {"title": "4.1. Overview", "content": "The comprehensive system overview is depicted in Figure 2. Our system consists of three core components that streamline threat modeling. The first component, Data Creation, utilizes the Microsoft Threat Modeling Tool (TMT) to manually generate samples for threat modeling, with 50 samples manually verified to establish a ground truth dataset. The second component, Prompt Engineering, involves the manual design of initial prompt for an LLM, which are then optimized to improve the model's response effectiveness. The final component, Model Fine-tuning, focuses on refining the threat modeling model through precise LLM fine-tuning, ensuring high accuracy and re- liability in the bank applications."}, {"title": "4.2. Dataset Creation and Verification", "content": "To study automatic threat modeling using LLM in banking systems, one needs to prepare a well-organized dataset. However, as far as we know, there is no pub- licly available dataset. Therefore, we need to prepare the dataset, the dataset should meet the following require- ments: 1) it should reflect real-world scenarios; 2) it should be generated using a publicly available and widely used tool in the security community; 3) it should include threats, mitigation, and the related control code. In light of these requirements, this work first uses the TMT to generate different DFDs for banking systems. TMT is a core part of the Security Development Lifecycle, enabling users to create data flow diagrams and identify potential threats early. Designed for non-security experts, it simplifies threat modeling by providing standard visual notations and guidance, helping to mitigate security risks in software development. We use the Windows System. The windows operating system is Microsoft Windows"}, {"title": "4.3. Prompt Engineering", "content": "Prompt engineering represents a critical aspect of leveraging language models effectively, acting as the inter- face between human intentions and machine understand- ing. Prompt engineering involves crafting inputs that guide AI models, particularly LLMs, to generate desired outputs with higher precision and relevance. This discipline has become increasingly significant with the rise of more sophisticated AI models that are capable of understanding and generating human-like text. Traditional prompt engineering has evolved from sim- ple iterative refinements to adopting more systematic ap- proaches. One prominent method within this evolution is the CoT prompting [10]. This technique involves in- structing the model to verbalize its intermediate reasoning steps or cognitive processes as it approaches a solution. By simulating a more transparent thought process, CoT helps align the model's responses more accurately with complex problem-solving tasks, significantly improving the output's clarity and correctness. Recent prompt engineering has focuses more on dy- namic generation of new prompts. Another innovative technique is OPRO (Optimization by PROmpting) [11], which focuses on the dynamic refinement of prompts in response to evolving dialogue contexts and the model's prior outputs. This adaptability is especially valuable in interactive settings where user queries can progressively modify the scope or specificity of the discussion, necessi- tating correspondingly nuanced adjustments from the AI. OPRO enables the model to respond effectively to such shifts, maintaining relevance and depth in its answers. In this context, we introduce a novel prompt engi- neering approach tailored for cybersecurity threat mod- eling. Our method identifies potential threats, suggests mitigations, and references applicable security controls, such as those specified in NIST 800-53. Integrating the strengths of CoT and OPRO, our technique establishes a robust framework for threat modeling. By using CoT, we instruct LLMs to methodically outline their reasoning, simulating an expert's analytical process in identifying and evaluating security threats. This clear reasoning is essential for validating AI-generated insights and ensures that each step is both logical and justifiable. The explicit articulation of thought processes not only deepens the model's analytical capabilities but also enhances the re- liability and traceability of its outputs, thereby enhancing domain knowledge application. Alongside CoT, we employ OPRO to dynamically adjust these prompts according to the ongoing context of the threat modeling session. This integration allows our method to adaptively respond to new information or changes in focus, ensuring that the model's analysis remains comprehensive and pertinent throughout the inter- action. By merging these strategies, our approach guides LLMs to generate detailed, actionable threat models that not only identify potential risks but also recommend suit- able mitigations aligned with NIST 800-53 control codes. This sophisticated prompting strategy significantly boosts the Al's capacity to emulate expert-level cybersecurity analysis and aligns its outputs with industry standards, providing a formidable tool for organizations aiming to enhance their security measures."}, {"title": "4.4. LLM Fine-tuning", "content": "Fine-tuning is a crucial step in adapting pre-trained language models to specific tasks, enhancing their ac- curacy and effectiveness in specialized domains. In the context of cybersecurity, particularly for threat identi- fication from Data Flow Diagrams (DFDs), fine-tuning enables the model to better understand domain-specific language patterns and structures associated with potential vulnerabilities. We employ Low-Rank Adaptation (LORA) [23] as the primary technique for fine-tuning, which is both efficient and effective for resource-constrained envi- ronments."}, {"title": "4.4.1. Low-Rank Adaptation (LoRA) for Fine-tuning.", "content": "LoRA is a parameter-efficient fine-tuning method that adapts large pre-trained models by injecting learnable low-rank matrices into the original model's weights. This approach significantly reduces the number of trainable parameters, making fine-tuning more computationally fea- sible while maintaining model performance. Let $W \\in R^{d\\times k}$ be the weight matrix of the pre-trained model, where $d$ is the input dimension and $k$ is the output dimension. In LoRA, the update to $W$ is represented as the product of two low-rank matrices: $\\Delta W = AB$ where $A \\in R^{d\\times r}$ and $B \\in R^{r\\times k}$, with $r < min(d, k)$ being the rank of the decomposition. This decomposition allows the original weight matrix to be updated as: $W_{new} = W + \\alpha \\cdot \\Delta W$ where $\\alpha$ is a scaling factor to control the magnitude of the adaptation, and $\\Delta W$ is the low-rank update applied to the original weight matrix $W$. Principles of LoRA Fine-tuning The core principles of the LORA fine-tuning method include:"}, {"title": "\u2022", "content": "Parameter Efficiency: By training only the low-rank matrices A and B, LoRA significantly reduces the number of trainable parameters. The total number of trainable parameters becomes $d \\times r + r\\times k$, where $r < min(d, k)$, making it highly efficient compared to traditional fine-tuning methods."}, {"title": "\u2022", "content": "Computational Efficiency: Since only the low-rank matrices A and B are optimized during training, the computational and memory requirements are substan- tially lower than those of standard fine-tuning meth- ods. This makes LORA suitable for environments with limited computational resources, such as edge devices or lower-end GPUs."}, {"title": "\u2022", "content": "Maintaining Performance: Despite its reduced pa- rameter count, LoRA maintains or even enhances the model's performance. The low-rank updates effec- tively capture domain-specific features without com- promising the model's generalization capability."}, {"title": "\u2022", "content": "Adaptability to Domain-specific Tasks: LoRA al- lows for efficient adaptation of pre-trained models to specialized tasks by focusing on learning task- specific information encoded within the low-rank matrices. In the context of threat modeling from DFDS, LORA helps the model recognize patterns and relationships specific to cybersecurity. The LoRA-based fine-tuning approach thus provides a robust framework for adapting pre-trained models to domain-specific tasks in a resource-efficient manner, mak- ing it well-suited for applications like automated threat modeling."}, {"title": "5. Experimental Setting", "content": ""}, {"title": "5.1. Data Preparation", "content": "For this study, we manually created 50 sample re- lated to the banking system domain, each representing a different application. Each sample simulates a unique banking system architecture, covering various aspects of threat modeling to ensure diversity and robustness in the training data. For each sample, it contains two fields:\n\u2022 Application Description: Each sample contains a textual description of the application system, outlin- ing its components, data flow, and overall architec- ture.\n\u2022 Ground Truth Threats and Mitigations: Each application description includes manually verified threats and mitigations, which serve as ground truth labels for training and evaluation."}, {"title": "5.2. Fine-tuning Configuration", "content": "The fine-tuning process leverages LoRA to enhance the model's performance in the banking threat modeling domain. Key parameters for LoRA include:\n\u2022 Rank (r): 32, specifying the rank of the low-rank de- composition, balancing between parameter efficiency and model adaptation.\n\u2022 Scaling Factor (lora_alpha): 64, controlling the adaptation strength of the model.\n\u2022 Target Modules: Includes projections such as \"q_proj\", \"k_proj\u201d, \u201cv_proj\", and \"o_proj\", which are specific to Llama models.\n\u2022 Dropout Rate: 0.1, applied to prevent overfitting during the adaptation process."}, {"title": "5.3. Training Configuration", "content": "The model training was conducted using the Trans- formers library with the following major hyperparameters:\n\u2022 Batch Size: 4 per device, with gradient accumulation steps set to 4, effectively increasing the batch size during training.\n\u2022 Optimizer: \"paged_adamw_32bit\", chosen for mem- ory efficiency and faster convergence.\n\u2022 Learning Rate: 1e-4, set for stable learning and effective adaptation.\n\u2022 Number of Epochs: 30, allowing sufficient training for the model to adapt to the threat modeling domain.\n\u2022 Evaluation Strategy: Evaluations are performed at regular intervals (every 20% of training steps), en- suring consistent monitoring of the model's perfor- mance."}, {"title": "5.4. Initial Prompt", "content": "For prompt engineering, we use the following initial prompt as a start point for instructing the LLMs. Initial Prompt for Prompt Engineering The STRIDE model is a systematic approach to identifying and analyzing potential security threats to a system. It helps in reasoning about and discov- ering threats by leveraging a comprehensive model of the target system, which includes detailed repre- sentations of processes, data stores, data flows, and trust boundaries. In this task, you will be provided with a description of a data flow diagram (DFD) for a specific application. Based on the provided DFD description, your objective is to identify all relevant security threats. For each identified threat, please specify the threat type, a detailed description of the threat, recommended mitigation strategies, and the corresponding mitigation code according to NIST SP 800-53 controls."}, {"title": "5.5. Prompt Configuration", "content": "The prompt engineering process includes two steps: CoT (Chain of Thoughts) and prompt evolution as shown in . For CoT, we incorporate \u201cfew-shot\" learning by utilizing two examples to guide prompt design and optimization, and \"zero shot\" by employing a step-by- step reasoning method to address specific threats within the banking system. The prompt evolution is implemented based on OPRO, with the following configurations:\n\u2022 Scorer:\n\u2022 Model type: \"gpt-3.5-turbo\"\n\u2022 Max output tokens: 1024. A higher token limit allows for comprehensive responses that can fully evaluate the effectiveness and completeness of the prompts.\n\u2022 temperature: 0.0. The model will produce the most likely output, which is beneficial for consistent scor- ing and easier comparative analysis.\n\u2022 Num Decodes: 1. Since the focus is on reliability and predictability for scoring, only one decode is needed to evaluate each input without introducing variability.\n\u2022 Batch size: 1. Maintain simplicity and control over the experiment. Each prompt is processed individu- ally, reducing complexity in handling outputs.\n\u2022 Num Servers: 1. Simplify the infrastructure require- ments and ensures that the environmental factors affecting model performance are consistent across all tests.\n\u2022 Optimizer:\n\u2022 Model type: \"gpt-3.5-turbo\"\n\u2022 Max output tokens: 512. A lower token count encourages the model to focus on conciseness and creativity within a shorter output, which might lead to more diverse and inventive prompt generation.\n\u2022 temperature: 1.0. Increase randomness and variabil- ity in responses. This setting is optimal for generating creative and diverse prompts.\n\u2022 Batch size: 1. Like the scorer, maintaining a batch size of 1 ensures that each generated prompt is eval- uated individually, allowing for precise adjustments and optimizations based on singular output analysis.\n\u2022 num_servers: 1. Consistency in computational en- vironment between scoring and optimizing, reducing any potential bias or variability introduced by differ- ent server setups.\n\u2022 Other settings:\n\u2022 Instruction position: \"Q_begin\" (the instruction is added before the original question.)"}, {"title": "5.6. Evaluation Metrics", "content": "To evaluate the performance of the proposed prompt engineering and the fine-tuned methods to identify the threats and proper mitigation control codes, we adopted semantic similarity analysis using BERT and set-based evaluation of mitigation control codes using precision, recall, and accuracy. BERT Similarity Score: We used the BERT model to compute the cosine similarity between the embeddings of the LLM-generated threats/mitigation strategies and the ground truth annotations. This allows us to capture not only exact matches but also paraphrased or contextually similar descriptions. The similarity score for each generated output is cal- culated as: $Similarity Score = cos(BERT(G_{truth}), BERT(G_{generated}))$ where $G_{truth}$ is the ground truth description, and $G_{generated}$ is the LLM-generated output. Higher similarity scores indicate better alignment with the ground truth."}, {"title": "5.6.1. Mitigation Control Code Evaluation.", "content": "To evaluate the accuracy of the mitigation control codes generated by the LLM, we treated this as a set-matching problem, comparing the sets of control codes from the generated output to the ground truth. We employed the following metrics to assess performance: Precision: Precision measures the proportion of cor- rectly generated control codes out of all codes predicted by the LLM. It is calculated as: $Precision = \\frac{|C_{generated} \\cap C_{truth}|}{|C_{generated}|}$ where $C_{generated}$ represents the set of control codes gener- ated by the LLM, and $C_{truth}$ represents the set of ground truth control codes. Recall: Recall measures the proportion of the correct control codes identified out of all the ground truth codes. It is calculated as: $Recall = \\frac{|C_{generated} \\cap C_{truth}|}{|C_{truth}|}$ Accuracy: Accuracy evaluates the overall correctness of the generated control codes, comparing the total num- ber of correctly generated codes to the total number of codes present in both the generated set and the ground truth set. It is calculated as: $Accuracy = \\frac{|C_{generated} \\cap C_{truth}|}{|C_{generated} \\cup C_{truth}|}$ The $C_{generated}$ and $C_{truth}$ denote the mitigation codes generated by LLM and the ground truth mitigation codes."}, {"title": "6. Evaluation", "content": "In this study, we aim to address the following research questions:\nRQ1: How does the performance of CoT+OPRO compare to the Initial Prompt, CoT, and OPRO?\nRQ2: How does fine-tuning improve the perfor- mance of LLMs compared to their base models?\nRQ3: How does the performance of our devel- oped system, ThreatModeling-LLM, compare to ex- isting methodologies? What are the effects of integrat- ing prompt engineering with fine-tuning within our ThreatModeling-LLM framework? In the following sections, we answer the research questions and present our empirical findings."}, {"title": "RQ1: How does the performance of CoT+OPRO compare to the Initial Prompt, CoT, and OPRO?", "content": "Figure 5 highlights two crucial components in our meta prompt design process: CoT and Prompt Evolution based on OPRO. These elements play a central role in refining prompt design to enhance threat identification and mitigation precision within a banking data flow diagram scenario. Starting with CoT", "few- shot\" learning by utilizing specific examples to guide prompt design and optimization. This technique leverages two example QA pairs, representing distinct scenarios or questions related to potential security vulnerabilities, to refine the analysis and understanding of security threats in banking data flow diagrams. The Prompt Evolution segment captures how iterative refinements based on OPRO significantly enhance output precision. Initial analysis precision is at 0.5, reflecting the early stage of addressing security vulnerabilities. Contin- uous iterations refine threat models, improve description clarity, and optimize mitigation strategies, progressively increasing precision. This process culminates in a final precision of 0.57, demonstrating a systematic approach to maximizing the effectiveness of prompt design for identifying and mitigating potential security threats in the banking context. presents a comparative analysis of four prompt engineering techniques": "Initial Prompt, CoT, OPRO, and the combined CoT+OPRO approach. Evalu- ated across four metrics (Accuracy, Precision, Recall, and Text Similarity), the results, based on GPT-3.5-turbo, re- veal that integrating CoT with OPRO achieves the highest performance across all metrics, particularly excelling in Precision and Text Similarity. The Initial Prompt method, serving as a baseline, shows limited effectiveness, with relatively lower scores in Accuracy (0.17), Precision (0.35), and Recall (0.27). CoT improves upon this baseline by significantly increasing both Accuracy and Recall, demonstrating that explicit reasoning steps contribute to better threat identification. CoT's Precision also surpasses the baseline, highlighting its capability to generate more relevant and precise out- puts. OPRO, when applied independently, achieves mod- erate improvements, especially in Precision, but does not match CoT's overall performance. The combination of CoT and OPRO demonstrates a synergistic effect, achieving the highest scores in all"}]}