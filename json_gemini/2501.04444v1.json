{"title": "A novel Facial Recognition technique with Focusing on Masked Faces", "authors": ["Dana A Abdullah", "Dana Rasul Hamad", "Hakem Beitollahi", "Ismail Y Maolood", "Abdulhady Abas Abdullah", "Aso Khaleel Ameen"], "abstract": "Recognizing the same faces with and without masks is important for ensuring consistent identification in security, access control, and public safety. This capability is crucial in scenarios like law enforcement, healthcare, and surveillance, where accurate recognition must be maintained despite facial occlusion. This research focuses on the challenge of recognizing the same faces with and without masks by employing cosine similarity as the primary technique. With the increased use of masks, traditional facial recognition systems face significant accuracy issues, making it crucial to develop methods that can reliably identify individuals in masked conditions. For that reason, this study proposed Masked-Unmasked Face Matching Model (MUFM). This model employs transfer learning using the Visual Geometry Group (VGG16) model to extract significant facial features, which are subsequently classified utilizing the K-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed to compare masked and unmasked faces of the same individuals. This approach represents a novel contribution, as the task of recognizing the same individual with and without a mask using cosine similarity has not been previously addressed. By integrating these advanced methodologies, the research demonstrates effective identification of individuals despite the presence of masks, addressing a significant limitation in traditional systems. Using data is another essential part of this work, by collecting and preparing an image dataset from three different sources especially some of those data are real provided a comprehensive power of this research. The image dataset used were already collected in three different datasets of masked and unmasked for the same faces. The current study provided %95 results of the used image dataset face recognition between masked and unmasked images. The results highlight the potential of this approach for enhancing accuracy in security and other applications where face recognition is critical, regardless of facial obstructions.", "sections": [{"title": "1.Introduction", "content": "In the present-day system recognitions called biometric applications have become a significant point for people all over the world to aid them in many essential platforms in society such as criminals [1] [2]. That is why developing these systems has been the focus of the researchers. Human recognition in a variety of ways such as facial recognition has become a hot topic among researchers. One of the most important ways that have not yet been tried is to recognize human faces using cosine similarity [3] . This research focuses on recognizing humans using the most advanced methods of recognition, which are artificial intelligence techniques and algorithms [4]. The success of this method will make it easier to solve the problem of identifying those who wear masks when committing a crime that is often not recognized by the face properly. Facial recognition technology is progressing at an accelerated pace. Artificial intelligence has made it easier for researchers to take advantage of techniques and algorithms to achieve excellent results in face detection [5]. Also, in the past decades, the facial recognition field has witnessed crucial development and become a mainstay in various applications such as user interaction systems and security [6] [7]. Facial feature analysis and accuracy detection are the two essential roles in the effectiveness of these applications [8]. Cosine similarity is an effective method for measuring the similarity between two vectors, as it evaluates the cosine of the angle between them [9]. A cosine value of zero indicates that the vectors are identical, signifying that the features of the masked and unmasked faces correspond to the same individual. This approach enables the recognition and matching of masked and unmasked faces for the same person with high precision. Facial recognition represents the image features which have already been extracted in the form of vectors. These vectors are naturally extracted by deep learning models like Convolutional Neural Network (CNN). There are some methods to organize the CNN architecture such as Visual Geometry Group (VGG), or it will be used in deep learning for image recognition tasks. Simplicity and fully connected layers are two significant characteristics that VGG16 have. It includes 13 layers of convolution followed by 3 connected layers which use very small receptive fields (3X3) filters. Another important key feature of VGG16 is that it supports 118 million parameters making the model have a very heavy memory storage and computational resources [10]. This model uses \"Rectified Linear Unit\" (ReLU) as an activation function which aids in introducing non-linearity. For vector imaged classification, KNN can be utilized, which is a popular technique, and it has been used for various studies and successfully outlined a comprehensive implementation [11].\nAs a result, to improve this field there are numerous areas available such as improved algorithms, complex features, incorporation of multi-modal data and feature extraction techniques. Working on image data is challenging due to complexity and invisibility features. The following descriptions are the challenges during the current study implementation. These challenges serve to support that facial recognition in masked conditions is still a challenging problem and that continuous innovations in detection algorithms, and improving the quality of datasets are still necessary."}, {"title": "2.Related work", "content": "In this section some related works are highlighted that are slightly related to current study, while facial recognition has seen significant advancements, research specifically focused on masked face recognition using cosine similarity has not been done. Recent studies have explored deep learning techniques to address this challenge. For example, [12] utilized pre-trained CNN techniques like VGG-16, ResNet-50 and AlexNet to extract features from the eye and forehead regions of masked faces.\nThe superior deep learning model is known as Additive Angular Margin loss function (ArcFace) where Angular Margin loss function has shown a better outlook in large scale face recognition functions due to the introduction of additive angular margin. In [12] work, a new ADDitive Angular Margin Loss, named ArcFace, is proposed to enhance face recognition by maximizing the class discriminability with a clear geometric meaning. The authors acknowledge that ArcFace is robust to label noise: therefore, they propose sub-center ArcFace, where the training data points belong to not one but several sub-centers per class. This enables the existence of the main sub-class for clean faces and other auxiliary sub-sigma for noisy or difficult faces to help in dealing with real-world noise in Web face databases. This paper also investigates the reverse process of synthesizing face images from feature vectors using only the ArcFace model without other networks. Despite improvements of discriminative feature embedding and identity preserving face synthesis in experiments, there are still some limitations in the ArcFace methodologically.\n[13] The researchers tested a generic face recognition model on masked face images without the need to further train the model. They used face embeddings from the model and classified them with three efficient classifiers: For classification we have K-Nearest Neighbors (KNN), Support Vector Machine (SVM) with One vs Rest strategy, and Mean Embeddings (ME) using Sk-learn Nearest Centroid implementation. Their approach enhanced the precision of the model in masked-face conditions by 9.09 per cent, proving that ways utilized herein are efficient even when no extra training is conducted.\n[14] Face recognition is one of the most useful biometric systems with many applications in security, advertisement and many others that seeks to identify particular individuals. This paper used HOG, LBP, PCA, SURF and HAARIS in detecting facial features and the KNN algorithm in comparing the stored training images to the new test images. Employing the 100 images of which there were 60 training images and 40 testing images, HOG algorithm had the highest accuracy rates; above 85%.\nClick or tap here to enter text.[16] is another effort in terms of face recognition without mask is close to the current work's attempt that only the techniques used are different, meaning that in their work, the researchers relied on structural similarities measures, feature similarity measures, edge detection and gaussian noise. They found three models: Feature-Based Structural Measure (FSM), Structural Similarity index measure (SSIM) and feature similarity index measure (FSIM). Within their study researchers presented the performance of similarity measure results for each person, here are the best results of each model will be displayed. As a result of their work, it has been found that SSIM records 0.5773 for person No. 8, while FSIM obtained 0.2803 for the same person No. 8, this is the highest results of these two models. However, FSM achieved 0.8661 for person No. 8 and 0.8791 for person N0. 5 which is the highest result of this model. This is proved that FSM outperforms SSIM and FSIM.\n[17] Due to the mandatory wearing of face masks as a measure of fighting COVID-19 this research came up with an AI based facial recognition system that recognizes masked faces. For face detection and mask identification the system employs MobileNetV2 and OpenCV face detector, while for facial recognition \u2013 FaceNet and a feedforward multilayer perceptron. The proposed model was trained on 13,359 images, 52.9% of which had masks, with the remaining 47.1% no masks at all. This resulted in 99.65 percent and 99.52 percent for detection of masks and recognizing that there are masks on the faces respectively and finally, 99.96 for faces which are not masked, so in this work more focuses was on the faces are wearing mask or not. Similarly, [18] is another work to recognize the individuals are wearing or not wearing masks, this paper then proposes an innovative approach to facial recognition from masked faces to suit different sectors. The approach utilizes SD-MobileNetV2 for mask detection, MLO and oval face detection for detecting the facial landmarks and RPCA to tackle the problem of occlusion in masked faces. Moreover, to enhance KNN features and the optimum number of neighbors for recognition the concept of particle swarm optimization (PSO) is incorporated. The method attained 97% recognition rate and distance and poses approximately 1m more than existing approaches, and it was also proved to be less sensitive to occlusion. [19] this paper focuses on the following: MASKED FACE RECOGNITION which will help during the COVID-19 pandemic. Based on the facial landmarks for the benchmarks IJB-B, IJB-C, FG-Net, SCface, MS-1MV2, synthetic masks were created and tested to demonstrate performance under several challenges namely pose, illumination, expression, age and image resolution. ResNet-100 was used as the feature network to extract features from the input image, and novel loss functions (such as Center Loss, Marginal Loss and Angular Softmax etc.) were also employed. The performance analysis based on verification and identification also present the model trained over the synthetic morphed face outperforms"}, {"title": "3. Methodology", "content": "In this section, the general and specific methodology of this work is presented through a series of successive paragraphs and figures. The first steps include data collection from different sources followed by data loading and data preprocessing stage. Then feature extracting, classifying and measuring similarity between vectors occur. Last, the model is trained and tested on all masked and unmasked images in order to assess the performance of such model.\nSo, with this study we proposed a wide range of methods to improve masked and unmasked face recognition using VGG16 for feature extraction, K_NN for classification and the metric of cosine similarity for measuring similarity among extracted features in forms of vectors. First, the primary facial features were detected from both, the masked and unmasked images through the pre-trained deep learning model called the VGG16, which is capable of identifying difficult visual patterns. The images obtained from prepared dataset then they were preprocessed to feed into VGG16 model, while keeping most of the layer of VGG16 model frozen and tuning other layers. The extracted feature embeddings procedure was converted to vectors and K_NN classified these vectors into put probable similar neighbors into the same bucket. Last, cosine similarity was computed in order to find the relation between masked and unmasked images, which can allow comparing feature embedding for the same subject under both conditions. This methodology provides a sound paradigm for both masked and unmasked face recognition."}, {"title": "3.1 Data collection", "content": "One of the primary challenges encountered in this study was the limited availability of image datasets containing the same individuals with and without masks. This scarcity of data is a significant limitation, as datasets featuring identical faces both masked and unmasked are exceedingly rare or virtually nonexistent. While there are numerous data sets available for unmasked faces, finding corresponding masked images of the same individuals is particularly challenging. To address this issue, we made extensive efforts to search and gather images from various sources. After a thorough and resource- intensive search, we successfully compiled a dataset of 858 images that meet the study's criteria. This dataset includes paired images of the same individuals, both with and without masks, enabling us to proceed with our analysis despite the data constraints. These data have been collected from three different sources such as, [20] Celab-A-HQ from the Kaggle website as it includes several images each image taken in different directions. The second part has been collected from Zenodo Website which is sourced in [21]. And other images were real data collected by the current research authors.\nFigure 2 shows the process of collecting data from the above-mentioned sources, based on this process firstly the image data was directly taken by the authors in order to utilized as a real data, however the real collected data was not enough to provide a promise result that is why the other sources were utilized to collect data like the previously mentioned sources then these data finally became a single and integrated dataset for training and testing phases."}, {"title": "3.2 Data Pre-processing", "content": "After collecting the required data and combining to be a one single dataset, then they have to be preprocessed as discussed in the next section.\nAfter collecting the image dataset, we proceeded with an extensive image preprocessing stage to ensure the data was well-prepared for model training. The preprocessing steps included the following:\n1.  Folder Organization: We created two separate folders: There should be one folder for people without masks, and the other one for people with masks. This kind of clear segregation is important in order to properly label and in further activities during the model training stages.\n2.  Image Resizing: To keep the scale consistent, all the images available in both folders were altered to fit the standard size. It is very important to do it because otherwise different images to be used in the process take different sizes, and it will mislead the model and slow down the training process.\n3.  Image Format Conversion: Due to the flexibility of the dataset that contained different file formats of images such as JPEG, BMP, GIF and many others, convert all images to PNG format. This brings consistency in the quality of images and does away with the problems that would be encountered during training of different file formats.\n4.  Labeling: Every image in the dataset were carefully annotated according to the given category. In the \u201cwithout masks\u201d set of images, the images were labelled as such, whereas images in the \u201cwith masks\" set were given labels that correspond to them. This labelling is required in especially in supervised learning wherein the model requires proper labelling of data for training.\n5.  Data Augmentation: To improve the dataset's quality and quantity, it was decided to apply several steps of data augmentation. These included different operations like rotation, flip, zooming, and shifting which helps in creating more variations artificially. There is one thing that augmentation is significant in making, which is avoiding overfitting and enhancing generality of the model.\n6.  Normalization: We normalized the pixel values of all images to bring them to a common scale, typically between 0 and 1. This step helps in speeding up the convergence of the model during training and ensures that the model performs optimally.\n7.  Noise Reduction: Techniques like Gaussian filtering was applied to reduce noise in the images. This process helps in improving the quality of the images, making it easier for the model to detect relevant features during training.\n8.  Color Space Conversion: Depending on the specific requirements of the model, we converted images from RGB to grayscale, where necessary. This conversion can reduce the computational load and simplify the model if color information is not critical for the task."}, {"title": "Proposed algorithms and techniques", "content": "To obtain the required results there are three steps (feature extraction, classification, measure similarity) in the (MUFM) model have been used and they have also been implemented with the same model. So, in this section these three techniques will be state, however the brief overview of development process using these three techniques are shown in Figure 5."}, {"title": "3.3 Convolutional Neural Network", "content": "In this research, the first crucial step is to extract important features from both masked and unmasked images of the same individual. To achieve this, a CNN is utilized, which is typically designed with a series of layers built specifically to capture and process visual patterns [23] . While general CNN architecture, consisting of standard layers, might perform adequately in some cases, it was found to be insufficient for this task of distinguishing features between masked and unmasked faces. The general CNN failed to capture the subtle variations and essential features needed for accurate recognition of the same person in both scenarios.\nIn order to avoid this peculiarity, it was decided to use a more complex, deep learning-based model for feature extraction. In such context, VGG16 that is a readily available deep architecture pre-trained CNN model has been chosen. VGG16 is specifically suitable for this purpose because of its capability to learn the intricate feature hierarchies and generate comparatively better discriminant embedding. This model enables the better extraction of specific facial features for a subject in both the masked and non-masked instances, or the general identification of the same subject in both these settings.\nVisual Geometry Group VGG16\nVGG16 is a method in CNN and was developed by the VGG at Oxford University in the year 2014. This has 16 layers, 13 of which are convolutional layers and three of which are fully connected layers; it is popular for images classification and feature extraction since its design is basic but effective. VGG16 makes uses of small filters of 3x3, ReLU activation functions, and max-pooling layers to obtain a pyramid of features ranging from simple edges to complex textures. Originally trained on millions of images of objects and animals from the ImageNet dataset, it is typically employed via transfer learning for other tasks such as the current forgery detection for extracting features in cases like face with mask and face without mask the current forgery detection. Role of VGG16 in Masked and Unmasked Face Recognition\nIn case of both mask-wearing and no mask, the VGG16 model is a feature extractor, which identifies only the visible part of the face (the jaw and eyes behind the mask and the forehead when the face is uncovered), while simultaneously training a network that learns to extract representations invariant to masking. Thus, it captures outstanding features that are edges, textures, and shapes, which enable the model to learn and generalize despite occlusion.\nTechniques for Feature Extraction\nPreprocessing: Input images are resized to 224x224 pixels and normalized to ensure consistent pixel values before being fed into VGG16.\nTransfer Learning and Fine-Tuning: VGG16's lower layers (which teach basic features) are frozen, while the upper layers are fine-tuned on a dataset of masked and unmasked faces, adapting the model to the task.\nFeature Embeddings: Both masked and unmasked images are converted into feature vectors (embeddings). These vectors are then compared using metrics like cosine similarity to determine if the images belong to the same person.\nVGG16 is used as a pre-trained model for feature extraction purposes, and the convolutional based of VGG16 was kept, the fully connected layers are either replaced. When the convolutional layer has ended the \"Global Average Pooling (GAP)\" was utilized to convert the feature maps into a single vector. The global features of the image were represented based on this vector, this will offer of reducing the dimensionality and making it easier to handle."}, {"title": "K-Nearest Neighbors (KNN)", "content": "KNN is a non-parametric algorithm used for classification, which works by finding the closest K data points (neighbors) to a given input based on a chosen distance metric, like Euclidean distance. In this research, KNN is applied after VGG16 has extracted features from both masked and unmasked images of the same individual. These features are transformed into vectors, representing the key facial characteristics.\nThe role of KNN here is to group and classify these feature vectors, identifying the nearest neighbors in the embedding space. Once KNN has separated these extracted features, they are ready for comparison using cosine similarity, which measures the similarity between the masked and unmasked face embeddings to determine if they belong to the same person."}, {"title": "Cosine Similarity", "content": "Cosine similarity is a metric used to measure the similarity between two vectors by calculating the cosine of the angle (0) between them. Mathematically, it is represented as the dot product of two vectors, A and B, divided by the product of their magnitudes, as shown in Equation 1:\nCosine Similarity = cos(0) = $\\frac{A\\cdot B}{||A||||B||}$ Equation 1\nHere, A. B is the dot product of the two vectors the graphics A and B, and ||A|| & ||B|| stands for their norms. Cosine similarity value is from -1 up to 1. Statistics showing that the value is equal to 1 mean that actually the vectors are similar, that is they have a positive orientation, while the value is equal to 0 implies that the vectors are perpendicular, that is they are not related and the angle between them is 90 degrees. Cosine similarity of -1 means vectors is at subterranean level of dissimilarity with angle of 180.\nIn the use of face recognition, therefore, a cosine similarity close to 1 means that the two vectors (faces) are closely related and should represent the same face. On the other end of scale, S < 0 means that the two faces are most different, in other words, the faces are orthogonal. This is why cosine similarity is helpful in areas such as machine learning and pattern recognition, as amply demonstrated in the word embedding problem above; it is based on the direction rather than the magnitude of the data. For this reason, it is a very effective measure for comparing the feature vectors and for applications where relative direction of data is important as compared to the direction magnitude.\nIn this study, the features from masked and unmasked images of the same person, as pre-processed by VGG16, are converted to vectors. They are then used the KNN method to group and categorize these vectors. Cosine similarity has its work to do after KNN has partitioned the features. They are used to quantify the level of similarity between the vectors which represents masked and unmasked face. Higher the value of cosine similarity, better the match of features extracted from two images implies that the system can identify the subject whether with or without a mask. Cosine similarity provides a better platform to find out the similarity of the facial structure or features under various conditions as shown in the Figure 6, how the cosine similarity works for input image data and apply feature extraction technique"}, {"title": "4.Experimental Study", "content": "The implementation of this report employs Feature extraction using the VGG16 model and K-NN in classifying the images of faces with and without masks. The code first links Google Drive to be able to use the dataset downloaded, then loads and preprocesses the images, including using data augmentation to add some level of randomness. Based on VGG16, a personalized model is developed and then pre- retrained and trained for the images' classification. Next, embeddings are obtained from the model and the k-neighbor nearest neighbors' classifier is applied on the images. Cosine similarity is then used as a method of detecting and pairing masked and unmasked images given their embeddings. Lastly, the code brings up two pictures \u2013 with and without the mask side by side if the model finds them similar, in order to show its matching function."}, {"title": "Results", "content": "The model is successfully able to put into operation a deep learning model using the VGG16 structure for identifying the same faces with and without the masks. To improve the model's training process initially, dense layers are added and Au has been used in order to increase the resistance of data augmented. The model loses accuracy only slightly when determining whether faces are masked or not.\nMoreover, embeddings extracted from the model are utilized by a K-NN classifier, which also performs well, accurately distinguishing between faces with and without masks. The use of cosine similarity to match image pairs further illustrates the model's ability to identify the same individuals regardless of mask usage, demonstrating its effectiveness in face recognition tasks involving masks.\nBased on the implementation of the current work there are comprehensive results were achieved; the majority of the Epochs were obtained over %90 or val_accuracy with each 20 Epochs the average result is %95 val_accuracy. The direct faces which are shown in Figure 7 between masked and unmasked images were detected properly."}, {"title": "Discussion", "content": "This code is capable of identifying the same faces with or without the influence of a mask, having adopted transfer learning from a VGG16 model. Using the code thus trained on the VGG16 algorithm, the current implementation recognizes the difference between a face that has been masked and a face that has been covered up. Freezing the base layers of VGG16 and adding some more additional dense layers also prevent the model from overfitting, which is mostly important if provided a large and complex dataset.\nThe ability of the model to generalize has been premised by the high accuracy recorded on both the training and the validation set. Among these, data augmentation is presented through the Image Data Generator class and is critical to generalization, as it adds variability in the training dataset thus preventing overfitting due to facial conditions.\nApart from the deep learning model, there is the application of a K-Nearest Neighbors (K-NN) classifier developed on embedding files from the VGG16 model, which forms the hybrid approach. The ability of the K-NN classifier, which achieved the perfect accuracy in the test set level, can be attributed to the discriminative characteristic of the learned embedding, in which the current model is turning out to be quite effective in discriminating between the same faces with and without the masks.\nVisualization of matched pairs of images (with and without masks) based on cosine similarity also shows a clear picture about the performance of the proposed model. This visualization does more than just ratify the model's proficiency; it is also beneficial for identifying how the changes in recognizing the same faces with masked and unmasked function optimally.\nAlthough the results obtained with the proposed model are promising, they must be evaluated again on a large dataset with a high level of variability in order to validate the reliability of the approach to real-world conditions. Moreover, it can be also beneficial to apply the model in conditions when it will be trained on new data from time to time (for example, using online learning) in order to improve its performance in conditions when the data changes its parameters from time to time."}, {"title": "5.Conclusion", "content": "This work was based on the identification of the same face both with and without the mask, applying such modern deep face recognition as VGG16, as well as such simple, but rather effective methods as KNN and cosine similarity. The study shown that even though the new challenges were introduced by masks the proposed system was able to reliably identify the individuals with the help of alignment between the masked and unmasked face. Thus, using the proposed modifications of the VGG16 model and embeddings with cosine similarity, the research obtained high accuracy with or without masks on faces. The results of the work indicate the effectiveness of the given approach to sustain facial recognition system performance effectively in occluded contexts where other methods can fail. The study also revealed aspects that could be further optimized within the system, including variety of tests and improved algorithms concerning different types of masks and degrees of their usage. Fields for further research should include how these challenges might be mitigated with the aim of enhancing the efficiency of the system. To sum up, the studies of this work make a positive contribution to the development of new facial recognition algorithms, as well as for optimizing the existing algorithms for mask wearing, and opens the door for the creation of versatile and reliable recognition systems in security and user interfaces."}]}