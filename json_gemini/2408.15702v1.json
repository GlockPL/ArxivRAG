{"title": "Evaluating Model Robustness Using Adaptive Sparse LO Regularization", "authors": ["Weiyou Liu", "Zhengyang Li", "Weitong Chen"], "abstract": "Deep Neural Networks (DNNs) have demonstrated remarkable success in various domains but remain susceptible to adversarial examples: slightly altered inputs designed to induce misclassification. While adversarial attacks typically optimize under Lp-norm constraints, attacks based on the Lo-norm, which prioritize input sparsity, are less studied due to their complex, non-convex nature. These sparse adversarial examples challenge existing defenses by altering a minimal subset of features, potentially uncovering more subtle DNN weaknesses. However, the current Lo-norm attack methodologies face a trade-off between accuracy and efficiency: either precise but computationally intense or expedient but imprecise. This paper proposes a novel, scalable, and effective approach to generate adversarial examples of the Lo norm, aimed at refining the robustness evaluation of DNNs against such perturbations.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNNs) have achieved unprecedented success in a myriad of applications, ranging from image recognition to natural language processing. However, their susceptibility to adversarial attacks, where subtly modified inputs can lead to erroneous model outputs, poses significant challenges to their reliability and security[16,17]. These adversarial perturbations not only compromise the reliability of DNNs in real-world applications but also expose underlying vulnerabilities in their decision-making mechanisms[18]. For example, in autonomous driving systems, adversarial attacks can manipulate sensor inputs, potentially leading to catastrophic outcomes.\nTraditionally, the generation of adversarial examples has focused on perturbations constrained by L2 norms, which primarily consider the magnitude of changes [19,20]. However, recent studies have shifted the focus towards Lo and Lo norms, led by insights from Croce & Hein[21,22], which highlight the efficacy of sparse perturbations. These perturbations are less perceptible, yet potent, challenging the conventional understanding and prompting a reevaluation of attack strategies emphasising sparsity."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Adversarial Attacks on Time Series Classification", "content": "The field of Time Series Classification (TSC) has seen significant advancements with the integration of deep learning techniques, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). These advancements have enabled models to effectively capture complex temporal dependencies and"}, {"title": "2.2 Sparse Adversarial Perturbations", "content": "In the context of adversarial attacks, sparsity-governed primarily by the Lo norm-emerges as a crucial yet underexplored dimension. Sparse adversarial examples, by altering a minimal number of input features, present a stealthier threat compared to their non-sparse counterparts. The strategic advantage of sparsity in evading detection and enhancing the transferability of attacks has been highlighted in studies by Papernot et al.[10] and Tram\u00e8r and Boneh[11]. These insights are essential for developing attacks that are both effective and difficult to detect, especially in time series data where perturbations must maintain temporal coherence."}, {"title": "2.3 Challenges in Lo Norm Optimization", "content": "Applying Lo norm attacks introduces complex challenges due to their non-convex and non-differentiable nature[23]. This has driven the development of innovative optimisation strategies that balance sparsity with attack effectiveness. For example, techniques by Addepalli et al. [24] and Bach et al. [25] illustrate the delicate interplay between sparsity and robustness. These approaches are critical for TSC models, where maintaining the integrity of the time series structure while introducing minimal changes is essential."}, {"title": "2.4 Existing Approaches and Limitations", "content": "Existing Lo norm attack methodologies often face a trade-off between accuracy and efficiency. Many methods achieve high accuracy but at the cost of computational intensity, or are computationally efficient but lack precision in generating effective adversarial examples. This trade-off limits their practical applicability, especially in real-time or resource-constrained environments. For instance, Louizos et al. [5] proposed differentiable approximations of the Lo norm to facilitate the development of sparser neural network architectures, while Schmidt et al. [6] explored Lo regularisation for adversarial robustness, highlighting the intersection of sparsity and security."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Overview of ASLO Strategy", "content": "The Adaptive Sparse and Lightweight Optimization (ASLO) strategy aims to generate adversarial examples by optimizing the Lo norm, focusing on achieving sparsity. The core idea is to dynamically adjust perturbation parameters in"}, {"title": "3.2 Basic Principles of ASLO", "content": "The ASLO method is designed to balance the trade-off between perturbation sparsity and attack efficacy. Traditional Lo norm attacks aim to modify the fewest possible input features, but this approach is often non-differentiable and challenging to optimize. ASLO introduces a differentiable approximation of the Lo norm, enabling the use of gradient-based optimization techniques."}, {"title": "3.3 Mechanism of ASLO", "content": "The ASLO method employs a differentiable approximation of the Lo norm, which enables the use of gradient descent for optimisation. The approximation is defined as follows:\n$l_0 (\\delta, \\sigma) = \\sum_{i=1}^{d} \\frac{\\delta_i^2}{\\delta_i^2 + \\sigma^2}$\nHere, $\u03b4_i$ represents the perturbation applied to feature i, and \u03c3 controls the sparsity of the perturbation. The term $\\frac{\\delta_i^2}{\\delta_i^2 + \\sigma^2}$ acts as a smooth approximation to the indicator function that would be 1 if $\u03b4_i \u2260 0$ and 0 if $\u03b4_i = 0$. This smoothness allows for gradient-based optimisation, which is typically not possible with the true Lo norm due to its non-differentiability. By adjusting \u03c3, the degree of sparsity can be controlled. A smaller \u03c3 results in a higher penalty for non-zero $\u03b4_i$, promoting sparsity."}, {"title": "3.4 Algorithmic Implementation", "content": "The implementation of ASLO involves iteratively adjusting and optimizing the perturbations. The steps are as follows:\nThe objective function J integrates the predictive accuracy of the model with the sparsity of the perturbations. It is defined as:\n$J(\\delta; x, y, \\theta, \\lambda, \\sigma) = L(f(x + \\delta, \\theta), y) + \\lambda l_0 (\\delta, \\sigma)$\nwhere L measures the misclassification error, and $\u03bbl_0$ penalises non-sparse perturbations. Specifically, $L(f(x + \u03b4, \u03b8), y)$ represents the loss function that measures how well the perturbed input x + d fools the model f. The term $\u03bbl_0 (\u03b4, \u03c3)$ adds a penalty for non-sparse perturbations, with A controlling the trade-off between attack efficacy and perturbation sparsity."}, {"title": "3.5 Example and Detailed Explanation", "content": "To better understand how ASLO works, consider the following example:\nAssume we have a time series input x and a model f. Our goal is to generate a perturbation & that misleads the model while altering as few data points as possible.\n1. Initialization: We start with an initial $\u03c3^{(0)}$ and set decay and increase rates na and ni. These parameters control how we adjust the sparsity of the perturbations during the optimization process.\n2. Perturbation Generation: For each iteration t, we generate a perturbation $\u03b4^{(t)}$ for the input samples using the current value of $\u03c3^{(t)}$. The perturbation is calculated to maximize the likelihood of misclassification while maintaining minimal changes to the original input.\n3. Evaluation and Adjustment: We evaluate the model's performance with the perturbed input x+f(t) and compute the objective function $J(\u03b4^{(t)}; x, y, \u03b8, \u03bb, \u03c3^{(t)})$. If the perturbation improves model performance (i.e., increases the likelihood of misclassification) and the objective function Jis less than a predefined threshold J*, we reduce o to increase the sparsity of the perturbation:\n$\u03c3^{(t+1)} = N_d. \u03c3^{(t)}$\nOtherwise, we increase o to explore more effective perturbations:\n$\u03c3^{(t+1)} = N_i\u00b7 \u03c3^{(t)}$\n4. Optimization: Using gradient descent, we optimize the objective function J to find the perturbation & that effectively misleads the model while ensuring sparsity. The objective function combines the model's predictive accuracy and the sparsity of the perturbations:\n$J(\\delta; x, y, \\theta, \\lambda, \\sigma) = L(f(x + \\delta, \\theta), y) + \\lambda l_0 (\\delta, \\sigma)$"}, {"title": "4 Experiment", "content": "This section delineates our experimental setup, which is bifurcated into two principal parts. Initially, we scrutinize the efficacy of the Adaptive Sparse (AS) regularization method under controlled conditions. Subsequently, we extend the application of the AS regularization method to prevalent adversarial attack techniques, facilitating a comparative analysis of its effectiveness."}, {"title": "4.1 Dataset", "content": "Our investigation leverages the UCR Archive-2018 dataset, encompassing 128 distinct time series datasets across diverse sectors, including healthcare, agriculture, finance, and engineering. This compilation served as a solid foundation for our analysis, with each dataset partitioned into training and testing subsets to ensure a comprehensive evaluation."}, {"title": "4.2 Part 1: Evaluation of the Adaptive Sparse Regularization Method", "content": "The initial phase of our experimental study focuses on evaluating the Adaptive Sparse LO (ASLO) regularization method. This novel approach aims to enhance model robustness by incorporating sparsity into adversarial perturbations, potentially reducing their detectability while maintaining or improving the attack's effectiveness.\nExperimental Setup To rigorously evaluate the ASLO regularization method, we employ a controlled variable approach that ensures uniformity in the adversarial attack framework while varying the regularization technique. This approach allows for a direct comparison of the impact of ASLO against traditional regularization methods, such as L1 and L2 regularization.\nThe experimental setup involves three key components:"}, {"title": "4.3 Part 2: Application of ASLO Regularization Across Common Adversarial Attack Methods", "content": "Given the iterative nature of our innovative ASLO regularization approach, our experiments were specifically tailored to include adversarial attack methods that generate perturbations through iterative processes. This focus ensures a direct"}, {"title": "5 Experimental Analysis", "content": null}, {"title": "5.1 Part 1: Evaluation of the Adaptive Sparse Regularization Method", "content": "Key Observations and Discussion In this section, we comprehensively evaluate the Adaptive Sparse LO (ASLO) regularization method. The focus is on its ability to produce significantly sparse perturbations, ensure a high Adversarial Success Rate (ASR), and minimize the perturbation distance required for successful attacks. We compare ASLO with traditional regularization methods, such as L1 and L2, emphasizing computational efficiency and ASLO's dynamic adjustment mechanism."}, {"title": "5.2 Part 2: Application of AS Regularization Across Common Adversarial Attack Methods", "content": "Testing the application of AS regularization across various adversarial attack methods, including GM_PGD, GM_PGD_L2, CW, CW_L2, reveals several insights. The use of AS_GM_PGD and AS_CW methods does not significantly impact the Attack Success Rate (ASR) compared to their respective non-AS regularized counterparts (GM_PGD and CW). However, it considerably reduces the required success distance, as depicted in Figure 1. The upper graph illustrates the disparities in ASR between the attack methods, and the lower graph"}, {"title": "6 Conclusions and Future Work", "content": "In this study, we have demonstrated an effective adaptive adversarial attack method for time series classification (TSC) models, with a focus on the application of Adaptive Sparse (AS) regularisation techniques. This approach not only successfully showed the capability to maintain high attack success rates while reducing perturbations, but also highlighted the limitations of current defence mechanisms against highly adaptive attacks. This underscores the need for future security strategies to consider the adaptiveness and intelligence of adversarial behaviours.\nOur findings prompt a reevaluation of the static nature of existing defence strategies, emphasizing the necessity of developing defence mechanisms that can adapt and respond quickly to the evolution of adversarial tactics. Future work will focus on exploring new regularisation techniques, self-learning and meta-learning algorithms, and other machine-learning paradigms to enhance model robustness and resilience against various attacks. Additionally, the application of AS regularization techniques across different attack methods and model architectures will be investigated to deepen our understanding of their versatility and limitations, and to explore integrating these techniques into TSC models to improve model performance."}]}