{"title": "VARIATIONAL BAYES GAUSSIAN SPLATTING", "authors": ["Toon Van de Maele", "Ozan \u00c7atal", "Alexander Tschantz", "Christopher L. Buckley", "Tim Verbelen"], "abstract": "Recently, 3D Gaussian Splatting has emerged as a promising approach for modeling 3D scenes using mixtures of Gaussians. The predominant optimization method for these models relies on backpropagating gradients through a differentiable rendering pipeline, which struggles with catastrophic forgetting when dealing with continuous streams of data. To address this limitation, we propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that frames training a Gaussian splat as variational inference over model parameters. By leveraging the conjugacy properties of multivariate Gaussians, we derive a closed-form variational update rule, allowing efficient updates from partial, sequential observations without the need for replay buffers. Our experiments show that VBGS not only matches state-of-the-art performance on static datasets, but also enables continual learning from sequentially streamed 2D and 3D data, drastically improving performance in this setting.", "sections": [{"title": "1 INTRODUCTION", "content": "Representing 3D scene information is a long-standing challenge for robotics and computer vision (\u00d6zye\u015fil et al., 2017). A recent breakthrough in this domain relies on representing the scene as a radiance field, i.e., using neural radiance fields (Mildenhall et al., 2020). Recently, Gaussian Splatting (Kerbl et al., 2023) has demonstrated the effectiveness of mixture models for 3D scene representation, as highlighted by a surge of subsequent research (see Chen and Wang (2024) for a survey). This approach leverages the ability of Gaussians to represent physical space as a collection of ellipsoids.\n\nThe dominant method for optimizing these models involves backpropagating gradients through a differentiable renderer with respect to the parameters of the mixture model. However, in many real-world applications, such as autonomous navigation, data is continuously streamed and must be processed sequentially. Traditional backpropagation-based methods are prone to catastrophic forgetting (French, 1999), leading to performance degradation as new data overwrites old knowledge. To mitigate this, replay buffers are often employed to retain and retrain on older data (Matsuki et al., 2024), which can be computationally expensive and memory intensive."}, {"title": "2 RELATED WORK", "content": "3D Gaussian Splatting (3DGS): Recently, 3DGS (Kerbl et al., 2023) introduced a novel approach for learning the structure of the world directly from image data by representing the radiance field as a mixture of Gaussians. This method optimizes the parameters of the Gaussian mixture model by backpropagating gradients through a differentiable rendering pipeline. The Gaussians serve as ellipsoid shape primitives for scene reconstruction, with each component associated with deterministic, learned features such as opacity and color. This approach has triggered significant advances in novel view synthesis and sparked interest in many applications across robotics, virtual/augmented reality, and interactive media (Fei et al., 2024; Chen and Wang, 2024).\n\nWhile our method also represents scenes as a mixture of Gaussians, it diverges from 3DGS by modeling a distribution over all features and parameters. Rather than relying on gradient-based optimization through differentiable rendering, we frame the learning process as variational inference. This approach allows us to perform continual learning through closed-form updates, enabling more efficient and robust handling of sequential data.\n\nImage representations: Gaussian mixture models have also been used for representing image data, particularly for data efficiency and compression (Verhack et al., 2020). Steered Mixture-of-Experts (SMOE) models, for instance, typically estimate parameters using the Expectation-Maximization (EM) algorithm, although some work has explored gradient descent optimization (Bochinski et al., 2018).\n\nOur approach can be seen as an SMoE model on 2D image data, but again, instead of relying on EM or gradient descent, we propose variational Bayes parameter updates for more robust and efficient parameter estimation.\n\nContinual learning aims to develop models that can adapt to new tasks from a continuous datastream without forgetting previously learned knowledge (Wang et al., 2024). Backpropagation-based methods, such as 3DGS, face significant challenges in this area due to catastrophic forgetting (French, 1999), where the model's performance on prior data deteriorates as it trains on new. This issue is particularly evident in real-world scenarios, such as localization and mapping, where data is streamed (Matsuki et al., 2024; Keetha et al., 2024). The common mitigation strategy involves maintaining a replay buffer of frames to revisit past data during updates (Sucar et al., 2021; Fu et al., 2024).\n\nIn contrast, Bayesian approaches offer a more elegant solution for online learning, especially when dealing with conjugate models, allowing for exact Bayesian inference in a sequential setting (Jones et al., 2024). We leverage this property to update Gaussian splats continuously as new data arrives, eliminating the need for replay buffers."}, {"title": "3 METHOD", "content": "VBGS relies on the conjugate properties of exponential family distributions. This is well suited for variational inference, as we can derive closed-form update rules for inferring the (approximate) posterior. We first write down the functional form of these distributions and then describe the particular generative model for representing space and color.\n\nIf the likelihood distribution is part of the exponential family, it can be written down as:\n\n$p(x|\\theta) = \\phi(x) \\exp (\\theta \\cdot T(x) \u2013 A(\\theta)),$ (1)\n\nwhere $\\theta$ are the natural parameters of the distribution, $T(x)$ is the sufficient statistic, $A(\\theta)$ is the log partition function, and $\\phi(x)$ is the measure function. This likelihood has a conjugate prior of the following form:\n\n$p(\\theta| \\eta_o, \\nu_o) = \\frac{1}{Z(\\eta_o, \\nu_o)} \\exp (\\eta_o \\cdot \\theta - \\nu_o \\cdot A(\\theta)),$ (2)\n\nwhere $Z(\\eta_o, \\nu_o)$ is the normalizing term. This distribution is parameterized by its natural parameters $(\\eta_o, \\nu_o)$. The posterior is then of the same functional form as the prior, whose natural parameters can be calculated as a function of the sufficient statistics of the data and the natural parameters of the prior, i.e. $(\\eta_o + \\Sigma T(x), \\nu_o + \\Sigma 1)$ (Murphy, 2013).\n\n3.1 THE GENERATIVE MODEL\n\nThe generative model considered is a mixture model with $K$ components, each characterized by two conditionally independent modalities: spatial position (s) and color (c). For 2D images, s represents the pixel location in terms of row and column coordinates (s \u2208 $\\mathbb{R}^2$), while for 3D data, it corresponds to Cartesian coordinates (s \u2208 $\\mathbb{R}^3$). The color component represents RGB values (c\u2208 $\\mathbb{R}^3$) for both 2D and 3D data. Both $s_k$, and $c_k$ are modeled as multivariate Normal distributions with parameters $(\\mu_{s,k}, \\Sigma_{s,k})$, and $(\\mu_{c,k}, \\Sigma_{c,k})$ respectively, and the components have a mixture weight z. The full generative model is visualized as a Bayesian network in Figure 1 and is factorized as:\n\n$p(s, c, z, \\mu_s, \\Sigma_s, \\mu_c, \\Sigma_c, \\pi) = (\\prod_{n=1}^N p(s_n|z_n, \\mu_{s}, \\Sigma_{s})p(c_n|z_n, \\mu_{c}, \\Sigma_{c})p(z_n|\\pi))$ (3)\n\n$= (\\prod_{k=1}^K p(\\mu_{k,s}, \\Sigma_{k,s})p(\\mu_{k,c}, \\Sigma_{k,c}))p(\\pi).$ (4)"}, {"title": "3.2 COORDINATE ASCENT VARIATIONAL INFERENCE", "content": "The joint distribution in Equation (4) is factorized into two main components: one representing the likelihoods of N data points ($s_n$, $c_n$) given their assignments, and another representing the priors over the mixture model parameters. The mixture components over space $p(s|z, \\mu_s, \\Sigma_s)$ are conditionally independent from the component over color $p(c|z, \\mu_c, \\Sigma_c)$, given mixture component z. The following distributions parameterize these random variables:\n\n$z_n \\sim Cat(\\pi)$ (5)\n\n$s_n|z_n = k \\sim MVN(\\mu_{k,s}, \\Sigma_{k,s})$ (6)\n\n$c_n|z_n = k \\sim MVN(\\mu_{k,c}, \\Sigma_{k,c})$ (7)\n\nThe parameters of these distributions are also modeled as random variables. Treating the parameters as latent random variables allows us to cast learning as inference using the appropriate conjugate priors. The conjugate prior to a multivariate Normal (MVN) is a Normal Inverse Wishart (NIW) distribution, and the Dirichlet distribution is the conjugate prior to a categorical (Cat) distribution:\n\n$\\mu_{k,s}, \\Sigma_{k,s} \\sim NIW(m_{o,s}, \\kappa_{o,s}, V_{o,s}, \\nu_{o,s})$ (8)\n\n$\\mu_{k,c}, \\Sigma_{k,c} \\sim NIW(m_{o,c}, \\kappa_{o,c}, V_{o,c}, \\nu_{o,c})$ (9)\n\n$\\pi \\sim Dirichlet(\\alpha_o),$ (10)\n\nTo estimate the parameters, we infer their posterior distribution. However, as computing this posterior is intractable, we resort to variational inference (Jordan et al., 1998). We use a mean-field approximation to make inference tractable, which assumes that the variational posterior factorizes across the latent variables. This factorization allows for efficient coordinate ascent updates for each variable separately. Specifically, the variational distribution q is decomposed as:\n\n$q(Z, \\mu_s, \\Sigma_s, \\mu_c, \\Sigma_c, \\pi) = (\\prod_{n=1}^N q(z_n)) (\\prod_{k=1}^K q(\\mu_{k,s}, \\Sigma_{k,s})) (\\prod_{k=1}^K q(\\mu_{k,c}, \\Sigma_{k,c}))q(\\pi),$ (11)\n\nWe select the distributions of the approximate posteriors to be from the same family as their corresponding priors. For the color variable c, we model the mean by a Normal distribution but keep the covariance fixed using a Delta distribution. This assures that the mixture components commit to a particular color and do not blend multiple neighboring colors in a single component.\n\n$q(z_n) = Cat(\\gamma_n)$ (12)\n\n$q(\\Sigma_{k,c}) = Delta(\\epsilon I),$ (15)\n\n$q(\\mu_{k,s}, \\Sigma_{k,s}) = NIW(m_{t,s}, \\kappa_{t,s}, V_{t,s}, \\nu_{t,s})$ (13)\n\n$q(\\mu_{k,c}) = Normal(m_{t,c}, \\kappa^{-1}_{t,c} \\epsilon I)$ (16)\n\n$q(\\pi) = Dirichlet(\\alpha_t)$ (14)\n\nwhere the subscript t indicates the parameters at timestep t, and $\\epsilon$ is a chosen hyperparameter.\n\nImages can be generated using the mixture model by computing the expected value of the color, conditioned on a spatial coordinate ($E_{p(c|s)} [c]$). For 3D rendering, we use the renderer from Kerbl et al. (2023), where the spatial component is first projected onto the image plane using the camera parameters, and the estimated depth is used to deal with occlusion. For more details, see Appendix B.\n\nTo estimate the posterior over the model parameters, we maximize the Evidence Lower Bound (ELBO) L with respect to the variational parameters (Jordan et al., 1998):\n\n$L = D_{KL} [q(z, \\mu_s, \\Sigma_s, \\mu_c, \\Sigma_c, \\pi) || p(z, \\mu_s, \\Sigma_s, \\mu_c, \\Sigma_c, \\pi|s, c)],$ (17)\n\nThis is done using coordinate ascent variational inference (CAVI) (Beal, 2003; Bishop, 2006; Blei et al., 2017) which contains two distinct steps, that are iteratively executed to optimize the model parameters. It parallels the well-known expectation maximization (EM) algorithm: the first step"}, {"title": "3.3 CONTINUAL UPDATES", "content": "computes the expectation over assignments q(z) for each data point. Instead of computing the maximum likelihood estimate as is done in EM, in the second step we maximize the variational parameters of the posterior over model parameters. More in detail, in the first step the assignment for each data point ($s_n$, $c_n$) is computed by deriving the ELBO with respect to $q(z_n)$.\n\n$\\log q(z_n = k) = \\log \\gamma_{nk} = E_{q(\\mu_{k,s},\\Sigma_{k,s})} [\\log p(s_n|\\mu_{k,s}, \\Sigma_{k,s})] $ (18)\n\n$+ E_{q(\\mu_{k,c},\\Sigma_{k,c})} [\\log p(c_n|\\mu_{k,c}, \\Sigma_{k,c})] $ (19)\n\n$+ E_{q(\\pi)} [\\log p(\\pi)] \u2013 \\log Z_n,$ (20)\n\nwhere $Z_n$ is a normalizing term, i.e., if $\\hat{\\gamma}_n$ is the unnormalized logit, we can find the parameters of the categorical distribution as $\\gamma_n = \\frac{\\hat{\\gamma}_n}{\\Sigma \\hat{\\gamma}_n}$.\n\nIn the second step, we compute the update of the approximate posteriors over model parameters. To this end, we derive the ELBO with respect to the natural parameters of the distributions. If the distribution is conjugate to the likelihood, we acquire updates of the following parametric form:\n\n$\\eta_k = \\eta_{o,k} + \\Sigma \\gamma_{k,n}T(x_n)$ (21)\n\n$\\nu_k = \\nu_{o,k} + \\Sigma \\gamma_{k,n},$ (22)\n\nwhere $\\eta$ and $\\nu$ are the natural parameters of the distribution over the likelihood's natural parameters, and $T(x_n)$ are the sufficient statistics of the data $x_n$.\n\nFor the approximate posterior $q(\\mu_{s,k}, \\Sigma_{s,k})$ over the parameters of the spatial likelihood, the sufficient statistics are given by $T(s_n) = (s_n, s_n \\cdot s^T_n)$. The NIW conjugate prior consists of a Normal distribution over the mean and an inverse Wishart distribution over the covariance matrix. Hence, for each of the prior's natural parameters, it has two values: $\\eta_{o,s} = (\\kappa_{o,s} \\cdot m_{o,s}, V_{o,s} + \\kappa_{o,s} \\cdot m_{o,s} \\cdot m^T_{o,s})$ and $\\nu_{o,s} = (\\kappa_{o,s}, \\nu_{o,s} + D_s + 1)$. Here, $m_{o,s}$ is the mean of the Normal distribution over the mean, $\\kappa_o$ is the concentration parameter over the mean, $\\nu_{o,s}$ indicates the degrees of freedom, $V_{o,s}$ the inverse scale matrix of the Wishart distribution, and D, the dimensionality of the MVN.\n\nFor the approximate posterior $q(\\mu_{c,k}, \\Sigma_{c,k})$ over the color likelihood, the sufficient statistics are again given by: $T(c_n) = (c_n, c_n \\cdot c^T_n)$. The prior is parameterized similarly as the NIW over s: $\\eta_{o,c} = (\\kappa_{o,c} \\cdot m_{o,c}, V_{o,c} + \\kappa_{o,c} \\cdot m_{o,c} \\cdot m^T_{o,c})$ and $\\nu_{o,c} = (\\kappa_{o,c}, \\nu_{o,c} + D_c + 1)$. However, as we model the prior over $\\Sigma_{k,c}$ as a delta distribution, we keep the values for $\\eta_{k,c}$ and $\\nu_{k,c}$ fixed.\n\nFinally, the conjugate prior for the approximate posterior q(\u03c0) over the component assignment likelihood z is a Dirichlet distribution. Here, sufficient statistics are given by T(x) = 1, and the prior is parameterized by the natural parameter $\\eta_{o,z} \\alpha$.\n\nThe proposed method supports continual learning because the parameters of each component are updated by aggregating the prior's natural parameters with the data's sufficient statistics via assignments q(z). This iterative update process is order-invariant, allowing the model to adapt without forgetting past knowledge. Components without recent data assignments revert to their prior values, preserving flexibility in the model. Crucially, assignments q(z) are always computed with respect to the initial posterior over parameters $q(\\mu_s, \\Sigma_s, \\mu_c, \\Sigma_c, \\pi)$. This ensures that components without prior assignments can still be used to model the data.\n\nConcretely, when data is processed sequentially, at each time point t, a batch of $D_t$ of data points ($s_n$, $c_n$) is available, and the variational posterior over model parameters is updated. For each of these data points, the assignments $\\gamma_{k,n}$ can be calculated similarly to Equation (18). We can rewrite the update steps from Equations (21) and (22), in a streaming way for T timesteps:\n\n$\\eta_k^T = \\eta_{o,k} + \\Sigma_{t=1}^T \\Sigma_{x_n \\in D_t} \\gamma_{k,n}T(x_n)$ (23)\n\n$\\nu_k = \\nu_{o,k} + \\Sigma_{t=1}^T \\Sigma_{x_n \\in D_t} \\gamma_{k,n}$ (24)\n\nHence, we can write an iterative update rule for the natural parameters at timestep t as a function of the natural parameters at t 1:"}, {"title": "3.4 COMPONENT REASSIGNMENTS", "content": "$\\eta_{t,k} = \\eta_{t-1,k} + \\Sigma_{x_n \\in D_t} \\gamma_{k,n}T(x_n)$ (25)\n\n$\\nu_{t,k} = \\nu_{t-1,k} + \\Sigma_{x_n \\in D_t} \\gamma_{k,n}$ (26)\n\nHere, $x_n$ is a placeholder for the particular data, e.g., when updating $q(\\mu_{s,k}, \\Sigma_{s,k})$, this would be $s_n$. At t = 0, the prior values for the natural parameters are used.\n\nNote that when $\\gamma_{k,n}$ is calculated using the initial parameterization of the variational posterior over parameters, applying these continual updates is identical to processing all the data in a single batch, avoiding the problem of catastrophic forgetting.\n\nIn many continual learning settings, the data statistics are not known in advance. The uniform initialization of the model might not adequately cover the data space, leading to a few components aggregating all the assignments with many components remaining unused. For example, consider a room where the largest object density is at the walls, while the center mainly consists of empty space. To mitigate this problem, we introduce a heuristic for reassigning the initial location of a components to data points which the model does not explain well.\n\nThe means of n components for both space and color are replaced by the location and color of n data points, sampled proportional to the negative ELBO under the current model. We choose n as a fraction (5%) of these unused components. A component is unused if the concentration parameters ($\\alpha_k$) of the prior over the mixture weights are equal to their prior value. By choosing n as a fraction of the available components, more reassignments occur for the first frames when the object/scene is not yet known, and fewer near the end. Additionally, this ensures we do not reassign components already used at a different location. After updating the component means for these \u201cinitial\u201d components, we can run CAVI as described in Section 3.2."}, {"title": "4 RESULTS", "content": "We evaluate our approach by benchmarking it against backpropagating gradients through a differentiable renderer. In particular, we compare the following models on the Tiny ImageNet (Le and Yang, 2015), Blender 3D (Mildenhall et al., 2020) and Habitat rooms (Savva et al., 2019) datasets:\n\nVBGS (Ours): We consider a variant of VBGS where the means of the initial posteriors ($q(\\mu_s, \\Sigma_s, \\mu_c, \\Sigma_c)$) are initialized on sampled points from the normalized data set (Data Init), as well as randomly initialized ($m_{k,s} \\sim U[-1, 1], m_{k,c} \\sim \\delta(0)$) (Random Init). For models with random initialization, data normalization is performed using estimated statistics; see Appendix C for further details.\n\nGradient: In 3DGS (Kerbl et al., 2023), the parameters are directly optimized using stochastic gradient descent on a weighted image reconstruction loss (($1 \u2013 \\lambda$) MSE + ASSIM). In the case of image data, $\\lambda$ is set to 0; in the case of 3D data $\\lambda$, it is set to 0.2. We use spherical harmonics with no degrees of freedom, i.e., the specular reflections are not modeled. In order to be able to compare performance w.r.t. model size, we also do not do densification or shrinking (Kerbl et al., 2023) (for these results see Appendix D.2). When optimizing for images, we use a fixed camera pose at identity and keep the z-coordinate of the Gaussians fixed at a value of 1. Similar to the VBGS approach, we also consider a variant where the means of the Gaussian components are randomly initialized (Random Init), and on sampled points from the dataset (Data Init).\n\n4.1 IMAGES\n\nWe first evaluate performance on the Tiny ImageNet test set consisting of 10k images. Reconstruction accuracy, measured in PSNR (dB), is evaluated across varying numbers of components (Figure 2a). Our results indicate that VBGS achieves reconstruction errors comparable to the gradient-based approach. We then evaluate the models in a continual learning setting, where data patches are sequentially observed and processed (Figure 2b). While VBGS maintains consistent reconstruction quality across all observed patches, the gradient-based method disproportionately focuses on the most"}, {"title": "4.2 BLENDER 3D OBJECTS", "content": "recent patch. The PSNR over timesteps is shown in Figure 3a. VBGS converges to the same accuracy as training on the static dataset, whereas Gradient degrades due to catastrophic forgetting.\n\nWe further compared the computational efficiency by measuring the wall-clock time required for the Gradient approach to reach the performance level of VBGS after a single update step. This was computed over all images of the Tiny ImageNet validation set. We observe that VBGS is significantly (t-test, p = 0) faster in wall clock time (0.03 \u00b1 0.03 seconds) compared to Gradient (0.05 \u00b1 0.02 seconds).\n\nTo assess performance in a continual learning setting, we divide each image into 8x8 patches, feeding the model one patch at a time (Figure 2b). VBGS performs a single update per patch, whereas the Gradient approach performs 100 training steps per patch with a learning rate of 0.1. Figure 3a shows how the reconstruction PSNR evolves at each time step over the tiny ImageNet test set for the model with capacity 10K components. It's important to note that the reconstruction error of VBGS after observing all the patches converges to the same value as observing the whole image at once, as the inferred posterior ends up being identical. Even though the gradient approach, when observing all data at once, achieves much higher PSNR, when the data is fed in continuously, it does not achieve that performance in the continual setting, as it always focuses on the latest observed patch (see Figure 2b).\n\nNext, we evaluate VBGS on 3D objects from the Blender dataset (Mildenhall et al., 2020). Model parameters are inferred using 200 frames which include depth information. VBGS is trained on the 3D point cloud, which is acquired by transforming the RGBD frame to a shared reference frame. In contrast, the gradient-based approach is optimized using multi-view image reconstruction. We evaluate reconstruction performance on 100 frames from the validation set. The results, measured as PSNR, for a model with a capacity of 100K components are shown in Table 1. Our results show improved performance for both methods when components are initialized from data rather than randomly. VBGS achieves performance similar to the gradient-based approach under Data Init conditions. In random initialization, our approach outperforms the gradient-based approach except for the \"ship\" object. For a more in-depth analysis where we vary the number of components, see Appendix D.1.\n\nNovel view predictions for the eight blender objects are shown in Figure 4a. These renders are generated using a VBGS with 100K components, observed from a camera pose selected from the validation set. Note that these are rendered on a white background, and only the 3D object is modeled by the VBGS. Figure 4b shows the reconstruction performance as a function of the number of available components. It can be observed that for lower component regimes, VBGS renders patches of ellipsoids, while the gradient approach fills the areas more easily. We attribute this to a strong prior over the covariance shape encoded in the Wishart hyperparameters."}, {"title": "4.3 HABITAT ROOMS", "content": "Here,$\\eta_{t,k} = \\eta_{t-1,k} + \\Sigma_{x_n \\in D_t} \\gamma_{k,n}T(x_n)$ (25)\n\n$\\nu_{t,k} = \\nu_{t-1,k} + \\Sigma_{x_n \\in D_t} \\gamma_{k,n}$ (26)\n\nFinally, we evaluate on a dataset of 3D rooms, which is relevant for e.g. autonomous navigation for robotics in unseen environments. We consider three rooms from the Habitat test suite (\"Van Gogh Room\u201d, \u201cApartment 1\u201d and \u201cSkokloster Castle\u201d) (Savva et al., 2019). The model parameters are inferred using 200 randomly sampled views from each room using the preprocessing pipeline of (Wang et al., 2023). On top of the models from the previous sections, we now also evaluate the model with component reassignments as detailed in Section 3.4.\n\nFigure 5a shows the qualitative performance for the various considered models. Similar to previous experiments, we observe that initializing using the data yields the best performance. In contrast to the earlier experiments, random initialization does not capture well the structure of the room. This is mainly due to the randomly sampled initial means of the mixture model not covering the data well. For the Gradient approach, there are too few components that provide a gradient signal to optimize the observed view. In contrast, for VBGS, a single component might always be closer to the data than others, aggregating all the assignments. This is further evidenced by the noticeable performance"}, {"title": "5 DISCUSSION AND CONCLUSION", "content": "improvement when using the reassign mechanism on top of VBGS. Quantitative results for all models can be found in appendix D.3\n\nWe also evaluate the models in a continual setting in Figure 5b, where 200 frames (RBGD + pose) of the environment are observed sequentially. Note that these frames are randomly sampled from the environment and are not captured using a trajectory through the room. We observe that the gradient approach does not integrate the information well, even though the views are randomly sampled and cover the room. VBGS, without reassignments, does integrate the information and reaches performance on par with the Gradient trained in the non-continual setting. Finally, we observe that adding reassignments drastically improves performance on all three rooms.\n\nIn this paper, we introduced a novel approach to optimizing Gaussian splats using variational Bayes. Our approach achieves comparable performance to backpropagation-based methods on 2D and 3D datasets, while offering key advantages for scenarios involving continual learning from streaming data.\n\nOne limitation of VBGS compared to 3D Gaussian Splatting (3DGS) is its reliance on RGBD data, as opposed to optimizing directly on RGB projections. For many use cases we envision, such as robot navigation, depth information is often readily available from stereo vision, lidar or other sensing technologies. Furthermore, most 3DGS approaches still rely on pre-computed camera parameters obtained from structure-from-motion algorithms (\u00d6zye\u015fil et al., 2017), which implicitly also provide depth information through triangulation. One could also use a pretrained neural network that predicts depth from monocular RGB data (Yang et al., 2024), as evidenced by Fu et al. (2024).\n\nAdditionally, 3DGS dynamically adjusts the model size by growing and shrinking as needed, a feature that we have not yet incorporated. In future work, we aim to explore principled approaches for dynamic model sizing, leveraging model evidence (Friston et al., 2023) to guide this process.\n\nWhile VBGS requires only a single update step per observation, this step is computationally more expensive than a single gradient descent iteration in backpropagation. For extremely large datasets that do not fit into memory, VBGS may need to process data in distinct update steps, where stochastic gradient descent (SGD) might offer better efficiency. However, in streaming or partial data settings, VBGS excels by allowing updates without replay buffers, opening avenues for active data selection, a potential future research direction. Investigating active learning mechanisms, particularly in robotic SLAM or autonomous systems, could optimize data usage, ensuring that only the most informative observations are processed.\n\nThe continual learning capabilities of VBGS make it especially suited for applications requiring real-time adaptation and learning, such as autonomous navigation, augmented reality, and robotics. By continuously integrating new information without the need for replay buffers, VBGS reduces the computational burden typically associated with processing large-scale data streams. Furthermore, the variational posterior over model parameters enables parameter-based exploration (Schwartenbeck et al., 2013), allowing agents to efficiently explore and build a model of their environment in real-time. This suggests exciting opportunities for VBGS to contribute to adaptive, real-time learning systems in various real-world domains."}, {"title": "6 CODE AVAILABILITY", "content": "The code for using VBGS is available in the vbgs repository, which can be found at the following link: https://github.com/VersesTech/vbgs."}]}