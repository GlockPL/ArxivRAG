{"title": "MICROFLOW: AN EFFICIENT RUST-BASED INFERENCE ENGINE FOR TINYML", "authors": ["Matteo Carnelos", "Francesco Pasti", "Nicola Bellotto"], "abstract": "MicroFlow is an open-source TinyML framework for the deployment of Neural Networks (NNs) on embedded systems using the Rust programming language, specifically designed for efficiency and robustness, which is suitable for applications in critical environments. To achieve these objectives, MicroFlow employs a compiler-based inference engine approach, coupled with Rust's memory safety and features. The proposed solution enables the successful deployment of NNs on highly resource-constrained devices, including bare-metal 8-bit microcontrollers with only 2kB of RAM. Furthermore, MicroFlow is able to use less Flash and RAM memory than other state-of-the-art solutions for deploying NN reference models (i.e. wake-word and person detection). It can also achieve faster inference compared to existing engines on medium-size NNs, and similar performance on bigger ones. The experimental results prove the efficiency and suitability of MicroFlow for the deployment of TinyML models in critical environments where resources are particularly limited.", "sections": [{"title": "1 Introduction", "content": "TinyML is a field of Machine Learning (ML) that focuses on small and low-power embedded devices [1]. The aim of TinyML is to enable these devices to perform intelligent tasks without relying on cloud-based servers or high-performance computing systems. The field has gained popularity in recent years due to the increasing demand for smart devices capable of performing intelligent real-time tasks without the need of cloud resources, which are usually power consuming and potentially unsafe regarding data security and privacy. The increasing emergence of Internet of Things (IoT) devices in domestic and industrial environments also contributed significantly to the field, making it possible to achieve even more integrated applications [2].\nA significant advantage of TinyML is its ability to operate on low-power devices, making it ideal for resource-constrained environments. That means devices can operate on batteries or solar power, enabling access to technology in areas with limited infrastructure. As of 2022, the global Micro-Controller Unit (MCU) market was valued at USD 25.48 billion and is expected to expand at a Compound Annual Growth Rate (CAGR) of 11.2% from 2023 to 2030 [3]. With such a large market, combined with the rise in popularity of AI applications [4], the potential for TinyML-powered devices is particularly high, with a lot of big companies investing in this research field [5]. Moreover, while traditional ML models require significant computing power and hardware resources, which can be expensive, TinyML models can run on low-cost MCU, making this technology even more accessible [6]. These feature are particularly important in developing countries [7] where access to electricity and computing resources can be a significant issue.\nOne of the major challenges when it comes to developing applications for embedded devices is their limited computational power. Traditional ML models and inference engines are often not suitable for IoT applications, since they require a significant amount of computing and storage resources. TinyML solutions instead run on MCUs with limited memory and processing capabilities, but this limits the deployment of complex NN architectures. Therefore, both inference engines and NN models need to be optimized to run on resource-constrained hardware. The lack of standardization in the TinyML ecosystem can pose further challenges for developers. With the wide range of hardware platforms available, it can be difficult to ensure compatibility and interoperability between different components."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 TinyML Concepts", "content": "TinyML refers to the deployment of ML models on small and low-power embedded devices. One of the best-known and most widely used TinyML applications is wake-word detection, also called keyword spotting or hotword [12]. This involves training a NN to identify a specific sound or phrase, such as \u201cHey Siri\u201d or \u201cOK Google\u201d, that triggers a device to begin listening for user commands. The trained model is then compressed and deployed to edge devices that will perform always-on inference. By locally doing wake-word detection on the device, latency and privacy are greatly improved. This type of application is commonly used in voice assistants and smart speakers developed by companies such as Amazon, Apple, Google, and others. Further TinyML applications include activity recognition [13], object detection [14], predictive maintenance [15], environmental monitoring [16], and many more.\nA typical ML application involves two main phases: training and inference. Both of them can be computationally intensive and require significant processing power. As the training phase is typically the most expensive [17], in TinyML it is typically carried out on a host system with access to high-performance computing resources, such as Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs), while the inference phase is performed by an optimized ML engine that runs on the MCU [18].\nML models demand a considerable amount of storage memory and consume a substantial amount of CPU cycles for computation. To overcome these issue, techniques such as quantization and pruning are employed to reduce their"}, {"title": "2.2 Programming Languages for TinyML Applications", "content": "A programming languages for embedded systems must provide low-level control of hardware resources, real-time responsiveness, and efficient memory management. Another crucial aspect is memory safety, which refers to the protection of a program's memory from errors such as buffer overflows, use-after-free, and dangling pointers. In recent years, memory safety has become a critical concern in software development, particularly for low-level programming languages. The Microsoft Security Response Center (MSRC) reported in 2019 that 70% of all security vulnerabilities were caused by memory safety issues [21]. Similarly, in 2020, a report from Google showed that 70% of all severe security bugs in Google Chromium were caused by memory safety problems [22]. However, typical programming languages used for TinyML can be either inefficient (e.g. Python) or memory unsafe (e.g. C/C++). These issues are particularly severe in bare-metal embedded devices, which lack the protection and abstractions provided by an operating system (OS).\nMemory-unsafe languages require programmers to manually manage memory usage. With these languages, programmers must carefully follow memory safety best practices and use static analysis tools to detect potential issues. Memory-safe languages, instead, provide features such as automatic memory management, safe pointer arithmetic, and bounds checking, which significantly reduce the risk of memory errors. However, memory-safe languages often rely on mechanisms such as garbage collection, which can introduce additional overhead and therefore impact performance [23]. It is therefore important to consider the trade-off between safety and performance when choosing a programming language for embedded systems.\nRust is a general-purpose programming language introduced by Mozilla in 2010 [24], which has gained popularity in recent years due to its unique features. It is increasingly being used in a variety of applications, including the Android Open Source Project (AOSP) [25] and the Linux Kernel [26]. Rust is designed to provide the efficiency and control of low-level languages like C and C++, while also prioritizing memory and thread safety. Instead of relying on garbage collection or manual memory management, Rust uses a system of \u201cownership\u201d and \u201cborrowing\" to ensure that memory is allocated and deallocated safely and efficiently. In this way, it is possible to write high-performance code without sacrificing safety or stability, making Rust a memory-safe programming language. Moreover, it offers several other benefits for embedded systems [27]. In particular, the ownership mechanism ensures at compile-time that peripherals and I/O lines are correctly configured and used in a mutually exclusive way. Its rich ecosystem and active community make it also possible to easily develop portable libraries for TinyML. Indeed there are many available and actively maintained libraries that offer non-standard support (i.e. targeting embedded platforms) and are useful for machine learning. The most popular example, mentioned later in the paper, is probably nalgebra\u00b2. All this motivated the choice of Rust for MicroFlow's implementation, offering robustness and efficiency for bare-metal TinyML.\""}, {"title": "2.3 Existing TinyML Frameworks", "content": "There has been a growing interest in TinyML in recent years, with a number of software frameworks for research and practical applications. Among these, TensorFlow Lite for Microcontrollers (TFLM) is a popular inference engine written in C++ and developed by Google [8, 28]. It is built on top of TensorFlow Lite [29] and is designed to be lightweight and efficient. TFLM supports a wide variety of ML models and many of the commonly used operations, such as convolution, pooling, and fully connected layers. Thanks to its popularity, the framework has inspired numerous projects that explore its potential further. One such project is MicroNets [30], which focuses on optimizing standard NNs to enable efficient inference using TFLM. However, TFLM uses an interpreter-based approach, which is less efficient and requires a significant amount of memory. Moreover, the framework supports only a limited number of 32-bit architectures.\nEmbedded Learning Library (ELL) is a C++ library by Microsoft designed for ML models on resource-constrained devices [31]. Unlike TFLM, ELL adopts a compiler-based approach, which makes it more efficient and suitable for small embedded devices. However, ELL is currently limited to a small number of ML algorithms and devices. While this may be sufficient for some applications, it is not suitable for developers who require more advanced or specialized models.\nARM-NN is an open-source C++ software library designed for accelerating ML development on ARM-based devices [9]. One of the key benefits of ARM-NN is its ability to provide optimized code for ARM MCUs, which are widely used in embedded systems. The library is designed to work with a variety of ML frameworks, including TensorFlow, Caffe, and PyTorch, making it a flexible option (although it does not support all the features or functionalities of those frameworks). Unfortunately, one obvious limitation is that it is only suitable for ARM-based devices.\nPlumerai is a startup company that specializes in developing ML tools and platforms for embedded systems [10]. One of Plumerai's key products is a inference engine that combines state-of-the-art techniques, such as Binarized Neural Networks (BNNs) [34], with a compiler-based approach to obtain a fast and efficient solution. However, the Plumerai software is written in C++ and it is currently proprietary (i.e. closed-source), which limits the ability to understand how the software works, make modifications or improvements, and address potential security vulnerabilities.\nuTensor is an extremely light-weight ML inference framework built on TensorFlow and optimized for ARM targets [32]. The framework is implemented in C++ and leverages the ARM Compute Library for optimized matrix operations, suitable only for ARM-based microcontrollers. However, it does not offer support for complex ML models or for those requiring more advanced optimization techniques.\nFinally, Tract is an inference engine written in Rust and developed by Sonos [33]. Unlike other Rust-based solutions that contain bindings to C/C++ libraries \u2013 effectively voiding the memory-safe guarantee of the language \u2013 Tract is completely self-contained and independent from non-Rust components. However, one of its main limitations is that it requires the Rust Standard Library, which is not available for bare-metal systems without an OS. This limitation affects the portability of Tract, making it suitable only for devices with significant processing power and memory resources."}, {"title": "3 System Design and Components", "content": ""}, {"title": "3.1 Design Goals", "content": "Following the review of existing frameworks, a number choices and principles have guided the design of MicroFlow, some of which are discussed next.\nPortability The embedded ecosystem is very fragmented, with many vendor-specific frameworks and architectures. It is therefore very difficult to develop a single software package that works efficiently and seamlessly on all the available devices. When using traditional programming languages, such as C or C++, the challenge is further accentuated because these languages are defined by common standards but implemented by different compilers, each with its own features and peculiarities. The Rust programming language, instead, provides a more convenient way to build portable software, since its ecosystem is more centralized. Moreover, the language is defined by the compiler itself, providing a single, official instance that takes care of building the code for the target architecture. Rust also comes with official built-in toolchain manager\u00b3 and package manager\u2074, enabling the creation of portable software without worrying about vendor-specific details."}, {"title": "3.2 Software Structure", "content": "The high-level structure of the proposed framework includes two major components: the MicroFlow Compiler, which resides on the host machine, and the MicroFlow Runtime, which resides on the target MCU. The goal is to delegate as much work as possible to the compiler, creating a lightweight runtime process that performs only the essential computations during program execution. The compiler is also responsible for analysing the model to determine the minimum amount of memory that must be statically allocated for runtime inference.\nEvery operator in MicroFlow derives from a template consisting of two parts: the parser and the kernel. The parser runs statically in the Compiler, preprocessing the model and preparing the weights for the runtime execution. The kernel runs on the Runtime component and takes care of the actual computation of the operator, propagating the input to the output. Each operator is isolated from the others, communicating only through input and output interfaces, and leaving no memory trace after its execution.\nAlthough MicroFlow is designed to support a multitude of NN operators, only the most commonly used are currently supported: FullyConnected, Conv2D, DepthwiseConv2D, AveragePool2D, Reshape, ReLU, ReLU6, and Softmax. With these operators, it is possible to implemenet a wide range of NN models, including Feedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs). These are further discussed in Sec. 5 and tested in Sec. 6. The scalable structure of MicroFlow enables future improvements and implementations of new operators."}, {"title": "3.3 MicroFlow Compiler", "content": "As discussed in the related work, typically there are two main methods used to develop inference engines for embedded systems:\n\u2022 In the interpreter-based approach, the inference engine dynamically parses and convert the model into machine instructions at runtime. This approach is generally more flexible and requires shorter compilation times, but it has some drawbacks. First of all, interpreting at runtime can introduce a significant performance overhead due to many additional operations, such as model parsing, type-checking, and memory allocation. The latter in particular introduces several risks such as memory leaks, heap fragmentation, and security vulnerabilities. Moreover, the interpreter itself can take up a significant amount of memory, regardless of the size of the network, and it cannot be optimized since the network size is not known a-priori.\n\u2022 In the compiler-based approach, the model is translated into machine code that can be executed directly by the processor. Model's parsing and evaluation are done at compile-time (on a host system), which can be relatively time-consuming and resource-intensive. Obviously, any changes to the model require recompilation, but the advantage is to have optimized code that can be executed quickly and efficiently. Memory management is also handled during the compilation stage, with all the memory allocations done statically. This avoids any risks related to dynamic memory management and reduces the inference engine's footprint, even because the latter is proportional to the original model's size, so small models can run on highly constrained devices. Moreover, the parts of the model that are not required at runtime (such as operator identifiers, names, and version numbers) can be stripped away, resulting in an even smaller binary size.\nThe latter approach best suites MicroFlow's design goals. In particular, MicroFlow Compiler processes a given NN model and generates the necessary inference code. The implementation is structured as a sub-crate of the main microflow crate, called microflow-macros, due to its extensive use of Rust macros. The compilation steps are illustrated in Fig. 2. The MicroFlow Compiler performs the first stage of the building process, producing the source code that is finally built by the Rust Compiler. The two primary components of the MicroFlow Compiler are the set of Rust macros used to generate the source code and the parsing process that analyzes the model. Both are discussed in the next subsections."}, {"title": "3.3.1 Macros", "content": "Rust macros play a vital role in MicroFlow. They provide a powerful mechanism for code generation and metaprogramming, enabling developers to write programs that generate or manipulates code at compile time. In Rust, there are two types of macros: declarative macros and procedural macros. Declarative macros, also known as macro_rules,"}, {"title": "3.3.2 Parsing", "content": "The parsing phase analyses the input model and generates the output code plus the memory structures, accordingly. The input of the parser comes from the macro, and it is the path of the model relative to the root of the crate. Although MicroFlow accepts, as input, NN models in the TFLite format, other formats (e.g. ONNX) could be included simply by expanding the parser. Under the hood, TFLite uses the FlatBuffers serialization format, which provides a lightweight and efficient solution for serializing and deserializing structured data suitable for embedded applications [8]. For this, FlatBuffers relies on a schema definition of the data's structure and layout. Therefore, there is not a single parser for FlatBuffers, but it depends on the schema. Fortunately, the format includes also a powerful code generation tool called flatc, which takes a FlatBuffers schema as input and automatically generates a parser handling serialization and deserialization of FlatBuffers files, based on the given schema.\nMicroFlow's parser starts by invoking the FlatBuffers deserializer generated by flatc, then proceeds extracting the operators of the NN, along with all the tensor dimensions, content, and relations. Subsequently, it generates an internal representation of the model by constructing a series of operators, each one associated with its respective parameters, such as the input/output tensors, weights, activation function, and other relevant attributes. Each operator contains also the stream of tokens needed by the macro to generate its runtime call. For example, the FullyConnected operator in the internal representation will contain the tokens that, once included in the generated source code, call the fully_connected() function at runtime, with all the required arguments.\nAn example of the parsing sequence is illustrated in Fig. 4. The internal representation captures the structure and characteristics of the model, enabling further processing and manipulation for efficient execution of the NN inference. Once this internal representation is built, along with the sequence of operators involved, the parser proceeds with the pre-processing phase."}, {"title": "3.3.3 Pre-processing", "content": "The pre-processing phase of the MicroFlow Compiler plays a crucial role to reduce the load at runtime by performing calculations and optimizations on constant values during compile-time. During this phase, the preprocess() function that is present in each operator of the internal representation is invoked. By offloading constant calculations to the pre-"}, {"title": "3.4 MicroFlow Runtime", "content": "The MicroFlow Runtime is the second main component of the software. As the name suggests, it contains all the implementation of the operators and, more generally, everything that is executed on the target MCU. A key difference between Compiler and Runtime components is that the latter cannot rely on the Rust standard library. In fact, on bare-metal MCU, there is no OS and the software can only access the most essential structures and components of the language from the core library. Since the latter is a subset of the standard library, the MicroFlow Runtime can run both on bare-metal and OS-based platforms.\nThis component must be operated efficiently in terms of computational performance and memory management. The runtime functions are called by the code generated by the compiler, after some static checks are performed to ensure reliable execution. In particular, the MicroFlow Runtime receives the sequence of operators to execute along with the tensors. The model does not need to be evaluated at runtime, unlike other solutions in the literature. For example, the TFLM inference engine consists only of a runtime module, which is an interpreter [8]. This causes overhead because all the operations carried out by the compiler need to be carried out at runtime by the interpreter instead.\nAnother responsibility of the MicroFlow Runtime is to manage memory allocation. However, since the model is fully analyzed before execution, the memory needed to perform inference is statically defined. As a result, the runtime module knows in advance the exact amount of memory needed and the specific locations where tensors should be stored. This enables a precise allocation of memory resources, optimizing their use and minimizing overhead."}, {"title": "3.4.1 External Libraries", "content": "The Runtime module relies on external libraries to perform matrix operations and manipulations. These libraries must be independent from the standard one and completely written in Rust to ensure memory safety. Also, MircoFlow needs a library that can be used with static memory allocation and generic types, or generics (discussed next). To this end, the nalgebra\u00b2 crate is a powerful linear algebra library, fully written in Rust \u2013 and thus memory-safe \u2013 that provides a comprehensive set of tools and structures for mathematical operations involving vectors, matrices, and other geometric entities. In line with MicroFlow's goals, it is designed to be efficient, generic, and easy to use. One of the key features of nalgebra is the support for both fixed-size and dynamically-sized matrices and vectors. Moreover, it puts a strong emphasis on generics, which play a significant role in the proposed software since they enhance the versatility and adaptability of the Runtime part, making it suitable for a wide range of models."}, {"title": "3.4.2 Generics", "content": "Generic programming is a fundamental concept in Rust that allows the creation of highly versatile and reusable code. It enables the definition of functions, structs, and traits that can work with multiple data types, providing a high level of"}, {"title": "4 Memory Management", "content": "Memory management is a critical aspect to perform inference on resource-constrained MCUs. Efficient memory utilization is also essential to ensure optimal performance, minimize memory footprint, and avoid issues such as memory leaks and crashes. In this section, the techniques and strategies adopted in MicroFlow are explained."}, {"title": "4.1 Ownership", "content": "To understand how memory is managed in MicroFlow, it is useful to first understand Rust's memory management, particularly its ownership concept [24]. In Rust, every value has a single owner responsible for its memory. Ownership can be transferred when a value is assigned or passed to a function, ensuring a clear, conflict-free memory management system. Rust then enforces borrowing rules to prevent data races by allowing either multiple immutable references or a single mutable reference to a value, thus avoiding concurrent modifications. Memory is automatically deallocated when an owner goes out of scope, preventing memory leaks and eliminating the need for manual memory management. Rust allows borrowing through references, which can be immutable or mutable, enabling safe data sharing without transferring ownership. The Rust compiler enforces these ownership and borrowing rules at compile-time, raising errors if violations occur, ensuring memory safety and preventing unsafe operations.\nIn MicroFlow, these concepts are used to ensure memory efficiency. First of all, since all the tensor dimensions are known at compile-time, the entire execution at runtime does not require any dynamic allocation on the heap. This results in an optimal memory utilization since everything is allocated on the stack and freed after use. By doing so, problems such as memory fragmentation and dangling pointers are avoided. Moreover, the code becomes more portable and user-friendly since the programmer does not have to provide a global heap allocator or a memory arena. Instead, the needed memory is statically defined and allocated on the stack.\nMicroFlow handles the transfer of responsibilities according to the following mechanism. Each operator takes ownership of the input tensor and immutably borrows the others. The operator then moves the output tensor to the input of the next operator, which in turn takes ownership of the tensor. This mechanism ensures that the lifetime of the input tensor is bound to the operator, dropping the tensor once all the values have been propagated to the output. Therefore, at any point in time, the current working operator uses the minimum amount of memory. For the other tensors, such as weights and biases, ownership transfer is not necessary because they have constant values that are never dropped. Instead, since they are only read, they can be efficiently accessed by a borrow request (i.e. an immutable reference)."}, {"title": "4.2 Static Allocation", "content": "Everything in MicroFlow is allocated on the stack. By doing so, the memory used by operators will peak when the most memory-intensive operators are executed, then it will be automatically freed. Therefore, once the entire inference process is finished, the memory allocated by the engine is null.\nAn interpreter-based system like TFLM, instead, allocates the tensor arena for the entire execution of the inference process. The arena's size is constant and it is not freed after use. It must also be big enough to accommodate the most memory-intensive operator. Moreover, in TFLM the programmer is responsible for manually allocating and deallocating memory, as well as determining the appropriate amount to be used. This can result in suboptimal memory allocations and potential runtime errors (i.e. in case the user allocates either too little or too much memory)."}, {"title": "4.3 Paging", "content": "With the ownership mechanism described in the previous section, an entire NN layer can be loaded in RAM during execution. However, certain MCU have limited RAM, which can be a challenge for large layers. For example, the ATmega328 of the popular Arduino Uno boards has 32kB of Flash memory and only 2kB of RAM [36]. This is not enough to perform inference with a NN's dense layer of 32 fully connected neurons, since the memory required would be approximately 5kB. Although the Flash memory can hold the entire NN, its size would cause a stack overflow.\nA few solutions exist to reduce the need of storing intermediate results [37, 38]. In MicroFlow, such limitation is overcome by dividing the layer into pages and loading them in RAM one at a time. This approach allows for efficient memory utilization and ensures that the MCU's RAM is not overwhelmed. A layer's page includes all the information related to the connections from the units of layer i to one unit of layer i + 1. For the above example, dividing the layer into 32 pages results in a manageable RAM usage of 163 bytes. Dividing and loading the layer in pages, however, increases the execution time. Therefore, in situations where memory resources are limited and slow inference times are acceptable, the paging approach can be a viable solution. On the other hand, if memory constraints are less stringent and faster inference times are crucial, loading the entire layer at once remains the preferred option."}, {"title": "4.4 Stack Overflow Protection", "content": "Although Rust is known for its emphasis on memory safety, this is not guaranteed for bare-metal programs in case of stack overflow. For example, on the broadly used ARM Cortex-M architectures, the stack can grow too large and collide with the region containing all the program's static variables, overwriting them and causing undefined behaviors.\nA solution is to use a \"flipped\" memory layout by placing the stack below the aforementioned region, thus possibly colliding only with the boundary of the allocated RAM space. On ARM Cortex-M, trying to read or write past this boundary produces a hardware exception, which can be handled by Rust. To this end, MicroFlow utilizes the flip-link crate, a replacement of the default Rust linker that can flip the memory layout. Currently, this crate works only for Cortex-M architectures, which are therefore the only ones with stack overflow protection in MicroFlow, but more platforms will be supported in the future."}, {"title": "5 MicroFlow Operators", "content": "NN processes are often represented by a sequence of operations in a computational graph. MicroFlow provides the most common ones to implement FNNs and CNNs, including FullyConnected, Conv2D, DepthwiseConv2D, AveragePool2D, Reshape, ReLU, ReLU6, and Softmax operators.\nFor TinyML applications, these operators need to be first quantized: that is, floating point numbers have to be converted into integer ones, which should be still accurate enough to perform inference correctly. After quantization, each number is mapped according to the following equation:\n$r = S(q - Z)$ (1)\nwhere r is the original floating-point value, q is the quantized fixed-point value, and S and Z are the quantization parameters, namely scale and zero point, respectively. The quantization takes place before deployment, either during or after training. Its parameters are calculated based on a representative sample of the input data. During inference, the engine uses the quantized data together with the quantization parameters. Some of the most popular ML frameworks provide support for quantization [39]. Among these, TensorFlow Lite [29] is perhaps the most widely used, since it provides a set of tools and libraries for developers to deploy and run ML models on mobile and embedded devices. It allows to train, quantize, and save a model for embedded applications in a dedicated TFLite format, which is the one adopted by MicroFlow \u2013 although any other quantization tool compatible with this format could be used.\nOnce an operator has been quantized, its implementation in MicroFlow follows the design described in Sec. 3. The operator is split into two components: the parser, which runs on the compiler, and the kernel, which runs at runtime. The goal of the parser is to facilitate the kernel's job by preparing the input, output, and intermediate tensors, and by pre-processing the constant values. The goal of the kernel is to propagate the input to the output in the most efficient way. An overview of the relation between the operator's components is shown in Fig. 7.\nUnfortunately, to the best of our knowledge, the mathematical derivation of the quantized operators used in MicroFlow is not available in the literature. This is essential, however, to distinguish which parts of the code can be generated at compile-time and which at run-time. Therefore, the remaining part of this section presents the quantized formulae of the implemented operators, derived from the original notions of NN quantization [20], and explains which constant terms are pre-computed by the MicroFlow Compiler. Further details are also available in A."}, {"title": "5.1 FullyConnected", "content": "The FullyConnected operator, also known as the dense or linear operator, is a key building block of many neural networks. In this operator, each input element is multiplied by a corresponding weight and summed to other weighted inputs and biases. The resulting sum is then passed through a non-linear activation function to produce the final output value. More formally, given $X \\in \\mathbb{R}^{m\\times n}$, $W \\in \\mathbb{R}^{n\\times p}$, and $b \\in \\mathbb{R}^{P}$, representing respectively the input, weights, and biases of the operator, the output $Y \\in \\mathbb{R}^{m\\times p}$ can be written as follows:\n$Y_{i,j} = b_j + \\sum_{k=1}^{n} X_{i,k}W_{k,j}$ (2)\nBy applying Eq. (1), the quantized version can be derived (details in A.1):\n$Y_{q,i,j} = z_y + \\frac{S_b}{S_Y} (b_{q,j}-z_b) + \\frac{S_X S_W}{S_Y} (\\sum_{k=1}^{n} X_{q,i,k}W_{q,k,j} -  (\\sum_{k=1}^{n} X_{q,i,k}) z_W -  (\\sum_{k=1}^{n} W_{q,k,j}) z_X + n z_X z_W)$ (3)"}, {"title": "5.2 Conv2D", "content": "The Conv2D (short for \"Convolutional 2D\") operator is a fundamental building block of CNNs. It performs a convolution operation on an input tensor using a set of learnable filters. Conv2D operators are commonly used for tasks such as image recognition, object detection, and image segmentation. They capture local patterns and spatial relationships in the input data, allowing the NN to learn hierarchical representations and extract meaningful features. Here tensors are composed by a set of matrices (batches) containing multiple values (channels), one for each position. The convolutional filters are represented by the batches, while the channels are merged together by a dot-product during convolution. The output tensor contains only one batch for each channel with the result of the convolution, applied at that position in the input matrix. The quantization process is carried out as follows: given $X \\in \\mathbb{R}^{m\\times n\\times c}$, $F \\in \\mathbb{R}^{m\\times n\\times c}$, and $b \\in \\mathbb{R}$, representing respectively an input region, a filter, and the bias, the output value $y \\in \\mathbb{R}$ for a given channel can be written as follows:\n$y = b + \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\sum_{k=1}^{C} X_{i,j,k}F_{i,j,k}$ (5)\nAfter quantization, the following expression is obtained (details in A.2):\n$Y_{q} = Z_{y} + \\frac{S_{b}}{S_{Y}} (b_{q} - z_{b}) + \\frac{S_{X}S_{F}}{S_{y}} (\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\sum_{k=1}^{C} X_{q,i,j,k}F_{q,i,j,k} - (z_{X}\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\sum_{k=1}^{C} F_{q,i,j,k}) + mn c z_{X}z_{F} - (z_{F}\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\sum_{k=1}^{C} X_{q,i,j,k}))$ (6)\nwhere $X_{q}$, $F_{q}$, $b_{q}$, and $y_{q}$ are the quantized versions of X, F, b, and y, respectively, $S_{X}$, $S_{F}$, $S_{b}$, and $S_{y}$ are the scales for X, F, b, and y, respectively, and $z_{X}$, $z_{F}$, $z_{b}$, and $z_{y}$ are the zero points. Since the following terms are constants, they can be pre-processed by the compiler:\n$Z_{y} + \\frac{S_{b}}{S_{Y}} (b_{q} - z_{b})$\n$\\frac{S_{X}S_{F}}{S_{y}} x \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\sum_{k=1}^{C} F_{q,i,j,k}$\n$Mn C z_{X}z_{F}$ (7)\nNote that the implementation of the Conv2D operator requires also an additional view extraction routine in the kernel to select the appropriate input elements used in each convolution, taking into account padding and stride (details in A.2)."}, {"title": "5.3 DepthwiseConv2D", "content": "The DepthwiseConv2D operator is a specific type of Conv2D operator, commonly used in efficient CNNs like MobileNet [40], which applies a separate filter for each input channel. This means the operator performs depthwise convolutions, as its name suggests, where each channel is convolved independently. The DepthwiseConv2D operator shares many properties and data structures with the conventional Conv2D. However, the process is different: instead of merging the channels with a dot-product, they are kept separate and convolved individually with the corresponding channels of the filter. In particular, DepthwiseConv2D operates on a single batch with a 3D weight matrix, where the third dimension represents the weights associated to each channel. Therefore, given $X \\in \\mathbb{R}^{m\\times n}$, $W \\in \\mathbb{R}^{m\\times n}$, and $b\\in \\mathbb{R}$, which represent the input region, the weights matrix for a given channel, and the bias, respectively, the output value y \u2208 R can be written as follows:\n$y = b+ \\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{i,j} W_{i,j}$ (8)"}, {"title": "5.4 AveragePool2D", "content": "The AveragePool2D operator is typically used to downsample the input data by partitioning it into non-overlapping regions and computing the average value within each of them."}]}