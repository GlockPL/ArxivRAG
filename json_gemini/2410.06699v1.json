{"title": "Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models", "authors": ["Yubo Wang", "Chaohu Liu", "Yanqiu Qu", "Haoyu Cao", "Deqiang Jiang", "Linli Xu"], "abstract": "Large vision-language models (LVLMs) integrate visual information into large language models, showcasing remarkable multi-modal conversational capabilities. However, the visual modules introduces new challenges in terms of robustness for LVLMs, as attackers can craft adversarial images that are visually clean but may mislead the model to generate incorrect answers. In general, LVLMs rely on vision encoders to transform images into visual tokens, which are crucial for the language models to perceive image contents effectively. Therefore, we are curious about one question: Can LVLMs still generate correct responses when the encoded visual tokens are attacked and disrupting the visual information? To this end, we propose a non-targeted attack method referred to as VT-Attack (Visual Tokens Attack), which constructs adversarial examples from multiple perspectives, with the goal of comprehensively disrupting feature representations and inherent relationships as well as the semantic properties of visual tokens output by image encoders. Using only access to the image encoder in the proposed attack, the generated adversarial examples exhibit transferability across diverse LVLMs utilizing the same image encoder and generality across different tasks. Extensive experiments validate the superior attack performance of the VT-Attack over baseline methods, demonstrating its effectiveness in attacking LVLMs with image encoders, which in turn can provide guidance on the robustness of LVLMs, particularly in terms of the stability of the visual feature space.", "sections": [{"title": "1 Introduction", "content": "Large vision-language models (LVLMs) have garnered considerable attention owing to their remarkable visual perception and language interaction capabilities [2, 43]. Compared to large language models (LLMs), LVLMs exhibit superiority in image understanding by leveraging visual models, making them highly effective for diverse multimodal tasks, such as image captioning [23, 24], visual question answering [4] and multimodal dialogue [10, 26, 48]. However, recent research [5, 31, 47] highlights their vulnerability to adversarial attacks, posing potential security concerns in practical domains such as medical image understanding [22] and document information extraction [25]. This underscores the importance of investigating the robustness of LVLMs from an attacker's perspective.\nAdversarial attacks involve the deliberate manipulation of input data to induce incorrect or specific predictions. In general, adversarial attacks can be classified into targeted attacks, which aim to mislead the model into generating specific outputs, and non-targeted attacks, which lead to any incorrect or undesired outputs.\nEarly research of adversarial attacks on visual models was primarily conducted to explore the security and robustness of image"}, {"title": "2 Related Work", "content": "Research in large vision-language models (LVLMs) has been advancing rapidly, driven by the efforts of researchers who design novel model architectures and employ specific training strategies to propel their development [3, 10, 17, 21, 23, 24, 26, 42, 48].\nThe architecture of a large vision-language model typically comprises three components: a pre-trained image encoder, an intermediate module facilitating the transformation of visual tokens into the language space, and a large language model. Various approaches have been employed in designing the intermediate modules. For instance, LLaVA [26] utilizes linear layers to project visual features into the language space, while the BLIP-2 [24] series (MiniGPT-4 [48], InstructBLIP [10]) adopt Q-Former to extract the most relevant visual features to the text prompts for the language models. Different LVLMs may employ diverse intermediate modules for visual feature extraction, while utilizing a common pre-trained image encoder (e.g. OpenAI CLIP [32] or EVA CLIP [16]) for feature encoding. These pre-trained image encoders have been trained with"}, {"title": "3 Methodology", "content": "In this section, we start with the problem formulation and then provide detailed explanations of our proposed method The framework of our method (VT-Attack) is shown in Figure 2, where we first construct adversarial examples against image encoders and proceed to attack LVLMs."}, {"title": "3.1 Problem Formulation", "content": "Let $F(x, q) \\rightarrow z$ denote a large visual language model parameterized by $\\theta$, where $x$ is the input image and $q$ is the prompt input to the LVLM. Let $I_\\phi$ denote the image encoder of the LVLM parameterized by $\\phi$, which encodes images into visual tokens $v$. Additionally, let $M_\\gamma$ represent the intermediate module, parameterized by $\\gamma$, that processes visual tokens output by $I_\\phi$ and transforms them into mapped visual tokens $p$:\n\n$v = I_\\phi(x), p = M_\\gamma (I_\\phi(x))$\n\nGiven the input prompt $q$ and the image $x$, the answer $z$ generated by LVLM can be represented as\n\n$z = F_\\theta(M_\\gamma (I_\\phi(x)), q)$\n\nLet $x_{adv} = x + \\Delta_{adv}$ denote the adversarial image being constructed, exhibiting subtle differences $\\Delta_{adv}$ from the clean image $x$. Our focus is on non-targeted attacks, where the adversarial image $x_{adv}$ leads the LVLM to generate any incorrect or unreasonable answers $\\hat{z}$ different from the original answer $z$ as follows.\n\n$z \\neq \\hat{z} = F_\\theta(M_\\gamma (I_\\phi (x + \\Delta_{adv})), q)$\n\nWith only access to the parameters and gradients of the image encoder $I_\\phi$, our method constructs adversarial images by setting optimization objectives based on the visual tokens $v$. During the generation process of $x_{adv}$, it is common to apply an $L_p$ norm constraint on the perturbation size, written as $||x - x_{adv}||_p = ||\\Delta_{adv}||_p \\leq \\epsilon$. It should be noted that setting $\\epsilon$ to a large value may compromise the stealthiness of the generated adversarial images."}, {"title": "3.2 Visual Feature Representation Attack", "content": "Large vision-language models commonly employ CLIP [32] as their image encoders which are typically based on the ViT architecture [12]. An input image is split into fixed-length patches, with each patch treated as a token and fed into the ViT.\nSubsequently, the ViT encodes the image and generates a series of visual tokens $v$ arranged in an $L \\times D$ matrix, which can be regarded as the visual feature representation of the image. After further integration of these visual tokens by the intermediate module, the language model can naturally generate outputs leveraging the visual information.\nHence, the features output by the image encoder provides crucial visual information to the entire model. Intuitively, if the visual features are disrupted and deviated from the original representation, subsequent modules will be unable to accurately interpret the image contents, leading to erroneous model outputs.\nMotivated by this, we apply a visual feature representation attack as illustrated in Figure 2 (a), aiming to maximize the loss between the feature representation in visual tokens of the adversarial image and the original representation:\n\n$\\max E[\\sum_i^L(I_\\phi(x_{adv}), I_\\phi(x))]$\ns.t. $||x - x_{adv}||_p \\leq \\epsilon$\n(1)"}, {"title": "3.3 Visual Token Relation Attack", "content": "While the visual feature attack explicitly disrupts individual visual tokens, it may not fully consider the interdependencies among these tokens. Therefore, we introduce visual token relation attack.\nThe self-attention [41] layers in the ViT image encoder are responsible for capturing the relationships and dependencies among image patches or tokens, corresponding to the relevance between different regions in the image [12]. These layers enable the model to weigh the importance of each token in relation to the others, allowing for a comprehensive understanding of the image's contexts.\nTherefore, within the visual tokens generated by the image encoder, each token tends to carry information of other tokens that have a higher degree of relationship with it. This enables the visual tokens to exhibit clustering properties, where tokens with higher correlation tend to be grouped together in the same cluster. As shown in Figure 3, visual tokens belonging to the same region or entity tend to cluster together. This clustering effect arises because tokens that are related or depict similar aspects of the image receive stronger attention connections through the self-attention mechanism.\nNevertheless, relying solely on the feature representation attack may not be sufficient to disrupt the clustering relationship among relevant visual tokens, as illustrated in Figure 4 (a). Although feature attack introduce deviations between visual tokens and their initial distribution, they may still exhibit proximity to the clustering centers."}, {"title": "3.4 Global Semantics Attack", "content": "The feature and relation attacks introduced above directly compromise the visual token sequence of length L generated by the image encoder, resulting in disruptions at both the representation and relationship levels. While the combination of these two attacks can effectively disrupt the information of visual tokens, we are still interested in attacks that change the semantics of images.\nThe information carried by the [CLS] token contains the most direct content of an image, unlike the visual tokens that encode specific visual features. We hypothesize that the disruptions of both local image details (feature and relation attacks) and global semantics are mutually reinforcing, contributing to a comprehensive destruction of visual tokens.\nTherefore, we incorporate the semantics attack as illustrated in Figure 2, which reduces the semantic similarity between the visual and text semantic information of the [CLS] token encoded by the CLIP image/text encoder:\n\n$\\max L(I_\\phi (x_{adv})[CLS], T_\\eta (t)[CLS])$\ns.t. $||x - x_{adv}||_p \\leq \\epsilon$\n(4)\n\nwhere $T_\\eta$ represents the CLIP text encoder corresponding to $I_\\phi$ and $t$ refers to the caption of an image. We utilize cosine similarity to preserve settings similar to contrastive learning [32]. We employ a variant of cosine similarity as the loss function $L(, ) = 1 + cos\\_sim( , )$ to align with the loss space of the previous two methods."}, {"title": "3.5 Visual Tokens Attack (VT-Attack)", "content": "By integrating the aforementioned three sub-attack methods, we introduce a unified attack approach named VT-Attack, as illustrated in Figure 2 (a). The proposed VT-Attack can comprehensively disrupt the embedded visual features, disturb the inherent relationships and weaken the semantic properties of visual tokens, by solving the following optimization problem:\n\n$\\max L_{Feature} + L_{Relation} + L_{Semantics}$\ns.t. $||x - x_{adv}||_p \\leq \\epsilon$\n(5)\n\nThe generation process of the adversarial image is illustrated in Algorithm 1. After obtaining the adversarial image $x_{adv}$ through $I_\\phi$, we input it to various LVLMs that utilize $I_\\phi$ as the image encoder, as depicted in Figure 2 (b). Due to the models' inability to perceive meaningful visual information, they tend to generate incorrect answers regardless of the types of questions concerning the image content."}, {"title": "4 Experiments", "content": "In this section, we present the experimental results of VT-Attack to demonstrate the effectiveness of the proposed method. Additionally, we provide experimental analysis of our approach for further exploration."}, {"title": "4.1 Experimental Settings", "content": "Victim models. We conduct experiments on a series of prominent baseline large vision-language models to validate the generality of our proposed method. The victim models include LLaVA [26], Otter [21], LLAMA-Adapter-v2 [17], and OpenFlamingo [3], which utilize OpenAI CLIP [32] as their image encoder, as well as BLIP-2 [24], MiniGPT-4 [48], and InstructBLIP [10], which employ EVA CLIP [16] as their image encoder. We also involve models without employing the pre-trained CLIP such as mPLUG-Owl-2 [42], utilizing a further trained ViT. We generate adversarial images using"}, {"title": "4.2 Main Results", "content": "We conduct our evaluation primarily on the image captioning task [34, 47], as it assesses the global comprehension ability of LVLMs towards images. We query the LVLMs using the prompt \"Describe the image briefly in one sentence.\" with adversarial images. The results are presented in Table 1. Note that \"VT-Attack\" refers to the combination of feature, relation and semantics attacks, while \"VT-Attack (F+R)\" refers to the combination of feature and relation attacks."}, {"title": "4.4 Further Analysis", "content": "The results of same adversarial image attacking different LVLMs. We compare the answers generated by different LVLMs given the same clean and adversarial images, as shown in Figure 7. For the same clean image, the models produce similar answers. However, the models generate completely unrelated answers for the same adversarial image. The results indicate that adversarial images lack valid visual information that can be perceived by LVLMs.\nThe impact of perturbation size to the performance. As shown in Figure 8 (b), the performance of the attack improves as the perturbation size increases from 1/255 to 8/255. However, further enlarging the perturbation size does not necessarily lead to performance improvement.\nThe impact of attacks on the perplexity of model outputs. We are curious about the self-confidence level of LVLMs in generating responses when the visual tokens are disrupted. Therefore, we compute the perplexity of the outputs from three models as shown in Table 4. The perplexity of answers corresponding to adversarial images is consistently higher than that of clean images. This indicates that the models exhibit vulnerability to compromised visual information, resulting in uncertain outputs."}, {"title": "5 Conclusion", "content": "In this paper, we focus on the robustness of LVLMs against non-targeted attacks. Existing methods often overlook the impact of compromised visual tokens on LVLMs. To this end, we propose a new adversarial attack method called VT-Attack, which disrupts the encoded visual tokens comprehensively from multiple perspectives. Experimental results have demonstrated the superiority of our approach over the baselines. This highlights the necessity for enhancing the adversarial defense capability of LVLMs. We hope our work can provide guidance for research on model defense, particularly in the defense of the visual token space."}, {"title": "A Victim Model Details", "content": "The details of victim large vision-language models are illustrated in Table 5. Various LVLMs employ LLMs with large number of parameters, including LLaMA [39], MPT [38], OPT [45], and Vicuna [8]. To reduce randomness, we consistently employ greedy search to generate answers for clean images or adversarial images."}, {"title": "B Additional Experimental Results", "content": "We conduct experiments on some newly released models and the results are shown in Table 6. The results indicate that our method is applicable to newly proposed LVLMs."}, {"title": "B.1 Attack Results on Latest LVLMS", "content": "We conduct experiments on some newly released models and the results are shown in Table 6. The results indicate that our method is applicable to newly proposed LVLMs."}, {"title": "B.2 VT-Attack Results against Different LVLMs", "content": "We provide additional qualitative results of VT-Attack against different LVLMs. Table 8, Table 9 and Table 10 present the results of attacking LLaVA [26], MiniGPT-4 [48] and mPLUG-Owl2 [42]. It can be observed that our proposed VT-Attack leads to outputs that are less relevant to the original answers compared to the baseline methods. This further illustrates the greater disruption caused by VT-Attack on information in visual tokens."}, {"title": "B.3 Results of Same Adversarial Image Attacking Various LVLMs", "content": "Figure 9 and Figure 10 present the additional comparison of the answers generated by different LVLMs when taking the same clean image and adversarial image as inputs. It can be observed that the same adversarial image leads to different outputs from different models. This indicates that the adversarial images whose encoded visual tokens are disrupted may not exhibit strong semantics; otherwise, different models should generate similar outputs."}, {"title": "B.4 Breakdown of LLaVA to Non-Visual Question Answering", "content": "In experiments we have observed that when adversarial images generated by VT-Attack are input to LLaVA [26], the question-answering capability of LLaVA not only exhibit failure to image-based queries, but also break in addressing non-visual prompts. Even when the questions posed are unrelated to the images such as \"What is artificial intelligence?\", the model fails to provide reasonable responses. One possible reason is that the intermediate module of LLaVA is only a linear layer with a significantly smaller number of parameters. Therefore, LLaVA is more sensitive to corrupted visual tokens compared to other LVLMs, even when queried with non-visual questions."}, {"title": "C Attack Performance over Iterations", "content": "The influence of attack iterations on attack performance (CLIP score) is illustrated in Figure 11, taking LLaVA [26] as an example. Increasing the number of attack iterations generally enhances the attack effectiveness. However, the improvement becomes less significant when the number of iterations exceeds 800."}, {"title": "D Prompts for Different Tasks", "content": "To investigate the generality of adversarial examples across different prompts, we employ three question-answering tasks. Below are prompts used for image captioning task.\n\nDescribe this image briefly in one sentence.\nGive a brief description of the image.\nCan you provide a brief description of this image?\nOffer a brief caption for this image.\nGive a short overview of this image.\nProvide a short summary of this picture.\nDescribe this picture in a few words.\nSummarize this image briefly.\nPlease provide a short title for this image.\nBriefly explain what is shown in this image.\n\nThe prompts for general VQA are illustrated below.\n\nIs there a mobile phone in this image?\nIs there any text or writing visible in the image?\nCan you see any shoes in this image?\nHow many pens are visible in this picture?\nAny signs of human activity in the image?\nWhere was this image taken?\nCan you see animals in this image?\nCan you identify any vehicles?\nAre there any objects related to food?\nDo you notice any body of water?\n\nThe prompts for detailed VQA are shown as below."}]}