{"title": "Learning Free Token Reduction for Multi-Modal LLM", "authors": ["Zihui Zhao", "Yingxin Li", "Yang Li"], "abstract": "Vision-Language Models (VLMs) have achieved remarkable success across a range of multimodal tasks; however, their practical deployment is often constrained by high computational costs and prolonged inference times. Since the vision modality typically carries more information than the text modality, compressing visual prompts offers a promising solution to alleviate these challenges. Existing approaches predominantly focus on refining model architectures or directly reducing the number of visual tokens. However, these methods often compromise inference performance due to a lack of consideration for the unique spatial and temporal characteristics of visual data. In this work, we propose a token compression paradigm that operates on both spatial and temporal dimensions. Our approach includes a learning-free, plug-and-play compression pipeline that can be seamlessly integrated into most Multimodal Large Language Model (MLLM) frameworks. By leveraging this method, we enhance the model's inference capability while simultaneously reducing its computational cost. Experimental results on the Video-QA task demonstrate the effectiveness of the proposed approach, showcasing significant improvements in efficiency without sacrificing performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated exceptional capabilities in tasks such as text generation, translation, and comprehension. To extend these reasoning abilities beyond text, Multimodal LLMs (MLLMs) integrate multimodal encoders and projectors to align various modalities (e.g., visual) with the feature space of text prompts. This integration positions MLLMs as powerful tools for a wide range of multimodal applications, including video understanding, real-time surveillance, and navigation.\nHowever, the widespread deployment of MLLMs is significantly constrained by their high computational requirements, particularly the lengthy inference times. To enable practical use, research has focused on reducing the computational demands of MLLM inference through two primary strategies: model compression and token compression. Model compression typically involves refining model architectures by pruning or quantizing intermediate tokens, while token compression methods often utilize clustering to select the most important tokens. Despite their contributions, these approaches are\neither architecture-specific or result in compromised inference\nperformance, especially at higher compression rates.\nThe computational burden of LLM inference arises not only\nfrom the large number of parameters but also from the quadratic\ncomplexity of attention calculations over long input token\nsequences. For MLLMs, this issue is further exacerbated by the\nlength of prefixed visual tokens, which significantly contributes\nto extended inference times.\nIn video-based modalities, redundancy exists across both\ntemporal and spatial dimensions, as evident in the embedded\ntoken space. This redundancy not only prolongs inference\nbut also distracts the model from focusing on the most\nrelevant information. By effectively compressing the prefixed\nvisual tokens across both spatial and temporal dimensions,\nit is possible to maintain competitive performance while\nsubstantially reducing inference time.\nIn this work, we exploit the redundancy in the temporal\ndimension and the sparsity in the spatial dimension of visual\nrepresentations. Specifically, we merge similar adjacent to-\nkens in the temporal dimension and prune irrelevant or less\ninformative tokens in the spatial dimension. Our proposed\nmethod is a learning-free, plug-and-play solution that operates\non MLLM prefixed tokens and is compatible with various\nMLLM architectures. To validate our approach, we conduct\nextensive experiments to assess its impact on model efficiency\nand performance. In summary, our key contributions are as\nfollows:\n\u2022\nWe analyzed the redundancy in visual representations\nwithin MLLMs and introduced a token-level compression\nmethod that is adaptable to various model architectures.\n\u2022\nWe proposed a temporal-spatial token compression strat-\negy for video modalities, designed to streamline token\nsequences while retaining essential information.\n\u2022 Experimental evaluations on a widely used benchmark\ndemonstrate that our method effectively reduces token\ncounts and improves the performance of Video-LLMs."}, {"title": "II. RELATED WORK", "content": "The field of LLM has witnessed significant achievements in\nrecent years. As the scale of LLM training data amount and\nmodel architecture increase [1], the models have demonstrated\nexceptional abilities on text understanding and reasoning. Partic-\nularly, base models like LLaMA [2]\u2013[4], Vicuna [1] and Mistral\n[5] have served as the main component of many works and\napplications. To extend the LLM reasoning and understanding"}, {"title": "A. Multi-Modal Large Language Models", "content": "ability to other modalities especially vision modality, researches\nhave developed various multi-modal encoders and multi-modal\nprojectors to align different modalities with the text prompt\nand LLM feature space to build Multi-modal Large Language\nModels (MLLM) [6]. These success lead to the applications on\nmore complex scenarios with richer information including video\nunderstanding and high-resolution images (e.g. Video-ChatGPT\n[7], Video-LLaMA [8] and Video-LLAVA [9]). However, as\nthe amount of vision tokens increase, the LLM inference\ntime significantly increases and the limited LLM context\nwindow length leads to modality imbalance. To address this\nissue, existing methods improve LLM inference efficiency by\nreducing the size of LLM backbone [10], [11] but few works\nconsider the efficiency of visual tokens."}, {"title": "B. MLLM Token Reduction", "content": "Due to the quadratic complexity of Transformer bottlenecks\nthe scaling of input sequence, many sparse attention based\nmethods reduce the quadratic complexity by conducting atten-\ntion operations within a certain context length rather than the\nfull context [12], [13]. Token merging adopts full attention\nbut reduces the number of tokens in each Transformer layers\nby selecting most representative ones [14]. Token pruning\nselectively removes tokens that contribute less to the final\noutput, thereby decreasing the model's memory usage and\nspeeding inference. LTP [15] learns which tokens to prune\nthe tokens in Transformer during the training process to retain\nthe most informative tokens. Zero-Tprune [16] leverages the\nattention mechanisms in pre-trained transformers, considering\nboth the importance and similarity of tokens in performing\ntoken pruning without retraining. While these methods perform\neffectively at lower compression rates, they often result in\nsignificant information loss, limiting their applicability at higher\ncompression levels. In this study, we propose a generalizable\narchitectural approach capable of achieving higher compression\nrates while enhancing model inference performance."}, {"title": "III. PRELIMINARY", "content": "While these methods perform effectively at lower compres-\nsion rates, they often result in significant information loss,\nlimiting their applicability at higher compression levels. In\nthis study, we propose a generalizable architectural approach\ncapable of achieving higher compression rates while enhancing\nmodel inference performance."}, {"title": "A. Vision Encoding", "content": "A commonly used visual encoder is the Transformer Encoder-based ViT. Given an image input $I \\in R^{c \\times h \\times w}$, it was\ndivided into patches with size $p \\times p$ ($\\times$ patches in total).\nVisual information from each patch is treated as an individual\ntoken, enriched with positional encoding, and processed by\na Transformer encoder. The Transformer encoder comprises\nmultiple layers of attention mechanisms and feed-forward MLP\nmodules.\nVideo data is typically represented using 2D encoding, where\nframes are processed individually as images by the Transformer\nencoder. The encoded outputs are then concatenated along the\ntemporal dimension to form feature embeddings."}, {"title": "B. Video-LLM", "content": "The general architecture of the Video multi-modal LLM\nis shown in Figure. 3. Video and text are fed into a visual\nencoder and text tokenizer separately. To further align the visual\nand verbal modalities before feeding into the LLM, the visual\nfeatures are projected through a multi-modal projector. The\nprojected visual features $F_V \\in R^{N_V \\times d}$ and textual information\n$F_T \\in R^{N_T \\times d}$ will be concatenated and fed into the LLM as a\nprompt.\nA significant challenge in this pipeline is the substantial data\nsize of visual features. In tasks such as video comprehension\nand text-based question answering, the number of visual tokens\nfar exceeds that of text tokens. This disparity arises from\nthe large number of video frames and the high token count\nper image. Such an imbalance between the visual and text\nmodalities can hinder the model's ability to fully understand\nboth types of input.\nMoreover, the high redundancy in the temporal dimension\nand the sparsity in the spatial dimension of visual data can\ndistract the language model from focusing on question-relevant\ninformation or lead to information loss. Therefore, effectively\ncompressing these visual tokens can not only accelerate the\nmodel's inference process but also enhance its reasoning and\ncomprehension capabilities."}, {"title": "IV. METHOD", "content": "To address the issue of excessive prefixed visual tokens,\nwe apply compression across both the temporal and spatial\ndimensions. We propose a non-learning-based compression\nmethod that can seamlessly integrate as a plug-and-play solution\ninto most popular MLLM architectures."}, {"title": "A. Temporal Compression", "content": "Differential-Based Score In video modalities, neighboring\nframes often exhibit significant redundancy, as static objects\nand backgrounds remain consistent over time. To address this,\nwe propose calculating the differences between adjacent frames"}, {"title": "B. Spatial Compression", "content": "Topic-based score In each frame of a video sample, only\na subset of image patches contains essential or task-relevant\ninformation. Effective comprehension can often be achieved\nby focusing on these key regions within the image. Therefore,\nwe utilize the [CLS] token from the visual encoder (e.g.,\nCLIP-ViT), which indicates the presence of subject-related\ninformation in each frame, to perform pruning of irrelevant\nimage patches."}, {"title": "Text-based Compression", "content": "Text-based Compression Since visual tokens are aligned\nwith text prompts in the same feature space, their correlations\ncan be leveraged to extract only the question-relevant infor-\nmation for LLMs. Specifically, for a projected visual token T\nand an embedded text prompt P, we can compute a similarity\nmatrix by multiplying the visual tokens with the text prompts:\n$M_p= T P^T$\nwith this similarity matrix, we can treat it as a weight matrix\nto select important tokens and prune those that are irrelevant\nto the text."}, {"title": "V. EXPERIMENTS", "content": "We apply our visual token compression method to the Video-LLaVA [9] framework. Our experiments are conducted on the\nMSVD-VQA benchmark and evaluated using the GLM-4 API.\nThe MLLM predictions are assessed based on accuracy, while\nthe average inference time is also recorded.\nOur evaluation covers three types of token compression:\ntemporal dimension token compression, spatial dimension token"}, {"title": "A. Experiment Results of Temporal Compression", "content": "As shown in Fig. 4, our proposed temporal dimension token\ncompression methods successfully improve model inference\naccuracy while significantly reducing the overall token count.\nBoth the similarity-based and differential-based scores in\ntemporal token compression enhance model performance,\nenabling it to outperform the base model with fewer visual\ntokens."}, {"title": "B. Experiment Results of Spatial Compression", "content": "Similar to the experiments conducted on the temporal dimen-\nsion, we also assessed the performance of token compression\non the spatial dimension. We applied different compression\nrates by reducing the spatial patch counts and evaluated both\ntopic-based and text-based compression methods.\nAs illustrated in Fig. 5, both the proposed topic-based\nand text-based compression methods effectively improve\nmodel inference performance while reducing the token count.\nHowever, the performance of the topic-based compression\nmethod declines when the compression rate reaches 25%,\nwhereas the text-based compression method continues to"}, {"title": "C. Experiment Results of Joint Compression", "content": "To validate that compression on both the spatial and temporal\ndimensions can be applied simultaneously across vertical\ndimensions, we conducted experiments with joint compression\non both dimensions. For the temporal dimension, we applied\nboth similarity-based and differential-based scores, while for\nthe spatial dimension, we adopted text-based compression to\noptimize performance and efficiency."}, {"title": "VI. CONCLUSION", "content": "In this work, we propose a learning-free, plug-and-play\ntoken reduction method for MLLM frameworks. We analyze\nthe temporal and spatial redundancies within video sequences\nand introduce a compression method that operates across both\ndimensions. For the spatial dimension, we leverage information\nsparsity to prune unimportant or text-irrelevant content. For\nthe temporal dimension, we exploit information redundancy in\nthe time series to merge similar tokens while preserving key\nevents.\nOur methods are evaluated through experiments on Video-QA tasks, demonstrating that both spatial and temporal token\ncompression significantly improve model inference efficiency\nand reasoning ability. By applying compression across both\ndimensions simultaneously, our approach achieves much higher\ncompression rates with improved model inference accuracy.\nThis compression strategy can be seamlessly integrated into\nmost popular MLLM frameworks or existing implementation\npipelines."}]}