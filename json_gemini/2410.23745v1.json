{"title": "Syno: Structured Synthesis for Neural Operators", "authors": ["Yongqi Zhuo", "Zhengyuan Su", "Chenggang Zhao", "Mingyu Gao"], "abstract": "The desires for better prediction accuracy and higher execution performance in neural networks never end. Neural architecture search (NAS) and tensor compilers are two popular techniques to optimize these two goals, but they are both limited to composing or optimizing existing manually designed operators rather than coming up with completely new designs. In this work, we explore the less studied direction of neural operator synthesis, which aims to automatically and efficiently discover novel neural operators with better accuracy and/or speed. We develop an end-to-end framework Syno, to realize practical neural operator synthesis. Syno makes use of a novel set of fine-grained primitives defined on tensor dimensions, which ensure various desired properties to ease model training, and also enable expression canonicalization techniques to avoid redundant candidates during search. Syno further adopts a novel guided synthesis flow to obtain valid operators matched with the specified input/output dimension sizes, and leverages efficient stochastic tree search algorithms to quickly explore the design space. We demonstrate that Syno discovers better operators with an average of 2.06\u00d7 speedup and less than 1% accuracy loss, even on NAS-optimized models.", "sections": [{"title": "Introduction", "content": "Deep learning with neural networks (NNs) has been a surprisingly effective algorithm breakthrough to handle many challenging tasks in various domains. Since its emergence in the last decade, people have been continuously seeking to improve both the quality (in terms of, e.g., prediction accuracy) and the performance (in terms of training and inference time) of NN models, in order to adapt them to more complicated real-world scenarios with lower computational cost.\nTwo complementary research paradigms have been developed. To systematically design new NN models with better accuracy quality, neural architecture search (NAS) [8, 20,"}, {"title": "Neural Architecture Search", "content": "As neural networks (NNs) are being applied to more and more domains, the needs of designing specific NN models are becoming increasingly prevalent. Neural architecture search (NAS) has emerged consequently to automatically design new model structures [8, 20, 42, 43], and indeed, many of the recently proposed models that showed state-of-the-art accuracy levels were discovered by NAS rather than manually crafted [28-30, 37]. NAS typically defines a highly modular search space by dividing the backbone model topology into basic units (called cells) of various sizes. It then proceeds to explore how to construct each cell by composing from several types of basic layers (a.k.a., operators) like convolutions, matrix multiplications (matmuls), and pooling. Throughout the search, the accuracy levels of the candidate cell structures are continuously evaluated. With such an automated flow, NAS is able to efficiently explore a much larger design space, and hence discover NN architectures with higher accuracy than manually designed models.\nBesides solely focusing on accuracy, performance-aware NAS methods aim to strike a better balance between prediction accuracy and execution speed [2, 3, 29, 37]. Specifically, they inherit the design space from traditional NAS while integrating hardware efficiency metrics. By considering factors such as latency alongside accuracy, the search process could yield model architectures that are not only high-quality but also high-performance\u00b9 on particular hardware.\nWe emphasize that both traditional and performance-aware NAS methods only compose existing operators, such as convolutions and matmuls, in a coarse-grained black-box way. Thus they are limited by these computationally expensive operators. The lack of flexibility to invent novel operators leaves ample opportunities for further optimizations, as we will demonstrate in this work."}, {"title": "Tensor Compilers", "content": "At the system level, an NN model is typically represented as a tensor program, in which the input/output and intermediate data are all cast as tensors, and a set of operators are applied to the tensor data to derive the final outcome. As a result, tensor compilers have recently gained great attention to accelerate NN execution, by applying general and specialized compile-time optimizations to compile the operator into high-performance kernels\u00b2 [5, 24, 36, 39, 40].\nTypically, kernels are written as loop nests, and each tensor compiler has its intermediate representation (IR) for describing and optimizing kernels. We here take Halide [24] as an example. Many tensor compilers use similar IR designs [5, 15, 32, 39]. Halide provides the separation of algorithm and schedule, where the algorithm is purely functional,"}, {"title": "Program Synthesis", "content": "Program synthesis is an approach that automatically generates a program that complies with several specifications, such as a set of input and output example pairs, or a set of assertions [11]. Theoretically speaking, the general concept of program synthesis can be applied to design new NN operators, but there exist several practical gaps. Traditional program synthesis only treated correctness as the target, such as TF-Coder [27] that synthesizes TensorFlow code to help programmers write correct code. But NN models, which are known to tolerate small errors, do not have a clear notion of correctness, while the goal is to improve inference accuracy. Also, existing program synthesis approaches can hardly scale, currently only limited to consider highly constrained program space. The complexity of loop nests in typical NN operators is well beyond their capabilities. We discuss these challenges in more detail in Section 3.\nNAS [17] relaxed the correctness objective to apply goal-directed program synthesis for NAS. They applied transformations to subgraphs in the model, which could generate new operators beyond traditional NAS. But they are still constrained by traditional operators like convolutions and matmuls, so the potential of intra-operator program synthesis remains unexploited. As a result, their speedups were light, as Section 9 will show. Ma et al. [21], on the other hand, pre-defined some fine-grained primitives common in traditional demosaicking pipelines to perform NAS, however they did not allow freely exploring new operators, either."}, {"title": "Motivation and Challenges", "content": "In this work, we aim to automatically and efficiently synthesize novel NN operators from very basic atoms in programming languages, with the hope of discovering new operators that have both high accuracy quality and high execution performance. Such automatic neural operator synthesis is highly profitable. State-of-the-art models today such as transformers and convolutional networks rely heavily on operators like attention and convolution that are constructed based on human insights. Automatic discovery of such operators can potentially create more promising model architectures.\nComparison with existing paradigms. Neural operator synthesis has a similar goal to performance-aware NAS, but aims to synthesize tensor programs at a much more fine-grained level rather than directly composing known operators like convolutions and matmuls. More specifically, operator synthesis involves writing various loop nests and the tensor expressions in the loop body. For example, for the convolution in Listing 1, the loops are implicitly defined by the iterators (i_Co, r_Ci, etc.), and the tensor expressions are realized with the coordinate expressions. Coordinate expressions are key to an operator because they specify how tensor elements are arranged and which are involved in the computation. Here, the simple addition of iterators (i_H + r_K_H) implies convolution, and the repeated uses of the reduction iterator (r_Ci) in two tensors imply contraction (a.k.a., tensor multiplication). The rich semantics of coordinate expressions can be exploited to synthesize novel operators. We note that such operator synthesis is impossible under existing NAS.\nAlthough it is always possible to lower existing operators to nested loops, it is not always possible to do the inverse. If a loop nest cannot be decomposed into several existing operators, it is likely we have discovered a novel operator.\nOn the other hand, operator synthesis is also significantly different from tensor compilers. Existing tensor compilers mostly preserve the semantic equivalence as discussed in Section 2.2. Thus they are unable to discover new operators. Actually, in operator synthesis, we deliberately avoid the exploration of semantically equivalent operators (see Section 6). If we synthesize equivalent operators, we would be very likely to redo those existing optimizations in tensor compilers. In this sense, tensor compilers and operator synthesis are orthogonal. We first synthesize novel operators, and then leverage tensor compilers to optimize its execution performance on the particular hardware.\nWe view neural operator synthesis as a specialized form of program synthesis in the NN domain. While traditional synthesis methods are limited to simple programs, we need to handle more complex operators with various nested loop structures and coordinate expressions. On one hand, the degree of freedom in directly writing loop nests and tensor expressions is huge, leading to an extremely large search space. On the other hand, as in performance-aware NAS, for"}, {"title": "Design Overview", "content": "We propose Syno, an end-to-end, automatic, and efficient framework for neural operator synthesis. Given a backbone NN model, Syno is able to synthesize novel linear operators with high quality (for accuracy) and high performance (for speed), which can be a drop-in replacement of the original operators (convolution, matmul, etc.) with the same input and output tensor shapes. The model topology and the non-linear activation layers are unaltered. Specifically, given the input and output tensor shapes, e.g., [N, Cin,H,W] and [N, Cout, H, W] for convolution, or [M,K] and [M, N] for matmul, Syno discovers novel operators that satisfy the accuracy and performance requirements, e.g., best performance with less than 1% accuracy loss. Note that the tensor shapes are specified as symbolic variables to allow one operator to fulfill different tensor sizes. The framework also supports a rich set of user-defined budgets such as FLOPs, memory usage, and number of parameters.\nWe limit our search in Syno to linear operators. First, linear operators are usually the performance bottleneck in NNs, constituting most of the computations, so reducing their complexity can have great gains. Second, non-linear activation layers like ReLU provide the non-linearity needed in NNs, so we keep them unaltered in the backbone model. Their performance impact is negligible because they can be readily fused into the previous operators by tensor compilers."}, {"title": "Primitives", "content": "Syno adopts a novel approach to synthesize candidate operators from a set of fine-grained primitives, whose semantics are defined with tensor coordinate expressions in a bottom-up way, as shown in Table 1. Compared with directly enumerating arbitrary raw arithmetic expressions as abstract syntax trees (ASTs) of integer expressions, this allows us to perform synthesis and search with the primitives in a more structured manner to ensure high quality."}, {"title": "Structured ASTS", "content": "Synthesizing expressions in a bottom-up way, i.e., first specifying the innermost atoms and then composing them, is common in program synthesis [11]. This is also a natural choice for NN operators. As can be seen in Listing 1, the output tensor are indexed by the output iterators such as i_H, and the coordinate expressions comprising the output iterators, e.g., i_H + r_K_H - K/2, are used to index the input tensors. Thus a straightforward way is to use the output iterators as atom coordinate expressions and enumerate more complicated expressions as the indices of input tensors.\nHowever, the operators synthesized in this way tend to have low quality. For example in Listing 1, if i_Co were only used in an expression i_Co / 2, then every two consecutive channels of out would have identical feature maps. This means tensor elements are replicated, and we perform redundant computations. To avoid this, we can require that i_Co % 2 must also be present in the enumerated coordinate expressions. This example inspires us to design a high-quality primitive that transforms a coordinate expression [i] with domain [N] to two coordinate expressions [i / B, i % B] of domains [N / B, B] where B divides N. Formally we write: [i]: [N] \u2190 [i / B, i % B]: [N / B, B].\nFurthermore, this bottom-up primitive also has top-down semantics, namely to flatten a tensor of shape [N / B, B] into [N] by merging the two dimensions. We name it as MERGE, which is actually a common tensor view operation. A view is just another way of accessing a tensor, and we identify that various arithmetic operations (+, *, /, etc.) on tensor coordinates actually correspond to views. For example, the addition of coordinate expressions is equivalent to extracting neighbor elements, which is UNFOLD. Similarly, adding a"}, {"title": "Advantages", "content": "The structured bottom-up primitives in Syno present several advantages. First, they ensure high quality of synthesized operators, in that they are differentiable [14] and do not discard input data or replicate data. The only relatively low-quality EXPAND and STRIDE are restrictively used and STRIDE is required to be paired with 1-to-many primitives to ensure the high-quality property. Second, they allow structured search. With the primitives, similar pGraphs are likely to share a subgraph, which makes the search space highly structural and enables the use of effective search algorithms (Section 7.2). Third, the primitives are expressive, as they are devised based on most basic arithmetic operations on coordinate expressions."}, {"title": "Design Details", "content": "To match an operator with different concrete input/output tensor shapes, and to support additional parameter variables in some primitives (e.g., MERGE needs a factor B), Syno uses symbolic shapes when synthesizing operators. We further split the symbols into two classes. Primary variables are for input/output dimensions, e.g., Cout, H. They are relatively large and thus are not allowed to appear in the denominator of a coordinate expression. Coefficient variables are only introduced by primitives, and are relatively small and allowed to appear in denominators. When enumerating the applicable primitives on a partial pGraph, the primitive parameters"}, {"title": "Canonicalization", "content": "The design space of synthesizing operators from our primitives is extremely large, with a lot of redundant operator constructs, especially those that can be readily discovered by tensor compilers. Take the partial pGraph Figure 3(a) as an example. In the left side, the topmost coordinate expressions are given by [i, j]: [A*B, C] \u2190 [C*i+j]: [A*B*C] \u2190 [(C*i+j)/(B+C), (C*i+j)%(B+C)]: [A, B*C].\nHowever, this simplifies to [i/B, C*(i%B)+j], corresponding to the right side [i, j]: [A*B, C] \u2190 [i/B, i%B, j]: [A, B, C] \u2190 [i/B, C*(i%B)+j]: [A, B*C].\nTo improve search efficiency, Syno uses a set of canonicalization rules to filter out uncanonical redundant candidates on the fly when new primitives are added to partial pGraphs (ISCANONICAL in Algorithm 1 Line 28). Syno does not aim to eliminate all redundancies, which is highly challenging, if not impossible, considering the rich primitive semantics. Also, comprehensive canonicalization checks are extremely expensive and sometimes undecidable [22]. On the other hand, Syno supports canonicalization rules to skip operators with similar computation results. We also note that the canonicalization rules in Syno are easily extensible. Developers can define new rules and plug them into the framework.\nContractions. Since weight tensors can be arbitrarily reshaped offline, there is no need to apply views to weights. Thus weight coordinates are directly used in SHARES for contractions. Moreover, since SHARE is symmetric, we always put weight coordinates as the right-hand-side inputs of SHARES.\nBetween views and contractions. We enforce a canonical order between views and contractions. The 1-to-1 views do not involve actual computations so they can be freely swapped with contractions. We thus push down all 1-to-1 views after contractions, as in Figure 3(b)."}, {"title": "Guided Search", "content": "We next describe the overall synthesis and search process in Syno. Section 7.1 discusses the bottom-up synthesis approach. A critical challenge is how to ensure the exact match"}, {"title": "Bottom-Up Synthesis with Shape Distance", "content": "As mentioned in Section 5, Syno performs bottom-up synthesis, starting from the output coordinates and iteratively applying sampled primitives for a limited number of steps. For example, as a subgraph of Figure 2, from output [i_H]: [H], we can get [i_H]: [H] \u2190 [i_H, r_K_H]: [H, K] \u2190 ([i_H, r_K_H], [r_K_H]): ([H, K], [K]) \u2190 ([i_H + r_K_H - K / 2], [r_K_H]): ([H], [K]). The weight ([K] here) does not need to be transformed further (Section 6), so we use the term shape to refer to the shape of the first tensor (data input), which is [H] in this case.\nThe data tensor shape of a complete pGraph should match exactly with the specified input shape (input in Algorithm 1 Line 12). While synthesizing with primitives ensures high quality, it also becomes hard to control the shape of the coordinates after applying primitives on a partial pGraph. Ideally, after flexibly exploring various primitives, when the partial pGraph gets close to its maximum size limit, the last few primitives need to move towards exactly matching with the input dimensions. For example, if the shape of the current partial pGraph is [Cin, s\u00af\u00b9*H, s*W, k], we can apply [Cin, s\u00af\u00b9*H, s*W, k] \u2190 [Cin, s\u00af\u00b9*H, s, W, k] \u2190 [Cin, H, W, k] \u2190 [Cin, H, W].\nWe propose a novel metric named shape distance as the minimum number of required primitives added onto the current pGraph to reach the input. In the above example, the shape distance of [Cin, s\u00af\u00b9*H, s*W, k] is 3. If the remaining allowed number of primitives is less than the shape distance, we can immediately terminate the current pGraph and backtrack (Algorithm 1 Line 20). This avoids deviating too far from the input coordinates.\nWe design a systematic method in Syno to compute the shape distance between the current shape and the desired input. We first divide the coordinates in the two shapes into reshape groups, where future primitives are only applied to the coordinates within each group to match them, but not across groups. In the above example, we can have three"}, {"title": "MCTS-Based Search", "content": "Our search algorithm is based on MCTS. We formulate our search problem as a Markov decision process, where we transit from one partial pGraph to another in the search space, with the action space being the primitives. The final states are complete pGraphs. The optimization goal is operators with both high accuracy and high inference speed. Given that operators with high accuracy are much less than those with high speed, we set a hard upper limit for FLOPs and use accuracy as the reward for MCTS to guide it to learn how to find expressive operators within a given FLOPs budget. We"}, {"title": "Code Generation", "content": "We implement two code generators for accuracy and speed evaluations, respectively. First, a PyTorch code generator is built to make use of the already highly-tuned operator libraries for training. Using the top-down semantics, each view primitive is lowered to its counterpart in PyTorch, and each contraction primitive is lowered to an einsum [25] expression, which is a general method for performing tensor contractions. The primitives are lowered in the topological order to ensure that dependencies are satisfied.\nAlthough the PyTorch approach is fast in compilation, it lacks compile-time optimizations such as operator fusion and other tiling schemes. We further build a TVM code generator, following the bottom-up semantics, to evaluate all coordinate expressions according to the kernel graph, and leverage TVM [5] for extensive compiler optimizations on specific hardware, e.g., our mobile CPUs and GPUs in Section 9.\nSome optimization passes unique to Syno are designed. An important one aims to automatically insert intermediate stages to eliminate redundant computation. Consider the example in Figure 4. A trivial code generator creates a loop nest of (H/s)*k*s iterations computing Y[i] = \u03a3\u03af\u03ba \u03a3\u03afX[i + ik - k/2 + s*is] as in the left side, but this is mathematically equivalent to Z[i\u2032] = \u2211\u00a1\u00b8X[i\u2032 + s*is], Y[i] = \u03a3iZ[i + ik \u2013 k/2], which corresponds to the partitioned subgraphs on the right. By doing so we reduce the FLOPs from k*H to (1+k/s)*H.\nGenerally speaking, FLOPs depend only on the output dimensions and the REDUCES, which are the spatial loops and reduction loops in the loop nest. The number of iterations is their product. In the case of 1-to-many primitives like UNFOLD, the output dimensions are increased, so if we perform any REDUCE after this, FLOPs are unnecessarily increased because we are evaluating k copies for each element.\nThis issue is unique to the Syno IR. We propose an optimization called rfactor, i.e., factorization of reduction domain, to deal with it. We enumerate the order of performing reductions, i.e., the order of lowering each REDUCE. If a REDUCE"}, {"title": "Evaluation", "content": "We follow Primer [28] to allocate a 30-minute training period on GPT-2 for each searched operator and compare their final language perplexity results. We then extend the training for the best-performing operator and the original model to reach 100,000 steps as shown in Figure 10. When searching for substitutions for the QKV projection, our best operator achieves a 1.1\u00d7 training speedup and reduces the perplexity to 99, outperforming the original model's perplexity of 111. More specifically, our operator constructs the original"}, {"title": "Ablation Studies", "content": "Canonicalization. To show the effectiveness of Syno canonicalization rules, we draw 6452 samples without canonicalization, among which only 86 are canonical. This implies that canonicalization cuts more than 70\u00d7 redundancy. Table 2 shows the canonical rates for different pGraph sizes.\nShape distance. To check the effectiveness of the shape distance metric, we evaluate the successful rate of random sample trials with or without shape distance. On a server machine with 192 virtual cores, 253 distinct operators are found after 5 million trials in 68.33 seconds, with shape distance enabled. However, without the shape distance, 500 million trials in 180.51 seconds yield no valid operators. Thus, shape distance is vital for avoiding useless synthesis."}, {"title": "Conclusions", "content": "This paper advocates the paradigm of neural operator synthesis, which automatically discovers novel NN operators with good accuracy and performance. A practical framework named Syno has also been implemented, using a rich set of fine-grained primitives to construct operators, applying canonicalization to eliminate redundancy, guided by a distance metric to improve synthesis efficiency. Syno is able to discover better NN operators than existing ones on various models, with higher performance and minor accuracy loss."}, {"title": "Shape Distance", "content": "Here we provide a formal proof for the shape distance formulation.\nFirst, we provide some technical details. The process of matching the dimensions in a partial pGraph to the input tensor is called finalization. The dimensions in a partial pGraph that belong to the data tensor are called finalizable dimensions, meaning that they may be used to finalize. For example, in the conv2d pGraph, if we just ignore the batch size N, then right before finalization, the topmost dimensions in the partial pGraph is ([Cin, H, W], [Cout, Cin, K, K]), and the finalizable dimensions are just [Cin, H, W].\nThe construction of a pGraph is separated into several stages, where each stage only samples specific kinds of primitives. Basically, there are Reduction Stage, Normal Stage, and Contraction Stage. In the Reduction Stage, only REDUCES are generated. In the Contraction Stage, only SHARE can be generated, and we define a helper primitive CONTRACTION to generate all needed SHARES for a contraction with a weight tensor, and assign all the needed dimensions to the weight tensor. We here note that CONTRACTION is a 1-to-many primitive, in that it may assign some dimensions to the weight tensor, reducing the number of elements in the finalizable dimensions. Normal Stage generates all the other primitives. To construct a pGraph, there is only 1 Reduction Stage which generates all the required REDUCES, and then the Normal Stage and Contraction Stage are performed alternately. The conv2d pGraph can be constructed in 3 stages, namely:\nReduction Stage [Cout, H, W] \nReduction Stage [Cout, H, W] \nShare\n\u2190\n\\frac{\\vec{p} \u00b7 \\vec{q}}{\\Vert \\vec{p} \\Vert \\Vert \\vec{q} \\Vert}\n\u2190"}]}