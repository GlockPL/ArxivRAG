{"title": "Principles for Responsible AI Consciousness Research", "authors": ["Patrick Butlin", "Theodoros Lappas"], "abstract": "Recent research suggests that it may be possible to build conscious Al systems now or in the near future. Conscious AI systems would arguably deserve moral consideration, and it may be the case that large numbers of conscious systems could be created and caused to suffer. Furthermore, Al systems or Al-generated characters may increasingly give the impression of being conscious, leading to debate about their moral status. Organisations involved in AI research must establish principles and policies to guide research and deployment choices and public communication concerning consciousness. Even if an organisation chooses not to study Al consciousness as such, it will still need policies in place, as those developing advanced Al systems risk inadvertently creating conscious entities. Responsible research and deployment practices are essential to address this possibility. We propose five principles for responsible research and argue that research organisations should make voluntary, public commitments to principles on these lines. Our principles concern research objectives and procedures, knowledge sharing and public communications.", "sections": [{"title": "1. Introduction", "content": "Whether any Al system could be conscious is a matter of great uncertainty. However, if AI consciousness is possible, it may be near at hand. Recent progress in AI has been astonishingly rapid. Furthermore, the features of the brain that are responsible for consciousness, according to some prominent neuroscientific theories, are likely to be reproducible in AI systems (Butlin, Long et al. 2023).\nThis suggests that research on Al consciousness, which has largely been philosophical until now, may be about to take an empirical turn. There are at least three possible objectives that could motivate private and public institutions to undertake empirical research on Al consciousness. First, researchers may attempt to build conscious systems, or systems reproducing elements that are connected to consciousness, in the belief that this will boost capabilities or make Al systems safer (Bengio, 2019, Graziano, 2017). Second, researchers may aim to learn about consciousness for its own sake. In this case, the objective could be either to develop a conscious Al system (Dossa et al. 2024) or to use AI as a tool for more general consciousness research, such"}, {"title": "2. Consciousness in A\u0399", "content": "In this section, we survey expert opinion on the prospect of consciousness in AI. As we will explain, some experts believe that it is likely to be feasible to build conscious AI systems in the near future. They hold what we call 'positive views'. Others are far more sceptical, holding 'negative views'. But positive views are sufficiently plausible and prevalent that the prospect of consciousness in AI should be taken seriously.\nIn perhaps the most systematic recent treatment of this topic, a large multidisciplinary team used neuroscientific theories of consciousness to draw up a list of fourteen \u2018indicators' of consciousness\u2014properties that Al systems might have that would make them more likely to be conscious (Butlin, Long et al. 2023). This paper did not find any existing AI system with more than a few of the indicators, but did argue that, in most cases, it appears to be possible to build systems with each of the indicators using current techniques. Where there are doubts about this, they arise because the indicators are specified using terms for which we lack operational consensus definitions (like \u2018belief\u201d), rather than because there are clear technological obstacles.\nA notable feature of this paper is that it assumes, and does not argue for, the philosophical thesis of computational functionalism about consciousness. This is the claim that it is necessary and sufficient for a system to be conscious if it implements computations of the right kind. Computational functionalists believe that human consciousness depends on the computational processes implemented by our brains, rather than, for instance, the fact that they are made up of networks of living cells. Computational functionalism is a mainstream view in philosophy of mind, although certainly not the consensus (Seth, 2024). The paper concludes that \u2018the evidence we consider suggests that, if computational functionalism is true, conscious Al systems could realistically be built in the near term' (p. 6).\nA further positive view about the prospect of Al consciousness is expressed by David Chalmers (2023), focusing on large language models (LLMs). Chalmers considers four pieces of evidence that suggest consciousness in LLMs\u2014that they 'report' consciousness, that they give the impression of being conscious to some users, that they exhibit impressive conversational abilities, and that they have a degree of general intelligence\u2014but claims that none of these yet constitutes strong evidence. He also considers several features that standard LLMs are said to lack that might be thought to be necessary for consciousness, including biology, senses and embodiment, world models and self-models, recurrent processing, a global workspace, and unified agency. He sees most of these features as reasonably likely to be found in successors to LLMs developed in the next decade. While he acknowledges biology as the exception, he also finds the claim that biology is necessary for consciousness 'highly contentious'. Although he"}, {"title": "3. Why AI Consciousness Matters", "content": ""}, {"title": "3.1 The Ethical Treatment of Conscious Artificial Systems", "content": "One reason why Al consciousness research matters is that consciousness or the related property of sentience may be sufficient for being a moral patient. An entity is a moral patient if it matters morally 'in its own right, for its own sake' (Kagan, 2019). Most people believe that almost all humans and many animals are moral patients, even if they think that the moral constraints on the ways in which we may treat animals are different from those concerning humans. Being a moral patient is a matter of being owed some moral consideration, and is compatible with being the object of a potentially wide range of different duties or obligations. Moral patienthood contrasts with the status of entities that matter morally but not \u2018in their own right and for their own sake'.\nFor example, a person's wheelchair may matter morally because it plays a large role in allowing them to flourish, but in this case it is clear that the wheelchair itself matters only derivatively, rather than in its own right. It is less clear that the moral significance of landscapes and works of art is merely derivative, but plausible that these entities matter in a different way from humans and animals, because they lack interests of their own. They arguably matter morally in their own right, but not for their own sake.\nIn considering the moral significance of consciousness, it is helpful to distinguish between consciousness and sentience, understood as the capacity for conscious experiences that feel good"}, {"title": "3.2 The Social Significance of Attributions of Consciousness to AI", "content": "Whether or not we build conscious AI systems, it is likely that we will build systems that give a compelling appearance of consciousness. In particular, as LLM-based systems are increasingly"}, {"title": "4. Principles for Responsible Research", "content": "We claim that organisations pursuing AI consciousness research\u2014or advanced Al research more broadly should adopt principles to mitigate the risks outlined in the previous section. We suggest five such principles in this section, stated in table 1. Our intention is that by adopting these principles, or similar ones, organisations will be more likely to achieve two higher-level goals. These are: to avoid contributing to any future mistreatment of AI moral patients; and to promote understanding of concepts, arguments and evidence concerning consciousness among the public and professionals in relevant fields. In this section, we explain our proposed principles and discuss some related issues. The five subsections of this section correspond to our five principles."}, {"title": "4.1 Objectives: Understanding AI Consciousness", "content": "In general, research on consciousness in Al risks contributing to the creation of future Al moral patients, which would be liable to be mistreated. Successful research projects in this area will yield information that could be useful to actors seeking to build conscious Al systems. Such projects may also yield information about how to build systems with useful new capabilities. So"}, {"title": "4.2 Development: Value and Constraints", "content": "Is it permissible for organisations to seek to develop conscious Al systems, or to build systems that they believe are likely to be conscious? Our view is that this is permissible only under strict conditions. Building experimental systems is likely to be necessary to make substantial progress in understanding AI consciousness, so it may be done responsibly in pursuit of the objectives stated in principle 1. Even in this case, however, suitable safeguards should be in place.\nSeveral kinds of measures are possible to minimise the potential suffering of conscious Al systems, which could be put in place when building experimental systems. These include: controlling the breadth of deployment and the ways in which systems are used; assessing the capabilities and potential for consciousness of systems at several stages of development and deployment; increasing capabilities gradually and only introducing those that are needed for the system's intended purpose (as far as this is possible, given the difficulties of predicting the capabilities of some systems in advance); and controlling access to information that would enable irresponsible actors to build systems that may be conscious. Some of these kinds of measures can also help to protect humans from risks from advanced systems.\nIt should be clear how controlling deployment and use will help to minimise the potential for suffering. This could mean running fewer instances of models, for less total time, and using them in a narrower range of ways. It would typically involve not granting public access, or granting it only with constraints, and using systems only in the ways necessary for the purposes of particular experiments. We discuss the other kinds of measures, including assessments, gradual development and control of information, in sections 4.3 and 4.4.\nIn considering whether and how to experiment with systems that may be conscious, it may be helpful to consider existing principles for ethical experimentation on human and animal subjects (Long et al. ms). Principles for the treatment of human subjects include appeals to respect, compassion and justice (Resnik, 2018). The first two entail that research on humans usually requires both consent and an endeavour to minimise risk and harm. Meanwhile, principles for the"}, {"title": "4.3 Phased Approach: Gradual Development with Monitoring", "content": "Among the safeguards that may be used when developing systems that may be conscious are making frequent assessments of systems' potential for consciousness and increasing capabilities gradually. These two kinds of measures are naturally combined as ways to prevent developments in technology from outrunning our understanding.\nThere are significant challenges involved in determining whether particular Al systems are conscious. However, we do have methods to make qualitative assessments of the probability of consciousness in particular systems, and research to improve these methods is ongoing (Butlin, Long et al. 2023, Long et al. ms). As methods improve, organisations should use them to assess whether systems are likely to be conscious at several stages of development. As Shevlane et al. (2023) describe in the context of evaluations for AI safety, there are reasons to evaluate systems before and during training, before deployment, and later, after deployment, when more is known about their capabilities. The process of assessing systems for consciousness should be formally instituted in organisations' policies and should be audited by independent experts.\nIt is likely to be valuable for organisations to also consult with outside experts on decisions about whether to proceed with projects that may involve building conscious systems. Expert consultants can provide advice that can help to make the cost-benefit judgements discussed in the previous section, especially by providing alternative perspectives reflecting different biases and concerns from those inside the organisation. In making some momentous decisions organisations might also engage with authorities and the public (Birch, 2024).\nIn addition to assessing existing systems for consciousness and reflecting carefully on costs and benefits before developing new ones, organisations should seek to make gradual, limited progress in capacities linked to consciousness in developing new systems. The purpose is to minimise the risk of developing systems with the capacity for much richer conscious experiences than we realise. The potential problem to be avoided here relates to the concept of overhangs, which has been identified in Al safety research: underexplored systems may have hidden capabilities, or latent attributes that would allow leaps in performance if unlocked (Dafoe, 2018).\nPutting these points together, we advocate a phased approach in which organisations work to understand the systems they have already built before moving on to new projects. They should then apply careful scrutiny to proposals, considering whether the proposed systems might be conscious and whether new features intended to enhance capabilities are warranted. We summarise this view in our third principle:"}, {"title": "4.4 Knowledge Sharing: Transparency with Limits", "content": "Given that the kind of AI consciousness research we would endorse has the objective of improving understanding, research organisations should share what they learn. Knowledge sharing makes for faster progress towards understanding through collaboration and scrutiny, and is essential for the value of understanding to be realised. In this case, researchers and authorities can only use knowledge to protect potential AI moral patients and promote public understanding if they can access it. As far as possible, research organisations should make information about their work available to the public, authorities and the research community.\nHowever, there are limits to responsible knowledge sharing. If a research team succeeded in building a system that they believed was conscious, they should not generally make the full technical details of the system public, because this would make it possible for others who might mistreat the system to replicate it. This would be especially important if the system had capabilities that would incentivise its replication and (mis)use. In cases where information is sufficiently sensitive, for reasons of this kind, it should be protected and made available only to vetted experts and authorities.\nThere are some areas of research where the information hazards are so great that the research should not be conducted (Bostrom, 2011). For example, some forms of research on biological weapons are prohibited by the Biological Weapons Convention (United Nations, 1972). However, as we have argued in responding to Metzinger's moratorium proposal, we do not believe that AI consciousness is a case of this kind. Some research on Al consciousness may be beneficial enough to be worth conducting even though it would generate hazardous information, partly because this sensitive information could be adequately protected.\nOur fourth principle is:"}, {"title": "4.5 Communication: Acknowledging Uncertainty", "content": "How research organisations communicate about AI consciousness matters because of the dangers of poorly-informed public opinion on this topic. Organisations' first priority in communications should be to avoid misleading the public. There are various ways in which communications might be misleading.\nOne potential problem is overconfident dismissals of the possibility of AI consciousness. It may be tempting for research organisations to dismiss this possibility so as to avoid disruptive attention. As well as being misleading, however, overconfident dismissals may discourage"}, {"title": "5. Maintaining Responsible Behaviour", "content": "Any organisation engaged in Al consciousness research will be made up of individuals with a variety of concerns and incentives, and these are liable to change over time. Even if such organisations initially endorse the five principles that we outline in this paper, they could be subject to incentives to abandon them in future. For example, a company may have a strong commercial incentive to ignore indications of consciousness in one of its systems if giving proper attention to these indications would interfere with deployment. Or an organisation may be tempted to pursue Al consciousness, and advertise themselves as doing so, if this will win them desirable attention. Furthermore, research always requires funding, so organisations will always be under pressure to please investors or other funders.\nThis means that it is important for organisations that currently aim to pursue AI consciousness research responsibly to act to ensure that they will continue to behave responsibly in the future. While it may be difficult for organisations to bind themselves irrevocably, it is possible to institute policies that will disincentivise irresponsible behaviour in the future.\nAlthough different strategies are likely to be appropriate for different organisations, the following suggestions illustrate the kinds of actions we think organisations should consider.\nFirst, one element of a strategy might be to make a public commitment to the principles outlined here, or to a similar set. This kind of outward-facing action could be supplemented by helping to establish or support external organisations whose function is to help promote responsible behaviour concerning AI consciousness. For example, a large company could provide funding for research elsewhere on the ethics of AI consciousness, or spin out a company specialising in consciousness evaluations.\nOne function of such external organisations might be to communicate with the public, experts and authorities in ways that are more difficult for large research organisations themselves. However, external organisations could also audit AI consciousness research, as we have suggested. For either of these functions to be performed effectively, external organisations must be independent, as far as possible, from those they are auditing or potentially criticising, so support must be set up in a way that promotes this independence\u2014for example, an auditor could provide services to several competing companies.\nSecond, research organisations should develop policies for reviewing projects and making choices that require attention to the ethical issues and principles discussed in this paper. They could also write conditions concerning responsibility into their codes of institutional values and practices, and into the rubrics they use to assess employee performance. Somewhat more substantively, they could appoint non-executive directors with the role of monitoring the organisation's adherence to principles like those described here, and using their power to maintain it.\nUltimately, we recognize that these measures may be ineffective in cases where strong incentives drive irresponsible behavior. However, their value lies in tipping the balance toward"}, {"title": "6. Conclusion", "content": "If building conscious AI systems is becoming a realistic possibility, then organisations involved in advanced Al research should adopt policies addressing this prospect. Moreover, some organisations may justifiably wish to explore AI consciousness, but they must carefully consider how to do so responsibly. To guide this effort, we have proposed five principles for responsible research in this emerging era, in which conscious Al seems feasible but there is considerable uncertainty and risk."}]}