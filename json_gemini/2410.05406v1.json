{"title": "Synthesizing Interpretable Control Policies through Large Language Model Guided Search", "authors": ["Carlo Bosio", "Mark W. Mueller"], "abstract": "The combination of Large Language Models (LLMs), systematic evaluation, and evolutionary algorithms has enabled breakthroughs in combinatorial optimization and scientific discovery. We propose to extend this powerful combination to the control of dynamical systems, generating interpretable control policies capable of complex behaviors. With our novel method, we represent control policies as programs in standard languages like Python. We evaluate candidate controllers in simulation and evolve them using a pre-trained LLM. Unlike conventional learning-based control techniques, which rely on black box neural networks to encode control policies, our approach enhances transparency and interpretability. We still take advantage of the power of large AI models, but leverage it at the policy design phase, ensuring that all system components remain interpretable and easily verifiable at runtime. Additionally, the use of standard programming languages makes it straightforward for humans to finetune or adapt the controllers based on their expertise and intuition. We illustrate our method through its application to the synthesis of an interpretable control policy for the pendulum swing-up and the ball in cup tasks. We make the code available at https://github.com/muellerlab/synthesizing_ interpretable_control_policies.git", "sections": [{"title": "I. INTRODUCTION", "content": "Control systems and artificial intelligence (AI) are two fields with immense practical impact, yet their integration often faces significant challenges. While control theory offers reliable methods to stabilize and steer complex systems, recent advances in machine learning (ML) have dramatically improved our ability to leverage large-scale data. However, the use of black box AI models, particularly neural networks, is not always suitable for critical control applications where transparency and verifiability are essential. Our work introduces a solution to this problem. Inspired by recent breakthroughs in Combinatorial Optimization [1], we propose representing control policies as programs written in standard languages like Python, and evolving them using a pre- trained LLM and a simulation framework for evaluation. Our approach still leverages the power of large Al models, but shifts the abstraction layer, moving the black box component from the runtime execution to the policy design phase (an outline of our method is shown in Fig. 1). The output of our control synthesis framework is a fully interpretable program representing a control policy for the system and task of interest. The key advantage of our approach is the use of code as the policy representation. Programming languages, being our primary tools for instructing machines, are inherently interpretable. A control engineer or system operator can read, understand, and even modify the policy directly, without needing to decipher complex neural network architectures or weight matrices.\nIn the following sections we frame in more detail this research by highlighting relevant previous work. In Section II we describe our methodology and infrastructure. Then, in Section III we show an application of the proposed method and in Section IV we provide a brief discussion and conclude by highlighting possible future directions."}, {"title": "A. Large Language Models", "content": "Recent developments in Large Language Models (such as [2]) have opened novel research areas focused on their applications. One popular application is LLMs for code. In fact, a lot of effort has been put in the development, training, and finetuning of large models for code generation [3]-[5]. With the rise of these models for code, also evaluation and benchmarking of these models have been popular research directions [6], [7]. On top of LLMs for code generation, extensive system-level research has been proposed with the aim of integrating LLMs with additional components to achieve more complex tasks than with single prompt engineering. A popular example is the combination of LLMs with evaluators, i.e. programmatic ways of scoring their outputs. This paradigm has been applied in various contexts, such as automated reasoning [8], code debugging [9], and algorithm design [10], [11]. The integration of this generation-evaluation technique in an evolutionary procedure alleviates LLM hallucination, and in some cases leads to self-improving loops which output high performance programs and new knowledge [1]. It is important to highlight that these results are not achieved thanks to domain-specific knowledge contained in the LLM training dataset. Instead, they are achieved through the combination of the capability of LLMs to generate functional code, and evolutionary optimization techniques. Our work shows how to effectively apply these LLM-based frameworks to control systems design."}, {"title": "B. Learning-Based Control", "content": "Control systems have benefited in many ways from learning-based techniques [12]. A popular and well-studied field is adaptive control, which revolves around the identification of discrepancies between a system's dynamics model and its real-world behavior, and update of model parameters to compensate this mismatch [13], [14]. Learning-based methods have found successful application in highly repetitive scenarios, where error patterns are iteratively approximated and incorporated in the controller over multiple executions of the same task. These techniques fall under the iterative learning control category [15]. Another family of methods, typically referred to as imitation learning, consists of approximating a control policy for a given task through a set of expert demonstrations (i.e. trajectories accomplishing the task of interest) [16]. Reinforcement learning (RL) represents a significant shift in control paradigms, offering a way to approximate optimal control policies through interaction with the environment [17]. Recent advances in RL have led to important results in areas where traditional methods fall short, such as locomotion [18], [19] and manipulation [20]. However, one fundamental issue preventing all these techniques to be safely and reliably deployed in real world contexts is their lack of interpretability."}, {"title": "C. Interpretability in Learning-Based Control", "content": "In the context of Machine Learning systems, interpretability is defined as the ability to explain or to present in understandable terms to a human [21]. This property is particularly crucial in the context of control systems and automation, where guaranteeing safety of operation is in many cases essential. In such safety-critical contexts, being able to inspect and understand which building block of a system led to a failure is of fundamental importance, and is not possible with black box components such as neural networks. Interpretability has been investigated through several different approaches [22]. One of them is the use of interpretable architectures for learning-based control. Some examples are decision trees [23] and fuzzy controllers [24], [25]. These techniques facilitate the tracing of the decision- making process, providing some level of insight into the system's behavior. Another approach that has been proposed to make learned components more interpretable is encouraging sparsity in parameter or weight matrices [26], as sparsity is often used as a proxy for interpretability [27]. We argue that interpretability in learning-based control should encompass not only the ability to read and (partially) understand the system's logic, but also the capacity to modify it with a clear understanding of the consequences. By representing control policies in a standard programming language, we aim to take a step towards the intuitive understanding required for the practical, safe deployment of learning-based control systems."}, {"title": "II. METHODOLOGY", "content": "Our work addresses the problem of finding a high- performance control policy for a control task of interest. We first describe the problem fundamentals, following the formalism of [28], and then the algorithmic aspects of our interpretable control synthesis approach.\nWe are concerned with the study of a discrete-time dynamical system with dynamics in the form\n$x_{t+1} = f(x_t, u_t)$,\nwhere $t \\in N$ is the time index, $x_t \\in R^n$ is the state of the system at time step t, and $u_t \\in U \\subset R^m$ is the control input. At each time step t, a stage reward\n$r_t = g(x_t, u_t)$\nis incurred. The general objective is to find a control policy $h(x_t)$ to maximize the cumulative reward\n$R = \\sum_{t=0}^{T} r_t$\nwhere T is a specified time horizon. It is well known that this problem is in general very complex, and in many cases only approximate solutions can be found. Our goal is to produce an approximate solution (i.e. a control policy approximately maximizing the cumulative reward) that is interpretable. A typical approach would be to pick a functional representation for the control policy (e.g. linear feedback, neural network) and optimize its parameters to maximize the cumulative reward. To guarantee interpretability, in our setting we represent the function h(\u00b7) directly as a program policy() in Python. Therefore, our control policy is encoded as\n$u_t = policy(x_t)$.\nThe goal is then to find a high-performing control program policy*(\u00b7) by approximating the solution to the following"}, {"title": "A. Specification", "content": "The input to our general synthesis framework is a specification file (Fig. 1a). The file is composed by three main parts:\n\u2022 A description of the control task to solve, together with packages and libraries the LLM-generated code can use;\n\u2022 An initial candidate control program in the form of starter code, which will be evolved across different iterations;\n\u2022 The implementation of the evaluation function used to score candidate programs.\nThe different parts of the specification are parsed and used at different stages of the pipeline. The starter code is pasted in the initial prompt (Fig. 1b) and fed to the LLM in the preliminary stages, when better programs have not yet been produced. The evaluation function is used in the Program Evaluation block (Fig. 1d) to quantify the performance of candidate programs. An example of a general specification structure for our control synthesis method is shown in Fig. 2."}, {"title": "B. Prompt Construction", "content": "The prompt (Fig. 1b) is a crucial component in the pipeline, as it triggers and steers the LLM generation. At each iteration, a prompt is constructed by concatenating two previously generated high-performing programs. During the initial phases of the algorithm execution, only the starter code (from the specification file) is available, and is directly pasted into the prompt. As the evolution progresses and higher-performing programs are generated, these are used in the prompt instead of the starter code. The prompt also contains an instruction to the LLM to improve upon the policies provided. The sampled policies are inserted in the prompt in the form policy-v0, policy-v1, and the function header of the policy to generate (in the form policy_vx) is appended at the end. An example of a general prompt structure is shown in Fig. 3."}, {"title": "C. Program Generation", "content": "A pre-trained LLM is the generation engine of the Program Generation block (Fig. 1c), in which candidate control programs are sampled. The LLM is queried with a prompt containing two high-performing control programs generated in previous iterations, and is instructed to improve them. At each token generation step the LLM produces a vector containing a numerical score for every token in the vocabulary. For token i, there is a score xi. These scores are then normalized to obtain a distribution to sample from. The sampling probabilities are obtained as\n$P_i = \\frac{e^{x_i/T}}{\\sum_{i}e^{x_i/T}}$,\nwhere T is the LLM temperature. A low T makes the token distribution narrow, a larger T makes it more uniform. In our implementation we set T = 1. The token generation is also steered by other hyperparameters. One of them is top_p (with value within [0,1]), which sets the percentile of the token options to consider. If top-p is 1, then the whole vocabulary is considered, otherwise a fraction is discarded and the probabilities pi are renormalized. We set top-p to 0.95. The repeat_last_n parameter sets the length (in number of tokens) of the sliding window used to check for repetition. We set repeat_last_n to 15 (i.e. the 15 previously generated tokens are not considered for sampling)."}, {"title": "D. Program Evaluation", "content": "Candidate control programs are parsed from the LLM output and fed to the Program Evaluation block (Fig. 1d). The evaluation consists of testing the programs and quantifying their performance using the evaluation function provided in the specification file. The candidate control policy is deployed in a simulation environment, in closed loop with the dynamics of the system of interest. The performance in simulation is quantified by a numerical score (in our case, the return of eq. 3). The score is then associated to the candidate control program to make a program-score pair. Syntactically incorrect programs are discarded, while promising program- score pairs are stored in the programs database, from which they are sampled to be added to the subsequent prompts, and evolved. The simulation is run inside a sandbox to prevent issues (syntactic or of other nature) contained the LLM- generated control policy to interrupt the outer optimization routine."}, {"title": "E. Programs Database", "content": "The generated high performing programs are stored in the programs database (Fig. 1e). At each iteration, two programs are sampled to be integrated into the prompt and fed back to the LLM, which is instructed to provide higher performing variants. To discourage getting stuck in local optima, an island approach is implemented, where different instances of the program search are run in parallel, independently [29]. Therefore, an equivalent number of program populations are stored and evolved. Periodically, the islands containing less promising populations are emptied and re-initialized with the best performing programs from other islands. When constructing a prompt, a two stage sampling procedure happens: first, an island is sampled, then programs contained within the selected island are sampled to be added to the prompt. More detail about the algorithm implementation can be found in [1]. In our case, we use 10 independently evolved islands."}, {"title": "III. SET UP AND CASE STUDIES", "content": "In the following we provide more detail about the practical aspects of the implementation of our method. We then introduce the control tasks that we found interesting for its application."}, {"title": "A. Setup", "content": "We run our control synthesis framework on a workstation equipped with an NVIDIA RTX 3090 GPU, which has enough memory to fully load the LLM for inference. As a simulation framework for program evaluation we use the open source simulator MuJoCo [30], through the well known DeepMind Control Suite [31] library. As LLM for program generation we used an 8-bit quantized version [32] of StarCoder2-Instruct [4], an open source 15 billion parameter model finetuned on code generation from natural language instructions. We applied our method to the pendulum swing-up and ball in cup tasks included in the DeepMind Control Suite. A supplementary video showing examples from the case studies can be found at https: //youtu.be/7T7yRGya-q8."}, {"title": "B. Pendulum swing-up", "content": "The pendulum swing-up task with input constraints is not easily solvable through a classical linear control. The pendulum has to accomplish a number of oscillations to accumulate enough energy, and then swing into the upright configuration. The maximum applicable torque is 1/6th as required to lift it from motionless horizontal. The dynamics equations, with the angle 0 representing the deviation from the upright position, are\n$\\ddot{\\theta} = -\\frac{g}{l} sin \\theta + b \\dot{\\theta} = u$,\nwhere $g = 9.81 ms^{-2}$ is the gravitational acceleration, $l = 0.5m$ is the length of the pendulum (massless) rod, $b = 0.4s^{-1}$ is a normalized damping coefficient, and u is a normalized torque input. When simulated, the dynamics equations are discretized through a semi-implicit Euler method and a sampling time of 15 ms (details in [30]). At each time step t a reward rt is computed. The overall score of the candidate control policy is the summation of individual rewards, i.e. R defined in eq. 3. For the pendulum swing-up task, the stage reward is defined as\n$r_t = \\begin{cases} 1 - \\frac{\\theta_t^2}{\\pi^2} - 0.1 u_t \\quad \\text{if } |\\theta_t| < 0.5\\\\ 0.1 u_t \\quad \\text{otherwise.} \\end{cases}$\nThis reward function led to the best results, and was shaped through trial and error. The goal is to reward configurations which are close to the upright position, and penalize the input. However, another common behavior arising is an indefinite uniform spinning after the first transient phase.\nTo find the high scoring policies for the pendulum swing- up task, our framework generated in the order of 104 sample programs per individual run. One run in particular led to the control policy shown in Fig. 4. As it is possible to observe, the policy is highly compact and interpretable. Rewritten in mathematical terms, it corresponds to\n$u_t = \\begin{cases} 5 \\theta_t - 0.9 \\dot{\\theta}_t \\quad \\text{if } |\\theta_t| < 0.5\\\\ sgn(\\dot{\\theta}) \\quad \\text{otherwise,} \\end{cases}$\nwhere sgn() is the sign function. Unpacking the policy, it applies positive work whenever the pendulum is not within a certain angular threshold from the upright position. Otherwise, it is a linear feedback controller. A user could make changes to this policy, such as tuning the linear control gains, or making the torque input a smoother function of the angular velocity, and keep iterating with the LLM in the loop. We also note that a Lyapunov indirect (local) stability analysis could be trivially carried out on eq. 9.\nPlots of the closed-loop dynamics of the swing-up task are shown in Fig. 5. It is possible to observe that in the first stage the policy is of bang-bang type, and in a second stage it switches to a linear feedback."}, {"title": "C. Ball in Cup", "content": "The ball in cup system is composed by a two-dimensional double integrator (the cup) and a ball attached to it through a string (which is a unilateral distance constraint between the two entities). The system is planar. A schematic of the system is shown in Fig. 6a. The task is to find a control policy for the cup (independently actuated along the horizontal and vertical direction) to catch the ball. In this scenario, the policy outputs a reference positions for the cup (Xref, Zref), which are then fed to a lower level linear controller. Therefore, this task is higher dimensional than the pendulum swing-up, but does not involve any stabilization of the system. For this task, the reward is defined as\n$r_t = \\begin{cases} 0.1 \\sqrt{u_{\\text{ball}}}\\\\ 1 \\end{cases} \\begin{matrix} \\text{if ball outside cup} \\\\ \\text{otherwise,} \\end{matrix}$\nwhere Xball, Zball, Xcup, Zcup are the x and z coordinates of ball and cup (at time t, omitted for simplicity), \u03b8t = atan2(xball - Xcup, Zball Zcup) is an angle measuring how close the ball is to be vertically aligned with the cup, and $u_{\\text{ball}} = \\sqrt{\\dot{x}_{ball} + \\dot{z}_{ball}}$ is the norm of the ball velocity (also at time t). Also in this case, the reward function was shaped through trial and error.\nV\n2\n2\nThe number of sampled programs is again in the order of 104. The best found policy is shown in Fig. 7, where the observation vector obs is 8-dimensional, and organized as (Xcup, Zcup, Xball, Zball, Xcup, \u017dcup, ball, \u017cball). At first, the policy is harder to parse compared to the pendulum swing- up case. However, all the reasoning steps are clearly outlined and again, being a Python program, it is easy to modify and customize. To support this claim, we manually proceed to simplify the policy. First, the cup is constrained to the box (x,z) \u2208 S = [-0.25, 0.25] \u00d7 [-0.25, 0.25]. Therefore, all the conditional statements checking for values outside this box (i.e. 0.8) are never visited and can be removed. It is possible to also see from the first conditional statements on Xref that the policy commands a narrow motion along x (roughly within [0.2, 0.25]). In fact Xref only gets values of either 0.0 if both Xcup and Zcup are above 0.2, or 1 otherwise (but the motion is constrained to S). Therefore, the policy mostly exploits a vertical stroke to swing the ball. The policy, cleaned of unused logic and with conditional statements grouped more meaningfully, is\n$\\begin{aligned} x_{\\text{ref}} &= \\begin{cases} 1 & \\text{if } x_{\\text{cup}} < 0.2 \\text{ or } z_{\\text{cup}} < 0.2\\\\ 0 & \\text{otherwise,} \\end{cases}\\\\ z_{\\text{ref}} &= \\begin{cases} 1 & \\text{if } \\dot{z}_{\\text{ball}} < 0.2 \\text{ or } z_{\\text{ball}} > 0.5 \\text{ or } \\dot{z}_{\\text{ball}} < -0.5\\\\ -1 & \\text{if } \\dot{x}_{\\text{cup}} > 0.2 \\text{ or } \\dot{z}_{\\text{cup}} < -0.2\\\\ 0 & \\text{otherwise,} \\end{cases} \\end{aligned}$\nAdditionally, we can leverage some intuition to even improve this policy. In fact, by visually inspecting the behavior of the system (a video can be found at https://youtu.be/ 7T7yRGya-q8), we noticed that a common failure is that the ball is not caught because it hits the sides of the cup. An intuitive modification would just be to insert a conditional statement that encourages the cup to lower slightly if the ball is swinging at a height (along z) which is greater than the cup's, to encourage catching. This modification can be easily achieved through, for example, the following statement:\nif Zball - Zcup > 0.1, then Zref \u2192 Zref - 0.1,\nwhich is equivalent to adding the following two lines of code as the last two lines of the policy in Fig. 7:"}, {"title": "IV. DISCUSSION AND CONCLUSIONS", "content": "We presented a novel approach to synthesize interpretable control policies. The interpretability is inherently guaranteed by the representation of control policies as programs in standard programming language, and casting the problem as a program synthesis problem, whose solution is achieved with the code generation capability of a Large Language Model. The policy representation through code allows a user not only to read and understand the control system's logic, but also to have intuitive understanding of the effects of a manual modification to the program. This unlocks properties such as explainability, modularity, verifiability, and paves the way to joint iterative synthesis approaches where a user can actively steer the automatic search and collaborate with an LLM in-the-loop to get different policies based on the design requirements.\nThe key reason for why this is possible is the shared language and formalism, i.e. Python in our case, between the user and the control system, as opposed to the use of black box models. Interpretability, however, comes with an increase in compute cost related to the absence of gradients to guide the optimization routine. In fact, as an example, a single threaded implementation on the workstation available to the authors takes in the order of 10 hours of wall-clock time to output programs able to successfully execute the proposed tasks.\nAnother aspect worth discussing is the role of randomness in the LLM generation process. At every generation step, a token is sampled from a distribution over (almost) all possible tokens in the underlying vocabulary. This offers opportunities for further investigations on the robustness of our method, starting for example with a careful tuning of hyperparameters.\nPicking the right reward function, as in many reinforce- ment learning settings, is also crucial for success. Future works can focus on how to make the algorithm more computationally efficient, potentially incorporating gradient- based optimization in-the-loop (such as in [33], for example). This would allow to take advantage of the LLM only as generator of a program skeleton, and unload it from the task of producing the right numerical quantities by tuning them with continuous optimization techniques. Compute availability is also an important aspect of the proposed framework. Implementing a distributed approach and running multiple LLM samplers in parallel allows to speed up the process and achieve high-performing programs in reduced time, as shown in [1].\nIt is also worth to mention that the amount of domain- specific information provided in the specification can significantly impact the performances (in terms of runtime and sample efficiency) of the control synthesis procedure. LLMs, in fact, are in general sensitive to prompt variations. As a simple example, prompting an LLM with a generic plain instruction like \u201cgenerate a function\", compared to instead providing more details about the task of interest, can make a significant difference. Even if both approaches, down the line, could lead to high-scoring programs, it is typically beneficial to provide context information for sample efficiency.\nTo conclude, we believe that code provides a compact, extremely expressive, and fully interpretable representation for a control policy. Thanks to these properties, we claim that our approach can reduce the gap between learning-based control systems and verifiable, reliable real world applications.\"\n    }"}]}