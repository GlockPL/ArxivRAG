{"title": "Enhanced Importance Sampling through Latent Space Exploration in Normalizing Flows", "authors": ["Liam A. Kruse", "Alexandros E. Tzikas", "Harrison Delecki", "Mansur M. Arief", "Mykel J. Kochenderfer"], "abstract": "Importance sampling is a rare event simulation technique used in Monte Carlo simulations to bias the sampling distribution towards the rare event of interest. By assigning appropriate weights to sampled points, importance sampling allows for more efficient estimation of rare events or tails of distributions. However, importance sampling can fail when the proposal distribution does not effectively cover the target distribution. In this work, we propose a method for more efficient sampling by updating the proposal distribution in the latent space of a normalizing flow. Normalizing flows learn an invertible mapping from a target distribution to a simpler latent distribution. The latent space can be more easily explored during the search for a proposal distribution, and samples from the proposal distribution are recovered in the space of the target distribution via the invertible mapping. We empirically validate our methodology on simulated robotics applications such as autonomous racing and aircraft ground collision avoidance.", "sections": [{"title": "Introduction", "content": "Safety-critical applications such as autonomous driving or aircraft controller design heavily rely on simulations to enhance safety through testing in controlled environments. Potential failures can be identified through simulation and then addressed before real-world deployment, reducing the risk of accidents (Corso et al. 2021). Failures are often rare and safety thresholds are strict, so the events of interest such as collisions or leaving a safe dynamic envelope-might be rarely encountered in simulation. Importance sampling (IS) is a variance reduction technique used in Monte Carlo simulations to bias the sampling distribution towards the rare event of interest (Owen 2013; Corso et al. 2021). IS uses a proposal distribution that focuses computational resources on scenarios likely to yield failure events, thus improving efficiency in failure detection. By assigning appropriate weights to sampled points, IS allows for more efficient estimation of the probability of failure compared to direct sampling from the target distribution.\nImportance sampling can fail when the proposal distribution does not effectively cover the target distribution. If"}, {"title": "Related Work", "content": "A rich body of literature exists on importance sampling methods, reflecting their widespread adoption for applications such as structural reliability analysis (Kurtz and Song 2013; Papaioannou, Papadimitriou, and Straub 2016; Geyer, Papaioannou, and Straub 2019) and safety validation for autonomous vehicles (Huang et al. 2019; Corso et al. 2021). IS methods can fail when the target distribution is not adequately explored, resulting in an inefficient sampler, i.e., the effective sample size is small compared to the actual number of samples drawn (Corso et al. 2021). Our proposed technique enhances IS coverage by fitting proposal distributions in the latent space of a normalizing flow, using the fact that the latent density is usually simpler than the target density.\nNormalizing flows have been used to provide expressive proposal distributions in IS methods (M\u00fcller et al. 2019; Gabri\u00e9, Rotskoff, and Vanden-Eijnden 2022; Samsonov et al. 2022). However, these approaches must interleave the sampling and training processes, resulting in complex training schemes and more strict assumptions to prove convergence (Samsonov et al. 2022). Our proposed methodology involves importance sampling in the latent space of a pre-trained flow model, providing more flexibility when the failure criteria or rare events of interest are updated yet the target distribution remains the same. Furthermore, our approach is beneficial if the flow is costly to train from scratch.\nResearchers have recently identified the utility of implementing Monte Carlo methods in the favorable geometry of normalizing flow latent spaces. Hoffman et al. (2019) propose a Hamiltonian Monte Carlo algorithm for sampling in the latent space of an inverse autoregressive flow (Kingma et al. 2016), which can improve mixing speed. Coeurdoux, Dobigeon, and Chainais (2023) propose a technique based on Langevin diffusion to correct for the topological mismatch between a latent unimodal distribution and a target distribution with disconnected support. Their goal is to limit out-of-distribution flow samples, whereas ours is to perform importance sampling for safety analyses. No\u00e9 et al. (2019) perform Metropolis Monte Carlo in the latent space of a Boltzmann generator to generate independent samples"}, {"title": "Importance Sampling and Normalizing Flows", "content": "We outline the fundamental theory behind importance sampling and normalizing flows before justifying the decision to perform IS in the latent space of a pre-trained flow model."}, {"title": "Importance Sampling", "content": "Simulations allow engineers to assess the performance of algorithms and models in diverse scenarios, including rare or dangerous scenarios that would be costly to replicate in real-world testing. Assessing the probability of failure events can require a prohibitively large number of Monte Carlo simulations, especially if the event of interest is rare.\nConsider an outcome space $x \\in \\mathbb{R}^n$ with probability density function $p(x)$ and a cost function $f(x)$ such that a failure event occurs if and only if $f(x) \\le 0$. The probability of failure $P_F$ is given by the integral\n$P_F = E_{p(x)}[1\\{f(x) \\le 0\\}] = \\int 1\\{f(x) < 0\\} \\cdot p(x)dx$   (1)\nWe can estimate $P_F$ via Monte Carlo simulations by drawing $N_s$ samples $\\{x_1, ..., x_{N_s}\\}$ from $p(x)$ and taking the mean:\n$\\hat{P_F} = \\frac{1}{N_s} \\sum_{i=1}^{N_s} 1\\{f(x_i) \\le 0\\}$  (2)\nThis estimate is unbiased and has a coefficient of variation\n$\\delta_{\\hat{P_F}} = \\frac{\\sqrt{1 - P_F}}{ \\sqrt{N_s} P_F}$   (3)\nSince the coefficient of variation is inversely proportional to the failure probability, many samples might be required to come up with a precise estimate of $P_F$, especially if $P_F$ is small (Papaioannou, Papadimitriou, and Straub 2016).\nImportance sampling is a Monte Carlo simulation technique that aims to reduce the variance of $P_F$ by sampling"}, {"title": "Normalizing Flows", "content": "Normalizing flows (Rezende and Mohamed 2015) are a class of generative model used for density estimation and generative sampling. Normalizing flows transform a real vector $u$ sampled from an easy-to-evaluate base distribution, denoted by $p_u(u)$, through a transformation $z = T(u)$ to produce a more expressive target density $p_*(z)$. A common choice of base distribution is a standard normal distribution. The transformation must be invertible and differentiable, i.e., a diffeomorphism. Imposing such topological constraints on a flow architecture ensures that the target density can be evaluated using the change of variables formula:\n$p_*(z) = p_u(u) |det J_T(u)|^{-1}$ where $z = T(u)$.   (6)\nThe Jacobian of transformation $T$ is denoted by $J_T$; its determinant is a volume-correcting term that adjusts the probability density function of the transformed variable. The transformation (\u201cflow\u201d) itself is typically composed of $D$ simpler transformations: $T = T_D \\circ T_{D-1} \\circ \\dots \\circ T_1$. Since the transformations are composable and each step (\u201clayer\u201d) is a diffeomorphism, we can set $z_0 = u$, $z_d = T_d \\circ \\dots \\circ T_1(z_0)$ and compute the Jacobian-determinant in the log domain as\n$\\log |\\det J_T(u)| = \\sum_{d=1}^{D} \\log |\\det J_{T_d}(z_{d-1})|$.   (7)"}, {"title": "Justification of Latent Space Sampling", "content": "Since normalizing flows learn an invertible mapping, we can fit proposal distributions over either the target space or the learned latent representation. As shown in Fig. 1, the target distribution can be highly non-isotropic in nature, with regions of extremely low density bordered by regions of very"}, {"title": "Methodology", "content": "In this section, we formalize our proposed technique for enhanced importance sampling and present our flow architecture, cost function formulation, and IS proposal-fitting methods."}, {"title": "Normalizing Flow Architecture", "content": "Recall that the flow transformation must be both invertible and differentiable. A sufficient condition for invertibility is enforcing the transformation to be monotonic. Durkan et al. (2019) propose the use of monotonic rational quadratic splines as building blocks for the transform. In this work we use piecewise rational quadratic coupling transforms to compose the flow models. Coupling transforms operate by splitting the input into two parts and applying an invertible function to one part while leaving the other part unchanged, thus ensuring efficient inversion and sampling capabilities (Dinh, Sohl-Dickstein, and Bengio 2016; Papamakarios et al. 2021). The flow parameters are optimized by minimizing the forward KL divergence between the learned and target distributions."}, {"title": "Cost Function Formulation", "content": "The cost function should be continuous and bias the search over simulator outcomes towards the rare or failure events of interest (Corso et al. 2021). However, computing the distance function for an arbitrary set is a hard problem, and even determining if an outcome falls inside or outside a set can be computationally expensive (Hormann and Agathos 2001). We propose a cost function geometry based on L\u00f6wner-John ellipsoids, i.e., the minimum volume ellipsoid that contains a set of points $X$ (Boyd and Vandenberghe 2004). We posit that minimum volume ellipsoids are a reasonable choice of cost function since 1) they overapproximate the convex hull of $X$, which is desirable for enforcing conservative safety thresholds and 2) Gaussian mixture models are commonly used for proposal distributions, and each mixture component has ellipsoidal level sets.\nConsider a finite set of points $X = \\{x_1, ..., x_m\\} \\subseteq \\mathbb{R}^n$. The problem of finding the minimum volume ellipsoid that covers $X$ is a convex optimization problem (Boyd and Vandenberghe 2004):\n$\\begin{aligned} & \\underset{A, b}{\\text{minimize}} & \\log \\text{det } A^{-1} \\\\ & \\text{subject to} & || A x_i + b ||_2 \\le 1, \\quad i=1, ..., m \\end{aligned}$   (8)\nwhere the variables are a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^n$. This formulation is attractive because the L\u00f6wner-John ellipsoid can be found quickly using standard optimization solvers such as CVXPY (Diamond and Boyd 2016).\nA question that naturally arises is how to obtain a set of points $X$ that is representative of the failure region. If the data used to train the flow model is available, then one solution is to construct $X$ from the failures in this dataset. However, if the dataset is relatively small or the failure events are extremely rare, then the set $X$ might not adequately cover the entire failure domain. In this event, the overapproximating nature of L\u00f6wner-John ellipsoids is advantageous. Another solution is to specify the set $X$ manually based on user insight (e.g., defining the corners of a safety threshold hypercube).\nOnce $X$ is obtained, the representative points are mapped deterministically to a set $U$ in latent space via the invertible flow mapping. Solving Eq. (8) with input $U$ yields a L\u00f6wner-John ellipsoid in latent space. The Mahalanobis distance $d_M$ between the ellipsoidal region and a sample from the IS proposal density is easily computed. If $d_M \\le 0$, the sample lies within the ellipsoid and is classified as a failure event."}, {"title": "Importance Sampling Methods", "content": "In this work we evaluate two importance sampling methods; however, any IS algorithm could be used in practice (Owen 2013; Corso et al. 2021). The first algorithm is the cross entropy (CE) method (De Boer et al. 2005; Geyer, Papaioannou, and Straub 2019), which attempts to learn the parameters of a parametric proposal distribution that minimizes the KL divergence between the optimal IS density and the proposal. The CE method introduces a series of intermediate failure domains that gradually approach the true failure domain. At each step, the intermediate failure region is defined such that $p\\cdot N_s$ samples fall in the region, where the $p$-quantile is chosen by the user. The proposal distribution parameters are then fit via maximum likelihood estimation over these samples. The expectation-maximization algorithm is often used to fit the search distribution, though it must be adjusted to account for importance-weighted samples (Geyer, Papaioannou, and Straub 2019).\nThe second method is sequential importance sampling (SIS) (Del Moral, Doucet, and Jasra 2006; Papaioannou, Papadimitriou, and Straub 2016). Like the CE method, SIS introduces a series of intermediate failure distributions that gradually approach the optimal IS density. Samples for each intermediate distribution are obtained by resampling weighted particles from the previous distribution and then moved to regions of high likelihood under the next failure distribution via Markov Chain Monte Carlo. In this work, we use a conditional sampling Metropolis-Hastings algorithm to move the samples (Papaioannou, Papadimitriou, and Straub 2016)."}, {"title": "Algorithm", "content": "Algorithm 1 presents the proposed latent IS methodology for estimating $P_F$. The algorithm takes as input a flow model and an importance sampling algorithm and outputs an estimate of $P_F$. Furthermore, failure events can be easily generated after learning the proposal distribution by generating samples in latent space and mapping them back to target space. Without loss of generality, the user can input a set of failure sets $\\{X_i\\}_{i=0}^n$, $X_i = \\{x_1, ..., x_m\\}$, with each set corresponding to a failure mode. The L\u00f6wner-John ellipsoid is solved for each failure set, and the cost function computes the Mahalanobis distance for each ellipsoid and returns the minimum value."}, {"title": "Experiments", "content": "This section first presents our simulated robotics datasets and evaluation metrics before discussing experimental results."}, {"title": "Data Simulators", "content": "We use three autonomous system simulators to validate our proposed approach.\nConsider a nonholonomic robot that moves in two dimensions. The three-dimensional robot state $s$ is defined as\n$s = [x \\quad y \\quad \\theta]^T$,"}, {"title": "Metrics", "content": "We compute a reference failure probability for each dataset using the Monte Carlo estimate given in Eq. (2) and then calculate the importance sampling estimate $\\hat{P_F}$ using Eq. (5). We return the relative error between these two values; a positive value indicates that the estimated failure likelihood is an overestimate of the true value. Next, we compute a series of metrics to evaluate the quality of the learned proposal density:"}, {"title": "Experimental Setup", "content": "We perform cross-entropy importance sampling and sequential importance sampling on all three datasets; furthermore, we evaluate each method in both latent space and target space. For the target space experiments, we compute the L\u00f6wner-John ellipsoids on $\\{X_i, i = 0, ..., n\\}$ directly. In the latent experiments, we first compute $U = T^{-1}(X)$ and then solve Eq. (8) with constraints based on the points in $U$.\nFor the nonholonomic robot data, we construct $X_1$ as a unit cube such that $x \\in [-1.0, -2.0]$, $y \\in [-2.25, -3.25]$, $\\theta \\in [1.25, 2.25]$, and $X_2$ as a unit cube such that $x \\in [0.75, 1.75]$, $y \\in [-3.25, -4.25]$, $\\theta \\in [-1.0, -2.0]$."}, {"title": "Discussion and Future Work", "content": "Importance sampling is a powerful method for computing the probability of rare events and validating autonomous systems (Corso et al. 2021). In this work we present a technique to improve importance sampling by first transforming the data with a normalizing flow. The invertible flow transformation warps non-isotropic target densities into a latent density which is more isotropic and easier to explore. We also propose an intuitive cost-function formulation that is simple to evaluate, even after undergoing an arbitrarily complex transformation to latent space. We experimentally show that conducting IS in latent space results in failure samples that more closely match the true distribution of failure events. Samples generated in latent space can be easily recovered in target space via the deterministic and invertible flow mapping. Our latent IS methods outperform target IS methods on a range of simulated autonomous systems.\nFuture work will provide a theoretical analysis of the benefits of latent IS, investigating the impact of maximum likelihood training on the warped geometry of the latent failure regions. Alternative cost-function formulations will be explored; for example, maximum-volume inscribed ellipsoids could result in $\\hat{P_F}$ estimates that are less conservative. Finally, the latent IS methods presented in this paper will be used to validate real-world, black-box autonomous systems with high-fidelity simulators."}]}