{"title": "Chasing Better Deep Image Priors between Over- and Under-parameterization", "authors": ["Qiming Wu", "Xiaohan Chen", "Yifan Jiang", "Zhangyang Wang"], "abstract": "Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep image priors (DIP) that regularize various image inverse problems. Meanwhile, researchers also proposed extremely compact, under-parameterized image priors (e.g., deep decoder) that are strikingly competent for image restoration too, despite a loss of accuracy. These two extremes push us to think whether there exists a better solution in the middle: between over- and under-parameterized image priors, can one identify \u201cintermediate\" parameter-ized image priors that achieve better trade-offs between performance, efficiency, and even preserving strong transferability? Drawing inspirations from the lottery ticket hypothesis (LTH), we conjecture and study a novel \"lottery image prior\" (LIP) by exploiting DNN inherent sparsity, stated as: given an over-parameterized DNN-based image prior, it will contain a sparse subnetwork that can be trained in isolation, to match the original DNN's performance when being applied as a prior to various image inverse problems. Our results validate the superiority of LIPs: we can successfully locate the LIP subnetworks from over-parameterized DIPs at substantial sparsity ranges. Those LIP subnetworks significantly outperform deep decoders under comparably compact model sizes (by often fully preserving the effectiveness of their over-parameterized counterparts), and they also possess high trans-ferability across different images as well as restoration task types. Besides, we also extend LIP to compressive sensing image reconstruction, where a pre-trained GAN generator is used as the prior (in contrast to untrained DIP or deep decoder), and confirm its validity in this setting too. To our best knowledge, this is the first time that LTH is demonstrated to be relevant in the context of inverse problems or image priors.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have been powerful tools for solving various image inverse problems such as denoising Zhang et al. (2017); Guo et al. (2019); Lehtinen et al. (2018); Jiang et al. (2022), inpainting Pathak et al. (2016); Yu et al. (2018; 2019b), and super-resolution Ledig et al. (2017); Lim et al. (2017); Zhang et al. (2018). Conventional wisdom believes that is owing to DNNs' universal approximation ability and learning from massive training data. However, a recent study Ulyanov et al. (2018) discovered that degraded images can be restored independently using randomly initialized and untrained convolutional neural networks well.\nDespite the advantageous performance, most DIP methods use highly over-parameterized CNNs with\na massive number of parameters (we show this in Table 3 in appendices). Over-parameterization causes\ncomputational inefficiency and overfitting (and thus proneness) to noises. This trend has naturally invited\nthe curious question: does DIP have to be heavily parameterized? That question is partially answered by\nHeckel & Hand (2018) by proposing the first under-parameterized, non-convolutional neural network for\nDIP named \"deep decoder\". Deep decoder shares across all pixels a linear combination over feature channels\nin each layer and thus has an extremely compact parameterization. Thanks to its under-parameterization,\ndeep decoder alleviates the \"noise overfitting noise\" in DIP. However, the empirical performance of deep\ndecoder is often not on par with overparameterized DIP models, especially on tasks like inpainting and\nsuper-resolution. This is potentially attributed to the extremely restricted capacity of deep decoder from the\nvery beginning. The dissatisfaction on both extremes, that is using either over-parameterized CNN DIPs or\nunder-parameterized deep decoders, pushes us to think if there is a better \"middle ground\": Between over-\nand under-parameterized image priors, can one identify \u201cintermediate\u201d parameterization better trade-offs\nbetween performance, efficiency, as well as even preserving strong transferability?\nIn this work, we adopt sparsity as our main tool for the above question. We seek more compactly pa-\nrameterized subnetworks by pruning superfluous parameters from the dense over-parameterized CNN DIPs\n(overview of our work paradigm is shown in Figure 1), essentially viewing pruning as a way to smoothly and\nflexibly \"interpolate\" between over- and under-parameterization. We have several motivations for choosing\nsparsity and pruning from over-parameterization. On one hand, compared with dense and overparameter-\nized DIP models, sparsity saves computations during inference and DIP fitting. Moreover, sparsity can serve\nas an effective prior that regularizes DNNs to have better robustness to noise Chen et al. (2022), that is\nimportant for DIP which is tasked to distinguish image content from noise. These two blessings combined\nmake sparsity a promising \"win-win\" for not only DIP efficiency but also performance. On the other hand,\nunlike deep decoder which sticks to a compact design, exploiting sparsity follows a different \"first redundant\nthen compact\" training route, which is widely found to enhance performance compared to training a com-\npact model directly from scratch Zhou et al. (2020). Since overparameterized (especially wide) DNNs have\nsmoother loss surfaces while smaller ones have more rugged landscapes, starting from overparameterization\ncan ease the training difficulty Safran et al. (2021) and may particularly help the \u201cchaotic\" early stage of\ntraining Frankle et al. (2020c).\nThe recently emerged Lottery Ticket Hypothesis (LTH) Frankle & Carbin (2018); Frankle et al. (2020a)\nsuggests that every dense DNN has an extremely sparse \"matching subnetwork\", that can be trained in\nisolation to match the original dense DNN's accuracy. While the vanilla LTH studies training from random\nscratch, the latest works also extend similar findings to fine-tune the pre-trained models Chen et al. (2020a;\n2021b). LTH has widespread success in image classification, language modeling, reinforcement learning and\nmulti-modal learning, e.g., Yu et al. (2019a); Renda et al. (2020); Chen et al. (2020a); Gan et al. (2021).\nDrawing inspirations from the LTH literature, we conjecture and empirically study a novel \"lottery image\nprior\" (LIP), stated as:\nGiven an (untrained or trained) over-parameterized DIP, it will have a sparse subnetwork that\ncan be trained in isolation, to match the original DIP's performance when being applied as a prior\nto regularizing various image inverse problems. Moreover, its performance shall surpass under-\nparameterized priors of similar parameter counts.\nDiving into this question has two-fold appeals. On the algorithmic side, as the first attempt to investigate\nLTH in the DIP scenario, it could help us understand how the topology and connectivity of CNN architectures\nencode natural image priors, and whether \"overparameterization + sparsity\" make the best DIP recipe. On\nthe practical side, the affirmative answer to this question can lead to finding compact DIP models that are\nmore performant than the deep decoder, hence yielding more computationally efficient solutions for image\ninverse problems without sacrificing performance."}, {"title": "2 Background Work", "content": "2.1 Over-parameterized Image Priors\nDespite CNNs' tremendous success on various imaging tasks, their outstanding performance is often at-\ntributed to massive data-driven learning. DIP Ulyanov et al. (2018) pioneered to show that CNN architecture\nalone has captured important natural image priors: by over-fitting a randomly initialized untrained CNN\nto a single degraded image (plus some early stopping), it can restore the clean output without accessing\nground truth. Follow-up work Mataev et al. (2019) strengths DIP performance by incorporating it with\nthe regularization by denoising (RED) framework and a series of works Mastan & Raman (2020; 2021) use\nthe contextual feature learning method to achieve the same goal of DIP. Besides natural image restoration,\nDIP was successfully applied to PET image reconstruction Gong et al. (2018), dynamic magnetic resonance\nimaging Jin et al. (2019), unsupervised image decomposition Gandelsman et al. (2019) and quantitative\nphase imaging Yang et al. (2021).\nThere have been several efforts toward customizing DIP network architectures. Liu et al. (2019) extends the\nDIP framework with total variation (TV) and this combination leads to considerable performance gains. Jo\net al. (2021) further propose the \"stochastic temporal ensemble (STE)\" method to prevent DIP models from\noverfitting noises and thus improving performances. Chen et al. (2020c) proposed a neural architecture search\n(NAS) algorithm, which searches for an optimal DIP neural architecture from a search space of upsampling\ncell and residual connections. The searched 'NAS-DIP' model can be reused across images and tasks. Arican\net al. (2022) observe that different images and restoration tasks often prefer different architectures, and\nhence design the image-specific NAS to find an optimal DIP network architecture for each specific image.\nWhile our work also pursues better DIPs, it substantially differs from those prior arts in terms of model\ncompactness (approaching the under-parameterization end) and reusability. The simple pruning-based recipe\nalso overcomes any extra design hassle (search space or algorithm) caused by NAS. We compare the results\nwith them in the paragraph \u201cComparisons with NAS-DIP and ISNAS-DIP models\" in Appendix B."}, {"title": "2.2 Under-parameterized Image Priors", "content": "Recall that, classical image regularizers in the spatial or frequency domains often rely on no or little learning\ncomponent. For example, Tomasi & Manduchi (1998) first proposed a bilateral filtering method to smooth\nimages while preserving edges by combining image values in a nonlinear way. After that, Sardy et al. (2001)\nproposed a wavelet-based estimator with a robust loss function to recover signals from noises. Later, Dabov\net al. (2007b) proposed a strategy based on enhancing the sparsity through grouping similar 2-D image\nfragments into 3-D data arrays, followed by a collaborative filtering procedure. And this algorithm achieves\nstate-of-the-art performances in terms of both peak signal-to-noise ratio and subjective visual quality. In\naddition to these traditional methods, Cao et al. (2008) proposed a Gaussian mixture model for image denois-\ning that partitions an image into overlapping patches and estimates the noise-free image patch parameters\nusing expectation maximization (EM) and Minimum mean square error (MMSE) techniques. Elad & Aharon\n(2006) also presented an approach to remove Gaussian noises based on sparse and redundant representations\nusing trained dictionaries obtained with the K-SVD algorithm.\nAs another important concern, while over-parameterized DIPs are prone to overfitting the corrupted image,\nthey can generate almost uncorrupted images surprisingly after a few iterations of gradient descent. Inspired\nby those, Heckel & Hand (2018) pioneered to design a concise and non-convolution neural network as the\n\"prior for image restoration tasks. Such under-parameterized DIPs not only improve the efficiency of DIP-\nbased restoration but also enable researchers to theoretically analyze the signal process. Besides, they find the\narchitecture's simplicity could effectively avoid overfitting in the restoration process. Heckel & Soltanolkotabi\n(2019) further analyzed the dynamics of fitting a two-layer convolutional generator to a noisy signal and\nprove that early-stopped gradient descent denoises/regularizes."}, {"title": "2.3 Lottery Ticket Hypothesis", "content": "The lottery ticket hypothesis (LTH) Frankle & Carbin (2018) states that the dense, randomly initialized\nDNN contains a sparse matching subnetwork, which could reach the comparable or even better performance\nby independently being trained for the same epoch number as the full network do. Since then, the statement\nhas been verified in a variety of fields. In natural language processing (NLP), Gale et al. (2019) established\nthe first rigorous baselines for model compression by evaluating state-of-the-art methods on transformer\nVaswani et al. (2017) and ResNet He et al. (2016). Later, Chen et al. (2020a) successfully found trainable,\ntransferrable subnetworks in the pre-trained BERT model. In reinforcement learning (RL), Yu et al. (2019a)\nconfirmed the hypothesis both in NLP and RL tasks, showing that \"winning ticket\" initializations generally\noutperform randomly initialized networks, even with extreme pruning rates. In lifelong learning, Chen\net al. (2020b) first demonstrated the existence of winning tickets via bottom-up pruning. In graph neural\nnetworks (GNNs), Chen et al. (2021c) first defined the graph lottery ticket and developed an unified GNN\nsparsification (UGS) framework to prune graph adjacency matrices and model weights. And in adversarial\nrobustness research, Cosentino et al. (2019) evaluated the LTH with adversarial training and they successfully\nproved this approach can find sparse, robust neural networks.\nSimultaneously, many researchers started to rethink the network pruning techniques and build a rigorous\nbaseline benchmark Liu et al. (2018); Evci et al. (2019). Some researchers explore the sparse subnetwork\nat the initialization stage (e.g., Wang et al. (2020) proposed the \"GraSP\" algorithm, which leverages the\ngradient flow.). And some try to find the winning tickets at early iterations Frankle et al. (2020d); Savarese\net al. (2019). For example, You et al. (2019) found the \u201cearly-bird\" tickets via low-cost training schemes\nwith a large learning rate. However, as the original LTH paper Frankle & Carbin (2018) said, the proposed\nmethod cannot scale to large model and datasets. Rewinding was proposed by Frankle et al. (2019) to solve\nthis dilemma. The found matching subnetworks also demonstrate transferability across datasets and tasks\nMorcos et al. (2019); Desai et al. (2019). Ma et al. (2021) thought the rewinding stage is not the only way\nto solve this problem and proposed a simpler yet more powerful approach called \"Knowledge Distillation\nticket\"."}, {"title": "3 Preliminaries and Approach", "content": "3.1 Finding Lottery Tickets\nNetworks For simplicity, we formulate the dense network output as $f(z, \\theta)$, where z is the input tensor\nand $\\theta \\in \\mathbb{R}^d$ is the model parameters. In the same way, a subnetwork is defined as $f(z, m \\odot \\theta)$ with the binary\nmask $m \\in {0,1}^d$, where $\\odot$ means element-wise product.\nAlgorithm 1 Single Image-based IMP\nInput: The desired sparsity s, the random code\nz, the untrained model $f_u$.\nOutput: A sparse DIP model $f(z;\\theta_m)$ with\nimage prior property.\nInitialization: Set $m_u = 1 \\in \\mathbb{R}^{\\|\\theta_0\\|}$. Set itera-\ntion i = 0, training epochs N and $j\\in [0, N]$.\nwhile the sparsity of $m_u < s$ do\n1. Train the $f_u (z; \\theta \\odot m_u)$ for N epochs;\n2. Create the mask $m$;\n3. Update the mask $m_u = m'u$;\n4. Set the model parameters: $f(z;\\theta;)$;\n5. create the sparse model: $f(z; \\theta; m_u)$;\n6. i++;\nend while\nAlgorithm 2 Weight-sharing IMP\nInput: The desired sparsity s, the random\ncode z, the untrained model $f_u$, \u00ee denotes the\ndegraded image and images from n domains\n$X_\\alpha \\in {X_1, X_2, ..., X_n}$.\nOutput: A sparse DIP model $f(z;\\theta_m)$ with\nimage prior property.\nInitialization: Set $m_u = 1 \\in \\mathbb{R}^{\\|\\theta_0\\|}$. Set itera-\ntion i = 0, training epochs N and $j\\in [0, N]$.\nwhile the sparsity of $m_u < s$ do\n1. $loss = \\Sigma_{\\alpha=1}E(f(z; \\theta_m); x_\\alpha)$;\n2. Train the $f_u(z; \\theta \\odot m_u)$ by Backpropaga-\ntion (loss) for N epochs;\n3. Update the mask $m_u = m$;\n4. Set the model parameters $f(z;\\theta;)$;\n5. create the sparse model $f(z; \\theta; m_u)$;\n6. i++;\nend while\nPruning Methods We use the classic iterative magnitude-based pruning (IMP) method Frankle & Carbin\n(2018), which iteratively prunes the 20% of the model weight each time. In each IMP iteration, models\nare trained towards the standard DIP objective to fit the degraded observations for a certain number of\ntraining steps following the original DIP Ulyanov et al. (2018). Our basic algorithm performs IMP over\njust one degraded image, and the algorithm is summarized in Algorithm 1. We further design an extended\nalgorithm, that can perform IMP for DIP over multiple degraded images, through backbone weight sharing:\nthe algorithm is outlined in Algorithm 2 (neither algorithm requires clean images). To show the non-triviality\nof the identified matching subnetworks, we also compare LIP with random pruning and SNIP Lee et al. (2018),\na pruning-at-initialization method. We also derive a new method based on empirical observations to decide\nwhen to stop IMP iterations to find matching networks with maximal sparsities without any reference to the\nclean ground truths. More details are in Appendix B.\nExperimental Setup For evaluation models, we use hourglass architecture Ulyanov et al. (2018) and deep\ndecoder Heckel & Hand (2018), as two representative untrained DNN image priors in the over- and under-\nparameterization ends, respectively. For evaluation datasets, we use the popular Set5 Bevilacqua et al. (2012)\nand Set14 Zeyde et al. (2010). We also evaluate the transferability of subnetworks on image classification\ndatasets such as ImageNet-20 Deng et al. (2009) and CIFAR10 Krizhevsky et al. (2009). For metrics, we\nmainly compare the PSNR and/or SSIM results between the restored image and the ground truth, as in\nFig. 11.\nThe parameter count of the original DIP model is 2.2 million (M); and that of the deep decoder is 0.1 M for\ndenoising and super-resolution experiments, and 0.6 M for inpainting experiments, all following the original\nsettings of Heckel & Hand (2018). The model sizes are plotted as horizontal coordinates in the figures.\nWe run all experiments with 10 different random seeds: every solid curve is plotted over the\n10-time average, and the accompanying shadow regions indicate the 10-time variance. Most\nplots see consistent results across random seed experiments and hence small variances. All\nimages used are summarized in Fig. 17."}, {"title": "4 LIP in Untrained Image Priors", "content": "Existence of LIPs In over-parameterized image priors, we first find the matching subnetworks with LIP\nproperty by implementing the single-image IMP on the DIP model. We apply the implemented Algorithm 1\non Set5 and Set14 to obtain the sparse subnetworks2 and evaluate these subnetworks on the denoising task.\nResults of single-image IMP in Fig. 3 (curves in gray colors) verify the existence of LIPs. To be specific,\nduring the IMP finding process, we can find the LIP subnetworks on untrained DIPs at sparsity as high as\n86.58%. While at the evaluation stage, the found LIP subnetworks with the modified objectives are still\napplicable, matching the dense performance at sparsity as high as 83.23%. We also compare the single-\nimage IMP with Random Pruning, and SNIP Lee et al. (2018). We observe from the first row in Fig. 3 that\nsingle-image IMP outperforms them at a wide sparsity range [20%, 96%].\nIs Multi-image IMP A Good Extension for Finding LIPs? Like the original DIP, single-image IMP\nover-fits an untrained DNN on a single image and learns the features in that specific image during iterative\nmagnitude pruning loops. To obtain the LIP subnets with more general features, we propose a new multi-"}, {"title": "5 Extending LIP to Pre-trained Neural Network Priors", "content": "Although the implementation process of the GAN CS task and the DIP restoration task is different, we still\nfind them related and could be unified for two reasons: 1) current research on LTH consists of two main\nstreams, networks with weights with randomly initialized weights Frankle & Carbin (2018) and networks\nwith pre-trained weights Chen et al. (2021b); 2) network structures with random weights or pre-trained\nweights can be priors, which is the commodity we view as the two could be unified. However, we do not\nintend to put the GAN CS part in a parallel position to the DIP part. We just want to demonstrate the\neffectiveness of our pruning method as an \"extension\" from training-from-scratch deep image prior to using\npre-trained neural networks as image priors.\nCompressive Sensing using Generative Models Compressive Sensing (CS) reconstructs an unknown\nvector from under-sampled linear measurements of its entries Foucart & Rauhut (2013), by assuming the"}, {"title": "6 Limitations and Future Work", "content": "Although the LIP subnetworks outperform the dense DIP model and Deep Decoder on many image restora-\ntion tasks, we also find some limitations of LIP. Firstly, the LIP subnetworks cannot directly transfer from\nlow-level (image restorations) to high-level vision tasks (image classifications) and we conjecturally attribute\nthis to the sparsity distribution discrepancy in the inner structure of LIP subnetworks. Secondly, LIP sub-\nnetworks fail to achieve comparable performances of the dense DIP model when at extreme sparsities (as high\nas 99.53%). Thirdly, we observe that the LIP subnetworks do not perform very well on specific restoration\ntasks such as super resolution because of the inherent difficulty for the deep image prior models to generate\nhigh-quality textures. In the future, we will keep exploring more efficient ways to construct more sparse and\ntrainable image prior models with powerful transferability."}, {"title": "7 Conclusion", "content": "This paper identifies the new intermediate solution between the under- and over-parameterized DIP regimes.\nSpecifically, we validate the lottery image prior (LIP), a novel, non-trivial extension of the lottery ticket\nhypothesis to the new domain. We empirically demonstrate the superiority of LIP in image inverse problems:\nthose LIP subnetworks often compare favorably with over-parameterized DIPs, and significantly outperform\ndeep decoders under comparably compact model sizes. They also possess high transferability across different\nimages as well as restoration task types."}, {"title": "A More Discussions of the Experiments", "content": "Images Used in Experiments In Fig. 17, we organize and present the images used in this paper with\ntheir names. Note that these images are sampled from Set5 Bevilacqua et al. (2012) and Set14 Zeyde et al.\n(2010) datasets, and we use their default names. Note that these names are used in Section 4 (in curves\nand analyses). Also, in Table 5, we conduct the evaluation experiments on BM3D Dabov et al. (2007a),\nCBM3D Dabov et al. (2007a) and Set12 Zhang et al. (2017) datasets for fair comparisons with NAS-DIP\nand ISNAS-DIP models.\nParameter Redundancy Problem The commonly used hourglass model in DIP Ulyanov et al. (2018)\nis highly complex in its parameters. The statistic results of parameter numbers is summarized in Table 3.\nWe compare the parameter numbers of dense DIP models with the identified winning tickets using LIP\nand discover that the winning tickets could perform better than the full model while containing 2 million\nparameters fewer. This phenomenon motivates us to suspect that there is a high possibility of finding the\nmatching subnetworks of the pristine dense DIP model, which indicates that the subnetworks may also\ncontain the outstanding image prior property as the dense one does.\nAs to the under-parameterized image prior networks (e.g., deep decoder), we also count the number of non-\nzero parameters in Table 2 for a fair comparison in restoration tasks with LIP networks. As shown in the\ntable, for denoising and super-resolution tasks, the number of parameters of the deep decoder is 0.1 million.\nFor inpainting tasks, the parameter number is 0.6 million. Therefore, for denoise and super-resolution tasks,\nwe choose the sparsity 97.19% of LIP for comparisons; for inpainting tasks, we choose the sparsity 73.70%\nfor comparisons.\nDefinition of Subnetworks and Finding Them Consider a network f(x; 0) parametered by @ with\ninput x, then a subnetwork is defined as $f (x; m\\odot \\theta)$, where $\\odot \\in {0,1}^d$, d = $\\|\\theta\\|_0$ and is the element-wise\nproduct. Let $A_T (f(x; \\theta))$ to be the training algorithm, that is, training model $f(x;\\theta)$ on the specific task\nT with t iterations. We also denote the random initialization weight as $0_0$ and the pre-trained weight as $0_p$;\n$0_i$ as weight at the i-th training iteration and $E_T (f(x; \\theta))$ the model performance evaluation. Following the\ndefinitions of Frankle et al. (2020a), we define that if the subnetworks is matching, it satisfies the following\nconditions (we use Op for example, to denote a pre-trained lottery ticket Chen et al. (2020a; 2021b); do can\nbe defined likewise):\n$E_T (A_T (f(x;\\theta_p)) \\leq E_T (A_T (f(x; m\\odot \\theta)).$"}, {"title": "B Ablation Study of LIP Subnetworks", "content": "The Effect of Weight Rewinding In this part, we study the effect of weight rewinding Frankle et al.\n(2019) when applied to the single-image IMP for DIP models. Weight rewinding is proposed to scale LTH\nup to large models and datasets. Specifically, we say we use p% weight rewinding if we reset the model\nweights at the end of each IMP iteration to the weights in the dense model after a p% ratio of training steps\nwithin a standard full training process, instead of the model's random initialization. For the single-image\nIMP in DIP, we consider 5%, 10% and 20% weight rewinding schemes. The resulting models are denoted\nas Rewind_5, Rewind_10 and Rewind_20, respectively. The results of different weight rewinding schemes\nare summarized in Fig. 9. We can see that weight rewinding is not beneficial for identifying LIP in the DIP\nsetting. Too much rewinding (10% and 20%) even hurts performance or fails it completely. We conjecture\nthat this is due to the extremely low data complexity in DIP (single image).\nLearning Curves of Subnetworks with Different Training Targets To better explain the success of\nthe LIP subnetworks, inspired by Figure 2 of Ulyanov et al. (2018), we further train the obtained subnetworks\n(LIP, SNIP and random pruning) with four different targets: 1) a natural image, 2) the same added with\nnoise, 3) the same after randomly permuting the pixels and 4) the white noise. We also train the dense\nmodel as the baseline results. The experimental details are summarized in the caption of Fig. 10. We first\nobserve that for LIP, SNIP, and dense models, the optimization converges much faster in case 1) and 2)"}]}