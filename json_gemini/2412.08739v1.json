{"title": "VEL: A Formally Verified Reasoner for OWL2 EL Profile", "authors": ["Atalay Mert Ileri", "Nalen Rangarajan", "Jack Cannell", "Hande McGinty"], "abstract": "Over the past two decades, the Web Ontology Language (OWL) has been instrumental in advancing the development of ontologies and knowledge graphs, providing a structured framework that enhances the semantic integration of data. However, the reliability of deductive reasoning within these systems remains challenging, as evidenced by inconsistencies among popular reasoners in recent competitions. This evidence underscores the limitations of current testing-based methodologies, particularly in high-stakes domains such as healthcare. To mitigate these issues, in this paper, we have developed VEL, a formally verified EL++reasoner equipped with machine-checkable correctness proofs that ensure the validity of outputs across all possible inputs. This formalization, based on Baader et al.'s algorithm (Baader, Brandt, and Lutz 2005), has been transformed into executable OCaml code using the Coq proof assistant's extraction capabilities. Our formalization revealed several errors in the original completeness proofs, which led to changes to the algorithm to ensure its completeness. Our work demonstrates the necessity of mechanization of reasoning algorithms to ensure their correctness at theoretical and implementation levels.", "sections": [{"title": "Introduction", "content": "During the nearly three decades since its initial proposal, Semantic Web(SW) has become a mature technology, which can support and enhance existing applications in several scenarios, ranging from e-commerce (Necula et al. 2018) to Human-Device interaction (Perera 2020), and product configuration (Bischof et al. 2018), just to name a few. The fundamental concept of the Semantic Web is to provide a semantic layer for data, enabling systems to manipulate this data through chains of meaningful deduction steps based on sound rules. This semantic framework allows SW data processing to offer meaningful explanations, which contrasts with the often opaque nature of sub-symbolic technologies like neural networks. However, a closer examination of SW systems reveals that the sophistication of their reasoning abilities can vary significantly (Colucci, Donini, and Di Sciascio 2024). Furthermore, due to the increased popularity of Neurosymbolic AI in recent years (Hitzler et al. 2022), there is a renewed interest in deductive reasoning algorithms.\nDue to their important role, it is necessary to ensure that the reasoner results are correct. However, like any other software system, reasoner implementations are susceptible to software bugs that undermine their correctness. A recent competition for reasoners showed that some of the widely used reasoners contain bugs since the results of the different reasoners didn't agree on the same queries. (Parsia et al. 2017) The ground truth needed to be determined by the majority vote. Reasoners' inability to agree on a correct result shows that testing-based methodologies cannot provide strong correctness guarantees required for critical domains such as healthcare.\nWe developed VEL, a formally verified EL++reasoner with machine-checkable correctness proofs to address this gap. Our proofs ensure the correctness of the output for each possible input. We based our formalization on Baader et al.'s algorithm. (Baader, Brandt, and Lutz 2005) During our mechanization, we discovered two errors in the published correctness proof that required changes to the algorithm. We obtained the executable code through the extraction functionality of the Coq proof assistant (The Coq Development Team 2024).\nContributions This paper has the following contributions:\n\u2022 Mechanized formalization of EL++description logic.\n\u2022 A modified subsumption checking algorithm that is provably correct.\n\u2022 An implementation of a subsumption checker with string and rational number concrete domains for EL++with a mechanized proof of correctness.\n\u2022 An executable OCaml code extracted from the verified implementation."}, {"title": "Related Work", "content": "In response to community feedback and the evolving needs of ontology developers, OWL2 was released as a W3C recommendation in 2009. Apart from addressing acute problems with expressivity, a goal in the development of OWL2 was to provide a robust platform for future development. OWL2 extends the W3C OWL Web Ontology Language with a small but useful set of features that have been requested by users, for which effective reasoning algorithms can be built, and those OWL tool developers are willing to support. Considerable progress has been achieved in the development of tool support for OWL2. The new syntax is currently supported by the new version of the OWL API.\nThe widely used Prot\u00e9g\u00e9 (Musen 2015) system has been extended with support for the additional constructs provided by OWL2 and most recently gaining community support from volunteer groups such as OBO Foundry Ontologies (Smith et al. 2007).\nThe standardization of OWL has sparked the development and adaption of a number of reasoners, including FacT++ (Tsarkov and Horrocks 2006), Pellet (Sirin et al. 2007), and HermiT (Glimm et al. 2014), and ontology editors, including Prot\u00e9g\u00e9 (Musen 2015). However, mostly due to funding reasons and research interest shifting towards AI, the research for better reasoners was abandoned (McGinty 2018) despite the fact that recent work showed that more research is necessary for correct implementations (Parsia et al. 2017).\nOWL Reasoners Over the past three decades, a number of RDF and OWL reasoners were implemented which are reviewed in various articles, most notably by Mishra et al. (Mishra and Kumar 2011) and Colucci et al. (Colucci, Donini, and Di Sciascio 2024). However, for our purposes, we will list a few notable reasoners that have been dominating the field for the past two decades (McGinty 2018).\n\u2022 ELK: ELK (Kazakov, Kr\u00f6tzsch, and Simancik 2012) is described as a high-performance reasoner for OWL EL ontologies. Unlike conventional tableau-based procedures, which test unknown subsumptions by trying to construct counter-models, the EL procedures derive new subsumptions explicitly using inference rules.\n\u2022 FACT++ : FaCT++ (Tsarkov and Horrocks 2006) implements a tableaux decision procedure for the well-known SHOIQ description logic, with additional support for datatypes, including strings and integers. The system employs a wide range of performance-enhancing optimizations, including standard techniques (absorption and model merging) and newly developed ones (ordering heuristics and taxonomic classification).\n\u2022 HermiT: HermiT (Glimm et al. 2014) is the first publicly-available OWL reasoner based on a novel hypertableau calculus, which provides much more efficient reasoning than any previously known algorithm.\n\u2022 Pellet: Pellet (Sirin et al. 2007) is the first sound and complete OWL-DL reasoner with extensive support for reasoning with individuals (including nominal support and conjunctive query), user-defined datatypes, and debugging support for ontologies.\n\u2022 Konclude: Konclude (Steigmiller, Liebig, and Glimm 2014) is a high-performance reasoner for the Description Logic SROIQV. The supported ontology language is a superset of the logic underlying OWL 2 extended by nominal schemas, which allows for expressing arbitrary DL-safe rules.\nMechanized Formal Logic A diverse selection of general-purpose logics such as propositional logic (Guo and Yu 2023), first-order predicate logic (Herbelin, Kim, and Lee 2017), quantified modal logic (de Almeida Borges 2022), and logic of bunched implications (Frumin 2022) are formalized in proof assistants for metatheoretical studies. Additionally, formalizations of more specialized logics such as separation logic (Jung et al. 2018), matching logic (Bereczky et al. 2022), alternating-time temporal logic (Zanarini, Luna, and Sierra 2012), linear logic (Xavier et al. 2018), and ALC description logic (Hidalgo-Doblado et al. 2014) serve as tools for verifying software. Our work adds EL++description logic to this growing body of work.\nVerified Reasoning Baader et al. use automated theorem proving to certify the results of the ELK reasoner for OWL2 EL (Baader, Koopmann, and Tinelli 2020). They implemented an algorithm that extracts a proof certificate from an ELK result and checks its correctness using LFSC proof checker. This approach incurs time and memory overheads due to certificate generation and proof checking at runtime and lacks support for required extensions such as concrete domains. In contrast, VEL does not incur any proof-checking overhead since the correctness proofs of our implementations are statically checked. Eliminating this overhead will make our reasoners more scalable compared to validation-based approaches. VEL also supports required extensions for wider applicability.\nHidalgo-Doblado et al. formalized ALC description logic and implemented a formally verified tableau-based reasoner in PVS. (Hidalgo-Doblado et al. 2014) Although it is a well-known description logic, ALC does not correspond to any OWL2 profile and lacks the support for datatypes and other extensions. These limitations restrict the usability of their implementation in applications. OWL2 guides VEL's development, and VEL supports necessary logic extensions to ensure' widespread usage of our implementations."}, {"title": "Formalization of EL++", "content": "The first step in implementing a verified reasoner is formalizing the logic it will operate on. We decided to keep our formalization as generic as possible to facilitate its use in other contexts. In this section, we will explain each part of our formalization."}, {"title": "Primitives", "content": "We parameterized our formalization over primitives such as names and concrete domain elements. We defined the names as decidable infinite types. A decidable infinite type has two components: a decision procedure for checking the equality of two terms and a function that takes a list of terms and returns a term that is not in the list. Such parametrization allows using different types without changing the core implementation. As a result, our formalization is easier to integrate in different applications."}, {"title": "Concrete Domains", "content": "Our formalization models each domain as a record with four definitions and three proofs. Our definitions include the domain, which is a non-empty set of concrete domain elements, a set of predicate names with a decidable membership, the arities of each predicate as a partial function from predicate names to natural numbers, and an application function that determines the result of a predicate application to a list of concrete domain elements\nThree required proofs are the specifications for arity definitions and the application function. The first one requires that arities are defined only for the predicates of the concrete domain. The second one provides a specification for the application function regarding input value validity. It states that if the function returns true for some predicate and list of concrete domain elements, then the predicate should belong to that concrete domain, each element should be in the domain of that concrete domain, and the number of elements should match the arity of the predicate.\nDuring our formalization, we discovered that predicate names of different domains should also be distinct, which wasn't required in the original paper. This necessity arises because rule CR9 in classification needs to decide that two predicate names belong to different domains (Baader, Brandt, and Lutz 2005). However, this is not a limitation in practice because shared names between domains can be replaced to ensure disjointness.\nSome rules in the classification algorithm require checking the satisfiability of a conjunction of predicate expressions or the implication between predicate expressions, interpreted as a first-order formula. To be able to ensure the correctness of satisfiability and implication checker implementations, we defined an evaluation function that takes a first-order sentence composed of predicate expressions and an assignment to feature names and returns a boolean that represents if the given sentence is satisfiable under the given assignment. Then, we defined the satisfiability and implication checker specifications in terms of the results of the evaluations. We decided to use dependent typing to ensure the implementation of the functions to satisfy their specification."}, {"title": "Language", "content": "We defined the language as an inductive type where each constructor corresponds to an EL++concept description constructor. We also defined a well-formedness predicate for concrete domain descriptions to ensure that predicate names belong to one of the concrete domains and the number of feature names passed to the predicate matches its arity. Unlike other restrictions, we decided to define this as a predicate instead of using dependent typing to keep the equality of concept descriptions decidable.\nOn top of concept descriptions, we defined constraints and knowledge bases. Our knowledge bases consist of lists of names that can be used, a CBox, and the proof that the only names appearing in the CBox are the ones in the lists."}, {"title": "Semantics", "content": "We formalized the provided model-theoretic semantics for EL++. We first defined what we call \"a base interpretation\". A base interpretation maps each name to its corresponding structure in the model, namely, each individual name to an individual in the domain, each concept name to a set of individuals, each role name to a pair of individuals, and each feature name to a partial function from individuals to concrete domain elements.\nThen, we defined the interpretation function that recursively constructs the interpretation of a concept description for a given base interpretation. Finally, we defined the notions of model and subsumption as described in Baader et. al.."}, {"title": "Implementation", "content": "Our implementation of the reasoner consists of four parts: transformation, normalization, A-extension, and classification. Transformation changes the problem from checking the subsumption of arbitrary concept descriptions to checking the subsumption of two concept names. Normalization breaks down complex constraints into simpler ones to make subsumption checking tractable. A-extension adds an axiom that ensures the completeness of the reasoner. Finally, classification computes concept inclusions from the constraints.\nOn top of our reasoner implementation, we implemented two concrete domains: rational numbers and strings. Project structure can be seen in Figure 1. Rounded rectangles represent our components implemented in the proof assistant, hexagons represent the proof assistant's functionalities, and normal rectangles represent runnable code."}, {"title": "Transformation", "content": "The first step of checking subsumption $C \\subseteq D$ is transforming the knowledge base to add $A \\subseteq C$ and $D \\subseteq B$ where A and B are fresh concept names, as described in Baader et. al. This transformation changes the problem from subsumption checking of arbitrary concept descriptions to subsumption checking of basic concepts while preserving the subsumption relationship. It returns the transformed knowledge base and the newly generated names to be used in A-extension and final subsumption checking."}, {"title": "Normalization", "content": "We implemented the normalization in two steps, as presented in the paper. Both steps follow a similar pattern where each rule is repeatedly applied until none of the rules can be applied anymore. Again, we used dependent typing to ensure the normalization implementation satisfies its specifications.\nFormalization of the Rules We formalized each rule in two parts: the condition and the application. Such separation allowed us to use a higher-order selection function that removes and returns an eligible constraint given a rule condition or tells us no such constraint exists. Then, the chosen constraint is passed into the application function, which creates the new constraints and adds them to the CBox.\nThis modular implementation allowed us to encapsulate each rule's effect on the CBox and prove the necessary theorems more easily, making the process tractable."}, {"title": "Termination Measure", "content": "We implement each normalization part as a recursive function that finds candidates and applies the corresponding rules. Defining a recursive function in Coq requires proving that it will terminate for all inputs. One way to do that is by providing a measure and proving that it decreases after each recursive call. We defined a measure based on the number of rule applications left. However, naive counting wasn't enough to give us a decreasing measure since applying a rule could create new candidates for application. To circumvent that problem, we used a weighted count where applying a rule that can create new candidates decreases the measure more than the new candidates increase. Listing 1 illustrates the part of the measure function for NF3 rule that counts nested R.C structures in a concept description."}, {"title": "Modified NF2", "content": "We had to make one modification regarding NF2 rule to the normalization routine to ensure that our measure decreases after every iteration. Measure for NF2 counts the number of 's that connect at least one complex concept description inside each constraint's left-hand side. This measure doesn't decrease when NF2 is applied to a constraint $C \\sqsubseteq D \\sqcap E$ where both C and D are complex concept descriptions. The resulting two constraints $A \\sqsubseteq E$, $C \\sqsubseteq A$ have the same total count with $C \\sqsubseteq D \\sqcap E$. However, if we immediately apply NF2 again to $A \\sqsubseteq D \\sqcap E$, we get $A \\sqsubseteq D \\sqcap B$, $C \\sqsubseteq A$, which has one less number of applications since both A and B are simple concept descriptions. So, we circumvented this problem by replacing NF2 with a new rule that applies NF2 twice when both concept descriptions are complex."}, {"title": "A-extension", "content": "We added a preprocessing step to address the original completeness proof error. We call this step A-extension of the knowledge base. Given a concept name A, A-extension of a CBox C is defined as\n$C^{A+} := C \\cup \\{ \\{ t \\} \\subseteq \\exists r_t.A \\}$\nwhere t and $r_t$ are fresh individual and role names, respectively. A-extension of a CBox ensures that in every $C^{A+}$ model, $A^{\\textit{I}}$ is nonempty. Nonemptiness plays a crucial role in the completeness proof, which will be explained in a future section.\nWe also proved that A-extension preserves subsumption with respect to A. This is due to the fact that each model of C where $A^{\\textit{I}}$ is nonempty can be extended to obtain a model of $C^{A+}$. Conversely, every model of $C^{A+}$ is a model of C. Therefore, the following theorem holds:\nTheorem 1 $C, B \\in N_C. A \\subseteq_C B \\leftrightarrow A \\subseteq_{C^{A+}} B$"}, {"title": "Classification", "content": "We followed a similar methodology to normalization by splitting rules into conditions and applications and then using a higher-level function to find candidates. This allowed us to use the same proof patterns we used in normalization.\nReachability A crucial operation for classification is reachability. Reachability states that a sequence of concept descriptions exists in which each consecutive pair is related by a role between two concept descriptions. We implemented reachability as a depth-first search that starts from the end and constructs the path until it reaches the beginning. To verify it, we defined a predicate that implements the rules in the paper, then dependently typed the function so it returns true if and only if the predicate holds.\nClassification Data Structure Our implementation of classification defines the two components, namely S and R, as functions from concept descriptions to a list of concept descriptions and a list of pairs of concept descriptions, respectively. This definition simplifies the proof scripts but incurs a large run-time overhead. The overhead stems from the fact that each update adds a conditional at the top level of the function. Therefore, every lookup must traverse thousands of nested conditionals to compute the function. Replacement of classification implementation with a more efficient data structure left as future work.\nTermination Measure For all the rules except CR6, we defined our measure as the number of rule applications left. For CR6, we measured the \"distance until full S\" for each concept description in BC. This measure relies on the fact that for any concept description $C \\in BC$, $S(C)$ is a subset of $BC \\cup \\{1\\}$. Since $BC \\cup \\{1\\}$ is finite and the contents of S are monotonically increasing, it gives us a proper measure. Unlike normalization, each rule application reduced the measure, so we didn't need to change the rules."}, {"title": "Subsumption Checking", "content": "Our subsumption checker function ties individual steps together and then checks the two conditions described in Baader et. al. Listing 2 displays the final subsumption checking algorithm between two concept descriptions."}, {"title": "String and Rational Number Domains", "content": "We implemented rational number and string domains described in Baader et. al. for our runnable code. In our implementation, we took advantage of the flexibility of representing names in our formalism to reduce the complexity of processing the predicate names with built-in arguments, such as \"equals to 1\". Instead of using strings as predicate names, we defined a type that carries those built-in arguments in them. Using a custom type allowed us to eliminate all the manipulation that may be required if strings or other less expressive types are used."}, {"title": "Proving Correctness", "content": "As presented in the original paper (Baader, Brandt, and Lutz 2005), we split the correctness of the algorithm into soundness and completeness. To ensure end-to-end correctness, we tied all the proofs together at the end to obtain the following succinct specification:\nTheorem 2 Correctness\n$\\forall C, C, D$.\ncheck\\_subsumption (C, C, D) = true $\\leftrightarrow C\\subseteq D$."}, {"title": "Soundness", "content": "Our soundness proof followed a similar structure to Baader et. al. 's pen-and-paper proofs. Required properties were invariant under each rule application, therefore we only needed to prove their invariance. The rest of the soundness proof mechanized smoothly."}, {"title": "Completeness", "content": "Completeness proof was more challenging to mechanize than soundness proof. It required us to define a new invariant, fix errors in the original proof, and employ non-constructive reasoning. We will explain our new invariant and the errors in the original proof below.\nRole Trees Point 2 of Claim 1 in the original completeness proof involved a property that holds for the final classification but doesn't necessarily hold in every intermediate step. The proof in the original paper uses induction over\nonly some rule applications, which requires us to do inductive proof over groups of rule applications instead of each application. This is not an easily mechanizable proof strategy. To mechanize the proof, we defined an invariant that implies the desired property for the final classification but is also preserved during each recursive call.\nTo define the invariant, we first defined a role tree, an inductive structure that holds the information on how a pair of concept descriptions can be included in the classification of a role. Listing 3 displays its formal definition.\nOur role tree has three constructors: one for each rule that extends the R component of a classification.\n\u2022 Exists corresponds to the addition by CR3 and forms the leaf nodes.\n\u2022 Trans corresponds to the addition by CR10 where t is the tree that contains the information on how (C, D) can be included in R(r1).\n\u2022 SplitR corresponds to the addition by CR11 where t1 and t2 are the trees that contain the information on how (C, E) and (E, D) can be included in R(r1) and R(r2), respectively.\nEven though role trees contained possible historical information, they didn't include what having that history entails. To establish this connection, We defined a function that constructs a proposition from a tree given a knowledge base and a classification. Function recursively traverses a given tree and returns the proposition that asserts that conditions of corresponding rules hold. Listing 4 shows its definition.\nFinally, we defined a predicate that asserts the root of a tree corresponds to a given (r, C, D) tuple. Given these three definitions, our invariant states that for all concept descriptions C and D, and role name r, if (C, D) is in R(r), then there exists a role tree such that, its root corresponds to (r, C, D) and the derived proposition holds."}, {"title": "Non-transitivity", "content": "Completeness proofs in Baader et. al. heavily rely on a relation we call BC\u00af-equivalence, denoted by ~.\nThe relation is defined over the set BC, which is the set of all concept descriptions in BC that are reachable from A. The relation C ~ D for concept descriptions C, D E BC is defined as either C is equal to D, or there exists an individual name a such that {a} \u2208 S(C) \u2229 S(D). Their formal definitions are as follows:\nBC\u00af = {C \u2208 BC | A \u2194 C'}\n~ = {(C, D) | C = D v \u2203 a. {a} \u2208 S(C) \u2229 S(D)}\nDuring our formalization, we discovered that ~ is not an equivalence relation because it wasn't transitive. Since the rest of the proof relied on it being an equivalence in multiple places, the completeness proof in the paper wasn't correct. Figure 2 shows an example where transitivity doesn't hold. The example contains a transformed and normalized CBox for query. In the example, X, {b}, {c} \u2208 BC\u00af, {b} ~ X, and X ~ {c} but it is not true that {b} ~ {c}. Therefore, ~ is not transitive under the old algorithm. It is also important to point out that the example does not demonstrate that the old algorithm is incomplete.\nTransitivity of the relation relies on the following two properties:\n1. For all models I of CBox C and individuals i and concept descriptions C and D, if i is in C and D is in S(C), then i is in DI.\n2. For all models I of CBox C and concept descriptions C and D, if C \u2192 D and C is nonempty, then D\u00b9 is nonempty.\nThe problem stemmed from the fact that in some models of C, A can be empty. We believe authors implicitly as- sumed that they could prove the completeness by reasoning about the models where A\u00b9 is nonempty since subsumption holds trivially in the models where A\u00b9 is empty. They used this assumption to prove that\n$\\forall C, D \\in BC^{-} . C ~ D \\rightarrow S(C) = S(D)$\nwhich doesn't hold if a model exists where A\u00b9 is empty. Since classifications and models are \u201cliving in separate levels,\" the statement cannot be qualified to hold only under the nonempty models, which makes the statement false.\nWe fixed this error by introducing A-extensions that enforce nonemptiness on the classification level. The models of an A-extension of a CBox are precisely the ones where A\u00b9 is nonempty. By restricting the models to nonempty ones, we recovered the transitivity of the relation, which fixed the major flaw in the proof of completeness. The question of whether the original algorithm is complete remains open."}, {"title": "Under-specification", "content": "We identified another error in the statement of a lemma, originally called Claim 2, used in constructing a counterexample model. Lemma stated that for each concrete domain dm and an equivalence class [C] with respect to ~, there exists a solution d([C], dm) that only satisfies the predicate tuples in S(C). Below is the original lemma from Baader et. al.\nClaim 2. For each $C \\in BC^{-}$ and each concrete domain dm, we can find a solution $d([C], dm)$ for $cond_m (S(C))$ such that, for all concepts $D \\in BC$ of the form $p(f_1,\u2026\u2026, f_k)$ with $p \\in P_{dm}$, we have $d([C], dm) |= D$ iff $D \\in S(C)$, where $cond_m (S(C))$ denotes the conjunction of all the predicates in S(C) that belongs to dm.\nThis solution defines the interpretation of feature names in the counterexample. The behavior of solutions from Claim 2 is only restricted in terms of the domain they are being selected for. Therefore, they may satisfy some predicate tuples not in S(C) from other domains. However, proof of Claim 3 relies on the fact that this solution doesn't satisfy any other predicate not in S(C), regardless of whether the predicate belongs to the chosen domain.\nWe fixed this error by reordering the quantification and further restricting the behavior of the solution. The definition of our lemma is\nLemma 3 Solution existence\n$\\exists \\delta . \\forall C \\in BC^{-}, dm, p \\in P_{dm}, f_1,\u2026\u2026, f_n$.\n($\\delta([C]) = p(f_1,\u2026, f_n) \\leftrightarrow p(f_1,\u2026, f_n) \\in S(C)$) $\\land$\n($\\forall f. (\\exists e. d([C], f) = e) \\leftrightarrow p \\in P_{dm}, f_1,\u2026, f_n. p(f_1,\u2026, f_n) \\in S(C) \\land f \\in (f_1,\u2026, f_n)$)\nOur lemma states that there exists a function from ~-equivalence classes to solutions, i.e., partial functions from feature names to concrete domain elements, such that, for all concept descriptions C in BC\u00af, all concrete domains"}, {"title": "Extraction", "content": "We used Coq's extraction procedure to obtain a runnable OCaml code. We extracted certain Coq types to native OCaml types for better run-time performance. More precisely, we extracted natural numbers and integers to Ocaml int type, rational numbers to zarith library's Q type, and strings to native Ocaml string type. This choice introduces possible unsoundness to the runnable code due to possible integer overflows since original Coq types are unbounded and cannot overflow. We believe that this is a reasonable tradeoff since 64-bit integers are very unlikely to overflow in practice. Using Coq's default extraction settings to obtain slower but non-overflowing implementation is still possible. We obtained the custom extraction code for these types from mlvoqe library. (Hietala et al. 2021)"}, {"title": "Trusted Computing Base", "content": "Our trusted computing base consists of Coq Proof Assistant, OCaml run-time and libraries, GLPK linear program solver, and our implementations of satisfiability and implication checkers for concrete domains."}, {"title": "Evaluation", "content": "We evaluated our work in two axes. We measured the required proof effort by comparing the number of lines of proof per line of implementation. We evaluated our extracted artifact for runtime performance on randomly generated knowledge bases of various sizes."}, {"title": "Proof Effort", "content": "We measured proof effort by lines of code and the number of definitions and theorems. Our 165 definitions consist of 1511 cloc. Our 387 theorems consist of 16977 cloc, giving us 11.3x proof overhead."}, {"title": "Performance Evaluation", "content": "To evaluate the performance of our implementation, we measured run times on randomly generated knowledge bases with different sizes and structures. We designed our test cases to measure the impact of concept and role inclusion axioms separately. All knowledge bases had 20 concept names, 10 role names, and 20 individual names. For concept inclusion tests, we kept the number of role inclusion axioms at 10 and the maximum length of the left side at 2. For role inclusion tests, we set the number of concept inclusions to 20 and the maximum length of the left side of role inclusions to 5. All the tests are run on AMD Ryzen 5 2.20 GHz CPU with 16 GB of RAM. The following table shows sample numbers where each row corresponds to a constraint type, and each column corresponds to the number of constraints of that type.\nOur results show a very high variance between different tests, indicating that, as expected, the structure of a knowledge base has more impact than its size when it comes to reasoner performance."}, {"title": "Conclusion and Future Directions", "content": "The Semantic Web, over the past three decades, helped enhance a wide range of applications. Its core principle of providing a semantic layer for data allows for meaningful deductions and explanations, setting it apart from sub-symbolic technologies like neural networks. However, the varying sophistication of SW systems' reasoning abilities highlights the need for reliable reasoning algorithms, particularly in the context of the growing interest in Neurosymbolic AI and Generative AI approaches that seek to utilize Semantic Web layers. The critical importance of ensuring the correctness of reasoner results cannot be overstated, especially as AI systems are increasingly used in sensitive and high-stakes domains.\nIn an attempt to address this gap, our work lays the foundation for formally verified reasoners for Neurosymbolic AI applications. We believe that, in the cases of high-stakes application domains, such as health, finance, and security, the benefit of increased trustworthiness outweighs the increased development effort. As AI continues to enter different and critical domains, ensuring the correctness of reasoning processes through formal verification might become essential in preventing errors and fostering trust in AI systems.\nAs for future directions, one immediate step would be incorporating domain and range restriction into EL++to increase its expressive power. This integration is theoretically described in (Baader, Brandt, and Lutz 2008). To implement the increase in expressive power, we would be required to modify the language and add new functionality to eliminate domain and range restrictions. In relation to this approach, we might work towards implementing reasoners for other OWL profiles since OWL EL's expressive power is limited.\nAnother future direction would be improving the implementation performance. Our work faithfully replicated the Baader et al.'s algorithm. During this process, we observed many places where the implementation can be modified for better run-time performance. For example, the algorithm can traverse the knowledge base and group the constraints based on which rule they may fit. This approach would eliminate the overhead of searching for a candidate one at a time, repeat testing of unviable candidates, and enable parallel application of the rules."}]}