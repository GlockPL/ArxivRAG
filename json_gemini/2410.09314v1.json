{"title": "LL-INSTRUCT: An Instruction-tuned model for English Language Proficiency Assessments", "authors": ["Debanjan Ghosh", "Sophia Chan"], "abstract": "We present LL-INSTRUCT: An 8B instruction-tuned model that is designed to generate content for English Language Proficiency Assessments (ELPA) and related applications. Our work involves creating a new dataset of 70K instructions and explanations in the ELPA domain and using these to fine-tune Llama-3 8B models (SFT) of different sizes (e.g., SFT-17K, SFT-50K and SFT-70K). Human evaluations are conducted over unseen instructions to compare these SFT models against SOTA models (e.g., Dolly-2, Mistral, Llama-3 base version, and GPT-3.5). The findings show although all three SFT models perform comparably, the model trained on largest instruction dataset SFT-70K - leads to the most valid outputs ready for assessments. However, although the SFT models perform better than larger model, e.g., GPT 3.5 on the aspect of explanations of outputs, many outputs still need human interventions to make them actual ready for real world assessments.", "sections": [{"title": "1 Introduction", "content": "Instruction tuning-or multitask prompted finetuning is an area within Natural Language Processing (NLP) that has led to state-of-the-art performance across a variety of tasks in recent years (Ouyang et al., 2022; Sanh et al., 2022; Wang et al., 2022; Peng et al., 2023). Broadly, this involves training a pre-trained Language Model (LM) using <INSTRUCTION, OUTPUT> pairs where the INSTRUCTION describes the task in natural language and OUTPUT is the desired outcome. As mentioned in Peng et al. (2023), current research primarily focuses on two sub-areas: scaling instruction-tuning models using more training data and resources and/or utilizing large amounts of human-annotated data for training. However, it is known that (a) gathering human-annotated data is expensive and time-consuming and (b) instruction-tuned models that are specialized for specific NLP tasks tend to struggle when applied to a diverse range of tasks (Zhang et al., 2023).\nIn response to these shortcomings, several recent models have been trained using a diverse set instructions generated via a semi-automated method. Wang et al. (2022) collected a small set of manually-written <INSTRUCTION, INPUT, OUTPUT> examples and then used the set of examples to prompt GPT-3 (Brown et al., 2020) to generate a larger set of more diverse instructions. The authors then fine-tuned GPT-3 using the generated tuples. This approach is named as SELF-INSTRUCT as the final model is trained on self-generated instructions.\nInspired by the aforementioned approach, in this paper we introduce (L)anguage (L)earning IN-STRUCT (henceforth, LL-INSTRUCT): an instruction tuned model specifically designed for English Language Proficiency Assessments (ELPA) and other related applications in the Educational Technology (EdTech) domain. The Language Learning market is experiencing a significant growth with a projection of surpassing 100 billion USD in the coming years. This coincides with the rise in the utilization of large pretrained LMs, with multiple organizations adopting automated content generation for ELPA such as Duolingo (Settles et al., 2020; Burstein et al., 2021) and Cambridge Assessment English (Galaczi, 2023).\nThere are several distinguishing aspects of LL-INSTRUCT that set it apart from other instruction-tuned models such as Alpaca (Taori et al., 2023), WizardLM (Xu et al., 2024), LLaMA-GPT4 (Peng et al., 2023), RoleLLM (Wang et al., 2024) and so on. First, the authors manually write 130 seed instructions in the form of <INSTRUCTION, INPUT,"}, {"title": "2 Datasets", "content": "In this section we describe in detail our approach of collecting training data to build the LL-INSTRUCT model. This includes: curating seed instructions, generating diverse instructions with a LLM, and filtering the generated instructions. The full flow is shown in Figure 1."}, {"title": "2.1 Curating Seed Instructions", "content": "To start, we curate a set of seed instructions so that they can be the basis for generating additional instructions. Note, a key differentiating factor in our work is that our instructions are solely based on ELPA as applicable in the field of EdTech.\nThe authors, who are familiar with standard ELPA, begin by converting publicly available ELPA items into instruction examples. In most"}, {"title": "2.2 Automated ELPA Instruction Generation", "content": "Similar to Wang et al. (2022) we generate new instructions via a bootstrapping method. In each step, we include four <INSTRUCTION, INPUT, OUTPUT, EXPLANATION> instruction tuples in the prompt: three seed tuples and one model-generated tuple. To promote diversity, we split the 130 seed tuples into two categories: (a) short tasks (e.g., grammar correction, convert a passive sentence into active, etc.) and (b) long tasks (e.g., write an email, write a short conversation, etc.) and then randomly choose either two short tasks and one long task or vice-versa in the prompt. Then, GPT-4 is prompted to generate ten new tuples, corresponding to ten new instructions.\nRefer to Section A.1.1 in the Appendix section for the prompt template. We have included a few requirements in the prompts that are intended to guide the model. For example, we ask that the new instructions be relevant to ELPA and not involve generating code or solving arithmetic problems."}, {"title": "2.3 ELPA Instruction Data Filtration", "content": "Despite explicitly prompting against it, we notice that the model sometimes generate tasks about factual information, such as \"What is the capital of Australia?\". As mentioned before, this kind of task data is not beneficial for ELPA and we aim to exclude such data in our fine-tuning. Evaluation of LLM outputs guided by another LLM has been shown to be effective (Chiang and Lee, 2023), thus, to remove factual data from the Automatic ELPA Instruction Data Generation round, we use another GPT-4 model.\nWe write a new prompt (refer to Section A.1.2 in the Appendix for the complete template) that includes examples of both factual tasks that we intend to exclude and non-factual tasks that we want. After conducting a small pilot test, we found that approximately 7% of non-ELPA tasks were produced by the bootstrapping step (Section 2.2), which have now been flagged and removed. Furthermore, we enhance ELPA instruction data quality through standard filtering and postprocessing, removing instructions with irrelevant terms, such as, video, image, graph, flowchart, etc. To maintain diversity, we avoid adding instructions too similar to existing ones, using a ROUGE-L metric to ensure no two instructions exceed a 0.75 similarity score (this was set empirically after tuning).\nFollowing the completion of all filtering processes, we are left with 70K instructional data. In the next section we present an evaluation of the generated data and describe the contents."}, {"title": "3 Evaluation of LL-INSTRUCT Data", "content": "Our work focuses on the quality of automatically generated LL-INSTRUCT data, crucial for tailoring SFT models to ELPA. Unlike Wang et al. (2022) and related studies, we conduct a large-scale evaluation to ensure the instructions' relevance and suitability for English language assessments.\nWe randomly selected 250 generated instruction tuples and carried out the evaluation in two stages. First, we classify the instruction tuples by language category (e.g., grammar, semantic, etc.) and language skills (e.g., speaking, writing) to demonstrate the types of instructions included in the dataset (Section 3.1). Next, we further focus into specific aspects of the instruction tuples, such as, output correctness, quality of explanation, etc. (Section 3.2)."}, {"title": "3.1 First Evaluation Task: Language Category and Skills", "content": "The language category of an instruction specifies the type of linguistic knowledge that the <INSTRUCTION, INPUT, OUTPUT> and the resulting ELPA item probes for. For example, some categories are grammar, vocabulary, semantic, pragmatic, and prose (i.e. prose writing). Likewise, we also categorized the instruction tuples to language skill such as reading and writing. Authors of this papers first conducted a pilot annotation task to determine the main categories and then jointly annotated the remaining examples. Figure 2a and Figure 2b present the main categories and skills identified from the example set, respectively.\nIn terms of the observed categories, instructions related to grammar (e.g., \u201crewrite the question in reported speech\u201d, \u201cidentify the preposition in the"}, {"title": "3.2 Second Evaluation Task: Instruction Quality", "content": "Here, we focus onto the following aspects of the instructions.\n\u2022 Validity: whether the example is valid and ready to appear in an English language assessment. We provide three options: valid and ready for assessment, only valid (i.e., needs some editing), and invalid.\n\u2022 Instruction type: whether the instruction is factual or not factual.\n\u2022 Input faithfulness: does the input matches or not matches to the instruction.\n\u2022 Output correctness: whether the output is correct (based on the instruction) or not.\n\u2022 Quality of explanation: does the explanation justify the output? We provide four options: yes, weak yes, weak no, and no.\nWe recruit ten expert annotators, each with a background in linguistics, computer science, and EdTech experience, to evaluate 50 instruction tuples per pair of annotators. We measure Krippendorf's \u03b1 (Krippendorff, 2011) on each aspect for each pair of annotation and then report the average \u03b1. They are: 0.49 for Validity (moderate agreement), 0.93 for Instruction type (almost perfect agreement), 0.67 for Input faithfulness (substantial agreement), 0.78 for Output correctness (substantial agreement), and 0.52 for Quality of explanation (moderate agreement). We focus into Validity and Quality of explanation aspects where the agreement is comparative lower than the other aspects. Regarding Validity, most of the disagreements occur between choosing the instructions as valid and ready vs. valid. We notice, in case of instructions like \"identify the tense/voice/verb type of the sentence,\" some annotators indicated that providing a list of possible options is typical in an actual assessment rather than directly asking for identification. Similarly, for instructions like \"provide a synonym/alternate ending/...,\" some annotators critiqued them as being too open-ended, suggesting that these usually avoid such ambiguity or provide options to choose from. Likewise for Quality of explanation aspect annotators sometimes"}, {"title": "4 Experimental Details", "content": "The experiment involves supervised fine-tuning (SFT) a Llama-3 8B model. The design choice to use a small 8B model is driven by two primary motivations: (a) to evaluate how effective a small SFT model can be for language learning applications, and (b) to ensure fast inference and moderate GPU requirements, thereby lowering the barrier to trying these model(s).\nFine-tuning Llama-3 8B Llama-3 8B was fine-tuned on subsets of the LL-INSTRUCT data of size 17K, 50K, and 70K. Each <INSTRUCTION, INPUT, OUTPUT, EXPLANATION> tuple was joined into one example using the following template:\nBelow is an instruction that describes a task.\nWrite a response that appropriately completes the request. ### Instruction: INSTRUCTION ### Input: INPUT ### Output: OUTPUT### Explanation: EXPLANATION\nHuggingface repository is used to perform the SFT. Parameter specifications can be found in the full training command in Section ?? in the Appendix.\nInference on Test Dataset To evaluate the quality of the SFT models (SFT-17K, SFT-50K, and SFT-70K) they are compared against several SOTA models (only pretrained without any additional fine tuning): base Llama-3 8B (Meta, 2024), GPT-3.5 (Brown et al., 2020), Mistral 7B (Jiang et al., 2023), and Dolly-2 8B (Conover et al., 2023). The prompt for inference is similar to the one used for fine tuning, except we start generating at the output:\nWrite the output by following the instruction and\nthe input, and then include an explanation for\nwhy the output is appropriate given instruction\nand input. Include a separator token '###' before\nthe explanation.\n### Instruction: INSTRUCTION ### Input: INPUT\n### Output:\nWe selected an unseen batch of 200 instructions to do the comparison, where the instructions are ranged over diverse tasks such as grammar, figurative language and prose."}, {"title": "5 Human Evaluation of Model Performance", "content": "For each test instruction, the authors jointly evaluated the output from each of seven models, i.e., a total of 200x7, 1400 outputs. We re-use the rubric that was used to evaluate the quality of LL-INSTRUCT in Section 3.2. We assessed dimensions of Validity, Output correctness, and Quality of explanation, but omitted Instruction type and Input faithfulness dimensions due to the non-factual nature of almost all instructions and and the lack of dedicated input entries in the test set, respectively."}, {"title": "5.1 Comparison Between All Models", "content": "An overview of how the models fared on each dimension can be seen in Figure 3. Table 3 summarizes the results, showing the percentage of data in each category. Additional tables that show the win-rate and tie-rate between each model can be found in Section A.2 in the Appendix. Let us examine the results for each of these three dimensions in turn.\nValidity SFT-70K and GPT-3.5 have the highest number of valid and ready generations where SFT-70K is marginally better (63.5% vs. 63%, see Table 3) followed by SFT-50K and SFT-17K almost equally. In contrast, Mistral, Llama-3, and Dolly-2 have fewer than 50% valid and ready generations, with Mistral leading at 45%, followed by Llama-3 at 26%, and Dolly-2 at 12%."}, {"title": "5.2 Qualitative Error Analysis", "content": "We conducted a thorough error analysis of all the model outputs and highlighted specific characteristics here.\nVerbose outputs and explanations Most often for GPT-3.5, Mistral, and Llama-3, while the output and explanation match the specification, an excessive amount of words and description is used. Consider the following instruction:\nINSTRUCTION: Write a [passage type = email]\nfrom [org = Blue Sky Airlines] to [person = Pas-\nsenger] regarding their lost luggage case. Explain\nthe steps the company is taking to locate the lug-\ngage and reassure them. The length of the answer\nshould be around 100 words.\nINPUT: noinput\nGPT-3.5 produced a 212-word email, while SFT models created emails around 100 words. Without specified word limits in instructions, GPT-3.5 often wrote very long responses, sometimes reaching 250-300 words. Llama-3 (base) and Mistral also frequently created longer responses.\nVerbose outputs are difficult to assess and edit, so they are not preferred. Our evaluation, however, overlooks verbosity unless it makes the output ineligible, since it does not impact Validity or Output Correctness.\nExplanations are often missing Even if we prompt the models to include explanation for the output, we find that often the explanations are missing. For instance, we notice that explanations are missing from 95% of Dolly-2 generations, 55% of Mistral generations, 51% of GPT-3.5 generations, and 10% of Llama-3 generations. On the contrary, only less than 1% of any fine-tuned model (SFT-17K, SFT-50K, and SFT-70K) is missing the explanation.\nFormatting errors of outputs Formatting errors in the generation can hinder the full automation of ELPA item generation. Common formatting issues include a numbered list being returned when only one item is requested, typically by the base Llama-3 model. Other problems involve the separator token ### (between the OUTPUT and EXPLANATION) being misplaced, repetition of the instruction in the output, and ignoring the word limit specifications, and so on. However, SFT models exhibit less frequently such formatting errors.\nOutputs are often in the proximity but do not follow the instruction exactly In general, we often notice errors where the models' interpretation of the instruction is close, yet it does not adhere to the request by missing some part of the instruction or simply adding extra information (hallucination)."}, {"title": "6 Related Work", "content": "Recent studies have demonstrated that LMs can effectively follow language instructions when fine-tuned using human-annotated datasets that pair instructions with outputs (?Sanh et al., 2022; Peng et al., 2023). However, to address the reliance (bottleneck) on large-scale human annotations, researchers such as (Ouyang et al., 2022; Wang et al., 2022) have developed general-purpose LMs designed to follow diverse sets of instructions. Our research is closely aligned with (Wang et al., 2022), which presented the idea of self-instruct (i.e., an iterative method for creating new instructions and outputs to enhance fine-tuning) and then taken by models such as Alpaca (Taori et al., 2023), WizardLM (Xu et al., 2024), LLaMA-GPT4 (Peng et al., 2023), RoleLLM (Wang et al., 2024). What sets our work apart is our specific focus on the language learning and assessment domain, where all instructions pertain to language categories like grammar, vocabulary, semantics, and pragmatics, and are spread across language skills such as reading and writing.\nSimilar to our approach, Humpback (Li et al., 2023) utilizes a collection of curated seed instructions to generate new ones. However, a key distinction is that Humpback derives its outputs from existing web corpora, whereas we generate all components of the <instruction, input, output> tuple using datasets generated by models like GPT-4. Although our work relates to the self-training literature, which typically defines a specific target as noted by (Wang et al., 2022), our approach is different. Despite our instructions being focused on language assessment, they exhibit wide diversity across various instruction types. Lastly, our research aligns with the concept of distillation (Hinton et al., 2015), as we extract new instructions from a teacher model (in this case, GPT-4). Additionally, we employ a separate language model as a discriminator to eliminate factual inaccuracies and non-ELPA instructions."}, {"title": "7 Conclusion and Future Work", "content": "We compiled a set of instruction seed data consisting of <INSTRUCTION, INPUT, OUTPUT, EXPLANATION> tuples designed for item generation in ELPA. Using these seed instructions, we prompted GPT-4 to generate a much larger dataset of instruction tuples (LL-INSTRUCT) for ELPA domain. Subsequently, we fine-tuned Llama-3 model using with different partitions (17K, 50K, and 70K) of the LL-INSTRUCT data. We compare the performance of the fine tuned models against various LM baselines including Dolly-2, Mistral, GPT-3.5, and Llama-3 (pretrained). The fine-tuned versions consistently demonstrated superior performance in terms of output validity, correctness, and explanation quality (Section 2.3).\nOur detailed error analysis identified common issues across the models, such as verbose responses (often from GPT-3.5), misunderstanding of instructions (often by Dolly-2), and formatting errors (e.g., Dolly-2 and Mistral). More importantly, we observe that while the fine-tuned Llama-3 models produced approximately 60% of outputs that were immediately test-ready, about 20-30% still required manual adjustments by Subject Matter Experts (SMEs) in the language learning field. This suggests that a combined human-AI approach would be most effective for advancing ELPA task designs.\nFor future work, we plan to improve our SFT model by aligning with human preference, e.g., DPO (Rafailov et al., 2023), specifically related to the unique language learning domain. We also plan to align the trained models to specific attributes (e.g. quality of explanation, output correctness) by post-hoc merging of parameters (similar to Jang et al. (2023))."}, {"title": "8 Ethics", "content": "The risks and harms of language models are well-documented. Bender et al. (2021) provides an overview, including: environmental and financial cost; unfathomable training data leading to encoded biases that reflect the dominant/hegemonic view; coherent output being mistaken as true knowledge.\nThis work uses GPT-4 (1.76 trillion parameters) for dataset generation. In addition to increased water consumption and carbon emissions (Strubell et al., 2020; George et al., 2023) when using a larger model, there is the risk of including harmful biases and misinformation in both the training data and in the fine-tuned models. The data was spot-checked and filtered using another LLM to remove factual data. To mitigate bias and fairness issues, we recommend adding additional checks, such as those implemented in Stowe et al. (2024), and involving human reviewers before rolling out any machine-generated content to learners.\nWe hope to show that a smaller model (i.e., a model that consumes less resources) can achieve the same performance as that of a larger model when training data is available. While smaller models are more accessible, they remain difficult to access in resource-limited environments where GPU compute is rare or expensive."}, {"title": "9 Limitation", "content": "The experiments were conducted for Llama-3 8B, and it is uncertain whether the findings will generalize to other models. The human evaluation of model performance was completed by the authors, who were also designed and conducted the experiment. As there may be unconscious biases on part of the authors, the dataset and annotations will be released upon acceptance."}, {"title": "A Appendix", "sections": [{"title": "A.1 Prompt templates", "sections": [{"title": "A.1.1 Prompt Template to generate ELPA Instructions", "content": "Generation prompt template\nYou are asked to come up with a set of 15 task instructions in English. These instructions should be useful for language\nlearners of English. These task instructions will be given to a GPT model and we will evaluate the GPT model for\ncompleting the instructions. Separate each instruction using \"###\".\nHere are the requirements:\n1. The type of instructions should be similar and related to the instructions in the prompt.\n2. These instructions should be related English language learning, such as grammars, semantics, pragmatics, etc.\n3. Please don't write instructions to write a code or program or answer a mathematical question.\n4. Please avoid generating factual instructions that ask specific questions on history, geography, politics, or science.\n5. The instructions should not contain racist, sexist, toxic, or otherwise potentially offensive language.\n6. Not all instructions require input. For example, when an instruction asks \"did you have lunch yet\", it is not necessary\nto provide a specific context. In this case, we simply put \"\" in the input field.\n7. The output should be an appropriate response to the instruction and the input.\nList of 15 tasks:\n// Here we insert a 3 seed instructions and 1 model-generated example.\n1. Instruction:\n1. Input:\n1. Output:\n1. Explanation:\n###\n###\n5. Instruction:\n5. Input:\n5. Output:\n5. Explanation:\n###\n6. Instruction:"}]}, {"title": "A.1.2 ELPA Instruction Data Filtration", "content": "Filtration prompt template\nGiven the following tuples of  your task is to evaluate the quality of the tuple(s) and accept or\nreject them based on the following requirements. Separate each example using \"###\".\nHere are the requirements:\n1. The  tuples are used for language learning in English.\n2. The output should not contain any verifiable factual information related to science, geography, history, business etc.\n3. The output should not contain any code, program, or mathematical formula.\n4. The output should be free of racist, sexist, toxic or otherwise potentially offensive language and imagery.\n5. The instruction should not contain inflammatory, highly controversial or upsetting topics.\n6. The output should not contain language or symbols that reinforce stereotypes.\n7. Return the full tuple  with Evaluation and Reason.\nSee the following examples.\n1. Instruction: Reply the statement with a proper answer.\nInput: we are all going to the movie at 7pm.\nOutput: Great, my work finishes at 6pm so perhaps I can join too.\nEvaluation: Accept.\nReason: The instruction, input, and the output does not contain any factual information, code, or inflammatory comment.\n###\n2. Instruction: Write an indirect answer to the question.\nInput: who is your favorite soccer player?\nOutput: I loathe soccer because it is not a manly sport.\nEvaluation: Reject.\nReason: The output seems toxic (\"not a manly sport\") and gender biased.\n###\n3. Instruction: This is an email written by a customer to a customer support team. Please give me a question that asks\nabout the main idea.\nInput: Email: Hello, Thanks for sending my order #3397-it arrived this morning. Unfortunately, the paint was not the\none I had asked for. I had selected color SP 944 but received SP 945 (Ocean Waves). They appear right next to each\nother on your Web site, so the two may have been confused at your end. Could you send me the correct paint, along\nwith additional samples that are close in color to SP 722? Thank you, Arun Phan\nOutput: What problem does Mr. Phan mention in his e-mail?\nEvaluation: Accept.\nReason: The instruction, input, and the output does not contain any factual information, code, or inflammatory comment.\n###\n4. Instruction: Write a 6-turn exchange between 3 people (Person-1, Person-2, and Person-3). They all work at the same\ncompany, and discuss thoughts on which division will end up occupying the space.\nInput: no-input.\nOutput:\n\"Person-1: Have you two taken a look at the progress they've made upstairs on the office expansion? It looks great!\nPerson-2: I know! I can't believe it! And the offices up there have amazing views of the city.\nPerson-3: I wonder which division will move up there when it's finished.\nPerson-2: I heard it's the research department.\nPerson-1: Ah, because the CEO is biased towards the department. In fact the CEO hired her husband to lead a new\nproject inside research.\nPerson-1: I think you're right, there!\"\nEvaluation: Reject.\nReason: The output seems toxic. The conversation is not suitable for a workplace environment.\n###\n5. Instruction: What is the capital of India?\nInput: no-input.\nOutput: New Delhi.\nEvaluation: Reject.\nReason: The output is factual."}, {"title": "A.3 Full evaluation examples", "sections": [{"title": "A.3.1 Example with a formatting error in the output", "content": "\u2022 Instruction: Generate a question to assess understanding of the main point in the following sentence.\n\u2022 Input: In spite of facing hardships, the team managed to win the championship."}, {"title": "A.3.2 Example where output is in the proximity but does not follow the instruction exactly", "content": "\u2022 Instruction: Paraphrase the following sentence.\n\u2022 Input: Nobody knew how much time she spent training for the Olympic Games."}, {"title": "A.3.3 Example where the task is easy for most models", "content": "\u2022 Instruction: Translate the given English sentence into simple words that a beginner might understand.\n\u2022 Input: The CEO convened an emergency meeting to discuss the catastrophic consequences of the\nfailed project."}, {"title": "A.3.4 Example where the task is difficult for most models", "content": "\u2022 Instruction: Write a sentence using an idiomatic expression.\n\u2022 Input:"}]}]}, {"title": "Acknowledgements", "content": "This document has been adapted by Steven Bethard, Ryan Cotterell and Rui Yan from the instructions for earlier ACL and NAACL proceedings, including those for ACL 2019 by Douwe Kiela and Ivan Vuli\u0107, NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, NAACL 2018 by Margaret Mitchell and Stephanie Lukin, BibTEX suggestions for (NA)ACL 2017/2018 from Jason Eisner, ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, ACL 2012 by Maggie Li and Michael White, ACL 2010 by Jing-"}]}