{"title": "HYPERSPECTRAL IMAGING-BASED PERCEPTION IN AUTONOMOUS DRIVING\nSCENARIOS: BENCHMARKING BASELINE SEMANTIC SEGMENTATION MODELS", "authors": ["Imad Ali Shah", "Jiarong Li", "Martin Glavin", "Edward Jones", "Enda Ward", "Brian Deegan"], "abstract": "Hyperspectral Imaging (HSI), known for its advantages\nover traditional RGB imaging in remote sensing, agriculture,\nand medicine, has recently gained attention for enhancing\nAdvanced Driving Assistance Systems (ADAS) perception.\nA few HSI datasets such as HyKo, HSI-Drive, HSI-Road, and\nHyperspectral City have been made available. However, a\ncomprehensive evaluation of semantic segmentation models\n(SSM) using these datasets is lacking. To address this gap, we\nevaluated the available annotated HSI datasets on four deep\nlearning-based baseline SSMs i.e. DeepLab v3+, HRNet,\nPSPNet, and U-Net along with its two variants: Coordinate\nAttention (UNet-CA) and Convolutional Block-Attention\nModule (UNet-CBAM). The original models' architectures\nwere adapted to handle the varying spatial and spectral\ndimensions of the datasets. These baseline SSMs were trained\nusing class-weighted loss function for individual HSI datasets\nand were evaluated over mean-based metrics i.e. intersection\nover union (IoU), recall, precision, F1 score, specificity and\naccuracy. Our results indicate that UNet-CBAM which\nextracts channel-wise feature extraction, outperforms other\nSSMs and shows potential to leverage spectral information\nfor enhanced semantic segmentation. This study establishes\nbaseline SSM based benchmark on available annotated\ndatasets for future evaluation of HSI-based ADAS\nperception. However, the limitations of current HSI datasets,\nsuch as limited dataset size, high class imbalance and lack of\nfine-grained annotations, remains a significant constraint for\ndeveloping robust SSMs for ADAS applications.", "sections": [{"title": "1. INTRODUCTION", "content": "Semantic segmentation plays a critical role in Advanced\nDriver Assistance Systems (ADAS) by enabling detailed\nscene understanding and object identification through per-\npixel classification [1]. While ADAS predominantly relies on\ntraditional RGB imaging, Hyperspectral Imaging (HSI)\npresents considerable advantages. HSI can capture hundreds\nof intensities across narrow spectral bands, including\nwavelengths beyond human visibility. This spectrally dense\ninformation enables more accurate analysis of material\ncomposition and object classification, making HSI a powerful\ntool, that has been proven in diverse fields such as ecosystem\nmonitoring and agriculture [2] as well as medicine [3].\nRecent advances in HSI sensor technology, particularly\nthe development of snapshot hyperspectral sensors, have\nmade these cameras smaller, cheaper, and capable of real-\ntime video capturing [4]. This progress opens new\npossibilities for HSI in dynamic and real-time applications\nsuch as ADAS and autonomous driving (ADAS/AD). As HSI\nhas the potential to address key limitations of RGB imaging,\nsuch as metamerism [5], and can thus improve object\nidentification, tracking, and general scene understanding in\nADAS/AD."}, {"title": "2. RELATED WORK", "content": "The use of deep learning techniques for HSI segmentation\nin ADAS/AD are limited, despite their extensive use in\ndomains like remote sensing. CNNs has demonstrated\nsuperior performance in extracting HSI spectral-spatial\nfeatures by utilizing 1D, 2D and 3D Convs. Others include\nRNNS, GANs, DBNs, GCNs, and Transformer-based ViT\nhave also demonstrated potential for HSI [17]. However, their\napplication to ADAS/AD-based HSI has yet to be explored.\nMost research in ADAS/AD perception has focused on\nRGB and multi-modal imaging. Benchmark datasets like\nKITTI [18] and Cityscapes [19] for RGB, while nuScenes\n[20], Waymo Open Dataset [21], KAIST Multi-Spectral [22]\nand FLIR ADAS Thermal [23] are widely considered\nstandards for multimodal based segmentation.\nHSI-based datasets are limited in number but are recently\ngaining attention. As shown in Table 1, the available fully\nannotated datasets such as HyKo v1-v2, HSI-Drive v1-v2,\nHS-City v1-v2, and HSI-Road. While these datasets provide\na foundation, their dataset size and diversity remain\ninsufficient for robust ADAS/AD applications.\nCurrently, there are no standardized baseline SSMs for\nHSI in ADAS/AD, as researchers are primarily evaluating\ntheir own datasets due to high-dimensional HSI data. This\nlack of standardized benchmarking affects the comparative\nanalysis in understanding the potential of HSI for ADAS/AD.\nOur work addresses this gap by establishing baseline SSMs\nbenchmarks across available datasets. This effort not only\nfacilitates comparative analysis but provides the foundation\nfor future works in HSI-based application for ADAS/AD."}, {"title": "3. METHODOLOGY AND EXPERIMENTATION", "content": "3.1. Datasets and Pre-Processing\nIn this work, we utilized the latest version of the four\npublicly available and annotated HSI datasets: HyKo v2,\nHSI-Drive v2, and HS-City v2 and HSI-Road. These datasets\nvary significantly in spatial-spectral resolution, and the\nnumber of labeled classes, as illustrated in Table 1.\nCurrent HSI datasets do not follow a standardized format,\nwith imbalanced classes, inconsistent annotation and varying\nspectral-spatial resolution. For instance, HSI-Road has only\ntwo classes (Road and Others) whereas HS-City v2 has 19\nclasses, making direct comparisons difficult. To address these\nissues, we redefined common class labels in the existing\nannotations of these datasets. These consolidated labels\ninclude Road, Vegetation, Sky, Metal (Cars, Traffic Sign,\nPoles, fence, etc.), Infrastructure (buildings, sidewalks, etc.),\nand People. While HSI-Road does not align with this\nstructure due to its two-class format, it was retained in the\nanalysis for comprehensive SSM evaluation. Moreover, Road\nMarkings and Glass labels were retained in HSI-Drive, due"}, {"title": "3.2. Experimentation Setup", "content": "3.2.1. Model Selection\nFour baseline SSM models were evaluated: U-Net,\nDeepLab v3+, PSPNet, and HRNet. These models were\nchosen based on their proven effectiveness and capacity to\nhandle the complexity of HSI data. In addition, two variants\nof U-Net were also included i.e. Coordinate Attention (UNet-\nCA), and Conv Block-Attention Module (UNet-\n\u0421\u0412\u0410\u041c). A\nBrief overview of these SSMs is as follows:\n\u2022\nDeepLab v3+ [11]: From the family of DeepLab, v3+\nleverages Atrous Conv and Spatial Pyramid based\npooling for multi-scale feature extraction and object\nlocalization.\n\u2022\nHRNet [12]: HRNet maintains high-resolution feature\nrepresentations throughout the segmentation process\nand preserves fine details.\n\u2022\nPSPNet [13]: PSPNet is based on Pyramid based\nPooling module, which facilitates the extraction of\nmulti-scale contextual information.\n\u2022\nU-Net [14]: U-Net is a standard encoder-decoder based\nCNN and is widely used in segmentation tasks for its\nability to preserve fine spatial information through skip\nconnections.\n\u2022\nUNet-CA [15]: A variant of U-Net, incorporates\ncoordinate attention mechanisms (AM) to enhance the\nmodel's focus on important spatial and spectral regions.\n\u2022\nUNet-CBAM [16]: Another U-Net variant, that\nintegrates CBAM to improve feature selection through\nchannel and spatial wise AM. This channel-wise feature\nextraction in CBAM is crucial for HSI to leverage the\nspectrally dense information."}, {"title": "3.2.2. Model Adaptation", "content": "To accommodate the varying input spatial and spectral\ndimensions of the HSI datasets, we modified the baseline\nSSM architectures. The modifications are as follows:\n\u2022\nInput and Output Layers Modification: The input\nlayer of models was adjusted to individual HSI dataset's\nspectral and spatial dimensions, and the output layers to\nthe required number of predicted classes.\n\u2022\nIntra-Modules Layers: The non-standardized spatial\ndimensions of HSI datasets required careful adjustment\nof intermediate layers. For instance, the U-Net model\nwas adjusted to handle the dynamic padding for\nadopting dimension mismatches between encoder and\ndecoder blocks. These modifications ensured model\nintegrity, and feature integration with smooth\ninformation flow."}, {"title": "3.2.3. Setup", "content": "Table 3 illustrates a comprehensive detail of the training\nsetup, input spatial dimension and hyperparameters of the\nmodel. All models were trained for 300 epochs using class\nweighted-based Cross Entropy loss function. Learning rate\n(lr) was scheduled using ReduceOnPlateau with restart to\n90% lr upon reaching minimum threshold. This approach\nhelped in escaping local minima and promote exploration of\nthe parameter space, thus improving convergence in\nchallenging hyperspectral optimization tasks.\nTo comprehensively evaluate and gain insights into model\nperformance, we employed Intersection over Union (IoU),\nPrecision (Prec), Recall (Rec), F1, Specificity (Spec), and\nAccuracy (Acc) metrics. These metrics were averaged i.e.\nmean (m) across all classes to provide a holistic models'\nassessment. Moreover, models were trained using mixed\nprecision, with no regularization or early stopping."}, {"title": "4. RESULTS AND DISCUSSION", "content": "As shown in Table 4, U-Net and its variants, especially\nUNet-CBAM, demonstrated the most promising results:\n\u2022\nUNet-CBAM consistently outperformed the other\nmodels, achieving mIoU and mF1 values of 65.31% and\n73.45% for HyKo2-VIS, 86.56% and 92.80% for HSI-\nDrive v2, 87.23% and 92.06% for HS-City v2, and\n96.56% and 98.26% for HSI-Road, respectively.\n\u2022\nThe integration of attention mechanisms (AM),\nparticularly the channel-wise extraction in CBAM,\nsignificantly enhanced feature extraction.\n\u2022\nCBAM's ability to capture both channel (spectral) and\nspatial correlations within HSI proved beneficial, as\nhowever, the performance of these models is constrained\nby inherent limitations of the HSI datasets, such as a limited\nnumber of images as shown in Table 2, and highly\nimbalanced classes with coarse labelling. As shown in Fig 2,\nthe example of HyKo2-VIS (mid column) lacks Road Mark\nannotation, and the models tries to segment it based on their\npreviously learned features. Similarly, in HSI-Drive v2 (left\ncolumn) the most complex region of the scene is not\nannotated. These limitations directly affect the robustness and\ngeneralizability of SSMs across diverse ADAS/AD\nconditions.\nThe results presented in this paper establish a standardized\nbaseline SSMs based benchmark, and the results of AM\nbased models provide potential for HSI-based segmentation\nin ADAS/AD applications. However, addressing the\nhighlighted limitation in HSI datasets in future works will be\nthe key for a more robust HSI solutions."}, {"title": "5. CONCLUSION", "content": "This paper presents a comprehensive evaluation of\nbaseline semantic segmentation models (SSM) for\nhyperspectral imaging (HSI) based datasets in the domain of\nAdvanced Driver Assistance Systems and Autonomous\nDriving (ADAS/AD) by assessing SSMs like U-Net,\nDeepLab v3+, PSPNet, HRNet and two U-Net variants\n(UNet-CA, UNet-CBAM). Among the evaluated SSMs,\nUNet-CBAM outperformed other models, providing the\npotential of channel-wise attention mechanisms (AM) for\neffectively leveraging spectral-dense HSI datasets. However,\nthe inherent limitations of available datasets, including small\ndataset sizes, non-standard spectral-spatial dimensions, high\nclass imbalance, and coarse annotations, remain significant\nchallenges. Future research should prioritize addressing these\nlimitations by developing larger, more diverse, and fine-\ngrained annotation based HSI datasets for developing more\nrobust and generalizable SSMs for ADAS/AD scenarios. The\nresults presented in this paper provide a foundation for the\nfurther development and evaluation of HSI-based\nsegmentation models in the ADAS/AD domain."}]}