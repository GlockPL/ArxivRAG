{"title": "Adapter-based Approaches to Knowledge-enhanced Language Models - A Survey", "authors": ["Alexander Fichtl", "Juraj Vladika", "Georg Groh"], "abstract": "Knowledge-enhanced language models (KELMs) have emerged as promising tools to bridge the gap between large-scale language models and domain-specific knowledge. KELMs can achieve higher factual accuracy and mitigate hallucinations by leveraging knowledge graphs (KGs). They are frequently combined with adapter modules to reduce the computational load and risk of catastrophic forgetting. In this paper, we conduct a systematic literature review (SLR) on adapter-based approaches to KELMs. We provide a structured overview of existing methodologies in the field through quantitative and qualitative analysis and explore the strengths and potential shortcomings of individual approaches. We show that general knowledge and domain-specific approaches have been frequently explored along with various adapter architectures and downstream tasks. We particularly focused on the popular biomedical domain, where we provided an insightful performance comparison of existing KELMs. We outline the main trends and propose promising future directions.", "sections": [{"title": "1 INTRODUCTION", "content": "The field of natural language processing (NLP) has, in recent years, been dominated by the rise of large language models (LLMs). These models are usually pre-trained on large amounts of unstructured textual data, which enables them to solve complex reasoning tasks and generate new text. Still, LLMs can lack awareness of structured knowledge hierarchies, such as relations between concepts and reasoning capabilities in knowledge-intensive tasks (Rosset et al., 2020; Hu et al., 2023). These drawbacks can lead to inaccurate predictions in downstream tasks and so-called \"hallucinations\" (Huang et al., 2023) within text generation, making LLMs less reliable in practice, an especially precarious issue in high-risk domains like healthcare or law.\n\nA potential solution to counteract hallucinations and improve the reliability of LLMs is knowledge enhancement: By leveraging expert knowledge from sources such as manually curated knowledge graphs (KGs), structured knowledge can be injected into LLMs. Such knowledge-enhanced language models (KELMs) are a promising approach for higher structured knowledge awareness, better factual accuracy, and less hallucinations (Colon-Hernandez et al., 2021; Wei et al., 2021; Hu et al., 2023). KGs are a vital part of knowledge engineering, a discipline that can be leveraged to make LLMs use advanced logic and formal expressions (Allen et al., 2023).\n\nUnfortunately, knowledge enhancement in the form of supervised fine-tuning (SFT) can be highly computationally expensive, especially for LLMs with billions of parameters. A promising research avenue to overcome this limitation is using lightweight and efficient adapter modules (Houlsby et al., 2019; Pfeiffer et al., 2020a). These modules can enhance the task performance of LLMs and, at the same time, be a very computationally efficient solution. Despite the rising popularity of this approach and to the best of our knowledge, a comprehensive overview of adapter-based KELMs is still missing in the NLP research landscape.\n\nTo bridge this research gap, we conduct a systematic literature review (SLR) on adapter-based knowledge enhancement of LLMs. Our contributions are (1) a novel review of adapter-based knowledge enhancement, (2) a quantitative and qualitative analysis of different methods in the field, and (3) a detailed categorization of literature and identification of the most promising trends. This work is a reworked and updated version of an SLR conducted as part of a mas-"}, {"title": "2 Background and Related Work", "content": "This section gives an overview of related work and existing surveys on knowledge enhancement. Knowledge graphs are the most common external knowledge source, so we start with their overview.\n\nKnowledge graphs (KGs) are a structured representation of world knowledge and have seen a rising prominence in NLP research over the past decade (Schneider et al., 2022). Hogan et al. (2020) define a KG as \"a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent relations between these entities\". Similarly, Ji et al. (2020) published a comprehensive survey on KGs and, following existing literature, defined the concept of a KG as \"$G = {E,R, F}$, where E, R and F are sets of entities, relations and facts, respectively; a fact is denoted as a triple $(h,r,t) \\in F$\". h,r,t denote the head, relation, and tail of a triplet. An Ontology is a formal representation of concepts and semantic relations between them \u2013 it provides a \"schema\" that a KG has to adhere to, making it possible to do reasoning and derive rules from KGs (Khadir et al., 2021).\n\nDepending on the source and purpose of a KG, entities and relations can take on various shapes. For example, in the biomedical knowledge graph UMLS (Bodenreider, 2004), a relation can take the shape of a single word like \"inhibits\", a short phrase like \"relates to\", or a compound term including, for example, chemical or medical categories such as \"[protein] relates to [disease]\" or \"[substance] induces [physiology]\". A textual connection is vital because it links the graph structure with natural language, simplifying the integration of information from KGs into language models and the associated learning processes. Other than UMLS, other examples of popular KGs are DBpedia (Auer et al., 2007) and ConceptNet (Speer et al., 2017).\n\nAt the time of writing, some reviews had already been published that gave an overview of KELMs and classified different approaches. Colon-Hernandez et al. (2021) review the existing literature and split the approaches to integrate structure knowledge with LMs into three categories: (1) input-centered strategies, centering around altering the structure of the input or selected data, which is fed into the base LLM; (2) architecture-focused approaches, which involve either adding additional layers that integrate knowledge with the contextual representations or modifying existing layers to alter parts like attention mechanisms; (3) output-focused approaches, which work by changing either the output structure or the losses used in the base model. Our study focuses on category (2) by examining the adapter-based mechanisms for injecting information into the model (see example in Figure 1), which were shown to be the most promising by the authors.\n\nThe second survey by Wei et al. (2021) reviews a large number of studies on KELMs and classifies them using three taxonomies: (1) knowledge sources, (2) knowledge granularity, and (3) application areas. Within (1), the knowledge sources include linguistic, encyclopedic, common-sense, and domain-specific knowledge. The second taxonomy (2) acknowledges the common approach of using KGs as a source of knowledge. Levels of granularity mentioned are text-based knowledge, entity knowledge, relation triples, and KG sub-graphs. Lastly, with the third taxonomy (3), the authors discuss how knowledge enhancement can improve natural language generation and understanding. They also review popular benchmarks that can be used for task evaluation of KELMS (Wei et al., 2021).\n\nAdapters are part of a broader paradigm of modular deep learning, described in detail by Pfeiffer et al. (2024). The two field studies by Colon-Hernandez et al. (2021) and Wei et al. (2021) on the classification of KELM approaches were a valuable starting point for exploring KELMs. Although they address some adapter-based studies like K-Adapter (Wang et al., 2020), most other adapter-based KELMs are missing. This lack of coverage led us to conduct a novel systematic literature search focusing specifically on the adapter-based KELMS, considering their rising popularity and importance."}, {"title": "3 Adapters", "content": "This section will give an overview of LLM adapters and their applications to establish a conceptual understanding of adapter-based enhancement.\n\nBroadly speaking, adapters are small bottleneck feed-"}, {"title": "3.1 Overview", "content": "Broadly speaking, adapters are small bottleneck feed-forward layers inserted within each layer of an LLM"}, {"title": "3.2 Adapter Types", "content": "Houlsby Adapter The Houlsby Adapter (Houlsby et al., 2019) was the first adapter to be used for transfer learning in NLP. The idea was based on adapter modules initially introduced by Rebuffi et al. (2017) in the computer vision domain. The two main principles stayed the same: Adapters require a relatively small number of parameters compared to the base model and a near-identity initialization. These principles ensure that the total model size grows relatively slowly when more transfer tasks are added, while a near-identity initialization is required for stable training of the adapted model (Houlsby et al., 2019). The optimal architecture of the Houlsby Adapter was determined by meticulous experimenting and tuning; the result can be seen in figure 2. In a classical transformer structure (Vaswani et al., 2017), the adapter module is added once after the multi-headed attention and once after the two feed-forward layers. The modules project the d-dimensional layer features of the base model into a smaller dimension, m, then apply a non-linearity (like ReLU) and project back to d dimensions. The configuration also hosts a skip-connection, and the output of each sub-layer is forwarded to a layer normalization (Ba et al., 2016). Including biases, 2md+d+m parameters are added per layer, accounting for only 0.5 to 8 percent of the parameters of the original BERT model used by the authors when setting m << d.\n\nBapna and Firat Adapter In contrast to the Houlsby Adapter, Bapna and Firat (2019) only introduce one adapter module in each transformer layer: they keep the adapters after the multi-headed attention (so-called \"top\" adapters) while dropping the adapters after the feed-forward layers (so-called \"bottom\" adapters) of the transformer (refer to Figure 2 for better understanding of the component positions). Moreover, while Houlsby et al. (2019) retrain layer normalization parameters for every domain, Bapna and Firat (2019) \"simplify this formulation by leaving the parameters frozen, and introducing new layer normalization parameters for every task, essentially mimicking the structure of the transformer"}, {"title": "Pfeiffer Adapter and AdapterFusion.", "content": "The approaches of Bapna and Firat (2019); Houlsby et al. (2019) did not allow information sharing between tasks. Pfeiffer et al. (2020a) introduce Adapter Fusion, a two-stage algorithm that addresses the sharing of information encapsulated in adapters trained on different tasks. In the first stage, they train the adapters in single-task or multi-task setups for N tasks similar to the Houlsby Adapter, but only keeping the top adapters, similar to the Bapna and Firat Adapter. As a second step, they combine the set of N adapters with AdapterFusion: They fix the parameters and all adapters $\\Phi$, and finally introduce parameters $\\Psi$ that learn to combine the N task adapters for the given target task (Pfeiffer et al., 2020a): $\\Psi_m \\leftarrow argmin_{\\Psi} L_m (D_m; \\Theta, \\Phi_1,..., \\Phi_N, \\Psi)$\n\nHere, $\\Psi_m$ are the learned AdapterFusion parameters for task m. In the process, the training dataset of m is used twice: once for training the adapters $\\Theta_m$ and again for training Fusion parameters $\\Psi_m$, which learn to compose the information stored in the N task adapters (Pfeiffer et al., 2020a). With their approach of separating knowledge extraction and knowledge composition, they further improve the ability of adapters to avoid catastrophic forgetting and interference between tasks and training instabilities. The authors also find that using only a single adapter after the feed-forward layer performs on par with the Houlsby adapter while requiring only half of the newly introduced adapters (Pfeiffer et al., 2020a). This fact makes the Pfeiffer adapter an attractive choice for many applications, further proven by its popularity among the papers in our review.\n\nWang et al. (2020) follow a substantially different approach where the adapters work as \"outside plug-ins\". In their work, an adapter model consists of K adapter layers (hence the name) that contain N transformer layers and two projection layers. Similar to the approaches above, a skip connection is added but instead applied across the two projection layers. The adapter layers are plugged in among varying transformer layers of the pre-trained model. The authors explain that they concatenate the output hidden feature of the transformer layer in the pre-trained model and the output feature of the former adapter layer as the input feature of the current adapter layer.\n\nAdapter architectures for knowledge enhancement exist that differ from the four adapter types mentioned here. For example, the \"Parallel Adapter\" (He et al., 2021a) or the adapter architecture by Stickland and Murray (2019)). However, as the upcoming comprehensive literature survey will show, these architectures are either unique to specific papers or have not found broader applications in the field of KELMs. Another popular type of efficient adaptation we'd like to mention for completeness is low-rank adaptation or LORA (Hu et al., 2022) and its quantized version QLORA (Dettmers et al., 2023). These approaches do not add new layers but rather enforce a low-rank constraint on the weight updates of the base model's layers. This methodology enables efficient fine-tuning of LLMs and also allows for domain adaption or knowledge enhancement with KGs (Tian et al., 2024)."}, {"title": "4 Methodology", "content": "This chapter details the methodology we employed for the systematic literature review. We largely followed the procedure of Kitchenham et al. (2009) for systematic literature reviews in software engineering. The search strategy for the systematic literature review of this study included literature that fulfilled the following inclusion criteria:\n\nPeer-reviewed articles from ACM\u00b9, ACL\u00b2, and IEEE Xplore\u00b3\nArticle abstracts that match the search string (\"adapter\" OR \"adapter-based\") AND (\"language model\" OR \"nlp\" OR \"natural language"}, {"title": "5 Results", "content": "This section will present the results of the SLR on adapter-based knowledge enhancement."}, {"title": "5.1 Overview", "content": "Table 1 shows the source distribution for all included papers. Fifty-nine papers were found by applying the search string as a query on the ACL, ACM, and IEEE search engines. Due to their importance for the field, we included three additional papers from other sources. These papers were found through online search and paper references during the general research process. In summary, after the abstract screening, 31 articles met all inclusion criteria (and no exclusion criteria). After the full paper screening, 26 papers formed the final paper pool of the survey. Table 2 gives an overview of all papers included in the survey. It includes information on the adapter type used in the paper, the domain and scope of the paper, and the downstream NLP tasks for which it was developed."}, {"title": "5.2 Data Analysis", "content": "We will now give a quantitative analysis showcasing and interpreting quantitative distributions, followed"}, {"title": "5.2.1 Quantitative Analysis", "content": "Yearly Distribution There has been a significant increase in publications on adapter-based approaches to knowledge-enhanced language models in recent years (Fig. 3). While only two papers were published in 2020, eleven new papers were published in 2023. This trend suggests growing interest and research activity in the domain."}, {"title": "Adapter Type Distribution", "content": "Next, we evaluate the popularity and variety of adapter types used across the papers (Fig. 4). The \"Pfeiffer\u201d and \"Houlsby\" adapter types stand out as the most common, which suggests that the closely related underlying architecture is the most popular methodology in the field. This popularity is likely not only an achievement of the adapter's performance but also due to the well-established Adapter-Hub platform (Pfeiffer et al., 2020b), which, although offering other options, uses adapters with the Pfeiffer configuration by default. This finding showcases a need and trend to build custom adapters well-suited to individual tasks. In the upcoming years, we will likely see many novel adapter architectures. The \"K-Adapter\u201d and \u201cBapna and Firat\" adapters are the less frequently mentioned architectures, suggesting that these approaches are less well-established."}, {"title": "5.2.2 Qualitative Analysis", "content": "This section of the analysis highlights recurring themes and individual insights from the papers. Fully summarizing all articles was outside the scope of this survey. However, we still provide an overview of the most common patterns.\n\nGeneral Knowledge The quantitative analysis showed that open-domain approaches are more popular than their close-domain counterparts. Subsequently, there is also a large variety in the used frameworks, knowledge sources, and overall goals. Two commonly used KGs for general knowledge are ConceptNet (Speer et al., 2017) for common-sense and DBpedia (Auer et al., 2007) for encyclopedic world knowledge. Two example works that use these KGs are Wold (2022) and the CKGA (\"knowledge graph-based adapter\") by Lu et al. (2023). Wold (2022) train adapter modules on sub-graphs of ConceptNet to inject factual knowledge into LLMs. They evaluate their framework on the Concept-Net Split of the"}, {"title": "Linguistic Knowledge", "content": "Instead of only including factual knowledge, some works inject additional linguistic knowledge into adapters (Majewska et al., 2021; Zou et al., 2022; Yu and Yang, 2023; Wang et al., 2020). While LLMs already encode a range of syntactic and semantic properties of language, Majewska et al. (2021) explain that LLMs \"are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic information\". Their paper explores the interplay between verb meaning and argument structure. They use knowledge to enhance LLMs with Pfeiffer Adapters to improve English event extraction and machine translation in other languages. Another example is the work of Zou et al. (2022) on machine reading comprehension (MRC). They proposed the K-MBAN model to integrate linguistic and factual external knowledge into LLMs through K-Adapters."}, {"title": "Domain-specific Knowledge", "content": "Chronopoulou et al. (2022) propose a parameter-efficient approach to domain adaptation using adapters. They represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. Their work focused on specializing adapters in website domains like booking.com and yelp.com. In another instance, Chronopoulou et al. (2023) propose \"AdapterSoup\". In this framework, they also use adapters for domain-specific tasks but use \"an approach that performs weight-space averaging of adapters trained on different domains\u201d. AdapterSoup can be helpful in various domain-specific approaches in low-resource settings, especially when only a small amount of data on a specific subdomain is obtainable and closely related adapters are available instead. Earlier, we saw that the biomedical domain is the most prevalent among the closed-domain approaches to adapter-based KELMs. We will briefly examine the relevant works in the following."}, {"title": "Biomedical Knowledge", "content": "We have found the works of DAKI (Lu et al., 2021), MoP (Meng et al., 2021), and KEBLM (Lai et al., 2023) to be the most impactful. According to the results of our literature survey, DAKI (\"Diverse Adapters for Knowledge Integration\") was the first work to use adapters specifically for knowledge enhancement in the biomedical domain. Lu et al. (2021) leverage data from the UMLS meta-thesaurus and UMLS Semantic Network groups concepts, but also from Wikipedia articles for diseases as proposed by He et al. (2020). Meng et al. (2021) recognize that KGs like UMLS, which can be several gigabytes large, are very expensive to train on in their entirety. They propose to use a \"Mixture of Partitions\" (MoP), which splits the KG into sub-graphs and combines later with AdapterFusion (Pfeiffer et al., 2020a). Finally, the KEBLM framework's trademark is that it allows the inclusion of a variety of knowledge types from multiple sources into biomedical LLMs. In contrast to DAKI, which uses more than one source, KEBLM includes a knowledge consolidation phase after the knowledge injection, where they teach the fusion layers to effectively combine knowledge from both the original PLM and newly acquired external knowledge by using an extensive collection of unannotated texts (Lai et al., 2023). For completeness, we refer to K\u00e6r J\u00f8rgensen et al. (2021) for information on the m-DAPT framework, which addresses multi-lingual domain adaptation for biomedical LLMs and KeBioSum (Xie et al., 2022), who state their work is the first study exploring knowledge injection for biomedical extractive summarization."}, {"title": "Performance Insights", "content": "In the papers covered by this survey, the performance of adapter-based KELMs on downstream tasks is consistently shown to be better than that of base LMs. For example, Diao et al. (2023) show an increase of +9% on Common-seCommon-sensettalmor-etal-2019-commonsenseqa with their mixture-of-adapters approach, while K\u00e6r J\u00f8rgensen et al. (2021) improve financial text classification on OMP-9 (Schabus et al., 2017) by +4%. While the task variation across domains is too diverse to be shown systematically in our survey, we report in detail on performance comparison in the biomedical domain in Table 3 in the next paragraph. Another interesting insight is found by He et al. (2021b), who show that adapter-based tuning mitigates forgetting issues better than regular fine-tuning since it yields representations with less deviation from those generated by the initial pre-trained language model."}, {"title": "Performance Comparison (Biomedical)", "content": "Table 3 gives an overview of the downstream task performance of several papers that are included in this survey. The focus lies on the biomedical domain, so the task overlap is high enough for an insightful comparison. The scores are reported for five downstream tasks, namely HoC (Baker et al., 2015), PubMedQA (Gu et al., 2020), BioASQ7b (Nentidis et al., 2020), MedNLI (Romanov and Shivade, 2018), and NCBI (Dogan et al., 2014), as well as three common biomedical language models (SciBERT (Beltagy et al., 2019), BioBERT (Lee et al., 2019), and PubMedBERT (Gu et al., 2020)). While adapter-based KELMs consistently improve performance in almost all instances, performance boosts across different tasks and models vary strongly. In this specific setting, we recommend the MoP (Meng et al., 2021) and KEBLM (Lai et al., 2023) frameworks since they show the highest performance boosts (e.g., PubMedQA (Jin et al., 2019) accuracy increase of around +7% and +8%, respectively) and overshadow the lower performing CPK and DAKI frameworks in all instances. MoP, in particular, is being continually used for biomedical knowledge enhancement, even in 2024 (Vladika. et al., 2024)."}, {"title": "6 Current and Future Trends", "content": "In this section, we outline the most important findings and trends and point out promising future directions:\n\nAdapter-based KELMs are a recent development in NLP, but interest in them is rising fast, with a linear yearly increase of published papers. We predict the growth trend to continue.\nVarious adapter architectures exist and are advanced by researchers to be more efficient while preserving task performance. This advancement has temporarily peaked with the Pfeiffer adapter, the most popular type. We expect future work to focus their updates on adapter architecture by overcoming the latency of sequential data processing in adapters and enabling hardware parallelism.\nResearch focuses on the open domain injecting general world knowledge into models. Within the closed domain, the biomedical domain is the most popular, owing to the existence of large biomedical KGs. We foresee the potential to apply adapter-based KELMs to other highly structured domains, such as the legal or financial domain (documents with rigid structure).\nThe largest improvements in task performance is seen in knowledge-intensive tasks like question answering and text classification, with more minor improvements for reasoning tasks like entailment recognition. Generative tasks, other than dialogue modeling, are rather unexplored. We envision a future popular use case that could use knowledge enhancement to improve the factuality and informativeness of generated text."}, {"title": "7 Conclusion", "content": "In this paper, we conducted a systematic literature review on approaches to enhancing language models with external knowledge using adapter modules. We portrayed which adapter-based approaches exist and how they compare to each other. We showed there is a steady growth of interest in this domain with each new year and highlighted the most popular adapter architectures (with \"Pfeiffer\" as the predominant one). We discovered a balance in popularity between open-domain approaches focusing on integrating general world knowledge into models and closed-domain approaches focusing on specialized fields, with biomedical as the most popular domain. With our review, we contribute a novel and extensive resource for this nascent yet fast-growing field, and we hope it will be a useful entry point for other researchers in the future."}, {"title": "Limitations", "content": "Our literature search methodology follows a strict search string and exclusion criteria. Subsequently, we might have overlooked some relevant work on adapter-based KELMs. Also, some reviewed papers were not adequately analyzed in this work due to space constraints, leading to potentially missing insights and a non-complete representation of the state of research on adapter-based enhancement. Additionally, due to the variety of applications and domains, we could not give precise guidelines on what methods to use under which circumstances. Still, we aimed to report on the most common patterns and trends discovered in the literature, which can serve as a basis for future research."}, {"title": "A Supplementary Survey Data", "content": "Articles on the following topics were excluded:\n\nArticles published before February 2, 2019\nDuplicate versions of the same article (when multiple versions of an article were found in different journals, only the most recent version was included)\nArticles where Adapters were used for NLP, but for use-cases other than knowledge-enhancement (such as few-shot learning or model debiasing)\nArticles written in a language other than English"}, {"title": "A.1 Methodology", "content": "Articles on the following topics were excluded:\n\nArticles published before February 2, 2019\nDuplicate versions of the same article (when multiple versions of an article were found in different journals, only the most recent version was included)\nArticles where Adapters were used for NLP, but for use-cases other than knowledge-enhancement (such as few-shot learning or model debiasing)\nArticles written in a language other than English"}, {"title": "A.2 Acronyms", "content": "AESRC2020: Accented English Speech Recognition Challenge 2020 (Shi et al., 2021)\nBLURB: Biomedical Language Understanding and Reasoning Benchmark (Gu et al., 2020)\nCCNet: Common Crawl Net (Wenzek et al., 2020)\nEE: Event Extraction\nEL: Entity Linking\nES: Extractive Summarization\nET: Entity Typing\nGLUE: General Language Understanding Evaluation (Wang et al., 2019)\nIE: Information Extraction\nKGD: Knowledge-grounded Dialogue\nLAMA: Concept-Net Split of LAMA Probe (Petroni et al., 2019)\nLM: Language Modeling\nMT: Machine Translation\nMultiWOZ: Multi-Domain Wizard-of-Oz dataset (Budzianowski et al., 2018)\nNER: Named Entity Recognition\nNLI: Natural Language Inference\nOOD: Out-of-domain Detection\nQA: Question Answering\nRC: Reading Comprehension\nRE: Relation Extraction\nRCL: Relation Classification\nSA: Sentiment Analysis\nSC: Sentiment Classification\nSF: Speech Foundation\nSL: Sequence Labelling\nSMATCH: Semantic Match Score (Cai and Knight, 2013)\nSR: Speech Recognition\nSTC: Sentence Classification\nTC: Text Classification\nTOD: Task-Oriented dialogue\nT-REX: A Large Scale Alignment of Natural Language with Knowledge Base Triples (Elsahar, 2017)\nVerbNet: A Broad-Coverage, Comprehensive Verb Lexicon (Schuler, 2006)\nVis-MDD: Visual Medical Disease Diagnosis (Tiwari et al., 2022)\nWMT20: Workshop on Machine Translation 2020\nWoW: Wizard-of-Wikipedia (Dinan et al., 2018)"}]}