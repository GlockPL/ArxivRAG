{"title": "Logic interpretations of ANN partition cells", "authors": ["Ingo Schmitt"], "abstract": "Consider a binary classification problem solved using a feed-forward artificial neural network (ANN). Let the ANN be composed of a ReLU layer and several linear layers (convolution, sum-pooling, or fully connected). We assume the network was trained with high accuracy. Despite numerous suggested approaches, interpreting an artificial neural network remains challenging for humans. For a new method of interpretation, we construct a bridge between a simple ANN and logic. As a result, we can analyze and manipulate the semantics of an ANN using the powerful tool set of logic. To achieve this, we decompose the input space of the ANN into several network partition cells. Each network partition cell represents a linear combination that maps input values to a classifying output value. For interpreting the linear map of a partition cell using logic expressions, we suggest minterm values as the input of a simple ANN. We derive logic expressions representing interaction patterns for separating objects classified as '1' from those classified as '0'. To facilitate an interpretation of logic expressions, we present them as binary logic trees.", "sections": [{"title": "Introduction", "content": "For relying on trained ANNs in critical applications, a human interpretation of their behavior is inevitable. An ANN is a numerical approach where unlike decision trees no navigation by Boolean decisions on single input attributes exists. Instead, complex mappings hide internal decision logic, resulting in a lack of trust (Miller, 2019; Kaur et al., 2022). Therefore, an ANN is often called a black-box solution. A task related to interpretation is to explain the classification result for a given input object.\nFor example, the Shapley-value (Shapley et al., 1953) and its adaptation SHAP (Lundberg et al., 2020) compute the importance of single input attributes as part of an additive measure. The SHAP approach generalizes the local interpretable model-agnostic explanations (LIME) (Ribeiro et al., 2016). Another proposal is to extract ridge and shape functions in the context of an extracted generalized additive model to illustrate the effect of attributes. Approaches to measuring the importance of an input attribute can be generalized to measure the strength of interactions among several attributes, see for example (Murofushi and Soneda, 1993).\nMany researchers, see (Kraus et al., 2023; Molnar, 2020) for an overview, strive to make the black-box behavior more transparent and more comprehensible.\nNote that decision tree classifiers and their derivatives (Suthaharan and Suthaharan, 2016) are based on Boolean logic decisions. Logic expressions are seen as much easier to interpret than numerical classifiers like ANNs. The work from (Balkenius and G\u00e4rdenfors, 1991; G\u00e4rdenfors, 1992; Blutner, 2004) is an early attempt to bridge neural networks (subsymbolic paradigm) with logic (symbolic paradigm). It defines schemata representing states of a neural network and fuzzy-logic-like operations (conjunction and disjunction). The operations, however, do not obey the laws of the Boolean algebra. A schema induces a partition of the input space that is essentially different from the partition proposed in our approach.\nThe work (beim Graben, 2004) discusses the concepts of 'incompatibility' and 'implementation' in the context of bridging the symbolic and the subsymbolic paradigms of information processing and relates both paradigms to the concepts of quantum mechanics.\nAn interesting approach (Balestriero, 2017; Yang et al., 2018) to bridge logic in the form of decision trees with a deep neural network is to simulate a decision tree using a neural network. After learning and combining the binnings of attribute values, linear networks are trained for every binning combination in order to learn the classification. In contrast, in our work, the starting point is a given trained simple"}, {"title": "Proposed Method in a Nutshell", "content": "For demonstrating a small ANN, Figure 1 visualizes a very small example network with two ReLU nodes\n$r := \\binom{r_1}{r_2} = \\binom{ReLU(n_1)}{ReLU(n_2)}$ with\n$ReLU(z) =\\begin{cases}z & \\text{if } z \\geq 0\\\\0 & \\text{otherwise}\\end{cases}$\nand four input nodes. The trained network is represented by two matrices of weights\n$w^I = \\binom{w_1 w_2 w_3 w_4}{w_5 w_6 w_7 w_8}$ and $w^O = (w_1^O w_2^O)$\nfor the input $n = w^I I$ and the output $o = w^O r$, respectively. The class decision for an input object $I= (i_1 i_2 i_3 i_4)$ is given by\n$\\hat{y} := \\theta_{\\tau}(o) = \\theta_{\\tau}(ANN(I)) = thr(w^O ReLU(w^I I)).$\nANN Partition\nThe first step is to partition the input space $\\mathbb{R}^n$ of an ANN into partition cells. The key elements of the partition are the ReLU-nodes, which represent hyperplane separations of the input space. Let's demonstrate the partition with our example of a trained, simple ANN. For any input object $I=(i_1 i_2 i_3 i_4)$, each ReLU-node $r_i$ is either inactive ($r_i = 0$) or active ($r_i > 0$), represented here by a binary code where 0 denotes inactive and 1 denotes active. With our two ReLU-nodes, we obtain one of four different cases 00,01,10,11 for every $I$, representing four partition cells. Every inactive ReLU-node deactivates the mapping of its subtree below. On the other hand, every active ReLU-node represents the identity function. As a result, we obtain a linear mapping for every partition cell $p$:\n1. $p_{00}: o_{00} = 0 \\cdot i_1 + 0 \\cdot i_2 + 0 \\cdot i_3 + 0 \\cdot i_4$ for partition condition: $n_1 \\leq 0 \\wedge n_2 \\leq 0$\n2. $p_{01}: o_{01} = (w_1^O w_1) \\cdot i_1 + (w_2^O w_2) \\cdot i_2 + (w_3^O w_3) \\cdot i_3 + (w_4^O w_4) \\cdot i_4$ for partition condition: $n_1 \\leq 0 \\wedge n_2 > 0$\n3. $p_{10}: o_{10} = (w_1^O w_1) \\cdot i_1 + (w_2^O w_2) \\cdot i_2 + (w_3^O w_3) \\cdot i_3 + (w_4^O w_4) \\cdot i_4$ for partition condition: $n_1 > 0 \\wedge n_2 \\leq 0$\n4. $p_{11}: o_{11} = o_{10} + o_{01}$ for partition condition: $n_1 > 0 \\wedge n_2 > 0$\nPlease note that a resulting linear combination is not an attribute-wisely additive measure. That is, negative weights may occur."}, {"title": "Logical Interpretation of an ANN Partition Cell", "content": "The second step of our approach is the logical interpretation of a partition cell. This requires interpreting the input value $x_i[j] \\in \\mathbb{R}$ of the j-th attribute of input object $i$ as the degree of fulfillment of an object property (Schmitt, 2006; Schmitt, 2008; Schmitt and Baier, 2013), akin to the membership value in fuzzy set theory (Zadeh, 1965). For logical interpretation, a map of all attribute values to the unit interval [0, 1] is required. Thus, we employ a monotonic increasing function $m_j()$, where $m_j(x_i[j]) \\in [0,1]$ for attribute $j = 1,...,n$ and all objects $i$. The function $m_j()$, is application dependent and can be seen as a fuzzification known from fuzzy set theory.\nFurthermore, we require $2^n$ minterm values $mt_i[k]$ instead of $n$ values $m_j(x_i[j])$ as input for the network. Minterm values are conjunctions of negated or non-negated attribute values $m_j(x_i[j])$, expressing interactions among attributes. In accordance with (Schmitt, 2022b; Schmitt, 2022a), we define\n$mt_i[k] := \\prod_{j=1}^n (1 - b_j) \\cdot m_j(x_i[j])^{1-b_j} \\cdot m_j(x_i[j])$                                                            (1)\nwhere $k = 0,..., 2^n - 1$ is the minterm identifier and $b = b_1 ... b_n$ is the bit code of $k$. We denote minterm-based training data as $TR_{mt} := (mt_i, y_i)$ and assume an ANN to be trained with $TR_{mt}$ and a loss function, e.g. MSE, with high accuracy.\nFor our small example and for an object $i$, let $a$ represent the value of attribute $A$ and $b$ represent the value of attribute $B$, that is, $a := m_1(x_i[1])$ and $b := m_2(x_i[2])$, indicating the degree of fulfillment that the value is high. We construct the input $I$ using minterm values as:\n$I = (i_1 i_2 i_3 i_4) := ([a b] [a \\overline{b}] [\\overline{a} b] [\\overline{a} \\overline{b}])^T$\n$= (mt_i[0] mt_i[1] mt_i[2] mt_i[3])^T$.\nA conjunction corresponds to multiplication, and a negation corresponds to subtraction from 1, see Equation 1. For example, $i_2$ corresponds to $\\overline{a}b = \\overline{a} \\wedge b$ and is evaluated as $(1 - a) \\cdot b$.\nEach ANN partition cell determines a linear combination of weights $m_w[k]$ on minterm values $mt_i[k]$. We will interpret $m_w$ as logic expressions. The integer $k=0,..., 2^n - 1$ identifies a minterm and its bits $b$ refer to attributes being negated (0) or non-negated (1). A logic expression can be viewed as a pattern based on logic (interactions between attributes) for identifying 1-objects and distinguishing them from 0-objects.\nFor example, let the linear combination of an ANN partition cell be $o = 0.9 \\cdot i_1 + 0.4 \\cdot i_2 + 0.7 \\cdot i_3 + 0.8 \\cdot i_4$, see Table 1.\nOur core idea is to scale all minterm weights to the range [0,1] and then to approximate every minterm weight as a bit code $2^0 2^{-1} 2^{-2} ...$ of a fixed number of digits, see Table 1. Every bit code level (column) induces a logic expression as a disjunction of minterms, see Table 2. The logic expression at the $2^{-1}$ level has double the factor value compared to the next level: $2^{-1} = 2 \\cdot 2^{-(1+1)}$. The linear combination over the minterm values is approximated by the sum of the arithmetic evaluations over all derived logical expressions.\nFrom the logic expressions on different bit code levels, we can readily derive a trend analysis (see Figure 3) in order to investigate the interactions between $a$ and $b$ and see their effects on the output value $o$.\nAfter having given a brief overview of our method, we will discuss it with more details."}, {"title": "ANN Partition Cells", "content": "Let's assume the trained network ANN has $l$ ReLU-nodes. We assign to every input object $mt_i$ the ReLU-status bit vector $relu_i \\in \\{0,1\\}^l$, where $relu_i[m]$ represents the status of the m-th ReLU-node for $m = 1,...,l$. A value of 1 (active node) for a ReLU-node indicates that its input value is non-negative, while 0 (inactive node) indicates a negative value. We interpret $relu_i$ as a bit code of an integer value $p_i \\in \\mathbb{0, ..., 2^l - 1}$:\n$p_i := \\sum_{m=1}^l relu[m] \\cdot 2^{l-m}.$\nWe refer to the value $p_i$ of the i-th input object as its partition cell number. Using equal partition cell num-"}, {"title": "Logic Expressions", "content": "The evaluation of a quantum-logic-inspired, binary CQQL classifier (Schmitt, 2022a) for a given object $o_i$ is defined as:\n$cl(o_i) := thr([e]^{o_i}) \\in \\{0,1\\},$\nwhere $e$ is a logic expression over $n$ input attributes $a_j$ and $[e]^{o_i} \\in [0, 1]$ is its arithmetic evaluation on the attribute values $m_j(o_i[j]) \\in [0,1]$. See for example following logic expression and its evaluation:\n$[a_1 \\wedge \\neg a_2]^{o_i} := m_1(o_i[1]) \\cdot (1 - m_2(o_i[2])).$\nA CQQL expression and its evaluation obey the laws of Boolean algebra, although the attribute values are no Boolean values.\nFrom Boolean algebra we know that every logic expression $e$ can be expressed in complete disjunctive normal form, meaning every $e$ can be identified by a subset of the set of all $2^n$ complete minterms. We denote the minterms being elements of the subset active with status value 1 and those not being elements of the subset inactive with status value 0. As result, every logic expression $e$ is identified by a bit vector $mw_e \\in \\{0,1\\}^{2^n}$ over all $2^n$ minterm status values. In terms of probability theory, all $2^n$ minterms are regarded as the total set of mutually exclusive complex events and the scaled attribute values as probabilities of probabilistically independent atomic events. To evaluate a logic expression $e$, along with the minterm values $mt_i$ of an input object $o_i$, we define the arithmetic evaluation as:\n$[e]^{o_i} := \\sum_{k=0}^{2^n-1} mw^e[k] \\cdot mt_i[k] = \\sum_{k:mw^e [k]=1} mt_i[k].                                  (3)"}, {"title": "Mapping Real-valued Minterm Weights to Binary Minterm Weights", "content": "Equation 2 corresponds to the scalar product between $mt_i$ and $m_w^p$. From linear algebra, we know that the scalar product is linear in both of its arguments. As a next step, we scale all minterm weights of all partition cells $m_w^p[k] \\in \\mathbb{R}$ with $p = 2^m$ for some integer $m$ to values from [0, 1] using a linear function. We select the linear function $f$ that is strictly increasing and maps the largest weight $max := max_{k,p=2^m} m_w^p[k]$ to 1 and the smallest weight $min := min_{k,p=2^m} m_w^p[k]$ to 0:\n$m_w^p[k]' := f(m_w^p[k]) \\in [0,1]$ where\n$f(x) := \\frac{x - min}{max - min}.$\nThis leads to the following equivalence:\n$ann'_p(mt_i) := \\sum_{k=0}^{2^n-1} f(m_w^p[k]) \\cdot mt_i[k] = f(ann_p(mt_i)).$\nFurthermore, since $\\sum_k mt_i[k] = 1$ and $0 \\leq mt_i[k] \\leq 1$, we obtain:\n$ann'_p(mt_i) \\in [0, 1].$\nSince $f$ is linear and strictly increasing the order relation of object evaluations is preserved for all $i_1, i_2, \\tau$ and $\\tau' := f(\\tau)$:\n$ann_p(mt_{i_1}) < \\tau < ann_p(mt_{i_2}) \\rightarrow$\n$ann'_p(mt_{i_1}) < \\tau' < ann'_p(mt_{i_2}).$\nAs result, scaling minterm weights to the unit interval does not modify the semantics of a classifier. In the following we assume normalized minterm weights and the new threshold $\\tau'$.\nAs the next step, we approximate every minterm weight $m_w^p[k] \\in [0,1]$ of a partition cell $p = 2^m$ by bit-values of different bit code levels $2^{-bcl}$:\n$m_w^p[k] = \\sum_{bcl=0}^{bcl_{max}} b[bcl,k, p] \\cdot 2^{-bcl}$\nwhere $b[bcl,k, p] \\in \\{0,1\\}$ is a three-dimensional bit tensor over bit code levels $bcl$, minterms $k$, and partition cells $p$. To minimize the absolute error between $m_w^p[k]$ and its bit code, the value $m_w^p[k]$ should be suitably rounded before deriving the bits.\nThe average absolute error of the bit-approximation of one minterm weight is not higher than $2^{-(bcl_{max}+1)}$. Since $\\sum_k mt_i[k] = 1$, the same average absolute error holds also between $[e]^{o_i}$ and the summarized bitwise $bcl$-evaluation of $e$ for an object $o_i$.\nThe constant $bcl_{max}$ defines how close the minterm weights are approximated and is chosen as a trade-off between too many bits resulting in too many logic expressions and too high approximation error. As the average absolute error vanishes exponentially, the value of $bcl_{max}$ can be small.\nFor every bit code level $bcl$ and partition cell $ANN_p$ we define the bit code vector $mw_{p,bcl}^e \\in \\{0,1\\}^{2^n}$ as\n$mw_{p,bcl}^e[k] := b[bcl,k,p]$\nand obtain in accordance to Equation 3 a logic expression $e_{p,bcl}$. Thus, we approximate $ANN_p(mt_i)$ as\n$ANN_p (mt_i) \\approx \\sum_{bcl=0}^{bcl_{max}} 2^{-bcl} \\cdot [e_{p,bcl}]^{o_i}$\n$[e_{p,bcl}]^{o_i} = \\sum_{k:mw_{p,bcl}^e[k]=1} mt_i[k].$"}, {"title": "Interpretation of a Logic Expression ep,bcl", "content": "Of course, a bit vector $mw_{p,bel}^e \\in \\{0,1\\}^{2^n}$ is not an intuitive representation of a logic expression $e_{p,bcl}$. It simply tells us for every minterm whether it is active or not. Therefore, we construct a quantum-logic-inspired decision tree (QLDT (Schmitt, 2022b)) from $mw_{p,bcl}^e$ and present it to the user for interpretation. The QLDT is just an alternative representation of $mw_{p,bcl}^e$. The main construction idea is to use the pairs $\\{(bit\\_code(k), mw_{p,bcl}^e[k])|k=0...2^n - 1\\}$ over the minterms $k$ as training data for learning a classical decision tree. The bit code levels of $k$ represent the original attributes being negated or non-negated. The attributes near the root of the resulting tree are more effective than those further away for distinguishing"}, {"title": "Experimental Study", "content": "We apply our approach to the binary classification problem of the banknote authentication dataset. This dataset contains wavelet-processed image data, which are used to determine the authenticity of banknotes. The training data include values for the four attributes: variance (v), skewness (s), curtosis (c), and entropy (e), as well as target information indicating whether the banknote is authentic or not. The dataset contains 1220 objects with known target values. We trained an ANN consisting of 24 input nodes representing minterms, three hidden ReLU-nodes $r_i$, and one output node with a carefully chosen threshold. As a result, the trained ANN demonstrates an accuracy of 100%.\nChecking all training data against ReLU-conditions yields four non-empty network partition cells, as shown in Table 3. Column 'B' represents the number of authentic banknotes, and 'B' represents the number of non-authentic banknotes contained in the respective partition cell. By examining the effect of the ReLU-nodes $r_i$ on authentication, we observe that $r_2$ separates the authentic banknotes from the non-authentic ones at best. Therefore, we focus exclusively on partition cell $ANN_2$ since it refers exclusively to $r_2$.\nTable 4 shows the minterm weights of partition cell $ANN_2$ after [0,1]-scaling and their approximated bit codes. Each bit code level (column) represents a set of active minterms, which is regarded as a logical expression. The weight sum 9.46 represents the total energy of the given minterm weights and is used to calculate the relative energy ($E$ in percent) of the column-wise logical expressions. For example, the relative energy of $bcl = 1$ with 11 set bits is approximately 58%, calculated as $2^{-1} \\cdot 11/9.46 \\cdot 100\\%$. The last column shows the approximated minterm weights as the sum of bits. The approximation error between the original minterm weights and their approximated bit code level representations is small.\nNext, we determine how many bit code levels we"}, {"title": "Conclusion", "content": "We addressed the problem of interpreting simple ANNs using logic expressions by constructing a bridge between them. Logic expressions are visualized as binary logic trees (QLDT).\nReLU-nodes of an ANN introduce non-linearity to the network, which is necessary for solving non-linear classification tasks. Our approach is to partition a trained simple ANN by use of ReLU-conditions in order to obtain linear maps for every partition cell. The minterm weights of a linear map are interpreted as bit numbers. Every bit code level of a partition cell defines a set of minterms and corresponds to a logic expression.\nThe main benefit of our approach is to interpret an ANN by means of logic. Logic provides a powerful toolset for analyzing how attributes interact with each other.\nOur approach relies on using minterm values as network input. Since there are $2^n$ minterms for $n$ object attributes, the number $n$ is restricted to be small. In future work, we will strive to adapt this approach to more complex neural networks."}]}