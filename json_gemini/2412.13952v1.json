{"title": "Prompting Strategies for Enabling Large Language Models\nto Infer Causation from Correlation", "authors": ["Eleni Sgouritsa", "Virginia Aglietti", "Yee Whye Teh", "Arnaud Doucet", "Arthur Gretton", "Silvia Chiappa"], "abstract": "The reasoning abilities of Large Language Models\n(LLMs) are attracting increasing attention. In this\nwork, we focus on causal reasoning and address\nthe task of establishing causal relationships based\non correlation information, a highly challenging\nproblem on which several LLMs have shown\npoor performance. We introduce a prompting\nstrategy for this problem that breaks the original\ntask into fixed subquestions, with each subques-\ntion corresponding to one step of a formal causal\ndiscovery algorithm, the PC algorithm. The pro-\nposed prompting strategy, PC-SUBQ, guides the\nLLM to follow these algorithmic steps, by se-\nquentially prompting it with one subquestion at a\ntime, augmenting the next subquestion's prompt\nwith the answer to the previous one(s). We eval-\nuate our approach on an existing causal bench-\nmark, CORR2CAUSE: our experiments indicate\na performance improvement across five LLMs\nwhen comparing PC-SUBQ to baseline prompt-\ning strategies. Results are robust to causal query\nperturbations, when modifying the variable names\nor paraphrasing the expressions.", "sections": [{"title": "1. Introduction", "content": "Advances in scaling Large Language Models (LLMs) and\ntraining data sizes have led to unprecedented capabilities\nacross a wide range of tasks. Several works (Huang &\nChang, 2023; Sun et al., 2023) focus on enhancing LLMs\non various reasoning tasks, such as arithmetic, common-\nsense, logical or causal reasoning. It is often argued that the\nability to demonstrate reasoning cannot be solely overcome\nby increasing model scale (Rae et al., 2021, inter alia). An\nactive area of research is therefore investigating prompt-\ning approaches in order to elicit and enhance reasoning in\nLLMs (Qiao et al., 2023). Wei et al. (2022) proposed Chain-\nof-Thought (COT) prompting, where a language model is\nprompted to generate a series of thoughts that mimic the\nreasoning process a person might employ when solving a\ntask. For settings in which the problem to solve is harder\nthan the demonstration examples, various works suggest to\ndecompose the original complex task into simpler subtasks\nthat are easier to solve (Zhou et al., 2023a; Khot et al., 2023;\nDua et al., 2022).\nAmong the different reasoning capabilities, the ability to\nreason about cause and effect is fundamental to human in-\ntelligence, playing a pivotal role in scientific understanding\nand accurate decision-making. Most existing research on\ncausal reasoning and LLMs investigates whether they can\nserve as a knowledge base for causal relationships (K\u0131c\u0131man\net al., 2023; Willig et al., 2022; Ze\u010devi\u0107 et al., 2023; Long\net al., 2022), thus assessing the commonsense reasoning\ncapabilities of LLMs. More recent work (Jin et al., 2023;\n2024) focuses, instead, on their formal (pure) causal reason-\ning skills, i.e., their ability to reason using known procedures\nand formal rules from the causal inference literature (Pearl,\n2000; Pearl & Mackenzie, 2018; Spirtes et al., 2000). One\nsuch ability, studied in Jin et al. (2024), is that of inferring\ncausal relationships from correlation statements. In this\ncase, correlations or conditional independencies about a set\nof variables are given in natural language, for instance, \u201cIce\ncream sales correlate with shark attacks", "Ice cream sales cause shark\nattacks\". This is an important skill for LLMs as the training\ncorpus may not directly contain certain causal relationships\nbetween variables, but instead events' co-occurrence or cor-\nrelation statements. To this end, Jin et al. (2024) formulated\nCORR2CAUSE, a causal benchmark that assesses this ability,\non which they showed LLMs to perform poorly.\nExpanding the task of inferring causal relationships from\ncorrelation statements, one could more generally infer the\ncausal structure from correlation statements, that is, identify\nthe entire structure instead of a specific causal relationship\nbetween a pair of variables. We refer to this task as Natural\nLanguage Causal Discovery (NL-CD) to contrast it with\nstandard/statistical Causal Discovery (S-CD) considered\nin the statistical causality literature (Pearl, 2000; Pearl &\nMackenzie, 2018; Spirtes et al., 2000), that identifies the\ncausal graph based on observed data. NL-CD, instead, is an\"\n    },\n    {\n      \"title\": \"2. Related Work\",\n      \"content\": \"LLM task that is based on natural language correlation state-\nments. Even though there exist known statistical algorithms\nfor certain causal tasks, our goal is to instead investigate the\nability of LLMs to solve such tasks. This motivation is also\nshared in Jin et al. (2024) and Jin et al. (2023) for causal\ntasks, in Markeeva et al. (2024) and Veli\u010dkovi\u0107 et al. (2022)\nfor algorithmic tasks and in other works for arithmetic\ntasks. In this work, we address NL-CD by introducing\na prompting strategy, PC-SUBQ. Inspired by prompting ap-\nproaches that decompose a complex task into simpler ones,\nPC-SUBQ leverages one of the most widely used algorithms\nfor S-CD, the PC algorithm (Spirtes et al., 2000), in order\nto decompose the NL-CD task. In particular, PC-SUBQ de-\ncomposes the original task into several fixed subtasks, each\ncorresponding to one step of the PC algorithm. We guide the\nLLM to follow these PC steps by having few-shot examples\nfor each subtask and sequentially prompting the LLM, aug-\nmenting the next subtask with the answer(s) to the previous\none(s). Given the final inferred graph, one can subsequently\npose various causal queries, such as the ones included in\nCORR2CAUSE. We assess PC-SUBQ on CORR2CAUSE\nand consistently observe improved performance compared\nto existing prompting strategies across five LLMs. More-\nover, unlike models fine-tuned on CORR2CAUSE (Jin et al.,\n2024), LLMs' performance using PC-SUBQ is robust to\nquery perturbations: even though the few-shot examples are\nbased on CORR2CAUSE textual expressions, performance\ndoes not drop when variable names are modified or expres-\nsions are paraphrased. In summary, our main contributions\nare:\n\u2022 We propose a prompting strategy that addresses NL-\nCD by guiding the LLM to reason through applying\nthe PC algorithm steps, thus also offering transparent\nand interpretable reasoning steps.\n\u2022 We evaluate PC-SUBQ on CORR2CAUSE and show\nthat it outperforms a range of widely used prompting\nstrategies across five LLMs.\n\u2022 We show that PC-SUBQ is robust to causal query per-\nturbations in terms of variable renaming and paraphras-\ning, and can be applied unchanged to examples with\nnatural stories.\nCausal reasoning and LLMs There is an increasing at-\ntention on the strengths and limitations of LLMs on causal\ntasks (K\u0131c\u0131man et al., 2023; Willig et al., 2022; Ze\u010devi\u0107\net al., 2023; Zhang et al., 2023). A large body of existing lit-\nerature assesses how well LLMs capture commonsense and\ndomain knowledge about causal relations (K\u0131c\u0131man et al.,\n2023; Long et al., 2022; Zhang et al., 2023; Tu et al., 2023;\nVashishtha et al., 2023; Jiralerspong et al., 2024). Con-\nsider, for example, the BIG-bench Cause and Effect task\nof deciding which of the two events, \\\"The driver turned\nthe wipers on\\\" and \\\"It started raining\\\", caused the other\n(Authors, 2023). This question could be answered by us-\ning commonsense knowledge about the purpose of wipers.\nCommonsense knowledge captured by LLMs can be used\nalongside existing causal methods as a proxy for human\ndomain knowledge (K\u0131c\u0131man et al., 2023; Long et al., 2022;\n2023). Indeed, recent studies suggest to improve S-CD\nmethods either by incorporating LLMs' imperfect knowl-\nedge about causal relationships (Long et al., 2023; Abdulaal\net al., 2024) or through providing an LLM-based oracle\nof conditional independence relations as input to the PC\nalgorithm (Cohrs et al., 2024). On a more critical perspec-\ntive, Willig et al. (2022) and Ze\u010devi\u0107 et al. (2023) argue\nthat LLMs are just \\\"causal parrots\\\" which recite the causal\nknowledge embedded in the training data, and so we cannot\nexpect any sort of generalisation.\nAnother way of inferring causal relationships is through for-\nmal causal reasoning, that is, by employing known rules and\nalgorithms from the causal inference literature (Pearl, 2000;\nSpirtes et al., 2000) to reach a causal conclusion. In this\ncase, the inference rules hold independently of the specific\ninstantiation of the variables, as opposed to commonsense\nreasoning which highly relies on the semantics of the vari-\nable names. Jin et al. (2024) and Jin et al. (2023) analyse the\ndifference between these two reasoning abilities, i.e., com-\nmonsense and formal causal reasoning, and introduce bench-\nmarks, CORR2CAUSE and CLADDER, respectively, to test\nthe formal causal reasoning abilities of LLMs. Specifically,\nCORR2CAUSE consists of queries about causal relationships\ngiven correlation statements, while CLADDER asks causal\ninference queries across all rungs of the Ladder of Causation\n(Pearl & Mackenzie, 2018) for a given underlying causal\ngraph. Both works observe that it is highly challenging for a\nrange of LLMs to answer these causal queries. In particular,\nJin et al. (2024) mainly evaluate off-the-shelf or fine-tuned\nLLMs on CORR2CAUSE and find them to perform poorly\nor not to be robust to causal query perturbations. We instead\npropose a multi-step prompting strategy (PC-SUBQ) that is\nrobust to causal query perturbations, produces interpretable\nreasoning steps and does not require LLM retraining.\nPrompting strategies Since the popularization of few-\nshot prompting (Brown et al., 2020), there has been a lot\nof follow-up work on prompting strategies. In Chain-of-\nThought (COT) prompting (Wei et al., 2022), a few chain\nof thought demonstrations are provided as examples in the\nprompt. This significantly improves the ability of LLMs\nto perform complex reasoning on a range of tasks, with\nexamples of surpassing even fine-tuned models. To address\nmore complicated tasks, several follow-up works suggest\nto decompose the original complex task into simpler sub-\ntasks that are easier to solve (Zhou et al., 2023a; Khot et al.,\n2023; Dua et al., 2022). Least-to-most prompting (Zhou\net al., 2023a) solves these simpler problems in sequence,\"\n    },\n    {\n      \"title\": \"3. Preliminaries\",\n      \"content\": \"with each subproblem being facilitated by previously solved\nones. Unlike least-to-most prompting that infers the sub-\nquestions using an appropriate prompt, we instead design\na fixed set of subquestions that is used unchanged across\nthe whole dataset. For that, we take advantage of the fact\nthat there is a common underlying (causal) algorithm that\ncan be followed to solve all causal queries, so we design\nthe subproblems accordingly to correspond to the PC steps.\nJin et al. (2023) use a prompting strategy comprising of a\nchain of subquestions without demonstrations, in order to\nanswer various causal query types based on a given causal\ngraph. Unlike the above works, we do not carry over to\nthe next subquestion all history of previous ones, but prop-\nagate much less information (usually just the final answer\nto the previous subquestion) in order to save context length\nand guide the LLM to focus on the most relevant parts of\ninformation. Finally, our work connects to literature on\nalgorithmic reasoning (Zhou et al., 2023b; Sel et al., 2023;\nNye et al., 2021). Zhou et al. (2023b) use an algorithmic\nprompt to unlock algorithmic reasoning abilities in LLMs.\nGraph definitions A graph G := <X, E) consists of a set\nof nodes X and a set of edges E connecting the nodes. An\nundirected graph consists of undirected edges denoted as\nX - Y, while a directed graph has directed edges denoted\nas X \u2192 Y. A path from X to Y is a sequence of dis-\ntinct nodes (X, . . ., Y) in which there is an edge between\nevery pair of successive nodes. A directed path is a path\nwhose edges are directed and pointing from preceding to-\nwards following nodes. A Directed Acyclic Graph (DAG)\nis a directed graph with no cycles, i.e. no directed paths\nstarting and ending at the same node. The skeleton of a\nDAG is the undirected graph resulting from ignoring all\narrowheads. When X \u2192 Y, X is called the parent of Y\nand, consequently, Y the child of X. A collider is a node\nwhere at least two arrowheads meet, for example, in the\npath (X, Z, Y), Z is a collider if X \u2192 Z and Y \u2192 Z. A\nv-structure consists of two edges whose arrows point to the\nsame node and whose tails are not connected by an edge: for\nexample, the path (X, Z, Y) forms a v-structure if X \u2192 Z,\nY\u2192 Z and X and Y are not connected. A path between\ntwo nodes is said to be unblocked conditioned on a set of\nnodes Z, if for every collider W in the path, either W or a\ndescendant of W is in Z and no non-collider in the path is\nin Z. A blocked path is a path that is not unblocked. Two\ndisjoint sets of nodes A and B are said to be d-separated\ngiven a set of node Z if every path between any node in A\nand any node in B is blocked conditioned on Z.\nMarkov condition Consider the nodes in a DAG G rep-\nresenting random variables with P(X) denoting their joint\ndistribution.\u00b9 The joint distribution is Markov to G if the fol-\nlowing condition, called the Markov condition, holds for all\nA, B, Z: A, B d-separated given Z in G \u21d2 A \u2212 B | Z.\nIn other words, the Markov condition entails a set of con-\nditional independence relations which can be found by the\nd-separations encoded in the graph. Two DAGs are called\nMarkov equivalent or alternatively said to belong to the same\nMarkov Equivalence Class (MEC) if they entail the same\nset of conditional independence relations and can be com-\npactly represented as a partially directed graph, so-called\nCompleted Partially Directed Acyclic Graph (CPDAG).\nCausal discovery A causal DAG is a DAG in which\nedges represent causal influence, with X \u2192 Y denoting\nthat X is a direct cause of Y. Causal Discovery (CD) is\nthe task of (possibly partially) learning the causal DAG\nbetween a set of variables based on a data sample from\ntheir joint distribution. CD methods can be categorised into\nconstraint-based (Spirtes et al., 2000), score-based-methods\n(Chickering, 2002) and methods restricting the function\nclass (Shimizu et al., 2006; Hoyer et al., 2008; Peters et al.,\n2014). Constraint-based methods rely on conditional inde-\npendence tests between the variables X in order to rule-out\nedges in the constructed DAG, assuming faithfulness, that\nis that the only conditional independencies are those im-\nplied by d-separation (Spirtes et al., 2000). The Peter-Clark\n(PC) algorithm (Spirtes et al., 2000) is one of the most\nwidely used constraint-based methods, that assumes no un-\nobserved confounders (direct common causes of two or\nmore measured variables). Its final output is a CPDAG or\nequivalently a MEC, including DAGs, entailing the same\nset of conditional independence relations, which cannot be\nfurther distinguished without additional assumptions.\nCORR2CAUSE benchmark We provide a description of\nthe CORR2CAUSE benchmark (Jin et al., 2024), which we\nuse in our evaluations. Each CORR2CAUSE record consists\nof three parts: a Premise consisting of correlation statements,\na Hypothesis with the causal question and a binary Output\nLabel to be predicted indicating whether the hypothesis is\nvalid (1) or invalid (0). For example:\nPremise: Suppose there is a closed system of 3 vari-\nables A, B and C. All the statistical relations among\nthese 3 variables are as follows: A correlates with\nC. B correlates with C. However, A is independent\nof B. Hypothesis: A directly causes C.\nOutput Label: 1\nThe benchmark is constructed as follows. First, all unique\nDAGs with N = 2,..., 6 nodes are generated. For ev-\nery DAG, the Markov condition implies certain conditional\nindependencies based on which DAGs are clustered into\"\n    },\n    {\n      \"title\": \"4. Proposed Prompting Strategy: PC-SUBQ\",\n      \"content\": \"MECs. For each MEC, several causal hypotheses between\nevery pair of variables are considered, such as direct causa-\ntion (as in the Hypothesis of the example above), confound-\ning or mediating. Given a set of conditional independencies\nand a causal hypothesis as input, the output label represents\nthe validity of the hypothesis and is generated as follows:\nif all DAGs in the MEC (implying the given conditional\nindependencies) satisfy the given hypothesis, then the hy-\npothesis is deemed valid, otherwise it is deemed invalid.\nEverything is, then, verbalised into natural language, with\nthe verbalised sets of conditional independencies compris-\ning the Premise. Variable names are kept symbolic, i.e.,\nA, B, C, ..., as in the example above, to disentangle the\nformal from the commonsense reasoning abilities of LLMs.\nFor a detailed description on the dataset construction one\ncould refer to Jin et al. (2024).\nWe address the NL-CD task of inferring causal structure\nfrom correlation statements. Prompting an LLM to decom-\npose a complex problem into simpler ones was recently\nshown to improve performance (Zhou et al., 2023a; Khot\net al., 2023; Dua et al., 2022). Inspired by this, we propose\na prompting strategy, PC-SUBQ, which exploits the steps\nof a popular CD algorithm, the PC algorithm (Spirtes et al.,\n2000), in order to decompose the NL-CD task. In particular,\nwe break the original task into several fixed subquestions,\neach corresponding to one step of the PC algorithm (Fig.\n1). We explicitly guide the LLM to follow these steps, by\nsequentially prompting the LLM with one PC subquestion\nat a time, augmenting the next subquestion's prompt with\nthe answer(s) to the previous one(s). Few-shot COT exam-\nples (Wei et al., 2022) are provided for every subquestion\nwith demonstrations of how to solve this specific subques-\ntion (Fig. 2). Subquestions and corresponding few-shots\nexamples are used unchanged across all NL-CD queries,\nsince PC is a common algorithm that can be followed to\nsolve all such causal queries.\nFigure 1 shows the 8 fixed PC-SUBQ subquestions that\nwe designed. PC-SUBQ sequentially prompts the LLM\nwith one of these 8 subquestions at a time. The coloring\nscheme demonstrates how answers to previous subques-\ntions are passed to the next ones, augmenting subsequent\nprompts. Notice that the rest of the history, for instance the\nreasoning steps, is not used in the prompt construction of\nfollowing subquestions. This allows reducing the context\nlength while enabling the LLM to focus on the most rele-\nvant information for solving the specific sub-task at hand.\n[Premise": "and [Hypothesis] refer to the given con-\nditional independence or correlation statements and to the"}, {"title": "5. Results", "content": "queried hypothesis about causal relationships, respectively\n(see also CORR2CAUSE in Sec. 3). In the following, we\nfirst provide the steps of the PC algorithm (Spirtes et al.,\n2000) and then their correspondence to our subquestions:\n0. Initialise fully connected graph: Construct a com-\nplete undirected graph of all variables.\n1. Construct skeleton: Eliminate as many edges as possi-\nble using conditional independence relations: remove\nedge X Y if X is independent of Y given some\nconditioning set Z.\n2. Orient edges to form v-structures: For all paths\n(X, Z, Y), such that edge X \u2013 Y is absent and Z is not\nin any conditioning set that makes X and Y indepen-\ndent: orient arrows such that a v-structure is created,\ni.e., X \u2192 Z and Y \u2192 Z.\n3. Orient edges to not form extra v-structures: Pos-\nsibly orient some more edges such that no extra v-\nstructures are introduced: For paths (X, Z, Y) with\nonly one of the two edges oriented towards Z, orient\nthe other edge away from Z.\nThe correspondence between subquestions (SUBQ) and PC\nalgorithm steps is as follows: SUBQ 1 refers to step 0 of\nthe PC algorithm, SUBQ 2 to step 1, SUBQ 3-6 to step 2\nand SUBQ 7 to step 3. The third step of the PC algorithm,\norienting v-structures, is the most complicated one, thus\nit was split into more fine-grained steps corresponding to\nfour subquestions. Overall, SUBQ 1-7 correspond to the\nfour steps of the PC algorithm, and aim to guide the LLM\nto infer the (possibly partially directed) causal graph. The\nlast SUBQ 8 questions, instead, the validity of the causal\nhypothesis between a pair of variables given the causal graph\ninferred from the previous seven subquestions. We prepend\nfew-shot examples, each with a COT reasoning, to each\nsubquestion. We use 1-4 shots per subquestion except for\nSUBQ 8 that uses 11 shots with diverse examples covering\na range of possible causal hypotheses. Figure 2 depicts\nsome indicative shots for some of the subquestions, while\nall few-shot examples are provided in Appendix A. Notice\nthat the few-shot COT examples that are provided for one\nsubquestion are \u201cindependent\" of the examples provided for\nanother, in the sense that they only demonstrate how to solve\nthis specific subquestion and not any of the previous ones.\nThis allows the demonstrations for two subquestions to not\nnecessarily refer to the same example query: for instance,\nin Fig. 2, the shot for SUBQ 4 refers to 3 variables, whereas\nthe shot for SUBQ 5 refers to 4 variables.\nEven though the few shots prepended to each subquestion\n(see Fig.2) are based on CORR2CAUSE, PC-SUBQ can be\nused unchanged, i.e., with the subquestions and correspond-\ning few shots fixed, with other benchmarks assessing LLMs'\nNL-CD skills. For instance, benchmarks containing natu-\nral variable names/scenarios, instead of symbolic, could be\nconsidered more appealing. A case study towards creating\nsuch a benchmark is provided in Jin et al. (2024) and, in the\nnext section, we provide PC-SUBQ's output on such natural\ncausal queries. Moreover, PC-SUBQ is robust to perturba-\ntions of the causal queries, such as different instantiation of\nthe variables or paraphrasing (see Sec. 5).\nIn the following, we first present results with our proposed\nPC-SUBQ strategy on CORR2CAUSE using a diverse list\nof five LLMs and compare them to the baseline prompting\nstrategies depicted in Fig. 3. Specifically, we evaluate on the\ntest partition of CORR2CAUSE:2 we do not use the training\nand validation partitions since we are not performing any\nfine-tuning. We then analyse the robustness of PC-SUBQ to\nvariable renaming and paraphrasing, including some natural\nstory scenarios. Finally, we present a per partition perfor-\nmance analysis for two dataset partitions: per hypothesis\nand per number of variables. For all our results, we report\ntwo evaluation metrics: F1-score and accuracy, with the\nformer being our main metric (similar to Jin et al. (2024)),\nsince the dataset is imbalanced (15.23% of the test set has\npositive label)."}, {"title": "6. Discussion and Conclusion", "content": "We introduced PC-SUBQ, a prompting strategy that enables\nLLMs to improve their performance on the task of inferring\ncausal relationships from correlation statements. PC-SUBQ\ndecomposes the original NL-CD task into several fixed sub-\nquestions, corresponding to the PC algorithm steps, which\ncompose the prompt chain given to an LLM. Evaluated on\nCORR2CAUSE, PC-SUBQ outperforms a range of widely\nused prompting strategies across five LLMs. Furthermore,\nperformance is robust to different variable instantiations or\nparaphrased causal queries. Remarkably, applied to exam-\nples with natural stories, PC-SUBQ produces correct rea-\nsoning traces and answers, even though few-shot demonstra-\ntions only contain examples with symbolic variable names.\nAdditionally, by guiding the LLM to reason through ap-\nplying algorithmic steps, we gain more transparency and\ninterpretability of the results: for instance, it is possible to\ntrace which steps are responsible for an erroneous final an-\nswer by inspecting the output. We provide such an example\nin Appendix E in which the erroneous final answer is only\ndue to an error in the reasoning of the final SUBQ 8 (con-\nfusing a directed edge with an undirected one), while the\ninferred graph of SUBQ 7 and all previous reasoning traces\nand answers up to SUBQ 7 are correct. The proposed ap-\nproach could be more generally applicable to address tasks\nthat have a common underlying algorithm which can be\napplied to all examples to solve the task: each subquestion\nwould then correspond to a step of this shared algorithm.\nFinally, as with all prompting frameworks, there is no need\nto fine-tune a model using several thousands of annotated\nexamples; instead, only a few (often less than 10) demon-\nstrations are enough.\nOur work has the following limitations. Current evaluations\nrefer only to the output of the final SUBQ 8, for which we\nhave ground truth available. Intermediate evaluations would\nbe desirable in order to quantitatively assess intermediate\nanswers, instead of only qualitatively inspecting the rea-\nsoning traces (as in Appendix E). It would be particularly\ninteresting to have evaluations for the output of SUBQ 7,\ni.e., the inferred causal graph. Nevertheless, since the causal\nhypotheses of CORR2CAUSE cover causal relationships be-\ntween every pair of variables, current evaluations should\nbe representative of the quality of the inferred graph. Ad-\nditionally, compared to few-shot prompting that requires\nonly one LLM call, the proposed chain of prompts induces\nincreased inference time due to the 16 LLM calls, two per\nsubquestion for the reasoning and answer parts, needed. Fu-\nture work should thus focus on making the strategy more\nefficient. Furthermore, when the task complexity increases,\nas measured by the number of variables, performance is af-\nfected. This limitation might be mitigated for natural stories,\nsince they often do not refer to more than 3 or 4 variables.\nIndeed, our results with such natural scenarios are very\npromising. Areas of future research could include construct-\ning more NL-CD benchmarks with natural stories, beyond\nthe two available ones, on which to employ PC-SUBQ. The\nformal causal reasoning of PC-SUBQ could be further com-\nbined with the commonsense knowledge captured by LLMs\nfor these scenarios. Finally, an alternative path to tackle\nNL-CD could be via Tool Use, by interrupting the LLM\nreasoning process and calling a registered Python tool (Yao\net al., 2023; Parisi et al., 2022), with the PC algorithm im-\nplemented. Notice that the LLM would still be required\nto understand the task in order to appropriately convert the\ninput for the Python function and decide when to call it. We\nthink of these two paths as complementary: as with other\ntasks, such as the ability to multiply two numbers or solve a\nmath or physics problem, it is desirable for the LLM to have\ncertain skills and not solely rely on external tools, which\nalso may not be always available."}]}