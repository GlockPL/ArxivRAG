{"title": "Constrained Intrinsic Motivation for Reinforcement Learning", "authors": ["Xiang Zheng", "Xingjun Ma", "Chao Shen", "Cong Wang"], "abstract": "This paper investigates two fundamental problems that arise when utilizing Intrinsic Motivation (IM) for reinforcement learning in Reward-Free Pre-Training (RFPT) tasks and Exploration with Intrinsic Motivation (EIM) tasks: 1) how to design an effective intrinsic objective in RFPT tasks, and 2) how to reduce the bias introduced by the intrinsic objective in EIM tasks. Existing IM methods suffer from static skills, limited state coverage, sample inefficiency in RFPT tasks, and suboptimality in EIM tasks. To tackle these problems, we propose Constrained Intrinsic Motivation (CIM) for RFPT and EIM tasks, respectively: 1) CIM for RFPT maximizes the lower bound of the conditional state entropy subject to an alignment constraint on the state encoder network for efficient dynamic and diverse skill discovery and state coverage maximization; 2) CIM for EIM leverages constrained policy optimization to adaptively adjust the coefficient of the intrinsic objective to mitigate the distraction from the intrinsic objective. In various MuJoCo robotics environments, we empirically show that CIM for RFPT greatly surpasses fifteen IM methods for unsupervised skill discovery in terms of skill diversity, state coverage, and fine-tuning performance. Additionally, we showcase the effectiveness of CIM for EIM in redeeming intrinsic rewards when task rewards are exposed from the beginning. Our code is available at https://github.com/x-zheng16/CIM.", "sections": [{"title": "1 Introduction", "content": "In the realm of Reinforcement Learning (RL), Intrinsic Motivation (IM) plays a vital role in the design of exploration strategies in both Reward-Free Pre-Training (RFPT) tasks and Exploration with Intrinsic Motivation (EIM) tasks [Barto, 2013]. It allows the RL agent to efficiently visit novel states by assigning higher intrinsic bonuses to unfamiliar states [Zhang et al., 2021]. Current IM methods can be classified into three categories: knowledge-based, data-based, and competence-based IM methods [Laskin et al., 2021].\nKnowledge-based and data-based IM methods are employed in both RFPT and EIM tasks to encourage the agent to explore novel regions. Knowledge-based IM methods maximize the deviation of the agent's latest state visitation from the policy cover (i.e., the regions covered by all prior policies) [Zhang et al., 2021]. These methods commonly estimate the density of the policy cover via the pseudo-count of state visit frequency [Bellemare et al., 2016; Fu et al., 2017], prediction errors of a neural network [Pathak et al., 2017; Burda et al., 2019], or variances of outputs of an ensemble of neural networks [Pathak et al., 2019; Lee et al., 2021; Bai et al., 2021]. Data-based IM methods, on the other hand, directly maximize the state coverage (i.e., the region visited by the latest policy) via maximizing the state entropy [Hazan et al., 2019; Mutti et al., 2021; Liu and Abbeel, 2021b; Seo et al., 2021]. However, knowledge-based and data-based IM methods are inefficient in RFPT tasks since they do not condition the latent skill variable, limiting the fine-tuning performance of the pre-trained policy in downstream tasks [Liu and Abbeel, 2021a]. Moreover, when utilized in EIM tasks, these IM methods introduce non-negligible biases to the policy optimization, leading to suboptimal policies [Chen et al., 2022]. Specifically, intrinsic objectives may result in excessive exploration even when the task rewards are already accessible. This distraction induced by intrinsic objectives can deteriorate the performance of the RL agent and impede the wider application of these methods in EIM tasks.\nCompetence-based IM methods are designed for unsupervised skill discovery in RFPT tasks. They primarily maximize the mutual information between the state representation and the latent skill variable to learn a latent-conditioned poilcy [Gregor et al., 2017; Sharma et al., 2020; Laskin et al., 2022]. The policy conditioned on the latent skill variable is required to change the state of the environment in a consistent and meaningful way, e.g., walking, flipping, pushing, to be finetuned efficiently in the downstream tasks. However, current competence-based IM methods have shown poor performance in the Unsupervised Reinforcement Learning Benchmark (URLB) [Laskin et al., 2021], a benchmark of IM methods evaluated in RFPT tasks. Intuitively, directly maximizing the mutual information does not guarantee extensive state coverage or the discovery of dynamic skills and easily converges to simple and static skills due to the invariance of the mutual information to scaling and invert-"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Markov Decision Processes", "content": "The discounted Markov Decision Process (MDP) is defined as M = (S, A, P, R, \u03b3, \u03bc), where S and A stand for the state space and the action space separately, P : S \u00d7 A \u2192 \u2206(S) is the transition function mapping the state s and the action a to the distribution P(s'|s, a) in the space of probability dis-tribution A(S) over S, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, \u03b3\u2208 [0, 1) is the discount factor, and \u03bc\u03b5 \u0394(S) is the initial state distribution. We focus on the episodic setting where the environment is reset once the agent reaches a final state sf, a terminated state within the goal subsets G or a truncated state sr. At the beginning of each episode, the agent samples a random initial state so ~ \u03bc; at each time t = 0, 1, 2, ..., T \u2013 1, it takes an action at \u2208 A computed by a stochastic policy \u03c0 : S \u2192 \u0394(A) or a deterministic one \u03c0 : S \u2192 A according to the current state st and steps into the next state st+1 ~ P(st, at) with an instant reward signal r = R(st, at, St+1) obtained."}, {"title": "2.2 Reward-Free Pre-Training and Exploration", "content": "RFPT and EIM are two types of intrinsically motivated RL tasks. To present the optimization objectives of RFPT and EIM, we first define the state distribution induced by the pol-icy \u03c0as\n\\(d_\\pi(s) = (1 \u2013 \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s|\\mu, \\pi) \\in \\mathcal{K}\\),\nwhere K is the collection of all induced distributions. The extrinsic objective (the expectation of the task reward) is then JE(d\u03c0) = Es~d\u03c0 [re], where re = Re(s, a, s') is the extrinsic task reward function. The intrinsic objective J\u2081 : K \u2192 R is defined as a differentiable function of the induced state distribution d with L-Lipschitz gradients.\nIn RFPT tasks, the task reward re is not available, and the agent aims to maximize only the intrinsic objective\n\\(L^{RFPT}(\\pi) = J_I(d_\\pi).\\)  (1)\nThe agent can learn either a policy \u03c0(a|s) without conditioning the latent skill variable when maximizing a knowledge-based or data-based intrinsic objective, or a latent-conditioned policy \u03c0(a|s,z) when maximizing a competence-based intrinsic objective. Common evaluation metrics for RFPT tasks include state coverage, skill diversity, and fine-tuning performance in downstream tasks.\nOn the contrary, in EIM tasks, the goal of the agent is to complete a specific downstream task and maximize only the expected task rewards. The optimization objective of EIM is\n\\(L^{EIM}(\\pi) = J_E(d_\\pi) + \\tau_k J_I(d_\\pi),\\) (2)\nwhere Tk is the coefficient of the intrinsic objective. Since the agent does not need to discover diverse skills for a specific task, J\u2081(d) in EIM tasks is commonly a knowledge-based or data-based intrinsic objective without conditioning the latent skill variable. The evaluation metric for EIM is only the expected task rewards."}, {"title": "2.3 Intrinsic Motivation Methods", "content": "We conduct a comprehensive comparison between our pro-posed CIM for RFPT and eighteen IM algorithms in re-gards of the intrinsic objective and the corresponding in-trinsic reward function in Table 1, including Intrinic Cu-riosity Module (ICM) [Pathak et al., 2017], Random Net-work Distillation (RND) [Burda et al., 2019], Disagreement (Dis.) [Pathak et al., 2019], MAximizing the Deviation from explored regions (MADE) [Zhang et al., 2021], Adversari-ally Guided Actor-Critic (AGAC) [Flet-Berliac et al., 2021], Maximum Entropy exploration (MaxEnt) [Hazan et al., 2019], Active Pre-Training (APT) [Liu and Abbeel, 2021b], Random Encoders for Efficient Exploration (RE3) [Seo et al., 2021], Variational Intrinsic Control (VIC) [Gregor et al., 2017], Diversity Is All You Need (DIAYN) [Eysen-bach et al., 2019], Variational Intrinsic Successor featuRes (VISR) [Hansen et al., 2020], Dynamics-Aware Discovery"}, {"title": "3 Constrained Intrinsic Motivation", "content": "In this section, we first present CIM for RFPT, a novel competence-based IM method that can learn dynamic and di-verse skills efficiently. Specifically, we propose a constrained intrinsic objective \\(J_{CIM}\\) for RFPT tasks, maximizing the conditional state entropy instead of the state entropy under a novel alignment constraint for the state representation. We then derive the corresponding intrinsic reward \\(r_{CIM}\\) based on the Frank-Wolfe algorithm. Secondly, we propose CIM for EIM to adaptively adjust the coefficient of the intrinsic ob-jective in Equation (2) based on constrained policy optimiza-tion to mitigate the bias introduced by the intrinsic objective. We then derive the adaptive coefficient \\(\\tau^{CIM}\\) based on the La-grangian duality theory."}, {"title": "3.1 Constrained Intrinsic Motivation for RFPT", "content": "In this section, we develop CIM for RFPT, a novel con-strained intrinsic objective for unsupervised RL. To better clarify the motivation for the design of the constrained intrin-sic objective, we first review current coverage- and mutual information-based methods and analyze their limitations.\nProblems of Previous Intrinsic Motivation Methods\nThough knowledge-based and data-based IM methods may perform well in terms of state coverage in certain RFPT tasks, these methods lack awareness of latent skill vari-ables and suffer from poor fine-tuning efficiency. To im-prove the fine-tuning performance in RFPT tasks, learning a latent-conditioned policy is necessary. However, exist-ing competence-based IM methods perform poorly regarding skill diversity, state coverage, and sample inefficiency. We conjecture that there are two main issues:\nIntrinsic objective. Maximizing only the mutual information is not suitable for dynamic and diverse skill discov-"}, {"title": "Design of Constrained Intrinsic Objective", "content": "To address the first issue, we propose choosing the condi-tional state entropy H(\u03c6(s)|z) as the intrinsic objective. This is a key difference between our method and previous IM methods. Intuitively, by maximizing H(\u03c6(s)|z), distances be-tween adjacent states within the trajectories sampled by one skill are enlarged, which indicates a more dynamic skill.\nFor the second issue, we propose maximizing a novel lower bound of the mutual information between the state represen-tation and the latent skill variable to make the trajectories sampled by different skills distinguishable, that is,\n\\(I(\\phi(s); z) \\ge \\log N \u2013 \\mathcal{L}_a(\\phi(s), z),\\) (3)\nwhere \\(\\mathcal{L}_a(\\phi(s), z)\\) is the alignment loss as follows:\n\\(\\mathcal{L}_a(\\phi(s), z) = \\sum_{i} \\mathcal{L}^{CIM}_{i}\\),\n\\(\\mathcal{L}^{CIM}_{i} = -diff(T_i)^Tz_i +\\\\\n\\log \\sum_{T_j \\in \\mathcal{S}-\\{T_i\\}} exp (diff(T_j))^Tz_i),\\) (4)\ndiff(\\(\\tau\\)) = \u03c6(s\u2032) \u2013 \u03c6(s),\nN is the total number of samples for estimating the mutual information, \u03c4 = (s, s') is the slice of a trajectory, and S\u00af is a set of negative samples that contains trajectories sampled via skills other than zi. We derive this lower bound based on Contrastive Predictive Coding [Oord et al., 2018] by regard-ing the latent skill z as the context and diff (7) as the predic-tive coding. Based on Equation (3), the alignment constraint on the state encoder network \u03c6(s) is\n\\(\\mathcal{L}_a(\\phi(s), z) \\le C,\\) (5)\nwhere C is a constant. Theoretically, as indicated in Table 1, C should represent the minimum of La(\u03c6(s), z). In practice, we do not need to know the exact value of C. Instead, at each policy iteration step, we take several stochastic gradient descent steps on the alignment loss La(\u03c6(s), z) to maximize the mutual information between the state representation \u03c6(s) and the latent skill z."}, {"title": "The complete constrained intrinsic objective of CIM for RFPT is thus", "content": "\\(max_{\u03c0}\\mathcal{J}^{CIM}(d_{\\phi(s)}) = H(\\phi(s)|z)\\\\\ns.t. \\mathcal{L}_a(\\phi(s), z) \\le C,\\) (6)\nwhere H(\u03c6(s)|z) is the conditional state entropy estimated in the state projection space, which depends on both the latent-conditioned policy network \u03c0(\u00b7|s, z) and the state encoder network \u03c6(s), d\u03c6(s) is the distribution of the latent state \u03c6(s) induced by the latent-conditioned policy \u03c0(\u00b7|s, z).\nInterpreting La(\u03c6(s), z) as an alignment loss provides us a novel insight to unify former competence-based IM meth-ods. We can derive the alignment loss li of all previous competence-based IM methods listed in Table 1, e.g.,\n\\(\\mathcal{L}_{IMSE} = ||\\phi(s) - z_i||_2^2,\\\\\n\\mathcal{L}_{YMF} = -Sc(\\phi(s_i), z_i),\\\\\\n\\mathcal{L}_{LSD} = -diff (T_i)^Tz_i + \\lambda (||\\phi_{diff}(T_i)|| \u2013 d(s, s')),\\\\\n\\mathcal{L}_{ICIC} = \u2212Sc(\\phi(T_i), q_z(z_i))+\\\\\n\\log \\sum_{T_j \\in \\mathcal{S}-\\{T_i\\}} exp (Sc(\\phi(T_j), q_z(z_i))),\\\\\n\\mathcal{L}_{BeCL} = -Sc(\\phi(s),\\phi(s_i))+\\\\\n\\log \\sum_{s_j \\in \\mathcal{S}-\\{s_i\\}} exp (Sc(\\phi(s),\\phi(s_i))),\\) (7)\nwhere d(s, s') in ILSD is the state distance function, Sc in IMF is the cosine similarity between two vectors, qz in ICIC is a projection network for the latent skill vector, and \u03c6(s) in BeCL is a state representation from a certain skill as the posi-tive sample while all others are negative samples.\nEstimation of Conditional State Entropy\nWe now introduce how to estimate the conditional state en-tropy H(\u03c6(s)|z) involved in Equation (6). Recall the defini-tion of the conditional state entropy\n\\(H(\\phi(s)|z) = E_{z\\sim p_z} [H(\\phi(s)|z = z)]\\\\ = E_{z\\sim p_z}E_{\\phi(s)\\sim d_{\\pi}} [\u2212 \\log d_{\\pi}(\\phi(s))] .\\) (8)\nTo estimate the outer expectation, we randomly sample the la-tent skill variables z from a prior distribution pz(z). For dis-crete skills, pz (z) can be a categorical distribution Cat(K, p) that is parameterized by p over a size-K the sample space, where pi denotes the probability of the i-th skill. For con-tinuous skills, we can select p(z) as a uniform distribution Unz (a, b) over the interval [a, b], where n\u2082 is the dimension of the skill.\nTo estimate the inner expectation, we roll out trajecto-ries using the latent-conditioned policy \u03c0(\u00b7|s, z) with z fixed. During the sampling phase, z is randomly sampled at the be-ginning of each episode and remains fixed throughout the en-tire trajectory. We store the state-skill pair (s,z) in the replay buffer. During the training phase, for each pair (s,z), we con-catenate them as [s, z] to be the input of the latent-conditioned policy \u03c0(.s, z).\nTo estimate the state density d\u03c0, instead of training a pa-rameterized generative model, we leverage a more practical"}, {"title": "non-parametric -nearest neighbor (\u03be-NN) estimator", "content": "\\(d_\\pi(s_i) = \\frac{1}{\\lambda (B_{\\xi}(S_i))} \\int_{B_{\\xi}(S_i)} d_\\pi(s)ds,\\) (9)\nwhere X is the Lebesgue measure on Rd, B\u025b is the smallest ball centered on si containing its \u00a7-th nearest neighbours.\nLower bound of conditional state entropy. Each skill can be stochastic if we directly maximize the conditional state entropy H(sz). To address this, we propose maximizing the lower bound of H(s|z) to encourage the skill z to pro-duce large state variations along the direction of z in the la-tent space instead of being fully stochastic. To derive the lower bound of H(s|z), we first define a projection func-tion gz(\u03c6(s)) = \u03c6(s)Tz for a fixed skill z. It is easy to verify that H(\u03c6(s)|z) \u2265 H(gz(\u03c6(s))|z)) with equality iff Sc(\u03c6(s), z) = 1, that is, H(gz(\u03c6(s))|z)) is a lower bound of H(\u03c6(s)|z). We thus can maximize H(gz(\u03c6(s))|z)) to max-imize H(\u03c6(s)|z) and estimate the distribution of the one-dimensional random variable gz(\u03c6(s)) for each z.\nIntrinsic reward. Based on the above design, we can derive the intrinsic reward of CIM for RFPT as rfIM(s) =\n\\(log ||g_z(\\phi(s)) \u2013 g_z(\\phi(s)^{\\xi})||.\\) Here, gz(\u03c6(s)) means the \u03be-th nearest neighbor of gz(\u03c6(s)). We adopted an average-distance version similar to APT to make training more stable:\n\\(r^{CIM}(s) = log \\Big(1+\\sum_{j=1}^\\xi ||g_z(\\phi(s)) - g_z(\\phi(s)^{\\xi}); ||\\Big)\\) (10)\nIntuitively, rfIM(s) measures how sparse the state s is in the projection subspace spanned by its corresponding latent skill z. This reward function can be justified based on the procedure of the Frank-Wolfe algorithm. Specifically, since LRFPT is concave in d\u33a2, maximizing LRFPT involves solving"}, {"title": "\\(d_\\pi^{k+1} \\in argmax(\u2207_{d_\\pi}\\mathcal{L}(d_{\\pi^k}), d_{\\pi^{k+1}} \u2013 d_{\\pi^k})\\)", "content": "iteratively [Hazan et al., 2019]. This iterative step is equivalent to policy opti-mization using a reward function proportional to \u2207d\u201eL(d\u03c0\u03ba)."}, {"title": "3.2 Constrained Intrinsic Motivation for EIM", "content": "In this section, we present our CIM for EIM, an adaptive co-efficient for the intrinsic objective in Equation (2). Currently, IM methods for EIM tasks commonly use a constant coeffi-cient or an exponentially decaying coefficient, which requires costly hyperparameter tunning. To avoid this, we propose re-formulating Equation (2) by regarding the extrinsic objective as a constraint for the intrinsic objective, i.e.,\n\\(max_{d_{\\pi} \\in \\mathcal{K}} J_I(d_{\\pi}), s.t. J_E(d_{\\pi}) \\ge R_k,\\) (11)\nwhere Rk represents the expected reward at the k-th it-eration of policy optimization. We approximate Rk via Rk = maxj\u2208{1,2,...,k\u22121} JE(d\u03c0;). We then leverage the La-grangian method to solve Equation (11). The correspond-ing Lagrangian dual problem is minx>0 maxd J1(d) +\n\\(\\lambda_k(J_E(d) \u2013 R_k)\\). The Lagrangian multiplier \u5165 is updated by Stochastic Gradient Descent (SGD), that is, k = Ak\u22121\n\\(\u03b7(J_E(d_{\\pi^k}) \u2013 R_{k\u22121})\\) where \u03b7 is the updating step size of \u5165k. Observing that Lk (d\u03c0, \u03bbk) \u221d JE(d\u2081) + 1\u00b9 J\u2081(d\u33a2), the adap-tive coefficient TCIM is then derived as\n\\(\\tau^{CIM}_k = min(\\{\\lambda_{k\u22121} \u2212 \u03b7(J_E(d_{\\pi^k}) \u2013 R_{k\u22121})\\}^{-1},1\\),\\) (12)\nwhere the outer minimization is to ensure numerical stabil-ity. As the Lagrangian multiplier k grows, the penalty for the violation of TIN CIM gradually tends to zero; that is, the bias introduced by the intrinsic objective J\u2081 is adaptively reduced."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Experimental setup for RFPT. We evaluate our intrinsic bonus rfIM for RFPT tasks on four Gymnasium environ-ments, including two locomotion environments (Ant and Hu-manoid) and two manipulation environments (FetchPush and FetchSlide). We compare CIM for RFPT with fifteen IM methods in Table 1, including 1) four knowledge-based IM methods: ICM, RND, Dis., MADE, and AGAC; 2) one data-based IM method: APT; 3) and nine competence-based meth-ods: DIAYN, VISR, DADS, APS, CIC, MOSS, BeCL, LSD, and CSD.\nExperimental setup for EIM. We evaluate our adap-tive coefficient TCIM TO for EIM in two navigation tasks (PointMaze_UMaze and AntMaze_UMaze) in D4RL [Fu et al., 2020], and four sparse-reward tasks (SparseHalfChee-tah, SparseAnt, SparseHumanoidStandup, and SparseGrid-World). TCIM is orthogonal with any intrinsic bonuses ri.\nUnless otherwise mentioned, we adopt the state-of-the-art data-based intrinsic bonus rAPT =\\(log(1 + \\frac{1}{\\xi}\\sum_{j=1}^k ||\\phi(8)\n\\phi(s)^{\\xi}||)\\). The total instant reward is then r = re + TCIMAPT\nWe compare CIM for EIM with three baseline coefficient schemes, i.e., the constant coefficient T = 1, the linearly decaying coefficient r = (1 \u2212 k/T), and the exponentially decaying coefficient T = 0.001k."}, {"title": "4.2 Results in RFPT Tasks", "content": "Visualization of skills. As previous works like LSD do, we train CIM for RFPT to learn diverse locomotion continuous skills in the Ant and Humanoid environment and diverse ma-nipulation skills in FetchPush and FetchSlide. The learned skills are visualized as trajectories of the agent on the x y plane in Figure 1 and Figure 2. Our CIM for RFPT outper-forms all 15 baselines in terms of skill diversity and state coverage. The skills learned via CIM are interpretable be-cause of our alignment loss; the direction of the trajectory on the x y plane changes consistently with the change in the direction of the skill. Specifically, CIM excels at learning dy-namic skills that move far from the initial location in almost all possible directions, while most baseline methods fail to discover such diverse and dynamic primitives. Their trajecto-ries are non-directional or less dynamic than CIM, especially in two locomotion tasks. Competence-based approaches like DIAYN, VISR, and DADS directly maximize the mutual in-formation objectives but learn to take static postures instead of dynamic skills; such a phenomenon is also reported in LSD and CIC. Although APS and CIC can learn dynamic skills by directly maximizing the state entropy, CIM discovers skills that reach farther and are more interpretable via maximizing the lower bound of the state entropy. As for the two vari-ants of CIC, MOSS and BeCL, they perform even worse than CIC in all tasks, reflecting their limitation in skill discovery. Lastly, LSD and CSD cannot learn dynamic skills within lim-ited environment steps in Ant and Humanoid due to their low sample efficiency. Though they perform better in manipula-tion tasks than locomotion tasks, their learned skills are ram-bling compared with our CIM."}, {"title": "ICIM, perform better than other styles like MSE and vMF. Besides, ICIM is the most effective. As shown in Figure 3a, our CIM can also be utilized to discover discrete diverse and dynamic skills, though it is mainly designed for continuous skills. Moreover, our CIM for RFPT is also robust to the number of skill dimensions, as shown in Table 5. Based on the ablation study, we can conclude that the two components of CIM, i.e., minimizing NCE-style alignment loss and maxi-mizing conditional state entropy, are equally critical. Specifi-cally, the results in Table 4 show that replacing the alignment loss of CIM with a trivial MSE loss reduces the state cover-age in Ant from 1042 to 64. Moreover, Table 2 reveals that the state coverage achieved by CIM can reach 1135 in the challenging 378-dimensional Humanoid, while that achieved by CIC and BeCL, which use similar NCE-style alignment losses, is lower than 100.", "content": ""}, {"title": "4.3 Results in EIM Tasks", "content": "In PointMaze, we directly train a policy to control the Point without learning low skills since the environment dynam-ics are simple. In AntMaze, we train a meta-controller on top of the latent-conditioned policy pre-trained via our CIM for RFPT method. The meta-controller observes the target goal concatenated to the state observation [s; sg] and out-puts the skill latent variable z at each timestep. We visual-ize the trajectories of the Ant in the x y plane as shown in Figure 3b, where the skills in a single trajectory gradually change to make the Ant turn a corner. Figure 3c shows that the Lagrangian-based adaptive coefficient TIM outperforms three baseline coefficients, especially in PointMaze. Specif-ically, we can observe a small peak in the early stage of the training in PointMaze, which means the agent can reach the randomly generated target point with a small probability at the beginning. However, as the training processes, the agent is distracted by the intrinsic bonuses when using a trivial coef-ficient T or T. Moreover, other latent-conditioned policies are of poor quality, and we fail to train a mete-controller on top of those policies. We also conduct experiments to demon-strate the performance of CIM for EIM across four sparse-reward locomotion tasks. The results in Table 6 indicate that TIM can effectively reduce the bias introduced by in-trinsic rewards, thereby enhancing test-time average episode rewards in EIM tasks."}, {"title": "5 Conclusion", "content": "In this paper, we proposed Constrained Intrinsic Motivation (CIM) for RFPT and EIM tasks, respectively. For RFPT tasks, we designed a novel constrained intrinsic objective to discover dynamic and diverse skills. For EIM tasks, we de-signed an adaptive coefficient TCIM for the intrinsic objective based on constrained policy optimization. Our experiments demonstrated that CIM for RFPT outperformed all fifteen baselines across various MuJoCo environments regarding di-versity, state coverage, sample efficiency, and fine-tuning per-formance. The latent-conditioned policy learned via CIM for RFPT was successfully applied to solve complex EIM tasks via training a meta-controller on top of it. We also empirically verified the effectiveness of our adaptive coefficient TCIM in multiple EIM tasks."}]}