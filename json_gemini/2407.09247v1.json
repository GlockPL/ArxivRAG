{"title": "Constrained Intrinsic Motivation for Reinforcement Learning", "authors": ["Xiang Zheng", "Xingjun Ma", "Chao Shen", "Cong Wang"], "abstract": "This paper investigates two fundamental problems that arise when utilizing Intrinsic Motivation (IM) for reinforcement learning in Reward-Free Pre-Training (RFPT) tasks and Exploration with Intrinsic Motivation (EIM) tasks: 1) how to design an effective intrinsic objective in RFPT tasks, and 2) how to reduce the bias introduced by the intrinsic objective in EIM tasks. Existing IM methods suffer from static skills, limited state coverage, sample inefficiency in RFPT tasks, and suboptimality in EIM tasks. To tackle these problems, we propose Constrained Intrinsic Motivation (CIM) for RFPT and EIM tasks, respectively: 1) CIM for RFPT maximizes the lower bound of the conditional state entropy subject to an alignment constraint on the state encoder network for efficient dynamic and diverse skill discovery and state coverage maximization; 2) CIM for EIM leverages constrained policy optimization to adaptively adjust the coefficient of the intrinsic objective to mitigate the distraction from the intrinsic objective. In various MuJoCo robotics environments, we empirically show that CIM for RFPT greatly surpasses fifteen IM methods for unsupervised skill discovery in terms of skill diversity, state coverage, and fine-tuning performance. Additionally, we showcase the effectiveness of CIM for EIM in redeeming intrinsic rewards when task rewards are exposed from the beginning. Our code is available at https://github.com/x-zheng16/CIM.", "sections": [{"title": "1 Introduction", "content": "In the realm of Reinforcement Learning (RL), Intrinsic Motivation (IM) plays a vital role in the design of exploration strategies in both Reward-Free Pre-Training (RFPT) tasks and Exploration with Intrinsic Motivation (EIM) tasks [Barto, 2013]. It allows the RL agent to efficiently visit novel states by assigning higher intrinsic bonuses to unfamiliar states [Zhang et al., 2021]. Current IM methods can be classified into three categories: knowledge-based, data-based, and competence-based IM methods [Laskin et al., 2021].\nKnowledge-based and data-based IM methods are employed in both RFPT and EIM tasks to encourage the agent to explore novel regions. Knowledge-based IM methods maximize the deviation of the agent's latest state visitation from the policy cover (i.e., the regions covered by all prior policies) [Zhang et al., 2021]. These methods commonly estimate the density of the policy cover via the pseudo-count of state visit frequency [Bellemare et al., 2016; Fu et al., 2017], prediction errors of a neural network [Pathak et al., 2017; Burda et al., 2019], or variances of outputs of an ensemble of neural networks [Pathak et al., 2019; Lee et al., 2021; Bai et al., 2021]. Data-based IM methods, on the other hand, directly maximize the state coverage (i.e., the region visited by the latest policy) via maximizing the state entropy [Hazan et al., 2019; Mutti et al., 2021; Liu and Abbeel, 2021b; Seo et al., 2021]. However, knowledge-based and data-based IM methods are inefficient in RFPT tasks since they do not condition the latent skill variable, limiting the fine-tuning performance of the pre-trained policy in downstream tasks [Liu and Abbeel, 2021a]. Moreover, when utilized in EIM tasks, these IM methods introduce non-negligible biases to the policy optimization, leading to suboptimal policies [Chen et al., 2022]. Specifically, intrinsic objectives may result in excessive exploration even when the task rewards are already accessible. This distraction induced by intrinsic objectives can deteriorate the performance of the RL agent and impede the wider application of these methods in EIM tasks.\nCompetence-based IM methods are designed for unsupervised skill discovery in RFPT tasks. They primarily maximize the mutual information between the state representation and the latent skill variable to learn a latent-conditioned poilcy [Gregor et al., 2017; Sharma et al., 2020; Laskin et al., 2022]. The policy conditioned on the latent skill variable is required to change the state of the environment in a consistent and meaningful way, e.g., walking, flipping, pushing, to be finetuned efficiently in the downstream tasks. However, current competence-based IM methods have shown poor performance in the Unsupervised Reinforcement Learning Benchmark (URLB) [Laskin et al., 2021], a benchmark of IM methods evaluated in RFPT tasks. Intuitively, directly maximizing the mutual information does not guarantee extensive state coverage or the discovery of dynamic skills and easily converges to simple and static skills due to the invariance of the mutual information to scaling and invert-"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Markov Decision Processes", "content": "The discounted Markov Decision Process (MDP) is defined as M = (S, A, P, R, \u03b3, \u03bc), where S and A stand for the state space and the action space separately, P : S \u00d7 A \u2192 \u2206(S) is the transition function mapping the state s and the action a to the distribution P(s'|s, a) in the space of probability distribution A(S) over S, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, \u03b3\u2208 [0, 1) is the discount factor, and \u03bc\u2208 \u0394(S) is the initial state distribution. We focus on the episodic setting where the environment is reset once the agent reaches a final state sf, a terminated state within the goal subsets G or a truncated state sr. At the beginning of each episode, the"}, {"title": "2.2 Reward-Free Pre-Training and Exploration", "content": "RFPT and EIM are two types of intrinsically motivated RL tasks. To present the optimization objectives of RFPT and EIM, we first define the state distribution induced by the policy \u03c0as d\u03c0(s) = (1 \u2013 \u03b3) \u03a3\u221et=0 \u03b3tP(st = s|\u03bc,\u03c0) \u2208 \u041a, where K is the collection of all induced distributions. The extrinsic objective (the expectation of the task reward) is then JE(d) = Es~d [re], where re = Re(s, a, s') is the extrinsic task reward function. The intrinsic objective J\u2081 : K \u2192 R is defined as a differentiable function of the induced state distribution d with L-Lipschitz gradients.\nIn RFPT tasks, the task reward re is not available, and the agent aims to maximize only the intrinsic objective\n$$L^{RFPT}(\\pi) = J_I(d_{\\pi}).$$\n(1)\nThe agent can learn either a policy \u03c0(a|s) without conditioning the latent skill variable when maximizing a knowledge-based or data-based intrinsic objective, or a latent-conditioned policy \u03c0(a|s,z) when maximizing a competence-based intrinsic objective. Common evaluation metrics for RFPT tasks include state coverage, skill diversity, and fine-tuning performance in downstream tasks.\nOn the contrary, in EIM tasks, the goal of the agent is to complete a specific downstream task and maximize only the expected task rewards. The optimization objective of EIM is\n$$L^{EIM}(\\pi) = J_E(d_{\\pi}) + \\tau_k J_I(d_{\\pi}),$$\n(2)\nwhere \u03c4k is the coefficient of the intrinsic objective. Since the agent does not need to discover diverse skills for a specific task, J\u2081(d) in EIM tasks is commonly a knowledge-based or data-based intrinsic objective without conditioning the latent skill variable. The evaluation metric for EIM is only the expected task rewards."}, {"title": "2.3 Intrinsic Motivation Methods", "content": "We conduct a comprehensive comparison between our proposed CIM for RFPT and eighteen IM algorithms in regards of the intrinsic objective and the corresponding intrinsic reward function in Table 1, including Intrinic Curiosity Module (ICM) [Pathak et al., 2017], Random Network Distillation (RND) [Burda et al., 2019], Disagreement (Dis.) [Pathak et al., 2019], MAximizing the Deviation from explored regions (MADE) [Zhang et al., 2021], Adversarially Guided Actor-Critic (AGAC) [Flet-Berliac et al., 2021], Maximum Entropy exploration (MaxEnt) [Hazan et al., 2019], Active Pre-Training (APT) [Liu and Abbeel, 2021b], Random Encoders for Efficient Exploration (RE3) [Seo et al., 2021], Variational Intrinsic Control (VIC) [Gregor et al., 2017], Diversity Is All You Need (DIAYN) [Eysenbach et al., 2019], Variational Intrinsic Successor featuRes (VISR) [Hansen et al., 2020], Dynamics-Aware Discovery"}, {"title": "3 Constrained Intrinsic Motivation", "content": "In this section, we first present CIM for RFPT, a novel competence-based IM method that can learn dynamic and diverse skills efficiently. Specifically, we propose a constrained intrinsic objective $J^{CIM}$ for RFPT tasks, maximizing the conditional state entropy instead of the state entropy under a novel alignment constraint for the state representation. We then derive the corresponding intrinsic reward $r^{CIM}$ based on the Frank-Wolfe algorithm. Secondly, we propose CIM for"}, {"title": "3.1 Constrained Intrinsic Motivation for RFPT", "content": "In this section, we develop CIM for RFPT, a novel constrained intrinsic objective for unsupervised RL. To better clarify the motivation for the design of the constrained intrinsic objective, we first review current coverage- and mutual information-based methods and analyze their limitations.\nThough knowledge-based and data-based IM methods may perform well in terms of state coverage in certain RFPT tasks, these methods lack awareness of latent skill variables and suffer from poor fine-tuning efficiency. To improve the fine-tuning performance in RFPT tasks, learning a latent-conditioned policy is necessary. However, existing competence-based IM methods perform poorly regarding skill diversity, state coverage, and sample inefficiency. We conjecture that there are two main issues:\nIntrinsic objective. Maximizing only the mutual information is not suitable for dynamic and diverse skill discov-"}, {"title": "Design of Constrained Intrinsic Objective", "content": "To address the first issue, we propose choosing the conditional state entropy H($(s)|z) as the intrinsic objective. This is a key difference between our method and previous IM methods. Intuitively, by maximizing H($(s)|z), distances between adjacent states within the trajectories sampled by one skill are enlarged, which indicates a more dynamic skill.\nFor the second issue, we propose maximizing a novel lower bound of the mutual information between the state representation and the latent skill variable to make the trajectories sampled by different skills distinguishable, that is,\n$$I(\\phi(s); z) \\geq log N - \\mathcal{L}_a(\\phi(s), z),$$\n(3)\nwhere La($(s), z) is the alignment loss as follows:\n$$\\mathcal{L}_a(\\phi(s), z) = \\sum_i \\mathcal{L}^{ICIM}_i,$$\n(4)\n$$\\mathcal{L}^{ICIM}_i = -diff (\\tau_i)^Tz_i +$$\n$$\\log \\sum_{\\tau_j \\in \\mathcal{S} - \\mathcal{U}\\{\\tau_i\\}} exp (diff (\\tau_j))^Tz_i),\\$$\n$$diff(\\tau) = \\phi(s') - \\phi(s),$$\nN is the total number of samples for estimating the mutual information, \u03c4 = (s, s') is the slice of a trajectory, and S\u00af is a set of negative samples that contains trajectories sampled via skills other than zi. We derive this lower bound based on Contrastive Predictive Coding [Oord et al., 2018] by regarding the latent skill z as the context and diff (\u03c4) as the predictive coding. Based on Equation (3), the alignment constraint on the state encoder network $(s) is\n$$\\mathcal{L}_a(\\phi(s), z) \\leq C,$$\n(5)\nwhere C is a constant. Theoretically, as indicated in Table 1, C should represent the minimum of La($(s), z). In practice, we do not need to know the exact value of C. Instead, at each policy iteration step, we take several stochastic gradient descent steps on the alignment loss La($(s), z) to maximize the mutual information between the state representation $(s) and the latent skill z."}, {"title": "Estimation of Conditional State Entropy", "content": "We now introduce how to estimate the conditional state entropy H($(s) z) involved in Equation (6). Recall the definition of the conditional state entropy\n$$H(\\phi(s)|z) = E_{z\\sim p_z} [H(\\phi(s)|z = z)]$$\n$$ = E_{z\\sim p_z}E_{\\phi(s)\\sim d_{\\pi}} [-\\log d_{\\pi}(\\phi(s))] .$$\n(8)\nTo estimate the outer expectation, we randomly sample the latent skill variables z from a prior distribution pz(z). For discrete skills, pz(z) can be a categorical distribution Cat(K, p) that is parameterized by p over a size-K the sample space, where pi denotes the probability of the i-th skill. For continuous skills, we can select p(z) as a uniform distribution Unz (a, b) over the interval [a, b], where n\u2082 is the dimension of the skill.\nTo estimate the inner expectation, we roll out trajectories using the latent-conditioned policy \u03c0(\u00b7|s, z) with z fixed. During the sampling phase, z is randomly sampled at the beginning of each episode and remains fixed throughout the entire trajectory. We store the state-skill pair (s,z) in the replay buffer. During the training phase, for each pair (s,z), we concatenate them as [s, z] to be the input of the latent-conditioned policy \u03c0(\u00b7|s, z).\nTo estimate the state density d, instead of training a parameterized generative model, we leverage a more practical"}, {"title": "Lower bound of conditional state entropy", "content": "Each skill can be stochastic if we directly maximize the conditional state entropy H($(s)|z). To address this, we propose maximizing the lower bound of H(s|z) to encourage the skill z to produce large state variations along the direction of z in the latent space instead of being fully stochastic. To derive the lower bound of H(s|z), we first define a projection function gz($(s)) = $(s)Tz for a fixed skill z. It is easy to verify that H($(s)|z) \u2265 H(gz($(s))|z)) with equality iff Sc($(s), z) = 1, that is, H(gz($(s))|z)) is a lower bound of H($(s) z). We thus can maximize H(gz($(s))|z)) to maximize H($(s) z) and estimate the distribution of the one-dimensional random variable gz($(s)) for each z.\nIntrinsic reward. Based on the above design, we can derive the intrinsic reward of CIM for RFPT as rfIM(s) =\nlog ||gz($(s)) \u2013 gz($(s))||. Here, gz($(s)) means the \u03be-th nearest neighbor of gz($(s)). We adopted an average-distance version similar to APT to make training more stable:\n$$r^{CIM}(s) = \\log {1 + \\frac{1}{\\xi} \\sum_{j=1}^{\\xi} ||g_z(\\phi(s)) - g_z(\\phi(s)_j)|||}.$$\n(10)\nIntuitively, rfIM(s) measures how sparse the state s is in the projection subspace spanned by its corresponding latent skill z. This reward function can be justified based on the procedure of the Frank-Wolfe algorithm. Specifically, since $L^{RFPT}$ is concave in d\u33a2, maximizing $L^{RFPT}$ involves solving"}, {"title": "3.2 Constrained Intrinsic Motivation for EIM", "content": "In this section, we present our CIM for EIM, an adaptive coefficient for the intrinsic objective in Equation (2). Currently, IM methods for EIM tasks commonly use a constant coefficient or an exponentially decaying coefficient, which requires costly hyperparameter tunning. To avoid this, we propose reformulating Equation (2) by regarding the extrinsic objective as a constraint for the intrinsic objective, i.e.,\n$$\\max_{d_{\\pi} \\in K} J_I(d_{\\pi}), s.t. J_E(d_{\\pi}) \\geq R_k,$$\n(11)\nwhere Rk represents the expected reward at the k-th iteration of policy optimization. We approximate Rk via Rk = maxj\u2208{1,2,...,k\u22121} JE(d\u03c0j). We then leverage the Lagrangian method to solve Equation (11). The corresponding Lagrangian dual problem is min\u03bb>0 maxd J1(d) +\n\u03bbk(JE(d) \u2013 Rk). The Lagrangian multiplier \u5165 is updated by Stochastic Gradient Descent (SGD), that is, k = Ak\u22121\n\u03b7(JE(d\u03c0\u03ba) \u2013 Rk\u22121) where \u03b7 is the updating step size of \u5165k. Observing that Lk (d\u03c0, \u03bbk) \u221d JE(d\u33a2) + 1\u00b9 J\u2081(d\u33a2), the adaptive coefficient TCIM is then derived as\n$$\\tau^{CIM}_{k} = \\min\\{\\{\\lambda_{k-1} - \\eta(J_E(d_{\\pi_k}) - R_{k-1})\\}^{-1},1\\},$$\n(12)\nwhere the outer minimization is to ensure numerical stability. As the Lagrangian multiplier k grows, the penalty for violating TIN CIM gradually tends to zero; that is, the bias introduced by the intrinsic objective J\u2081 is adaptively reduced."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Experimental setup for RFPT. We evaluate our intrinsic bonus rfIM for RFPT tasks on four Gymnasium environments, including two locomotion environments (Ant and Humanoid) and two manipulation environments (FetchPush and FetchSlide). We compare CIM for RFPT with fifteen IM methods in Table 1, including 1) four knowledge-based IM methods: ICM, RND, Dis., MADE, and AGAC; 2) one databased IM method: APT; 3) and nine competence-based methods: DIAYN, VISR, DADS, APS, CIC, MOSS, BeCL, LSD, and CSD.\nTk\nExperimental setup for EIM. We evaluate our adaptive coefficient TCIM TO for EIM in two navigation tasks (PointMaze_UMaze and AntMaze_UMaze) in D4RL [Fu et al., 2020], and four sparse-reward tasks (SparseHalfCheetah, SparseAnt, SparseHumanoidStandup, and SparseGridWorld). TCIM is orthogonal with any intrinsic bonuses ri. Unless otherwise mentioned, we adopt the state-of-the-art data-based intrinsic bonus rAPT = log(1 + 1/k=1 ||$(8)-$(s)|). The total instant reward is then r = re + TCIMAPT. We compare CIM for EIM with three baseline coefficient schemes, i.e., the constant coefficient T = 1, the linearly decaying coefficient r = (1 \u2212 k/T), and the exponentially decaying coefficient T = 0.001k."}, {"title": "4.2 Results in RFPT Tasks", "content": "Visualization of skills. As previous works like LSD do, we train CIM for RFPT to learn diverse locomotion continuous skills in the Ant and Humanoid environment and diverse manipulation skills in FetchPush and FetchSlide. The learned skills are visualized as trajectories of the agent on the x y plane in Figure 1 and Figure 2. Our CIM for RFPT outperforms all 15 baselines in terms of skill diversity and state coverage. The skills learned via CIM are interpretable because of our alignment loss; the direction of the trajectory on the x y plane changes consistently with the change in the direction of the skill. Specifically, CIM excels at learning dynamic skills that move far from the initial location in almost all possible directions, while most baseline methods fail to discover such diverse and dynamic primitives. Their trajectories are non-directional or less dynamic than CIM, especially in two locomotion tasks. Competence-based approaches like DIAYN, VISR, and DADS directly maximize the mutual information objectives but learn to take static postures instead of dynamic skills; such a phenomenon is also reported in LSD and CIC. Although APS and CIC can learn dynamic skills by directly maximizing the state entropy, CIM discovers skills that reach farther and are more interpretable via maximizing the lower bound of the state entropy. As for the two variants of CIC, MOSS and BeCL, they perform even worse than CIC in all tasks, reflecting their limitation in skill discovery. Lastly, LSD and CSD cannot learn dynamic skills within limited environment steps in Ant and Humanoid due to their low sample efficiency. Though they perform better in manipulation tasks than locomotion tasks, their learned skills are rambling compared with our CIM."}, {"title": "State coverage", "content": "To make a quantitative comparison between various IM methods, we measure their state coverage. The state coverage in Ant and Humanoid is determined by calculating the number of 2.5 \u00d7 2.5 m\u00b2 bins occupied on the x-y plane, based on 1000 randomly sampled trajectories. This was then averaged over five runs. For FetchPush and FetchSlide, we use smaller bins. As shown in Table 2, CIM significantly outperforms all the baseline methods in two torqueas-input locomotion tasks and is comparable in two positionas-input manipulation tasks. Although the state coverage of CIM is slightly lower than APT and CIC in FetchPush and FetchSlide, the skills learned via CIM are more interpretable, as shown in Figure 2."}, {"title": "Fine-tuning efficiency in URLB", "content": "We also evaluate CIM for RFPT in URLB, a benchmark environment for RFPT in terms of fine-tuning efficiency. The results are presented in Table 3. The score (the last line of the table) is standardized by the performance of the expert DDPG, the same as in URLB and CIC. CIM performs better in Run and Walk tasks and achieves the highest average score. The dynamic skills learned through CIM for RFPT can be adapted quickly to diverse fine-tuning tasks, including flipping and standing. Our experiments also show that the skill dimension nz = 3 is better for CIM to discover flipping skills than n\u2082 = 2. The fixed skill selection mechanism for CIM is the same as CIC."}, {"title": "4.3 Results in EIM Tasks", "content": "In PointMaze, we directly train a policy to control the Point without learning low skills since the environment dynamics are simple. In AntMaze, we train a meta-controller on top of the latent-conditioned policy pre-trained via our CIM for RFPT method. The meta-controller observes the target goal concatenated to the state observation [s; sg] and outputs the skill latent variable z at each timestep. We visualize the trajectories of the Ant in the x y plane as shown in Figure 3b, where the skills in a single trajectory gradually change to make the Ant turn a corner. Figure 3c shows that the Lagrangian-based adaptive coefficient TIM outperforms three baseline coefficients, especially in PointMaze. Specifically, we can observe a small peak in the early stage of the training in PointMaze, which means the agent can reach the randomly generated target point with a small probability at the beginning. However, as the training processes, the agent is distracted by the intrinsic bonuses when using a trivial coefficient T or T. Moreover, other latent-conditioned policies are of poor quality, and we fail to train a mete-controller on top of those policies. We also conduct experiments to demonstrate the performance of CIM for EIM across four sparsetk-reward locomotion tasks. The results in Table 6 indicate that TIM can effectively reduce the bias introduced by intrinsic rewards, thereby enhancing test-time average episode rewards in EIM tasks."}, {"title": "5 Conclusion", "content": "In this paper, we proposed Constrained Intrinsic Motivation (CIM) for RFPT and EIM tasks, respectively. For RFPT tasks, we designed a novel constrained intrinsic objective to discover dynamic and diverse skills. For EIM tasks, we designed an adaptive coefficient TCIM for the intrinsic objective based on constrained policy optimization. Our experiments demonstrated that CIM for RFPT outperformed all fifteen baselines across various MuJoCo environments regarding diversity, state coverage, sample efficiency, and fine-tuning performance. The latent-conditioned policy learned via CIM for RFPT was successfully applied to solve complex EIM tasks via training a meta-controller on top of it. We also empirically verified the effectiveness of our adaptive coefficient TCIM in multiple EIM tasks."}]}