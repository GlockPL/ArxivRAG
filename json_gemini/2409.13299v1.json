{"title": "OMG-RL: Offline Model-based Guided Reward Learning for Heparin Treatment", "authors": ["Yooseok Lim", "Sujee Lee"], "abstract": "Accurate diagnosis of individual patient conditions and appropriate medication dosing strategies are core elements of personalized medical decision-making processes. This therapeutic procedure, which entails recursively assessing the patient's condition and administering suitable medications, can effectively be modeled as a reinforcement learning (RL) problem. Crucially, the success of RL in this context depends on the establishment of a well-defined reward function that accurately represents the optimal treatment strategy. However, defining the learning direction in RL with only a limited set of explicit indicators complicates the task due to the inherent complexity of the required domain knowledge. This approach may also increase the likelihood that the RL policy does not adequately reflect the clinician's treatment intentions, which are determined by considering various situations and indicators.\nIn this study, we focus on developing a reward function that reflects the clinician's intentions and introduce Offline Model-based Guided Reward Learning (OMG-RL), which performs offline inverse reinforcement learning (IRL) aligned with the offline RL environment. Through OMG-RL, we learn a parameterized reward function that includes the expert's intentions from limited data, thereby enhancing the agent's policy. We validate the proposed approach on the heparin dosing task. The results demonstrate that policy learning through OMG-RL is meaningful and confirm that the learned policy is positively reinforced in terms of activated partial thromboplastin time (aPTT), a key indicator for monitoring the effects of heparin. This approach can be broadly utilized not only for the heparin dosing problem but also for RL-based medication dosing tasks in general.", "sections": [{"title": "I. INTRODUCTION", "content": "MEDICATION dosing is a crucial component of the patient treatment process. For instance, anticoagulants such as heparin and warfarin are widely used to prevent thrombosis [1]\u2013[4], while propofol is administered to maintain stable conditions in anesthetized patients during surgical procedures [5], [6]. Precise chemotherapy is also vital for cancer patients [7]. In medication dosing, key monitoring indicators play an essential role in ensuring appropriate dosage levels. For example, aPTT is used to adjust heparin, bispectral index and effect-side concentration guide anesthesia dosing, and cholesterol levels determine statin dosages [3]-[5], [8]. These indicators are vital in guiding clinicians to administer medications accurately.\nHowever, clinicians also consider emergency situations, comorbidities, genetic factors, and concurrent medications [3], [4]. Heparin dosing guidelines, for instance, vary de-pending on the patient's specific condition, such as venous thromboembolism versus coronary artery disease [3]. This reflects the complexity of determining appropriate medication dosages, which must consider various patient-specific factors and history. Therefore, a key objective in medication dosing is to derive logical dosages that encompasses multiple indi-cators. Implementing this comprehensive approach in dosing algorithms represents a significant advancement in the field.\nReinforcement learning (RL) provides a framework to de-rive personalized treatment policies by considering individual patient characteristics [9]. Recently, RL has been applied to various medication dosing issues [10]\u2013[13], with offline RL techniques becoming particularly notable for their effective-ness in settings where creating simulation environments is challenging [10]-[13]. Notably, Xihe et al. [12] used batch constrained Q-learning (BCQ) to optimize heparin dosing policies, while Smith et al. [13] demonstrated the utility of conservative Q-Learning (CQL) in applying RL based on patient group characteristics.\nIn such studies that utilize RL approaches, the precise definition of the Markov decision process (MDP) is essential for effective problem-solving. Particularly, the reward function is a crucial element of the MDP in that it determines the learning direction of the target policy. Although previous approaches have often relied on specific clinical indicators such as aPTT to define rewards [12]\u2013[15], it is clear that clinicians' decision-making processes consider a broader range of factors. This understanding indicates that reward functions in RL must be defined to reflect a more comprehensive set of variables. By incorporating diverse indicators that encompass both clinical and patient-specific factors, reward functions can better align with the complex decision-making processes of medical experts, thereby enhancing the efficacy and applica-bility of RL-based treatment strategies.\nConsidering these aspects, we adopt an inverse reinforce-ment learning (IRL) approach in this study [16]\u2013[18]. IRL, a category of imitation learning, learns a parameterized reward"}, {"title": "II. RELATED WORK", "content": "The application of artificial intelligence in clinical decision-making, particularly in medication dosing, has significantly advanced, with RL emerging as a crucial technology. RL has enhanced patient outcomes and improved treatment effi-ciency across various medical conditions by providing tailored treatment strategies. Despite these advancements, the field of heparin dosing remains a complex challenge that has seen concentrated efforts from researchers seeking to optimize dosing policies using limited data through sophisticated RL techniques. Although these studies have made substantial contributions, there remains a gap in effectively integrating these advanced methodologies into everyday clinical practice, indicating the need for further innovation and refinement in RL applications for medication dosing [12]\u2013[15].\nOffline RL operates by utilizing finite batch data to perform reinforcement learning without the need for a simulation environment. It proves particularly effective in domains where simulating an environment is challenging [23]. In the offline model-free approach, optimal policies are learned solely from batch data. Fujimoto et al. [24] developed BCQ, which en-hances the deep Q-learning framework. BCQ minimizes errors from distribution discrepancies by favoring actions that are both high-value and similar to those within the batch data. Following this, various researchers have aimed to stabilize the Q-function by incorporating uncertainty quantification techniques, such as ensembles [25]\u2013[27]. Jaques et al. [28] subtly integrate user preferences into Q-function learning, ensuring the resultant policy more closely aligns with actual behavior. Kumar et al. [29] introduced CQL, which controls overestimations in state values by setting a conservative lower bound on the values of actions taken outside the observed data distribution, thus reducing learning bias. Additionally, CQL enhances this lower bound by incorporating a term that maximizes the value of actions consistent with the data distribution. Kostrikov et al. [30] refine policy improvement by treating the value function as a probabilistic variable and estimating the best action values through state-conditional upper bounds, rather than relying on the most recent policy to evaluate unseen actions. This extensive research continues to refine and optimize RL for offline environments.\nThe offline model-based RL approach introduces a dynamic model to extend learning capabilities to the entire state-action space beyond the provided batch data, thereby enhancing the generalization of RL policies. This method utilizes supervised learning and generative modeling techniques as alternative strategies for policy learning. These techniques are especially useful in studies modeling complex, high-dimensional states such as those found in vision applications [31]. Yu et al. [32] proposed model-based offline policy optimization (MOPO), which effectively adapts the model-based approach for offline use. MOPO employs a dynamic model in a dyna-style configu-ration [33], quantifying the uncertainties encountered in data-limited scenarios to adjust the reward structure accordingly. RL policies trained with these adjusted rewards exhibit ro-bustness against out-of-distribution states and actions. Further extending this approach, Yu et al. [34] introduced conservative offline model-based policy optimization (COMBO), which integrates CQL with MOPO to conservatively estimate the Q-function. COMBO penalizes out-of-distribution states gener-ated during dynamic model simulations (rollouts), thus lever-aging the generalization advantages of model-based algorithms without the limitations imposed by uncertainty quantification. In this study, we adopt a model-based RL strategy that expands upon COMBO to take advantage of on supervised learning models and perform rollouts in offline settings, enhancing both the adaptability and efficacy of our approach.\nIRL is a methodology that derives reward functions using expert trajectories [16]\u2013[18]. An expert trajectory, character-ized by its demonstration of suboptimal yet effective outcomes, represents an experience of a policy that has satisfactorily achieved the problem's objective from a reinforcement learn-"}, {"title": "III. BACKGROUND", "content": "RL provides a framework for solving sequential decision-making problems, with the MDP serving as a fundamental problem definition in RL. An MDP is defined as a tuple (S, A, r, P, \u03b3), where S represents the set of states, A the set of actions, P the state transition probability $P(s_{t+1} = s'|S_t = s, a_t = a)$, r : S \u00d7 A \u2192 R the reward function, and \u03b3\u2208 (0,1) the discount factor. The goal of an RL agent is to discover a policy \u03c0: S \u00d7 A \u2192 (0,1) that maximizes the cumulative reward $\\sum_{t=0} \\gamma^t r_t$, utilizing the trajectory \u03c4 = $(s_t, a_t,r_t)_{t=0}^{T}$ .\nMaximum entropy IRL aims to learn reward functions from expert demonstrations, utilizing an optimality variable O to measure the effectiveness of a trajectory. The variable $O_t$ serves as an indicator to evaluate optimality at state $s_t$ and action $a_t$ at time step t, with the conditional probability defined as $P(O_t|s_t,a_t) = exp(r(s_t,a_t))$. Thus, the optimality of a trajectory adheres to the following equation:\n$\u0440(\u0442|0_{1:T}) = \\frac{\u0440(\u0442, 0_{1:T})}{P(0_{1:T})} \\propto p(\u03c4) \\prod_{t=1}^{T} exp(r(s_t, a_t)) $  (1)\nGiven a set of trajectories {Ti} sampled from a policy \u03c0* and a reward function ry, the reward function can be optimized by learning \u03c8 in a direction that increases the likelihood"}, {"title": "IV. METHODS", "content": "In this section, we outline OMG-RL, our approach for learning medication dosing policies. Initially, we construct a dynamic model, a supervised learning-based model that describes patient state transitions and facilitates agent roll-outs. Next, we undertake conservative policy evaluation and improvement to correct for state transition estimation errors from the dynamic model. Lastly, we guide the reward function to increase the entropy of the expert data and simultaneously perform reward function learning and policy updates within a singular learning loop.\nThe dynamic model, denoted as \u00ce, is an ensembel model that takes state-action pairs (st, at) as inputs and predicts the subsequent state and reward (st+1,rt). In model-based RL, using ensemble methods to construct T significantly enhances performance [32]. In this study, we train seven neural networks composed of fully connected layers and select the five with the highest performance metrics for RL learning. The networks are trained using maximum likelihood estimation as follows:\n$L = E_{(s,a,s',r)\\sim D}[log\\hat{T}(s', r|s, a)]$(4)\nEmploying a dyna-style approach [33], we integrate the dynamic model into RL. This technique uses augmented data for policy evaluation between iterative learning cycles. Data from original batch Dbatch and data obtained through rollouts using the current policy Dsample are employed. In each iteration, an initial state s is randomly selected from\nHere, \u03b1 acts as the trade-off factor between the regulariza-tion term and the policy loss function. $B^{\\pi}$ is the Bellman operator, ensuring that updates adhere to the principles of dynamic programming. The distribution p(s, a), sampled from \u03c0 using the rollout policy, approximates the discounted marginal state distribution $d^{\\pi} (s)\u03c0(\u03b1|s)$. For state-action pairs from p(s, a) that are out-of-distribution, we minimize their Q-function values to prevent overfitting to potentially erroneous data. Conversely, for state-action pairs from the trusted data in Dbatch, we maximize their Q-function values, reinforcing the reliability of learned policies.\nThe distribution df, which includes both Dbatch and Dsample, balances the data used for updating Q-values ac-cording to a specified ratio f, integrating both empirical and simulated data for a comprehensive policy evaluation."}, {"title": "V. EXPERIMENT AND RESULT", "content": "We validated our proposed approach in a heparin dosing environment. This section describes the construction of this environment and analyzes the results obtained."}, {"title": "VI. CONCLUSION", "content": "In this study, we have departed from conventional methods that typically rely on clinical knowledge to define reward functions. Instead, we implemented IRL, which derives reward estimations directly from data, to inform the learning of RL policies. Our data source, the MIMIC-III electronic medical records, served as a foundation for this analysis, with specific attention to preprocessing heparin administration records for the purpose of model validation. This led to the development of the Offline Model-based Guided Reward Learning (OMG-RL) model, designed to facilitate IRL within an offline frame-work.\nThroughout various experiments, our approach demon-strated significant validity. Particularly, we noted that the behavior of ry closely mirrored rp, established through clinical insights, underscoring the effectiveness of IRL in capturing complex decision-making processes inherent in medical treat-ment. This finding supports the potential of IRL-based policy learning to reach substantial performance benchmarks, com-parable to those grounded in traditional domain expertise. We anticipate that our methodology will offer a robust alternative for managing medication dosing tasks and potentially influ-ence other areas where reward definition poses a substantial challenge.\nHowever, the study is not without limitations. The discrete nature of the action space used in our experiments stands in contrast to the continuous variables typically encountered in medication dosing, suggesting a need for models that accom-modate continuous action frameworks to enhance predictive accuracy. Furthermore, while our theoretical approach has been substantiated through methodological rigor, additional empirical validation is necessary across broader demographic and geographic patient data sets to ensure generalizability and applicability in real-world clinical settings. Future research will aim to refine our reinforcement learning framework to address these challenges, enhancing its practical relevance and efficacy."}]}