{"title": "Align LLaVA: Cascaded Human and Large Language Model Preference Alignment for Multi-modal Instruction Curation", "authors": ["Hongzhe Huang", "Zhewen Yu", "Jiang Liu", "Li Cai", "Dian Jiao", "Wenqiao Zhang", "Siliang Tang", "Juncheng Li", "Hao Jiang", "Haoyuan Li", "Yueting Zhuang"], "abstract": "Recent advances in Multi-modal Large Language Models (MLLMs), such as LLaVA-series models, are driven by massive machine-generated instruction-following data tuning. Such automatic instruction collection pipelines, however, inadvertently introduce significant variability in data quality. This paper introduces a novel instruction curation algorithm, derived from two unique perspectives, human and LLM preference alignment, to compress this vast corpus of machine-generated multimodal instructions to a compact and high-quality form: (i) For human preference alignment, we have collected a machine-generated multimodal instruction dataset and established a comprehensive set of both subjective and objective criteria to guide the data quality assessment critically from human experts. By doing so, a reward model was trained on the annotated dataset to internalize the nuanced human understanding of instruction alignment. (ii) For LLM preference alignment, given the instruction selected by the reward model, we propose leveraging the inner LLM used in MLLM to align the writing style of visual instructions with that of the inner LLM itself, resulting in LLM-aligned instruction improvement. Extensive experiments demonstrate that we can maintain or even improve model performance by compressing synthetic multimodal instructions by up to 90%. Impressively, by aggressively reducing the total training sample size from 158k to 14k (9\u00d7 smaller), our model consistently outperforms its full-size dataset counterpart across various MLLM benchmarks. Our project is available at https://github.com/DCDmllm/Align2LLaVA.", "sections": [{"title": "1 Introduction", "content": "Recent triumphs in artificial intelligence have been driven notably by the development of large language models (LLMs) (Achiam et al. 2023; Team et al. 2023; Dubey et al. 2024). Inspired by the impressive instruction-following capability of LLMs, visual instruction tuning (Liu et al. 2023b) has been proposed to extend LLMs into Multimodal LLMs (MLLMs) to perceive and understand visual signals (e.g., images). Due to the scarcity of visual-language instruction-following data, contemporary MLLMs prefer to utilize the data reformation approach (Liu et al. 2023b; Zhang et al. 2024b), which leverages text-only LLMs conditioned on image captions and bounding boxes to create synthetic multi-modal instructions involving visual content. Following the evolution, other works such as ALLaVA (Chen et al. 2024) and VIGC (Wang et al. 2024), have extended this automatic paradigm by utilizing visually-capable LLMs, demonstrating the potential value of utilizing synthetic instruction generation.\nDespite their efforts and achievements, the simply synthetic instruction generation paradigm may result in suboptimal multimodal instruction following capability, which can be summarized as two key aspects: (i) Inherently Noisy Instructions: The generated questions and corresponding responses of instructions may be incongruent or incomplete with the visual content, as they are produced by text-only large language models. Regarding question generation, common issues include irrelevance to the image, independence from visual cues, and repetitive queries in multi-turn dialogues. On the other hand, the corresponding responses may suffer from textual hallucinations or ignored visual details, as they are produced by text-only large language models. Figure 1(a) shows an example of the synthetic instructions. Of the two LLM-generated questions, the previous one demonstrates superior quality by incorporating queries relevant to image content, and providing subtle prompts to stimulate reasoning without revealing specific visual information. Conversely, the subsequent question lacks pertinence to the image, as it cannot be accurately answered based on the visual data provided. Moreover, comparing the two responses to the first question, the first one is notably deficient in visual details, as highlighted in the upper part of Figure 1(b). Such details are crucial for comprehensive and persuasive analysis. These inappropriate questions and responses inevitably compromise the accurately visual-language alignments of the MLLMs. (ii) Internally Linguistic Gap: We refer the linguistic characteristic to the specific habits of vocabulary selection, grammar usage and sentence structuring used to express particular semantics. We argue that well-trained LLMs have unique writing style preferences, which are expressed in the output probabilities of candidate tokens when generating new tokens. In Figure 1(b), we present an instance of sentence writing manner gap. A comparison between the original instruction (top) and the LLM-revised instruction (bottom) reveals subtle stylistic variations, including word choice, sentence structure, and the degree of complexity in expressing the same idea. During the visual instruction tuning phase, the writing manner gap forces the LLM to change its original writing style, leading to perfor-"}, {"title": "2 Related Work", "content": "Learning from Human Knowledge The integration of human knowledge is becoming increasingly pivotal in aligning model behavior with human intent (Lee et al. 2023; Nakano et al. 2021; Wu et al. 2023; Xu et al. 2024; Zhang et al. 2024a; Yu et al. 2024; Zhang et al. 2023, 2022, 2024c). A crucial aspect is training a reward model (MacGlashan et al. 2017) to directly capture human preferences for the outputs generated by the model. Recently, researchers have proposed to use reinforcement learning (Schulman et al. 2017) to fine-tune language models (Ouyang et al. 2022) and diffusion models (Lee et al. 2023; Xu et al. 2024; Zhang et al. 2024a) utilizing the signals from the reward model. Additionally, there are works that leverage human knowledge to directly enhance the quality of datasets (Yu et al. 2024; Zhang et al. 2023). These works integrate human knowledge with image-text data to mitigate visual and textual biases present in large-scale image-text datasets.\nInstruction Data Selection Recent researches show that instruction tuning could enable large language models (LLMs) to be tailored to specific domains, tasks, or applications by providing explicit instructions or guidelines (Wei et al. 2021). In order to enhance the instruction-following abilities of LLMs, previous works mainly focus on increasing the data sizes through various strategies (Honovich et al. 2022; Wang et al. 2022; Taori et al. 2023; K\u00f6pf et al. 2024). However, LIMA (Zhou et al. 2024) illustrates that even a small number of constructed high-quality instructions could empower the model with a powerful instruction-following capability.\nInspired by LIMA (Zhou et al. 2024), Instruction Mining (Cao, Kang, and Sun 2023) adopts a linear quality rule and bag of indicators to evaluate the quality of instruction-following data. AlpaGasus (Chen et al. 2023c) directly leverages an external LLM (ChatGPT) to score each instruction and then selects 9K Alpaca data with a threshold. While their model surpasses the performance of the official Alpaca model which is trained on the complete dataset, this approach may neglect the intrinsic abilities of the base model, relying excessively on external models. Li et al. (2023a) present a self-guided approach for LLMs to independently identify and choose relevant instruction pairs from extensive open-source datasets and in their approach they introduce an Instruction-Following Difficulty (IFD) metric as a tool to identify gaps in a model's responses versus its autonomous generation capability. MoDS (Du, Zong, and Zhang 2023) focuses on instruction selection through three criteria: quality (instructional data fidelity), coverage (variety of instruction types), and necessity (impact of instruction on LLM fine-tuning)."}, {"title": "3 Method", "content": "Our visual instruction curation pipeline comprises three sequential steps, as depicted in Figure 2. Initially, two distinct datasets, enriched with human preferences, are constructed focusing on the question and answer components of instructions, respectively (Section 3.1). Leveraging these human-ranked datasets, two separate reward models are trained to align with human values, and subsequently serve as human-like evaluators to filter large-scale synthetic visual instruction data through a two-stage process (Section 3.2). Finally, an inner LLM is employed to refine the selected instructions in a rewrite-review manner, ensuring alignment with the LLM's writing style while preserving the original semantic content (Section 3.3).\n3.1 Human Preferences Collection\nThe initial step of our pipeline involves the curation of two distinct visual instruction datasets, each accompanied by corresponding human preference rankings. As depicted in Step 1 of Figure 2, these datasets prioritize either the question or answer component of the instructions. The data collection process unfolds in two sequential phases: visual instruction generation and human preference annotation. The anticipated outcome is a question dataset ranked by comprehensiveness and visual relevance, as well as an answer dataset evaluated based on accuracy and answer alignment with the provided image and question.\nVisual Instruction Generation To initiate our pipeline, a seed dataset of 960 images is randomly selected from the LLaVA-instruct-158K (Liu et al. 2023b) dataset. Each image is paired with exactly one group of instructions generated by GPT-4 (Achiam et al. 2023). In alignment with LLaVA, these instructions are classified into three categories: detail description, complex reasoning, and conversation. Two notable characteristics of this categorization exert a subtle influence on our filtration strategy. (i) Multi-turn conversations. The conversation category uniquely incorporates multiple question-answer pairs within a single instance, in contrast to the singular structure of the remaining categories. This necessitates specialized treatments during the filtration process, which will be addressed in the appendix. (ii) Ignorance of detail description questions. The questions of detail description instructions are randomly sampled from"}, {"title": "3.2 Human Knowledge Alignment", "content": "To align large-scale visual instruction data with human knowledge, we employ a human-preference-based filtration process utilizing curated datasets enriched with human feedback. Initially, two distinct reward models are trained separately on question and answer datasets to learn human preferences for these respective components. Subsequently, these trained reward models function as human-like evaluators to filter large-scale synthetic dataset across two sequential stages.\nReward Model Training To provide a multi-dimensional assessment on visual instructions, we propose to train two distinct reward models on the human-ranked datasets, dedicated to evaluating the question and answer parts, respectively. By aligning these models with human preferences, we aim to cultivate a in-depth understanding of complex semantics and subtle nuances within instructions.\nFollowing InstructGPT (Ouyang et al. 2022), we structure human preferences as pairwise comparisons, and adopt a pairwise ranking loss for model training. Specifically, within the question dataset, each pair of questions associated with the same image are compared, resulting in a preference pair indicated by $(I, Q_i, Q_j)$ if $Q_i$ is superior to $Q_j$. Similarly, the answer dataset is organized as preference pairs $(I, Q_0, A_i, A_j)$, where $A_i$ is preferred over $A_j$ within the context of image $I$ and seed question $Q_0$. This yields at most $C$ question preference pairs and $\\tau$ answer preference pairs for each image.\nSubsequently, two reward models are independently trained on question and answer preference pairs, with loss functions defined as:\n$loss_Q(\\theta_Q) = -E_{(I,Q_i,Q_j)\\sim D_{HF}}[log(\\sigma($\n$\\frac{f_{\\theta_Q}(I, Q_i) - f_{\\theta_Q}(I, Q_j)}{2}))]$, (1)\n$loss_A(\\theta_A) = -E_{(I,Q_0,A_i,A_j)\\sim D_{HF}}[log(\\sigma($\n$\\frac{g_{\\theta_A}(I, Q_0, A_i) - g_{\\theta_A}(I, Q_0, A_j)}{2}))]$, (2)\nwhere $f$ is the question reward model parameterized by $\\theta_Q$ that takes an image $I$ and a question $Q$ as input, and $g$ is the answer reward model parameterized by $\\theta_A$, provided with an extra answer $A$. Both $f_{\\theta_Q}(I, Q)$ and $g_{\\theta_A}(I, Q, A)$ are scalar scores for given instructions. $\\sigma$ is the sigmoid function, and $D_{HF}^Q$, $D_{HF}^A$ are the question and answer dataset, respectively.\nInstruction Data Filtration Leveraging the reward models well-aligned with human preferences, we implement a two-stage filtration on large-scale datasets, acquiring a substantially smaller yet high-quality dataset. Initially, a visual instruction dataset is synthesized, with each image accompanied by one question and three answers, formally represented as $D = \\{(I_n, Q_n, A_n^1, A_n^2, A_n^3)\\}_{n \\in N}$, where $N = \\{1, ..., n_0\\}$ is the index set of the full dataset.\nIn the first filtration stage, we rank all instances in $D$ based on their question quality using the question reward model $f_{\\theta_Q}$, defined as $r_n^Q = f_{\\theta_Q}(I_n, Q_n)$. Retaining the top $\\alpha\\%$ of instances yields dataset $D' = \\{(I_{n'}', Q_{n'}', A_{n'}^1, A_{n'}^2, A_{n'}^3)\\}_{n' \\in N'}$, where $N'$ is a subset of $N$, with size $|N'| = \\alpha\\% \\cdot |N|$. The second stage concentrates on evaluating answer quality. For each image in $D'$, the answer reward model $g_{\\theta_A}$ assigns scores to the three answer options, computed as $r_{n'}^i = g_{\\theta_A}(I_{n'}', Q_{n'}', A_{n'}^i)$ for $i \\in \\{1, 2, 3\\}$. The answer with the highest score, denoted as $A_{n'}^*$, is selected for each image. Subsequently, these selected image-question-answer triplets undergo a ranking process analogous to the first stage, where the top $\\beta\\%$ instances"}, {"title": "3.3 LLM Characteristic Alignment", "content": "Inner LLM Rewrite To harmonize instructions with the writing style of the inner LLM, we finally introduce an LLM alignment step. This alignment comprises two sequential phases: rewriting and review. During the rewriting phase, the inner LLM is tasked with modifying both question and answer components to conform to its characteristic writing style while maintaining semantic equivalence. Accompanying each revised instruction is an explanatory note detailing the modifications, thereby enhancing the process's robustness and interpretability. Crucially, the LLM is instructed to maintain the original phrasing if it already adheres to the LLM's stylistic expectations, mitigating the risk of compromising high-quality instructions.\nInner LLM Review In the subsequent review phase, both the original and revised instructions are presented to the LLM. The model is prompted to determine whether the revised content more closely aligns with its writing style while satisfying the semantics preserving demand. This serves as an extra stage of quality assurance. To encourage logical decision-making, the LLM is also required to provide a rationale for its selection. By refining instruction formats to better suit the target MLLM, this alignment process ultimately enhances training efficiency."}, {"title": "4 Experiments", "content": "4.1 Setting and Benchmark\nExperimental Setting For the synthetic visual instruction set, we first generated instructions for 158K images from the MSCOCO (Lin et al. 2015) dataset, using CogVLM-17B (Wang et al. 2023) with 4-bit quantization. The effectiveness of our curation pipeline was assessed by applying it to this synthetic data with a 30% sampling rate for both questions and answers, resulting in a reduced dataset constituting 9% of the original. This aligned instruction dataset, combined with the image-caption and short VQA data from the LLaVA-1.5-665K (Liu et al. 2023a) dataset, was subsequently used to fine-tune the LLaVA-1.5 (Liu et al. 2023a) model, initialized with weights from the previous stage pre-trained on caption datasets. We compared the resulting model against the baseline of LLaVA-1.5 fine-tuned on the entire synthetic instruction dataset with caption and visual question answering (VQA) data, as well as other state-of-the-art methods.\nBenchmarks This evaluation suite comprises academic VQA benchmarks, including VQAv2 (Goyal et al. 2017),"}, {"title": "4.2 Overall Performance", "content": "Table 1 presents the main experiment results. Our method outperforms the baselines on the majority of benchmarks while achieving highly competitive results on the remaining ones. A key finding is the significant reduction in synthetic instruction data required for our model to achieve such strong performance. Specifically, our model was fine-tuned on only 9% of the synthetic instruction data compared to the full-data baseline using the same model architecture. This outcome strongly suggests that our proposed alignment pipeline not only effectively compresses data size, but also enhances data quality and training efficiency."}, {"title": "4.3 Ablation Studies", "content": "Ablation on Human Alignment To investigate the impact of human knowledge alignment, we conducted an ablation study comparing our human-preference-based data filtration method to a random sampling baseline. The results, presented in Table 2, demonstrate the consistent superiority of our proposed method. Notably, the performance gains are particularly pronounced on instruction-following benchmarks. For instance, on MME (Fu et al. 2023), we observe a significant improvement from 1405.3 to 1448.0, while on LLaVA-Bench-In-the-Wild (Liu et al. 2023b), the score rises from 41.6 to 50.2. These substantial gains strongly suggest that aligning data with human preferences is a critical factor in enhancing the model's capacity to generate responses that closely correspond to queries and visual context. This alignment ultimately contributes to the production of human-aligned outputs that effectively fulfill user intents."}, {"title": "4.4 In-Depth Analysis", "content": "We also conducted a series of experiments to further analyze the effectiveness of Align\u00b2LLaVA."}, {"title": "5 Conclusion", "content": "This paper contributes to the advancement of high-quality visual instruction synthesis by introducing a novel filtration pipeline that significantly reduces dataset size. Our pipeline sequentially refines instruction quality through alignment with human preferences and large language model (LLM) writing style. By applying this pipeline to synthetic data, we demonstrate that the resulting substantially smaller dataset yields performance on par with the full instruction dataset when fine-tuning multimodal large language models (MLLMs). Our results highlight the potential for improving instruction quality via human-in-the-loop refinement and offer a path towards more efficient MLLM training."}, {"title": "A Training Details", "content": "For the balance between speed and precision, all models are trained with BF16 and TF32 enabled, using LoRA with r = 128 and $\\alpha$ = 256. Model parameters are updated using AdamW optimizer with $\\beta_1$ = 0.9, $\\beta_2$ = 0.999, $\\epsilon$ = 1\u0435 - 8 and weight decay 0. For all models, learning rate is set to $2e^{-5}$, with warmup ratio 0.03 and cosine decay schedule. Reward models are trained on 4 x RTX 3090s for 5 epochs, using global batch size 64, spending 8 hours for the question model and 9 hours for the answer model. LLaVA-1.5 instruction fine-tuning is completed on 8 \u00d7 A100s for 1 epoch with global batch size 128. The training takes 5.5/9.5 hours (7B/13B models) on our aligned dataset and 7/12 hours (7B/13B models) on the full dataset."}, {"title": "B Detailed Guidelines for Human Annotation", "content": "Question Annotation Guidelines:\n\u2022 Correctness. Questions are evaluated for their alignment with both image content and general world knowledge. Questions containing inaccuracies, contradictions, or illogical statements are penalized.\n\u2022 Fluency. Questions are assessed for grammatical correctness, clarity, and coherence. Ambiguous or poorly structured questions are unacceptable.\n\u2022 Relevance. Questions are expected to be highly dependent on the visual information provided in the image. Questions easily answered without image context or those that could be directly inferred from general knowledge receive lower scores. Moreover, questions that inadvertently revealed image details were penalized. To ensure question solvability, answers are required to be definitively obtainable from the image, in conjunction with real-world knowledge.\n\u2022 Question Distribution (for multi-turn conversation). In multi-turn dialogues, questions are additionally evaluated based on their diversity in difficulty and subject matter. Ideally, questions should progress from simpler to more complex, covering a broad range of image elements. Repetitive or redundant questions are considered suboptimal."}, {"title": "C Adaptation to Multi-Turn Conversations", "content": "To accommodate the conversation type instructions with multi-turn question-answer pairs, adaptations are necessary. Within each example (I, Q0, A0) from the seed dataset, the question and answer components can be decomposed into sequences: $Q_0 = \\{q_{0,1},\u2026\u2026,q_{0,t}\\}$ and $A_0 = \\{a_{0,1},..., a_{0,t}\\}$, where $(q_{0,i}, a_{0,i})$, $i \\in \\{1,...,t\\}$ represents a single-turn question-answer pair, and the number of turns is determined by the question sequence.\nOur approach involves a hybrid strategy: evaluating multi-turn questions holistically, while assessing answers on a per-turn basis. This distinction is grounded in the disparate criteria for these two components. Given the inherent interconnectedness and potential redundancies within multi-turn question sequences, a comprehensive evaluation is essential to capture their overall quality. Conversely, the standards for answer quality are predominantly determined on the current question and image, rendering contextual factors less critical. Consequently, isolating answer evaluation to the current turn optimizes computational efficiency and prevents the introduction of irrelevant information.\nSpecifically, for the question component, human evaluators assess question groups holistically within the same image, and the question reward model also processes entire question groups as input, with individual questions concatenated. For multi-turn answers, each t-turn example (I, Qo, A0, A1, ..., A\u2081) is decomposed into t single-turn examples (I, q0,1, a0,1, ..., a\u03b9,1), ..., (I, q0,t, a0,t, ..., a\u03b9,t) for subsequent processing, which mirrors the single-turn pipeline. The sole deviation occurs in the second filtration stage of human alignment, where the optimal answer is independently selected for each turn, and their average score serve as the overall answer quality metric for the multi-turn instance."}, {"title": "D Prompts", "content": "For question generation using text-only GPT4, we directly use the same prompts as LLaVA, and discard the generated answer parts. For answer generation, we adjust the prompts and provide the questions to ensure the model only produces answers. Moreover, since visual-capable GPT-4V"}]}