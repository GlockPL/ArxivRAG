{"title": "A Survey of Models for Cognitive Diagnosis: New Developments and Future Directions", "authors": ["FEI WANG", "WEIBO GAO", "QI LIU", "JIATONG LI", "GUANHAO ZHAO", "ZHENG ZHANG", "ZHENYA HUANG", "MENGXIAO ZHU", "SHIJIN WANG", "WEI TONG", "ENHONG CHEN"], "abstract": "Cognitive diagnosis has been developed for decades as an effective measurement tool to evaluate human cognitive status such as ability level and knowledge mastery. It has been applied to a wide range of fields including education, sport, psychological diagnosis, etc. By providing better awareness of cognitive status, it can serve as the basis for personalized services such as well-designed medical treatment, teaching strategy and vocational training. This paper aims to provide a survey of current models for cognitive diagnosis, with more attention on new developments using machine learning-based methods. By comparing the model structures, parameter estimation algorithms, model evaluation methods and applications, we provide a relatively comprehensive review of the recent trends in cognitive diagnosis models. Further, we discuss future directions that are worthy of exploration. In addition, we release two Python libraries: EduData for easy access to some relevant public datasets we have collected, and EduCDM that implements popular CDMs to facilitate both applications and research purposes.", "sections": [{"title": "1 INTRODUCTION", "content": "Measurement is an integral part of modern science as well as of engineering, commerce, and daily life [123]. Through measurement, we learn quantitatively about the things around us and even human beings. Cognitive diagnosis, as a representative, aims to measure the cognitive status of individuals, especially the ability levels such as knowledge structures and processing skills, so as to provide information about their cognitive strengths and weaknesses [74]. For example, through cognitive diagnosis, we can learn about whether a student has mastered specific knowledge concepts [74] or whether a patient is mentally healthy [32]. Therefore, cognitive diagnosis provides informative results for test developers and test takers, as well as helps with personalized support such as training course planning for employers and learning resource recommendations for students.\nUnlike conventional physical measurement objects such as length and weight, a person's ability level is a psychological characteristic not directly observable. Therefore, the fundamental idea of measuring human ability is by conducting tests and inferring examinees' ability through their performance. Francis Galton is thought as the first to apply statistical methods to the study of human differences and inheritance of intelligence, and proposed the first personality test [117]. Using the scores obtained in a test is a straightforward way to represent a person's overall ability level, and is widely adopted in IQ tests, teaching, etc. However, although test scores reflect examinees' ability level to some extent, they are not the ability level themselves. By contrast, cognitive diagnosis infers the ability level hidden in the responses. The complete cognitive diagnosis procedure (especially traditional cognitive diagnosis) requires multiple steps, including preparatory work such as deciding the measurement goal, arranging the knowledge structures, and constructing tests. A simplified procedure is illustrated which includes: 1) Test construction: rigorous questionnaires or test items (e.g., Q1 ~ Q5) can be constructed for response collection in fields such as education and psychotherapy [59]. The relation between items and relevant attributes (i.e., skills or knowledge concepts) are usually provided by experts. 2) Response data collection: the responses here mostly refer to binary 0/1 values indicating the results of examinees' answers (e.g., incorrect/correct) or discrete scores obtained on the items (e.g., 5 points out of 8). In some situations, responses are not limited to question answering, for example, the outcome of adversarial games [56] and law cases [3]. 3) Cognitive diagnosis model (CDM) designing: well-designed CDMs are an important guarantee of valid diagnostic results. 4) Psychological factor estimation: based on the collected response data, the psychological factors (e.g., the ability parameters) within CDMs are estimated. 4) Diagnosis feedback: the diagnosed ability levels are then fed back to the examinees.\nThe feedback can be different depending on the CDMs, such as overall ability (\u00a73.1), whether or not mastered certain attributes (\u00a73.2), and proficiency levels of certain attributes (\u00a74.2)."}, {"title": "2 THE OVERVIEW OF COGNITIVE DIAGNOSIS MODELS", "content": "Cognitive diagnosis models are essential for cognitive diagnosis, aiming to infer the unobservable ability levels from the observable responses to test items. Essentially, most existing CDMs are/contain simulations of examinees' cognitive processes. Specifically, when answering the test items, examinees go through a cognitive process that handles the items with their knowledge status and then provide their responses to the items. The responses depend on multiple factors, including the characteristics of both items and examinees. Therefore, the central problem of cognitive diagnosis is to model the relation between the examinees' ability levels and their observable behaviors such as responses to test items [43]. This simulation can be formulated as:\nPr(response) = f (\u03b8, \u03b2, \u03a9),\nwhere \u03b8, \u03b2 are the parameters indicating the examinees' abilities and items' features, respectively. \u03a9 represents the possible parameters required by the CDM itself (empty for some models).\nOriginating from psychometrics, the models for measuring human abilities have been developed for decades. The proposal of item response theory (IRT) can be traced back to the 1950s by Fredrick Lord [94]. IRT is a general framework for specifying mathematical functions that describe the interactions of persons and test items, where unidimensional parameters were adopted to describe the abilities of the persons. However, as suggested by researchers such as Glaser [52] and Mislevy [104], IRT and its previous test theories (e.g., classical test theory [37]) only measure the macro ability of individuals. Psychology was suggested to be combined with psychometrics in order to model the micro knowledge structure and cognitive processing of persons during the assessments so that the diagnostic results can be more instructional. The term cognitive diagnosis model (CDM) is originally adopted to denote such models. The proposal and usage of Q-matrix was a significant milestone of CDM [124]. Subsequently, representative models such as AHM [75], DINA [28] and NIDA [68] were proposed based on different assumptions to simulate the knowledge structures or cognitive processes, and each examinee is classified into a mastery pattern representing his/her mastery of each specific skill. These models fall into the cognition level paradigm [104].\nIn recent years, some researchers have been rethinking the issue of cognitive diagnosis from the perspective of machine learning and have proposed novel solutions [86]. Collaborative filtering and matrix factorization methods were adopted to model learners' ability and to predict learners' test performance [129, 130, 134]. Gierl et al. [51] proposed a neural network-based ability classifier trained with the data generated by a pre-trained attribute hierarchical model. More recently, Wang et al. recognized the limitations of expert-designed interaction functions and proposed a new data-driven cognitive diagnosis framework called NeuralCD [145]. Deep learning-based models incorporated with the theories/hypotheses from psychometrics have the advantage of better fitting ability of the sophisticated cognitive process, as well as promising interpretability. Since then, such deep learning-based paradigm gradually becomes a new tendency and has been attracting increasing attention [97, 146, 150]. In addition to the usage of various deep learning technologies, this"}, {"title": "2.1 Preliminary", "content": "Suppose there are examinees S = {S\u2081, . . ., S\u2081}, test items Q = {Q1, . . ., Qj } and K item attributes (i.e., skills or knowledge concepts). The responses are denoted as R = {R\u00a1, i = 1, . . ., I}, where Ri denotes the responses of S\u012f. R\u2081 = {(Si, Qj, rij), Si \u2208 S, Qj \u2208 Q} denotes S\u00a1's responses and rij is the response result of Si on Qj. Usually there is an expert-labeled Q-matrix Q = {qjk}J\u00d7K, where qjk=1 (or 0) indicating that item Qj requires (or does not require) the mastery of attribute k to answer it correctly. Sometimes there are extra multifaceted information available, such as item content and examinees' background, which we denote as X. The problem of cognitive diagnosis can be generally defined as follows:\nProblem Definition. With the input R, Q and possible X, the goal of cognitive diagnosis is to output examinees' ability levels \u03b8\u2081 (i = 1, . . ., I), where \u03b8; is either unidimensional or multidimensional indicating the overall ability levels or the mastery levels of each attribute.\nThe practicability of cognitive diagnosis is based on several basic assumptions.\nAssumption 1. Constant ability. The cognitive status of concern, i.e., ability level, remains unchanged during the process of answering the test items.\nIt is reasonable to assume that a person's ability level does not change in a short time (e.g., during a standard test), during which the person's ability level can be measured based on the responses to test items. Here lies the big difference between cognitive diagnosis and knowledge tracing, of which the latter also attracted wide attention in recent years. Knowledge tracing focuses on modeling the changing patterns of online learners' knowledge states (either explainable or unexplainable), which highly relies on sequential modeling methods such as hidden Markov chains and Recurrent Neural Networks. The cognitive process is usually neglected in the knowledge tracing models, and predicting learners' future performance is the most adopted task. By contrast, cognitive diagnosis aims to measure learners' ability level within a certain period of time. It mines the response data of learners, models the cognitive process of answering items, and provides the values of learners' ability levels within a certain metric space.\nAssumption 2. Constant item characteristics. The characteristics of a test item remain constant over all of the testing situations where it is used [111].\nSome statistics of the item such as the correct rate can be influenced by the examinees. However, the characteristics of a test item, such as the difficulty, discrimination, and relevant knowledge concepts, reflect the essential features of the item and should not change. This type of stability contributes to the fairness of test items for all examinees, and suggests that test items can be represented by fixed parameter values that reflect these characteristics.\nAssumption 3. Monotonicity. The probability of a correct response to the test item increases, or at least does not decrease, as the locations of examinees increase on any of the coordinate dimensions [111].\nMost cognitive diagnosis models adopt the monotonicity assumption in their modeling of cognitive processes, especially IRT-based and MIRT-based models. This assumption suggests that a"}, {"title": "2.2 A Brief Review of Cognitive Diagnosis Model Development", "content": "Without cognitive diagnosis, the most widely adopted method to evaluate a learner's ability is through their scores obtained in tests. Classical Test Theory (CTT) [37] was proposed to eliminate the errors existing in the scores. However, the score is the observed reflection of ability with the influence of factors such as question attributes and other psychological characteristics. To extract the actual ability hidden in the observations, cognitive diagnosis models sprouted from Psychometrics and have undergone decades of study. The development of cognitive diagnosis can be summarized from the aspects of both model structures and data characteristics.\n2.2.1 The development of model structures. Basically, the development of cognitive diagnosis models can be divided into two stages, i.e., psychometrics-based models and machine learning-based models.\nPsychometrics-based models. At the first stage of cognitive diagnosis development, ability mea-surement were based on psychometrics. Early research works were summarized as the ability level paradigm [104], as they used unidimensional or multidimensional latent vectors to represent exam-inees' overall ability levels. Representative methods include IRT and multidimensional IRT (MIRT). According to [111], its popularity is generally attributed to the work of Fredrick Lord and Georg Rasch starting in the 1950s and 1960s. Both IRT and MIRT have undergone lots of development and there is a large class of implementations. \u00a73.1 will provide a more detailed review of representative models. With the demand of measuring fine-grained ability, i.e., mastery of knowledge concepts or skills, the cognition level paradigm was proposed to improve diagnostic performance.\nMachine learning-based models. Later in 2000s, there came increasingly more studies using machine learning (ML) models for cognitive diagnosis, such as clustering algorithms [24, 57], support vector machine [84, 174], matrix factorization [129, 130, 134] and fuzzy set [157]. \u00a74.1 gives a review of relevant research using machine learning models for cognitive diagnosis before deep learning-based methods become popular. Although Gierl et al. [51] made early attempts to use artificial neural networks as classifiers for cognitive diagnosis, the popularity of deep learning (DL)-based CDMs is mostly attributed to the work of Wang et al. [145] which preliminarily validated the superiority of using data-driven deep learning methods in cognitive diagnosis and inspired numerous research works [45, 46, 97, 120]. More recently, Li et al. [78] put emphasis on inductive diagnosis and proposed an encoder-decoder-like CDM. We will provide a comprehensive summary and comparison in \u00a74.\n2.2.2 The changes of exploited data. As shown in the early research works, the response data for diagnosing examinees' ability is collected from scale-based tests, where scales (e.g., questionnaires, test papers) are constructed and tests are intentionally organized. Only numerical data, i.e., correct, incorrect, and scores, is utilized in early psychometrics-based methods, such as IRT, MIRT, and DINA. After that, some psychometrics-based models leverage simple hierarchical structures among a small number of knowledge concepts by either using them to help with the defining of Q-matrix or explicitly modeling the hierarchical structures, such as AHM. Overall, the data types that can be utilized in pure psychometrics-based models are limited due to the simplicity of model structures.\nWith the usage of machine learning, researchers have been making attempts to leverage more types of data that contain relevant information for cognitive diagnosis. Especially after NeuralCD [145] validated the superiority of using deep learning methods in cognitive diagnosis, including better fitting ability and extensibility without losing explainability, following studies started to explore diverse types of data, including test item contents, examinees' background information and sophisticated graph-structured data. Moreover, the response data in consideration is not limited to scale-based tests. The popularity of online learning systems provides more opportunities of accessing learners' daily behavioral data as well as diverse data types. Therefore, cognitive diagnosis can be conducted even without organizing tests in an interruptive way if we regard learners' ability as constant during a short time period. Some researchers addressed the data sparsity problem within learners' response logs [146, 164], and considered supplementary data such as response time and hints [130, 158, 167, 168]. More detailed behaviors such as keystrokes and eye tracking can be available, however, they still need further exploration."}, {"title": "3 PSYCHOMETRICS-BASED COGNITIVE DIAGNOSIS MODELS", "content": "Traditional cognitive diagnosis models (CDMs) utilize statistical methods based on psychometrics to model examinees' cognitive status. These models can be categorized into two paradigms, i.e., the ability level paradigm and the cognition level paradigm.\n3.1 Ability Level Paradigm\nIn the ability level paradigm, researchers focus on the estimation of the overall ability of examinees reflected on tests. Traditional models following the ability level paradigm usually model examinees' overall ability levels by low-dimensional 0, which can be jointly estimated with low-dimensional item parameters such as discrimination and difficulty. As mentioned before, we include Item Response Theory (IRT) [16, 67] and Multidimensional IRT (MIRT) [111] into the review even if they were proposed before the term cognitive diagnosis appeared.\n3.1.1 Item Response Theory (IRT). IRT [16, 67] is one of the most classical latent trait methods for measuring human cognitive status. The core assumption of IRT is that the relation between examinees' responses and their ability levels can be modeled by a continuous mathematical function. i.e., Pr(rij = 1) = f (0\u2081, \u03b2j), where \u03b8\u00a1 is a scalar parameter indicating the ability level of examinee i, and \u1e9ej denotes the latent traits of test item j. Many IRT-based models are simply called IRT, among which the most representative models include one-parameter logistic IRT (1PL-IRT), two-parameter logistic IRT (2PL-IRT) and three-parameter logistic IRT (3PL-IRT). 1PL-IRT only uses a scalar parameter bj to capture the difficulty of item j. 2PL-IRT adds an extra scalar parameter aj to indicate the discrimination of item j. While 3PL-IRT adds a parameter cj for item j, which is mostly interpreted as the probability of correctly guessing the answer. These models take dichotomous response scores into consideration, i.e., rij = 0, 1 indicating incorrect and correct responses respectively. Their formulas are as follows:\n1PL-IRT: Pr(rij = 1|0i, bj) = \u03c3(\u03b8; \u2212 bj) = \\frac{1}{1+e^{-(\\theta_i-b_j)}}.\nDespite IRT models represent the ability levels in the simplest scalar form, their excellent mathematical properties (e.g., the convex property of the interaction function) allow them to be applied to a variety of downstream tasks such as computerized adaptive testing. Although IRT models for dichotomous items attract more attention, there have been extended models for polytomous items. For polytomous items, scores can be multiple-graded. For example, for an item whose full score is 10, the obtained scores might be 0, 5, 8 and 10. Researchers have developed various polytomous IRT models such as Partial Credit Rasch Model [101], Rating Scale Model [4] and Graded Response Model [118].\n3.1.2 Multidimensional Item Response Theory (MIRT). MIRT [111] extends the ability modeled in IRT to multidimensional cases. Similar to exploratory factor analysis (EFA), MIRT allows for the exploration of multidimensionality and complex relationships between observed variables and latent traits, particularly in the context of assessments or tests. The simplest form of MIRT is a direct extension from 2PL-IRT and is given as Pr(rij = 1|0i, aj, bj) = 1/(1+e^(a0i+b;)), where aj denotes item discrimination on multiple dimensions, and bj is related to item difficulty. In practice, a small dimension is usually enough for MIRT, and each dimension of 0; represents a specific ability required to successfully answer the item. However, when using low-dimensional parameters, the ability is hard to explain explicitly. just like factor analysis. Besides relating to EFA and extending the dimension of IRT, MIRT also connects the cognition level paradigm. In the cognition level paradigm, researchers focus more on the fine-grained cognitive states of examinees, such as proficiency levels on pre-defined knowledge concepts. Along this line, da Silva et al. [26] introduce the Q-matrix into the interaction function of MIRT to obtain examinees' knowledge-concept-wise latent traits. To ensure the identifiability of MIRT, item discrimination vector aj of different items are usually rotated to the same value to acquire identifiable estimations of examinees' abilities [7].\n3.2 Cognition Level Paradigm\nIn the cognitive level paradigm, researchers focus on the estimation of the fine-grained cognitive states of students. For instance, in K-12 course learning, test designers require diagnosing students' proficiency level on knowledge concepts (e.g., the concept linear function in mathematics) from their test performances. Therefore, CDMs in the cognitive level paradigm are utilized to estimate student knowledge proficiencies in this scenario. Since such a diagnosis process can be viewed as classifying students to an \u201cideal\u201d proficiency pattern that is most suitable for his/her test"}, {"title": "3.2.1 Rule Space Method (RSM) And Its Variations.", "content": "The RSM proposed by Tatsuoka in the 1980s [124], is a statistical modeling approach used for cognitive diagnosis. Compared with IRT, RSM focuses on representing the cognitive processes that individuals use to respond to test items, which is more fine-grained. It seeks to identify the specific cognitive rules or strategies that individuals employ when answering test items. Four steps are included in RSM, i.e., item decomposition, rule specification, rule space construction, and rule space matching.\nThe RSM is a fundamental yet significant method in traditional CD, and is the basis of many DCMs [74, 75]. One shortcoming of the traditional RSM is that it views knowledge concepts as independent entities and ignores their hierarchical relationship in the cognitive process (e.g., knowledge dependency). Therefore, Leighton et al. [75] proposed the Attribute Hierarchy Method (AHM) to address this issue. Compared to the original RSM, the AHM assumes that attributes (i.e., knowledge concepts or skills that are required for students to solve a test item) are organized in a hierarchical structure, which can be represented by an adjacent matrix of attributes. Then the AHM limits the rule space defined in the RSM, such that the mastery of any child attribute should be no less than the mastery of its parent attribute. Then the AHM matches each student into the most similar ideal response patterns, with the corresponding rule as the diagnostic result of the student."}, {"title": "3.2.2 DINA And Relevant CDMs.", "content": "Deterministic Input, Noisy \u201cAnd\u201d Gate (DINA) model [28] is a representative and recognized CDM. DINA and its relevant CDMs diagnose students' knowledge concept-wise abilities from binary response data and expert-labeled question-knowledge relation-ship. The core assumption of DINA is that the proficiency of different knowledge concepts is non-compensatory. That is, the model assumes that mastery of all required attributes is necessary for a correct response, but also allows for the possibility of guessing. In DINA, each student i's ability status is modeled as a binary knowledge mastery pattern vector \u03b8\u2081 = (\u03b8\u2081\u2081, . . ., \u03b8\u03af\u03ba). Here K denotes the number of knowledge concepts, and the binary value \u03b8ik \u2208 {1,0} denotes whether or not student i has mastered the knowledge concept k. Items are modelled by \"slip\" and \"guess\" parameters and a pre-given binary Q-matrix Q = (q1, ..., qj) = (qjk)J\u00d7K. The interaction function of DINA is defined as Pr(rij = 1|0i, qj, sj, gj) = (1 \u2013 sj)Nijg-Nij, where Nij = \u03a0\u039a-109k is the indicator of whether the student has mastered all required knowledge concepts of the item. The sj"}, {"title": "3.2.3 General Diagnostic Model (GDM).", "content": "The General Diagnostic Model (GDM) [139] is a general framework that subsumes many classical and well-known CDMs like DINA, IRT and MIRT [14, 140]. As a general framework, GDM is suitable for various real-world scenarios, including but not limited to dichotomous/polytomous response scores, binary/continuous/polytomous ordinal knowledge mastery levels, etc. We introduce the general form of GDM in this section.\nFormally, let @ be a K-dimensional skill profile consisting of polytomous or dichotomous skill attributes \u03b8k(k = 1, . . ., K). Then the probability of a polytomous response score x \u2208 {0, ..., mj} to item j under the GDM with an individual with skill profiles e is defined as\nPr(rj = x|0 = (01,..., \u03b8\u03ba)) = \\frac{exp [Bjx + \\sum_{k=1}^{K} Yjxkhj(qjk, \\theta_k)]}{1 + \\sum_{y=1}^{mj} exp [\\beta_{jy} + \\sum_{k=1}^{K} Yjykhj (qjk, \\theta_k)]}.\nwhere \u1e9e jx and y jxk (j = 1, 2, . . ., J) are estimable item parameters. Each element qjk, j = 1, 2, . . . J, k = 1, 2, . . ., K of the Q-matrix is a constant, as in other DCMs like DINA. The helper function h(\u00b7, \u00b7) maps qjk and Ok to a real number, which considers the fact that the knowledge profile @ might be polytomous. By elaborately designing the form of the parameter y and the helper function, GDM can be flexibly applied to either binary or polytomous response data. As a constraint latent class model, the parameter estimation of GDM is usually done with expectation-maximization (EM) algorithm [33, 65].\nDue to its generality and flexibility, there are also many other versions for GDM to adjust some special scenarios, like mixture distribution extensions of GDM which consider the ability prior of student groups, and hierarchical extensions of GDM which consider the multilevel distribution of student abilities. Indeed, many traditional CDMs, including CDMs in the ability level paradigm like IRT and MIRT [14], and CDMs in the cognitive level paradigm like LCDM and DINA [140], have been proven to be special cases of GDM."}, {"title": "3.2.4 Other Traditional CDMs.", "content": "Besides representative CDMs introduced above, there are also some other traditional CDMs that focus on different research challenges in the context of educational measurement. For instance, the Reparameterized Unified Model (RUM) [58], as a refinement of DINA, aims to construct a cognitive diagnosis assessment system that includes DCM models, estimation procedure, classification algorithm and model-and-data checking function. De La Torre [29] proposed the G-DINA, as a general cognitive diagnosis framework similar to GDM, which subsumes many existing CDMs like DINA. Henson et al. [60] proposed the Log-Linear Cognitive Diagnosis Model (LCDM) based on GDM, which integrates the log-linear model into the calculation"}, {"title": "4 MACHINE LEARNING-BASED COGNITIVE DIAGNOSIS MODELS", "content": "In recent years, with the development of AI-based education, cognitive diagnosis has raised the attention of researchers from computer science, especially artificial intelligence. CDMs based on machine learning (ML), especially deep learning (DL), are consequently proposed [86]. With the advantages of better fitting ability and more flexible structures to make use of different types of educational data (e.g., response, item content, knowledge concept structure), machine learning-based CDMs have achieved much success, leading to a new trend of research. We roughly classify these works according to their proposed time and technology/target in Fig. 9.\n4.1 Non-deep-learning Models\nIn the earlier works, there were several attempts using clustering algorithms to classify examinees into different clusters, where each of the clusters represents a type of knowledge mastery pattern. For example, Chiu [24] adopted K-means clustering combined with hierarchical agglomerative cluster analysis, Guo et al. [57] adopted spectral clustering algorithms. Support vector machine (SVM) was also used in several studies for cognitive diagnosis [84, 174], where examinees' response logs are input as features and possible knowledge mastery statuses are predicted by SVM. Supervised training data including the known knowledge status is required for SVM-based methods, which limits the practicality of these models. Collaborative filtering methods such as matrix factorization were also adopted to solve the cognitive diagnosis problem in education [129, 130, 134]. However, as these models focus on predicting students' performance instead of diagnosing students' knowledge proficiencies, the estimated student parameters are not explicitly explainable. Wu and Liu et al. [89, 157] proposed FuzzyCDF which integrates the fuzzy set to handle both objective and subjective test items. Wu et al. [156] introduced a variational Bayesian inference algorithm for IRT, which provides a faster and more accurate human ability estimation compared to traditional IRT, especially on large-scale datasets."}, {"title": "4.2 Deep learning-Based Models", "content": "Integrating deep learning methods has been a new trend in cognitive diagnosis. According to the model architectures and their emergence time, deep learning-based CDMs can be generally classified into mastery pattern classifier, cognitive interaction simulator, and encoder-decoder-based architectures. In \u00a74.2.1 we will review these types of CDMs in detail, following an exploration of multifaceted information (\u00a74.2.2) and other topics in cognitive diagnosis research (\u00a74.2.4).\n4.2.1 DL-based Architectures.\n(1) Mastery Pattern Classifier.\nArtificial neural networks were initially adopted in cognitive diagnosis in a reverse manner compared to traditional models. Typically, as introduced in the Introduction, CDMs simulate the cognitive mechanism of the human's item-answering process. Therefore, the goal of cognitive diagnosis, i.e., humans' ability levels, is actually at the input side of traditional models because it's the cause of human responses. The ability evaluation is actually done through model training instead of model inference. In contrast, Gierl et al. [51] proposed a reversed model structure based on neural networks, which takes the examinee's response pattern (i.e., a binary vector that indicates his/her responses) as input, and directly outputs the examinee's attribute pattern (i.e., a vector that indicates his/her mastery on each knowledge concept). As a result, the CDM becomes a classification model, which can be abstracted as:\n0 = 9NN(Response, \u03a9).\nAs empirical response data with the examinee's true ability levels is unavailable, the model is trained with simulated data, which is generated using traditional CDMs such as AHM. Similarly, Cui et al. [25] adopted the self-organizing map to construct the classification model for ability evaluation. The main advantage of these methods is that the ability evaluation can be done with model inference after model training, even if the examinee is out of the training data. However, as pointed out by Briggs et al [15], the performance of these models is limited by the CDMs that generate the training data. When the items and/or data generation model is flawed, the trained CDM will naturally incorporate those flaws. Moreover, the ability estimation can be unstable after multiple times of model training.\n(2) Cognitive Interaction Simulator.\nMost deep learning-based CDMs still follow the traditional practice, i.e., model the cognitive interaction during the item answering process like a simulator. The differences mainly lie in how the interactions are modeled. As pointed out by Wang et al. [145], traditional psychometric-based CDMs rely on domain experts to design interaction functions for predicting examinees' responses. Although this approach offers high interpretability, it is costly, and the fixed function form often leads to weak fitting and generalization capabilities. Wang et al. [145] made an initial break through along this line. They abstracted the cognitive factors involved in the process of answering questions and attributed them to student factors, exercise factors, and interaction functions. A neural network-based framework called NeuralCD was proposed, which can be generally formulated as follows:\nResponse = fNn (\u03b8, \u03b2, \u03a9).\nHere, \u03b8 represents the examinee's ability level and is modeled with a multi-dimensional continuous vector, where the value of each entry indicates the proficiency of a specific knowledge concept. The item parameters \u1e9e here can be the knowledge difficulty, item discrimination, etc. The main difference between NeuralCD and psychometric-based CDMs is that NeuralCD introduced the data-driven strategy to learn the interaction function with neural networks from actual response data. The"}, {"title": "monotonicity", "content": "assumption was adopted together with the knowledge relevancy vector to ensure the explainability of diagnostic results, i.e., the estimated 0. Wang et al. also illustrated the generality and extensibility of NeuralCD. The simplicity, robust generalizability, and psychological interpretability based on the monotonicity assumption make NeuralCD highly attractive. Consequently, NeuralCD has inspired the emergence of quite a few DL-based CDMs.\nDeep neural network (DNN). The DNN is the most straightforward deep learning method to construct the interaction functions. Wang et al. [145] have provided a basic implementation based on NeuralCD called NeuralCDM. Formally, NeuralCDM reconstructs the response of each learner i to the test item j with the formula Pr(rij = 1) = fpnn(qj \u00b0 (0i - bj) * aj), where the interaction function fDNN consists of multiple fully connected layers. The ability status of learner i is represented with a K-dimensional vector, \u03b8\u00a1 \u2208 RK, where K denotes the total number of knowledge concepts corresponding to all test items. Each dimension \u03b8ik, ranging from 0 to 1, represents the proficiency level of learner i on concept k. In NeuralCDM, the test item is considered as a difficulty parameter bj \u2208 RK and a discrimination parameter aj \u2208 R\u00b9, similar to MIRT. qj \u2208 RK is the jth row of the Q-matrix indicating the knowledge concepts required by item j. The symbol \u25cb denotes the element-wise product. Notably, NeuralCDM strictly adheres to the monotonicity assumption by restricting the layer weights to be nonnegative.\nSome deep learning-based CDMs are direct extensions of NeuralCDM. Wang et al. [146] proposed KaNCD to model the latent knowledge associations with the aim of mitigating the low knowledge coverage problem as well as improving the diagnostic performance. Ma et al. [97] proposed KSCD that introduces additional parameters for knowledge concepts to better capture the relationship among students, items, and knowledge concepts. Meanwhile, Li et al. [76] added the guessing and slipping parameters of test items and replaced the first-layer of the NeuralCDM interaction module with IRT to enhance the interpretability while maintaining data-driven generalization, effectiveness, and efficiency. Cheng et al. [22] added a knowledge point importance vector to the input layer of NeuralCDM. Similarly, Li et al. [80] added parameters to depict the impacts of different knowledge concepts, as well as the guessing and slipping factors, thereby proposing the CDMFKC model. Wang et al. [150] improved NeuralCDM by aggregating the knowledge concepts by converting them into a graph structure and only considering the leaf node of the knowledge concept tree. Some extensions made use of extra information, such as the text content [146]. We will introduce them in Section 4.2.2.\nNeural architecture search (NAS). In addition to widely used neural networks, some studies have opted for the more complex neural architecture search (NAS)-based approach to construct the"}, {"title": "interaction", "content": "function fNN, offering an intriguing alternative. Despite this shift, these endeavors still operate within the NeuralCD framework and can thus be represented using Eq. (6). The primary distinction lies in implementing the interaction function fNN using NAS instead of conventional neural networks. For example, Yang et al. [162, 163", "103": "or model the propagation of the influence among the mastery of different knowledge concepts [46, 122", "45": "combines the IRT, DINA and neural networks, and predicts learners' scores on both objective and subjective items. Wang et al. [151"}]}