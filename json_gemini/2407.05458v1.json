{"title": "A Survey of Models for Cognitive Diagnosis: New Developments and Future Directions", "authors": ["FEI WANG", "WEIBO GAO", "QI LIU", "JIATONG LI", "GUANHAO ZHAO", "ZHENG ZHANG", "ZHENYA HUANG", "MENGXIAO ZHU", "SHIJIN WANG", "WEI TONG", "ENHONG CHEN"], "abstract": "Cognitive diagnosis has been developed for decades as an effective measurement tool to evaluate human cognitive status such as ability level and knowledge mastery. It has been applied to a wide range of fields including education, sport, psychological diagnosis, etc. By providing better awareness of cognitive status, it can serve as the basis for personalized services such as well-designed medical treatment, teaching strategy and vocational training. This paper aims to provide a survey of current models for cognitive diagnosis, with more attention on new developments using machine learning-based methods. By comparing the model structures, parameter estimation algorithms, model evaluation methods and applications, we provide a relatively comprehensive review of the recent trends in cognitive diagnosis models. Further, we discuss future directions that are worthy of exploration. In addition, we release two Python libraries: EduData for easy access to some relevant public datasets we have collected, and EduCDM that implements popular CDMs to facilitate both applications and research purposes.", "sections": [{"title": "1 INTRODUCTION", "content": "Measurement is an integral part of modern science as well as of engineering, commerce, and daily life [123]. Through measurement, we learn quantitatively about the things around us and even human beings. Cognitive diagnosis, as a representative, aims to measure the cognitive status of individuals, especially the ability levels such as knowledge structures and processing skills, so as to provide information about their cognitive strengths and weaknesses [74]. For example, through cognitive diagnosis, we can learn about whether a student has mastered specific knowledge concepts [74] or whether a patient is mentally healthy [32]. Therefore, cognitive diagnosis provides informative results for test developers and test takers, as well as helps with personalized support such as training course planning for employers and learning resource recommendations for students.\nUnlike conventional physical measurement objects such as length and weight, a person's ability level is a psychological characteristic not directly observable. Therefore, the fundamental idea of measuring human ability is by conducting tests and inferring examinees' ability through their performance. Francis Galton is thought as the first to apply statistical methods to the study of human differences and inheritance of intelligence, and proposed the first personality test [117]. Using the scores obtained in a test is a straightforward way to represent a person's overall ability level, and is widely adopted in IQ tests, teaching, etc. However, although test scores reflect examinees' ability level to some extent, they are not the ability level themselves. By contrast, cognitive diagnosis infers the ability level hidden in the responses. The complete cognitive diagnosis procedure (especially traditional cognitive diagnosis) requires multiple steps, including preparatory work such as deciding the measurement goal, arranging the knowledge structures, and constructing tests. A simplified procedure includes: 1) Test construction: rigorous questionnaires or test items (e.g., Q1 ~ Q5) can be constructed for response collection in fields such as education and psychotherapy [59]. The relation between items and relevant attributes (i.e., skills or knowledge concepts) are usually provided by experts. 2) Response data collection: the responses here mostly refer to binary 0/1 values indicating the results of examinees' answers (e.g., incorrect/correct) or discrete scores obtained on the items (e.g., 5 points out of 8). In some situations, responses are not limited to question answering, for example, the outcome of adversarial games [56] and law cases [3]. 3) Cognitive diagnosis model (CDM) designing: well-designed CDMs are an important guarantee of valid diagnostic results. 4) Psychological factor estimation: based on the collected response data, the psychological factors (e.g., the ability parameters) within CDMs are estimated. 4) Diagnosis feedback: the diagnosed ability levels are then fed back to the examinees.\nThe feedback can be different depending on the CDMs, such as overall ability (\u00a73.1), whether or not mastered certain attributes (\u00a73.2), and proficiency levels of certain attributes (\u00a74.2)."}, {"title": "2 THE OVERVIEW OF COGNITIVE DIAGNOSIS MODELS", "content": "Cognitive diagnosis models are essential for cognitive diagnosis, aiming to infer the unob- servable ability levels from the observable re- sponses to test items. Essentially, most existing CDMs are/contain simulations of examinees' cognitive processes. Specifically, as illustrated when answering the test items, ex- examinees go through a cognitive process that handles the items with their knowledge status and then provide their responses to the items. The responses depend on multiple factors, in- cluding the characteristics of both items (e.g., difficulty [111], relevant knowledge concept, guessing [28]) and examinees (e.g., ability, gaming behavior [158]). Therefore, the central problem of cognitive diagnosis is to model the relation between the examinees' ability levels and their observable behaviors such as responses to test items [43]. This simulation can be formulated as:\n\\Pr(response) = f (\u03b8, \u03b2, \u03a9),\nwhere \u03b8, \u03b2 are the parameters indicating the examinees' abilities and items' features, respectively. \u03a9 represents the possible parameters required by the CDM itself (empty for some models).\nOriginating from psychometrics, the models for measuring human abilities have been developed for decades. The proposal of item response theory (IRT) can be traced back to the 1950s by Fredrick Lord [94]. IRT is a general framework for specifying mathematical functions that describe the interactions of persons and test items, where unidimensional parameters were adopted to describe the abilities of the persons. However, as suggested by researchers such as Glaser [52] and Mislevy [104], IRT and its previous test theories (e.g., classical test theory [37]) only measure the macro ability of individuals. Psychology was suggested to be combined with psychometrics in order to model the micro knowledge structure and cognitive processing of persons during the assessments so that the diagnostic results can be more instructional. The term cognitive diagnosis model (CDM) is originally adopted to denote such models\u00b9. The proposal and usage of Q-matrix was a significant milestone of CDM [124]. Subsequently, representative models such as AHM [75], DINA [28] and NIDA [68] were proposed based on different assumptions to simulate the knowledge structures or cognitive processes, and each examinee is classified into a mastery pattern representing his/her mastery of each specific skill. These models fall into the cognition level paradigm [104].\nIn recent years, some researchers have been rethinking the issue of cognitive diagnosis from the perspective of machine learning and have proposed novel solutions [86]. Collaborative filtering and matrix factorization methods were adopted to model learners' ability and to predict learners' test performance [129, 130, 134]. Gierl et al. [51] proposed a neural network-based ability classifier trained with the data generated by a pre-trained attribute hierarchical model. More recently, Wang et al. recognized the limitations of expert-designed interaction functions and proposed a new data-driven cognitive diagnosis framework called NeuralCD [145]. Deep learning-based models incorporated with the theories/hypotheses from psychometrics have the advantage of better fitting ability of the sophisticated cognitive process, as well as promising interpretability. Since then, such deep learning-based paradigm gradually becomes a new tendency and has been attracting increasing attention [97, 146, 150]. In addition to the usage of various deep learning technologies, this"}, {"title": "2.1 Preliminary", "content": "Suppose there are examinees S = {S\u2081, . . ., S\u2081}, test items Q = {Q1, . . ., Qj } and K item attributes\n(i.e., skills or knowledge concepts). The responses are denoted as R = {R\u00a1, i = 1, . . ., I}, where Ri\ndenotes the responses of S\u012f. R\u2081 = {(Si, Qj, rij), Si \u2208 S, Qj \u2208 Q} denotes S\u00a1's responses and rij is the\nresponse result of Si on Qj. Usually there is an expert-labeled Q-matrix Q = {qjk}J\u00d7K, where qjk=1\n(or 0) indicating that item Qj requires (or does not require) the mastery of attribute k to answer\nit correctly. Sometimes there are extra multifaceted information available, such as item content\nand examinees' background, which we denote as X. The problem of cognitive diagnosis can be\ngenerally defined as follows:\nProblem Definition. With the input R, Q and possible X, the goal of cognitive diagnosis is to out-\nput examinees' ability levels \u03b8\u2081 (i = 1, . . ., I), where \u03b8; is either unidimensional or multidimensional\nindicating the overall ability levels or the mastery levels of each attribute.\nThe practicability of cognitive diagnosis is based on several basic assumptions.\nAssumption 1. Constant ability. The cognitive status of concern, i.e., ability level, remains\nunchanged during the process of answering the test items.\nIt is reasonable to assume that a person's ability level does not change in a short time (e.g., during\na standard test), during which the person's ability level can be measured based on the responses to\ntest items. Here lies the big difference between cognitive diagnosis and knowledge tracing, of which\nthe latter also attracted wide attention in recent years. Knowledge tracing focuses on modeling\nthe changing patterns of online learners' knowledge states (either explainable or unexplainable),\nwhich highly relies on sequential modeling methods such as hidden Markov chains and Recurrent\nNeural Networks. The cognitive process is usually neglected in the knowledge tracing models, and\npredicting learners' future performance is the most adopted task. By contrast, cognitive diagnosis\naims to measure learners' ability level within a certain period of time. It mines the response data\nof learners, models the cognitive process of answering items, and provides the values of learners'\nability levels within a certain metric space.\nAssumption 2. Constant item characteristics. The characteristics of a test item remain constant\nover all of the testing situations where it is used [111].\nSome statistics of the item such as the correct rate can be influenced by the examinees. However,\nthe characteristics of a test item, such as the difficulty, discrimination, and relevant knowledge\nconcepts, reflect the essential features of the item and should not change. This type of stability\ncontributes to the fairness of test items for all examinees, and suggests that test items can be\nrepresented by fixed parameter values that reflect these characteristics.\nAssumption 3. Monotonicity. The probability of a correct response to the test item increases,\nor at least does not decrease, as the locations of examinees increase on any of the coordinate\ndimensions [111].\nMost cognitive diagnosis models adopt the monotonicity assumption in their modeling of cog-\nnitive processes, especially IRT-based and MIRT-based models. This assumption suggests that a"}, {"title": "2.2 A Brief Review of Cognitive Diagnosis Model Development", "content": "Without cognitive diagnosis, the most widely adopted method to evaluate a learner's ability is through their scores obtained in tests. Classical Test Theory (CTT) [37] was proposed to eliminate the errors existing in the scores. However, the score is the observed reflection of ability with the influence of factors such as question attributes and other psychological characteristics. To extract the actual ability hidden in the observations, cognitive diagnosis models sprouted from Psychometrics and have undergone decades of study. The development of cognitive diagnosis can be summarized from the aspects of both model structures and data characteristics.\n2.2.1 The development of model structures. Basically, the development of cognitive diagnosis models can be divided into two stages, i.e., psychometrics-based models and machine learning- based models. Each stage of the development can be further divided into two sub-stages as follows:\nPsychometrics-based models. At the first stage of cognitive diagnosis development, ability mea- surement were based on psychometrics. Early research works were summarized as the ability level paradigm [104], as they used unidimensional or multidimensional latent vectors to represent exam- inees' overall ability levels. Representative methods include IRT and multidimensional IRT (MIRT). According to [111], its popularity is generally attributed to the work of Fredrick Lord and Georg Rasch starting in the 1950s and 1960s. Both IRT and MIRT have undergone lots of development and there is a large class of implementations. \u00a73.1 will provide a more detailed review of representative models. With the demand of measuring fine-grained ability, i.e., mastery of knowledge concepts or skills, the cognition level paradigm was proposed to improve diagnostic performance. The\n2.2.2 The changes of exploited data. In the early research works, the response data for diagnosing examinees' ability is collected from scale-based tests, where scales (e.g., questionnaires, test papers) are constructed and tests are intentionally organized. Only numerical data, i.e., correct, incorrect, and scores, is utilized in early psychometrics-based methods, such as IRT, MIRT, and DINA. After that, some psychometrics-based models leverage simple hierarchical structures among a small number of knowledge concepts by either using them to help with the defining of Q-matrix or explicitly modeling the hierarchical structures, such as AHM. Overall, the data types that can be utilized in pure psychometrics-based models are limited due to the simplicity of model structures.\nWith the usage of machine learning, researchers have been making attempts to leverage more types of data that contain relevant information for cognitive diagnosis. Especially after NeuralCD [145] validated the superiority of using deep learning methods in cognitive diagnosis, including better fitting ability and extensibility without losing explainability, following studies started to explore diverse types of data, including test item contents, examinees' background information and sophisticated graph-structured data. Moreover, the response data in consideration is not limited to scale-based tests. The popularity of online learning systems provides more opportunities of accessing learners' daily behavioral data as well as diverse data types. Therefore, cognitive diagnosis can be conducted even without organizing tests in an interruptive way if we regard learners' ability as constant during a short time period. Some researchers addressed the data sparsity problem within learners' response logs [146, 164], and considered supplementary data such as response time and hints [130, 158, 167, 168]. More detailed behaviors such as keystrokes and eye tracking can be available, however, they still need further exploration."}, {"title": "3 PSYCHOMETRICS-BASED COGNITIVE DIAGNOSIS MODELS", "content": "Traditional cognitive diagnosis models (CDMs) utilize statistical methods based on psychometrics to model examinees' cognitive status. These models can be categorized into two paradigms, i.e., the ability level paradigm and the cognition level paradigm\n3.1 Ability Level Paradigm\nIn the ability level paradigm, researchers focus on the estimation of the overall ability of examinees reflected on tests. Traditional models following the ability level paradigm usually model examinees' overall ability levels by low-dimensional 0, which can be jointly estimated with low-dimensional item parameters such as discrimination and difficulty. As mentioned before, we include Item Response Theory (IRT) [16, 67] and Multidimensional IRT (MIRT) [111] into the review even if they were proposed before the term cognitive diagnosis appeared.\n3.1.1 Item Response Theory (IRT). IRT [16, 67] is one of the most classical latent trait methods for measuring human cognitive status. The core assumption of IRT is that the relation between examinees' responses and their ability levels can be modeled by a continuous mathematical function. i.e., \\Pr(rij = 1) = f (0\u2081, \u03b2j), where \u03b8\u00a1 is a scalar parameter indicating the ability level of examinee i, and \u1e9ej denotes the latent traits of test item j. Many IRT-based models are simply called IRT, among which the most representative models include one-parameter logistic IRT (1PL-IRT), two- parameter logistic IRT (2PL-IRT) and three-parameter logistic IRT (3PL-IRT). 1PL-IRT only uses a scalar parameter bj to capture the difficulty of item j. 2PL-IRT adds an extra scalar parameter aj to indicate the discrimination of item j. While 3PL-IRT adds a parameter cj for item j, which is mostly interpreted as the probability of correctly guessing the answer. These models take dichotomous response scores into consideration, i.e., rij = 0, 1 indicating incorrect and correct responses respectively. Their formulas are as follows:\n\\1PL-IRT: Pr(rij = 1|0i, bj) = \u03c3(\u03b8; \u2212 bj) = 1/(1+e-(\u03b8i-bj)), (1)\n\\1PL-2RT: Pr(rij = 1|0i, aj, bj) = 1/(1+e-aj (0i-bj)), (2)\n\\1PL-3RT: Pr(rij = 1|0i, aj, bj, cj) = cj + (1 \u2212 cj) 1/(1+ e-aj (0;-bj)), (3)\nDespite IRT models represent the ability levels in the simplest scalar form, their excellent mathematical properties (e.g., the convex property of the interaction function) allow them to be applied to a variety of downstream tasks such as computerized adaptive testing. Although IRT models for dichotomous items attract more attention, there have been extended models for polytomous items. For polytomous items, scores can be multiple-graded. For example, for an item whose full score is 10, the obtained scores might be 0, 5, 8 and 10. Researchers have developed various polytomous IRT models such as Partial Credit Rasch Model [101], Rating Scale Model [4] and Graded Response Model [118].\n3.1.2 Multidimensional Item Response Theory (MIRT). MIRT [111] extends the ability modeled in IRT to multidimensional cases. Similar to exploratory factor analysis (EFA), MIRT allows for the exploration of multidimensionality and complex relationships between observed variables and latent traits, particularly in the context of assessments or tests. The simplest form of MIRT is a direct extension from 2PL-IRT and is given as Pr(rij = 1|0i, aj, bj) = 1/(1+e^(a0i+b;)), where aj denotes item discrimination on multiple dimensions, and bj is related to item difficulty. In practice, a small dimension is usually enough for MIRT, and each dimension of 0; represents a specific ability required to successfully answer the item. However, when using low-dimensional parameters, the ability is hard to explain explicitly. just like factor analysis. Besides relating to EFA and extending the dimension of IRT, MIRT also connects the cognition level paradigm. In the cognition level paradigm, researchers focus more on the fine-grained cognitive states of examinees, such as proficiency levels on pre-defined knowledge concepts. Along this line, da Silva et al. [26] introduce the Q-matrix into the interaction function of MIRT to obtain examinees' knowledge-concept-wise latent traits. To ensure the identifiability of MIRT, item discrimination vector aj of different items are usually rotated to the same value to acquire identifiable estimations of examinees' abilities [7].\n3.2 Cognition Level Paradigm\nIn the cognitive level paradigm, researchers focus on the estimation of the fine-grained cognitive states of students. For instance, in K-12 course learning, test designers require diagnosing students' proficiency level on knowledge concepts (e.g., the concept linear function in mathematics) from their test performances. Therefore, CDMs in the cognitive level paradigm are utilized to estimate student knowledge proficiencies in this scenario. Since such a diagnosis process can be viewed as classifying students to an \u201cideal\u201d proficiency pattern that is most suitable for his/her test"}, {"title": "3.2.1 Rule Space Method (RSM) And Its Variations.", "content": "The RSM proposed by Tatsuoka in the 1980s [124], is a statistical modeling approach used for cognitive diagnosis. Compared with IRT, RSM focuses on representing the cognitive processes that individuals use to respond to test items, which is more fine-grained. It seeks to identify the specific cognitive rules or strategies that individuals employ when answering test items. Four steps are included in RSM, i.e., item decomposition, rule specification, rule space construction, and rule space matching.\nThe RSM is a fundamental yet significant method in traditional CD, and is the basis of many DCMs [74, 75]. One shortcoming of the traditional RSM is that it views knowledge concepts as independent entities and ignores their hierarchical relationship in the cognitive process (e.g., knowledge dependency). Therefore, Leighton et al. [75] proposed the Attribute Hierarchy Method (AHM) to address this issue. Compared to the original RSM, the AHM assumes that attributes (i.e., knowledge concepts or skills that are required for students to solve a test item) are organized in a hierarchical structure, which can be represented by an adjacent matrix of attributes. Then the AHM limits the rule space defined in the RSM, such that the mastery of any child attribute should be no less than the mastery of its parent attribute. Then the AHM matches each student into the most similar ideal response patterns, with the corresponding rule as the diagnostic result of the student."}, {"title": "3.2.2 DINA And Relevant CDMs.", "content": "Deterministic Input, Noisy \u201cAnd\u201d Gate (DINA) model [28] is a representative and recognized CDM. DINA and its relevant CDMs diagnose students' knowledge concept-wise abilities from binary response data and expert-labeled question-knowledge relation- ship. The core assumption of DINA is that the proficiency of different knowledge concepts is non-compensatory. That is, the model assumes that mastery of all required attributes is necessary for a correct response, but also allows for the possibility of guessing. In DINA, each student i's ability status is modeled as a binary knowledge mastery pattern vector \u03b8\u2081 = (\u03b8\u2081\u2081, . . ., \u03b8\u03af\u03ba). Here K denotes the number of knowledge concepts, and the binary value \u03b8ik \u2208 {1,0} denotes whether or not student i has mastered the knowledge concept k. Items are modelled by \"slip\" and \"guess\" parameters and a pre-given binary Q-matrix Q = (q1, ..., qj) = (qjk)J\u00d7K. The interaction func- tion of DINA is defined as \\Pr(rij = 1|0i, qj, sj, gj) = (1 \u2013 sj)Nijg-Nij, where \\Nij = \u03a0\u039a-109k is the indicator of whether the student has mastered all required knowledge concepts of the item. The sj\nT\nK\nk=1ik"}, {"title": "3.2.3 General Diagnostic Model (GDM).", "content": "The General Diagnostic Model (GDM) [139] is a general framework that subsumes many classical and well-known CDMs like DINA, IRT and MIRT [14, 140]. As a general framework, GDM is suitable for various real-world scenarios, including but not limited to dichotomous/polytomous response scores, binary/continuous/polytomous ordinal knowledge mastery levels, etc. We introduce the general form of GDM in this section.\nFormally, let @ be a K-dimensional skill profile consisting of polytomous or dichotomous skill attributes \u03b8k(k = 1, . . ., K). Then the probability of a polytomous response score x \u2208 {0, ..., mj} to item j under the GDM with an individual with skill profiles e is defined as\n\\Pr(rj = x|0 = (01,..., \u03b8\u03ba)) = exp [Bjx + \u03a3k=1 Yjxkhj(qjk, \u03b8k)]/[1 + \u03a3+1 exp [\u1e9ejy + \u03a3k=1Yjykhj (qjk, 0k)]], (4)\nmj\nwhere \u1e9e jx and y jxk (j = 1, 2, . . ., J) are estimable item parameters. Each element qjk, j = 1, 2, . . . J, k = 1, 2, . . ., K of the Q-matrix is a constant, as in other DCMs like DINA. The helper function h(\u00b7, \u00b7) maps qjk and Ok to a real number, which considers the fact that the knowledge profile @ might be polytomous. By elaborately designing the form of the parameter y and the helper function, GDM can be flexibly applied to either binary or polytomous response data. As a constraint latent class model, the parameter estimation of GDM is usually done with expectation-maximization (EM) algorithm [33, 65].\nDue to its generality and flexibility, there are also many other versions for GDM to adjust some special scenarios, like mixture distribution extensions of GDM which consider the ability prior of student groups, and hierarchical extensions of GDM which consider the multilevel distribution of student abilities. Indeed, many traditional CDMs, including CDMs in the ability level paradigm like IRT and MIRT [14], and CDMs in the cognitive level paradigm like LCDM and DINA [140], have been proven to be special cases of GDM."}, {"title": "3.2.4 Other Traditional CDMs.", "content": "Besides representative CDMs introduced above, there are also some other traditional CDMs that focus on different research challenges in the context of educational measurement. For instance, the Reparameterized Unified Model (RUM) [58], as a refinement of DINA, aims to construct a cognitive diagnosis assessment system that includes DCM models, estimation procedure, classification algorithm and model-and-data checking function. De La Torre [29] proposed the G-DINA, as a general cognitive diagnosis framework similar to GDM, which subsumes many existing CDMs like DINA. Henson et al. [60] proposed the Log-Linear Cognitive Diagnosis Model (LCDM) based on GDM, which integrates the log-linear model into the calculation"}, {"title": "4 MACHINE LEARNING-BASED COGNITIVE DIAGNOSIS MODELS", "content": "In recent years, with the development of AI-based education, cognitive diagnosis has raised the attention of researchers from computer science, especially artificial intelligence. CDMs based on machine learning (ML), especially deep learning (DL), are consequently proposed [86]. With the advantages of better fitting ability and more flexible structures to make use of different types of educational data (e.g., response, item content, knowledge concept structure), machine learning- based CDMs have achieved much success, leading to a new trend of research. We roughly classify these works according to their proposed time and technology/target in Fig. 9.\n4.1 Non-deep-learning Models\nIn the earlier works, there were several attempts using clustering algorithms to classify examinees into different clusters, where each of the clusters represents a type of knowledge mastery pattern. For example, Chiu [24] adopted K-means clustering combined with hierarchical agglomerative cluster analysis, Guo et al. [57] adopted spectral clustering algorithms. Support vector machine (SVM) was also used in several studies for cognitive diagnosis [84, 174], where examinees' response logs are input as features and possible knowledge mastery statuses are predicted by SVM. Supervised training data including the known knowledge status is required for SVM-based methods, which limits the practicality of these models. Collaborative filtering methods such as matrix factorization were also adopted to solve the cognitive diagnosis problem in education [129, 130, 134]. However, as these models focus on predicting students' performance instead of diagnosing students' knowledge proficiencies, the estimated student parameters are not explicitly explainable. Wu and Liu et al. [89, 157] proposed FuzzyCDF which integrates the fuzzy set to handle both objective and subjective test items. Wu et al. [156] introduced a variational Bayesian inference algorithm for IRT, which provides a faster and more accurate human ability estimation compared to traditional IRT, especially on large-scale datasets."}, {"title": "4.2 Deep learning-Based Models", "content": "Integrating deep learning methods has been a new trend in cognitive diagnosis. According to the model architectures and their emergence time, deep learning-based CDMs can be generally classified into mastery pattern classifier, cognitive interaction simulator, and encoder-decoder-based architectures. In \u00a74.2.1 we will review these types of CDMs in detail, following an exploration of multifaceted information (\u00a74.2.2) and other topics in cognitive diagnosis research (\u00a74.2.4).\n4.2.1 DL-based Architectures.\n(1) Mastery Pattern Classifier.\nArtificial neural networks were initially adopted in cognitive diagnosis in a reverse manner compared to traditional models, as depicted by Fig. 10 (a). Typically, as introduced in the Introduction, CDMs simulate the cognitive mechanism of the human's item-answering process. Therefore, the goal of cognitive diagnosis, i.e., humans' ability levels, is actually at the input side of traditional models because it's the cause of human responses. The ability evaluation is actually done through model training instead of model inference. In contrast, Gierl et al. [51] proposed a reversed model structure based on neural networks, which takes the examinee's response pattern (i.e., a binary vector that indicates his/her responses) as input, and directly outputs the examinee's attribute pattern (i.e., a vector that indicates his/her mastery on each knowledge concept). As a result, the CDM becomes a classification model, which can be abstracted as:\n0 = gNN(Response, \u03a9). (5)\nAs empirical response data with the examinee's true ability levels is unavailable, the model is trained with simulated data, which is generated using traditional CDMs such as AHM. Similarly, Cui et al. [25] adopted the self-organizing map to construct the classification model for ability evaluation. The main advantage of these methods is that the ability evaluation can be done with model inference after model training, even if the examinee is out of the training data. However, as pointed out by Briggs et al [15], the performance of these models is limited by the CDMs that generate the training data. When the items and/or data generation model is flawed, the trained CDM will naturally incorporate those flaws. Moreover, the ability estimation can be unstable after multiple times of model training.\n(2) Cognitive Interaction Simulator.\nMost deep learning-based CDMs still follow the traditional practice, i.e., model the cognitive interaction during the item answering process like a simulator, as depicted by Fig. 10 (b). The differences mainly lie in how the interactions are modeled. As pointed out by Wang et al. [145], traditional psychometric-based CDMs rely on domain experts to design interaction functions for predicting examinees' responses. Although this approach offers high interpretability, it is costly, and the fixed function form often leads to weak fitting and generalization capabilities. Wang et al. [145] made an initial break through along this line. They abstracted the cognitive factors involved in the process of answering questions and attributed them to student factors, exercise factors, and interaction functions. A neural network-based framework called NeuralCD was proposed, which can be generally formulated as follows:\nResponse = fNn (\u03b8, \u03b2, \u03a9). (6)\nHere, \u03b8 represents the examinee's ability level and is modeled with a multi-dimensional continuous vector, where the value of each entry indicates the proficiency of a specific knowledge concept. The item parameters \u1e9e here can be the knowledge difficulty, item discrimination, etc. The main difference between NeuralCD and psychometric-based CDMs is that NeuralCD introduced the data-driven strategy to learn the interaction function with neural networks from actual response data. The"}, {"title": "4.2.2 Integration of Multifaceted Information.", "content": "Despite significant progress in cognitive diagnosis interaction function technology, the bottleneck of cognitive diagnosis arises from the initialization of diagnosis factors (learner features and test item features) solely based on their IDs. Thus, researchers have begun exploring ways to enhance the expressive ability of diagnosis factors with multifaceted information, including side-information and domain priors, aiming to further improve the interpretability and performance of diagnosis models.\nLet X denote the multifaceted information. We hence model the CDM that introduces multifaceted information as follows:\nResponse = fNn (\u03b8, \u03b2, \u03a9|0 \u2190 fNNuser (\u03a7, \u03a9user), \u03b2 \u2190 fNNitem (\u03a7, \u03a9item)), (8)\nwhere fNNuser and fNNitem are feature extraction functions to extract useful clues from multifaceted information to generate solid diagnosis factors, i.e., learner cognitive traits and test item traits. fNN is the interaction function for the diagnosis prediction. The multifaceted information X commonly utilized in cognitive diagnosis can be categorized into three types as follows.\nLearner-side information. From the perspective of learners, extra information typically in- cludes learner features or profiles, such as age, gender, behaviors, and preferences, as well as contextual information like school details, family income, and parental occupation, all of which are relevant to the learner. The learner-side information can provide richer insights than mere IDs. Zhou et al. [175] leveraged learner features (e.g., gender, age, region) as the initial profile and contextual information related to the learners (e.g., school details and parents' occupation) to uncover implicit relations between learners' contextual information and their performance in practice. This study not only enhances diagnostic performance but also sets the groundwork for subsequent research on fairness in education [173].\nItem-side information. In addition to relevant knowledge concepts/skills, commonly employed item-side information encompasses the item contents, such as texts and images, as well as exceptional factors like guessing and slipping, which offer detailed semantic cues to represent item traits. Song et al. [120], Cheng et al. [21], Gao et al. [47], and Wang et al. [146] extracted item difficulty and discrimination from text or image content, enhancing the extensibility of CDMs to cold-start items. Gao et al. [45] established semantic relationships between item text and knowledge concepts, enhancing the interpretability of cognitive diagnosis.\nRelational graph-based information. Many studies introduce relational graphs based on the educational priors, such as knowledge concept (KC) graphs and item-concept association graphs, to enhance the representations of both learners and items. Gao et al. [46] and Su et al. [122] modeled the heterogeneous graph structures of learner-item-knowledge to fully explored higher-order interaction relationships between the nodes and the dependency relationships between knowledge concepts within a concept map, thus enhancing the representation of learner cognitive states and item features. Li et al [79] proposed the HierCDF framework to model the influence of hierarchical knowledge structures on cognitive diagnosis. Song et al. [120] focused on the effective fusion of knowledge concept maps with knowledge concept dependency relations and item features. Jiao et al. [66] revealed the relationships between knowledge concepts and items, as well as concept dependency relations within the knowledge concept map, enhancing the representation of item and learner features."}, {"title": "4.2.3 Combination of cognitive diagnosis and knowledge tracing.", "content": "As mentioned in \u00a72.1, cognitive diagnosis assumes constant ability, which is a big difference compared to knowledge tracing. This assumption is reasonable in circumstances when examinees' ability is stable in a relatively short time period. However, there are indeed situations that need to model the changes in examinees' ability levels. For example, at an online learning platform, a learner may receive exercises related to a certain knowledge concept multiple times to practice. This type of learning process involves changes in learners' abilities, which the learners and maybe other platform users care about."}, {"title": "4.2.4 Other Issues.", "content": "Considering the practical demands of real-world scenarios", "sparse issue": "hat introduces biases into their diagnosis results. To address this challenge"}, {"title": "A Survey of Models for Cognitive Diagnosis: New Developments and Future Directions", "authors": ["FEI WANG", "WEIBO GAO", "QI LIU", "JIATONG LI", "GUANHAO ZHAO", "ZHENG ZHANG", "ZHENYA HUANG", "MENGXIAO ZHU", "SHIJIN WANG", "WEI TONG", "ENHONG CHEN"], "abstract": "Cognitive diagnosis has been developed for decades as an effective measurement tool to evaluate human cognitive status such as ability level and knowledge mastery. It has been applied to a wide range of fields including education, sport, psychological diagnosis, etc. By providing better awareness of cognitive status, it can serve as the basis for personalized services such as well-designed medical treatment, teaching strategy and vocational training. This paper aims to provide a survey of current models for cognitive diagnosis, with more attention on new developments using machine learning-based methods. By comparing the model structures, parameter estimation algorithms, model evaluation methods and applications, we provide a relatively comprehensive review of the recent trends in cognitive diagnosis models. Further, we discuss future directions that are worthy of exploration. In addition, we release two Python libraries: EduData for easy access to some relevant public datasets we have collected, and EduCDM that implements popular CDMs to facilitate both applications and research purposes.", "sections": [{"title": "1 INTRODUCTION", "content": "Measurement is an integral part of modern science as well as of engineering, commerce, and daily life [123]. Through measurement, we learn quantitatively about the things around us and even human beings. Cognitive diagnosis, as a representative, aims to measure the cognitive status of individuals, especially the ability levels such as knowledge structures and processing skills, so as to provide information about their cognitive strengths and weaknesses [74]. For example, through cognitive diagnosis, we can learn about whether a student has mastered specific knowledge concepts [74] or whether a patient is mentally healthy [32]. Therefore, cognitive diagnosis provides informative results for test developers and test takers, as well as helps with personalized support such as training course planning for employers and learning resource recommendations for students.\nUnlike conventional physical measurement objects such as length and weight, a person's ability level is a psychological characteristic not directly observable. Therefore, the fundamental idea of measuring human ability is by conducting tests and inferring examinees' ability through their performance. Francis Galton is thought as the first to apply statistical methods to the study of human differences and inheritance of intelligence, and proposed the first personality test [117]. Using the scores obtained in a test is a straightforward way to represent a person's overall ability level, and is widely adopted in IQ tests, teaching, etc. However, although test scores reflect examinees' ability level to some extent, they are not the ability level themselves. By contrast, cognitive diagnosis infers the ability level hidden in the responses. The complete cognitive diagnosis procedure (especially traditional cognitive diagnosis) requires multiple steps, including preparatory work such as deciding the measurement goal, arranging the knowledge structures, and constructing tests. A simplified procedure includes: 1) Test construction: rigorous questionnaires or test items (e.g., Q1 ~ Q5) can be constructed for response collection in fields such as education and psychotherapy [59]. The relation between items and relevant attributes (i.e., skills or knowledge concepts) are usually provided by experts. 2) Response data collection: the responses here mostly refer to binary 0/1 values indicating the results of examinees' answers (e.g., incorrect/correct) or discrete scores obtained on the items (e.g., 5 points out of 8). In some situations, responses are not limited to question answering, for example, the outcome of adversarial games [56] and law cases [3]. 3) Cognitive diagnosis model (CDM) designing: well-designed CDMs are an important guarantee of valid diagnostic results. 4) Psychological factor estimation: based on the collected response data, the psychological factors (e.g., the ability parameters) within CDMs are estimated. 4) Diagnosis feedback: the diagnosed ability levels are then fed back to the examinees.\nThe feedback can be different depending on the CDMs, such as overall ability (\u00a73.1), whether or not mastered certain attributes (\u00a73.2), and proficiency levels of certain attributes (\u00a74.2)."}, {"title": "2 THE OVERVIEW OF COGNITIVE DIAGNOSIS MODELS", "content": "Cognitive diagnosis models are essential for cognitive diagnosis, aiming to infer the unob- servable ability levels from the observable re- sponses to test items. Essentially, most existing CDMs are/contain simulations of examinees' cognitive processes. Specifically, as illustrated when answering the test items, ex- examinees go through a cognitive process that handles the items with their knowledge status and then provide their responses to the items. The responses depend on multiple factors, in- cluding the characteristics of both items (e.g., difficulty [111], relevant knowledge concept, guessing [28]) and examinees (e.g., ability, gaming behavior [158]). Therefore, the central problem of cognitive diagnosis is to model the relation between the examinees' ability levels and their observable behaviors such as responses to test items [43]. This simulation can be formulated as:\n$\\Pr(response) = f (\u03b8, \u03b2, \u03a9)$,\nwhere \u03b8, \u03b2 are the parameters indicating the examinees' abilities and items' features, respectively. \u03a9 represents the possible parameters required by the CDM itself (empty for some models).\nOriginating from psychometrics, the models for measuring human abilities have been developed for decades. The proposal of item response theory (IRT) can be traced back to the 1950s by Fredrick Lord [94]. IRT is a general framework for specifying mathematical functions that describe the interactions of persons and test items, where unidimensional parameters were adopted to describe the abilities of the persons. However, as suggested by researchers such as Glaser [52] and Mislevy [104], IRT and its previous test theories (e.g., classical test theory [37]) only measure the macro ability of individuals. Psychology was suggested to be combined with psychometrics in order to model the micro knowledge structure and cognitive processing of persons during the assessments so that the diagnostic results can be more instructional. The term cognitive diagnosis model (CDM) is originally adopted to denote such models\u00b9. The proposal and usage of Q-matrix was a significant milestone of CDM [124]. Subsequently, representative models such as AHM [75], DINA [28] and NIDA [68] were proposed based on different assumptions to simulate the knowledge structures or cognitive processes, and each examinee is classified into a mastery pattern representing his/her mastery of each specific skill. These models fall into the cognition level paradigm [104].\nIn recent years, some researchers have been rethinking the issue of cognitive diagnosis from the perspective of machine learning and have proposed novel solutions [86]. Collaborative filtering and matrix factorization methods were adopted to model learners' ability and to predict learners' test performance [129, 130, 134]. Gierl et al. [51] proposed a neural network-based ability classifier trained with the data generated by a pre-trained attribute hierarchical model. More recently, Wang et al. recognized the limitations of expert-designed interaction functions and proposed a new data-driven cognitive diagnosis framework called NeuralCD [145]. Deep learning-based models incorporated with the theories/hypotheses from psychometrics have the advantage of better fitting ability of the sophisticated cognitive process, as well as promising interpretability. Since then, such deep learning-based paradigm gradually becomes a new tendency and has been attracting increasing attention [97, 146, 150]. In addition to the usage of various deep learning technologies, this"}, {"title": "2.1 Preliminary", "content": "Suppose there are examinees S = {S\u2081, . . ., S\u2081}, test items Q = {Q1, . . ., Qj } and K item attributes\n(i.e., skills or knowledge concepts). The responses are denoted as R = {R\u00a1, i = 1, . . ., I}, where Ri\ndenotes the responses of S\u012f. R\u2081 = {(Si, Qj, rij), Si \u2208 S, Qj \u2208 Q} denotes S\u00a1's responses and rij is the\nresponse result of Si on Qj. Usually there is an expert-labeled Q-matrix Q = {qjk}J\u00d7K, where qjk=1\n(or 0) indicating that item Qj requires (or does not require) the mastery of attribute k to answer\nit correctly. Sometimes there are extra multifaceted information available, such as item content\nand examinees' background, which we denote as X. The problem of cognitive diagnosis can be\ngenerally defined as follows:\nProblem Definition. With the input R, Q and possible X, the goal of cognitive diagnosis is to out-\nput examinees' ability levels \u03b8\u2081 (i = 1, . . ., I), where \u03b8; is either unidimensional or multidimensional\nindicating the overall ability levels or the mastery levels of each attribute.\nThe practicability of cognitive diagnosis is based on several basic assumptions.\nAssumption 1. Constant ability. The cognitive status of concern, i.e., ability level, remains\nunchanged during the process of answering the test items.\nIt is reasonable to assume that a person's ability level does not change in a short time (e.g., during\na standard test), during which the person's ability level can be measured based on the responses to\ntest items. Here lies the big difference between cognitive diagnosis and knowledge tracing, of which\nthe latter also attracted wide attention in recent years. Knowledge tracing focuses on modeling\nthe changing patterns of online learners' knowledge states (either explainable or unexplainable),\nwhich highly relies on sequential modeling methods such as hidden Markov chains and Recurrent\nNeural Networks. The cognitive process is usually neglected in the knowledge tracing models, and\npredicting learners' future performance is the most adopted task. By contrast, cognitive diagnosis\naims to measure learners' ability level within a certain period of time. It mines the response data\nof learners, models the cognitive process of answering items, and provides the values of learners'\nability levels within a certain metric space.\nAssumption 2. Constant item characteristics. The characteristics of a test item remain constant\nover all of the testing situations where it is used [111].\nSome statistics of the item such as the correct rate can be influenced by the examinees. However,\nthe characteristics of a test item, such as the difficulty, discrimination, and relevant knowledge\nconcepts, reflect the essential features of the item and should not change. This type of stability\ncontributes to the fairness of test items for all examinees, and suggests that test items can be\nrepresented by fixed parameter values that reflect these characteristics.\nAssumption 3. Monotonicity. The probability of a correct response to the test item increases,\nor at least does not decrease, as the locations of examinees increase on any of the coordinate\ndimensions [111].\nMost cognitive diagnosis models adopt the monotonicity assumption in their modeling of cog-\nnitive processes, especially IRT-based and MIRT-based models. This assumption suggests that a"}, {"title": "2.2 A Brief Review of Cognitive Diagnosis Model Development", "content": "Without cognitive diagnosis, the most widely adopted method to evaluate a learner's ability is through their scores obtained in tests. Classical Test Theory (CTT) [37] was proposed to eliminate the errors existing in the scores. However, the score is the observed reflection of ability with the influence of factors such as question attributes and other psychological characteristics. To extract the actual ability hidden in the observations, cognitive diagnosis models sprouted from Psychometrics and have undergone decades of study. The development of cognitive diagnosis can be summarized from the aspects of both model structures and data characteristics.\n2.2.1 The development of model structures. Basically, the development of cognitive diagnosis models can be divided into two stages, i.e., psychometrics-based models and machine learning- based models. Each stage of the development can be further divided into two sub-stages as follows:\nPsychometrics-based models. At the first stage of cognitive diagnosis development, ability mea- surement were based on psychometrics. Early research works were summarized as the ability level paradigm [104], as they used unidimensional or multidimensional latent vectors to represent exam- inees' overall ability levels. Representative methods include IRT and multidimensional IRT (MIRT). According to [111], its popularity is generally attributed to the work of Fredrick Lord and Georg Rasch starting in the 1950s and 1960s. Both IRT and MIRT have undergone lots of development and there is a large class of implementations. \u00a73.1 will provide a more detailed review of representative models. With the demand of measuring fine-grained ability, i.e., mastery of knowledge concepts or skills, the cognition level paradigm was proposed to improve diagnostic performance. The\n2.2.2 The changes of exploited data. In the early research works, the response data for diagnosing examinees' ability is collected from scale-based tests, where scales (e.g., questionnaires, test papers) are constructed and tests are intentionally organized. Only numerical data, i.e., correct, incorrect, and scores, is utilized in early psychometrics-based methods, such as IRT, MIRT, and DINA. After that, some psychometrics-based models leverage simple hierarchical structures among a small number of knowledge concepts by either using them to help with the defining of Q-matrix or explicitly modeling the hierarchical structures, such as AHM. Overall, the data types that can be utilized in pure psychometrics-based models are limited due to the simplicity of model structures.\nWith the usage of machine learning, researchers have been making attempts to leverage more types of data that contain relevant information for cognitive diagnosis. Especially after NeuralCD [145] validated the superiority of using deep learning methods in cognitive diagnosis, including better fitting ability and extensibility without losing explainability, following studies started to explore diverse types of data, including test item contents, examinees' background information and sophisticated graph-structured data. Moreover, the response data in consideration is not limited to scale-based tests. The popularity of online learning systems provides more opportunities of accessing learners' daily behavioral data as well as diverse data types. Therefore, cognitive diagnosis can be conducted even without organizing tests in an interruptive way if we regard learners' ability as constant during a short time period. Some researchers addressed the data sparsity problem within learners' response logs [146, 164], and considered supplementary data such as response time and hints [130, 158, 167, 168]. More detailed behaviors such as keystrokes and eye tracking can be available, however, they still need further exploration."}, {"title": "3 PSYCHOMETRICS-BASED COGNITIVE DIAGNOSIS MODELS", "content": "Traditional cognitive diagnosis models (CDMs) utilize statistical methods based on psychometrics to model examinees' cognitive status. These models can be categorized into two paradigms, i.e., the ability level paradigm and the cognition level paradigm\n3.1 Ability Level Paradigm\nIn the ability level paradigm, researchers focus on the estimation of the overall ability of examinees reflected on tests. Traditional models following the ability level paradigm usually model examinees' overall ability levels by low-dimensional 0, which can be jointly estimated with low-dimensional item parameters such as discrimination and difficulty. As mentioned before, we include Item Response Theory (IRT) [16, 67] and Multidimensional IRT (MIRT) [111] into the review even if they were proposed before the term cognitive diagnosis appeared.\n3.1.1 Item Response Theory (IRT). IRT [16, 67] is one of the most classical latent trait methods for measuring human cognitive status. The core assumption of IRT is that the relation between examinees' responses and their ability levels can be modeled by a continuous mathematical function. i.e., $\\Pr(rij = 1) = f (0\u2081, \u03b2j)$, where \u03b8\u00a1 is a scalar parameter indicating the ability level of examinee i, and \u03b2j denotes the latent traits of test item j. Many IRT-based models are simply called IRT, among which the most representative models include one-parameter logistic IRT (1PL-IRT), two- parameter logistic IRT (2PL-IRT) and three-parameter logistic IRT (3PL-IRT). 1PL-IRT only uses a scalar parameter bj to capture the difficulty of item j. 2PL-IRT adds an extra scalar parameter aj to indicate the discrimination of item j. While 3PL-IRT adds a parameter cj for item j, which is mostly interpreted as the probability of correctly guessing the answer. These models take dichotomous response scores into consideration, i.e., rij = 0, 1 indicating incorrect and correct responses respectively. Their formulas are as follows:\n$\\1PL-IRT: Pr(rij = 1|0i, bj) = \u03c3(\u03b8; \u2212 bj) = 1/(1+e-(\u03b8i-bj)), (1)$\n$\\1PL-2RT: Pr(rij = 1|0i, aj, bj) = 1/(1+e-aj (0i-bj)), (2)$\n$\\1PL-3RT: Pr(rij = 1|0i, aj, bj, cj) = cj + (1 \u2212 cj) 1/(1+ e-aj (0;-bj)), (3)$\nDespite IRT models represent the ability levels in the simplest scalar form, their excellent mathematical properties (e.g., the convex property of the interaction function) allow them to be applied to a variety of downstream tasks such as computerized adaptive testing. Although IRT models for dichotomous items attract more attention, there have been extended models for polytomous items. For polytomous items, scores can be multiple-graded. For example, for an item whose full score is 10, the obtained scores might be 0, 5, 8 and 10. Researchers have developed various polytomous IRT models such as Partial Credit Rasch Model [101], Rating Scale Model [4] and Graded Response Model [118].\n3.1.2 Multidimensional Item Response Theory (MIRT). MIRT [111] extends the ability modeled in IRT to multidimensional cases. Similar to exploratory factor analysis (EFA), MIRT allows for the exploration of multidimensionality and complex relationships between observed variables and latent traits, particularly in the context of assessments or tests. The simplest form of MIRT is a direct extension from 2PL-IRT and is given as Pr(rij = 1|0i, aj, bj) = 1/(1+e^(a0i+b;)), where aj denotes item discrimination on multiple dimensions, and bj is related to item difficulty. In practice, a small dimension is usually enough for MIRT, and each dimension of 0; represents a specific ability required to successfully answer the item. However, when using low-dimensional parameters, the ability is hard to explain explicitly. just like factor analysis. Besides relating to EFA and extending the dimension of IRT, MIRT also connects the cognition level paradigm. In the cognition level paradigm, researchers focus more on the fine-grained cognitive states of examinees, such as proficiency levels on pre-defined knowledge concepts. Along this line, da Silva et al. [26] introduce the Q-matrix into the interaction function of MIRT to obtain examinees' knowledge-concept-wise latent traits. To ensure the identifiability of MIRT, item discrimination vector aj of different items are usually rotated to the same value to acquire identifiable estimations of examinees' abilities [7].\n3.2 Cognition Level Paradigm\nIn the cognitive level paradigm, researchers focus on the estimation of the fine-grained cognitive states of students. For instance, in K-12 course learning, test designers require diagnosing students' proficiency level on knowledge concepts (e.g., the concept linear function in mathematics) from their test performances. Therefore, CDMs in the cognitive level paradigm are utilized to estimate student knowledge proficiencies in this scenario. Since such a diagnosis process can be viewed as classifying students to an \u201cideal\u201d proficiency pattern that is most suitable for his/her test"}, {"title": "3.2.1 Rule Space Method (RSM) And Its Variations.", "content": "The RSM proposed by Tatsuoka in the 1980s [124], is a statistical modeling approach used for cognitive diagnosis. Compared with IRT, RSM focuses on representing the cognitive processes that individuals use to respond to test items, which is more fine-grained. It seeks to identify the specific cognitive rules or strategies that individuals employ when answering test items. Four steps are included in RSM, i.e., item decomposition, rule specification, rule space construction, and rule space matching.\nThe RSM is a fundamental yet significant method in traditional CD, and is the basis of many DCMs [74, 75]. One shortcoming of the traditional RSM is that it views knowledge concepts as independent entities and ignores their hierarchical relationship in the cognitive process (e.g., knowledge dependency). Therefore, Leighton et al. [75] proposed the Attribute Hierarchy Method (AHM) to address this issue. Compared to the original RSM, the AHM assumes that attributes (i.e., knowledge concepts or skills that are required for students to solve a test item) are organized in a hierarchical structure, which can be represented by an adjacent matrix of attributes. Then the AHM limits the rule space defined in the RSM, such that the mastery of any child attribute should be no less than the mastery of its parent attribute. Then the AHM matches each student into the most similar ideal response patterns, with the corresponding rule as the diagnostic result of the student."}, {"title": "3.2.2 DINA And Relevant CDMs.", "content": "Deterministic Input, Noisy \u201cAnd\u201d Gate (DINA) model [28] is a representative and recognized CDM. DINA and its relevant CDMs diagnose students' knowledge concept-wise abilities from binary response data and expert-labeled question-knowledge relation- ship. The core assumption of DINA is that the proficiency of different knowledge concepts is non-compensatory. That is, the model assumes that mastery of all required attributes is necessary for a correct response, but also allows for the possibility of guessing. In DINA, each student i's ability status is modeled as a binary knowledge mastery pattern vector \u03b8\u2081 = (\u03b8\u2081\u2081, . . ., \u03b8\u03af\u03ba). Here K denotes the number of knowledge concepts, and the binary value \u03b8ik \u2208 {1,0} denotes whether or not student i has mastered the knowledge concept k. Items are modelled by \"slip\" and \"guess\" parameters and a pre-given binary Q-matrix Q = (q1, ..., qj) = (qjk)J\u00d7K. The interaction func- tion of DINA is defined as \\Pr(rij = 1|0i, qj, sj, gj) = (1 \u2013 sj)Nijg-Nij, where $Nij = \u03a0\u039a-109k$ is the indicator of whether the student has mastered all required knowledge concepts of the item. The sj\nT\nK\nk=1ik"}, {"title": "3.2.3 General Diagnostic Model (GDM).", "content": "The General Diagnostic Model (GDM) [139] is a general framework that subsumes many classical and well-known CDMs like DINA, IRT and MIRT [14, 140]. As a general framework, GDM is suitable for various real-world scenarios, including but not limited to dichotomous/polytomous response scores, binary/continuous/polytomous ordinal knowledge mastery levels, etc. We introduce the general form of GDM in this section.\nFormally, let @ be a K-dimensional skill profile consisting of polytomous or dichotomous skill attributes \u03b8k(k = 1, . . ., K). Then the probability of a polytomous response score x \u2208 {0, ..., mj} to item j under the GDM with an individual with skill profiles e is defined as\n$\\Pr(rj = x|0 = (01,..., \u03b8\u03ba)) = exp [Bjx + \u03a3k=1 Yjxkhj(qjk, \u03b8k)]/[1 + \u03a3+1 exp [\u1e9ejy + \u03a3k=1Yjykhj (qjk, 0k)]], (4)$\nmj\nwhere \u1e9e jx and y jxk (j = 1, 2, . . ., J) are estimable item parameters. Each element qjk, j = 1, 2, . . . J, k = 1, 2, . . ., K of the Q-matrix is a constant, as in other DCMs like DINA. The helper function h(\u00b7, \u00b7) maps qjk and Ok to a real number, which considers the fact that the knowledge profile @ might be polytomous. By elaborately designing the form of the parameter y and the helper function, GDM can be flexibly applied to either binary or polytomous response data. As a constraint latent class model, the parameter estimation of GDM is usually done with expectation-maximization (EM) algorithm [33, 65].\nDue to its generality and flexibility, there are also many other versions for GDM to adjust some special scenarios, like mixture distribution extensions of GDM which consider the ability prior of student groups, and hierarchical extensions of GDM which consider the multilevel distribution of student abilities. Indeed, many traditional CDMs, including CDMs in the ability level paradigm like IRT and MIRT [14], and CDMs in the cognitive level paradigm like LCDM and DINA [140], have been proven to be special cases of GDM."}, {"title": "3.2.4 Other Traditional CDMs.", "content": "Besides representative CDMs introduced above, there are also some other traditional CDMs that focus on different research challenges in the context of educational measurement. For instance, the Reparameterized Unified Model (RUM) [58], as a refinement of DINA, aims to construct a cognitive diagnosis assessment system that includes DCM models, estimation procedure, classification algorithm and model-and-data checking function. De La Torre [29] proposed the G-DINA, as a general cognitive diagnosis framework similar to GDM, which subsumes many existing CDMs like DINA. Henson et al. [60] proposed the Log-Linear Cognitive Diagnosis Model (LCDM) based on GDM, which integrates the log-linear model into the calculation"}, {"title": "4 MACHINE LEARNING-BASED COGNITIVE DIAGNOSIS MODELS", "content": "In recent years, with the development of AI-based education, cognitive diagnosis has raised the attention of researchers from computer science, especially artificial intelligence. CDMs based on machine learning (ML), especially deep learning (DL), are consequently proposed [86]. With the advantages of better fitting ability and more flexible structures to make use of different types of educational data (e.g., response, item content, knowledge concept structure), machine learning- based CDMs have achieved much success, leading to a new trend of research. We roughly classify these works according to their proposed time and technology/target in Fig. 9.\n4.1 Non-deep-learning Models\nIn the earlier works, there were several attempts using clustering algorithms to classify examinees into different clusters, where each of the clusters represents a type of knowledge mastery pattern. For example, Chiu [24] adopted K-means clustering combined with hierarchical agglomerative cluster analysis, Guo et al. [57] adopted spectral clustering algorithms. Support vector machine (SVM) was also used in several studies for cognitive diagnosis [84, 174], where examinees' response logs are input as features and possible knowledge mastery statuses are predicted by SVM. Supervised training data including the known knowledge status is required for SVM-based methods, which limits the practicality of these models. Collaborative filtering methods such as matrix factorization were also adopted to solve the cognitive diagnosis problem in education [129, 130, 134]. However, as these models focus on predicting students' performance instead of diagnosing students' knowledge proficiencies, the estimated student parameters are not explicitly explainable. Wu and Liu et al. [89, 157] proposed FuzzyCDF which integrates the fuzzy set to handle both objective and subjective test items. Wu et al. [156] introduced a variational Bayesian inference algorithm for IRT, which provides a faster and more accurate human ability estimation compared to traditional IRT, especially on large-scale datasets."}, {"title": "4.2 Deep learning-Based Models", "content": "Integrating deep learning methods has been a new trend in cognitive diagnosis. According to the model architectures and their emergence time, deep learning-based CDMs can be generally classified into mastery pattern classifier, cognitive interaction simulator, and encoder-decoder-based architectures. In \u00a74.2.1 we will review these types of CDMs in detail, following an exploration of multifaceted information (\u00a74.2.2) and other topics in cognitive diagnosis research (\u00a74.2.4).\n4.2.1 DL-based Architectures.\n(1) Mastery Pattern Classifier.\nArtificial neural networks were initially adopted in cognitive diagnosis in a reverse manner compared to traditional models, as depicted by Fig. 10 (a). Typically, as introduced in the Introduction, CDMs simulate the cognitive mechanism of the human's item-answering process. Therefore, the goal of cognitive diagnosis, i.e., humans' ability levels, is actually at the input side of traditional models because it's the cause of human responses. The ability evaluation is actually done through model training instead of model inference. In contrast, Gierl et al. [51] proposed a reversed model structure based on neural networks, which takes the examinee's response pattern (i.e., a binary vector that indicates his/her responses) as input, and directly outputs the examinee's attribute pattern (i.e., a vector that indicates his/her mastery on each knowledge concept). As a result, the CDM becomes a classification model, which can be abstracted as:\n0 = gNN(Response, \u03a9). (5)\nAs empirical response data with the examinee's true ability levels is unavailable, the model is trained with simulated data, which is generated using traditional CDMs such as AHM. Similarly, Cui et al. [25] adopted the self-organizing map to construct the classification model for ability evaluation. The main advantage of these methods is that the ability evaluation can be done with model inference after model training, even if the examinee is out of the training data. However, as pointed out by Briggs et al [15], the performance of these models is limited by the CDMs that generate the training data. When the items and/or data generation model is flawed, the trained CDM will naturally incorporate those flaws. Moreover, the ability estimation can be unstable after multiple times of model training.\n(2) Cognitive Interaction Simulator.\nMost deep learning-based CDMs still follow the traditional practice, i.e., model the cognitive interaction during the item answering process like a simulator, as depicted by Fig. 10 (b). The differences mainly lie in how the interactions are modeled. As pointed out by Wang et al. [145], traditional psychometric-based CDMs rely on domain experts to design interaction functions for predicting examinees' responses. Although this approach offers high interpretability, it is costly, and the fixed function form often leads to weak fitting and generalization capabilities. Wang et al. [145] made an initial break through along this line. They abstracted the cognitive factors involved in the process of answering questions and attributed them to student factors, exercise factors, and interaction functions. A neural network-based framework called NeuralCD was proposed, which can be generally formulated as follows:\nResponse = fNn (\u03b8, \u03b2, \u03a9). (6)\nHere, \u03b8 represents the examinee's ability level and is modeled with a multi-dimensional continuous vector, where the value of each entry indicates the proficiency of a specific knowledge concept. The item parameters \u1e9e here can be the knowledge difficulty, item discrimination, etc. The main difference between NeuralCD and psychometric-based CDMs is that NeuralCD introduced the data-driven strategy to learn the interaction function with neural networks from actual response data. The"}, {"title": "4.2.2 Integration of Multifaceted Information.", "content": "Despite significant progress in cognitive diagnosis interaction function technology, the bottleneck of cognitive diagnosis arises from the initialization of diagnosis factors (learner features and test item features) solely based on their IDs. Thus, researchers have begun exploring ways to enhance the expressive ability of diagnosis factors with multifaceted information, including side-information and domain priors, aiming to further improve the interpretability and performance of diagnosis models.\nLet X denote the multifaceted information. We hence model the CDM that introduces multifaceted information as follows:\nResponse = fNn (\u03b8, \u03b2, \u03a9|0 \u2190 fNNuser (\u03a7, \u03a9user), \u03b2 \u2190 fNNitem (\u03a7, \u03a9item)), (8)\nwhere fNNuser and fNNitem are feature extraction functions to extract useful clues from multifaceted information to generate solid diagnosis factors, i.e., learner cognitive traits and test item traits. fNN is the interaction function for the diagnosis prediction. The multifaceted information X commonly utilized in cognitive diagnosis can be categorized into three types as follows.\nLearner-side information. From the perspective of learners, extra information typically in- cludes learner features or profiles, such as age, gender, behaviors, and preferences, as well as contextual information like school details, family income, and parental occupation, all of which are relevant to the learner. The learner-side information can provide richer insights than mere IDs. Zhou et al. [175] leveraged learner features (e.g., gender, age, region) as the initial profile and contextual information related to the learners (e.g., school details and parents' occupation) to uncover implicit relations between learners' contextual information and their performance in practice. This study not only enhances diagnostic performance but also sets the groundwork for subsequent research on fairness in education [173].\nItem-side information. In addition to relevant knowledge concepts/skills, commonly employed item-side information encompasses the item contents, such as texts and images, as well as exceptional factors like guessing and slipping, which offer detailed semantic cues to represent item traits. Song et al. [120], Cheng et al. [21], Gao et al. [47], and Wang et al. [146] extracted item difficulty and discrimination from text or image content, enhancing the extensibility of CDMs to cold-start items. Gao et al. [45] established semantic relationships between item text and knowledge concepts, enhancing the interpretability of cognitive diagnosis.\nRelational graph-based information. Many studies introduce relational graphs based on the educational priors, such as knowledge concept (KC) graphs and item-concept association graphs, to enhance the representations of both learners and items. Gao et al. [46] and Su et al. [122] modeled the heterogeneous graph structures of learner-item-knowledge to fully explored higher-order interaction relationships between the nodes and the dependency relationships between knowledge concepts within a concept map, thus enhancing the representation of learner cognitive states and item features. Li et al [79] proposed the HierCDF framework to model the influence of hierarchical knowledge structures on cognitive diagnosis. Song et al. [120] focused on the effective fusion of knowledge concept maps with knowledge concept dependency relations and item features. Jiao et al. [66] revealed the relationships between knowledge concepts and items, as well as concept dependency relations within the knowledge concept map, enhancing the representation of item and learner features."}, {"title": "4.2.3 Combination of cognitive diagnosis and knowledge tracing.", "content": "As mentioned in \u00a72.1, cognitive diagnosis assumes constant ability, which is a big difference compared to knowledge tracing. This assumption is reasonable in circumstances when examinees' ability is stable in a relatively short time period. However, there are indeed situations that need to model the changes in examinees' ability levels. For example, at an online learning platform, a learner may receive exercises related to a certain knowledge concept multiple times to practice. This type of learning process involves changes in learners' abilities, which the learners and maybe other platform users care about."}, {"title": "4.2.4 Other Issues.", "content": "Considering the practical demands of real-world scenarios, recent studies have been focusing on developing cognitive diagnosis methods tailored for specific application contexts. These contexts often come with their own complexities and data constraints, necessitating specialized approaches to effectively address the unique challenges they present.\nCognitive diagnosis under limited data scenarios. The practice data of learners in real learning scenarios often face limitations. For instance, the self-driven nature of online learning leads many learners to selectively engage with items they excel in or practice irregularly, resulting in a 'sparse issue' that introduces biases into their diagnosis results. To address this challenge, Yao et al. [164"}]}]}