{"title": "Spatial Information Integration in Small Language Models for Document Layout Generation and Classification", "authors": ["Pablo Melendez Abarca", "Clemens Havas"], "abstract": "Document layout understanding is a field of study that analyzes the spatial arrangement of information in a document hoping to understand its structure and layout. Models such as LayoutLM (and its subsequent iterations) can understand semi-structured documents with SotA results; however, the lack of open semi-structured data is a limitation in itself. While semi-structured data is common in everyday life (balance sheets, purchase orders, receipts), there is a lack of public datasets for training machine learning models for this type of document. In this investigation we propose a method to generate new, synthetic, layout information that can help overcoming this data shortage. According to our results, the proposed method performs better than LayoutTransformer, another popular layout generation method. We also show that, in some scenarios, text classification can improve when supported by bounding box information.", "sections": [{"title": "Introduction", "content": "In the deep learning era, data quantity and quality have become a necessity to keep improving existing models. Different fields benefit from data augmentation techniques, for example, in text classification tasks, changes at character, word, or sentence level are useful [5]. For images, flipping, cropping, or rotating are basic augmentation techniques [27], and the same goes for sound perturbation for audio tasks [1]; however, classical data augmentation techniques [36] can fall short for data-hungry machine learning solutions, since these are limited to creating new samples by modifying existing ones, and manually creating large amounts of new samples is costly and time consuming.\nAlthough generative artificial intelligence has existed for a while, it was not until the second decade of the 21st century that architectures such as the variational autoencoder (VAE) [22] or the generative adversarial network (GAN) [15] were able to output new, artificially generated, data. Later, the transformer architecture was introduced [42], which has allowed for more groundbreaking advancements in the field, the main example being the Generative Pretrained Transformer (GPT) [35] and its subsequent iterations.\nThanks to these advancements, the creation and consumption of new -synthetic- data is now available for fields such as text, audio, images, and sometimes all at the same time multimodal models [6][13][7][33]. However, some specific fields that could benefit from generative solutions for the creation of new data have not been the center of attention. An example of this are semi-structured documents (e.g., receipts, purchase orders, invoices, etc.). Semi-structured documents might have a fixed set of sections that should be present but not necessarily a fixed layout.\nAvailability for this type of data is scarce, since it usually contains sensitive information like full names, addresses, bank account details, and/or sums of money. This makes downstream tasks (e.g., optical character recognition (OCR) [8][32], intelligent character recognition (ICR) [34]) more challenging since there are not enough public data out there to better train automatic document processing models (e.g., LayoutLM [43]). To solve this challenge, solutions such as LayoutTransformer [16] and LayoutGPT [14] have been proposed to generate new layout samples.\nLarge language models (LLMs), and particularly small large language models (SLMs), have seen a rise in popularity in the last year, with models such as Llama2-7B [29], Llama3-8B [30] and Mistral-7B [31] leading the charge. Though not as powerful as bigger models, SLMs provide a major advantage which is the possibility of running them locally. Running these models locally has further implications, for instance, increased privacy and reduced costs. Considering these facts, a new layout generation technique is proposed in this work, one that leverages scarce data, as well as the generalization capabilities and increased privacy of SLMs.\nAlthough this research does not create complete synthetic documents, layout generation is a small but important step on the way to generating fully-fledged documents. Generating document layouts with SLMs could also mean that other document processing tasks could be carried out by leveraging SLMs and spatial information, an example of that is document text classification. Reliable, automatic text classification can be crucial when the amount of data at hand is large, as it can reduce processing time and potentially lower the cost [17]. Motivated by that, the second part of the investigation aims to test the efficacy of SLMs for document text classification taking"}, {"title": "Related Work", "content": "Semi-structured documents\nSemi-structured data is characterized for not being completely raw but also for not following a strict, fixed, structure; however, there is not a precise definition of what represents a semi-structured document [2]. Smith et al. [26] suggests that semi-structured documents are defined by a mix of physical structure (e.g., section boundaries) and content indicators (e.g., words in sections). Physical structure elements can be paragraph boundaries, while content indicators can be specific words or titles. In this investigation a small dataset of semi-structured documents, specifically receipts, is used for the experiments.\nDocument processing: analysis and understanding\nThe retrieval of information from documents is essential to increase the amount of knowledge [9]. Knowledge does not reside only in the text but also in the layout, Tang et al. [39] define document processing as a two-part process: document analysis and document understanding. Document analysis studies the geometric distribution of the information, while document understanding studies the logical structure of the geometric blocks (spatial information). Hoping to accelerate the process, automatic document processing methods have been proposed, among those are OCR [32][8] and solutions that build upon OCR while including machine learning algorithms and architectures to improve performance (e.g., LayoutLM [43]).\nIn the case of semi-structured documents, and since machine learning models require large amounts of data to better understand the inputs that produce an output, availability of this type of data (specifically receipts and/or proofs of purchase) becomes essential to improve the performance of these models. Despite the existence of some datasets for document layout analysis most of these belong"}, {"title": "Layout Generation", "content": "Using generative AI, different solutions have been proposed to generate layout information for different applications. LayoutGAN [23], as its name implies, makes use of generative adversarial networks [15] to generate new layout information. It is trained on around 25 000 documents containing sections such as heading, title, and captions. The generator takes a set of labels representing graphic elements with random probabilities and samples from both Uniform and Gaussian distributions [23]. A second proposed method is LayoutVAE, based on a variational autoencoder it generates stochastic layouts for different scenes as explained in [21]. LayoutVAE was trained with different datasets for different purposes (none for document layout generation), 5000 training images from the MNIST [11] dataset, and around 113,000 from the COCO dataset [25]. Next comes LayoutTransformer [16], this method improves upon the other two by taking advantage of self-attention to understand the relationships between elements in a scene; for document generation, LayoutTransformer was trained on around 320,000 layouts.\nTo compare the results of our method, we use LayoutTransformer as a baseline.\nThis work proposes a method that uses local SLMs to produce new document layouts. It is believed that these can perform better thanks to their generalization capabilities (due to the massive amount of pretraining data), and their versatility for automated content generation. Three main advantages have been identified: 1) the use of natural language to specify which elements (labels) should be present in the new layout, 2) the possibility of obtaining accurate results even with less than one hundred documents for fine-tuning thanks to the mix of high-quality training samples and the LLM generalization capabilities, and 3) the ability to create new synthetic layout coordinates. This spatial information can later be used in downstream tasks (e.g: developing fully-fledged documents that can be used to train smaller models for document processing). By using open-source SLMs it is possible to keep the privacy of the data if the application requires it."}, {"title": "Text classification and the impact of spatial information", "content": "Using bounding boxes to support document text classification tasks using language models appears to be an unexplored field; nonetheless, [38] states that by explicitly incorporating spatial information into a recurrent neural network for vision applications, classification performance improves, and previously, [18] used two encoding techniques from bag-of-visual-words to add spatial information to text categorization, which resulted in improved results. In the second part of this investigation and to understand if text classification performance improves, spatial information in the form of bounding boxes is added explicitly to the instruction of different SLMs."}, {"title": "Materials and Methodology", "content": "The dataset is composed of one hundred and seven (107) semi-structured document images of proofs of purchase. For each of the images, the following information is available:\na) Manually annotated bounding boxes and labels for each section (layout information). Eight elements were identified (Logo: 173, Header: 211, VAT_Table_Summary: 520, PaymentInformation: 558, LineItemTable: 1176, Footer: 450, Contact: 1055, and InvoiceDetails: 1588, for a total of 5731 labeled elements). The software used for manual annotation was LabelStudio [41], an open-source platform for data labeling.\nb) All strings in the document and bounding boxes for each of them. Strings and corresponding bounding boxes were obtained with a proprietary OCR solution belonging to the local company.", "sections": [{"title": "Language Models", "content": "For this investigation, small but capable language models are chosen. Special attention is given to models with good reasoning and text classification capabilities. Models like Meta's Llama2-7B and Mistral's Mistral-7B were considered, but during the development of the investigation better, more capable models were released. These newer models are also small in terms of parameters, but perform much better in many benchmarks. Models are run locally using LM Studio", "sections": [{"title": "Meta's Llama3-8B and Llama3.1-8B.", "content": "Meta's Llama3-8B and Llama3.1-8B both present a decoder-only architecture, a tokenizer with a vocabulary of 128K tokens, and grouped query attention (GQA) for better inference. Both models are pretrained on \u2248 15T tokens collected from public sources, with 5% of the dataset in other languages other than English. While Llama3.1-8B is an upgraded version of Llama3-8B, specifics are not provided other than being trained on more, higher quality, synthetic data, having extended context length, stronger reasoning capabilities, and being fully multilingual."}, {"title": "Google's Gemma2-9B and BERT-base-cased (German).", "content": "Open-source Google's model, Gemma2-9B[10], uses a tokenizer with a vocabulary of 256k tokens and GQA, same as Llama3-8B and Llama3.1-8B. The model is pretrained on 8T tokens, has a context length of 8K tokens, and it uses a mix of local sliding window and global attention. The primary language of the pretraining data is English.\nAlso developed by Google, the Bi-directional Encoder Representations from Transformers model (BERT) [12] for the German language is an open-source model with an encoder-only architecture that is mainly used for question-answering, text generation, summarization, and text classification. It is trained on German Wikipedia, open legal information and news articles."}]}, {"title": "Llama3-8B for layout generation", "content": "As baseline, the LayoutTransformer model is used with pretrained weights (pretrained for 10 epochs at a learning rate of 1e-5 on 10,000 samples of the PubLayNet dataset for layout generation) and is then fine-tuned with 87% of the small dataset of proofs of purchase for another 40 epochs at a learning rate of", "sections": [{"title": "Baseline.", "content": null}, {"title": "Proposed approach.", "content": "In the proposed approach, the Llama3-8B base model is finetuned with 87% of the small dataset of proofs of purchase. No pretraining with any other layout dataset is performed. The model is loaded in 4-bit quantized form to be able to fine-tuning it locally and the Llama3-8B tokenizer is used for tokenization. For each sample of the dataset (i.e., each document) labels and coordinates of the bounding boxes are presented as a prompt to the model. The prompt instruction is the following: \"Provide bounding box coordinates x1, y1, x2, y2 for these sections of a receipt document: <labels>\", where \"labels\" correspond to every label in the sample separated by a comma. The answer has the following format: \"Label: x1, y1, x2, y2\" for each label, and each is printed in a new line. See Table 1 for an example. All tokenized entries are padded to ensure that all have the same size at training time. We use Low Rank Adaptation (LoRA) for fine-tuning, with a rank of 32, a scaling factor of 64, a dropout of 0.05, no bias, and targeting all linear layers of the model. The model is then fine-tuned for 4 epochs at a learning rate of 1.5e-4 using the AdamW optimizer to optimize learning rate and weight decay separately. Once the model has been finetuned, the next step is to prompt it to return layout information based on provided labels by the user. Finally, the generated coordinates for each label are drawn using Python (as shown in Figure 1).\nSince LayoutTransformer is not conditional (i.e., desired labels cannot be provided in advance) an image-to-image comparison is not a good way to measure performance, instead the performance of the models is measured following this process: Multiple layout samples are generated for both models, and bounding boxes for individual labels are collected. For each label, two clusters are created: one that contains all the origin points (top-left corner) and a second one that contains all the closing points (bottom-right corner) of the bounding boxes. Label clusters are also created for the testing subset, which contains the remaining 13% of the layouts, and serves as ground truth. Once all clusters have been created for the baseline and for the proposed approach, cluster centroids and bounding box average area sizes are calculated at label level. These are then compared to those of the ground truth; Mahalanobis distance between centroids and area size differences are provided and used as performance indicators. Mahalanobis distance is preferred over the"}]}, {"title": "Text classification with spatial information", "content": "The small dataset of proofs of purchase makes available all strings and their corresponding bounding boxes. Since labels and bounding boxes for each document's main sections are also available, it is of interest to run a text classification experiment to determine the impact of adding spatial information. For this purpose, the data are further prepared: For each document, each string is associated with one of the main section labels, this is done by determining which main section bounding box contains the string's bounding box. If the string is contained by more than one main section, then the section with the smallest bounding box size is chosen as the string's new label. For example,  is contained by sections \"Contact\" and \"Header\", \"Contact\"'s area is 100 and \"Header\"'s area is 200, in this case  is classified as \"Contact\". If a string does not fully fall within a main section bounding box, the string is classified as \"Undefined\" and removed. In the end, there are 5731 classified strings as ground truth, 85% is used for training and validation, and 15% for testing.", "sections": [{"title": "BERT for text classification.", "content": "BERT model is fine-tuned to perform text classification. Two approaches are followed. In the first approach, the model is fine-tuned without adding any type of spatial information, in the second approach, spatial information is explicitly added to the string in the form of coordinates only separated by a single space. In both training sessions, the model is trained for 3 epochs, with a learning rate of 5e-5, using cross-entropy loss function and AdamW optimizer."}, {"title": "Large language models for text classification.", "content": "For text classification, Llama3-8B, Llama3.1-8B and Gemma2-9B are also prompted. In this case, the SLMs are not fine-tuned but a few-shot approach is used when prompting them. This is done to test the generalization capabilities of larger, more capable models. Same as with BERT, in the first run no spatial information is provided, and in the second run bounding box coordinates are added, both for the examples and for the non-classified strings. To avoid any issues with the context window of the models, the testing set is divided into batches of 60 strings with a final batch of 51 strings.\nFor all models, classification accuracy and the weighted F1, precision and recall scores are provided. We report the weighted scores for F1, precision, and recall due to our dataset not having the same amount of samples for each class."}]}]}, {"title": "Experimental results", "content": "As seen in Table 2 LayoutTransformer and the proposed method with Llama3-8B got pretty similar results with respect to the Mahalanobis distance to origin and closing points. The distance to the", "sections": [{"title": "Layout generation", "content": null}, {"title": "Text classification", "content": "For each SLM, the classification task was performed three different times for each scenario (without bounding boxes vs. with bounding boxes). With respect to text classification (see Table 3) with non-finetuned SLMs without bounding boxes in the prompt, it was observed that, as expected, models with more parameters performed better, as seen by Gemma2-9B achieving 47% accuracy with a standard deviation of 0.58%, then followed by the most recent version of Meta's Llama at the moment of writing (Llama3.1-8B) with 44% accuracy with a standard deviation of 0.58%. The original Llama3-8B achieved the lowest accuracy with 42% with a standard deviation of 3.21%. Nonetheless, when doing text classification with non-finetuned SLMs using bounding boxes within the prompts, no significant difference in accuracy was observed for Gemma2-9B, which remained at 47% with a standard deviation of 1.15%. Llama3.1-8B worsened with an accuracy of 44% with a standard deviation of 2.08% and Llama3-8B also went down by 1% with a standard deviation of 2.89%.\nDespite the massive parameter difference between BERT and the other SLMs, when using finetuned BERT without bounding boxes the text classification accuracy went up to 62% with a standard deviation of 4.51%, outperforming all other SLMs by at least 15%, and unlike the SLMs, finetuning BERT concatenating the bounding box information increased the text classification accuracy by 12% with a standard deviation of 4.13% (thus outperforming the other SLMs by at least 27%)."}]}, {"title": "Discussion", "content": "Results obtained for layout generation using the proposed method with Llama3-8B showed that this approach is at least as good as LayoutTransformer when it comes to positioning of the origin (x1, y1) and closing (x2, y2) points of the bounding boxes. Nonetheless, the similarity between both methods in this regard might be explained by the multiple intralabel overlapping (see Table 2) when using LayoutTransformer, which might give this method an unfair advantage since complete overlapping (e.g., two or more bounding boxes having the exact same coordinates) was noticed in some of the instances, thus reducing the Mahalanobis distance.\nRegarding the area size of the bounding boxes, the proposed method performed better, not only by better approaching the ground truth area sizes of most labels, but also by respecting interlabel dimensions, as shown in Figure 3. As an example, within the ground truth samples, the average area size of label I exceeded that of label H, which is also the case for the new generations using Llama3-8B, in the case of LayoutGeneration, the average area size of label H exceeded that of label I. The same behavior could be observed between labels LIT and F.\nFor text classification, it was expected to see BERT perform better by adding spatial information at training time; however, an improvement of 12% in accuracy came as a surprise given the simplicity of the approach (i.e., simply concatenating spatial information to the strings). On the other hand, the task was more challenging for the SLMs given the simpler approach (few-shot prompting); however, spatial information having no impact in accuracy also came unexpectedly. Though adding bounding boxes did not have"}, {"title": "Conclusion", "content": "In this investigation, an LLM-powered, layout generation technique has been provided by finetuning a Llama3-8B model using only a limited amount of labeled bounding boxes. The results have shown that the proposed method outperforms the LayoutTransformer approach (which additionally had to be pretrained on a much bigger dataset), especially when multiple instances of the same label exist and intra-label overlapping is undesired. By taking advantage of LLM prompting, the proposed method is also fully conditional, meaning that the user is in charge of specifying which and how many labels are wanted in the page. Although the initial results for all labels are largely positive, generalizing these findings across all labels may be premature, given the varying number of instances between them, which range into the hundreds. Underrepresented labels, such as L and H, would benefit from more instances to further influence the weights of the network.\nIn the future, it would be valuable to compare this feature more extensively with other conditional methods. New, synthetic, layout samples could be later used for other downstream tasks, such as continuing to create fully synthetic proof of purchase documents or even feeding other models for further training. The later must be done carefully, since it has been demonstrated that models decrease quality when trained on recursively generated data [4] [37]; however, Meta and Google have shown that there is a right way to do knowledge transfer via knowledge distillation [40]. This work also showed that accuracy improves by simply concatenating spatial information to the target strings when fine-tuning BERT for text classification; however, this cannot be said of a simpler method like few-shot learning even if using bigger, more capable, models such as Llama3.1-8B and Gemma2-9B. In the future, and since context window is no longer a limitation for recent models, a many-shot approach [3] [20] where many examples are provided at prompting time and that better represents each label's ratio might also yield better results. Lastly, improving the quality of the samples and limiting the text classification task to just include the most problematic labels (those with lower precision) are also worth testing."}]}