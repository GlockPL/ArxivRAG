{"title": "TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding", "authors": ["Zhaoxuan Wu", "Zijian Zhou", "Arun Verma", "Alok Prakash", "Daniela Rus", "Bryan Kian Hsiang Low"], "abstract": "We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.", "sections": [{"title": "1 Introduction", "content": "Transformer-based large language models (LLMs) have shown remarkable abilities to solve different tasks across various domains, such as natural language (Zhao et al., 2023), computer vision (Yin et al., 2024), robotics (Zeng et al., 2023), code generation (?), among others (Maslej et al., 2024). However, the autoregressive nature of LLMs (i.e., generating one token at a time) leads to an increasingly sluggish inference speed as the model size increases.\nTo address this problem, a recent widely-used approach is speculative decoding (SD) (Cai et al., 2024; Cheng et al., 2024; Leviathan et al., 2023; Li et al., 2024a,b): It achieves faster inference by using a small draft model to rapidly generate a sequence of (draft) tokens and then a large target model to verify whether to accept or reject them in parallel. When a token is rejected, the draft model generates a new sequence of tokens in the next step, starting from the most recently accepted token. A key aspect of SD is to determine the optimal number of draft tokens (i.e., draft window size) to generate and verify in each step. Generating more draft tokens allows the target model to verify a longer sequence at once (given sufficient computing resources/capacity for parallel inferences), which can potentially boost inference speed. However, doing so increases the risk of wasting computing resources since all tokens following the first rejected token must be discarded. In contrast, generating fewer draft tokens reduces this risk but limits the potential benefit of SD since the computing resources are not effectively utilized. Therefore, the optimal selection of draft tokens that would be accepted when verified by the target model in parallel is critical to improving both inference speed and resource utilization (Liu et al., 2024d).\nMost existing works have focused on optimizing draft token selection for individual user requests (Agrawal et al., 2024; Huang et al., 2024; Liu et al., 2024c; Mamou et al., 2024), but may not work well for profit-driven LLM inference service providers who must manage multiple user requests under a limited inference capacity. Moreover, LLM inference service providers typically charge users based on the number of tokens served (Fireworks AI, 2025; Replicate, 2025). Hence, they are incentivized to maximize the total number of tokens served (i.e., throughput) across all user requests while ensuring fast response time to meet service level agreement (Wieder et al., 2011). So, they would employ computing clusters to process large batches of user requests simultaneously and use SD to further improve the inference speed.\nSuch batch processing of user requests entails a fundamentally different optimization objective for SD compared to handling individual requests. For SD of a single request, supposing a fast draft model with negligible runtime, the objective is to maximize the draft window size as long as the target model can verify all draft tokens in parallel by fully utilizing the inference capacity. It can be naively extended to batch processing by widening the draft window for all requests until the inference capacity is reached. This is inefficient as each request may require a different optimal draft token selection due to varying difficulty in speculation (i.e., generating tokens to match the target model's output).\nThis paper presents a theoretical framework that dynamically optimizes the draft token selection for every user request from the perspective of a capacity-limited LLM inference service provider who aims to maximize resource utilization. Since draft token verification is the most time-consuming component of SD, we propose TETRIS, a method that greedily selects draft tokens with a high likelihood of acceptance by the target model. The name of our method is derived from the shape of its selected tokens, as shown in Fig. 1. We demonstrate that TETRIS strictly outperforms standard SD by achieving higher total throughput. Our work bridges a critical yet overlooked gap in current research, allowing service providers to improve total throughput with batch SD. The specific contributions of our work here are summarized below:\n\u2022 In Sec. 3, we introduce the problem of optimal draft token selection in multi-request settings, and in Sec. 4.1, we propose TETRIS, a novel method that selects optimal draft tokens in log-linear time for the target model's verification.\n\u2022 In Sec. 4.2, we theoretically show that TETRIS achieves optimal throughput at each decoding step and globally in the absence of drafting time (i.e., time to generate draft tokens) under reasonable token acceptance assumptions.\n\u2022 In Sec. 5, our empirical results show that TETRIS consistently outperforms standard SD and existing methods that use dynamic draft windows for a batch in terms of total throughput and end-to-end latency (including drafting time), highlighting the potential of TETRIS to improve inference speed in real-world model service deployments."}, {"title": "2 Related Work", "content": "Speculative Decoding (SD). By employing a draft-then-verify strategy for lossless accelerations of LLM inference, SD has attracted significant attention recently (Ryu and Kim, 2024; Xia et al., 2024). Recent advancements based on SD have focused on developing more efficient draft models by producing multiple drafts for the next few tokens (Cai et al., 2024; Cheng et al., 2024; Li et al., 2024b). Additionally, some methods have optimized the speculation accuracy by aligning the draft model with the target model (Liu et al., 2024e; Zhou et al., 2024) or leveraging the target model itself to draft via techniques like layer skipping (Zhang et al., 2024). To facilitate more efficient verification, tree attention has been proposed for speedy tree-structured candidate verification (Miao et al., 2024; Spector and Re, 2023). In contrast, our work explores a complementary approach that intervenes between the draft and target models, performing strategic draft token selection to improve throughput over batched requests. Our method can be seamlessly integrated with the above techniques for a more efficient SD system.\nLLM Scheduling. With the growing popularity of LLM as a service, several works have considered improvements to the scheduling of LLM services. These works can be broadly categorized into client-side and server-side approaches. Server-side approaches (Fu et al., 2024; Kim et al., 2024; Liu et al., 2024d; Wang et al., 2024a) have focused on increasing the throughput of LLM services, which may lead to an unfair allocation of inference resources to users, hence causing starvation. On the other hand, client-side approaches (Liu et al., 2024b; Sheng et al., 2024) have focused on improving user satisfaction by improving client-side metrics (e.g., decreasing maximal waiting time or end-to-end latency). Our work considers the scenario where the LLM inference service provider employs SD to ensure user satisfaction with inference speed while simultaneously aiming to maximize service throughput to optimize profitability.\nDraft Window Optimization. In the foundational paper on SD, the authors have proposed to generate a window of draft tokens (Leviathan et al., 2023). The optimal draft window is theoretically determined under an impractical assumption of identical conditional acceptance rates for all draft tokens (Leviathan et al., 2023). Empirically, such an acceptance rate can be estimated by a moving average of past requests (Liu et al., 2024d). Other heuristics for finding the optimal draft window include stopping the draft generation when the draft model's confidence score falls below a predetermined threshold (Kim et al., 2023; Liu et al., 2024a) or when an entropy-controlled criterion is met (Agrawal et al., 2024). Cai et al. (2024) have proposed taking the union of these two heuristics. These existing works have operated at a single-request level, except that of Liu et al. (2024d) which adaptively determines a single draft window for all requests in a batch. Note that considering each request independently or using a common draft window for a batch can lead to inefficiencies in allocating verification budgets (i.e., inference capacity) across multiple requests, especially when operating under the limited computing resources of an LLM inference service provider."}, {"title": "3 Problem Setup", "content": "This section first introduces speculative decoding and then describes the optimal draft token selection problem and the performance metrics used."}, {"title": "3.1 Speculative Decoding (SD)", "content": "SD is an efficient inference method designed to accelerate the decoding process in LLMs and involves two phases: drafting followed by verification. Initially, a lightweight draft model, denoted as S, quickly generates candidate draft tokens. Subsequently, these tokens are verified against the generations from the target model, denoted as M, which is also often referred to as the verification model. SD allows parallelized verifications of tokens by M, as opposed to the conventional autoregressive decoding used in language models. Hence, SD yields significant improvement in decoding speed.\nSpecifically, the draft model generates k draft tokens $d_1,..., d_k$ in an autoregressive manner where k is the draft window size. Given a prompt or prefix x, the generation process follows $d_i \\sim p_S(\\cdot|x, d_1,..., d_{i-1})$. For notational simplicity, we denote $p_S(d_i) = p_S(d_i|x, d_1,..., d_{i-1})$. The verification follows a rejection sampling procedure. If $p_S(d_i) \\leq p_M(d_i)$, the draft token $d_i$ is accepted. Otherwise, we reject the draft token with a probability of $1 - p_M(d_i)/p_S(d_i)$ and then output a new token sampled from an adjusted distribution $p_M(d') = norm(max(0, p_M(d') - p_S(d')))$, where $norm(\\cdot)$ normalizes the probability distribution. Hence, the acceptance of draft tokens depends on both $p_S(\\cdot)$ and $p_M(\\cdot)$ and plays a vital role in the effectiveness of SD. A higher acceptance suggests the possibility of greater speedup gain with a larger k. We defer a more detailed discussion of the acceptance rate estimation in App. B.1. However, we highlight that the effectiveness of SD is limited by the computing resources available. Using a draft window exceeding the capacity for parallel inferences that the server can manage degrades the performance, which we show empirically later in Sec. 5. Consequently, it is essential to carefully select the draft window size for each request, leading to our proposed method outlined next."}, {"title": "3.2 Optimal Draft Token Selection", "content": "We first define a set of other notations used throughout our paper. We consider a specific LLM inference service provider with a limited capacity C, which represents the maximum number of parallel inferences its computing resources can perform. The capacity depends on the server configurations of the service provider in practice. At each time step, the server processes a batch of N requests $r_1,r_2,\u2026\u2026,r_N$, each with a partially complete sequence $S_{i,t_i} = (d_{i,1},..., d_{i,t_i})$ where $t_i$ represents the number of tokens verified/served so far for request $r_i$. We allow a variable draft window size $k_i$ for each request $r_i$. The draft model S drafts a set $D := \\{(i,t)|i \\in [N], t \\in [t_i + k_i]\\}$ such that $|D| = \\sum_{i=1}^N k_i = C$. For each $(i, t) \\in D$, we send $S_{i,t}$ to have its last token verified by M. We aim to optimally choose the set D at each time step to maximize the performance of the server in terms of generation throughput, which we define below.\nPer-step Throughput. For each step of SD, we are mainly concerned with maximizing the per-step throughput, i.e., the number of tokens served at each time step. Mathematically, let $1_{i,t}$ be an indicator variable representing whether the last token of $S_{i,t}$ is accepted, let $T_{step}$ be the time per step. The per-step throughput is then defined as\n$G_{step} := (E[\\sum_{(i,t)\\in D} 1_{i,t}] + N)/T_{step}$.\nNote that at least one token is always generated by SD via the bonus token mechanism (Leviathan et al., 2023). Thus, without considering drafting time, the throughput of SD is theoretically at least as good as that of autoregressive decoding.\nTotal Throughput. The total throughput is calculated as the average per-step throughput over a total of T steps with a fixed $T_{step}$ for each step:\n$G:= T^{-1} \\sum_{i=1}^T G_{step}$.\nNote that it is theoretically difficult to find an optimal draft token selection strategy that maximizes G as the relationship between previously verified tokens and the distribution of acceptance rate for the remaining tokens is extremely complex. However, under a mild assumption on token acceptance rate, the optimality of G is equivalent to the optimality of $G_{step}$, as explained formally in Sec. 4.2 later."}, {"title": "4 TETRIS: Optimal Draft Token Selection", "content": "In this section, we introduce the details of the TETRIS for batch SD and provide an analysis of its time complexity and optimality. Overall, we leverage the insight that SD suffers from a cascading failure rate in a single sequence but not across different sequences. More specifically, we distinguish between two types of tokens involved in drafting: sequential and parallel. For each request $r_i$, all pairs $(i, \\cdot) \\in D$ are sequential, i.e., for all $j < k$, $(i, j)$ must be accepted for $(i, k)$ to be accepted as well, implying a cascade of the failure rate. On the other hand, for $i \\neq j$, $(i, \\cdot)$ and $(j, \\cdot)$ are parallel, as the failure rate of $(i, \\cdot)$ does not influence that of $(j, \\cdot)$. We highlight that the distinct nature of the two modes serves as the fundamental motivation of our proposed approach for an improved $G_{step}$, and consequently the total throughput G."}, {"title": "4.1 Our Approach and Design", "content": "We introduce inter-dependencies among requests within a batch. We favor parallel tokens when selecting sequential tokens leads to an excessive cascading of failure rates, and vice versa. To achieve this, we propose to introduce a manager to actively select the best draft tokens that are most likely to be successfully verified by the target model, thus maximizing the expected number of output tokens. The manager is integrated into the speculative decoding framework and functions as an intermediary between the draft model and the target model. It operates on the draft tokens and auxiliary outputs (e.g., token distributions, hidden states) from the draft model and strategically selects those that will be sent for verification by the target model.\nAt each step, define $p_{i,j}$ the conditional acceptance rate of the token at index (i, j) given its corresponding prefix. Let $B_{i,j} := (i, j, \\prod_{t=1}^j p_{i,t})$ be the tuple containing token indices and the probability of all selected tokens in row i up to j being accepted. Instead of simply selecting a fixed window of draft tokens for verification, we greedily look for tokens with the highest cumulative acceptance rate $\\prod_{t=1}^j p_{i,t}$ (and not the standalone acceptance rate $p_{i,j}$). We let the draft model propose the extra draft tokens beyond the server capacity and then select a set $D^*$ of tokens such that it maximally utilizes the compute resource by ensuring $|D^*| = C$. This process dynamically allocates longer draft windows for requests with \"easy\u201d tokens and shorter windows for \"hard\" ones, reducing resource wastage while sufficiently leveraging speculation, as illustrated in Fig. 1. TETRIS is outlined in Alg. 1."}, {"title": "4.2 Analysis", "content": "We now present our theoretical results, which show the per-step and global optimality of TETRIS.\nTheorem 1 (Per-step Optimality of TETRIS). In the absence of drafting time, given the true acceptance rate $a_{i,j}$ of each draft token (i, j), Alg. 1 produces the optimal per-step throughput defined in Sec. 3.\nThe proof is delayed to App. A.1. While we have established the local optimality of TETRIS in Theorem 1, such local optimality does not trivially generalize to maximizing total throughput. Nevertheless, we show, in Theorem 2, that TETRIS is optimal in a slightly simpler scenario that retains sufficient complexity of interest.\nAssumption 1. $\u2200j$, all tokens in the j-th sequence have an identical acceptance rate denoted as $a_j$.\nTheorem 2 (Global Optimality of TETRIS under Assumption). Under Assumption 1, in the absence of drafting time, TETRIS searches for the optimal G under the same capacity. Morever, if $a_1 = a_2 = ... = a_N$, TETRIS has the same G as standard batched speculative decoding.\nThe proof is delayed to App. A.3. Overall, we established both per-step and global optimality of TETRIS under theoretical assumptions. In practice, the drafting time can be hidden with appropriately designed pipeline (Liu et al., 2024c; Wang et al., 2024b) which parallelizes the execution of the draft model and the target model. The true acceptance rates are inaccessible in practice, we thus rely on surrogate measures and show their empirical effectiveness, which we will discuss next."}, {"title": "4.3 Practical Implementations", "content": "The acceptance rate of a draft token depends on $max(p_M(d_i)/p_S(d_i),1)$. However, the TETRIS manager does not have access to $p_M(\\cdot)$ before verification. In practice, we use the draft model's output probability as a surrogate of the token acceptance rate (Kim et al., 2023; Zhang et al., 2024). We show in Sec. 5 that this surrogate empirically results in strong performance. Additionally, while we theoretically show that Alg. 1 achieves a time complexity of $O(C log N)$ (see App. A.2), we can additionally leverage the parallelism of GPU to achieve empirical negligible overhead of using TETRIS (< 0.3ms compared to the average draft time per token of > 2.5ms) via the scatter_max operation directly implemented on GPU. Lastly, the autoregressive token drafting can also be parallelized across requests. Hence, drafting a batch of requests with a common window size of k tokens takes the same time as a single request in practice."}, {"title": "5 Experiments", "content": "We evaluate the effectiveness and efficiency of TETRIS against baseline methods. We first validate the necessity of dynamic draft token selection and improvement of token acceptance with TETRIS in Sections 5.1 and 5.2. Then, we show the empirical end-to-end speedup in Sec. 5.3. We also discuss the potential further improvement in empirical results with the future implementation of speculative decoding pipelines in Sec. 5.4.\nSettings. We perform experiments on target models of various parameter sizes, including Vicuna-33B-v1.3, Llama-3.1-70B-Instruct, and Llama-3.1-405B-Instruct. We use Vicuna-68M and Llama-3.2-1B-Instruct as their respective draft models. Depending on the size of the models, different server configurations and tensor parallel sizes are adopted, detailed in Tab. 1. TETRIS is evaluated for generation of answer completion for questions extracted from ShareGPT (Anon, 2023), Chatbot Arena (Zheng et al., 2023), Domain Tough Questions (YAV-AI, 2024), and synthetic tasks generated from Shakespeare's The Sonnet. The standard speculative decoding (SD) (Leviathan et al., 2023) and dynamic speculative decoding (DSD) (Liu et al., 2024d) are baseline methods that we compare to. We vary the drafting window sizes, allowing up to 3 extra draft tokens for TETRIS while keeping the same number of tokens sent for verification by the target model for fair comparison. TETRIS is implemented in vLLM (Kwon et al., 2023)."}, {"title": "5.1 Variations in Draft Quality", "content": "We begin by emphasizing the importance of setting an appropriate draft window size. Using Setting 2, we collect the oracle optimal draft window size to adopt for each SD step. Notably, the results in Fig. 2 show flat curves with long-tail distributions for various datasets, revealing significant variations in optimal window size per step. This diversity highlights the potential suboptimality of a fixed draft window, as it fails to adapt to the inherent characteristics of the draft-target model combination or a batch of sequences. By tailoring the draft token selection in a batch, TETRIS is expected to achieve higher efficiency and better alignment with the model's token acceptance patterns, hence improving overall performance."}, {"title": "5.2 Effect of Extra Draft Tokens", "content": "Having extra draft tokens provides TETRIS with greater flexibility in selecting which draft tokens to send for verification. To empirically show this effect, we define the verification success rate (VSR),\n$VSR = \\frac{Accepted tokens}{Tokens sent for verification}$,\nwhich measures the quality of the draft tokens selected by TETRIS. We show in Fig. 3 that increasing the number of extra draft tokens consistently increases the VSR metric across all settings. This finding confirms the effectiveness of TETRIS's strategy for draft token selection utilizing extra draft tokens. It also validates the empirical usefulness of the draft model's output probabilities as a surrogate of the selection criteria, as stated in Sec. 4.3."}, {"title": "5.3 Evaluation of TETRIS", "content": "To evaluate the effectiveness of TETRIS, we perform comprehensive experiments on various datasets and report metrics, including the total throughput and end-to-end latency. We compare to standard SD and DSD. Throughout the experiments, we maintain a consistent system load of 64 batched requests to ensure consistency, reproducibility, and fairness in comparisons. Note that all experiments include drafting time.\nTotal Throughput. We measure the performance of a speculative decoding method using the total throughput, which includes both accepted draft tokens by the target model and the bonus tokens, which make up the final completion. As shown in Fig. 4, TETRIS achieves up to approximately 5.25% improvement in terms of total throughput compared to the best baseline, depending on the draft-target setting and the nature of the task performed. The maximum gap between TETRIS and standard SD is up to 9.27%. Importantly, TETRIS consistently outperforms the standard SD and DSD across all settings of the draft window sizes. This shows the robustness of TETRIS to different hyperparameter choices. Additionally, it is evident that having more speculative tokens (i.e., a larger draft window size) does not always improve the performance, as having too many parallel executions of the target model exceeding the servers' parallel inference capacity degrades performance.\nEmpirically, we observe that TETRIS achieves optimal performance when the number of extra draft tokens is set to 1 or 2. These results are partly attributed to the current sequential draft-target implementation for the speculative decoding pipeline, as more extra draft tokens take time to generate autoregressively. Remarkably, this pipeline can be better designed to amplify the benefit of TETRIS, which we defer the discussion to Sec. 5.4. Moreover, while DSD is expected to outperform standard SD, we note that it is not always the case in empirical experiments. This behavior may result from the difficulty of accurately estimating the conditional token acceptance rate in practice and the quality of the fitted latency prediction model.\nEnd-to-end Latency. We also measure the end-to-end latency of each request. This metric measures the average latency of the speculative decoding system in finishing completions, which can affect user satisfaction. We summarize the results in Tab. 2 and defer the figures to App. C.2. Overall, TETRIS achieves up to 6.13% improvement in latency as compared to the best baseline and up to 9.32% improvement against standard SD."}, {"title": "5.5 Ablation Study", "content": "Robustness to Variations in Draft Quality. We artificially introduce additional variations in draft quality by mixing datasets of different difficulty levels. We create synthetic prompts designed for models to repeat lines from a poem named Sonnet. Since Sonnet is relatively easy for the small draft model, it achieves a high rate of successful verification by the target model. We then construct a new dataset, Mix, by randomly mixing Sonnet and a more challenging dataset, Tough, in equal proportions. As shown Tab. 4, the performance improvement of TETRIS over the best baseline suffers only a marginal or no decline, indicating its robustness to substantial variations in draft quality.\nExtension to Medusa. The Medusa model generates multiple subsequent draft tokens using a single forward pass (as opposed to autoregressive generation) through multiple decoding heads (Cai et al., 2024). Leveraging Medusa, it is possible to generate extra draft tokens for TETRIS at minimal marginal computational cost. We show in App. C.4 that integrating TETRIS to Medusa achieves a 3.19% improvement in total throughput.\nOther Ablations. We also include ablations on TETRIS's improvement in verification success rate (VFS) in App. C.5, and the effect of batch size on the performance in App. C.6."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we study the problem of optimizing batch speculative decoding to maximize throughput in multi-request settings, such as those faced by model service providers. To this end, we propose TETRIS, a novel method that efficiently selects optimal draft tokens for the LLM verification in log-linear time. We have theoretically shown that, in the absence of drafting time, TETRIS achieves optimal throughput both at each decoding step and globally under reasonable assumptions about token acceptance rates. Our empirical results further validate that TETRIS consistently outperforms standard speculative decoding and existing dynamic draft window selection methods, even when accounting for the extra time required for drafting extra tokens. These results highlight the potential of TETRIS to improve inference efficiency in real-world model service deployments. A key future direction is adapting TETRIS to tree decoding, a notable feature in recent advancements in speculative decoding (Cai et al., 2024; Li et al., 2024a,b)."}, {"title": "7 Limitations", "content": "In this paper, our empirical experiments only demonstrate results using the current sequential speculative decoding pipeline implemented on VLLM. That is, the target model stays idle while waiting for draft tokens from the draft model. Consequently, the performance improvement of TETRIS is heavily dependent on the trade-off between the additional runtime required to generate extra draft tokens and the gain in token acceptance achieved through TETRIS. Such trade-off limits the practical effectiveness of TETRIS, especially when a slow draft model is required. We anticipate that future implementations of a parallelized pipeline could potentially reveal greater speedups with TETRIS. However, we have not yet integrated such features into VLLM for testing in empirical experiments."}, {"title": "A Leftover Proofs", "content": "A.1 Proof of Theorem 1.\nTheorem 1 (Per-step Optimality of TETRIS). In the absence of drafting time, given the true acceptance rate $a_{i,j}$ of each draft token (i, j), Alg. 1 produces the optimal per-step throughput defined in Sec. 3.\nProof. We prove it by contradiction. Let the selection of Alg. 1 be $D^*$. Suppose the actual optimal solution is $D' \\neq D^*$. Let $D = D' \\cap D^*$ be the overlapping tokens selected by both Alg. 1 and the actual optimal solution. Note that the tokens in each row are selected sequentially (i.e., tokens cannot be skipped in a row).\nCase 1: TETRIS selects some token $d \\in D^* \\backslash D$ before selecting $D$. In this case, the E[1] of the token d is higher than the token last selected in D. This suggests that the optimal selection should include d. However, it can be observed that $d \\notin D'$ since otherwise $d \\in D$. This contradicts the fact that $D'$ is optimal.\nCase 2: TETRIS selects $D$ first before selecting other tokens. Since Alg. 1 always selects the token with the highest E[1], every element in $D^* \\backslash D$ is larger than or equal to that in $D' \\backslash D$. As such, we have $E[\\sum_{p \\in D'} 1_p] < E[\\sum_{p \\in D^*} 1_p]$. However, this contradicts the fact that $D'$ is optimal as Alg. 1 has a higher number of accepted tokens. Therefore, Alg. 1 must be optimal.\nCombining the two cases finishes the proof.\nA.2 Running Time of TETRIS\nLemma 1. Alg. 1 achieves a time complexity of $O(C log N)$.\nProof. Note that Alg. 1 maintains a heap. The heap is initialized with N items. Since only C pairs are selected, there are 2C operations of enqueue and dequeue. Following classic results of heap operation, each enqueue of dequeue operation requires $O(log C)$ time. As such, the overall time complexity of TETRIS is $O(C log N)$.\nA.3 Proof of Theorem 2.\nTheorem 2 (Global Optimality of TETRIS under Assumption). Under Assumption 1, in the absence of drafting time, TETRIS searches for the optimal G under the same capacity. Morever, if $a_1 = a_2 = ... = a_N$, TETRIS has the same G as standard batched speculative decoding.\nProof. The proof of global optimality is established on Theorem 1. Since all tokens in each row have the same acceptance rate. After each step, we have the same distribution of 1 no matter what tokens are accepted, where 1 is the indicator variable of whether the token is accepted. As such, at each step, performing TETRIS is per-step optimal by Theorem 1. Moreover, since the state at each step is identical, a per-step optimal strategy is also globally optimal."}, {"title": "B Additional Related Work", "content": "B.1 Acceptance Rate\nThe acceptance rate plays a vital role in the effectiveness of speculative decoding. A higher acceptance rate should be paired with a larger draft window size k to achieve optimal speedup. In the typical rejection sampling setting of speculative decoding, the acceptance of draft tokens depends on the probability distributions of both the draft and target models. When the probability distribution of the draft model, $p_S(\\cdot)$, closely approximates that of the target model, $p_M(\\cdot)$, a higher number of tokens are accepted on average. Since the value of k is chosen in the drafting process, we do not have access to $p_M(\\cdot)$ and have to rely on $p_S(\\cdot)$ to estimate the acceptance rate.\nLeviathan et al. (2023) derive that the expected acceptance rate is 1 minus the KL divergence between the token distributions of the draft and the target model. Hence, the acceptance rates for all draft tokens are considered constant. Liu et al. (2024d) assume uniform token acceptance behavior across diverse requests. It proposes SmartSpec, which calculates the average acceptance rate from past generation steps. Li et al. (2024a) and Wang et al. (2024a) utilize the draft model's confidence score (i.e., the output probability of each token) to estimate the acceptance rate. Chen et al. (2024) make the positional acceptance assumption so that the acceptance rate of tokens is determined solely by their position (i.e., number of tokens away) relative to the already accepted tokens. Agrawal et al. (2024) instead consider an approximate lower bound on the expected acceptance rate of a token that depends on the entropy of prediction probabilities of the draft model $p_S(\\cdot)$. Noting the acceptability of diverse tokens, especially in the real world with a high value of temperature hyperparameter, Medusa proposes to use both a hard threshold and an entropy-dependent threshold as a criterion to accept draft tokens (Cai et al., 2024). In Medusa, the first token is always accepted using greedy decoding to ensure at least one token is generated in each step."}, {"title": "C Additional Results", "content": "C.1 Dataset License\nShareGPT (Anon, 2023): Apache license 2.0; Arena (Zheng et al., 2023): CC; Domain-specific Tough Questions (YAV-AI, 2024): MIT.\nC.2 Plots for End-to-end Latency\nWe provide an extended discussion on the improvement of end-of-end latency from Sec. 5.3. In Fig. 7, we show the plots for the end-to-end latency over all speculative decoding configurations and settings used in the paper. TETRIS consistently outperforms the existing baselines and achieves up to 6.13% improvement over the best baseline and up to 9.32% maximum gap over standard SD. Therefore, TETRIS has demonstrated to effectively reduce end-to-end request latency, which is also essential for enhancing the user experience with LLM inference service providers.\nC.3 Plots for Projected Improvement based on TER\nComplementary to Tab. 3, which contains the numerical results for the projected improvement of TETRIS in terms of the projected throughput $\\hat{G}^{(TER)}$, we also show the plots in Fig. 8 to visually illustrate the effectiveness of our method. The dotted lines for TETRIS (drawn in blue, orange, and green) represent the projected throughput calculated based on the throughput of the standard SD and also the TETRIS\u2019s improvement in terms of target efficiency rate (TER, as defined in Eq. (2)). We note that these improvement numbers are theoretically computed and are not yet realizable in empirical settings due to the lack of parallelized pipeline implementations of speculative decoding in vLLM.\nC.4 Extension to Medusa\nWe evaluate the top-1 proposal version (i.e., only draft the most likely token for each position) of Medusa and its integration with TETRIS. As the Medusa model outputs multiple subsequent tokens in a single forward pass, we leverage this feature to produce extra draft tokens for TETRIS. We show the results in Tab. 5. We achieved a throughput improvement of 3.19% as compared to the baseline Medusa. The development of such multi-token prediction models, including models like EAGLE (Li et al., 2024b) and DeepSeek-V3 (DeepSeek-AI et al., 2024) presents further potential for TETRIS to achieve greater speedups. Other improvements in engineering, including using tree-decoding and using a larger target model also potentially further boost the speedup."}]}