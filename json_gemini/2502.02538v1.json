{"title": "Flow Q-Learning", "authors": ["Seohong Park", "Qiyang Li", "Sergey Levine"], "abstract": "We present flow Q-learning (FQL), a simple\nand performant offline reinforcement learning\n(RL) method that leverages an expressive flow-\nmatching policy to model arbitrarily complex ac-\ntion distributions in data. Training a flow policy\nwith RL is a tricky problem, due to the iterative\nnature of the action generation process. We ad-\ndress this challenge by training an expressive one-\nstep policy with RL, rather than directly guiding\nan iterative flow policy to maximize values. This\nway, we can completely avoid unstable recursive\nbackpropagation, eliminate costly iterative action\ngeneration at test time, yet still mostly maintain\nexpressivity. We experimentally show that FQL\nleads to strong performance across 73 challeng-\ning state- and pixel-based OGBench and D4RL\ntasks in offline RL and offline-to-online RL.", "sections": [{"title": "1. Introduction", "content": "Offline reinforcement learning (RL) enables training an ef-\nfective decision-making policy from a previously collected\ndataset without costly environment interactions (Lange et al.,\n2012; Levine et al., 2020). The essence of offline RL\nis constrained optimization: the agent must maximize re-\nturns while staying within the dataset's state-action distribu-\ntion (Levine et al., 2020). As datasets have grown larger and\nmore diverse (Collaboration et al., 2024), their behavioral\ndistributions have become more complex and multimodal,\nand this often necessitates an expressive policy class (Man-\ndlekar et al., 2021) capable of capturing these complex dis-\ntributions and implementing a more precise behavioral con-\nstraint. In this work, we aim to develop a scalable offline RL\nmethod by leveraging flow matching (Lipman et al., 2023;\nLiu et al., 2023; Albergo & Vanden-Eijnden, 2023), a sim-\nple yet powerful generative modeling technique alternative\nto denoising diffusion (Sohl-Dickstein et al., 2015; Ho et al.,\n2020). By employing an expressive flow policy, we can ef-\nfectively model the arbitrarily complex action distribution"}, {"title": "2. Preliminaries", "content": "Offline RL. In this work, we assume a Markov deci-\nsion process M (Sutton & Barto, 2005) defined by a tu-\nple (S, A, r, p, p), where S is the state space, A = Rd\nis the d-dimensional action space, r(s, a) : S \u00d7 A \u2192 R\nis the reward function, p(s) \u2208 \u0394(S) is the initial state\ndistribution, and p(s' | s,a) : S \u00d7 A \u2192 \u2206(S) is the\ntransition dynamics distribution, where we denote the set\nof probability distributions over a space X as (X) and\nuse gray to denote placeholder variables. The goal of of-\nline RL is to find the parameter \u03b8 of a policy \u03c0\u03c1(\u03b1\u03c2) :\nS \u2192 A(A) that maximizes the average discounted return\nR(\u03c0\u03bf) = \u0395\u03c4~pe (1) [\u2211h=0Yhr(sh, ah)] from a dataset\nD\n=\n{(n)}n\u20ac{1,2,...,N} without environment interac-\ntions, where denotes a trajectory (so, ao,...,SH, \u0430\u043d),\n\u03b3 denotes a discount factor, and po(t) is defined as\n\u03c1(5\u03bf)\u03c0\u03bf(\u03b1\u03bf | so)p(S1 | So,ao)\u2026\u2026\u03c0\u03bf(\u03b1\u0397 | SH). In this\nwork, we also consider offline-to-online RL, whose goal is\nto further fine-tune the offline pre-trained policy with a mod-\nest amount of online environment interactions.\nBehavior-regularized actor-critic.\u00b9 Behavior-regularized\nactor-critic (Wu et al., 2019; Fujimoto & Gu, 2021; Tarasov\net al., 2023a) is one of the simplest (yet effective) offline\nRL frameworks. In its most basic form, it minimizes the"}, {"title": "3. Flow Q-Learning", "content": "We now introduce our method for effective data-driven\ndecision-making, flow Q-learning (FQL). Our desider-\nata are twofold: we want to leverage an expressive flow-\nmatching policy to deal with complex behavioral action dis-\ntributions; we also want to keep the method as simple as pos-\nsible so that practitioners can easily implement and use it.\nNa\u00efve approach. Perhaps the simplest way to train a flow\npolicy for offline RL is to replace the BC loss with a flow-\nmatching loss (Equation (5)) in the behavior-regularized\nactor-critic framework (Equation (2)). Formally, this na\u00efve\napproach minimizes the actor loss L\u201e(0) defined by\n\\begin{equation}\nL_{\\pi}(\\theta) = \\mathbb{E}_{s\\sim D, a^{\\pi}\\sim \\pi_{\\omega}}[-Q_{\\phi}(s, a^{\\pi})] + \\alpha \\mathcal{L}_{Flow}(\\theta).\\\n\\end{equation}\n\\begin{equation}\nL_{\\pi}(\\omega) = \\mathbb{E}_{s\\sim D, \\mathbf{z}\\sim \\mathcal{N}(0,I^{d})}[||\\mu_{\\omega}(s, \\mathbf{z}) - \\mu_{\\theta}(s, \\mathbf{z}) ||^{2}] .\\\n\\end{equation}\nIntuitively, the corresponding flow policy \u03c0\u03bf is \u201csteered\u201d\nto maximize the value function while minimizing the BC\nloss. This is analogous to Diffusion-QL (Wang et al., 2023)\nfor diffusion policies. However, unlike the Gaussian case,\nthe flow or diffusion objective requires backpropagation"}, {"title": "Remark: Connection to Wasserstein Regularization", "content": "Our distillation loss in Equation (7) has an intriguing con-\nnection to Wasserstein behavioral regularization. Let \u00a7\nbe a random variable following the d-dimensional stan-\ndard normal distribution, N(0,Ia). For s \u2208 S, let\n\u03bd\u03bf(\u03c2), \u03bd\u03c9(5) \u2208 \u0394(A) be the push-forward distributions\nof \u00a7 by \u03bce(s,) and \u03bc\u03c9(s,\u00b7), respectively. Then, the\ndistillation loss in Equation (7) is an upper bound on\nthe squared 2-Wasserstein distance between \u06f7\u06f5(s) and\n\u03bd\u03bf(\u03c2):\n\\begin{equation}\n\\begin{split}\n\\mathcal{L}_{Distill}(\\omega) &= \\mathbb{E}_{s \\sim D, \\mathbf{z} \\sim \\mathcal{N}(0, I^{d})}[||\\mu_{\\omega}(s, \\mathbf{z}) - \\mu_{\\theta}(s, \\mathbf{z}) ||^{2}] \\\\\n&>  \\mathbb{E}_{s\\sim D} \\inf_{\\lambda \\in \\Lambda(\\nu_{\\omega}, \\nu_{\\theta})} [\\mathbb{E}_{(\\mathbf{z}, \\mathbf{a}) \\sim \\lambda} || \\mathbf{a} - \\mu_{\\theta}(s, \\mathbf{z}) ||^{2}] \\\\\n&=  \\mathbb{E}_{s\\sim D} [W_{2}(\\nu_{\\omega}, \\nu_{\\theta})^{2}],\n\\end{split}\n\\end{equation}\nwhere \u039b(\u03bd\u03c9, \u03bde) denotes the set of coupling distributions\nof \u03bdw and ve, and W2 denotes the 2-Wasserstein distance\nwith the Euclidean metric in the action space."}, {"title": "4. Prior Work", "content": "Offline RL and offline-to-online RL. The goal of offline\nRL is to train a policy using only previously collected data.\nHundreds of offline RL methods and techniques have been\nproposed so far, and many of them are based on a single\ncentral idea: maximizing the return while minimizing a\ndiscrepancy measure between the state-action distribution of\nthe dataset and that of the learned policy (Levine et al., 2020;\nSikchi et al., 2024). Previous works have implemented\nthis high-level objective in diverse ways through behavioral\nregularization (Nair et al., 2020; Fujimoto & Gu, 2021;\nTarasov et al., 2023a), conservatism (Kumar et al., 2020), in-\nsample maximization (Kostrikov et al., 2022; Xu et al., 2023;\nGarg et al., 2023), out-of-distribution detection (Yu et al.,\n2020; Kidambi et al., 2020; An et al., 2021; Nikulin et al.,\n2023), dual RL (Lee et al., 2021a; Sikchi et al., 2024), and\ngenerative modeling (Chen et al., 2021; Janner et al., 2021;\n2022). After finishing offline RL training, we can further\nfine-tune the policy with additional online rollouts. This\nsetting is often referred to as offline-to-online RL, for which\nseveral techniques have been proposed (Lee et al., 2021b;\nSong et al., 2023; Nakamoto et al., 2023; Ball et al., 2023;\nYu & Zhang, 2023). Our method, FQL, is mainly designed\nfor offline RL, but we show that it can also be directly fine-tuned with online rollouts without any algorithmic changes.\nRL with diffusion and flow models. Motivated by the re-\ncent successes of iterative generative modeling techniques,\nsuch as denoising diffusion (Sohl-Dickstein et al., 2015;\nHo et al., 2020; Dhariwal & Nichol, 2021) and flow match-\ning (Lipman et al., 2023; Esser et al., 2024), researchers\nhave developed diverse ways to integrate them into RL. Pre-\nvious works have applied iterative generative models to\nplanning and hierarchical learning (Janner et al., 2022; Ajay\net al., 2023; Zheng et al., 2023; Liang et al., 2023; Li et al.,"}, {"title": "4.1. How Have Previous Works Trained Diffusion and Flow Policies with RL?", "content": "Various approaches have been proposed for training diffu-\nsion or flow policies with RL. In this section, we provide an\nin-depth review of these methods, discuss their advantages\nand limitations, and explain how FQL relates to prior work.\nPrior methods can be categorized into several groups based\non their policy extraction strategies (Park et al., 2024a).\n(1) Weighted behavioral cloning. One straightforward ap-\nproach to modulating a diffusion or flow policy is to as-\nsign weights to transition samples based on the correspond-\ning learned values. The most basic form uses advantage-\nweighted regression (AWR) (Peters & Schaal, 2007; Peng\net al., 2019; Nair et al., 2020) with the following objective:\n\\begin{equation}\n\\max_{\\theta} \\mathbb{E}_{s,a\\sim D} [e^{\\alpha (Q(s,a) - V(s))} \\mathcal{L}_{Flow}(\\theta)],\n\\end{equation}\nwhere a is an inverse temperature hyperparameter, and\nQ(s, a): S \u00d7 A \u2192 R and V(s) : S \u2192 R are state-action\nand state value functions, respectively (Sutton & Barto,\n2005). For diffusion policies, LFlow (0) is replaced with a\ndiffusion loss. Intuitively, this objective makes the policy\nselectively clone transitions with high advantages. Among\nprevious works, QGPO (Lu et al., 2023b), EDP (Kang et al.,\n2023), QVPO (Ding et al., 2024a), and QIPO (Anonymous,\n2025b) are mainly based on weighted behavioral cloning.\nWeighted behavioral cloning is simple and easy to imple-\nment. However, it is known to be one of the least effec-\ntive policy extraction methods (Fu et al., 2022; Park et al.,\n2024a), due to the small number of effective samples and\nlimited expressivity. In our experiments, we empirically\nshow that weighted behavioral cloning generally leads to\nsubpar performance, especially on complex tasks.\n(2) Reparameterized policy gradient. Another popular\napproach to guide an iterative generative model is to di-\nrectly maximize the value function Q(s, a) with reparame-\nterized gradients, while regularizing it with a flow or diffu-\nsion loss, as in Equation (6). Among previous approaches,\nDiffusion-QL (Wang et al., 2023), DiffCPS (He et al., 2023),\nConsistency-AC (Ding & Jin, 2024), SRDP (Ada et al.,\n2024), and EQL (Zhang et al., 2024) implement this scheme\nwith backpropagation through time."}, {"title": "5. Experiments", "content": "In this section, we empirically evaluate the performance\nof FQL, comparing it to previous offline RL and offline-\nto-online RL approaches on a variety of challenging tasks.\nWe also provide extensive analyses and ablations on policy\nextraction strategies and FQL's design choices."}, {"title": "5.1. Experimental Setup", "content": "Benchmarks. We use the recently proposed OGBench\ntask suite (Park et al., 2025) as the main benchmark (Fig-\nure 4). OGBench provides a number of diverse, challeng-\ning tasks across robotic locomotion and manipulation, with\nboth state and pixel observations, where these tasks are gen-\nerally more challenging than standard D4RL tasks (Fu et al.,\n2020), which have been saturated as of 2025 (Tarasov et al.,\n2023a; Rafailov et al., 2024; Park et al., 2024a). While OG-\nBench was originally designed for benchmarking offline\ngoal-conditioned RL, we use its reward-based single-task\nvariants (\"-singletask\") to make it compatible with stan-\ndard reward-maximizing offline RL algorithms. We employ\n5 locomotion and 5 manipulation environments where each\nenvironment provides 5 separate tasks, bringing the total to\n50 state-based OGBench tasks. In addition, we consider 5\ndiverse OGBench visual manipulation tasks to challenge\nthe agent's ability to handle 64 \u00d7 64 \u00d7 3-sized image ob-\nservations. Finally, we also employ relatively challenging 6\nantmaze and 12 adroit tasks from the D4RL benchmark.\nMethods. For our offline RL experiments, we use the fol-\nlowing 9 recent methods as representative examples of a va-\nriety of algorithm types and policy extraction strategies.\n(1) Gaussian policies. For standard offline RL methods\nthat use Gaussian policies, we consider BC, IQL (Kostrikov\net al., 2022), and ReBRAC (Tarasov et al., 2023a). In par-\nticular, ReBRAC is known to achieve state-of-the-art perfor-\nmance on many D4RL tasks (Tarasov et al., 2023b), and is\nthe closest Gaussian baseline to FQL in that both are based\non behavior-regularized actor-critic (Section 2)."}, {"title": "5.2. Results and Q&As", "content": "We present our results via the following Q&As.\nQ: How good is FQL for offline RL?\nA: FQL achieves the best or near-best performance on most\ntasks, especially in complex manipulation environments.\nTable 2 summarizes the aggregated benchmarking result on\na total of 73 state- or pixel-based offline RL tasks across\nrobotic locomotion and manipulation. We find that FQL gen-\nerally achieves better performance than previous methods,\nincluding ones based on Gaussian and diffusion policies.\nIn particular, FQL leads to consistently better performance\nthan its closest diffusion baseline (CAC), and often signifi-\ncantly outperforms its closest Gaussian baseline (ReBRAC)\nespecially on manipulation tasks, which feature highly mul-\ntimodal distributions. We also highlight that FQL achieves\nthe best performance of 84% on one of the hardest tasks in\nthe D4RL benchmark, antmaze-large-play (Table 3).\nQ: Can't I just use existing policy extraction schemes?\nA: You can, but previous policy extraction schemes gener-\nally lead to (often much) worse performance.\nThis can be seen by comparing the performances of FQL\nand {FAWAC, FBRAC, IFQL}, which are the closest flow-\nbased baselines to FQL, but with different policy extraction\nmechanisms. In particular, FBRAC is exactly the same as\nFQL except that it uses backpropagation through time. We\nemphasize again that these baselines are implemented on\nthe same codebase, use the same architecture, and are in-\ndividually tuned for each environment (Table 6). Figure 5\ncompares their offline RL performances aggregated over the\n50 state-based OGBench tasks in Table 2. The results show\nthat policy extraction alone can significantly affect perfor-\nmance, consistent with findings in Gaussian policies (Park\net al., 2024a). The results also indicate that our one-step\nguidance is the most effective, significantly outperforming\nthe other previous extraction strategies (Section 4.1).\nQ: Can FQL be fine-tuned with online rollouts?\nA: Yes, FQL can be directly fine-tuned without any modifica-\ntions, and often significantly outperforms previous methods.\nSpecifically, we can fine-tune FQL simply by adding new on-\nline transitions to the dataset D, while continuing to train all\nnetworks using the same objective as in offline training. To\nshow how effective FQL is for fine-tuning, we evaluate it on\n5 representative OGBench tasks across different categories\n(Table 4) as well as the 10 D4RL antmaze and adroit\ntasks used by Tarasov et al. (2023b). Figure 6 shows the\ntraining curves of FQL and previous approaches on these 15\ntasks, where online fine-tuning starts at 1M gradient steps\nQ: What are the important hyperparameters of FQL?\nA: The most important hyperparameter is the BC coefficient.\nFigure 7 shows the ablation results of the BC coefficient\na on three tasks. This hyperparameter needs to be tuned\nfor each environment based on the suboptimality of the\ndataset, as is typical for most offline RL methods (Tarasov\net al., 2023b; Park et al., 2024a). Other than a, the default\nhyperparameters of FQL work well, although tuning some\nadditional hyperparameters (e.g., target value aggregation\ndescribed in Appendix B) can slightly boost performance\non some tasks. We provide an extensive ablation study on a\ntotal of 4 factors of FQL in Appendix C.\nQ: Do I need to tune flow-related hyperparameters?\nA: No, in general.\nFor example, Figure 8 shows how the time sampling dis-\ntribution for flow matching affects performance, where we\nconsider the uniform distribution, Unif([0, 1]) (default), the\nbeta distribution used by Black et al. (2024), and the logit\nnormal distribution used by Esser et al. (2024). The re-\nsults suggest that time distributions matter only marginally,\nand the simplest uniform distribution is often sufficient to\nachieve the best performance. Similarly, we find that the per-\nformance is generally robust to the number of flow steps (the\ndefault is 10), as long as it is not too small (see Appendix C)."}, {"title": "Q: How fast is FQL?", "content": "A: FQL is one of the fastest flow-based offline RL methods.\nFigure 9 shows that, in terms of both training and inference\ncosts, FQL is only slightly slower than Gaussian policy-\nbased offline RL methods, while being faster than most flow-\nbased baselines. See Figure 11 for the detailed comparison\nresults.\nQ: Are flow policies better than diffusion policies?\nA: Maybe, but we do not make such a claim in this paper.\nThe main contribution of this paper is our policy extraction\nscheme (one-step guidance), not just the use of flow match-\ning itself. Although we show that one-step guidance com-\nbined with flow matching (i.e., FQL) achieves better perfor-\nmance than previous policy extraction schemes for diffusion\nand flow policies (Table 2), we believe it is possible to ap-\nply our one-step guidance to diffusion policies with appro-\npriate modifications to convert SDEs to ODEs (Song et al.,\n2021) to achieve similar performance, given the equivalence\nbetween the two frameworks (Gao et al., 2024). Neverthe-\nless, flow matching has one arguably clear advantage over\ndenoising diffusion: it is much simpler to implement!"}, {"title": "6. Closing Remarks", "content": "We presented flow Q-learning (FQL), a simple and perfor-\nmant offline RL method that leverages an expressive flow\npolicy and reparameterized policy gradients, without suf-\nfering from backpropagation through time. We showed\nthat FQL generally leads to the best performance on chal-\nlenging tasks across robotic locomotion and manipulation,\noffline RL and offline-to-online RL, as well as state- and\npixel-based settings. FQL, however, is not perfect; see Ap-\npendix A for the limitations of FQL.\nAs a closing remark, we would like to reiterate one particu-\nlarly appealing property of FQL \u2014 simplicity: one small\nalgorithm box (Algorithm 1) essentially captures the entire\ntraining objectives of FQL (modulo minor details), includ-\ning all of flow matching, iterative sampling, and value learn-\ning. Given that offline RL is notoriously sensitive to imple-\nmentation details in general (Tarasov et al., 2023b), we be-\nlieve proposing a simple yet performant method is a particu-"}, {"title": "A. Limitations", "content": "One potential limitation of FQL is that it requires numerically solving ODEs during training to minimize the distillation\nloss (Equation (7)). While this is not necessarily a significant speed bottleneck on both state- and pixel-based tasks in our\nexperiments (as shown in Figure 11) since flow matching happens in the relatively low-dimensional action space (as opposed\nto image generation), we believe this may further be improved by incorporating a more advanced one-step distillation method,\nsuch as shortcut models (Frans et al., 2025). Another limitation is that it does not have a \u201cbuilt-in\" exploration mechanism\nfor online fine-tuning. For example, FQL does not achieve the best online fine-tuning on the puzzle-4x4 task (Table 4), in\nwhich exploration can help avoid local optima. While we find that FQL without any additional exploration bonuses is enough\nto achieve strong performance on many challenging tasks (Figure 6), we believe it can be further improved by combining FQL\nwith a more principled exploration strategy or additional specialized fine-tuning techniques, leaving them for future work."}, {"title": "B. Implementation Details", "content": "In this section, we describe the full implementation details of FQL.\nFlow matching. As mentioned in Section 2, we use the simplest flow-matching objective (Equation (5)) based on linear\npaths and uniform time sampling. We use a step count of 10 for the Euler method across all tasks, and for simplicity, we do\nnot use sinusoidal embeddings for the time variable. See Figures 10c and 10d for ablation studies on these flow-related\nhyperparameters.\nValue learning. Following standard practice in RL, we train two Q functions to improve stability. We take the mean of\nthe two Q values for the Q loss term in the actor objective (Equation (8)). We also use the mean for the target value in the\ncritic objective (Equation (1)) by default, but we use the minimum of the two Q values (which is often referred to as clipped\ndouble Q-learning (Fujimoto et al., 2018)) for the adroit and OGBench antmaze-{large, giant} tasks, as we find it to\nbe slightly better. See Figure 10b for an ablation study on this choice.\nOnline fine-tuning. For offline-to-online RL, we simply add online transitions to the dataset, without distinguishing them\nfrom the offline transitions (i.e., we do not use balanced sampling, unlike Lee et al. (2021b); Nakamoto et al. (2023); Ball\net al. (2023)). We continue to train the components of FQL with the same objective as in offline training (Algorithm 1).\nNetwork architectures. For FQL, we use [512, 512, 512, 512]-sized multi-layer perceptions (MLPs) for all neural networks.\nWe apply layer normalization (Ba et al., 2016) to value networks to further stabilize training. We find that using a large\nenough network is especially important in navigation environments (e.g., antmaze).\nImage processing. For pixel-based environments, we use a smaller variant of the IMPALA encoder (Espeholt et al., 2018)\nand apply a random-shift augmentation with a probability of 0.5, following the official implementation of Park et al. (2025).\nIn addition, we use frame stacking with three images, which we find to be important on some pixel-based tasks, such as\ncube and puzzle.\nTraining and evaluation. We train FQL with 1M gradient steps for state-based OGBench tasks and 500K steps for D4RL\nand pixel-based OGBench tasks, and evaluate the agent every 100K steps using 50 episodes. For OGBench, following the\nofficial evaluation scheme (Park et al., 2025), we report the average success rates across the last three evaluation epochs\n(800K, 900K, and 1M for state-based tasks and 300K, 400K, and 500K for pixel-based tasks). For D4RL, following\nTarasov et al. (2023b), we report the performance at the last epoch. For offline-to-online RL results (Table 4), we report the\nperformances at 1M and 2M steps.\nBC coefficient a. The most important hyperparameter of FQL is the BC coefficient a in Equation (8). We perform a\nhyperparameter search over {1000, 3000, 10000, 30000} for adroit tasks and {3, 10, 30, 100, 300, 1000} for the other\ntasks, and use the best one for each environment. We use larger values for adroit tasks simply because their return scale is\nsignificantly larger than that of the other tasks. We believe normalizing the Q loss as in Fujimoto & Gu (2021) would lead\nto more similar a values across different tasks. While we do not apply this normalization technique in our experiments,\nwe recommend enabling Q normalization for new tasks (which is available in our official implementation) and tuning a\nstarting from {0.03, 0.1, 0.3, 1, 3, 10}. See Figure 10a for an ablation study on the BC coefficient.\nHyperparameters. We refer to Tables 5 to 7 for the complete list of hyperparameters."}, {"title": "C. Ablation Study", "content": "In this section, we ablate several components of FQL and study how they affect performance. Figure 10 shows our ablation\nresults, where we present training curves of FQL with different hyperparameters on a representative selection of tasks.\nBC coefficient a. As discussed in the main paper, the BC coefficient a is the most important hyperparameter of FQL.\nFigure 10a demonstrates that a needs to be tuned for each task based on the suboptimality of the dataset, as is typical for\nmost offline RL methods (Park et al., 2024a).\nTarget value aggregation methods. As discussed in Appendix B, we train two Q functions (Q1 and Q2) and use their\nmean, (Q1+Q2)/2, for target values in the critic loss by default, but we use their minimum, min(Q1, Q2), for some tasks,\nsuch as adroit. We present the ablation results in Figure 10b with the BC coefficient a individually tuned for each ablation\nsetting. The results show that not using clipped double Q-learning often leads to better performance, which is aligned with\nrecent findings in online RL (Ball et al., 2023; Nauman et al., 2024; Lee et al., 2025).\nFlow steps. To numerically solve ODEs, we use the Euler method, which requires a pre-specified number of steps. In this\nwork, we use 10 steps for all experiments. Figure 10c shows the ablation results, which suggest that the performance is\ngenerally robust to the number of flow steps, as long as it is not too small.\nTime distributions for flow matching. In this work, we use the uniform distribution, Unif([0, 1]), to sample time steps for\nflow matching. Prior works have considered other time distributions as well. For example, Esser et al. (2024) use the logit\nnormal distribution to emphasize intermediate steps (i.e., first sample t from the standard normal distribution, t ~ N(0, I),\nand then map it via the sigmoid function, t \u2190 1/(1+e-t)), and Black et al. (2024) employ a beta distribution, Beta(1, 1.5),\nto make the flow model focus more on the initial steps. We evaluate these three strategies and report the results in Figure 10d.\nThe results suggest that the performance is generally robust to the choice of the time distribution, and the simplest uniform\ndistribution is often enough to achieve the best performance."}, {"title": "D. Additional Results", "content": "Full results. We present the full per-task offline RL results in Table 3 and the full offline-to-online RL results in Table 4 and\nFigure 12. The results are averaged over 8 seeds (4 seeds for pixel-based tasks), and we report standard deviations after \u201c+\u201d\nin tables and 95% bootstrap confidence intervals as shaded areas in plots. In tables, we denote values at or above 95% of\nthe best performance in bold, following OGBench (Park et al., 2025). Results without standard deviations or confidence\nintervals indicate that they are taken from prior work; the D4RL results of BC, IQL, ReBRAC, and Cal-QL are taken from\nTarasov et al. (2023b), and the antmaze results of IDQL and SRPO are from Hansen-Estruch et al. (2023) and Chen et al.\n(2024b), respectively."}, {"title": "E. Experimental Details", "content": "We implement FQL and many of the baselines in JAX (Bradbury et al., 2018) on top of OGBench's reference implementa-\ntions (Park et al., 2025). We provide our full implementation and exact commands to reproduce the main results of FQL at\nhttps://github.com/seohongpark/fql.\nE.1. Environments, Tasks, and Datasets\nOGBench (Park et al., 2025). OGBench is our main benchmark, and we use 10 environments, 50 state-based tasks, and 5\npixel-based tasks from OGBench. Since OGBench was originally designed for offline goal-conditioned RL, we use the\nsingle-task variants (\u201c-singletask\") of OGBench tasks to benchmark standard reward-maximizing offline RL methods,\nwhich are available from the 1.1.0a1 version of OGBench. Each OGBench environment provides five evaluation goals,\neach of which defines a different task (-singletask-task1 to -singletask-task5), and one of them is set to be a\ndefault task (-singletask without a suffix). Given an evaluation goal, the corresponding singletask variant labels the"}]}