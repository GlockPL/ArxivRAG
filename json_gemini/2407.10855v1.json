{"title": "Weighted Grouped Query Attention in Transformers", "authors": ["Sai Sena Chinnakonduru", "Astarag Mohapatra"], "abstract": "The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslie et al., 2023) respectively.\nIn this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.", "sections": [{"title": "1 Introduction", "content": "At the core of language models lies an autoregressive transformer model (Vaswani et al., 2023) that generates one token at a time based on the input sequence and the previous sequence of output tokens it has generated so far. It is a sequential process, and the workload is memory-bound (Kwon et al., 2023). As we scale up the model size, the inference cost becomes expensive because we need to load the model into our GPU VRAM. The original transformer paper came out in 2017 and was trained on P100 GPUs with 5.3 TFLOPs double-precision performance and 16 GB of memory, compared to the current GPU, A100, which has 80 GB of GPU memory and 9.7 TFLOPs for fp64. There has been a significant increase in the computation capability of GPUs, with only a modest increase in memory. In the ZeRO paper (Rajbhandari et al., 2020), the authors demonstrated that GPT-2 (Radford et al., 2019), which has 1.5B parameters, required 3 GB of memory for its weights, and it could not be trained on 32 GB of memory due to the additional memory footprint of the activations and gradients. This also raises challenges in full parameter fine-tuning of these models as the memory requirements increase exponentially (Lv et al., 2024).\nThe current state-of-the-art models have significantly higher parameters, which also increase the inference cost. According to a recent estimate, processing a large language model (LLM) request can be 10x more expensive than a Google search query Dastin 2023. Due to the sequential nature of autoregressive models, the workload needs to load the model into memory and store the KV heads based on the tokens generated so far. Additionally, some decoding techniques, like beam search(Freitag and Al-Onaizan, 2017), can consume additional memory space by storing the KV heads for different paths and can lead to fragmentation of contiguous memory (Kwon et al., 2023). Hence, to resolve the memory-bound workload, the authors of the paper on MQA and GQA suggested grouping the query heads and aggregating the key-value heads after pre-training, followed by uptraining with 5-10% of the pre-training steps and then supervised fine-tuning on a downstream task. This approach led to performance converging with MHA while being more memory efficient. In this paper, we propose a parametric way of aggregating the key-value heads (WGQA) instead of the heuristic method of taking the element-wise mean of the corresponding key and value heads. We also explore different"}, {"title": "2 Related Work", "content": "This work is focused on achieving better performance over GQA and MQA, which are similar to model-pruning methods, except that we aggregate the pruning layers. These kinds of work improve memory bandwidth and exploit the computational speed of GPUs. (Pope et al., 2022) showed that MQA is helpful for long input training and inference due to the reduced memory overhead.\nThere are other techniques for improving the memory bandwidth overhead from keys and values. Quantization (Dettmers et al., 2022); (Frantar et al., 2023) reduces the size of model parameters and activations by using INT8 or bfloat16 precision, instead of float32. There are other parameter-efficient fine-tuning (PeFT) techniques, LoRA ((Hu et al., 2021)), which decompose the projection heads into a lower dimension and then compute the gradient steps, followed by composing the full-weight matrix again for gradient update. QLoRA ((Dettmers et al., 2023)) augmented LoRA by quantizing the static weight matrices, which further reduced the memory footprint.\nAll the existing decoder-only models like Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), Qwen (Bai et al., 2023) and OLMo (Groeneveld et al., 2024) are using grouped query attention instead of multi-head attention to reduce memory footprint. In our survey, our implementation is a novel way of grouping the key and value heads that are data-dependent and results in better performance."}, {"title": "3 Method", "content": "The attention module in the transformer architecture has three main components, query, key and value each with a dimension of (d, d), where d is the token embedding length. In Multi-head attention for h number of heads, the projection matrices have the dimension of (d, i), which transforms the input embeddings (n, d), where n is the sequence length of the input text, to h projections each of dimension (d, ), followed by concatenation to get the Q, K and V. Then the self-attention score is given by\n$score = softmax(\\frac{Q K^T}{\\sqrt{d}})V$ (1)\nIn grouped query attention, query heads are divided into G groups, reducing the number of key-value heads by a factor of . Hence, the projection dimensions to obtain Q, K and V are (n, d, d), (n,df, d) and (n, d\u33a2, df) respectively for a batch size of 1. For GQA, G = h/2 and for MQA, G = 1. The WGQA module adds extra scalar or vector parameters depending on the configuration for key-value heads for (W1,k, W2,k...Wh,k) and (W1,\u03c5, W2,v...Wh,v).\n$K = \\begin{bmatrix}  W_{1k}K_1 \\\\ +\\\\ W_{2k} K_2 \\\\  \\cdots \\\\ +\\\\ W_{(h-1)k} K_{h-1} \\\\ +\\\\ W_{hk} K_{h}  \\end{bmatrix}$ (2)\nThe modified K and V matrices are plugged into Eq 1 for attention computation. There are additional 2h parameters for weighted GQA (WGQA), 2 (COLWGQA) for weight vectors for the columns, and 2d (ROWWGQA) for weight vectors for the rows in each attention layer. These learnable parameters are multiplied with the key and value heads as shown in fig. 1. The injected weights are either initialized with a value of the mean of the number of heads in a group or a random standard Gaussian distribution. This adds no"}, {"title": "4 Implementation Details", "content": "We ran our experiments on T5-small and T5-base models implemented using Hugging Face transformers. All the models are initialized with pre-trained weights and fine-tuned on specific datasets using AdamW optimizer with 0.001 initial learning rate and scheduled linear decay. Key-value head grouping is only applied to decoder self-attention and cross-attention blocks, as mentioned in the original paper (Ainslie et al., 2023)."}, {"title": "4.1 Configuration", "content": ""}, {"title": "4.2 Data and Fine-tuning", "content": "We fine-tuned and evaluated our models using the CNN/Daily Mail, WMT 2014 German-English translation, and Multi-news datasets. We used only 500k rows for fine-tuning the WMT 2014 dataset due to limited computing resources. We trained all our models for 3 epochs with a batch size of 8 for the summarization tasks and a batch size of 32 for the translation task. We used an input length of 512 and an output length of 256 for the CNN/Daily"}, {"title": "4.3 Experimentation", "content": "We ran all the experiments shown in table 1 with T5-base, and with T5-small we ran only a few experiments on CNN daily mail as shown in the table 2.\nWeighted Grouped-Query Attention: In this approach, new parameters, a single scalar value for each key, and a value head in the decoder's attention blocks are used. A weighted sum is then taken during the forward propagation, allowing the model to learn these parameters during fine-tuning.\nGrouped-Query Attention: In GQA, key and value heads in the decoder's attention blocks are mean pooled to form G groups (Ainslie et al., 2023), which are then fine-tuned.\nMulti-Query Attention: MQA involves mean pooling all key-value heads in the decoder's attention blocks to form a single key-value head that is shared across all query heads.\nWeighted Multi-Query Attention: It is similar to Weighted Grouped Query Attention, but here we just group to only one key and value head.\nRow-wise Weighted Grouped-Query Attention: Here instead of scalar weights, we introduce a column vector of size d for each key and value head, which is used to scale the weights along each row as shown in fig. 1.\nColumn wise Weighted Grouped-Query Attention: In this, instead of scalar weights, we introduce a row vector of size kudim for each key and value head, which is used to scale the weights along each column as shown in fig. 1.\nFor all the weighted grouped query attention configurations, we performed two types of experiments that differ in how the weights are initialized for additional introduced parameters - initializing additional parameters with weights of kvheads/h and random initialization. The rationale behind initializing with kvheads/h is that it is equivalent to starting with the mean pooled Grouped Query Attention."}, {"title": "5 Results and Discussion", "content": "The weighted aggregation performed better than GQA in all our experiments. The ROUGE score (Ganesan, 2018) improved from 43.5 (GQA) to 43.7 (WGQA) and 43.8 (COLWGQA) for the multi-news summarization dataset. Similarly, for CNN/Daily Mail, the R1 score improved from 41.7 (GQA) to 41.9 (WGQA), and for the translation downstream task in WMT14 we reported the Bleu score (Saadany and Or\u0103san, 2021), the performance improved from 26.1 (GQA) to 26.3 (WGQA) (Table 1). During the fine-tuning stage, the number of parameters increased from GQA by 576 for WGQA, 36,864 for column-based COLWGQA, and 442,368 for row-based ROWWGQA. The WGQA performed well given the parameter and performance trade-off across the datasets.\nInitializing the weights with an average of the number of heads in a group performed significantly better than random Gaussian initialization across all the datasets. Also, WMQA, which is a weighted version of MQA, performed better than MQA and approached the performance of GQA. This can lead to even more parameter savings. We validated our results with the scaling laws by testing our models on a smaller architecture, T5-small, for the CNN/Daily Mail dataset (Table 2). Hence, increasing the model size results in better evaluation metrics, and we believe that bigger models would widen the performance gap between WGQA and GQA.\nTo check whether the learned weights in the WGQA configuration differ from those in the GQA configuration, we conducted a statistical analysis. We grouped the key and value heads of the WGQA model according to the learned weights and calculated the mean absolute loss for each layer. In the attention blocks, we calculated the mean for each head separately and observed that the weights are significantly different, with the mean absolute difference centering around 0.1 as shown in fig. 2. The p-value, le \u2013 6 was less than the significance level of 0.05, rejecting the null hypothesis of zero mean absolute difference."}, {"title": "6 Conclusion", "content": "This paper focuses on improving the GQA algorithm by introducing a novel way of aggregating the KV heads. From the scaling laws, we can extrapolate that the performance will improve with model size, and the models converge into different parameter spaces, as shown in the mean absolute plot. Given the prevalence of the GQA-based decoder model in Large Language Models, this technique can aid in building more accurate models with the overhead of linearly scaling weights during training only."}, {"title": "7 Limitations and Future Work", "content": "For summarization tasks, we used the ROUGE score, which is not an ideal metric and it doesn't give the whole picture to validate our increase in performance. Due to limited computing resources, we didn't pre-train our model from scratch or fine-tune on larger datasets and models, which would give better results for comparison.\nIn GQA, the grouped key value heads are repeated to match the dimension of query heads. In the future, we can introduce parameters that can dynamically repeat the key value heads. Specifically, in Grouped Query models such as Llama(Touvron et al., 2023) and OpenELM (Mehta et al., 2024), instead of sharing the key and value heads, we propose multiplying them with weights to create distinct heads. This approach would allow the model to differentiate between the heads, potentially enhancing performance. Additionally, we aim to implement this using decoder-only models, which is the current norm in language models."}]}