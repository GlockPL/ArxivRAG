{"title": "NO DATA, NO OPTIMIZATION: A LIGHTWEIGHT METHOD TO DISRUPT NEURAL NETWORKS WITH SIGN-FLIPS", "authors": ["Ido Galil", "Moshe Kimhi", "Ran El-Yaniv"], "abstract": "Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping only a handful of sign bits in their parameters. We introduce Deep Neural Lesion (DNL), a data-free, lightweight method that locates these critical parameters and triggers massive accuracy drops. We validate its efficacy on a wide variety of computer vision models and datasets. The method requires no training data or optimization and can be carried out via common exploits software, firmware or hardware based attack vectors. An enhanced variant that uses a single forward and backward pass further amplifies the damage beyond DNL's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet reduces accuracy by 99.8%. We also show that selectively protecting a small fraction of vulnerable sign bits provides a practical defense against such attacks.", "sections": [{"title": "Introduction", "content": "Deep neural networks (DNNs) power a wide range of applications, including safety-critical tasks such as autonomous driving, unmanned aerial vehicle (UAV) navigation, medical diagnostics, and robotics, where real-time decision-making is essential. However, the increasing reliance on DNNs also raises concerns about their resilience to malicious attacks. Ensuring the robustness of DNNs is crucial to maintaining their reliability in such critical applications.\nIn this paper, we expose a critical vulnerability in DNNs that allows for severe disruption by flipping as few as one to ten sign bits, a tiny fraction of the model's parameters. Our method demonstrates how a small number of bit flips, within models containing up to hundred millions of parameters, can cause catastrophic degradation in performance.\nWe systematically analyze and identify the parameters most susceptible to sign flips, which we term \"critical parameters.\" Our approach disrupts models by flipping the sign bits in their weights, achieving significant performance degradation with minimal computational effort across a wide range of model architectures. Crucially, our approach is data-agnostic: it requires only direct access to model weights, bypassing any need for training or validation data.\nOur method is an extremely lightweight, Pass-free Attack, which needs no additional computational passes and is driven by a magnitude-based heuristic, considering inductive bias on learnable features and the flow of information within the networks. We also propose an enhanced 1-Pass Attack that employs a single forward and backward pass (on random inputs) to refine the selection of critical parameters. Despite this small increase in computation, the attack remains highly efficient, preserves its data independence, and inflicts even greater damage on the model.\nMalicious actors can exploit the identified parameter vulnerability through multiple system layers, including file-system intrusions, firmware compromises, direct memory access (DMA) from compromised peripherals, or memory-level exploits. In each case, once attackers gain access to the model's parameters, they can flip a small number of high-"}, {"title": "Problem Setup", "content": "Modern deep learning frameworks typically store parameters in the IEEE 754 32-bit floating-point format. Each float has a sign bit, eight exponent bits, and 23 mantissa bits: $(-1)^s \\times 2^{(e-127)} \\times (1 + \\frac{m}{2^{23}})$. We focus on a standard supervised learning scenario where a model $f_\\theta$ is trained on a dataset with distribution $D$. Let $X, Y$ be the input and label spaces, $(X, Y) \\sim D$, where $X \\in X$ and $Y \\in Y$. A trained model $f_\\theta$ seeks to minimize the expected risk $\\min_\\theta E_{(X,Y)\\sim D} [L(f_\\theta(X), Y)]$, where $L$ is a loss function. Once trained, $\\theta$ is deployed for inference.\nAn attacker, who lacks access to $D, P(X)$, or $P(Y)$, nonetheless obtains control over the model parameters $\\theta$ and flips a small subset of $k$ bits in $\\theta$ to produce $\\theta^{(k)}$. The adversary can gain access directly through software, firmware, or hardware-level exploits, namely Bit flip attacks [33]. Below, we outline several exploits that adversaries can leverage to execute malicious bit-flipping operations on model parameters.\nA rootkit [12, 41, 39] is malicious software running with high-level (kernel or ring-0) privileges, allowing it to intercept or modify operations. Once installed, a rootkit can scan the system's memory or storage for the model's parameter files, then surgically flip bits in place. By concealing its processes and hooking system APIs, the rootkit can evade detection from common antivirus tools and monitoring systems, enabling stealthy, ongoing tampering with model parameters without triggering suspicious activity logs.\nFirmware exploits [15] (e.g., SSD/HDD controllers, GPU firmware, BIOS, or microcode patches) can give attackers privileged memory access or the ability to inject custom commands that flip bits in system memory or on storage media. By compromising firmware updates or exploiting known bugs, attackers can precisely manipulate parameter bits.\nDMA from untrustworthy peripherals [30] can read and write system memory without involving the CPU or the operating system's normal access controls. If attackers gain low-level access to a DMA device (e.g., via Thunderbolt or FireWire interfaces), they can directly overwrite targeted bits in protected memory regions.\nRowhammer [18, 17, 40] exploits the electrical interference between neighboring rows in modern DRAM modules. By rapidly accessing (\"hammering\") one row, an attacker causes bits in adjacent rows to flip, even without direct write permissions. Rowhammer attacks typically rely on high-frequency memory accesses that defeat standard refresh mechanisms; once carefully controlled, these flips can be directed at specific bit positions.\nGPU cache tampering [24, 46], which exploits a compromised kernel driver or malicious GPU code, can manipulate cache management routines to induce bit flips in stored parameters. Similar to Rowhammer's repeated DRAM accesses, continuously evicting and reloading specific cache lines may corrupt targeted parameters. Because GPU caches are often less scrutinized than CPU caches, this tampering can remain undetected, leading to stealthy yet severe degradation of model performance.\nVoltage/frequency glitching [32, 45, 47, 7] manipulates the operating voltage or clock frequencies to induce computational errors. Certain voltage ranges can systematically cause specific bits to flip in registers or memory segments.\nIn all cases, the attacker's objective is to significantly degrade performance with minimal bit flips for stealth and practicality. $\\min_\\theta \\max_\\theta E_{(X,Y)\\sim D} [L(f_{\\theta^{(k)}}(X), Y)]$,\nwhere both finding minimal $k$ and flipping $k$ bits to produce $\\theta^{(k)}$ are discrete optimization problems.\nIn other words, the attacker's goal is to induce a significant performance drop while flipping only a handful of bits, both for stealth and practical reasons, as fewer corruptions are less likely to be detected and can be exploited by the mentioned hardware attacks. For instance, Rowhammer-based exploits [18] typically induce only sporadic bit upsets in adjacent cells, making massive coordinated flips infeasible. Notably, the attacker does not have access to any training or validation data, nor do they conduct extensive inference passes or iterative gradient-based searches. Such lightweight attacks are realistic in settings where the attacker's computational resources on the victim device are minimal, or where repeated forward/backward passes might raise suspicion. We therefore distinguish two scenarios: a pass-free attack, which uses no extra computation beyond reading the model weights, and a 1-pass attack, which uses only a"}, {"title": "Accuracy Reduction Metrics", "content": "To measure the effect of bit flips, let $\\theta^{(k)}$ be the set of parameters obtained by flipping exactly $k$ sign bits in $\\theta$. If Acc($\\theta$) is the model's original accuracy, we define:\n$AR(k) = \\frac{Acc(\\theta) - Acc(\\theta^{(k)})}{Acc(\\theta)}$,\nwhich captures the drop in accuracy induced by $k$ flips. For a broader view, we also define:\n$mAR(N) = \\frac{1}{N} \\sum_{k=1}^{N} AR(k)$,\nso that a single number can represent the model's overall vulnerability across different flip counts. Because practical hardware attacks often manage only a handful of flips, we mainly focus on small $k$ (e.g., $k \\le 10$)."}, {"title": "Locating Models' Most Critical Parameters", "content": "Considering the FP32 representation, while exponent flips can alter a weight's magnitude, flipping the most significant sign bit instantly switches a parameter from positive to negative (or vice versa). This produces a drastic effect on the learned features, as shown in Figure 1, motivating us to focus on those bits. Moreover, localizing the sign bit in memory is straightforward (e.g., always the MSB), making it appealing as a simple target for adversaries. Various hardware-based studies show that repeated access patterns more reliably flip the same bit position across different addresses than arbitrarily chosen bits [18, 40]. Hence, focusing on sign bits aligns with how hardware attacks often achieve consistent flips in a specific bit offset across multiple weights, increasing the chance of our targeted attack success rate.\nFlipping random sign bits in a network's parameters typically has a negligible impact on performance. Indeed, our experiments (visualized in Figure 2) show that for many architectures, flipping even up to 100,000 bits (up to 8% of the parameters of some models) does not reduce the accuracy consistently-indicating that most parameters are not \u201ccritical.\" These findings motivate a more targeted strategy to identify and flip only the most sensitive parameters."}, {"title": "Magnitude-Based Strategy", "content": "Drawing inspiration from the pruning literature, we first examine magnitude-based strategies. Just as magnitude pruning removes low-magnitude weights to minimize the impact on final predictions [6], we hypothesize that flipping the sign of high-magnitude parameters causes significant disruption. Formally, the parameter score function is defined as follows\n$S(\\theta_i) = |\\theta_i|$,\nAs far as we are aware, this work is the first to evaluate the efficacy of a magnitude-based attack, a surprisingly simple yet powerful strategy that disrupts neural networks without data, optimization or prior knowledge. In Figure 3, the second boxplot from the left, shows that focusing on the top-k largest weights (in absolute value) significantly disrupts most evaluated models."}, {"title": "One-Flip-Per-Kernel Constraint", "content": "Empirical analyses of CNN filters [20, 52] highlight the importance of early-stage kernels (e.g., Gabor-like or Sobel-like) in extracting fundamental visual features. These studies reveal that flipping a single sign bit in a kernel can completely disrupt its feature extraction capability, altering the information the model relies on (see Figure 1 for the effect of sign flips on a real kernel). However, flipping multiple bits within the same kernel often merely changes its orientation or slightly modifies its functionality, rather than fully destroying the feature, as demonstrated in Figure 5. We observe this phenomenon consistently across multiple architectures. Below are a few examples:"}, {"title": "Layer Selection", "content": "Beyond which parameters to flip, we also investigate where in the network to apply the attack. One might intuitively expect that targeting final layers-being closer to the classifier-would cause greater damage. However, our experiments reveal that in many architectures, early-layer manipulations are disproportionately damaging. Drawing on an analogy from neuroscience, early lesions (e.g., in the retina or optic nerve) can cause severe or total blindness [16, 42, 5]. Similarly, flipping a single parameter in a fundamental feature detector (e.g., Sobel and Gabor filters) sends erroneous signals throughout subsequent layers, often leading to compounding error. Figure 1 illustrates this: a sign flip in a low-level \"edge-detection\" filter causes the network to misinterpret critical structural cues, compounding errors to later layers and severely degrading performance -more so than flips occurring in higher-level layers."}, {"title": "Enhanced Attack Using a Single Forward Pass", "content": "When a single forward (and backward) pass is within the attacker's budget, we propose an enhanced attack, called 1P-DNL, inspired by gradient-based pruning methods [22, 31, 23, 48, 44]. These methods typically assign a saliency or importance score to each parameter $\\theta_i$ by measuring how altering that parameter (e.g., pruning or modifying it) would affect the network's loss or outputs. Although pruning and sign-flip attacks differ in goal, the underlying idea of identifying the \"most critical\" weights is similar."}, {"title": "Hybrid Importance Score", "content": "We define a hybrid importance scoring function that combines magnitude-based saliency with second-order information. Specifically, we start from a second-order Taylor expansion of a scalar loss function $R(\\theta)$ around $\\theta_i$ [22, 10]. Let $\\alpha$ and $\\beta$ be tunable coefficients controlling the relative weight of magnitude- and gradient-based terms. For a given parameter $\\theta_i$,\n$S(\\theta_i) = \\alpha |\\theta_i| + \\beta |\\frac{\\partial R}{\\partial \\theta_i} + \\frac{1}{2} \\sum_j H_{ii} \\theta_i^2 + \\sum_{j \\neq i} |$,\nwhere $H$ is the Hessian of R with respect to $\\theta$. In our case, we let $\\alpha = \\beta = 1, R(\\theta)=\\sum_i c_i$, where $c_i$ is the output logit (or class score) for a Gaussian input. Although the summation over $j \\neq i$ captures inter-weight coupling, we approximate $H_{ij} = 0$ for $j \\neq i$ (a common diagonal approximation in second-order pruning [22]), significantly"}, {"title": "Additional Analysis", "content": "We further assess our sign-flip attack on various datasets to confirm its broad applicability beyond ImageNet. In particular, we evaluate DNL and 1P-DNL on DTD [3], FGVC-Aircraft [28], Food101 [1], and Stanford Cars [19]. In Figure 7 and Figure 8, we plot the average accuracy reduction across EfficientNetB0 [43], MobileNetV3-Large, and ResNet-50 [11] after applying DNL and 1P-DNL respectively. In all cases, flipping as few as one or two sign bits leads to a sudden collapse in model performance, reaffirming that this vulnerability is not tied to a specific dataset or image distribution. Most notably, applying DNL disrupts all models across all datasets with $AR(5) \\geq 85\\%$, and 1P-DNL achieve $AR(4) \\geq 90\\%$ across all data and models. For additional evaluations on each dataset, see Appendix C.\nTaken together, these results underscore both the potency and the generality of our approach: across diverse vision tasks and architectures, zero or one-pass sign-flip attacks consistently induce dramatic failures with minimal computational overhead or data requirements."}, {"title": "Impact of Model Size on Attack Success", "content": "To assess whether model size influences the effectiveness of our attacks, we evaluate both DNL and 1P-DNL, across five families of architectures with varying parameter counts: ResNet, RegNet, EfficientNet, ConvNeXt [25], and ViT"}, {"title": "Selective Defense Against Sign-Flips", "content": "A straightforward way to protect against sign-flip attacks is to maintain multiple copies of the model's sign bits and compare them at inference time. Flipping just a few bits in one or two copies would not suffice to degrade the predictions, since a majority vote of the copies will apply before prediction, forcing an attacker to corrupt majority of the copies simultaneously. Although effective, this strategy inflates memory usage and computational overhead.\nA more memory-efficient approach is to employ error-correcting codes (ECC) Peterson and Weldon [35], such as Hamming codes, which detect and correct single-bit errors per memory word. ECC helps safeguard against sign flips by automatically reversing small-scale corruptions. However, as the number of bits or parameters grows, stronger ECC schemes (e.g., multi-bit correction) can be required, further increasing memory overhead."}, {"title": "Related Work on Bit-Flip Attacks", "content": "Early works such as Terminal Brain Damage (TBD) Hong et al. [13] illustrated how manipulating exponent bits could severely harm floating-point networks. However, TBD excludes sign bits, which we find can be far more devastating to overall accuracy with fewer flips. Other methods, including [38, 51], perform iterative gradient-based flips. For example, Rakin et al. [38] requires multiple samples to compute gradients and can disrupt ResNet-50's accuracy by ~ 99.7% using 11 bit flips. Yao et al. [51] similarly needs iterative optimization, reaching significant disruption at the cost of 23 flips.\nRecent variants have attempted to relax data requirements. For instance, [8, 34] generate pseudo-samples or use partial data statistics to guide which bits to flip. Although they lessen the need for a large labeled dataset, they still rely on model feedback or approximate gradients. In contrast, our sign bit flipping approach is lightweight, data-agnostic, and can degrade a large variety of networks by by over 99.8% with few flips. This distinction stems from focusing on sign bits, which, due to the abrupt change from + to \u2013, often exert a disproportionate influence on learned representations.\nWhile prior research highlights the vulnerability of neural networks to parameter corruption, the striking simplicity and severity of sign flips merit closer scrutiny. As shown in Table 3, these flips can be carried out without data or optimization, and are straightforward to locate in memory, making them both feasible and devastating in real-world scenarios."}, {"title": "Concluding Remarks", "content": "We presented a previously unknown vulnerability that is critical for the security of DNNs. We also showed a simple way to make models robust to such vulnerabilities.\nOur findings highlight a pressing need to re-examine security and robustness in DNN deployments, particularly in safety-critical contexts. Beyond inspiring the development of targeted sign-flip defenses, our results also open questions about model architectures and training regimes that can inherently mitigate parameter vulnerabilities. We hope our work will encourage the research community to explore architectural, optimization, and hardware-level strategies to build DNNs more robust against sign-bit attacks."}, {"title": "Compare to Bit Flip Attacks", "content": "Following Section 6, Table 3 compares bit-flip attacks on ImageNet1K and highlights how our approach differs from prior methods. BFA [38] and DeepHammer [51] rely on iterative gradient-based optimization and require partial data or repeated inference steps. ZeBRA [34] removes the need for real data but still employs optimization. In contrast, our methods (DNL and 1P-DNL) are both data-agnostic and optimization-free, targeting sign bits with a lightweight, single-pass or pass-free strategy. We compare the complexity of prior art to ours in Table 2. Despite this simplicity, they match or surpass prior work in accuracy reduction (AR) while needing only a handful of bit flips as shown in Table 3; for instance, 1P-DNL degrades ResNet-50 by 99.8% with just two sign flips. This balance of minimal computation and high impact underscores the severity of sign-bit vulnerabilities in modern DNNs."}, {"title": "Weight Score Ablation", "content": "We evaluate several parameter scoring functions from the pruning literature and compare their effectiveness in identifying high-impact weights for sign-flip attacks. As shown in Figure 11, we measure the mean accuracy reduction $mAR_{10}$ across 48 ImageNet models under the following scoring functions:\nMagnitude-based: $S(\\theta_i) = |\\theta_i|$.\nGraSP: $S(\\theta_i) = |\\theta_i \\bigodot H_g|$, following the gradient-flow preservation principle of Wang et al. [48] where $H_g$ is the hessian vector product.\nGraSP (Gauss-Newton Approx.): Similar to GraSP but approximates the Hessian $H$ with the square of first-order gradients.\nSynFlow: $S(\\theta_i) = |g \\bigodot \\theta_i|$, akin to gradient times weight.\nOptimal Brain Damage (OBD): $S(\\theta_i) \\approx \\frac{1}{2} \\theta_i H_{ii} \\theta_i [22]$.\nHybrid (Ours): As we define in Equation (3) with and without second order term.\nwhere $g = \\frac{\\partial R}{\\partial \\theta}, H = \\frac{\\partial^2 R}{\\partial \\theta^2}$.\nWe observe that certain models are vulnerable to second-order-based scores (e.g., OBD) even when they prove more resilient to pure magnitude-based attacks. Nevertheless, other architectures appear more robust against OBD or GraSP while showing larger drops under magnitude-based score. Motivated by these mixed results, our hybrid score combines both magnitude and gradient terms. This blend consistently identifies critical weights even in cases where either component alone fails to degrade accuracy. Overall, the hybrid approach delivers the most reliable performance drop across the tested models."}, {"title": "Additional Datasets Evaluation", "content": "Figures 12, 13, and 14 analyze individual dataset results on these three popular classifiers. Each shows a steep drop in accuracy with very few sign flips, highlighting the generality of the attack. Notably, although these models differ"}]}