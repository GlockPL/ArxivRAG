{"title": "Automatic Scene Generation: State-of-the-Art Techniques, Models, Datasets, Challenges, and Future Prospects", "authors": ["Awal Ahmed Fime", "Saifuddin Mahmud", "Arpita Das", "Md. Sunzidul Islam", "Hong-Hoon Kim"], "abstract": "Automatic scene generation is an essential area of research with applications in robotics, recreation, visual representation, training and simulation, education, and more. This survey provides a comprehensive review of the current state-of-the-arts in automatic scene generation, focusing on techniques that leverage machine learning, deep learning, embedded systems, and natural language processing (NLP). We categorize the models into four main types: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Transformers, and Diffusion Models. Each category is explored in detail, discussing various sub-models and their contributions to the field.\nWe also review the most commonly used datasets, such as COCO-Stuff, Visual Genome, and MS-COCO, which are critical for training and evaluating these models. Methodologies for scene generation are examined, including image-to-3D conversion, text-to-3D generation, UI/layout design, graph-based methods, and interactive scene generation. Evaluation metrics such as Fr\u00e9chet Inception Distance (FID), Kullback-Leibler (KL) Divergence, Inception Score (IS), Intersection over Union (IoU), and Mean Average Precision (mAP) are discussed in the context of their use in assessing model performance.\nThe survey identifies key challenges and limitations in the field, such as maintaining realism, handling complex scenes with multiple objects, and ensuring consistency in object relationships and spatial arrangements. By summarizing recent advances and pinpointing areas for improvement, this survey aims to provide a valuable resource for researchers and practitioners working on automatic scene generation.", "sections": [{"title": "1 Introduction", "content": "Automatic scene generation is the process of creating different world representations using different techniques from machine learning, deep learning, embedded systems, NLP, etc. Manual scene generation is time-consuming. Also, user has to learn different techniques and applications of graphics to generate any scene. This can limit the use of scene generation in different applications. Automatic scene generation can be useful for the user in the non-graphics domain [1]. Scene generation has many applications in different fields like robotics, recreation, visual representation, training and simulation, education, research, and many more.\nMany research have taken place to generate 3D scenes or 2D images automatically in previous years. Some research works used text descriptors as input from the users and generated 3D scenes or 2D images based on the user specifications. Some took 2D images to generate 3D scenes using different deep-learning models [2]. There are some researchers who developed 2D images using bounding boxes. Many researchers used graphs to generate a scene. The graph nodes are mainly the objects of the scene and the edges of the graph represent the relations between the objects. Some researchers also used images of objects to generate a 2D image of the scene by merging those objects. Many researchers also added more flexibility to the system by creating an interactive method where user can modify the scene as per needed [3]. Where most of the works are considered as supervised classification problem by assigning class to each object of different categories, some researches considered it as semi-supervised classification problem having objects of unknown class [4][5]. Many researchers also used videos and audio signals as input and generated scenes by applying different techniques to extract features from those videos or audio signals. Some researchers also worked on scene layout by using different deep-learning models. Some researchers tried to develop a hybrid model that can take more than one type of input to generate the final scene. Sora, Gemini and ChatGPT are well known for their efficient scen\nMost of the scene generation systems are developed using different deep learning-based algorithms. The most applied algorithms are GAN, VAE, Transformer, and Diffusion models. Different researchers modified the architecture of these base models to generate their systems. Based on the type of input, different types of datasets are used for both the training and evaluation of the systems. Most of the papers frequently leverage the COCO-Stuff [6] and Visual Genome datasets [7]. Additionally, other datasets play crucial roles in various paper proposals. Also, many loss functions and optimizers are implemented in different systems. Every system's performance is evaluated using many types of evaluation metrics. The most used metrics are Fr\u00e9chet inception distance (FID), Kullback-Leibler (KL) Divergence, Inception score (IS), Intersection over Union (IoU), and Mean Average Precision (mAP). However, many authors also applied many other evaluation metrics for analyzing their system's performance quantitatively. Almost every paper also did a qualitative comparison with other existing state-of-art systems. Some authors also evaluated their method by the users' inspection to identify whether the output of the systems can fulfill the user's satisfaction or not.\nIn the paper, we tried to summarize papers on scene generation of previous years as many as possible. The paper is divided into different categories based on the type of input of the papers as well as their implementation methods. The categories are i) Image to 3D, ii) Text to 3D, iii) UI/Layout Design, iv) Box to Image, v) Graph, vi) Mask to Image, vii) Interactive Scene, viii) Semi Supervised, ix) Text to Image, x) Video, xi) Image reconstruction, xii) Others. We also worked on finding out the datasets and models they used, the loss functions, optimizers, and metrics to evaluate their system's performances and their contributions. We tried to find out the limitations and challenges they faced during the development of the systems. To summarize, our contribution is:\n1. We summarized the most recent papers on scene generation based on 13 categories and the method of developing the systems for scene generation.\n2. We described the datasets and models the authors of the papers used to train and evaluate the system. Also, the loss functions, optimizers, and metrics mentioned in the papers are analyzed.\n3. We tried to find out the challenges and limitations of the development of proposed systems and solutions to resolve the problem mentioned in the papers.\nThe following sections provide more details about the whole paper. Section 2 describes the models that are used for developing scene generation systems in different papers. Section 3 explains about the datasets used in different papers we summarized. Section 4 explains the papers that worked to generate images using bounding boxes. Section 5 describes about the papers that used graphs as input. Section 6 summarized the papers that developed a hybrid system for scene or image generation. Section 7 summarized the papers that used masks to generate images. Section 8 describes papers developing interactive scenes. Section 9 explains about the papers that used semi-supervised methods for scene generation and section 10 summarizes the papers that took text descriptors as input to generate an image. Section 11 summarized the papers that used videos as input. Section 12 explains papers related to image reconstruction. Section 13 describes the papers that designed user interfaces or layouts for scene generation. Section 14 and Section 15 explain the papers for 3D scene generation that take image and text as input respectively. Section 16 summarizes other papers related to scene generation. Section 17 and Section 18 describe the loss functions and evaluation techniques used in different papers we summarized respectively. Section 19 explains about the use cases of scene generation. Section 20 explains the challenges and limitations mentioned in the papers on scene generation. Finally, section 22 concludes the importance of the scene generation survey and the overall contribution of our paper."}, {"title": "2 Base models", "content": "Most of the scene generation models follow some basic architecture. On top of those base models, most of the other researchers customize or optimize their structure. Four base models for scene generation and their sub-models are mentioned in this section.\n2.1 Variational Autoencoder (VAE)\nThe idea of autoencoders is a setting of an encoder and a decoder that stores information in a small dimension and reconstructs it when necessary. However, the problem with autoencoders is that they can't generate new content. To solve this issue Variational Autoencoder (VAE) comes into action [8]. It takes the same input as the autoencoder and transforms it into probability distribution within latent space instead of a single point. The decoder takes a sample from the distribution and tries to reconstruct it back to the original data. During training, it tries to adjust the weight of both the encoder and decoder to reduce the reconstruction loss. It tries to balance two things at the same time the reconstruction loss and the regularization term. The combined loss is mentioned in the equation 1. This regularization term pushes the latent space to maintain the chosen distribution, preventing overfitting and promoting generalization. The basic architecture of VAE is mentioned in figure 2.\n$\\mathcal{L}_{VAE} = -E_{q(z|x)}[log\\,p(x|z)] + KL[q(z|x)||p(z)]$\t\t(1)\nWhere, x represents an image, and z represents latent variables. $KL[q(z|x)||p(z)]$ is the Kullback-Leibler divergence term and $-E_{q(z|x)}[log\\,p(x|z)]$ is the reconstruction loss.\nConditional variational autoencoder (CVAE)\nOne of the problems with VAE is the user doesn't have any control over the reconstruction process. The conditional VAE gives a way to guide image generation through class labels or prompt text [9]. Conditional VAE takes additional input along with the input image from the user.\nRecursive Neural Network (RVNN VAE)\nRecursive Neural Network or RVNN is able to extract detailed and structured information from data [10]. It uses the same weight again and again on the structured data. This structure is useful for tree-like structures. [11] is using RvNN with VAE architecture.\nGCN-VAE\nA graph is a common structure to represent knowledge with interconnection. To grab graph information including all its relations to pass through the VAE architecture Graph Convolution Neural Network is used [12]. [13] [14] are a few examples of using GCN-VAE for scene generation.\nVQ-VAE\nVQ-VAE has two primary differences from VAEs: firstly, the encoder network produces discrete codes instead of continuous ones; secondly, the prior is learned instead of static [15]. A discrete latent representation is learned by incorporating concepts from vector quantization (VQ). By utilizing the VQ approach the model can avoid posterior collapse problems, which occur when latents are neglected and paired with a strong autoregressive decoder. By associating these representations with an autoregressive prior, the model can produce high-quality photos, videos.", "subsections": []}, {"title": "2.2 GAN", "content": "The generative adversarial network is a two-player game cutting-edge approach for image generation [16]. It consists of one generator and a discriminator. The generator tries to generate images as real as possible. For this, it takes a fixed-length vector with random variables from the Gaussian distribution. This vector is close to the latent variable for VAE. With the help of this vector, the generator generates the image of the same domain. The discriminator works as a pure binary classifier in the model. It takes each image and tries to predict whether it is real or fake. GAN is an unsupervised learning model. During training time, if the discriminator identifies the fake image then the discriminator stays the same but the generator gets updated. When the generator fools the discriminator, the discriminator gets updated. This process continues until the generator generates all images that the discriminator can't identify. After training, the discriminator is discarded and only the generator is used to generate new images. The mathematical statement to optimize for GAN is mentioned in equation 2. The architecture of the GAN model is shown in figure 3.\n$\\underset{G}{min}\\underset{D}{max} V(D, G) = E_{x \\sim p_{data}(x)} [log D(x)] + E_{z \\sim p_{z}(z)} [log(1 \u2013 D(G(z)))]$\t\t(2)\nHere, D represents the discriminator, G represents the generator, $P_{data}(x)$ is the distribution of real data, and $p_{z}(z)$ is the distribution of the noise vector (z)."}, {"title": "BicycleGAN", "content": "To restrict few number of real samples conversing again and again in the output, BicycleGan was proposed [17]. It adds a bijection between output and latent space. One way the output is generated from the latent space is that the output is again encoded to the latent space. This way it prevents the model generate only few number of samples from the input sample."}, {"title": "BigGAN", "content": "The main difference between other GANs and BigGANs is the size of the model [18]. It increases the batch size and width of each layer for the model which improves its performance. It also adds a skip connection from the latent variable to the generator. This model also introduced a new variation of orthogonal regularization."}, {"title": "CycleGAN", "content": "As the name mentioned, CycleGAN works in a cyclic order [19]. First, from input, it tries to generate the image. After that, it works in the opposite direction, where it tries to generate an input image from the generated image. Then the mean squared error is calculated from the generated image and input image. This process is able to map between paired and unpaired images."}, {"title": "GauGAN", "content": "GauGAN is developed by NVIDIA. It is capable of taking text, semantic segmentation, sketch, and style within a single GAN and generating an image based on that [20]. In the example video, they sketched an area and selected a rock, and based on that it generated a rock at that place. The second version of this model is also published."}, {"title": "InfoGAN", "content": "InfoGan encourages the GAN network to learn interpretable and meaningful information by maximizing the mutual information between a small set of observation and noise variables [21]. It is more interested in mutual information that helps the model to optimize it properly. As a result, it can separate writing style from the digit shape or visual concepts from the face like hairstyle, emotion, or the presence of eyeglasses."}, {"title": "LayoutGAN", "content": "LayoutGAN takes multiple layers of the 2D plane and tries to generate semantics and 2D relations among the objects [22]. It applies multiple self-attention layers on those planes to generate a meaningful layout from it. For this purpose, the rendering layer is the beginning challenge."}, {"title": "MaskGAN", "content": "MaskGAN maps semantic segmentation to target image [23]. It also allows the property to manipulate segmentation maps interactively to generate new maps. These manipulations are learned as traversing on the mask manifold, that gives better result. This model is based on two components Dense Mapping Network and Editing Behavior Simulator. The author of this model also contributed by generating high-resolution face images."}, {"title": "OC-GAN", "content": "Complex scenes with multiple objects are hard to generate. By addressing this issue, OC-GAN [24] mainly tried to solve two problems i) some spurious objects are generated without corresponding bounding boxes and ii) Overlapping boxes merge two objects. The author proposed an Object-Centric Generative Adversarial Network to learn about individual objects and the spatial relationships among objects in the scene."}, {"title": "SAGAN", "content": "For picture generation challenges, attention-driven, long-range dependency modeling is made possible by the Self-Attention Generative Adversarial Network, or SAGAN [25]. Using only spatially localized points from lower-resolution feature maps, conventional convolutional GANs produce high-resolution details. With SAGAN, cues from every feature position can be used to produce details. Additionally, the discriminator may verify the consistency of highly detailed features located in distant regions of the image with one another."}, {"title": "SN-GAN", "content": "Spectral normalization in Generative Adversarial Networks (GANs) resolves problems like vanishing gradient, exploding gradient, and model collapsing using Spectral normalization. It also makes the training process easier and smoother. Spectral normalization, dividing by each weight spectral norm, acts as a form of normalization, that keeps the model mathematically predictive."}, {"title": "StyleGAN", "content": "One kind of generative adversarial network is StyleGAN [26]. Drawing on style transfer literature, it employs an alternate generator architecture for generative adversarial networks. Otherwise, it uses a training regimen that increases gradually, similar to Progressive GAN. Among its other peculiarities is that, unlike conventional GANs, it derives from a fixed value tensor rather than stochastically generated latent variables. After being converted by an 8-layer feedforward network, the stochastically generated latent variables are employed as style vectors in the adaptive instance normalization at each resolution. Finally, it uses mixing regularization, a type of regularization in which two style latent variables are mixed during training."}, {"title": "Vector Quantised Generative Adversarial Network (VQ-GAN)", "content": "VQGAN (Vector Quantized Generative Adversarial Network) combines the power of Vector Quantized with GAN to generate higher quality image [27]. The vector quantizer takes the output from the image generator and divide some small fixed number of discrete codebook vectors. Then again reconstruct the image from this vector. This reduces noise in the image and also able to add constraints on certain objects in a generation."}, {"title": "2.3 Transformer", "content": "Taking inspiration from RNN and CNN, Transformers [28] understand the context of data and generate new data. This consists of an encoder and decoder network. It takes sequential data as input for the beginning and converts it to a numerical representation called embedding. Embedding contains the semantic meaning of the content. Along with this embedding, some additional information as a vector feeds into the encoder model. This vector contains positional information in a specific pattern. Multiple self-attention is used as multi-head attention to capture different interrelations among the tokens. Softmax activation is used to calculate the weight of the attention. To stabilize and speed up training layer normalization and residual connections are used with multi-Head attention. To capture more complex patterns in the data a feedforward neural network is used. Multiple blocks of attention are stuck one after another to capture hierarchical and abstract features in data. The decoder network is almost the same as the encoder layer for the Transformer. Instead of input embedding it uses the output embedding. Additionally, it has a linear classifier and softmax layer to generate probabilistic output.\nAutoregressive Transformers\nAutoregressive models generate the next based on the previous measurement from the sequence \u0130t is closely related to knowledge-based statistical time-series analysis that predicts the next thing based on probability. The autoregressive technique can be applied with Transformers for sharpening, up-scaling, and reconstructing images while maintaining the quality. [29] is an example for Autoregressive Transformers to generate indoor scene."}, {"title": "2.4 Diffusion", "content": "Diffusion [30] is a layer-by-layer approach trained in both forward and backward processes to generate new images. For forward diffusion models, it takes the input image and adds controlled Gaussian noise at each step. Using the reparameterization trick, all steps can be done at the same time. At the end of the network, it becomes only noise with very little information about the original image. In the backward process, at each step, the diffusion model tries to predict the noise of that step and subtract that noise from the input of that step. That makes it less noisy than it was as input of that block. The basic architecture of the diffusion model is shown in figure 5.\n$x_t \\sim q(x_t|x_o) = N(x_t,\\sqrt{\\bar{a}}_t x_o,(1 \u2013 \\bar{a}_t)I$\t\t(3)\nNoise are added through equation 3 at each step. Here, I is the identity matrix, N represents the normal distribution, xo is the initial image, and xt image includes noise after t-steps. \u0101t represents summation of all 1 \u2013 \u1e9e where \u1e9e is the standard deviation of noise for that step.\nDenoising Diffusion Probabilistic Model (DDPM)\nThe diffusion model consists of a forward noising and backward denoising process. The author of this model was the first to implement Diffusion for high-quality image generation. They demonstrated that the greatest quality results are produced when a certain parameterization of diffusion models demonstrates an equivalency with annealed Langevin dynamics during sampling and with denoising score matching over several noise levels during training. [31] is one example of utilizing this diffusion for image generation.\nConditional Diffusion\nFor conditional diffusion with the basic denoising training [32], it is possible to add an additional input variable as a condition. This variable will guide the model to generate specific things. This guide can be scaler input, class label or text sequence. CLIP embeddings [33] is one of the most popular guidings for the diffusion model.\nText-guided Diffusion\nText-guided diffusion is another conditional diffusion that takes text as the condition [34]. Then the model converts the text to embedding and uses it as a guide. GLIDE [35], DALL-E [36] are some examples of text-guided diffusion models which are vastly popular.\nStable Diffusion\nStable Diffusion is capable of taking an image or text as an input prompt and converts to an image. It is a user-friendly model, that can be used with simple consumer-grade GPU. It is also capable of generating video or animation. Stable diffusion uses UNet architecture to predict noise.\nLatent Diffusion\nIn general, the diffusion model works in pixel space, as a result it takes a huge space and time to convert image to image. To solve this, Latent Diffusion is introduced [37]. It has two key points i) Perceptual image compression: it converges images in lower dimensions without losing much information. ii) Latent Diffusion: This small dimensional data is used for further training. That makes the table stable and faster.\nLayout Diffusion\nLayoutDiffusion depicts layout generation as a discrete denoising diffusion process since the layout is usually represented as a series of discrete tokens [38]. It gains the ability to reverse a mild forward process, where in neighboring step layouts do not significantly differ from one another and layouts grow more chaotic as forward steps increase. Finally, a piecewise linear noise schedule combined with a block-wise transition matrix."}, {"title": "3 Dataset", "content": "Most of the papers frequently leverage the COCO-Stuff [6", "7": "as they provide rich annotations that facilitate a deeper understanding of relationships between objects within scene graphs or bounding boxes. Additionally", "6": "which comprises approximately 118", "39": "."}, {"7": "several researchers have opted to utilize the VG-MSDN dataset [40", "41": "is a framework designed for human-object interaction modeling", "purposes.\nSUNCG\nSUNCG[42": "is a comprehensive dataset featuring synthetic 3D scenes", "43": "is a manually collected large-scale densely annotated dataset of floor plans from real residential buildings which consists of more than 80k floorplan images of real residential buildings.\nSynscapes\nSynscapes [44", "45": "dataset in a graph format", "46": "one can select objects with diverse characteristics such as size", "47": "is a large-scale face image dataset that has 30", "splits": "a training set with around 3.3 million examples and a validation set with approximately 16", "48": "dataset provides detailed insights into the semantic composition of urban street environments. With a focus on dense semantic segmentation", "49": "is a novel dataset for road safety understanding in unstructured environments. It consists of 10", "50": "dataset is a comprehensive collection of over 27", "51": "dataset comprises meticulously curated synthetic object point clouds", "52": "is a large-scale repository that contains over 300 million models", "53": "is a large-scale", "54": "contains 3D vertex coordinates of 50", "55": "for single-image 3D in the wild consists of annotations of detailed 3D geometry for 140", "58": "is a popular dataset for mobile robotics and autonomous driving. It contains traffic scenarios recorded with RGB cameras", "classes": "road, vertical, and sky, and there are also annotated 252 acquisitions with 10 object categories and labeled 216 images with 11 classes. Classes include buildings, trees, sky, cars, signs, roads, pedestrians, fences, poles, sidewalks, and bicyclists.\nDIODE\nDiode [56", "57": "is a multi-"}]}