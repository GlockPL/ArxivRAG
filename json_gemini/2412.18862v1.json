{"title": "WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting", "authors": ["Chenghao Qian", "Yuhu Guo", "Wenjing Li", "Gustav Markkula"], "abstract": "Abstract\u20143D Gaussian Splatting (3DGS) has gained signif-icant attention for 3D scene reconstruction, but still suffers from complex outdoor environments, especially under adverse weather. This is because 3DGS treats the artifacts caused by adverse weather as part of the scene and will directly reconstruct them, largely reducing the clarity of the reconstructed scene. To address this challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing clear scenes from multi-view images under different weather conditions. Specifically, we explicitly categorize the multi-weather artifacts into the dense particles and lens occlusions that have very different characters, in which the former are caused by snowflakes and raindrops in the air, and the latter are raised by the precipitation on the camera lens. In light of this, we propose a dense-to-sparse preprocess strategy, which sequentially removes the dense particles by an Atmospheric Effect Filter (AEF) and then extracts the relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally, we train a set of 3D Gaussians by the processed images and generated masks for excluding occluded areas, and accurately recover the underlying clear scene by Gaussian splatting. We conduct a diverse and challenging benchmark to facilitate the evaluation of 3D reconstruction under complex weather scenarios. Extensive experiments on this benchmark demonstrate that our WeatherGS consistently produces high-quality, clean scenes across various weather scenarios, outperforming existing state-of-the-art methods. See project:https://jumponthemoon.github.io/weather-gs", "sections": [{"title": "I. INTRODUCTION", "content": "3D scene reconstruction has significant applications in diverse fields, including robotics, virtual reality (VR), and au-tonomous driving. However, the acquisition of accurate scene representations under adverse weather conditions presents significant challenges. Specifically, the presence of weather particles such as snowflakes and raindrops can drastically degrade captured image quality and hinder precise recon-struction. In more extreme cases, precipitation can adhere to imaging sensors that cause significant occlusion and distortion, making the reconstruction even more difficult.\nExisting methods primarily address challenges related to low illumination [1], [2] and blur effects [3], [4], neglecting the impact of weather-related issues. Recently, DerainNeRF [5] proposes a Neural Radiance Field (NeRF)-based frame-work [6] that can effectively remove water droplets from camera lenses. However, the continuous volumetric repre-sentation within NeRF makes it susceptible to inaccuracies when dealing with dynamic weather particles like falling snowflakes and raindrops. This is because the varying positions, shapes, and visibility of these particles across different views introduce inconsistencies during reconstruction train-ing, resulting in blurred or inaccurately rendered regions, as shown in Fig. 1b. Additionally, the high computational cost of NeRF limits the applicability of NeRF-based methods in real-time applications.\nMore recently, 3D Gaussian splatting (3DGS) [7] has emerged as a promising alternative to NeRF due to its high visual fidelity and computational efficiency. 3DGS represents each 3D point as a flexible Gaussian splat, making it highly adaptable to scene dynamics, such as falling weather parti-cles. The Gaussian distribution inherently filters and smooths small-scale noise, offering the potential to reduce weather-related artifacts. However, dense particles from raindrops and snowflakes, as well as significant occlusions caused by precipitation on the lens, are comparatively consistent across multiple views and can therefore be directly reconstructed by 3DGS, which substantially reduces the clarity of the scene, as shown in Fig. 1c. This issue has not been extensively explored, thus hindering the application of 3DGS under challenging weather conditions.\nTo address these limitations, we propose WeatherGS, a novel 3DGS-based framework capable of reconstructing clean 3D scenes from complex weather scenarios. Weath-erGS adopts a dense-to-sparse strategy to preprocess multi-view images by first removing weather particles with an Atmospheric Effect Filter (AEF), followed by extracting occlusions with a Lens Effect Detector (LED). This step generates a cleaner set of images, providing a robust founda-tion for 3D reconstruction training. Finally, we use the pre-processed images and generated masks to train a set of 3D"}, {"title": "III. METHOD", "content": "3DGS represents a scene by defining an explicit radiance field to model light distribution in a 3D space. It initializes the scene with a collection of 3D Gaussians, each param-eterized by its center position \u03bc, opacity \u03b1, 3D covariance matrix \u03a3, and RGB color (or spherical harmonics) c. These 3D Gaussians are then projected into the 2D image space for rendering. Given the viewing transformation W and 3D covariance matrix \u03a3, the projected 2D covariance matrix \u03a3\u2032 is computed using:\n$\\Sigma' = JWEW^\\mathsf{T}J^\\mathsf{T}$,\nwhere J is the Jacobian of the affine approximation of the projective transformation. Gaussian distances are calculated through the viewing transformation W and sorted by prox-imity. Alpha compositing is then used to determine the final pixel color:\n$C = \\sum_{n=1}^{N} C_n \\alpha_n \\prod_{j=1}^{n-1} (1 - \\alpha_j)$,\nwhere cn denotes the learned color, while N is the number of Gaussian kernels. The final opacity is determined by the product of learned opacity \u03b1n and the Gaussian, described as follows:\n$G(x') = \\alpha_n exp(-\\frac{1}{2}(x' - \\mu_n)^\\mathsf{T} \\Sigma'^{-1} (x' - \\mu_n))$,\nwhere x\u2032 represents the point in a 3D space and \u00b5\u03b7 de-notes the mean of 3D Gaussian parametrized position. The rendered pixels of 3D points are then compared with the original input image pixels to compute photometric loss to update 3D Gaussian parameters."}, {"title": "B. Multi-view Image Preprocessing", "content": "When capturing outdoor environments with cameras, im-ages inevitably contain dense artifacts from weather condi-tions like snowflakes and rain streaks. Worse, these may ad-here to the lens, causing significant occlusions. Such artifacts can mislead the 3D reconstruction model, as they are often interpreted as part of the scene and directly reconstructed, resulting in texture inconsistencies and distorted 3D models.\nTo address these challenges, we propose a novel frame-work that effectively generates high-quality multi-view im-ages by removing multi-weather artifacts, thereby helping the 3DGS model render clean scenes, as shown in Fig. 2. To be specific, we classify the weather artifacts into two categories: the weather particles caused by the raindrops and snowflakes, and the occlusions caused by the precipitation on the lens. We find that weather particles are generally dense but small in scale, while occlusions are relatively sparse but typically larger in size. Therefore, we believe it is more reasonable to use different mechanisms to handle them separately. Inspired by this, we propose a dense-to-sparse framework to process these two types of artifacts separately. First, the Atmospheric Effect Filter (AEF) is used to remove weather particles, followed by the application of the Lens Effect Detector (LED) to extract occlusion information. Note that the AEF module assists the LED module in practice, as the LED can more easily extract occlusions once the dense particles have been removed.\nTo remove weather particles, we use diffusion models [31], which are highly effective at reconstructing clean images from noisy inputs. However, directly applying diffusion models can lead to content distortion due to the inherent randomness in the diffusion process. Accordingly, we integrate weather-specific priors following Liu et al. [32] to guide the diffusion model, enabling it to selectively remove weather-induced artifacts while preserving the underlying scene details.\nIn detail, we extract semantic features from input weather images using a pre-trained CLIP [33] vision encoder as task guidance. These features are then utilized within ResNet [34] and cross-attention layers in the diffusion model, along with the original images encoded by VAE [35], to train the model for a specific image restoration task. After training, the learned layers can act as task plugin and be integrated into diffusion models to guide the diffusion model in remov-ing weather particles while preserving fine-grained spatial details. To facilitate the use of multiple plugins for different weather conditions, we implement a method that allows users to use text instructions and input images to automatically select the most appropriate plugins. Specifically, we encode the input weather-removal text instruction t and the input weather image i using the encoders Er and Ei into vectors, as introduced in [32]. We then compare the cosine similarity score between encoded features and apply a threshold \u03b8 to select the best-fitting task plugin D\u2217 as follows:\n$D^* = \\begin{cases}\nD_{derain}, & \\text{if } cos(E_T(t), E_V(i)) > \\theta \\\\\nD_{desnow}, & \\text{otherwise}\n\\end{cases}$,\nWith the selected task plugin\u2019s guidance, we can instruct the diffusion model to obtain a relatively clean weather image i\u2032 with dense weather particles removed as illustrated in Fig. 3.\nAlthough AEF can remove most of weather particles in the image, it doesn\u2019t resolve the issue of occlusion caused by precipitation on the camera lens. Thus, we incorporate LED which is composed of the detection module in AttGAN [24] to identify the occluded areas, similar to [5]. Specifically, we feed the images pro-cessed by AEF into the LED to generate a confidence map C\u2032(x, y) \u2208 [0, 1] that represents the likelihood of each pixel location (x, y) being occluded. For each image, a binary mask M is generated based on the following equation:\n$M(x, y) = I(C(x, y) \\geq t)$.\nHere, I is the threshold function with the threshold t. The function M(x, y) assigns a value of 1 to the pixel at location (x, y) if its confidence score exceeds the threshold t, indicating it is covered by occlusions. Otherwise, the pixel is set to 0."}, {"title": "C. 3D Scene Reconstruction with Occlusion Masks", "content": "With the restored images and generated lens effect masks, we reconstruct the scene using 3DGS. We initialize 3D Gaussians with a set of points from SfM [36]. The training and rendering follow steps introduced in Section III-A. To update the parameters of 3D Gaussians, we utilize stochastic gradient descent with a combination of L1 and D-SSIM loss functions. Additionally, masks generated by the lens effect detector are utilized to exclude occluded areas of the lens from loss computation during training. The L1 loss is defined as:\n$L_1 = \\sum_t ||(\\hat{I}(t) - I(t)) \\odot (1 - M)||^2$,\nwhere \u2299 denotes element-wise multiplication, \u00ce(t) represents the rendered pixel color at location t, I(t\u2032) denotes the ground truth color value, and M represents the lens effect mask. Similarly, the D-SSIM loss is defined as:\n$L_{D-SSIM} = 1 - SSIM(\\hat{I} \\odot (1 - M), I \\odot (1 - M))$,\nwhere SSIM is the structural similarity index applied to the masked images. The final training loss is given by:\n$L = (1 - \\lambda)L_1 + \\lambda L_{D-SSIM}$,\nwhere \u03bb \u2208 [0, 1] is a weighting factor that balances the contribution of the two loss functions. After the training process is complete, we are able to recover the clear 3D scene."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the effectiveness of the 3D reconstruction models under adverse weather conditions, we conduct a new, diverse, and challenging benchmark that includes both synthetic and real-world scenarios. For the synthetic dataset, we use basic scenes provided by Deblur-NeRF [37] and edit them in Blender [38] to generate multi-view weather images. To simulate realistic snowy conditions, we employ a snow material add-on and apply snow cover to various scene elements. Additionally, we design a particle system capable of emitting particles with customized physics that closely resemble real snowflakes and raindrops. To enhance the authenticity of the captured images and better emulate real-world camera effects, we apply motion blur during the scene rendering process. Furthermore, to achieve a denser weather effect, we utilize OpenCV to enhance the overall visual realism of the weather conditions. In total, we created three distinct scenes (i.e., Tanabata, Factory, Pool), each featuring snowy and rainy weather. For the real-world dataset, we generate two scenes by extracting keyframes from publicly available online videos [39], [40] captured in snowy and rainy weather, using them as multi-view image inputs. We develop the atmospheric effect filter using the pretrained task plugins from Liu et al. [32]. To remove atmospheric occlusions, we guide a pretrained Stable Diffusion model [31] using text prompts based on specific weather effects. We train the Lens Effect Detector using the dataset provided by Qian et al. [24] and further enhance the generated masks with averaging. Our 3D reconstruction model employs the 3D Gaussian Splatting framework [7], trained on multi-view images with corresponding masks for 30,000 iterations on a single RTX 3090 GPU."}, {"title": "B. Comparisons with the State-of-the-art Methods", "content": "Fig. 4 shows the qualitative comparisons between the proposed WeatherGS and the baselines. We can obtain several observations. First, WeatherGS is highly effective at removing snowflakes, rain streaks, and occlusions from camera lenses, resulting in visually appeal-ing 3D scenes with high-quality rendered images in both synthetic and real-world scenarios. For instance, in snowy scenes, WeatherGS effectively removes dense snowflakes that obstruct the view of the scene. In contrast, NeRF-based methods exhibit significant blurring due to over-smoothing of inconsistent views. Similarly, in rainy scenes, Weath-erGS successfully removes the raindrops adhering to the camera lens, whereas other methods only partially mitigate the weather effects. This effectiveness is attributed to the proposed dense-to-spare strategy, where the LED module easily extracts occlusions after the AEF module removes the dense weather particles. Second, although DerainNeRF can remove water droplets from the camera lens, it pro-duces inconsistent reconstructions, leading to fragmented and blurred areas, which is also observed in vanilla-NeRF. These inconsistencies stem from the varying appearances of snowflakes and rain streaks across different images, which disrupt NeRF model training. The quantitative results of snowy and rainy scenes are listed in Table. I and Table. II. The results show that our model achieves the highest PSNR, as well as the lowest LPIPS on average, indicating superior similarity to clean scenes compared to other approaches. However, in certain scenes, vanilla 3DGS and GS-W occasionally surpass our method in SSIM. This can be attributed to the use of preprocessed images in our model for training, whereas vanilla 3DGS and GS-W utilize the original input images. Although the latter can reconstruct weather particles, their sparse distribution in these cases does not significantly im-pact the SSIM metric. Despite this, WeatherGS outperforms in reconstructing scenes that are closer to clean, realistic appearances across various weather conditions"}, {"title": "C. Ablation Studies", "content": "To evaluate the effectiveness of our framework, we com-pare our model\u2019s performance with and without the prepro-cessing modules, as well as using different 3D reconstruction model backends. Comparisons are conducted across all syn-thetic scenes, including both snowy and rainy scenarios."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "In this work, we introduce WeatherGS, a novel approach for 3D scene reconstruction from multi-view images affected by adverse weather, built on 3D Gaussian Splatting (3DGS). By utilizing a specially designed dense-to-sparse mechanism, WeatherGS effectively addresses the challenges posed by weather-induced artifacts, such as dense particles and severe lens occlusions, significantly enhancing the generalization capability of 3DGS in outdoor environments. Furthermore, we establish a diverse and challenging benchmark to evaluate the performance of 3D reconstruction models under various weather conditions. Both qualitative and quantitative results on this benchmark demonstrate that WeatherGS consistently outperforms state-of-the-art reconstruction methods, render-ing clear scenes regardless of weather conditions."}]}