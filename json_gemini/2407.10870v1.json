{"title": "GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM", "authors": ["Keshav Bimbraw", "Ye Wang", "Jing Liu", "Toshiaki Koike-Akino"], "abstract": "Large vision-language models (LVLMs), such as the Generative Pre-trained Transformer 4-omni (GPT-40), are emerging multi-modal foundation models which have great potential as powerful artificial-intelligence (AI) assistance tools for a myriad of applications, including healthcare, industrial, and academic sectors. Although such foundation models perform well in a wide range of general tasks, their capability without fine-tuning is often limited in specialized tasks. However, full fine-tuning of large foundation models is challenging due to enormous computation/memory/dataset requirements. We show that GPT-40 can decode hand gestures from forearm ultrasound data even with no fine-tuning, and improves with few-shot, in-context learning.", "sections": [{"title": "I. INTRODUCTION", "content": "LARGE language models (LLMs) [1], such as generative pre-trained transformers (GPTs) [2], have recently emerged as powerful general assistance tools and exhibited tremendous capabilities in a wide range of applications. LLMs are often configured with billions of parameters to capture linguistic patterns and semantic relationships in natural language processing, enabling text generation, summarization, translation, reasoning, question-answering, etc.\nMore recently, large multi-modal models (LMMs) [3] with the capability to understand both natural language and other modalities, such as images and sounds, have offered new opportunities for biomedical applications. For example, it was demonstrated that large vision-language models (LVLMs) such as GPT-40 [4] and LLaVa [5] could be a viable tool for medical applications [6], including surgical oncology [7] and radiology diagnosis [8]\u2013[10]. We examine the capabilities of GPT-40 for sonography [11], to analyze and decode ultrasound images.\nMusculoskeletal ultrasound is a non-invasive and non-radiative imaging technique that uses ultrasound waves to visualize muscles, tendons, ligaments, and joints. For instance, ultrasound measurements can be used to visualize the anatomical aspects of the forearm, to estimate hand gestures [12], [13]. This is applicable to several domains, such as control of prosthetic hands [14], teleoperation of robotic grippers [15], and controlling virtual reality interfaces [16]. In particular, modern deep learning methods have shown improved performance to estimate different hand gestures [17]. It is highly expected that the use of LVLMs like GPT-40 to classify ultrasound images can provide a lot more information through human readable explanations of the model's predictions, which aids understanding of the reasoning behind gesture recognition. In addition, contextual information can be potentially leveraged to improve the classification performance.\nAlthough the pre-trained LVLMs work well for a general task, its performance is often limited for specialized tasks such as biomedical dataset. Given such a dataset, fine-tuning can greatly improve the performance for downstream tasks in general. Nevertheless, fine-tuning LVLMs is challenging due to the substantial amount of labelled data required [18]."}, {"title": "II. MOTIVATION", "content": "LVLMs have the capability to handle tasks that involve both images and texts. They have proven to be useful for understanding medical image data, especially with extensive fine-tuning [8]. Since full fine-tuning of LVLMs requires substantial computational resources, we first examined to see how GPT-40 would perform without fine-tuning. GPT-4o was provided a forearm ultrasound image, and asked a simple question \"What can you tell me about this image?\". The LVLM was able to identify that it is an ultrasound image, and gave some additional information about generic ultrasound images and their visual properties. We then examined whether it could infer some additional information when it is given some context. To this end, a follow-up question was asked: \u201cThis is forearm ultrasound data. Can you tell me what the hand might be doing while this data was acquired?\u201d. The LVLM gave some more information about physiology of hand movement and how different hand movements would lead to different ultrasound images. The full conversation can be seen in Fig. 1.\nThis motivated us to experiment with GPT-40 to see if it could classify forearm ultrasound images corresponding to different hand movements. We are also interested in evaluating its performance while varying the amount of data and context that it is exposed to."}, {"title": "III. METHODOLOGY", "content": "For this study, ultrasound data was acquired from 3 subjects. The study was approved by the institutional research ethics committee (IRB reference number 23001). Written informed consent was given by the subjects before data acquisition. Per subject, data was acquired for 5 hand gestures as shown in Figs. 2: (1) index flexion; (2) all pinch; (3) hand horns; (4) fist; and (5) open hand. These are based on activities of daily living and the chosen gestures are a subset of the dataset in [12]."}, {"title": "A. Data Acquisition", "content": "The ultrasound data was acquired using a Sonostar 4L linear palm Doppler ultrasound probe [21]. A custom-designed 3D-printed wearable was strapped onto the subject's forearm. The data from the probe was streamed to a Windows system over Wi-Fi, and screenshots of the ultrasound images were captured using a custom Python script. The 4L linear probe has 80 channels of ultrasound data, and the post-processed beamformed B-mode data is obtained, from which 350 \u00d7 350-pixel images are acquired.\nFor each subject, 5 sessions of data were collected. In each session, subjects performed a sequence of 5 gestures. Within each session, this sequence was repeated 4 times, resulting in 20 sub-sessions. For our study, we analyzed 10 frames per sub-session, resulting in a total of 1000 images (i.e., 20 sub-sessions, 10 frames/sub-session, 5 gestures) per subject."}, {"title": "B. Large Vision-Language Model (LVLM)", "content": "We use GPT-40 [4] as one of state-of-the-art LVLMs. GPT-40 is a multi-modal generative pre-trained transformer designed by OpenAI. It is said that GPT-40 uses more than 175 billion parameters. GPT-4o integrates texts and images in a single model, enabling it to handle multiple data types simultaneously. This multi-modal approach enhances accuracy and responsiveness in human-computer interactions. For inference, Azure OpenAI module within OpenAI's Python library was used [22]. Azure cloud computing was used within a Linux system with Python 3.11 for scripting.\nThe image data needs to be converted to a text format so that GPT-40 can understand it. For this study, the ultrasound image data was encoded to base64 using the Python base64 library, resulting in a text-based representation suitable for transmission or embedding [23]. The ultrasound image is represented as a long string of text upon encoding."}, {"title": "C. GPT-40 Prompts", "content": "The conversation flow we use is described in Fig. 3. To effectively utilize GPT-40, we designed the conversation as follows.\n1) System Message: We began with a system message to set context and guidelines for the conversation. GPT-40 was informed that it would serve as a helpful research assistant and will assist in classifying hand gestures using forearm ultrasound data.\n2) In-Context Learning (ICL): We used an ICL strategy which provides training examples in contexts. We use a few forearm ultrasound image samples along with the class labels for the in-context examples to assist GPT-40 for specialized classification tasks. Note that ICL does not involve any 'learning' procedure such as fine-tuning, adaptation, or post-training.\n3) Query for Classification: The GPT-40 was then asked to predict the hand gesture class based on the given ultrasound image. It was explicitly instructed to provide just the class number, which can be saved for further analysis."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "The performance was evaluated with few-shot in-context strategies: 0-shot; 1-shot; 2-shot; and 3-shot ICL. Two experiments were carried out: within-session analysis and cross-session analysis. For the former, for a given subject, of the 40 images per class in session 1, the last sub-session (last 10 images) were used for evaluation, and the remaining were used for training. For the latter, the last sub-session of session 5 was used for evaluation, while the remaining data was used as ICL training samples. For the three different experiments, different data was used for training and evaluation. For 0-shot strategy, the LVLM was shown images in the test set directly and asked what class out of the 5 it belonged to."}, {"title": "A. Within-Session Analysis", "content": "For the 1, 2, and 3-shot strategies, the data-split is described below.\n1) 1-Shot: The first image per class from sub-session 1 was shown to the model along with the class label before asking the question. This leads to a total of 5 images and their corresponding class-labels shown. This can be seen in Fig. 3.\n2) 2-Shot: The first two images per class from sub-session 1 were shown to the model along with the class labels, leading to a total of 10 images shown.\n3) 3-Shot: The first image per class from sub-sessions 1, 2, and 3 were shown to the model along with the class label, leading to a total of 15 images shown."}, {"title": "B. Cross-Session Analysis", "content": "For the 1, 2, and 3-shot strategies, the data-split is described below.\n1) 1-Shot: The first image per class from sub-session 1 was shown to the model along with the class label before asking the question. This leads to a total of 5 images and their corresponding class-labels shown. The training data shown in similar to the within-session experiment.\n2) 2-Shot: The first image per class (sub-session 1) from sessions 1 and 2 were shown to the model along with the class labels, leading to a total of 10 images shown.\n3) 3-Shot: The first image from sub-session 1 per class from sessions 1, 2, and 3 were shown to the model along with the class label, leading to a total of 15 images shown."}, {"title": "C. Evaluation Metrics", "content": "To evaluate the performance, the predicted class labels from GPT-40 were compared to the true values. Classification accuracy was used as a metric for evaluating the performance. Confusion matrices were used to visualize the performance for different scenarios. Precision, recall and F1 scores were also calculated for each confusion matrix."}, {"title": "V. RESULTS", "content": "This section provides the results for within-session and cross-session experiments for O-shot, 1-shot, 2-shot, and 3-shot ICL strategies."}, {"title": "A. Within-Session Experiment", "content": "The confusion matrix, summed over the three subjects for the within-session experiment can be seen in Fig. 4(a)\u2013(d) for 0, 1, 2, and 3-shot strategies respectively.\nThe classification accuracy, along with the precision, recall, and F1 scores are summarized in table I.\nTable III shows the classification accuracy averaged over three subjects for within-session experiment. For 0-shot strategy, the average classification accuracy was 19.3% (\u00b11.0%). For 1-shot, 2-shot and 3-shot strategies, we achieved 60.0% (\u00b115.9%), 74.0% (\u00b112.0%), and 72.0% (\u00b116.0%) respectively. It clearly demonstrates that in-context examples can significantly improve the classification accuracy even without fine-tuning the pre-trained LVLM. A slight decline of 2 percentage points is observed when the training examples increase from 2 to 3 per class. It may be within a statistical fluctuation due to the small number of test samples."}, {"title": "B. Cross-Session Experiment", "content": "The confusion matrix, summed over the three subjects for the cross-session experiment can be seen in Figs. 4(e)-(h) for 0, 1, 2, and 3-shot strategies respectively. The classification accuracy, along with the precision, recall, and F1 scores are summarized in table II.\nFor 0-shot case in Fig. 4(a), the classification accuracy is comparable to a random guess because of 5 classes. For 1-shot strategy in Fig. 4(b), it was 52%. For 2-shot in Fig. 4(c), it was 56%, which increased to 70% for 3-shot case as in Fig. 4(d). This trend is encouraging since increasing the number of in-context samples can improve the performance of GPT-40 to classify forearm ultrasound images to predict the hand gestures they correspond.\nThis was repeated for subjects 2 and 3. Table III shows the classification results averaged over the three subjects. For 0-shot strategy, the average classification accuracy was 20.0% (\u00b10.0%). For 1-shot, 2-shot and 3-shot strategies, it was obtained to be 33.3% (\u00b116.7%), 51.3% (\u00b115.5%), and 61.3% (\u00b122.3%) respectively. These results show a clear improvement in the classifier performance for an increasing number of in-context samples. It was interesting to observe that the standard deviation increases sharply as the number of training examples increases from 2 to 3 per class.\nThe results for the case where the input samples were picked randomly from the training data is shown in Table III. While the performance with in-context learning was better than 0-shot case, it was worse than non-randomized case. Increasing the number of training samples did not clearly improve the average classification across subjects. For 0-shot strategy, the classification accuracy was 21.3% (\u00b13.0%). For 1-shot strategy, the average classification accuracy was 42.0% (\u00b110.6%). For 2-shot strategy, the average classification accuracy was 45.3% (\u00b15.0%). And for 3-shot strategy, the average classification accuracy was 43.3% (\u00b14.2%). The results are summarized in Fig. 6."}, {"title": "VI. DISCUSSION", "content": "Several additional experiments were carried out for within-session data from subject 1 to understand GPT-40's performance and reasoning. All these experiments were done for a 1-shot strategy. The baseline confusion matrix is shown in Fig. 5(a). For this case, the accuracy is 86%, with the macro average precision, recall, and F1 scores being 0.9, 0.86, and 0.85, respectively."}, {"title": "A. Results with different prompts", "content": "We wanted to see how GPT-40 would perform with prompts less and more descriptive than the prompts shown in Fig. 3.\n1) Less descriptive information: For this experiment, we did not provide the system message. And for training, we only stated the class label with the image. As the question, we just asked 'What class does the image belong to? Only give the class number.' With this minimal information, the confusion matrix obtained is shown in Fig. 5(b). For this case, the accuracy is 82%, with the macro average precision, recall, and F1 scores being 0.86, 0.82, and 0.82, respectively. It was interesting to see that there was only a decline of 4% in the classification accuracy from the baseline of Fig. 5(a), meaning that we can provide it a lot less information without compromising significantly on the accuracy.\n2) More descriptive information: For this experiment, we provided a lot more contextual information to GPT-40 both in the system message, as well as in the final question. We mentioned that it should focus on the arrangement of regions with different brightness. We also mentioned that the anatomical and physiological properties visualized in the ultrasound image are distinct for different hand gestures. The confusion matrix is shown in Fig. 5(c). For this case, the accuracy is 80%, with the macro average precision, recall, and F1 scores being 0.87, 0.8, and 0.8, respectively.\nIt was interesting to see that providing so much extra information did not really help improve the performance. Rather, it decreased the performance compared to the less descriptive information case by 2%."}, {"title": "B. Reasoning ability", "content": "With the flow shown in Fig. 3, we wanted to understand why GPT-4o made that particular estimation. Fig. 7 shows the user asking questions to GPT-40, and it answering why it made that particular estimation compared to the other classes. Based on this conversation, we can make the following conclusions.\n1) Logical Coherence: GPT-40 demonstrates a structured approach to reasoning, with each successive step logically following the previous one. This indicates an ability to maintain logical consistency.\n2) Contextual Understanding: The model incorporates context into its reasoning, ensuring that decisions are relevant to the given scenario. It takes into consideration the information provided during training, as well as in the system message.\n3) Decision-Making: GPT-40 was able to express why the image does not belong to the other classes. It provides a clear delineation between the the different classes, such as for class 5 (open hand), it stated that there is a different distribution of bright and dark areas with more spread out experience, and hence, the image does not belong to class 5.\nWhile the model's reasoning is not fully trustworthy and VLMs are prone to hallucinations [24], it is encouraging to see that VLMs like GPT-40 can be used to understand better why it made a particular prediction. More effective conversations with contextual clues may improve its performance."}, {"title": "C. Different input formats", "content": "Radiologists often look at stacked medical images to understand medical image data. This is done especially with time-varying data to visualize how the physiological features change with time [25]. We wanted to see how GPT-40 would perform for different stacks of ultrasound images. Fig. 8 shows a stacked image sample with 4 ultrasound image frames.\n1) Two images as input: Using two stacked ultrasound frames as input for 1-shot strategy, instead of one image per class, 1 image with two ultrasound frames corresponding to the class were shown. This can be visualized in the top row of Fig. 8. The classification results are shown in Fig. 9(a). For this case, the accuracy is 78%, with the macro average precision, recall, and F1 scores being 0.83, 0.78, and 0.77, respectively.\n2) Four images as input: Using 4 stacked ultrasound frames as input for 1-shot strategy, instead of one image per class, 1 image with 4 ultrasound frames corresponding to the class were shown. This can be visualized in Fig. 8. The classification results are shown in Fig. 9(b). For this case, the accuracy is 72%, with the macro average precision, recall, and F1 scores being 0.84, 0.72, and 0.68, respectively.\nAlthough more training samples are provided by stacking frames, the classification accuracy was degraded. It may be because the image format is different for the testing image and the relative image resolution is lower when stacked. We believe that the performance can be improved by better designing prompts."}, {"title": "D. Future work", "content": "We conducted experiments to understand capabilities of GPT-40 for hand gesture classification based on forearm ultrasound data. We explored some interesting features of using VLMs for this task. Future work would include extensive cross validation analysis, in addition to acquiring data from more subjects. More rigorous prompt engineering should be considered as well. We are also interested in exploring VLM's cross-subject generalizability for medical image datasets. In addition, the comparison to retrieval augmented generation (RAG) [26] and parameter efficient fine-tuning (PEFT) [27] methods should follow."}, {"title": "VII. CONCLUSIONS", "content": "In this work, we show that we can use a large vision-language model (LVLMs), GPT-40 as a powerful AI assistance tool for understanding and interpreting forearm ultrasound data. We show that by providing some examples of ultrasound images, we can improve its performance for hand gesture classification based on forearm ultrasound data. For within-session performance, we show that the average gesture classification accuracy reached 74.0% for 5 hand gestures with just 2 training samples, and for cross-session performance, it reached 61.3% for just 3 training samples per class. Our approach can be used in cases where full-fine tuning of these models is challenging because of enormous compute/memory/dataset requirements. This research opens up exciting avenues for research in utilizing large vision-language models for medical imaging."}]}