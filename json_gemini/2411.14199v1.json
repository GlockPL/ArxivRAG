{"title": "OPENSCHOLAR: SYNTHESIZING SCIENTIFIC\nLITERATURE WITH RETRIEVAL-AUGMENTED LMS", "authors": ["Akari Asai", "Jacqueline He", "Rulin Shao", "Weijia Shi", "Amanpreet Singh", "Joseph Chee Chang", "Kyle Lo", "Luca Soldaini", "Sergey Feldman", "Mike D'arcy", "David Wadden", "Matt Latzke", "Minyang Tian", "Pan Ji", "Shengyan Liu", "Hao Tong", "Bohao Wu", "Yanyu Xiong", "Luke Zettlemoyer", "Graham Neubig", "Dan Weld", "Doug Downey", "Wen-tau Yih", "Pang Wei Koh", "Hannaneh Hajishirzi"], "abstract": "Scientific progress depends on researchers' ability to synthesize the growing body\nof literature. Can large language models (LMs) assist scientists in this task? We\nintroduce OPENSCHOLAR, a specialized retrieval-augmented LM that answers\nscientific queries by identifying relevant passages from 45 million open-access\npapers and synthesizing citation-backed responses. To evaluate OPENSCHOLAR,\nwe develop SCHOLARQABENCH, the first large-scale multi-domain benchmark\nfor literature search, comprising 2,967 expert-written queries and 208 long-form\nanswers across computer science, physics, neuroscience, and biomedicine. On\nSCHOLARQABENCH, OPENSCHOLAR-8B outperforms GPT-40 by 5% and Pa-\nperQA2 by 7% in correctness, despite being a smaller, open model. While GPT40\nhallucinates citations 78\u201390% of the time, OPENSCHOLAR achieves citation ac-\ncuracy on par with human experts. OPENSCHOLAR's datastore, retriever, and\nself-feedback inference loop also improves off-the-shelf LMs: for instance, OPEN-\nSCHOLAR-GPT40 improves GPT-40's correctness by 12%. In human evaluations,\nexperts preferred OPENSCHOLAR-8B and OPENSCHOLAR-GPT40 responses over\nexpert-written ones 51% and 70% of the time, respectively, compared to GPT40's\n32%. We open-source all of our code, models, datastore, data and a public demo.", "sections": [{"title": "INTRODUCTION", "content": "Synthesizing knowledge from scientific literature is essential for uncovering new research directions,\nrefining methodologies, and supporting evidence-based decisions. However, the vast volume of\npapers published annually makes it increasingly difficult for researchers to stay informed. Effective\nsynthesis requires precise retrieval, accurate attribution, and real-time access to current literature.\nWhile large language models (LLMs) show promise in assisting researchers, they face significant\nchallenges, including hallucinations (Mallen et al., 2023; Mishra et al., 2024), reliance on outdated\npre-training data (Kasai et al., 2023), and a lack of transparent attribution. For instance, when tasked\nwith citing up-to-date literature, GPT-4 fabricated citations in 78-90% of cases across fields like\ncomputer science and biomedicine in our experiments."}, {"title": "OPENSCHOLAR: OPEN RETRIEVAL-AUGMENTED LM TO SYNTHESIZING\nSCIENTIFIC LITERATURE", "content": "OPENSCHOLAR (detailed in Figure 2) is a new retrieval-augmented LM designed to ensure reliable,\nhigh-quality responses to a range of information-seeking queries about scientific literature.\nTask formulation. Given a scientific query x, the task is to identify relevant papers, synthesize their\nfindings, and generate a response y that effectively addresses the query. This response should be\naccompanied by a set of citations, C = C1, C2, . . ., CK, wherein each citation ci corresponds to an"}, {"title": "OPENSCHOLAR RETRIEVAL PIPELINE", "content": "Figure 2 (top left) shows our retrieval pipeline, consisting of a datastore D, a bi-encoder retriever Obi,\nand a cross-encoder reranker @cross. We first select initial candidate paragraphs using D and Obi, as\nwell as external APIs, and then refine and identify the top N relevant paragraphs using cross.\nCollect scientific papers to construct datastore. While prior work often uses a small subset of\npapers, such as arXiv papers from 2023-2024 (Zheng et al., 2024), it is important to have a diverse\nset of papers to improve the quality and coverage of model generation (Shao et al., 2024). To this\nend, we use peS2o (Soldaini et al., 2024) as our retrieval source, which consists of open-access\nacademic papers from S2ORC (Lo et al., 2020). We built our datastore using peS2o v3, which\nincludes 45 million papers up until October 2024. Following prior work (Shao et al., 2024), we split\nthe main text of each paper into discrete, 250-word text blocks (as determined by white space) and\nconcatenate the paper title to each block to formulate passages in D. Our datastore consists of 234\nmillion passages. To our knowledge, this is the largest open-sourced datastore for scientific literature.\nRetrieve initial paragraphs. We retrieve passages from three sources: (1) the peS2o datastore\nusing our trained retriever, (2) publicly available abstract from papers returned via the Semantic\nScholar API (Kinney et al., 2023) based on search keywords, and (3) publicly available texts from\npapers retrieved through a web search engine using the original query x. For (1), we first generate\nembeddings of each passage in D using the passage bi-encoder bi, which processes text chunks (e.g.,\nqueries or passages) into dense vectors (Karpukhin et al., 2020) offline. Off-the-shelf retrieval models\noften struggle in out-of-domain scenarios (Thakur et al., 2021). To overcome this limitations, we\ndevelop bi by continually pre-training Contriever (Izacard et al., 2022) on the peS2o datastore in\nan unsupervised fashion to improve domain-specific retrieval performance (see Appendix C.1 for\ndetails). During inference, we encode the query using Obi and retrieve the top 100 passages through a\nnearest neighbor search (Karpukhin et al., 2020). For (2), we first generate keywords from the query\nx using a generator LM. These keywords are then used to retrieve the top 10 papers for each, as\nranked by citation count, via the Semantic Scholar Search API. This approach addresses a limitation\nof the Semantic Scholar API, which cannot effectively handle long, question-like search queries. For\n(3), we obtain the top 10 search results using the You.com retrieval API, restricting the search to\nacademic platforms such as ArXiv and PubMed. If the papers are open-access, we extract and add\ntheir full texts to the candidate pool; otherwise, we include only their abstracts.\nRerank and finalize top N paragraphs. After the initial stage, we have gathered over 100, or\neven a thousand of relevant passages per query. However, passages retrieved by the bi-encoder\nmay include unhelpful context due to deep interactions between a query and passages, as they are\nencoded separately (Asai et al., 2023). Feeding a large number of documents that might including\nirrelevant content to LLMs can cause efficiency and performance issues, even with state-of-the-art\nmodels (Liu et al., 2024; Xu et al., 2023a). To overcome these challenges, we use a cross-encoder\nreranker (Nogueira & Cho, 2019; Xiao et al., 2023), denoted as @cross. For each candidate paragraph,\nthe cross-encoder reranker jointly encodes and computes the relevance score between the input query\nand each of the passages. We then use the relevance score to rank the passages accordingly. To train\n@cross for scientific domains, we fine-tune a BGE-reranker (Xiao et al., 2023) using synthetic data\ngenerated by Llama 3 70B Instruct. Specifically, we randomly generate queries based on abstracts\nfrom peS2o and retrieve the top 10 passages. Llama 3 70B Instruct then assigns relevance scores\nfrom 1 to 5 for these passages, where we consider scores of 4 or 5 as positive, and scores of 1 or 2 as\nnegative. Passages with a score of 3 are discarded. More details of cross training are in Appendix C.2.\nDuring reranking and finalization of top N passages, we also implement additional meta-filtering,\nwhich includes: (1) limiting the number of passages per paper to three passages, and (2) incorporating\nnormalized citation counts into relevance scores predicted by the cross-encoder."}, {"title": "INFERENCE: ITERATIVE GENERATION WITH RETRIEVAL-AUGMENTED SELF-FEEDBACK", "content": "In standard retrieval-augmented generation (RAG; Lewis et al. 2020; Ram et al. 2023), a generator\nLM takes in the original input x and top N retrieved passages P and generates the output yo. Although"}, {"title": "TRAINING: HIGH-QUALITY SYNTHETIC DATA GENERATION WITH INFERENCE PIPELINE", "content": "Building powerful LMs that can effectively synthesize scientific literature is challenging due to the\nlack of training data for this problem. While there are some resources to train scientific LMs (Wadden\net al., 2024), most tasks do not require open-retrieval settings and are single-paper tasks. As a result,\nmost prior work in this area (Skarlinski et al., 2024) rely on proprietary LMs, which poses challenges\nfor reproducibility and inference costs.\nWe leverage our inference-time pipeline to synthetically generate high-quality training data through\nself-feedback, so that the resulting model can get better at generating higher-quality output without\ngoing through the self-feedback process (Figure 2 bottom).\nQuestion and response generations. Our data generation process involves three steps: first, selecting\nthe top-cited papers from D; second, generating information-seeking queries based on their abstracts;\nand third, using the OPENSCHOLAR inference-time pipeline to produce high-quality responses. We\ngenerate data using LLama 3.1 70B (Dubey et al., 2024). Specifically, we begin by sampling 1 million\npaper abstracts from the peS2o dataset and the retrieve papers' meta information such as publication\nyear or citations. We then randomly select 10,000 papers that published later than 2017, and then\nprompt an LM to generate literature review questions or information-seeking queries based on each\nabstract, that may require multiple papers to answer. Next, we employ our OPENSCHOLAR pipeline\nto produce the final output y\u0442, along with intermediate generations such as feedback F and initial\noutputs.\nData filtering. Despite its effectiveness and scalability, synthetic data may also contain issues such as\nhallucinations, repetitive writing, or limited instruction-following (Li et al., 2024c). To address this,\nwe introduce a two-step data filtering process: pairwise-filtering and rubric-filtering, leveraging the"}, {"title": "SCHOLARQABENCH: REALISTIC LITERATURE REVIEW EVALUATION\nBENCHMARK ANNOTATED BY PH.D. EXPERTS", "content": "Challenges and overview. Prior studies on building LLMs to synthesize scientific literature employ\neither small-scale, single-domain human evaluation (Agarwal et al., 2024; Zheng et al., 2024) or over-\nsimplified multiple-choice QA setups (Skarlinski et al., 2024). Building high-quality benchmarks for\nliterature review has two major challenges. First, creating such datasets is resource-intensive, as it\nrequires Ph.D.-level domain expertise and research experience, particularly when annotating realistic\nquestions and high-quality answers. Second, even when high-quality data is available, reliably\nevaluating long-form natural language responses presents a significant challenge, especially in expert\ndomains (Xu et al., 2023b; 2024). This contrasts with benchmarks for other scientific processes, such\nas automated experimental code generation, for which clearer evaluation criteria, such as Pass@1,\nare more readily available (Si et al., 2024).\nTo address these gaps, we introduce SCHOLARQABENCH, a benchmark that supports diverse formats\nof scientific literature synthesis tasks, including closed-form classification, multiple-choice, and long-\nform generation, as shown in Table 1. We adopt three existing single-paper datasets, and then construct\na suite of high-quality, expert annotated datasets for computer science, biomedicine, physics, and\nneuroscience (Section 3.1). We also build a reliable automatic evaluation pipeline (Section 3.2).\nTable 1 provides a list of tasks in SCHOLARQABENCH, and Figure 3 shows an example and an\noverview of the evaluation pipeline."}, {"title": "DATA CURATION", "content": "SCHOLARQABENCH is designed to evaluate model capabilities in automating scientific literature\nreview. The curation process is guided by three key factors: Diversity of tasks: SCHOLARQABENCH\nincludes tasks with a range of input-output formats; Diversity of disciplines: Unlike previous\nanalyses that often focus on a single discipline such as computer science, SCHOLARQABENCH\nspans four scientific disciplines; Inclusion of multi-paper tasks: Unlike prior work that focuses on\nunderstanding single, pre-selected papers, all tasks require retrieving from the entire open-access\ncollection of full text of papers (Section 3.1.1) and four datasets specifically require reasoning over\nmultiple retrieved papers (Section 3.1.2)."}, {"title": "SINGLE-PAPER TASKS", "content": "For single-paper tasks, we curate and adapt existing widely-used single-paper datasets. Figure15\nshows examples of single-paper tasks; more details are in Appendix B.2."}, {"title": "MULTI-PAPER TASKS", "content": "Single-paper, closed-set tasks may provide reliable evaluations. However, they may not be reflective\nof realistic scenarios, in which complex, open-ended questions are asked independently from existing\npapers, and require multi-paper retrieval and reasoning. Few datasets (Xu et al., 2024; Malaviya et al.,\n2023) explore multi-paper setups with realistic queries, and most lack a reliable evaluation pipeline\nor human-written references. We address this gap by curating three new long-form QA datasets,\nannotated by experts, for these challenging settings (details in Appendix B.2). Furthermore, our\nmulti-paper tasks include four scientific disciplines."}, {"title": "METRICS AND EVALUATION PROTOCOLS", "content": "We developed a multifaceted automatic evaluation pipeline to facilitate reproducible and efficient\nevaluations, complementing expert assessments. An overview of our evaluations is in Figure 3."}, {"title": "EXPERIMENTS AND RESULTS", "content": "Models. First, we evaluate both open-weight and proprietary LMs, including Llama 3.1 (8B, 70B)\nand GPT-40 (gpt-40-2024-05-13). In this setup, each LM generates an answer independently,\nwithout external retrieval, and provides a list of referenced paper titles. For evaluation, we verify\nwhether the generated paper titles exist. If they do, we retrieve their corresponding abstracts to\nuse as citations. For multi-paper tasks, we further evaluate other proprietary systems: Perplexity\nPro, and PaperQA2 (Skarlinski et al., 2024), a concurrent literature review agent system that uses\nGPT4o for reranking, summarization, and answer generation. Then, we evaluate models using our\nOPENSCHOLAR-DATASTORE (+OSDS), where we retrieve the top N passages, and concatenate\nand feed them together with the original input. Lastly, we evaluate our proposed OPENSCHOLAR,\nleveraging our custom inference-time pipeline using trained 8B model models (OS-8B), as well as\nLlama 3.1 70B and GPT40 (OS-70B, OS-GPT40)."}, {"title": "ANALYSIS", "content": "Ablation studies. We conduct ablations to assess the effectiveness of individual components of\nOPENSCHOLAR (inference and training). Specifically, we remove each of the inference-time pro-\ncedures, reranking, feedback and attribution, and for OS-8B, we ablate the training, where we use\nLlama3-8B without any further training."}, {"title": "EXPERT EVALUATION", "content": "To complement our automatic evaluations and better understand the effectiveness and limitations of\nOPENSCHOLAR, we conducted human evaluations. This study involved over 100 literature review\nquestions and more than 15 participants, including Ph.D. students, research scientists, and university\nprofessors with expertise in the relevant fields. In total, we curated more than 400 fine-grained expert\nevaluations on human and model answers."}, {"title": "HUMAN EVALUATION DESIGN", "content": "Evaluations against human experts. For human evaluations, we use 108 question-answer pairs\nfrom SCHOLARQA-MULTI, written by experts. We run three models on these questions to generate\nanswers with citations: GPT40 (without external retrieval), OPENSCHOLAR with GPT4o as the\ngenerator (OS-GPT40), and OPENSCHOLAR with our trained 8B model (OS-8B). Expert annotators\nare then asked to evaluate the model-generated answers against human-written answers.\nEach evaluation involves presenting a question, a model-generated answer, and a human-written\nanswer. Expert annotators then conduct fine-grained assessments of each answer and provide pairwise\npreference judgments between the two. For fine-grained evaluations, we use the five-scale evaluation\ncriteria described in Section 3 (Cov, Org, Rel), with annotators scoring both model and human\nanswers using the same rubrics. For usefulness (Use), annotators assign scores on a scale from 1-5,\nwhich we convert into three classes: Not Useful (1-2), Neutral (3), and Useful (4-5). We then calculate\nthe percentage of answers that fall into the Useful category. For pairwise preference, annotators\neither choose one of the answers or mark a \"tie\" if they judge both answers to be of equal quality.\nOptionally, experts provide explanations on why one answer is better than the other."}, {"title": "HUMAN EVALUATION RESULTS", "content": "Results of human evaluations. Table 4 presents the average scores for each evaluation aspect,\nalongside the relative win rates against human responses. Figure 5 illustrates the score distributions\nfor Human, GPT40, and OPENSCHOLAR with Llama 3 8B and GPT40. Notably, both OS-GPT40\nand our OS-8B versions outperform human answers in over 50% of cases, with their advantage\nprimarily attributed to their ability to provide a greater breadth and depth of information (coverage;\nCov). In contrast, GPT4o, which lacks retrieval capabilities, demonstrates significantly limited\ncoverage and wins in fewer than 35% of cases, with its overall usefulness rated much lower than\nresponses from humans and the other two models. These results highlight that even for state-of-the-art models, synthesizing and answering scientific literature review questions remains a challenging\ntask, consistent with our findings on SCHOLARQABENCH. Overall, OPENSCHOLAR-GPT40 and\nOPENSCHOLAR-8B are rated as Useful in 80% and 72% of the queries, respectively.\nWhile OPENSCHOLAR with a smaller open 8B LM already suppresses human experts, the 8B\nmodel's output is judged to be less organized or fluent than the current state-of-the-art private LM-"}, {"title": "RELATED WORK", "content": "Scientific LMs. Scientific LMs have spanned various domains, including biomedical (Phan et al.,\n2021; Yuan et al., 2022; Luo et al., 2022), medical (Singhal et al., 2023a; Gu et al., 2024; Tan\net al., 2023; Singhal et al., 2023b), biomedical (Zhang et al., 2024b; Fang et al., 2024; Li et al.,\n2024a), geoscience (Feng et al., 2023), astronomy (Nguyen et al., 2023) and multidisciplinary science\n(Shaikh et al., 2023), with some models such as SciGLM (Zhang et al., 2024a) and UniSmart (Chi\net al., 2024) that aim to cover diverse scientific domains in a single model. Recently, several works\nshow that powerful general-purpose LLMs can also show strong capabilities in scientific tasks, such\nas medical question answering (AI4Science & Quantum, 2023; Singhal et al., 2023a), chemistry\nexperimentation (Zheng et al., 2023b) and applied mechanics (Brodnik et al., 2023). However,\nthe language model's reliance on information memorized within its parameters leads to frequent\nhallucinations in its output (Li et al., 2024b).\nLMs to assist scientists. Recent studies have also examined LLMs' capabilities to assist scientists in\nperforming a range of scientific procedures, including generating novel research ideas (Baek et al.,\n2024; Yang et al., 2023) and automating experimental code generation (Huang et al., 2023; Tian\net al., 2024). Our work, however, focuses specifically on benchmarking and developing methods\nfor automating literature reviews and addressing questions related to up-to-date research-tasks that\nare crucial to, and particularly challenging, for scientific inquiry. Several concurrent studies have\nattempted to build retrieval-augmented pipelines using proprietary LLMs and external APIs (e.g.,\nSemantic Scholar API) for scientific literature review agents (Agarwal et al., 2024; Skarlinski et al.,\n2024; Wang et al., 2024). While these studies and our research all explore the potential of retrieval-\naugmented LMs in automating literature synthesis, prior works often relied on proprietary, black-box\nsystems and limited evaluations, which commonly entail small-scale human evaluation or simplified"}, {"title": "CONCLUSION", "content": "In order to further research on LM-based systems that can assist scientific progress, we introduce\nOPENSCHOLAR and SCHOLARQABENCH, which can help navigate the complex, ever-growing\ntask of scientific literature review. OPENSCHOLAR, a retrieval-augmented system, leverages open-\ncheckpoint LLMs and trained retrieval models to iteratively refine scientific output, addressing\nchallenges such as hallucinations and citation accuracy. SCHOLARQABENCH, a novel large-scale\nbenchmark, provides a standardized way to evaluate literature review automation across multi-\nple scientific domains. In evaluations using SCHOLARQABENCH, OPENSCHOLAR demonstrates\nsubstantial improvements, outperforming existing systems, including GPT-40 and the concurrent\nproprietary system PaperQA2. Our expert evaluation across three scientific disciplines reveals that\nSCHOLARQABENCH, when paired with fully open-checkpoint models and open-access data stores,\ngenerates answers that are more helpful than those produced by expert annotators, who required an\nhour per annotation. This approach also significantly increases coverage. OPENSCHOLAR using our\ntrained 8B and GPT4o achieves a 51% and 70% win rate against human-generated answers. We open-\nsource the OPENSCHOLAR code, data, model checkpoints, datastores, and SCHOLARQABENCH,\nalong with a public demo, to support and accelerate future research efforts."}, {"title": "LIMITATIONS", "content": "We highlight several limitations of our work in this section. It is important to note that we do not claim\nthat LM-based systems can fully automate scientific literature synthesis. To further advance research\nin this area, we are releasing both SCHOLARQABENCH and OPENSCHOLAR to the community.\nLimitations of SCHOLARQABENCH. There are several limitations to SCHOLARQABENCH. First,\ndue to the cost and time required to engage expert annotators individuals with either a Ph.D. or\nare currently pursuing one in relevant fields-the evaluation dataset with human-written answers is\nrelatively small (e.g., 110 for CS-LFQA and 108 for expert-written answers). This limited dataset\nmay introduce statistical variance and potential biases stemming from the specific expertise of the\nannotators. To support future research in expanding the size and scope of SCHOLARQABENCH, we\nopen-source our data and annotation pipelines.\nSecond, our automatic evaluation pipelines may not always perfectly capture the quality of generated\ncontent. For example, in SCHOLAR-CS, we combine various components (e.g., length, excerpts,\nrubric items) using heuristically determined weight terms. Further, we discovered that often annotators\nasked for specific kinds of ancillary information in their rubrics-background, elaborations and\nchallenges even though these aspects might not be strictly required to answer the question. In our"}]}