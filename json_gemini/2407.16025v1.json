{"title": "Exploring and Addressing Reward Confusion in Offline Preference Learning", "authors": ["Xin Chen", "Sam Toyer", "Florian Shkurti"], "abstract": "Spurious correlations in a reward model's training data can prevent Reinforcement Learning from Human Feedback (RLHF) from identifying the desired goal and induce unwanted behaviors. This paper shows that offline RLHF is susceptible to reward confusion, especially in the presence of spurious correlations in offline data. We create a benchmark to study this problem and propose a method which can significantly reduce reward confusion by leveraging transitivity of preferences while building a global preference chain with active learning.", "sections": [{"title": "1. Introduction", "content": "For many real-world tasks, designing adequate reward functions is challenging. This has led to the rise of Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017).\nIn this work, we study a failure mode of offline RLHF that we refer to as reward confusion. This occurs when the reward R in a Markov Decision Process (MDP) is a function of features 21,..., Zn inferred from the observation-action pair (o, a). In a simplified scenario, R relies on 21 but not 22, yet 21 and 22 are highly correlated in the training data. An empirical risk minimizer might mistakenly conclude that 22 affects R. As we'll see, this incorrect dependence can lead to failures when training a policy against the learned reward function. We graphically illustrate this problem in Figure 1.\nTo better understand this phenomenon, we designed a benchmark environment called Confusing Minigrid (CMG) that tests reward confusion in models. We carefully designed six tasks with three types of spurious information for minigrid: (1) Position; (2) Color; and (3) Extra observations. An insightful toy example is given in Appendix A.\nOur main contribution is an algorithm named InformationGuided Preference Chain (IMPEC) designed to address this problem. Its design is a combination of two techniques."}, {"title": "2. Related Work", "content": "Causal Confusion The problem of causal confusion, which refers to models learning to depend on spurious correlations in the training data, has been studied in behavioral cloning (De Haan et al., 2019), reinforcement learning (Li et al., 2020), and reward learning (Tien et al., 2022).\nPast work shows empirically and theoretically that spurious correlations and confounders in the training set can worsen an agent's deployment performance (Zhang et al., 2020; Kumor et al., 2021). Reward confusion is essentially causal confusion that occurs during reward learning.\nGoal Misgeneralization While past work on causal confusion studies it as a cause of complete failure to learn goal-directed behavior, it can also make agents optimize for incorrect goals, i.e. goal misgeneralization. For example, in Procgen's CoinRun, the coin to be picked up is always on the right. RL agents can confuse \"running to the right\" with"}, {"title": "3. Method", "content": "Models We consider an agent interacting with an environment following Markov Decision Process (MDP) defined by (S, A, P, R). S is the state space, A is the action space, p : S\u00d7S \u00d7 A \u2192 [0, \u221e) represents the transition probability density. A rollout \u00a7 = (st, at) is a sequence of states and actions. Given a dataset of unranked rollouts \u039e, our algorithm actively collects ranking information to sort rollouts into an ordered list T = (\u00a71,2,..., \u00a7n). The ranking of rollout \u03be\u2208 T is denoted by \u03c8\u03b5. T. On each transition, the environment emits a reward R : S \u00d7 A \u2192 R. Our goal is to learn a correct R* that induces correct policies.\nPreferences We model the human's probability of strictly preferring \u00a71 in a pair (\u00a71, \u00a72) through the Shepard-Luce choice rule (Shepard, 1957; Luce, 1959):\n\\(P[\u00a7_1 \\succ \u00a7_2] = \\frac{\\exp \\sum_t r(o^1_t, a^1_t)}{\\exp \\sum_t r(o^1_t, a^1_t) + \\exp \\sum_t r(o^2_t, a^2_t)}\\)\nWe extend this model to a ternary one by allowing the human to flag when two rollouts are equally good, \u00a71 = \u00a72. When training the reward network, we use cross-entropy loss to produce predictions for preferring (\u00a71, \u00a72)."}, {"title": "3.1. Information-Guided Preference Chain (IMPEC)", "content": "Key intuition: Increase Contrast Among Valuable Rollouts. In most preference comparison algorithms, a rollout \u00a71's relation is considered explicitly only with another one \u00a72. Suppose that in the ground truth, \u00a71 > \u00a72 > \u00a73 > \u00a74, and we already know \u00a71 > \u00a72, \u00a73 > \u00a74. To figure out \u00a71 and \u00a72's relationship with \u00a73 and \u00a74, the most efficient query is whether \u00a72 > \u00a73. Once we establish that, we can immediately obtain the preference relations on all four rollouts. Many active learning objectives do not explicitly consider building a longer chain of preferences, or how much extra information can be derived after obtaining a preference label. To do so, the algorithm needs to be aware of the relative position one rollout belongs to.\nCreating and Maintaining a Preference Chain We maintain an ordered chain for rollouts. Starting from an empty chain, for each new rollout we queried from the dataset, we imitate insertion sort by recursively finding the ranking of it using human's preference labels. Hence, by the time we observe all the rollouts, we have a sorted list of rollouts, ordered according to human preferences. Rollouts can have identical returns, so we treat each element of the chain as a bucket b \u2208 B of rollouts with the same return. If the human decides that a new rollout new is equally preferred to m in bucket bm, then new will be added to bm. On the other hand, if new > \u00a7m and new < \u00a7m-1 (\u00a7m-1 resides in a previous bucket bm-1), then the algorithm will insert a new bucket containing only new in between bm and bm-1. This ensures that bo contains the best rollouts seen so far and bn contains the least preferred rollouts (where n is the chain length). We illustrate this process in Figure 2.\nOur reward model is a Bayesian neural network (BNN) (Blundell et al., 2015) which maintains a Gaussian distribution over a network's weights and biases. As we will see, this allows us to incorporate epistemic uncertainty over reward functions into the selection procedure.\nIn the noiseless case, insertion sort needs O(log n) queries to find the position for new. However, we have access to a partially trained reward network, which we use to guess the rank for new, reducing the number of buckets we must search over. The network will first update its reward predictions on rollouts in each bucket Rb, then predict the incoming rollout's reward Renew \u00b7\nInstead of using a point estimate R\u025bnew, we use an interval [R&new - \u20ac, R&new + e] to fast-guess where \u00c9new may belong.\nIMPEC queries human preferences of two pairs (\u00a7new, \u03be\u03b9) and (\u03benew, \u03beu). \u03be\u03b9 and \u03beu are the rollouts that are closest"}, {"title": "4. Experiments", "content": "Baselines The most straightforward baseline is the standard RLHF algorithm (Christiano et al., 2017). We additionally implemented two RLHF with active learning methods, to be compared with IMPEC: pairwise information gain (B\u0131y\u0131k et al., 2019) and pairwise volume removal (Sadigh et al., 2017). The information gain (IG) method does active learning with an acquisition function derived from IG. This is similar to our method but reasons only about individual preference pairs and not about the result of the ranking process.\nThe volume removal method was designed in the linear reward setting to reduce the volume of weight vectors supported under the posterior after each preference update.\nOffline Dataset and Tasks In real-world settings where failures are much more costly than minor failures, rollout datasets will be skewed towards higher-return rollouts. We emulate this by constraining the number of rollouts in the low reward region (return \u2264 5) to be at most 10% of the dataset. Detailed information of each task and their added spurious correlations can be found in Appendix B.\nQuery and Data Budgets Preference comparison algorithms typically obtain n pairs of (query, label), [(\u00a7i1, \u00a7i2), labeli]=1 for binary classification. For simpler tasks (Empty, DynObs, Lava, and Fetch), we set the query budget to be 300. That is, baselines have access to 300 (query, label) pairs, [(\u00a7i1, \u00a7i2), label\u2081]309. IMPEC requires additional queries to precisely rank each sampled rollout within the candidate list, so we constrain it to use 150 pairs [(\u00a7i1, \u00a7i2), labeli] 159, and use the remaining 150 query budget to perform insertion sort for a selected subset of rollouts (decided by IG). For the harder tasks (Go to Door and Lava-Position), all algorithms are given a budget of 600 (query, label) pairs. IMPEC can access 400 data pairs, and use the remaining 200 query budget for sorting.\n1150\nPreference Learning and Reinforcement Learning We first perform offline reward learning, then apply online reinforcement learning using the learned reward function to obtain a policy. The RL agent receives rewards from the learned function instead of the environment, and is trained with Proximal Policy Optimization (Schulman et al., 2017).\nDetailed experiment and hyperparameter settings can be found in Appendices E and F."}, {"title": "5. Results", "content": "Performance on all six tasks can be found in Table 1. Due to limited computational resources, we repeat each run over five seeds.\nWe compute the p-values of the results being better than baseline performance, see Appendix G. Except for the task Go To Door where all algorithms perform poorly, IMPEC has a higher mean return than other algorithms, and often a lower standard deviation."}, {"title": "5.1. Ablation Studies", "content": "To understand what leads to IMPEC's performance, we experiment with removing three different components: (1) the active learning process; (2) preference derivations; and (3) the ranking process. The results can be found in Table 2."}, {"title": "6. Discussion", "content": "Experiment Results While the p-values of IMPEC are much lower than those of the other algorithms, a p-value < 0.05 is only achieved for the Lava Position task. To improve IMPEC so that it performs significantly better than the baseline, more research iterations still need to be performed.\nLimitations and Future Work A limitation of IMPEC is its potential sensitivity to noise in preferences. In our experiments, we keep a relatively low noise level, and we believe more sophisticated algorithms could improve robustness to noise, perhaps inspired by past work on noisy binary search (Karp & Kleinberg, 2007; Chiu, 2019).\nOur results suggest a deeper connection between the quality of preference datasets and the efficiency of preference learning algorithms. In Appendix D, first steps of a graph theoretic analysis for thinking about preference dataset quality are shown. We are interested in further exploring the influence of graph-theoretic qualities and their effects on preference learning, and using the insights in future algorithm design."}, {"title": "7. Conclusion", "content": "This work studied the reward confusion problem, generalizing it beyond the traditional understanding of causal confusion and goal misgeneralization. Our experiments on Confusing Minigrid benchmark show that reward confusion in offline preference learning can lead to undesired policy behaviors. The benchmark is easy to configure, and we expect it to be particularly useful for iterative research.\nIn addition, we proposed IMPEC to reduce the impact of reward confusion. IMPEC exploits preference transitivity"}, {"title": "A. Reward Confusion Example", "content": "Consider a task where an agent learns to park a car in a goal location. The agent can observe the position of the car and the state of a bottle of water in the car. As the car moves, the water moves in the bottle. When the car is properly parked, the water stops moving. In this case, the fact that the human stays in the goal position is a z\u2081 factor that highly correlates with the water state 22. In CMG, we simplify this to a navigation problem: An agent needs to move to and stay in the goal grid. Besides having access to the necessary information to complete this task, it can also see the state of a glass of water it holds while moving around. We empirically observe that a standard preference learning agent can misunderstand the goal as \"stabilizing the water\" by standing still at a random grid instead of \u201cstaying in the goal position\u201d, even when we have provided failure rollouts in its training dataset. Even when training and testing the agent in the same environment, reward confusion arises."}, {"title": "B.1. Spurious Correlations from Extra Observations", "content": "This set of tasks tests whether spurious correlations with redundant observation dimensions can interfere with learning. In these tasks, an agent needs to navigate to the goal cell. It can observe the state of a glass of water it is holding, which exhibits \"ripples\" as it moves, and calms down if the agent's position remains unchanged. On rollouts where the agent moves straight to the goal and stops, the level of ripples will be predictive of whether the agent has reached the goal, even though in general it is possible to cause the ripples to disappear by stopping in any location and not just at the goal. The training and testing variants of these tasks are the same.\nEmpty This is the simplest task in the benchmark. The agent needs to navigate to the goal cell, with all other cells being empty. The environment gives a positive reward when the agent reaches the goal, and zero otherwise."}, {"title": "B.2. Spurious Correlations from Distributional Shifts", "content": "We design these tasks with different training and testing variants. The training environments have a 90% probability where the goal configuration is spurious.\nLava-Position It is a variant of Lava with changing goal positions. In the training variant, the goal cell is usually located at one particular location. In the test variant, the goal grid can appear at other locations too.\nGo to Door In this task, an agent is asked to move to a position adjacent to the goal door embedded in one of the four walls surrounding the grid. There are always four doors in the environment, and the goal door is most likely to be placed in the upper wall during training. During testing, the goal door can be placed in any of the four walls.\nFetch The agent's goal in this task is to pick up a key. There is usually a distractor object in the environment that an agent can also pick up. In the training variant of this task, most keys are yellow, and most distractor objects are non-yellow. At the test time, the keys and distractor objects can appear in any color with equal chance."}, {"title": "C. The Information Gain Objective Derivation", "content": "This derivation is adapted from (B\u0131y\u0131k et al., 2019).\n\\(1(0;\u03c8\u03b5 | \u03a4,\u03be) = H(0|T,\u03be) \u2013 H(0|\u03c8,T,\u03be)\\)\n= \u2212\u0395\u03b8,\u03c4,\u03be [log P(0|T, \u03be)] + \u0395\u03b8,\u03c8\u03b5,\u03be,\u03c4 [log P(0|\u03c8,T,\u03be)]\n= \u2212E0,4,5,1 [log P(0|T, \u03be)] + E\u0473,\u03c8,\u03b5,\u03c4 [log P(0|\u03c8,T,\u00a3)]\n= \u0395\u03bf,\u03c8\u03b5,\u03b5,\u03c4 [log P(0|\u03c8, T, \u03be) \u2013 log P(0|T, \u03be)]\n= \u0395\u03b8,\u03c8\u03b5,\u03b5,\u03a4  log \\( \\frac{P(\u03c8,\u03a4, \u03be)}{P(\u03a4,\u03be)} \\frac{P(\u03c8\\T, 0,\u03be)P(T, \u03b8, \u03be)}{P(\u03c8,\u03a4,\u03be)} \\) \n= Eo,,, log \\( \\frac{P(\u03c8|T, 0,\u03be)P(T, \u03b8, \u03be)}{P(T,\u03be)} \\) \n=  \u0395\u03b8,\u03c8\u03b5,\u03b5,\u03c4 log \\( \\frac{P(\u03c8\\T, 0,\u03be)P(T, \u03b8, \u03be)}{P(T,\u03be)} \\) \nEvaluating this expression requires us to compute a conditional probability with respect to 0, which is a random variable capturing our current uncertainty over the reward network weights. We can approximate the distribution p(0) by sampling"}, {"title": "D. Graph-theoretical approach", "content": "The Preference Datasets We visualize the preference datasets gathered by the baseline and IMPEC on Lava-Position in Figure 5. The baseline dataset is randomly sampled at the start of the training, while we take a snapshot of the IMPEC dataset (which is constructed iteratively) at its last training epoch. The datasets are visualized as graphs, with each node being a unique rollout, and each edge representing a preference label. Both IMPEC and the baseline have a query budget of 600 pairwise queries, and IMPEC uses the first 400 of its queries to do pairwise comparisons between randomly selected rollouts, as opposed to actively selected rollouts. The IMPEC and baseline datasets have 405 and 538 unique rollouts as well as 791 and 594 unique edges, respectively. We include several other graph statistics in Table 4.\nIMPEC and the baseline exhibit three notable differences in the graph properties. The first is their clustering coefficient, which measures the degree to which nodes tend to cluster together. The IMPEC graph has a higher clustering coefficient because of the many preferences it derives from the ranked chain. This is relevant to the number of chains in the graph: since most nodes are connected to IMPEC's central cluster through some edges, it creates many more chains between two nodes across the graph. In both graphs, we also observe that there are many chains of length 1 that are not connected to any larger cluster. We suspect that the algorithms' sample efficiency can be further improved if these pairs can be meaningfully linked to the clusters.\nFinally, we measure the graph's efficiency, which is a metric form network science that is intended to measure the flow of information between \u201ccommunicating\u201d nodes (Latora & Marchiori, 2001). The assumption which underlies the graph efficiency metric is that more distant nodes are less efficient at information exchange. To avoid inflating the metric with the many length 1 that are not connected to the rest of the graph, we compute efficiency only on the two graphs' biggest connected components. We empirically observe that the IMPEC dataset graph has a higher average global efficiency than the"}, {"title": "E. Detailed Experiment Settings", "content": "Dataset Creation For each task, we first train an RL agent to the expert level, saving its policies at various timesteps. We then take 3 of its policy snapshots - an almost random policy, an expert policy, and one in-between to generate rollouts. The number of rollouts falling within the low-reward (\u2264 5) region are controlled to take up within 10% of the dataset.\nIMPEC Training We typically train an algorithm with 20 epochs. For IMPEC, we evenly divide its query budget from epoch 1 to epoch 15, using up all queries in this period and ranking as much uncertain rollouts as possible. We then use the remaining 5 epochs for learning the full dataset. After sampling the initial preference pairs, IMPEC will not obtain new rollouts from the dataset, and perform learning only by querying humans for ranking the given rollouts.\nEnvironment Observations The environment observations are in the vector form, which contains [agent position, agent direction, special grid info]. The special grid can be the goal/lava/door, etc., and its information is an encoding of its grid type, current position, and color.\nAblation Studies In the \"no active learning\" experiment, we turn off the active selection function, and randomly pick rollouts from the candidate list. For \u201cno derived prefs\", we remove the preference derivation part of the learning. Note that the preference pairs generated during the sorting process are still added to the dataset. Finally, \u201cno ranking\" means that the algorithm still selects preference pairs with an information gain acquisition function, but does not maintain a preference chain (so there are also no transitively derived preferences). This is simply the information gain algorithm of B\u0131y\u0131k et al. (2019)."}, {"title": "F. Training Hyperparameters", "content": "The preference learning hyperparameters:\nThe PPO training hyperparameters:"}, {"title": "G. The Complete P-Value Table", "content": null}, {"title": "H. Baseline Comparison and Data Scaling", "content": "We expect that the baseline should suffer less from reward confusion if given more comparison data. To test this hypothesis, Figure 6 shows return for the baseline with different query budgets, along with IMPEC results from our main experiments (which used 150 preference pairs and 300 queries). We tested the baseline with 6\u00d7 the data used by IMPEC (i.e., 900 preference pairs and queries), then gradually pushed up the amount to 1500 (10\u00d7 of IMPEC data). With 6\u00d7 the data, we still see some learning failures, and the seeds' standard deviation decreases to IMPEC's level only when we use 10\u00d7 data."}, {"title": "I. Learning Curves for All Tasks", "content": null}]}