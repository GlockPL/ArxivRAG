{"title": "NOMTO: Neural Operator-based symbolic Model approximaTion and discovery", "authors": ["Sergei Garmaev", "Siddhartha Mishra", "Olga Fink"], "abstract": "While many physical and engineering processes are most effectively described by non-linear symbolic models, existing non-linear symbolic regression (SR) methods are restricted to a limited set of continuous algebraic functions, thereby limiting their applicability to discover higher order non-linear differential relations. In this work, we introduce the Neural Operator-based symbolic Model approximaTion and discovery (NOMTO) method, a novel approach to symbolic model discovery that leverages Neural Operators to encompass a broad range of symbolic operations. We demonstrate that NOMTO can successfully identify symbolic expressions containing elementary functions with singularities, special functions, and derivatives. Additionally, our experiments demonstrate that NOMTO can accurately rediscover second-order non-linear partial differential equations. By broadening the set of symbolic operations available for discovery, \u039d\u039f\u039c\u03a4\u039f significantly advances the capabilities of existing SR methods. It provides a powerful and flexible tool for model discovery, capable of capturing complex relations in a variety of physical systems.", "sections": [{"title": "1 Main", "content": "Many physical and engineering processes are most effectively described by concise mathematical expressions derived through meticulous observation and analysis. The accuracy of these models is highly dependent on the quality and quantity of available data. With the emergence of large-scale datasets across diverse physical and engineering domains, deriving compact mathematical models in the form of symbolic expressions has become increasingly attainable. This methodology, known as symbolic regression (SR), aims to identify mathematical expressions that most accurately represent given datasets. SR has become indispensable in fields such as physics, biology, and engineering, where it advances knowledge and fosters innovation by uncovering underlying principles and facilitating the development of interpretable predictive models.\nIn recent years, deep learning-based approaches have significantly advanced the field of SR by leveraging neural networks to identify mathematical expressions directly from data. Techniques based on transformer architectures and reinforcement learning [1, 2, 3] have expanded SR's capabilities by incorporating advanced deep learning strategies. For example, reinforcement learning facilitates more adaptive and flexible model discovery processes [1, 4], while transformer-based approaches have demonstrated effectiveness in capturing complex dependencies within data [5]. These model-based approaches require extensive training on the data labeled with known governing symbolic expressions prior to inferring another symbolic expressions. Despite these advancements, such methods often struggle to generalize to out-of-distribution equations and require extensive training datasets [6].\nIn contrast to model-based approaches, search-based SR methods [7] do not require extensive training datasets and can effectively operate in out-of-distribution domains [6]. These methods directly manipulate symbolic operations directly and are not restricted to the set of functions used during training, distinguishing them from model-based approaches. Among search-based techniques, genetic programming, stands out as a prominent method. It evolves populations of candidate solutions through genetic operators such as crossover and mutation [8, 6], representing mathematical expressions as sequences of variables, constants, and operators. Genetic programming has been widely applied as a model discovery tool across a wide range of disciplines, including astronomy [9, 10], materials science [11, 12], and medical sciences [13, 14]. However, genetic program-ming is inherently heuristic and often lacks systematic exploration of the solution space. Unlike gradient-based optimization techniques, which offer a more direct and efficient path to optimal solutions, genetic programming relies on evolutionary strategies that are less targeted and more exploratory [6, 15]. These limitations become particularly pronounced in high-dimensional datasets and when dealing with complex mathematical operations, ultimately constraining the effectiveness and scalability of genetic programming-based methods [1]."}, {"title": "2 Results", "content": "We evaluated the performance of the proposed NOMTO algorithm on symbolic regression tasks using a rep-resentative selection of symbolic expressions from the Nguyen [26] and Keijzer [27] benchmark sets. Beyond elementary expressions, we extended our evaluation to expressions containing differential operators and special functions, demonstrating NOMTO's unique ability to handle tasks where other SR algorithms typically fail. Furthermore, we showcase the algorithm's performance in model discovery tasks, successfully recovering the governing equations of two systems: a two-dimensional diffusion system governed by the heat equation and a two-dimensional advection-diffusion system governed by the Burgers' equations."}, {"title": "2.1 Symbolic Regression Task", "content": "To assess NOMTO's effectiveness in symbolic regression, we compared its performance with state-of-the-art methods, EQL and KAN. This comparison aims to evaluate the impact of employing neural operators in NOMTO's computational graph, contrasting them with the exact operations used in EQL and the learnable activation functions in KAN.\nThe symbolic expressions chosen for this evaluation ensure a representative mix of various symbolic opera-tions. For a fair comparison, we prepared consistent datasets that satisfy the input data requirements for different algorithms. EQL and KAN use point-wise input data, while NOMTO operates on discretized functions."}, {"title": "2.2 Expressions with Derivatives and Special Functions", "content": "In this experiment, we evaluate NOMTO's ability to discover expressions involving derivatives and special functions, such as the Airy function Ai and the gamma function \u0393. Discovery of these operations has been challenging for existing non-linear symbolic regression methods. To showcase this capability, NOMTO was applied to identify expressions containing not only algebraic operations but also derivatives and special functions. The test set of expressions includes one-dimensional derivatives, Ai, and \u0393. For this experiment, the independent variable x1 = x1(t) was modeled as a randomly generated function of t. The ground truth values of the test expressions were computed point-wise for special functions, while derivatives were numerically calculated with respect to t over the interval t \u2208 [\u221210, 10]."}, {"title": "2.3 Governing equations rediscovery", "content": "In this experiment, we demonstrate NOMTO's ability to rediscover the patial differential equation governing a two-dimensional diffusion system using simulation data. We compared the performance of two different NOMTO variants, one utilizing FNO and the other CNO as surrogate models.\nThe training data were generated by numerically solving the heat equation within a two-dimensional rectan-gular domain. A detailed description of the data preparation process is provided in Appendix B. Each training sample has dimensions (50, 50, 50), corresponding to discretization points along the x-, y- and t-axes. We con-ducted a total of 1,000 simulations with randomly sampled initial conditions to ensure diversity within the training dataset.\nThe original two-dimensional heat equation contains two second-order partial derivatives. To assess NOMTO'S capability to handle three-dimensional arrays (i.e., two spatial coordinates and time), we selected a minimal set of library operations \u2013 {\u2202/\u2202t, \u0394}. The surrogate neural operators were trained on a grid matching the simulation grid. The FNO surrogates were configured with three layers, four modes, and 32 hidden channels, whereas the CNO models comprised four layers, each containing four residual blocks and a channel multiplier of 32. The training dataset consisted of random three-dimensional mixtures of Gaussians, discretized on the grid of the simulated system to ensure compatibility between the training data and simulation data. The NOMTO algo-rithm's computational graph was designed with a depth of two. The input to NOMTO was a three-dimensional tensor representing the temperature field u, and the ground truth output was the three-dimensional tensor \u2202u/\u2202t , with the time derivative calculated numerically using second-order central differences. NOMTO was trained to minimize the loss function:\n$\\mathcal{L} = \\frac{\\partial u}{\\partial t} - \\frac{\\partial u}{\\partial t} + \\sum_{i,j} w_{ij}|^{1/2},$ (1)\nwhere \u2202u/\u2202t represents the outputs of NOMTO, and $w_{ij}$ are the weights on the edges of the computational graph. Since the initial computational graph in this experiment contained at most eight edges, we did not apply any additional pruning during the optimization process. The resulting expressions discovered by the two algorithms are presented in Figure 2.\nThe coefficients of the expressions dentified by NOMTO closely matched those of the original heat equation, accurate to the first decimal place. However, NOMTO-FNO variant exhibited slightly higher errors compared to the NOMTO-CNO variant. This discrepancy is likely attributable to the inherent approximation errors in the FNO surrogate models.\nFigure 2 demonstrates that NOMTO's predictions of \u2202u/\u2202t closely follow the true dynamics, with residuals remaining negligible across most of the simulation domain. This indicates that NOMTO effectively captures the underlying behavior of the heat equation, showcasing its capability to accurately model complex physical systems. However, as depicted in Figure 2, errors are more pronounced near the domain boundaries for NOMTO-FNO compared to the NOMTO-CNO variant. This discrepancy is likely due to inherent approximation errors in FNO surrogates. Enhancing the accuracy of the surrogate models could further reduce prediction errors and improve the precision of the determined coefficients."}, {"title": "2.3.2 Two-dimensional Burgers' Equation", "content": "In a subsequent experiment, we applied the NOMTO algorithm to rediscover the Burgers' equation. Unlike the heat equation, Burgers' equation incorporates both diffusion and convection terms, introducing additional complexity for the NOMTO algorithm. We generated simulation data by solving the two-dimensional Burgers' equation within a square domain. Detailed simulation parameters and procedures are provided in Appendix C."}, {"title": "3 Discussion", "content": "This work introduces NOMTO, a novel non-linear symbolic regression algorithm that leverages neural operators to extract symbolic models from data. By incorporating sophisticated operations into its solution search space, NOMTO significantly expands the applicability of traditional symbolic regression methods. We demonstrated the algorithm's ability to identify expressions containing derivative operations and its utility as a model discovery tool for systems governed by non-linear partial differential equations. The capability to perform immediate inference and back-propagate through differential operators enables NOMTO to efficiently explore the solution space, more efficiently compared to previous evolutionary symbolic regression approaches.\nNOMTO performs well on standard symbolic benchmark expressions, and effectively handles cases involving singularities, such as division operations and the Gamma function. Although NOMTO encounters challenges in determining precise coefficients in these cases due to data clipping procedures, it successfully identifies the general structure of the expressions. This ability underscores NOMTO's potential to handle complex operations, thereby extending the boundaries non-linear symbolic regression methods. Furthermore, NOMTO's capacity to automatically discover non-linear models involving derivatives and special functions represents a significant advancement in symbolic regression. This is particularly evident in its application to discovering partial differential equations, as demonstrated by our experiments with the two-dimensional heat and Burgers' equations.\nHowever, NOMAD exhibits limitations when dealing with expressions that have low magnitudes within the search domain. This challenge can potentially be mitigated by developing more accurate neural operator surrogates that better approximate library operations across a wider range of input function magnitudes. This would directly translate to more precise resulting symbolic expressions. Furthermore, exploring more efficient sparsity-enforcing techniques and optimization strategies could further refine the algorithm's effectiveness.\nIn summary, NOMTO presents a promising framework for symbolic regression and model discovery by com-bining the robustness and interpretability of symbolic models with generalization capabilities of neural operators. Its ability to generalize across a diverse array of functions and non-linear operators facilitates breakthroughs in fields such as fluid dynamics, materials science, biological systems modeling, and climate science. Researchers in these areas can leverage NOMTO to develop various closure models and uncover underlying governing equations from experimental or simulation data, accelerating the development of interpretable models and enhancing our understanding of complex systems."}, {"title": "4 Methodology", "content": "In this chapter, we first introduce the concept of the expression tree and the library of operations in Section 4.1. Next, we describe the training procedure for the neural operator blocks in Section 4.2. Subsequently, we provide a detailed overview of the overall architecture of NOMTO, including the neural operator blocks, in Section 4.3. Finally, the optimization setup and expression extraction process are discussed in Section 4.4."}, {"title": "4.1 Expression Tree and Library", "content": "One effective method for representing a symbolic expression is through an expression tree, a graph where each node represents an operation, variable, or constant. Before addressing the symbolic regression problem, it is essential to select a comprehensive set of symbolic operations that will constitute the expression tree. This"}, {"title": "4.2 Neural Operators as Surrogate Library Operations", "content": "Neural operators are a class of algorithms specifically designed to approximate nonlinear operators. Unlike traditional neural networks, which learn mappings between finite-dimensional input and output vectors, neural operators are developed to learn mappings between infinite-dimensional functional spaces [28, 29]. This capa-bility enables them to approximate non-linear operators and compute their numerical derivatives - tasks that were previously unachievable with point-wise symbolic regression methods. Additionally, neural operators can construct differentiable surrogate models for functions with singularities by learning manually redefined versions of these functions. In this work, we utilize Fourier Neural Operators (FNOs) [25] and Convolutional Neural Operators (CNO) [23] to approximate symbolic operations. FNOs accurately capture clipped discontinuities (jumps) in discretized output functions. FNOs achieve this by learning mappings in the Fourier domain, making them particularly effective for such tasks. However, CNO have been previously shown to outperform FNO on a set of PDE benchmarks [23], making it more suitable for smoother approximations. We refer to different NOMTO variants as NOMTO-CNO and NOMTO-FNO depending on the underlying type of neural operators.\nNeural operators, by design, process functions as inputs and produce functions as outputs, enabling the expansion of expression search space beyond algebraic operations. In practice, both input and output functions are discretized. We synthesize input functions for training as mixtures of random Gaussian components, with the number of components uniformly sampled from the interval [10,50]. These input functions are discretized over 100 equidistant points in the interval [-10,10]. The corresponding output functions are computed on the same discretization grid using the exact mathematical operations from the library, as illustrated in Figure 3. To further refine the target function values, we apply the hyperbolic tangent (tanh) function to the output function values. The choice of tahn as the projection space is motivated by its ability to effectively handle both low- and high-magnitude values, improving the neural operator's approximation capabilities. Unary library operations require a single input argument, so their corresponding neural operator surrogates process one discretized input function. In contrast, binary operations take two discretized inputs functions as separate input channels. Each neural operator surrogate model is trained on a dataset of 105 samples, ensuring a robust approximation of the respective operation.\nCertain functions in the library, such as \u0393 and div, can produce outputs with extremely large magnitudes. To mitigate this, we project the output functions into the tanh space. To prevent gradient explosion during the training of neural operators and subsequently during the training of the full algorithm (described in de-tails below), we clip both the output values and the neural operator predictions to the range [-tanh(1000), tanh(1000)].\nAnother implementation challenge arises from library functions like \u221a. and ln, which have undefined regions within their discretization domains. However, it is important to note that this limitation does not impact the method's applicability to real-world data, as experimental measurements are generally well-defined across the domain of interest. The undefined regions of these functions pose challenges only during the optimization process when searching for a symbolic expression. To address this, we manually redefine the values of such library operations to ensure they are well-defined across the entire support. These redefined versions are detailed in Equation 8. The neural operators are subsequently trained to predict these redefined functions, enabling robust and consistent performance regardless of the input domain.\n$\\sqrt{x} = \\begin{cases}0, & \\text{if } x \\leq 0, \\\\ \\sqrt{x}, & \\text{if } x > 0,\\end{cases} \\quad \\ln x = \\begin{cases}\\ln|x|, & \\text{if } x \\neq 0, \\\\ -\\infty, & \\text{if } x = 0.\\end{cases}$ (8)"}, {"title": "4.3 Composing NOMTO Computational Graph", "content": "To integrate the pre-trained neural operator surrogates within the proposed framework, we construct neural op-erator blocks. Each neural operator block comprises either an FNO or a CNO model corresponding to a specific library function, a cropping layer, and an inverse projector denoted as tanh\u22121. The cropping layer ensures that predictions generated by the neural operator models remain within the range [-tanh(1000), tanh(1000)]. This constraint prevents extreme values that could destabilize the training process. Following the cropping layer, the inverse projector maps these constrained predictions back to the original functional space, maintaining the con-sistent and coherent function representations across different neural operator blocks. Once constructed, these neural operator blocks are integrated into the complete NOMTO architecture. Importantly, the configuration of each neural operator block remains fixed during the symbolic expression search process.\nSpecifically, neural operator blocks, corresponding to unary operations have one connecting edge per node in the previous layer, while nodes for binary operations have two connecting edges per node in the previous layer. In the case of binary operations, each edge represents a connection to one of the binary operation's inputs, as illustrated in Figure 1 (Step 2). The computational graph maintains a directed structure, with nodes in the last layer connected to a single output node that represents the value of the resulting symbolic expression.\nEach edge in the NOMTO computational graph indicates a connection between two operations. To represent the operations' coefficients, we assign a weight $w_{ij}^{(l)}$ to the edge, connecting i-th node of layer l \u2212 1 to the j-th node of layer l if j \u2208 1, 2, ..., m, and to the node $j + \\lfloor(j \u2212 m)/2\\rfloor$ of layer l if j \u2208 m + 1,m + 2, ..., m + 2n, as depicted in Figure 1. Here, m is the number of unary operations, and n is the number of binary operations in the library. The weights $w_{ij}^{(l)}$ are initialized with values drawn from a normal distribution N(0,0.1) for all experiments."}, {"title": "4.4 Optimization and Expression Extraction", "content": "As a result, the output node of NOMTO represents the value of an expression f, that is described by the preceding computational graph. The ground truth values of symbolic expression and the values of the output node are used to construct a loss function, defined as the $l_1$-norm of the prediction error $||f \u2212 \\hat{f} ||_1$. The weights on the edges of the graph are trained using the RMSProp algorithm [30]. To enforce sparsity in the model, we apply a modified $l_{1/2}$-norm penalty on the network weights as an additional regularization term to the primary loss function. The modified $l_{1/2}$-norm is defined as:\n$l_{1/2}(w) = \\begin{cases}|w|^{1/2}, & \\text{for } |w| \\geq a \\\\ \\frac{|w|^4}{8a^3} - \\frac{3|w|^2}{8a} + \\frac{3}{8}a, & \\text{for } |w| > a,\\end{cases}$ (9)\nwhere a is set to 0.01. Thus the total loss function becomes:\n$\\mathcal{L} = ||f - \\hat{f}||_1 + \\lambda ||W||_{1/2},$ (10)\nwhere w represents the set of all NOMTO's weights. In scenarios where the number of edges is relatively high, we employ periodic pruning of the weights to accelerate convergence and eliminate insignificant terms. This"}, {"title": "A Predicted symbolic expressions", "content": ""}, {"title": "B Heat equation simulation", "content": "To generate the data for the PDE rediscovery experiment, a numerical simulation of the two-dimensional heat equation was performed. The heat equation describes the diffusion of heat in a given domain over time and is represented as:\n$\\frac{\\partial u}{\\partial t} = \\alpha \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right),$ \nwhere \u03b1 is the thermal diffusivity, u = u(x,y,t) is the temperature distribution as a function of spatial coordinates x and y and time t.\nTo numerically solve this equation, we employ the finite difference method, which approximates the deriva-tives in space and time using discrete values on the grid. The simulation was conducted over a two-dimensional rectangular domain of size Lx \u00d7 Ly discretized into a uniform grid of Nx \u00d7 Ny points, with \u0394x and \u0394y representing the grid spacing in the x- and y-directions, respectively. We have set Lx = Ly = 20, Nx = Ny = 50 and \u0394x = \u0394y \u2248 0.4081.\nThe time variable was discretized using a time step \u0394t = 0.001 for a total simulation time T = 5. At each time step n, the temperature at a grid point (i,j) was updated using the following explicit finite difference scheme:\n$u^{n+1}_{i,j} = u^{n}_{i,j} + \\alpha \\Delta t \\left(\\frac{u^{n}_{i+1, j} - 2u^{n}_{i, j} + u^{n}_{i-1, j}}{\\Delta x^2} + \\frac{u^{n}_{i, j+1} - 2u^{n}_{i, j} + u^{n}_{i, j-1}}{\\Delta y^2} \\right)$ \nThe initial temperature distribution, u(x, y, 0), was initialized as a mixture of 100 Gaussians with covariances uniformly sampled from the [5, 10] interval. We further apply Neumann boundary conditions on the edges of the simulation domain.\nThe temperature field is saved with 0.1 time step during the simulation. The resulting sample of one simulation is represented by a tensor of shape (50,50,50), where the axes correspond to the x-, y- and time-axes. In total, we prepare 1,000 simulation samples with random initial conditions."}, {"title": "C Burgers' equation simulation", "content": "The Burgers' equations can model various physical phenomena such as turbulence and shock wave formation. The equations are given by:\n$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} = \\nu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right),$ \n$\\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} = \\nu \\left(\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2} \\right),$ \nwhere u = u(x, y, t) and v = v(x, y, t) represent the velocity components in the x- and y-directions, respec-tively, \u03bd is the kinematic viscosity, and t denotes time. In this experiment, the kinematic viscosity is set to \u03bd = 2.\nTo numerically solve these equations, we employed the explicit finite difference method with an upwind scheme for the convection terms to enhance numerical stability in convection-dominated flows.\nThe simulation was conducted over a two-dimensional rectangular domain. The domain and discretization grid are completely identical to the ones that were used for the heat equation simulation (Appendix B). The time variable was discretized using a time step dt = 0.001 to satisfy the Courant-Friderichs-Lewy (CFL) condition for numerical stability. The total simulation time was T = 0.5. We subsample the resulting velocity fields that correspond to each simulation step with the stride of 10, which results in Nt = 50 time frames.\nAt each time step, the velocity components at a grid point (i,j) were updated using the following explicit finite difference scheme:\n$u^{n+1}_{i,j} = u^{n}_{i,j} - \\Delta t \\left(u^{n}_{i,j} \\frac{u^{n}_{i,j} - u^{n}_{i-1, j}}{\\Delta x} + v^{n}_{i,j} \\frac{u^{n}_{i,j} - u^{n}_{i, j-1}}{\\Delta y} \\right) + \\nu \\Delta t \\left(\\frac{u^{n}_{i+1, j} - 2u^{n}_{i,j} + u^{n}_{i-1, j}}{\\Delta x^2} + \\frac{u^{n}_{i, j+1} - 2u^{n}_{i,j} + u^{n}_{i, j-1}}{\\Delta y^2} \\right)$,\n$v^{n+1}_{i,j} = v^{n}_{i,j} - \\Delta t \\left(u^{n}_{i,j} \\frac{v^{n}_{i,j} - v^{n}_{i-1, j}}{\\Delta x} + v^{n}_{i,j} \\frac{v^{n}_{i,j} - v^{n}_{i, j-1}}{\\Delta y} \\right) + \\nu \\Delta t \\left(\\frac{v^{n}_{i+1, j} - 2v^{n}_{i,j} + v^{n}_{i-1, j}}{\\Delta x^2} + \\frac{v^{n}_{i, j+1} - 2v^{n}_{i,j} + v^{n}_{i, j-1}}{\\Delta y^2} \\right),$\nThe initial velocity fields u(x, y, 0) and v(x, y, 0) were initialized as mixtures of 100 two-dimensional Gaus-sian functions. The Gaussians were defined with random means \u03bci uniformly sampled within the domain and covariance matrices \u03a3i = diag(\u03c32x,i, \u03c32y,i) with \u03c3x,i, \u03c3y,i uniformly sampled from the interval [1,3]. The ampli-tudes of Gaussians were uniformly sampled from [-1,1]. The resulting initial conditions were multiplied by 0.1 coefficient to limit the initial velocities and ensure numerical stability. Next, Neumann boundary conditions were applied on all edges of the simulation domain, ensuring zero-gradient conditions on the boundaries.\nIn total, we prepared 1000 simulation samples, each with random initial conditions as described above. The dataset consists of tensors representing the evolution of the velocity fields u and v over time."}]}