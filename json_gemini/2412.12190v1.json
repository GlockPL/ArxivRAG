[{"title": "iMoT: Inertial Motion Transformer for Inertial Navigation", "authors": ["Son Minh Nguyen", "Linh Duy Tran", "Duc Viet Le", "Paul J.M Havinga"], "abstract": "We propose iMoT, an innovative Transformer-based inertial odometry method that retrieves cross-modal information from motion and rotation modalities for accurate positional estimation. Unlike prior work, during the encoding of the motion context, we introduce Progressive Series Decoupler at the beginning of each encoder layer to stand out critical motion events inherent in acceleration and angular velocity signals. To better aggregate cross-modal interactions, we present Adaptive Positional Encoding, which dynamically modifies positional embeddings for temporal discrepancies between different modalities. During decoding, we introduce a small set of learnable query motion particles as priors to model motion uncertainties within velocity segments. Each query motion particle is intended to draw cross-modal features dedicated to a specific motion mode, all taken together allowing the model to refine its understanding of motion dynamics effectively. Lastly, we design a dynamic scoring mechanism to stabilize iMoT's optimization by considering all aligned motion particles at the final decoding step, ensuring robust and accurate velocity segment estimation. Extensive evaluations on various inertial datasets demonstrate that iMoT significantly outperforms state-of-the-art methods in delivering superior robustness and accuracy in trajectory reconstruction.", "sections": [{"title": "Introduction", "content": "Inertial odometry systems are intended to estimate the three-dimensional trajectories of a moving body by jointly analyzing motion, rotation, and geographically magnetic interactions from Inertial Measurement Unit (IMU) signals. Given privileges typified by energy efficiency, privacy preservation, and environmental robustness over other modalities, these approaches become more essential for tracking and tracing missions, including virtual and augmented reality, biodiversity monitoring, and rescue operations, especially in harsh environments, such as areas shrouded in smoke, or mist where common modalities (e.g., radio, acoustic, and visual signals) are no longer reliable for tracking purposes.\nContemporary approaches to inertial navigation are commonly classified into three distinct research paradigms: physics-based methods, heuristic priors-based methods, and data-driven priors-based methods. Since quadratic errors are leaked through the double integral of noisy IMU measurements whilst computing displacements, classical physics-based methods, exemplified by strap-down inertial navigation systems (SINS) (Titterton and Weston 2004) result in huge drift accumulation in a very short time. Heuristic priors-based methods, represented by pedestrian dead reckoning (PDR) approaches (Jimenez et al. 2009; Tian et al. 2015), decompose trajectory estimation into discrete components: step detection, step-length estimation, and heading estimation, all operating under assumptions of regular human gait patterns. However, these methods often face limitations in adaptability, with step detection and step-length estimation being confined to specific scenarios and occasionally relying on fixed parameters. Moreover, heading estimation may suffer from inaccuracies induced by gravitational and magnetic disturbances. In an advanced manner, data-driven priors-based methods (Chen et al. 2018a; Herath, Yan, and Furukawa 2020; Liu et al. 2020) utilize end-to-end deep learning architectures to directly estimate velocity segments from IMU sequences, significantly reducing drifting errors in trajectory reconstruction. Despite their progress, these methods fail to consider modality distinctions between acceleration and angular rates, as well as motion uncertainties among individuals, resulting in compromised accuracy.\nIn this paper, we present a multimodal inertial Motion Transformer (iMoT), which maximizes the utilization of complementary features between motion and rotation inputs to dynamically represent motion uncertainties for different instantaneous velocity segments. Firstly, we address the practical challenges of input tokenization in iMoT. Conventional tokens captured at a single time step with multiple variates (i.e., channels) often struggle to convey a complete chain of motion events due to excessively local receptive fields and time-unaligned events represented by concurrent time points (Zhang and Yan 2023). In contrast, we consider the entire time series of each variate as an input token.\nSecondly, given the multivariate time series nature of IMU sequences, we extend the time series decomposition (Hyndman and Athanasopoulos 2018), a widely adopted pre-processing technique, into Progressive Series Decoupler (PSD), a trainable module that can be seamlessly integrated within encoder layers. Specifically, PSD enhances unimodal features of acceleration and angular velocity signals and aids the absorption of such information by progressively decomposing their intricate temporal patterns into more interpretable components, which better highlight critical motion events such as half-turns, U-turns, and periods of stillness. To reflect modality differences, we propose Adaptive Positioning Encoding (APE), which associates acceleration and angular tokens with appropriate positional embeddings based on their temporal contents. These fully embedded tokens are then processed through a self-attention module to capture multivariate correlations. Additionally, we introduce Adaptive Spatial Sync (ASC) at every residual connection to retain fine-grained details across channels.\nThirdly, we recognize that motion modes can vary between individuals and over time, affecting both the direction and magnitude of instantaneous velocity segments. Then, each velocity segment can be defined by the most likely combination of these motion modes. Inspired by Particle Filter techniques, we model motion uncertainties for a given velocity segment by manipulating a set of learnable query motion particles during decoding. Specifically, each query motion particle represents the velocity of a specific motion mode, functioning as a learnable positional embedding to probe related cross-modal features. This probing initiates a cyclic process, where the cross-modal features extracted in each decoding step are continuously used to inform and refine the query particles in successive steps. Finally, we propose a Dynamic Scoring Mechanism (DSM) to optimize the query particle set during both training and testing phases.\nOur main contributions are four-fold: (1) We propose both novel inertial Transformer encoder and decoder networks (iMoT) that harness the complementary effects of motion and rotation modalities for uncertainty modeling to enhance positional accuracy. (2) In encoding context features, we first introduce Progressive Series Decoupler (PSD) to highlight motion events within each modality, followed by Adaptive Positional Encoding (APE) to include distinctions between modalities in position encoding. Additionally, we implement Adaptive Spatial Sync (ASC) at every residual connection to ensure the seamless integration of spatial details. (3) In decoding, we introduce a novel concept of query motion particles that utilize cross-modal information retrieval from context features to learn all possible motion modes accounting for uncertainties in motion. At the final decoding step, these adjusted particles are collectively considered to determine the desired velocity segments through a unique Dynamic Scoring Mechanism (DSM). (4) We rigorously validate the contribution of each proposed module and demonstrate the overall effectiveness of iMoT against state-of-the-art (SoTA) odometry methods over four benchmark inertial datasets."}, {"title": "Related Work", "content": "Physics-based methods (no priors). A strap-down inertial navigation system (SINS)(Savage 1998) first rotates acceleration measurements from the body frame to the navigation frame using a rotation matrix derived from integrating angular velocity measurements in order to subtract the Earth's gravity. Locations are then acquired by double-integrating the linear acceleration(Shen, Gowda, and Roy Choudhury 2018). Due to noisy sensing compounded by multiple integrations, these strap-downs incur quadratic error propagation heavily, drifting far away from desired locations.\nHeuristic priors. To alleviate drifting errors accrued in SINS, step-based pedestrian dead reckoning (PDR) approaches (Jimenez et al. 2009; Tian et al. 2015) leverage human motion regularities by separately detecting steps, estimating step length and heading before updating the location with each step. These approaches yield impressive results under controlled environments where the assumption remains valid. Some other methods (Janardhanan, Dutta, and Tripuraneni 2014; Kourogi and Kurata 2014) that incorporate principal component analysis and frequency domain analysis have been developed to enhance rotational accuracy by determining body motion directions. However, these heuristic-based methods fall short of robustness, as their inner components are interdependent on gravitational and magnetic factors, and confined to specific scenarios.\nData-Driven priors. Significant efforts have recently been directed toward developing deep learning networks to extract useful features from IMU measurements for enhanced position estimation. RIDI (Yan, Shan, and Furukawa 2018) introduces a two-stage system that first regresses low-frequency corrections to the acceleration before double-integrating it into displacement. To bypass the noisy double integration, IoNeT (Chen et al. 2018a) employs LSTMs to directly regress polar velocity segments and changes in heading rates, which are then cumulatively computed to approximate translations. Similarly, RONIN framework (Herath, Yan, and Furukawa 2020) explores three types of neural networks to assess their distinct contributions to inertial navigation. Building on RoNIN, TLIO (Liu et al. 2020) integrates displacement estimates with raw IMU measurements using a stochastic-cloning Extended Kalman Filter (EKF) to derive position, orientation, and sensor biases. More recently, CTIN (Rao et al. 2022) adapts ResNet blocks into a Transformer architecture, which incorporates motion uncertainties by optimizing covariance of velocity segments.\nTransformer. Transformers have found extensive applications in natural language processing (Vaswani et al. 2017; Devlin et al. 2018) and computer vision (Dosovitskiy et al. 2020; Carion et al. 2020). Our approach is partially inspired by DETR (Carion et al. 2020) that utilizes query object tokens during decoding to extract object-related information for detection. Based on this idea, we propose a novel concept of learnable query motion pairs, yet consisting of query motion particles and their associated content features, for modeling motion uncertainties. In an advanced manner, these query pairs, especially the query motion particle set, are not only externally updated in the form of learnable positional embeddings but also undergo internal motion refinement within transformer decoder layers. This dual updating process enables more precise modeling of motion dynamics."}, {"title": "Proposed Method", "content": "Inertial odometry methods aim to reconstruct a traveled trajectory from corresponding IMU sequences, denoted as\n$\\mathbf{A}=\\left\\{A_{a}, A_{g}\\right\\} \\forall A \\in \\mathbb{R}^{2 D \\times T}$, where $A_{a} \\in \\mathbb{R}^{D \\times T}$ and $A_{g} \\in \\mathbb{R}^{D \\times T}$ represent acceleration and angular velocity of $D \\times T$ instances recorded over $D = 3$ channels along x-, y-, z-axes within 1 second, respectively. In this work, we propose inertial Motion Transformer (iMoT), a novel transformer encoder-decoder structure designed to iteratively refine query motion particles to learn all possible motion modes for every single instantaneous velocity segment. This approach allows iMoT to effectively adapt to variations in velocity segments, thereby enhancing trajectory synthesis. The overall structure is illustrated in Fig.1.\nIn practice, all movements are inherently reflected by fluctuations in the motion and rotation modalities within IMU sequences. To seamlessly encode such motion context, we directly reshape raw IMU sequences into input tokens compatible with the transformer encoder. However, existing spatial tokens formed by a single time step $t \\in T$ along $D$ channels often struggle to express contextual information due to excessively local receptive fields and time-unaligned events. In this way, series variations are significantly influenced by the sequence order, which might undermine the attention mechanism when improperly applied to the temporal dimension. As a result, the proposed model may be compromised in its ability to capture essential series representations and portray multivariate correlations, limiting its capacity and generalizability on diverse IMU data. Therefore, we first tokenize the sequence into temporal variate tokens $A_{a}$ and $A_{g}$ of $T$-dimensions, which offer a broad receptive field to capture important events along the temporal axis. Following this, we introduce a series of specialized modules incorporated in encoder layers to enhance the exploitation of these tokens.\nTo promote the capture of necessary motion events during encoding, Progressive Series Decoupler (PSD) is introduced at the facade of each encoder layer, breaking down complex temporal patterns of motion and rotation series into two more interpretable components, namely seasonal signals and trend-cycle signals. That is, seasonal signals carry prominent information about symmetric human motions (e.g., leg movements, hip swiveling, and arm swinging), which is particularly useful for tracing repeated sequence patterns (e.g., walking, strolling, and running). On the other hand, trend-cycle signals exhibit longer-term movements, revealing sudden motion changes (e.g., sudden stopping and turning). Unlike the prior work (Wu et al. 2021), we design a centered moving average to smooth out periodic fluctuations and eliminate some of the randomness in the data, leaving a smooth trend-cycle component. The seasonal signal is then computed as the residual:\n$\\begin{aligned}\nA_{t} &=\\text { AvgPool }_{k_{2} \\times k_{1}}(\\text { Padding }(\\mathbf{A})) \\\\\nA_{s} &=\\mathbf{A}-A_{t}\n\\end{aligned}$\nHere, $A_{s} \\in \\mathbb{R}^{2 D \\times T}$, and $A_{t} \\in \\mathbb{R}^{2 D \\times T}$ denote the seasonal and the extracted trend-cycle parts, respectively. The Padding operation is used to keep the series length unchanged. The $\\text { AvgPool }_{k_{2} \\times k_{1}}(\\cdot)$ operation depicts the application of a moving average of order $k_{1}$ followed by another moving average of order $k_{2}$. Note that $k_{2}$ must be less than $k_{1}$, and both should be either odd or even numbers to ensure the symmetry of the weighted average applied to the observations from both the inner and outer sides. As depicted in Fig.2, we denote $A_{s}, A_{t} = SeriesBreaker(\\mathbf{A})$, an internal operation embedded within PSD module that is progressively learned over layers. After undergoing PSD, sequence tokens $\\mathbf{A}$ are associated with their decoupled components $A_{t}$ and $A_{s}$ together as standard input tokens $\\tilde{\\mathbf{A}} \\in \\mathbb{R}^{6 D \\times T}$ to the subsequent blocks. This approach helps isolate and interpret different motion cues with much greater ease, enhancing the model's ability to handle complex IMU data effectively.\nPositional Embeddings play a crucial role in extracting, storing, and pooling context features in both encoding and decoding phases. As presented in Fig.1, we initialize Adaptive Positional Encoding (APE) with common base sinusoidal positional embeddings $E_{A} \\in \\mathbb{R}^{D \\times T}$. From these base embeddings, we start to learn temporal scaling factors according to content discrepancies between modalities, resulting in adaptive positional embeddings $E_{A} \\in \\mathbb{R}^{6 D \\times T}$.\n$E_{A}=\\left[\\text { MLP }\\left(A_{a}\\right) \\cdot E_{A}, \\operatorname{MLP}\\left(\\tilde{A}_{g}\\right) \\cdot E_{A}\\right]$\nwhere $A_{a} \\in \\mathbb{R}^{3 D \\times T}$, and $A_{g} \\in \\mathbb{R}^{3 D \\times T}$ are fully integrated versions of acceleration and angular tokens, respectively. MLP(\u00b7) refers to a multilayer perceptron network, and [\u00b7] denotes a concatenation operation. Subsequently, the attention module of j-th transformer encoder layer is applied to capture multivariate correlations between temporal tokens, which is formulated as below: 1\n$\\operatorname{SelfAttn}\\left(\\text {query}=\\tilde{A}^{j-1}+E_{A}^{j-1}, \\text { key }=\\tilde{A}^{j-1}+E_{A}^{j-1}, \\text { value }=\\tilde{A}^{j-1}\\right)$\nIn this equation, $\\tilde{A}^{j}$ represents the updated tokens at layer j, while $\\tilde{A}^{j-1}=\\left[\\tilde{A}_{a}^{j-1}, A_{g}^{j-1}\\right]$ and $E_{A}^{j}$ are the tokens and positional embeddings from the previous layer, respectively. By applying adaptive scaling factors to the common base positional encodings $E_{A}$, iMoT becomes permutation-invariant to token orders, thus allowing it to tailor the positional encoding to specific characteristics of motion and rotation data. Staying in the flow of these benefits, the self-attention module, denoted as SelfAttn can better encode unique cross-modal interactions into contextual features.\nSince contextual features are typically encoded in a temporal manner where cross-channel interactions at each time step are not considered, we propose an Adaptive Spatial Sync (ASC) module to compensate for the shortage of such spatial information. Placed at every residual connection, the ASC module can infuse fine-grained channel details directly along with two other information flows.\nA given instantaneous velocity segment varies across individuals and is influenced by their unique gaits, giving rise to uncertainties in motion. To address this, we introduce P motion pairs of query motion particles $ \\hat{v} \\in \\mathbb{R}^{P \\times 2}$ and their query content features $C \\in \\mathbb{R}^{P \\times T}$ into decoder layers, as illustrated in Fig.1. Each pair represents a specific motion mode, collectively modeling motion uncertainties. By iteratively refining the query motion particles based on their updated content features for individual velocity segments, the model can quickly adapt to motion variability and effectively approximate the desired velocity segments.\nThe query motion particles, which characterize respective velocity, are represented as positional embeddings and dynamically updated according to the predicted velocity within each decoder layer as follows:\n$E=\\operatorname{MLP}\\left(P E\\left(\\hat{v}^{j-1}\\right)\\right)$\nwhere $P E$ denotes the sinusoidal positional encoding operation, which is conditioned on velocity of the query particles $\\hat{v}^{j-1}$ in the previous layer $j-1$. The resulting learnable positional embeddings $E$ are added to the content features $C^{j}$ (i.e., $C^{0}$ initialized with a zero matrix) and then passed into the SelfAttn module of the decoding layer $j$:\n$C_{s a}^{j}=\\operatorname{SelfAttn}\\left(\\text {query}=C^{j-1}+E, \\text { key }=C^{j-1}+E, \\text { value }=C^{j-1}\\right)$\nHere, $C_{s a}^{j} \\in \\mathbb{R}^{P \\times T}$ represents the updated query content. To enable the pooling of cross-modal information from encoded context features, we concatenate positional embeddings with the updated content information as queries in the cross-attention module. This also allows us to decouple the contributions of content and position to the attention weights. To align with positional embeddings from the encoder, we learn an MLP on the content information to generate a scaling vector for the positional embeddings in the decoder. Specifically, two cross-attention modules are employed singly for retrieving features regarding specific motion modes from both motion $A_{a}$ and rotation tokens $A_{g}$:\n$\\begin{aligned}\nC^{j} &=\\operatorname{SelfAttn}\\left(\\begin{array}{l}\\text { query }=\\left[C_{s a}^{j}, \\operatorname{MLP}\\left(C^{j-1}\\right) \\cdot E\\right], \\text { key }=\\left[\\tilde{A}_{a}, \\mathbf{E}_{a}\\right], \\\\\n\\text { value }=A_{a}\n\\end{array}\\right) \\\\\nC^{j} &=\\operatorname{SelfAttn}\\left(\\begin{array}{l}\\text { query }=\\left[C_{s a}^{j}, \\operatorname{MLP}\\left(C^{j-1}\\right) \\cdot E\\right], \\text { key }=\\left[\\tilde{A}_{g}, \\mathbf{E}_{g}\\right], \\\\\n\\text { value }=A_{g}\n\\end{array}\\right) \\\\\nC^{j} &=\\operatorname{MLP}\\left(\\left[C^{j}, C^{j}\\right]\\right)\n\\end{aligned}$\nwhere $C^{j} \\in \\mathbb{R}^{P \\times T}, C^{j} \\in \\mathbb{R}^{P \\times T}$, and $C^{j} \\in \\mathbb{R}^{P \\times T}$ denote the acceleration-augmented content features, angular velocity-augmented content features, and the updated query content features for each query motion particle, respectively. Additionally, $\\mathbf{E}_{a} \\in \\mathbb{R}^{3 D \\times T}, \\mathbf{E}_{g} \\in \\mathbb{R}^{3 D \\times T}$ represent positional embeddings of motion and rotation tokens in $E_{A}$.\nIn addition to external updates of positional embeddings, employing motion particles as learnable queries also enables internal, layer-by-layer updates, allowing the particle set to rapidly adapt to motion variability. For each decoding layer $j$, we utilize an MLP to predict relative velocity adjustments $\\Delta \\hat{v}=\\left[\\Delta v_{x}, \\Delta v_{y}\\right]$ based on the updated content features. However, these MLPs share the same parameters across layers to ensure consistent updates throughout the model.\n$\\begin{array}{c}\n\\Delta \\hat{v}=\\operatorname{MLP}\\left(\\left[C^{j}\\right]\\right) \\\\\n\\hat{v}^{j+1}=\\hat{v}^{j}+\\Delta \\hat{v}\n\\end{array}$\nAt the final layer, we start with the calculation of Euclidean distances between neighboring particles $\\hat{v}$ and the ground-truth velocity particle $v_{G T} \\in \\mathbb{R}^{1 \\times 2}$ to establish an inverse score list $S=\\left[S_{0}, \\ldots, S_{P-1}\\right] \\in \\mathbb{R}^{1 \\times P}$ that assigns higher scores to particles closer to $v_{G T}$. To optimize the particle set, we focus on making significant adjustments to distant particles while giving less attention to the closer ones when computing the mean particle $v_{m} \\in \\mathbb{R}^{1 \\times 2}$. This strategy encourages a more compact distribution of particles around the desired velocity particle $v_{G T}$. The calculation is formalized as follows:\n$\\begin{aligned}\nS_{p} &=\\frac{e^{-d\\left(\\hat{v}_{p}, v_{G T}\\right)}}{\\sum_{i=0}^{P-1} e^{-d\\left(\\hat{v}_{i}, v_{G T}\\right)}} \\\\\nv_{m} &=\\sum_{i=0}^{P-1}\\left(1-S_{p}\\right) \\hat{v}_{p} \\\\\nJ_{v e l} &=\\left|\\hat{v}_{G T}-v_{m}\\right|\\end{aligned}$\nwhere $\\gamma$ is the weighting factor largely controlling the influence of distant particles on the mean particle $v_{m}$, and $d(\\cdot)$ represents the Euclidean distance operation. $J_{v e l}$ is the mean square error loss between mean particles and ground-truth velocity particles over a batch of B samples. To maintain stability during both training and testing, we further introduce an entropy loss function to maximize the entropy of the particle score list S:\n$J_{\\text {ent }}=-\\frac{1}{B P} \\sum_{b=0}^{B-1} \\sum_{p=0}^{P-1}-S\\log (S+\\epsilon)$\nHere, $J_{\\text {ent }}$ denotes the entropy loss function and $\\epsilon$ is a small constant of $1 e^{-10}$ to stabilize the loss. This loss encourages a more uniform distribution of estimated particles around the ground-truth particles, allowing the use of average pooling of the estimated particles to approximate the desired velocities once training is complete. While the combined constraints of $J_{v e l}$ and $J_{\\text {ent }}$ could steer the proposed model to significantly minimize velocity discrepancies and adjust the particle distribution, we empirically find some instabilities during testing. Specifically, the absence of ground-truth particles for generating the score list necessitates the use of average pooling to approximate these particles, leading to insufficient fine-grained velocity segments and thereby degrading the quality of trajectory reconstruction. To address this issue, we employ an MLP that attends to all aligned particles at the last layer, considering both x-, and y-directions, to directly learn a dynamic two-dimensional score list $S^{d}=\\left[S_{0}^{d}, \\ldots, S_{P-1}^{d}\\right] \\in \\mathbb{R}^{2 \\times P}$:\n$\\begin{aligned}\nS^{d} &=\\operatorname{MLP}\\left(V^{T}\\right) \\\\\nv_{m} &=S^{d} \\cdot \\hat{v}\n\\end{aligned}$\nwhere $\\hat{v}$ denotes transposed motion particles. This approach adaptively transforms the particle distribution and pools the mean particle accordingly during both training and testing, thereby eliminating the need for the entropy loss $J_{\\text {ent }}$. As a result, iMoT can be efficiently optimized using only the velocity loss $J_{v e l}$."}, {"title": "Experiment", "content": "In this section, we empirically verify the effectiveness of our method in controllable and dynamic scenarios.\nDataset Four popular benchmark datasets are used for evaluation: RIDI (Yan, Shan, and Furukawa 2018), RONIN (Herath, Yan, and Furukawa 2020), OxIOD (Chen et al. 2018b), and IDOL (Sun, Melamed, and Kitani 2021). For controlled scenarios, RIDI and OxIOD are configured with predefined attachments, such as pocket, handheld, bag, body, and trolley, which are specified separately for each sequence. To provide end-users with greater freedom of movement in practice, IDOL and RoNIN are designed with dynamic motion contexts, where devices are naturally placed across all recorded sequences. Notably, RoNIN is the largest dataset, containing more than 40 hours of IMU data from 100 human subjects performing natural human motions.\nEvaluation Metric Four types of metrics are used for the quantitative trajectory evaluation:\n\u2022 Absolute Trajectory Error (ATE) (m) is calculated as the average Root Mean Squared Error (RMSE) between the estimated and ground-truth trajectories as a whole.\n\u2022 Distance-Relative Trajectory Error (D-RTE) (m), is calculated as the average RMSE between the estimated and the ground-truth over a fixed distance $d_{r}$ (i.e., 1 m).\n\u2022 Time-Relative Trajectory Error (T-RTE) (m) is the average RMSE over a regular period $t_{r}$ (i.e., 1 minute).\n\u2022 Position Drift Error (PDE) (%) measures the drifting error at the final position relative to the traveled distance.\nImplementation Details To initialize PSD module in the encoder, we implement AvgPoolk2\u00d7k1 with k1 and k2 set to 9, 3, respectively. For decoding, we empirically find that using a set of P = 128 query motion particles representing 128 motion modes is sufficient. Depending on the sampling rate of each dataset, the token dimension is set to 100 for IMU sequences recorded at 100 Hz and to 200 for sequences recorded at 200 Hz. The network, consisting of N = 2 encoder layers and M = 2 decoder layers, is trained with the learning rate of 1e-4 and batch size of B = 128 using Adam optimization. The training is performed with PyTorch version 2.4.0 on an H100 GPU with 80 GB of memory."}, {"title": "Ablation Study", "content": "As presented in Tab.1, we develop 14 configurations on the largest RoNIN dataset, where all the proposed modules are progressively incorporated or removed, to meticulously examine both stand-alone and complementary contributions of the proposed modules.\nProgressive Series Decoupler. To assess the isolated impact of PSD, we compare the performance between the baseline model (i), where all proposed modules are excluded, and model (viii) with only PSD enabled. The findings reveal considerable improvements across all error metrics when PSD is activated, especially a 13.89% reduction in ATE. Furthermore, the effectiveness of PSD is consistently observed even when jointly incorporated with other modules. For instance, there is an 8.16% ATE reduction between model (iv) with PSD and model (v) without PSD, and an 8.29% reduction between the full version (xv) and model (xiv), which is identical except for the absence of PSD. These consistent improvements across different configurations firmly verify the necessity of the additional information provided by PSD.\nAdaptive Positional Encoding. To verify the stand-alone influence of APE, we collate the difference in performance between the baseline (i) and model (x) with only APE enabled. The improvements, particularly ~ 11.11% in ATE reduction underscores APE's significant role in producing adaptive positional embeddings accounting for modal distinctions. Furthermore, consistent performance gains across various configurations pairs, e.g., (iii) vs. (ii), (vi) vs. (v), (vii) vs. (iv), and (xiii) vs. (xv) further validates the effectiveness of the proposed module with higher confidence.\nAdaptive Spatial Sync. The ASC module is designed to compensate for the absence of cross-channel interactions. Although its contribution margin may appear smaller than others, particularly a 7.41% reduction in ATE when upgrading the baseline model (i) to the configuration (ix) with the inclusion of ASC, it has consistently delivered improvements both in standalone and complementary scenarios. Typically, the performance gains from the configuration (vi) to the full version (xv) further confirm the effectiveness of fusing spatial interactions, as evidenced by a reduction in ATE from 5.39 m to 5.31 m.\nWe first examine the bare influences of manipulating query motion particles using the combined constraints of $J_{v e l}$ and $J_{\\text {ent }}$ to describe uncertainties in motion by comparing the baseline (i) with model (ii). Unlike other cases, this results in a deterioration of 0.62 m in ATE. Further investigation reveals that directly adding query particles to existing models, such as model (viii) with PSD (which already outperformed the baseline (i) by 13.89%), leads to worse performance. For instance, the upgraded model (xii) performs 1.45 m worse than model (viii) and 0.55 m worse than the baseline (i). This can be attributed to the inefficacy of the above constraints in optimizing the particle set, which becomes severe as extra sources are introduced by PSD. A similar phenomenon is observed in configurations (x) and (iii), where introducing the particles exacerbates existing performance. However, these issues are significantly alleviated, and performance gains are achieved when the DSM module is introduced. Specifically, the configuration (iv) with DSM surpasses all its previous versions, including (ii) and (i). Similar enhancements are witnessed in comparisons of (iii) vs. (vii) and (xii) vs. (v). Besides, an intriguing observation is that (viii) and (x), which integrate only a single module (e.g., PSD or APE), outperform their counterparts (v) and (vii), which further incorporate both the query particles and DSM. This raises questions about the efficacy of these two components when combined. To investigate, an additional experiment was conducted, comparing the first three basic modules combined in (xi) with the full version (xv). Notably, the full version, which incorporates query particles and DSM, achieves a further 3.99 % improvement in ATE over the combined model (xi). These findings underscore the critical importance of the query particle set in fully exploiting basic components, while also highlighting the necessity of a flexible optimization method like DSM to maximize their effectiveness.\nThe number of query motion particles plays a crucial role in extracting cross-modal interactions from context features and describing motion uncertainties thereafter. As shown in Fig.4, using $J_{v e l}$ and $J_{\\text {ent }}$ with average pooling after training cannot tolerate a higher number of particles, limiting the model's capacity. However, the introduction of DSM allows the model to effectively accommodate more particles, enhancing its ability to represent motion uncertainties. Our experiments reveal that 128 particles are optimal for modeling uncertainties across different velocity segments in RONIN. Fewer than 128 particles may fail to cover enough motion modes for accurate velocity synthesis, while more than 128 particles could introduce redundancy, potentially diluting cross-modal information with unnecessary motion modes through their positional embeddings."}, {"title": "State-of-The-Art Performance", "content": "The results presented in Tab. 2 exhibit a detailed evaluation of trajectory errors for various odometry methods across four benchmark datasets: RIDI, RONIN, OxIOD, and IDOL, using three standard metrics: ATE, T-RTE, and D-RTE. Throughout the experiment, our proposed method consistently outperforms other SoTA approaches, particularly in its ability to generalize to unseen subjects. For example, in dynamic scenarios from the RONIN dataset, where recording devices are freely held during movement, our method achieves an ATE of 5.31 m for unseen subjects, significantly outperforming RIDI (15.75 m) by 66.29%, and other robust methods that account for motion uncertainties, such as CTIN (6.89 m) and TLIO (6.77 m), by 22.93% and 21.57%, respectively. This trend of superior performance is consistent across all evaluated datasets in terms of D-RTE and T-RTE as well. For instance, on the IDOL dataset, our model demonstrated a 15.43% improvement in T-RTE and a 12.50% improvement in D-RTE compared to the second-best model, RoResnet18. These results can be attributed to our method's distinct advantage in effectively modeling motion uncertainties with learnable query motion particles. While other methods show a significant performance decline in unseen dynamic scenarios, the minimal error increase between seen and unseen subjects in our method highlights its generalization capability.\nFor stability verification, we further visualize the detailed metrics over the entire RONIN dataset. As illustrated in Fig. 5, the cumulative distribution function (CDF) of the trajectory errors clearly shows that the proposed method consistently outperforms the others, as indicated by the uppermost positions of the red curve. Specifically, in 80% of the cases predicted by our method, the ATE remains below approximately 6.5 m, denoted as P(X < 6.5) = 0.8. In contrast, other methods only achieve an ATE upper bound of around 10 m for 80% of the examples. Furthermore, the network also exhibits the lowest position drift error (PDE %) with the highest confidence and fewest outliers, further highlighting its robustness and precision in trajectory prediction."}, {"title": "Conclusion", "content": "This paper introduces iMoT, an innovative transformer architecture designed to capture and model motion uncertainties within instantaneous velocity segments. Extensive experimental results demonstrated that PSD greatly enhances the encoding of complex temporal patterns, while the manipulation of the query particle set effectively learns and represents various motion modes, accounting for motion variability. Our approach not only improves trajectory reconstruction quality but also exhibits robust generalization across a wide range of dynamic scenarios, setting a new SoTA standard in handling motion uncertainty for odometry tasks."}, {"title": "Appendices", "content": "Trajectory reconstruction is performed by doing integration of predicted velocity segments. The major metric used to evaluate the accuracy of positioning is a Root Mean Squared Error (RMSE) with various definitions of the inside estimation error: RMSE =\n$\\sqrt{\\frac{1"}, {"below": "n\u2022 Absolute Trajectory Error (ATE) is RMSE of squared Euclidian error distances $E_{t}(x_{t}, \\hat{x_{t}}) = \\sqrt{d((x_{t} - \\hat{x_{t}}))^{2}}$\n\u2022 Time-Relative Trajectory Error (T-RTE) is the time-normalized RMSE of average errors $E_{t}(x_{t}, \\hat{x_{t}}) = \\sqrt{d((x_{t+t_{r}} - x_{t}) - (\\hat{x_{t+t_{r}"}]}, {}]