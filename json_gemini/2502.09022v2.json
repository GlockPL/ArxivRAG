{"title": "Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning", "authors": ["Lin Zhang", "Lijie Hu", "Di Wang"], "abstract": "Transformer-based language models have achieved significant success; however, their internal mechanisms remain largely opaque due to the complexity of non-linear interactions and high-dimensional operations. While previous studies have demonstrated that these models implicitly embed reasoning trees, humans typically employ various distinct logical reasoning mechanisms to complete the same task. It is still unclear which multi-step reasoning mechanisms are used by language models to solve such tasks. In this paper, we aim to address this question by investigating the mechanistic interpretability of language models, particularly in the context of multi-step reasoning tasks. Specifically, we employ circuit analysis and self-influence functions to evaluate the changing importance of each token throughout the reasoning process, allowing us to map the reasoning paths adopted by the model. We apply this methodology to the GPT-2 model on a prediction task (IOI) and demonstrate that the underlying circuits reveal a human-interpretable reasoning process used by the model.", "sections": [{"title": "Introduction", "content": "In recent years, the Transformer architecture, introduced by (Vaswani et al., 2017), has become an efficient neural network structure for sequence modeling (Brown et al., 2020). Previous research (Hou et al., 2023; Dong et al., 2021) has confirmed that large models rely primarily on reasoning rather than mere memorization when answering questions. However, the \"thought process\" of these models remains unclear, as shown in Figure 1. How can we explore the thought process employed by large models during reasoning (Wei et al., 2022; Kojima et al., 2022)? Addressing this question is not only crucial for deepening our understanding of these models but also essential for developing the next generation of reliable language-based reasoning systems (Creswell and Shanahan, 2022; Creswell et al., 2022; Chen et al., 2023; Hu et al., 2024a; Cheng et al., 2024; Yang et al., 2024a).\nInfluence functions focus on analyzing models from the perspective of their training data (Koh and Liang, 2017). They have been shown to be versatile tools applicable to a wide range of tasks, including understanding model behavior, debugging models, detecting dataset errors, and generating visually indistinguishable adversarial examples(Hu et al., 2024b,c). As a variant of influence functions, self-influence is a technique for evaluating the impact of specific inputs within a neural network on the model's output. For different tokens within an input sample, self-influence scores reflect the significance of each token across various layers of the model. By tracking the influence changes of different tokens throughout the reasoning process of large language models (LLMs), it is possible to map the thought process executed by the model. However, directly calculating self-influence for all parameters in LLMs is practically infeasible due to the enormous computational resources and substantial memory consumption required.\nAs a result, circuit analysis within the framework of Mechanistic Interpretability (MI) (Olah, 2022; Nanda, 2023) has become a focal point of research. MI aims to discover, understand, and verify the algorithms encoded in model weights by reverse engineering the model's computations into human-understandable components (Meng et al., 2022; Geiger et al., 2021; Geva et al., 2020; Zhang et al., 2024; Hong et al., 2024). A key method in this field is circuit analysis (Conmy et al., 2023; Olah et al., 2020). In this approach, neural networks are conceptualized as computational graphs, where circuits represent sub-graphs composed of interconnected features and the weights that link them. These circuits function as fundamental computational units and building blocks of the network"}, {"title": "Related Work", "content": "Interpretability Methods in Language Models.\nInterpretability paradigms for AI decision-making range from black-box techniques, which focus on input-output relationships, to internal analyses that delve into model mechanics (Bereska and Gavves, 2024). Behavioral interpretability (Warstadt et al., 2020; Covert et al., 2021; Casalicchio et al., 2019) treats models as black boxes, examining robustness and variable dependencies, while attributional interpretability (Sundararajan et al., 2017; Smilkov et al., 2017; Shrikumar et al., 2017) traces outputs"}, {"title": "Preliminary", "content": "Automate Circuit Finding. Edge Attribution Patching (EAP) is a gradient-based method designed to efficiently identify circuits responsible for specific behaviors in neural networks. It estimates the importance of each edge by calculating the change in the model's loss when that edge is corrupted. The score for an edge (u, v) is given by:\n$(z_u - z'_u) \\nabla_v L(s)$\nwhere $\\nabla_v L(s)$ is the gradient of the loss function L with respect to the input of node v, and $z_u$ and $z'_u$ represent the clean and corrupted inputs to node u, respectively. EAP-IG extends this approach by incorporating Integrated Gradients, which computes gradients along a linear path between clean and corrupted inputs. The integrated gradients score for an edge (u, v) is:\n$\\frac{(z_u - z'_u)}{m} \\sum_{k=1}^m \\nabla_v L(s + \\frac{k}{m}(z - z'))$\nwhere m is the number of steps used to approximate the integral, and z and z' represent the clean and corrupted inputs. EAP-IG addresses the issue of near-zero gradients in important features, providing a more accurate estimation of edge importance and a more faithful representation of the model's behavior. Both methods aim to identify the most crucial edges in a model's circuit, but EAP-IG achieves this with greater precision and reliability. EAP-IG-KL further runs EAP-IG with Kullback-Leibler (KL) divergence as the loss improves upon"}, {"title": "Method", "content": "We propose a novel mechanistic interpretation framework, SICAF, to trace and analyze the thought process that language models (LMs) employ during complex reasoning tasks. Our method comprises two primary steps: first, we apply an automatic circuit-finding approach to identify the critical circuits within the model; second, we calculate the self-influence $I_H(x, x)$ for each layer of the circuit to assess the contribution of individual tokens to the model's decision-making process. By examining contributions at each layer, we can infer the thought process manifested by the model during reasoning. This layer-wise self-influence analysis provides a detailed understanding of the internal reasoning process while ensuring computational feasibility by focusing exclusively on the model's most essential components.\nTo effectively locate the most important subgraphs, or circuits, in the model, we utilize advanced automatic circuit-finding methods, including Edge Attribution Patching (EAP), EAP-IG, and EAP-IG-KL. These methods allow us to identify and isolate the key circuits that contain essential features and their connecting weights, which are necessary for task completion. By focusing on these circuits rather than analyzing the entire model, we streamline the analysis and capture the primary information flow, thus attributing the model's output to specific components. This approach not only enhances efficiency but also enables us to focus on the most impactful areas of the model, laying a strong foundation for subsequent self-influence analysis.\nIn our approach, EAP identifies important edges by measuring the change in the loss function when each edge is perturbed, effectively constructing a \"map\" of the critical connections within the network. EAP-IG and EAP-IG-KL extend this by incorporating Integrated Gradients (IG) and Kullback-Leibler (KL) divergence, respectively. EAP-IG improves fidelity by more accurately capturing the importance of edges with low gradients, while EAP-IG-KL leverages KL divergence to ensure that the circuits faithfully represent the model's behavior under various interventions. Together, these methods allow us to efficiently locate the most faithful circuits in the network, ensuring that only the most relevant parts of the model are included in the analysis.\nOnce critical circuit components have been isolated, the key remaining step is to interpret the computations performed by these components. Few methods have been proposed to interpret extracted circuits. Kevin Wang et al. (Wang et al., 2022) explored this by knocking out a single node-a (head, token position) pair in the circuit-revealing"}, {"title": "Experiment", "content": "Experimental Setting\nDataset. We use the IOI dataset (Wang et al., 2022), designed to evaluate models' ability to perform indirect object identification tasks. Each entry consists of sentences with names and contexts, requiring the model to accurately predict the indirect object. The dataset contains minimal pairs of clean and corrupted inputs for direct comparison, testing the model's robustness in distinguishing between potential candidates, even when distractor names are introduced. For more dataset and metric details, see Appendix A.1.\nBaselines. As SICAF is a mechanistic interpretation framework, we mainly implement it with previ-"}, {"title": "Node Distribution Across Model Layers", "content": "As shown in Figure 2, the node distribution across model layers varies significantly between methods. EAP primarily activates nodes in the final layers (especially Layer 12), indicating that the model relies heavily on these layers to make final decisions in the IOI task. This pattern suggests that EAP emphasizes the semantic aggregation and decision generation occurring in the later layers. In contrast, EAP-IG and EAP-IG-KL demonstrate a more balanced node distribution across the early, middle, and final layers, with EAP-IG-KL showing substantial node activity between Layers 9 and 12. This pattern indicates that EAP-IG-KL captures more complex, multi-layered computational processes, utilizing information from a broader range of layers. Overall, the three methods share a consistent structure, with nodes concentrated in the first and last few layers, suggesting that these layers play a crucial role. The first layer likely helps in initial processing of input, while the last few layers appear critical for aggregating information before"}, {"title": "The Model's Thought Process", "content": "Figures 3 and 4 provide insights into the layer-wise processing differences among the methods:EAP, EAP-IG, and EAP-IG-KL,specifically regarding their handling of self-influence scores. As shown in Figure 3, EAP concentrates self-influence scores predominantly in the final layers, indicating a focus on quickly aggregating high-level information. This approach makes EAP suitable for tasks where immediate decision-making is crucial, as it enables rapid synthesis of key contextual elements. However, this reliance on the final layers limits EAP's capability to capture intermediate reasoning steps, which are essential for handling more nuanced and multi-step reasoning tasks.\nIn contrast, Figure 4 illustrates that EAP-IG and EAP-IG-KL distribute influence scores more evenly across early, middle, and final layers, enabling a structured and gradual accumulation of information. This balanced distribution aligns well with tasks requiring multi-step reasoning, as it supports the retention and transformation of information throughout the model's layered structure. EAP-IG-KL, in particular, achieves a high level of balance, ensuring stable self-influence scores across layers. This feature suggests that EAP-IG-KL is not only more robust in handling complex reasoning tasks but also better equipped to leverage information from both lower and higher layers.\nThe common hierarchical reasoning path followed by all three methods is best illustrated using the sentence \u201cThen, Christina and Amy went to the restaurant. Amy gave a ring to...\u201d as an example. Here, in the early layers, key entities like \"Christina\" and \"Amy\" are identified, setting a foundational context that informs subsequent reasoning steps. Moving to the middle layers, the model interprets the action verb \u201cgave,\u201d constructing relationships that frame \u201cAmy\u201d as the active entity in giving an item to another person. In the final layers, the model synthesizes this accumulated information, allowing it to conclude that \u201cAmy\" is the subject and infer the likely recipient.\nThe distinction among methods lies in the nuances of this shared reasoning path. EAP's emphasis on final-layer influence results in faster decision-making but may overlook subtler contextual nuances essential for complex inferences. On the other hand, EAP-IG distributes influence with a greater emphasis on intermediate layers, focusing on refining relational structures and contextual relationships as the reasoning pathway progresses. EAP-IG-KL exhibits the most balanced distribution, making it highly adaptable for tasks that require intricate relationships and consistent reasoning across all layers.\nThe detailed self-influence scores in Table 2 further support this analysis. For instance, in Layer 0, the token \u201cChristina\u201d exhibits a high self-influence score, reinforcing its role as a key contextual marker that informs initial reasoning. As the model advances, tokens such as \"gave\" and \"Amy\" demonstrate increased influence in the final layers, highlighting their relevance in constructing the concluding inference. Additional token influence scores across layers and for other samples are available in Appendix C, providing a comprehensive view of each method's impact across the reasoning process."}, {"title": "Conclusion", "content": "We propose a new mechanistic interpretation framework, SICAF, to trace and analyze the thought processes that language models (LMs) employ during complex reasoning tasks, and we validate our approach on the GPT-2 model in the IOI reasoning task. By applying circuit analysis and self-influence functions, we successfully mapped the reasoning pathways within the GPT-2 model during the IOI task. Our method reveals a hierarchical structure in the model's reasoning process, distinguishing key entities and relationships in a manner that resembles human reasoning steps. Overall, our findings contribute to a more interpretable and sys-"}, {"title": "Limitations", "content": "While our study successfully elucidates certain thought processes within language models, it has some limitations. First, the analysis was conducted primarily on the GPT-2 model and may not generalize to larger or different architectures without adaptation. Additionally, calculating self-influence requires computationally intensive methods, which may pose scalability challenges for more complex models. Finally, our work focused on a single task (Indirect Object Identification, or IOI), and the applicability of these findings to other natural language processing tasks remains an open question. Future research should explore the adaptability of this approach across varied tasks and model architectures, as well as investigate methods to optimize computational efficiency."}]}