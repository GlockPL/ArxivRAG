{"title": "THE SAME BUT DIFFERENT: STRUCTURAL SIMILARITIES AND DIFFERENCES IN MULTILINGUAL LANGUAGE MODELING", "authors": ["Ruochen Zhang", "Qinan Yu", "Matianyu Zang", "Carsten Eickhoff", "Ellie Pavlick"], "abstract": "We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? Using English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results provide new insights into how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously.", "sections": [{"title": "1 INTRODUCTION", "content": "As large language models (LLMs) have become the undisputed state of the art for building English language technology, there is decided interest in replicating their success across the full range of human languages. However, very little is known about the internal structure of LLMs, and whether such structure is conducive to acquiring broad multilingual capabilities. In fact, recent research has produced seemingly contradictory findings, such as evidence that multilingual models adopt language-specific representations (Tang et al., 2024; Choenni et al., 2024), while simultaneously showing good transfer across languages even in cases that would appear to have no superficial similarities that can be exploited to aid such transfer (Pires et al., 2019). Given the importance of building technology for diverse languages, there is a need for a more precise understanding of how LLMs represent structural similarities and differences across languages, and whether such representations accord with our intuitive understanding of how languages work.\nIn this paper, we employ tools from the growing subfield of mechanistic interpretability in order to ask whether the internal structure of LLMs show correspondence to the linguistic structures which underlie the languages on which they are trained. We focus on only the most minimal criteria of correspondence. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? While these questions seem simple, their answers are non-obvious. LLMs readily employ overlapping circuitry for tasks that do not necessarily seem \u201cthe same\" to humans (Merullo et al., 2024), and at the same time, neural networks frequently differentiate concepts due to surface form variation."}, {"title": "2 ANALYSIS METHODS", "content": "In this work, we are interested in analyzing how large language models (LLMs) trained in different languages differ in terms of the algorithms and mechanisms they invoke to handle various aspects of language processing. To do this, we employ a few recently developed analysis techniques, described below. These techniques are similar in spirit, but differ in certain details that matter for our analysis. For the most part, we find converging evidence for the paper's main claims across both techniques. When results differ in interesting ways, we comment in our results sections."}, {"title": "2.1 PATH PATCHING", "content": "Path patching (Wang et al., 2023; Goldowsky-Dill et al., 2023; Vig et al., 2020; Hanna et al., 2023; Tigges et al., 2023b) has become the most standard and widely-accepted technique within the still-new subfield of mechanistic interpretability. The goal of path patching is to localize specific circuits within the weights in a trained neural network that play a causal role in model behavior. The setup requires a pair of contrastive inputs, one referred to as the clean input and the other as the corrupted input.\nPath patching caches the activations for both inputs and then replaces the values of individual heads on the clean input with the values those heads would have taken had they been run on the corrupted input. In this way, the method aims to find the specific important head which maximally explains the final logits. Working backward, i.e., through patching the important heads at each layer, path patching has been used to identify full circuits that carry out the task. On its own, path patching only identifies important heads. To gain insight into the specific functions of these heads, path patching is usually used with logit attribution (Nostalgebraist, 2020; Belrose et al., 2023; Dar et al., 2023; Yu et al., 2023) which projects activations into the vocabulary space, as well as with bespoke analysis techniques invented by prior work to explain specific types of heads, such as duplicate-token detection heads or copy heads (Wang et al., 2023).\nThe advantage of path patching is primarily its wide adoption, which makes it easier to trust results, and enables us to compare with prior work in order to vet the results we are seeing (i.e., checking that we reproduce prior work when we expect to do so). The primary downside is that the method"}, {"title": "2.2 INFORMATION FLOW ROUTES", "content": "Given the aforementioned drawbacks of path patching, we also employ the information flow routes method (Ferrando & Voita, 2024; Tufanov et al., 2024). At each timestep, this method computes the contribution of each head to the residual streams. These heads are then aggregated over the entire attention block to construct a graph of the information flow that show which residual stream at which layer is important to the current residual stream value update across different timesteps. In this method, different from path patching where we can see circuit components across layers, the importance can only be computed every two layers. Compared to path patching, information flow routes have the advantage of not requiring minimal pairs of inputs. The tradeoff is that (1) the method is new and thus we do not benefit from clear expectations about what it should yield in certain settings and (2) this method tends to discover more generic components and larger circuits. In our paper, we thus use both information flow routes, and path patching along with logit attribution in conjunction for analysis at different granularity levels. Unless otherwise stated, both methods produce results that are consistent with the primary claims we make in each experimental section."}, {"title": "3 TASKS WITH COMMON LINGUISTIC STRUCTURE ACROSS LANGUAGES", "content": null}, {"title": "3.1 QUESTIONS", "content": "We first ask whether large language models (LLMs) will learn to use shared circuitry for different languages, specifically for aspects of language processing that reflect similar structures across languages. That is: if two distinct languages use similar morphosyntactic structure but realize it using different lexical items, will an LLM treat these as the same (handling them with the same abstract internal circuit) or as different (handling them with distinct, language-specific circuits)? We ask this question first in the case of multilingual models, and second in the case of monolingual models."}, {"title": "3.2 INDIRECT OBJECT IDENTIFICATION (IOI) TASK", "content": "We set up our data following the original IOI paper (Wang et al., 2023). IOI consists of sentences such as \"Susan and Mary went to the bar. Susan gave a drink to [BLANK]\u201d, in which the model is expected to predict Mary.\nThese names are tokenized as one single token in the models. To diversify the data, we use fifteen different templates for the actions. We also use ChatGPT to translate the templates and names into Chinese and have them manually inspected by native speakers.\nWang et al. (2023) identifies a specific circuit within GPT2 (Radford et al., 2019) for performing the IOI task. At a high level, the circuit reveals that the model runs the following algorithm: first, it identifies that there is a duplicated name (in the above example, Susan). Second, it inhibits attention to this duplicated name. Third, it copies the remaining (non-duplicated) name. These steps are carried out by a set of functionally specialized attention heads: Previous Token Heads, Duplicate Token Heads, Induction Heads, S-inhibition Heads, and Name Mover Heads. Duplicate Token Heads attend to the token position of Susan and identify this name token is mentioned twice in the sentence. S-inhibition Heads inhibit the Name Movers' attention to both occurrences of Susan. The Name Mover Heads output the remaining name (Mary). More detailed descriptions of these heads are given in prior work (Wang et al., 2023; Merullo et al., 2024). For our purposes, what is important is whether the same heads are important and perform the same roles across languages."}, {"title": "3.3 MODELS", "content": "We use BLOOM-560M\u00b9 (Workshop et al., 2022) as our multilingual model. We first evaluate whether BLOOM is able to complete the task by checking if it always prefers the correct name"}, {"title": "3.4 RESULTS ON MULTILINGUAL MODEL", "content": "We first validate whether the English and Chinese tasks share the same important heads. We use the information flow routes technique across 50 examples respectively in English and Chinese. We calculate the activation frequency for every head component. If one head is highly activated then it indicates this head is important for the task. In Figure 1, we observe a high overlap in the important"}, {"title": "3.5 RESULTS ON MONOLINGUAL MODELS", "content": "While the cross-lingual circuit overlap in a multilingual model is not obvious (i.e., it requires ab-stracting over patterns that manifest using disjoint sets of surface forms), it is also reasonable to assume that the convergence stems from the model's broader multilingual training data and objec-tive. We thus ask whether similar patterns arise even when models are trained independently in English and in Chinese. For this analysis, as mentioned, we use the English-only model GPT2-small and a distilled version of the Chinese-only model CPM-Generate (Zhang et al., 2020) for their exact same architecture."}, {"title": "4 TASKS WITH LANGUAGE-SPECIFIC STRUCTURE", "content": null}, {"title": "4.1 QUESTIONS", "content": "The above results show that LLMs are capable of recognizing structural parallels across languages, and even that monolingual models, trained without explicit influence from other languages, converge on similar algorithms for similar structures. We next ask: what happens when languages exhibit dissimilar structures? In reality, languages exhibit immense morphological and syntactic variation, and there is never a perfect one-to-one correspondence between the structural elements of different languages. A straightforward example of this phenomenon is the fact that English includes morphological markers for tense (e.g., walk vs. walked) while Chinese does not. We thus ask, given such variation, do LLMs adopt different circuitry for different languages, or rather do they continue to invoke shared circuitry but employ language-specific subroutines as needed?"}, {"title": "4.2 PAST TENSE TASK", "content": "We construct minimum pair templates in English: \u201cNow I verbpresent. Yesterday I also verbpast.\" and \u201cYesterday I verbpast. Now I also verbpresent.\u201d. For Chinese, we construct the exact same template in"}, {"title": "4.3 MODELS", "content": "We use Qwen2-0.5B-instruct7 (Yang et al., 2024) for the task for its better benchmark performance compared to BLOOM. The final verb collection contains 62 English verb instances and their Chi-nese translation and themselves are both single tokens after being processed by the Qwen tokenizer. For the English task, Qwen reaches 96.77% accuracy for the normal input and 58.06% for the zero-rank rate. In Chinese, as there are no minimal pairs, we can only obtain a zero-rank rate of 25.81%."}, {"title": "4.4 RESULTS", "content": "In Figure 4, compared to the active heads distribution on the IOI task, we notice that some early-layer heads have high activation frequencies that are shared in both English and Chinese tasks. However, there are much fewer shared heads in the later layers of the model. Specifically for the Chinese task, we observe that almost no heads have high activation frequency in the later layers (Layer 19-20). We posit that some heads that are shown to be more active in these later layers are responsible for the additional inflection rule unique to the English task in our setting, which is unnecessary to the Chinese task."}, {"title": "4.5 FUNCTION-SPECIFIC HEADS", "content": "Through path patching, we observe that heads 19.4 and 19.59 exert the most significant influence on the final logits (Refer to Appendix C for details). Examining the top promoted words from the most positive projections into the model vocabulary space (Figure 4 (c)), we find that they generally favor past-tense words in English (e.g., was, were, and other past-tense verbs unrelated to the target verb). Among the top-promoted tokens, we also notice Chinese verbs with the suffix\u201d\u4e86\u201d (e.g., \u8fdb\u884c\u4e86,can be translated to \"done\".), where\u201d\u4e86\u201d is commonly used to indicate the completion of an action. Even when provided with Chinese input, these heads continue to strongly encode past-tense concepts. In Figure 4(a), head 19.4 appears frequently activated for English but not for Chinese, suggesting that these heads are predominantly engaged in English processing rather than in Chinese.\nTo validate our findings, we ablate the heads 19.4 and 19.5 and analyze their impact on the tasks. The rank of correct past tense verbs in English remains relatively unchanged. Whereas other non-relevant past tense verbs move backward slightly to make spaces for the corresponding present tense verbs, resulting in their ranks getting promoted by an average of 83.21 positions. In the Chinese task, the rank of the correct verb moves forward by an average of 4.58 positions. Notably, after ablating these past-tense heads and projecting the final-layer logits to the vocabulary space, present-tense verbs emerge as the second most probable token (See Figure 11 for details). These results suggest that in English, past-tense heads actively suppress present-tense verbs to disambiguate between verb tenses. However, these heads do not play a similar role in the Chinese task.\nIn Ferrando & Voita (2024), they mention that patching can only emphasize the heads that are important for the original tasks but not the contrastive baseline. In the past tense task, we locate past tense heads that are in charge of disambiguating past tense verbs from present tense ones via path patching. These heads are discovered due to the additional morphological markers between the original and contrastive sentence. However, how do models know to promote that specific verb (with or without the marker) without confusing it with others?\nTo address this question, we compute the total score of each head projecting onto either past tense or present tense verb groups (see Appendix C.1). The primary heads identified are 21.3, 21.4, and 21.13. We observe that these heads not only emphasize different morphological variants of the target verb but also promote tokens that are orthographically similar or semantically related, such as synonyms and antonyms (see Figure 11 in Appendix). These heads are crucial for both English and Chinese tasks. To assess their impact, we perform ablation studies on these heads and observe significant impacts on model performance. In English, the average rank of past tense verbs drops by 141.95, while present tense verbs experience a much larger average demotion of 221.39. This disparity suggests that past tense verbs are less affected, possibly because the identified past tense heads remain active, suppressing present tense verbs. As shown in Figure 11, the top-promoted words after ablation are still predominantly general past tense verbs. For Chinese, the model performance also declines, with verbs in general experiencing an average rank demotion of 201.82. These ablation results indicate that the copy heads are essential for both English and Chinese tasks, while the past tense heads are specifically activated in English, where they are crucial for task completion. However, qualitative inspection also suggests that the semantics of the verb is not entirely decoupled from the tense, highlighting a need for further work in order to produce more nuanced descriptions of the function of these heads in this circuit, and in general."}, {"title": "4.6 FEED-FORWARD NETWORKS", "content": "Previous work (Merullo et al., 2023) discusses the role of the feed-forward networks (FFN) in re-trieving the correct transformation required for the past tense tasks. Specifically, these FFNs are found in the last few layers of the models where the ranked position of the present-tense verb and past-tense verb switch. Comparing the English and Chinese tasks, we assume that these FFN lay-ers play a role in predicting the past-tense verb in English but remain unactivated for the Chinese task. When we ablate out the FFNs from layer 20-24, the accuracy on the English task drops from 97.44% to 47.44% and the zero-rank rate from 58.06% to 17.74%. However, for the Chinese task, the zero-rank rate is barely changed (25.08% to 24.19%).\nIn the English example, when examining top predicted tokens qualitatively in Figure 5, we see that the rank of the correct past tense answer visited moved back by 1 position after ablating the MLP layers. Additionally, it is worth noting that morphological variants (E.g. visit, visits, etc.) or semantically relevant tokens (E.g. \u53c2\u89c2, \u8bbf\u95ee11) move to the top predictions. These are also tokens that are actively promoted by the copy heads mentioned earlier. In the Chinese example, we see that the top token as the correct prediction still remain at the same position, unaffected by the FFN layers removal."}, {"title": "5 RELATED WORK", "content": "Mechanistic Interpretability We built on previous work on mechanistic interpretability to reverse engineer the mechanism of neural network. Circuits are a significant paradigm of model analysis that has emerged from this field. Originated from the vision model (Olah et al., 2020) and continued to transformer language model (Meng et al., 2023; Wang et al., 2023; Hanna et al., 2023; Varma et al., 2023; Merullo et al., 2024; Lieberum et al., 2023; Tigges et al., 2023a), works in mechanistic interpretability try to characterize the function of individual components in completing certain tasks. Beyond the work in understanding the role of attention heads (Olsson et al., 2022; Chen et al., 2024; Singh et al., 2024; Gould et al., 2023; McDougall et al., 2023a), efforts are made in understanding neurons and feed-forward network in (Vig et al., 2020; Finlayson et al., 2021; Sajjad et al., 2022; Gurnee et al., 2023; Voita et al., 2023; Merullo et al., 2023). These studies mainly use causal methods with minimal pairs to locate the components and their functionalities (Vig et al., 2020; Chan et al., 2022; Geiger et al., 2021; 2023; Meng et al., 2023; Wang et al., 2023; Chan et al., 2023; Cohen et al., 2023). However, these methods heavily rely on manual inspection, which limits their scalability to larger models. Therefore other recent works have attempted to automate the causal process (Conmy et al., 2023; Bills et al., 2023; Syed et al., 2023; Hanna et al., 2024) or use input attribution to trace"}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "We investigate when and how large language models (LLMs) encode structural similarities between languages. Specifically, we ask (1) whether linguistic tasks that are structurally similar across languages are handled by the same LLM circuitry and (2) whether linguistic tasks that employ language-specific structure are handled by specialized LLM circuitry. Through a series of exper-iments employing different tasks and models in both English and Chinese, we find that indeed cross-lingual structural similarity is represented within the LLM using shared circuits which in-voke the same algorithm independent of language. We find that these shared algorithms emerge even in monolingual models trained on different languages. We also show that when tasks require language-specific components-specifically past tense morphology in English\u2013LLMs localize the corresponding processing such that it plays a causal role in some languages but not in others.\nTaken together, our results provide a first step in using mechanistic analysis about LLMs inner workings in order to ask higher-level questions about how linguistic structures are reflected in state-of-the-art AI models. Our results are limited to specific tasks and datasets, future work will be necessary to replicate and generalize our findings. Even so, our studies yield several interesting and encouraging insights which can inspire new lines of inquiry.\nFor example, we find that the same circuit emerges across different models trained on entirely dif-ferent languages. This raises questions about what type of structure is shared across otherwise-dissimilar languages, or even shared universally? That is, what regularities exist in the distribution of both English and Chinese such that they give rise to the same highly specific functional com-ponents organized in the same way? While the details of this discussion are specific to LLMs, the spirit shares much with long-running linguistic discussions of universality and of learnability in lan-guage (Yang, 2004). Deeper cross-disciplinary collaborations could possibly exploit LLMs in order to gain fresh perspectives on such questions. Another important direction concerns practical consid-erations for developing state-of-the-art technology in languages other than English. This includes not only building LLMs in other languages (Wu et al., 2024), but also ensuring their safety and ro-bustness (Yong et al., 2023; Deng et al., 2023) across languages. Insights into specific mechanisms that are shared between languages could guide more principled approaches to transferring parame-"}]}