{"title": "GraphFSA: A Finite State Automaton Framework for Algorithmic Learning on Graphs", "authors": ["Florian Gr\u00f6tschla", "Jo\u00ebl Mathysa", "Christoffer Rauna", "Roger Wattenhofera"], "abstract": "Many graph algorithms can be viewed as sets of rules that are iteratively applied, with the number of iterations dependent on the size and complexity of the input graph. Existing machine learning architectures often struggle to represent these algorithmic decisions as discrete state transitions. Therefore, we propose a novel framework: GraphFSA (Graph Finite State Automaton). GraphFSA is designed to learn a finite state automaton that runs on each node of a given graph. We test GraphFSA on cellular automata problems, showcasing its abilities in a straightforward algorithmic setting. For a comprehensive empirical evaluation of our framework, we create a diverse range of synthetic problems. As our main application, we then focus on learning more elaborate graph algorithms. Our findings suggest that GraphFSA exhibits strong generalization and extrapolation abilities, presenting an alternative approach to represent these algorithms.", "sections": [{"title": "1 Introduction", "content": "While machine learning has made tremendous progress, machines still have trouble generalizing concepts and extrapolating to unseen inputs. Large language models can write spectacular poems about traffic lights, but they still fail at multiplying two large numbers. They do not quite understand the multiplication algorithm since they do not have a good representation of algorithms. We want to teach machines some level of \"algorithmic thinking.\" Given some unknown process, can the machine distill what is going on and then apply the same algorithm in another situation? This paper concentrates on one of the simplest processes: finite state automata (FSA). An FSA is a basic automaton that jumps from one state to another according to a recipe. FSAs are the simplest, interesting version of an algorithm. However, if we assemble many FSAs in a network, the result is remarkably powerful regarding computation. Indeed, the simple Game of Life is already Turing-complete.\nBuilding on the work of Grattarola et al. [6] and Marr and H\u00fctt [17], this paper presents GraphFSA (Graph Finite State Automaton), a novel framework designed to learn a finite state automata on graphs. GraphFSA extracts interpretable solutions and provides a framework to extrapolate to bigger graphs, effectively addressing the inherent challenges associated with conventional methods. To better understand the capabilities of GraphFSA, we evaluate the framework on various cellular automata problems. In this controllable setting, we can test the model's abilities to extrapolate and verify that GraphFSA"}, {"title": "2 Related work", "content": "Finite state automaton. A Finite State Automaton (FSA), also known as a Finite State Machine (FSM), is a computational model used to describe the behavior of systems that operates on a finite number of states. It consists of a set of states, a set of transitions between these states triggered by input symbols from a finite alphabet, and an initial state. FSAs are widely employed in various fields, including computer science, linguistics, and electrical engineering, for tasks such as pattern recognition [16], parsing, and protocol specification due to their simplicity and versatility in modeling sequential processes. Additionaly, there is a growing interest in combining FSA with neural networks to enhance performance and generalization, a notion supported by studies by de Balle Pigem [1] on learning FSAs. Additional research by Mordvintsev [18] shows how a differentiable FSA can be learned.\nCellular automaton. Cellular Automata (CA) are discrete compu- tational models introduced by John von Neumann in 1967 von Neu- mann and Burks [29]. They consist of a grid of cells, where each cell is in a discrete state and evolves following transition rules based on the states of neighboring cells. In our context, learning these tran- sition functions from data and modeling the appropriate domain is"}, {"title": "3 The GraphFSA framework", "content": "We present GraphFSA, a computational framework designed for exe- cuting Finite State Automata (FSA) on graph-structured data. Draw- ing inspiration from Graph Cellular Automata and Graph Neural Net- works, GraphFSA defines an FSA at each node within the graph. This FSA remains the same across all nodes and encompasses a pre- determined set of states and transition values. While all nodes abide by the rules of the same automaton, the nodes are usually in dif- ferent states. As is customary for FSAs, a transition value and the current state jointly determine the subsequent state transition. In our approach, transition values are computed through the aggregation of neighboring node states. As the FSA can only handle a finite num- ber of possible aggregations, we impose according limitations on the aggregation function. For the execution of the framework, nodes are"}, {"title": "3.1 Formal definition", "content": "More formally, the GraphFSA F consists of a tuple (M, Z, A, T). F is applied to a graph G = (V, E) and consists of a set of states M, an aggregation A and a transition function T. At time t, each node v \u2208 V is in state sv,t \u2208 M. In its most general form, the aggregation A maps the multiset of neighboring states to an aggregated value a \u2208 Z of a finite domain.\n\n$a_{v,t} = A({{S_{u,t} | u \\in N(v)}})$\n\nHere {{}} denotes the multiset and N(v) the neighbors of v in G. At each timestep t, the transition function T: M \u00d7 Z \u2192 M takes the state of a node sv,t and its corresponding aggregated value av,t and computes the state for the next timestep.\n\n$S_{v,t+1} =T(S_{v,t}, a_{v,t})$\n\nFor notational convenience, we also define the transition matrix T of size M \u00d7 |Z| where Tm,a stores T(m, a). Moreover, we introduce the notion of a state vector s for each node v \u2208 V at time t, which is a one-hot encoding of size |M| of the node's current state.\nAggregation functions. The transition value a for node v at time t is directly determined by aggregating the multi-set of states from all neighboring nodes at time t. The aggregation A specifies how the aggregated value is computed from this neighborhood information. Note that this formulation of the aggregation A allows for a general framework in which many different design choices can be made for a concrete class of GraphFSAs. Throughout this work, we will focus on the counting aggregation and briefly touch upon the positional aggregation.\nCounting aggregation: The aggregation aims to count the occur- rence of each state in the immediate neighborhood. However, we want the domain Z to remain finite. Note that due to the general graph topology of G, the naive count could lead to Z growing with n the number of nodes. Instead, we take inspiration from the dis- tributed computing literature, specifically the Stone Age Computing Model by Emek and Wattenhofer [3]. Here, the aggregation is per- formed according to the one-two-many principle, where each neigh- bor can only distinguish if a certain state appears once, twice, or more than twice in the immediate neighborhood. Formally, we can generalize this principle using a bounding parameter b, which de- fines up to what threshold we can exactly count the neighbor states. The simplest mode would use b = 1, i.e., where we can distinguish if a state is part of the neighborhood or not. Note that this is equivalent to forcing the aggregation to use a set instead of a multi-set. For the general bounding parameter b, we introduce the threshold function \u03c3, which counts the occurrences of a state in a multi-set.\n\n$\\sigma(m, S) = min(b, |{{s | s = m, s\\in S}}|)$"}, {"title": "3.2 Expressiveness", "content": "While the execution of GraphFSA may bear resemblance to one round of the Weisfeiler-Lehman test, this does not hold in general. In the example Figure 1, they are the same because the cho- sen GraphFSA-instantiation is powerful enough to distinguish all occurrences of states appearing in the neighborhoods. More specif- ically, nodes can distinguish if they have 0 or more than one neighbor in a certain state (\"1+ aggregation\"), which is sufficient for the given graph. In general, the aggregation function A distin- guishes GraphFSA from the standard WL-refinement notion as it restricts the observation of the neighborhood from the usual multi- set observations (as these cannot be bounded in size). Consequently, GraphFSA is strictly less expressive than 1-WL, resulting in a trade- off between using a more expressive aggregator and maintaining the simplicity or explainability of the resulting model, particularly as the state space grows exponentially. Figure 2 presents an example com- paring GraphFSA's execution with a 2+ aggregator to WL refine- ment. Here, we can observe that GraphFSA is less expressive than 1-WL. However, for a large enough b, GraphFSA can match 1-WL on restricted graph classes of bounded degree. In conclusion, while the finite number of possible aggregations restricts GraphFSA, it also keeps the mechanism simple. Our evaluation demonstrates that this can improve generalization performance for a learned GraphFSA."}, {"title": "3.3 Visualization and interpretability", "content": "The GraphFSA framework offers inherent interpretability, facilitat- ing visualization akin to a Finite State Automaton (FSA). This visu- alization approach allows us to easily explore the learned state tran- sitions and the influence of neighboring nodes on state evolution. We outline two primary visualization techniques for GraphFSA:\nComplete FSA visualization. This method provides a comprehen- sive depiction of the entire FSA representation within the GraphFSA model, encompassing all states and transitions. Each node in the graph corresponds to a distinct state, while directed edges represent transitions between states based on transition values. An example is visualized in Figure 5.\nPartial FSA visualization. Tailored for targeted analysis of the GraphFSA model's behavior, this approach proves especially useful when dealing with large state spaces or focusing on specific behav- iors. This method visualizes a selected starting state by evaluating the model across multiple problem instances and tracking all transi- tions that occur. Then, the visualization is constructed by exclusively displaying the states and transitions that have been used from the specified starting state."}, {"title": "3.4 GRAB: The GraphFSA dataset generator", "content": "Accompanying the GraphFSA execution framework, which outlines the general interface and underlying principles shared amongst in- stances of GraphFSA, we provide GRAB, a flexible dataset generator to evaluate learned state automatons. Note that the introduced frame- work specifies the model structure but allows for different training methodologies that can be used to train specific model instances. The discrete nature of state transitions makes training these mod- els a challenging task. This work proposes one possible way to train such models using input and output pairs. For alternative approaches, we deem it crucial to provide a systematic tool that can be used to develop new training strategies and further provide a principled way of thoroughly assessing the effectiveness across training meth- ods. Therefore, GRAB provides an evaluation framework for learn- ing state automatons on graphs. GRAB generates the ground truth for a synthetic FSA, which can be used for learning and testing on various graphs. In particular, GRAB provides out-of-the-box support for several graph sizes and supports testing on multiple graph distri- butions.\nIn the dataset construction process, we first define the characteris- tics of the ground truth GraphFSA, specifying the number of states and configuring the number of initial and final states. Next, we gen- erate a random Finite State Automaton (FSA) by selecting states for each transition value, adhering to the constraints of the final state set (final states cannot transition to other states). We then sample graphs from predefined distributions, offering testing on a diverse set of graph types such as trees, grids, fully connected graphs, regular graphs, and Erdos-Renyi graphs. The starting states for each node within these generated graphs are randomly initialized from the set S. Finally, we apply the GraphFSA model to these input graphs to compute the outputs according to the defined state transitions. For the dataset generation, we emphasize evaluating generalization, which we believe to be one of the essential abilities of the GraphFSA frame- work. Therefore, we establish an extrapolation dataset to assess the model's capability beyond the graph sizes encountered during train- ing. This dataset contains considerably larger graphs, representing a substantial deviation from the training data. However, if the cor- rect automaton can be extracted during training, good generalization"}, {"title": "4 Empirical evaluation", "content": "The GraphFSA framework is designed to extract state automaton representations with discrete states and transitions to learn to solve the task at hand through an algorithmic representation. Our evaluation consists of two key components to showcase this. In the first part, we focus on classic cellular automaton problems. These automatons serve as a foundational component of our study as they represent the simplest possible version of an algorithm. However, despite their simplicity, they can becomes increasingly powerful and exhibit complex behaviour when assembled together in a network. We successfully learn the underlying automaton rules, demonstrating the ability and advantage of the GraphFSA framework to capture simple algorithmic behaviour.\nIn the second part of our empirical study, we evaluate our models using our proposed graph automaton benchmark and a set of more challenging graph algorithms. GRAB enables a comprehensive in- vestigation of GraphFSA's performance across a broad range of prob- lems. We are particularly interested in the generalisation ability of the GraphFSA Framework. Therefore, we perform extrapolation experi- ments where we train on smaller graphs and subsequently test its per- formance on larger instances. Finally, we use the GraphFSA frame- work to learn a selection of elementary graph algorithms, demon- strating the framework's capability and potential for algorithm learn- ing."}, {"title": "4.1 Training", "content": "We take inspiration from prior research on training differentiable finite state machines [18] and propose an adaption to train Diff-FSA, which follows the GraphFSA framework, on general graphs. To maintain differentiability, we relax the requirement for choos- ing a single next state for a transition in T during training and instead model it as a probability distribution over possible next states, resulting in a probabilistic automaton. We thus compute P(X = m | M \u00d7 Z) for m \u2208 M and parameterize Diff-FSA with a matrix T' of size M \u00d7 Z \u00d7 M that holds the probabilities for all possible transitions and states. This means that the transition matrix T' contains the probabilities of transitioning from a state m\u2081 to a state m2 given a specific transition value. To train the model with a given step number t, we execute Diff-FSA for t rounds and compute the loss based on the ground-truth output states of"}, {"title": "4.2 Classical cellular automata problems", "content": "As a first step towards learning algorithms, we focus on cellular au- tomata probelms. We consider a variety of different settings such as 1D and 2D automata, these include more well known instances such as Wireworld or Game of Life, which despite their simple rules are known to be Turing complete. For a more detailed explanation of the datasets, we refer to the Appendix.\n1D-Cellular Automata The 1D-Cellular Automata dataset con- sists of one-dimensional cellular automata systems, each defined by one of the possible 223 rules. Each rule corresponds to a distinct mapping of a cell's state and its two neighbors to a new state.\nGame Of Life. The GameOfLife dataset follows the rules defined by Conway's Game of Life [4]. A cell becomes alive if it has ex- actly 3 live neighbors. If a cell has less than 2 or more than 4 live neighbors, it dies, otherwise it persists. Furthermore, we consider a a variation where cells are hexagonal instead of the traditional square grid, which we refer to as the Hexagonal Game of Life. A visual representation is depicted in Figure 4.\nWire World. WireWorld [2] is an automaton designed to simulate the flow of an electron through a circuit. Cells can take one of four states: empty, electron head, electron tail, and conductor. Figure 4 shows an initial grid starting state and the evolution of WireWorld over multiple time steps."}, {"title": "4.2.1 Results", "content": "For Cellular Automata problems, we generate training data for each problem by initializing a 4x4 input grid or path of length 4 (for 1D problems) with random initial states and apply the problem's rules for a fixed number of steps to produce the corresponding output states. Our models are trained on these input-output pairs, using the same number of iterations as during dataset creation to ensure that we can extract the rule. Additionally, we test on an extrapolation dataset consisting of larger 10x10 grids. This dataset is used to evaluate the model's ability to adapt to varying rule iterations with four different iteration numbers.\n1D Cellular Automata. On this dataset, Diff-FSA uses the \"po- sitional neighbor-informed aggregation\" aggregation technique. No- tably, our model can successfully learn the rules for one-step training data (t=1). For the complete results, we refer to the Appendix.\n2D Cellular Automata. For 2D Cellular Automata (CA) problems, Diff-FSA employs the counting aggregation. For Wireworld, we use a state space of four and a bounding parameter of three. We train all models to learn the one-step transition rules. During training, all models and baselines report high accuracy. We then investigate the generalization capabilities and especially the iteration stability of the learned models. We run the trained models on larger 10xs10 grids for more time steps t than during training. The results are depicted in Figure 6. Note that the recurrent GNN and GNCA both deteriorate outside the training distribution. However, Diff-FSA exhibits good iteration stability across the whole range of iterations. For the Game of Life variations, we provide a detailed comparison in the Appendix."}, {"title": "4.3 Evaluation with GRAB", "content": "We use GRAB to create diverse datasets to benchmark the different models. These datasets encompass different graph types and sizes and generate a synthetic state automaton to generate the ground truth. This allows us to precisely control the setup by adjusting the number of states of the ground truth automaton. For each specific dataset configuration, we train 10 different model instances. Extended results with more runs can be found in the Appendix."}, {"title": "4.4 Graph algorithms", "content": "To showcase the potential of the GraphFSA framework for algorithm learning, we further evaluate it on more challenging graph algorithm problems. We use the same graph algorithms as Gr\u00f6tschla et al. [7], which, among other setups, includes a simplified distance computation and path finding. Detailed descriptions of these datasets can be found in the Appendix. Similar to the data generated by GRAB, our training process incorporates a random number of iterations, with the sole condition that the number of steps is sufficiently large to ensure complete information propagation across the graph. For problems involving small training graphs, we train the Diff-FSA model with approximately ten iterations and apply the additive final state loss to ensure iteration stability and establish distinct starting and final states.\nThe main aim is to validate that our proposed model is capable of learning more challenging algorithmic problems. It consistently performs better than GNCA. Compared to the recurrent GNN, it does not perform as well across all selected problems, but can learn correct solutions, as seen in the Distance task illustrated in Table 2. However, recall that Diff-FSAoperates on discrete states, making it much more challenging to learn. On the other hand, we can extract the learned solution and analyse the learned mechanics. A visualization of such a learned model is depicted in Figure 3 and Figure 5. This has the advantage over the recurrent baseline in that the learned mechanics"}, {"title": "5 Limitations and future work", "content": "The main advantage of the GraphFSA framework is its use of discrete states. This allows us to interpret the learned mechanics through the lens of a state automaton. Moreover, it can perform well in scenarios where the underlying rules of a problem can be modeled with discrete transitions while requiring that inputs and outputs can be represented as discrete states. This is the case for several algorithmic settings but limits the model's applicability to arbitrary graph learning tasks. To broaden the applicability of the GraphFSA framework, future work could investigate methods to map continuous input values to discrete states, potentially with a trainable module. Another aspect to inves- tigate is the study of more finite aggregations for GraphFSA. These can heavily influence a model's expressivity or align its execution to a specific task. Moreover, Diff-FSA only represents one possible approach to train models within the GraphFSA framework. Training models that yield discrete transitions remains challenging and could further improve performance and effectiveness."}, {"title": "6 Conclusion", "content": "We present GraphFSA, an execution framework that extends finite state automata by leveraging discrete state spaces on graphs. Our re- search demonstrates the potential of GraphFSA for algorithm learn- ing on graphs due to its ability to simulate discrete decision-making processes. Additionally, we introduce GRAB, a benchmark designed to test and compare methods compatible with the GraphFSA frame- work across a variety of graph distributions and sizes. Our evaluation shows that Diff-FSAcan effectively learn rules for classical cellular automata problems, such as the Game of Life, producing structured and interpretable representations in the form of finite state automata. While this approach is intentionally restrictive, it simplifies complex- ity and aligns the model with the task at hand. We further validate our methodology on a range of synthetic state automaton problems and complex algorithmic datasets. Notably, the discrete state space within GraphFSA enables Diff-FSAto exhibit robust generalization capabilities."}, {"title": "A Extended dataset descriptions", "content": "As a first interesting step towards learning algorithms, we focus on cellular automata probelms. We consider a variety of different set- tings such as 1D and 2D automata, these include more well known instances such as Wireworld or Game of Life, which despite its sim- ple behaviour is known to be Turing complete.\n1D-Cellular Automata The 1D-Cellular Automata dataset con- sists of one-dimensional cellular automata systems, each defined by one of the possible 256 rules. Each rule corresponds to a distinct mapping of a cell's state and its two neighbors to a new state. Figure 7 provides a graphical illustration of one such automata. Note, that we need todistinguish between the left and right neighbors in order to capture all rules in GraphFSA."}, {"title": "Game Of Life", "content": "The GameOfLife dataset captures the essence of Conway's Game of Life that progresses based on its initial state and a set of simple rules.\nThe progression of Conway's Game of Life is dictated by a set of simple rules applied to each cell in the grid, considering its neigh- bors. Using the Moore neighborhood, which includes all eight sur- rounding cells, these rules are as follows:\n1. Birth: A cell that is dead will become alive if it has exactly three living neighbors.\n2. Survival: A cell that is alive will remain alive if it has two or three living neighbors.\n3. Death:\n(a) Loneliness: A cell that is alive and has fewer than two living neighbors will die, mimicking underpopulation.\n(b) Overpopulation: A cell that is alive and has more than three living neighbors will die, representing overpopulation.\nToroidal vs. non-toroidal For this dataset, we consider both toroidal and non-toroidal variations:\n1. Toroidal: In the toroidal variation, the board's edges wrap around, creating a continuous, closed surface. This means cells on the edge have neighbors on the opposite edge.\n2. Non-Toroidal: In the standard, non-toroidal variation, cells on the board's edge only consider neighbors within the boundary.\nOur dataset consists of input/output pairs where we randomly ini- tialize the grid and then apply the Game of Life rules for a fixed number of steps. We represent this dataset through grid graphs and use the Moore neighborhood."}, {"title": "Hexagonal Game Of Life", "content": "The Hexagonal Game Of Life intro- duces a variation where cells are hexagonal as opposed to the tradi- tional square grid. This change in cell structure offers a fresh set of neighbour cells, which can lead to distinct patterns and evolutions. A visual representation can be found in Figure 4."}, {"title": "Wire World", "content": "The WireWorld dataset revolves around the cellular automaton 'Wire World' where cells can take one of four states: empty, electron head, electron tail, and conductor. It's especially renowned for its capability to implement digital circuitry. In the dataset, we observe the evolution of a given cellular configuration over specified iterations."}, {"title": "A.2 Graph algorithms", "content": "Taking inspiration from the datasets utilized by Gr\u00f6tschla et al. [7], we create datasets for various classical graph problems. To ensure versatility and scalability, we generate new graphs for each problem instance and compute the corresponding ground truth during dataset creation. This approach enables us to construct datasets that not only encompass graphs of specific sizes but also facilitate evaluation on larger extrapolation datasets. We explore the following graph prob- lems that helps us to explore different capabilities of our model."}, {"title": "A.2.1 Distance", "content": "The distance problem involves determining whether each node in the graph has an even or odd distance from the root node. To formulate this problem, we define input values for each problem instance, rep- resenting each node's state in the graph. Among these nodes, one is designated as the root, while the others are marked as non-root inputs. The output is assigned a binary value (0 or 1) for each node based on distance mod 2 from the root, where \"distance\" represents the length of the shortest path between the root and a node."}, {"title": "A.2.2 RootValue", "content": "In the root value problem, we want to propagate a value from the root throughout a path graph. One node in the graph is assigned a root label and a binary value (0,1). The objective is to propagate this binary value from the root across the entire graph."}, {"title": "A.2.3 PathFinding", "content": "The PathFinding problem determines whether a given node lies on the path between two marked nodes within a tree. The dataset com- prises different trees, and two nodes are explicitly marked as relevant nodes within each tree. The objective is to predict whether a specific node in the tree, which is not one of the labeled nodes, lies on the path connecting these two marked nodes."}, {"title": "A.2.4 PrefixSum", "content": "The PrefixSum Dataset involves paths represented as sequences of nodes where each has an initial binary feature (either 1 or 0). The task is to predict the sum of all initial features to the right of each node in the path, considering modulo two arithmetic."}, {"title": "B Additional model evaluation", "content": ""}, {"title": "B.1 1D cellular automata", "content": "We evaluate the models on 1D Cellular Automata evaluation for the different baselines and consider a larger graph during and multiple timesteps t for our evaluation. We report the results in Table 3 for models that were trained on t = 1 and in Table 4 for models trained on t = 2."}]}