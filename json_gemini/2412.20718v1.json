{"title": "M\u00b3oralBench: A MultiModal Moral Benchmark for LVLMS", "authors": ["Bei Yan", "Jie Zhang", "Zhiyuan Chen", "Shiguang Shan", "Xilin Chen"], "abstract": "Recently, large foundation models, including large language models (LLMs) and large vision-language models (LVLMs), have become essential tools in critical fields such as law, finance, and healthcare. As these models increasingly integrate into our daily life, it is necessary to conduct moral evaluation to ensure that their outputs align with human values and remain within moral boundaries. Previous works primarily focus on LLMs, proposing moral datasets and benchmarks limited to text modality. However, given the rapid development of LVLMs, there is still a lack of multimodal moral evaluation methods. To bridge this gap, we introduce M\u00b3oralBench, the first MultiModal Moral Benchmark for LVLMs. M\u00b3oralBench expands the everyday moral scenarios in Moral Foundations Vignettes (MFVs) and employs the text-to-image diffusion model, SD3.0, to create corresponding scenario images. It conducts moral evaluation across six moral foundations of Moral Foundations Theory (MFT) and encompasses tasks in moral judgement, moral classification, and moral response, providing a comprehensive assessment of model performance in multimodal moral understanding and reasoning. Extensive experiments on 10 popular open-source and closed-source LVLMs demonstrate that M\u00b3oralBench is a challenging benchmark, exposing notable moral limitations in current models. Our benchmark is publicly available\u00b9.", "sections": [{"title": "1. Introduction", "content": "With rapid advancements in artificial intelligence, large foundation models, including large language models (LLMs) and large vision-language models (LVLMs), have become indispensable tools across fields such as healthcare, law, and finance [1-3]. As these models take on an increasing part in decision-making and daily applications, it is necessary to conduct moral evaluation to ensure that their outputs align with human values and remain within moral boundaries [4]. Therefore, evaluating the inherent morality of these models has emerged as an urgent task.\nMorality has long been a prominent topic in psychology, with a large number of research emerging in the field of moral psychology. Moral Foundations Theory (MFT) [5] stands out as a widely accepted theoretical framework, proposing that core fundamental moral values-developed through evolutionary processes to address social and environmental needs-underpin human morality. With continued development and refinement, the theory identifies six moral foundations: Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression [5, 6]. MFT has significant influence on psychology and other related fields, leading to the development of psychometric tools such as Moral Foundations Questionnaire (MFQ) [7], Sacredness Scale (MFSS) [7], and Moral Foundations Vignettes (MFVs) [8], which facilitate cross-cultural and cross-ethnic studies of human moral identity.\nIn response to the need for moral evaluation of large foundation models, researchers have begun drawing from moral psychology, treating these models as subjects to explore their performance across various moral dimensions [9, 10]. Some studies have applied MFT to model evaluation, directly using or adapting psychological tools like MFQ to assess models' tendencies in specific moral dimensions [9, 11]. Other studies have developed independent morality taxonomy, designing their own moral evaluation datasets and benchmarks [10, 12-16]. However, existing moral evaluation methods primarily focus on LLMs and are limited to text modality, providing a preliminary view of language models' moral understanding. With the emergence of LVLMs, models are now capable of processing combined image and text inputs, allowing them to generate more nuanced moral responses that cross-modality boundaries. Text-only evaluation is insufficient to fully capture the moral judgements and behaviors of multimodal models in real-world scenarios, emphasizing the need for a new"}, {"title": "2. Related Work", "content": "2.1. Moral Foundations Theory\nMoral Foundations Theory (MFT) is a widely accepted theoretical framework in moral psychology [5]. It posits that human moral intuitions stem from several foundational moral dimensions. The first version identifies five moral foundations: Care/Harm (dislike for suffering of others), Fairness/Cheating (proportional fairness, Loyalty/Betrayal (group loyalty), Authority/Subversion (respect for authority and tradition), and Sanctity/Degradation (concerns for purity and contamination) [8]. With development and refinement, Liberty/Oppression, which emphasizes concerns on oppression and coercion [6], has been expanded as the sixth foundation. The multidimensional moral value system of MFT provides theoretical support for understanding human morality. Building on MFT, researchers have developed a range of psychometric tools to quantitatively"}, {"title": "2.2. Moral Evaluation Datasets and Benchmarks", "content": "To ensure models operate within human moral norms, researchers have developed various moral evaluation datasets and benchmarks. ETHICS [12], an early effort in this field, consists of crowdsourced moral judgements in contextualized scenarios. Scruples [13] provides a large-scale collection of real-life anecdotes and ethical dilemmas sourced from the internet, designed to test models' abilities to make judgements and respond to complex ethical situations. MoralExceptQA [15] introduces a challenge set for moral judgements on cases that involve potentially permissible moral exceptions. MoralBench [11] adapts MFQ and MFVs to offer a more nuanced assessment of alignment across different moral dimensions. MoralChoice [10] evaluates models by presenting low-ambiguity and high-ambiguity moral scenarios to assess responses. CMoralEval [14], which includes Chinese moral anomalies collected from TV programs and newspapers, examines model responses to diverse moral situations, especially for Chinese language models. Similarly, DAILYDILEMMAS [16] explores model responses to complex ethical dilemmas, analyzing the values behind the chosen actions. However, these datasets and benchmarks are primarily designed for LLMs and are limited to text-only modality. To bridge the gap for multimodal moral evaluation benchmark, we propose M\u00b3oralBench."}, {"title": "2.3. Large Vision-Language Models", "content": "Building on the success of LLMs, LVLMs have rapidly advanced, achieving remarkable visual perception and reasoning capabilities. Researchers continue to develop state-of-the-art LVLMs through various methods. For example, LLaVA [19] introduces instruction tuning into the multimodal domain, establishing as one of the most mature open-source multimodal models. GLM-4V [20] enhances large language models' visual understanding and question-answering capabilities by integrating visual information. InternLM-XComposer2-VL [21] demonstrates powerful cross-modal reasoning through interactions between multilayered visual encoders and language generation modules. MiniCPM-Llama2-V2.5 [22] combines LLaMA2 with visual encoding modules to achieve streamlined yet efficient multimodal understanding. mPLUG-Owl2 [23], Phi-3-Vision [24], Qwen-VL [25], and Yi-VL [26] further propelled the development of LVLMs. Additionally, many powerful closed-source LVLMs, such as Gemini-1.5-Pro [27] and GPT-40 [17], have released their APIs, driving advancements in downstream applications. We conduct experiments on the above LVLMs to comprehensively evaluate their multimodal moral performance."}, {"title": "3. M\u00b3oralBench", "content": "3.1. Overview\nIn this paper, we propose the first multimodal moral benchmark, M\u00b3oralBench, which adopts Moral Foundations Theory (MFT) as theoretical framework. We expand the scenarios in Moral Foundations Vignettes (MFVs), creating 1160"}, {"title": "3.2. Image Generation", "content": "We generate moral scenario images based on MFVs. As introduced in Section 2.1, MFVs consist of 116 scenarios depicting violations of moral foundations and 16 scenarios depicting violations of social conventions, each of which is a brief description of a behavior. As our focus on moral evaluation, we only select the 116 scenarios involving moral violations. \nTo expand the dataset and improve generalizability, we use GPT-40 to replicate these scenarios, creating 10 diverse versions with identical core events but different characters and locations for each scenario, resulting in a total of 1,160 moral scenarios. \nAs the original scenarios are relatively brief, we employ GPT-40 to transform each one into a detailed image generation prompt, including specific details about locations and characters. To clarify the main character's role as the central figure, especially in scenarios involving multiple characters, we also leverage GPT-40 to add a line of dialogue for the main character (the subject of the scenario description). This addition also addresses the limitations of conveying emotions through images alone, as the dialogue provides extra character information and contextual details, enhancing the understanding of the main character's perspective. \nWe utilize the advanced text-to-image diffusion model, SD3.0, to convert the generated prompts into images. To ensure high-quality outputs, we generate 10 candidate images for each scenario and manually select the image that best aligns with the scenario's description. Specifically, we discard images with quality issues, such as extra limbs, cloned faces or missing arms, retaining those where the characters and locations match the prompt, and the relationships between the main characters are clearly depicted.\nWe incorporate the dialogues through speech bubbles. Since it is challenging to directly instruct the generation model to produce images with precise speech bubbles, we"}, {"title": "3.3. Instruction Design", "content": "Existing moral evaluation benchmarks mainly fall into two categories: moral judgement, where the model determines whether a behavior is morally acceptable or identifies which person is morally wrong [11-13, 15]; and moral response, where the model is asked to chooses the action that aligns with moral norms in different contextual scenarios [10, 16]. To enhance the depth and breadth of evaluation, our benchmark introduces an additional task: moral classification. \nMoral Judgement. Moral judgement task evaluates whether the model can accurately determine if the behavior depicted in the scenario image is morally wrong. Since all generated scenarios involve moral violations, to ensure a balanced dataset and enable thorough assessment, we create a contrasting \"morally acceptable\" version for each moral scenario image by replacing the main character's original dialogue with a morally neutral one. The model is then required to judge these contrasting images simultaneously, allowing us to assess its ability to detect subtle moral differences in similar scenarios and further evaluate its sensitivity and accuracy in moral judgement.\nMoral Classification. Moral classification task assesses whether the model can recognize the specific moral foundation violated by the behavior in the scenario image. The options include the six moral foundations (e.g., Care/Harm, Fairness/Cheating) as well as a distractor \"not morally wrong\". We aim to measure the model's understanding of these moral foundations and its capability to apply this understanding in complex multimodal contexts.\nMoral Response. In moral response task, the model is asked to make a choice among two potential responses within a given moral scenario. Specifically, the model is instructed to assume the role of the speaking character in the image and must choose between the previously generated morally violating line and the morally neutral line. Unlike the first two tasks, which explicitly prompt the model to perform intentional moral evaluation, this task is designed to examine whether the model would unintentionally select immoral answers, offering insight into its intrinsic moral tendencies and moral values behind its choices."}, {"title": "3.4. Evaluation Metric", "content": "Moral tasks are inherently complicated and diverse. We aim to accurately capture the model's underlying tendencies under different moral scenarios, assessing the model's moral preferences and consistency more reliably. To achieve this, we quantify the model's likelihood of choosing each option in a given multimodal scenario by calculating the probability of each choice, thus providing a reliable foundation for subsequent analysis.\nDue to the computational complexity involved in mapping token space to option space and the fact that some closed-source model APIs do not provide direct access to token probabilities[10], we estimate these probabilities through Monte Carlo sampling [30]. Specifically, for the LVLM with parameters \u03b8, we sample M responses {\u03b1\u2081, ..., \u03b1M } by \u03b1\u2c7c ~ p\u03b8(a\u1d62,\u209c) for each image-instruction pair (i, t) \u2208 I \u00d7 T. The likelihood of choosing a specific option o\u1d62,\u209c is quantified as follows:\np\u03b8(o\u1d62,\u209c) = 1/M \u03a3\u2c7c=\u2081M I[\u03b1\u2c7c = o\u1d62,\u209c] (1)\nWe consider the option with the highest probability as the model's preference option for each image-instruction pair and calculate the accuracy as the evaluation metric. Higher accuracy indicates better abilities in moral understanding and reasoning, with greater alignment between the model's intrinsic moral inclinations and human moral standards. The average accuracy across the three tasks is considered as the overall score."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nWe evaluate our MoralBench on 8 popular open-source LVLMS, GLM-4V [20], Phi-3-Vision [24], mPLUG-Owl2 [23], Yi-VL [26], LLaVA_1.5 [19], MiniCPM-Llama2-V2.5 [22], InternLM-XComposer2-VL [21], and Qwen-VL [25], as well as 2 powerful closed-source LVLMS, GPT-40 [17] and Gemini-1.5-Pro [27]. For each image-instruction pair in the benchmark, we set M = 5, i.e., generating 5 responses from each model, to obtain their preference options. The temperature is set to 1.0 during evaluation to control the randomness of the generated responses."}, {"title": "4.2. Evaluation Results", "content": "Overall Performance. Table 2 illustrates the moral evaluation results of all LVLMs on M\u00b3oralBench. Overall, closed-source models outperform open-source models, with GPT-4o achieving the highest score, followed by Gemini-1.5-Pro. This disparity can be attributed to the fact that closed-source models are primarily designed for commercial purposes, with a stronger emphasis on alignment with human values, particularly in addressing issues related to safety, ethics, and preventing harmful outputs. Commercial companies like OpenAI and Google typically implement dedicated moral review mechanisms to ensure that their models' outputs adhere to ethical standards. As a result, closed-source models generally demonstrate superior performance in moral evaluations.\nAmong open-source models, GLM-4V demonstrates the best overall performance, achieving close results in moral response task to closed-source models. During the evaluation, we observe that several open-source models struggle to consistently adhere to instructions. We think it may be due to the challenging and novel nature of multimodal moral\nAll task instructions are presented as multiple-choice questions. To mitigate the effect of potential position bias in LVLMs [28, 29], i.e., the tendency to choose a certain option in multiple-choice questions, we ensure that the arrangement of options is randomized in all instructions and that the reference options are evenly distributed."}, {"title": "5. Discussion", "content": "Ethical and Social Concerns. Our research performs a multimodal moral evaluation of LVLMs, probing potential moral issues within these models. All images in our benchmark are generated by SD3.0, and we have manually verified that they contain no identifiable data or depictions of explicit violence or gore, ensuring no adverse impact on individuals or communities. While the benchmark includes moral violation scenarios that may be offensive or evoke discomfort, our aim is to provide new insights into the inherent moral understanding in LVLMs. We expect that our work will contribute to the development of reliable and safe AI models that are highly aligned with human values.\nLimitations. There exists certain limitations in our benchmark. On one hand, our moral scenario images are generated by the text-to-image diffusion model, SD3.0. Although SD3.0 is one of the most advanced generative models available, its outputs can still exhibit issues, such as extra limbs or cloned faces. We have manually filtered out images with obvious quality problems during the generation process to mitigate these effects. Secondly, we convey scene information through a multimodal approach by adding textual dialogue to the images, rather than using purely visual input. This design choice prevents us from isolating the influence of each modality on the model's moral abilities. Additionally, our instructions are relatively limited, relying solely on closed-ended multiple-choice questions without including open-ended VQA. We will explore ways to address these limitations in future work."}, {"title": "6. Conclusion", "content": "In summary, we propose M\u00b3oralBench, to our knowledge, the first multimodal moral evaluation benchmark for LVLMs. M\u00b3oralBench is grounded in Moral Foundation Theory and builds upon the scenarios in Moral Foundation Vignettes that depict violations of six distinct moral foundations. We expand this set of scenarios using GPT-4o and generate corresponding moral scenario images with SD3.0. To provide a comprehensive evaluation of models' inherent morality, we design instructions for three distinct moral tasks: moral judgement, moral classification, and moral response. Our extensive experimental results on ten prominent open-source and closed-source LVLMs indicate that closed-source models currently outperform open-source models in moral ability. However, there is still room for improvement, particularly in understanding and adhering to the moral foundations of Loyalty/Betrayal and Sanctity/Degradation."}, {"title": "7. Moral Foundations Theory", "content": "7.1. Moral Foundations\nWe provide a more detailed explanation of the six moral foundations [5, 6] as follows:\n\u2022 Care/Harm: This foundation arises from the evolutionary need to care for vulnerable offspring. It is triggered by visual and auditory signs of suffering, distress, or neediness, primarily from one's own children but also from other children, animals, or even representations like stuffed toys. It underpins virtues such as kindness and compassion while opposing cruelty, and its expression varies across cultures.\n\u2022 Fairness/Cheating: Rooted in the need for reciprocal relationships, this foundation is evolved to detect cheating and cooperation. It motivates tit-for-tat responses and fairness judgements, extending beyond direct interactions to include third-party evaluations and even inanimate exchanges. It promotes virtues like justice and trustworthiness.\n\u2022 Loyalty/Betrayal: Emerging from the benefits of cohesive coalitions in intergroup competition, this foundation supports group loyalty and solidarity. Originally activated by tribal and intergroup dynamics, it now extends to modern phenomena like sports fandom and brand allegiance. Loyalty is praised, while betrayal is condemned.\n\u2022 Authority/Subversion: This foundation is based on navigating dominance hierarchies effectively to gain social advantages. It governs interactions with authority figures and institutions and is associated with virtues like obedience and deference in hierarchical societies. Its interpretation varies across cultures and political ideologies.\n\u2022 Sanctity/Degradation: Evolving as a behavioral immune system to avoid pathogens and parasites, this foundation is linked to disgust and reactions to impurity. It manifests in moral judgements about dietary practices, bodily integrity, and social deviance, often promoting virtues like temperance and chastity in certain cultures.\n\u2022 Liberty/Oppression foundation: This foundation centers on the feelings of reactance and resentment toward those who dominate or restrict individual freedom. It is motivated by the hatred of bullies and oppressors, driving people to unite in solidarity to resist or overthrow domination. While it often conflicts with the authority foundation, it fosters virtues such as independence, equality, and the courage to oppose injustice."}, {"title": "7.2. Moral Foundations Vignettes", "content": "Moral Foundations Vignettes (MFVs) [8] are carefully constructed scenarios designed to isolate and evaluate specific moral foundations by reflecting their core principles. These vignettes are tailored to represent plausible everyday events, avoiding overtly political or culturally bound content, and are formulated to encourage respondents to imagine themselves as third-party witnesses to moral violations. Compared to other moral questionnaires that may involve relatively abstract concepts, MFVs provide concrete, third-person scenarios rooted in everyday life. This specificity makes them particularly suitable for visualization, enabling the construction of moral scenario image for our benchmark.\nCare violation scenarios focus on three types of harm: emotional harm to humans, physical harm to humans, and physical harm to non-human animals.\nFairness violation scenarios emphasize instances of cheating or free-riding, such as dishonesty in work or academic settings.\nLoyalty violations are framed around individuals prioritizing personal interests over group loyalty. Groups are defined broadly to include family, country, or organizations, and scenarios feature public behavior that threatens group reputation.\nAuthority violations involve disobedience or disrespect toward authority figures (e.g., parents, teachers, judges) or institutions (e.g., courts, police).\nSanctity vignettes feature violations that evoke physical disgust, such as contamination concerns or sexually deviant acts. Examples include behaviors like eating a dead pet dog or urinating in a public pool.\nLiberty scenarios depict coercive actions or restrictions on freedom, typically imposed by those in positions of power (e.g., a boss or parent)."}, {"title": "8. Image Generation Details", "content": "8.1. Image Generation Prompts\nIn image generation process, we utilize GPT-40 [17] to expand the dataset by imitating similar scenarios from MFVs and converting these brief descriptions into detailed image prompts and main character dialogues. Additionally, to generate two contrasting versions of the scenarios, a morally wrong line and a morally neutral line of dialogue is created for each main character in the scenarios."}]}