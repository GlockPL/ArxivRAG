{"title": "From Simple to Professional: A Combinatorial Controllable Image Captioning Agent", "authors": ["Xinran Wang", "Muxi Diao", "Baoteng Li", "Haiwen Zhang", "Kongming Liang", "Zhanyu Ma"], "abstract": "The Controllable Image Captioning Agent (CapAgent) is an innovative system designed to bridge the gap between user simplicity and professional-level outputs in image captioning tasks. CapAgent automatically transforms user-provided simple instructions into detailed, professional instructions, enabling precise and context-aware caption generation. By leveraging multimodal large language models (MLLMs) and external tools such as object detection tool and search engines, the system ensures that captions adhere to specified guidelines, including sentiment, keywords, focus, and formatting. CapAgent transparently controls each step of the captioning process, and showcases its reasoning and tool usage at every step, fostering user trust and engagement. The project code is available at https://github.com/xin-ran-w/CapAgent.", "sections": [{"title": "Introduction", "content": "Image captioning [5, 3, 13, 4, 1, 6, 15, 12, 16, 17], a pivotal task in computer vision and natural language processing, aims to articulate the visual content of an image through text. It underpins a wide range of applications, including assistive technologies, content creation, and image understanding.\nWith the emergence of visual language models (VLMs), the ability to generate detailed image descriptions has seen significant advancements. Large multimodal models, such as GPT-40 [8], can produce richly detailed captions with a simple prompt, e.g., \u201cPlease describe this image in detail.\". However, such generic prompts lack constraints, often leading to captions that may not align with user preferences. One approach to addressing this is by crafting professional instruction with complex combinatorial constraints, e.g., \"within 80 words in Chinese\", \"if the image contains more than three objects using bullet format to describe each object\", to control the generated caption. While effective, creating such tailored instructions is time-consuming and cumbersome for general users. Additionally, these instructions often need to be customized based on the specific visual content of each image, adding another layer of complexity. To simplify this process, we apply instruction evolving to transform simple user instructions into context-aware professional instructions tailored to the visual content of the corresponding images.\nHowever, even with professional instructions, current large multimodal models face challenges in achieving fine-grained control over caption generation, particularly when handling combinatorial constraints across multiple descriptive aspects. To address this, we introduce CapAgent, an agent-based system equipped with a suite of tools designed to transform static caption generation into a dynamically controllable process. CapAgent empowers users to specify and enforce multiple types of constraints, as defined in prior work [18], such as: (1) Format constraint (e.g., caption length, caption format); (2) Semantic constraint (e.g., user-defined interests, important details of key objects, caption sentiment); (3) Lexical constraint (e.g., captioning the image with input keywords); (4) Utility constraint (e.g., sentiment, lexical choices).\""}, {"title": "Method", "content": "Our method has two steps. The first step, instruction evolving, helps users convert simple instructions into professional instructions. The second step is to apply an agent system, named CapAgent, to generate captions that follow professional instructions by using various tools. We'll go into the details of these two steps in the following section."}, {"title": "Instruction Evolving", "content": "When interacting with chatbots, general users tend to prefer providing simple instructions over complex ones [18]. However, simple instructions often lack sufficient detail, leaving the model uncertain about the user's exact needs. While complex instructions with combinatorial constraints can better articulate the user's intent, enabling the model to generate outputs more aligned with their expectations, crafting such detailed instructions is often time-consuming and burdensome for general users."}, {"title": "CapAgent", "content": "Although professional instruction may express user intent, MLLM may not generate a caption fully following the professional instruction. In this section, we introduce the CapAgent, an agent system with a variety of tools specifically designed to control the image captioning process. As shown in Figure 3, like most general agents, CapAgent's workflow includes three main steps: planning, tool usage, and observation. When the user inputs an image and a caption query, the CapAgent will generate a thought and a corresponding action to tackle the user request. In each action, the agent will generate a block of Python code and execute the code to observe the action result."}, {"title": "Thought, Action and Observation", "content": "Verbal reasoning is a fundamental ability of human intelligence [7]. Given user instruction s, our agent will generate the initial thought and action, to and ao. The action is a piece of Python code and it will be executed on a local Jupyter server. The execution result will be fed back to the CapAgent to generate the thought and action for the next step until all user requirements are met. Here, we use the ReAct [20] prompt to let the planner generate thought and action for each step."}, {"title": "Retrieval Augmented Planning", "content": "To enhance the precision of thought and action generation within our model, we have curated a set of thought, action, and observation chain examples to guide CapAgent in adapting to the task and producing outputs in an appropriate format. Given the limited context capacity of MLLM, we cannot embed all examples within the prompt, necessitating a selection process. We hypothesize that similar user demands should elicit similar thought processes from the agent. To this end, we employ Retrieval-Augmented Generation (RAG) [10, 9] to identify the top N most analogous examples based on the user's specific requirements.\nFormally, given a user instruction s and a database of chain-of-thought examples $C = {c_i}_{i=1}^N$, our objective is to extract a subset $C_s = {c_i}_{i=1}^{N_s}$ of size N from C that closely matches the user's specific requirements. We achieve this by first applying the document embedding model BGE-M3 [2] to embed each $c_i$ into a vector database, transforming each chain-of-thought example into a vector representation $v_i$. The user's instruction s is also embedded into the same vector space, resulting in $v_s$. The similarity between the user's instruction embedding $v_s$ and each thought chain example embedding $v_i$ is calculated using cosine similarity:\n$sim (v_s, v_i) = \\frac{v_s \\cdot v_i}{\\|v_s\\| \\|v_i\\|}$"}, {"content": "The top N thought chain examples with the highest similarity scores are selected to form the subset\n$C_s$. This can be represented as:\n$C_s = TopN (\\{c_i | sim (v_s, v_i)\\}_{i=1}^T)$"}, {"content": "After that, we put $C_s$ into the end of the system prompt to enable CapAgent to generate the correct thoughts and actions."}, {"title": "Tools for Combinatorial Caption Control", "content": "We have developed a comprehensive suite of tools to facilitate CapAgent's combinatorial control over the captioning process. These tools leverage the APIs of foundational models, including GPT-40 [8], GroundingDINO [11], and DepthAnything v2 [19].\nVisual question answering tool This tool is instrumental in generating basic captions and respond-ing to queries about objects within an image. It serves as the initial step in providing a descriptive narrative of the visual content.\nCaption sentiment modification tool This tool allows for the adjustment of caption sentiment. By providing an initial caption and a desired sentiment, it generates a new caption that aligns with the specified emotional tone while retaining the original content.\nCaption expansion tool When the length of the basic caption is shorter than the user requirement, CapAgent can use this tool to extend the caption content by asking and answering questions about the image until it meets the required caption length.\nCaption condensation tool Conversely, when the initial caption exceeds the specified length, this tool is used to condense the caption. It removes superfluous details, ensuring the caption remains concise and relevant.\nObject counting tool Designed to cater to the need for quantification, this tool enables CapAgent to count specific objects within an image. It is particularly useful when users require a numerical description of elements within the visual content.\nSpatial relation tool This tool is crucial for describing the spatial layout of images, especially when the content is confined to a specific space, such as a living room. It is especially invaluable for providing environmental context to vision-impaired individuals. Enabled by object detection and depth estimation vision expert models, it accurately portrays the spatial relationships within the image."}, {"title": "Visualization", "content": "The visualization examples in Figure 4 and Figure 5 demonstrate the model's ability to generate detailed and contextually rich captions for a variety of image categories, including historical events, movie posters, and other topics. Additionally, the professional instructions generated by CapAgent are tailored to meet the specific requirements based on the content of the image and the user's input. For instance, when processing a movie poster, the professional instruction explicitly specifies that the final caption should include keywords such as \u201cVenom 3\u201d and \u201cHD wallpapers\u201d, ensuring relevance to both the visual content and the context of the image. These captions not only describe the scenes but also capture the sentiment and key elements outlined in the professional instructions. As a result, they align with the user's expectations and provide a comprehensive visualization of the depicted events and products."}, {"title": "Conclusion", "content": "In conclusion, CapAgent showcases a powerful capability to bridge the gap between user-provided simple instructions and professional-grade image captions. By leveraging advanced tool integration and professional instruction generation, the system delivers contextually accurate, sentiment-aligned, and detail-rich descriptions for a wide range of images. This ensures a seamless alignment with user intent while maintaining precision and contextual relevance."}]}