{"title": "DiffStyleTTS: Diffusion-based Hierarchical Prosody Modeling for Text-to-Speech with Diverse and Controllable Styles", "authors": ["Jiaxuan Liu", "Zhaoci Liu", "Yajun Hu", "Yingying Gao", "Shilei Zhang", "Zhenhua Ling"], "abstract": "Human speech exhibits rich and flexible prosodic variations. To address the one-to-many mapping problem from text to prosody in a reasonable and flexible manner, we propose DiffStyleTTS, a multi-speaker acoustic model based on a conditional diffusion module and an improved classifier-free guidance, which hierarchically models speech prosodic features, and controls different prosodic styles to guide prosody prediction. Experiments show that our method outperforms all baselines in naturalness and achieves superior synthesis speed compared to three diffusion-based baselines. Additionally, by adjusting the guiding scale, DiffStyleTTS effectively controls the guidance intensity of the synthetic prosody.", "sections": [{"title": "Introduction", "content": "Speech synthesis, also known as text-to-speech (TTS), aims to turn text into almost human-like audio. Currently, most TTS models consist of three main components: a text analysis front-end, an acoustic model, and a vocoder. Among them, the naturalness and prosodic performance of speech primarily depend on the design of the acoustic model.\n\nThe acoustic model, at the heart of TTS, can be categorized as autoregressive and non-autoregressive. Autoregressive acoustic models, like Tacotron (Wang et al., 2017) and Transformer TTS (Li et al., 2019), have issues with word skipping, repeated reading and inference time increasing linearly with length of the Mel-spectrogram. Non-autoregressive acoustic models, like FastSpeech2 (Ren et al., 2021), excel in rapidly synthesizing high-quality speech. However, they are constrained by using a simple regression objective function for optimization, lacking probabilistic modeling, and the unimodal characteristics of Gaussian distribution don't conform to the true distribution of acoustic features, which affects the prediction accuracy. The mean of the distribution also results in the problem of over-smoothing predictions, which restricts the diversity of generated prosodic features. These issues lead to weak fluctuations and unnaturalness in prosodic transfer and control tasks, Additionally, traditional prosodic transfer methods like Global Style Tokens (GST) (Wang et al., 2018) lack controllability over the intensity of prosodic transfer.\n\nThe recently emerged diffusion model has significant advantages in describing the complex distribution of high-dimensional and multi-modal features. In particular, the guidance of the conditional diffusion model (Dhariwal and Nichol, 2021) can well control the results. It effectively addresses issues like over-smoothing predictions and a lack of diversity through multi-step sampling. Currently, acoustic models based on the diffusion model, such as Diff-TTS (Jeong et al., 2021), Grad-TTS (Popov et al., 2021), DiffSinger (Liu et al., 2022), Guided-TTS (Kim et al., 2022), ProDiff (Huang et al., 2022), CoMoSpeech (Ye et al., 2023), etc., primarily use the Mel-spectrogram as the prediction target. There have been limited studies on predicting speech prosodic features via the conditional diffusion model. DiffProsody(Oh et al., 2024) is a diffusion-based prosody prediction model that constrains prosodic features through a discriminator, but it still lacks controllability over prosody during inference. In summary, the flexible transfer and control of speech prosody still remains underexplored.\n\nTherefore, we propose a novel acoustic model, DiffStyleTTS, based on a conditional diffusion module and an improved classifier-free guidance (Ho and Salimans, 2021). It hierarchically models prosodic features using both coarse-grained style conditions and fine-grained prosodic descriptions, balances the diversity and quality of prosody"}, {"title": "DiffStyleTTS", "content": "In this section, we propose DiffStyleTTS, a multi-speaker acoustic model that employs hierarchical prosody modeling and utilizes FastSpeech2 as its backbone. As shown in Figure 1, the encoder and decoder use the feed-forward Transformer (FFT) of FastSpeech2, along with a 5-layer convolutional PostNet (Shen et al., 2018) in the decoder. We use an embedding lookup table to capture the unique vocal characteristics of each speaker and a HiFi-GAN vocoder (Kong et al., 2020) to synthesize speech waveforms. The main modifications are replacing FastSpeech2's original variance adaptor with a conditional diffusion module for hierarchical prosody modeling and introducing a GST module for style control."}, {"title": "Hierarchical Prosody Modeling", "content": "The DiffStyleTTS achieves hierarchical prosody modeling by considering prosodic features at two levels: coarse-grained implicit style conditions and fine-grained explicit prosodic descriptions. Implicit style conditions encompass broad descriptions of entire sentences, which are difficult to define intuitively and are encoded from the Mel-spectrogram during the training of the whole acoustic model. Explicit prosodic features include fine-grained prosodic descriptions of phonemes, such as pitch, energy, and duration, which can be directly and easily extracted from speech waveforms.\n\nIn DiffStyleTTS, the method of GST (Wang et al., 2018) is adopted to extract implicit style conditions from audio as shown in Figure 1. The implicit style conditions are decoupled into style"}, {"title": "The Conditional Diffusion Module", "content": "The conditional diffusion module is guided with implicit style conditions to predict explicit prosodic features that align with it. The guided generation of the conditional diffusion module can be usually categorized into two approaches: classifier guidance (Dhariwal and Nichol, 2021) and classifier-free guidance (Ho and Salimans, 2021). Classifier guidance requires an additional classifier, which slows down the inference speed, and its quality impacts the effectiveness of category generation. Therefore, DiffStyleTTS employs classifier-free guidance, which can avoid these issues as it doesn't require direct calculation of the classifier gradient.\n\nFirst, as shown in Figure 2, based on the mathematical principles of DDPM (Ho et al., 2020), the diffusion process of explicit prosodic features is defined by a fixed Markov chain from the initial data xo to the latent variable xt as\n\n$q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t | x_{t-1}),$ (1)\n\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I),$ (2)\n\nwhere $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$, $\\alpha_t = 1 - \\beta_t$, $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$, $t = 0, 1,\\dots, T$, and $T$ is the step size. When adding a small Gaussian noise at each step, the module selects a small positive constant $\\beta_t$ from a variance table, which we define as a cosine schedule (Nichol and Dhariwal, 2021) to prevent rapid noise accumulation from linear addition. Then, the reverse process is also defined by a Markov chain from $x_T$ to $x_0$ parameterized by the $\\theta$ as\n\n$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t),$ (3)\n\n$p_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)),$ (4)\n\nwhich shows the step-by-step denoising of an isotropic Gaussian noise $x_T \\sim N(0, I)$ to restore the original data $x_0$.\n\nTo guide the conditional diffusion module's output using classifier-free guidance, two denoisers with identical architectures are designed to employ"}, {"title": "Training and Inference", "content": "Referring to Figure 1 and Figure 3(a), processed phonemes from the text analysis front-end are fed into the text encoder to generate text embeddings. This, along with implicit style conditions from the GST module, is then input into the conditional diffusion module, which includes two denoisers $\\Psi_{\\theta_1}(x_t, t, y, c)$ and $\\Psi_{\\theta_2}(x_t, t, y)$ trained via Eq. (5)(6). During training, the log scales of raw phoneme-wise pitch and duration, and raw phoneme-wise energy, are targeted for sampling. The guiding scale $\\eta$ and the correction scale $\\gamma$ are not involved in the training process. We add the implicit style conditions and the embeddings of raw pitch and energy to the text embeddings, then employ the length regulator to align the length based on raw duration. Frame-wise speaker embeddings are added before feeding into the decoder. Finally, decoded Mel-spectrograms are converted into speech waveforms using the pre-trained HiFi-GAN vocoder. The total loss function includes the\n\ndiffusion module loss, the loss of decoding Mel-spectrograms, and the residual loss of PostNet\n\n$L_{total} = L_{diff\\_c(\\theta_1)} + L_{diff\\_nc(\\theta_2)} + L_{decoder} + L_{mel}.$ (11)\n\nReferring to Figure 1 and Figure 3(b), three main inference modes are designed based on the trained DiffStyleTTS model.\n\n(1) Diversified controllable inference. By tuning the guiding scale $\\eta$ and the correction scale $\\gamma$, we can adjust the diversity and guidance intensity of explicit prosodic features, achieving diversified and controllable prosody prediction.\n\n(2) Prosodic transfer inference. Given a reference utterance and a specified speaker ID, prosodic features are transferred from the reference utterance to this speaker. By tuning the guiding scale $\\eta$ and the correction scale $\\gamma$, we can adjust the intensity of prosodic transfer.\n\n(3) Prosodic control inference. Given a specified speaker ID and a token ID in the GST module, we can set the weights of other tokens to 0 and the weight of this token to 1, synthesizing prosody controlled only by that token. Besides, we allow for the flexible combination of style token weights, enabling the enhancement or diminishment of certain prosodic style. We can also scale the pitch, energy, and duration by multiplication with scaling factors to control prosodic values.\n\nAdditionally, we introduce a temperature hyperparameter $\\tau$ (Popov et al., 2021) to sample terminal condition $x_T$ from $N(0, \\tau^{-1}I)$ instead of $x_T$ from $N(0,I)$. Previous work has found that tuning $\\tau$ can help to improve the quality of output."}, {"title": "Experimental Setup", "content": "We evaluated the proposed DiffStyleTTS model using a 54-hour private Mandarin Chinese dataset comprised of recordings from 9 male speakers of different ages, all of which belonged to the genres of novel, narration or story reading. Our dataset has high recording quality and diverse prosodic styles. We randomly sampled 20 utterances from each speaker's recordings, and the total 180 utterances were reserved for validation and test, while the rest were used for training. All phoneme durations were extracted by an internal forced align"}, {"title": "Model Configuration", "content": "The encoder encoded phonemes to 256-D text embeddings using 4 FFT blocks. while the decoder used 6 FFT blocks. The 5-layer convolutional Post-Net in the decoder is comprised of 512 filters with shape 5\u00d71 with batch normalization, followed by tanh activations on all but the final layer. The architecture of two denoisers utilized a bidirectional dilated convolution (Kong et al., 2021) similar to WaveNet (Oord et al., 2016), for predicting waveform signals. It consists of a stack of 12 residual layers, each layer with residual channels $C = 3$ and a kernel size of 3. The input tensor had a shape of $[B, C, L]$, where $B$ was the batch size, $C$ was the residual channels, and $L$ was the length of phonemes. In the GST module, the token embeddings size was set to 256 and the token size was configured to 10. the multi-head attention with 4 attention heads used a softmax activation to output weights over the tokens."}, {"title": "Performance of Synthetic Speech", "content": "Subjective and objective evaluations were conducted to evaluate the performance of speech synthesized using DiffStyleTTS. In addition to the FastSpeech2 (Ren et al., 2021) baseline, a Fast-Speech2 model with ground truth phoneme-wise prosodic features, a Grad-TTS (Popov et al., 2021) model, a Guided-TTS (Kim et al., 2022) model and a DiffProsody (Oh et al., 2024) model were also built for comparison. First, the naturalness mean opinion scores (MOS) of all models were evaluated by a listening test. A total of 14 participants evaluated 18 utterances for each model, selecting two samples from each speaker. Second, the accuracy of predicted prosodic probability distributions were evaluated using Jensen-Shannon (JS) Divergence. Third, the efficiency of different models were evaluated using the real time factor (RTF). Fourth, different settings of $\\eta$ and $\\gamma$ used only for prosodic transfer and control can affect the MOS of DiffStyleTTS, as shown in Table 2. Therefore, we select the optimal configuration in this section. We employed the diversified controllable inference mode with $\\eta = 1.0$ and $\\gamma = 0.7$ in DiffStyleTTS. In all experiments, the step size T of all diffusion models was set to 200 to ensure rigor. Additionally, we ran multiple trials from different terminal conditions $x_T$ to sample various explicit prosodic features, and then averaged scores for final evaluation results.\n\nThe results are shown in the first six rows of Table 1. We can see that our proposed DiffStyleTTS outperformed all baselines in naturalness and JS Divergence. DiffStyleTTS also achieved faster synthesis speed than the two diffusion models using Mel-spectrograms as modeling targets."}, {"title": "Prosodic Control", "content": "To study the effect of the guiding scale $\\eta$ in classifier-free guidance on balancing prosodic diversity and quality, we selected an utterance with pronounced prosodic variations and synthesized it"}, {"title": "Prosodic Transfer", "content": "To evaluate the effect of DiffStyleTTS in prosodic transfer, we used FastSpeech2+GST, i.e., incorporating a GST module before the variance adaptor in FastSpeech2, and the DiffProsody as the baselines, we selected one reference utterance each from two speakers (A and B) with distinct prosodic styles. Then, we randomly selected two sentences each from the other eight speakers, excluding the reference speakers, resulting in a total of 16 sentences for prosodic transfer. The prosodic transfer inference mode was used to transfer the prosody of the reference utterance to the other eight speakers. We conducted a subjective preference (%) test involving 12 participants to compare the two models and calculated the p-value of t-test to assess significance of differences. As shown in Table 3, DiffStyleTTS significantly outperformed two baselines on this prosodic transfer task (p < 0.05). We also investigated the effect of the guiding scale $\\eta$ on the intensity of prosodic transfer, setting $\\gamma = 0.7$. The"}, {"title": "Ablation Studies", "content": "We conducted ablation studies to demonstrate the effectiveness of key components in DiffStyleTTS. The results are presented in the last three rows of Table 1.\n\nTo verify the guiding effect of implicit style conditions on explicit prosodic features, we can observe that removing the implicit style conditions c from the denoiser $\\Psi_{\\theta_1}(x_t, t, y, c)$, prohibiting the use of classifier-free guidance, i.e., w/o ISC, but retaining them added to text embeddings, led to a decrease in MOS and an increase in JS divergence. When the implicit style conditions added to the text embeddings were removed and only retained as the condition c in $\\Psi_{\\theta_1}(x_t, t, y, c)$, i.e., w/o AISC, we can also observe a decrease in MOS, which verifies the implicit style conditions contain prosodic features. Furthermore, removing the text embedding conditions y from the denoiser $\\Psi_{\\theta_1}(x_t, t, y, c)$, i.e., w/o TEC, resulted in poor quality of synthetic prosody, indicating the importance of text embedding conditions on prosodic alignment. In summary, experiments show that these key components contributed significantly to the performance of DiffStyleTTS."}, {"title": "Conclusion", "content": "This paper proposes a multi-speaker acoustic model, DiffStyleTTS, based on a conditional diffusion module and an improved classifier-free guidance. We hierarchically model prosodic features at both implicit and explicit levels. Text embeddings and implicit style conditions are combined as the diffusion module's conditions. To predict explicit prosodic features, the dynamic thresholding method is employed to improve classifier-free guidance and then adjust the guidance intensity. Experiments show that our proposed model achieves higher naturalness compared to all baselines, and faster synthesis speed compared to diffusion-based baselines. Additionally, DiffStyleTTS demonstrates superior prosodic transfer capabilities and flexibility in comprehensive prosodic transfer and control."}, {"title": "Limitation", "content": "Although our work on prosody prediction has made it more flexible and controllable, we haven't successfully decoupled prosody from speaker timbre. In addition, we can also observe some overlapping samples after dimensionality reduction, indicating that the tokens are not entirely independent and share common prosodic styles. This suggests that the implicit style conditions need further classification. To achieve better disentanglement of prosody will be a task for our future work."}]}