{"title": "MUSICONGEN: RHYTHM AND CHORD CONTROL FOR TRANSFORMER-BASED TEXT-TO-MUSIC GENERATION", "authors": ["Yun-Han Lan", "Wen-Yi Hsiao", "Hao-Chung Cheng", "Yi-Hsuan Yang"], "abstract": "Existing text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporally-conditioned Transformer-based text-to-music model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automatically-extracted rhythm and chords as the condition signal. During inference, the condition can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets-one derived from extracted features and the other from user-created inputs-demonstrates that MusiConGen can generate realistic backing track music that aligns well with the specified conditions. We open-source the code and model checkpoints, and provide audio examples online, https://musicongen.github.io/musicongen_demo/.", "sections": [{"title": "1. INTRODUCTION", "content": "The realm of text-to-music generation has seen significant progress over the recent years [1-11]. These models span various genres and styles, largely leveraging textual prompts to guide the creative process. There have been two primary methodological frameworks so far. The first employs Transformer architectures to model audio tokens [12] derived from pre-trained audio codec models [13-15]; noted examples include MusicLM [1] and MusicGen [2]. The second employs diffusion models to represent audio through spectrograms or audio features, such as AudioLDM 2 [4] and JEN-1 [5].\nText-to-music generation model generally relies on the global textual conditions to guide the music generation process. Textual prompts serving as high-level conceptual guides, however, introduce a degree of ambiguity and verboseness into the music generation for describing the musical features [7]. This inherent vagueness poses a challenge in precisely controlling temporal musical features such as melody, chords and rhythm, which are crucial for music creation. Building on the success of MusicGen-melody [2] in melody control, our focus now shifts to enhancing chord and rhythm control, aiming to create a more integrated approach to music generation that captures the full spectrum of musical elements.\nIn view of the limits of the prior works, we introduce in this paper MusiConGen, a Transformer-based text-to-music model that applies temporal conditioning to enhance control over rhythm and chord. MusiConGen is finetuned from the pretrained MusicGen framework [2]. We design our temporal condition controls in a way that it supports not only musical features extracted from reference audio signals, but also the aforementioned user-provided text-like symbolic inputs such as BPM value and chord progression. For effective conditioning of such time-varying features, we propose \"adaptive in-attention\" conditioning by extending the in-attention mechanism proposed in the MuseMorphose model [18].\nIn our implementation, we train MusiConGen on a dataset of backing track music comprising 5,000 text-audio pairs obtained from YouTube. This training utilizes beat tracking and chord recognition models to extract necessary condition signals without the need for manual labeling. We note that rhythm and chord controls are inherently critical for backing tracks, for backing tracks often do not include the primary melody and their purpose is mainly to provide accompaniment for a lead performer.\nMoreover, instead of using the adapter-based finetuning methods [16, 17, 19], we apply the straightforward \"direct finetuning\" approach to accommodate the domain shift from general instrumental music (on which MusicGen was trained) to the intended backing track music. We leave the use of adapter-based finetuning as future work. To make our approach suited for operations on consumer-grade GPUs, we propose a mechanism referred to as \"jump finetuning\" instead of finetuning the full MusicGen model.\nWe present a comprehensive performance study involving objective and subjective evaluation using two public-domain datasets, MUSDB18 [20] and RWC-pop-100 [21]. Our evaluation demonstrates MusiConGen's enhanced ability to offer nuanced temporal control, surpassing the original MusicGen model in producing music that aligns more faithfully with the given conditions.\nThe contributions of this work are two-fold. First, to our best knowledge, this work presents the first Transformer-based text-to-music generation model that follows user-provided rhythm and chords conditions, requiring no reference audio signals. Second, we present efficient training configuration allowing such a model to be built by finetuning the publicly-available MusicGen model with customer-level GPU, specifically 4x RTX-3090 in all our experiments. We open-source the code, checkpoint, and information about the training data of MusiConGen on GitHub. 1"}, {"title": "2. BACKGROUND", "content": "2.1 Codec Models for Audio Representation\nIn contemporary music generation tasks, audio signals are typically compressed into more compact representations using two main methods: Mel spectrograms and codec tokens. Mel spectrograms provide a two-dimensional time-frequency representation, adjusting the frequency axis to the Mel scale to better align with human auditory perception. Codec tokens, on the other hand, are often residual vector quantization (RVQ) tokens that are encoded from audio signals by a codec model [13-15]. Following MusicGen, we employ in our work the Encodec (32k) [14] as the pretrained codec model to encode audio data at a sample rate of 32,000 Hz. This Encodec model comprises 4 codebooks, each containing 2,048 codes, and operates at a code frame rate $f_s$ of 50 Hz.\n2.2 Classifier-Free Guidance\nClassifier-free guidance [22] is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier. This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage. The mathematical expression is as $\\nabla_x \\log p_\\theta(x|c) = (1-\\gamma)\\nabla_x \\log p_\\theta(x) + \\gamma\\nabla_x \\log p_\\theta(x|c)$. Here, $\\gamma$ represents the guidance scale, which adjusts the influence of the conditioning information. We perform a weighted average of $f_\\theta(x, c)$ and $f_\\theta(x)$ when sampling from the output logits.\n2.3 Pretrained MusicGen Model\nThe pretrained model used in our study is a MusicGen model with 1.5B parameters, equipped with melody control (i.e., MusicGen-Melody). The melody condition employs a chromagram of 12 pitch classes at a frame rate $f_M$, denoted as $M \\in \\mathbb{R}^{T_{f_M}\\times 12\\times 1}$, derived from the linear spectrogram of the provided reference audio. For text encoding, the model leverages the FLAN-T5 [23] as a text encoder to generate conditioning text embeddings, represented as $T \\in \\mathbb{R}^{T+5\\times d_{t5}\\times 1}$. Both the melody and text conditions undergo linear projection into a $D$-dimensional space before being prepended to the input audio embedding. Regarding the input audio for training, audio signals are initially encoded into RVQ tokens, $X_{rvq} \\in \\mathbb{R}^{T_{fs} \\times 1\\times 4}$, using the pretrained Encodec model. These tokens are then formatted into a \"delay pattern\" [2], maintaining the same sequence length. Subsequently, an embedding lookup table, $W_{emb} \\in \\mathbb{R}^{N\\times D\\times 4}$, where $N$ represents for numbers of codes in a codebook, is used to represent the associated codes, summing contributions from each codebook of $X_{rvq}$ to form the audio embedding $X_{emb} \\in \\mathbb{R}^{T_{fs}\\times D\\times 1}$. The input representation is then fed to the self-attention layers via additive sinusoidal encoding."}, {"title": "3. METHODOLOGY", "content": "Our method seeks to efficiently finetune the foundational MusicGen model using time-varying symbolic rhythm and chord conditions as guiding conditions. To achieve this, we must carefully consider both the representation of these conditions and the finetuning mechanism as follows:"}, {"title": "3.1 Representing Temporal & Symbolic Conditions", "content": "Chords. For chord condition, we employ two methods. The first prepend method is similar to the melody control method of MusicGen, denoted as $C_{pre} \\in \\mathbb{R}^{T_{f_M} \\times 12}$ where $C_{pre}$ maintains the same resolution (i.e. frame rate $f_M$ and sequence length) as MusicGen's melody condition $M$. This allows us to utilize the pretrained melody projection weights from MusicGen as initial weights. Furthermore, we have noted that chord transitions can lead to asynchronization issues. To address this, we introduce a second frame-wise chord condition, $C_{sum} \\in \\mathbb{R}^{T_{fs} \\times 12\\times 1}$, which matches the resolution of the audio codec tokens, thus providing a solution for the synchronization problem.\nRhythm. To control rhythm, we derive conditions from both the beat and the downbeat. The beat represents the consistent pulse within a piece of music, and the downbeat signifies the first and most emphasized beat of each measure, forming the piece's rhythmic backbone. We encode beat and downbeat information into one-hot embedding each at a frame rate of $f_s$. For the beat embedding, a soft kernel is applied to allow for a tolerance of 70ms. Subsequently, the beat and downbeat arrays are summed to yield the frame-wise rhythm condition $R \\in \\mathbb{R}^{T_{fs} \\times 1}$."}, {"title": "3.2 Finetuning Mechanisms", "content": "The finetuning mechanism we employ consists of two parts: 1) jump finetuning, and 2) an adaptive in-attention mechanism. As illustrated in Figure 1, our proposed model activates condition gates at the \"block\" level, treating four consecutive self-attention layers as a block.\nJump finetuning is designed to specifically target the first self-attention layer within each block for finetuning, while freezing the remaining three self-attention layers of the same block, as shown in Figure 1 (b). Doing so reduces the number of parameters of finetuning while maintaining the flexibility to learn to respond to the new conditions by refining the first self-attention layer per block.\nThe adaptive in-attention mechanism is designed to improve control over chords and rhythm. It is an adaptation of the in-attention technique of MuseMorphose [18], whose main idea is to augment every intermediate output of the self-attention layers with copies of the condition. Unlike the original implementation that augment all the self-attention layers, we selectively apply it to the first three-quarters of self-attention blocks (e.g., for a model with 12 blocks, in-attention is applied to first 9 blocks) to relax the control in the last few blocks for better balancing on rhythm and chords. This leads to better result empirically, as will be shown in Section 5.2 and Table 3."}, {"title": "4. EXPERIMENTAL SETUP", "content": "4.1 Datasets\nWe finetuned the model using a dataset of ~250 hours backing track music sourced from YouTube, comprising 5K songs across five genres: Rock, Funk, Jazz, Blues, and Metal, with 1K songs per genre. After preprocessing (see Section 4.2), the training data contained 80,871 clips."}, {"title": "4.2 Dataset Pre-processing Details", "content": "The training and evaluation datasets consist of full-song data, with durations ranging from 2 to 5 minutes per song. Below are the preprocessing details for each type of input:\nAudios: All audio data have vocals removed. For the training and RWC dataset, we employed the source separation model Demucs [24, 25] to eliminate the vocal stem. In the MUSDB18 dataset, which already features isolated stems, we combined the bass, drum, and others stems to form the dataset. Each song was segmented into 30-second clips, ensuring each clip starts at a downbeat.\nDescriptions: For the training set, the text prompts were simply extracted from the titles of the corresponding YouTube videos. For the two evaluation datasets, we tasked ChatGPT [26] to generate 16 distinct text prompts, covering the five genres included by the training set. Here is an example-\"A smooth acid Jazz track with a laid-back groove, silky electric piano, and a cool bass, providing a modern take on Jazz. Instruments: electric piano, bass, drums.\" At inference time, we randomly selected one of the 16 text prompts in a uniform distribution.\nChords: The RWC dataset comes with ground truth labeled chords. For both the training set and MUSDB18, we used the BTC model [27] as the chord extraction model to predict symbolic chords with time tags for each clip. The detailed chord quality extends to the seventh note. We then translated the extracted chord symbols with time tags into a 12-pitch chromagram in the order of C, C#, ..., B. The chromagram's frame rate for the frame-wise condition $C_{sum}$ is $f_s$, and for the prepend condition $C_{pre}$ it is $f_M$.\nRhythm: Except for RWC, beat and downbeat were extracted using the RNN+HMM model [28] from the Madmom library [29]. The timing format for beats and downbeats was transformed into a one-hot representation matching the audio token frame rate $f_s$. A soft kernel was applied to the one-hot beat array to create a softened beat array. The rhythm representation $R$ was the frame-wise summation of the softened beat array and downbeat array."}, {"title": "4.3 Training Configuration", "content": "The proposed rhythm and chord-conditioned Transformer was built upon the architecture of the medium-sized (1.5B) MusicGen-melody, featuring $L = 48$ self-attention layers with dimension $D = 1,536$ and 24 multi-head attention units. The condition dropout rate is 0.5 and guidance scale is set to be $\\gamma = 3$ for classifier-free guidance. We finetuned only a quarter of the full model, which corresponds to 352 million parameters, while keeping both the audio token embedding lookup table and the FLAN-T5 text encoder frozen. The training involved 100K finetuning steps, carried out over approximately 2 days on 4 RTX-3090 GPUs, with a batch size of 2 per GPU for each experiment."}, {"title": "4.4 Objective Evaluation Metrics", "content": "We employed metrics to evaluate controllability of chords and rhythm, textual adherence and audio fidelity. For the first two metrics, we used the rhythm and chord conditions from a clip in a evaluation dataset to generate music (along with a text prompt generated by ChatGPT; see Section 4.2), applied the Madmom and BTC models on the generated audio to estimate beats and chords, and evaluated how they reflect the given conditions. See Figure 2 for examples.\nChord. We used the mir_eval [30] package to measure 3 different degrees of frame-wise chord correctness: majmin, triads and tetrads. The majmin function compares chords in major-minor rule ignoring chord qualities outside major/minor/no-chord. The triads function compares chords along triad (root & qulaity to #5), while the tetrads compares chords along tetrad (root & full quality).\nRhythm F1 measurement follows the standard methodology for beat evaluation. We measured the beat accuracy also via mir_eval, assessing the alignment between the beat timestamps of the generated music and the reference rhythm music data, with a tolerance window of 70ms.\nCLAP [31, 32] score examines the textual adherence by the cosine similarity between the embedding of the text prompt and that of the generated audio in a text-audio joint embedding space learned by contrastive learning. Here, we used the LAION CLAP model trained for music [33], music_audioset_epoch_15_esc_90.14.pt.\nFAD is the Fr\u00e9chet distance between the embeddings distribution from a set of reference audios and that from the generated audios [34, 35]. The metric represent how realistic the generated audios are compared to the given reference audios. The audio encoder of FAD we used is VGGish [36] model which trained on an audio classification task. The reference set of audios was from MUSDB18 or RWC depending on the evaluation set."}, {"title": "4.5 Subjective Evaluation Metrics", "content": "We also did a listening test to evaluate the followings aspects: text relevance, rhythm consistency, and chord relevance. Text relevance concerns how the generated audio clips reflect the given text prompts. Rhythm consistency is about how steady the beats is within an audio clip. (We found that, unlike the case of objective evaluations, minor out-of-sync beats at the beginning of a clip were deemed acceptable here perceptually.) Chord relevance concerns how a generated clip follows the given chord progressions."}, {"title": "5. EXPERIMENTAL RESULTS", "content": "5.1 Objective Evaluation: Temporal Conditions\nWe assessed the audio generated under various condition combinations applied to the training model, including the proposed method and its ablations with either chord- or rhythm-only as the temporal condition, or using both but without the frame-wise chord condition. The finetuning configurations and mechanisms for these models were the same. Moreover, we considered the baseline as follows. The pretrained MusicGen-melody model originally processes text and melody conditions T, M. We simply used the prepend chord condition $C_{pre}$ as input to the linear projection layers originally pretrained to take the melody condition, without finetuning the entire model at all. In addition, we appended to the end of the text prompt BPM information (e.g., \"at BPM 90\") as the rhythm condition.\nResult shown in Table 2 leads to many findings. Firstly, a comparison between the result of the proposed model (first row) and the baseline (last row) demonstrates nicely the effectiveness of the proposed design. The proposed model leads to much higher scores in almost all the metrics. Moreover, it performs similarly well for the two evaluation datasets, suggesting that MusiConGen can deal with both conditions extracted from a reference audio signals or provided by creators in a symbolic text-like format.\nSecondly, although the baseline model does not perform well, it still exhibits some level of chord control, showing the knowledge of melody can be transferred to chords.\nFinally, from the ablations (middle three rows), chord-only and rhythm-only did not work well for rhythm and chord control respectively, which is expected. Compared to the proposed model, excluding per-frame chord condition degrades both chord and rhythm controllability, showing that chord and rhythm are interrelated.\n5.2 Objective Evaluation: Finetuning Mechanisms\nBesides the proposed finetuning method, we evaluated the following alternatives. Finetuned baseline is a baseline model that was finetuned using the prepended chords ($C_{pre}$) instead of melody $M$ the frame-level conditions, employing the jump finetuning mechanism but no in-attention. Jump finetuning without in-attention (ablation 1) and jump finetuning with full in-attention (abal-"}, {"title": "5.3 Subjective Evaluation", "content": "We evaluated three models in the listening test: the baseline, the finetuned baseline, and the proposed model. Each model generates a music clip using the ChatGPT-generated text prompts, along with the BPM and chords from the RWC dataset, namely considering text-like symbolic rhythm and chord conditions. Besides the audios generated by the three models, we also included real audios from the RWC dataset as the real audio. We note that the real audios would have perfect rhythm and chord controllability (for they are where the conditions are from), but the textual adherence would be bad because RWC songs are J-Pop rather than any of the five genres (i.e., Rock, Funk, Jazz, Blues, and Metal) described by the text prompts.\nWe had 23 participants in the user study, 85% of whom have over three years of musical training. Each time, we displayed the given text, rhythm and chord conditions, and asked a participant to rate the generated audio and the real audio (anonymized and in random order) on a five-point Likert scale. The result is shown in Figure 3.\nSeveral findings emerged. Firstly, the proposed model demonstrated superior chord control compared to the other two models, although it still fell short of matching the real audio. Secondly, the proposed model has no significant advantage on rhythm consistency against the finetuned baseline. As suggested by the examples on our demo page, we found that being on the precise beat onset does not significantly impact rhythm perception. Thirdly, our model had lower text relevance than the finetuned baseline, suggesting that our model may have traded text control for increased temporal control of rhythm and chords."}, {"title": "6. CONCLUSION AND FUTURE WORK", "content": "This paper has presented conditioning mechanisms and finetuning techniques to adapt MusicGen for better rhythm and chord control. Our evaluation on backing track generation shows that the model can take condition signals from either a reference audio or a symbolic input. For future work, our user study shows room to further improve the rhythm and chord controllability while keeping the text relevance. This might be done by scaling up the model size, better language model, or audio codecs. It is also interesting to incorporate additional conditions, such as symbolic melody, instrumentation, vocal audio, and video clips."}]}