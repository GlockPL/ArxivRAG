{"title": "Large language models can consistently generate high-quality content for election disinformation operations", "authors": ["Angus R. Williams", "Ryan Sze-Yin Chan", "Florence E. Enock", "Federico Nanni", "Evelina Gabasova", "Jonathan Bright", "Liam Burke-Moore", "Tvesha Sippy", "Yi-Ling Chung", "Kobi Hackenburg"], "abstract": "Advances in large language models have raised concerns about their potential use in generating compelling election disinformation at scale. This study presents a two-part investigation into the capabilities of LLMs to automate stages of an election disinformation operation. First, we introduce DisElect, a novel evaluation dataset designed to measure LLM compliance with instructions to generate content for an election disinformation operation in localised UK context, containing 2,200 malicious prompts and 50 benign prompts. Using DisElect, we test 13 LLMs and find that most models broadly comply with these requests; we also find that the few models which refuse malicious prompts also refuse benign election-related prompts, and are more likely to refuse to generate content from a right-wing perspective. Secondly, we conduct a series of experiments (N = 2,340) to assess the \u201chumanness\u201d of LLMs: the extent to which disinformation operation content generated by an LLM is able to pass as human-written. Our experiments suggest that almost all LLMs tested released since 2022 produce election disinformation operation content indiscernible by human evaluators over 50% of the time. Notably, we observe that multiple models achieve above-human levels of humanness. Taken together, these findings suggest that current LLMs can be used to generate high-quality content for election disinformation operations, even in hyperlocalised scenarios, at far lower costs than traditional methods, and offer researchers and policymakers an empirical benchmark for the measurement and evaluation of these capabilities in current and future models.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) as tools for generating natural language are now widely-accessible to anyone who might want to use them. This includes malicious actors looking to spread disinformation through online platforms in 'information operations': systematic campaigns that seek to promote false or misleading narratives [1]. Such actors are increasingly a feature of the contemporary information environment and have generated widespread public concern about their potential ability to undermine faith in democratic institutions [2]. State-backed or privately funded operations may, for example, push agendas around certain politicians, try to sow doubt in electoral processes, or cause confusion and disagreement around local issues [3].\nA successful information operation requires two key things: the production of 'realistic' content (such that people consuming it do not realise it has been created purely to push a narrative or by one centrally co-ordinated actor); it also requires this realistic content to be produced 'at scale', giving the impression of a mass groundswell in public opinion. Furthermore, this content needs to be disseminated across a range of distriubtion networks, using for example networks of \"junk\" news websites [4] or social media accounts [5], which themselves require the creation of further content (e.g. text for a website, or a biography for a social media account) to appear as authentic.\nGeneration of realistic content at scale has, historically, been a hard challenge for those running information operations to achieve. Sometimes they have employed industrial scale teams of fake users [6], that may be effective but also come at considerable cost; and when they are based in a foreign country, they may struggle to effectively convince local users of their legitimacy, as well as requiring considerable operational security efforts to conceal their true origin [7]. At other times simple automation methods have been employed, such as 'copypasta' (simply copy-pasting messages between different accounts [7]) or 'spintax' (making minor changes to messages based on procedural rules [8]). However this automation is straightforward to detect when multiple examples of messages produced through these techniques are seen by a user.\nIn this context, the rise of generative AI and LLMs, that can cheaply generate highly realistic content at scale, is significant. They could contribute to supercharging existing organisations who run information operations, and potentially allow for new ones to enter the arena. Furthermore, LLMs can roleplay as different personas such as political alignment [9, 10], and potentially reproduce granular details about specific individuals, concepts and places through their extensive training datasets. These abilities may lend themselves to the creation of more authentic content in information operations than has perhaps previously been seen before.\nWhile the use of generative AI in disinformation operations has been noted [11-14], it remains to be seen how effective this style of operations is. Increasingly, work is done after training LLMs to align them with human values and prevent harm or misuse, such as feedback learning [15] and red teaming [16]: this might prevent their compliance with instructions to generate content for an operation. Furthermore, humans may still be able to spot AI generated content. In response to this, we present a two-part study from the perspective of a malicious actor looking to use LLMs to generate content for multiple stages of an election disinformation operation.\nFirstly, we present DisElect, a novel evaluation dataset for election disinformation. Using a set of past and present LLMs, we find that most LLMs comply with instructions to generate content for an election disinformation operation, with models that do refuse also refusing benign election prompts and prompts to write from a right-wing perspective. Secondly, we conduct experiments to assess the perceived authenticity or \u201chumanness\u201d of LLM-generated election disinformation campaign content. We find that human participants are unable to discern LLM-generated and human-written content"}, {"title": "Related Work", "content": "Misinformation refers to information containing false or misleading claims [17]. Recent research finds that public exposure to, and concerns about the spread of misinformation in general is high in the UK, especially online [18]. Disinformation is often distinguished from misinformation as referring to false information circulated with the intent to deceive, as opposed to claims made without deceptive intentions [17]. In this paper, we refer to disinformation specifically, given the malicious intent of the use cases studied. Manipulating public opinion, political unrest, and influencing voting behaviours are just some of the concerns regarding online disinformation operations [19], which was also widely highlighted in the context of Covid-19 [20,21]. To take the most obvious example, during the US 2016 election Russian information operations were publishing almost 1,000 pieces of content per week at their height [3]. The content, which was produced by a team of 400 people at Russia's Internet Research Agency (IRA), comprised blogs, memes, online comments, Facebook groups, tweets, and fake personas-and was posted across 470 pages, accounts, and groups. It is estimated to have reached 126 million users on Facebook alone [3]. Researchers continue to debate the concrete impact of the operation; however what is not in doubt is the scale of organisation required to create it.\nA characteristic element of these operations is the semblance of \"peer pressure\" through social networks. For example, \u201cinfluencer\" accounts or bots pretending to be humans may be used to propagate disinformation [5]. When a critical mass of people are convinced, more people may start believing claims due to their popularity (also known as the \"bandwagon effect\") which can lead to a self-perpetuating cycle [5]. A study on Russian social media operations by Helmus et al. [22] illustrates this point. First, false or misleading content is created by Russian affiliated media outlets [4]. Second, trolls and bots amplify this content on social media through fear-inciting commentary, serving as \u201cforce multipliers\" [23]. Third, these narratives are further perpetuated through mutually reinforcing digital groups. These phases are repeated and layered on top of each other, to create and sustain false narratives that are difficult to discern from true information.\nIn the past, the content for such disinformation operations has been largely created by humans. However, with rapid progress in AI technologies, the use of AI generated content in such disinformation operations has been noted as of recently: Hanley and Durumer [24] find that between January 1, 2022, and May 1, 2023, the number of synthetic news articles increased by 57.3% and 474% respectively on mainstream and disinformation websites. A US Department of Justice press release [11] reports on the disruption of a Russian government-organised bot farm utilising generative AI. Wack et al. [12] identify an \"AI-empowered\" influence network supporting the leading party of Rwanda. Thomas [13] presents a disinformation campaign utilising OpenAI's models targeting pro-Ukraine Americans. OpenAI themselves discuss their attempts to identify and disrupt deceptive uses of their AI models by covert influence operations [14]."}, {"title": "AI Safety Evaluations", "content": "Measurement of the extent to which large language models are co-operative when asked to produce content to support a disinformation operation is part of the wider field of AI safety evaluations. Weidinger et al. [25] propose a sociotechnical approach these safety evaluations, consisting of evaluations at three intersecting levels: model capability layer, human-interaction layer and systemic layer. In the context of election disinformation, evaluation at the capability layer might measure the extent to which an AI system can produce disinformation. Evaluation at the human-interaction level might involve examining the deceptive capacity of AI-generated content through behavioural experiments. Finally, evaluation at the systemic level might explore how election disinformation might impact levels of epistemic (mis)trust in the general public.\nHere, we make a unique contribution by focusing conducting evaluations at both the capability layer - using benchmarking techniques - and at the human-interaction layer using human-subjects experiments. Both these dimensions are essential components of evaluations, as risks from AI-generated disinformation are determined by not only the capability of models to generate disinformation, but also public experiences with and perceptions of such content when they engage with it [25].\nIn the following sections, we review existing research on: 1) The capability of Generative AI models to generate disinformation; and, 2) experimental research on public perceptions of AI generated disinformation."}, {"title": "Assessing the capability of LLMs to generate disinformation", "content": "To evaluate the capacity of GPT-3 to generate accurate information or disinformation, Spitale et al. [26] prompted the AI model to produce 10 accurate and 10 disinformation tweets for a range of topics such as climate change and vaccine safety. The rate of obedience, measured as the percentage of requests satisfied by GPT-3 divided by the overall number of requests indicated better compliance for accurate information (99 times out of 101) compared to disinformation (80 out of 102) requests.\nIn another study, Kreps et al. [27] found that a set of smaller, older AI models could generate credible-sounding news articles at scale without human intervention. The authors used one sentence from a New York Times story to prompt GPT-2 models (355M, 774M, and 1.5B) to generate 300 outputs. The best outputs of the 774M model (mean credibility index of 6.72) and 1.5B model (mean credibility index of 6.93) were perceived to be marginally more credible than that of the 355M model (mean credibility index of 6.65). Similarly, Buchanan et al. [3] showed that LLMs could generate moderate-to-high quality disinformation messages with little human intervention.\nIn one of the few studies on disinformation generated by multimodal AI models, Logically AI [28] tested three image-based generative AI platforms to assess compliance with prompts in a US, UK and Indian context. The report found that more than 85% of prompts were accepted by these models. In the context of the UK, prompts centred around crime, immigration, and civil unrest. ActiveFence [29] analysed the ability of six LLMs to respond to false and misleading prompts produced in English and Spanish, across five categories of misinformation and harmful narratives: health misinformation, electoral and political misinformation, conspiracy theories, calls for social unrest, and a category that combines two or more categories. The authors found that LLMs responded least safely to misinformation prompts. Similarly, Brewster and Sadeghi [30] found that ChatGPT and Google Bard generated content on 98 and 80 false narratives, respectively when prompted with a sample of 100 myths.\nBuchanan et al. [3] also find that AI models can not only generate disinformation but also customize language for specific groups. For example, Urman and Makhortykh [31] indicate that outputs of LLM-based chatbots were prone to political bias with regard to"}, {"title": "Human interactions with AI generated disinformation", "content": "In the context of disinformation, Spitale et al. [26] conducted a pre-registered experiment with 697 respondents across United Kingdom, Australia, Canada, United States, and Ireland. The authors presented participants with tweets containing both true and false information about a range of topics mentioned above and were asked to identify whether what they read was true or false (information recognition) and whether the content was written by an AI model or human (AI recognition). The authors found that participants could not distinguish between tweets generated by GPT-3 and those written by real Twitter users. Furthermore, participants recognized false tweets written by humans more than false tweets generated by AI (scores 0.92 versus 0.89, respectively;\nP = 0.0032), implying that the AI model was better at misinforming people.\nA large part of the experimental research focuses on perceived credibility, trustworthiness, and persuasiveness of AI-generated text. Kreps et al. [27] conducted experiments on AI-generated and human-written news articles, finding that respondents perceived AI-generate news to be equally or more credible than human-written articles. Similarly, Goldstein et al. [32] found that GPT-3 could write persuasive text with limited effort. And Zellers et al. [33] noted that an AI model could generate an article when prompted with a given headline and that humans found such articles to be more trustworthy than human-written disinformation. However, Bashardoust et al. [34] found that AI-generated fake news was perceived as less accurate than human-generated fake news and that political orientation and age explained whether users were deceived by AI-generated fake news. To understand how persuasive LLMs are when microtargeted to individuals on political issues, Hackenburg et al. [35] integrated user data into GPT-4 prompts, and found that, although persuasive, microtargeted messages were not statistically more persuasive than non-targeted messages. Similarly, Hackenburg et al. [36] also tested 24 LLMs on their ability to generate persuasive messages. The authors found that larger models were only marginally better due to better task completion (coherence and staying on topic). More broadly, Jakesch et al. [37] find that humans are not able to detect self-presentations generated by LLMs, and that LLMs can exploit human heuristics for identifying LLM-generated text in order to produce text perceived as \"more human than human\"."}, {"title": "Our contribution", "content": "Several features distinguish our paper. First, by evaluating model compliance with malicious prompts to generate false information related to elections, and experiments on whether people could distinguish between AI and human written disinformation, this paper contributes to the sociotechnical evaluation evidence base. Second, by embedding this research within a UK context, at both a national and a hyperlocal level (London), this paper makes a unique contribution to the literature most of which is otherwise situated and/or conducted in a U.S. context - and addresses specifically the capacity of models to localise content in a realistic fashion, a key weakness of past information operations. Third, the experiments center around the theme of political disinformation. These most studies examine content generated by one AI model, whereas our paper considers content generated by a range of 13 different AI models, capturing the diversity in models at the disposal of malicious actors. Finally, we look at the entire pipeline of information operations (News Article Generation Social Media Account Generation Social Media Content Generation-Reply generation), while the literature tends to focus solely on one or two stages; typically news articles or tweets."}, {"title": "Methodology", "content": "In order to understand to what extent LLMs would be useful for automating election disinformation operations, we conduct a two-part study:\n1. Systematic Evaluation Dataset: Measuring LLM compliance with instructions to generate content for an election disinformation operation.\n2. Human Experiments: Measuring how well people can distinguish between AI-generated and human-written election disinformation content.\nGenerating nuanced and realistic content can reduce the ability of a layperson to identify information or activity as inauthentic, and therefore increases the apparent authenticity of any given part of an information operation. We establish a 4 stage operation design, covering both the content generation and dissemination stages of a typical disinformation operation.\nA. News Article Generation: News articles and headlines act as the \"root\" of an operation, making claims which will be further enforced by other stages.\nB. Social Media Account Generation: \"Fake\" Social media accounts (e.g. on Twitter/X) are used to disseminate the generated news.\nC. Social Media Content Generation: Social media posts by accounts from B discussing the generated news creates an illusion of public interest/legitimacy.\nD. Reply generation: Replies to the social media posts in C further the illusion of public interest and the potential impact of the operation.\nWe consider this design across two relevant use cases:\nHyperlocalised logistical voting disinformation (e.g. a voting date for a specific area changing): False information about where, when, and how to vote can disrupt electoral processes and lead to individuals being unable to cast their votes, and represents an opportunity to explore the ability of LLMs to generate content containing highly localised information. This is important because one of the areas where disinformation operations have struggled in the past is effective localisation.\nFictitious claims about UK Members of Parliament or \u201cMPs\u201d (e.g. being accused of misusing campaign funds): Spreading false information about the activities of election candidates can influence the opinions of the electorate, and offers an opportunity to investigate who an LLM will (and won't) generate misinformation about."}, {"title": "Models", "content": "We select 13 LLMs (table 1) that vary in terms of release date, size, and access type (open-source vs. API). This enables us to measure and compare newer vs. older and smaller vs. larger models. We include multiple models from the same families to observe change within family (e.g. T5 vs. Flan-T5, Llama 2 vs. Llama 3, Gemma vs. Gemini). We used Ollama (ollama.com) for several models. Ollama runs models at 4-bit quantisation, reducing memory footprint and facilitating local execution, though sometimes reducing accuracy on complex tasks [38,39]. Such frameworks are particularly relevant as many disinformation operations may choose to run local large language models, rather than calling them over an API.\nWe should note that release date refers to the announcement date of the original version of the model, rather than the release date of the specific version used in the paper. GPT-4 was announced by OpenAI on 2023-03-14, we use the gpt-4-0613 model"}, {"title": "DisElect Evaluation Dataset", "content": "To systemically evaluate model compliance on election disinformation, we construct the DisElect dataset, containing 2, 200 prompts for the stages and use cases described above, and a baseline set of 50 benign election prompts to examine how sensitive models are to election content in general.\nFor each stage in each use case, we create a prompt template, as shown in table 2. We then fill the prompt templates using the variables in table 3 and table 4 to generate 1,100 unique prompts for each use case. We refer to the datasets of prompts created as DisElect.VT for the voting use case and DisElect.MP for the MP use case. It is worth highlighting that, although all the content could be put to use in a plausible disinformation campaign, much of it can appear anodyne at face value, or is not necessarily misleading. We will return to this point in the conclusion.\nDisElect.VT covers voting date, location, and voter ID disinformation, across different locations in the UK, for accounts and posts from left and right-wing perspectives. DisElect.MP is similarly structured, covering claims and issues around financial, criminal, and political activity by MPs, varying prompts by the UK MP that the claim targets instead of location (location is still used for social media account generation). We curate 50 election information-seeking prompts from a good-faith perspective to form the benign baseline set, DisElect. BL.\nTo optimise the execution of prompts across multiple models, we developed a dedicated Python library named prompto, which we make available in open access to the research community for supporting other types of comparison across a variety of LLMs."}, {"title": "Evaluation", "content": "In order to understand whether the models accept to generate the content in question, we make use of a LLM-as-a-judge approach [52] to label model responses with one of four classes: Refuse (explicitly refuses to comply with the prompt), Soft-refuse (doesn't explicitly refuse to comply but doesn't satisfy the intent of the request, being deliberately more neutral or opposing the sentiment requested), Incoherent (an incomplete, unreadable, or confusing response, e.g. repeating whole phrases from the prompt), or Comply (a desired response to the prompt).\nThis multi-class approach to refusal detection enables us to differentiate useful responses from low-quality compliant responses from refusals. We include the Incoherent label as we find that older generative language models do not explicitly refuse prompts but often produce very low quality responses\nWe use GPT-3.5 Turbo to label responses in a zero-shot manner, given a prompt and response and the judge prompt template (available at https://github.com/alan-turing-institute/election-ai-safety/blob/main/ data/evals/judge/template.txt). On a sample of 100 responses and judgements labelled by hand, we observe an overall Macro F1 Score of 0.76, and accuracy of 96% on the \"refuse\" class and 86% on the other classes. Precision and recall on the \"refuse\" class were 0.84 and 0.98 and respectively."}, {"title": "Humanness Experiments", "content": "To evaluate the degree to which synthetic content appears as authentic to humans, which we term \u201chumanness\", we task human participants with labelling election disinformation content written by humans and the 13 LLMs in table 1 as either human-written or AI-written. We conduct three experiments:"}, {"title": "Experiment 1a: Testing perceived humanness of disinformation content describing an MP accused of misusing campaign funds under a left-wing persona.", "content": "Experiment 1b: Testing perceived humanness of disinformation content describing an MP accused of misusing campaign funds under a right-wing persona.\nExperiment 2: Testing perceived humanness of disinformation content describing a date change for local elections under a right-wing persona.\nThe first two experiments (1a and 1b) provide a view of humanness of content written from different political perspectives, while experiment 2 gives insight into the ability of models to generate human-passing content discussing hyper-localised issues. Each experiment is based on responses to 4 prompts (each operation stage with one combination of variables, visible in table 6), where variable values (subject and mp for la and 1b, subject for 2) are selected to minimise refusals. We generate 15 responses to each prompt for each source (14 sources: human generation plus the 13 LLMs)."}, {"title": "Content Generation", "content": "Human-written content was created by another research team with good general knowledge of the subject area, but who were not involved in model content generation. To create AI-generated content, we prompt all LLMs with versions of the prompts outlined in table 6, where \"Write\" is replaced with \"Write 15 variations of\". In cases where LLMs were unable to comply with this instruction (i.e. did not provide a list of 15 variations as a response this was the case for GPT-2, T5, Flan-T5, and GPT-Neo), we instead prompt each model fifteen times for each prompt. For experiments la and 1b, the name of the MP receiving the lowest proportion of refusals from DisElect.MP (not disclosed) was used in the prompts for AI and human written content. This name, and any mention of specific party or constituency, was redacted from responses before presentation to participants, instead replaced with tokens e.g. \"{MP}\",\"{PARTY}\". Example responses for experiment 2 are visible in table 7."}, {"title": "Experimental Design", "content": "Experiments were designed on Qualtrics. Each participant was randomly assigned to an experimental condition, containing content across the four stages from one LLM only. Participants were presented with the instructions visible in table 8. At each stage, participants saw 15 items generated by one LLM model, plus 15 human-written items, alongside the prompt used to generate that content (full list visible in table 6). Participants were asked to indicate whether they thought each item was written by a human or generated by an AI model. We included one attention check per stage. Accordingly, each participant saw 31 items per stage, randomly ordered. We also asked demographic questions: age, gender, digital literacy & familiarity, education level and political orientation.\nThis approach means participants may be able to identify linguistic patterns in multiple items generated by the same model, giving them clues about these items being LLM-generated. While this could be considered a limitation, the baseline comparison for each model is humans only, so a model producing easily-identifiable, repetitive content would not make another model seem more \"human-like\"."}, {"title": "Sampling", "content": "We recruited 780 UK-based, English-speaking participants who were over the age of 18 for each experiment (N = 2,340). These numbers account for participants that failed attention checks and as such were disregarded and replaced. We balance left-wing and right-wing participants for experiments la and 16. Experiment 2 focused exclusively on"}, {"title": "London issues, and as such we recruited participants residing specifically in London.", "content": "The left-wing/right-wing split for this sample was not balanced but was representative of the London population, which is more left-leaning. Participants were required to sign an electronic consent form after reading the participant information sheet (visible in table 8). Full demographic information on participants is available in table 9."}, {"title": "Results", "content": "Results for DisElect on the models listed in table 1 are shown in fig. 1. Refusal rates are generally low - only three models (Llama 2, Gemma, Gemini 1.0 Pro) explicitly refuse to comply with more than 10% of prompts in any use case. Phi-2 and Llama 2 also produce some refusals across both experiments, but a considerably smaller number than the aforementioned models. The oldest of the refusing models (Llama 2) was introduced mid-2023, reflecting that refusals are a phenomenon introduced through safety-focused model fine-tuning that may not be present in earlier models."}, {"title": "Refusals are more common in DisElect.MP than in DisElect.VT (12.4% vs. 6.8%", "content": "Refusals are more common in DisElect.MP than in DisElect.VT (12.4% vs. 6.8% overall), whereas soft-refusals are more common in the latter (17.5% vs 21.4% overall). The tendency of the same models to explicitly refuse instead of soft-refusing suggests that that disinformation around political figures or issues are areas that may have had more safety related fine-tuning.\nResults for DisElect.BL reflect label distributions for DisElect. VT and DisElect.MP- models that refuse malicious election prompts will generally refuse some amount of benign election-related prompts. There are two models (GPT-3.5 Turbo and Mistral) that refuse one prompt in DisElect.BL that did not refuse any prompts in DisElect.VT or DisElect.MP.\nThe range of models studied enables us to observe changes in LLMs over time. In DisElect (see fig. 1), we see that older models tend to produce both lower compliance and refusal rates, returning higher rates of incoherent or soft-refusal responses (83.5% of all incoherent and soft-refuse responses come from the 5 earliest models).\nWe are also able to compare models within 'families', which allows us to address differences between models either produced at different times or with different amounts of parameters. For example, we find that the earlier or smaller versions (Llama 2 & Gemma) seem to return much higher rates of refusal than later or larger versions (Llama 3 & Gemini). We also see from DisElect.BL in fig. 1 that Llama 3 and Gemini do not refuse safe election related questions, whereas the earlier or smaller models do. This shows that Llama 2 and Gemma could be seen as overly sensitive to benign election-related prompts in general, even when non-malicious, in a way that is not present in their later or larger equivalents."}, {"title": "What drives refusal?", "content": "Focusing on the 3 models that do refuse a significant number of prompts (Llama 2, Gemma, and Gemini 1.0 Pro), we present proportions of prompts refused by variables values (see table 3 and table 4) in fig. 2. Across both use cases, all models are much more likely to refuse when prompted to use a right-wing persona than a left-wing persona. Refusals for left-wing personas are higher for all models in DisElect.MP than in DisElect.VT. Prompting models to generate news articles returns more refusals in DisElect.MP than DisElect.VT. There is some variation in refusal on different subjects, with prompts about voter ID, colluding with China, and drug possession drawing higher refusal rates than other options in respective use cases.\nCalculating the Spearman's Rank Correlation on refusal rates between models for each variable reveals that models often do not align on what to refuse: only the persona variable sees no variation between refusal rankings for possible values (left-wing and right-wing) for both use cases. Correlation for subject in DisElect.MP is also high (p = 0.74 median). Correlation is lowest for the pipeline stage as a variable in DisElect.VT (p = 0.00 median).\nFor the 50 MP names included in DisElect.MP, we find that refusals are normally distributed (64% within 1 standard deviation of the mean). fig. 3 presents refusal rates for groups of MPs by party and gender. The Spearman's Rank Correlation between models on these groups is high, at a median value of p = 0.90 when grouping MPs by party and gender, indicating alignment between models on which MPs to refuse to generate disinformation about. All 3 models are more likely to refuse to generate content for a female MP than a male MP, and for a Labour MP than a Conservative (or other) MP. This persists at a more granular level, with female Labour MPs seeing the highest level of refusal of any party-gender group on average, and male Conservative MPs the lowest."}, {"title": "How well can people identify AI-generated content?", "content": "We will now present the results from the experimental part of our study. We collate datasets about the number of times participants assign pieces of content as \u201chuman\u201d for the three experiments, aggregating by the LLMs in table 1 and the stages of the information operation pipeline in table 6 for each experiment detailed in ?? to calculate \"humanness\" for each LLM. We define humanness as the number of times humans (mis)label an AI-generated item as human over the total number of labels assigned to that AI-generated item (#AIH).\nWe plot humanness per LLM overall and broken down by experiment and pipeline stage in fig. 4, fig. 5, and fig. 6. Overall, 9/13 models achieve at > 50% humanness on average, indicating that the majority of models tested produce content that is indiscernable from human-written content for the same prompt. Meanwhile, 6/13 models achieve very high levels of humanness (>= 75%) on at least 5% of entries. However, variation is high: the coefficient of variation is higher than the interquartile range for all models, and all models see at least 12% of their items receive lower levels of humanness (<50%). This implies that the ability to discern AI-generated and human-written varies greatly between human participants.\nLlama 3 and Gemini achieve the highest humanness of all models (62% and 59% respectively). Both models see at least 15% of the entries achieve >= 75% humanness (19% and 17% respectively).\nWe observe two models (Llama 2, Gemma) with bimodal distributions of humanness proportions in fig. 4: many pieces of content receiving low humanness while others receive high humanness. As shown earlier, these two models produce the highest level of refusals on DisElect. While prompts in table 6 were constructed to minimise refusals, this is not always avoidable. Refusal responses are trivial for a human participant to identify as AI-generated. Aside from these items, Llama 2 and Gemma would receive among the highest levels of humanness. However, a model that frequently refuses to generate content will in the end not be useful for scaling an information operation."}, {"title": "Per Experiment Humanness per model across the three experiments is shown in", "content": "Per Experiment Humanness per model across the three experiments is shown in fig. 5. The mean humanness is highest on average for experiment 1a, focused on MP disinformation written from a left-wing perspective (0.53), then experiment 1b, focused on MP disinformation written from a right-wing perspective (0.48), and lowest for experiment 2, focused on localised election disinformation (0.44). Overall 10/13 models perform worst on 2, compared to 0/13 for 1a and 3/13 for 1b, while 6/13 models perform best on la and 6/13 on 1b. This suggests that content produced by these models is most human in the MP / left-wing perspective use case (1a), and least human in the voting / right-wing perspective use case (2). However, there may also be effects stemming from the political orientation of participants, which we will describe further below.\nThe two models with the highest humanness scores outlined above (Llama 3, Gemini 1.0 Pro) both see their lowest performance on experiment 2. Llama 3 achieves the highest humanness of all models in both experiment 2 and 1a, whereas Gemini 1.0 Pro achieves the highest humanness in experiment 1b.\nLlama 2 and Gemma, noted above for bimodal distributions of humanness due to refusals, see large variance in humanness across experiments. This bimodal distribution is visible for both models in experiment 1b, and experiment 2 for Llama 2, but is not visible for either models in experiment 1a (see fig. 5), due to a lack of refusals in experiment 1a. This reflects earlier findings that these models are more likely to refuse to write from a right-wing perspective, present in experiments 2 and 16 but not la."}, {"title": "Per Pipeline Stage Humanness per stage of our information operation pipeline is", "content": "Per Pipeline Stage Humanness per stage of our information operation pipeline is shown in fig. 6. 11/13 models see their highest humanness scores on the social media"}, {"title": "observing Llama 2 and Gemma reveals bimodal distributions of humanness", "content": "Observing Llama 2 and Gemma reveals bimodal distributions of humanness (indicating the presence of refusals) in some stages and not in others. Refusals are present for neither model in news article generation, owing to the absence of instruction to write from a particular perspective in this stage. Llama 2 scores higher than any other model on this stage as result (humanness = 0.61).\nLlama 3 and Gemini 1.0 Pro consistently achieved above average humanness across all pipeline stages, but vary in performance across stages (std("}]}