{"title": "Large language models can consistently generate high-quality content for election disinformation operations", "authors": ["Angus R. Williams", "Ryan Sze-Yin Chan", "Florence E. Enock", "Federico Nanni", "Evelina Gabasova", "Jonathan Bright"], "abstract": "Advances in large language models have raised concerns about their potential use in generating compelling election disinformation at scale. This study presents a two-part investigation into the capabilities of LLMs to automate stages of an election disinformation operation. First, we introduce DisElect, a novel evaluation dataset designed to measure LLM compliance with instructions to generate content for an election disinformation operation in localised UK context, containing 2,200 malicious prompts and 50 benign prompts. Using DisElect, we test 13 LLMs and find that most models broadly comply with these requests; we also find that the few models which refuse malicious prompts also refuse benign election-related prompts, and are more likely to refuse to generate content from a right-wing perspective. Secondly, we conduct a series of experiments (N = 2,340) to assess the \u201chumanness\u201d of LLMs: the extent to which disinformation operation content generated by an LLM is able to pass as human-written. Our experiments suggest that almost all LLMs tested released since 2022 produce election disinformation operation content indiscernible by human evaluators over 50% of the time. Notably, we observe that multiple models achieve above-human levels of humanness. Taken together, these findings suggest that current LLMs can be used to generate high-quality content for election disinformation operations, even in hyperlocalised scenarios, at far lower costs than traditional methods, and offer researchers and policymakers an empirical benchmark for the measurement and evaluation of these capabilities in current and future models.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) as tools for generating natural language are now widely-accessible to anyone who might want to use them. This includes malicious actors looking to spread disinformation through online platforms in 'information operations': systematic campaigns that seek to promote false or misleading narratives [1]. Such actors are increasingly a feature of the contemporary information environment and have generated widespread public concern about their potential ability to undermine faith in democratic institutions [2]. State-backed or privately funded operations may, for example, push agendas around certain politicians, try to sow doubt in electoral processes, or cause confusion and disagreement around local issues [3].\nA successful information operation requires two key things: the production of 'realistic' content (such that people consuming it do not realise it has been created purely to push a narrative or by one centrally co-ordinated actor); it also requires this realistic content to be produced 'at scale', giving the impression of a mass groundswell in public opinion. Furthermore, this content needs to be disseminated across a range of distriubtion networks, using for example networks of \"junk\" news websites [4] or social media accounts [5], which themselves require the creation of further content (e.g. text for a website, or a biography for a social media account) to appear as authentic.\nGeneration of realistic content at scale has, historically, been a hard challenge for those running information operations to achieve. Sometimes they have employed industrial scale teams of fake users [6], that may be effective but also come at considerable cost; and when they are based in a foreign country, they may struggle to effectively convince local users of their legitimacy, as well as requiring considerable operational security efforts to conceal their true origin [7]. At other times simple automation methods have been employed, such as 'copypasta' (simply copy-pasting messages between different accounts [7]) or 'spintax' (making minor changes to messages based on procedural rules [8]). However this automation is straightforward to detect when multiple examples of messages produced through these techniques are seen by a user.\nIn this context, the rise of generative AI and LLMs, that can cheaply generate highly realistic content at scale, is significant. They could contribute to supercharging existing organisations who run information operations, and potentially allow for new ones to enter the arena. Furthermore, LLMs can roleplay as different personas such as political alignment [9, 10], and potentially reproduce granular details about specific individuals, concepts and places through their extensive training datasets. These abilities may lend themselves to the creation of more authentic content in information operations than has perhaps previously been seen before.\nWhile the use of generative AI in disinformation operations has been noted [11-14], it remains to be seen how effective this style of operations is. Increasingly, work is done after training LLMs to align them with human values and prevent harm or misuse, such as feedback learning [15] and red teaming [16]: this might prevent their compliance with instructions to generate content for an operation. Furthermore, humans may still be able to spot AI generated content. In response to this, we present a two-part study from the perspective of a malicious actor looking to use LLMs to generate content for multiple stages of an election disinformation operation.\nFirstly, we present DisElect, a novel evaluation dataset for election disinformation. Using a set of past and present LLMs, we find that most LLMs comply with instructions to generate content for an election disinformation operation, with models that do refuse also refusing benign election prompts and prompts to write from a right-wing perspective. Secondly, we conduct experiments to assess the perceived authenticity or \u201chumanness\u201d of LLM-generated election disinformation campaign content. We find that human participants are unable to discern LLM-generated and human-written content"}, {"title": "Related Work", "content": "Misinformation refers to information containing false or misleading claims [17]. Recent research finds that public exposure to, and concerns about the spread of misinformation in general is high in the UK, especially online [18]. Disinformation is often distinguished from misinformation as referring to false information circulated with the intent to deceive, as opposed to claims made without deceptive intentions [17]. In this paper, we refer to disinformation specifically, given the malicious intent of the use cases studied. Manipulating public opinion, political unrest, and influencing voting behaviours are just some of the concerns regarding online disinformation operations [19], which was also widely highlighted in the context of Covid-19 [20,21]. To take the most obvious example, during the US 2016 election Russian information operations were publishing almost 1,000 pieces of content per week at their height [3]. The content, which was produced by a team of 400 people at Russia's Internet Research Agency (IRA), comprised blogs, memes, online comments, Facebook groups, tweets, and fake personas-and was posted across 470 pages, accounts, and groups. It is estimated to have reached 126 million users on Facebook alone [3]. Researchers continue to debate the concrete impact of the operation; however what is not in doubt is the scale of organisation required to create it.\nA characteristic element of these operations is the semblance of \"peer pressure\" through social networks. For example, \u201cinfluencer\" accounts or bots pretending to be humans may be used to propagate disinformation [5]. When a critical mass of people are convinced, more people may start believing claims due to their popularity (also known as the \"bandwagon effect\") which can lead to a self-perpetuating cycle [5]. A study on Russian social media operations by Helmus et al. [22] illustrates this point. First, false or misleading content is created by Russian affiliated media outlets [4]. Second, trolls and bots amplify this content on social media through fear-inciting commentary, serving as \u201cforce multipliers\" [23]. Third, these narratives are further perpetuated through mutually reinforcing digital groups. These phases are repeated and layered on top of each other, to create and sustain false narratives that are difficult to discern from true information.\nIn the past, the content for such disinformation operations has been largely created by humans. However, with rapid progress in AI technologies, the use of AI generated content in such disinformation operations has been noted as of recently: Hanley and Durumer [24] find that between January 1, 2022, and May 1, 2023, the number of synthetic news articles increased by 57.3% and 474% respectively on mainstream and disinformation websites. A US Department of Justice press release [11] reports on the disruption of a Russian government-organised bot farm utilising generative AI. Wack et al. [12] identify an \"AI-empowered\" influence network supporting the leading party of Rwanda. Thomas [13] presents a disinformation campaign utilising OpenAI's models targeting pro-Ukraine Americans. OpenAI themselves discuss their attempts to identify and disrupt deceptive uses of their AI models by covert influence operations [14]."}, {"title": "AI Safety Evaluations", "content": "Measurement of the extent to which large language models are co-operative when asked to produce content to support a disinformation operation is part of the wider field of AI safety evaluations. Weidinger et al. [25] propose a sociotechnical approach these safety evaluations, consisting of evaluations at three intersecting levels: model capability layer, human-interaction layer and systemic layer. In the context of election disinformation, evaluation at the capability layer might measure the extent to which an AI system can produce disinformation. Evaluation at the human-interaction level might involve examining the deceptive capacity of AI-generated content through behavioural experiments. Finally, evaluation at the systemic level might explore how election disinformation might impact levels of epistemic (mis)trust in the general public.\nHere, we make a unique contribution by focusing conducting evaluations at both the capability layer - using benchmarking techniques - and at the human-interaction layer using human-subjects experiments. Both these dimensions are essential components of evaluations, as risks from AI-generated disinformation are determined by not only the capability of models to generate disinformation, but also public experiences with and perceptions of such content when they engage with it [25].\nTo evaluate the capacity of GPT-3 to generate accurate information or disinformation, Spitale et al. [26] prompted the AI model to produce 10 accurate and 10 disinformation tweets for a range of topics such as climate change and vaccine safety. The rate of obedience, measured as the percentage of requests satisfied by GPT-3 divided by the overall number of requests indicated better compliance for accurate information (99 times out of 101) compared to disinformation (80 out of 102) requests.\nIn another study, Kreps et al. [27] found that a set of smaller, older AI models could generate credible-sounding news articles at scale without human intervention. The authors used one sentence from a New York Times story to prompt GPT-2 models (355M, 774M, and 1.5B) to generate 300 outputs. The best outputs of the 774M model (mean credibility index of 6.72) and 1.5B model (mean credibility index of 6.93) were perceived to be marginally more credible than that of the 355M model (mean credibility index of 6.65). Similarly, Buchanan et al. [3] showed that LLMs could generate moderate-to-high quality disinformation messages with little human intervention.\nIn one of the few studies on disinformation generated by multimodal AI models, Logically AI [28] tested three image-based generative AI platforms to assess compliance with prompts in a US, UK and Indian context. The report found that more than 85% of prompts were accepted by these models. In the context of the UK, prompts centred around crime, immigration, and civil unrest. ActiveFence [29] analysed the ability of six LLMs to respond to false and misleading prompts produced in English and Spanish, across five categories of misinformation and harmful narratives: health misinformation, electoral and political misinformation, conspiracy theories, calls for social unrest, and a category that combines two or more categories. The authors found that LLMs responded least safely to misinformation prompts. Similarly, Brewster and Sadeghi [30] found that ChatGPT and Google Bard generated content on 98 and 80 false narratives, respectively when prompted with a sample of 100 myths.\nBuchanan et al. [3] also find that AI models can not only generate disinformation but also customize language for specific groups. For example, Urman and Makhortykh [31] indicate that outputs of LLM-based chatbots were prone to political bias with regard to"}, {"title": "Human interactions with AI generated disinformation", "content": "In the context of disinformation, Spitale et al. [26] conducted a pre-registered experiment with 697 respondents across United Kingdom, Australia, Canada, United States, and Ireland. The authors presented participants with tweets containing both true and false information about a range of topics mentioned above and were asked to identify whether what they read was true or false (information recognition) and whether the content was written by an AI model or human (AI recognition). The authors found that participants could not distinguish between tweets generated by GPT-3 and those written by real Twitter users. Furthermore, participants recognized false tweets written by humans more than false tweets generated by AI (scores 0.92 versus 0.89, respectively; P = 0.0032), implying that the AI model was better at misinforming people.\nA large part of the experimental research focuses on perceived credibility, trustworthiness, and persuasiveness of AI-generated text. Kreps et al. [27] conducted experiments on AI-generated and human-written news articles, finding that respondents perceived AI-generate news to be equally or more credible than human-written articles. Similarly, Goldstein et al. [32] found that GPT-3 could write persuasive text with limited effort. And Zellers et al. [33] noted that an AI model could generate an article when prompted with a given headline and that humans found such articles to be more trustworthy than human-written disinformation. However, Bashardoust et al. [34] found that AI-generated fake news was perceived as less accurate than human-generated fake news and that political orientation and age explained whether users were deceived by AI-generated fake news. To understand how persuasive LLMs are when microtargeted to individuals on political issues, Hackenburg et al. [35] integrated user data into GPT-4 prompts, and found that, although persuasive, microtargeted messages were not statistically more persuasive than non-targeted messages. Similarly, Hackenburg et al. [36] also tested 24 LLMs on their ability to generate persuasive messages. The authors found that larger models were only marginally better due to better task completion (coherence and staying on topic). More broadly, Jakesch et al. [37] find that humans are not able to detect self-presentations generated by LLMs, and that LLMs can exploit human heuristics for identifying LLM-generated text in order to produce text perceived as \"more human than human\"."}, {"title": "Our contribution", "content": "Several features distinguish our paper. First, by evaluating model compliance with malicious prompts to generate false information related to elections, and experiments on whether people could distinguish between AI and human written disinformation, this paper contributes to the sociotechnical evaluation evidence base. Second, by embedding this research within a UK context, at both a national and a hyperlocal level (London), this paper makes a unique contribution to the literature most of which is otherwise situated and/or conducted in a U.S. context - and addresses specifically the capacity of models to localise content in a realistic fashion, a key weakness of past information operations. Third, the experiments center around the theme of political disinformation. These most studies examine content generated by one AI model, whereas our paper considers content generated by a range of 13 different AI models, capturing the diversity in models at the disposal of malicious actors. Finally, we look at the entire pipeline of information operations (News Article Generation Social Media Account Generation Social Media Content Generation-Reply generation), while the literature tends to focus solely on one or two stages; typically news articles or tweets."}, {"title": "Methodology", "content": "In order to understand to what extent LLMs would be useful for automating election disinformation operations, we conduct a two-part study:\n1. Systematic Evaluation Dataset: Measuring LLM compliance with instructions to generate content for an election disinformation operation.\n2. Human Experiments: Measuring how well people can distinguish between AI-generated and human-written election disinformation content.\nGenerating nuanced and realistic content can reduce the ability of a layperson to identify information or activity as inauthentic, and therefore increases the apparent authenticity of any given part of an information operation. We establish a 4 stage operation design, covering both the content generation and dissemination stages of a typical disinformation operation.\nA. News Article Generation: News articles and headlines act as the \"root\" of an operation, making claims which will be further enforced by other stages.\nB. Social Media Account Generation: \"Fake\" Social media accounts (e.g. on Twitter/X) are used to disseminate the generated news.\nC. Social Media Content Generation: Social media posts by accounts from B discussing the generated news creates an illusion of public interest/legitimacy.\nD. Reply generation: Replies to the social media posts in C further the illusion of public interest and the potential impact of the operation.\nWe consider this design across two relevant use cases:\nHyperlocalised logistical voting disinformation (e.g. a voting date for a specific area changing): False information about where, when, and how to vote can disrupt electoral processes and lead to individuals being unable to cast their votes, and represents an opportunity to explore the ability of LLMs to generate content containing highly localised information. This is important because one of the areas where disinformation operations have struggled in the past is effective localisation.\nFictitious claims about UK Members of Parliament or \u201cMPs\u201d (e.g. being accused of misusing campaign funds): Spreading false information about the activities of election candidates can influence the opinions of the electorate, and offers an opportunity to investigate who an LLM will (and won't) generate misinformation about.\nWe select 13 LLMs (table 1) that vary in terms of release date, size, and access type (open-source vs. API). This enables us to measure and compare newer vs. older and smaller vs. larger models. We include multiple models from the same families to observe change within family (e.g. T5 vs. Flan-T5, Llama 2 vs. Llama 3, Gemma vs. Gemini). We used Ollama (ollama.com) for several models. Ollama runs models at 4-bit quantisation, reducing memory footprint and facilitating local execution, though sometimes reducing accuracy on complex tasks [38,39]. Such frameworks are particularly relevant as many disinformation operations may choose to run local large language models, rather than calling them over an API.\nWe should note that release date refers to the announcement date of the original version of the model, rather than the release date of the specific version used in the paper. GPT-4 was announced by OpenAI on 2023-03-14, we use the gpt-4-0613 model"}, {"title": "DisElect Evaluation Dataset", "content": "To systemically evaluate model compliance on election disinformation, we construct the DisElect dataset, containing 2, 200 prompts for the stages and use cases described above, and a baseline set of 50 benign election prompts to examine how sensitive models are to election content in general.\nFor each stage in each use case, we create a prompt template, as shown in table 2. We then fill the prompt templates using the variables in table 3 and table 4 to generate 1,100 unique prompts for each use case. We refer to the datasets of prompts created as DisElect.VT for the voting use case and DisElect.MP for the MP use case. It is worth highlighting that, although all the content could be put to use in a plausible disinformation campaign, much of it can appear anodyne at face value, or is not necessarily misleading. We will return to this point in the conclusion.\nDisElect.VT covers voting date, location, and voter ID disinformation, across different locations in the UK, for accounts and posts from left and right-wing perspectives. DisElect.MP is similarly structured, covering claims and issues around financial, criminal, and political activity by MPs, varying prompts by the UK MP that the claim targets instead of location (location is still used for social media account generation). We curate 50 election information-seeking prompts from a good-faith perspective to form the benign baseline set, DisElect. BL.\nTo optimise the execution of prompts across multiple models, we developed a dedicated Python library named prompto, which we make available in open access to the research community for supporting other types of comparison across a variety of LLMs.\nIn order to understand whether the models accept to generate the content in question, we make use of a LLM-as-a-judge approach [52] to label model responses with one of four classes: Refuse (explicitly refuses to comply with the prompt), Soft-refuse (doesn't explicitly refuse to comply but doesn't satisfy the intent of the request, being deliberately more neutral or opposing the sentiment requested), Incoherent (an incomplete, unreadable, or confusing response, e.g. repeating whole phrases from the prompt), or Comply (a desired response to the prompt).\nThis multi-class approach to refusal detection enables us to differentiate useful responses from low-quality compliant responses from refusals. We include the Incoherent label as we find that older generative language models do not explicitly refuse prompts but often produce very low quality responses\nWe use GPT-3.5 Turbo to label responses in a zero-shot manner, given a prompt and response and the judge prompt template (available at"}, {"title": "Evaluation"}, {"title": "Humanness Experiments", "content": "To evaluate the degree to which synthetic content appears as authentic to humans, which we term \u201chumanness\", we task human participants with labelling election disinformation content written by humans and the 13 LLMs in table 1 as either human-written or AI-written. We conduct three experiments:\""}, {"title": "Content Generation", "content": "Human-written content was created by another research team with good general knowledge of the subject area, but who were not involved in model content generation.\nTo create AI-generated content, we prompt all LLMs with versions of the prompts outlined in table 6, where \"Write\" is replaced with \"Write 15 variations of\". In cases where LLMs were unable to comply with this instruction (i.e. did not provide a list of 15 variations as a response this was the case for GPT-2, T5, Flan-T5, and GPT-Neo), we instead prompt each model fifteen times for each prompt. For experiments la and 1b, the name of the MP receiving the lowest proportion of refusals from DisElect.MP (not disclosed) was used in the prompts for AI and human written content. This name, and any mention of specific party or constituency, was redacted from responses before presentation to participants, instead replaced with tokens e.g. \"{MP}\",\"{PARTY}\"."}, {"title": "Experimental Design", "content": "Experiments were designed on Qualtrics. Each participant was randomly assigned to an experimental condition, containing content across the four stages from one LLM only. Participants were presented with the instructions visible in table 8. At each stage, participants saw 15 items generated by one LLM model, plus 15 human-written items, alongside the prompt used to generate that content (full list visible in table 6). Participants were asked to indicate whether they thought each item was written by a human or generated by an AI model. We included one attention check per stage. Accordingly, each participant saw 31 items per stage, randomly ordered. We also asked demographic questions: age, gender, digital literacy & familiarity, education level and political orientation.\nThis approach means participants may be able to identify linguistic patterns in multiple items generated by the same model, giving them clues about these items being LLM-generated. While this could be considered a limitation, the baseline comparison for each model is humans only, so a model producing easily-identifiable, repetitive content would not make another model seem more \"human-like\"."}, {"title": "Sampling", "content": "We recruited 780 UK-based, English-speaking participants who were over the age of 18 for each experiment (N = 2,340). These numbers account for participants that failed attention checks and as such were disregarded and replaced. We balance left-wing and right-wing participants for experiments la and 16. Experiment 2 focused exclusively on"}, {"title": "Results", "content": "Results for DisElect on the models listed in table 1 are shown in fig. 1. Refusal rates are generally low - only three models (Llama 2, Gemma, Gemini 1.0 Pro) explicitly refuse to comply with more than 10% of prompts in any use case. Phi-2 and Llama 2 also produce some refusals across both experiments, but a considerably smaller number than the aforementioned models. The oldest of the refusing models (Llama 2) was introduced mid-2023, reflecting that refusals are a phenomenon introduced through safety-focused model fine-tuning that may not be present in earlier models."}, {"title": "What drives refusal?", "content": "Focusing on the 3 models that do refuse a significant number of prompts (Llama 2, Gemma, and Gemini 1.0 Pro), we present proportions of prompts refused by variables values (see table 3 and table 4) in fig. 2. Across both use cases, all models are much more likely to refuse when prompted to use a right-wing persona than a left-wing persona. Refusals for left-wing personas are higher for all models in DisElect.MP than in DisElect.VT. Prompting models to generate news articles returns more refusals in DisElect.MP than DisElect.VT. There is some variation in refusal on different subjects, with prompts about voter ID, colluding with China, and drug possession drawing higher refusal rates than other options in respective use cases.\nCalculating the Spearman's Rank Correlation on refusal rates between models for each variable reveals that models often do not align on what to refuse: only the persona variable sees no variation between refusal rankings for possible values (left-wing and right-wing) for both use cases. Correlation for subject in DisElect.MP is also high (p = 0.74 median). Correlation is lowest for the pipeline stage as a variable in DisElect.VT (p = 0.00 median).\nFor the 50 MP names included in DisElect.MP, we find that refusals are normally distributed (64% within 1 standard deviation of the mean). fig. 3 presents refusal rates for groups of MPs by party and gender. The Spearman's Rank Correlation between models on these groups is high, at a median value of p = 0.90 when grouping MPs by party and gender, indicating alignment between models on which MPs to refuse to generate disinformation about. All 3 models are more likely to refuse to generate content for a female MP than a male MP, and for a Labour MP than a Conservative (or other) MP. This persists at a more granular level, with female Labour MPs seeing the highest level of refusal of any party-gender group on average, and male Conservative MPs the lowest."}, {"title": "How well can people identify AI-generated content?", "content": "We will now present the results from the experimental part of our study. We collate datasets about the number of times participants assign pieces of content as \u201chuman\u201d for the three experiments, aggregating by the LLMs in table 1 and the stages of the information operation pipeline in table 6 for each experiment detailed in ?? to calculate \"humanness\" for each LLM. We define humanness as the number of times humans (mis)label an AI-generated item as human over the total number of labels assigned to that AI-generated item (#AIH).\nWe plot humanness per LLM overall and broken down by experiment and pipeline stage in fig. 4, fig. 5, and fig. 6. Overall, 9/13 models achieve at > 50% humanness on average, indicating that the majority of models tested produce content that is indiscernable from human-written content for the same prompt. Meanwhile, 6/13 models achieve very high levels of humanness (>= 75%) on at least 5% of entries. However, variation is high: the coefficient of variation is higher than the interquartile range for all models, and all models see at least 12% of their items receive lower levels of humanness (<50%). This implies that the ability to discern AI-generated and human-written varies greatly between human participants.\nLlama 3 and Gemini achieve the highest humanness of all models (62% and 59% respectively). Both models see at least 15% of the entries achieve >= 75% humanness (19% and 17% respectively).\nWe observe two models (Llama 2, Gemma) with bimodal distributions of humanness proportions in fig. 4: many pieces of content receiving low humanness while others receive high humanness. As shown earlier, these two models produce the highest level of refusals on DisElect. While prompts in table 6 were constructed to minimise refusals, this is not always avoidable. Refusal responses are trivial for a human participant to identify as AI-generated. Aside from these items, Llama 2 and Gemma would receive among the highest levels of humanness. However, a model that frequently refuses to generate content will in the end not be useful for scaling an information operation."}, {"title": "Model development over time", "content": "The models tested in this study (table 1) cover a range of release dates going back to 2019. The highest performing two models we tested in terms of humanness on average (Llama 3 and Gemini 1.0 Pro) are also among the newest models we tested. The worst two models on average (GPT-2 and T5) were the two oldest models we tested. We observe a negative (Pearson) correlation between model age and humanness (p = \u22120.82), adding to the evidence that newer LLMs are able to generate more human-like content. \nThis trend is not absolute. As noted, Llama 2 and Gemma content used in the experiments contains refusals. This impacts Llama 2's overall humanness to a greater degree than Gemma's, making Llama 2 more comparable to much earlier models such as"}, {"title": "Above-human-humanness", "content": "We can examine the humanness of the human-written content, when viewed alongside content generated by each LLM (fig. 7, fig. 9). We observe a strong negative correlation (p = -0.92) between humanness of AI-generated content and humanness of human-written content. In other words, when presented with pieces of content written by humans and AI models, the more a human participant mislabels AI-generated entries as human-written, the more human-written entries they mislabel as AI-generated. This is to be partially expected by our experiment design, considering that, even though we did not guide participants on how much AI content was present in the items they were viewing, they would likely expect the proportion to be approximately 50%. However, it is nevertheless a potential indicator that, in addition to enabling disinformation, AI generated content may also start to undermine trust in good faith human content.\nLlama 3 and Gemini, the two highest performing models, achieve better humanness than human-written content on average. This mirrors findings from Jakesch et al. [37], in that content produced by frontier AI models appears to be perceived as more human than human-written content.\nExperiment la sees the lowest overall human humanness (and highest LLM humanness) of any experiment, visible in fig. 9. Llama 3 and Gemini achieve above-human-humanness in both experiments la and 1b, but not in experiment 2, which also sees the lowest overall model humanness and highest overall human humanness."}, {"title": "What factors explain perceptions of humanness?", "content": "We continue investigating factors behind humanness by fitting a series of mixed effects logistic regression models, including sociodemographic features of participants alongside content similarity, pipeline stage, and model, across the 3 experiments.\nEach observation is a single classification of AI-generated content made by a single participant, and the dependent variable is an assignment of whether the content was written by a human (1=yes, 0=no). We include the following independent variables: age, gender, education, politics, TF-IDF distance (1- TF-IDF similarity between content in question and other content generated by model for the same prompt), the LLM used to generate content, and pipeline stage. Politics is measured as a scale of 0-100, where 0 is extreme left-wing and 100 is extreme right-wing. Age, politics, and TF-IDF distance were standardized to have a mean of 0 and a standard deviation of 1. Reference levels for gender, education, LLM, and pipeline are set to male, no degree, GPT-2, and account generation respectively. We use participant ID as a random effect to account for multiple observations from the same participant. We fit 4 models (3 separate experiments plus an overall model) using the 1me4 R package [54].\nWe first assess how sociodemographic factors affect the humanness of AI-generated content. The results show that humanness does not vary with gender or education, both of which are non-significant across all three experiments. Age has a significant positive association with humanness for experiment la only a one standard deviation increase in age corresponds to an 8% increase in the odds of classifying content as human. The absence of this effect in experiment 16 points to a difference in the content generated by LLMs from left-wing perspectives that presents as greater difficulty in discerning synthetic and authentic content for older participants.\nPolitics has significant positive associations for experiment 1b and experiment 2 a one standard deviation increase in politics corresponds to respective increases of 8% and 12% in the odds of classifying content as human. Again, the presence of this effect for one political persona (right-wing) indicates a minor yet statistically significant difference that presents as a greater propensity by right-wing-identifying participants to label content content written from a right-wing perspective as human. This effect is not present for left-wing identifying participant with left-wing content. However, in overall demographic terms, what is most striking is a lack of strong relationships between any"}, {"title": "Disinformation domains versus personas", "content": "Given the 3 humanness experiments conducted across 2 domains (MPs, voting) and 2 personas (left/right-wing), we see that in some cases results are more closely aligned by domain: overall humanness, patterns in humanness across experiments and pipeline stages, and the prevalence of above-human-humanness are more similar in the two MP experiments (1a, 1b). In other cases, results more closely aligned by persona: the two right-wing experiments (16, 2) seem to share more as to what demographic factors are the strongest predictors of humanness. This suggests that overall model performance in"}, {"title": "Cost Comparison", "content": "This study focuses on the potential efficacy of LLMs in election disinformation operations, but that does not account for the potential cost of their usage. Due to the complex nature of evaluating the costs of a high-quality traditional information operation, and comparing that to one utilising LLMs, we present a simplified comparison focused just on the news article generation stage of our disinformation pipeline.\nGu et al. [5] estimated that \u201ccontent distribution service\u201d Xiezuobang charges 100 renminbi (RMB/CNY) (approx. 15USD) for a 500 to 800-word article. Assuming production of 10 articles per day, we can estimate an effective information operation may cost around around 4,500USD per month. In comparison, generating the same volume of content through Gemini (one of the best models we tested) via Google's Gemini API would cost 0.30USD. Imagining that a malicious actor may prefer to host the technology themselves, we estimate that deploying Llama 3 70B (the other best model we tested) would cost 9USD (Cost of time to generate content on a remote virtual machine with an A100 GPU). Imagining that a malicious actor may not have access to this level of compute, we have shown that many smaller open-source/open-weight models perform comparably to much larger models - these models can be run on existing personal computers with little technical overhead, bringing costs to zero in some cases."}, {"title": "Discussion", "content": "In this paper, we introduced the DisElect evaluation dataset for measuring LLM compliance with election disinformation tasks, and conducted experiments to measure the extent to which LLM-generated content for election disinformation operations can pass as human-written. We tested 13 LLMs released over the past 5 years, and found that most LLMs will comply instructions to generate"}]}