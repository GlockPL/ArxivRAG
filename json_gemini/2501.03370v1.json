{"title": "Advanced Machine Learning Techniques for Social Support Detection on Social Media", "authors": ["Olga Kolesnikova", "Moein Shahiki Tasha", "Zahra Ahani", "Ameeta Agrawal", "Rau'l Monroy", "Grigori Sidorov"], "abstract": "The widespread use of social media highlights the need to understand its impact, particularly the role of online social support. This study uses a dataset focused on online social support, which includes binary and multiclass classifications of social support content on social media. The classification of social support is divided into three tasks. The first task focuses on distinguishing between supportive and non-supportive. The second task aims to identify whether the support is directed toward an individual or a group. The third task categorizes the specific type of social support, grouping it into categories such as Nation, LGBTQ, Black people, Women, Religion, and Other (if it does not fit into the previously mentioned categories). To address data imbalances in these tasks, we employed K-means clustering for balancing the dataset and compared the results with the original unbalanced data. Using advanced machine learning techniques, including transformers and zero-shot learning approaches with GPT3, GPT4, and GPT4-0, we predict social support levels in various contexts. The effectiveness of the dataset is evaluated using baseline models across different learning approaches, with transformer-based methods demonstrating superior performance. Additionally, we achieved a 0.4% increase in the macro F1 score for the second task and a 0.7% increase for the third task, compared to previous work utilizing traditional machine learning with psycholinguistic and unigram-based TF-IDF values.", "sections": [{"title": "1. Introduction", "content": "Platforms such as YouTube, Instagram, Snapchat, Tik-Tok, and Facebook have become integral to daily life world-wide (Statista, 2024). These platforms allow users to observe and interact with others, obtaining social rewards (Meshi et al., 2015) that act as reinforcers, bringing people back to these sites repeatedly and for substantial durations (Stewart, 2016). However, problematic social media use has been consistently linked to negative mental health outcomes. A meta-analysis concluded that greater problematic Facebook use is associated with higher levels of depression and anxiety (Marino et al., 2018). Another recent meta-analysis, which focused on problematic use across any social media platform, found similar associations, indicating that greater problematic social media use correlates with increased depression, anxiety, and loneliness (Huang, 2022). One potential mechanism for mediating this relationship could be the amount of social support an individual receives. Researchers have speculated that more time on social media leads to a lack of face-to-face social interaction and that this lack of in-person social support may be responsible for the association between problematic social media use and negative mental health (Shensa et al., 2017). Indeed, more time spent on social media has been associated with less perceived real-life social support (McDougall et al., 2016). Despite these findings, more comprehensive data and an understanding of the complex relationship between social media use and mental health is still needed. To address this gap (Ahani et al., 2024b) have introduced a new dataset focused on online social support. Online social support involves behaviors, communication, and interactions that demonstrate care and appreciation for individuals, thus fostering a sense of belonging and helping to cope with life's challenges (Ko et al., 2013). This dataset features two binary levels: the first distinguishes between social support and non-social support, while the second categorizes social support into individual and group support. The group support category is further divided into six subcategories: Nation, Women, Black People, LGBTQ, Religion, and Other. In their study Ahani et al. (2024b) applied a range of machine learning techniques to classify the data, including traditional classifiers.\nThis research employs advanced techniques such as Transformers, and zero-shot learning approaches with models like GPT-3, GPT-4, and GPT-4-0. The aim is to develop prediction models for social support across various contexts.\nThe Transformer model was initially applied to natural language processing (NLP) tasks, where it delivered significant performance improvements (Devlin et al., 2018). For instance, Vaswani et al. (2017) introduced the Transformer architecture, which is based on the attention mechanism, to tackle machine translation and English constituency parsing tasks. Devlin et al. (2018) later developed BERT (Bidirectional Encoder Representations from Transformers), a new language representation model that pre-trains a transformer on the unlabeled text while considering the context of each word bidirectionally. Upon its"}, {"title": "2. Related work", "content": "Social support is the perception that one is cared for, valued, and part of a network of reciprocal obligations (Cobb, 1976). Ahani et al. (2024b) conducted a study on online social support by collecting data from YouTube comments. The research analyzed comments from 17 videos across various categories such as nation, black people, women, religion, LGBTQ, and others. Initially, 66,272 comments were gathered. After removing duplicates and non-English comments, the dataset was refined to 42,695 comments. From this dataset, 5,000 comments containing specific keywords and another 5,000 randomly selected comments were chosen. Comments associated with the selected videos underwent no additional filtering or selection, allowing for an accurate assessment of the distribution of supportive comments for each video and the exploration of related aspects. The study examined the psycholinguistic, emotional, and sentiment features through Tash et al. (2024b,c): 1. Supportive detection with LIWC 2. Supportive detection with TF-IDF 3. Supportive detection with TF-IDF and LIWC combined. Subsequently, traditional machine learning algorithms were applied. The best results are presented in Table 1.\nHope is an extraordinary human ability that allows individuals to envision future events and their potential outcomes with flexibility. Balouchzahi et al. (2023); Arif et al. (2024) introduced PolyHope, the first multiclass hope speech detection dataset in English. The dataset was developed by collecting around 100,000 English tweets, which were then preprocessed down to approximately 23,000 tweets. From this, a random sample of 10,000 tweets was chosen for annotation. After annotation, the dataset included 4,081 tweets labeled as NotHope, 2,335 as GeneralizedHope, 982 as RealisticHope, and 858 as UnrealisticHope, resulting in a total of 8,256 tweets. To evaluate the dataset's effectiveness, various baseline models were tested using different learning approaches, including traditional machine learning, deep learning Tash et al. (2024a); Ahani et al. (2024a); Ahani et al., and transformer-based methods. The results for the top models in each learning approach, showing the average macro F1 scores for both binary and multiclass classification tasks on the PolyHope dataset, are shown in Table 2.\nZero-shot learning (ZSL) is a difficult task because there is no labeled data available for unseen classes during the training phase. The primary focus of the work by Xiong et al. (2021) is on tackling the Extreme Zero-Shot Multi-label Classification (EZ-XMC) problem, which involves classifying text instances into numerous labels without supervision. The authors introduce MACLR (Multi-scale Adaptive Clustering with Label Regularization), a method that pre-trains Transformer-based encoders using self-supervised contrastive learning to generate effective semantic embeddings. They utilize four public EZ-XMC datasets for their experiments. The MACLR method is compared against several baseline models, including TF-IDF, XR-Linear, GloVe, SentBERT, Paraphrase MPNet, SimCSE, and ICT, as well as few-shot learning models like Astec, SiameseXML, ZestXML, and a fine-tuned SentBERT. MACLR outperformed all other baselines, achieving notable results such as a P@1 score of 18.74% in few-shot scenarios with only 1% sampled labels. The findings indicate that MACLR significantly improves precision and recall across datasets and remains robust even with minimal supervision, demonstrating its effectiveness in domains with many cold-start labels.\nSignificant advancements in natural language processing have been propelled by large language models (LLMs), sparking widespread interest in artificial intelligence among both academics and the general public since the launch of OpenAI's ChatGPT in late 2022.\nThe use of LLMs in computational sociology, specifically for supervised text classification tasks, is explored by Chae and Davidson (2023). Four LLM architectures were evaluated: two models based on the BERT framework and two variants of OpenAI's GPT-3, namely Ada and Davinci. Social media posts about politicians were used as data, sourced from a widely used Twitter dataset consisting of 1,691 tweets and a new collection of 2,400 Facebook comments, both centered on the 2016 US Presidential election. Experiments ranged from prompt-based zero-shot learning to fine-tuning with thousands of annotated examples. It was found that LLMs could achieve high accuracy in text classification, often surpassing traditional machine learning baselines. Key findings included the efficiency of fine-tuning smaller models for cost-effective yet accurate performance and the necessity of evaluating model biases and ensuring transparency and reproducibility in research. The best result is achieved by GPT-3 Davinci, GPT-3 Ada, and BART-MNLI"}, {"title": "3. Methodology", "content": "The dataset utilized in this study, as referenced from Ahani et al. (2024b), was derived from YouTube comments extracted from 17 diverse videos covering topics such as nationality, race, gender, religion, and LGBTQ issues, among others. Initially, a pool of 66,272 comments was gathered, which was subsequently refined to 42,695 comments after eliminating duplicates and non-English comments. Following this, a subset of 10,000 comments containing specific keywords was randomly selected. It's noteworthy that no further filtering or selection processes were applied to comments associated with the chosen videos. This methodology facilitated an accurate examination of the distribution of supportive comments across each video while also exploring associated facets. Additional dataset details are included in Table 4. You can also find more examples across various tasks in the Table 4.\nAt first, data preprocessing consisted of eliminating duplicate comments and choosing only English tweets. Then, standardization of text data was conducted through tokenization, lower-casing, punctuation removal, stop word removal, and stemming or lemmatization. Emojis and emoticons were converted into text representations using the emot library \u00b9 Next, abbrevia-"}, {"title": "3.3. Experiments", "content": "Transformers are a type of deep learning model architecture known for their effectiveness in natural language processing tasks such as translation, summarization, and text generation. They utilize self-attention mechanisms to capture long-range dependencies in text, enabling the processing of entire sentences simultaneously. The Hugging Face platform is a prominent provider of transformer-based tools and models, offering an extensive library called Transformers. This library includes pre-trained models like BERT, GPT, and T5, which can be easily fine-tuned for various NLP tasks. Hugging Face also provides user-friendly APIs and a model hub, facilitating seamless integration and deployment of transformer models in applications.\nZero-shot learning is a machine learning technique where a model is able to make predictions on new, previously unseen classes or data without having been explicitly trained on them. This is achieved by leveraging knowledge from related tasks or using semantic information about the unseen classes, allowing the model to generalize its understanding to novel situations.\nZero-shot learning enables the models to classify comments as \"Supportive\u201d or \u201cNon-Supportive\u201d without requiring additional training data. Specific prompts were designed to instruct the models to determine the supportiveness based solely on the given comment. Multiple prompts were used to improve reliability and the most frequent response was selected as the final prediction. This method leverages the models' ability to understand and analyze text contextually, providing accurate sentiment analysis without the need for extensive labeled datasets. The parameter details can be found in Table 3. This work utilizes models like DeBERTa and BART, including variants such as\n\u2022 MoritzLaurer/deberta-v3-large-zeroshot-v2.0\n\u2022 MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n\u2022 MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n\u2022 sileod/deberta-v3-base-tasksource-nli\n\u2022 facebook/bart-large-mnli\nThese prompts were provided to the models, and their responses were analyzed to determine the final supportiveness label for each comment."}, {"title": "3.4. Dataset Balancing", "content": "In this study, the dataset was divided into training and testing subsets, with their distributions outlined in Table 6. The BERT/DistilBERT model was utilized for classification. To balance the dataset, the k-means algorithm was applied, as it is a widely accepted and standard baseline method for clustering."}, {"title": "4. Results and Analysis", "content": "Table 7 provides a comprehensive comparison of transformer-based models across different tasks, highlighting their performance measured by the macro F1-score. In Task 1, models such as 'bert-base-multilingual-cased' and 'distilbert-base-uncased' demonstrated strong macro F1-scores of 0.7636 and 0.7746, respectively, indicating their effectiveness in classifying comments as supportive or non-supportive. Task 2 showcased similar trends, with models like 'bert-base-multilingual-cased' and 'roberta-base' achieving impressive macro F1-scores of 0.8146 and 0.8357, respectively, reflecting their ability to handle varying comment contexts. Meanwhile, in Task 3, 'distilbert-base-uncased' and 'roberta-base' emerged as top performers, exhibiting macro F1-scores of 0.7863 and 0.7951, respectively, underscoring their proficiency in accurately classifying comments across different supportiveness levels."}, {"title": "4.0.2. Zero-shot setting", "content": "Table 8 presents the performance of various Hugging Face models across different tasks, with a focus on the macro Fl-score. For Task 1, the 'MoritzLaurer/deberta-v3-large-zeroshot-v2.0' model achieved a macro F1-score of 0.57, while other models like 'mDeBERTa-v3-base-mnli-xnli' and 'DeBERTa-v3-base-mnli-fever-anli' yielded macro F1-scores ranging from 0.47 to 0.58, indicating varying degrees of effectiveness in classifying comments for supportiveness. In Task 2, the 'deberta-v3-large-zeroshot-v2.0' model demonstrated a macro F1-score of 0.39, while other models ranged from 0.40 to 0.64, suggesting differing capabilities in handling comment classification tasks. Similarly, for Task 3, models exhibited macro F1-scores ranging from 0.24 to 0.48, with the 'deberta-v3-large-zeroshot-v2.0' model achieving a score of 0.48, showcasing variations in performance across different models and tasks."}, {"title": "4.1. Balanced data sets", "content": "To balance the dataset, the k-means algorithm was employed, as it is a widely recognized and standard baseline method for clustering. The bar chart illustrates the F1-scores for three tasks when using normal and balanced datasets. For Task1 and Task2, the normal dataset outperformed the balanced version, achieving F1-scores of 0.80 and 0.86, respectively, compared to 0.75 in both cases for the balanced dataset. However, Task3, which involves six labels with a highly skewed distribution, experienced a significant performance drop. The F1-score fell sharply from 0.67 (normal) to just 0.11 (balanced).\nThis drastic decline in Task3 can be attributed to two key factors. First, the severe imbalance in label distribution resulted in some classes having far fewer examples than others. When k-means clustering was applied, it attempted to balance the dataset by oversampling minority classes and undersampling majority classes. However, this process led to the removal of a substantial amount of valuable data, including critical information from the majority classes."}, {"title": "5. Error analysis", "content": "The confusion matrix presented shows the performance of a classification model distinguishing between \"Supportive\" and \"Non-Supportive\" instances. The overall accuracy of the model is 84.21%, with a misclassification rate of 15.79%. The model performs well in identifying \"Non-Supportive\" instances, achieving an 89.00% true positive rate. However, it struggles more with \"Supportive\" instances, with only a 67.58% true positive rate and a relatively high false negative rate of 32.42%. This indicates that the model is more prone to incorrectly labeling \"Supportive\" instances as \"Non-Supportive\" compared to the reverse. Improving the model's sensitivity to \"Supportive\" instances while maintaining its strong performance on \"Non-Supportive\" instances would enhance overall accuracy.\nThe confusion matrix indicates that the classification model has an overall accuracy of 89.98%, with a misclassification rate of 10.02%. For the \"Group\" class, the model correctly identified 94.32% of instances, but incorrectly classified 5.68% as \"Individual\". For the \"Individual\" class, 71.39% were correctly identified, while 28.61% were misclassified as \"Group\". This disparity suggests the model is more proficient at identifying \"Group\" instances compared to \"Individual\" instances, which are misclassified at a higher rate. Improving the classification of \"Individual\" instances could enhance overall performance.\nThe confusion matrix for this multi-class classification problem shows varying levels of accuracy across different classes. The model performs exceptionally well in classifying \"Nation\" (92.97%) and \"LGBTQ\" (94.81%) instances. However, it struggles with \"Religion,\" where it correctly classifies only 47.37% of instances, misclassifying a significant portion as \"Nation.\" \"Other\" has a moderate classification accuracy of 75.19%, with notable misclassifications across other classes, particularly \"Nation\" (16.35%). The \"Black Community\" and"}, {"title": "6. Discussion", "content": "The current study reveals that users on YouTube tend to express more support for groups than for individuals, with significant support for different nations without heavy religious influence. Recent data indicates a focus on nations and communities like LGBTQ+ individuals and Black people, highlighting a social emphasis on national and community identities over religious considerations. This study serves as a foundational step in introducing the task of social support detection aimed at fostering support and positivity as an alternative to merely filtering out hate speech."}, {"title": "7. Conclusions and future work", "content": "In this study, we utilized a novel dataset focused on social support, categorized into individual and group levels. The group category was further subdivided into various segments, including Nation, Other, LGBTQ, Black people, Women, and Religion. To analyze this data, we employed several models, starting with zero-shot learning using large language models (LLMs) such as GPT-3, GPT-4, and GPT-4-turbo.\nFor classification, we also utilized a range of models available on Hugging Face, the results of which are detailed in Table 4. Our approach yielded significant improvements in macro F1 scores, with an increase of 7% in Task 2 and 8% in Task 3. Notably, the transformers model, roberta-base, consistently demonstrated superior performance across all three tasks, outperforming all other models we tested.\nLooking ahead, our future work will focus on integrating different LLMs, enhancing prompt engineering, and exploring few-shot learning techniques to further improve classification accuracy."}, {"title": "8. Limitation", "content": "Despite the promising results, our study has several limitations. Firstly, the novel dataset on social support, while comprehensive, may have inherent biases due to its specific categorization into individual and group levels and further subdivision into segments such as Nation, Other, LGBTQ, Black people, Women, and Religion. These categorizations might not fully capture the complexity and nuances of social support across different contexts. Additionally, while we employed advanced models including zero-shot learning with GPT-3, GPT-4, and GPT-4-turbo, the generalizability of these models is limited by the quality and diversity of the training data. Furthermore, the superior performance of the roberta-base transformer model across all tasks suggests a potential over-reliance on this specific architecture, which may not necessarily translate to other datasets or real-world applications. Lastly, our current approach does not fully leverage few-shot learning, which could potentially enhance model performance in low-resource scenarios."}]}