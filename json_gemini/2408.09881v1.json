{"title": "Uncertainty Quantification of Pre-Trained and Fine-Tuned Surrogate Models using Conformal Prediction", "authors": ["Vignesh Gopakumar", "Ander Gray", "Joel Oskarsson", "Lorenzo Zanisi", "Stanislas Pamela", "Daniel Giles", "Matt Kusner", "Marc Peter Deisenroth"], "abstract": "Data-driven surrogate models have shown immense potential as quick, inexpensive approximations to complex numerical and experimental modelling tasks. However, most surrogate models characterising physical systems do not quantify their uncertainty, rendering their predictions unreliable, and needing further validation. Though Bayesian approximations offer some solace in estimating the error associated with these models, they cannot provide guarantees, and the quality of their inferences depends on the availability of prior information and good approximations to posteriors for complex problems. This is particularly pertinent to multi-variable or spatio-temporal problems. Our work constructs and formalises a conformal prediction framework that satisfies marginal coverage for spatio-temporal predictions in a model-agnostic manner, requiring near-zero computational costs. The paper provides an extensive empirical study of the application of the framework to ascertain valid error bars that provide guaranteed coverage across the surrogate model's domain of operation. The application scope of our work extends across a large range of spatio-temporal models, ranging from solving partial differential equations to weather forecasting. Through the applications, the paper looks at providing statistically valid error bars for deterministic models, as well as crafting guarantees to the error bars of probabilistic models. The paper concludes with a viable conformal prediction formalisation that provides guaranteed coverage of the surrogate model, regardless of model architecture, and its training regime and is unbothered by the curse of dimensionality.", "sections": [{"title": "1 Introduction", "content": "Partial Differential Equations (PDEs) governing physical processes are solved using complex numerical modelling simulation codes. Though these codes offer us a mathematically rigid method of solving the relevant equations, the obtained solutions are often limited to discretised domains and require computationally expensive iterative solvers like the finite volume schemes and the finite element methods. Such simulation codes have become central to multiple scientific disciplines in biology (Hospital, Adam et al., 2015), engineering (Giudicelli et al., 2024), and climate science (Danabasoglu et al., 2020) (Lavin et al., 2021), but they possess difficulty in deploying for rapid iterative modelling, often required while exploring a design space of solutions. Machine learning offers an alternative data-driven route for obtaining quick, inexpensive approximations to numerical simulations (Bertone et al., 2019; Karniadakis et al., 2021). Data-driven surrogate models are often built by distilling the spatio-temporal information characterised by the simulations into parameterised machine learning models. Due to the efficiency, cost-effectiveness, and relative accuracy of modelling, neural networks have become ubiquitous within scientific modelling, and they are of primary importance in tackling large-scale PDEs in climate (Lam et al., 2023; Kurth et al., 2023), computational fluid dynamics (Jiang et al., 2020; Pfaff et al., 2021), and nuclear fusion (van de Plassche et al., 2020; Gopakumar and Samaddar, 2020).\nAs effective as these surrogate models (emulators) are, they remain approximations of the true physical system under study. There are often many layers of approximations as the simulation codes approximate the PDEs and the PDEs themselves are approximations to the true physical system. They often fail to quantify the uncertainty associated with their approximation, i.e. the approximation error of the emulator with that of the numerical code. Irrespective of the domain of operation they were trained on, models often claim to provide confident outputs. This is problematic for two reasons: (a) without an assessment of the confidence of the output, erroneous predictions can lead to severe consequences downstream; (b) The cost of training these models can be prohibitively high, and without appropriate uncertainty quantification the overconfident predictions will have limited utility.\nSeveral works have attempted to provide uncertainty estimation for these surrogate models (Geneva and Zabaras, 2020; Alhajeri et al., 2022; Zou et al., 2024; Psaros et al., 2023), but they fail to provide statistical guarantees over the error bars attached to the prediction and to scale to complex scenarios (Abdar et al., 2021). They also require ensemble training Lakshminarayanan et al. (2017), extensive sampling (MacKay, 1992), and perhaps architectural modifications (Gal and Ghahramani, 2016), successively increasing the amount of computational resources required. There remains a concerning question of validating the surrogate model's outputs for a specific downstream application. Conformal prediction (CP) (Vovk et al., 2005) provides a framework that allows us to compute statistically guaranteed error bars over pre-trained and fine-tuned models i.e. the error bars are calibrated to provide the required coverage. Conformal prediction relies on the calibration of model performance across a specific dataset representative of the desired prediction distribution, and utilising the calibration measures to help provide valid error bars for the model output.\nIn this paper, we conduct a thorough empirical study on conformal prediction for pre-trained and fine-tuned, neural-network-based surrogate models to equip their predictions with calibrated, guaranteed error bars, even on out-of-training-distribution scenarios. We demonstrate that we can obtain guaranteed coverage across spatio-temporal domains (high-dimensional outputs), irrespective of the choice of model, data and training conditions. Our work provides a rigorous method to check the usefulness of a pre-trained surrogate model, its validity and applicability in an inference/production scenario.\nWe conduct an empirical study that demonstrates at scale the estimation of statistically guaranteed error bars for pre-trained and fine-tuned surrogate models using conformal prediction. Our focus lies on extending the conformal prediction framework to work for models predicting over a spatio-temporal domain. Through experimentation of increasing complexity, we demonstrate how we can provide guaranteed error bars for various kinds of neural-network-based surrogate models. Our experiments demonstrate that this method allows us to obtain guaranteed error bars with meaningful estimation even when we predict on data representing a different physical setting from that with which the model was trained on. Our study shows that irrespective of the dimensions of the output (our experiments explore models with output dimensions upwards of 20 million), we can get guaranteed coverage. We explore various methods of conformal prediction, comparing cost, performance, and associated architectural modifications. We conclude by stating that using conformal prediction we can obtain guaranteed marginal coverage across the spatio-temporal domain of interest, irrespective of the choice of model (both deterministic and probabilistic), training data and conditions."}, {"title": "1.1 Pathway to Addressing Climate Change", "content": "Machine-learning-based surrogate modelling promises computational efficiency and allows for data-driven discovery at scale. In modelling tasks, such as computational fluid dynamics, nuclear fusion, and weather forecasting, it is increasingly crucial to provide accurate, robust, and timely uncertainty estimates. In such safety-critical systems Knight (2002), it is impervious that we supplement model predictions with calibrated uncertainty estimations. This can lead to improved decision-making processes in downstream tasks. The CP framework demonstrated in this paper pushes the envelope forward in uncertainty quantification for complex scientific modelling at scale, demonstrating industry-level safety-critical applications."}, {"title": "2 Conformal Prediction", "content": "Conformal prediction (Shafer and Vovk, 2008) answers the following question: Given some arbitrary dataset (X1,Y1), (X2,Y2), ..., (Xn, Yn), and some machine learning model f : X \u2192 Y trained on this dataset, what is the accuracy of f at predicting the next true label Yn+1 at query point Xn+1. CP (Vovk et al., 2005) extends the point prediction \u0177 of f to a prediction set $\\mathcal{C}_{\\alpha}$, which is guaranteed to contain the true label $Y_{n+1}$ with probability\n$\\begin{equation}\nP(Y_{n+1} \\in \\mathcal{C}^{\\alpha}) \\geq 1 - \\alpha.\n\\tag{1}\n\\end{equation}$\nCP is attractive for quantifying errors in machine learning since the inequality in Equation (1) is guaranteed regardless of the selected machine learning model and the training dataset {\\(X_i, Y_i\\)\\}_{i=1}^n, other than that the samples used for the calibration (estimating model performance) are exchangeable (a weaker form of i.i.d.).\nThere have been several variants of CP since its original proposal by (Vovk et al., 2005). Inductive conformal prediction (Papadopoulos, 2008), which we pursue in this work, splits the data into a training set (the usual training set for the underlying ML model) and a calibration set, used to construct prediction set $\\mathcal{C}^{\\alpha}$ for each \u03b1 value. The prediction sets are constructed, such that Equation (1) holds, by comparing the trained ML model to the unseen calibration data using a non-conformity score.\nThe inductive CP framework explored further in this paper follows a three-step procedure as outlined in Figure 1. Initially, we perform a calibration of the model outputs, where a non-conformity metric of the model's performance is evaluated. This is followed by the quantile estimation, which is performed by obtaining the required quantile from the cumulative distribution of the non-conformity scores obtained in the previous calibration step. The final step involves estimating the prediction sets by applying the estimated quantile over the model predictions in the interested prediction regime."}, {"title": "2.1 Conformal Prediction over a Spatio-Temporal Domain", "content": "Though conformal prediction as a method of performing uncertainty quantification was introduced for fixed single-point outputs in (Vovk et al., 2005), it has been gaining popularity for usage across spatio-temporal data in the recent past (Sun, 2022). In (Stankeviciute et al., 2021), inductive conformal prediction is extended further to provide uncertainty guarantees on time-series data for RNNs, where (Xu and Xie, 2021) explores the same for dynamic"}, {"title": "2.1.2 Overview", "content": "The models we consider are those trained to model the evolution of spatio-temporal field variables defined by physical processes, such as those described in numerical PDE modelling, fusion diagnostics and weather forecasting. Each modelling task is perceived as an initial value problem, where the calibration and prediction set are pairs of inputs and outputs characterised by the initial condition and their respective solutions. Within the case of PDE modelling, each datapoint is a single numerical simulation, whereas, for experimental modelling (such as fusion and weather), the datapoint is a single forecast driven by an initial condition.\nWithin our framework, we perform calibration for each cell individually, estimating the marginal coverage for each cell of the spatio-temporal tensor as demonstrated in Figure 2 i.e., the coverage is guaranteed for each cell taken individually. CP is performed across each spatio-temporal point output by the model, resulting in upper and lower coverage bands for each point. Upon calibrating for the error bars cell-wise, we averaged over each to estimate the coverage across the simulation domain. Within the estimation of the prediction sets, we don't consider the influence of adjacent field points and implicitly expect the model to extract that within the learning process. We expect the discretised spatio-temporal domain to be the same across the calibration and prediction set.\nWithin the scope of this paper, a formulation of the CP framework, aiming to provide statistically valid, dimensionality independent, marginal coverage over spatio-temporal prediction models is proposed. We demonstrate the efficacy of our approach through comprehensive and rigorous experimentation across a range of spatio-temporal models from toy PDE problems to complex multi-physics models and state-of-the-art neural weather and Fusion models."}, {"title": "2.1.3 Formal Definition", "content": "We define a parametric model Y = f(X) that learns to map the evolution of an initial temporal sequence of spatial fields (X \u2208 $\\mathbb{R}^{T_{in}\\times N_x\\times N_y\\times N_{var}}$) to a later temporal sequence of spatial fields ($\\tilde{Y}$ \u2208 $\\mathbb{R}^{T_{out}\\times N_x\\times N_y\\times N_{var}}$). The model inputs and outputs are characterised by 4D tensors, where Tin, Tout represents the temporal dimension of the initial states and forecast respectively, Nx represents the x-dimension, Ny represents the y-dimension and Nvar the field dimension (number of modelled variables). The calibration procedure is defined as $\\hat{q}$ = $\\mathcal{\\hat{C}}(\\tilde{Y}, Y)$, utilising the model prediction (TildeY) and ground truth (Y) to estimate the quantile ($\\hat{q}$) associated with the desired coverage. The operation is executed in a point-wise manner since both $\\tilde{Y}$, Y, $\\hat{q}$ \u2208 $\\mathbb{R}^{T_{out} \\times N_x\\times N_y\\times N_{var}}$. The quantile is further utilised (as given in Section 2.2) to obtain the lower (L) and upper error bars (U) across each of the cells to form the prediction set C, where L and U have the same dimensionality as $\\hat{q}$. At evaluation for a prediction point Xn+1 with true label Yn+1, the expectation of coverage, i.e. the prediction set, is guaranteed to satisfy\n$\\begin{equation}\nE\\[ (Y_{n+1} \\geq L) \\cap (Y_{n+1} \\leq U)\\] \\geq 1 - \\alpha.\n\\tag{2}\n\\end{equation}$\nEquation (2) is statistically guaranteed to hold for each cell of the spatio-temporal tensor, provided we provide sufficient samples and exchangeability is maintained (Vovk, 2012)."}, {"title": "2.2 Non-conformity Scores", "content": "A non-conformity score can be described as a measure of the model's performance with respect to the calibration dataset. Thus, the scores are expressed as a function of the trained model, the calibration inputs, and outputs (Angelopoulos and Bates, 2023). Being expressed as a measure of the deviation of the model from ground truth, we focus on three methods of estimating the non-conformity scores for a model:\nQuantile Regression (CQR): Outlined by (Romano et al., 2019), three models are trained to perform conformal prediction. The lower and upper coverage bands are estimated by models trained to output the 100 \u00d7 \u03b1th and 100 \u00d7 (1 \u2212 \u03b1)th percentile, while the median is learned by training the model to output the 50th percentile. During training the models are estimated to output the \u03b1th percentile by optimising the quantile loss (Koenker, 2005). The non-conformity score measures the distance of the calibration data point from the nearest coverage band: s(x,y) = max{f(x) \u2212y,y \u2212 f(x)}. Using the upper and lower quantiles, CQR offers an initial coverage that can be adjusted and calibrated to perform conformal prediction: Using the required quantile (9), the prediction set is obtained as {f(x) \u2013 \u011d, f(x) + \u011d}.\nAbsolute Error Residual (AER): Requiring only a single model, trained to approximate the deterministic function itself, this method uses a more straightforward estimate of the model error to calibrate the coverage. Non-conformity scores are estimated by taking the absolute error of the model across the labelled calibration dataset (Lei et al., 2018): s(x,y) = |y \u2212 f(x)|. Upon deriving the quantile (\u011d), the prediction set is obtained as : {f(x) - \u011d, f(x) + \u011d}. This method requires no structural changes to the model, and only one deterministic model is required to obtain valid prediction sets. The method uses the distribution of the absolute error residuals (AER) from the model to construct sets with the correct coverage.\nStandard Deviation (STD): The models are probabilistic, i.e., they predict a distribution rather than a deterministic quantity, where the outputs are characterised by a mean \u00b5(x) and standard deviation \u03c3(x). The non-conformity score, $s(x, y) : \\frac{\\[y-\\mu(x)]}{\\sigma(x)}$ takes into account the uncertainty of the model and conformalises it to provide validity over the predictive uncertainty. The standard deviation of the output distribution provides estimates of the error bars and, thus is an initial measure of the coverage, which can be adjusted to ensure the correct coverage using conformal prediction. Upon deriving the quantile (\u011d), the prediction set is obtained as :"}, {"title": "2.3 Exchangeablity Assumptions", "content": "Within the experiments considered within this paper, surrogate modelling of spatio-temporal data is treated as an initial value problem (IVP). To map the evolution of the system under study, the surrogate model takes in the initial state(s) of the system and evolves the system autoregressively in time. By treating the modelling task as an initial value problem, each input-output pair (Xi, Yi) from the calibration and the prediction dataset is assumed to be exchangeable. The initial condition associated with each IVP is sampled i.i.d. from a distribution characterising the possible (large) range of interested initial conditions for the prediction task. The assumption relies on the further caveat that as outlined in Section 2.1.3, the structure of each spatio-temporal output is maintained. The CP framework requires exchangeability not just across the calibration data but across the prediction data as well (Angelopoulos and Bates, 2023). \u03a4o maintain exchangeability across the calibration and prediction datasets, we have to ensure that the full spatio-temporal context window across them remains the same and assume that they arise from the same distribution. The exchangeability assumption is more complex in the case of experimental data, and we discuss this further within the experiments in Section 3.7 and Section 3.8.\nWithin Section 3 we consider two types of experiments: surrogate modelling over simulations and experimental data. Within the simulation context (Section 3.1 to Section 3.6), the modelling task takes in the initial state of the system at t = 0 and then evolves the system (autoregressively or in one-shot) to time t = T. The model always starts at the initial condition(s) of the simulation and maps till a fixed time state T. Whereas in those modelling experimental data (Section 3.7 and Section 3.8) the modelling task takes the system at t = T tot = T + \u2206T. The starting time instance (initial condition) T changes in each exchangeable data pair but the model evolves it for a fixed time duration of \u2206T. Within the simulation setup, it is easier to gather data as required, exploring a data-abundant scenario. In the case of real-world experiments, data is scarce and we had to rely on breaking down available data to obtain the calibration data. We characterise this difference in how the exchangeable pairs are conceived for both the simulation and experimental data in Figure 4."}, {"title": "3 Experiments", "content": "To evolve spatio-temporal data describing physical systems, we train several surrogate models using simulation data built from numerical solvers designed for the PDE of interest. Across our experiments, we choose different kinds of surrogate models. Multi-Layer Perceptrons (MLPs) (Haykin, 1994), U-Nets (Ronneberger et al., 2015), Fourier Neural Operators (FNOs) (Li et al., 2021), Vision Transformers (Yin et al., 2022b), and Graph Neural Networks Scarselli et al. (2009) are utilised. These networks are commonly used as surrogate models within physics and engineering disciplines (M\u00e1nek et al., 2023; Gupta and Brandstetter, 2023; Wen et al., 2023; Geneva and Zabaras, 2022; Sanchez-Gonzalez et al., 2020).\nMLPs have been used as surrogates for modelling wind turbine blades (Lalonde et al., 2021), classifiers for high energy physics (Baldi et al., 2016), and for designing fusion reactors (M\u00e1nek et al., 2023). U-Nets and FNOs are structured as neural-PDE based approximators that can learn across the operator space rather than the function space of the mapping. They find utility in designing surrogates for fluid dynamics (Gupta and Brandstetter, 2023), studying carbon capture and mitigation (Wen et al., 2023), weather modelling (Kurth et al., 2023) and modelling plasma evolution within a nuclear fusion device (Gopakumar et al., 2023). Transformers have found application in PDE solving (Li et al., 2023b) and weather modelling (Nguyen et al., 2023). GNNs have been deployed to model Lagrangian dynamics of a PDE and in solving particle-in-cell scenarios (Brandstetter et al., 2022); they also have been applied to weather forecasting (Lam et al., 2023).\nThe MLP represents a fully connected neural network. The U-Net is a fully convolutional autoencoder (Hinton and Salakhutdinov, 2006) with skip connections (He et al., 2016; Takamoto et al., 2022). The FNO is an operator learning method that learns the kernel integration across the Fourier space (Li et al., 2021). Vision Transformers is an attention-based model that learns spatio-temporal rollouts through sequence patching of the spatial outputs and auto-regressive rollouts. Graph Neural Networks deploy message-passing across nodes representing the spatio-temporal information to model the evolution of a physical system. Depending on the specific experiment we adjust the model setup to either do a one-step forward-in-time mapping or perform an auto-regressive time roll-out over multiple time steps. All models were trained on an NVIDIA A100 GPU, while the calibration and estimation of prediction sets were done on a standard laptop."}, {"title": "3.1 1D Poisson Equation", "content": "The Poisson equation is a generalisation of the Laplace Equation. It relates the Laplacian of a field to the distribution of the sources/sinks in the field (Hackbusch, 2017). The elliptic PDE models various physical phenomena, such as electrostatics, gravitation and fluid dynamics. The Poisson equation is a steady-state problem (spatial-only), where the interest lies in mapping the initial distribution of the field to its final state. The state is given along the x-axis, discretising the domain [0,1] into 32 uniform grid points. An initial dataset consisting of 5000 simulation data points is generated and used to train an MLP. Another 1000 data points are generated to calibrate the trained model using the CP framework along with another 1000 for validating the procedure. The training data, calibration, and validation datasets are sampled from the same distribution within this case. Further details about the physics, data generation strategies and model training can be found in Appendix A.\nMultiple MLPs are trained to demonstrate the efficacy of different non-conformity scores while using the CP framework. To perform the CQR, three models are trained, modelling the 5th, 50th and 95th quantile, while for AER a single model is trained to output the evolved field using an L1-norm. For CP using STD, an MLP with dropout is trained to obtain a probabilistic model that learns the distribution of the evolved state. The performance comparison of each non-conformity score is laid out in Table 1.\nFigure 5 visualises the \u03b1 = 0.1 prediction sets (90% confidence intervals) of CQR (left), AER (centre), and STD (right). Considering the simplicity of the physical dynamics, the MLP learns the mapping to near perfection leading to tighter uncertainty estimates, with a narrow set characterising the uncertainty.2"}, {"title": "3.2 1D Convection-Diffusion Equation", "content": "Moving further up in complexity from a steady-state (spatial-only) to a dynamical case (spatio-temporal), consider the convection-diffusion equation in one dimension. As a combination of a parabolic and hyperbolic PDE, the equation is crucial in modelling transport phenomena in various science and engineering applications (Chandrasekhar, 1943). Following the schema laid out in Algorithm: CP Structure, an initial training dataset of 3000 simulations is generated by sampling the initial conditions and the physical factors of the PDE (diffusion coefficient and convection velocity) from a hypercube. A 1D U-Net is trained as the surrogate to model the spatio-temporal evolution given by the convection-diffusion equation. The model takes in the first 10 time instances to map to the next 10 time instances. The physics, sampling strategies, and model details can be found in Appendix B. The calibration and validation datasets are generated from a slightly shifted domain space as that used for training, where the simulation physics is modelled with less diffusion and more convection. This helps to explore a scenario of deploying a pre-trained surrogate model on an out-of-training-distribution setting.\nIn Figure 6, we calibrate the model with a dataset that characterises less diffusion and more convection than in the training dataset. The training dataset had instances of the PDE with diffusion coefficient sampled from the domain D\u2208 [sin(x/2\u03c0),sin(x/\u03c0)] and convection velocity c\u2208 [0.1,0.5]. The calibration and prediction dataset was sampled with diffusion coefficient D\u2208 [sin(x/4\u03c0), sin(x/2\u03c0)] and convection velocity c\u2208 [0.5,1.0]; see Appendix B for more details. Even when tested on this data from a new distribution, we obtain valid prediction sets with guaranteed error bars. The coverage offered by each"}, {"title": "3.3 2D Wave Equation", "content": "The wave equation is a second-order hyperbolic partial differential equation that describes the propagation of waves through a medium with numerous applications in acoustics, optics and quantum mechanics (Tipler, 2008). The experiment focuses on the spatio-temporal dynamics of a 2D Gaussian under the constraints of the wave equation. The detailed description of the physics, the numerical solvers, and the data generation strategy are given in Appendix C. To build the pre-trained model to evaluate the CP framework, initially, a training dataset of 500 simulations is generated by varying the parameterisation of the initial condition, i.e., the amplitude and position of the Gaussian. The data is used to train a U-Net and an FNO that models the temporal evolution of the wave dynamics.\nU-Net is designed to perform a single feed-forward map, taking in 20-time instances of the field to produce an output of shape [30, 33, 33], characterised by 30-time instances across a 33 \u00d7 33 spatial domain. A calibration and validation dataset comprising 100 simulations each is generated from the same distribution as that of the training data to evaluate CP for the wave equation. Coverage is estimated at each point in the output tensor as given in Section 2.1.3.\nComparing the different methods used for CP (in Figure 3 and Table 1), we see that the AER method is the least computationally intensive and offers valid coverage. This is further accentuated by Figure 7, where the tightness of the fit and the coverage obtained by performing CP at various values of \u03b1 is given. All methods provide valid coverage in tandem with the ideal coverage as expected by Equation (1), where those using quantile regression and AER errors provide overly conservative error bars. This is expected as Equation (1) offers an inequality, offering wide predictive sets for the specific value.\nIn Figure 7, we take the slice along the y-axis of the prediction sets at various \u03b1 levels to visualise the uncertainty of the predicted field for that slice. The slices represent the variation of the field along the y-axis for the final time instance of the prediction. Figure 7 shows that the tighter fits are obtained using the AER and STD methods, where the quantile regression fails to provide a tight fit although it provides good coverage.\nTo further iterate the impact of the CP framework as a method of uncertainty quantification, we further generate calibration and validation data, but this time from a different physical model. The newer datasets are sampled by solving the wave equation with wave velocity at half the speed of that used for the training data. This allows us to further benchmark the CP formulation in the paper in cases where the calibration and prediction regimes are outside of the training distribution. As given in Figure 2, when utilised outside of the training domain, the uncertainty captured by dropout (STD) fails to capture the modelling failure. However, by calibrating the uncertainty using the CP framework, it can capture the model's (in)capabilities with statistical guarantees, irrespective of the training conditions of the pre-trained model. This becomes particularly important when we consider the utility of the surrogate model in out-of-training-distribution scenarios, where we will need more data to fine-tune the network."}, {"title": "3.3.2 FNO", "content": "Furthering the experiment's complexity, FNOs are trained in an autoregressive framework to model the spatio-temporal evolution of the wave. The autoregressive model takes in the first 20 time instances to output the next 10 time instances, further unrolled for the next 60 time instances. We perform conformal prediction across the entire time rolled-out output of shape [60, 33, 33]. Since the FNO offers the best performance when trained using a relative LP norm, we did not perform CQR for experiments using the FNO.\nWhile performing conformal prediction over the FNO, we also demonstrate its utility in obtaining statistically guaranteed coverage for inference over a physics scenario different from the one on which the FNO was trained on as mentioned in the earlier section (also see Appendix C). The FNO is applied in the same in-distribution and out-of-distribution setting as the U-net. Through this, we demonstrate the validity of the error bars even when predicting out-of-training-distribution data.\nFigure 22 (in the appendix) shows that we obtain guaranteed coverage for predictions across calibration methods, even when tested outside the training distribution. As shown in Figure 8, the CP framework provides valid error bars for the desired coverage, irrespective of the training conditions. The only criterion that needs to be met is that the prediction regime and the calibration regime are exchangeable. The inductive CP framework can provide valuable uncertainty quantification, even when deployed to solve for those unseen sets of solutions. Furthermore, FNO offers a tighter fitting coverage than the U-Net used for the wave equation; see Figure 3."}, {"title": "3.4 2D Navier-Stokes Equations", "content": "The 2D Navier-Stokes equations are a family of PDEs that describe the motion of viscous fluids. The equations establish a mathematical model for the conservation of mass, momentum, and energy in a fluid flow. Considering its complexity and strong non-linearity the Navier-Stokes equations are solved using computational fluid dynamics (CFD). Neural-PDEs have emerged as a suitable method to assist complex CFD simulators to solve the Navier-Stokes equations at scale (Azizzadenesheli et al., 2024). Within this experiment, an FNO is trained to model the evolution of vorticity as described in the original FNO paper (Li et al., 2021). Further descriptions of the physics formulation, datasets and training can be found in Appendix D. The model was trained from simulations with viscosity v = 1e - 3 and then calibrated to form the prediction set for viscosity v = 1e - 4. The datasets were gathered from the original FNO paper Li et al. (2021). The coverage obtained across the out-of-distribution prediction sets can be seen in Figure 3. We modify the FNO in (Li et al., 2021) with dropout layers, to train a probabilistic neural operator. As shown in Figure 9, deploying the CP framework across the probabilistic outputs to be further calibrated to obtain guaranteed coverage."}, {"title": "3.5 2D Magnetohydrodynamics", "content": "Magnetohydrodynamics (MHD) governs the evolution of a plasma state within a fusion device, such as a Tokamak. It is characterised by the coupling of Navier-Stokes equation of fluid dynamics and Maxwell's equations of electromagnetism (Bellan, 2006). The evolution of MHD fields involves modelling multi-physics systems with coupled variables (\u03a6) that are integrated together. We model the evolution of multiple plasma blobs in a non-uniform temperature field with a reduced-MHD model as described by (Hoelzl et al., 2021). The model governs the spatio-temporal evolution of the density (\u03c1), electrostatic potential (\u03a6) and temperature (T) within a simple circular geometry, modelled in the toroidal coordinates Rand Z. The full description of the MHD physics we are interested in modelling is provided in Appendix E.\nThe dataset and the trained model were taken from (Gopakumar et al., 2024). The data was constructed by running simulations of the reduced-MHD cases using the code JOREK (Hoelzl et al., 2021). The physics of the equation was kept stationary throughout the campaign; however, the initial conditions that characterise the blobs were changed within each simulation (see Appendix E). A total of 2000 simulations/data points were generated, out of which 1000 were used for training, 500 for calibration and 500 for validating the prediction sets."}, {"title": "3.6 Foundational Physics Models", "content": "Foundation models (Bommasani et al., 2022) capable of performing multi-task modelling across a range of PDE-related tasks have been developed in the recent past (McCabe et al., 2023; Alkin et al., 2024; Hao et al., 2024; Rahman et al., 2024). These models rely on deploying novel transformer-based architectures that deploy an attention mechanism across the spatio-temporal domain to pre-train across various families of PDEs, allowing complex multi-physics scenarios to be modelled. The underlying principle relies on the idea that PDEs describing various physical systems, share similar differential operators. The models have shared embeddings across the variables to characterise the different physical operators, such as diffusion or convection. This allows them to learn the global behaviour governing different PDEs, leaving the local features to be learnt in the fine-tuning stage Alkin et al. (2024).\nAs these models scale and find further application across a range of domains, it becomes increasingly critical to quantify the uncertainty associated with the model predictions. Being pre-trained on diverse PDE datasets, these models often find applications in similar tasks. Considering the complexity of scale and the importance of the task, it is hard to validate the utility of these pre-trained models in an application without being able to provide UQ estimates that can be guaranteed. The same extends to fine-tuned models, trained for a specific downstream application. By performing conformal prediction across the model outputs at each space, we are able to validate the model performance for a specific application. CP relies on requiring ground truth data to help provide statistical validity, enforcing some computational effort in generating the numerical data. However, in the case of the application of a fine-tuned model for a specific application, calibration data already exists as the fine-tuning data. The central utility of this CP framework within the domain of surrogate modelling becomes clearer when extended to the fine-tuning setting where the prediction domain is the same as the training domain, reducing the need for further collection of data for calibration.\nWithin this experiment, we restrict our focus on applying the CP framework across one of the early attempts at a Foundational Physics Model, demonstrated in (McCabe et al., 2023). The authors modify the architecture of an adaptive vision transformer (AViT) (Yin et al., 2022a) with shared embedding and normalisation strategy across the variable space. The model uses an AViT backbone which attends over the space and time axis sequentially. The model is trained autoregressively to output the next time instance of the field(s) across the spatial domain. For further details on the model architecture, training setup, and the datasets used for pre-training, we refer to the original paper (McCabe et al., 2023)."}, {"title": "3.6.1 PRE-TRAINED MODEL: ZERO-SHOT LEARNING", "content": "Within the first experiment, the efficacy of the pre-trained foundational physics model to evolve the unseen"}]}