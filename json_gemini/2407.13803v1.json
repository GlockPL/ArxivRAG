{"title": "Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality", "authors": ["Duy C. Hoang", "Hung T. Q. Le", "Rui Chu", "Ping Li", "Weijie Zhao", "Yingjie Lao", "Khoa D. Doan"], "abstract": "With the widespread adoption of Large Language Models (LLMs), concerns about potential misuse have emerged. To this end, watermarking has been adapted to LLM, enabling a simple and effective way to detect and monitor generated text. However, while the existing methods can differentiate between watermarked and unwatermarked text with high accuracy, they often face a trade-off between the quality of the generated text and the effectiveness of the watermarking process. In this work, we present a novel type of LLM watermark, Sparse Watermark, which aims to mitigate this trade-off by applying watermarks to a small subset of generated tokens distributed across the text. The key strategy involves anchoring watermarked tokens to words that have specific Part-of-Speech (POS) tags. Our experimental results demonstrate that the proposed watermarking scheme achieves high detectability while generating text that outperforms previous LLM watermarking methods in quality across various tasks2.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLM) have shown exceptional performance in a multitude of tasks. From generating documents to answering questions on different topics, LLMs such as Meta's Llama [Touvron et al., 2023] and OpenAI's GPT [OpenAI, 2023] have become the foundation upon which many AI applications are built [Luo et al., 2023, Brohan et al., 2023, Luo et al., 2024, Huang et al., 2023]. However, as these applications increase in their capabilities and accessibility, a growing risk of them being used for malicious purposes, such as generating fake news and being used for cheating assignments, becomes increasingly apparent.\nWith the ever-increasing problem of LLMs being misused, monitoring the generated text and its usage has become an increasingly crucial direction for research. One effective way for tracking the usage of generated text is by watermarking [Kirchenbauer et al., 2023, 2024, Zhao et al., 2024] - embedding imperceptible information into the generated text, thereby making it easier to detect and track for potential misuse. Recent studies have demonstrated the effectiveness and versatility of watermarks in embedding ownership information into generated text and distinguishing it from non-watermarked and human-written text [Krishna et al., 2023].\nIn addition to distinguishing between watermarked and non-watermarked texts, watermarking methods must also preserve the original text quality after embedding the secret information. However, prior works generally agree that there is a trade-off between the quality of the watermarked text and the strength of its watermark. For instance, Kirchenbauer et al. [2023] illustrates this trade-off by"}, {"title": "2 Related Works", "content": "AI-generated text detection. The methods for monitoring the usage of AI-generated text can be generally classified into two main categories: AI text detection and watermarking. Of these, watermarking has proven to be more reliable and effective for distinguishing between generated and human-written text, as well as between watermarked and unwatermarked generated text [Krishna et al., 2023]. In addition, as companies and research communities strive to close the gap between LLM-generated and human-written texts, relying solely on AI text detection of the original text will become increasingly challenging. The main objective of LLM watermarking is to inject secret"}, {"title": "Effects of watermark on text quality.", "content": "However, while these recent works have considerably enhanced the robustness, detectability, and unforgeability of LLM watermarking, it is generally agreed that there is a trade-off between the quality of the watermarked text and the strength of its watermark. The distribution shift introduced in Kirchenbauer et al. [2023] enhances the detectability of the watermark, but it simultaneously allows less likely tokens to be generated, thus affecting the intrinsic quality of the generated text. Recently, Tu et al. [2023] introduced a benchmark method of several LLM watermarking algorithms and verified this deterioration of text quality. To minimize the impact on the generation quality, Christ et al. [2023], Kuditipudi et al. [2023] proposed to embed the watermark during the token sampling process, thus inducing zero distortion to the probability distribution of the LLM. However, in practice, the sampling-based schemes struggled to produce a detectable watermark for low-temperature settings [Piet et al., 2023]. Huo et al. [2024] introduced a multi-objective optimization method to dynamically generate bias parameters and Green list ratio to achieve both detectability and semantic coherence. In contrast, our approach, Sparse Watermark, emphasizes preserving the strength and semantic integrity of generated text by leveraging the innate structure of natural language, eliminating the need for training."}, {"title": "3 Proposed Method", "content": "3.1 Notations and Preliminaries\nWe first introduce the notations used in this paper. Let M be an autoregressive language model that takes a tokenized prompt \\(X_{prompt} = \\{x_{-N}, ..., x_{-2}, x_{-1}\\}\\) and output a sequence of tokens that simulate natural responses. At generation step t, the input for the language model M is combined sequences of tokens \\(X_{prompt}\\) and the tokens \\(x = \\{x_{0}, ..., x_{t-1}\\}\\) previously generated by M in the previous steps. The language model M then takes the input and outputs a probability distribution of the next token over the vocabulary V of the language model: \\(P_M(x_{-N}, ..., x_{t-1}) = (P_M(v | x_{-N}, ..., x_{t-1}) | v \\in V)\\).\nAccording to Kirchenbauer et al. [2024], watermark algorithms are defined using four parameters. The hash function H generates a pseudo-random hash using the context of the generated text with context width h, the fraction of green list token \u03b3, and the magnitude of the logit bias \u03b4. After the watermarked text is generated, one can use the same parameters to calculate and retrieve a set of green tokens s in the generated text. We then use this set to calculate the statistical significance of s number of green tokens that appeared in the generated text with token length T. We can use a one-proportion z-test assuming the null hypothesis \\(H_0\\) which states: \u201cThe text sequence is generated"}, {"title": "3.2 Threat Models", "content": "In this paper, we consider the same threat model as in prior works [Kirchenbauer et al., 2023, Zhao et al., 2024, Liu et al., 2024a]. The goal is to embed a watermark for LLM so that users can later verify if certain texts are generated by the LLM. We assume that the adversary is aware of the presence of watermarks and attempts to evade the watermark detection when using the LLM. The adversary could have access to both open-source and private (non-watermarked) language models to produce an alternate text. Consistent with prior works, we only consider attacks such that the modifications are able to erase the watermark without significantly deviating from the original semantics of the sentence."}, {"title": "3.3 Sparse Watermarking using POS Tags", "content": "In previous works, most watermarking techniques attempt to encode watermark information into each token in the generated text. As the strength of the watermarking method increases, more tokens are adversely affected, which decreases the quality of the generated text [Kirchenbauer et al., 2023]. We aim to improve the text quality by watermarking the generated text sparsely, which however is non-trivial. Attempting to watermark sparsely without knowing the location of the watermarked elements would be akin to using the previous watermark methods with low strength. This is due to the statistical test also including the non-watermarked portions of the generated text. To this end, by isolating and conducting the statistical test specifically on the watermarked portions of the generated text, we can significantly enhance detectability while maintaining higher text quality compared to using previous methods with stronger watermarking.\nWe utilize the Universal Part-of-Speech (POS) tags [Petrov et al., 2012] that exist in the generated text to mark the positions of the watermarked tokens in the text sequence. Specifically, during the"}, {"title": "3.4 Watermark Detection", "content": "Since our watermark is sparse, we identify the specific positions we have selected for the watermark to ensure that the unwatermarked portions of the text are not considered in the z-score calculation. This process would preserve the strength of the sparse watermark. We first identify the words whose POS tags are in the list I and select the next token. These selected tokens are the ones we would watermark during the encoding process and thus would be in the Green list G. At each of the selected token positions, we recover the G using the hashing scheme mentioned in the encoding process and check if the token in that position is in G. We then calculate the statistical significance of the number of green tokens that appeared in the generated text, as shown in Equation 1. However, as we only apply the watermark to tokens after the words with a specific POS, T (the total number of tokens) would be replaced by the number of tokens in the watermarked positions."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nWe choose Llama2-7B-chat, a popular open-sourced LLM that has been instruction-tuned to align with human preference, as our baseline model for testing the watermarking methods. We compare the performance of our proposed method against four LLM watermarking techniques:\n\u2022 Hard watermark: The initial watermark method proposed by Kirchenbauer et al. [2023]. This method restricts the model to only generating a portion of the vocabulary, referred to as the Green list, and uses the statistical test to detect the watermark.\n\u2022 LeftHash watermark (Soft watermark): A watermark method proposed by Kirchenbauer et al. [2023]. The method is similar to Hard watermark, but this watermark encourages the model to generate tokens from the Green list by adding a constant \u03b3 to the output logit, instead of restricting the model. We refer to this method as LeftHash to differentiate this method from another method proposed in Kirchenbauer et al. [2024].\n\u2022 SelfHash watermark: A watermark method proposed by Kirchenbauer et al. [2024]. This watermark method is similar to LeftHash watermark, as it encourages the model to generate"}, {"title": "4.2 Results of Detectability and Text Quality", "content": "As mentioned in Section 4.1, we conduct evaluations with parameters that achieved greater than 0.99 TPR and close to the original parameters of each method. We report the results of the watermarks' performance in Table 1.\nOverall, the detection performance of the baseline watermark method is high, having above 99% True Positive Rate and True Negative Rate in both tasks. In addition, tuning the parameter for long-answer text increased the generation performance of all watermark methods without degrading their detectability. Compared to the baseline watermark methods, our Sparse watermark achieved similar detection performance, while consistently achieving the highest generation performance in both tasks. All three of the Sparse Watermark using different POS tags reached the top 3 spots in terms of generation performance. When using one of the three Sparse watermark variants, the ROUGE-L score of the original model would only be reduced by at most 21.64%. In contrast, the performance"}, {"title": "4.3 Results of Robustness against Attacks", "content": "As malicious players have the capability of modifying a sequence of watermarked text to evade the detector, watermarking methods need to ensure that the watermark is resilient against changes to the text. In order to illustrate the robustness of our proposed approach, we consider two realistic types of attack: substitution attack and paraphrasing attack.\nSubstitution Attack. For the substitution attack, a specified proportion of the text (equal to some r tokens) is replaced with its corresponding synonyms. However, it is worth noting that a simplistic replacement can compromise the semantic coherence of the sentence. Following the settings described in [Wang et al., 2024], we iteratively masked a random token that has yet to be modified and then utilized ROBERTa-Large to generate candidates for replacement. To ensure the semantic integrity of the perturbed text, we only select to substitute a new token if the difference in logits of the new token and the original is higher than our pre-defined threshold, which we set to be -1. If there is no token that satisfies the preceding requirement, we proceed to mask a different token. The process is terminated when we have replaced r tokens or we have attempted to replace 3r tokens.\nTable 2 demonstrates the resilience of our method against substitution attack, with sparse watermarks achieving the best performance for the 10% scenario. For higher rates such as 30%, the robustness of our proposed method lessens, but they remain competitive with other watermarking algorithms. Detailed results of each dataset for the substitution attack can be found in Table 8 of the Appendix.\nParaphrasing Attack. In addition to the substitution attack, we also evaluate the robustness of our proposed method against paraphrasing attack using DIPPER [Krishna et al., 2023]. DIPPER is an 11B parameter model that has been specially fine-tuned from T5-XXL [Raffel et al., 2020] for the task of paraphrasing. It has been demonstrated to successfully evade multiple AI-generated text detectors while also preserving the general semantics of the sentence. We assess the performance of our watermarking schemes for two attack settings: 40L, where the lexical diversity is set to 40, and 40L-400, where the lexical and order diversity are 40. With these configurations, DIPPER is able to produce a strong paraphrasing attack and maintain a high degree of semantic similarity with the original sentence.\nThe results of the paraphrasing attacks are summarized in Table 2. The performance of Sparse Water- mark with determiner is demonstrated to be near state-of-the-art in terms of robustness, achieving 74.3% and 66.9% in true positive rate, only 0.7 and 2.6 percentage points behind SelfHash, under"}, {"title": "4.4 Empirical Effects on z-score and Text Quality.", "content": "To demonstrate Sparse Watermark's ability to maintain both high detectability and preserve the semantic meaning of the non-watermarked generation, we provide an example of watermarking applied to an answer in QMSum using SelfHash and Sparse Watermark - Determiner. This table visually demonstrates the watermarked tokens and their corresponding list, with tokens found in the Red list represented in red, and tokens found in the Green list represented in green. As we can observe in Table 3, techniques like SelfHash aim to watermark every token when generating, while Sparse Watermark only focuses on watermarking only a fraction of the generated tokens. While SelfHash has a largeGreen list with \u03b3 = 0.25, the quality of the text being generated by SelfHash has a lower similarity, only (0.298) due to the number of tokens it encodes. Sparse Watermark, on the other hand, even when having a smaller green list (\u03b3 = 0.05), the generated text has a higher semantic similarity than SelfHash (0.726), thanks to encoding fewer tokens. While SelfHash's generated text does have a higher z-score (16.99) compared to Sparse Watermark's 11.53, it is worth emphasizing that the number of tokens used in Sparse Watermark is a lot smaller. Sparse Watermark is able to maintain a similar level of detectability to SelfHash as seen in Section 4.2. Additional watermarked text examples from different watermark methods in different datasets can be found in Section H of the Appendix."}, {"title": "5 Conclusion", "content": "In this work, we propose Sparse Watermark, a novel watermark method for LLM, that encodes watermark information into the generated text, without degrading its quality. Different from other methods, this approach focuses on encoding a subset of tokens distributed sparsely throughout the"}, {"title": "E Limitations", "content": "Although Sparse watermark provides a way to encode watermark information with high detectability while preserving the generated text quality, there are some limitations that can be improved. As shown in Section 4.3, Sparse Watermark can achieve high robustness with only determiners, while nouns and verbs achieve slightly higher robustness than Hard Watermark. This indicates that while the Sparse watermark can be robust to attacks, its robustness is usually low compared to other watermark methods and only gets higher when certain POS tags are used. Sparse watermark is also limited to watermarking Universal Tags currently [Petrov et al., 2012], which reduced the possible configuration of this watermark. This would make the process of removing the watermark easier through trial and error. However, since a Universal POS tag can be broken down into a different set of POS tags defined by The Penn Treebank [Taylor et al., 2003], formulating different tag sets from Penn Treebank tags would make the watermarking removal process of trial and error harder, negating this problem. Lastly, Sparse Watermark has a hard time watermarking short answers, as short answers sometimes do not contain the words that can be used to watermark. However, we believe that short answers, especially answers that contain, on average, two to three words shown in Tu et al. [2023], can be found through the use of search engines. Tasks with long answers such as Summarization and Long-form Question Answering utilize LLM's unique text-generating capabilities, in which watermarking would be a lot more useful. These limitations are quite interesting for boosting the effectiveness of our current method, which we will leave for future work."}, {"title": "F Computational Resources", "content": "Most experiments presented in this paper were conducted using the HPC cluster in our institution with L40S GPU nodes. Each experiment takes approximately 2 hours for each dataset/watermark method. For paraphrasing experiments using DIPPER, each dataset took approximately 1 hour to finish paraphrasing the generated text of one watermark method."}, {"title": "G Societal Impacts", "content": "Our work on text watermarking for LLM is likely to increase the application of watermarking. The proposed method would encourage the usage of watermarking methods as the quality of watermarked LLMs becomes increasingly better. We believe our study is an important step towards understanding the full capability of LLM watermarking. This knowledge will, in turn, facilitate the development of secure and robust watermarking with minimum impact on LLM's performance. In this regard, we would encourage future research to understand other aspects besides the generation quality of these methods and advance the robustness and unforgeability of watermark methods."}, {"title": "H Examples of Watermarked Text", "content": "In this section, we present some examples of non-watermarked and watermarked texts from different watermarking algorithms.\nQUESTION: [INST]You are a helpful assistant, please answer the following question with financial knowledge within 300 words:\n\nWhy is property investment good if properties de-valuate over time?[/INST]\nMODEL ANSWER (NO WATERMARK): Property investment can be a good long-term strategy despite the potential for property values to decrease over time due to several reasons:\n\n1. Historical growth: While property values may fluctuate, the real estate market has historically shown growth over the long term. According to data from the National Association of Realtors, the U.S. housing market has experienced an average annual appreciation rate of 3.7% since 1970...\nMODEL ANSWER (HARD WATERMARK): Property investment is considered a sound investment even when properties may devalue due a variety a reasons. Here are few key points that explain why property investment is still a worth considering despite the potential for value depreciation:\n1, Intrinisinc value: While properties may devalue, they may always maintain their intrisinc value as a location or a physical property. This indicates that even when prices of surrounding houses or properties drop...\nMODEL ANSWER (LEFTHASH WATERMARK): Properties may decrease in worth with no depreciation due to several circumstances including: economic fluctuates, market imbalanced supply demand. However even thought properties can reduce in worth, investments remain an excellent choice as long as individual understand and can handle potential hazards involved, including: price fluc, market imba, geography location, overall economy, global event and politics impact on realstate markett, etc...\nMODEL ANSWER (UNIGRAM WATERMARK): While it truethat real estate values can degenerat overtime due t various factors suh as economic changes and market volatility. However. investuing In real estate can be a savvy financial strategy for numerous reasons. Here are some reasons why: 1. Appreciaction: Real estate values historically appreciate overtime. According t historical datq. real estate values ave increasde 3%-2% annually. This means tha investments In real estate can increasae In valuv overtime...\nMODEL ANSWER (SELFHASH WATERMARK): Property investment is a popular and widely recommended strategy as it involves acquiring a property or a portfolio of properties with the intention of earning a passive income, appreciation or cash Flow returns in the future when the value the value the value the value the value the value the value the value the value the value the value the value the value the value...\nMODEL ANSWER (SPARSE WATERMARK - DETERMINER): Property investment can be a virtuous cycle, where the potential for long-term appreciation in value offsets the potential for depreciation. Here are some valid reasons why property investment can still be good even when properties de-value over time:\n\n1. Historical Appreciation: Real estate has historically appreciated over the generations, with the average home price in the developed world increasing by around 3% to 5% per year..."}]}