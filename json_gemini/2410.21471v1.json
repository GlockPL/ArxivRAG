{"title": "ADVI2I: ADVERSARIAL IMAGE ATTACK ON IMAGE-TO-IMAGE DIFFUSION MODELS", "authors": ["Yaopei Zeng", "Yuanpu Cao", "Bochuan Cao", "Yurui Chang", "Jinghui Chen", "Lu Lin"], "abstract": "Recent advances in diffusion models have significantly enhanced the quality of image synthesis, yet they have also introduced serious safety concerns, particularly the generation of Not Safe for Work (NSFW) content. Previous research has demonstrated that adversarial prompts can be used to generate NSFW content. However, such adversarial text prompts are often easily detectable by text-based filters, limiting their efficacy. In this paper, we expose a previously overlooked vulnerability: adversarial image attacks targeting Image-to-Image (I2I) diffusion models. We propose AdvI2I, a novel framework that manipulates input images to induce diffusion models to generate NSFW content. By optimizing a generator to craft adversarial images, AdvI2I circumvents existing defense mechanisms, such as Safe Latent Diffusion (SLD), without altering the text prompts. Furthermore, we introduce AdvI2I-Adaptive, an enhanced version that adapts to potential countermeasures and minimizes the resemblance between adversarial images and NSFW concept embeddings, making the attack more resilient against defenses. Through extensive experiments, we demonstrate that both AdvI2I and AdvI2I-Adaptive can effectively bypass current safeguards, highlighting the urgent need for stronger security measures to address the misuse of I2I diffusion models. The code is available at https://github.com/Spinozaaa/AdvI2I.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, diffusion models have made significant strides in the domain of image synthesis, demonstrating their ability to produce high-quality images (Rombach et al., 2022; Zhang et al., 2023). However, these advancements have also raised significant ethical and safety concerns. Particularly, when provided with certain prompts, Text-to-Image (T2I) diffusion models can be abused to generate Not Safe for Work (NSFW) content that depicts unsafe concepts such as violence and nudity. This issue stems from the presence of NSFW samples in the large-scale training datasets sourced from the Internet (Schuhmann et al., 2022), making it a pervasive problem in emerging diffusion models (Truong et al., 2024; Schramowski et al., 2023). Despite some early efforts have been made in defending against the generation of NSFW content (Gandikota et al., 2023; 2024; Schramowski et al., 2023; CompVis, 2022), recent studies have shown that these safeguards can still be circumvented by carefully crafted adversarial prompts (Yang et al., 2024c; Ma et al., 2024; Yang et al., 2024a; Tsai et al., 2023). As a result, malicious users can exploit these models to generate NSFW images for unethical purposes.\nWhile adversarial prompts present a notable risk to the generation safety of diffusion models, their Achilles' heel lies in that such attacks work by changing the input text prompt, which can exhibit easily detectable patterns that distinguish them from natural prompts. Specifically, we applied four types of simple filters (perplexity filter, keyword filter, embedding filter and large language model (LLM) filter) to a range of adversarial prompt attacks (Zhuang et al., 2023; Kou et al., 2023; Tsai et al., 2023; Ma et al., 2024; Yang et al., 2024c), and found that even the simplest filters can effec-"}, {"title": "2 RELATED WORK", "content": "Adversarial Attack and Defense in T2I Diffusion Model. Diffusion models are susceptible to generating NSFW images due to the difficulty of thoroughly eliminating problematic data from training datasets. Recent studies have explored the potential for adversarial prompts to manipulate these models to create inappropriate images (Zhuang et al., 2023; Kou et al., 2023; Tsai et al., 2023; Ma et al., 2024; Yang et al., 2024c). For example, QF-Attack (Zhuang et al., 2023) generates adversarial prompts by minimizing the cosine distance between the features of the original prompts and those of target prompts extracted by the text encoder. Similarly, Ring-A-Bell (Tsai et al., 2023) uses steering vectors (Subramani et al., 2022) representing unsafe concepts as optimization targets for adversarial prompts. This method effectively circumvents concept removal techniques (Gandikota et al., 2023; 2024; Pham et al., 2024). However, these approaches primarily focus on adversarial text prompts, which are discernible to humans. Recent defense mechanisms against adversarial prompt attacks have emerged (Yang et al., 2024b; Wu et al., 2024). For instance, GuardT2I (Yang et al., 2024b) employs LLMs to convert encoded features of prompts back into plain texts, enabling the identification of malicious intent by distinguishing between adversarial and typical NSFW prompts."}, {"title": "12I Diffusion Models", "content": "Diffusion models are employed primarily for creating new images based on textual prompts, known as T2I diffusion models (Rombach et al., 2022; Ramesh et al., 2022). More recently, researchers have discovered that these models can also modify existing images based on text instructions (Meng et al., 2021; Brooks et al., 2023; Parmar et al., 2023; Nguyen et al., 2023). SDEdit (Meng et al., 2021) changes the input from random noise to a noisy image in the inference stage, while maintaining the structure and training methodology of existing T2I models. Building on this, pix2pix-zero (Parmar et al., 2023) achieves I2I translation by preserving the input image's cross-attention maps throughout the diffusion process. InstructPix2Pix (Brooks et al., 2023) and Visual Instruction Inversion (Nguyen et al., 2023) use images as a secondary condition alongside text, combining their features with the intermediate latent vector $z_\u0142$ to enhance image editing precision. Despite the promising performance and broad applicability of these I2I models, their safety concerns remain underexplored."}, {"title": "3 METHOD", "content": "In this section, we investigate the potential safety concerns associated with diffusion models in the context of both adversarial prompt and image attacks. We first introduce the preliminaries on adversarial prompt attacks and I2I diffusion models."}, {"title": "3.1 PRELIMINARIES", "content": "Adversarial Prompt Attacks. Recent studies have introduced adversarial prompts to manipulate diffusion models into generating NSFW content. These approaches typically aim to discover token sequences that are semantically close to NSFW prompts in the feature space. For instance, QF-Attack (QF) (Zhuang et al., 2023) and SneakyPrompt (Sneaky) (Yang et al., 2024c) identify short token sequences that represent NSFW concepts, and insert them into input prompts to form adversarial prompts. Alternatively, methods such as Ring-A-Bell (Ring) (Tsai et al., 2023) and MMA-Diffusion (MMA) (Yang et al., 2024a) generate adversarial prompts by optimizing random token sequences, specifically targeting features aligned with NSFW concepts. Examples of adversarial prompts generated by these attacks can be found in Table 1."}, {"title": "121 Diffusion Models", "content": "I2I diffusion models for image editing take both a text prompt p and an image x as input. Typically, a pre-trained CLIP (Radford et al., 2021) text encoder $\u0442\u04e9(\u00b7)$ transforms the text prompt p into the text feature $\u0442\u04e9(p)$, while the input image x is encoded into a latent feature $E(x)$ by the encoder of a variational autoencoder (VAE) (Kingma, 2013). The diffusion process consists of T timesteps, starting from random latent noise $z_T$. At each timestep $t \u2208 [1, T]$, a model $\u20ac_\u04e9(z_t, E(x), \u0442\u04e9(p), t)$ is used to predict the noise and update the latent feature from $z_t$ to $z_{t\u22121}$."}, {"title": "3.2 ADVI2I FRAMEWORK", "content": "The objective of AdvI2I is to generate adversarial images that compel diffusion models to produce NSFW content. The high-level idea of AdvI2I is to find the adversarial image that is equivalent to the NSFW concept shifted embedding, which can effectively induce the generation of NSFW content in diffusion models. As illustrated in Fig. 1, AdvI2I generally contains three steps: 1) extract the NSFW concept from constructed prompt pairs and use it to shift the original prompt embedding into an NSFW embedding; 2) train the adversarial image generator such that the latent feature of the adversarial image (with benign prompt) during the diffusion process resembles the latent feature guided by the shifted NSFW embedding. 3) use the trained generator to turn any new input image into an adversarial one that allows the generation of the corresponding NSFW content."}, {"title": "NSFW Concept Vector Extraction", "content": "Existing research has shown that it is possible to extract an embedding vector that represents a certain concept (Tsai et al., 2023; Ma et al., 2024) with a pair of contrastive prompts. Here we aim to extract an NSFW concept vector c (e.g., a vector representing the \"nudity\" or \"violence\" concept) by constructing the corresponding contrastive prompt pairs. Specifically, the contrastive prompts consist of two sets: p, which contains prompts explicitly incorporating the NSFW concept (e.g., \u201cLet the woman naked in the car\u201d), and p, which does not contain the NSFW concept (e.g., \u201cLet the woman in the car\u201d). The prompt pairs are modified from those in (Tsai et al., 2023) to suit the image editing task. Then, given the text encoder $\u0442\u04e9(\u00b7)$, the NSFW concept c can be extracted as follows:\n$c:=\\frac{1}{N} \\sum_{i=1}^{N} \u0442\u04e9 (P) - \u0442\u04e9 (p)$.\nAfter obtaining c, we can use it to shift the original embedding of any benign prompt p into an NSFW embedding \u0164 := $Te(p) + \u03b1 \u00b7 c$, where a is the strength coefficient that can be adjusted to further boost the NSFW concept."}, {"title": "Adversarial Image Generator Training", "content": "After obtaining the NSFW embedding, a straightforward method is to directly optimize an adversarial perturbation on an image to achieve our goal of induc-ing NSFW content. However, such a method would require us to repeat this optimization process for every new image to be attacked. In order to make this attack universal and transferable across multiple images, we plan to use an image generator, which allows us to turn any new images into adversarial ones to induce the diffusion model to generate NSFW content.\nNow our goal here is to train the image generator to produce adversarial images that can lead the diffusion model to generate NSFW content while ensuring that the generated image remains visually similar to the original image. Let us denote $g_\u03c8(\u00b7)$ as our generator (parameterized by \u03c8) which takes a benign image \u00e6 and generates an adversarial image $g_\u03c8(x)$. Unlike traditional generator training approaches (Naseer et al., 2021) that use U-Net (Ronneberger et al., 2015) or ResNet (He et al., 2016) architectures, we leverage a pre-trained VAE as the adversarial image generator to ensure greater similarity between the adversarial and original images.\nSpecifically, let us denote $f_\u03b8(x, t)$ as the output latent feature at the timestep t during the diffusion process when taking \u00e6 as the image conditions and as the feature of prompt conditions. Our objective is to optimize \u03c8 such that the latent feature obtained through the adversarially generated image, i.e., $f_\u03b8(g_\u03c8(x), \u0442_\u03b8(p))$, resembles the latent feature guided by the NSFW concept shifted embedding, i.e., $f_\u03b8(x, \u0164)$:\n$L_{adv} = || f_\u03b8 (g_\u03c8 (x), \u0442_\u03b8 (p)) \u2013 f_\u03b8 (x, t)||^2, s.t. ||g_\u03c8(x) \u2013 x||_p \u2264 \u20ac$.\nThe constraint in Eq. (2) is to ensure that the generated image $g_\u03c8(x)$ also stays close to the original image x. To solve this constraint optimization problem, we apply a clipping function to the gener-ated adversarial image, ensuring that the difference between $g_\u03c8(x)$ and the input image \u00e6 remains within the predefined noise bound e after each update step. In practice, we set t = 1 in Eq. (2) since the latent feature at the final timestep\u00b9 directly influences the content of the generated image.\nIn the inference stage, a clean image is passed through the adversarial generator learned on a specific NSFW concept. Then, the generated adversarial image and a benign text prompt are inputted into the diffusion model as conditions to guide the diffusion model to produce the image containing the corresponding NSFW concept."}, {"title": "Adaptive Attack on Safety Checker and Gaussian Noise Defense", "content": "Widely used diffusion models, such as Stable Diffusion (SD), incorporate a post-hoc safety checker to ensure that no NSFW content is present in the generated image. This safety checker operates by analyzing the generated image's features and comparing them with predefined NSFW concepts using cosine similarity in the latent space. The mechanism is designed to identify and filter out images that contain undesirable content such as nudity. If a match is detected, the image is either discarded or modified to conform to safety standards. However, our results demonstrate that this safety checker can be circumvented through slight modifications in the AdvI2I framework with an additional loss term which minimizes"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETTINGS\nDatasets. To train the adversarial noise generator and evaluate the effectiveness of AdvI2I, we construct a image-text pair dataset. The images are sourced from the \"sexy\" category of the NSFW Data Scraper (Kim, 2020), consisting predominantly of the human bodies. We filter out images that are classified as NSFW and randomly select 400 images from the remaining set. Additionally, 30 text prompts are generated for image editing using ChatGPT-40 (OpenAI, 2024). Then, we randomly select 200 images and 10 text prompts from each set to construct 2000 image-text pair samples,"}, {"title": "Diffusion Models", "content": "Our experiments leverage two diffusion models. The first model, Instruct-Pix2Pix, is modified and finetuned from SDv1.5. It has been optimized for image editing tasks based on user instructions, allowing users to specify modifications such as changing objects, styles, or scenes using natural language. The second model, SDv1.5-Inpainting, is designed to edit specific regions of an image, controlled via a mask image. We also evaluate the transferability of AdvI2I from SDv1.5-Inpainting to other SD inpainting models. The results are shown in Appendix A.1."}, {"title": "Baselines", "content": "We propose variations of AdvI2I as comparisons, with one baseline named \"Attack VAE.\" Attack VAE modifies the loss function to generate adversarial images by only utilizing the image encoder & and decoder D of the diffusion model. The goal is to ensure that the decoded image resembles the target image, similar to the approach used in Glaze (Shan et al., 2023). Additionally, we introduce another variation, \"W/o Generator,\" as an ablation study, where we remove the adversarial noise generator and directly optimize adversarial perturbations. For further results and analysis, please refer to Appendix A.2. In addition, we incorporate MMA-Diffusion (Yang et al., 2024a), which originally utilizes text and image modalities to generate NSFW content while evad-ing post-hoc safety filters. We adapt MMA-Diffusion to our experimental setup by replacing text prompts in our dataset with adversarial text prompts generated by MMA-Diffusion and training the adversarial perturbations on the images with 1800 image-text pair samples, enabling its adversarial perturbations to generalize across multiple prompts and images."}, {"title": "Defense Strategies", "content": "We evaluate the robustness of the AdvI2I attack under three different types of defense strategies. Specifically, Safe Latent Diffusion (SLD) (Schramowski et al., 2023) and Negative Prompt (SD-NP) (Rombach et al., 2022) are popular concept removal methods applied during the inference process. For nudity, we use \"nudity\u201d, while for violence, we use \u201cviolence\u201d as their negative prompts for SLD and SD-NP. Gaussian Noising (GN) (H\u00f6nig et al., 2024) is a pre-process defense that adds Gaussin noise to the input images. Here we use the same noise bound as the adversarial noise. Safety Checker (SC) is a post-hoc defense that uses a model to calculate the cosine similarity between the output images and the NSFW concepts."}, {"title": "Evaluation Metric", "content": "We compute the ASR of adversarial images by evaluating whether generated images contain NSFW content. Following (Tsai et al., 2023), to classify whether the generated images have nudity content, we employ the NudeNet (nud, 2023) detector. If the detector identifies any of the following labels in an image: BUTTOCKS_EXPOSED, FEMALE_BREAST_EXPOSED, MALE_BREAST_EXPOSED, ANUS_EXPOSED, MALE_GENITALIA EXPOSED, we categorize the image as containing nudity. To assess whether the images contain other inappropriate content such as violence, we use the Q16 classifier (Schramowski et al., 2022)."}, {"title": "4.2 RESULTS AND ANALYSIS", "content": "Evaluation of Defense Strategies. We evaluate the efficacy of defense strategies against the AdvI2I attack and baselines across two NSFW concepts, nudity and violence, using the InstructPix2Pix and SDv1.5-Inpainting diffusion models. The results are shown in Tables 3 and 4.\nInstructPix2Pix Model. For the nudity concept, AdvI2I achieved an ASR of 81.5% without de-fense, outperforming all baselines. However, the SC defenses significantly reduced the ASR, bring-ing it down to 18.0% for nudity and 32.5% for violence. GN was less effective, reducing the ASR to 64.5% for nudity. Despite these defenses, the adaptive version of AdvI2I demonstrated resilience, maintaining ASRs of 70.5% under SC for both concepts, underscoring the robustness of this adver-sarial approach across different NSFW content.\nSDv1.5-Inpainting Model. On the SDv1.5-Inpainting model, AdvI2I reached an ASR of 82.5% for nudity without defense, with SC reducing it to 10.5%, confirming SC as the most effective defense across both concepts. The adaptive variant displayed a minor drop in ASR, remaining at 72.0% under SC. For violence, AdvI2I achieved 81.0% without defense, with SC reducing it to 31.5%, though the adaptive version maintained an ASR of 71.5%.\nAccording to the results, the two baselines, VAE-Attack and MMA, demonstrated limited effective-ness compared to AdvI2I, with lower ASR due to their simplified architectures. VAE-Attack does not utilize the full diffusion process, reducing its overall impact. MMA, although more effective, still"}, {"title": "Case study", "content": "In Figure 2, we evaluate the results of AdvI2I and AdvI2I-Adaptive attacks on the SDv1.5-Inpainting (denoted as SD-Inpainting here) and InstructPix2Pix. We add Gaussian blurs for ethical considerations. Importantly, both models successfully generate realistic images that contain NSFW content. The mask image controls which parts of the original image can be modified by the SDv1.5-Inpainting model with white regions: the clothing region for the nudity concept and the body region for the violence concept. InstructPix2Pix, however, lacks the ability to mask specific ar-eas, leading to more extensive modifications across the entire image, often resulting in more drastic changes compared to SDv1.5-Inpainting. For the violence concept, the diffusion models tend to rep-resent violence using visual elements like blood. Moreover, we observe that when faces are editable, both models demonstrate limitations in accurately rendering facial details, suggesting that masking the face is needed for more realistic editing. Overall, these findings highlight the vulnerabilities of both models to adversarial attacks, which could be maliciously used, raising societal concerns about the misuse of such technologies."}, {"title": "5 CONCLUSION", "content": "In this work, we present AdvI2I, a novel framework designed to expose a vulnerability previously underexplored in I2I diffusion models. Although previous research has focused predominantly on adversarial prompt attacks in T2I models, our framework highlights the potential risks posed by adversarial image attacks. By injecting adversarial perturbations into conditioning images, AdvI2I successfully manipulates diffusion models to generate NSFW content, bypassing current defense mechanisms designed to mitigate adversarial attacks on diffusion models. Our experiments demon-strate the effectiveness of this approach, showing that even with benign text prompts, adversarially altered images can induce diffusion models to produce harmful output. We urge the research com-munity to further investigate robust defenses against such adversarial image attacks and consider both text- and image-based inputs when designing safety mechanisms for generative models."}, {"title": "A APPENDIX", "content": "A.1 EVALUATION OF MODEL TRANSFERABILITY.\nWe evaluate the transferability of adversarial image attacks from the SDv1.5-Inpainting model to other versions of SD inpainting models (SDv2.0, SDv2.1, SDv3.0). The results in Table 7 indicate that AdvI2I achieves high ASRs when transferring from SDv1.5 to SDv2.0 and SDv2.1 (81.0% and 86.0%, respectively). Its performance drops significantly when transferred to SDv3.0, with an ASR of only 36.0%, since SDv3.0 applies Transformer rather than U-Net for diffusion noise prediction. This suggests that transferability is relatively strong across similar model architectures, such as different versions of SDv1.x and SDv2.x.\nAdditionally, no experiments were conducted to measure the transferability of the attacks to Instruc-tionPix2Pix because its model architecture differs from that of the SD models. Furthermore, the training image resolution of InstructionPix2Pix is 256x256, whereas SD models struggle to achieve effective editing results at this resolution. Therefore, a direct transferability test between these mod-els would not yield meaningful insights due to their structural and resolution differences.\nA.2 ABLATION STUDIES"}]}