{"title": "IMPROVING DEEP REGRESSION WITH TIGHTNESS", "authors": ["Shihao Zhang", "Yuguang Yan", "Angela Yao"], "abstract": "For deep regression, preserving the ordinality of the targets with respect to the feature representation improves performance across various tasks. However, a theoretical explanation for the benefits of ordinality is still lacking. This work reveals that preserving ordinality reduces the conditional entropy $H(Z|Y)$ of representation Z conditional on the target Y. However, our findings reveal that typical regression losses do little to reduce $H(Z|Y)$, even though it is vital for generalization performance. With this motivation, we introduce an optimal transport-based regularizer to preserve the similarity relationships of targets in the feature space to reduce $H(Z|Y)$. Additionally, we introduce a simple yet efficient strategy of duplicating the regressor targets, also with the aim of reducing $H(Z|Y)$. Experiments on three real-world regression tasks verify the effectiveness of our strategies to improve deep regression.", "sections": [{"title": "INTRODUCTION", "content": "Classification and regression are two fundamental tasks in machine learning. Classification maps input data to categorical targets, while regression maps the data to continuous target space. Representation learning for classification in deep neural networks is well-studied, but is less explored for regression. One emerging observation in deep regression is the importance of feature ordinality. Preserving the ordinality of targets within the feature space leads to better performance, and various regularizers have been proposed to enhance ordinality. But what is the link between ordinality and regression performance?\nThe information bottleneck principle suggests that a neural network learns representations Z that retain sufficient information about the target Y while compressing irrelevant information. The two aims can be regarded as minimizing the conditional entropies $H(Y|Z)$ and $H(Z|Y)$, respectively. Compression reduces representation complexity, prevents overfitting, and bounds the generalization error. We find that preserving ordinality enhances compression by minimizing $H(Z|Y)$, i.e., the conditional entropy of the learned representation Z with respect to the target Y. Following, we refer to this conditional entropy as tightness, and its compression as as tightening the representation.\nBut are ordinal feature spaces not learned naturally by the regressor? We explore this question through gradient analysis and comparing the differences between regression and classification. We find that typical regressors are weak in tightening the learned representations. Specifically, given a fixed linear regressor with weight vector 0, the update direction of $z_i$ for a given sample i tends to follow the direction of 0. The movement of $z_i$ can be regarded as a probability density shift. Deep regressors update and tighten the representation in limited directions perpendicular to \u03b8. In contrast, we find that deep classifiers update $z_i$ more flexibly and in diverse directions, leading to better-tightened representations. Such a finding sheds insight into why reformulating regression as a classification task may be more effective and why classification losses benefit representation learning for regression.\nSo how can regression representations be further tightened? We take inspiration from classification, where one-hot encodings allow a separate set of classification weights $\\Theta_k$ for each class k. Similarly,"}, {"title": "RELATED WORK", "content": "Regression representation learning. Existing works mainly focus on the properties of continuity and ordinality. For continuity, DIR tackles missing data by smoothing based on the continuity of both targets and representations. VIR computes the represen- tations with additional information from data with similar targets. Preserving the representation's continuity can also encourage the feature manifold to be homeomorphic with respect to the target space and is highly desirable.\nFor ordinality, RankSim explicitly preserves the ordinality for better performance. Conr further introduces a contrastive regularizer to preserve the ordinality. It is worth mentioning that the continuity sometimes overlaps with the ordinality, and obtaining neighbor samples in continuity also requires ordinality. Although ordinality plays a key role in regression representation learning, it importance and characteristics are underexplored. This work tackles these questions by establishing connections between target ordinality and representation tightness.\nRecasting regression as a classification. For a diverse set of regression tasks, formulating them into a classification problem yields better performance. Previous works have hinted at task-specific reasons. For pose estimation, classification provides denser and more effective supervision. For crowd counting, classification is more robust to noise. Later, Pintea et al. empirically found that classification helps when the data is imbalanced, and Zhang et al. suggests regression lags in its ability to learn a high entropy feature space. A high entropy feature space implies the representations preserve necessary information about the target. In this work, we provide a derivation and further suggest regression has insufficient ability to compress the representations."}, {"title": "ON THE TIGHTNESS OF REGRESSION REPRESENTATIONS", "content": null}, {"title": "NOTATIONS & DEFINITIONS", "content": "Consider a dataset ${x_i, z_i, y_i}_{i=1}^N$ sampled from a distribution P, where $x_i$ is the input, $y \\in \\mathbb{R}$ is the corresponding label, and $z_i \\in Z \\subset \\mathbb{R}^d$ is the feature corresponding to the input $x_i$ extracted by a neural network. A regressor $f_\\Theta$ parameterized by $\\Theta$ maps $z_i$ to a predicted target $\\hat{y}_i = f_\\Theta(z_i)$. Specifically, when $f_\\Theta$ is a linear regressor, which is typically the case in deep neural networks, we have $\\hat{y}_i = \\Theta^Tz_i$. The encoder and $f_\\Theta$ are trained by minimizing a task-specific regression loss $L_{reg}$. Typically, the mean-squared error is used, i.e. $L_{reg} = \\frac{1}{N}\\sum_{i=1}^N(\\hat{y}_i - y_i)^2$.\nTo formulate regression as a classification problem, the continuous target y is quantized to K classes $y \\in \\{1,\\dots, K\\}$, and the cross-entropy loss is used to train the encoder and classifiers"}, {"title": "ORDINALITY AND TIGHTNESS", "content": "This section shows that preserving ordinality tightens the learned representation, and conversely, tightening the representation will help preserve ordinality. A lower $H(Z|Y)$ represents a higher compression. The compression is maximized when $H(Z|Y)$ is in its minimal ($H(Z|Y) = -\\infty$ for differential entropy and $H(Z|Y) = 0$ for the discrete entropy).\nFirst, we define ordinality following:\nDefinition 1 (Ordinality). The ordinality is perfectly preserved if $\\forall i, j, k$, the following holds: $d(y_i, y_j) \\leq d(y_i, y_k) \\Rightarrow d(z_i, z_j) \\leq d(z_i, z_k)$.\nTheorem 1 Let $B(z, \\epsilon) = \\{z' \\in Z|d(z,z') < \\epsilon\\}$ be the closed ball center at z with radius $\\epsilon$. Assume that $\\forall(x,z,y) \\in P$ and $\\forall \\epsilon > 0, \\exists (x', z', y') \\in P$ such that $z' \\in B(z, \\epsilon)$ and $y' \\neq y$. Then if the ordinality is perfectly preserved, $\\forall(x_i, z_i, y_i), (x_j, z_j, y_j) \\in P$, the following hold: $y_i = y_j \\Rightarrow d(z_i, z_j) = 0$.\nThe detailed proof of Theorem 1 is given in Appendix A.1. Theorem 1 states that if the ordinality is perfectly preserved, then the tightness (i.e. $H(Z|Y)$) is minimized. This suggests that preserving ordinality will tighten the representations. The assumption in Theorem 1 aligns with the learning target that learning continuously changes representations from continuous targets.\nConversely, if the representations can be correctly mapped to the target and are perfectly tightened, then the representations collapse into a manifold homeomorphic to the target space (e.g., collapse into a single line when the target space is a line) []. Thus, ordinality will be perfectly preserved locally. Note that reserving ordinality globally constrains the line to be straight, which is not necessary."}, {"title": "REGRESSION TIGHTNESS", "content": "Why are additional efforts to emphasize ordinality necessary? In this work, we find that standard deep regressors are weak in their ability to tighten the representations due to the gradient update direction with respect to the representations. Consider a fixed linear regression with a typical regression loss (e.g., MSE, L1), which has the following gradient with respect to $z_i$:\n$\\frac{\\partial L_{reg}}{\\partial z_i} = L'_{reg} (\\Theta^Tz_i - y_i)\\Theta^T$. (1)\nHere, the direction of $\\frac{\\partial L_{reg}}{\\partial z_i}$ is determined solely by the direction of $\\Theta$. As such, during learning, all the $z_i$ are moved either towards the direction of $\\Theta$ (or away). This movement can be regarded as a probability density shift, so regression suffers from a weak ability to change the probability density in directions perpendicular to $\\Theta$, which indicates a limited ability to tighten the representations in those directions. In other words, regressors can only move $z_i$ to $S_{y_i}$, but cannot tighten $S_{y_i}$, where $S_{y_i} = \\{z|f_\\Theta(z) = y_i\\}$ is the solution space of $f_\\Theta(z) = y_i$. More generally, for a differentiable regressor, we have the following:\nTheorem 2 Assume $f_\\Theta$ is differentiable and $S_y$ is a convex set, then $\\forall z', z'_i \\in S_y$:\n$\\frac{\\partial L_{res}}{\\partial z_i}(z' - z'_i) = 0$, (2)\nwhere $y'$ is the predicted target of $z_i$.\nThe detailed proof of Theorem 2 is given in Appendix A.2. The regressor $f_\\Theta$ is generally differentiable for gradient backpropagation, and $S_y$ is commonly a convex set with widely used regressors, such as the linear regressor. Theorem 2 shows that the gradient with respect to the representation will be perpendicular to its solution space and has no effect within the solution space. In other words, with a"}, {"title": "COMPARISON IN TIGHTNESS FOR CLASSIFICATION", "content": "Comparing classification with regression, we find classification has a higher flexibility to tighten representations in diverse directions $\\theta_k$, which suggests an ability to better tighten the representation. For the gradient with respect to $z_i$ over a batch of b samples:\n$\\frac{\\partial L_{CE}}{\\partial z_i} = (\\frac{\\partial L_{CE}}{\\partial g_\\Theta(z_i)}\\frac{\\partial g_\\Theta(z_i)}{\\partial z_i}) \\propto (\\frac{\\partial L_{CE}}{\\partial g_\\Theta(z_i)} \\sum_{j=1}^K \\frac{\\partial g_\\Theta(z_i)}{\\partial \\Theta_j}) = \\frac{1}{b} \\sum_{i=1}^b \\sum_{j=1}^K (P_{ij} - \\delta_{Y_i}) \\Theta_j^T$, (4)\nwhere $P_{ij} = \\frac{exp(\\Theta_j^Tz_i)}{\\sum_{k=1}^K exp(\\Theta_k^Tz_i)}$ is the probability of sample i belonging to class j. Here, the direction of $\\frac{\\partial L_{CE}}{\\partial z_i}$ is affected by all $\\Theta_k$, and $z_i$ will approach $\\Theta_{y_i}$ with training. In contrast, the direction of $\\frac{\\partial L_{Reg}}{\\partial z_i}$ is purely determined by $\\Theta$. Classification moves $z_i$ to its corresponding classifier $\\Theta_{y_i}$, even if the sample is correctly classified. At the same time, regression does not have any effect on $z_i$ if it is correctly predicted (i.e., $l'_{reg} = 0$). This suggests classification has a higher ability to tighten the representations in the solution space $S_y$. Here $S_{y_i}$ for classification is defined as the set of $z_i$ that are classified as class $y_i$.\nIn reality, the classifiers $\\Theta_k$ are not fixed and are updated with training. The gradient with respect to $k$th classifier $\\theta_k$ over a batch of b samples is given as:\n$\\frac{\\partial L_{CE}}{\\partial \\Theta_k} = \\sum_{i:y=k}^b \\frac{\\partial L_{CE}}{\\partial z_i} \\propto \\frac{1}{b} \\sum_{i=1}^b \\sum_{j=1}^K (P_{ik} - \\delta_{y_i,k})z_i^T$. (5)\nwhere $P_{ik} = \\frac{exp(\\Theta_k^Tz_i)}{\\sum_{l=1}^K exp(\\Theta_l^Tz_i)}$ is the probability of sample i belongs to the class k, and $\\delta_{y_i,k}$ is the Kronecker delta function. For classification, the direction of $\\Theta_k$ will biased to the $z_i$ with respect to the class k, while $z_i$ will also bias to its corresponding classifier. In contrast, for regression, the direction of $\\Theta$ will tend to be the weighted mean of the direction of $z_i$. Thus, the effect of the many $z_i$ on the direction of $\\Theta$ will offset each other and have a limited impact. As a result, changes in the directions of $\\Theta_k$ are generally greater than the change of the $\\Theta$ direction in regression, and therefore classification can move z more flexible and thus can potentially better tighten the representation."}, {"title": "METHOD", "content": "Our analysis in Sec. 3 inspires us to tighten the regression representations. To this aim, we introduce the Multiple Target (MT) strategy and the Regression Optimal Transport Regularizer (ROT-Reg) to tighten the representations globally (i.e., $\\min_z H(Z|Y)$) and locally (i.e., $\\min_z H(Z|Y = y_i), Z\\in B(z_i, \\epsilon), \\epsilon$ control the degree of locality). Inspired by the effect of multiple classifiers in classification, the MT strategy introduces additional targets as constraints to compress the representations. For ROT-Reg, we exploit it to encourage representations to have local structures similar to the targets, which implicitly tightens the representations."}, {"title": "TARGET SPACE WITH EXTRA DIMENSIONS BETTER TIGHTEN THE FEATURE SPACE", "content": "Our analysis in Sec. 3.4 suggests that classification outperforms regression in its ability to compress the representations in multiple directions, which come from multiple classifiers. Inspired by this, we introduce a simple yet efficient strategy, which adds extra dimensions for the target space to bring in extra regressors as constraints. Here, the additional regressors have a similar effect as individual classifiers. As shown in Figure 1, the additional constraints will result in a lower-dimensional $S_{y_i}$, which indicates higher compression. The number of additional targets depends on the intrinsic dimension of the feature manifold. In our Multiple Targets strategy, the final predicted target is the average over the multiple predicted targets:\n$\\hat{Y}_i = \\frac{1}{M}\\sum_{t=1}^M \\hat{y}_i^t$ (6)\nwhere M is the number of the total target dimension and $\\hat{y}_i^t$ is the $t^{th}$ predicted target."}, {"title": "REGRESSION OPTIMAL TRANSPORT REGULARIZER (ROT-REG)", "content": "The MT strategy tightens the representations globally through additional regressors. We propose to further tighten the representations locally. Specifically, we preserve the local similarity relations between the target and representation space. The local similarities are characterized by a self entropic optimal transport model. The model determines the optimal plan is to move a set of samples to the set itself with minimal transport costs, while each sample cannot be moved to itself.\nFormally, Given a set $S = \\{s_1, ..., s_n \\}$, the corresponding weight vector $p \\in \\mathbb{R}^n$ reflects how many masses the samples have, where the weights simplify the simplex constraint $\\sum_{i=1}^n p_i = 1$. Usually, one can easily implement p as a uniform distribution, i.e., $p_i = \\frac{1}{n}, \\forall i \\in [n]$. $C^S$ is the transport cost between $s_i$ and $s_j$, which is usually adopted as the Euclidean distance between the samples, and $T^S$ indicates how many masses are transported from the locations of $s_i$ to $s_j$. The self entropic optimal transport is defined as follows:\n$T(S) = \\underset{T}{argmin} \\langle C^S, T \\rangle + \\gamma \\Omega(T)$ (7)\n s.t. $T1_n = p, T1_n = p, T_{ii} = 0 \\forall i \\in [n]$, (8)\nwhere $\\gamma$ is a trade-off parameter, and $\\Omega(T) = \\sum_{i=1}^n\\sum_{j=1}^n T_{ij}^S log T_{ij}^S$ is the negative entropic regularization, which is used to smoothen the solution and speed up the optimization (Cuturi, 2013).\nGiven the solution $T^S$ minimizing the above objective, the element $T_{ij}^S$ measures the similarity relation between samples $s_i$ and $s_j$, since two samples with a large distance $C_{ij}^S$ will induce a small transport mass $T_{ij}^S$ between them. As a result, the optimal total transport cost $\\langle C^S, T^S \\rangle$ reflects the tightness of the samples."}, {"title": "EXPERIMENTS", "content": "We experiment on three deep regression tasks: age estimation, depth estimation, and coordinate prediction and compare with RankSim, Ordinal Entropy (OE) , and PH-Reg. RankSim preserves ordinality explicitly to serve as an ordinality baseline. OE leverages classification for better regression representations and serves as a regression baseline. PH-Reg preserves the topological structure of the target space by the Topological autoencoder and tightens the representation by Birdal's regularizer , serving as a topology baseline. More details are given in Appendix B.1."}, {"title": "REAL-WORLD DATASETS: AGE ESTIMATION AND DEPTH ESTIMATION", "content": "For age estimation, we use AgeDB-DIR and evaluate using Mean Absolute Error (MAE) as the evaluation metric. \u03b3 and \u03bb are set to 0.1 and 100, respectively. For depth estimation, we use NYUD2-DIR and evaluate using the root mean squared error (RMSE) and the threshold accuracy $\u03b4_1$ as the evaluation metrics. \u03b3 and \u03bb are set to 0.05 and 10, respectively. We set the total target dimension M to be 8 for both tasks. Both AgeDB-DIR and NYUD2-DIR contain three disjoint subsets (i.e., Many, Med, and Few) divided from the whole set. We exploit the regression baseline models of , which use ResNet-50 as the backbone, and follow their setting for both tasks."}, {"title": "$L_{oe}$ PRESERVES THE LOCAL SIMILARITY RELATIONSHIPS", "content": "The effectiveness of $L_{oe}$ is verified with the coordinate prediction task from . This task predicts data coordinates sampled from manifolds such as Mammoth, Torus, and Circle, which have different topologies. The inputs are noisy data samples and the goal is to recover the true data coordinates."}, {"title": "TIGHTNESS AND ORDINALITY AFFECT EACH OTHER", "content": "Compression for a better ordinality. We examine the impact of tightness on the ordinality. Table 4 presents the Spearman's rank correlation coefficient and Kendall rank correlation coefficient between the feature similarities (based on Cosine distance and Euclidean distance) and the label similarities. The two correlation coefficients measure how well ordinality is preserved. Since tightness compresses the feature manifold, reducing its volume, we use volume as a proxy for tightness, and the volume is approximated by the mean of the similarities between samples. The experiments are conducted on NYUD2-DIR, we randomly sample 1000 pixels from a batch of 8 test images. The label similarities are calculated as the Euclidean distances between the 1000 pixels, while the corresponding feature similarities are the distances between their corresponding representations. The results in Table 4 show standard regression fails to preserve the ordinality, while MT and $L_{oe}$ both improve the ordinality, although they are designed to tighten the representations. Combining both has a similar effect on preserving ordinality as RankSim, which is specific designed for this purpose. The lower volumes of our method compared to the baseline indicate that the feature manifold is more compressed.\nOrdinality for a better compression. To further verify that preserving ordinality leads to better compression, we visualize the feature manifold of the depth estimation task in 3D space. This is done by changing the last hidden layer's feature space to three dimensions. As shown in Figure 3, explicitly preserving the ordinality (i.e., +RankSim) compresses the feature manifold into a thin line, which shows a similar effect to explicitly tightening the representations (i.e., +MT)."}, {"title": "TIGHTNESS OF REGRESSION", "content": "Our theoretical analysis in Sec. 3 focuses on the gradient direction of representations. However, in reality, the neural network updates its parameters to update the representations indirectly. Here, we verify our analysis by visualizing the updating of z and $\\Theta$ in the depth estimation task.\nThe update of z. We change the last hidden layer's feature space to 2 dimensional for visualization. We randomly sample 1000 pixels from a batch of 8 images in the test set of NYUD2-DIR to visualize the feature manifold."}, {"title": "ABLATION STUDY", "content": "We conduct the ablation study on AgeDB-DIR for age estimations."}, {"title": "CONCLUSION", "content": "In this paper, for the regression task, we provide a theoretical analysis that suggests preserving ordinality enhances the representation tightness, and regression suffers from a weak ability to tighten the representations. Motivated by classification and the self entropic optimal transport, we introduce a simple yet effective method to tighten regression representations."}, {"title": "APPENDIX", "content": null}, {"title": "PROOF OF THEOREM 1", "content": "Theorem 1 Let $B(z, \\epsilon) = \\{z' \\in Z|d(z,z') < \\epsilon\\}$ be the closed ball center at z with radius $\\epsilon$. Assume that $\\forall(x,z,y) \\in P$ and $\\forall \\epsilon > 0, \\exists (x', z', y') \\in P$ such that $z' \\in B(z, \\epsilon)$ and $y' \\neq y$. Then if the ordinality is perfectly preserved, $\\forall(x_i, z_i, y_i), (x_j, z_j, y_j) \\in P$, the following hold: $y_i = y_j \\Rightarrow d(z_i, z_j) = 0$.\nProof\n$d(z_i, z_j) = d(z_i - z_k + z_k - z_j)$ (11)\n$\\leq d(z_i - z_k) + d(z_k - z_j)$, (12)\nwhere $z_k \\in B(z, \\epsilon)$. Since $d(y_k, y_j) \\leq d(y_k, y_i)$, and the ordinality is perfectly preserved, we have:\n$d(z_k - z_j) \\leq d(z_i - z_k)$. (13)\nThus:\n$0 \\leq d(z_i, z_j) \\leq 2d(z_i - z_k) \\leq 2\\epsilon$. (14)\nLet $\\epsilon \\rightarrow 0$, the result follows."}, {"title": "PROOF OF THEOREM 2", "content": "We first give a lemma:\nLemma 1 Let $S_y = \\{z|g(z) = y\\}$ be a convex set, where $z \\in \\mathbb{R}^n$ is the representation, y is the target and g is the regressor. Assume g is differentiable, then $\\forall z_k, z_i, z_j \\in S_y$, we have:\n$\\nabla g(z_k)(z_i - z_j) = 0$. (15)\nProof Let $z = (1 - \\epsilon)z_k + \\epsilon z_i$, where $\\epsilon \\in [0, 1]$. Since g is differentiable, using Taylor expansion, we have:\n$g(z) = g((1 - \\epsilon)z_k + \\epsilon z_i)$ (16)\n$= g(z_k + \\epsilon(z_i - z_k))$ (17)\n$= g(z_k) + \\epsilon\\nabla g(z_k)(z_i - z_k) + o(\\epsilon)$. (18)\nSince $S_y$ is a convex set, we have $z \\in S_y$. Thus:\n$g(z) = g(z_k) + \\epsilon\\nabla g(z_k)(z_i - z_k) + o(\\epsilon)$ (19)\n$y = y + \\epsilon\\nabla g(z_k) (z_i - z_k) + o(\\epsilon)$ (20)\n$\\frac{o(\\epsilon)}{\\epsilon} = \\nabla g(z_k)(z_k - z_i)$. (21)\nLet $\\epsilon \\rightarrow 0$:\n$\\nabla g(z_k)(z_k - z_i) = \\underset{\\epsilon \\rightarrow 0}{lim} \\frac{o(\\epsilon)}{\\epsilon} = 0$. (22)\nSimilarly, we have:\n$\\nabla g(z_k)(z_k - z_j) = 0$. (23)\nCombining the two equations above, we have:\n$\\nabla g(z_k)(z_i - z_j) = \\nabla g(z_k)(z_i - z_k + z_k - z_j)$ (24)\n$= \\nabla g(z_k)(z_i - z_k) + \\nabla g(z_k)(z_k - z_j)$ (25)\n$= 0$. (26)"}, {"title": null, "content": "Theorem 2 Assume fe is differentiable and $S_y$ is a convex set, then $\\forall z'_i, z'_j \\in S_y$:\n$\\frac{\\partial L_{res}}{\\partial z_i}(z' - z'_i) = 0$, (27)\nwhere $y'$ is the predicted target of $z_i$.\nProof\n$\\frac{\\partial L_{reg}}{\\partial z_i} = \\frac{\\partial L_{reg}(g(z_i) - y_i)}{\\partial z_i}$ (28)\n$= \\frac{\\partial L_{reg}(g(z_i) - y_i)}{\\partial (g(z_i) - y_i)} \\frac{\\partial (g(z_i) - y_i)}{\\partial z_i}$ (29)\n$= L'_{reg}(g(z_i) - y_i)\\nabla g(z_i)$. (30)\nBased on Lemma 1, we have:\n$\\nabla g(z_i)(z'_i - z'_j) = 0$. (31)\nThus,\n$\\frac{\\partial L_{reg}}{\\partial z_i} (z'_i - z'_j) = L'_{reg} (g(z_i) - y_i)\\nabla g(z_i) (z'_i - z'_j)$ (32)\n$= L'_{reg}(g(z_i) - y_i) \\times 0$ (33)\n$= 0$. (34)"}, {"title": "DETAILS ABOUT THE REAL-WORLD TASKS", "content": "For the age estimation on AgeDB-DIR, we adopt the suggested hyper-parameters to train the RankSim, where \u03bb, \u03b3 are set to 2, 1000, and the results of OE and PH-Reg are adopted from their published papers. the evaluation metric MAE: $\\frac{1}{N}\\sum_{i=1} |y_i - \\hat{y}_i|$, where N is the total number of samples, $y_i, \\hat{y}_i$ are the label and predicted result.\nFor depth estimation on NYUD2-DIR, we adopt the suggested hyper-parameters of OE and PH-Reg to train the models. For RankSim, we train the model with y range from 1 to 1000. We report the best results for all the three baselines. The evaluation metric: threshold accuracy $\u03b4_1$ = $\\frac{\\# of Y_p, s.t. max(\\frac{y_p}{\\hat{y}_p}, \\frac{\\hat{y}_p}{y_p}) < 1.25}{}$, and the root mean squared error (RMS): $\\sqrt{\\frac{1}{\\#p}\\sum_p(Y_p - y_p)^2}$."}, {"title": "VISUALIZATIONS", "content": null}, {"title": "VISUALIZATION OF THE UPDATING OF z", "content": "We provide the visualization of the feature similarities in Figure. 5."}, {"title": "VISUALIZATION OF THE UPDATING OF z", "content": "The visualizations of feature manifolds at each epoch are provided in Figure 6. For the neural collapse of regression, the feature manifold will collapse into a single line when the target space is a line and the compression is maximized. This trend can be observed in Figure 6, where the feature manifold looks like a thick line and evolves toward a thinner line over training. However, standard regression's limited ability to tighten representations results in a slower collapse. In contrast, our proposed method and RankSim both accelerate this collapse, as shown in Figure 3."}]}