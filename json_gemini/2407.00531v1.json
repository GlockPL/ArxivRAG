{"title": "Interpreting Pretrained Speech Models for Automatic Speech Assessment of Voice Disorders", "authors": ["Hok Shing Lau", "Mark Huntly", "Nathon Morgan", "Adesua Iyenoma", "Biao Zeng", "Tim Bashford"], "abstract": "Speech contains information that is clinically relevant to some diseases, which has the potential to be used for health assessment. Recent work shows an interest in applying deep learning algorithms, especially pretrained large speech models to the applications of Automatic Speech Assessment. One question that has not been explored is how these models output the results based on their inputs. In this work, we train and compare two configurations of Audio Spectrogram Transformer [1] in the context of Voice Disorder Detection and apply the attention rollout method [2] to produce model relevance maps, the computed relevance of the spectrogram regions when the model makes predictions. We use these maps to analyse how models make predictions in different conditions and to show that the spread of attention is reduced as a model is finetuned, and the model attention is concentrated on specific phoneme regions.", "sections": [{"title": "1 Background", "content": ""}, {"title": "1.1 Voice for Health", "content": "Speech generation involves a complex coordination of organs, as a result, speech contains information about the body from cognitive [3] and mental state [4] to respiration conditions [5] [6]. The decade of research effort into the discovery and utilisation of speech biomarkers, the characteristics of voice in speech associated with certain diseases, for diagnosis, has become more feasible with the advances in Artificial Intelligence (AI) that can learn the association and make clinical predictions. Automatic Speech Assessment, leveraging speech biomarkers, AI and mobile technology to assess patient health remotely, is expected to bring many benefits to early identification and remote monitoring.\n\nA general pipeline of Automatic Speech Assessment involves the following steps: 1). Patient records and uploads speeches in audio waveforms; 2). audio waveforms are pre-processed and Voice parameters, such as fundamental frequency, jitter and"}, {"title": "1.2 Deep Learning Speech Model for Automatic Speech Assessment", "content": "In recent years, there has been a growing interest in applying deep learning to the problem [9] [10]. This can be done in two approaches, The first approach involves training deep learning models in an end-to-end fashion, such that the model makes clinical predictions directly from the audio, however, proper generalization of the model requires a large amount of manually annotated data, which is time-consuming or not feasible for pathology with smaller sample size.\n\nThe second approach involves using a pre-trained deep learning model as a feature extractor and fine-tuning the speech model with lesser annotated data. This type of model is trained on a large corpus of speech data to learn a set of features i.e. representation to capture the attributes of the speech, which then be used in many ASR tasks. It has been shown speech representations capture human perceptual understanding [11] and preserve consistent attributes within the speech such as speaker, language, emotion, and age. As speech contains rich information about the conditions of several important organs, with the rise of these models, there have been several works exploring and evaluating their potential for identifying disease [12] [13]. However, deep learning models lack interpretability, which hinders their applications in healthcare sectors."}, {"title": "1.3 Interpreting speech models", "content": "With the rise of concern about Al reliability, tools have been developed to understand models, in general, there are two approaches: white-box and black-box. Black-box approaches systematically probe the model with various tasks and data to estimate its behaviour in a generalised situation, which is known as global explanations, although few approaches such as LIME [14] and SHAP [15] allow local explanations. Black-box approaches are model-agnostic. One example group is perturbation-based methods, as the name suggested, it aims to test model robustness against data perturbation such as adversarial attack or noise, by applying different perturbations.\n\nWhite-box approaches consist of analysing mathematically the relation between the output and the inputs, therefore, they can provide the local explanation of how output is inferred from the input by the model for a given circumstance. However, they often require specific model architectures and properties such as the existence of activations. In terms of neural networks, there are gradient-based methods that use backpropagation such as Grad-CAM [16] and Integrated Gradient [17], attribute-based such as LRP [18], attention-based such as attention flow and attention rollout [19] [2]."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Data selection", "content": "The Saarbr\u00fccken Voice Database [22] contains recordings from 1002 speakers exhibiting a wide range of voice disorders (454 male and 548 female) and 851 controls (423 male and 428 female). The age of speakers varies from 6-94 years (pathological) and 9-84 years (control). Each recording session contains recordings of /i/, /a/ and /u/ vowels recorded on neutral, higher, lower, rising and falling pitch, and a recording of the short phrase \u201cGuten Morgen, wie geht es Ihnen?\u201d. Audio is sampled with professional recording devices at 16-bit 50kHz.\n\nIn this work, we group the participants by two categories: gender and pathological status, where pathological status falls into three classes: organic, inorganic and healthy."}, {"title": "2.2 Model Training", "content": "In this work, we examine the Audio Spectrogram Transformer AST [1], a Transformer architecture modified from the Vision Transformer [24] and designed for spectrograms. Initially, the input audio waveform of t seconds is padded to the maximum size for the model T seconds and converted into a sequence of 128-dimensional log Mel filterbank (fbank) features computed with a 25ms Hamming window every 10ms, which results in a 128\u00d7100T spectrogram as input to the AST. The spectrogram is then divided into 16x16 patches and each patch is flattened with a linear projection layer to produce a sequence of embeddings with size 768. Trainable positional embedding (size 768) is added to each of the embeddings to provide the spatial structure of the spectrogram, and class token [CLS] embedding (size 768) is append at the beginning of the sequence, and fed to a transformer encoder. The encoder's output at the class token [CLS] is extracted as the speech representation. The model we use is pretrained on AudioSet [25] and is implemented and available in HuggingFace Transformers [26].\n\nWe train the model on the task of binary classification: pathological (organic and inorganic) or healthy subjects. The dataset is stratified and split into train, development and test sets with proportions of 80%, 10% and 10% of the entire dataset. We examine two configurations of the classification models: ast_freeze and ast_finetuned. In ast_freeze, An AST model is set to be non-trainable, and a linear layer is added on top of the model to project the embedding into classification outputs. Construction of ast_finetuned is identical to ast_freeze except the AST model is set to be trainable, and the entire model is fully finetuned."}, {"title": "2.3 Model Decision Interpretation", "content": "We implement Chefer et al's attention rollout [2] to visualise model decision-making. The method uses the model's attention layers to produce relevancy maps that visualise the computed relevancy of the spectrogram regions. A relevance map R is initialised with the identity matrix i.e. R := I . It uses the attention map A of each attention layer to update the scores on the relevance map. Since each attention map is comprised of multiple heads, the gradients are used to average across heads. The final attention map of a layer \\(\\bar{A}\\) is\n\n\n\\(\\bar{A} = E((VA A)^+)\\)"}, {"title": "3 Result", "content": ""}, {"title": "3.1 Model Performances", "content": ""}, {"title": "3.2 Analysis", "content": "show separations between genders rather than pathological status (pathological vs healthy), in other words, the speech representations contain more information about speakers' gender than the status regarding the potential voice pathologies. On the other hand, it shows the opposite trend when based AST model is fully trained (B, ast_finetuned). None of the models shows a clear separation between organic and inorganic pathologies.\n\nFigure 3 shows two of the example visualizations, more are available in the appendix. From the available visualizations, we can see the highest relevance scores are not necessarily assigned to the highest-intensity region such as fundamental frequency and the formats. A more common pattern that appeared on both of the models is that they give higher scores to phoneme \"/\u01dd/\" and segment \"/e/ /s/ /i/ /n/\". When the model was fine-tuned, we found more concentration, and the location often changed/shifted, however, no noticeably consistent patterns were concluded.\n\nTraditional voice parameters offer more explainability than the deep learning model, but iterative experiments are required to find the optimal feature selections. Since a lot of voice parameters are derived from spectrogram representations, the relevance map of the spectrogram allows us to prioritise the selection of features.\n\nThere are models pre-trained using the phonetic aspects of speech, such as [29], but the AST models we examine are not trained with this in mind. However, the"}, {"title": "4 Conclusion", "content": "In this work, we train and compare two configurations of Audio Spectrogram Transformer [1] in the context of Voice Disorder Detection and apply the attention rollout method [2] to produce the relevance map of the models. We use these maps to analyse the computed relevancy of the spectrogram regions. Through the analysis, we found the models cannot fully identify the difference between organic and inorganic voice disorders, models give higher scores to the phoneme \u201c//\u201d and segment \u201c/e/ /s/ /i/ /n/\". While the model is fine-tuned, we found that the spread of attention has often been reduced. These findings demonstrate the potential importance of phoneme features for Automatic Speech Assessment.\n\nAs a follow-up work, we will explore and compare how the model decision-making changes when performing organic vs non-organic using the same data selection and models, and we will compare the results with [30]. One of the issues we did not address in this work is the impact of recording environments on model decision-making, which is important to model robustness under suboptimal environments.\n\nIn the future, we will explore voice parameters and develop speech tasks that match the pattern on the relevance map of the spectrogram, we plan to conduct experiments in other pathology speech databases to test whether these features and speech tasks allow better performance. We will also conduct a similar analysis on other models that are pre-trained using the phonetic aspects of speech, which would likely influence the selection of the interpretation method."}, {"title": "5 Appendix", "content": ""}, {"title": "5.1 Model configuration", "content": ""}, {"title": "5.2 Visualisations", "content": "Figures in this section are Spectrogram (left) and the Relevance Map of ast_freeze (middle) and ast_finetuned (right) for speech samples, title follows the naming convention: \"PredictionResults-Speaker ID_Gender_PathologicalStatus\", there are four cases for PredictionResults:\n\n\u2022 O: where both ast_freeze and ast_finetuned predict correctly,\n\u2022 X: where both ast_freeze and ast_finetuned predict incorrectly,\n\u2022 A: where the AST model predicts incorrectly after fine-tuning, i.e. only ast_freeze predicts correctly\n\u2022 B: where the AST model predicts correctly after fine-tuning, i.e. only ast_finetuned predicts correctly"}]}