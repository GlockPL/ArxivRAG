{"title": "LLMs Do Not Think Step-by-step In Implicit Reasoning", "authors": ["Yijiong Yu"], "abstract": "It has been well-known that Chain-of-Thought\ncan remarkably enhance LLMs' performance\non complex tasks. However, because it also\nintroduces slower inference speeds and higher\ncomputational costs, many researches have at-\ntempted to use implicit CoT, which does not\nneed LLMs to explicitly generate the interme-\ndiate steps. But there is still gap between their\nefficacy and typical explicit CoT methods. This\nleaves us a doubt that, does implicit CoT really\nequal to explicit CoT? Therefore, in this study,\nwe address this question through experiments.\nWe probe the information of intermediate steps\nfrom the model's hidden states when it is per-\nforming implicit CoT. The results surprisingly\nindicate that LLMs hardly think about inter-\nmediate steps, suggesting they may just rely\non experience rather than strict step-by-step\nreasoning. Moreover, we find LLMs' implicit\nreasoning capabilities are susceptible and un-\nstable, reaffirming the necessity of explicit CoT\nto effectively support complex tasks.", "sections": [{"title": "Introduction", "content": "Advancements in Large Language Models (LLMS)\nhave unveiled unprecedented capabilities in han-\ndling complex reasoning tasks. Chain-of-Thought\n(CoT) prompting (Wei et al., 2022; Yu et al., 2023),\nin particular, has demonstrated substantial improve-\nments in the reasoning abilities of LLMs by ex-\nplicitly mapping out intermediate reasoning steps.\nMoreover, recent works of CoT training, such as\nOpenAI 01 (Qin et al., 2024) further demonstrate\nthe power of CoT.\nHowever, the CoT approach, despite its effi-\ncacy, it notably incurs slower inference speeds and\nhigher computational costs. These drawbacks have\nspurred some researches on alternative reasoning\nmethodologies that bypass the explicit generation\nof intermediate tokens, leveraging the model's in-\nherent \"vertical\" reasoning capabilities through its\ninternal processing layers. For example, (Deng\net al., 2024) remove the intermediate steps and fine-\ntune the model to let model learn implicit CoT, and\n(Deng et al., 2023) train a emulator which emulate\nthe intermediate states in CoT reasoning and train\na student model to generate answers from these\nimplicit states. This form of reasoning does not\nneed to output intermediate results as tokens, called\nimplicit reasoning or vertical reasoning, which con-\ntrasts with the \"horizontal\" reasoning, i.e. typi-\ncal CoT. Figure 1 shows the difference between\nexplicit CoT and implicit CoT. Although the con-\ncept of \"implicit CoT (reasoning)\" is rarely directly\nmentioned, in many scenarios that require low la-\ntency, users usually ask LLMs to output the final\nanswer directly, which actually has forced LLMs\nto adopt the implicit reasoning way.\nDespite the theoretical appeal of implicit reason-\ning as a more efficient alternative to traditional CoT\nmethods, empirical evidence suggests the perfor-\nmance of implicit CoT still lag behind explicit CoT.\nMoreover, though some previous researches have\nconfirmed the concept of implicit reasoning and\nattempted to analyze its process and efficacy (Yang\net al.; Wang et al.; Allen-Zhu and Li), they usually\nmore focus on using knowledge-based problems to\nexamine whether LLMs can recall their paramet-\nric knowledge during implicit reasoning, instead of\ninvestigating more basic and generic forms of multi-\nstep problems such as arithmetic. So far, there is\nstill no clear and widely accepted conclusion on\nthe rationale of implicit reasoning.\nThis situation makes us wonder fundamental\nquestions about the nature of the implicit reason-\ning, such as \"Are LLMs doing the same thing in\nthe processes of implicit and explicit CoT?\" and\n\"Can the hidden, internal and layer-by-layer pro-\ncessing truly serve as an equivalent to explicit CoT\nreasoning?\u201d To answer these questions, our study\ndesigns a elaborate set of experiments aimed at un-\ncovering the implicit reasoning processes within a\nlarge model, specifically targeting the process of\nhandling multi-step arithmetic problems without\nresorting to outputting explicit intermediate steps.\nIn our experiment, we leverage a powerful\nopen-source model, Qwen2.5-72B-Instruct (Team,\n2024), with 80 layers, to tackle simple arithmetic\nproblems that are easily solvable via typical CoT\nreasoning (Ye et al., 2024). However, we force\nthe model to direct give the answer without out-\nputting steps, so that we can examine whether these\ntasks can be addressed through implicit reasoning\nand how implicit reasoning happens. The arith-\nmetic problems has controllable number of rea-\nsoning steps, with each intermediate result being\nknown. By investigating the hidden states asso-\nciated with the final token of the given problem\nstatement across layers and employing a simple\nlinear classifier to probe those intermediate results,\nwe aim to find out if the model really calculates the\nintermediate results in its implicit thinking process.\nThe experiment results are surprising and\ncounter-intuitive: we find the model hardly calcu-\nlates the intermediate results in implicit reasoning,\ndespite it can often give the correct answer of the\nmulti-step problem. Moreover, through slightly\nmodifying the problem without even increase its\ndifficulty, we find implicit reasoning is more un-\nstable and susceptible. This finding suggests in\nimplicit reasoning, the model may not strictly fol-\nlow a step-by-step reasoning process, but relies\nsolely on an intuitive and direct way of thinking to\ncomplete the task, belonging to System 1 thinking\n(Kahneman, 2011), which is faster but less reliable.\nIn conclusion, we think LLMs, despite they can\noften directly give the correct answer of a multi-\nstep problem, especially when with a larger size,\nthey are not really doing step-by-step reasoning\n(at least in arithmetic problems), unless adopting\nexplicit CoT. Implicit reasoning may just be an illu-\nsion created by LLMs' powerful memory and rich\nexperience, which is fundamentally different from\nconventional reasoning. Our study provides critical\ninsights into the mechanics of implicit reasoning\nand emphasizes the ongoing necessity for explicit\nCoT methodologies in enhancing LLMs ability on\ncomplex tasks."}, {"title": "Approach", "content": "To present the reasoning steps clearly, we adopt\nsimple multi-step arithmetic problems with only\naddition and subtraction. Usually, when given such\nproblems, modern LLMs will automatically use a\nCoT manner to address them. To investigate the\nprocess of implicit reasoning, we use prompt to\nforce the model to give the answer without using\nCoT. Therefore, an example of our prompt, which\nis a 5-step problem, is as follows:\nE = 8;\nD = E-5;\nC = D + 2;\nB = C + 5;\nA = B - 1;\nQuestion: What is the value of A? You must\nanswer directly with A=xxx.\nAnswer: A=\nWe randomly change the value in the problem\nto generate 2000 different samples, and each in-\ntermediate results are record. For example, the\nintermediate results of the above example should\nbe [8, 3, 5, 10, 9], i.e. the corresponding value of\nE, D, C, B and A. The result of the last step is the\nfinal answer.\nThe model will direct output the answer after our\nprompt, thus we take the last token of the prompt\nas our main research object and record its hidden\nstates of each layers. Then, we adopt a typical\nlinear probing method, which uses an 1-layer MLP,\nto predict each of the intermediate results from the\nhidden states. We control all of the intermediate\nvalues is within -10 to 10 so that the probe is a\n21-class classifier (each value corresponds to one\nclass).\nWe use 1600 samples to train the classifier for\n10 epochs and 400 samples for testing its accuracy.\nAnd respectively for each hidden state of the 20\ngroups, we use it as the input feature to train an\nindividual classifier. In training and testing, we set\nthe result of each step as the label respectively to\nalso train an individual classifier. Therefore, we\nfinally get 20 * num_steps classifiers. If the classi-\nfier of the k-th hidden state shows high accuracy in\nthe n-th step, it represents the model has calculated\nthe result of the n-th step in the k-th hidden state.\nWe choose a large model, Qwen2.5-72B-Instruct\n(Team, 2024), to perform implicit reasoning, be-"}, {"title": "Expriment Design", "content": "2.1\nTo present the reasoning steps clearly, we adopt\nsimple multi-step arithmetic problems with only\naddition and subtraction. Usually, when given such\nproblems, modern LLMs will automatically use a\nCoT manner to address them. To investigate the\nprocess of implicit reasoning, we use prompt to\nforce the model to give the answer without using\nCoT. Therefore, an example of our prompt, which\nis a 5-step problem, is as follows:\nE = 8;\nD = E-5;\nC = D + 2;\nB = C + 5;\nA = B - 1;\nQuestion: What is the value of A? You must\nanswer directly with A=xxx.\nAnswer: A=\nWe randomly change the value in the problem\nto generate 2000 different samples, and each in-\ntermediate results are record. For example, the\nintermediate results of the above example should\nbe [8, 3, 5, 10, 9], i.e. the corresponding value of\nE, D, C, B and A. The result of the last step is the\nfinal answer.\nThe model will direct output the answer after our\nprompt, thus we take the last token of the prompt\nas our main research object and record its hidden\nstates of each layers. Then, we adopt a typical\nlinear probing method, which uses an 1-layer MLP,\nto predict each of the intermediate results from the\nhidden states. We control all of the intermediate\nvalues is within -10 to 10 so that the probe is a\n21-class classifier (each value corresponds to one\nclass).\nWe use 1600 samples to train the classifier for\n10 epochs and 400 samples for testing its accuracy.\nAnd respectively for each hidden state of the 20\ngroups, we use it as the input feature to train an\nindividual classifier. In training and testing, we set\nthe result of each step as the label respectively to\nalso train an individual classifier. Therefore, we\nfinally get 20 * num_steps classifiers. If the classi-\nfier of the k-th hidden state shows high accuracy in\nthe n-th step, it represents the model has calculated\nthe result of the n-th step in the k-th hidden state.\nWe choose a large model, Qwen2.5-72B-Instruct\n(Team, 2024), to perform implicit reasoning, be-"}, {"title": "Results of Probing Intermediate Steps", "content": "2.2\nThe results in Figure 2 and Figure 3 shows the\naccuracy of probing the intermediate result of each\nstep when the problem is 3-step or 5-step. It is\nclear that the results of the first step and the last\nstep can always be probed successfully in the back\nlayers, indicating the model does memorize the\ninput value (i.e. the result of the first step) and does\nconceive the final answer (i.e. the result of the last\nstep). By contrast, the result of the second step can\nbarely be probed with a lower accuracy, and the\nresults of other steps in the middle can hardly be\ndetected. It looks that the curve of the last step just\nsurges in the last layers, even without waiting for\nthe processing of the 3rd or 4th step.\nHowever, since the result of the second step can\nbe detected to some extent, both in 3-step or 5-step\nproblems, this suggests that LLMs may have the\nability to perform a 2-hop reasoning (the 3-step\nproblem actually only needs 2 hops because the\nresult of the first step is already given) in implicit\nreasoning, but not at all when there are more steps\ninvolved.\nThis finding indicates that, in generic cases, there\nis actually not a specific state where the model cal-\nculates the results of the intermediate steps, even\nwhen it correctly give a answer of the multi-step\nproblem. It actually skips the intermediate steps\nand come up with the final result directly. There-\nfore, we posit that perhaps due to a large model's\nstrong abstraction and memory abilities, it has\nlearned a large number of answers to mathemat-\nical problems during the training stage. Therefore,\nit can almost directly map problems with multiple\nsteps to their answers through intuition and mem-\nory, thus producing a \u201cimplicit reasoning\u201d effect.\nBut in fact, its mechanism is not equivalent to the\nexplicit CoT process at all."}, {"title": "Result of Slightly Perturbing the Prompt", "content": "2.3\nTo further show the difference between implicit\nreasoning and explicit reasoning, we slightly mod-\nify the problem by 2 methods: 1. reversing the\norder of the equations; 2. dividing all values by\n10. For humans, such modifications hardly increase\nany difficulty of the problem. The examples of the\nmodified problem are as follows:\nReverse\nA = B - 1;\nB = C + 5;\nC = D + 2;\nD = E-5;\nE = 8;\nDivide\nE = 0.8;\nD = E - 0.5;\nC = D + 0.2;\nB = C + 0.5;\nA = B - 0.1;\nWe evaluate the model's performance in these 3\ntypes of prompts while the original values of the\nproblems are the same. We test with prompt styles\nof both the implicit reasoning way (as shown in\nsection 2.1) and the explicit reasoning way (adding\n\"let's think step by step\").\nFrom the results in Table 1, we can clearly see\nthat, compare to the original problems, the mod-\nified problems significantly degrade the perfor-\nmance when using implicit reasoning. While the\nperformance of explicit reasoning is always perfect.\nThis contrast further demonstrate our inference that,\nin implicit reasoning, the model is actually answer-\ning directly by experience and intuition, but not\nby reasoning step-by-step. This cause the way of\nimplicit reasoning less robust and less reliable."}, {"title": "Conclusion", "content": "3\nIn this study, we investigate the mechanism of\nLLMs doing implicit reasoning, and get a non-\ntrivial finding that, unlike some previous studies\nwhich envisioned implicit reasoning as a substitute\nfor explicit reasoning, implicit reasoning cannot be\non par with explicit reasoning methods because it\nactually does not follow a step-by-step process but\njust intuitively thinks of the answer, which makes\nit less reliable. This finding remind us that there\nis no free lunch, that is, under current technologi-\ncal conditions, there may not be a perfect solution\nthat can make LLMs output very few tokens while\nkeeping the accuracy on solving complex problems.\nWhen you ask LLMs to give the answer directly,\nyou should know that it has not actually undergone\na real reasoning. Scaling the test-time by using ex-\nplicit CoT may still be the most feasible method to\nfurther propel the capabilities of LLMs at present."}]}