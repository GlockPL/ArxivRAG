{"title": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning", "authors": ["Zhishuai Liu", "Weixin Wang", "Pan Xu"], "abstract": "We study off-dynamics Reinforcement Learning (RL), where the policy training and deployment environments are different. To deal with this environmental perturbation, we focus on learning policies robust to uncertainties in transition dynamics under the framework of distributionally robust Markov decision processes (DRMDPs), where the nominal and perturbed dynamics are linear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that enjoys an average suboptimality  \u00d5(dHmin{1/p, H}/\u221aK), where K is the number of episodes, H is the horizon length, d is the feature dimension and p is the uncertainty level. This result improves the state-of-the-art by O(dH/min{1/p, H}). We also construct a novel hard instance and derive the first information-theoretic lower bound in this setting, which indicates our algorithm is near-optimal up to O(\u221aH) for any uncertainty level p\u2208 (0,1]. Our algorithm also enjoys a 'rare-switching' design, and thus only requires O(dH log(1+ H\u00b2K)) policy switches and O(d\u00b2 H log(1 + H2K)) calls for oracle to solve dual optimization problems, which significantly improves the computational efficiency of existing algorithms for DRMDPs, whose policy switch and oracle complexities are both O(K).", "sections": [{"title": "1 Introduction", "content": "In dynamic decision-making and reinforcement learning (RL), Markov decision processes (MDPs) offer a well-established framework for understanding complex systems and guiding agent behavior (Sutton and Barto, 2018). However, MDPs encounter significant challenges in practical applications due to incomplete knowledge of model parameters, especially transition probabilities. This sim-to-real gap, representing the difference between training and testing environments, can lead to failures in fields like infectious disease control and robotics (Farebrother et al., 2018; Zhao et al., 2020; Laber et al., 2018; Liu et al., 2023; Peng et al., 2018). \u03a4\u03bf address these challenges, off-dynamics RL provides a framework where policies are trained on a source domain and deployed to a distinct target domain, promoting robust performance across varying environments (Eysenbach et al., 2020; Jiang et al., 2021). Within this framework, distributionally robust Markov decision processes (DRMDPs) have emerged as a promising way to model transition uncertainty. DRMDPs focus on learning robust policies that perform well under worst-case scenarios (Nilim and El Ghaoui, 2005; Iyengar, 2005). Prior works (Zhang et al., 2021a; Yang et al., 2022; Panaganti et al., 2022; Shi and Chi, 2022; Yang et al., 2023b; Shen et al., 2024) have proposed algorithms mainly for tabular DRMDP settings, where the number of states and actions is finite, which are infeasible in large state and action spaces.\nIn environments characterized by large state and action spaces, function approximation techniques are crucial to overcome the computational burden posed by high dimensionality. Linear function approximation methods, based on relatively simple function classes, have shown significant theoretical and practical successes in standard MDP environments (Jin et al., 2020; He et al., 2021, 2023; Yang and Wang, 2020; Hsu et al., 2024). However, their application in DRMDPs introduces additional complexities. These complexities arise from the nonlinearity caused by the dual formulation in the worst-case analysis, even when the transition dynamics in the source domain are modeled as linear. Recently, Liu and Xu (2024a) provided the first theoretical results in the online setting of d-rectangular linear DRMDPs, a specific type of DRMDPs where the nominal model is a linear MDP (Jin et al., 2020) and the uncertainty set is defined based on the linear structure of the nominal transition kernel. Apart from this, online DRMDP with linear function approximation is largely underexplored and it is not clear how far existing algorithms are from optimal. Consequently, two natural questions arise:\nCan we improve the current results for online DRMDPs with linear function approximation?\nWhat is the fundamental limit in this setting?\nIn this paper, we provide an affirmative answer to the first question and answer the second question by providing an information theoretic lower bound for the online setting of d-rectangular linear DRMDPs. In particular, motivated by the adoption of variance-weighted ridge regression to achieve nearly optimal result in standard linear MDPs (Zhou et al., 2021a; Zhou and Gu, 2022; Zhang et al., 2021b; Kim et al., 2022; Zhao et al., 2023; He et al., 2023; Hu et al., 2023), we propose a variance-aware distributionally robust algorithm to solve the off-dynamics RL problem. Due to the nonlinearity caused by the dual optimization of DRMDPs, the adoption of variance information in linear DRMDPs is highly nontrivial. The only existing algorithm that incorporates variance information in learning linear DRMDPs requires coverage assumptions on the offline dataset (Liu and Xu, 2024b), which is infeasible in our setting where the algorithm needs to interact with the environment in an online fashion. Therefore, our work poses a distinct algorithm design and calls for different theoretical analysis techniques. Specifically, our main contributions are summarized as follows:\n\u2022 We propose a novel algorithm, We-DRIVE-U, for d-rectangular linear DRMDPs with total-variation (TV) divergence uncertainty sets. We-DRIVE-U is designed based on the optimistic principle (Jin et al., 2018, 2020; He et al., 2023) to trade off the exploration and exploitation during interacting with the source environment to learn a robust policy. The key idea of We-DRIVE-U lies in incorporating the variance information into the policy learning. In particular, a carefully designed optimistic estimator of the variance of the optimal robust value function is established at each episode, which will be used in variance-weighted regressions under a novel 'rare-switching' regime to update the robust policy estimation.\n\u2022 We prove that We-DRIVE-U achieves  (dH \u00b7 min{1/p, H}/\u221aK) average suboptimality when the number of episode K is large, which improves the state-of-the-art result (Liu and Xu, 2024a) by O(dH/min{1/p, H}), We highlight that the average suboptimality of We-DRIVE-U demonstrates the 'Range Shrinkage' property (refer to Lemma F.10) through the term min{1/p, H}. We further established an information-theoretic lower bound \u03a9(dH1/2 . min{1/p, H}/\u221aK), which shows that We-DRIVE-U is near-optimal up to O(\u221aH) for any uncertainty level p\u2208 (0,1].\n\u2022 We-DRIVE-U is favorable in applications where policy switching is risky or costly, since We-DRIVE-U achieves O(dH log(1 + H\u00b2K)) global policy switch (refer to Definition 5.7). Moreover, we note that calls for oracle to solve dual optimizations (4.3) are one of the main sources of computation complexity in DRMDP with linear function approximation. Thanks to the specifically designed \u2018rare-switching' regime, We-DRIVE-U achieves O(d2H log(1 + H\u00b2K)) oracle complexity (refer to Definition 5.8). Both results improve exiting online DRMDP algorithms by a factor of K. Thus, We-DRIVE-U enjoys low switching cost and low computation cost.\nNovelty in Algorithm and Hard Instance Design The variance estimator and the variance- weighted ridge regression in We-DRIVE-U lead to two major improvements on the average suboptimality compared to the previous result: 1) the incorporation of variance information enables us to leverage the recently discovered 'Range Shrinkage' property for linear DRMDPs, which is crucial in achieving the tighter dependence on the horizon length H; 2) inspired by previous works on standard MDPs (Azar et al., 2017; He et al., 2023), we design a new \u2018rare-switching' regime (refer to Remark 5.10) and monotonic robust value function estimation (refer to Remark 4.3). Together with the optimistic variance estimator, we achieve the tight dependence on d. As for the lower bound, we construct a novel family of hard-to-learn linear DRMDPs, showing the 'Range Shrinkage' property on robust value functions for any policy \u03c0.\nTechnical Challenges The incorporation of variance information poses unique challenges to our theoretical analysis. In particular, in order to get the near-optimal upper bound on average suboptimality, we need to bound the variance-weighted version of the d-rectangular estimation error (see (5.1) for more details), instead of the vanilla one in (5.1) of Liu and Xu (2024a). However, this term is in general intractable through direct matrix analysis. To solve this challenge, we seek to convert the variance-weighted d-rectangular estimation error to the vanilla version, which requires a precise upper bound on the variance estimator. Intuitively, the variance estimator should be close to the true variance when the 'sample size' k is large. While different from the recent study (Liu and Xu, 2024b) on the offline linear DRMDP, our variance estimator is an optimistic one and thus cannot be trivially upper bounded by the true variance. To this end, we carefully analyze the error of the variance estimator in the large k regime, and meticulous calculation shows that the optimistic variance estimator can be upper bounded by a clipped version of the true variance (refer to Lemma B.7).\nNotations For any positive integer H \u2208 Z+, we denote [H] = {1,2,\u2026\u2026,H}. For any set S, define \u2206(S) as the set of probability distributions over S. For any function V : S \u2192 R, define [PhV](s,a) = Es'~Ph(\u00b7|s,a) [V(s')], and [V(s)]a = min{V(s), a}, where a > 0 is a constant. For a vector x, define xj as its j-th entry. Moreover, denote [xi]i\u2208[d] as a vector with the i-th entry being xi. For a matrix A, denote \u03bb\u03af(A) as the i-th eigenvalue of A. For two matrices A and B, denote A\u2aafB as the fact that B - A is a positive semi-definite matrix. For any P, Q \u2208 \u2206(S), the total variation divergence of P and Q is defined as D(P||Q) = 1/2 \u222bs |P(s) \u2013 Q(s)|ds."}, {"title": "2 Related Work", "content": "Distributionally Robust MDPs There has been a large body of works studying DRMDPs under various settings, for instance, the setting of planning and control (Xu and Mannor, 2006; Wiesemann et al., 2013; Yu and Xu, 2015; Mannor et al., 2016; Goyal and Grand-Clement, 2023) where the exact transition model is known, the setting with a generative model (Zhou et al., 2021b; Yang et al., 2022; Panaganti and Kalathil, 2022; Xu et al., 2023; Shi et al., 2023; Yang et al., 2023a), the offline setting (Panaganti et al., 2022; Shi and Chi, 2022; Blanchet et al., 2023) and the online setting (Dong et al., 2022; Liu and Xu, 2024a; Lu et al., 2024). Among tabular DRMDPs, the most relevant studies to ours are Shi et al. (2023); Lu et al. (2024). In particular, Shi et al. (2023) studies tabular DRMDPs with TV uncertainty sets. They provide an information-theoretic lower bound, as well as a matching upper bound on the sample complexity. The key message is that the sample complexity bounds depend on the uncertainty level, and when the uncertainty level is of constant order, policy learning in a DRMDP requires less samples than in a standard MDP. Further, Lu et al. (2024) studies the online tabular DRMDPs with TV uncertainty sets, they provide an algorithm that achieves the near-optimal sample complexity under a vanishing minimal value assumption to circumvent the curse of support shift.\nOnline Linear MDPs and Linear DRMDPs The nominal model studied in our paper is assumed to be a linear MDP with a simplex feature space. There is a line of works studying online linear MDPs (Yang and Wang, 2020; Jin et al., 2020; Modi et al., 2020; Zanette et al., 2020; Wang et al., 2020a; He et al., 2021; Wagenmaker et al., 2022; Ishfaq et al., 2023), and the minimax optimality of this setting is studied in the recent work of He et al. (2023). In particular, they adopt the variance-weighted ridge regression scheme and the \u2018rare-switching' policy update strategy in their algorithm design. The setting of online linear DRMDP is relatively understudied, with both the lower bound and the near-optimal upper bound remain elusive. Specifically, the only work studies the online linear DRMDP setting is Liu and Xu (2024a). Under the TV uncertainty set, their algorithm, DR-LSVI-UCB, achieves an average suboptimality of the order  \u00d5(d2H2/\u221aK). However, recent evidence from studies (Liu and Xu, 2024b; Wang et al., 2024) on offline linear DRMDPs suggests that this rate is far from optimality. In particular, Liu and Xu (2024b) proves that their algorithm, VA-DRPVI, achieves an upper bound on the suboptimality in the order of \u00d5(dH min{1/\u03c1, \u0397}/\u221a/K). Nonetheless, their algorithm and analysis are based on a pre-collected offline dataset which satisfies some coverage assumption, and thus cannot be utilized in the online setting, where a strategy on data collection is required to deal with the challenge of exploration and exploitation trade-off."}, {"title": "3 Preliminary", "content": "We use a tuple DRMDP(S, A, H,U\u03c1(P\u25e6), r) to denote a finite horizon distributionally robust Markov decision process (DRMDP), where S and A are the state and action spaces, H \u2208 Z+ is the horizon length, P\u25e6 = {Ph}Hh=1 is the nominal transition kernel, UP(P\u25e6) = {Uh (P)}h\u2208 [H] denotes an uncertainty set centered around the nominal transition kernel with an uncertainty level p \u2265 0, r = {rh}Hh=1 is the reward function. A policy \u03c0= {\u03c0\u03b7}Hh=1 is a sequence of decision rules. For any policy \u03c0, we define the robust value function V\u03c0\u03c1 (s) = inf P\u2208UP(P\u25e6) EP [\u2211Hh=1rh(St, at)|S1 = s,\u03c0] and the robust Q-function Q\u03c0\u03c1 (s, a) = inf P\u2208UP(P\u25e6) EP [\u2211Hh=1rh(St, at)|S1 = s,a1 = a, \u03c0] for any (h,s,a) \u2208 [H] \u00d7 S \u00d7 A. Moreover, we define the optimal robust value function and optimal robust state-action value function: for any (h,s,a) \u2208 [H] \u00d7 S \u00d7 A, V \u2217\u03c1 (s) = sup\u03c0\u2208\u03a0 V\u03c0\u03c1 (s), Q\u2217\u03c1 (s, a) = sup\u03c0\u2208\u03a0 Q\u03c0\u03c1 (s, a), where \u03a0 is the set of all policies. Correspondingly, the optimal robust policy is the policy that achieves the optimal robust value function \u03c0\u2217 = argsup\u03c0\u2208\u03a0V\u03c0\u03c1 (s)."}, {"title": "Assumption 3.1", "content": "Given a known feature mapping \u03c6 : S \u00d7 A \u2192 Rd satisfying  \u2211di=1 \u03c6i(s, a) = 1, \u03c6i(s, a) \u2265 0, for any (i, s, a) \u2208 [d] \u00d7 S \u00d7 A, we assume the reward functions {rh}Hh=1 and nominal transition kernels {Ph}Hh=1 are linearly parameterized. Specifically, for any (h, s, a) \u2208 [H] \u00d7 S \u00d7 A, rh(s,a) = (\u03c6(s, a), \u03b8h), Ph(\u00b7|s, a) = (\u03c6(s, \u03b1), \u03bc\u03b7(\u00b7)), where {\u03b8h}Hh=1 are known vectors with bounded norm ||\u03b8h||2 \u2264 \u221ad and {\u03bc\u03b7}Hh=1 are unknown probability measures over S."}, {"title": "In d-rectangular linear DRMDPs", "content": "the uncertainty set UP(P\u25e6) is defined based on the linear structure of P\u25e6 satisfying Assumption 3.1. In particular, we first define the factor uncertainty sets as Uni(\u03bc\u03b7) = {\u03bc : \u03bc\u2208 \u2206(S), D(\u03bc||\u03bc\u03b7,i) \u2264 \u03c1}, \u2200(h,i) \u2208 [H] \u00d7 [d]. In this work we choose D(.||.) as the total variation (TV) divergence. Then we define the uncertainty set as UP(P\u25e6) = {\u2211di=1 \u03c6i(s, a)\u03bc\u03b7,i(\u00b7) : \u03bc\u03b7,i(\u00b7) \u2208 Uni (\u03bc\u03b7), \u2200i \u2208 [d]}.\nLiu and Xu (2024a) show that the following robust Bellman equations hold, that is for any policy \u03c0,\nQ\u03c0\u03c1 h (s, a) = rh(s, a) + inf Ph(\u00b7|s,a)\u2208U\u03c1 (s,a;\u03bch) [PhV\u03c0\u03c1 h+1](s, a), (3.1a)\nV\u03c0\u03c1 h (s) = Ea\u223c\u03c0\u03b7(s) [Q\u03c0\u03c1 h(s, a)], (3.1b)\nas well as the robust Bellman optimality equations\nQ\u2217\u03c1 h (s, a) = rh(s, a) + inf Ph(\u00b7|s,a)\u2208U\u03c1 (s,a;\u03bch) [PhV \u2217\u03c1 h+1](s, a), (3.2a)\nV\u2217\u03c1 h (s) = max a\u2208A Q\u2217\u03c1 h(s, a). (3.2b)\nIn the context of online DRMDPs, an agent actively interacts with the nominal environment within K episodes to learn the optimal robust policy. Specifically, at the start of episode k, an agent chooses a policy \u03c0k based on the history information and receives the initial state sk1. Then the agent interacts with the nominal environment by executing \u03c0k until the end of episode k, and collects a new trajectory. The goal of the agent is to minimize the average suboptimality after K episodes, which is defined as\nAveSubopt(K) = 1/K \u2211Kk=1 [V \u2217(sk1) \u2013 V\u03c0k\u03c1 (sk1)].\nLu et al. (2024) recently show that in general sample efficient learning in online DRMDPs is impossible due to the curse of support shift, i.e., the nominal kernel and target kernel do not share the same support. By designing proper feature mappings, we show that their hard example implies the same hardness result for the online linear DRMDP setting."}, {"title": "Proposition 3.2", "content": "(Hardness result) There exists two d-rectangular linear DRMDPs {M0, M1}, such that inf ALGS supe\u2208 {0,1} E[AveSuboptM\u03f5, ALG (K)] \u2265 \u03a9(\u03c1\u00b7 H), where AveSuboptM\u03f5, ALG(K) is the average suboptimality of algorithm ALG under the d-rectangular linear DRMDP M\u03f5.\nNote that the lower bound in Proposition 3.2 does not converge to zero as K increases, which means that in general no algorithm can guarantee to learn the optimal robust policy approximately. To circumvent this problem, in the rest of paper we focus on a tractable subclass of d-rectangular linear DRMDP following Liu and Xu (2024a); Lu et al. (2024), which is formally defined in the following assumption."}, {"title": "Assumption 3.3", "content": "(Fail-state). Assume there exists a \u2018fail state' sf in the d-rectangular linear DRMDP, such that for all (h, a) \u2208 [H] \u00d7 A, rh(sf, a) = 0, Ph(sf|sf, a) = 1."}, {"title": "With Assumption 3.3", "content": "we can follow the framework in Liu and Xu (2024a), where we have the following results on robust value functions and dual formulation that are helpful in solving the optimization in (3.2).\nProposition 3.4 (Remark 4.2 of Liu and Xu (2024a)). Under Assumption 3.3, for any (\u03c0, h, a) \u2208 \u03a0\u00d7 [H] \u00d7 A, we have Q\u03c0\u03c1 h (sf, a) = 0, and V\u03c0\u03c1 h(sf) = 0. Moreover, for any function V : S \u2192 [0, H] with mins\u2208s V(s) = V(sf) = 0, we have inf\u03bc\u2208\u03c5\u03c1 (\u03bc\u25e6) Es\u223c\u03bcV(s) = max a\u2208[0,H] {Es\u223c\u03bc\u25e6 [V(s)]a \u2013 \u03c1\u03b1}."}, {"title": "4 Algorithm Design", "content": "One prominent property of the d-rectangular DRMDP is that the robust Q-functions possess linear representations with respect to the feature mapping \u03c6. In particular, under Assumptions 3.1 and 3.3, Liu and Xu (2024a) show that for any (\u03c0,s,a,h) \u2208 \u03a0 \u00d7 S \u00d7 A \u00d7 [H], the robust Q-function Q\u03c0\u03c1 h (s, a) has a linear form as follows\nQ\u03c0\u03c1 h (s, a) = (rh(s,a) + \u03c6(s, a) Tv\u03c0\u03c1 h )1{s \u2260 sf },\nwhere v\u03c0\u03c1 h = (v\u03c0\u03c1 h,1 , . . . . . , v\u03c0\u03c1 h,d )T , v\u03c0\u03c1 h,i = max\u03b1\u2208[0,H] {Ti,h(a) \u2013 \u03c1\u03b1}, Ti,h(\u03b1) = E\u03bch [Vh+1(s')] and \u03b1\u2208 [0, H] is the dual variable derived from the dual formulation (see Proposition F.1 for more details). Moreover, the robust Bellman optimality equation (3.2) shows that the greedy policy with respect to the optimal robust Q-function is exactly the optimal robust policy \u03c0\u2217. Therefore, the core idea behind the algorithm design is to estimate the optimal robust Q-function using linear function approximation, and then find \u03c0\u2217 by the greedy policy derived from the estimated optimal robust Q-function. We present our algorithm in Algorithm 1. In the sequel, we provide detailed discussion about the components in our algorithm design."}, {"title": "4.1 Variance-Weighted Ridge Regression for Online DRMDPs", "content": "From Line 6 to 17 of Algorithm 1, we adopt the backward induction procedure to update the robust Q-function estimation. In particular, for any (k,h) \u2208 [K] \u00d7 [H], suppose we have an estimated robust value function Vk,h+1. By the robust Bellman optimality equation (3.2) and Proposition 3.4, conducting one step backward induction on Vk,h+1 leads to the following linear form (Liu and Xu, 2024a):\nrh(s, a) + inf Ph\u2208U\u03c1 (s,\u03b1;\u03bch) Ph[Vk,h+1](s, a) = \u03c6(s, a)T (\u03b8h + vkh )1{s \u2260 sf }, (4.1)\nwhere vkh,i := max \u03b1\u2208[0,H] {zh,i(a) \u2013 \u03c1\u03b1} and zh,i(a) := E\u03bch, [Vk,h+1(s')]a, for any i \u2208 [d]. Note that under Assumption 3.1, for any a \u2208 [0, H], zh,i(a) is the i-th element of the parameter of the following linear formulation, [Ph[Vk,h+1]a](s, a) = (\u03c6(s,a), zh(a)). Thus, we can estimate zh(a) from data to get estimations of zkh,i(a), \u2200i \u2208 [d]. To this end, we introduce the variance-weighted ridge regression regime to estimate zkh(a) as follows\nzk(a) = argmin z\u2208Rd 1/k \u2211k\u03c4 =1 \u03c3\u03c4\u22121,h (y\u03c4,h(z + \u03c6(s\u03c4, a\u03c4) \u2212 [Vk,h+1(s\u03c4 +1)]a) 2 + \u03bb||z|| 2 , (4.2)\n= 2/k \u2211k\u03c4 =1 \u03c3\u03c4\u22121,h \u2211\u03c4 \u03c6(s\u03c4, a\u03c4) [Vk,h+1(s\u03c4 +1)],\nwhere \u03a3k,h = \u03bb\u0399 + \u2211k\u03c4 =1 \u03c3\u03c4\u22121,h\u03c6(s\u03c4, a\u03c4)\u03c6(s\u03c4, a\u03c4)T , \u03c3\u03c4,h are regression weights that will be formally introduced later. We then approximate vkh by solving the optimization problem element-wisely\nvkh,i = max \u03b1\u2208[0,H] {zh,i(\u03b1) \u2013 \u03c1\u03b1}, i\u2208 [d]. (4.3)"}, {"title": "Further, we incorporate a bonus term", "content": "\u00cekh(s, a) = \u03b2 \u2211di=1 \u03c6i(s, a) \u221a 1 {\u03a3klast i,h = 1}, where \u03b2 = O(H\u221adx + Further, we incorporate a bonus term \u03a3 klast < klast }, into the robust Q-function estimation. Inspired by He et al. (2023), we also establish pessimistic estimated robust Q-functions by the same backward induction procedure, which will be helpful in constructing the variance estimator \u03c3k,h as shown in the next section. In particular, given Vk,h we estimate\nzh(a) = argmin z\u2208Rd 1/k \u2211k\u03c4 =1 (y\u03c4,h(z + \u03c6(s\u03c4, a\u03c4) \u2212 [Vk,h+1(s\u03c4 +1)]a) 2 + \u03bb||z|| 2 ,\n= 2/k \u2211k\u03c4 =1 \u03c3\u03c4\u03c6(s\u03c4, a\u03c4) [Vk,h+1(s\u03c4 +1)],\nand then get the estimation\n\u03bckh,i = max \u03b1\u2208[0,H] {zh,i(\u03b1) \u2013 \u03c1\u03b1}, i\u2208 [d]. (4.4)\nNext, by incorporating a penalty term Skh(s, a) = \u03b2 \u2211di=1 \u03c6i(s, a) \u221a 1 {\u03a3last << klst }, where \u03b2 = O(H\u221ax + 1\u03a3 klast < klst }, into the robust Q-function estimation. Liu and Xu (2024b) also construct pessimistic robust Q-function estimations, but 1) they do not construct the estimation episodically, 2) their pessimistic estimators are used to get the optimal robust policy estimation. While ours are used to construct the variance estimator, as is shown in the next section."}, {"title": "4.2 Variance Estimator with Refined Dependence on Problem Parameters", "content": "In this section, we construct the weights used in (4.2) and aim to get an optimistic estimator for the variance of the optimal robust value function, V \u2217\u03c1,V\u2217\u03c1 h . Inspired by He et al. (2023), the variance estimator at episode k should be a uniform variance upper bound for all subsequent episodes. To obtain the optimistic estimator for V \u2217\u03c1,V\u2217\u03c1 h+1, we first solve regression problems to obtain the estimator for Vk,h+1, which is denoted as Vkh+1. Then we analyze the error between Vkh+1 and V \u2217\u03c1,V\u2217\u03c1 h to finish the construction. Different from Liu and Xu (2024b, Equation (5.2)), the variance estimator here is not trivially constructed from subtracting a specific penalty term because we should guarantee the monotonicity of estimated variance for the online exploration.\nThe variance of estimated optimistic value function Vk,h+1 can be denoted by\n[VhVh+1] (s, a) = [P(Vk,h+1) 2] (s, a) \u2013 ([PVk,h+1](s, a))2. (4.5)\nUnder Assumption 3.1, P(Vk,h+1) 2 and PVk,h+1 on the RHS of (4.5) are linear in \u03c6(s, a) based on Jin et al. (2020, Proposition 2.3). Thus we can approximate them as follows\n[VhVh+1](s, a) \u2248 [VhVh+1] (s,a) = [\u03c6(s, a)T zh,2][0,H2] \u2013 [\u03c6(s, a)T zh,1][0,H].\nwhere zkh,1 and zkh,2 are solutions to the following ridge regression problems\nzh,2 = argmin z\u2208Rd 1/k \u2211k\u03c4 =1 (zh(s\u03c4, a\u03c4) \u2212 (Vk,h+1(s\u03c4 +1)) 2 ) 2 + \u03bb||z|| 2 ,\nzh,1 = argmin z\u2208Rd 1/k \u2211k\u03c4 =1 (zh(s\u03c4, a\u03c4) \u2212 Vk,h+1(s\u03c4 +1)) 2 + \u03bb||z|| 2 ."}, {"title": "Different from the variance estimation in", "content": "He et al. (2023), we construct both 2, and 2,1 by solving vanilla ridge regressions, instead of variance-weighted ridge regressions. This specific choice of parameter estimation will simplify our analysis of the variance estimation error, while fully capture the variance information. Now we can construct \u03c3k,h, which is the estimated variance of the optimal robust value function V \u2217h in episode k, as follows\n\u03c3k,h = \u221a[VhVh+1] (sh, ah) + Ekh + d3H. Dkh + 1/2, (4.6)\nwhere Ekh represents the error between the estimated variance and the true variance of V Vkh+1, and Dkh represents the error between the true variance of Vh+1 and the true variance of V 1. We define Ekh, Dkh as follows\nEkh = min {3||\u03c6(s, a) ||Akh , H2} + min {2H3||\u03c6(s, a) ||Akh , H2},\nDkh = min 4H($(s,a)1 -(s,)) + 23||\u03c6(sh, all, .h}."}, {"title": "Algorithm 1 Weighted Distributionally Robust Iterative Value Estimation with UCB (We-DRIVE-U)", "content": "1: Initialization: hyperparameters \u03b2, \u03b2, \u03b2 > 0 and \u03bb > 0. Set klast = 0; for each stage h \u2208 [H], set \u03a30,h, \u03a31,h, A1,h\u2190 \u03bbI and set Q0h(\u00b7,\u00b7) \u2190 H, Q0h(\u00b7, \u00b7) \u2190 0\n2: for episode k = 1,\u2026, K do\n3: Receive the initial state s1k\n4: Set VH+1() 0, VH+1() \u2190 0\n5: if there exists a stage h' \u2208 [H] such that det (\u03a3k,h') \u2265 2det(\u03a3klast,h') then\n6: for stage h = H,\u2026\u2026 ,1 do\n7: if h = H then\n8: Vk,H \u2190 0, Vk,H \u2190 0\n9: else\n10: Compute vkh,i, \u2200i \u2208 [d] according to (4.3) and vkh,i , \u2200i \u2208 [d] according to (4.4).\n11: end if\n12: Qkh(s, a) \u2190 min {rh(s,a) + \u03c6(s, a)T vkh + \u00cekh(s,a), Qk\u22121,h(s,a), H \u2212 h + 1}1{s \u2260 sf }\n13: Qh(s, a) \u2190 max {rh(s,a) + \u03c6(s, a)T vkh \u2212 Skh(s, a), Qh-1,h(s,a), 0}1{s \u2260 sf }\n14: Set the last updating episode klast \u2190 k\n15: Vkh(s) \u2190 max a Qkh(s, a), Vkh(s) \u2190 max a Qh(s, a)\n16: \u03c0(s) \u2190 argmax a\u2208A Qkh(s, a)\n17: end for\n18: else\n19: Vkh(s) \u2190 Vk\u22121h(s), Vkh(s) \u2190 Vk\u22121h(s), \u03c0h(s) \u2190 \u03c0k\u22121h(s) for all h\u2208 [H]\n20: end if\n21: for stage h = 1,\u2026\u2026\u2026, H do\n22: Take the action ah1\u2190\u03c0k (sh1)\n23: Calculate the estimated variance \u03c3k,h according to (4.6) and \u03c3k,h according to (4.7)\n24: \u03a3k+1,h \u2190 \u03a3kh + \u03c3k,h\u03c6(sh1, ah1)\u03c6(sh1, ah1)T , Ak+1,h\u2190 Akh + \u03c3k,h\u03c6(sh1, ah1)\u03c6(sh1, ah1)T\n25: Receive next state sh+1 1\n26: end for\n27: end for\nDifferent from the variance estimation in He et al. (2023), we construct both \u03a32, and \u03a32,1 by solving vanilla ridge regressions, instead of variance-weighted ridge regressions. This specific choice of parameter estimation will simplify our analysis of the variance estimation error, while fully capture the variance information. Now we can construct ok,h, which is the estimated variance of the optimal robust value function V \u2217h in episode k, as follows\nOk,h = \u221a(VhVh+1) (sh, ah) + Ekh + d3H. Dkh + 1/2, (4.6)\nwhere Ekh represents the error between the estimated variance and the true variance of V Venh+1, and Dkh represents the error between the true variance of Vh+1 and the true variance of V 1. We define Ekh, Dkh as follows\nEkh = min 3||\u03c6(s, a) ||Akh , H2 + min 2H3||\u03c6(s, a) ||Akh , H2 ,\nDkh = min 4H($(s,)-$(s,)) + 23||\u03c6(sh, all, .h."}, {"title": "Remark 4.1", "content": "We highlight that Algorithm 1 is the first algorithm adopting 'rare-switching' update strategy for distributionally robust RL. Different from He et al. (2023), the \u2018rare-switching' condition on Line (5) is set at the beginning of each episode. This is achieved"}]}