{"title": "Federated Learning in Practice: Reflections and Projections", "authors": ["Katharine Daly", "Hubert Eichner", "Peter Kairouz", "H. Brendan McMahan", "Daniel Ramage", "Zheng Xu"], "abstract": "Federated Learning (FL) is a machine learning technique that enables multiple entities to collaborat-\nively learn a shared model without exchanging their local data. Over the past decade, FL systems have\nachieved substantial progress, scaling to millions of devices across various learning domains while offer-\ning meaningful differential privacy (DP) guarantees. Production systems from organizations like Google,\nApple, and Meta demonstrate the real-world applicability of FL. However, key challenges remain, includ-\ning verifying server-side DP guarantees and coordinating training across heterogeneous devices, limiting\nbroader adoption. Additionally, emerging trends such as large (multi-modal) models and blurred lines\nbetween training, inference, and personalization challenge traditional FL frameworks. In response, we\npropose a redefined FL framework that prioritizes privacy principles rather than rigid definitions. We\nalso chart a path forward by leveraging trusted execution environments and open-source ecosystems to\naddress these challenges and facilitate future advancements in FL.", "sections": [{"title": "1 Evolution of Federated Learning", "content": "Federated Learning (FL) was introduced around 2016 as a privacy enhancing technique that directly applies\nthe principle of data minimization by focused collection and immediate aggregation [35], which \u201cenables\nmobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoup-\nling the ability to do machine learning from the need to store the data in the cloud.\" [26, 47] FL quickly became a\nwidely-acknowledged paradigm of distributed learning from decentralized data, and has been adopted in\nvarious applications beyond the original on-device training scenarios: for example, the FL paradigm has\nbeen applied to collaborative learning across multiple institutions (silos) with richer computation resources\nthan mobile devices, or to learning over Internet-of-Things devices with more limited resources. In 2019,\nout of the discussion in the Workshop on Federated Learning and Analytics at Google, Kairouz et al. [41]\nproposed a broader definition of FL:\n\nFederated learning is a machine learning setting where multiple entities (clients) collaborate in solving a\nmachine learning problem, under the coordination of a central server or service provider. Each client's raw\ndata is stored locally and not exchanged or transferred; instead, focused updates intended for immediate\naggregation are used to achieve the learning objective.\n\nFederated Analytics (FA) was introduced later as \u201cthe practice of applying data science methods to the analysis\nof raw data that is stored locally on users' devices. Like FL, it works by running local computations over each device's\ndata, and only making the aggregated results \u2014 and never any data from a particular device \u2014 available to product\nengineers.\" [28] The discussions in this manuscript will primarily focus on FL unless otherwise specified,"}, {"title": "", "content": "although they should also be applicable to FA as the two paradigms are closely related to each other and\nshare similar privacy principles [11, 68].\nBoth FL and FA have made remarkable progress in theory and practice in recent years [25, 41, 44, 64, 68,\n77]. However, despite this progress, production FL systems continue to face a number of existing and new\nchallenges:\n\nVery large and often multi-modal models achieve unprecedented performance for various tasks, but\ntypically are orders of magnitude beyond what has been considered in classical cross-device FL ap-\nplications.\n\nCurrent federated learning systems provide little in the way of verifiability of server-side computa-\ntions, and even verification of client-side work can be difficult. This limits the ability of a user or\nexternal auditor to confirm the privacy properties of the system, and generally the extent to which\ntrust in the service provider can be minimized.\n\nFinally, our practical experience with cross-device FL is that it is often possible but seldom easy. Co-\nordinating training across loosely synchronized and heterogeneous devices (heterogeneous in com-\npute, bandwidth, availability for training, data, and even software version) produces countless oper-\national challenges that hinder the broader adoption of FL.\n\nTo facilitate the advancement of next generation federated technologies considering the above-mentioned\nchallenges and opportunities, we revisit the defining characteristics and propose a new definition of FL,\nwhich aims not to draw a hard line between what \u201cis\u201d and \u201cisn't\u201d FL, but rather highlight the principles\nand aspirations of research and infrastructure. Before proceeding, we first review the privacy principles\ninitially presented by Bonawitz et al. [11]:\n\nThe user has transparency, auditability, and control over what data is used, what purpose it is used\nfor, and how it is processed. This includes forward-looking transparency, retrospective auditability\nof computation or release details, control of at least the immediate use of data (e.g. in training) in\naddition to others.\n\nProcessing of user data (whether training examples or gradients) should encode data minimization by\nreducing the information any actor has access to at every node in the system. This includes things like\nsending only focused, minimal updates back to the service provider (rather than raw data), aggreg-\nating the updates in memory, sharing only select updates with the engineers that have requested the\ncomputation, and using secure enclaves and/or cryptographic primitives to hide potentially sensitive\ndata from various actors in the system.\n\nReleased outputs should provide formal data anonymization guarantees, ensuring that released out-\nputs do not reveal anything unique to an individual. In other words, aggregate statistics, including\nmodel parameters, when released to an engineer (or beyond) should not vary significantly based on\nwhether any particular user's data was included in the aggregation.\n\nPrivacy claims are verifiable ideally by the users themselves, by external auditors, and the service\nprovider.\n\nTo more effectively capture the aforementioned privacy principles and address the outlined challenges, we\npropose the following new definition:\n\nFederated learning (FL) is a machine learning setting where multiple entities (clients) collaborate in\nsolving a machine learning problem, under the coordination of a service provider. A complete FL sys-\ntem should enable clients to maintain full control over their data, the set of workloads allowed to access\ntheir data, and the anonymization properties of those workloads. FL systems should provide appropriate\ntransparency and control to the users whose data is managed by FL clients."}, {"title": "2 Advances in Federated Learning in Practice", "content": "In recent years, practical FL systems have harnessed significant advancements by the community: we can\nscale to millions of devices and many domains; we can apply secure multiparty computation protocols at\nscale and combine them with central or distributed DP; we can successfully train production models with\nmeaningful DP guarantees while achieving high utility. In this section, we summarize recent progress of\nFL in practice by reexamining the open problems in FL, taking a retrospective view inspired by Kairouz\net al. [41]. Rather than conducting an exhaustive review of recent publications, we emphasize the practical\ndevelopments and highlight avenues where more research is needed. The discussions heavily focus on\nthe progress in industry applications built on large-scale systems that are primarily consolidated from the\nkeynote talks and discussions from the Federated Learning and Analytics in Practice Workshop [75], and\nbiased to cross-device federated learning (compared to cross-silo or other settings) due to the familiarity of\nthe authors.\nApplications At Google, FL has been applied to training several machine learning models powering\nadvanced features in mobile keyboard (Gboard) including next word prediction [31, 61, 76], smart com-\npose and on-the-fly rescoring for suggestions [76], and emoji suggestion [56]. Some additional applica-\ntions include keyword spotting model for virtual assistants [32], smart text selection on Android [33, 34],\nsmart reply and other assistive suggestions in Android Messages [27], and improving user experience on\nPixel phones [30]. FA has been applied to Google Health Studies to power privacy-preserving health re-\nsearch [29], and in Apple Photos to identify iconic scenes [4].\nSystems Several production systems have been built and discussed, e.g., the cross-device federated sys-\ntem at Google [10], Apple [50, 54, 65], Meta [38, 62]. Large-scale cross-device systems such as these share\nchallenges related to computation and resource constraints, including: limited server-side control over\nclient participation because devices can only train when they meet (restrictive) local criteria (e.g. being\nconnected to an unmetered network and having appropriate power/charging and idle status); limited and\nheterogeneous computation power per device; and limited bandwidth as well as relatively high likelihood\nof dropping out mid-computation. Real-world systems have developed different approaches to tackle the\nclient scheduling challenges from intermittent connection and stragglers. For example, Bonawitz et al. [10]\nused oversampling and dropout, and Huba et al. [38] used asynchronicity.\nPrivacy Federated learning realizes the data minimization privacy principle [11] in collaborative learning,\nand can combine with other techniques to strengthen privacy protection. For example, secure aggregation\nmethods are used to enhance guarantees for data minimization, and differential privacy (DP) methods\nare used to provide data anonymization guarantees. Single-server secure aggregation (SecAgg) [9] can\nguarantee that an honest-but-curious server can only observe the aggregated updates derived from many"}, {"title": "3 Challenges and Opportunities", "content": "The previous section outlined significant advancements in deploying federated learning systems across\nvarious domains. However, despite this progress, several key challenges remain, which we will examine in\ndetail in this section.\n3.1 Scaling to Large Foundation Models\nRecently, large foundation models [8] have attracted much attention in both academia and industry, and\nhave drastically changed the machine learning paradigm. Such models (e.g., OpenAI GPTs [52] and Dall-\nEs [59], Google PaLMs [3] and Geminis [66], and Meta Llamas [22, 67]) have very large parameter size that\ncan easily scale up to hundreds of billions parameters, and are pre-trained with a very large dataset that\ncan have (tens of) trillions of tokens. The scale of the model and data are both much larger than previous\ndeep learning applications and what has been explored in cross-device federated learning.\nSuch foundation models are strong few-shot and zero-shot learners and can accomplish various tasks\nwith the help of instruction tuning and prompt engineering [13, 53, 70, 71], outperforming previous domain\nspecific smaller models. The development of large language models relies heavily on extensive, high-\nquality user data, underscoring the growing importance of privacy-preserving techniques in the training\nprocess. Here are four primary avenues for incorporating user data into foundation models.\nPost-training, popularized by instruction tuning [53] that combines supervised fine-tuning and RLHF\ntechniques, has become a standard for training large models. User instructions are crucial for aligning\nlarge models, but may also contain sensitive private information [79] that can be memorized [51].\nUser data can be particularly helpful when adapting foundation models to specific domains, for ex-\nample, for medical usage [12, 60]. For improving user typing experience in virtual keyboard, early\nexperiments [73] suggest the current practices of leveraging large models still cannot compete with"}, {"title": "", "content": "what can be achieved by privacy-preserving training with user data. Fine-tuning pre-trained large\nfoundation models on domain specific user data is therefore important but has been shown to carry\nseveral privacy risks [42].\nThere is a growing interest in training smaller foundation models of billions parameters instead of\ntens of billions parameters to reduce serving cost and inference latency, and deploying on-device to\nimprove privacy. Early experiments [18, 79] suggest high-quality in-domain data can be used to close\nthe gap between large and small foundation models.\nIn addition, there are concerns that foundation models have exhausted the available public data on\nthe web, and the public data will be more and more polluted by hallucinated content generated by\ncurrent large models [58].\nFederated learning of large language models is an active research topic, with several surveys released in\nthe last two years [16, 72, 78, 80, 82]. While researchers have been working hard to develop new algorithms\nscaling up the model size in FL, the current FL system can only reliably train models with millions of\nparameters in practical applications (especially in the cross-device FL setting, see Section 2). We highlight\nchallenges in scaling to large foundation models in FL. The communication and computation resource re-\nquirements have been important considerations through the multi-year development of FL. More recently,\nwe have observed for cross-device FL that computation and memory constraints of mobile devices have\nbecome the main bottleneck for training large models. Large foundation models bring this challenge to the\nnext level. The opportunities of private training for LLMs and challenges of on-device training motivate us\nto rethink the design of federated learning systems.\n3.2 Verifying Server-side Privacy Guarantees\nAs discussed in the introduction, protecting the privacy of users that participate in federated training is of\nutmost importance since FL's primary motivation is privacy. We now turn our attention to describing the\nremaining challenges in this space.\nThe first generation of FL algorithms and systems (referred to as FL 2017-2020) offered data minimiza-\ntion but still suffered from the possibility of exposing private information through model updates, which\ncan be exploited by a malicious service provider. Indeed, without proper safeguards, a dishonest or com-\npromised service provider could analyze unaggregated updates to infer private details about individual\nparticipants [7, 63].\nSince then, several techniques have been developed to mitigate some of these risks, including secure\nmultiparty computation (SMPC) schemes, such as those based on honest-majority cohorts [9], non-colluding\nsecure aggregators [65], and hardware-based trusted execution environments (TEEs) [37]. These methods\nstrengthened the data minimization guarantees and ensured that an honest-but-curious server\u00b9 can only\nsee aggregated model updates.\nAnother important development that happened is incorporating data anonymization in federated sys-\ntem by using differential privacy (DP) [23]. Recent work [74, 75] has demonstrated the feasibility of training\nhigh-utility models with DP, ensuring that model parameters remain statistically indistinguishable whether\nor not a particular device's data is included.\nWe have also seen attempts at combining data minimization and data anonymization techniques. For\nexample, distributed DP based FL systems [2, 34, 39] combine single-server secure aggregation protocols\nwith on-device noise to ensure that the service provider can only see a differentially private aggregate.\nUnder distributed DP, clients first compute minimal application-specific reports, perturb these slightly with\nrandom noise, and then execute a private aggregation protocol. The server then has access only to the\noutput of the private aggregation protocol. The noise added by individual clients is typically insufficient for"}, {"title": "", "content": "a meaningful local DP guarantee on its own. After private aggregation, however, the output of the private\naggregation protocol provides a stronger DP guarantee based on the total sum of noise added across all\nclients. This applies even to someone with access to the server under the security assumptions necessary\nfor the private aggregation protocol.\nDistributed DP represented a major leap forward in improving the privacy guarantees of an FL system.\nHowever, distributed DP algorithms suffer from principal limitations that stem from the complexities in-\nvolved in implementing state-of-the-art DP mechanisms in a distributed setting. These mechanisms either\nrequire complex random device sampling protocols, which are difficult to achieve securely in a distributed\nenvironment [65], or depend on statefulness [40, 49], which poses additional implementation challenges.\nConsequently, a notable performance gap remains between centralized DP models and distributed DP mod-\nels.\nAnother critical challenge in achieving robust verifiable privacy guarantees lies in ensuring resilience\nagainst Sybil attacks [21]. In such attacks, a malicious service provider could inject specially crafted mes-\nsages into the secure aggregation process to extract sensitive information about a specific individual. De-\nveloping scalable and robust defenses against this sort of vulnerability, particularly in SMPC-based secure\naggregation schemes, remains an open problem.\nWhile substantial progress has been made in training models with meaningful differential privacy in\nfederated settings, further work is needed to ensure external verification of these privacy guarantees. Ad-\ndressing the gap between centralized and distributed DP, as well as mitigating the risks posed by ad-\nversarial behaviors, will be crucial for the continued adoption and trustworthiness of federated learning\nin production systems.\n3.3 Addressing System Challenges\nThe last few years saw several large-scale deployments of federated systems from various companies, in-\ncluding Google [10], Apple [50, 54, 65], and Meta [38, 62]. Google's cross-device federated learning system\n[10] features various synchronization points - in part to support the synchronous, round-based FedAvg\nlearning algorithm [47], in part to aid data minimization by supporting the secure aggregation protocol [9].\nNotably, [38] propose an asynchronous system instead, in large part due to system design considerations;\nhere we elaborate on the challenges faced by the synchronous system introduced in [10]. Specifically, co-\nhort formation - collecting a set of devices that execute a federated computation - and aggregation represent\npoints where the system blocks and a decision has to be made when to proceed or fail. Blocking means in\nmost cases keeping devices waiting, which is inefficient, increases the probability of devices dropping out\nand hence downstream failures, and can induce bias [41]. Hard cut-offs or making the associated pro-\nceed/fail decision - leads to a variety of problems in understanding and therefore debugging, maintaining\nor improving the system:\nHard cut-offs lead to bifurcation points (phase transitions). Like in non-linear dynamical systems\nor deep neural nets, small upstream changes can induce sudden / large qualitative downstream\nchanges; likewise, large upstream changes may not have any expected downstream effects.\nSynchronization points are all potential failure points - places where computations can fail under\nnormal operation because some timeout is hit or a threshold is not reached, significantly complicating\ndebugging because an entire class of errors may be fine and not indicate a real problem.\nLots of knobs make the system harder to operate, monitor, and optimize: time-outs, or thresholds\n(e.g., reporting goal for minimum number of participating clients every round), and overallocation of\ndevices to mitigate dropout lead to more telemetry, documentation, and require system understand-\ning.\nSynchronization points imply coordination across components, leading to complex architectures, cas-\ncading errors and network effects."}, {"title": "", "content": "Synchronization points can cause problems for A/B experiments; while a typical set up would split\ne.g. devices into control and treatment groups that differ in one setting and are independent of each\nother, synchronization points introduce dependencies and thus violate the assumption behind A/B\nexperiments.\n4 A Path to the Future\nBuilding upon years of development for advanced FL (Section 2), we discuss a potential path towards\nthe future that instantiates the new FL definition (Section 1) to address challenges in Section 3. There is\ngrowing interest in confidential cloud computation based on hardware and encryption for privacy and\nsecurity, including for private inference [5] and federated learning systems [38].\nOur recent work in Eichner et al. [24] proposed a system design for confidential federated computation\nthat leverages Trusted Execution Environments (TEEs) to significantly improve privacy claims with ex-\nternal verifiability, while simultaneously improving system robustness and scalability. In contrast to earlier\ndesigns, confidential federated computations allow the device to verifiably limit any server-side processing\nof uploaded messages to a fixed, known set of approved, privacy preserving workloads. Before upload,\ndevices encrypt messages with a public key whose private key is held by a TEE-hosted ledger service.\nDevices verify that the public keys are generated by a ledger binary built from known OSS source code\nand running on a physical TEE with known confidentiality and integrity guarantees. The ledger, in turn,\nenforces that decryption keys are given only to workflow binaries consistent with a device-approved access\npolicy associated with the message at upload time. The ledger does so by confirming that the workflow\nbinaries are built from approved OSS source code and running on a physical TEE, following the same pro-\ncedure as the device used to verify the ledger's integrity.\nConfidential federated computations can thereby establish an externally verifiable chain of trust, where\nmessages uploaded to the server can be decrypted only in accordance with a device-approved access policy\nconsisting of a graph of permitted transformations on the uploaded data. These properties can be checked"}, {"title": "", "content": "by anyone with access to the source code, which can include the general public when devices enforce that\nthe ledger and workloads are reproducibly built from OSS components. Devices retain complete control\nover what data processing steps can be applied to uploaded data, including requiring specific data minim-\nization or anonymization constraints prior to release of data derived from device uploads. For example, the\ndevice might require that a federated learning workflow combine intermediate aggregates with a differen-\ntially private algorithm like [49], releasing only DP model parameters to the service provider.\nIn this way, confidential federated computations promise solutions to some privacy and system chal-\nlenges discussed in Section 3. The application to cross-device federated learning described in Eichner et al.\n[24] allows cross-device confidential federated computations to remove synchronicity requirements at up-\nload and aggregation time, allowing for better scaling across clients while preserving federated learning's\nstandard approach to data minimization [11]. However, even confidential cross-device federated computa-\ntions require clients to be online to contribute to model training, retaining the well-known bias issues intro-\nduced by client heterogeneity. And it is still limited to models of ~10-20M parameters, the current practical\nbound determined by network and device constraints. Applying methods like LoRA [36] or prompt tuning\n[43] to FL [17, 20] could increase the trainable model size, but training models up to billions of parameters\nremains out of reach even when adopting parameter efficient training.\nUnlike traditional cross-device federated learning, confidential federated computations have the poten-\ntial to apply to large language models and other generative artificial intelligence models. By utilizing the\nchain of trust, resource-intensive and round-dependent per-client computation such as gradient computa-\ntion can be moved from mobile devices to TEEs as shown in Figure 1, while preserving externally verifiable\ndifferential privacy. For example, the device can verify that the server applies differentially private aggreg-\nation properly after per-client model updates in TEEs, and all workload specific transformations adhere\nto an access policy. In both the traditional and updated notion of FL, per-device information is never vis-\nible to the service provider, now enforced via encryption and TEEs rather than via on-device computation\nplacement. Access policies could also be used to provide external verifiability of other kinds of private\ntraining algorithms, including techniques that use batches of data spanning multiple clients, which facilit-\nates the integration of the latest centralized training practice. By uploading pre-processed and encrypted\ndata, communication costs are reduced compared to previous FL practice.\nConfidential federated computations offer the ability to train much larger models in more flexible ways\nbut there are significant challenges. For one, access policies should be able to enforce correct application\nof stateful DP algorithms [19, 49] in horizontally-scaled deployments (multiple worker TEEs being used\nin parallel), yet access policies should always remain sufficiently straightforward for humans to interpret\nsuch that they are convincing to external researchers wishing to validate the access policy's guarantees.\nSecond, some DP algorithms require that the orchestrator responsible for passing information between\nTEEs does not know which devices are participating in each round [55, Table 3]). Special care is required\nto identify such additional constraints and encode them into the access policy and/or OSS TEE binaries\nsuch that they can be externally verified alongside the more straightforward logical transformations. Third,\nTEE integrity bugs or side-channel leakage to the service operator have the potential to expose private data\neven if the other aspects of the system are working correctly. Finally, there is overhead associated with\nrunning logic in TEEs and potential bottlenecks associated with the ledger in our system. These are some\nof the many challenges we hope to explore with the community as federated learning and analytics evolve\nto incorporate new server-side hardware capabilities.\n5 Conclusion\nSince its introduction, federated learning has evolved significantly in its practical applicability as well as by\nincorporating complementary privacy technologies such as differential privacy. But challenges in the field\nremain. This paper outlines some scalability and system challenges, especially concerning large foundation\nmodels, as well as the need for verifiability of server-side privacy guarantees. This work proposes a new\ndefinition of FL to address the evolving landscape of technologies and applications, prioritizing privacy\nprinciples over computation placement."}]}