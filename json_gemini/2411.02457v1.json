{"title": "A Multi-Task Role-Playing Agent Capable of Imitating Character Linguistic Styles", "authors": ["Siyuan Chen", "Qingyi Si", "Chenxu Yang", "Yunzhi Liang", "Zheng Lin", "Huan Liu", "Weiping Wang"], "abstract": "The advent of large language models (LLMs) has significantly propelled the advancement of Role-Playing Agents (RPAs). However, current Role-Playing Agents predominantly focus on mimicking a character's fundamental attributes while neglecting the replication of linguistic style, and they are incapable of effectively replicating characters when performing tasks beyond multi-turn dialogues, which results in generated responses that lack authenticity. The reason current RPAs lack this capability is due to the nature of existing character datasets, which lack collections of character quotations and are limited to multi-turn dialogue tasks, constraining the RPA's performance across other task domains and failing to mimic a character's linguistic style. To address this gap, we developed a multi-task role-playing dataset named MRstyle, which encompasses a substantial number of real individuals along with their quotations and covers seven different tasks. On this basis, we develop StyleRPA, a Multi-Task Role-Playing Agent (MRPA) that significantly outperforms recent open-source LLMs and RPAs baselines on 7 tasks including Dialogue, Dictionary, Composition, Story Generation, Product Description, Music Commentary, and Open Question Answering. The code and data will be released.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (OpenAI, 2024;\nTouvron et al., 2023) are progressively revolution-\nizing the paradigm of human-computer interaction.\nTo satisfy fundamental psychological needs of hu-\nman such as love and belonging (Maslow, 1943),\nthese models are deployed as Role-Playing Agents\n(RPAs) (Li et al., 2023; Zhou et al., 2023; Wang\net al., 2024b), engaging in meaningful dialogues\nwith users by skillfully simulating a variety of char-\nacters with diverse attributes.\nThe realization of an ideal RPA presents two sig-\nnificant challenges. First, it should not only main-\ntain consistency in character information but also\naccurately mimic the character's linguistic style to\nsimulate specific characters more accurately and\nauthentically, as personalized linguistic style is crit-\nical in daily interactions and creation (Leech and\nShort, 2007) and lacking linguistic style could re-\nduce user engagement and credibility. Unfortu-\nnately, existing datasets(Chen et al., 2024; Zhou\net al., 2023; Gao et al., 2023) for character typ-\nically consist of fictional characters and contain\nonly basic information. However, These pieces of\ninformation is insufficient to accurately replicate a\ncharacter's linguistic style. Adding a person's quo-\ntations can effectively address this limitation, as\nsuch quotations frequently encapsulate their unique\nlinguistic patterns and habits. This limitation ham-\npers the model's ability to replicate how a charac-\nter would speak in real-life situations, potentially\nmaking the user experience feel less credible and\nrealistic. Second, An effective RPA should pos-\nsess the capability to execute a diverse array of\ntasks. Nevertheless, the current dataset is limited\nto dialogue-based tasks, with little application in\nother contexts such as article or comment genera-\ntion, which is inadequate for training a robust RPA\nmodel. As a result, when the model engages in var-\nious types of downstream tasks, fails to accurately\nreplicate the distinctive linguistic traits of specific\ncharacters.\nTo tackle these two challenges, we developed a\ndataset named MRstyle, which comprises a consid-\nerable number of real individuals and their quota-\ntions and covers seven different tasks. Specifically,\nto enhance the understanding of semantic styles (as\npreviously discussed), intuitively, we propose an\napproach of learning diverse linguistic styles from\nreal individuals and their quotations. Inevitably,\nsome character's quotations remain inaccessible.\nAccordingly, we designed two types of scenarios:"}, {"title": "2 Related Work", "content": "2.1 Personalized Dialogue\nPersonalized dialogue systems, which generate re-\nsponses in accordance with specific personas, have\ngarnered significant attention in recent research.\nThe focus was on maintaining consistency between\npersona attributes and the generated responses\n(Cheng et al., 2023; Chen et al., 2023). Persona-\nbased dialogue datasets (Zhang et al., 2018; Di-\nnan et al., 2019; Xu et al., 2022) are constructed\nthrough interactions between two human speakers,\neach providing approximately five sentences to de-\nscribe their persona traits, which are then reflected\nin the subsequent dialogue. Although these studies\nexplored character-related issues in dialogue, the\npersonas used are often limited to basic personal\ndetails such as name, age, and experience (Jang\net al., 2022; Gao et al., 2023). Responses generated\nbased on these personas typically only convey ba-\nsic character information and fail to reflect distinct\nlinguistic styles, resulting in replies that resemble\ngeneric answers and lack personalized characteris-\ntics.\n2.2 Role-Playing Agents\nRecent research advanced the key capabilities of\nlarge-scale language models (LLMs) within the\nfield of RPAs (Park et al., 2023; Shanahan et al.,\n2023; Wang et al., 2024a). These agents can as-\nsume various roles by effectively simulating role\ndata (Tu et al., 2024; Wang et al., 2024b), which\nis derived from prompt contexts, training sets, or\nexternal databases. Existing studies have enhanced\nRPAs' performance by setting role descriptions as\nsystem prompts (Li et al., 2023; Tu et al., 2023).\nFurthermore, specialized role training corpora can\nbe provided and fine-tuned based on open-source\nfoundational models (Shen et al., 2023). To further\nenhance the capabilities of RPAs, researchers have\nintegrated retrieval-augmented methods; for exam-\nple, Salemi et al. (2024) combined role-playing\nmodel training with retrieval information to im-\nprove agent performance in role-playing tasks. The\ncombination of these approaches demonstrates that\nhigh-quality role data is crucial for the performance\nof RPAS."}, {"title": "3 MRstyle", "content": "The process of collecting MRstyle is primarily di-\nvided into two phases: Character Data Collection\nand Task Data Construction, as detailed in Sections\n3.1 and 3.2. Character Data Collection is aimed\nat gathering character information and quotations,\nwhile Task Data Construction focuses on building\ndownstream task data.\n3.1 Character Data Collection\nThe MRstyle dataset is primarily composed of two\ncategories of characters. The first category includes\ncharacters with quotations, referred to as seed char-\nacters, from which RPAs can infer the linguistic"}, {"title": "3.2 Task Data Construction", "content": "style based on these quotations. For this latter\ngroup, we establish associations with similar seed\ncharacters to enable RPAs to deduce the linguistic\nstyle of sprout characters using the quotations from\ntheir corresponding seed characters and generate\nappropriate responses. Our character collection\nprocess is outlined below:\nCollect characters' information. First, we ex-\ntracted a substantial amount of Chinese character\ndata from Wikipedia and eliminated any tags as-\nsociated with political or commercial activities to\nensure that the dataset was free from biased or pro-\npagandistic content. We standardized the informa-\ntion and further removed entries with insufficient\ncontent to guarantee that each character possessed\nadequate information. This process ensured that all\ncharacters were supported by sufficiently rich data.\nCollect characters' quotations. Subsequently,\nwe extracted a selection of quotations from\nWikipedia\u00b9 and conducted a rigorous screening\nprocess, eliminating quotations attributed to politi-\ncally affiliated figures, those with negative social\nimpacts, as well as quotations reflecting political\nstances or personal opinions to ensure the neutrality\nand objectivity of the dataset. Acknowledging that\nthe quotations available on Wikipedia were insuf-\nficiently comprehensive and lacked certain classic\nsayings, we supplemented these figures' quotations\nby sourcing additional material from the internet.\nFor contemporary figures, we gathered their public\nspeeches and incorporated them into the dataset;\nfor historical figures, we selected notable excerpts\nfrom their classical writings to ensure that each\nfigure had no fewer than three quotations, thereby\nguaranteeing both completeness and breadth of con-\ntent.\nUltimately, we acquired 485 seed characters ac-\ncompanied by quotations, each with 3 to 10 quo-\ntations, and 947 budding characters without quo-\ntations. Each budding character was associated\nwith a corresponding similar seed character. All\nthese character information is organized in a key-\nvalue format, and further details can be found in\nthe appendix C."}, {"title": "3.3 Dataset Analysis", "content": "GPT-4 to generate instruction-response pairs tai-\nlored to different tasks. This is crucial for ensur-\ning that the model maintains a consistent character\nlinguistic style when handling various tasks. To\nenhance the model's reasoning and generalization\nabilities, we also developed specific CoT for each\ncharacter, guiding the model to imitate the charac-\nter's linguistic style rather than merely reproducing\nquotations.\nCollecting multi-task data. We selected two\nopen-source Chinese datasets, NaturalConv\u00b2 and\nYeungNLP/firefly-train-1.1M\u00b3, to obtain original\ndata for different tasks. NaturalConv is a dataset\nof daily conversations used to construct character\nresponses in daily dialogue, and YeungNLP/firefly-\ntrain-1.1M is a multi-task dataset utilized for gen-\nerating character responses across six additional\ntasks: Dictionary, StoryGeneration, Compostion,\nProductDesc, OpenQA, MusicComment. For each\ntask, we matched 3-4 pieces of data to each charac-\nter, directly reusing the instructions from the task\ndata we collected. Then, based on their original\nresponses, we constructed new responses that align\nwith the character's linguistic style.\nGenerate responses by posterior information.\nWe observed that using prompt engineering to\nmake GPT-40 directly generate responses imitating\na character results in the model simultaneously fo-\ncusing on content generation, the generated content\ntends to lack a distinctive character-specific linguis-\ntic style and appears more like a generic response.\nTo mitigate this issue, we introduce a posterior in-\nformation data construction method. Initially, we\nprovide the model with a reference response for\noriginal task and make it to contemplate how to\nproduce a similar reply using the imitated charac-\nter's linguistic style-termed posterior information\n(For example, the xx in a character's quote can be\neffectively integrated with the xx in the story). Fol-\nlowing this, we instructed GPT-40 to emulate the\ncharacter based on their quotes alongside the poste-\nrior information to generate responses. We sampled\nand verified the generated results, and found that\nthe response generation effect based on posterior\ninformation was better than directly imitating the\ncharacter's quotations to generate responses. More\ndetailed information is provided in the appendix E.\nThe MRstyle framework offers the following advan-\ntages: (1) A substantial number of real characters\nwith authentic quotations. It includes 485 charac-\nters with authentic quotations and 947 characters\nwithout quotations. Comprehensive information\nhas been meticulously compiled for each character,\nensuring that this information is publicly available\nand transparent, without involving personal opin-\nions, biases, or commercial actions. For the first\ntime, we systematically organized characters' quo-\ntations and established connections between similar\ncharacters. (2) Multiple task inclusion. The frame-\nwork encompasses seven distinct tasks, amounting\nto a total of 31,117 data entries. Each character has\nresponses under each task, and we ensure that their\nresponses align with the linguistic style of the char-\nacter. (3) The CoT for inferring linguistic style from\ncharacter quotations. Personalized Prompts and\nCoT were designed for each character under every\ntask, guiding the model to infer the character's lin-\nguistic style, thus generating responses that more\nclosely match the character's manner of speaking.\nFurther details can be found in the appendix A.\nOur train-test split strategy focuses on two di-\nmensions: (1) Instruction-based splitting, used to\nassess the model's generalization capability on new\ninstructions. For this, we selected 3,000 test sam-\nples across seven tasks; (2) Character-based split-\nting, used to evaluate the model's ability to general-"}, {"title": "4 StyleRPA", "content": "ize to new characters, which were not encountered\nby the model during the training. We selected 10\nnew characters for testing, and the model had not\nencountered these characters during training.\nAfter constructing the MRstyle dataset, we en-\ndeavor to fully leverage this data to promote\nmodel's ability to imitate the linguistic style of the\ngiven characters. Inspired by the multi-task train-\ning paradigm adopted by previous RPAs (Du et al.,\n2024; Lu et al., 2024), we devise a two-scenario\ninstruction fine-tuning framework based on poten-\ntial real-world user inputs and choose Qwen2-7B-\nBase as the backbone to develop a multi-tasks RPA\nnamed StyleRPA. Our model can adapt to character\ndata with varying amounts of information and is\nable to better reason and mimic the given charac-\nter's linguistic style.\n4.1 MRstyle-instruct for Scenarios\nTo address the various characteristics that need to\nbe simulated in RPAs, which can lead to significant\ndifferences in user input, we have simulated the\nfollowing scenarios to enhance the model's gener-\nalization capability as much as possible.\nSeed Characters. In real-world scenarios, quota-\ntions from certain individuals, such as well-known\nactors or writers, are relatively easy to obtain.\nThese individuals are referred to as \"seed charac-\nters\" in our MRstyle dataset. We designed two\ntypes of input formats for instruction fine-tuning\ndata for these characters: 1) Providing both char-\nacter information and quotations. In this case, the\nmodel infers the character's linguistic style based\non the given quotations. 2) Providing only the in-\nformation of the widely acknowledged character.\nThe model weights contain quotes from these char-\nacters, We expect the model to imitate the target\ncharacter's linguistic style solely based on their\nidentity, by the linguistic style into the model's\nweights. Correspondingly, we also designed two\noutput formats: 1) Using CoT reasoning, where the\nmodel infers the linguistic style from the quotations\nand generates a response, applicable to Input 1. 2)\nDirectly generating responses that imitate the char-\nacter, applicable to both Input 1 and Input 2. Our\ninstruction fine-tuning data covers all combinations\nof inputs and outputs, with the ratio of Input 1 to\nInput 2 being 4:1. Detailed data is presented in the\nappendix C."}, {"title": "4.2 Model Training", "content": "Budding Characters. Considering that the quo-\ntations of some characters are difficult to obtain,\nwe are often only able to collect their information.\nThese characters are referred to as \"budding char-\nacters\" in our MRstyle dataset. Due to the lack of\nquotation data, we need to retrieve similar charac-\nters and infer the linguistic style of the budding\ncharacters based on their quotations. For such\ncharacters, we have designed three input forms\nto handle potential scenarios: 1) Providing both\nthe target character's information and quotations\nfrom similar characters. In this case, the model\ninfers the linguistic style of the target character\nfrom the quotations of similar characters and gen-\nerates appropriate responses. 2) Providing only\nthe target character's information without any ref-\nerence characters. In this scenario, we associate\nthe target character with similar seed characters\nin MRstyle. Since seed characters have available\nquotations, the model can infer the linguistic style\nof the target character and generate responses. 3)\nProviding well-known characters without any ad-\nditional background information. We expect the\nmodel to internalize the character's linguistic style\ndirectly into its weights and generate responses\nconsistent with that character's style, without rely-\ning on external knowledge. Correspondingly, we\nhave also designed two output forms: 1) Using CoT\nreasoning, where the model deduces the target char-\nacter's linguistic style based on similar characters'\nquotations, applicable to Input 1 and Input 2. 2)\nDirectly generating responses in the style of the\ntarget character, applicable to Input 1, Input 2, and\nInput 3. Our instruction fine-tuning dataset covers\nall input-output combinations, with the ratios of\nInput 1, Input 2, and Input 3 being 2:2:1.\nWe organized the characters in the MRstyle dataset\naccording to the two aforementioned scenarios,\nconstructing the Pstyle-instruct to align with our\ninput-output settings. We mixed Pstyle-instruct\nwith seven general task datasets, for model fine-\ntuning. Although there are already numerous\ninstruction-tuning datasets for role-playing, these\ndatasets fail to sufficiently model character-specific\nlinguistic styles, limiting their ability to accurately\nimitate particular personas. Pstyle-instruct aims to\nfill this gap, enabling the model to emulate differ-\nent characters' linguistic styles across various sce-\nnarios. During training, we also introduced some\ngeneral fine-tuning data to enhance the model's"}, {"title": "5 Experiments", "content": "generalization capabilities, maintaining a ratio of\n6:5 between general and ours data.\n5.1 Experimental Setup\nBaseline. We consider baselines of three\ngenres:(1) Open-source Large Language Mod-\nels(LLMs) including ChatGLM3-6B (GLM, 2024),\nYi-6B-Chat (AI et al., 2024). (2) Open-source\nCharacter Large Language Model(CLLMs)\nincluding CharacterGLM-6B (Zhou et al., 2023),\nChatPLUG (Tian et al., 2023). (3) GPT-40 with\nrole mimicry capability(Wang et al., 2024b). More\ndetailed information is provided in the appendix F.\nEvaluation metrics. The similarity of linguis-\ntic style, such as speaking rhythm, tone, and into-\nnation, is challenging to quantify using the over-\nlap between predicted and true values. Therefore,\nwe did not employ automated metrics to evaluate\nour model. To more accurately assess the quality\nof the generated content's linguistic style, we re-\nferred to the method used in AlpacaEval (Dubois\net al., 2024). We utilized GPT as the evaluator,\nas its reliability has been demonstrated (Fu et al.,\n2023; Gilardi et al., 2023). Considering the cost\nof GPT-4, we opted for a lighter model, GPT-40,\nwith comparable performance for evaluation. Our\nevaluator's prompts were slightly modified from\nthe AlpacaEval prompts, focusing on the similarity\nof the character's linguistic style and the accuracy\nof the generated responses. During evaluation, the\nsimilarity between the model-generated responses\nand the actual linguistic style of these characters'\nquotations will be compared, as detailed in the ap-\npendix F. These prompts facilitate sample compar-\nison and ranking, leading to a winning rate evalua-\ntion. Specifically, the performance of each model\nwas compared against GPT-40, which is capable\nof mimicking specific characters' linguistic styles,\nand the winning rate of each model was recorded.\nThe winning rate of GPT-4o was obtained by com-\nparing it with StyleRPA. Such a comparison pro-\nvides a clearer reflection of the performance differ-\nences across the seven tasks."}, {"title": "5.2 Experiment and Analysis", "content": "Performance of new instructions. Table 2\npresents the GPT evaluation performance of the\nmodels' ability to mimic a character's linguistic\nstyle across seven tasks. These characters had ap-\npeared during the model's training phase, but the"}, {"title": "5.3 Ablation Study", "content": "data for each task were entirely new. We found\nthat StyleRPA outperformed all baseline models\non six tasks. Although it was slightly inferior to\nGPT-40 in the multi-turn dialogue task due to lim-\nitations in model size and corpus volume, it still\nsurpassed the existing baseline models. In con-\ntrast, previous open-source large language models\nand role-playing models showed inadequate perfor-\nmance in mimicking specific characters' linguistic\nstyles.\nPerformance of unseen characters.\nTable 3\npresents the GPT evaluation scores for mimicking\nthe linguistic style of characters on seven tasks,\nwhere the model does not encounter these charac-\nters or their quotations during training. Given that it\nis difficult to verify whether any open-source large\nmodels or role-playing models have seen these char-\nacters during training, we only compared our model\nwith GPT-40 in this section. Even characters not\nin the training data, StyleRPA outperformed GPT-\n40 in three tasks, and on other tasks, our model's\nperformance was generally close to that of GPT-40.\nConsidering that GPT-40 has access to these charac-\nters' quotations during training, while this informa-\ntion is entirely new to our model, this demonstrates\nthat the inclusion of character quotations and CoT\nreasoning significantly enhances the model's ability\nto mimic the linguistic style of previously unseen\ncharacters.\nPerformance of seed character and budding\ncharacter. We utilized GPT to evaluate the\nmodel's performance in mimicking seed charac-\nters and budding characters. The linguistic style of\nthis group of characters is inferred from the quo-\ntations of related characters. As shown in Table\n2, For the seed character, StyleRPA outperformed\nGPT-40 on six tasks. For the budding character,\nStyleRPA outperformed GPT-40 on only 4 tasks.\nIn conclusion, StyleRPA performs better in imitat-\ning seed characters, and it is more challenging to\nimitate budding characters, with a slight decrease,\nbut it is generally better than GPT-40. The latter in-\ndicates that even in the absence of quotations from\nthe target character, our model can still accurately\ninfer and imitate the character's linguistic style.\nConsidering the cost issue of GPT's API, we ran-\ndomly selected 200 samples from the test set for\nevaluation during the ablation experiments. These\n200 samples covered all 7 tasks. After averaging\nthe evaluation results, we compared them with the\nresults of StyleRPA to reflect the overall impor-\ntance of each module. We denote s1 as the perfor-\nmance of seed characters, s2 as the performance\nof unseen characters, and s3 as the performance of\nbudding characters.\nEffect of CoT. Table 4 illustrates the perfor-\nmance of StyleRPA when trained without the use\nof Chain-of-Thought (CoT) reasoning. In the ab-\nsence of CoT, the model's performance deterio-\nrated across all three scenarios, with particularly\npronounced declines observed in the second and\nthird scenarios. This decline can be attributed to\nthe fact that these scenarios necessitate the model's\nability to infer character linguistic styles based on\nnew quotations and similar characters. Omitting\nCoT compromises its reasoning capabilities, ulti-\nmately impacting the quality of generated outputs.\nEffect of quotations. The incorporation of char-\nacter quotations markedly improves the ability of\nRPAs to emulate the linguistic styles of characters,\nas shown in Table 5. When there is no character\nquotations as a reference, the model can only rely\non the knowledge acquired during training to rea-\nson and match the linguistic style of the given char-\nacter during the generation process. This makes it\ndifficult for the model to fully mimic the specific\ncharacteristics of a character. From the results, it"}, {"title": "6 Conclusion", "content": "can be seen that character quote data is very impor-\ntant for improving the imitation ability of RPAS.\nEffect of Similar Characters. When given\ncharacters do not have a corpus of quotations, pro-\nviding the model with quotations from similar char-\nacters can also enable the model to infer the char-\nacter's linguistic style and generate responses, as\nshown in 6. In the absence of similar characters,\nthe model is limited to deducing the character's\nlinguistic style solely based on its information, im-\npairing its capacity to replicate the given character.\nThe experimental results prove that it is important\nto provide similar characters with quotations when\ncharacters do not have a corpus of quotations.\nThe paper represents a pioneering effort to investi-\ngate the imitation of character linguistic styles by\nlarge models across various tasks. We introduce\na multi-task role-playing dataset, MRstyle, which\nencompasses a substantial collection of real-life\ncharacters along with their quotations, spanning\nseven distinct tasks. Furthermore, We developed\na fine-tuning dataset for MRstyle-instruct through\nthe integration of diverse scenarios. On the basis of\nMRstyle-instruct, we empower Qwen2 to become\na multi-task Role-Play Agent StyleRPA, which\nproficient in emulating diverse character linguis-\ntic styles. Experimental results show that StyleRPA\nsignificantly outperforms existing RPAs on mul-\ntiple tasks, and is even on par with the powerful\nGPT-40. In conclusion, the contributions of this\npaper lie at promoting the research on Role-Play\nAgent understanding from the task, dataset and\nmodel perspectives.\nLimitations\nAlthough this work provides a comprehensive ex-\nploration of Role-Playing Agent Capable of Imi-\ntating Character Linguistic Styles on multi-tasks\nfor the first time, there are still some limitations\nthat can be left for further research. Firstly, our\ndataset only contains Chinese data, lacking data in\nsome other languages. Secondly, our model has\nnot exceeded GPT-40 in the multi-turn dialogue\nscenario, and we hope to narrow this gap in the fu-\nture. Finally, our model's ability to imitate unseen\ncharacters needs to be improved. With the emer-\ngence of more powerful LLMs, we look forward\nto developing more realistic and powerful RPAs in\nthe future.\nEthical Considerations\nThe proposed MRstyle dataset is constructed based\non the academic datasets like NaturalConv and\nYeungNLP/firefly-train-1.1M. Furthermore, all of\nour character information and quotes are sourced\nfrom Wikipedia and Wikiquote, which are free and\nopen datasets for research use with licenses like\nMIT License1 or CC-BY-SA4.0 License 2. We\neliminate any characters associated with political\nor commercial activities to ensure that the dataset\nwas free from biased or propagandistic content and\nquotations attributed to politically affiliated figures,\nthose with negative social impacts, as well as quo-\ntations reflecting political stances or personal opin-\nions to ensure the neutrality and objectivity of the\ndataset. The resulting dataset MRstyle is also a free\nand open resource for the community to study the\nRPA. Thus, the authors foresee no ethical concerns"}]}