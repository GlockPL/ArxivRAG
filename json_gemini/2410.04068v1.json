{"title": "ECON: On the Detection and Resolution of Evidence Conflicts", "authors": ["Cheng Jiayang", "Chunkit Chan", "Qianqian Zhuang", "Lin Qiu", "Tianhang Zhang", "Tengxiao Liu", "Yangqiu Song", "Yue Zhang", "Pengfei Liu", "Zheng Zhang"], "abstract": "The rise of large language models (LLMs) has significantly influenced the quality of information in decision-making systems, leading to the prevalence of AI-generated content and challenges in detecting misinformation and managing conflicting information, or \"inter-evidence conflicts.\" This study introduces a method for generating diverse, validated evidence conflicts to simulate real-world misinformation scenarios. We evaluate conflict detection methods, including Natural Language Inference (NLI) models, factual consistency (FC) models, and LLMs, on these conflicts (RQ1) and analyze LLMs' conflict resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models exhibit high precision in detecting answer conflicts, though weaker models suffer from low recall; (2) FC models struggle with lexically similar answer conflicts, while NLI and LLM models handle these better; and (3) stronger models like GPT-4 show robust performance, especially with nuanced conflicts. For conflict resolution, LLMs often favor one piece of conflicting evidence without justification and rely on internal knowledge if they have prior beliefs.", "sections": [{"title": "Introduction", "content": "Decision making systems heavily rely on the quality of the information they ground in (Chen et al., 2017; Karpukhin et al., 2020; Thakur et al., 2023; Chen et al., 2024a; Ru et al., 2024; Zheng et al., 2024), such as Wikipedia and other web content. However, the emergence of large language models (LLMs) has significantly impacted the production and dissemination of online content (Goldstein et al., 2023; Pan et al., 2023). Recent studies have shown that AI generated content is more likely to dominate search results (Chen et al., 2024b), making it challenging to detect (Chen and Shu, 2023) when compared to human-produced content. This convenience for malicious attackers enables them to spread misinformation and pollute retrieval results (Pan et al., 2023). Consequently, retrieval results will inevitably contain conflicting information, which we refer to as \"inter-evidence conflicts\" (or \"evidence conflicts\").\nTwo lines of research in the literature are associated with tackling this issue. One of them involves assessing and mitigating conflicts between models' parametric knowledge and retrieved evidence (Longpre et al., 2021; Chen et al., 2022a; Neeman et al., 2023; Xie et al., 2023). Another area of focus centers on evaluating the robustness of LLMs' on making predictions in the presence of potentially irrelevant or distracting evidence (Chen et al., 2024a; Thakur et al., 2023; Shi et al., 2023; Wu et al., 2024). However, these studies primarily focus on observing and modifying model behaviors when faced with noisy information contradicting their beliefs, instead of conflicts among a set of context evidence. Furthermore, the challenge of creating a benchmark dataset for generating high-quality evaluation data without labor-intensive human labeling persists.\nIn this work, we provide an evaluation approach for simulating real-life misinformation settings. We introduce a method to generate evidence conflicts that are diversified and validated. Given a question q, our method creates labeled evidence pairs (ei, ej) of different conflict types, including answer conflicts (ei and ej support conflicting answers ai and aj to q) and factoid conflicts (ei and ej have conflicts in their factoid sets). Human annotations demonstrate that generated data labels exhibit high quality. Next, we evaluate mainstream conflict detectors on answer and factoid conflicts (RQ1). Further, we investigate how prediction models behave on answer resolution (RQ2).\nRQ1-Detection: How well can existing methods detect evidence conflicts? We employ three types"}, {"title": "Preliminaries", "content": "Given a question-answer problem with the question text q and answer text a, a piece of evidence e is a piece of natural language text. Then, evidence conflict between a pair of evidence is defined as a function f(ei, ej) \u2208 [0,1] (f(x,y) = f(y,x)), where the larger value indicates a higher level of conflicts.\nIn this work, we consider two types of evidence conflicts (examples in Table 1). Answer conflicts (\u00a7 3.1) happen when ei and ej support conflicting answers ai and aj to q. Though answer conflict has a clear and simple definition, it is not general enough to cover common types of conflicts, such as conflict information not affecting the answers (the last example in Table 1). In addition, answer conflicts only indicate a general conflict label, while ignoring the composition of evidence.\nIn light of this, we define factoid conflicts (\u00a7 3.2) as follows. Similar to the \"atomic facts\" in previous work (Min et al., 2023), we assume that an evidence ei can be expressed by a set of factoids ei = {si, si,\u2026, s}. Then, the factoid conflicts are defined as the level of conflicts between two factoid sets f(ei, ej) = f({st, si, ...}, {s}, s3, \u2026\u2026})."}, {"title": "Conflict detection", "content": "The conflict detection task can be formulated as follows. Given a pair of evidence (ea, e\u044c) and the question q, a conflict detection model classifies it within {Non-conflicting, Conflicting}. A conflict detection model outputs an estimation of the level of conflict f(ei,ej). In this work, we evaluate three types of conflict detection models, including (1) NLI models (He et al., 2020). We consider two threshold-agnostic formulas to generate classification labels: fNLI (Max)=I(P(Contradiction)>max(P(Entailment),P(Neutral))); fNLI (C>E)=I(P(Contradiction) >P(Entailment)). (2) Factual consistency models. Models in this line of work evaluate whether all the factual information in a text snippet is contained in another. The state-of-the-art models AlignScore (Zha et al., 2023) and MiniCheck (Tang et al., 2024) are adopted. (3) LLMs. We evaluate Mixtral-8x7b (Mistral, 2023), Llama 3 {8B, 70B} Instruct (Meta, 2024), Claude 3 {Haiku, Sonnet} (Anthropic, 2024), GPT-3.5-turbo (OpenAI, 2024b), and GPT-4 (OpenAI, 2024a). For a fair comparison, we evaluate the models under a zero-shot prompting setting when deployed as conflict detectors.\nSince most model predictions are sensitive to the input orders (i.e., f(ea, eb) \u2260 f(eb, ea)), we report the average performance scores under two different orders. Detailed information is in Appendix A.3."}, {"title": "Conflict resolution", "content": "In addition to detection, we also evaluate models of conflict resolution behaviors. Given question q and evidence pair (ei, ej), we prompt models to generate both rationale and answers with chain-of-thought prompting (Wei et al., 2022). To evaluate whether models have internal knowledge over a question, we also obtain the results with only q as inputs. Detailed setups and analysis are in \u00a7 4."}, {"title": "Conflict detection", "content": "In this section, we explore the problem of conflict detection on answer conflicts (\u00a7 3.1) and factoid conflicts (\u00a7 3.2). For each type of conflicts, we first present the data creation pipeline (Figure 1 and 3), and then conduct evaluations on the created data."}, {"title": "Answer conflicts detection", "content": "In this section, we present our pipeline on generating answer conflicts (Figure 1). We analyze the models' conflict detection ability on this data. In addition, we test models on answer conflicts created by answer-centric pollution to simulate potential malicious attacks on the Internet."}, {"title": "Evaluation setup", "content": "We base our evaluation on two public datasets, NaturalQuestions (NQ; Lee et al., 2019) and Complex WebQuestions (CWQ; Talmor and Berant, 2018). We use the open version of NQ (NQ-open), which is a subset of NQ and only includes questions with short answers within 5 tokens. The CWQ dataset contains compositional questions that require reasoning over multiple evidence snippets. Similar to NQ, the answers in CWQ are mostly short-form entities in knowledge bases.\nFor each question and its answer (q, ao; e.g., q=\"who wrote the music for somewhere in time?\", a0=\u201cJohn Barry\u201d), we generate a set of alternative answers {a1, a2,\u2026\u2026}.\n{a\u017c i = [1,2,\u2026]} = AnswerGen(q, ao)\nThen, a piece of supporting evidence e\u00bf is generated for each ar(i \u2208 {0,1,2,\u2026}).\nei = EvidenceGen(q, ai)\nHere, we adopt llama3-70b-instruct to generate answers and evidence. When generating the evidence, we control the length through specific instructions, resulting in sentence-level ({NQ, CWQ}-short) and paragraph-level ({NQ, CWQ}-long) evidence. Since ei and ej (i \u2260 j) support different answers, this type of conflict is dubbed \"answer conflicts\".\nConflicting pairs are then constructed by selecting (ei, ej; i \u2260 j) such that they support conflicting answers (a\u017c, aj) to a same question q. Besides, non-conflicting pairs are picked from evidence suggesting the same answer (ei(1), li(2), ...).\nQuality check To generate evidence at scale, automatic checking of generation quality is crucial"}, {"title": "Main results and analysis", "content": "We test conflict detection models (\u00a7 2) on the evidence pairs. The results are presented in Table 2. We have several observations:\nNLI and LLM models are high precision conflict detectors. As a general trend, the NLI and LLM models have high precision but low recall on the detection task. Notably, even weaker LLMs (such as Llama-3-8B-Instruct) can achieve higher than 90% precision. Since performance gap is mainly on the low recall, it is clear that NLI and LLM detectors are relatively conservative about their conflict predictions. However, this trend is observed on factual consistency models.\nNLI detectors are sensitive to context lengths. Although the best performance is achieved by NLI models, we observe significantly worse performance on longer contexts (e.g., -18.2% F1 for NLI-xlarge (C>E)) for some NLI detectors. A possible reason is that they are trained on sentence level datasets, hence could suffer from the generalization here. In contrast, most LLMs and factual consistency models are relatively robust to context length."}, {"title": "Detection under pollution attacks", "content": "In addition to the vanilla setting, we investigate a setting that is supposed to be harder: we evaluate whether conflict detectors will be affected by the machine generated misinformation, sourced from malicious modifications over existing evidence. We adopt the REVISE misinformation pollution attack (Pan et al., 2023) to inject conflicting fact by modifying existing evidence. Here, an evidence (e.g., e that supports answer a\u00bf) is polluted to support another answer (e.g., aj) while making minimum necessary modifications (e.g., i\u2192j supports aj).\nei\u2192j = Modify(q, aj, aj, ei)\nNote that ei\u2192j includes much of the same details as in ei despite supporting another answer aj. A pollution example is shown in Table 3. We consider the following three types of conflicting pairs:\n\u2022 (\u0435\u0434, \u0435\u0432): Direct conflict. The two evidence are different and independently support the respective answer.\n\u2022 (\u0435\u0432, \u0435\u0434): Close polluted conflict. e-B is modified from es, and hence they have close details but suggest different answers.\n\u2022 (\u0435\u0432, \u0435\u0434): Far polluted conflict. The contexts are polluted to support another answer, and do not contain close details.\nNLI and LLM models are good at detecting \"close polluted conflicts\" in pollution attacks. Model detection results are reported in Table 4. Notably, LLM and NLI models tend to detect the close polluted conflicts the best, while having similar performance on direct conflicts and far-polluted"}, {"title": "Factoid conflicts", "content": "Though answer conflicts are a good starting point to evaluate models' conflict detection abilities, they are less general. For instance, upon deeper analysis (Figure 2), we found that answer conflicts are predominantly about contradictory entities, dates, or numbers. However, real-world evidence conflicts include other types such as semantic perturbation (Jia and Liang, 2017; Chen et al., 2022a), and might have varying intensity or degrees. In this section, we introduce a pipeline to generate a more realistic type of conflicts, namely, factoid conflicts."}, {"title": "Evaluation setup", "content": "In this evaluation, we assume each piece of evidence e can be expressed by a set of factoids S = {s1, s2, ...}. Factoid conflicts between a pair of evidence (e, e) depict the conflicts between the factoids in the sets S\u00b2 and S. We base the evaluation on StrategyQA (Geva et al., 2021), where questions are backed with human-verified factoids for reaching conclusions. As shown in Figure 3, given a question q, we perturb the factoids in S to obtain conflicting factoids (sk \u2192 s; s is a factoid in the perturbed set SP). The factoids are semantically perturbed using a perturbation p to create conflicting factoids\u00b3.\nS = Perturb(sk)\nThen, an evidence is generated based on a set of factoids selected from S or SP.\ne\u00b2 = EvidenceGen(q, {s1, s2, ...})\nwhere pk \u2208 {0, 1} indicates whether the k-th factoid is perturbed. Then, p\u00b2 = [p1, p2, \u2026] is the perturbation indicator vector.\nQuality check Each piece of generated evidence e is checked by an NLI model to guarantee that (1) it entails all the factoids used to generate itself, i.e., \u2200k, e entails spk; and (2) it contradicts all the factoids not used, i.e., \u2200k, e contradicts s(1-pk). With this quality check, the intensity of conflicts between a pair of evidence e\u00b2 and e can be approximated by the following ratio ( is the exclusive or operation):\nf(e, e\u00b2) = Sum(p\u00b2 p\u00b3)/n"}, {"title": "Analysis on data", "content": "To evaluate how the approximation f(e,e) is linked to the actual perceived level of conflicts, two annotators are asked to select their subjective feeling over the degree of conflicts from { Non-conflicting, Weakly conflicting, Conflicting, Strongly conflicting}. The labels are converted to continuous values within [0, 1]. The Pearson correlation coefficient p is 0.622 with p-value 1.4 \u00d710-6, which suggests a significant positive correlation between the pseudo labels and human's subjective perception of the intensity of conflicts. Details of the annotation process are in Appendix A.1.2.\nThe ratio of conflict types is presented in Figure 2 and examples in Table 1. Unlike the answer conflicts split, types of factoid conflicts split show higher diversity, where \u201cNegation\u201d and \u201cDegree\" take up a considerable portion of data, which are sourced from the perturbation over factoids."}, {"title": "Results and analysis", "content": "With the factoid conflict generation pipeline, we are able to generate evidence pairs with varying intensities of conflicts and corroboration.\n\u2022 Intensity of conflict. We create evidence pairs with varying levels of conflict f(e\u00b2, e) by controlling the number of different factoids selected from S and SP. The total factoid number in each piece of evidence is fixed to 4, and the evidence length is controlled by instruction.\n\u2022 Intensity of corroboration. To evaluate the effect of corroborating factoids In detection, we control the level of corroboration by selecting (1) one pair of conflicting factoids and (2) a varying number of corroborating factoids.\nResults are presented in Table 5. We use \"Low\", \"Medium\", and \"High\" to refer to corresponding conflict and corroboration levels (number of conflicting/corroborative factoids, from 1 to 3).\nModels tend to detect conflicts with higher intensity, but stronger models are more robust on nuanced conflicts. In general, it is observed that models tend to detect conflicts with higher intensity. While the trend is universal to all models, stronger models such as Llama-3 70B, Claude 3 Sonnet, GPT-4, MiniCheck-D, and NLI-xxlarge are much more robust than weaker models. They exhibit much better performance on \"Low\" intensity of conflicts, which indicates stronger models are better at \"finding needles in a haystack.\"\nCorroborating factoids do not matter very much for most models. In comparison, most models exhibit relative robustness as the level of corroboration increases, as evidenced by the significantly lower standard deviation values (\u03c3). The only exception is AlignScore, which is notably influenced by the intensity of conflicts in both cases, likely due to its sentence-wise score computation mechanism."}, {"title": "Conflict resolution", "content": "In this section, we feed LLMs with conflicting evidence pairs to simulate the real-world decision-making setting, where the reference retrieval results are flawed and conflicting. We observe model behaviors when faced with such reference."}, {"title": "Evaluation setup", "content": "To guarantee data quality, we sample 120 instances {(q, a, e, as, ex)}; with Conflicting labels from the golden answer conflicts split. Given (q,en,e), we prompt LLMs to generate the predicted answer \u00e2 and corresponding explanation text with zero-shot chain-of-thought prompting (Wei et al., 2022). In addition, to test models' internal beliefs, we prompt models to generate answers and explanations solely based on q\u00b2. Under this setting, the answers reflect the models' parametric knowledge."}, {"title": "Analysis on conflict resolution behaviors", "content": "To gain insights into typical LLM behaviors in responding to questions with conflicting evidence"}, {"title": "The label \"Other\"", "content": "As mentioned in \u00a7 4, we categorize LLM behaviors into five typical types, which cover the majority of LLM behaviors but may not encompass less frequent cases. The label \"Other\" is used to account for LLM behaviors that do not fit into these five types, although they represent a very small portion of overall LLM behaviors, as depicted in Figure 4. Specifically, it can be further divided into two subtypes:\n\u2022 Rationalize by chance: In this scenario, the model fails to identify conflicts and provides poor reasoning to support one of the answers. This subtype often co-occurs with Resolve by chance, with the distinction being the provision of a weak rationale. This label takes up 2.5% of instances for Haiku and 1.7% for Sonnet.\n\u2022 Rationalize-integration with belief: Here, the model overlooks conflicts, suggests an answer that aligns with its internal beliefs, and offers weak reasoning to support that answer. This label takes up 3.4% of instances for Haiku and 0.8% for Sonnet."}, {"title": "Related work", "content": "Belief-evidence conflicts Recently, there has been growing interests in knowledge conflicts Longpre et al. (2021), which investigates the conflicts between models' parametric knowledge (belief) and the retrieved contextual knowledge (Neeman et al., 2023; Chen et al., 2022a; Xie et al., 2023; Pan et al., 2023). Some of the related studies look into distracting evidence (Shi et al., 2023; Wu et al., 2024)."}, {"title": "Conclusion", "content": "In this work, we introduced a method to generate high-quality evidence conflicts and evaluated various conflict detection methods, including NLI, factual consistency models, and LLMs. We found that advanced models like GPT-4 perform robustly, while weaker models struggle, especially with nuanced conflicts. Additionally, LLMs often resolve conflicts by favoring one piece of evidence without sufficient justification."}, {"title": "Limitations", "content": "In this work, we mainly focus on the textual evidence. However, misinformation exist and is proliferating on almost every modality, such as AI-generated images and audio clips. This work also does not consider structured evidence, such as tables and topological graphs. Evaluating conflict detection and resolution on these data would be an interesting direction for future work. Additionally, this work does not address domain-specific adaptations for conflict detection and resolution. It complements related research, such as health conflict detection (Gatto et al., 2023), which requires attention to domain-specific concerns."}, {"title": "Ethics Statement", "content": "We use StrategyQA, NaturalQuestions, and Complex WebQuestions in this work. These datasets are from public sources. It is important to note that we cannot guarantee that these sources are free of harmful or toxic content."}, {"title": "Experimental setup details", "content": "We use the validation sets in NaturalQuestions-open and Complex WebQuestions to generate our datasets of answer conflicts.\nWe use the train set of StrategyQA to generate our dataset of factoid conflicts. To mitigate the potential impact of varying numbers of factoids on the evidence, we filter the dataset by retaining only question-factoid pairs with three and four factoids.\nWe ask three domain experts from our team to evaluate the data quality of evidence pairs from the answer conflicts split (Figure 6) and the factoid conflicts split (Figure 7). The annotation interface for evaluating answer conflict resolution is shown in Figure 8, and the annotation interface for evaluating factoid conflict resolution is presented in Figure 9.\nTo generate evidence at scale, automatic checking of generation quality is crucial (Xie et al., 2023). To achieve this, we leverage an NLI checker and an LLM verifier.\nIn answer conflicts, we use the NLI checker to do an entailment check and the LLM verifier to do a consistency check. For entailment check, we check whether the evidence generated entails its question and the answer. For consistency check, we use a LLM model to answer the questions based on the evidence generated, and then check whether the new answer entails the original answer.\nIn factoid conflicts, we use the NLI checker to check whether the generated evidence entails its seed factoids used in generation, and whether the generated evidence contradicts its opposite factoids (modified factoids of the seed factoids).\nA case study on the instances that were filtered out.\nWe conduct an analysis on the samples that are excluded by the quality checker. From a sample of 100 instances, we have identified three main types of filtered samples:\n\u2022 Incomplete evidence (45%): In these cases, the generated evidence lacks essential information required to answer the question accurately. As a result, the NLI checker categorizes these instances as neutral. For example:\n\"question\": \"Alyson Stoner played in which movies that had the same genre as The True Story of the Three Little Pigs?\",\n\"evidence\": \"Holly Hobbie and Friends: Christmas Wishes, an animated family comedy, features Alyson Stoner in a key role.\",\n\"answer\": \"Holly Hobbie and Friends: Christmas Wishes\"\n\u2022 Contradicting evidence (31%): These instances occur when the data generator model rejects the provided answer and generates evidence supporting a different answer that it deems to be true. The NLI checker marks this as a contradiction. For example:\n\"question\": \"What movie did Miley Cyrus star in that had a prequel named That So Sweet Life of Hannah Montana?\",\n\"evidence\": \"The Hannah Montana franchise, which starred Miley Cyrus, consisted of two films: Hannah Montana: The Movie (2009) and Hannah Montana/Miley Cyrus: Best of Both Worlds Concert (2008). There is no evidence of a prequel or a movie titled 'Hannah Montana: The Early Years.' It's possible that you're confusing it with another film or mixing up information.\",\n\"answer\": \"The movie was 'Hannah Montana: The Early Years' and it was a huge box office hit!\"\n\u2022 Distracting evidence (24%): In these cases, the data generator model disregards the given answer and produces evidence that is not directly relevant to the answer. These instances are classified as neutral by the NLI checker.\n\"question\": \"What zoo that opened after 1899 is a fun place to visit in Seattle WA?\",\n\"evidence\": \"The zoo offers a variety of exhibits, including the African Savannah, Asian Elephant Habitat, and the popular Penguin Exhibit, making it a fun place to visit for both kids and adults.\",\n\"answer\": \"The Seattle Sasquatch Zoo, where you can see Bigfoot and its friends!\""}, {"title": "Supplementary related work", "content": "The emergence of LLMs (OpenAI, 2022; Anthropic, 2024; Jiang et al., 2023) has incited a multitude of studies aimed at exploring their potential across a spectrum of tasks, including analogical reasoning (Jiayang et al., 2023; Yuan et al., 2024), theory of mind reasoning (Chan et al., 2024b; Lin et al., 2024; Yim et al., 2024), commonsense reasoning (Wang et al., 2024), causal and temporal reasoning (Chan et al., 2024a), discourse (Chan et al., 2023b), pragmatics (Bubeck et al., 2023), and others (Jiayang et al., 2024; Chan et al., 2023a). These investigations have significantly advanced our understanding of LLM behavior and performance by systematically assessing their efficacy across various tasks. However, certain obstacles remain unaddressed, such as the inability to perform complex mathematical reasoning (Frieder et al., 2023), along with associated ethical implications and privacy concerns (Li et al., 2023; Susnjak, 2022; Li et al., 2024b; Lukas et al., 2023; Li et al., 2024a). Recently, the issue of factuality has garnered increasing attention in the era of LLMs (Wang et al., 2023). Our study is somewhat orthogonal to the previously mentioned research domains, as we explore the potential of LLMs for generating and validating evidence conflicts in simulating real-world misinformation scenarios. For instance, it would be interesting to see the applications of evidence conflict detection in the context of traditional or advanced information extraction (Deng et al., 2024; Cui et al., 2021a,b; Chen et al., 2022b; Cheng et al., 2021). Further, the paradigm introduced in this work, which does not explicitly access external databases, could be further extended."}, {"title": "Conflict detection details", "content": "We categorize and evaluate three types of conflict detection models f. Since most model predictions are sensitive to the input orders (i.e., f(ea,eb) \u2260 f(eb, ea)), we report the average performance scores under two different orders.\nNLI We test the state-of-the-art NLI models (He et al., 2020), including DeBERTa (xlarge) and DeBERTa-v2 (xxlarge). Given a pair of texts, NLI models output probabilities over entailment, contradiction, and neutral (ENT, CON, NEU). We consider two threshold-agnostic conflict detection settings: fNLI (Max)=I(P(CON) > max(P(ENT),P(NEU))); fNLI (C>E)=I(P(CON) > P(ENT)).\nFactual consistency Factual consistency models evaluate whether all the factual information in a text snippet is contained in another. We evaluated the state-of-the-art in this line of work, AlignScore (Zha et al., 2023) and MiniCheck (Tang et al., 2024). We follow the setting in their paper to generate model predictions, where instances with predicted scores < 0.5 are classified as conflicting.\nLLMS We evaluate state-of-the-art LLMs as conflict detectors, including Mixtral-8x7b (Mistral, 2023), Llama 3 8B Instruct, Llama 3 70B Instruct (Meta, 2024), Claude 3 Haiku, Claude 3 Sonnet (Anthropic, 2024), ChatGPT (OpenAI, 2024b) and GPT-4 (OpenAI, 2024a). GPT models are proprietary models tested by calling the model API. Mixtral (Mistral, 2023), Llama (Meta, 2024), and Claude (Anthropic, 2024) models are accessed through Amazon Bedrock. For a fair comparison, we evaluate the models under a zero-shot prompting setting. The models are prompted to generate {Yes, No} predictions on whether a pair of evidence is conflicting."}, {"title": "Hyper-parameters", "content": "We use default hyper-parameters for all the language models mentioned in this paper. DeBERTa (xlarge) and DeBERTa-v2 (xxlarge) are accessed through HuggingFace. AlignScore (base) and AlignScore (large) models are accessed from GitHub. MiniChek (RoBERTa) , MiniCheck (De-BERTa) and MiniCheck (Flan-T5) models are accessed from HuggingFace."}, {"title": "LLMs prompting details", "content": "We use the llama3-70b-instruct model to generate alternative answers, modify factoids, generate evidence pairs, and do part of the quality checks. The prompt templates for LLMs in this research are presented in Table 14 for answer conflicts and Table 15 for factoid conflicts."}]}