{"title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation", "authors": ["Xianfeng Tan", "Yuhan Li", "Wenxiang Shang", "Yubo Wu", "Jian Wang", "Xuanhong Chen", "Yi Zhang", "Ran Lin", "Bingbing Ni"], "abstract": "Standard clothing asset generation involves creating forward-facing flat-lay garment images displayed on a clear background by extracting clothing information from diverse real-world contexts, which presents significant challenges due to highly standardized sampling distributions and precise structural requirements in the generated images. Existing models have limited spatial perception and often exhibit structural hallucinations in this high-specification generative task. To address this issue, we propose a novel Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance structure determinacy and mitigate hallucinations by assimilating external knowledge from LLM and databases. RAGDiffusion consists of two core processes: (1) Retrieval-based structure aggregation, which employs contrastive learning and a Structure Locally Linear Embedding (SLLE) to derive global structure and spatial landmarks, providing both soft and hard guidance to counteract structural ambiguities; and (2) Omni-level faithful garment generation, which introduces a three-level alignment that ensures fidelity in structural, pattern, and decoding components within the diffusing. Extensive experiments on challenging real-world datasets demonstrate that RAGDiffusion synthesizes structurally and detail-faithful clothing assets with significant performance improvements, representing a pioneering effort in high-specification faithful generation with RAG to confront intrinsic hallucinations and enhance fidelity.", "sections": [{"title": "1. Introduction", "content": "While the high quality of images generated by diffusion models [27, 57] has significantly lowered the barriers for individuals to engage in convenient content creation across various tasks, the generation of 2D standard clothing assets from in-the-wild data has been relatively underexplored. Standard clothing asset generation refers to generating forward-facing flat-lay garment images [78] on a clear background by recovering clothing information from arbitrary real-world scenes (e.g., garments hung on hangers, worn by models, or casually laid on chairs in Fig. 1). Standard garment serves as an important intermediary variable connecting multiple downstream applications, such as garment design, product display, and virtual fitting [24, 80, 81]. Standard clothing generation is a challenging high-specification generative task primarily due to the highly constrained manifold of standard clothing distributions [78] and the high sensitivity of users, which demands a strong fidelity to the generated results. In practice, we have observed that the quality of existing general generative models [52, 69] fails to reach a level that is realistically usable, especially concerning the structure of garments. Actually, these structurally unreliable outcomes, referred to as structure hallucinations [4], are commonly seen in the outputs of large generative models [44, 48], manifesting as distorted facial expressions, palms with 6 fingers, and nonsensical text [4], etc. Current generative models often prioritize compatibility with multi-modal or multi-conditional inputs [34, 49], sampling acceleration [38, 57], and zero-shot editing capabilities [16, 25]. A limited number of works [1, 20, 47] have attempted to mitigate the phenomenon of structural hallucinations through extensive training with a large number of trainable parameters. However, the unsustainable training costs and data requirements render this approach impractical [7]. In this study, we take a step towards addressing the prevalent structure hallucinations in the generation process, aiming for higher specification image generation in the future.\nInspired by the recent advancements in retrieval-augmented NLP [9, 51], we propose RAGDiffusion, a more general and efficient solution, by introducing Retrieval-Augmented Generation (RAG) [51] to enhance structural determinacy during the generation process. Our insight is to eliminate the inherent uncertainty of generative processes by assimilating structural representations and knowledge from external models and databases. This approach avoids the traditional method of heavily compressing visual concepts into a large model through costly training, thereby alleviating the burden on generative models. RAGDiffusion, as a RAG paradigm for addressing the issue of structural illusions, includes two processes: information aggregation based on retrieval, and conditional generation with omni-level fidelity. (1) Retrieval-based Structure Aggregation: Contrastive learning [15, 23] is introduced to train a dual tower network to extract multi-modal structure embeddings from images of two branches as well as attributes derived from a large language model (LLM) [3, 5]. Considering potential encoding bias in real-world data, we propose a structure retrieval named Structure Locally Linear Embedding (SLLE) to project the predicted structure embedding towards a standard manifold [54, 70] as well as offer a silhouette landmark. The remapped latent structure embedding and the landmarks provide both soft and hard structure guidance respectively to eliminate structural hallucinations. (2) Omni-level Faithful Garment Generation: To enhance the faithfulness of the standard garment, RAGDiffusion involves three-level alignment into the generation process of the diffusion model to provide feedback on structure, pattern and decoding faithfulness for garment components. Specifically, we align spatial structure with Embedding Prompt Adapter and Landmark Guider, ensure the generated pattern detail matches the conditioning by ReferenceNet [28], and reduce distortion during reconstruction from VAE [32] by Parameter Gradual Encoding Adaptation (PGEA) to adapt the SDXL [48] backbone to a more powerful VAE.\nTo the best of our knowledge, RAGDiffusion stands as a pioneering multi-modal RAG method to solve intrinsic hallucination and unfaithfulness during image synthesis. Our innovative insight of incorporating external prior knowledge with retrieval to mitigate uncertainty has the potential to catalyze advancements across a broader spectrum of faithful generation applications as showcased in training-free demos in Sec. 4.5. We summarize the contributions of this paper as follows:\n\u2022 We propose RAGDiffusion, a novel RAG framework for standardized clothing generation with representation learning and SLLE to offer structure determinacy and eliminate hallucination.\n\u2022 We employ a three-level alignment in RAGDiffusion, addressing structure, pattern, and decoding aspects, ensuring faithfulness of clothing on this high-specification task.\n\u2022 Comprehensive experiments on challenging in-the-wild sets demonstrate RAGDiffusion is capable of synthesizing both structure and detail faithful clothing assets, outperforming current methods by a substantial margin. Furthermore, the ablation study has validated the effectiveness of different parts of RAGDiffusion."}, {"title": "2. Related works", "content": "Retrieval-augmented generative models. Retrieval-augmented strategies leverage external databases to enhance the capabilities of generative models across a variety of tasks. For instance, the RETRO [8] modifies an LLM to effectively utilize external databases, achieving impressive performance. In the realm of image synthesis, retrieval has been employed in both GANs [12, 59] and diffusion models [7, 56], accommodating 3D generation [55], video generation [76] and artistic styles [53]. The essence of these works lies in retrieving similar images to serve as mimetic references for generating specified content, particularly in cases where the generative models are insufficiently trained. In contrast, our RAGDiffusion aims to leverage retrieval to aggregate structural information, thereby mitigating intrinsic hallucinations and boosting the performance of large base generative models on high-standard tasks.\nControllable text-to-image diffusion models. To attain conditional control in text-to-image diffusion models, ControlNet [75], T2I-Adapter [43] and IP-Adapter [69] incorporate additional trainable modules to fuse conditions on feature maps. Additionally, recent investigations have utilized a variety of prompt engineering techniques [33, 67, 79] and implemented cross-attention constraints [14, 30, 64, 82] to facilitate more controllable generative processes. Furthermore, some studies further investigate multi-condition or multi-modal generation within a single diffusion model [29, 34, 49, 83]. However, these approaches rely on the associative capabilities of stable diffusion, which do not guarantee fidelity in terms of structure and detail in the generated results, particularly when dealing with the highly standardized sampling of standard garments.\nImage restoration. Recent studies [39, 60, 71] have made pioneering strides in developing universal models for natural image restoration. These investigations encompass tasks such as weather effect removal, low-light image enhancement, denoising, and deblurring. One of the most popular applications is image inpainting [11, 26, 41, 48, 65] which utilizes text prompts to guide the generation of content within editing regions. Most similar to our task, document image restoration [62, 68, 73] addresses issues such as dewarping [17, 21, 40, 72], deshadowing [35, 36, 74], and illumination correction [18, 62, 73]. However, unlike 2D document restoration, standard garment generation is inherently a 3D semantic content restoration task and may be influenced by factors such as occlusions, accessories, and human body parts, presenting greater challenges in semantic localization. Consequently, existing image restoration techniques fall short of achieving faithful cloth generation."}, {"title": "3. Method", "content": "An overview of the RAGDiffusion is presented in Fig. 2. The backbone of RAGDiffusion employs the SDXL [52], with the preliminary detailed in Appendix. Given an in-the-wild clothing image \\(x_{itw} \\in \\mathbb{R}^{H \\times W \\times 3}\\), RAGDiffusion is aimed to generate an authentic standard flat lay in-shop garment image \\(x_{std}\\). A dual tower StructureNet extracts latent structure embeddings by contrastive learning as detailed in Sec. 3.1, and SLLE retrieves and fuses structure embeddings and landmarks as in Sec. 3.2. Three-level alignment generation involves Embedding Prompt Adapter, ReferenceNet, and PGEA for structure, pattern, and decoding faithfulness described in Sec. 3.3.\n3.1. Dual-tower embeddings extraction\nTo eliminate structural hallucinations, a straightforward approach is to extract structural features from images and feed them into generative networks, as seen in StyleGAN [31] and LADI-VTON [42], which use additional latent coding to module conditions. Accordingly, we extract latent structure embeddings through contrastive learning [50], using garments that share similar canny but are texture-dissimilar as training pairs. This process exploits the structural similarities between specialized pairs, which are not emphasized during the conventional training of diffusion.\nContrastive learning. Given a batch of N (in-the-wild clothing \\(X_{itw}\\), standard clothing \\(X_{std}\\)) pairs, StructureNet \\(g_{\\theta}\\) is trained to predict which of the \\(N \\times N\\) possible \\((x_{itw}, x_{std})\\) pairings across a batch actually occurred according to structure similarity. To do this, StructureNet learns a multi-modal embedding space by jointly training a dual tower encoder tailored for \\(X_{itw}\\) and \\(X_{std}\\) to maximize the cosine similarity of the embeddings of the N real pairs (marked with superscript +) in the batch while minimizing the similarity of the \\(N^2-N\\) incorrect pairs(marked with superscript -). To be specific, the LLM is used to extract 10 types of discrete attributes of clothing (e.g., Category, FitType, CollarTechnique, IfTopTuckIn), which incorporates general in-context image knowledge from the language model [3, 5], eliminating semantic deficiencies and ambiguities present in vision. Then we assign a learnable embedding \\(f_{attr}\\) to each discrete attribute. The image structure features \\(f_{img}\\) are extracted by the twin ViT [19] encoder and concatenated with the attributes embeddings \\(f_{attr}\\) to form the final latent structure embeddings \\(e\\) after non-linear projection head. We optimize the InforNCE loss [15] over these latent structure embeddings \\(\\{e_{itw}, e_{std}\\}\\) from \\(\\{x_{itw}, x_{std}\\}\\) as:\n\\[\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(<e_{itw,i}, e_{std,i}> / \\tau)}{\\exp(<e_{itw,i}, e_{std,i}> / \\tau) + \\sum_{s \\neq i} \\exp(<e_{itw,i}, e_{std,s}> / \\tau)},\\tag{1}\\]\nwhere \\(<,>\\) is cosine similarity between two vectors, N is the batch size, and \\(\\tau\\) is a temperature scalar.\n3.2. Retrieval-augmented SLLE\nBy integrating structural knowledge from StructureNet into the stable diffusion (SD), we have improved the style and structure of generated flat-lay clothing as shown in Sec. 4.3. However, due to the limited spatial perception inherent in the SD [10, 37], it often struggles to accurately represent the length and contours of clothing items. This issue becomes particularly evident when the fit of garments on human bodies may not be well illustrated (Fig. 5), where we rely solely on soft embedding guidance \\(e_{std}\\). Furthermore, since StructureNet \\(g_{\\theta}\\) is trained on a limited set of contrastive pairs, it may perform poorly with out-of-distribution samples, making it unreliable for real-world applications.\nWe have observed that creating a comprehensive database of standard flat-lay clothing for retrieval is easier than gathering comprehensive in-the-wild samples (numerous scenarios). In this context, we set standard clothing \\(e_{std}\\) as basic vectors and propose Structure Locally Linear Embedding (SLLE), which is a manifold projection [54] to correct each predicted structure embedding \\(e_{itw}\\) into the target space of standard embedding \\(e_{std}\\) to mitigate potential error. Meanwhile, the retrieval database provides structure landmarks to strengthen explicit spatial constraints.\nMemory database. To execute SLLE, we establish a retrieval memory database. We first encode the collected standard flat-lay clothing image dataset into a series of latent structure embeddings \\(e_{std}\\) with StructureNet \\(g_{\\theta}\\). The silhouette landmarks \\(L_{sil}\\) are also extracted to designate areas for generated content. Thus an external memory database \\(\\mathcal{D}\\) that consists of embedding-landmark pairs \\((e_{std}, L_{sil})\\) is obtained. These structure embeddings are utilized in matching algorithms [22], enabling a given in-the-wild garment to find the most compatible standard features \\(e_{std}\\) and landmarks during inference. Notice that landmarks can be any structural figure. Here, we use the outline of clothing, primarily to tackle the challenges of limited spatial perception in SD (e.g. length and contour) [10, 37].\nStructure LLE algorithm. In this process, retrieval-augmented SLLE drags in-the-wild embedding closer to the standard flat-lay garment embeddings to void outliers during inference, as well as offer silhouette landmarks. Motivated by the successful practice of classic locally linear embedding in [6], we assume that the garment embedding or landmark and its nearby points are locally linear on the manifold, eliminating the need to project them into a higher-dimensional space as vanilla LLE [54] does. Specifically, given an extracted garment embedding \\(e_{itw}\\), the goal of SLLE is to reconstruct embedding \\(e_{itw}\\) with standard embeddings \\(e_{std}\\) as basis vectors. We start by searching the K nearest standard embeddings \\(\\{e_{std}^1, ..., e_{std}^K\\}\\) from the standard garment memory database \\(\\mathcal{D}\\) using cosine similarity. Thus K corresponding flat-lay cloth silhouette landmarks \\(\\{L_{sil}^1, ..., L_{sil}^K\\}\\) are obtained as well. Next, a linear combination of these neighbors is sought to reconstruct \\(e'_{itw}\\) by minimizing the error \\(||e'_{itw} - e_{itw}||_2\\), which could be formulated as the least-squared optimization problem:\n\\[\\min_{W} || e_{itw} - \\sum_{i=1}^K W_i e_{std}^i ||^2, \\quad \\text{s.t.} \\sum_{i=1}^K W_i = 1, \\tag{2}\\]\nwhere \\(w_i\\) is the barycentric weight of the i-th nearest embedding \\(e_{std}^i\\). The optimal weights \\(\\{w_1,...,w_K\\}\\) can be determined by solving Eq. (2). Subsequently, we can reconstruct the shape embedding as \\(e'_{itw} = \\sum_{i=1}^K W_i e_{std}^i\\). Ideally, the reconstructed embedding \\(e'_{itw}\\) serves as an in-domain data point that retains structural accuracy and appropriate fitness, albeit with some information loss. In practice, we use a linear combination of original structure embedding \\(e_{itw}\\) and reconstructed one \\(e'_{itw}\\) as the final structure representation during inference:\n\\[\\hat{e}_{itw} = \\alpha \\cdot \\sum_{i=1}^K W_i e_{std}^i + (1-\\alpha) \\cdot e_{itw},\\tag{3}\\]\nwhere \\(\\alpha \\in [0, 1]\\) controls the trade-off and is set to be 0.5. The final landmark \\(\\hat{L}_{sil}\\) is also fused with optimal weights.\n3.3. Omni-level faithful garment generation\nStructure faithfulness. Inspired by the global semantics control practice by IP-Adapter [69], we adopt a similar Embedding Prompt Adapter (EP-Adapter) to condition the high-level semantics of structure embeddings. While maintaining the text branch unchanged, fused structure embeddings \\(\\hat{e}_{itw}\\) after SLLE are fed into additional projection layers to generate key and value matrices for the structure representations. Two parallel cross-attention layers process the text modality and the structure embedding modality, with the attention results being summed to replace the original single text one: Attention(Q, \\(K_{text}\\), \\(V_{text}\\)) + Attention(Q, \\(K_{emb}\\), \\(V_{emb}\\)). The EP-Adapter enhances the global/inner structural faithfulness of generation results by incorporating prior knowledge from LLM and structural training pairs as a soft constraint. Meanwhile, the contour landmark \\(\\hat{L}_{sil}\\) involves external spatial structural information. A Landmark Guider [28] with 4 convolution layers (4 x 4 kernels, 2 \u00d7 2 strides, 16, 32, 64, 128 channels) is incorporated to align the silhouette mask with noise \\(z_t\\) as an explicit and hard constrain.\nPattern faithfulness. Inspired by success in human editing [13, 28], we introduce an additional U-Net encoder (i.e. ReferenceNet [28]) to precisely preserve the fine-grained details of clothing assets, which is isomorphic to the main generative U-Net (i.e. MainNet) and shares same initial parameter weights. The latent of in-the-wild garment image is passed through ReferenceNet to obtain the intermediate key and value features \\(\\{z_k, z_v\\} \\in \\mathbb{R}^{N \\times l \\times d}\\) in self-attention, which are concatenated with the features \\(\\{z_k, z_v\\} \\in \\mathbb{R}^{N \\times l \\times d}\\) from the MainNet along the l dimension to obtain the final \\(\\{z_k, z_v\\} \\in [\\mathbb{R}^{N \\times 2l \\times d}\\). Then we compute the self-attention on the concatenated features as:\n\\[\\text{Attention} \\{\\{z_k, z_v\\}, z_q\\} = \\text{softmax}(\\frac{z_q z_k^T}{\\sqrt{d}})z_v,\\tag{4}\\]\nwhere \\(z_q\\) represents Query features in self-attention from the MainNet. ReferenceNet plays an important role in preserving the texture of garments when it has complicated patterns or logo prints.\nDecoding faithfulness. With ReferenceNet for pattern faithfulness, we still observe fine-grained distortion, because of the image decoding degradation caused by the high compression ratio of SDXL's VAE, as shown in Fig. 5. Although the VAE in FLUX [1] greatly alleviates the loss by inflating latent channels, challenges intrinsic to the DiT framework [47], such as slow convergence, and high data requirements, hinder its widespread adoption in the community. To address this, we propose a versatile three-stage parameter gradual encoding adaption (PGEA) to align the SDXL UNet with the FLUX VAE. Specifically, we expand the channels in conv in and conv out layers (from 4 to 16) in U-Net to match FLUX VAE, enabling direct modification on the SD config file to load adapted weights. Stage I: we focus on distilling the knowledge from the original conv in layer to the adapted conv in layer. The input image is encoded through different VAEs and passed into the 4 and 16-channel conv in layers respectively, applying a reconstruction loss between their output to update the 16-channel conv in layer for fast adaption. Stage II: we train the UNet on a standard text-to-image task for 20,000 steps, where only the 16-channel conv in and conv out layers are updated. Stage III: we train and update the entire UNet on the standard text-to-image task for 200,000 steps. The complete training of PGEA lasts for 4 days on 8 80GB-A100 GPUs. It is noteworthy that the adapted general UNet with extremely low decoding loss can be applied to various downstream tasks."}, {"title": "4. Experiments", "content": "4.1. Experimental setup\nDatasets. We collected 65,131 pairs of (in-the-wild upper clothing, standard flat lay clothing) for training and 1969 pairs for testing, named STGarment. Among the in-the-wild clothing, there are three main displays: clothing worn on a person, clothing laid indoors, and clothing hung on hangers. We use Qwen2-VL-7B [5] to provide prompt annotations for each pair along with 10 discrete attributes (3.5 seconds per image). Additionally, we matched each pair of images with the most structurally similar flat-lay clothing for StructureNet training based on canny image similarity. We employed a clustering approach to filter and construct a memory database for retrieval, which contains 4,000 embedding-landmark pairs from the training set. Please refer to the Appendix for more details.\nImplementation details. We initialize the RAGDiffusion with a pre-trained SDXL model and train it on STGarment dataset using an AdamW optimizer with the learning rate of 5e - 5 at a resolution of 768 \u00d7 768. The models are trained for 5 days on 8 80GB-A100 GPUs with DeepSpeed [2] ZERO-2 to reduce memory usage, at a batch size of 15. K = 4 in SLLE in Eq. (2). The StructureNet (i.e. embedding encoder) utilizes CLIP-ViT-L/14 image encoder as the backbone and is fine-tuned on nearly 200 million various garment images following DinoV2 [45]. StructureNet is further trained on STGarment with contrastive learning for 4 days on 4 80GB-A100 GPUs at a batch size of 128. At inference time, we run RAGDiffusion on a single NVIDIA RTX 3090 GPU for 30 sampling steps with the DDIM sampler [57]. Please refer to the Appendix for more details.\nEvaluation protocols. About generation quality, We utilize LPIPS [77] and SSIM [63] to assess the reconstruction accuracy of the generated garments by comparing them with the provided ground truth (GT) images. Additionally, we employ FID [46] and KID [58] metrics to evaluate the realism and authenticity of the generated distributions. In terms of retrieval ability, we evaluate the top-1 accuracy and top-5 accuracy of retrieval results from the memory database \\(\\mathcal{D}\\) across 1969 test samples, alongside the average Intersection over Union (IoU) of the corresponding silhouette masks with GT ones. Given the absence of GT masks of test samples in the memory database \\(\\mathcal{D}\\), we define the retrieved masks to be accurate if their IoU exceeds 0.85.\nBaselines. Standard garment generation from images is a new task. In general, conditional generation using SD involves three classic methods: IP-Adapter [69], ReferenceNet [28], and ControlNet [75]. ControlNet is primarily designed for spatial-aligned conditional injection, which is not suitable for our task. Therefore, we trained IP-Adapter and ReferenceNet as powerful and intuitive baselines based on SDXL. Additionally, we also trained the classic image-to-image model Paint-by-Example [66] (PBE) as a baseline."}, {"title": "4.2. Generation comparisons with baselines", "content": "Qualitative results. Fig. 3 presents a qualitative comparison between RAGDiffusion and baselines on the STGarment dataset. Paint-by-example has yielded entirely unsuccessful outputs. Meanwhile, IP-Adapter exhibits a high-level perception of garment semantics, generating roughly accurate structure even in layered and side-view situations, although it suffers from severe texture distortion. In contrast, ReferenceNet manages to maintain correct textures in challenging scenarios but encounters structural confusion and distortion. This phenomenon may stem from ReferenceNet's over-reliance on local texture features, lacking the benefits of a global perspective in challenging cases. RAGDiffusion employs a retrieval-aggregate approach to capture structural information and integrates conditional controls from EP-Adapter, Landmark Guider, and ReferenceNet, yielding promising performance in both structurally faithful and detail-oriented garment conversion. Notably, while all models are trained on the same dataset, RAGDiffusion assimilates high-quality contour landmarks and structure embeddings as external prior to producing visually compelling results that enhance depth and realism.\nQuantitative results. As indicated in Tab. 1, we quantitatively evaluate the generation quality of various methods on the STGarment dataset, demonstrating that RAGDiffusion significantly outperforms all baseline approaches. This confirms RAGDiffusion's ability to deliver superior and accurate garment generation across diverse scenarios. Retrieval, as a crucial component of RAGDiffusion for assimilating external knowledge, plays a significant role. Using the evaluation metrics described in Sec. 4.1, we report the retrieval accuracy at different sizes of the external memory database \\(\\mathcal{D}\\) in Tab. 3. Specifically, we employ clustering and downsampling algorithms to iteratively eliminate outliers and highly similar samples from the original memory database, constructing retrieval libraries of four different scales: 1000, 2000, 4000, and 8000 samples. When the memory database \\(\\mathcal{D}\\) is too large, outliers may adversely affect quality if embeddings are inaccurate, while redundant samples reduce retrieval efficiency. Conversely, if the memory database \\(\\mathcal{D}\\) is too small, the retrieval library may lack sufficient representativeness and completeness, adversely affecting the generation performance for specific categories. Notably, a memory database comprising 4000 samples achieved the best trade-off in performance. Please refer to the Appendix for more technical details.\nUser study. When metrics like FID and SSIM assess the realism and authenticity of standard garment generation, they tend to be less sensitive to high-frequency information such as contours, collar variations, buttons, and color shifts [16]. We conducted user studies involving 25 participants to evaluate different methods based on user preferences across 200 randomly selected image pairs from the test set. Participants were asked to assign a preference score (ranging from 1 ~ 10) in terms of structural fidelity and detail fidelity for each sample generated by anonymized methods. As illustrated in Fig. 4, we report the distribution of the scores, encompassing medians, means, quartiles, and outliers. Our findings reveal that RAGDiffusion is significantly preferred over all baseline methods in terms of structural fidelity (mean = 8.34) and detail fidelity (mean = 8.52). Additionally, the narrow range of outliers in our method indicates a more stable generation across various scenes."}, {"title": "4.3. Ablation study", "content": "Retrieval. One of the key processes of SLLE is to retrieve silhouette landmarks that enhance explicit spatial constraints. Consequently, we removed landmarks as the baseline named \"w/o Retrieval\" in Tab. 2 and Fig. 5. The model without retrieval exhibits significant contour errors, particularly around the back collar, sleeve length, and garment length. This is attributed to the fact that retrieval-augmented SLLE incorporates priors from LLM, which transcend visual limitations and aid the generative model in accurate spatial structures of clothing through the use of landmarks. Furthermore, we investigate the role of SLLE in embedding remapping in Sec. 4.4.\nStructureNet. The StructureNet, based on contrastive learning, provides latent structure embeddings that are utilized for similarity-based retrieval in SLLE and feature injection in the EP-Adapter. To validate the necessity of our proposed StructureNet, we removed the structure embedding and the subsequent retrieval process as the baseline labeled \"w/o StructureNet\". As illustrated in Tab. 2 and Fig. 5, the model lacking latent structure embedding frequently displays artifacts in the internal structures of the clothing, such as inaccurate collars, missing pockets, and confusion due to occlusions. This demonstrates that structure embedding serves as global guidance and plays a significant role in optimizing the internal structures of apparel.\nPGEA. PGEA is employed to mitigate information loss during the VAE encoding and decoding process. As illustrated in Tab. 2 and Fig. 5, the model utilizing PGEA achieves clearer and more accurate edges in high-frequency texture simulation; additionally, it shows significant improvements in the recovery of logos and text. PGEA effectively alleviates the long-standing issue of distortion in fine patterns that has hindered the widespread application of AIGC in commercial scenarios."}, {"title": "4.4. Component analysis", "content": "Latent structure embedding distributions. We further visualize the learned latent embedding distribution in Fig. 7 to gain an in-depth comprehension of how the embeddings work in retrieval and the EP-Adapter. For this purpose, we sample N = 200 pairs of (in-the-wild cloth embedding \\(e_{itw}\\), standard cloth embedding \\(e_{std}\\)) pairs from our dual-tower StructureNet. We then employ t-SNE [61] to project each feature representation into a point within Fig. 7. Notably, 1) the dual-branch embeddings (\\(e_{itw}\\), \\(e_{std}\\)) corresponding to the same category exhibit clustering behavior, indicating that the learned priors in StructureNet effectively perform structural alignment cross domains, thus facilitating retrieval based on structural similarity; and 2) representations from different categories are clearly dispersed, demonstrating that the embeddings encompass discriminative structural information, which aids in generation through the EP-Adapter. Lastly, we also visualize the distribution of cosine similarity between a given sample and images from the external memory database \\(\\mathcal{D}\\) in Fig. 8. The cosine similarities present a normal distribution, affirming that the constructed embedding library is representative and comprehensive. Furthermore, the examples visualized for different similarity retrieval results effectively illustrate the efficacy of the landmark retrieval mechanism.\nNumber of retrieved embeddings. As the number K of retrieved nearest neighbors in Eq. (2) during SLLE plays a fundamental role in the properties of the final structure embeddings and landmarks, we demonstrate the fusion sensitivity to K in Fig. 6. A smaller K value facilitates the sharp edge of landmarks, whereas a larger K value enhances the accurate representation of the reconstructed embedding \\(e_{itw}\\) in Eq. (3). It can be observed that the optimal performance occurs at K = 4, which also serves as the default parameter setting for other experimental parts."}, {"title": "4.5. Extended applications of RAG", "content": "The core concept of RAGDiffusion lies in leveraging retrieval to provide structural determinism, thereby enhancing the quality and fidelity of generated content. This approach has broad applicability beyond garment generation. When equipped with a comprehensive retrieval database and an accurate retrieval mechanism, it can effectively address issues such as incorrect hand structure and prompt control failures, minimizing structural hallucinations.\nIn extended studies, we employ a training-free RAG system in three tasks in Fig. 9: Pose Control, Finger Refining, and Background Replacing, with retrieval banks consisting of 30 landmarks for each task. As an ablation, we employ SD 1.5 to directly generate images from prompts, which often results in (a) poses that misalign with the descriptions, (b) structural inaccuracies, and (c) outputs of low quality. Subsequently, we utilize GPT-40 to retrieve corresponding landmarks from the external demo banks, employing pre-trained ControlNet [75] or IP-Adapter [69] for conditioning. This intervention significantly improves the quality of the generated outputs and eliminates structural hallucinations. While this represents a training-free toy demonstration, it showcases the extensive potential of RAG-assisted generation, particularly in professional commercial contexts that demand high standards and rigor."}, {"title": "5. Conclusion", "content": "In conclusion, RAGDiffusion presents a significant advancement in the generation of standardized clothing assets by effectively addressing the prevalent challenges of structural hallucinations and fidelity in generated images. By integrating Retrieval-Augmented Generation with a dual-process framework-focusing on retrieval-based structure aggregation and omni-level faithful generation-we demonstrate a robust approach to creating high-quality garment images that maintain structural integrity and detail accuracy. Comprehensive experiments and in-depth analysis demonstrate notable performance improvements over existing generative models. We hope that RAGDiffusion will help open avenues for RAG in diverse high-specification, faithful generation tasks, bringing us closer to the goal of professional content creation for all."}, {"title": "6. Additional analysis: enhancing generalization through external retrieval databases", "content": "Generally speaking, Retrieval-Augmented Generation (RAG) can significantly enhance generalization and robustness in new scenarios simply by updating the retrieval database, without the need for retraining. This provides a cost-effective and convenient maintenance solution for large generative models, avoiding the expensive process of retraining. We have also observed similar phenomena in RAGDiffusion.\nRAGDiffusion is trained on upper-body clothing data and has not encountered lower-body garments during training. In this testing phase, we collect embeddings and corresponding landmarks for 856 lower-body items and incorporate them into the external memory database. Subsequently, we gather 50 lower-body in-the-wild clothing samples as a test set, using a ReferenceNet version as a baseline for comparison. The results in Fig. 11 demonstrate that retrieval significantly improves generalization capabilities. By injecting the embeddings of lower-body garments along with their corresponding contour landmarks as conditional constraints, RAGDiffusion produces accurate results in the lower-body domain, showcasing its strong out-of-distribution (OOD) compatibility. In contrast, the ReferenceNet version is noticeably confused by the concept of lower-body garments and fails to yield meaningful garment structures. This boost in generalization increases the operational maturity of RAGDiffusion, enabling it to effectively handle various OOD images submitted by users."}, {"title": "7. Limitations and future work", "content": "We present RAGDiffusion", "processes": "constructing the embedding space through contrastive learning, adjusting the U-Net backbone based on PGEA, establishing the memory database, and training the diffusion model. These separate processes impose higher demands on researchers' expertise and coding capabilities. However, this modularity is inherent to the use of retrieval-augmented techniques, which necessitate multiple stages. Maintaining independent memory databases, independent latent spaces, and separate diffusion training ensure that the memory database can be freely adjusted as needed, effectively decoupling it from the diffusion model.\nSecondly, we have observed a possibility of color bias in nearly pure-colored garments, particularly under very bright or very dark situations. This phenomenon is commonly encountered in image-based image editing, as the reconstruction loss constraints of Stable Diffusion are not particularly sensitive to color discrepancies. Additionally, the illumination variance can further impact the perceived colors of clothing. We note that extended training durations, along with the incorporation of contrast-enhancing data augmentation techniques, can partially alleviate this issue.\nIn our future work, we aim to enhance RAGDiffusion in both its application depth and breadth. Firstly, by strengthening the injection of color information and incorporating lighting simulation, we hope to address the potential color bias observed in garments. Secondly"}]}