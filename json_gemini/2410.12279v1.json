{"title": "Beyond Oversmoothing: Evaluating DDPM and MSE for Scalable Speech Synthesis in ASR", "authors": ["Christoph Minixhofer", "Ondrej Klejch", "Peter Bell"], "abstract": "Synthetically generated speech has rapidly approached human levels of naturalness. However, the paradox remains that ASR systems, when trained on TTS output that is judged as natural by humans, continue to perform badly on real speech. In this work, we explore whether this phenomenon is due to the oversmoothing behaviour of models commonly used in TTS, with a particular focus on the behaviour of TTS-for-ASR as the amount of TTS training data is scaled up. We systematically compare Denoising Diffusion Probabilistic Models (DDPM) to Mean Squared Error (MSE) based models for TTS, when used for ASR model training. We test the scalability of the two approaches, varying both the number hours, and the number of different speakers. We find that for a given model size, DDPM can make better use of more data, and a more diverse set of speakers, than MSE models. We achieve the best reported ratio between real and synthetic speech WER to date (1.46), but also find that a large gap remains.", "sections": [{"title": "I. INTRODUCTION", "content": "Synthetic data for model training has gained traction across several domains, including computer vision, natural language processing, and speech technology [1], [2], [3], [4]. Recent works have increasingly used Text-to-Speech (TTS) systems are to produce synthetic speech for training Automatic Speech Recognition (ASR) [5], [6], [7], [8], [9], [10]. Doing so can reduce the reliance on large volumes of real speech data, which is both difficult and expensive to obtain, and can come with ethical challenges [11].\nAs we have shown in previous work, high-quality synthetic speech often evaluated by mean opinion scores (MOS)-does not always translate to improved ASR performance, measured by word error rate (WER) [12]. This highlights a limitation of current TTS systems: their tendency to generate speech that sounds natural but lacks the variability of real human speech. This limitation is particularly evident in ASR training, where variability in speech patterns is crucial for effective generalization.\nPrevious work in TTS-for-ASR has often relied on models optimized using mean squared error (MSE) loss [13], [14]. While these models produce intelligible and stable speech [11], they often generate repetitive outputs that fail to capture the diversity needed for ASR. Recent research suggests that models such as Denoising Diffusion Probabilistic Models (DDPMs) [15], which introduce stochasticity during generation, may better approximate the complexity of real speech by capturing a broader range of variability. By learning the reverse diffusion process, DDPMs are encouraged to generate from the entire distribution, thus producing more varied and representative synthetic speech. Nevertheless, even these models face challenges in fully bridging the gap between synthetic and real speech distributions [10].\nRecent work has also explored the scaling laws of synthetic data in various domains. A plateau in test error has been identified when training large language models on synthetic data [16]. Similarly, limited scaling behaviour after training on a certain number of synthetic data-points was found in computer vision [17]. We propose a power-law for TTS-for-ASR with respect to dataset size (Section II-E). Our findings suggest that the TTS training dataset size affects the performance of ASR models trained on synthetic data in two distinct phases. Initially, ASR performance rapidly improves as TTS learns the key aspects of speech generation. However, this is followed by a phase of diminishing returns, where additional TTS training data has minimal impact on further reducing the gap between ASR models trained on real and synthetic data. Our work contributes two key findings: (1) We introduce a two-term power law for the relationship between dataset size used to train TTS and ASR performance when trained using synthetic data. (2) We show that DDPM-based models show a more favorable scaling curve compared to MSE, but they encounter the same limitation of diminishing returns.\nThese findings indicate that while advanced models like DDPMs offer some improvements, other approaches beyond scaling synthetic data will be necessary to fully close the performance gap between synthetic and real speech data in ASR training."}, {"title": "II. METHODOLOGY", "content": "To investigate the theoretical higher capability to capture the complex multi-modal distributions of DDPM models over MSE-based models for TTS-for-ASR, we introduce a framework which systematically evaluates both approaches."}, {"title": "A. Modeling Text-to-Speech", "content": "Generative models for speech synthesis can be viewed as conditional models, where the goal is to generate realistic speech conditioned on specific inputs, such as text and speaker identity [18]. The challenge in this is ensuring that the generated speech captures the full distribution of the real speech one of the failure modes when aiming to generate diverse outputs is mode collapse, where the model fails to generate the full range of possible outputs and instead focuses on a limited subset. In MSE-based models, mode collapse manifests as oversmoothed, low-variance outputs that do not capture the full variability of real speech. In contrast, the stochastic nature of DDPMs, combined with their ability to model complex distributions, should inherently mitigate the risk of mode collapse."}, {"title": "B. Mean Squared Error (MSE) Models", "content": "Let x denote the target speech feature (e.g., a mel spectrogram) and c represent the conditioning variables, including textual input (as a phone sequence) and speaker identity (as a vector). The goal of the TTS model fe is to learn the conditional distribution p(x | c). For MSE-based models, this is typically achieved by minimizing the L2 loss between the generated output fo(c) and the ground truth x:\n$L_{MSE}(\\theta) = E_{x,c} [||x - f_{\\theta}(c)||^2]$"}, {"title": "C. Denoising Diffusion Probabilistic Models (DDPMs)", "content": "In contrast, DDPMs seek to model the entire distribution p(x | c) by introducing a stochastic process. First, a forward process progressively adds Gaussian noise to the data over a series of steps, degrading the data into a noise distribution. Then, a reverse process is learned, which gradually removes the noise at each step, reconstructing the original data from the noisy version. This approach differs fundamentally from traditional Mean Squared Error (MSE)-based models used in speech synthesis by focusing on learning this noise-reversal mechanism instead of directly predicting waveforms.\nIn the forward diffusion process, Gaussian noise is gradually added to the target speech feature xo, such as a mel spectrogram, over a series of timesteps n = 1, 2, . . ., N. The result is a progressively noisier version of the speech feature, denoted as In, which approaches a Gaussian distribution as n reaches the maximum value N. This forward process is defined as:\n$q(x_n | x_{n-1}) = N(x_n; \\sqrt{1 - \\beta_n}x_{n-1}, \\beta_nI)$"}, {"title": "D. Model Architectures", "content": "We used an architecture consisting of two U-Net [20] models: one for generating prosodic features and another for producing mel spectrograms. Both are trained using the MSE and diffusion approaches outlined in Sections II-B and II-C.\nU-Net for Prosody Representation: The first U-Net, denoted as U-NetENC, generates a two-dimensional representation of prosody, utilizing the Continuous Wavelet Transform (CWT) of pitch, energy as in FastSpeech [13]. To arrive at a unified target for generation, we also include the CWT of the phone duration. The prosody representation P is conditioned on the phone sequence and speaker identity:\n$P = U-\\text{Net}_{ENC}(\\text{phone sequence}, s)$"}, {"title": "E. Scaling Laws for TTS-for-ASR", "content": "We introduce a scaling law framework to describe how TTS-for-ASR performance in WERR scales with the dataset size D. As training data increases, the model performance could theoretically reach, but not surpass, 1, since this would be equivalent to training the ASR model with real data [12].\nWe hypothesise that the performance follows a two-regime scaling law with respect to dataset size. This scaling law consists of the variance-limited phase parametrized by \u03b1 in which any additional data used to train TTS improves ASR as the new data it allows model to better approximate the target speech distribution. This is then folowed by a resolution-limited regime parametrized by \u03b3 in which the TTS models' complexity limits any further increases in usefulness of the synthetic speech for ASR.\n$WERR(D) \\propto D^{-\\alpha} + D^{-\\gamma}$"}, {"title": "F. Evaluation", "content": "In evaluation we focus on the models' ability to generate synthetic speech that is suitable for ASR training, particularly as the amount of training data and speaker diversity increases. We utilize the WER ratio (WERR) as the primary metric, defined as the WER of an ASR system trained on synthetic data (WER-Synth) divided by the WER of the same system trained on real data (WER-Real). This ratio provides a direct measure of how well the synthetic speech matches the characteristics of real speech in terms of ASR performance [12]."}, {"title": "III. EXPERIMENTS", "content": "We set out to evaluate the effectiveness of Denoising Diffusion Probabilistic Models (DDPM) compared to Mean Squared Error (MSE) models in generating synthetic speech for Automatic Speech Recognition (ASR) training. Specifically, we aim to assess how these models perform when scaling the amount of training data and increasing speaker diversity. We hypothesize that DDPM models, due to their probabilistic nature, will outperform MSE models in scenarios with larger datasets and greater speaker diversity."}, {"title": "A. Experimental Setup", "content": "Datasets: All datasets used in these experiments are derived from the LibriHeavy [22] corpus. To ensure fair comparisons, we create three subsets for each dataset size and speaker diversity condition: (1) a subset for training the TTS system, (2) a subset for training the ASR system, and (3) a subset for evaluating the ASR system. Within each experiment, the same set of speakers and their proportions are maintained across all three subsets, ensuring consistency in speaker representation. However, the specific speakers and their proportions vary between different experiments. This variability across experiments is acceptable because we focus on comparing the word error rate (WER) ratio, which measures the relative performance of the ASR system, rather than absolute WER. Additionally, there is no overlap in transcripts between the TTS training, ASR training, and ASR evaluation sets, ensuring that performance differences are attributed to variations in the TTS models and dataset conditions, rather than differences in the evaluation data.\nTTS Model Configuration: The TTS systems used in these experiments are based on a U-Net architecture, with configurations tailored for both DDPM and MSE models. Both models also incorporate a Flan-T5-Base [21] language model for conditioning on the original text using cross-attention.\nThe MSE model minimizes the mean squared error between the generated and true mel spectrograms (see Section II-B). The DDPM model is trained with a diffusion loss function, learning to predict the noise added during the diffusion process (see Section II-C).\nAll models were trained using a batch size of 16 and used a cosine learning rate schedule starting at 4 \u00d7 10-5. The training was conducted over 500,000 iterations with an exponential moving average (EMA) decay rate of 0.9999 to stabilize the model parameters. For inference, a DDIM sampler with 20 sampling steps was used, applying a classifier-free guidance weight of 7.5 and a rescale factor of 0.7 to control over-exposure during generation. In line with previous work, the training utilized a rescaled noise schedule to ensure zero terminal signal-to-noise ratio (SNR), and the inference was performed starting from the last timestep, ensuring congruency between the training and inference processes [23]. We make our code and detailed configuration publicly available."}, {"title": "IV. RESULTS & DISCUSSION", "content": "Here, we present the performance results of two models-Mean Squared Error (MSE) and Denoising Diffusion Probabilistic Models (DDPM)\u2014across varying dataset sizes and levels of speaker diversity. We particularly focus on the models' scalability with data and speaker diversity, in terms of the Word Error Rate (WER) ratio achieved."}, {"title": "A. Data Scaling and Performance Differences", "content": "As shown in Figure 2, the MSE and DDPM model exhibited vastly differing scaling properties. The MSE model, while performing reasonably well with smaller datasets, showed limited improvement as the dataset size increased. Even when scaled to much larger datasets, the MSE model did not approach the desired performance threshold of WER ratios close to 1. This suggests that, for the MSE model, further scaling of data offers diminishing returns, rendering it less suitable for scenarios requiring vast amounts of training data. We hypothesize that this limitation stems from the model's intrinsic bias toward more oversmoothed outputs, which fail to leverage the additional variance present in larger datasets.\nIn contrast, the DDPM model showed different scaling behaviour. While initially underperforming in smaller data regimes (\u2248 300 hours and less), the model showed significant improvements as the dataset size increased."}, {"title": "B. Speaker Diversity", "content": "Next, we investigate the relationship between speaker diversity and model performance. Across all dataset sizes, MSE models consistently outperformed DDPM models when trained on smaller datasets (\u2248 100 hours), regardless of the level of speaker diversity. For example, at 100 hours, the MSE model achieved WER ratios of 3.62, 3.66, and 3.92 for low, medium, and high diversity, respectively. By comparison, the DDPM model produced WER ratios of 8.07, 8.33, and 7.44 under the same conditions. These results suggest that, in lower data regimes, the more deterministic nature of the MSE model is advantageous. However, this trend shifted as dataset sizes increased. With 500 hours of training data and beyond, the highest speaker diversity always performed best for the DDPM model. However, as can be seen in Table I, the relative difference between the lowest and highest diversity diminished as training data size increased, from 8% with 100 hours to 4% for 5000 hours. This indicates that a similar effect to the diminishing returns in dataset size also applies to speaker diversity."}, {"title": "C. The Scaling Function", "content": "We additionally investigate the values of \u03b1 and \u03b3 when fit to the model results as described in Section II-E. The better scaling properties in the inital, variance-limited stage of the MSE model are reflected in a higher \u03b1 (2.93) than for the DDPM model (1.86). However, \u03b3 was significantly lower for the MSE model (0.01), leading to earlier stagnation in WERR than for the DDPM model (0.06). However, even with its improved scaling properties, and our optimistic assumption that the model never completely stagnates and keeps following the power-law, at least a million hours of synthetic training data would be required to reach the same performance as using real data. Further research is needed to explain this gap."}, {"title": "V. CONCLUSION", "content": "Our findings demonstrate that while MSE models perform well with smaller datasets, their scalability is limited, resulting in diminishing performance improvements as data increases. In contrast, DDPM models exhibit better scalability, particularly with large and diverse datasets, though the data requirements to achieve real-speech performance remain significant. Overall, DDPM models provide more potential for future large-scale speech synthesis applications, particularly as larger datasets become more accessible. However, our proposed scaling law indicates that they would require at least one million hours of synthetic speech to match the performance of real speech, far exceeding the size of available open datasets [22], [24], [25]. Additionally, while DDPM models handle speaker diversity better with larger datasets, the diminishing returns observed in both data size and diversity highlight the need for alternative approaches to further reduce the performance gap between synthetic and real speech in ASR tasks. Future work should explore methods beyond dataset scaling to overcome these limitations."}]}