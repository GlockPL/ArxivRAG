{"title": "Lipschitz Lifelong Monte Carlo Tree Search for\nMastering Non-Stationary Tasks", "authors": ["Zuyuan Zhang", "Tian Lan"], "abstract": "Monte Carlo Tree Search (MCTS) has proven highly effective in solving complex\nplanning tasks by balancing exploration and exploitation using Upper Confidence\nBound for Trees (UCT). However, existing work have not considered MCTS-\nbased lifelong planning, where an agent faces a non-stationary series of tasks\n\u2013 e.g., with varying transition probabilities and rewards \u2013 that are drawn sequen-\ntially throughout the operational lifetime. This paper presents LiZero for Lips-\nchitz lifelong planning using MCTS. We propose a novel concept of adaptive UCT\n(aUCT) to transfer knowledge from a source task to the exploration/exploitation\nof a new task, depending on both the Lipschitz continuity between tasks and the\nconfidence of knowledge in in Monte Carlo action sampling. We analyze LiZero's\nacceleration factor in terms of improved sampling efficiency and also develop ef-\nficient algorithms to compute aUCT in an online fashion by both data-driven and\nmodel-based approaches, whose sampling complexity and error bounds are also\ncharacterized. Experiment results show that LiZero significantly outperforms ex-\nisting MCTS and lifelong learning baselines in terms of much faster convergence\n(3~4x) to optimal rewards. Our results highlight the potential of LiZero to ad-\nvance decision-making and planning in dynamic real-world environments.", "sections": [{"title": "1 Introduction", "content": "Monte Carlo Tree Search (MCTS) has demonstrated state-of-the-art performance in solving many\nchallenging planning tasks, from playing the game of go Silver et al. [2016] and chess to logistic\nplanning Silver et al. [2017]. It performs look-ahead searches based on Monte Carlo sampling of\nthe actions to balance efficient exploration and optimized exploitation in the large search space. Re-\ncent efforts have focused on developing MCTS algorithms for real-world domains that require the\nelimination of certain standard assumptions. Examples include MuZero Schrittwieser et al. [2020a]\nthat leverages the decoding of hidden states to avoid requiring the knowledge of the game dynam-\nics; and MAzero Liu et al. [2024] that performs multi-agent search through decentralized execution.\nHowever, existing work have not considered lifelong non-stationarity of task dynamics, which may\nmanifest itself in many open world domains, as the task environment can often vary over time or\nacross scenarios. It requires novel MCTS algorithms that can adapt in response, accumulate, and\nexploit knowledge throughout the learning process.\nWe consider MCTS-based lifelong planning under non-stationarity. An agent faces a series of chang-\ning planning tasks \u2013 e.g., with varying transition probabilities and rewards \u2013 which are drawn se-\nquentially throughout the operational lifetime. Transferring knowledge from prior experience to\ncontinually adapt Monte Carlo sampling of the actions and thus speed up searches in new tasks\nis a key question in this setting. We note that although continual and lifelong planning has been\nstudied in reinforcement learning (RL) context, e.g., learning models of the non-stationary task\nenvironment Xie et al. [2020], identifying reusable skills Lu et al. [2020], or estimating Bayesian"}, {"title": "2 Background", "content": "Monte Carlo Tree Search (MCTS) Kocsis and Szepesv\u00e1ri [2006], Silver et al. [2016],\nSchrittwieser et al. [2020a] is a heuristic search algorithm often applied to problems modeled\nas MDPs to handle exploration and exploitation dynamically. MCTS builds a search tree by\nexploring actions from the current state, simulating outcomes, and using those results to update\nestimated values for the selected actions. Normally, the problems solved by MCTS can be modeled\nusing a Markov Decision Process (MDP) Even-Dar et al. [2004], which is formally defined as a\ntuple: (S, A, R, P), where S is the state space, and A is the action space, $R_\\theta$ is the reward of taking\naction a in state s and P is the transition probability matrix.\nIn the MCTS framework, the Upper Confidence Bound for Trees (UCT) Coulom [2006] and\nits variant, polynomial Upper Confidence Trees (pUCT) Matsuzaki [2018], Auger et al. [2013],\nare among the most commonly used selection strategies for balancing exploration and exploita-\ntion during node selection. Although these bounds are theoretically grounded and have achieved\ngreat empirical success, they are based on static environment assumptions and do not consider\ndynamic, non-stationary environments Pourshamsaei and Nobakhti [2024], Hernandez-Leal et al."}, {"title": "3 Our Proposed Solution", "content": ""}, {"title": "3.1 Deriving adaptive Upper Confidence Bound (aUCT)", "content": "To derive the proposed aUCT rule, we consider set of m past known MDPs $M_1, ..., M_m$ and\ntheir leaned search policies $\\pi_1,... \\pi_m$. Let S and A be their state and action spaces, respectively\u00b9, $\\theta$\n$N_i(s, a)$ be the visit count of MPD M\u1d62 to state-action pair $(s \\in S,a \\in A)$, $W(s, a)$ to denote\nits sampled return, and $Q^\\theta_i(s,a) = \\frac{W_i(s,a)}{N_i(s,a)}$ be the learned estimate for Q-value of\nMDP $M_i$. Our goal is to apply these knowledge toward learning a new MDP, denoted by $M$. To\nthis end, we derive a new Lipschitz upper confidence bound for $M$, which utilizes and transfers\nthe knowledge from past MDPs $M_1, ..., M_n$, thus obtaining an improved Monte Carlo action\nsampling strategy that limits the tree search on $M$ to a smaller subsets of sampled actions. We use\n$N(s, a)$ to denote the visit count of the new MDP to $(s \\in S, a \\in A)$, $W(s, a)$ to denote the sampled\nreturn, and thus $Q^N(s, a) = \\frac{W(s, a)}{N(s, a)}$ to denote its current Q-value estimate.\nOur key idea in this paper is that an improved upper confidence bound for the new MDP $M$ can be\nobtained by (i) analyzing the Lipschitz continuity between the past and new MDPs with respect to\nthe upper confidence bounds and (ii) taking into account the confidence and aleatory uncertainty\nof the learned Q-value estimates to determine to what extent the learned knowledge from each\n$M_i$ is pertinent. Intuitively, the more similar $M$ and $M_i$ are and the more samples (and thus\nhigher confidence) we have in the learned Q-value estimates, the less exploration we would need\nto perform for solving $M$ through MCTS. Our analysis will lead to an improved upper confidence\nbound that guides the MCTS on the new MDP $M$ over a much smaller subset of action samples,\nthus significantly improving the search performance. We start with introducing a definition of the\ndistance between any two given MDPs, $M = (R, P), M' = (R', P')$, with reward functions R, R'\nand state transitions P, P', respectively. We choose a positive scaling factor $\\kappa > 0$ to combine the\ndistances with respect to transition probabilities and rewards. Proofs of all theorems and corollaries\nare presented in the appendix."}, {"title": "Definition 3.1.", "content": "Give two MDPs $M = (R, P), M' = (R', P')$, and a distribution for sampling the\nstate transitions $U : S \\times A \\times S' \\rightarrow [0, 1]$, we define the pseudometric between the MDPs as:\n$d(M, M') = \\Delta R + \\kappa \\cdot \\Delta P$\n$= E_{(s,a,s') \\sim u} [|R - R'| + \\kappa |P - P'|]."}, {"title": "Here", "content": "$d(M, M')$ is our definition of distance between two MDPs, $M$ and $M'$. We choose $U$ to\nbe uniform distribution for sampling the state transitions in this paper. In Section 4, we discuss"}, {"title": "Theorem 3.2", "content": "(Lipschitz aUCT Rule). Consider two MDPs M and M' with visit count N, N' and\ncorresponding estimate Q-values $Q^M(s, a), Q^{M'}(s, a)$, respectively. With probability at least (1\u2013\u03b4)\nfor some positive \u03b4 > 0, we have\n$|Q^M(s, a) - Q^{M'}(s,a)| \\leq L \\cdot d(M, M') + P(N, N') \\quad(1)$\nwhere L = 1/(1 \u2212 \u03b3) is a Lipschitz constant, d(M, M') is the distance between MDPs, and\nP(N, N') is given by\nP(N, N') = \\frac{2R_{max}}{1 - \\gamma^2} \\sqrt{\\frac{ln(2/\\delta)}{min(N,N')}} \\quad (2)$"}, {"title": "In the theorem", "content": "above, we show that the estimate Q-values between two MDPs are bounded by\ntwo terms, i.e., a Lipschitz continuity term depending on the distance d(M, M') between the two\nenvironments and a confidence term depending on the number N, N' of samples used to estimate the\nQ-values. The Lipschitz continuity term measures how much the learned knowledge of source MDP\nM is pertinent to the new MDP M', while the confidence terms P(N, N') quantifies the sampling\nbias arising from statistical uncertainty due to limited sampling in MCTS. We note that as the number\nof samples N goes to infinity, we have $Q^M(s, a) \\rightarrow Q^M(s, a)$ in Theorem 3.2, approaching the true\nQ-value $Q^M(s, a)$ of the new MDP. Our theorem effectively provides an upper confidence bound\nfor the true Q-value of the new MDP, based on knowledge transfer from the source MDP. We also\nnote that as both numbers N, N' goes to infinity, the confidence term becomes P(N, N') \u2192 0.\nOur theorem recovers the Lipschitz lifelong RL Lecarpentier et al. [2021b] as a special case of our\nresults, with respect to the true Q-values of the two MDPs."}, {"title": "Corollary 3.3", "content": "(aUCT bound in lifelong planning). Given MDPs $M_1, ..., M_m$, the new MDP's\ntrue Q-value is bounded by $Q(s, a) \\leq U_{auct}$ with probability at least (1 \u2013 \u03b4). The aUCT bound\n$U_{aUCT}$ is given by\n$U_{aUCT}(s, a) \\coloneqq min_{1 \\leq i \\leq m} [Q^{M_i}(s, a) + L \\cdot d(M, M_i) + \\frac{2R_{max}}{1 - \\gamma^2} \\sqrt{\\frac{ln(2/\\delta)}{N_i(s, a)}}] \\quad (3)$"}, {"title": "Obtaining", "content": "this corollary is straightforward from Theorem 3.2 by taking $N \\rightarrow \\infty$ and considering\nthe tightest bound of all knowledge transfers. In the context of MCTS-based lifelong planning, the\nmore knowledge we have from solving past tasks, the more likely we can easily plan a new task, as\nthe aUCT bound $U_{aUCT}(s, a)$ is taken over the minimum of all past tasks. The confidence of past\nknowledge, i.e., the statistical uncertainty due to sampling number $N_i$, also affects the knowledge\ntransfer to the new task."}, {"title": "3.2 Our Proposed LiZero Algorithm Using aUCT", "content": "We use the derived aUCT to design a highly efficient LiZero algorithm for MCTS-based life-\nlong planning. The LiZero algorithm transfers knowledge from past known tasks by computing"}, {"title": "UaUCT (s, a)", "content": "in Corollary 3.3. It requires efficient estimate of the distance d(M, M\u1d62) (as defined\nin Definition 3.1) between the source MDPs and the new (target) MDP. We will present practical\nalgorithms for such distance estimate in the next section and present analysis on the sampling com-\nplexity and error bounds. We will first introduce our LiZero algorithm in this section. We note that,\nduring MCTS, direct exploration/search in the new task M also produces new knowledge and leads\nto improved UCT bound of M. Therefore, our proposed LiZero combines both knowledge transfer\nthrough UaUCT (s, a) and knowledge from direct exploration/search in M.\nThe search in our proposed LiZero algorithm is divided into three stages, repeated for a certain\nnumber of simulations. First, each simulation starts from the internal root state and finishes when\nthe simulation reaches a leaf node. Let $Q(s,a) = \\frac{W(s,a)}{N(s, a)}$ be the current estimate of\nthe new MDP and $N(s) = \\Sigma_{a \\in A}N(s, a)$ be the visit count to state s \u2208 S. For each simulated\ntime-step, LiZero chooses an action a by maximizing a combined upper confidence bound based on\naUCT, i.e.,\na = argmax\\limits_a [\\frac{W(s, a)}{N(s,a)} + C\\sqrt{\\frac{ln N(s)}{N(s,a)}} + C_{(s, a)}, U_{aUCT}(s, a)]\nIn practice, we can also use the maximum possible return $R_{max}/(1 - \\gamma)$ as an initial value of\nthe search. Next, at the final time-step of the simulation, the reward and state are computed by\na dynamics function. A new node, corresponding to the leaf state, is then added to the search\ntree. Finally, at the end of the simulation, the statistics along the trajectory are updated. Let G\nbe the accumulative (discounted) reward for state-action (s, a) from the simulation. We update the\nstatistics by:\n$Q^{N+1}(s, a) := \\frac{N(s, a) \\cdot Q^M(s, a) + G}{N(s, a) + 1}$\nN(s, a) := N(s, a) + 1\nIntuitively, at the start of task M's MCTS, there are not sufficient samples available, and thus\nUaUCT(s, a) serves as a tighter upper confidence bound than that resulted from the Monte Carlo\nactions sampling in M. As more samples are obtained during the search process, the standard UCT\nbound is expected to become tighter than UaUCT(s, a). The use of both bounds will ensure both\nefficient knowledge transfer and task-specific search. The pseudo-code of LiZero is provided in\nAppendix A.2.\nFor the proposed LiZero algorithm, we prove that it can result in accelerated convergence in MCTS.\nMore precisely, we analyze the sampling complexity for the learned Q-value estimate $Q^M(s, a)$ to\nconverge to the true value $Q^M(s, a)$, and demonstrate a strictly positive acceleration factor, com-\npared to the standard UCT. The results are summarized in the following theorem."}, {"title": "Theorem 3.4.", "content": "To ensure the convergence in a finite state-action space, $max_{(s,a)} |Q^M(s, a)\n- Q^M(s, a)| \\leq \\epsilon$ with probability 1 \u2013 8, the number of samples required by standard UCT is\n\u00d5$(\\frac{|S| |A|}{(1 - \\gamma)^3 \\epsilon^2} ln \\frac{1}{\\delta}) \\quad(4)$\nwhile the proposed LiZero algorithm requires:\n\u00d5 $(\\frac{1}{\\Gamma} (\\frac{|S| |A|}{(1 - \\gamma)^3 \\epsilon^2} ln \\frac{1}{\\delta})) \\quad(5)$\nwhere \u0393 > 1 is an acceleration factor given by\n$\\Gamma = \\frac{\\Sigma_{(s,a) \\in S_1 \\cup S_0} (\\Delta_M^{normalized}(s,a))^2}{\\Sigma_{(s,a) \\in S_1} (\\frac{1}{(\\Delta_M^{normalized}(s,a))^2}) + \\Sigma_{(s,a) \\in S_0} (\\Delta_M^{normalized}(s,a))^2}} \\quad(6)$\nand $S_1 = \\{(s,a) | \\exists i : U_{aUCT}(s,a) < Q^*_1(s,a^*)\\}$ is a state-action set where $U_{aUCT}$ of action\na is lower than the optimal return of $a^*$ in state s; and $\\Delta_M^{normalized}(s,a^*)$ is $\\frac{[Q^*_M(s, a^*) - Q^M(s,a)]}{Rmax}$ a\nnormalized advantage in the range of [0, 1]."}, {"title": "The theorem", "content": "shows that LiZero achieves a strictly improved acceleration \u0393 > 1 with a reduced\nsampling complexity (by 1/\u0393), in terms of ensuring convergence to the optimal estimates, i.e.,\n$max_{(s,a)} |Q^M(s, a) - Q^M(s,a)| \\leq \\epsilon$ with probability 1 \u2013 8. Since the normalized advantage\n$\\Delta_M^{normalized}(s,a^*)$ is in [0, 1], we have $1/\\Delta_M^{normalized}(s,a^*)$ \u2265 1. It is then easy to see that the value of I depends on\nthe cardinality $|S_1|$ and the normalized advantage $\\Delta_M^{normalized}(s,a^*)$. More precisely, LiZero achieves higher\nacceleration when (i) our aUCT makes more actions a less favorable, as $U_{aUCT}(s, a) < Q^*_M(s, a^*)$\nimplies that the sub-optimality of action a in s can be more easily determined due to aUCT; or (ii)\naUCT helps establish tighter bounds in cases with a smaller advantage, which naturally requires\nmore samples to distinguish the optimal actions \u2013 since \u0393 increases as the normalized advantage\nbecomes smaller for $(s, a) \\in S_1$, while being larger for $(s, a) \\in S_0$. These explain LiZero's abil-\nity to achieve much higher acceleration and lower sampling complexity, resulted from significantly\nreduced search spaces. We will evaluate this acceleration/speedup through experiments in Section 5."}, {"title": "4 Estimaing aUCT in Practice", "content": "To deploy LiZero in practice, we need to estimate aUCT, and in particular, the distance $d_{m,M_i}$\nbetween two MDPS. Sampling all transitions based on a uniform distribution U, as defined in Def-\ninition 3.1, is clearly too expensive. Thus, we develop efficient algorithms to estimate the distance\nmetric, from either available state-action samples using a data-driven approach or a parameterized\ndistance using a model-based (deep learning) approach. In this section, we also provide rigorous\nanalysis on the sampling complexity and error bounds of the proposed algorithms for distance es-\ntimate. The results allow us to readily implement LiZero in practical environments. We will late\nevaluate the performance of different distance estimaters in Section 5 and present the numerical\nresults.\nMore precisely, we first propose an algorithm to estimate the distance between two MDPs, M and\nM', using trajectory samples drawn from their search policies during MCTS and then making the\nuse of importance sampling to mitigate the bias. We will start with analyzing a stationary search\npolicy and then extend the results to a non-stationary policy update process, by modeling it as a\nfiltration - i.e., an increasing sequence of o-algebra. Next, since many practical problems are faced\nwith extremely large or even continuous action and state spaces (i.e., A and S), we further consider\na model-based approach by learning neural network approximations of the MDPs \u2013 denoted by\nparameter sets $\\phi$ and $\\phi'$, respectively \u2013 and then computing an upper bound on the distance using a\nparameterized distance of the neural network models. Analysis on sampling complexity and error\nbounds are provided as theorems in this section."}, {"title": "4.1 Sample-based Distance Estimate", "content": "During MCTS, transition samples are collected from the search to train a search policy \u03c0. It is\neasy to see that we can leverage these transition samples to estimate distance $d(M, M')$ between\ntwo MDPs, as long as we address the bias arising from gap between search policy and desired\nsampling distribution U in the distance definition $d(M, M')$. It also allows us to obtain a consistent\nestimate of MDP distance, without depending on the search policy that is updated during training.\nWe note that this bias can be addressed by importance sampling.\nLet $\\Delta X(s, a) = \\Delta R + \\kappa\\Delta P$ be the distance metric for a given state-action pair (s, a). We can\nrewrite the distance as $d(M, M') = E_{(s,a)~u}[\\Delta X(s, a)]$. We denote $p_u(s, a)$ as the probability (or\ndensity) of sampling (s, a) according to distribution U. Importance sampling implies:\n$E_{(s,a)~u}[\\Delta X(s, a)] = E_{(s,a)~\\pi} [\\frac{p_u(s, a)}{\\pi(s, a)} \\Delta X(s, a)] \\quad (7)$\nwhich can be readily computed from the collected transition samples, following the search policy\n\u03c0(s, \u03b1). Therefore, for a given set of samples $\\{(s_i,a_i), \\forall i = 1,...,n\\}$ collected from a search\npolicy \u03c0(s, a), we can estimate the distance by the empirical mean:\n$\\hat{d_1} = \\frac{1}{n} \\Sigma_{i=1}^n w_i \\Delta X(s_i, a_i), \\quad with w_i = \\frac{U(s_i, a_i)}{\\pi(s_i, a_i)} \\quad(8)$\nwhere $w_i$ is the importance sampling weight."}, {"title": "As long", "content": "as the state-action pairs with \u03c0(s, a) > 0 cover the support of U, this estimator satis-\nfies $E[\\hat{d_1}] = d(M, M')$, meaning it is unbiased. Let a be the \"coverage\" of policy \u03c0(s, a), i.e.,\n\u03c0(s, \u03b1) \u2265 a > 0, and $p_{max}$ be the maximum desired sampling probability. We summarize this\nresult in the following theorem and state the sampling complexity for estimator $\\hat{d_1}$ to e-converge to\nd(M, M')."}, {"title": "Theorem 4.1", "content": "(Sampling Complexity under Stationarity). Assume that for any (s,a), the reward\nplus transition difference is bounded, i.e., $\\Delta X(s,a) \\in [0,b]$, and that there exists a such that\n\u03c0(s, a) \u2265 a > 0. When n independent samples are used to estimate $\\hat{d_1}$, we have\n$Pr{\\mid\\hat{d_1} \u2013 d(M, M')\\mid \\leq \\epsilon} \\geq 1 \u2013 \\delta \\quad (9)$\nfor any \u03b4\u2208 (0, 1), if the number of samples satisfy\nn\u2265 (\\frac{1}{2\\epsilon^2}) (\\frac{p_{max}}{\\alpha})^2 b^2 ln(\\frac{2}{\\delta}) \\quad (10)$"}, {"title": "Thus", "content": "we obtain a convergence guarantee in the sense of arbitrarily high probability 1 \u2013 \u03b4 and\narbitrarily small error e, for estimating d(M, M') using $\\hat{d_1}$. $\\hat{d_1}$ is unbiased and ensures convergence\nto the true distance as the number of samples is sufficiently large.\nWe note that in many practical settings, the search policy \u03c0 would not stick to a stationary distribu-\ntion. In contrast, it is continuously updated in each iteration, resulting in a non-stationary sequence\nof policies over time, i.e., \u03c0\u2081, \u03c0\u2082,..., \u03c0k. Thus, the transition samples (sk, ak)'s we obtain at each\nstep k for estimating the distance d(M, M') are indeed drawn from a different \u03c0\u03ba. We cannot as-\nsume that the samples follow a stationary distribution (nor that {\u2206Xk} are i.i.d.) in importance\nsampling. To address this problem, we model the non-stationary process of policy updates as a fil-\ntration - i.e., an increasing sequence of o-algebra. In particular, we make the following assumption:\nat the k-th sampling step, the environment is forcibly reset to a predetermined policy $$\\pi_{\\epsilon}$ or inde-\npendently draws a state from an external memory. This assumption is reasonable because, in many\nepisodic learning scenarios, the environment is inherently divided into episodes: at the beginning of\neach episode, the state is reset to some initial distribution (e.g., the opening state in Atari games or\nthe initial pose in MuJoCo). This naturally results in the \u201creset\u201d assumption.\nIn this setup, the policy \u03c0\u03ba at step k is determined by information at step k \u2212 1 or earlier. Conse-\nquently, once \u03c0\u03ba is fixed, the distribution (marginal) of $$\\Delta X = \\frac{p_u(S_k,a_k)}{\\pi_{\\kappa}(S_k, a_k)}X(S_k, a_k)$ is also fixed.\nTherefore, we can establish the filtration {$F_k, k = 1, 2, . . .$} as follows:\n$\\mathcal{F}_{k-1} = \\sigma{\\pi_1,..., \\pi_k, (S_1, A_1), ..., (S_{k-1}, A_{k-1})}, \\quad(11)$\nwhere \u03c3{\u00b7} denotes the smallest o-algebra generated by the random elements. Thus, we obtain:\n$\\mathbb{E}[\\Delta X_k|\\mathcal{F}_{k-1}] = \\mathbb{E}_{(s_k,a_k)~\\pi_{\\kappa}} \\frac{p_U(S_k,a_k)}{\\pi_{\\kappa}(S_k, a_k)} \\Delta X(S_k, a_k)$"}, {"title": "Theorem 4.2", "content": "(Sampling Complexity under Non-Stationarity). Under the same conditions as Theo-\nrem 4.1 when n independent samples are used to estimate $\\hat{d_2}$, we have\n$Pr{\\mid\\hat{d_2} \u2013 d(M, M')\\mid \\leq \\epsilon} \\geq 1 \u2013 \\delta \\quad (13)$\nfor any \u03b4\u2208 (0, 1), if the number of samples satisfy\nn\u2265 2(\\frac{1}{\\epsilon^2})(\\frac{p_{max}}{\\alpha})^2 b^2 ln(\\frac{2}{\\delta}) (14)\nIt implies that more samples are needed considering the non-stationarity of policy update process\nfor distance estimate."}, {"title": "4.2 Model-based Distance Estimate", "content": "When the action and state spaces, A and S are very large or even continuous, employing the sample\nbased method will become increasingly expensive. Therefore, we propose a model-based approach\nto first approximate the dynamics of MDPs M and M' using two neural networks and then estimate\nd(M, M') based on the parameterized distance between the neural networks.\nTo this end, we need to establish a bound on d(M, M') using the distance between their neu-\nral network parameters. We use a neural network $$\\Psi_{\\theta} : S \u00d7 A \u2192 \u2206(S)$ to model the MDP\ndynamics. Many model-based learning algorithms, such as PILCO Deisenroth and Rasmussen\n[2011],MBPO Janner et al. [2019],PETS Chua et al. [2018],MuZero Schrittwieser et al. [2020a],\ncan be employed to learn the models of M and M'. Let \u03c6 be the neural network parameters of\nMDP M and \u03c6' be the neural network parameters of MDP M'. We define a distance in the parame-\nter space:\n$\\rho_{para} = \\rho(\\phi, \\phi') \\geq 0, \\quad (15)$\nwhere p is a distance or divergence measure in the parameter space, such as the l\u2082-norm, l\u2081-norm,\nor certain kernel distances. Intuitively, if \u03c6 and \u03c6' are very close, it indicates that the two neural\nnetworks are similar in fitting the dynamics of the respective MDPs. It suggests that the two MDPS\nshould have a small distance. To provide a more rigorous characterization of this concept, we\npresent the following theorem, which demonstrates that under proper assumptions, the distance\n$d_{para}$ based on neural network parameters can serve as an upper bound for the desired d(M, M').\nLet $\\kappa$ = $R_{max})/(1 \u2013 \u03b3)$ be a constant."}, {"title": "Theorem 4.3.", "content": "If the neural networks modeling M and M' satisfy the Lipschitz condition, i.e., there\nexists a constant L > 0 such that \u2200(s, a), $||$$\\Psi_{\\varphi}$(s, a) \u2013 $$\\Psi_{\\phi'}$(s, a)||$1 \u2264 L\u00b7 \u03c1(\u03c6, \u03c6'), then we have:\nd(M, M') \u2264 (1 + \u043a)Ldpara(M, Mi) (16)\nThe theorem indicates that by learning neural networks to model the MDP dynamics, we can es-\ntimate the distance d(M, M') by estimating the distance between the neural network parameters.\nThis parameterized distance can be computed for event continuous action and state spaces."}, {"title": "5 Evaluations", "content": "Our experiments evaluate LiZero on series of ten learning tasks with varying transition probabilities\nand rewards. We demonstrate LiZero's ability to transfer past knowledge in MCTS-based planning,\nresulting in significant convergence speedup (3~4x) and early reward improvement (about 31%\naverage improvement during the first half of learning process) in lifelong planning problems. All\nexperiments are conducted on a Linux machine with AMD EPYC 7513 32-Core Processor CPU and\nan NVIDIA RTX A6000 GPU, implemented in python3. All source codes are made available in the\nsupplementary material."}, {"title": "6 Conclusions", "content": "We study theoretically the transfer of past experience in MCTS-based lifelong planning and develop\na novel aUCT rule, depending on both Lipschitz continuity between tasks and the confidence of\nknowledge in Monte Carlo action sampling. The proposed aUCT is proven to provide positive\nacceleration in MCTS due to cross-task transfer and enable the development of a new lifelong MCTS\nalgorithm, namely LiZero. We also present efficient methods for online estimation of aUCT and\nprovide analysis on the sampling complexity and error bounds. LiZero is implemented on a non-\nstationary series of learning tasks with varying transition probabilities and rewards. It outperforms\nMCTS and lifelong RL baselines with 3~4x speed-up in solving new tasks and about 31% higher\nearly reward."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Proof of Theorem 3.2", "content": "Proof. Proof of Theorem 3.2 Since in the MCTS UCB algorithm, the estimated Q-values are ob-\ntained through multiple simulations, we need to analyze how the differences in simulation results\nbetween two MDPs affect the estimated Q-values.\nHowever, due to the randomness involved in the simulation process of the two MDPs:\n\u2022 Transition randomness: Due to different transition probabilities, the two MDPs may\nmove to different next states even when starting from the same state and action.\n\u2022 Action selection randomness: When using the UCB algorithm, action selection depends\non the current statistical information, which in turn relies on the past simulation results.\nThe randomness mentioned above makes it impossible for us to compare two independent random\nsimulation processes directly Qiao et al. [2024], Gao et al. [2024], Riis [2024], Chen et al. [2024],\nZhang et al. [2025], Yin [2025].\nTo eliminate the impact of randomness, we need to construct a coupled simulation process for the\ntwo MDPs in the same probability space, allowing for a direct comparison between them. Then we\nwill incorporate the additional errors caused by randomness into the analysis as error terms. For this\npurpose, we present the following assumptions."}, {"title": "Assumption A.1.", "content": "Let us temporarily assume that the actions selected in each simulation are the\nsame for the two MDPs.\n\u2022 Initial action consistency: The simulation starts from the same states\n\u2022 Action selection consistency: The same action a is chosen in each state.\nNote: This is a strong assumption and may not hold in practice. We will discuss its impact later.\nThus", "as": "n$\\Delta G = G_M - G_{M'} = \\Sigma_{t=0}^{T} \\gamma^t (R(s_t, a_t) - R'(s'_t, a_t)) \\quad(17)$\nWhere $s_M$ and $s_{M'}$ are the states of the two MDPs at step t, and $a_t$ is the action selected at step t.\nSo we can get\n$|Q^M_n (s, a) - Q^{M'}_n"}]}