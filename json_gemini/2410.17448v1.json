{"title": "IN CONTEXT LEARNING AND REASONING FOR SYMBOLIC REGRESSION WITH LARGE LANGUAGE MODELS", "authors": ["Samiha Sharlin", "Tyler R. Josephson"], "abstract": "Large Language Models (LLMs) are transformer-based machine learning models that have shown remarkable performance in tasks for which they were not explicitly trained. Here, we explore the potential of LLMs to perform symbolic regression - a machine-learning method for finding simple and accurate equations from datasets. We prompt GPT-4 to suggest expressions from data, which are then optimized and evaluated using external Python tools. These results are fed back to GPT-4, which proposes improved expressions while optimizing for complexity and loss. Using chain-of-thought prompting, we instruct GPT-4 to analyze the data, prior expressions, and the scientific context (expressed in natural language) for each problem before generating new expressions. We evaluated the workflow in rediscovery of five well-known scientific equations from experimental data, and on an additional dataset without a known equation. GPT-4 successfully rediscovered all five equations, and in general, performed better when prompted to use a scratchpad and consider scientific context. We also demonstrate how strategic prompting improves the model's performance and how the natural language interface simplifies integrating theory with data. Although this approach does not outperform established SR programs where target equations are more complex, LLMs can nonetheless iterate toward improved solutions while following instructions and incorporating scientific context in natural language.", "sections": [{"title": "1 Introduction", "content": "Data analysis is ubiquitous in all disciplines, where identifying correlations between variables is key to finding insights, informing conclusions, supporting hypotheses, or developing a new theory. For scientific data, we often aim to find expressions with few adjustable parameters explaining the data while ensuring that they align with theory. Symbolic regression is a machine learning technique that approaches equation-based scientific discovery \u2013 given a dataset, it searches through some \u201cspace of possible equations\u201d and identifies those that balance accuracy and simplicity. It is different from conventional regression methods, as symbolic regression infers the model structure from data rather than having a predetermined model structure.\nMathematically, symbolic regression is formulated as some form of optimization, not just of the constants in an equation, but as a search through \u201cequation space\" for optimal expressions. In this way, symbolic regression is a form of machine learning as data is received, an internal model is updated to match the data; when the model fits the data well and can make predictions about unseen data, the algorithm is said to have \u201clearned\" the underlying patterns in the data. In contrast to popular machine learning algorithms like neural networks, symbolic regression does not just only fit the constants in an equation but also finds functional forms that match the data."}, {"title": "2 Methods", "content": "In our initial tests, we asked GPT-4 to generate mathematical expressions from scientific data. We used a simple prompt to assess its capability and tested GPT-3.5-turbo and GPT-4 at varying temperatures (Figures 1). In all cases, the models produced expressions while \u201challucinating\u201d arbitrary coefficients. We revised the prompts to ask the LLMs to \u201cshow all steps\" to gain insight into how the model selects these values [63]. In response, the output either provided Python code for optimization or, at times, mathematical steps for optimization."}, {"title": "System design", "content": "Therefore, we designed a workflow (Figure 2) where we task GPT-4 with suggesting expressions without fitting constants, and subsequently, we optimize the coefficients of the expressions using SciPy outside the LLM. A Python class takes in expressions, optimizes them, then calculates their complexity and mean squared error (MSE). These results are stored as a dictionary, the text of which is passed back to GPT-4 in a subsequent prompt asking to suggest better expressions. We initially evaluated GPT-3.5-turbo, but found it less reliable in following instructions than GPT-4, more frequently generating expressions that didn't parse."}, {"title": "Prompt Engineering", "content": "We prototyped our system by testing its ability to rediscover the Langmuir adsorption isotherm ($q = c_1 *p/(c_2+p)$) from experimental data [69]. This enabled us to quickly identify major structural improvements to the workflow; we further tailored the prompts while testing on more difficult problems."}, {"title": "Removing bias", "content": "We aimed to make the workflow run smoothly without any human intervention and therefore, it was important to obtain machine-readable and precise output from GPT-4 to ensure the SciPy function runs without any errors. A simple way to illustrate the expected outcome was by providing examples in a few-shot context [31,82]. While this led to expressions matching the required syntax, we noticed the generated expressions resembled the examples we provided (Figure 3). While this taught the LLM correct syntax, it introduced bias that severely compromised the search."}, {"title": "Recording analysis in a scratchpad", "content": "Studies have shown that LLM performance can be improved by slowing down the model or breaking down its tasks into smaller steps [61,62]. One popular strategy is the \"scratch pad\" technique [64], which mimics how we solve problems by jotting down notes before presenting a final answer in exams. We implemented this in our workflow, instructing GPT-4 to generate responses in two parts: data analysis and observations in a scratch pad, followed by its conclusions. After implementing this technique, the model immediately generated higher-quality expressions (Figure 4)."}, {"title": "Preventing redundant expressions", "content": "GPT-4 often generated expressions like $x + c_1$ and $x - c_1$, implying they are different. However, since the constants are yet to be fitted, these expressions are the same from a symbolic regression perspective. While a computer algebra system like SymPy [83] could in principle catch some redundant expressions by simplification to a canonical form [6, 28], this wouldn't distinguish \u201cSR-similar\" expressions that become equivalent after fitting constants. Instead, we used prompt engineering to guide generation toward unique expressions: we added a note in the iteration prompt with examples showing how expressions in symbolic regression are similar before parameters are optimized. While this didn't completely resolve the issue, we did observe a reduction in occurrences, and at times, the scratchpad revealed GPT-4 correctly addressing this by taking these examples into account (see Figure 5)."}, {"title": "Avoiding uninteresting expressions", "content": "During the iterative runs, GPT-4 attempted to improve its accuracy by repeatedly adding linear terms to suggested expressions from previous iterations. To address this issue, we encouraged the model to explore diverse expressions in the prompt. Additionally, in cases involving datasets with multiple independent variables, GPT-4 sometimes recommended excluding variables that exhibited weak correlation with the overall dataset pattern. While this may be useful in some contexts, we wanted expressions that made use of all of the available data, so we explicitly instructed the use of all variables. Additional constraints we implemented included limiting the types of math operators to include and preventing generation of implicit functions, as shown in Figure 6."}, {"title": "Consider scientific context", "content": "Our primary motivation for building this system was to test whether providing scientific context could shape the expressions generated by the LLM. SR programs often successfully generate expressions that fit the data well and are simple, yet they may not adhere to scientific principles or be otherwise \u201cmeaningful.\" Yet scientists often have valuable insights into their domain that extend beyond these constraints, and they may not always"}, {"title": "3 Results", "content": "We evaluate our workflow using experimental datasets associated with meaningful scientific context. SR benchmarks often use synthetic data; we designed our tests around a benchmark [88] specifically curated for evaluating SR algorithms for scientific data. From here, we selected three experimental datasets from astronomical observations (Bode's Law, Hubble's Law, and Kepler's Law). To these, we added two experimental chemistry datasets: Langmuir and Dual-site Langmuir adsorption isotherm models [9].\nWe also tested our workflow using a dataset on friction losses in pipe flow [89]. This phenomena doesn't have an established target model, and has been the subject of prior study by other SR programs [6]. We included the whole dataset for each problem in the prompt to GPT-4 at the start and in each iteration. Because the Hubble and Leavitt's datasets had an unreasonable number of digits following each entry, we rounded to 3 decimal places when sending data to GPT-4 but used the original dataset when optimizing constants in SciPy. This reduced the tokens (and cost) of running GPT-4. We pass the entire dataset SR is different than other machine learning models, as we feed the entire dataset for training where the test is the output expression [88].\nWe conducted eight sets of experiments on each of the six datasets, running each 5 times to evaluate robustness. In four of the experiments, we used basic binary operators (+, -, *, \u00f7), incorporating 'sqrt' for Kepler's data and '^' and 'exp' for Bode's data. Empirical relations often involve field-specific operators, so the equation search should account for this. We refer to this set of tests as an \"easy search\" because the space of possible equations is constrained to that generated by the operators in our dataset. In addition, we conducted four further tests adding common unary operators (sqrt, log, exp, square, cube) alongside the basic ones to evaluate a more difficult search.\nTemperature is a hyperparameter used in stochastic models like LLMs to regulate the randomness of the model output [90, 91]. It adjusts the probabilities of the predicted words in the softmax output layer of the model. Lowering temperature favors words with higher probability, so when the model randomly samples the next word from the probability distribution, it will be more likely to choose a more predictable response. We tested the Langmuir dataset with five different temperature settings (0, 0.3, 0.5, 0.7, 1) and found 0.7 to be performing the best, which we later used for the rest of the datasets."}, {"title": "Hubble", "content": "Hubble's Law is represented by the simple equation $y = c1*x$, which is often a first guess when few operators are made available. However, including context plays a role; this is especially apparent in the \u201chard\u201d search, where removing the scratchpad for reasoning or the context inhibited performance. This dataset is particularly noisy, as well; including the noisy data as context actually inhibits the search."}, {"title": "Bode", "content": "The results for Bode's Law indicate that, similar to other SR programs [88], GPT-4 encounters difficulty accurately rediscovering the target model expression. Curiously, it performs best when the context is excluded and worst when the data is excluded. This suggests that the context we provided was counterproductive for the search, and purely reasoning about the data led greater success. We also note that GPT-4 finds the target model as $- C\u2081 \u00d7 exp(C\u2082 \u00d7 x1)+C3$ when all operators are provided, which is a symbolically-equivalent way of expressing Bode's Law, though lacking the interpretability of the original form."}, {"title": "Kepler", "content": "We find strong evidence that the prompt has triggered GPT-4's memorization of Kepler's Law: the scratchpad reveals GPT-4 associates the variable names in the context with Kepler's Law, and it not only guesses the right answer in the first iteration, it names Kepler's Law in its justification. Perhaps because this relationship is routinely taught in high school and college physics courses, and thus likely to be more represented in GPT-4's training data than the other relationships. The hard search for Kepler's Law with all tools on has led GPT-4 to explore more complex expressions since the target model complexity is only 5 . GPT-4 also finds the target model in the form $\\frac{C1}{x1^2}$, with c2 left as a constant for optimization instead of 3. However, SciPy optimizes and fits c2 with to a floating point power (\u2248 1.5). We did not consider this to be a rediscovery of the target model as such expressions are not dimensionally consistent."}, {"title": "Langmuir", "content": "Langmuir's model is more obscure, and was almost never guessed in the first round. In easier searches, GPT-4 consistently found it within 15 iterations; inclusion of context and data seemed to improve performance. However, the hard search was much more challenging, with Langmuir only discovered once, in the case with all tools on."}, {"title": "Dual-site Langmuir", "content": "The dual-site Langmuir model is particularly challenging for SR, because the target model does not significantly fit the data much better than many shorter expressions. In fact, GPT-4 found one expression that fits the data more accurately at complexity 11 . However extrapolation shows this model is not theoretically correct .\nThis dataset was previously explored in literature [9,28] with three SR algorithms: Bayesian-based SR (BMS) [6], genetic programming-based SR (PySR) [88], and mixed-integer non-linear programming-based SR [92]. Because this expression is longer, we extended the run to 50 iterations, and dual-site Langmuir was still not found for any of the experimental settings with easy search. We modified the feedback loop passed into GPT-4 for this dataset to send more accurate and longer expressions in lieu of passing the entire Pareto front (keeping the top five expressions based on MSE in the loop). We found that when using basic operators and context, GPT-4 was getting close to finding the target model upon running 15 iterations with this feedback loop. To investigate this further, we ran with basic operators (for runs with and without context) for 50 iterations and found the target model in 1/5 runs for both tests, indicating that more iterations were needed. Our prompts instruct GPT-4 to generate shorter expressions, so it outputs simpler expressions with reasonable MSEs; parsimony is a common goal in SR. But when targeting models like dual-site Langmuir, these instructions may have been a liability, and we needed to adjust the prompt and feedback sent to GPT-4 to allow for the exploration of more complex expressions."}, {"title": "Nikuradse", "content": "Finally, to test the scalability of the approach to larger datasets and to test a problem without a known target model, we evaluated the Nikuradse dataset, which is experimental data for turbulent friction in rough pipes conducted by Johann Nikuradse in the early 1930s [89] The Nikuradse dataset contains over 350 measurements; including the whole dataset in our prompts to GPT-4 exceeded the token limit. Even with long context windows, analyzing large datasets would be expensive, since each iteration is more costly, and because generating longer expressions requires more iterations. So, we developed a cost-saving scheme: only send a portion of the data in the prompt to GPT-4, while fitting and evaluating the generated expressions using the whole dataset .\nSince GPT-4 generated longer and more complex expressions for this dataset (seven or more fitted constants were common), numerical optimizing was also more challenging. We found the optimized coefficients varied slightly due to stochasticity in the basin-hopping algorithm; this could lead to inaccurate sorting of the generated expressions. So, we optimized constants ten times for each expression, then selected that with the lowest mean absolute error. Additionally, we stored the fitted parameters in the feedback loop to assess the expressions sent to GPT-4 for feedback. To manage the context window and encourage longer expressions, we used the modified feedback loop that proved modestly successful for the dual-site Langmuir dataset.\nThough there is no definitive target model for the Nikuradse data, we compared other candidate model expressions from different SR programs in the literature [6]. We modified the basic prompt to encourage GPT-4 to explore longer expressions and also tested the effect of \u201cprodding\" GPT-4 by sharing just the MSE achieved by a literature model (without leaking the model), and challenging it to do better. We conducted six experiments on Nikuradse data, with three slightly different versions of the prompt (P1,P2, and P3) and two sets (S1 and S2) of data points one (S1) with 36 (10%) random data points and the second (S2) with another random 36 (20%) data points. Overall, feeding in more data generated better-fitted and more complex expressions. Figure 9 shows the top expressions with the lowest MAE out of the six experiments that were explored with binary math operators (+, -, \u00f7, \u00d7 and ^).\nWe found an optimal expression from GPT-4 with a complexity of 41 and an MAE of 0.01086 . The MAE is approximately three times worse than the top-performing model identified by the Bayesian Machine Scientist (BMS). BMS uses Markov Chain Monte Carlo (MCMC)-based SR and discovered a more accurate expression at complexity 37 with an MAE of 0.00392. However unlike GPT-4, BMS evaluates thousands of expressions in more than 18000 Monte Carlo (MC) steps to identify this expression. Our GPT-4 model uses a portion of the data to find the best expression from only a pool of (50*3 + 3) 153 expressions. We additionally assessed BMS on its default move probabilities and using 40 temperatures for parallel tempering as mentioned in the paper, and ran it for 153 MC steps with Nikuradse data; it generated a constant function, and upon running it longer (1000 MC steps), it suggests an expression with MAE 0.13436 and complexity 25. Thus, we can see that GPT-4's incremental suggestions for new expressions (at least in the initial stages of the search) are of much higher quality than those by BMS. However, BMS is far more efficient in terms of compute, it generates more expressions at a far lower cost. Using GPT-4 to generate 18000 trial expressions would\""}, {"title": "4 Discussion and Conclusion", "content": "SR programs that optimize for speed aim to generate expressions quickly. In contrast, our proposed method emphasizes informed optimization, leveraging contextual information more effectively. The clearest cases of \"leveraging the context\" occurred when GPT-4's first guess included the target model among three expressions. But even when the search took longer, we found incorporation of the context, data, and scratchpad to be helpful for improving the quality of generated expressions. However, including noisy data in the context sometimes undermined the search, as did including lower-quality scientific context. Nonetheless, this comes with great computational expense - especially since large datasets and long reasoning chains require so many tokens.\nIn general, we found natural language to be a rather clumsy interface for controlling expression length. Different prompts and feedback mechanisms led to distributions of expressions with varying length. Classical approaches that incorporate expression length into measures of fitness or score are certainly more precise for controlling length, even if expression length is an imperfect measure of parsimony and meaningfulness in SR [93]. We ran separate, focused tests to evaluate the effect of prompting on expression length, and GPT-4 did not obey instructions that requested, for example, \"expressions of length 17.\" Even with a scratchpad available, it failed to both measure complexity of an expression accurately and to generate expressions of the target length. The LLM is better-suited for creative generation, while deterministic Python tools are more effective (and cheaper) at procedural tasks such as counting complexity.\nTesting our approach on equation rediscovery using GPT-4 invariably involves a form of \"test set leakage\" since these expressions are on Wikipedia and countless additional Internet sources. Indeed, the data are publicly available, as well, though we think it unlikely that LLMs trained on natural language would devote a sufficient fraction of their network to memorize these datasets. We found strong evidence of this because of our scratchpad implementation, which revealed when it was thinking of Kepler's law before seeing the data. Nonetheless, we found evaluating the model outputs to be informative.\nMoreover, we foresee a use case for scientists trying to solve a mystery about their data while having a great deal of context to potentially include. This context may include experimental details, instrument specifications, and literature, and even a \u201cmemorized\" explanation of the data by an LLM or retrieved from context in a retrieval-augmented generation scheme [94] would be valuable. A true blind test would be to rediscover a novel scientific law using an LLM with a knowledge cutoff predating the seminal publication.\nThere are two ways to implement in-context learning in LLMs through prompts. One is the Few-Shot prompting method, where we condition the model on a few specific examples related to the task that helps the model understand and perform the task more accurately. The other is the Zero-Shot prompting method, where the output relies solely on template-based prompts without specific task examples, allowing the model to infer how to handle the task from general instructions. Our approach does not provide examples of symbolic regression procedures within the prompts. Instead, we guide the model to engage in freeform chain-of-thought reasoning about the context as it prepares suggestions for new equations.\nBiasing the search space in a standard SR program can be challenging and requires advanced software and coding skills. Interdisciplinary work demands significant time and resources from researchers. Natural language interfaces in LLMs can help reduce some of these barriers by making program execution more accessible, even without expertise in software development. Well-crafted prompts empower language models to perform diverse tasks, allowing them to"}]}