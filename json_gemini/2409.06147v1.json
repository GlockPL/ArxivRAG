{"title": "MULTICLASS ARRHYTHMIA CLASSIFICATION USING SMARTWATCH\nPHOTOPLETHYSMOGRAPHY SIGNALS COLLECTED IN REAL-LIFE SETTINGS", "authors": ["Dong Han", "Jihye Moon", "Lu\u00eds Roberto Mercado D\u00edaz", "Darren Chen", "Devan\nWilliams", "Eric Y. Ding", "Khanh-Van Tran", "David D. McManus", "Ki H. Chon"], "abstract": "Most deep learning models of multiclass arrhythmia\nclassification are tested on fingertip photoplethysmographic\n(PPG) data, which has higher signal-to-noise ratios compared\nto smartwatch-derived PPG, and the best reported sensitivity\nvalue for premature atrial/ventricular contraction (PAC/PVC)\ndetection is only 75%. To improve upon PAC/PVC detec-\ntion sensitivity while maintaining high AF detection, we use\nmulti-modal data which incorporates 1D PPG, accelerome-\nters, and heart rate data as the inputs to a computationally ef-\nficient 1D bi-directional Gated Recurrent Unit (1D-Bi-GRU)\nmodel to detect three arrhythmia classes. We used motion-\nartifact prone smartwatch PPG data from the NIH-funded\nPulsewatch clinical trial. Our multimodal model tested on\n72 subjects achieved an unprecedented 83% sensitivity for\nPAC/PVC detection while maintaining a high accuracy of\n97.31% for AF detection. These results outperformed the best\nstate-of-the-art model by 20.81% for PAC/PVC and 2.55%\nfor AF detection even while our model was computationally\nmore efficient (14 times lighter and 2.7 faster).\nIndex Terms- Atrial fibrillation, premature atrial con-\ntraction, premature ventricular contraction, deep learning,\nwearable device", "sections": [{"title": "I. INTRODUCTION", "content": "Atrial fibrillation (AF) is the most prevalent malignant car-\ndiac dysrhythmia. The prevalence of AF in the U.S. is ex-\npected to rise to 12.1 million (projections to 3.4% of the total\npopulation) in 2030 as the population ages [1]. Long-term\nmonitoring for AF is effective for incident AF detection, as\nmost cases of early stages of AF are brief and intermittent\n[2]. However, existing conventional gold standard methods\nfor continuous AF monitoring via ECG suffer from poor pa-\ntient acceptability and low long-term adherence, largely due\nto their reliance on gel electrodes and multiple leads. An al-\nternate solution for continuous monitoring is via a smartwatch\nwith photoplethysmography (PPG), which provides a conve-\nnient solution for continuous monitoring of AF [3]. Recent\nworks using smartwatches have shown accurate AF detection\n[4, 5, 6], but most algorithms are not able to discriminate pre-\npremature atrial and ventricular contractions (PAC/PVC) [7]. It\nis important to distinguish PAC and PVC, as frequent occur-\nrences of these rhythms can mimic AF dynamics, thereby re-\nducing accuracy of AF detection [8].\nWhile it is relatively easy to detect PAC and PVC in ECG\nsignals [9], it is rather difficult to detect these rhythms in\nPPG due to inexact signature waveforms representing these\narrhythmias [7]. Another challenge with PPG for arrhyth-\nmia detection is that motion noise artifacts are a significant\nissue in smartwatch data, as they are known to distort the\nPPG waveforms and mimic irregular dynamics seen in AF\n[8]. To combat these two issues, more comprehensive smart-\nwatch databases are needed to account for diverse sets of mo-\ntion artifacts and to have a sufficient number of PAC and PVC\ncases to train deep learning methods. However, long duration\nrecordings of smartwatch PPG data require time-consuming\nadjudication of AF and PAC/PVC rhythms, using simultane-\nous recordings of ECG signals as the gold standard.\nA further complication of using a smartwatch for ar-\nrhythmia detection is that most prior work relied on data\ncollected in a controlled environment (typically with min-\nimum or unrealistic motion artifacts) or PPG data from a\nfingertip, which exhibits higher signal-to-noise ratios (SNR)\nand less motion noise than does smartwatch PPG data [10].\nOne of the first works on PAC/PVC detection from a fin-\ngertip PPG is by Poh et al. [11], which reported 72.2%\nsensitivity and 85.0% positive predictive value (PPV) using\na 1D-DenseNets model. However, testing results were based\non the same training subjects instead of using independent\nsubjects from the training datasets. A work by Liu et al. [12]\nreported subject-independent testing with 75.4% sensitivity\nand 82.7% PPV for PAC/PVC detection using a 1D VGG-16\nmodel on motion-free fingertip PPG data. Since these finger-\ntip PPG data were recorded in clinics with minimal motion\nartifacts, it is not likely that the same results can be achieved\nwhen real-life smartwatch PPG data are used.\nTo address the current limitations of accurate PAC/PVC\ndetection from PPG, we recently conducted an NIH-funded\nstudy called \"Pulsewatch\" [13] which continuously recorded"}, {"title": "II. DATASET DESCRIPTION", "content": "PPG data from older stroke survivors in their homes via\nsmartwatches with simultaneous reference ECG for 14 days.\nIn this work, we aim to improve the sensitivity of PAC/PVC\ndetection, while maintaining a high accuracy on AF de-\ntection using multi-modal data, including PPG, heart rates\n(HRs), and accelerometer (ACC), with a computationally ef-\nficient deep learning model. We also aim to demonstrate the\ngeneralizability of our deep learning model by using many\nindependent testing subjects and many PPG segments.\nA. Pulsewatch dataset\nWe recently conducted an NIH-funded clinical trial called\nPulsewatch\u00b9 to determine the feasibility of detecting AF us-\ning smartwatches in real-life conditions [13]. Participants\n(n=106) in the Pulsewatch clinical trial continuously wore\nthe smartwatch system (which also included a smartphone for\ndata collection) with a reference ECG chest patch for 14 days\nin their everyday lives. Demographic and medical history in-\nformation of the recruited participants (aged \u226550 with a his-\ntory of ischemic stroke) can be found in [13]. Formal ethi-\ncal approval for this study was obtained from the University\nof Massachusetts Medical School Institutional Review Board\n(approval numbers H00016067 and H00009953). Written in-\nformed consent was collected from all patient participants.\nThe smartwatch system (Samsung Gear S3 or Galaxy Watch\n3, Samsung, San Jose, CA, USA) recorded single-channel\nPPG and accelerometer signals at 50 Hz [14]. Arrhythmias\nwere adjudicated in each 30-second segment by three experts\n[15] using the aligned single-channel ECG (Cardea SOLO,\nCardiac Insight Inc., Bellevue, WA, USA) that was sampled\nat 250 Hz.\nWe used our previously developed automated motion and\nnoise detection algorithm [16] on the Pulsewatch data, and\nonly those segments that were determined to be relatively\nclean PPG segments (\u22645 seconds of motion noise) were used\nfor subsequent multiclass arrhythmia classification. This tol-\nerance of \u22645 sec of motion noise was used primarily to in-\ncrease the number of usable PPG data segments for arrhyth-\nmia detection, as we found in our previous study that this did\nnot cause many false AF alerts [8, 16].\nB. Training and testing datasets\nThe number of segments with AF and PAC/PVC differ among\nsubjects, with some subjects having many instances of these\nrhythms while others had few to none. Hence, how to deter-\nmine which subjects to use for training and which for subject-\nindependent testing became a challenging issue for address-\ning the generalizability of the proposed method. Therefore,"}, {"title": "III. METHODS", "content": "While most prior works used complicated deep learning mod-\nels for multiclass arrhythmia classification (Poh et al.'s 1D-\nDenseNets [11], Liu et al.'s 1D VGG-16 [12], and Chen et\nal.'s 2D-DenseNets [17]), a recent work [18] has shown that\na simpler model structure such as the 1D bi-directional Gated\nRecurrent Unit (1D-bi-GRU) can provide accurate detection\nof motion artifacts in PPG data. Furthermore, the same model\nstructure trained on 1D PPG signals worked moderately better\nthan did a 2D time-frequency spectrogram (TFS) [18]. Hence,\nwe used a 1D time series as the input and implemented the\n1D-Bi-GRU model as described in [18]. This model is par-\nticularly well suited for capturing temporal dependencies in\nPPG signals, which are critical for distinguishing subtle dy-\nnamics present in cardiac arrhythmias.\nWe also added HR as an input because our prior work has\nshown that cardiac arrhythmias can be accurately discrimi-\nnated using HRs [19]. Moreover, ACC signals were used to\ntrain the network as motion artifacts could occur so that the\nnetwork learned to ignore those contaminated PPG segments.\nA. Signal Processing of the Time-Series Data\n1). 1D Time Series Data Preparation\nThe left, middle, and right top rows of Fig.1 show represen-\ntative ECG signals for normal sinus rhythm (NSR), AF, and\nPAC/PVC, respectively. Row (2) of Fig.1 shows the corre-\nsponding and simultaneously measured PPG, filtered with a\n6th-order Butterworth bandpass IIR filter (0.5 to 20 Hz) [20].\nEach filtered PPG was normalized to [0, 1] based on each seg-\nment's minimum and maximum values. The third row shows"}, {"title": "B. Machine Learning Model Design: 1D-Bi-GRU Model", "content": "As described in [18] and shown in Fig.2, our input time se-\nries has a dimension of (L, d) (L=1,500 samples in our case,\nwhile d is the number of input channels). The first layer is a\n1D convolutional neural network (CNN) to embed the input\ntime series with 4d filters with a kernel size of 5, a stride size\nof 1, and a padding size of 2 to ensure the output dimension\nis (L, 4d). The second layer is a bi-GRU layer, which com-\nbines the outputs of two GRU networks (with 128 units each)\nthat process the input-embedded information in the opposite\ndirections, allowing for each sample to consider both preced-\ning and proceeding samples. A batch normalization is then\napplied, followed by a dropout of 20% to avoid overfitting.\nLastly, a dense layer combines the output of the previous lay-\ners (L, 256) into a dimension of (L, 3) for predicting three\nclasses (0=NSR, 1=AF, and 2=PAC/PVC)."}, {"title": "C. Machine Learning Model Training Process", "content": "As shown in Table 1, the number of NSR segments is 3 and\n5 times more than AF and PAC/PVC segments, respectively.\nTo prevent over-fitting, up-sampling of the minority classes\nwas implemented in the training and validation sets to ensure\nunbiased performance in the testing data.\nA batch size of 32 was used, as it showed a faster and more\nstable training validation process compared to a batch size\nof 512. The cross-entropy loss function was used for three-"}, {"title": "D. Evaluation Metrics", "content": "For the performance evaluation of the proposed and other\ncompared methods, we calculated five key metrics: sensitivity\n(sens.), specificity (spec.), precision (prec.), negative predic-\ntive value (NPV), and accuracy (accu.) in keeping with other\npublications [11, 12, 19]. We performed subject-independent\ntesting as detailed in section B."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "A. Effectiveness of using HR as an Input for Multiclass\nArrhythmia Classification\nTable 2 shows the comparison of two other methods versus\nour proposed 1D-Bi-GRU model with different combinations\nof inputs for the multiclass classification results. As shown,\nthe best performance metrics were with our proposed model\nwith four time series (1D PPG, HR, ACC, and \"zoomed-in\"\nHR) as the inputs. To date, the best reported sensitivity for\nPAC/PVC detection is 75.4% by Liu et al. [12], and we im-\nplemented Liu et al. and Chen et al.'s models, retrained them\nusing the same data, and reported the results in the first and\nsecond rows of Table 2, for proper comparison. The best sen-\nsitivity for PAC/PVC after retraining Liu et al.'s model [12]\nis only 63%, as shown in the first row of Table 2. However,\nour approach resulted in 84% sensitivity for PAC/PVC detec-\ntion. Note that HR information is especially needed for higher\nperformance metrics; when only PPG was used as the input\nsignal, the PAC/PVC detection sensitivity was only 67%. It\nincreased to >81% when HR data are added, as shown in the\nbottom two rows of Table 2.\nIt should be noted that the approach by Liu et al. [12] used\nfingertip PPG data which has a higher SNR, but the method"}, {"title": "B. Efficiency in Computational Cost using HR as an In-put", "content": "Our model is computationally efficient, potentially allowing\nfor real-time classification of cardiac arrhythmias on wear-\nable devices. Table 3 shows that our best proposed model\nhas only 120,224 parameters, which is only 1/14th of the\nnumber of parameters used in Liu's model [12]. Our best\nproposed model only uses 0.89 billion floating-point opera-\ntions per second (GFLOPs where G=giga/billion), which is\n2.7 times faster than Liu's model [12]."}, {"title": "V. CONCLUSION", "content": "In this work, we proposed a computationally efficient 1D-\nBi-GRU model to classify NSR, AF, and PAC/PVC using\nsmartwatch PPG data collected in real-life settings. Our\nmodel showed accurate AF classification as well as the high-\nest PAC/PVC sensitivity value ever reported in the literature,\nbesting even those studies which used higher SNR-based\nPPG data collection modalities (e.g., fingertip PPG). Our bet-\nter performance metrics were mainly due to the addition of\nHR as an input to the 1D-Bi-GRU model."}]}