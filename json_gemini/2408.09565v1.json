{"title": "Grammatical Error Feedback: An Implicit Evaluation Approach", "authors": ["Stefano Bann\u00f2", "Kate Knill", "Mark J. F. Gales"], "abstract": "Grammatical feedback is crucial for consolidating second language (L2) learning. Most research in computer-assisted language learning has focused on feedback through grammatical error correction (GEC) systems, rather than examining more holistic feedback that may be more useful for learners. This holistic feedback will be referred to as grammatical error feedback (GEF). In this paper, we present a novel implicit evaluation approach to GEF that eliminates the need for manual feedback annotations. Our method adopts a grammatical lineup approach where the task is to pair feedback and essay representations from a set of possible alternatives. This matching process can be performed by appropriately prompting a large language model (LLM). An important aspect of this process, explored here, is the form of the lineup, i.e., the selection of foils. This paper exploits this framework to examine the quality and need for GEC to generate feedback, as well as the system used to generate feedback, using essays from the Cambridge Learner Corpus.", "sections": [{"title": "Introduction", "content": "In natural language processing, computer-assisted language learning (CALL) is a well-established research area (Chapelle, 2001). An important part of this process is to give feedback on the use of grammar by a learner. This feedback has usually been in the form of grammatical error detection (GED) (Leacock et al., 2014), and grammatical error correction (GEC) (Wang et al., 2021; Bryant et al., 2023), and the latter has been the subject of four shared tasks over the past 15 years. While highlighting errors and providing a grammatically corrected text is beneficial for second language (L2) learners (Hyland and Hyland, 2006), it is even more valuable to offer specific feedback that comments on grammatical errors, explains them, and provides suggestions for improvement. As shown in Ellis et al. (2006), giving metalinguistic explanations (i.e., explicit feedback) helps learners understand their errors and learn how to avoid them in the future in a more effective way than providing mere recasts (i.e., implicit feedback). The recent advent of large language models (LLMs) has significantly altered the world of CALL. LLMs have been investigated for L2 holistic (Yancey et al., 2023) and analytic (Bann\u00f2 et al., 2024) assessment as well as GEC (Coyne et al., 2023b; Fang et al., 2023; Wu et al., 2023; Katinskaia and Yangarber, 2024). An interesting aspect of natural language generation (NLG) models, such as LLMs, is that they offer the opportunity to automatically generate free-form feedback for candidates, which will be referred to as grammatical error feedback (GEF) hereafter. GEF differs from GED and GEC in that it is not meant to provide feedback on all errors made, but rather provide a more holistic and compact interpretation of the errors being made, along with explanations of the error and suggested improvements. One of the challenges for developing GEF systems is that assessment is difficult. For GED and GEC, it is possible to generate reference transcriptions against which systems can be assessed. Given the vast array of appropriate grammatical feedback that can be generated for an essay, it is not feasible to generate a set of reference feedback that sufficiently covers the complete spectrum of possible valid output. To address this problem in this work, we introduce a novel implicit evaluation approach to GEF that does not require manual annotations. Rather than explicitly evaluating the feedback, the proposed scheme adopts a grammatical lineup approach similar to voice lineups in forensic speaker recognition (McDougall, 2013). The task is then to appropriately pair feedback and essay from the possible set of comparisons. An important aspect of any lineup is the generation of the foils, i.e., the alternatives to the matched"}, {"title": "Related work", "content": "For grammatical error annotation in CALL, the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017) has become a widely-used tool for extracting and categorising grammatical error information. Specifically, it extracts error edits from parallel original and corrected sentences. Examples of ERRANT edit labels are R: VERB: FORM, which indicates the need to replace an incorrect verb form, and M: DET, which indicates a missing determiner. The prefix R: stands for replace, M: for missing, and U: for unnecessary (see Section 4 for an example). ERRANT labels error types as OTHER when edits do not fall under any other category. A large part of errors labelled as OTHER are paraphrases. ERRANT has become a standard approach for GEC, but its output information, included in a so-called M\u00b2 file, is not always easily readable, and, despite categorising errors based on parts of speech, it does not provide clear natural-language-based descriptions of errors or motivate corrections in a comprehensible way for learners and teachers. Other previous works on feedback generation followed the organisation of a shared task on feedback comment generation for language learning called FCG GenChal 2022 (Nagata et al., 2021, 2023). The main limitations of this task were that it only targeted preposition errors and only leveraged incorrect sentences tagged with span-based error annotations as inputs, hence without their respective corrections. Participants such as Behzad et al. (2023), Stahl and Wachsmuth (2023), Jimichi et al. (2023), Ueda and Komachi (2023), and Koyama et al. (2023) tackled this problem by fine-tuning systems based on T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and GPT-2 (Radford et al., 2019). In the context of this shared task, Coyne (2023) and Coyne et al. (2023a) developed a bipartite typology for L2 feedback which includes abstract tags (e.g., idiom, language transfer, praise, etc.) and grammatical pattern tags (e.g., conditional, possessive, relative clause, etc.). A recent study explored the application of LLMS for grammatical error explanation (Song et al., 2024). The study employed a two-step pipeline that first used fine-tuned and prompted LLMs to extract structured atomic token edits. Subsequently, GPT-4 (OpenAI, 2024) was prompted to explain each edit. Despite its novelty and appeal, this approach has several limitations, i.e., it a) can only be applied to sentences but not longer compositions; b) only consists of four operation edit-level types, namely insert, delete, replace, and relocate; and c) needs manual annotators to check that the error explanations are correct, which is generally a costly operation. It is also worth mentioning the recent work by Stahl et al. (2024) on joint essay scoring and feedback generation, although it focuses on L1 essays."}, {"title": "Grammatical error feedback", "content": "The aim of grammatical error feedback (GEF) is to supply holistic feedback to the user on their use of grammar. This feedback should summarise the forms of grammatical error being made in easy to interpret, informative, form, not simply listing all the errors being made by the candidate in their essay (see Appendix B for an example). As the form of feedback will be free-form in nature, NLG conditioned on the essay is used with the associated flexibility of response. The standard approach to this, and the one adopted in this work, is the use of LLMs with the appropriate prompt and inputs."}, {"title": "Grammatical lineup", "content": "As previously discussed, generating a set of references of free-form feedback for an essay is highly challenging given the wide-range of possible forms of feedback possible. Rather than adopting an explicit error measure, as done in GED and GEC, for GEF an implicit performance metric based on matching against a grammatical lineup is proposed. This requires no manual references, but does require the generation of a suitable lineup, i.e., a set of foils, that enables the assessment of GEF quality. The overall framework is shown in Figure 2. The generation of a suitable set of foils is the most important choice in the lineup. For this work to ensure that there are no biases introduced by the essay content, sets of essays are generated based on different levels of manual correction of the learner essay. Here, five different versions of the essay were used:\n\u2022 original: the original essay as written by the learner;\n\u2022 25% corrected: a version where only 25% of the errors are randomly corrected;\n\u2022 50% corrected: a version where only 50% of the errors are randomly corrected;\n\u2022 75% corrected: a version where only 75% of the errors are randomly corrected;\n\u2022 100% corrected: a version where all errors are corrected.\nIt is then possible to use this set of essays as the basis for both the generation of the parade, i.e. the foils, as well as the feedback. The task will then be to match both the feedback and parade entry from the same essay. The quality of the matching process, indicated by the \"?\" blocks in Figure 2, is important for the assessment process. Rather than asking the system to select over a range, it uses a simple binary selection process of asking whether the feedback and essay representation match. This is motivated by the use of LLMs for comparative rather than absolute assessment (Liusie et al., 2024). Another important aspect of designing the parade is to ensure that the quality of the feedback is being assessed rather than simply matching lexical entries in the parade and the feedback. Good feedback is expected to make use of examples of grammatical errors in the essay. This can easily bias the process as the essays and feedback do not contain the same grammatical errors by design. Two forms of parade are investigated in this work. Essay-type-based evaluation: In this configuration, we ask the LLM to decide whether a given feedback response matches a given essay using a lineup that includes all essay versions. Each essay version is fed into the system one at a time, and the LLM should simply output \"yes\" or \"no\". For each feedback-essay pair, we extract the probability of \"yes\" being the first predicted token. The prompt can be found in Appendix A, and this type of evaluation is illustrated in Figure 3."}, {"title": "Data", "content": "The Cambridge Learner Corpus (CLC) is an ever-growing collection of L2 written data obtained from Cambridge English exams. According to O'Keeffe and Mark (2017), as of 2017, it included 266,600 examination scripts and 143 L1 backgrounds collected in the period between 1993 and 2012. The corpus includes data from lower to advanced proficiency levels annotated according to the Common European Framework of Reference (CEFR) (Council of Europe, 2001) and features manual annotations with information about errors according to a taxonomy of about 80 error types described in Nicholls (2003). An example drawn from the data in XML format is the following:\nI am looking forward to <NS type=\u201cFV\u201d>\nhear hearing </NS> from you.\nin which FV indicates a verb form error and hear is corrected to hearing.\nFor our experiments, we extracted 300 essays written by learners representing 28 L1 backgrounds and 40 nationalities. To ensure representative data across proficiency levels, we selected 50 essays per proficiency level ranging from Al to C2."}, {"title": "Experimental setup", "content": "As should be clear at this point, we can identify three major steps in our proposed pipeline: GEC, GEF generation, and GEF discrimination.\nGEC: In addition to manual corrections, we use automatic corrections obtained employing GEC-ToR (Omelianchuk et al., 2020), Gramformer \u2013 which is a publicly available T5-based GEC system and GPT-4o. In order to have an exhaustive comparison, each system should represent a different category of GEC systems according to the taxonomy illustrated in Omelianchuk et al. (2024), i.e., edit-based systems, sequence-to-sequence models, and LLM-based systems, respectively. Each of the 5 essay versions \u2013 including the 100% corrected one \u2013 is fed into the GEC systems to obtain the respective grammatically corrected version. Both GECTOR with ROBERTa (Liu et al., 2019) as pretrained encoder \u2013 and Gramformer are used off-the-shelf. The essays are segmented into sentences before being fed into these models to exploit them under optimal conditions, as their maximum sequence lengths are 50 and 64 tokens, respectively. To segment the essays, we use the spaCy sentencizer and then double-check and fix the obtained sentences manually. Conversely, when we use GPT-40, we directly feed the entire essays into the model. The prompt can be found in Appendix A. To evaluate GEC, we use the ERRANT F0.5 score as the primary metric, which is calculated by extracting the GEC edits and then comparing the reference and hypothesis. Additionally, we use the Generalized Language Evaluation Understanding (GLEU) metric (Napoles et al., 2015), which is inspired by BLEU (Papineni et al., 2002) and rewards the overlap of n-grams between the correction and the reference, while penalising n-grams in the correction that remain unchanged when they have been altered in the reference.\nGEF generation: Each version of the essay and its respective corrected version are fed into an LLM in order to obtain a natural language feedback response. We consider three LLMs for our experiments, i.e., Llama 3 8B (Llama Team, 2024), GPT-3.5 (gpt-3.5-turbo-0125) (Brown et al., 2020), and GPT-40. In order to check whether the information derived from GEC is useful, we also ask the models to generate feedback only based on the essays without input of their respective corrected versions. The prompts used for GEF generation can be found in Appendix A.\nGEF discrimination: For each essay, we now have a natural language GEF response. An example can be found in Appendix B. To check for feedback correctness, we proceed to the next step, which we call GEF discrimination, implemented using another LLM with the two different evaluation methods described in Section 4, i.e., essay-type-based evaluation and feedback-based evaluation. As in the previous step, we consider Llama 3 8B, GPT-3.5, and GPT-40. The evaluation metric employed for GEF discrimination is Accuracy. Specifically, for the essay-type-based evaluation method, let:\n$E = \\{E_0, E_{25}, E_{50}, E_{75}, E_{100}\\}$\nbe the set of essay versions (i.e., original, 25% corrected, 50% corrected, 75% corrected, and 100% corrected) and F a given GEF response. For each essay version Ei, we calculate the probability:\n$P(E_i|F)$ for i = 0, 25, 50, 75, 100\nand then we compute the Accuracy Acc:\n$Acc = \\frac{C}{N}$ with $E = arg max P(E_i|F)$"}, {"content": "where C is the count of correctly predicted matches and N is the total number of GEF responses tested.\nSimilarly, for the feedback-based evaluation method, let:\n$F = \\{F_0, F_{25}, F_{50}, F_{75}, F_{100}\\}$"}, {"title": "Experimental results", "content": "GEC results: While GEC is not the main focus of this work, it is important to briefly discuss these results, as they will partly influence the outcomes obtained in the subsequent steps.\nTable 1 reports the ERRANT F0.5 scores when comparing the automatic corrections from our three GEC systems across all the essay versions (except for 100% corrected) to the manual references. The results show that GPT-40 achieves the best results on the original and 25% corrected essays, whereas GECTOR performs better on the 50% corrected and 75% corrected essays. This discrepancy is most likely due to GPT's tendency to overcorrect grammatical errors (Fang et al., 2023; Wu et al., 2023) as it appears from the high Recall scores (see Tables 9, 10, 11, and 12 in Appendix D). These results are confirmed when we evaluate the performance of the three systems in terms of GLEU, as can be seen in Table 2, where we compare the source and hypothesis of each essay version to the corrected manual reference. In this case, even Gramformer outperforms GPT-40 on the 50% corrected and 75% corrected essay versions.\nGEF results: We now move on to the core part of this section, where we report and analyse the GEF results, starting from a comparison of the impact of all GEC systems when using GPT-40 both for GEF generation and GEF discrimination (see Table 3). For the essay-type-based evaluation method, the results show that the best performance is achieved when using GECTOR as the GEC system. This indicates that potential self-bias in the GEF generation phase is ruled out, as GECTOR even outperforms GPT-40. However, as mentioned in Section 6, the reader should take into account the high presence of lexical information since the GEF response is matched directly to the essay versions when employing this evaluation method. Therefore, under these circumstances, even the No GEC system achieves acceptable results. For this reason, we also consider the feedback-"}, {"title": "Discussion", "content": "In this study, we found that providing GEC information to the LLM is fundamental in order for it to produce correct feedback. Using a sort of a chain-of-thought approach, whereby we also use the GEC version of an essay, helps the LLM provide a better GEF response. On the other hand, when GEF generation cannot exploit the information derived from GEC, we generally observe significantly lower results. Furthermore, having a high-performing GEC system improves the quality of a GEF response. In our experiments, we found that using GECTOR achieves better results than using GPT-40. These results also exclude the presence of self-bias derived from the GEC step. Self-bias is also ruled out in the other two steps of the pipeline since we replaced GPT-40 with GPT-3.5 and Llama 3 and still obtained consistent results. Additionally, in our study we showed that our implicit evaluation approach is particularly effective for GEF because it is intrinsically: a) cheap because it does not require manual GEF annotations; b) flexible because foils and lineups can be customised. Finally, when it comes to choosing one of the evaluation methods proposed in the study, we need to keep in mind that LLMs seem to be strongly biased by lexical information. Therefore, we used a feedback-based evaluation method with edited M2 files in which the system does not exploit lexical information derived from elements in the lineup."}, {"title": "Conclusions and future work", "content": "In this paper, we presented a novel implicit evaluation framework to provide grammatical error feedback to L2 learner essays in a compact, comprehensive, and unstructured form. After editing our dataset by varying the error rates of its essays, our proposed pipeline is divided into three major steps: GEC, GEF generation, and GEF discrimination. This framework is cheaper than those previously proposed as it does not require human feedback annotations but only parallel original and corrected essays. Furthermore, it is quite flexible since it is based on lineups of various essay versions and feedback responses derived from these, which can be modified and customised. Future work will extend this framework to written data in other languages and spoken data, possibly by including multimodal LLMs, such as SALMONN (Tang et al., 2024) and Qwen-Audio (Chu et al., 2023)."}, {"title": "Appendix A: Prompts", "content": "The prompt given to GPT-40 for GEC is the following:\nRead the following essay written\nby an L2 learner of English:\n[ESSAY]\nProvide the grammatically\ncorrected version of the essay\nwithout adding any comment, note,\nor explanation."}, {"title": "Appendix A.2 Prompts for GEF generation", "content": "With GEC: the prompt used for GEF generation when exploiting the information derived from GEC is the following:\nRead the following essay written\nby an L2 learner of English and\nits respective corrected version:\nOriginal: [ESSAY]\nCorrected: [CORRECTED ESSAY]\nProvide grammatical feedback\nto the learner based on the\ndifferences between the original\nand the corrected version.\nStart your feedback with \"Dear\nlearner\".\nWhen using this prompt on the 100% corrected version of the essays, we noticed that the systems tended to also include a revised version of the essays along with the feedback response. Therefore, in such cases, we slightly modified the prompt as follows:\nRead the following essay written\nby an L2 learner of English and\nits respective corrected version:\nOriginal: [ESSAY]\nCorrected: [CORRECTED ESSAY]\nProvide grammatical feedback to\nthe learner based on the\ndifferences between the original\nand the corrected version. You\ndon't have to provide a revised\nversion of the essay. Start your\nfeedback with \"Dear learner\".\nWithout GEC: the prompt used for GEF generation without including the information derived from GEC is the following:\nRead the following essay written\nby an L2 learner of English:\n[ESSAY]"}, {"title": "A.3 Prompts for GEF discrimination", "content": "Prompt for essay-type-based evaluation:\nRead the following essay written\nby an L2 learner of English:\n[ESSAY]\nNow, read the following feedback:\n[FEEDBACK]\nIs it correct, appropriate and\nthorough? Just say yes or no\nwithout providing any additional\ncomment, note or explanation.\nPrompt for feedback-based evaluation:\nRead the following feedback\nresponse to an essay written by\nan L2 learner: [FEEDBACK]\nNow, read this ERRANT (ERRor\nANnotation Toolkit) file:\n[ERRANT M2 FILE]\nin which M: missing, R:\nreplace, U: unnecessary, ADJ:\nadjective, ADV: adverb, CONJ:\nconjunction, DET: determiner,\nNOUN: noun, PART: particle, PREP:\npreposition, PRON: pronoun,\nPUNCT: punctuation, VERB: verb,\nCONTR: contraction, OTHER:\nother, MORPH: morphology, ORTH:\northography, SPELL: spelling,\nWO: word order, FORM: form,\nINFL: inflection, NUM: number,\nPOSS: possessive, SVA: agreement,\nTENSE: tense.\nDoes the feedback response\nexplain the ERRANT file\ncorrectly, appropriately and\nthoroughly? Just say yes or no\nwithout providing any additional\ncomment, note or explanation."}, {"title": "Appendix B: GEF example", "content": "The following is an example of GEF response obtained using GPT-40:\nDear learner,\nThank you for sharing your essay.\nHere are some grammatical points\nto help you improve your writing\nbased on the differences between\nyour original version and the\ncorrected version:\n1. **Spelling**: In your\noriginal sentence, you wrote\n\"liste\" instead of \"listen.\" Make\nsure to double-check the spelling\nof words to ensure accuracy.\n2. **Verb Choice**: You used\n\"see\" in \"I can see funny videos.\"\nWhile \"see\" is not incorrect,\n\"watch\" is a more appropriate\nverb when referring to viewing\nvideos or TV shows. \"Watch\"\nimplies that you are actively\npaying attention to the videos.\n3. **Verb Tense**: You\nwrote \"It is expensive\" in your\noriginal sentence. The corrected\nversion uses \"It was expensive.\"\nThis change is made because you\nare referring to the cost of the\nphone at the time of purchase,\nwhich is in the past. Using the\npast tense \"was\" is more accurate\nin this context.\nBy paying attention to these\ndetails, you can make your\nwriting clearer and more precise.\nKeep practicing, and you will\ncontinue to improve!\nBest regards, [Your Name]\nThis was generated after feeding the original and manually corrected versions of this essay written by an Al learner:\nOriginal: Hello Mike,\nI bought a mobile phone, I like it because\nI can liste to music and I can see funny\nvideos. It is expensive but it is good and\nit is black.\nWrite soon."}, {"title": "Appendix C: ERRANT M2 files", "content": "The following examples are related to the essay reported in Appendix B and can be matched with the example of the original ERRANT M2 file shown in Section 4 in order to have a comprehensive view."}, {"title": "C.1 Standard", "content": "A 15 16|||R:SPELL|||listen|||REQUIRED|||-NONE-|||0\nA 21 22|||R:VERB|||watch|||REQUIRED|||-NONE-|||0\nA 26 27|||R:VERB: TENSE |||was ||| REQUIRED|||-NONE-|||0"}, {"title": "C.2 Replaced corrected word", "content": "A 15 16|||R:SPELL|||liste|||REQUIRED|||-NONE-|||0\nA 21 22|||R:VERB|||see|||REQUIRED|||-NONE-|||0\nA 26 27|||R:VERB:TENSE|||is|||REQUIRED|||-NONE-|||0"}, {"title": "C.3 No lexical information", "content": "A 15 16|||R:SPELL||||||REQUIRED|||-NONE-|||0\nA 21 22|||R:VERB||||||REQUIRED|||-NONE-|||0\nA 26 27|||R:VERB:TENSE||||||REQUIRED|||-NONE-|||0"}, {"title": "Appendix D: Further experimental results", "content": "In this section, we report further experimental results. Specifically, Tables 9, 10, 11, and 12 show the results for GEC in terms of Precision, Recall, and F0.5 scores for the Manual, GPT-40, GECTOR, and Gramformer GEC systems, respectively.\nFigure 6 shows the confusion matrices for five GEC systems (i.e., Manual, GPT-40, GECTOR, Gramformer, and No GEC) with feedback-based evaluation method (no lexical information). Similarly, Figure 7 shows the confusion matrix for the Manual GEC system with feedback-based evaluation method (no lexical information) using a lineup of 9 elements.\nFigure 8 illustrates the mean probabilities for \"Yes\" being the first predicted token by GPT-40 in the GEF discrimination step at varying error rate for all the GEC systems considered in our experiments. In this case, we also used the feedback-based evaluation method (no lexical information). Figure 9 shows the mean probabilities of \u201cYes\u201d being the first predicted token for the Manual GEC system only when using the feedback-based evaluation (no lexical information) and 9 elements in the lineup."}]}