{"title": "Grammatical Error Feedback: An Implicit Evaluation Approach", "authors": ["Stefano Bann\u00f2", "Kate Knill", "Mark J. F. Gales"], "abstract": "Grammatical feedback is crucial for consolidating second language (L2) learning. Most research in computer-assisted language learning has focused on feedback through grammatical error correction (GEC) systems, rather than examining more holistic feedback that may be more useful for learners. This holistic feedback will be referred to as grammatical error feedback (GEF). In this paper, we present a novel implicit evaluation approach to GEF that eliminates the need for manual feedback annotations. Our method adopts a grammatical lineup approach where the task is to pair feedback and essay representations from a set of possible alternatives. This matching process can be performed by appropriately prompting a large language model (LLM). An important aspect of this process, explored here, is the form of the lineup, i.e., the selection of foils. This paper exploits this framework to examine the quality and need for GEC to generate feedback, as well as the system used to generate feedback, using essays from the Cambridge Learner Corpus.", "sections": [{"title": "Introduction", "content": "In natural language processing, computer-assisted language learning (CALL) is a well-established research area (Chapelle, 2001). An important part of this process is to give feedback on the use of grammar by a learner. This feedback has usually been in the form of grammatical error detection (GED) (Leacock et al., 2014), and grammatical error correction (GEC) (Wang et al., 2021; Bryant et al., 2023), and the latter has been the subject of four shared tasks over the past 15 years. While highlighting errors and providing a grammatically corrected text is beneficial for second language (L2) learners (Hyland and Hyland, 2006), it is even more valuable to offer specific feedback that comments on grammatical errors, explains them, and provides suggestions for improvement. As shown in Ellis et al. (2006), giving metalinguistic explanations (i.e., explicit feedback) helps learners understand their errors and learn how to avoid them in the future in a more effective way than providing mere recasts (i.e., implicit feedback).\nThe recent advent of large language models (LLMs) has significantly altered the world of CALL. LLMs have been investigated for L2 holistic (Yancey et al., 2023) and analytic (Bann\u00f2 et al., 2024) assessment as well as GEC (Coyne et al., 2023b; Fang et al., 2023; Wu et al., 2023; Katinskaia and Yangarber, 2024). An interesting aspect of natural language generation (NLG) models, such as LLMs, is that they offer the opportunity to automatically generate free-form feedback for candidates, which will be referred to as grammatical error feedback (GEF) hereafter. GEF differs from GED and GEC in that it is not meant to provide feedback on all errors made, but rather provide a more holistic and compact interpretation of the errors being made, along with explanations of the error and suggested improvements.\nOne of the challenges for developing GEF systems is that assessment is difficult. For GED and GEC, it is possible to generate reference transcriptions against which systems can be assessed. Given the vast array of appropriate grammatical feedback that can be generated for an essay, it is not feasible to generate a set of reference feedback that sufficiently covers the complete spectrum of possible valid output. To address this problem in this work, we introduce a novel implicit evaluation approach to GEF that does not require manual annotations. Rather than explicitly evaluating the feedback, the proposed scheme adopts a grammatical lineup approach similar to voice lineups in forensic speaker recognition (McDougall, 2013). The task is then to appropriately pair feedback and essay from the possible set of comparisons.\nAn important aspect of any lineup is the generation of the foils, i.e., the alternatives to the matched pair. There are a number of considerations for the selection of a foil. It is important that the attribute being assessed is distributed over the lineup. For feedback it is thus important to avoid, or at least assess the effect of, differing content as the system may have a bias to particular content, such as performing simple lexicon matching in pairs rather than assessing feedback. Another crucial stage in this process is the selection stage, obtaining the best matching pair. Again, this can exploit the emergent properties of LLMs.\nIn the next section, we briefly review related literature. Section 3 explains the concept of GEF in more detail and Section 4 presents grammatical lineups. In Section 5, we describe the data used in our experiments and, in Section 6, we go through each step of our proposed pipeline. Experimental results are presented in Section 7 and discussed in Section 8. Finally, in Section 9, we move on to the conclusions and discuss potential future directions."}, {"title": "Related work", "content": "For grammatical error annotation in CALL, the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017) has become a widely-used tool for extracting and categorising grammatical error information. Specifically, it extracts error edits from parallel original and corrected sentences. Examples of ERRANT edit labels are R: VERB: FORM, which indicates the need to replace an incorrect verb form, and M: DET, which indicates a missing determiner. The prefix R: stands for replace, M: for missing, and U: for unnecessary (see Section 4 for an example). ERRANT labels error types as OTHER when edits do not fall under any other category. A large part of errors labelled as OTHER are paraphrases. ERRANT has become a standard approach for GEC, but its output information, included in a so-called M\u00b2 file, is not always easily readable, and, despite categorising errors based on parts of speech, it does not provide clear natural-language-based descriptions of errors or motivate corrections in a comprehensible way for learners and teachers.\nOther previous works on feedback generation followed the organisation of a shared task on feedback comment generation for language learning called FCG GenChal 2022 (Nagata et al., 2021, 2023). The main limitations of this task were that it only targeted preposition errors and only leveraged incorrect sentences tagged with span-based error annotations as inputs, hence without their respective corrections. Participants such as Behzad et al. (2023), Stahl and Wachsmuth (2023), Jimichi et al. (2023), Ueda and Komachi (2023), and Koyama et al. (2023) tackled this problem by fine-tuning systems based on T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and GPT-2 (Radford et al., 2019). In the context of this shared task, Coyne (2023) and Coyne et al. (2023a) developed a bipartite typology for L2 feedback which includes abstract tags (e.g., idiom, language transfer, praise, etc.) and grammatical pattern tags (e.g., conditional, possessive, relative clause, etc.).\nA recent study explored the application of LLMS for grammatical error explanation (Song et al., 2024). The study employed a two-step pipeline that first used fine-tuned and prompted LLMs to extract structured atomic token edits. Subsequently, GPT-4 (OpenAI, 2024) was prompted to explain each edit. Despite its novelty and appeal, this approach has several limitations, i.e., it a) can only be applied to sentences but not longer compositions; b) only consists of four operation edit-level types, namely insert, delete, replace, and relocate; and c) needs manual annotators to check that the error explanations are correct, which is generally a costly operation. It is also worth mentioning the recent work by Stahl et al. (2024) on joint essay scoring and feedback generation, although it focuses on L1 essays."}, {"title": "Grammatical error feedback", "content": "The aim of grammatical error feedback (GEF) is to supply holistic feedback to the user on their use of grammar. This feedback should summarise the forms of grammatical error being made in easy to interpret, informative, form, not simply listing all the errors being made by the candidate in their essay (see Appendix B for an example). As the form of feedback will be free-form in nature, NLG conditioned on the essay is used with the associated flexibility of response. The standard approach to this, and the one adopted in this work, is the use of LLMs with the appropriate prompt and inputs."}, {"title": "Grammatical lineup", "content": "As previously discussed, generating a set of references of free-form feedback for an essay is highly challenging given the wide-range of possible forms of feedback possible. Rather than adopting an explicit error measure, as done in GED and GEC, for GEF an implicit performance metric based on matching against a grammatical lineup is proposed. This requires no manual references, but does require the generation of a suitable lineup, i.e., a set of foils, that enables the assessment of GEF quality.\nThe generation of a suitable set of foils is the most important choice in the lineup. For this work to ensure that there are no biases introduced by the essay content, sets of essays are generated based on different levels of manual correction of the learner essay. Here, five different versions of the essay were used:\n\u2022 original: the original essay as written by the learner;\n\u2022 25% corrected: a version where only 25% of the errors are randomly corrected;\n\u2022 50% corrected: a version where only 50% of the errors are randomly corrected;\n\u2022 75% corrected: a version where only 75% of the errors are randomly corrected;\n\u2022 100% corrected: a version where all errors are corrected.\nIt is then possible to use this set of essays as the basis for both the generation of the parade, i.e. the foils, as well as the feedback. The task will then be to match both the feedback and parade entry from the same essay. The quality of the matching process, indicated by the \"?\" blocks in Figure 2, is important for the assessment process. Rather than asking the system to select over a range, it uses a simple binary selection process of asking whether the feedback and essay representation match. This is motivated by the use of LLMs for comparative rather than absolute assessment (Liusie et al., 2024).\nAnother important aspect of designing the parade is to ensure that the quality of the feedback is being assessed rather than simply matching lexical entries in the parade and the feedback. Good feedback is expected to make use of examples of grammatical errors in the essay. This can easily bias the process as the essays and feedback do not contain the same grammatical errors by design. Two forms of parade are investigated in this work.\nEssay-type-based evaluation: In this configuration, we ask the LLM to decide whether a given feedback response matches a given essay using a lineup that includes all essay versions. Each essay version is fed into the system one at a time, and the LLM should simply output \"yes\" or \"no\". For each feedback-essay pair, we extract the probability of \"yes\" being the first predicted token."}, {"title": "Data", "content": "The Cambridge Learner Corpus (CLC) is an ever-growing collection of L2 written data obtained from Cambridge English exams. According to O'Keeffe and Mark (2017), as of 2017, it included 266,600 examination scripts and 143 L1 backgrounds collected in the period between 1993 and 2012. The corpus includes data from lower to advanced proficiency levels annotated according to the Common European Framework of Reference (CEFR) (Council of Europe, 2001) and features manual annotations with information about errors according to a taxonomy of about 80 error types described in Nicholls (2003)."}, {"title": "Experimental setup", "content": "As should be clear at this point, we can identify three major steps in our proposed pipeline: GEC, GEF generation, and GEF discrimination.\nGEC: In addition to manual corrections, we use automatic corrections obtained employing GEC-ToR (Omelianchuk et al., 2020), Gramformers which is a publicly available T5-based GEC system and GPT-4o. In order to have an exhaustive comparison, each system should represent a different category of GEC systems according to the taxonomy illustrated in Omelianchuk et al. (2024), i.e., edit-based systems, sequence-to-sequence models, and LLM-based systems, respectively. Each of the 5 essay versions \u2013 including the 100% corrected one is fed into the GEC systems to obtain the respective grammatically corrected version.\nBoth GECTOR with ROBERTa (Liu et al., 2019) as pretrained encoder \u2013 and Gramformer are used off-the-shelf. The essays are segmented into sentences before being fed into these models to exploit them under optimal conditions, as their maximum sequence lengths are 50 and 64 tokens, respectively. To segment the essays, we use the spaCy sentencizer and then double-check and fix the obtained sentences manually. Conversely, when we use GPT-40, we directly feed the entire essays into the model.\nTo evaluate GEC, we use the ERRANT F0.5 score as the primary metric, which is calculated by extracting the GEC edits and then comparing the reference and hypothesis. Additionally, we use the Generalized Language Evaluation Understanding (GLEU) metric (Napoles et al., 2015), which is inspired by BLEU (Papineni et al., 2002) and rewards the overlap of n-grams between the correction and the reference, while penalising n-grams in the correction that remain unchanged when they have been altered in the reference.\nGEF generation: Each version of the essay and its respective corrected version are fed into an LLM in order to obtain a natural language feedback response. We consider three LLMs for our experiments, i.e., Llama 3 8B (Llama Team, 2024), GPT-3.5 (gpt-3.5-turbo-0125) (Brown et al., 2020), and GPT-40. In order to check whether the information derived from GEC is useful, we also ask the models to generate feedback only based on the essays without input of their respective corrected versions. The prompts used for GEF generation can be found in Appendix A.\nGEF discrimination: For each essay, we now have a natural language GEF response. An example can be found in Appendix B. To check for feedback correctness, we proceed to the next step, which we call GEF discrimination, implemented using another LLM with the two different evaluation methods described in Section 4, i.e., essay-type-based evaluation and feedback-based evaluation. As in the previous step, we consider Llama 3 8B, GPT-3.5, and GPT-40.\nThe evaluation metric employed for GEF discrimination is Accuracy. Specifically, for the essay-type-based evaluation method, let:\n$E = \\{E_0, E_{25}, E_{50}, E_{75}, E_{100}\\}$\nbe the set of essay versions (i.e., original, 25% corrected, 50% corrected, 75% corrected, and 100% corrected) and F a given GEF response. For each essay version $E_i$, we calculate the probability:\n$P(E_i|F)$ for i = 0, 25, 50, 75, 100\nand then we compute the Accuracy Acc:\n$Acc = \\frac{C}{N}$ with $\\hat{E} = \\text{arg max}_{i} P(E_i|F)$\nwhere C is the count of correctly predicted matches and N is the total number of GEF responses tested.\nSimilarly, for the feedback-based evaluation method, let:\n$F = \\{F_0, F_{25}, F_{50}, F_{75}, F_{100}\\}$"}, {"title": "Experimental results", "content": "GEC results: While GEC is not the main focus of this work, it is important to briefly discuss these results, as they will partly influence the outcomes obtained in the subsequent steps.\nThe results show that GPT-40 achieves the best results on the original and 25% corrected essays, whereas GECTOR performs better on the 50% corrected and 75% corrected essays. This discrepancy is most likely due to GPT's tendency to overcorrect grammatical errors (Fang et al., 2023; Wu et al., 2023) as it appears from the high Recall scores (see Tables 9, 10, 11, and 12 in Appendix D).\nThese results are confirmed when we evaluate the performance of the three systems in terms of GLEU, as can be seen in Table 2, where we compare the source and hypothesis of each essay version to the corrected manual reference. In this case, even Gramformer outperforms GPT-40 on the 50% corrected and 75% corrected essay versions.\nGEF results: We now move on to the core part of this section, where we report and analyse the GEF results, starting from a comparison of the impact of all GEC systems when using GPT-40 both for GEF generation and GEF discrimination (see Table 3). For the essay-type-based evaluation method, the results show that the best performance is achieved when using GECTOR as the GEC system. This indicates that potential self-bias in the GEF generation phase is ruled out, as GECTOR even outperforms GPT-40. However, as mentioned in Section 6, the reader should take into account the high presence of lexical information since the GEF response is matched directly to the essay versions when employing this evaluation method. Therefore, under these circumstances, even the No GEC system achieves acceptable results.\nFor this reason, we also consider the feedback-based evaluation method, for which the results are reported in Table 4. In this case, we can see a large drop in performance for the No GEC system when compared to the others, while GECToR is still our best GEC system overall, and Gramformer outperforms GPT-40 when using the replaced corrected word and no lexical information M2 files. The presence of lexical information, especially when we replace the corrections with the originally incorrect words, boosts the performance of all systems. Note that these figures are not directly comparable to those in Table 3 because they have different lineups, i.e., the former has essays, while the latter has GEF responses. Therefore, the fact that, when using the replaced corrected word format with manual corrections, the feedback-based evaluation method (73.20%) apparently performs better than the essay-type-based method (61.26%) should not mislead the reader.\nMore performance figures using the feedback-based evaluation method (no lexical information) can be found in the confusion matrices in Figure 6 (see Appendix D). These results are in line with the ones previously obtained on GEC: when using GPT-4o as GEC system, performances are slightly better on the original and 25% corrected essays, but they get worse as the number of errors decreases. Furthermore, an interesting perspective is offered by Figure 5, which shows the mean probabilities of the first predicted token being \u201cYes\u201d at varying error rates using manual corrections. It can be observed that the peaks are where we would expect them to be. Here, we only report these results using the manual corrections, but we can observe very similar figures for the other systems, as shown in Figure 8 (see Appendix D). As can be expected, this trend is even more evident when we use the essay-type-based evaluation as well as when we use the standard and replaced corrected word formats of M\u00b2 files, as these contain lexical information.\nFor the same reason, hereafter, we will focus on the analysis of the remaining results by only employing the feedback-based no lexical information M2 format and essay-type-based evaluation methods, so to have a sort of a lower and an upper bound, respectively. As mentioned in Section 6, we did not use only GPT-40 for GEF generation but also GPT-3.5 and Llama 3.\nTables 5 and 6 compare the performances of the three LLMs when used for GEF generation employing the essay-type-based and the feedback-based (no lexical information) evaluation methods, respectively. As can be observed, when we use Llama 3 and GPT-3.5, the results are significantly lower but very consistent with the results obtained using GPT-40 for GEF generation.\nIn order to rule out self-bias in the last part of our pipeline, i.e., GEF discrimination, we also try replacing GPT-4o with GPT-3.5 and Llama 3 in this final step.\nTables 7 and 8 show the results for both evaluation methods using the three LLMs. The performances are consistent with what we have observed thus far. The only exception arises when we use Llama 3 for GEF discrimination as it seems to prefer the GEF responses generated by GPT-3.5 instead of itself or GPT-40. However, this does not compromise the consistency of the results because we still do not observe any type of self-bias.\nFinally, we test the sensitivity of the system to the lineup, by increasing its foils. For the sake of brevity and clarity, we only show these results when using the manual corrections and GPT-40 for GEF generation and discrimination and employing the feedback-based evaluation method with M2 files without lexical information. In addition to original, 25% corrected, 50% corrected, 75% corrected, and 100% corrected, we also consider four additional essay versions setting the error rate at 15%, 40%, 60%, and 85%. When increasing the number of elements in the lineup to 9, as expected, the overall Accuracy decreases from 65.80% to 54.00%, but the confusion matrix reported in Figure 7 (see Appendix D) shows a leading diagonal that is consistent with the one related to the previous lineup of 5 elements (see Figure 6a for comparison). Additionally, in Figure 9 (see Appendix D), we also report the mean probabilities of the first predicted token being \u201cYes\u201d at varying error rates after increasing the lineup elements to 9, similarly to what we did in Figure 5. We can still observe very consistent results."}, {"title": "Discussion", "content": "In this study, we found that providing GEC information to the LLM is fundamental in order for it to produce correct feedback. Using a sort of a chain-of-thought approach, whereby we also use the GEC version of an essay, helps the LLM provide a better GEF response. On the other hand, when GEF generation cannot exploit the information derived from GEC, we generally observe significantly lower results. Furthermore, having a high-performing GEC system improves the quality of a GEF response. In our experiments, we found that using GECTOR achieves better results than using GPT-40. These results also exclude the presence of self-bias derived from the GEC step. Self-bias is also ruled out in the other two steps of the pipeline since we replaced GPT-40 with GPT-3.5 and Llama 3 and still obtained consistent results. Additionally, in our study we showed that our implicit evaluation approach is particularly effective for GEF because it is intrinsically: a) cheap because it does not require manual GEF annotations; b) flexible because foils and lineups can be customised.\nFinally, when it comes to choosing one of the evaluation methods proposed in the study, we need to keep in mind that LLMs seem to be strongly biased by lexical information. Therefore, we used a feedback-based evaluation method with edited M2 files in which the system does not exploit lexical information derived from elements in the lineup."}, {"title": "Conclusions and future work", "content": "In this paper, we presented a novel implicit evaluation framework to provide grammatical error feedback to L2 learner essays in a compact, comprehensive, and unstructured form. After editing our dataset by varying the error rates of its essays, our proposed pipeline is divided into three major steps: GEC, GEF generation, and GEF discrimination. This framework is cheaper than those previously proposed as it does not require human feedback annotations but only parallel original and corrected essays. Furthermore, it is quite flexible since it is based on lineups of various essay versions and feedback responses derived from these, which can be modified and customised.\nFuture work will extend this framework to written data in other languages and spoken data, possibly by including multimodal LLMs, such as SALMONN (Tang et al., 2024) and Qwen-Audio (Chu et al., 2023)."}, {"title": "Appendix A: Prompts", "content": "The prompt given to GPT-40 for GEC is the following:\nRead the following essay written\nby an L2 learner of English:\n[ESSAY]\nProvide\nthe grammatically\ncorrected version of the essay\nwithout adding any comment, note,\nor explanation.\nA.2 Prompts for GEF generation\nWith GEC: the prompt used for GEF generation when exploiting the information derived from GEC is the following:\nRead the following essay written\nby an L2 learner of English and\nits respective corrected version:\nOriginal: [ESSAY]\nCorrected: [CORRECTED ESSAY]\nProvide grammatical feedback\nto the learner based on the\ndifferences between the original\nand the corrected version.\nStart your feedback with \"Dear\nlearner\".\nWhen using this prompt on the 100% corrected version of the essays, we noticed that the systems tended to also include a revised version of the essays along with the feedback response. Therefore, in such cases, we slightly modified the prompt as follows:\nRead the following essay written\nby an L2 learner of English and\nits respective corrected version:\nOriginal: [ESSAY]\nCorrected: [CORRECTED ESSAY]\nProvide grammatical feedback to\nthe learner based on the\ndifferences between the original\nand the corrected version. You\ndon't have to provide a revised\nversion of the essay. Start your\nfeedback with \"Dear learner\".\nWithout GEC: the prompt used for GEF generation without including the information derived from GEC is the following:\nRead the following essay written\nby an L2 learner of English:\n[ESSAY]"}, {"title": "Prompts for GEF discrimination", "content": "Prompt for essay-type-based evaluation:\nRead the following essay written\nby an L2 learner of English:\n[ESSAY]\nNow, read the following feedback:\n[FEEDBACK]\nIs it correct, appropriate and\nthorough? Just say yes or no\nwithout providing any additional\ncomment, note or explanation.\nPrompt for feedback-based evaluation:\nRead the following feedback\nresponse to an essay written by\nan L2 learner: [FEEDBACK]\nNow, read this ERRANT (ERRor\nANnotation Toolkit) file:\n[ERRANT M2 FILE]\nin which M: missing, R:\nreplace, U: unnecessary, ADJ:\nadjective, ADV: adverb, CONJ:\nconjunction, DET: determiner,\nNOUN: noun, PART: particle, PREP:\npreposition, PRON: pronoun,\nPUNCT: punctuation, VERB: verb,\nCONTR: contraction, OTHER: other, MORPH: morphology, ORTH:\northography, SPELL: spelling, WO: word order, FORM: form, INFL: inflection, NUM: number, POSS: possessive, SVA: agreement, TENSE: tense.\nDoes the feedback response\nexplain the ERRANT file\ncorrectly, appropriately and\nthoroughly? Just say yes or no\nwithout providing any additional\ncomment, note or explanation."}, {"title": "Appendix B: GEF example", "content": "The following is an example of GEF response obtained using GPT-40:\nDear learner,\nThank you for sharing your essay.\nHere are some grammatical points\nto help you improve your writing\nbased on the differences between\nyour original version and the\ncorrected version:\n1. **Spelling**: In your\noriginal sentence, you wrote\n\"liste\" instead of \"listen.\" Make\nsure to double-check the spelling\nof words to ensure accuracy.\n2. **Verb Choice**: You used\n\"see\" in \"I can see funny videos.\"\nWhile \"see\" is not incorrect,\n\"watch\" is a more appropriate\nverb when referring to viewing\nvideos or TV shows. \"Watch\"\nimplies that you are actively\npaying attention to the videos.\n3. **Verb Tense**: You\nwrote \"It is expensive\" in your\noriginal sentence. The corrected\nversion uses \"It was expensive.\"\nThis change is made because you\nare referring to the cost of the\nphone at the time of purchase,\nwhich is in the past. Using the\npast tense \"was\" is more accurate\nin this context.\nBy paying attention to these\ndetails, you can make your\nwriting clearer and more precise.\nKeep practicing, and you will\ncontinue to improve!\nBest regards, [Your Name]\nThis was generated after feeding the original and manually corrected versions of this essay written by an Al learner:"}]}