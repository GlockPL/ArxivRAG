{"title": "A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future", "authors": ["Shilin Sun", "Wenbin An", "Feng Tian", "Fang Nan", "Qidong Liu", "Jun Liu", "Nazaraf Shah", "Ping Chen"], "abstract": "Artificial intelligence (AI) has rapidly developed through advancements in computational power and the growth of massive datasets. However, this progress has also heightened challenges in interpreting the \"black-box\" nature of AI models. To address these concerns, explainable AI (XAI) has emerged with a focus on transparency and interpretability to enhance human understanding and trust in AI decision-making processes. In the context of multimodal data fusion and complex reasoning scenarios, the proposal of Multimodal explainable AI (MXAI) integrates multiple modalities for prediction and explanation tasks. Meanwhile, the advent of Large Language Models (LLMs) has led to remarkable breakthroughs in natural language processing, yet their complexity has further exacerbated the issue of MXAI. To gain key insights into the development of MXAI methods and provide crucial guidance for building more transparent, fair, and trustworthy AI systems, we review the MXAI methods from a historical perspective and categorize them across four eras: traditional machine learning, deep learning, discriminative foundation models, and generative LLMs. We also review evaluation metrics and datasets used in MXAI research, concluding with a discussion of future challenges and directions.", "sections": [{"title": "I. INTRODUCTION", "content": "ADVANCEMENTS in AI have significantly impacted computer science, with works like Transformer [1], BLIP-2 [2] and ChatGPT [3] excelling in natural language processing (NLP), computer vision, and multimodal tasks by integrating diverse data types. The development of related technologies has driven the advancement of specific applications. For example, in autonomous driving, systems need to integrate data from various sensors, including vision, radar, and LiDAR, to ensure safe operation in complex road environments [4]. Similarly, health assistants require transparency and trustworthiness to be easily understood and verified by both doctors and patients [5]. Understanding how these models combine and interpret different modalities is crucial for enhancing model credibility and user trust. Moreover, increasing model scales pose challenges in computational cost, interpretability, and fairness, driving the demand for Explainable AI (XAI) [6]. As models, including generative LLMs, become increasingly complex and data modalities more diverse, single-modal XAI methods can no longer meet user demands. Therefore, Multimodal eXplainable AI (MX\u0391\u0399) addresses these challenges by utilizing multimodal data in either the model's prediction or explanation tasks, as shown in Fig. 1. We categorize MXAI into three types based on the data processing sequence: data explainability (pre-model), model explainability (in-model), and post-hoc explainability (post-model). In multimodal prediction tasks, the model processes multiple data modalities, such as text, images, and audio. In multimodal explanation tasks, various modalities are used to explain the results, offering a more comprehensive explanation of the final output.\nTo review the history of MXAI and anticipate its development, we first categorized different periods and retrospectively examined various models from a historical perspective (as illustrated in Fig. 2). During the traditional machine learning era (2000-2009), the availability of limited structured data favored interpretable models like decision trees. In the deep learning era (2010-2016), the advent of large annotated datasets, such as ImageNet [7], coupled with increased computational power, led to the rise of complex models and explainable studies, including visualizing neural network kernels [8]. In the discriminative foundation models era (2017-2021), the emergence of Transformer models, leveraging large-scale text data and self-supervised learning, revolutionized NLP. This shift sparked significant research into interpreting attention mechanisms [1], [9]-[11]. In the generative large language models era (2022-"}, {"title": "II. PRELIMINARIES", "content": "We categorize Artificial Intelligence (AI) into four eras based on key technological milestones: the release of the ImageNet dataset in 2009 [7], marking deep learning's rise over traditional machine learning, the introduction of the Transformer model in 2017 [1], distinguishing deep learning from discriminative models, and the advent of ChatGPT in 2022 [3], ushering in the era of generative large models.\nIn this era, MXAI methods are characterized by manual feature engineering and rule-based systems, which ensure transparent interpretability [16], [17]. Decision trees and basic visualization tools further support interpretability, with evaluations, focused on accuracy and human-centered metrics [18], [19]. Despite their advantages, these methods are limited by scalability and flexibility issues due to small datasets and modality constraints, thus setting the stage for the development of more advanced MXAI methods in subsequent years.\nMXAI methods leverage deep neural networks to process and interpret multimodal data. Key characteristics include integrating different data types, developing multimodal architectures that combine Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), and focusing on joint representation learning [20], [21]. This era lays the groundwork for modern MXAI by advancing data integration and interpretability techniques.\nThis era is characterized by the development and widespread adoption of models like Transformer [1], CLIP [22], and their variants. During this era, MXAI methods focus on enhancing the interpretability of these models by leveraging multimodal data, such as text, images, and audio. Researchers aim to explain model decisions through techniques like attention visualization, feature attribution, and gradient-based methods. This era features advancements in multimodal inputs with foundation models, providing comprehensive explanations and improving the trustworthiness of models.\nIn this era, MXAI methods evolve to address the distinct challenges and opportunities presented by generative LLMs like ChatGPT [3]. Unlike the more transparent Transformer-based discriminative models, LLMs often restrict direct access, necessitating innovative interpretability methods. These techniques leverage the interactivity of LLMs to enhance adaptive explanations,"}, {"title": "B. Comparison with Previous Surveys", "content": "Based on [12], we present the latest and most comprehen- sive comparison of XAI-related reviews from the past four years XAI-related reviews in the last four years as Table I. Despite numerous XAI reviews published over the last five years (see Table I), some are mostly limited to specific time eras or focused on XAI applications for particular technologies. For example, recent XAI researchs [30], [31] have largely concentrated on summarizing developments after Transformer and has predominantly focused on unimodal explainability. There is a notable gap in exploring the interpretability of LLMs from a historical and developmental perspective, as well as leveraging multimodal information. While Rodis et al. [25] review MXAI methods in terms of both models and post-hoc interpretability, it primarily addresses work post-2016 excluding LLMs and does not cover the explainability of Transformer and their variants.\nIn summary, as shown in Table I, XAI methods have evolved with advancements in artificial intelligence, with Transformer and large model interpretability emerging from this progres- sion. As multimodal data becomes more prominent, MX\u0391\u0399 methods are gaining increasing attention [25]. This paper addresses shortages in existing reviews by offering a com- prehensive historical overview of MXAI methods, segmented into four eras: traditional machine learning (See Section III), deep learning (See Section IV), discriminative foundation models (See Section V), and generative large language models (See Section VI). Each era is examined with respect to data, models, and post-hoc explainability, and we also summarize relevant datasets and evaluation metrics for assessing MXAI"}, {"title": "III. THE ERA OF TRADITIONAL MACHINE LEARNING", "content": "As shown in Table II, data explainability in this era focuses primarily on data dimensionality reduction, including feature selection and extraction methods. Model explainability mainly involves shallow machine learning models, such as decision trees and Bayesian models. Post-hoc explainability techniques mainly are model-agnostic and model-specific approaches. These methods lay a crucial foundation for multimodal ex- plainability research, with their techniques and insights ex- tended to multimodal contexts.\nWith advancements in technology, the proliferation of data generation has led to the emergence of high dimensionality issues, commonly referred to as dimensional catastrophe. To address this, explainability methods based on dimensional- ity reduction aim to reduce redundancy and computational load. Among these methods, feature selection retains original features and interpretability by selecting relevant subsets. In contrast, feature extraction creates new features through mathematical transformations, enhancing processing efficiency but potentially reducing interpretability.\nFeature selection methods are used to choose important features from a dataset and are categorized into filter, wrapper, and embedded methods. Filter methods, like those based on mutual information, assess each feature's relevance in- dependently of learning algorithms [16]. Wrapper methods, exemplified by Recursive Feature Elimination (RFE) [56], evaluate subsets based on model performance degradation. Embedded methods, integrated into training processes like tree-based models, determine feature importance, improving transparency in model decision-making [59].\nFeature extraction methods transform original data into a low-dimensional feature space, compressing data complex- ity. Unlike feature selection methods, it creates new fea- tures rather than selecting existing ones. Principal Compo- nent Analysis (PCA) [23] determines principal components by eigenvectors of the data's covariance matrix, enabling dimensionality reduction and data structure visualization. Lin- ear Discriminant Analysis (LDA) [71] maximizes inter-class distance and minimizes intra-class variance, enhancing data interpretability. t-SNE [73] preserves local structure between data points in high-dimensional space to handle modal dif- ferences. UKDR [84] extends kernel dimensionality reduction for unsupervised learning, offering superior low-dimensional embeddings compared to t-SNE and PCA.\nLogistic Regression (LR) is used for binary classification, while Linear Regression is its counterpart for continuous outcomes. Both models assume linear relationships between predictors and outcomes, limiting flexibility but ensuring"}, {"title": "IV. THE ERA OF DEEP LEARNING", "content": "As shown in Table III, in this era, the shift from tradi- tional machine learning-centered on manual feature engi- neering-to deep neural networks brings new challenges in interpretability. MXAI focuses on making the \"black box\" nature of deep models more transparent. Key efforts include balancing model performance with interpretability and devel- oping both local and global explanation techniques to provide insights into model decisions and overall behavior, enhancing the trustworthiness of systems.\nData quality analysis evaluates and interprets data from various sources to ensure integrity, consistency, and reliability, thereby improving the performance of deep learning models [134], [135]. Traditional approaches, such as additive noise models with Bayesian or maximum likelihood estimation, often overlook data correlations, lim- iting the effectiveness of fusion algorithms [136]. To enhance data handling, methods like optimally weighted averages are employed for specific datasets, such as weather models [137]. However, challenges like conflicts and inconsistencies in mul- timodal data persist [138]. To address these issues, information theory-based approaches dynamically reconfigure systems to manage sensor data inconsistencies [139]. Additionally, first-order optimization techniques in weighted least squares are used to capture underlying data structures and reconstruct missing values, thereby enhancing the robustness of data integration methods [140]. These combined strategies improve the overall reliability and performance of deep learning models by ensuring high-quality data.\nData interaction analysis is dedicated to understanding and explaining the interactions between different types and sources of data. Such analyses not only help to improve the performance of models but also enhance their transparency and trust. To enhance the accuracy and coherence of generated descriptions, Vinyals et al. [141] model the correspondence between images and text using an encoder-decoder structure with an attention mechanism. Chen et al. [142] synchronize text, audio, and video features by timestamp, utilizing the temporal relationships in multimodal data. Venugopalan et al. [143] explain the model's decision- making by learning feature representations between video and text, employing attention mechanisms and feature fusion. Sun et al. [144] address sequence alignment by proposing an efficient strategy based on sampling multiple video clips."}, {"title": "B. Model explainability", "content": "These methods involve selecting from a predefined set of inherently interpretable (white box) techniques [12]. For example, Ustun et al. [145] introduce supersparse linear integer models for optimizing medical scoring systems, providing a clear, interpretable framework for healthcare professionals. Similarly, Lakkaraju et al. [146] propose interpretable decision sets that combine description and prediction, enhancing model transparency. In contrast, Jung et al. [147] develop simple rule-based ap- proaches for complex decisions, ensuring understandability even in intricate scenarios. Furthermore, Alonso et al. [148] review current trends in the interpretability of fuzzy systems, highlighting their intuitive rule-based structure. Lastly, Lou et al. [149] present accurate intelligible models with pairwise interactions, achieving high performance while maintaining in- terpretability through visualized feature interactions. Together, these works underscore the importance of model interpretabil- ity across various applications.\nGiven the com- plexity of deep models, interpretation efforts focus primarily on decomposability and algorithmic transparency.\nFor decomposability, methods for deep model data process- ing include unit response visualization [8], deconvolutional networks [150], and CNN-specific neuron preference [151]. For algorithmic transparency, approaches involve guiding model structure revisions through interpretability [152], [153], abstraction analysis of deep image representations [154]- [156], and convergence studies on feature learning in deep net- works [157]. Additionally, Koh et al. [158] assess how training data affects models by evaluating the influence of individual data points without retraining. Shwartz-Ziv et al. [159] use an information-theoretic framework to analyze deep network be- havior, offering insights into internal representation evolution and improving training efficiency during diffusion phases.\nAttention-based networks effectively direct information flow by weighting inputs or internal features, excelling in tasks like non-sequential nat- ural language translation [1], fine-grained image classifica- tion [160], and visual question answering [161]. While at- tention units are not explicitly designed for human-readable explanations, they provide insights into information traversal within the network. And Das et al. [162] adopt datasets"}, {"title": "C. Post-hoc explainability", "content": "Multi-layer neural net- works (MLPs), recognized for their capacity to model complex relationships, have driven advancements in interpretability techniques such as model simplification, feature relevance estimation, text and local interpretation, and model visu- alization [182]. For instance, [168] introduces interpretable mimic learning, a knowledge-distillation method that uses gradient boosting trees to create interpretable models with performance comparable to deep learning models. Similarly, Che et al. [169] present Treeview to enhance interpretability by partitioning the feature space with a tree structure. Fea- ture relevance methods are employed to streamline complex models, thereby improving efficiency, robustness, and trust in predictions [170], [171]. Additionally, theoretical validation efforts for multilayer neural networks have made progress in model simplification, nonlinear feature correlation, and hidden layer interpretation [172], [173].\nResearch on CNNS interpretability primarily focuses on two methods: mapping in- puts to outputs to elucidate decision-making processes and ex- amining how intermediate layers perceive the external world.\nThe first approach involves reconstructing feature maps from selected layers to understand activation effects. For in- stance, Zeiler et al. [152] reconstruct high activations to reveal"}, {"title": "V. THE ERA OF DISCRIMINATIVE FOUNDATION MODELS", "content": "This era emphasizes large-scale pre-trained models based on Transformer as concluded in Table IV. These models are trained by unsupervised or self-supervised methods and leverage transfer learning to excel across various tasks with minimal task-specific data. They mark a shift from earlier advancements in neural network architectures and supervised learning that characterized the previous Deep Learning era (2010-2016).\nAnalyse multimodal datasets: Srinivasan et al. [183] propose a multimodal interaction approach that integrates direct manipulation, natural language, and flexible unit vi- sualizations to enhance visual data exploration. Noroozi et al. [184] simplify complex data through effective visualization and analysis. To address limitations in VQA datasets, Kafle et al. [185] introduce the TDIUC dataset to evaluate VQA"}, {"title": "B. Model explainability", "content": "In the realm of model behavior explanations, methods are divided into architecture-dependent and architecture-independent categories. Architecture- dependent methods analyze the internal mechanisms and structure of the model, while architecture-independent methods focus on the relationship between inputs and outputs. For example, DIME [234] enables precise analysis of multimodal models by breaking them down into unimodal contributions (UCs) and multimodal interactions (MIs), and is adaptable across various modalities and architectures. Grad-CAM++ [24] provides fine-grained visual explanations by highlighting relevant input regions, and LIFT-CAM [235] combines layer-wise relevance propagation with feature map activations for improved interpretability across different models. Additionally, some unimodal approaches can be extended to multimodal scenarios, maintaining independence from specific architectures [236]-[239]. For architecture- dependent approaches, we focus primarily on methods related to Transformer and CLIP models.\nTransformer models have significantly advanced NLP by identifying patterns in large datasets, but their internal work- ings remain opaque. Various interpretation methods have been developed, each with distinct advantages and limitations. Prob- ing tasks provide a straightforward means to evaluate specific knowledge within Transformers. For instance, Cao et al. [200] use probing tasks to assess patterns learned during pre-training, while Hendricks et al. [201] apply these tasks to image- language Transformers using detailed image-sentence pairs."}, {"title": "C. Post-hoc explainability", "content": "Counterfactuals aim to infer the causes of predictions and their relationships under input distortions. To enhance VQA model interpretability, Agarwal et al. [215] reveal and reduce spurious associations using invariant and covariant editing, providing causal explanations and enhancing model transparency and reliability. Chen et al. [221] assess and improve model robustness by generating and analyzing counterfactual samples. Niu et al. [216] adopt counterfactuals for causal inference to analyze and explain language bias. Pan et al. [222] provide counterfactual examples allow users to investigate and understand how the VQA model produces its results and the underlying causes. In visual captioning, counterfactual interpretations emphasize observa- tions leading to certain outcomes [223], [224]. Counterfactual reasoning is also applied in reinforcement learning settings for non-autoregressive image captioning [225] and scene-graph representation [226].\nUnbalanced datasets or inappropriate feature selection can compromise input data quality and affect model fairness. Pena et al. [227] identify sources of bias in"}, {"title": "VI. THE ERA OF GENERATIVE LARGE LANGUAGE MODELS", "content": "This era focuses on advancing generative tasks by the foun- dations established by Discriminative Models (2017-2021). Unlike their predecessors, these models, such as GPT-4 [240], BLIP-2 [2] and their successors, generate coherent, contex- tually relevant text, enhancing explainability by providing natural language explanations for outputs. This advancement bridges the gap between human understanding and machine decision-making, enabling more nuanced interactions and in- sights into model behavior.\nLLMs can effectively explain datasets through interactive visualization and data analysis. LIDA [241] generates grammar-agnostic visualizations and infographics to understand the semantics of the data, enu- merate the relevant visualization goals, and generate visual- ization specifications. Other methods [242]-[245] enhance the explainability of the datasets by analyzing the datasets.\nBy combining multimodal information and powerful natural language processing capabilities, LLMs can provide com- prehensive, in-depth, customized, and efficient explanations of data [13]. Bordt et al. [246] explore the capabilities of LLMs in understanding and interacting with glass-box models, identifying unexpected behaviors, and suggesting repairs or improvements. The focus is on leveraging multimodal data interpretability to enhance these processes.\nData selection is crucial in this era. It improves model performance and accuracy, reduces bias, enhances generalization, saves training time and resources, and boosts interpretability, making decision-making more trans- parent and aiding in model improvement [302]. Multimodal C4 [247] enhances dataset quality and diversity by integrat- ing multiple sentence-image pairs and implementing rigorous image filtering, excluding small, improperly proportioned im- ages and those containing faces. This approach underscores text-image correlations, bolstering the robustness and inter- pretability of multimodal model training. A new generative AI paradigm based on heuristic hybrid data filtering is proposed to enhance user immersion and increase interaction levels between video generation models and language tools (e.g., ChatGPT [3]) [248]. It enables the generation of interactive environments from individual text or image prompts.\nIn addition to the above, some works aim to improve the robustness of the model to distributional variations and out- of-distribution data [249], [250].\nWhile MLLMs can process and in- tegrate data from different modalities, they typically capture relationships implicitly. In contrast, graph modeling explicitly represents data nodes (e.g., objects in images, concepts in text) and their relationships (e.g., semantic associations, spatial"}, {"title": "B. Model explainability", "content": "In this era, the process explanation of MXAI emphasizes multimodal in-context learning (ICL) and multimodal Chain of Thought (CoT). The prominence of ICL comes from its ability to avoid extensive updates to numerous model parameters by using human-comprehensible natural language instructions [303]. Emu2 [254] enhances task-independent ICL by extending multimodal model gen- eration. Link context learning (LCL) [304] focuses on causal reasoning to improve learning in Multimodal Large Language Models (MLLMs). A comprehensive framework for multi- modal ICL (M-ICL) is proposed by [255] in models like DEFICS [256] and OpenFlamingo [257], encompassing a wide range of multimodal tasks. MM-Narrator [258] utilizes GPT- 4 [240] and multimodal ICL for generating audio descriptions (AD). Further advancements in ICL and new multimodal ICL variants are explored by [259]. MSIER [260] uses a neural network to select instances that enhance the efficiency of multimodal context learning.\nMultimodal CoT addresses the limitations of single- modality models in complex tasks where relying solely on text or images fails to capture comprehensive information. Text lacks visual cues, and images miss detailed descriptions, limiting the model's reasoning abilities [305]. Multimodal CoT integrates and reasons across multiple data types, such as text and images [261]-[264]. For instance, image recognition can be broken down into a step-by-step cognitive process, constructing a chain of networks that generate visual biases, which are added to input word embeddings at each step [261]. Zhang et al. [262] first generate rationales from visual and language inputs, then combine these with the original inputs for reasoning. Hybrid rationales [306] use textual rationale to guide visual rationale, merging features to provide a coherent and transparent justification for answers."}, {"title": "C. Alignment with human cognition", "content": "Aligning MLMs with human cognition is essential for improving interpretability, transparency, and credibility. Since language primarily functions as a communication tool rather than a thought process, integrating multimodal information is crucial for enhancing model reasoning capabilities [336]. Future research will focus on refining multimodal fusion, de- veloping explanations that mirror human cognitive processes, and ensuring MLMs' effectiveness in real-world scenarios. Models that are more robust, comprehensible, and effective in addressing complex challenges will be proposed."}, {"title": "D. MXAI without ground truths", "content": "MLLMs encounter significant interpretability challenges, particularly with ground truths. The complexity of multimodal data complicates the establishment of ground truths due to varying expressions, structures, and semantics. Ground truths are often subjective and costly to define, with human anno- tation inconsistencies adding to the challenge. Additionally, the opaque nature of deep learning models further complicates aligning outputs with ground truths. For example, cross-modal reasoning, crucial for tasks like visual question answering, requires integrating information from multiple modalities, making validation of ground truths even more complex. Evalu- ating MLLMs in dynamic environments, such as autonomous driving, introduces further difficulties as models must accu- rately process and interpret rapidly changing information. Ad- dressing these challenges demands standardized datasets and benchmarks, improved model transparency, advanced cross- modal fusion techniques, and focused research on MLLMs interpretability relative to ground truths."}, {"title": "IX. CONCLUSION", "content": "This paper categorizes Multimodal Explainable AI (MXAI) methods across four historical eras: traditional machine learn- ing, deep learning, discriminative foundation models, and generative large language models. We analyze MXAI's evo- lution in terms of data, model, and post-hoc explainability, and review relevant evaluation metrics and datasets. Looking ahead, key challenges include scaling explainability tech- niques, balancing model accuracy with interpretability, and addressing ethical concerns. The ongoing advancement of MXAI is crucial for ensuring AI systems are transparent, fair, and trustworthy."}]}