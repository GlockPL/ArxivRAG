{"title": "Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training", "authors": ["Ziqing Wen", "Ping Luo", "Jiahuan Wang", "Xiaoge Deng", "Jinping Zou", "Kun Yuan", "Tao Sun", "Dongsheng Li"], "abstract": "Large language models (LLMs) have shown impressive performance across a range of natural language processing tasks. However, their vast number of parameters introduces significant memory challenges during training, particularly when using memory-intensive optimizers like Adam. Existing memory-efficient algorithms often rely on techniques such as singular value decomposition projection or weight freezing. While these approaches help alleviate memory constraints, they generally produce suboptimal results compared to full-rank updates. In this paper, we investigate the memory-efficient method beyond low-rank training, proposing a novel solution called Gradient Wavelet Transform (GWT), which applies wavelet transforms to gradients in order to significantly reduce the memory requirements for maintaining optimizer states. We demonstrate that GWT can be seamlessly integrated with memory-intensive optimizers, enabling efficient training without sacrificing performance. Through extensive experiments on both pre-training and fine-tuning tasks, we show that GWT achieves state-of-the-art performance compared with advanced memory-efficient optimizers and full-rank approaches in terms of both memory usage and training performance.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have made remarkable strides since the release of ChatGPT, a groundbreaking chatbot based on LLM technology [1]. Their promising performance and scalability have led to rapid adoption across a range of interdisciplinary fields, including science [2-4], medicine [5-8], and policy-making in biology [9]. The impressive capabilities of LLMs arise from their vast number of parameters and the massive datasets used for training. However, training these models imposes significant demands on the optimizer, with Adam [10] emerging as the most commonly used optimizer due to its fast convergence, robustness, and strong experimental results.\nDespite these advantages, Adam's memory requirements present a major drawback. Specifically, it consumes twice the memory of the model itself (as illustrated in Figure 1), making memory usage a key bottleneck in LLM training. For instance, pre-training a LLaMA 7B [11] model requires approximately 58GB of GPU memory: 14GB for model weights, 14GB for gradients, 2GB for activations, and 28GB for Adam optimizer states under BF16 precision [12]. When the model size exceeds 100B parameters, as with GPT-3 [1] (175B parameters), the additional memory overhead from Adam can exceed 350GB, requiring at least five additional NVIDIA A100 80GB GPUs.\nGiven the high costs of training LLMs, methods that reduce the batch size or use larger GPUs quickly become unsustainable. To mitigate these issues, many studies have focused on optimizing memory usage during large-scale model training. A promising solution is low-rank training [12, 13], which has made significant progress in alleviating memory constraints. These memory-efficient methods can be broadly classified into two categories: weight-based and gradient-based approaches [14]. Notable examples include Low-Rank Adaptation (LoRA) [13] and Gradient Low-Rank Projection (GaLore) [12].\nLORA leverages the low-rank nature of model parameter updates by freezing the pre-trained model weights and reparameterizing weight updates with trainable low-rank decomposition matrices. Extensive experiments have demonstrated that LORA is effective in fine-tuning tasks, but its performance in pre-training remains limited. Additionally, the strong low-rank assumptions underlying LoRA often lead to suboptimal outcomes [15, 16]. To address these limitations, Re-LoRA [17] extends LoRA's applicability to pre-training by periodically updating the frozen model parameters. However, this periodic update process requires full-rank training, which significantly undermines the memory efficiency gains Re-LoRA could otherwise achieve [12].\nIn contrast to LoRA, which decomposes the update matrix, GaLore reduces the optimizer state memory usage by performing Singular Value Decomposition (SVD) on the gradients, since the memory occupied by optimizer states is directly tied to the gradient size (as shown in Figure 1). GaLore has demonstrated effective performance in both pre-training and fine-tuning tasks. However, its performance still lags behind that of full-rank methods, and it only captures information within the projected subspace, discarding gradient information outside of it. This limitation becomes particularly problematic when using lower ranks or when gradients are noisy, leading to significant convergence issues [12, 18]."}, {"title": "2 Results", "content": "In this section, we demonstrate that the GWT, as a memory-efficient optimization technique, can achieve performance on par with or even surpassing that of full-rank optimizers, while significantly reducing memory usage and increasing training throughput. Specifically, we evaluate GWT in two contexts: pre-training LLaMA [11] models on the Colossal Clean Crawled Corpus (C4) [24] English benchmark, and fine-tuning the RoBERTa-base model [25] on the General Language Understanding Evaluation (GLUE) [26] benchmark. Additionally, we present an ablation study to explore the impact of hyperparameters, including \\alpha, learning rate (lr), and the GWT level l.\nFor the GWT implementation, we employ the discrete Haar and discrete Daubechies-2 (DB2) wavelets as filters. A detailed description of the experimental setup can be found in Appendix A."}, {"title": "2.1 Pre-training LLaMA on C4", "content": "We integrate our Gradient Wavelet Transform (GWT) method with the Adam optimizer [10], using default hyperparameters (\u03b2\u2081 = 0.9, \u03b22 = 0.999, \u03f5 = 10\u207b\u2076), to evaluate its performance against existing memory-efficient algorithms on LLaMA models for the C4 pre-training task. For a comprehensive comparison, we include our reproduced results for full-rank Adam [10] and GaLore, and reference results from prior works such as LORA [13], ReLoRA [17], Fira [14], and APOLLO [19].\nIn all experiments, we use a learning rate of 0.01 for both GWT and GaLore and apply cosine annealing for the learning rate schedule. For GaLore-r, we set the model rank to 1/4 and 1/8 model rank, denoted as GaLore-1/4 and GaLore-1/8, corresponding to the use of a 2/3-level GWT. We follow the initialization strategy from prior works [12] to initialize the network weights and enable GWT in both the"}, {"title": "2.2 Fine-Tuning on GLUE Benchmarks", "content": "In this section, we further evaluate our method by fine-tuning the RoBERTa-base model on the GLUE benchmark. We report performance metrics across various tasks: accuracy for MNLI, Matthew's correlation for COLA, Pearson correlation for STS-B, F1 score for MRPC, and accuracy for the remaining tasks. It is important to note that the optimizer memory ratio during fine-tuning is typically very low, such as 1/96. For GWT, this would require approximately 7 wavelet transformations, which introduces significant computational overhead and contradicts the original intent of GWT. Therefore, for a fair comparison, we use GaLore-1/4 and Haar-2, referencing the results from LoRA and full-rank Adam as reported in prior work [12]. In the fine-tuning task, our primary focus is on evaluating the effectiveness of GWT. There is, however, considerable potential for accelerating multi-level GWT, which we plan to explore in future work.\nThe experimental results are presented in Table 3. As shown, GWT proves to be effective not only for pre-training but also for fine-tuning, offering competitive performance across multiple GLUE tasks. In contrast, LoRA is primarily designed for fine-tuning tasks and does not exhibit the same versatility across both pre-training and fine-tuning stages as GWT does."}, {"title": "2.3 Hyperparameter Study", "content": "In this section, we investigate the effects of the hyperparameters \\alpha and GWT level l. A more detailed discussion of these two hyperparameters is provided in Section 4.4.\nScale \\alpha. We evaluate Adam with 2-level Haar GWT using different values of \\alpha at a fixed learning rate (lr = 0.01). As shown in Figure 3a and 3b, we observe that our GWT is invariant to the hyperparameter. For \\alpha values larger than 0.1, the final experimental results remain relatively stable and do not differ significantly. Based"}, {"title": "3 Discussion", "content": "In this paper, we investigate memory-efficient optimization algorithms that extend beyond conventional low-rank decomposition and quantization techniques. Drawing inspiration from the limitations of current optimization methods and the proven success of WT in image and signal processing, we propose a unified framework that integrates GWT into standard optimizers. This framework bridges the gap between gradient compression and wavelet transforms, enabling more efficient training of LLMs by reducing memory usage and accelerating training speed.\nTo evaluate the effectiveness of our approach, we integrate GWT into the Adam [10] and Adafactor [27] optimizers. Our experimental results show that GWT achieves state-of-the-art performance across multiple stages of the LLM training process and is fully compatible with other optimizers beyond Adam. This gradient compression technique not only improves final validation scores but also enhances throughput, offering a novel memory-performance trade-off that complements existing methods, particularly those based on low-rank optimizations.\nOur proposed GWT and its implementation draw inspiration from GaLore [12]. Both GWT and GaLore are closely related to Projected Gradient Descent (PGD) [28, 29], which projects the gradient onto a subspace before performing the update. However, there are key distinctions between GWT and GaLore. The primary difference lies in the projection operation: GWT not only projects the gradient onto the subspace but also leverages additional information from this projection, such as the"}, {"title": "4 Methods", "content": "In this section, we first provide an overview of the basic theory behind the wavelet transform. We then explain how the GWT method is applied to the Adam optimizer [10], followed by a discussion of a unified framework that enables the integration of GWT into other optimizers, such as Adafactor [27]. We also cover the associated hyperparameters for GWT."}, {"title": "4.1 Discrete Haar Wavelet Transform", "content": "Wavelet transforms can be categorized into continuous and discrete types. In this paper, we focus on the 1-dimensional Discrete Haar wavelet Transform (DHT) as a case to illustrate our approach. DHT is one of the simplest and most widely used wavelet transforms in signal and image processing due to its low computational cost [34]. The DHT decomposes a signal (or image) into two components: a) Approximation coefficients (Low-frequency): Which represent the smooth or average part of the signal/image. b) Detail coefficients (High-frequency): Capture the difference or finer details of the signal/image.\nFor example, for an input signal or image represented by the sequence of values [X\u2081, X\u2082, X\u2083, X\u2084, X\u2085, X\u2086, X\u2087, X\u2088], the 1-level DHT (Haar-1) computes the approximation"}, {"title": "4.2 Adam with GWT", "content": "The Adam optimizer [10] adaptively adjusts the learning rates for each parameter based on its past gradient states, while also incorporating momentum to accelerate training. The update rule for the weights is given by the following recursive equation:\nW\u209c\u208a\u2081 = W\u209c - \u03b7 \u22c5 \u011e\u209c, (4)\nwhere \u03b7 > 0 denotes the learning rate, represents the element-wise multiplication, W\u2208R\u1d50\u02e3\u207f and\nM\u209c = \u03b2\u2081 \u22c5 M\u209c\u208b\u2081 + (1 \u2212 \u03b2\u2081) \u22c5 G\u209c, V\u209c = \u03b2\u2082 \u22c5 V\u209c\u208b\u2081 + (1 \u2212 \u03b2\u2082) \u22c5 G\u209c\u00b2, \u011e\u209c =  M\u209c/\u221aV\u209c+\u03f5 (5)\nHere, \u03b2\u2081, \u03b2\u2082 \u2208 [0,1) are the decay rates hyperparameters which are set as 0.9, 0.999 commonly, \u03f5 > 0 to preserve numerical stability, G\u209c \u2208 R\u1d50\u02e3\u207f denotes the batch gradient at time step t, G,  M\u209c/\u221aV\u209c+\u03f5 represents the element-wise multiplication and division, and M, V \u2208 R\u1d50\u02e3\u207f represents the first/second-order moment in Adam optimizer.\nFor ease of presentation, we adopt a 1-level DHT in the following discussion. To integrate GWT with the Adam optimizer, we begin by applying GWT to the gradient matrix G\u209c at time step t (Eq. (3)), focusing on the approximate coefficients. This results in a compressed gradient matrix (approximation coefficient) A\u209c \u2208 R\u1d50\u02e3\u00bd and a detail coefficient matrix D\u209c \u2208 R\u1d50\u02e3\u00bd. We then update the Adam states by replacing the original gradient G\u209c in the update rule (Eq. (5)) with the smaller matrix A\u209c to calculate Gt. To preserve the consistency of the state update, we scale the wavelet detail coefficients D\u209c by dividing them by D\u209c/\u221aV\u209c + \u03f5 based on the linear properties of DHT (Eq. (3)). Finally, project \u011e\u209c back to the original space and update the weights. The update rule can be rewritten by the following\n[A\u209c, D\u209c] = G\u209cH, M\u209c = \u03b2\u2081 \u22c5 M\u209c\u208b\u2081 + (1 \u2212 \u03b2\u2081)A\u209c, V\u209c = \u03b2\u2082V\u209c\u208b\u2081 + (1 \u2212 \u03b2\u2082)A\u209c\u00b2,\nAt =  Mt/\u221aVt + e , D\u209c =  Dt/\u221aVt + e , \u011e\u209c = [A\u209c, D\u209c]H, W\u209c\u208a\u2081 = W\u209c - \u03b7 \u22c5 G\u209c. (6)"}, {"title": "4.3 General Optimizer with GWT", "content": "In general, backpropagation in deep learning training follows the update rule:"}, {"title": "4.4 Additional Hyperparameters", "content": "In addition to the hyperparameters required by the original optimizer, several additional hyperparameters are needed to apply our GWT method. The hyperparameter \\alpha stands for the scale factor. Specifically, the scale hyperparameter \u03b1 determines the magnitude of the update for the GWT module of \u03b7 \u00b7 \u03b1 (\u03b7 for the modules without GWT). The H, H hyperparameters depend on the GWT basis used in the process (e.g., Haar, DB2, Rbio2.6, etc.). The choice of filter impacts the transformation applied during the wavelet decomposition and reconstruction. l stands for the GWT level, 1 denotes 1-level GWT which can reduce the optimizer memory up to 50%, and 2 denotes 2-level GWT which reduces the memory usage up to 75%, respectively."}, {"title": "Appendix A Experiment Details", "content": ""}, {"title": "A.1 Network Architecture", "content": "In this section, we describe the network architectures of the LLaMA (Large Language Model Meta AI) [36] and ROBERTa (Robustly Optimized BERT Approach) [25] models, which complement the experimental details provided in the main text. LLaMA is a family of foundational language models based on transformers [37]. For this paper, we"}, {"title": "A.2 Experiment Hyperparameters", "content": "In this section, we provide detailed information on the hyperparameters and experimental setup used in the experiments discussed in the main text. For the LLaMA pre-training tasks, we follow the experimental configuration used in previous work [12], which employs the default Adam hyperparameters (\u03b2\u2081 = 0.9, \u03b22 = 0.999, \u03f5 = 10\u207b\u2076), a maximum sequence length of 256 tokens, and gradient accumulation with a batch size of 512. Additionally, we apply a learning rate warm-up for the first 10% of the iterations and use a cosine annealing schedule with the initial learning rate.\nFor the GWT method, we perform hyperparameter tuning on the scale parameter \u03b1 from the set [0.1, 0.15, 0.2, 0.25], selecting the value that yields the best performance across all LLAMA pre-training tasks. We found that our method is not sensitive to scale, and setting \u03b1 to 0.25 yields good results in most pretraining tasks, which is consistent with the hyperparameters used in previous work [12, 19]. For the baseline methods, we use their default hyperparameters: \u03b1 = 0.25 and lr = 0.01 for GaLore, and lr = 0.001 for Adam (0.0005 for Adam in LLaMA 1b). Consistent with prior work [12], we enable GWT and GaLore in the attention and MLP modules.\nFinally, we summarize the hyperparameter \u03b1, the GPUs used (RTX 3090), and the training time (in hours) for our method required to reproduce the results shown of Table 1 in Table A2.\nFor the GLUE fine-tuning tasks, we set the \u03b1 hyperparameter to 2 for both GWT and GaLore (enable GWT and GaLore in all modules). The lr for GWT is chosen from the interval [1\u00d710\u207b\u2076, 1\u00d710\u207b\u2075], GaLore is tuned within the interval [5\u00d710\u207b\u2076, 5\u00d710\u207b\u2075]. For each GLUE task, we use a batch size of 16 (except for COLA, where the batch size is 32), train for 30 epochs, and set the maximum sequence length to 512. We enable GWT and GaLore across all network modules for all tasks. We present the"}, {"title": "Appendix B Benchmark Details", "content": "In this section, we provide a detailed description of the benchmarks used in the paper, serving as a supplement to the results section 2.\nC4 benchmark. We adopt Colossal Clean Crawled Corpus (C4) [24] benchmark of the English version for all the pre-training tasks. The C4 English benchmark is a colossal, cleaned version of Common Crawl's web crawl corpus based on the Common Crawl dataset. Includes 305GB of English-language text and is mainly intended to pre-train language models. The validation complexity is obtained by taking the validation loss and exponentiating it with the base of Euler's number e."}]}