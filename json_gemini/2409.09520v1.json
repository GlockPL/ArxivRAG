{"title": "Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment*", "authors": ["Xin Hu", "Janet Wang", "Jihun Hamm", "Rie R Yotsu", "Zhengming Ding"], "abstract": "Current Al-assisted skin image diagnosis has achieved dermatologist-level performance in classifying skin cancer, driven by rapid advancements in deep learning architectures. However, unlike traditional vision tasks, skin images in general present unique challenges due to the limited availability of well-annotated datasets, complex variations in conditions, and the necessity for detailed interpretations to ensure patient safety. Previous segmentation methods have sought to reduce image noise and enhance diagnostic performance, but these techniques require fine-grained, pixel-level ground truth masks for training. In contrast, with the rise of foundation models, the Segment Anything Model (SAM) has been introduced to facilitate promptable segmentation, enabling the automation of the segmentation process with simple yet effective prompts. Efforts applying SAM predominantly focus on dermatoscopy images, which present more easily identifiable lesion boundaries than clinical photos taken with smartphones. This limitation constrains the practicality of these approaches to real-world applications. To overcome the challenges posed by noisy clinical photos acquired via non-standardized protocols and to improve diagnostic accessibility, we propose a novel Cross-Attentive Fusion framework for interpretable skin lesion diagnosis. Our method leverages SAM to generate visual concepts for skin diseases using prompts, integrating local visual concepts with global image features to enhance model performance. Extensive evaluation on two skin disease datasets demonstrates our proposed method's effectiveness on lesion diagnosis and interpretability.", "sections": [{"title": "1. Introduction", "content": "Current Al-assisted diagnostic systems demonstrate expert-level capability in classifying skin cancers, which are often identified visually [3, 4, 9, 26, 29, 32]. Given that early and accurate diagnosis is important in improving treatment outcomes, these systems can significantly contribute to teledermatology as diagnostic and decision-support tools [10]. By utilizing photos captured from portable devices like smartphones, these methods promote diagnostic accessibility in rural areas [6]. However, such systems are susceptible to under-diagnosis due to non-standardized acquisition environments and protocols. Therefore, the model's ability to accurately recognize the region of interest (ROI) within noisy backgrounds is crucial for precise diagnosis.\nWhile image segmentation techniques such as MaskR-CNN [16], DeepLab [5], and Panoptic Segmentation [22] can be employed to localize ROIs and enhance diagnostic accuracy, they rely on fine-grained bounding box or pixel-wise semantic annotations. To acquire such annotations"}, {"title": "2. Related Works", "content": "Image segmentation techniques such as MaskRCNN [16], DeepLab [5], and Panoptic Segmentation [22] are effective for localizing regions of interest and enhancing model accuracy, but they typically require fine-grained bounding box or pixel-wise semantic annotations. With a growing focus on scalability, pre-trained foundational models have gained prominence in machine learning, serving as robust starting points for various downstream tasks [30]. Responding to this trend, the Segment Anything Model (SAM) has emerged, pushing image segmentation into the realm of foundational models.\nSAM is a promptable segmentation model pre-trained on SA-1B, a vast dataset containing over 1 billion masks derived from 11 million licensed and privacy-preserving images. This extensive training ensures strong generalization across diverse data distributions. SAM supports flexible prompts such as single points, sets of points, bounding boxes, or text. Its architecture is elegantly simple yet effective: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and these inputs are fused in a lightweight mask decoder to predict segmentation masks. Specifically, SAM's image encoder utilizes a Masked Autoencoder (MAE) [15] pre-trained on a Vision Transformer (ViT) [8]. This adaptation allows it to handle high-resolution inputs while capturing fine-grained details and long-range dependencies. The prompt encoder converts prompts into fixed-length embeddings that capture semantic meanings. These embeddings are combined with image encoder outputs to generate a set of feature maps used by the mask decoder to produce segmentation masks."}, {"title": "2.1. Visual Segment Generation", "content": "Image segmentation techniques such as MaskRCNN [16], DeepLab [5], and Panoptic Segmentation [22] are effective for localizing regions of interest and enhancing model accuracy, but they typically require fine-grained bounding box or pixel-wise semantic annotations. With a growing focus on scalability, pre-trained foundational models have gained prominence in machine learning, serving as robust starting points for various downstream tasks [30]. Responding to this trend, the Segment Anything Model (SAM) has emerged, pushing image segmentation into the realm of foundational models.\nSAM is a promptable segmentation model pre-trained on SA-1B, a vast dataset containing over 1 billion masks derived from 11 million licensed and privacy-preserving images. This extensive training ensures strong generalization across diverse data distributions. SAM supports flexible prompts such as single points, sets of points, bounding boxes, or text. Its architecture is elegantly simple yet effective: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and these inputs are fused in a lightweight mask decoder to predict segmentation masks. Specifically, SAM's image encoder utilizes a Masked Autoencoder (MAE) [15] pre-trained on a Vision Transformer (ViT) [8]. This adaptation allows it to handle high-resolution inputs while capturing fine-grained details and long-range dependencies. The prompt encoder converts prompts into fixed-length embeddings that capture semantic meanings. These embeddings are combined with image encoder outputs to generate a set of feature maps used by the mask decoder to produce segmentation masks."}, {"title": "2.2. Skin Lesion Analysis", "content": "Skin lesion images are typically captured in two forms: dermatoscopy images and clinical photos. Dermoscopy images are close-up views of pigmented skin lesions captured by using professional microscopy, a dermatoscope. These images focus on the lesion, typically excluding any background, resulting in a uniform and consistent visual presentation. Clinical photos, on the other hand, are often taken with portable devices like smartphones and contain more noisy backgrounds. Extensive research has explored the use of deep Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) [12] for skin lesion segmentation to standardize lesion variations and enhance diagnostic accuracy [14, 21, 24, 27, 34, 37]. CNN-based methods for skin lesion segmentation typically employ supervised learning with large labeled datasets to extract spatial features and semantic maps from images. In contrast, GAN-based approaches address data scarcity through unsupervised learning but still require fine-grained groundtruth"}, {"title": "3. Methods", "content": "Our objective is to develop an accurate and interpretable diagnostic model for complex image data. Given a batch of skin disease images, $X = \\{x_1, x_2, x_3, . . ., x_i, ..., x_N \\}$, where N is the total number of images, the model aims to predict the most likely skin disease for each image, represented as $\\{y_i\\}_{i=1}^N$. Here, $y_i \\in R^C$, with C representing the total number of condition categories, and $y_{i,c}$ the presence of skin disease category c in $x_i$.\nConsidering the skin images are usually captured from smartphones and tablets in the field, they lack standardization and often contain confounding background noise. The ability to easily discern the region of interest (ROI) or lesion in a poor-quality or noisy image is crucial to accurate diagnosis. In this study, we will explore the Segment Anything Model (SAM) [23] to detect visual concepts with specific prompts, then develop a cross-attentive fusion model by leveraging the global image information and local visual cues to improve the skin disease diagnosis performance."}, {"title": "3.1. Preliminary and Motivation", "content": "Our objective is to develop an accurate and interpretable diagnostic model for complex image data. Given a batch of skin disease images, $X = \\{x_1, x_2, x_3, . . ., x_i, ..., x_N \\}$, where N is the total number of images, the model aims to predict the most likely skin disease for each image, represented as $\\{y_i\\}_{i=1}^N$. Here, $y_i \\in R^C$, with C representing the total number of condition categories, and $y_{i,c}$ the presence of skin disease category c in $x_i$.\nConsidering the skin images are usually captured from smartphones and tablets in the field, they lack standardization and often contain confounding background noise. The ability to easily discern the region of interest (ROI) or lesion in a poor-quality or noisy image is crucial to accurate diagnosis. In this study, we will explore the Segment Anything Model (SAM) [23] to detect visual concepts with specific prompts, then develop a cross-attentive fusion model by leveraging the global image information and local visual cues to improve the skin disease diagnosis performance."}, {"title": "3.2. Visual Concepts Identification", "content": "Due to the often complicated background noise, end-to-end deep learning is not sufficient to fully extract informative features from raw images with disease labels. Though SAM can capture local visual concepts of lesions, the original SAM needs bounding boxes or coarse masks as input and might produce features containing background noise. Thus, we modify variants of SAM in public repository [2] which utilizes Grounding DINO [25], to obtain visual concepts and provide related bounding boxes.\nAs shown in Figure 2, we use keywords describing human body parts and \u201clesion\u201d as text prompts and input them with the original image to the SAM variant to generate bounding boxes, visual concepts, and segmentation masks. Figure 1 lists some detected visual concepts. It is noted that the text prompt \"lesion\u201d might correctly localize the pathological area for certain skin diseases whose lesion boundaries are clear, but it is challenging for other complicated conditions, such as \"Mycetoma\". Still, these captured visual concepts can offer extra information and help us identify the relevant segments of the body or the lesion.\nWe define local visual concepts as $\\{V_1, V_2,...\\} \\in V$. By focusing on these localized visual elements, our model"}, {"title": "3.3. Cross-Attentive Fusion Model", "content": "Due to the unreliability of certain visual concepts from SAM (Figure 1), we concurrently leverage both global image features and local visual concepts. As illustrated in Figure 2, our proposed dual-branch framework is designed to harness the complementary strengths of these two sources of information. This approach allows for a more robust and comprehensive understanding of the diseases by integrating detailed local visual concepts with overarching global image features, thereby improving the overall accuracy and reliability of the analysis.\nFirstly, we introduce a global feature encoder, denoted as $F_g(\\cdot)$, to extract the global features $Z_g \\in R^{N \\times 1 \\times D}$ for the entire image. This is based on the premise that the overall image encapsulates most of the essential information. However, it is crucial to pinpoint the specific location of the skin disease's origin. Moreover, global features often encompass background noise, which can degrade the prediction performance. Thus, while global features provide a broad context, their inherent noise necessitates complementary local analysis for precise disease localization and classification.\nTo address this issue, we propose exploring local visual concepts that potentially preserve more focused information about skin disease, thereby enhancing model decision-making. As illustrated in Figure 2, we input the local visual concepts $V$ into the local encoder $F_l(\\cdot)$ to extract local features $Z_l \\in R^{N \\times n \\times D}$, where n represents the total number of visual concepts. This method effectively filters out background noise and irrelevant objects, ensuring the model focuses on the most severely affected and informative pixels.\nTo efficiently leverage the complementary information between local concepts and global images, we propose a cross-attentive module where local concepts serve as \"query\" prompts to highlight the most salient areas within the global features. Specifically, the attention map $M \\in R^{N \\times n \\times 1}$ acts as a soft mask between the local concept features $Z_l$ and the global features $Z_g$. This attention map is normalized using a global softmax function to capture the most relevant segments between the local and global features. This component further strengthens the model's ability to focus on the critical areas, thereby enhancing the overall precision and effectiveness of the diagnosis system."}, {"title": "3.4. Interpretable Skin Diagnosis", "content": "Having obtained the refined latent features $O$, it is essential to identify which parts contribute the most to future observations in the medical field. Typically, most datasets lack such detailed labels because annotating them is labor-intensive and usually requires professional expertise to ensure label quality in medical research. Inspired by the weakly supervised setting described by [20], we introduce the Class Activation Map (CAM) and develop multi-instance learning (MIL) loss for the final prediction, instead of relying on traditional classification loss.\nIn our framework, we utilize a classifier $F_c(\\cdot)$, as shown in Figure 2, to transform the latent features $O$ into $O_{cam} \\in R^{N \\times n \\times C}$. Subsequently, a top-k strategy is employed to identify the concepts that contribute the most, denoted $\\hat{O}_{cam} \\in R^{N \\times k \\times C}$. The final prediction is based on the average value of these top-k concepts, as given by:\n$O_{pred} = F_{AVG}(\\hat{O}_{cam})$, (2)\nwhere $F_{AVG}(\\cdot)$ represents the average pooling function along the dimension k. The final loss is formulated as:\n$L = L_{MIL}(O_{pred}, y_i)$, (3)\nwhere $L_{MIL}(\\cdot)$ denotes the multi-instance learning loss function. This approach allows us to effectively utilize the weakly supervised signals to identify and focus on the most"}, {"title": "4. Experiments", "content": "We evaluated our methods using two datasets: MIND-the-SKIN [1] and SCIN [33]. The MIND-the-SKIN project aims to address challenges in Neglected Tropical Diseases (NTDs), a diverse group of skin conditions prevalent in impoverished tropical communities, affecting over 1 billion people. A crucial aspect of the project involves data collection in rural West Africa using portable devices and the development of AI-based diagnostic tools. For our evaluation, we utilized a subset of this dataset comprising 1,731 clinical photos representing five common NTD conditions: leprosy, Buruli ulcers, yaws, scabies, and mycetoma.\nExisting skin disease datasets featuring clinical photos [7, 11, 13, 28, 36] are pre-processed to center at lesions and have reduced background noise. In contrast, the SCIN dataset retains its original noise and more closely mirrors real-world inputs, making it a valuable resource for our investigation. The SCIN dataset was collected through a voluntary image donation platform from Google Search users in the United States, with each case including up to three images, all diagnosed by up to three dermatologists. This process results in a weighted skin condition label for each case. To ensure label accuracy, we selected the condition with the highest weight as the final label, excluding ambiguous cases where multiple conditions had equal probabilities. Additionally, to facilitate reliable evaluation and maintain consistency with the NTD dataset in terms of the number of"}, {"title": "4.1. Datasets", "content": "We evaluated our methods using two datasets: MIND-the-SKIN [1] and SCIN [33]. The MIND-the-SKIN project aims to address challenges in Neglected Tropical Diseases (NTDs), a diverse group of skin conditions prevalent in impoverished tropical communities, affecting over 1 billion people. A crucial aspect of the project involves data collection in rural West Africa using portable devices and the development of AI-based diagnostic tools. For our evaluation, we utilized a subset of this dataset comprising 1,731 clinical photos representing five common NTD conditions: leprosy, Buruli ulcers, yaws, scabies, and mycetoma.\nExisting skin disease datasets featuring clinical photos [7, 11, 13, 28, 36] are pre-processed to center at lesions and have reduced background noise. In contrast, the SCIN dataset retains its original noise and more closely mirrors real-world inputs, making it a valuable resource for our investigation. The SCIN dataset was collected through a voluntary image donation platform from Google Search users in the United States, with each case including up to three images, all diagnosed by up to three dermatologists. This process results in a weighted skin condition label for each case. To ensure label accuracy, we selected the condition with the highest weight as the final label, excluding ambiguous cases where multiple conditions had equal probabilities. Additionally, to facilitate reliable evaluation and maintain consistency with the NTD dataset in terms of the number of"}, {"title": "4.2. Implementation Details", "content": "We use the pretrained ViT [8] as the backbone to extract global features from each input image. The global feature extracted by ViT, combined with our top-k mechanism, serves as the baseline for comparison. ViT produces a 768-dimensional feature map with a resolution of 14 \u00d7 14, which we pass through a linear layer to reduce the dimensionality to 256 for further refinement. For the local encoder, we extract visual concepts using our SAM variant, generating 256-dimensional vectors. These local features are refined with two convolutional layers, also outputting 256-dimensional vectors. To maintain consistent input, we fix the number of visual concepts at 30 for the NTD dataset and 25 for the SCIN dataset, based on the maximum number of captured concepts. For samples with fewer visual concepts, we use a random perturbation padding method to ensure consistency in input size. In the cross-attentive module, we implement a multi-head transformer with 8 heads, where each transformer module consists of 1 attention layer with a dimensionality of 256. The classification module includes 3 convolutional layers, with two dropout layers (dropout rate of 0.7) between them to regularize intermediate features. For hyperparameter choice, we train the whole model with a learning rate of 1e-4 using the Adam optimizer, and the batch size is set as 128 for training and 1 for testing. Considering the overfitting problem, we train all model parts simultaneously with 30 epochs. We choose k = 5 for the NTD dataset and 2 for the SCIN dataset. All experiments are conducted with one RTX 4090 GPU."}, {"title": "4.3. Comparison Results", "content": "We first tested our proposed method on the NTD dataset from the MIND-the-SKIN Project. As shown in Table 2, we conducted five random splits of cases into training and validation sets and compared our method with three other competitive methods. An immediate observation is that our method consistently outperforms the others in terms of classification accuracy for all splits, validating the effectiveness of leveraging both global and local features for skin lesion"}, {"title": "4.4. Interpretable Results", "content": "Figure 6 shows examples of \u201cBuruli ulcer\u201d and \"Yaws,\u201d where the diseased areas are easily identifiable on the skin. The distribution of the row scores in the CAM indicates that both cases have a high probability of the correct disease and low confidence in others. The column score distribution demonstrates that our model focuses more on the lesion part rather than the whole body. The selected bounding box (third column of the figure) and segmentation mask (fourth column) provide professionals with an intuitive yet reliable explanation for the prediction. Note that we report the final prediction using the top-5 visual concepts, and we select the top-1 from these to represent the visual concept that contributes most to the prediction, considering that multiple concepts might confuse the medical diagnosis."}, {"title": "4.5. Ablation Study", "content": "Table 4 indicates the performance comparison of different feature combinations on the MIND-the-SKIN dataset with train/total=0.5. The \u201cLocal-only\" and \"Baseline\" denote the methods that only use local or global features individually for the final prediction. The \"Concatenation-1\" represents that we simply concatenate global feature ton local features as n+1 dimensional feature \u039f'\u0395 $R^{N \\times(n+1)\\times D}$. \u201cConcatenation-2\" follows the feature fusion method in [35] to combine local and global features in"}, {"title": "5. Conclusion", "content": "In this paper, we leverage the foundation AI model, SAM, to automatically segment visual skin images and propose a cross-attention model designed to harness complementary information between local visual concepts and global features in challenging clinical skin disease images. To effectively explain our model's decision-making process, we integrate CAM and multi-instance learning to identify the most influential concepts, which are generated by SAM using stochastic text prompts. Our experiments demonstrate that the proposed method consistently outperforms competitive approaches across various metrics, underscoring the effectiveness of the dual-branch design. In addition, our method provides better interpretability, offering explainable predictions that enhance the reliability of AI-based diagnoses for medical professionals. This interpretability is crucial for building trust and improving diagnostic accessibility in future applications."}]}