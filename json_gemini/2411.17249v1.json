{"title": "Buffer Anytime: Zero-Shot Video Depth and Normal from Image Priors", "authors": ["Zhengfei Kuang", "Tianyuan Zhang", "Kai Zhang", "Hao Tan", "Sai Bi", "Yiwei Hu", "Zexiang Xu", "Milos Hasan", "Gordon Wetzstein", "Fujun Luan"], "abstract": "We present Buffer Anytime, a framework for estimation of depth and normal maps (which we call geometric buffers) from video that eliminates the need for paired video-depth and video-normal training data. Instead of relying on large-scale annotated video datasets, we demonstrate high-quality video buffer estimation by leveraging single-image priors with temporal consistency constraints. Our zero-shot training strategy combines state-of-the-art image estimation models based on optical flow smoothness through a hybrid loss function, implemented via a lightweight temporal attention architecture. Applied to leading image models like Depth Anything V2 and Marigold-E2E-FT, our approach significantly improves temporal consistency while maintaining accuracy. Experiments show that our method not only outperforms image-based approaches but also achieves results comparable to state-of-the-art video models trained on large-scale paired video datasets, despite using no such paired video data.", "sections": [{"title": "1. Introduction", "content": "Acquiring depth and normal maps from monocular RGB input frames has been a fundamental research topic in computer vision for decades. Serving as a bridge between 2D images and 3D representations, advances in this field have enabled breakthrough applications across domains such as embodied AI, 3D/4D reconstruction and generation, and autonomous driving.\nRecent advances in foundational models, including image/video diffusion models [1, 5, 6, 24, 38] and large language models (LLMs) [16, 48], has accelerated the development of powerful models for image and video buffer estimation. By buffers we mean information such as per-pixel depth, normals, lighting, or material properties; in this paper we specifically focus on depth and normals (i.e., geometry buffers). Empowered by large-scale datasets captured in synthetic environments and the real world, recent works [19, 32, 58] have demonstrated impressive results in predicting various types of buffers from images. A promising line of recent work [30, 45] further extends the use of large-scale models for video buffer prediction, showing superior video depth predictions with high fidelity and consistency across frames.\nOur work originates from the following question: Can image-based buffer estimation models help with the task of video buffer estimation? Comparing to mainstream image/video generative models that take input in lower dimensions as conditions (i.e., text or a single frame) and generate higher-dimensional outputs (i.e., image or video), the buffer estimation models are usually conditioned on RGB image/video of the same size as the desired results; the input already contains rich structural and semantic information. As a result, image inversion models are much more likely to produce consistent contents given similar input conditions compared to text-to-image/video generation models. This observation drives us to explore the possibility of upgrading existing image models for video buffer generation.\nIn this paper, we demonstrate a positive answer for the above question by showing an effective video geometry buffer model trained from image priors without any supervision from ground truth video geometry data. We propose Buffer Anytime, a flexible zero-shot training strategy that combines the knowledge of an image geometry model with existing optical flow methods to ensure both temporal consistency and accuracy of the learned model predictions. We apply the training scheme on two state-of-the-art image models, Depth Anything V2 [56] (for depth estimation) and Marigold-E2E-FT [20] (for normal estimation), and show significant improvements in different video geometry estimation evaluations. We summarize the major contributions of our work as:\n\u2022 A zero-shot training scheme to fine-tune an image geometric buffer model for video geometric buffer generation;\n\u2022 A hybrid training supervision that consists of a regularization loss from the image model and an optical flow based smoothness loss;\n\u2022 A lightweight temporal attention based architecture for video temporal consistency;\n\u2022 Our proposed models outperform the image baseline models by a large margin and are comparable to state-of-the-art video models trained on paired video data."}, {"title": "2. Related Work", "content": "Our work intersects with several active research areas in computer vision. We first review recent advances in monocular depth and normal estimation, particularly focusing on large-scale and diffusion-based approaches, and video methods with their efforts in maintaining temporal consistency. We then examine video diffusion models that inspire our temporal modeling strategy."}, {"title": "2.1. Monocular Depth Estimation", "content": "Monocular depth estimation has evolved through several paradigm shifts. Early works [28, 34, 44] relied on handcrafted features and algorithms, while subsequent deep-learning methods [18, 22, 57, 59] improved performance through learned representations. MiDaS [41] and Depth Anything [55] further advanced the field by leveraging large-scale datasets, with Depth Anything V2 [56] enhancing robustness through pseudo depth labels. Recent works incorporating diffusion models [19, 26, 32] achieved fine-grained detail, while E2EFT [20] improved efficiency through single-step inference and end-to-end fine-tuning. While these advances represented major progress for single-image depth estimation, they did not address temporal consistency in videos. NVDS [53] tackles this challenge by introducing a stabilization framework and video depth dataset, followed by ChronoDepth [45] and DepthCrafter [30] which leveraged video diffusion models (e.g. Stable Video Diffusion [5]) to boost prediction quality. Unlike these approaches that require annotated datasets, our method achieves comparable results without ground truth depth maps."}, {"title": "2.2. Monocular Surface Normal Estimation", "content": "Surface normal estimation has evolved significantly since the pioneering work of Hoiem et al. [27, 28], who introduced learning-based approaches using handcrafted features. The advent of deep learning sparked numerous neural network-based approaches [3, 4, 12, 15, 17, 50, 51]. Recent advances include Omnidata[14] and its successor Omnidata v2 [31], which leverage large-scale diverse datasets with sophisticated 3D data augmentation. Another line of research [19, 20, 26, 40] focuses on jointly predicting normal and depth maps in a unified framework to enforce cross-domain consistency. Recently, DSINE [2] enhanced robustness by incorporating geometric inductive bias into data-driven methods. However, these works focus solely on single-image predictions without addressing temporal coherence."}, {"title": "2.3. Video Diffusion Models", "content": "Recent advances in video diffusion models enable high-quality video generation from multimodal input conditions, such as text [6, 24, 39, 47], image [5, 23, 54], camera trajectory [25, 33] and human pose [10, 29, 46]. Among them, AnimateDiff [24] introduced plug-and-play motion modules for adding dynamics to the existing image model Stable Diffusion [1], supporting generalized video generation for various personalized domains. These advances inspired our temporal modeling approach, though we differ by leveraging image-based priors and optical-flow models without requiring direct video-level supervision."}, {"title": "3. Method", "content": "We first formulate our problem as follows: given an input RGB video consisting of K frames, $I_{1,...,K} \\in \\mathbb{R}^{K\\times H\\times W\\times 3}$, we aim to predict the corresponding depth maps $D_{1,...,K} \\in \\mathbb{R}^{K\\times H\\times W}$, and surface normal maps $N_{1,...,K} \\in \\mathbb{R}^{K\\times H\\times W\\times 3}$ represented in camera coordinates. For convenience, we will mainly focus on describing the task of depth estimation without loss of generality. While existing state-of-the-art methods for video depth prediction models are trained from paired datasets, i.e., a set of data pairs $(I_{1,...,K}, D_{1,...,K})_{1,..., N_{data}}$ of input frames and ground truth depth maps, our model does not require any paired video datasets, instead only relying on the RGB video data $(I_{1,...,K})_{1,..., N_{data}}$.\nThe key insight of our approach is to combine image-based diffusion priors and optical-flow based temporal stabilization control. Given an image depth prediction model $f_{image} (I) = D_{image}$ trained by large-scale image paired datasets to reconstruct the underlying data prior $p_{image} (D|I)$, our goal is to develop an upgraded video model $f_{video}$ that is backed by $f_{image}$ and able to predict depth maps from videos, i.e., $f_{video}(I_{1,...,K}) = D_{video}$."}, {"title": "3.1. Training Pipeline", "content": "To achieve the two conditions, we design a novel training strategy (Fig. 4) that employs two different types of losses: A regularization loss that forces the model to produce results aligned with the image model, and an optical flow based stabilization loss as described in Section 3.2. In depth estimation, the regularization loss is based on the affine-invariant relative loss in previous works [20, 55]:\n$L_{depth} = \\frac{1}{HW} || \\frac{\\hat{D} - D}{D} ||_{1}$,\nwhere H, W are the image sizes, $\\hat{D}$ is the predicted depth map of the k-th frame normalized by the offset $t = median(D_k)$ and the scale $s = \\frac{1}{HW} \\times \\sum_{x} |D_k(x) - t|$, and $D$ is the normalized depth map from the image model. In normal estimation, we leverage the latent representation of the backbone model, and simply apply an $L_2$ loss on the predicted latent maps z:\n$L_{normal} = \\frac{1}{HW} \\sum_{x} ||z_k - \\hat{z}_k||_2$.\nTo speed up training, we randomly select one frame from the video in each iteration and calculate the regularization loss on this frame only. The overall training loss is:\n$L = w_{reg} \\cdot L_{depth / normal} + L_{stable}$,"}, {"title": "3.2. Optical Flow Based Stabilization", "content": "Single-view image predictors usually suffer from inconsistent results across frames due to the ambiguity of affine transformation (i.e., scale and offset) of the prediction and uncertainty from the model. To alleviate this problem, a reasonable approach is to align the depth predictions between the corresponding pixels across different frames. Inspired by previous network prediction stabilizing works [9, 52, 53], we apply a pre-trained optical flow estimator to calculate the correspondence between adjacent frames for the temporal consistency stabilization. Specifically, given the predicted optical flow maps between two adjacent frames $I_k, I_{k+1}$ are $O_{k\\rightarrow k+1}$ and $O_{k+1\\rightarrow k}$, a stabilization loss between the two frames can be defined as:\n$L_{stable} = \\frac{1}{2HW} \\sum_x |I_k(x) - I_{k+1}(O_{k\\rightarrow k+1}(x))|_1 + \\frac{1}{2HW} \\sum_x |I_{k+1}(x) - I_k (O_{k+1\\rightarrow k}(x))|_1$.\nIn practice, however, the optical flow prediction can be inaccurate or wrong due to the limitations of the pretrained model, harming the effectiveness of the loss as Fig. 3 shows. To prevent that, we add two filtering methods to curate the correctly corresponded pixels across the frames. The first method applies the cycle-validation technique that is commonly used in many previous image correspondence methods. Here we only select the pixels in $I_k$ that satisfy:\n$||O_{k\\rightarrow k+1}(O_{k+1\\rightarrow k}(X)) - X||_2 \\leq T_c$,\nwhere $T_c$ is a hyper-parameter threshold. The second technique is based on the observation that the stabilization loss can be incorrectly overestimated near the boundary areas in the depth frames due to the inaccuracy of the optical flow. Here, our solution is to apply the Canny edge detector [8] on predicted depth maps, and then filter out the losses on the pixels that are close to the detected edges (i.e., the Manhattan distance is smaller than 3 pixels). The combination of these two filters effectively removes outliers and improves the robustness of our model."}, {"title": "3.3. Model Architecture", "content": "For generating consistent and high-fidelity video results across frames, choosing a powerful and stable image-based backbone model that robustly preserves the structure of the input frames is crucial: it can greatly reduce the inconsistency and ambiguity of the image results, facilitating our video model training process. Recent advances in large-scale image-to-depth and image-to-normal models have brought up many powerful candidates. In this work, we opt to use Depth Anything V2 [56] for the backbone of our depth prediction model, and Marigold-E2E-FT [20] for our normal prediction model.\nDepth Anything V2 [56] is a Dense Vision Transformer (DPT) [42] that consists of a Vision Transformer (ViT) [13] as an encoder, and a lightweight refinement network that fuses the feature outputs of several ViT blocks together and produces the final results. Here we freeze the ViT backbone and only fine-tune the refinement network. As Fig. 4 (a) shows, we inject three temporal blocks in between the fusion blocks, as a bridge to connect the latent maps across different frames. Notice that the ViT blocks are fully detached from the gradient flow, which helps reducing the memory cost during training, enabling support for longer videos.\nMarigold-E2E-FT [20] is a Latent Diffusion Model [43] built upon Stable-Diffusion V2.0 [1]. As Fig. 4 (b) demonstrates, we insert temporal layers between the spatial layers. The original U-Net layers and the autoencoder are both fixed during training. The temporal blocks in both models are structured similarly to the blocks in AnimateDiff [24], consisting of several temporal attention blocks followed by a projection layer. The final projection layer of each block is zero-initialized to ensure the model acts the same as the image model when training begins.\nOur framework combines the power of state-of-the-art single-view models with temporal consistency through a carefully designed training strategy and architecture. The optical flow based temporal stabilization and regularization losses work together to ensure both high-quality per-frame predictions and temporal coherence, while our lightweight temporal blocks enable efficient processing of video sequences. By freezing the backbone networks and only training the temporal components, we maintain the strong geometric understanding capabilities of the pretrained models while adding temporal reasoning abilities. In the following section, we conduct extensive experiments to validate our design choices and demonstrate the effectiveness of our approach across various video geometry estimation tasks."}, {"title": "4. Experiments", "content": "All of our experiments are conducted on NVIDIA H100 GPUs with 80GB memory. Based on memory constraints, we set the maximum sequence length to 110 frames for depth estimation and 32 frames for normal estimation. We collect approximately 200K videos for training, with each clip containing 128 frames. We use the AdamW [35] optimizer with learning rates of $10^{-4}$ for depth and $10^{-5}$ for normal estimation. We train on 24 H100 GPUs with a total batch size of 24. The entire training process takes approximately one day for 20,000 iterations."}, {"title": "4.1. Video Depth Estimation Results", "content": "We evaluate our method on the benchmark provided by DepthCrafter [30], which adapts standard image depth metrics for video evaluation. For each test video, the evaluation first solves for a global affine transformation (offset and scale) that best aligns predictions to ground truth across all frames, then computes metrics on the transformed predictions. We report three metrics: Mean Absolute Relative Error (AbsRel), the percentage of pixels within 1.25\u00d7 of ground truth ($\\delta_1$), and optical-flow-based smoothness error (OPW), defined similarly to our smoothness loss. We evaluate on three datasets: ScanNet [11] (static indoor scenes), KITTI [21] (street views with LiDAR depth), and Bonn [36] (dynamic indoor scenes), using the same test splits as DepthCrafter.\nAs shown in Tab. 1, our model significantly improves upon its backbone, Depth Anything V2 [56], in both quality and temporal smoothness. Notably, we achieve comparable performance to DepthCrafter, the current state-of-the-art trained on large-scale annotated video datasets, particularly on ScanNet and KITTI datasets. We also demonstrate qualitative comparisons in Fig. 5. Where our model produces more visually stable results than Depth Anything V2, while successfully preserving the structure of the image model prediction."}, {"title": "4.2. Video Normal Estimation Results", "content": "In the absence of a standard video normal estimation benchmark, we establish our evaluation protocol based on the image-level metrics from [2]. We select two datasets containing continuous frames: Sintel [7] (synthetic dynamic scenes) and ScanNet [11]. For each scene, we uniformly sample 32 frames as test sequences. We evaluate using three image-based metrics: mean and median angles between predicted and ground truth normals, percentage of predictions within 11.25\u00b0 of ground truth, plus the video smoothness metric from our depth evaluation.\nResults in Tab. 2 show that our model maintains performance comparable to the image backbone on per-frame metrics while significantly improving temporal smoothness. The limited improvement in image-based metrics is expected, as these metrics primarily assess per-frame accuracy rather than temporal consistency. The substantial improvement in the smoothness metric demonstrates our model's ability to generate temporally coherent predictions, as visualized in Fig. 6."}, {"title": "4.3. Ablation Study", "content": "We conduct ablation studies on KITTI depth estimation to validate our design choices. We compare our full model against four variants: (1) Ours $w_{reg.} = 0.1$ and (2) Ours $w_{reg.} = 3$ use different regularization loss weights; (3) Ours no mask omits the optical flow masking; and (4) Ours all frames applies regularization to all frames instead of a single random frame. Our full model outperforms the first three variants, validating our design choices. Interestingly, Ours all frames shows similar performance to our standard model, suggesting that single-frame regularization sufficiently maintains alignment with the image prior."}, {"title": "5. Discussion", "content": "In this work, we present a zero-shot framework for video geometric buffer estimation that eliminates the need for paired video-buffer training data. By leveraging state-of-the-art single-view priors combined with optical flow-based temporal consistency, our approach achieves temporally stable and coherent results that match or surpass those of methods trained on large-scale video datasets, as demonstrated in our experiments.\nWhile our approach highlights the power of combining image model priors with optical flow smoothness guidance, there are areas for improvement. First, as our model builds upon image model priors, it may struggle in extreme cases where the backbone model completely fails. Second, while optical flow provides smoothness and temporal consistency between adjacent frames, it only account for correlations across continuous frames. It may fail to, for instance, capture consistent depth information for objects that temporarily leave and re-enter the scene. To tackle these problems, we believe promising future directions are to incorporate large-scale image models with limited video supervision for a hybrid training, or to develop more sophisticated cross-frame consistent guidance (e.g. losses defined in 3D space).\nIn summary, we propose this framework as a promising step toward reducing reliance on costly video annotations for geometric understanding tasks, offering valuable insights for future research in video inversion problems."}, {"title": "6. More Video Results", "content": "In addition to the qualitative comparisons in the paper, we provide more animated results in our supplementary web-site for better visualization of the prediction quality."}, {"title": "7. More Implementation Details", "content": "All models are implemented in PyTorch [37]. We utilize the official implementations of Depth Anything V2 [56] and Marigold-E2E-FT [20], adapting temporal blocks from the UnetMotion architecture in the Diffusers [49] library. Experiments are conducted on NVIDIA H100 GPUs with 80GB memory. Due to memory constraints, we limit the maximum sequence length to 110 frames for depth estimation and 32 frames for normal estimation.\nFor training, we use a dataset of approximately 200K videos, with each clip containing 128 frames. We employ the AdamW [35] optimizer with learning rates of $10^{-4}$ and $10^{-5}$ for depth and normal estimation, respectively. Training begins with a 1,000-step warm-up phase, during which the learning rate increases linearly from 0 to its target value. The training process runs on 24 H100 GPUs with a total batch size of 24 and incorporates Exponential Moving Average (EMA) with a decay coefficient of 0.999. The complete training cycle requires approximately one day to complete 15,000 iterations."}, {"title": "7.1. Details of the Deferred Back-Propagation", "content": "In our normal model, we employ deferred back-propagation as proposed by Zhang et al. [60] to reduce memory consumption. Algorithm 1 outlines the detailed implementation steps. Notably, the gradients obtained by back-propagating $\\mathcal{L}_{def}$ are equivalent to those computed from the pixel-wise loss function $\\mathcal{L}_{pix}$ across all decoded frames:\n$\\mathcal{L}_{def} \\stackrel{?}{=} \\frac{1}{K} \\sum_{k} \\sum_{x} L_{pix} (D(z_k) \\bullet z_k)$\n$\\stackrel{(7)}{=} \\frac{1}{K} \\sum_{k} \\sum_{x} L_{pix} (D(S\\mathcal{G} (g_k)) \\bullet z_k )$\n$\\stackrel{(8)}{=}  \\frac{1}{K} \\sum_{k} \\sum_{x}  S\\mathcal{G} (g_k) \\cdot \\frac{d \\mathcal{L}_{pix} (D(z_k))}{dz_k} \\bullet z_k  $\n$\\stackrel{(9)}{=}  \\frac{1}{K} \\sum_{k}  S\\mathcal{G} (g_k) \\bullet S\\mathcal{G} (\\frac{d \\mathcal{L}_{pix} (D(z_k))}{dz_k} ) z_k   $\n$\\stackrel{(10)}{=}   \\frac{1}{K}  \\sum_{k}    S\\mathcal{G} (g_k) z_k  $"}, {"title": "7.2. Details of the Optical Flow Based Stabilization", "content": "Algorithm 2 presents the pseudo-code for our optical flow based stabilization loss calculation. The loss is computed separately for forward optical flow (previous frame to next frame) and backward flow (next frame to previous frame), then combined together. This stabilization algorithm is applied to both depth and normal models. In our experiments, we set the threshold $\\tau_{e}$ to 0.34."}, {"title": "8. Additional Ablation Studies", "content": "We extend our ablation studies beyond the main paper by comparing our model with additional variants: Model with $\\mathcal{L}_{1}$ replaces $\\mathcal{L}_{2}$ with $\\mathcal{L}_{1}$ for the affine-invariant relative loss in the depth model; Model w/o fine-tuning maintains a fixed refinement network from the backbone model while training only the temporal layers. Additionally, we evaluate an enhanced version utilizing \"oracle\" knowledge: Model with DepthCrafter incorporates a single frame from DepthCrafter [30] prediction per iteration as regularization guidance.\nAs shown in Table 4, our model demonstrates superior performance compared to the first two variants, validating the effectiveness of both our architectural and loss function designs. The Model with DepthCrafter achieves better results that comparable to DepthCrafter itself, suggesting potential for future improvements through enhanced image priors."}]}