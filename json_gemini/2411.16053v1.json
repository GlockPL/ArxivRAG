{"title": "UnitedVLN: Generalizable Gaussian Splatting for Continuous Vision-Language Navigation", "authors": ["Guangzhao Dai", "Jian Zhao", "Yuantao Chen", "Yusen Qin", "Hao Zhao", "Guosen Xie", "Yazhou Yao", "Xiangbo Shu", "Xuelong Li"], "abstract": "Vision-and-Language Navigation (VLN), where an agent follows instructions to reach a target destination, has recently seen significant advancements. In contrast to navigation in discrete environments with predefined trajectories, VLN in Continuous Environments (VLN-CE) presents greater challenges, as the agent is free to navigate any unobstructed location and is more vulnerable to visual occlusions or blind spots. Recent approaches have attempted to address this by imagining future environments, either through predicted future visual images or semantic features, rather than relying solely on current observations. However, these RGB-based and feature-based methods lack intuitive appearance-level information or high-level semantic complexity crucial for effective navigation. To overcome these limitations, we introduce a novel, generalizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables agents to better explore future environments by unitedly rendering high-fidelity 360\u00b0 visual images and semantic features. UnitedVLN employs two key schemes: search-then-query sampling and separate-then-united rendering, which facilitate efficient exploitation of neural primitives, helping to integrate both appearance and semantic information for more robust navigation. Extensive experiments demonstrate that UnitedVLN outperforms state-of-the-art methods on existing VLN-CE benchmarks.", "sections": [{"title": "1. Introduction", "content": "Vision-and-Language Navigation (VLN) [7, 16, 18, 60] requires an agent to understand and follow natural language instructions to reach a target destination. This task has recently garnered significant attention in embodied AI [62, 82]. Unlike traditional VLN, where the agent navigates a predefined environment with fixed pathways, Continuous Environment VLN (VLN-CE) [3, 115] presents a more complex challenge. In VLN-CE, the agent is free to move to any unobstructed location, making low-level ac-"}, {"title": "2. Related Work", "content": "Vision-and-Language Navigation (VLN). VLN [7, 60, 62, 82] recently has achieved significant advance and increasingly introduced several proxy tasks, e.g., step-by-step instructions [7, 62], dialogs-based navigation [104], and object-based navigation [82, 132], et al. Among them, VLN in the Continuous Environment (VLN-CE) [7, 33, 44, 100, 114] entails an agent following instructions freely move to the target destination in a continuous environment. Simi-"}, {"title": "3. Method", "content": "Task Setup. UnitedVLN focuses on the VLN-CE [60, 62] task, where the agent is required to follow the natural instructions to reach the target location in the Continuous Environment. The action space in VLN-CE consists of a set of low-level actions, i.e., turn left 15 degrees, turn right 15 degrees, or move forward 0.25 meters. Following the standard panoramic VLN-CE setting [3, 43, 59], at each time step t, the agent receives a 360\u00b0 panoramic observations that consists of 12 RGB images $I_t = \\{r_{t,i}\\}_{i=1}^{12}$ and 12 depth images $D_t = \\{d_{t,i}\\}_{i=1}^{12}$ surrounding its current location (i.e., 12 view images with 30\u00b0 each separation). For"}, {"title": "3.1. Neural Point Initialization", "content": "During navigation, the agent gradually stores visual observations of each step online, by projecting visual observations, including current nodes and visited nodes, into point cloud B and feature cloud M. Meanwhile, at each step, we also use a waypoint predictor [43] pre-trained on MP3D dataset [11] to predict navigable candidate nodes, following the practice of prior VLN-CE works [2, 3, 106]. Note that B and M share a similar way of construction just different in projected subjects, i.e., images (B) and feature map (M). Here, we omit the construction of M for sake of readability. Please see for more details in supplementary material about the construction of M.\nPoint Cloud B stores holistic appearances of observed environments (e.g., current node and visited nodes), which consists of pixel-level point positions and colors, as shown in Figure 2. Specifically, at each time step t, we first use 12 RGB images $I_t = \\{r_{t,i}\\}_{i=1}^{12}$ to enumeratively project pixel colors $\\{c_{t,i} \\in R^{H \\times W \\times 3}\\}_{i=1}^{12}$, where $H \\times W \\times 3$ denotes RGB images resolution. For the sake of calculability, we omit all the subscripts (i, h, w) and denoted it as l, where l ranges from 1 to L, and L = 12\u00b7H\u00b7W. Then, we use $D_t = \\{d_{t,i}\\}_{i=1}^{12}$ to obtain the point positions. Through camera extrinsic [R, T] and intrinsics K, each pixel in the i-view image rt,i is mapped to its 3D world position $P_{t,l} = [P_x, P_y, P_z]^T$ using depth images dt,i, as\n$P_{t,l} = [d_l R^{-1} K^{-1} [h\\ w\\ 1]^T - T]^T.$\nThus, for all 12 views in each step, the holistic appearances of observed environments are stored, by gradually perceiving point colors and their positions into the point cloud B, as\n$B_t = B_{t-1} \\cup \\{\\[P_{t,j}, C_{t,j}]\\}_{j=1}^{L}.$"}, {"title": "3.2. Search-Then-Query Sampling", "content": "We propose a Search-Then-Query sampling (STQ) scheme to query K-nearest points for each point in B and M within a certain search range, improving model efficiency. After sampling, it will be fed into different MLPs to regress neural properties and images/feature Gaussians, respectively.\nSampling in 3DGS. To obtain more representative points, we filter low-information or noisy points in the source point cloud B by two steps, i.e., point search and point query. For point search, we first build an occupancy tree to represent each point $p_l$ occupancy in B with a coarse-grained grid way, based on KD-Tree [36] algorithm. Then, with an initial occupancy $\\epsilon$ threshold, we quickly detect all satisfied points $P$ in B based on its occupancy from the occupancy tree. After detecting satisfied points, we search it K nearest points by a K-nearest matrix of distance $D_{kn}$, as\n$D_{kn} = D_{tree} \\( \\{p_i \\in P  \\|\\ d_{oc}(p_i) < \\epsilon^2 \\}, K \\).$\nwhere doc(pi) denotes the square distance of pi the nearest neighbor grid point, and Dtree denotes K-nearest search from occupancy tree.\nBased on Dkn, for these satisfied points filtered by Eq. 4, we then calculate the total distance (i.e., density) to its neighbored points. Following HNR [115], we query points with local maxima in the density distribution in its surroundings, which represent the most representative and dense structural information. In this way, the dense source points P is reformulated to sparse new points P':\n$P' = \\{q \\|\\  q \\in P, \\ d \\< \\Gamma \\( \\frac{1}{\\mid \\Sigma_j D_{ij} \\mid}  \\) \\cup A  \\( \\frac{1}{\\Sigma_j D_{ij}} \\) \\},\\$\nwhere Dij denotes the distance between the i-th query point and its j-th neighbor. The \u0393 and \u039b denote density and peak selection functions, respectively.\nWith sampled points P' and their corresponding colors C', we next to regress its image/feature Gaussians via 3DGS. Specifically, for each point in B', we first use a multi-input and single-output UNet-like architecture [109] to encode points with different scales to obtain neural descriptors F. Then, we regress it to several Gaussian properties, i.e, rotation matrix $R \\in \\mathbb{R}^4$, scale factor $S \\in \\mathbb{R}^3$, opacity $\\alpha \\in [0, 1]$, as"}, {"title": "3.3. Separate-Then-United Rendering", "content": "As shown in Figure 2, we render future observations, i.e., the feature map (in NeRF branch), image and feature map (in 3DGS branch), with the obtained feature radiance r (cf. in Eq. 10), volume density \u03c3 (cf. in Eq. 11) and image/feature Gaussian G\u25b3/G\u2207 (cf. in Eq. 21). Specifically, by leveraging the differentiable rasterizer, we first render image/feature Gaussian to image/feature map Ig/F, as,\n$I_g = \\Omega\\{\\mathbb{R}, S, \\alpha, P', C' \\| (K, \\[R, T]))\\},\\$\n$F_g = \\Omega\\{\\mathbb{R}, S, \\alpha, P', F', \\| (K, \\[R, T]))\\},\\$\nwhere K and [R, T] denote the camera intrinsic and extrinsic, and \u03a9 denotes Gaussian rasterization.\nBased on $I_g$ and $F_g$, we then use two visual encoders to extract their corresponding image/feature-based embedding $f_g^i / f_g^f$, following the patch position and patch embedding [28]. After that, we use multi-head cross-attention to bridge appropriate semantics from $f_g^f$ into $f_g^i$, for better generalization. To sum up, the aggregation representation $f_{rf}^g$ can be formulated as,\n$f_g^i, f_g^f = g_1(I_g), g_2(F_g),$\n$f_{rf}^g = CA(f_g^i, f_g^i, f_g^f),$\nwhere $g_1$ and $g_2$ denote two visual encoder of CLIP-ViT-B [85] for encoding image and feature map, and CA denotes operation of multi-head cross-attention.\nIn NeRF branch, with the obtained feature radiance r and volume density \u03c3 of sampled points along the ray, we render the future feature Ff by using the volume rendering [75]. Similarly, with the patch embedding and position, we use the encoder $ of$ Transformer [105] to extract the feature embedding $f_n^g$ for Ff in NeRF, as\n$f_n^g = g(F_f).$\nTo improve navigation robustness, we aggregate future feature embedding $f_{rf}^g$ (cf. in Eq. 15) and $f_n^g$ (cf. in Eq. 16) as a view representation of future environment. Similarly, in this way, we also obtain other 11 future-view embedding. After that, we aggregate all 12 future-view embeddings Ffuture via average pooling and project them to a future node. Finally, we use a feed-forward network (FFN) to predict navigation goal scores between the candidate node Fcandidate and the future node Ffuture in the topological map, following practices of previous methods [3, 115]. Note that the scores for visited nodes are masked to avoid agent unnecessary repeated visits. Based on navigation goal scores, we select a navigation path with a maximum score, as\n$S^{path} = \\{Max(\\[FFN(F_{candidate}), FFN(F_{future})]\\}.$"}, {"title": "3.4. Objective Function", "content": "According to the stage of VLN-CE, UnitedVLN mainly has two objectives, i.e., one aims to achieve better render quality of images (cf. Eq 12) and features (cf. Eq 13) in the pre-training stage and the other is for better navigation performance (cf. Eq 17) in the training stage. Please see supplementary material for details about the setting of loss."}, {"title": "4. Experiment", "content": "4.1. Datasets and Evaluation Metrics\nDatasets. To improve the rendered quality of images and features, we first pre-train the proposed 3DGS-based UnitedVLN on the large-scale indoor HM-3D dataset. Following the practice of prior VLN-CE works [3, 43, 106], we evaluate our UnitedVLN two VLN-CE public benchmarks, i.e, R2R-CE [60] and RxR-CE [62]. Please see supplementary material for more details about the illustration of datasets."}, {"title": "4.4. Ablation Study", "content": "We conduct extensive ablation experiments to validate key designs of UnitedVLN. Results are reported on the R2R-CE val unseen split with a more complex environment and difficulty, and the best results are highlighted.\nEffect on each component of UnitedVLN. In this part, we analyze the effect of each component in UnitedVLN. As illustrated in Table 3, A0 (Base) denotes that only use the baseline model for VLN-CE. Compared with A1, the performance of A2 (Base + STQ) on Section 3.2 is significantly improved, by improving +3.0(%) on SR. It proves that STQ can effectively enhance the performance of navigation by performing efficient neural point sampling. Compared with"}, {"title": "4.5. Qualitative Analysis", "content": "To validate the effect of UnitedVLN for effective navigation in a continuous environment, we report the visualization comparison of navigation strategy between the baseline model (revised ETPNav) and Our UnitedVLN. Here, we also report each node navigation score for a better view, as shown in Figure 3. As shown in Figure 3, the baseline model achieves navigation error as obtains limited observations by relying on a pre-trained waypoint model [43] while our UnitedVLN achieves correct decision-marking of navigation by obtaining full future explorations by aggregating intuitive appearances and complicated semantics information. This proves the effect of RGB-united-feature future representations, improving the performance of VLN-CE."}, {"title": "5. Conclusion and Discussion", "content": "Conclusion. We introduce UnitedVLN, a generalizable 3DGS-based pre-training paradigm for improving Continuous Vision-and-Language Navigation (VLN-CE). It pursues full future environment representations by simultaneously rendering the visual images the semantic features with higher-quality 360\u00b0 from sparse neural points. UnitedVLN has two insightful schemes, i.e., Search-Then-Query sampling (STQ) scheme, and separate-then-united rendering (STU) scheme. For improving model efficiency, STQ searches for each point only in its neighborhood and queries its K-nearest points. For improving model robustness, STU aggregate appearance by splatting in 3DGS and semantics information by volume-rendering in NeRF for robust navigation. To the best of our knowledge, UnitedVLN is the first work that integrates 3DGS and NeRF into the united model for assembling the intuitive appearance and complicated semantics information for VLN-CE in a generalized way. Extensive experiments on two VLN-CE benchmarks demonstrate that UnitedVLN significantly outperforms state-of-the-art models.\nDiscussion. Some recent work (e.g., HNR [115]) share the same spirit of using future environment rendering but there are some distinct differences: 1) There are different paradigms (3DGS vs. NeRF). 2) There are different future explorations (RGB-united-feature vs. feature). 3) There are different scalability (efficient-and-fast rendering vs. inefficient-and-slow rendering). For the proposed UnitedVLN framework, this is a new paradigm that focuses on the full future environment representation besides just rough on single-and-limited features or images. Both STQ"}, {"title": "Appendix", "content": "This document provides more details of our method, experimental details and visualization examples, which are organized as follows:\n\u2022 Model Details (cf. \u00a7 A);\n\u2022 Experiment Details (cf. \u00a7 B);\n\u2022 Visualization Example (cf. \u00a7 C).\nThe anonymous code of UnitedVLN is available: https://anonymous.4open.science/r/UnitedVLN-08B6/\nA. Model Details\nA.1. Details of Feature Cloud.\nFeature cloud M focuses on fine-grained contexts of observed environments, consisting of neural-point positions and grid features of features map. Following HNR [115], we use a pre-trained CLIP-ViT-B [85] model to extract grid features $\\{h_{t,i} \\in  ^{\\'W\\'\\times D'}\\}_{i=1}^{12}$, for 12 observed RGB images $R_t = \\{r_{t,i}\\}_{i=1}^{12}$ at time step t. Here, $H' \\times W' \\times D'$ denotes the resolution of feature maps extracted by CLIP. For the sake of calculability, we omit all the subscripts (i, h', w') and denote its as u, where u ranges from 1 to U, and U = 12\u00b7H'W'. Then, each grid feature $9_{t,u} \\in \\mathbb{R}^D$ in M is mapped to its 3D world position $Q_{t,j} = [q_x, q_y, q_z]$ following the mapping in Eq. 1. Besides the horizontal orientation $O_{t,j}$, we also calculate size grid feature scale $s_{t,j}$ using camera's horizontal field-of-view O_{HFOV}, as follows\n$s_{t,u} = 1/W' \u00b7 \\[tan(O_{HFOV}/2) \u00b7 d_{t,u}],$\nwhere W' is the width of the features maps extracted by the CLIP-VIT-B for each image. In this way, all these grid features and their spatial position of feature maps are perceived in the feature cloud M:\n$M_t = M_{t-1} \\cup \\{\\[Q_{t,u}, H_{t,u}, O_{t,u}, s_{t,u}]\\}_{u=1}^U$\nA.2. Details of Pcd U-Net.\nAs shown in Fig. 6, we utilize a multi-input and single-output UNet-like architecture (Pcd U-Net) to encode points in the point cloud with different scales to obtain neural descriptors. Specifically, the UNet-like architecture has three set abstractions (SA) and three feature propagations (FP) including multilayer perceptron (MLP), point-voxel convolution (PVC) [74], Grouper block (in SA) [80], and Nearest-Neighbor-Interpolation (NNI). Through the above modules, we downsample the original point cloud with decreasing rates and concatenate the downsampled point clouds with different levels of feature maps as extra inputs. Thus, the neural descriptors F can formulated as,\n$F' = U((P', C'), (P', C') \u2193r_1, (P', C')\u2193r_2),$\nwhere the U denotes UNet-based extractor network for point cloud. And, the P' and C' denote sampled point coordinates and colors (cf. in Eq. ??). The \u2193 represents a uniform downsampling operation and r denotes the sampling rate where r\u2081 > r2. This operation allows the extractor to identify features across various scales and receptive fields. The obtained single output feature vector serves as the descriptor for all points.\nAfter that, with the obtained neural descriptors, coordinates as well as colors, we use different heads to regress the corresponding Gaussian properties in a point-wise manner. As shown in Fig. 6, the image Gaussian regressor contains three independent heads, such as convolutions and corresponding activation functions, i.e., rotation quaternion (R), scale factor (S), and opacity (\u03b1). Meanwhile, in this way, we use the neural descriptors P to replace colors and obtain feature Gaussians, following general point-rendering practice [109]. Thus, the images Gaussians G and feature Gaussians G in the 3DGS branch can be formulated as,\nG\u25b3, G\u2207 = {R, S,\u03b1, P', C'}, {R, S, a, P', F}.$\nA.3. Details of Volume Rendering.\nWe set the k-nearest search radius R as 1 meter, and the radius R for sparse sampling strategy is also set as 1 meter. The rendered ray is uniformly sampled from 0 to 10 meters, and the number of sampled points is set as 256.\nA.4. Details of Gaussian Splatting.\n3DGS is an explicit representation approach for 3D scenes, parameterizing the scene as a set of 3D Gaussian primitives, where each 3D Gaussian is characterized by a full 3D covariance matrix \u03a3, its center point x, and the spherical harmonic (SH). The Gaussian's mean value is expressed as:\n$G(x) = e^{-(\\bar{x})^T -1(\\bar{x})}.$\nTo enable optimization via backpropagation, the covariance matrix \u03a3 could be decomposed into a rotation matrix (R) and a scaling matrix (S), as\n$\u03a3 = RSS^T RT.$\nGiven the camera pose, the projection of the 3D Gaussians to the 2D image plane can be characterized by a view transform matrix (W) and the Jacobian of the affine approximation of the projective transformation (J), as,\n$\u03a3' = JWEW^T JT,$\nwhere the \u03a3' is the covariance matrix in 2D image planes. Thus, the a-blend of N ordered points overlapping a pixel is utilized to compute the final color C of the pixel:\n$C = \\Sigma_{i\\inN} C_i\\alpha_i \\[1 - \\alpha_j]."}]}