{"title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge\nGraph-based False Premise Questions", "authors": ["Yanxu Zhu", "Jinlin Xiao", "Yuhang Wang", "Jitao Sang"], "abstract": "Recent studies have demonstrated that large\nlanguage models (LLMs) are susceptible to be-ing misled by false premise questions (FPQs),\nleading to errors in factual knowledge, know\nas factuality hallucination. Existing bench-\nmarks that assess this vulnerability primar-ily rely on manual construction, resulting in\nlimited scale and lack of scalability. In this\nwork, we introduce an automated, scalable\npipeline to create FPQs based on knowledge\ngraphs (KGs). The first step is modifying\ntrue triplets extracted from KGs to create false\npremises. Subsequently, utilizing the state-of-the-art capabilities of GPTs, we generate\nsemantically rich FPQs. Based on the pro-posed method, we present a comprehensive\nbenchmark, the Knowledge Graph-based False\nPremise Questions (KG-FPQ), which contains\napproximately 178k FPQs across three knowl-edge domains, at six levels of confusability,\nand in two task formats. Using KG-FPQ, we\nconduct extensive evaluations on several repre-sentative LLMs and provide valuable insights.\nThe KG-FPQ dataset and code are available\nat https://github.com/yanxuzhu/KG-FPQ.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Zhao et al., 2023)\nexcel in natural language understanding and gen-\neration but often produce texts that deviate from\nreal-world factual knowledge, a problem known as\nfactuality hallucination (Huang et al., 2023). This\nissue restricts their applicability in scenarios requir-ing high factual accuracy.\nRecent studies (Vu et al., 2023; Yuan et al.,\n2024) have demonstrated that False Premise Ques-\ntions (FPQs) can induce factuality hallucination in\nLLMs, as these models often respond directly to\nFPQs without verifying their validity. A FPQ is a\nquestion that contains incorrect facts which are not"}, {"title": "2 Related Work", "content": "Evaluation of Factuality Hallucination Many\nbenchmarks evaluate factuality hallucination (Lin\net al., 2022; Li et al., 2023; Min et al., 2023; Muhl-gay et al., 2024) due to the risks it poses in practical\nLLM applications. The evaluation formats are pri-marily divided into discriminative evaluation (Lin\net al., 2022; Li et al., 2023; Muhlgay et al., 2024)\nand generative evaluation (Lin et al., 2022; Min\net al., 2023), which respectively assess the abil-ity of LLMs to distinguish factual statements and\ngenerate factual content (Zhang et al., 2023). Hal-lucination induced by FPQs belongs to factuality\nhallucination, and this paper evaluates this vulnera-bility in both discriminative and generative formats.\nFalse Premise Questions Existing FPQ bench-marks (Yu et al., 2023; Kim et al., 2023; Hu et al.,"}, {"title": "3 KG-FPQ Benchmark Construction", "content": "We explore an automated, scalable method to\nconstruct FPQs. The first step involves extracting\ntrue triplets from knowledge graphs (KGs) and edit-ing them into false triplets. Subsequently, GPTs\nare utilized to generate FPQs based on these false\ntriplets. Specifically, We extract triplets from the\nKG\u00b9 in the form of <subject, relation, object>\nand edit the object to create false triplets <subject,\nrelation, edited object>. We design editing meth-ods from two perspectives: 1) the edited object\nat varying distances from the subject in the KG;\n2) the edited object having varying associations\nwith the original object in the KG. As the example\nshown at the bottom of Figure 1, we edit the true\ntriplet <John Lennon, place of death, New York\nCity> to the false triplet <John Lennon, place of\ndeath, Liverpool>. Liverpool is a 1-hop neighbor\nof John Lennon and belongs to the same concept\nas New York City but has a different relation to\nthe subject. There are six editing methods to cre-ate false triplets varying in levels of confusabil-ity. After editing, we utilize GPT-3.5 (OpenAI,\n2023) and GPT-4 (OpenAI, 2024)\u00b2 to generate\nFPQs in Yes-No and WH formats respectively cor-responding to discriminative and generative eval-uation of hallucination (Zhang et al., 2023). By\nthe proposed method, we present a comprehensive\nbenchmark, the Knowledge Graph-based False\nPremise Questions, which contains FPQs across\nthree knowledge domains, at six levels of confus-ability, and in two task formats. The comparison\nbetween KG-FPQ and other datasets is detailed in\nTable 1.\nWe evaluate the performance of several repre-sentative and advanced LLMs on KG-FPQ across\nboth discriminative and generative tasks. Since\nmanual evaluation of the generative task is costly,\nwe introduce an automated evaluator named FPQ-\nJudge to identify whether responses of LLMs to\nFPQs are misled by the false premises, achieving\na 93% accuracy rate on a manually annotated test\nset. Through extensive experiments, we reach three"}, {"title": "3.1 Triplets Extraction and Editing", "content": "We utilize KoPL, a high-quality subset of Wikidata,\nas our KG. KOPL contains a limited set of concepts\nand relations, where each entity uniquely belongs\nto one concept. We follow the steps shown in the\nleft of Figure 2 to extract and edit triplets. First,\nwe select entities from three domains: Art, People\nand Place, based on their concepts, and filter the\nrelations for each domain. The filtering rules are\ndetailed in Appendix A, and Table 3 lists the repre-"}, {"title": "3.2 Data Generation and Verification", "content": "As shown in the right of Figure 2, firstly, we sam-ple 1k triplets to validate the feasibility of gener-ating FPQs with GPTs. Through manual check,\nwe identify that triplets containing certain relations\neasily make the generated data semantically inco-herent, so we exclude those data for further data\ngeneration. Secondly, we generate the full dataset,\nutilizing GPT-3.5 to create Yes-No questions and\nGPT-4 to create WH-questions. We prompt GPTs\nto generate TPQs based on true triplets and then\nreplace the original object with the edited object\nfrom false triplets through string matching. There-fore, we create one TPQ and six FPQs in each\nformat based on each true triplet, with these FPQs\nin each format differing only in the edited object.\nThe prompts used for data generation are shown\nin Appendix A. Finally, to ensure data quality, we\nperform a manual review of the dataset, with par-ticular attention to WH-questions, correcting some\ngrammatical and semantic errors.\nBy our method, the number of triplets extracted\nfrom KoPL can be relatively large, and we sam-"}, {"title": "4 Experiment Settings", "content": "We propose an automated and scalable\npipeline combining KGs and GPTs for con-structing FPQ datasets, by editing true triplets\ninto false triplets and utilizing GPTs to gener-ate FPQs."}, {"title": "4.1 Tasks", "content": "Discriminative Task The first task involves the\ndiscriminative task, where LLMs are required to\nanswer Yes-No questions in KG-FPQ with \u201cYes\u201d\nor \u201cNo\u201d only, without providing explanations. An\nexample for FPQ in Yes-No format is that Did John\nLennon die in Liverpool?.\nGenerative Task The second task involves the gen-erative task, where LLMs are required to answer\nthe WH-questions in KG-FPQ. An example for\nFPQ in WH format is that Where in Liverpool did\nJohn Lennon die?. If LLMs recognize the false\npremises in FPQs, they will deny the false premises"}, {"title": "4.2 Models", "content": "We select several representative and advanced open-source chat models of various sizes. Models\nin the 6B~8B range include ChatGLM3-6B (Du\net al., 2022), Baichuan2-7B-Chat (Baichuan, 2023),\nLlama2-7B-Chat (Touvron et al., 2023), Qwen1.5-7B-Chat (Bai et al., 2023), and Llama3-8B-instruction 3. Models in the 13B~14B range\ninclude Baichuan2-13B-Chat (Baichuan, 2023),\nLlama2-13B-Chat (Touvron et al., 2023), and\nQwen1.5-14B-Chat (Bai et al., 2023). We also eval-uate advanced two closed-source LLMs, GPT-3.5\nand GPT-4 on the discriminative task. We set the\ntemperature parameter to 0.6 and the top_p param-eter to 0.9 for all models in both the discriminative\ntask and the generative task."}, {"title": "4.3 Evaluation", "content": "Evaluation Procedure Our evaluation procedure\nis shown in Figure 4. First, we input the Yes-No\nformat TPQs into the LLMs. If the LLMs answer\n\"Yes\", it indicates that the LLMs have stored rele-vant background knowledge for the question. We\nthen continue with the corresponding FPQs in both\nthe discriminative task and the generative task. If\nthe LLMs answer \u201cNo\u201d, we do not proceed with the\nFPQs for that TPQ. This approach aims to reduce\nthe hallucination caused by a lack of background\nknowledge. To increase the robustness of the as-sessment, we input each question three times to\nobtain three responses, and then perform a hard\nvote to get a final answer label. The prompt tem-"}, {"title": "5 Results", "content": "Table 10 presents the complete evaluation results of\nall models for FPQs on both the discriminative task\nand the generative task across three domains. Ta-ble 2 presents the results of the Art domain, which\nwe use as an example for preliminary analysis. It\ncan be observed that the accuracy of LLMs varies\nacross FPQs with different levels of confusability,\nand their performance also differs based on the\ntask format. In Section 5.1, we will further ana-lyze the relationship between the confusability of\nFPQs and the factuality hallucination. In Section\n5.2, we will examine the impact of task format on\nfactuality hallucination. Additionally, Section 5.3\nand Section 5.4 will provide detailed analyses from\nthe perspectives of knowledge domains and model"}, {"title": "5.1 Impact of confusability of FPQs", "content": "As shown in Figure 3, we design editing methods\nfrom two perspectives, distance and association,\nand create FPQs at six levels of confusability. In\nthis section we will discuss the impact of confus-ability of FPQs from these two perspectives."}, {"title": "5.1.1 Impact of Distance", "content": "To investigate the impact of the distance between\nthe edited object and the subject within the KG, the\naverage accuracy of all LLMs on NSC and NNSC\nis calculated in both discriminative and generative\ntasks across three domains, as illustrated in Fig-ure 5. The results demonstrate that, the average\naccuracy for NSC is consistently lower than for\nNNSC across all domains, and this phenomenon is\nmore pronounced in the discriminative task. This\nindicates that FPQs formed when the edited ob-ject in the false triplets is a neighbor of the subject\nare more confusing to LLMs, resulting in a higher\nprobability of factuality hallucination.\nFurthermore, we conduct a more detailed exami-nation of NSC and NDC to investigate the impact of\nthe number of hops between the edited object and\nthe subject. The complete results are shown in Ap-pendix C.1, and we analyze the NSC in Art domain\nas an example in this section, with results presented\nin Figure 6. It is observed that for most models, the\naccuracy improves as the number of hops between\nthe edited object and the subject increases, indicat-ing a reduction in factuality hallucination, and this\ntrend is more evident in discriminative tasks.\nIn conclusion, when the edited object and the\nsubject in the false triplets has a closer distance,"}, {"title": "5.1.2 Impact of Associations", "content": "To explore the impact of the associations between\nthe edited object and the original object on FPQs-induced factuality hallucination, we calculate the\naverage accuracy of all LLMs on NSC vs. NDC,\nNNSC vs. NNDC, NNSR vs. NNDR, and NNSC\nvs. NNSR in both tasks across three domains, as\nillustrated in Figure 7.\nFrom the comparison of NSC vs. NDC, and\nNNSC vs. NNDC in upper Figure 7, it is evident\nthat in all domains, whether in the discriminative\nor generative task, the average accuracy for NSC is\nconsistently lower than for NDC, and as the same,"}, {"title": "5.2 Impact of Task Format", "content": "We analyze the overall performance of each LLM\nin both discriminative and generative tasks, with\nthe complete results shown in Appendix C.2. This\nsection provides an analysis of the Art domain,\nwith results presented in Figure 8. It is evident that\nfor almost all LLMs, the overall accuracy in the\ngenerative task is lower than in the discriminative\ntask, suggesting that LLMs perform worse at gen-erating factual statements than at distinguishing\nthem when faced with FPQs. This highlights that"}, {"title": "5.3 Impact of Knowledge Domain", "content": "Following the procedure shown in Figure 4, we first\nevaluate LLMs on Yes-No format TPQs, with the\nresults presented in Table 12. We propose a hypoth-esis: From the domain perspective, higher accuracy\non TPQs indicates that LLMs are more familiar\nwith the knowledge in that domain, and therefore,\nthe accuracy on FPQs in that domain should also\nbe higher, implying that LLMs are less likely to\nbe misled by FPQs. To verify it, we compare the\nresults of TPQs and FPQs, shown in Figure 9. The\naverage accuracy of TPQs is higher in the People\ndomain compared to Art and Place, whereas the\naverage accuracy of FPQs is highest in the Place\ndomain compared to Art and People. This indicates"}, {"title": "5.4 Impact of Model size", "content": "The evaluated models are classified into 3 cate-gories according to their size: 6B~8B, 13B~14B,\nand the GPT series. We then calculate the average\naccuracy for FPQs of each categories across 3 do-mains, as shown in Figure 10. It can be observed\nthat, regardless of the task format, the average ac-curacy tends to increase with larger model sizes.\nThis indicates that larger models are more fac-tual in answering FPQs. A similar analysis is con-ducted for TPQs as presented in Appendix C.4. The\nGPT series demonstrate the highest performance\non TPQs, while the 6B~8B LLMs outperform the\n13B~14B LLMs, which is counterintuitive.\nObserving Table 12, we find that the accuracy\nof the Baichuan2 series is significantly higher than\nthat of other models, and the accuracy of Llama2-13B-Chat is even far below the random guessing\nprobability of 0.5. We undertake a closer exam-"}, {"title": "6 Conclusion and Discussion", "content": "To evaluate factual hallucination induced by false\npremise questions in LLMs, we develop an auto-mated and scalable pipeline to construct FPQs by\nediting the triplets in a KG and utilizing GPTs\nto generate data. Based on the proposed method\nwe create a comprehensive benchmark, KG-FPQ,\noffering multiple perspectives for evaluation. Us-ing KG-FPQ, we assess several advanced LLMs.\nThrough extensive experiments, we reach three es-sential conclusions: (1) FPQs with different levels\nof confusability have varying degrees of impact on\nLLMs. (2) LLMs perform worse at generating fac-tual statements than at distinguishing them when\nfaced with FPQs. (3) Knowledge proficiency of\nLLMs varies across domains, and there is no pos-itive correlation between knowledge proficiency\nand the ability to resist the interference of FPQs.\nBased on analysis in Section 5.1, we speculate\nthat the internal knowledge storage structures of\nLLMs may resemble knowledge graphs, which we\nwill explore further in future research. Additionally,\nFPQs can be exploited as prompt injection attacks,\nleading LLMs to generate non-factual texts and\nspread misinformation online. In order to identify\nand mitigate more potential vulnerabilities, we will\nexpand the variety of FPQs for red teaming LLMs."}, {"title": "Limitations", "content": "We propose a comprehensive FPQ benchmark,\nbased on which we evaluate the FPQ-induced fac-tual hallucinations in several advanced LLMs in\nboth discriminative and generative formats. How-ever, our work still faces limitations and challenges.\nFirstly, the structured knowledge stored in knowl-edge graphs is difficult to update in line with devel-opments in the real world, which may lead to mis-judgments in some cases. Secondly, as mentioned\nin Section 5.4, certain models exhibit an inherent\nbias in the discriminative evaluation, consistently\nfavoring one type of answer when responding to\ndiscriminative questions. Although we have taken\nmeasures to enhance the robustness of our evalu-ation, this bias remains unavoidable. Lastly, we\nfine-tune an evaluator for generative hallucination\nevaluation, achieving high accuracy in our task.\nHowever, this evaluator cannot detect all halluci-nation in the responses of LLMs, and its general-ization performance to other tasks remains to be\nexplored. More precise and comprehensive hal-lucination detection is still a challenge in the era\nof LLMs, which we aim to further explore in the\nfuture."}, {"title": "A Benchmark", "content": "Filter Rules There are 363 relations in KoPL, and\nwe apply the following rules to select relations for\neach domain: (1) The relation is associated with\ncorresponding domain. For example, the relation\ncontinent is associated with the Place domain but\nnot the Art domain. (2) The relation is informative\nand does not cause ambiguity. For example, the\nrelation sex or gender is informative and exact,\nbut the relation family are ambiguous. The data\nselecters are the co-authors. Table 3 shows the\nrepresentative concepts, relations and subjects in\nKG-FPQ.\nPrompt Templates for Data Generation Table 4\npresents the prompt template used for GPT-3.5\nto generate Yes-No questions, and Table 5 is the\nprompt template used for GPT-4 to generate WH-questions. We prompt GPTs to generate true\npremise questions based on true triplets and then re-place the original object with the edited object from\nfalse triplets through string matching. For each do-main, we select 3 representative true triplets and\nmanually craft them into demonstrations. During\ngeneration in each domain, these three demonstra-tions remain fixed. The instruction is indicated by\nthe yellow text, the demonstrations are represented\nby the pink text, and the query data is descripited\nby the purple text."}, {"title": "B Experiment Settings", "content": "Prompt Templates for Evaluation Table 6\npresents the prompts used for evaluation.\nPrompt Templates for Training Data Genera-tion Table 7 presents the prompt template used for\nGPT-3.5 to generate factual answers, and Table 8\nis the prompt template used to generate non-facutal\nanswers. For each domain, we select 3 represen-tative true triplets and manually craft them into\ndemonstrations. During generation in each domain,\nthese three demonstrations remain fixed. The in-struction is indicated by the yellow text, the demon-strations are represented by the pink text, and the\nquery data is descripited by the purple text.\nAn Example for Training Data Table 9 shows the\nexamples of training data. This training set includes\n13k examples where the answer is a true/false ref-erence answer generated by GPT-3.5. Additionally,\nit comprises approximately 15k examples where\nthe answer is generated by one of the evaluated\nmodels from Section 4.2, with the label derived\nfrom human annotation. The goal of FPQ-judge"}, {"title": "C Additional Results", "content": "evaluate truth for the questions in KG-FPQ\nonly, without the need to generalize to new ques-tions. Therefore, we include as many questions as\npossible in the training set.\nParameters for Fine-tuning During LoRA fine-tuning, the following parameters are used:\n\u2022 r = 8 (LoRA rank)\n\u2022 lora_alpha = 32 (LoRA scaling factor)\n\u2022 lora_dropout = 0.05 (dropout rate)\n\u2022 learning_rate = 1e - 4\nC Additional Results\nTable 10 presents the evaluation results of all mod-els for FPQs on Yes-No Question Task and WH-Question Task."}, {"title": "C.1 Impact fo Distance", "content": "In NSC and NDC, we categorize FPQs into five\ntypes based on the number of hops as shown in\nTable 11, and calculate the accuracy for each cate-gory. The formula is as follows:\naccuracy = correct number in each category / total number in each category\nFigure 11 presents the accuracy of all models in\nNSC by hops, and Figure 12 presents the accuracy\nof all models in NDC by hops."}, {"title": "C.2 Imapct of Task Foramt", "content": "We calculate the overall performance of each model\nin the discriminative and the generative task across\ndomains with the following formula:\naccuracy = correct NSC + + correct NNDR / 6 \u00d7 total number of FPQs\nFigure 13 presents the results in People and Place\ndomains. It is evident that for almost all LLMs, the\noverall accuracy in generative task is lower than in\ndiscriminative task."}, {"title": "C.3 Impact of Knowledge Domain", "content": "Table 12 presents the evaluation results of all mod-els for Yes-No format TPQs."}, {"title": "C.4 Impact of Model Size", "content": "Figure 14 compares the average accuracy of TPQs\nacross different model size. The evaluated models\nare classified into 3 categories according to their\nsize: 6B~8B, 13B~14B, and the GPT series. We\ncalculate the average accuracy of each category by\nthe following formula:\naccuracy = \u2211acc of each model in the category / total number of models in the category\nWe found that the 6B 8B LLMs outperform the\n13B 14B LLMs, which is counterintuitive. Ob-serving Table 12, we find that the performances of\nthe Baichuan2 series and Llama2-13B-Chat are at\ntwo extremes. Therefore, we undertake a closer\nexamination of these three models as presented in\nFigure 15."}]}