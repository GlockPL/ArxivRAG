{"title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions", "authors": ["Yanxu Zhu", "Jinlin Xiao", "Yuhang Wang", "Jitao Sang"], "abstract": "Recent studies have demonstrated that large language models (LLMs) are susceptible to being misled by false premise questions (FPQs), leading to errors in factual knowledge, know as factuality hallucination. Existing benchmarks that assess this vulnerability primarily rely on manual construction, resulting in limited scale and lack of scalability. In this work, we introduce an automated, scalable pipeline to create FPQs based on knowledge graphs (KGs). The first step is modifying true triplets extracted from KGs to create false premises. Subsequently, utilizing the state-of-the-art capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed method, we present a comprehensive benchmark, the Knowledge Graph-based False Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three knowledge domains, at six levels of confusability, and in two task formats. Using KG-FPQ, we conduct extensive evaluations on several representative LLMs and provide valuable insights. The KG-FPQ dataset and code are available at https://github.com/yanxuzhu/KG-FPQ.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Zhao et al., 2023) excel in natural language understanding and generation but often produce texts that deviate from real-world factual knowledge, a problem known as factuality hallucination (Huang et al., 2023). This issue restricts their applicability in scenarios requiring high factual accuracy.\nRecent studies (Vu et al., 2023; Yuan et al., 2024) have demonstrated that False Premise Questions (FPQs) can induce factuality hallucination in LLMs, as these models often respond directly to FPQs without verifying their validity. A FPQ is a question that contains incorrect facts which are not explicitly stated but might be mistakenly believed by the questioner (Yu et al., 2023). For example, as shown at the top of Figure 1, when asked with a true premise question (TPQ), the LLM can answer correctly, indicating that the LLM possesses relevant knowledge. However, as depicted in the middle of Figure 1, when the TPQ is transformed into a FPQ, the LLM is induced to hallucinate.\nExisting FPQ benchmarks (Yu et al., 2023; Kim et al., 2023; Hu et al., 2023; Vu et al., 2023) primarily rely on manual construction, resulting in limited scale and lack of extensibility. Yuan et al. (2024) construct their dataset by corrupting triplets in Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and filling them into human-written templates, which limits the coverage of knowledge domains. Additionally, these studies lack a thorough evaluation of factuality hallucination induced by FPQs.\nWe explore an automated, scalable method to construct FPQs. The first step involves extracting true triplets from knowledge graphs (KGs) and editing them into false triplets. Subsequently, GPTs are utilized to generate FPQs based on these false triplets. Specifically, We extract triplets from the KG in the form of <subject, relation, object> and edit the object to create false triplets <subject, relation, edited object>. We design editing methods from two perspectives: 1) the edited object at varying distances from the subject in the KG; 2) the edited object having varying associations with the original object in the KG. As the example shown at the bottom of Figure 1, we edit the true triplet <John Lennon, place of death, New York City> to the false triplet <John Lennon, place of death, Liverpool>. Liverpool is a 1-hop neighbor of John Lennon and belongs to the same concept as New York City but has a different relation to the subject. There are six editing methods to create false triplets varying in levels of confusability. After editing, we utilize GPT-3.5 (OpenAI, 2023) and GPT-4 (OpenAI, 2024) to generate FPQs in Yes-No and WH formats respectively corresponding to discriminative and generative evaluation of hallucination (Zhang et al., 2023). By the proposed method, we present a comprehensive benchmark, the Knowledge Graph-based False Premise Questions, which contains FPQs across three knowledge domains, at six levels of confusability, and in two task formats. The comparison between KG-FPQ and other datasets is detailed in Table 1.\nWe evaluate the performance of several representative and advanced LLMs on KG-FPQ across both discriminative and generative tasks. Since manual evaluation of the generative task is costly, we introduce an automated evaluator named FPQ-Judge to identify whether responses of LLMs to FPQs are misled by the false premises, achieving a 93% accuracy rate on a manually annotated test set. Through extensive experiments, we reach three essential conclusions: (1) In terms of confusability, when the edited object has a closer distance with the subject or has a stronger association with the original object, FPQs are more confusing to LLMs. (2) In terms of task formats, LLMs perform worse at generating factual statements than at distinguishing them when faced with FPQs. (3) In terms of knowledge domains, knowledge proficiency of LLMs varies across domains, and there is no positive correlation between knowledge proficiency and the ability to resist the interference of FPQs. Our contributions can be summarized as follows:\n\u2022 We propose an automated and scalable pipeline combining KGs and GPTs for constructing FPQ datasets, by editing true triplets into false triplets and utilizing GPTs to generate FPQs.\n\u2022 Based on the proposed method, we create a comprehensive benchmark, KG-FPQ, containing FPQs across three knowledge domains, at six levels of confusability, and in two task formats.\n\u2022 We fine-tune an automated evaluator for generative hallucination evaluation, FPQ-Judge, achieving 93% accuracy on a manually annotated test set. Furthermore, we conduct an in-depth evaluation of factuality hallucination induced by FPQs on several representative LLMs, yielding valuable insights."}, {"title": "2 Related Work", "content": "Evaluation of Factuality Hallucination Many benchmarks evaluate factuality hallucination (Lin et al., 2022; Li et al., 2023; Min et al., 2023; Muhlgay et al., 2024) due to the risks it poses in practical LLM applications. The evaluation formats are primarily divided into discriminative evaluation (Lin et al., 2022; Li et al., 2023; Muhlgay et al., 2024) and generative evaluation (Lin et al., 2022; Min et al., 2023), which respectively assess the ability of LLMs to distinguish factual statements and generate factual content (Zhang et al., 2023). Hallucination induced by FPQs belongs to factuality hallucination, and this paper evaluates this vulnerability in both discriminative and generative formats.\nFalse Premise Questions Existing FPQ benchmarks (Yu et al., 2023; Kim et al., 2023; Hu et al., 2023; Vu et al., 2023) primarily rely on manual construction, resulting in limited scale, lack of extensibility and high labor costs. Yuan et al. (2024) construct their dataset by corrupting triplets in Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and filling them into human-written templates, which limits the coverage of knowledge domains. Additionally, these studies lack a thorough evaluation of factuality hallucination induced by FPQs. KG-FPQ is automatically constructed and offers multiple perspectives for evaluation and analysis."}, {"title": "3 KG-FPQ Benchmark Construction", "content": "3.1 Triplets Extraction and Editing\nWe utilize KoPL, a high-quality subset of Wikidata, as our KG. KOPL contains a limited set of concepts and relations, where each entity uniquely belongs to one concept. We follow the steps shown in the left of Figure 2 to extract and edit triplets. First, we select entities from three domains: Art, People and Place, based on their concepts, and filter the relations for each domain. The filtering rules are detailed in Appendix A, and Table 3 lists the representative concepts, relations, and entities for each domain.\nSubsequently, we extract true triplets from KOPL and edit them into false triplets. The editing methods, illustrated in Figure 3, can be categorized into six types across two perspectives: 1) the edited object at varying distances from the subject in the KG; 2) the edited object having varying associations with the original object in the KG. In detail, when the edited object is a neighbor of the subject, their maximum distance is set to five hops. Through editing, we get six different false triplets for each true triplet, resulting in six corresponding FPQs during data generation. False triplets created by different editing methods exhibit varying levels of confusability. For instance, as shown in Figure 3, Neighbor-Same-Concept (NSC) indicates that the edited object, Liverpool, is a 1-hop neighbor of the subject and belongs to the same concept as the original object, which might be challenging for LLMs to recognize. In contrast, Not-Neighbor-Different-Concept (NNDC) indicates that the edited object, Mona Lisa, is not a neighbor of the subject and belongs to a different concept from the original object, making it somewhat easier to identify.\n3.2 Data Generation and Verification\nAs shown in the right of Figure 2, firstly, we sample 1k triplets to validate the feasibility of generating FPQs with GPTs. Through manual check, we identify that triplets containing certain relations easily make the generated data semantically incoherent, so we exclude those data for further data generation. Secondly, we generate the full dataset, utilizing GPT-3.5 to create Yes-No questions and GPT-4 to create WH-questions. We prompt GPTs to generate TPQs based on true triplets and then replace the original object with the edited object from false triplets through string matching. Therefore, we create one TPQ and six FPQs in each format based on each true triplet, with these FPQs in each format differing only in the edited object. The prompts used for data generation are shown in Appendix A. Finally, to ensure data quality, we perform a manual review of the dataset, with particular attention to WH-questions, correcting some grammatical and semantic errors.\nBy our method, the number of triplets extracted from KoPL can be relatively large, and we sample approximately 5k triplets from each domain to construct KG-FPQ. Finally, KG-FPQ contains: 4,969\u00d76\u00d72FPQs in the Art domain, 4, 897\u00d76\u00d72 FPQs in the People domain and 4,994 \u00d76\u00d72 FPQs in the Place domain. Due to the randomness in triplet extraction and LLMs generation, our pipeline can generate FPQs dynamically, which can be used for the dynamic testing of LLMs. This is highly beneficial for improving the factuality of LLMs."}, {"title": "4 Experiment Settings", "content": "4.1 Tasks\nDiscriminative Task The first task involves the discriminative task, where LLMs are required to answer Yes-No questions in KG-FPQ with \u201cYes\u201d or \u201cNo\u201d only, without providing explanations. An example for FPQ in Yes-No format is that Did John Lennon die in Liverpool?.\nGenerative Task The second task involves the generative task, where LLMs are required to answer the WH-questions in KG-FPQ. An example for FPQ in WH format is that Where in Liverpool did John Lennon die?. If LLMs recognize the false premises in FPQs, they will deny the false premises and provide explanations. If LLMs fail to identify the false premises, they may be misled by FPQs and generate information with fctuality hallucination.\n4.2 Models\nWe select several representative and advanced open-source chat models of various sizes. Models in the 6B~8B range include ChatGLM3-6B (Du et al., 2022), Baichuan2-7B-Chat (Baichuan, 2023), Llama2-7B-Chat (Touvron et al., 2023), Qwen1.5-7B-Chat (Bai et al., 2023), and Llama3-8B-instruction. Models in the 13B~14B range include Baichuan2-13B-Chat (Baichuan, 2023), Llama2-13B-Chat (Touvron et al., 2023), and Qwen1.5-14B-Chat (Bai et al., 2023). We also evaluate advanced two closed-source LLMs, GPT-3.5 and GPT-4 on the discriminative task. We set the temperature parameter to 0.6 and the top_p parameter to 0.9 for all models in both the discriminative task and the generative task.\n4.3 Evaluation\nEvaluation Procedure Our evaluation procedure is shown in Figure 4. First, we input the Yes-No format TPQs into the LLMs. If the LLMs answer \"Yes\", it indicates that the LLMs have stored relevant background knowledge for the question. We then continue with the corresponding FPQs in both the discriminative task and the generative task. If the LLMs answer \u201cNo\u201d, we do not proceed with the FPQs for that TPQ. This approach aims to reduce the hallucination caused by a lack of background knowledge. To increase the robustness of the assessment, we input each question three times to obtain three responses, and then perform a hard vote to get a final answer label. The prompt templates for evaluation are presented in Table 6.\nEvaluation for Generative Task Since manual evaluation of the generative task is costly, we introduce an automated evaluator named FPQ-Judge, which is a LoRA-tuned Llama3-8B-instruction model designed to classify whether the answers of LLMs to FPQs are misled by the false premises. The training set for FPQ-Judge consists of triplets in the form of (question, answer, label), where the label indicates whether the answer is true or false. This training set includes 13k examples where the answer is a true/false reference answer generated by GPT-3.5. Additionally, it comprises approximately 15k examples where the answer is generated by one of the evaluated models in Section 4.2, with the label derived from human annotation. To assess the performance of FPQ-Judge, we conduct tests on both a GPT-3.5 generated dataset with a size of 3k and a human annotated dataset with a size of 6.3k. FPQ-Judge achieves an accuracy of 99.32% on the GPT-3.5 generated test set and 93% on the manually annotated test set. The prompt templates used for GPT-3.5 to generate traing data, the example of the training data, and the training parameters are provided in the Appendix B.\nMetrics We use accuracy as the evaluation metric. In the discriminative task, we calculate accuracy by string matching the responses of LLMs: for TPQs, answering \u201cYes\u201d is considered correct; for FPQs, answering \u201cNo\u201d is considered correct. In the generative task, an answer is considered correct if FPQ-Judge marks it as correct."}, {"title": "5 Results", "content": "Table 10 presents the complete evaluation results of all models for FPQs on both the discriminative task and the generative task across three domains. Table 2 presents the results of the Art domain, which we use as an example for preliminary analysis. It can be observed that the accuracy of LLMs varies across FPQs with different levels of confusability, and their performance also differs based on the task format. In Section 5.1, we will further analyze the relationship between the confusability of FPQs and the factuality hallucination. In Section 5.2, we will examine the impact of task format on factuality hallucination. Additionally, Section 5.3 and Section 5.4 will provide detailed analyses from the perspectives of knowledge domains and model scales, respectively.\n5.1 Impact of confusability of FPQs\nAs shown in Figure 3, we design editing methods from two perspectives, distance and association, and create FPQs at six levels of confusability. In this section we will discuss the impact of confusability of FPQs from these two perspectives.\n5.1.1 Impact of Distance\nTo investigate the impact of the distance between the edited object and the subject within the KG, the average accuracy of all LLMs on NSC and NNSC is calculated in both discriminative and generative tasks across three domains, as illustrated in Figure 5. The results demonstrate that, the average accuracy for NSC is consistently lower than for NNSC across all domains, and this phenomenon is more pronounced in the discriminative task. This indicates that FPQs formed when the edited object in the false triplets is a neighbor of the subject are more confusing to LLMs, resulting in a higher probability of factuality hallucination.\nFurthermore, we conduct a more detailed examination of NSC and NDC to investigate the impact of the number of hops between the edited object and the subject. The complete results are shown in Appendix C.1, and we analyze the NSC in Art domain as an example in this section, with results presented in Figure 6. It is observed that for most models, the accuracy improves as the number of hops between the edited object and the subject increases, indicating a reduction in factuality hallucination, and this trend is more evident in discriminative tasks.\nIn conclusion, when the edited object and the subject in the false triplets has a closer distance, the FPQs are more confusing for LLMs, and more likely to cause factuality hallucination. Conversely, as the distance between them increases, the likelihood of factuality hallucination decreases. This trend is more pronounced in the discriminative task than in the generative task.\n5.1.2 Impact of Associations\nTo explore the impact of the associations between the edited object and the original object on FPQs-induced factuality hallucination, we calculate the average accuracy of all LLMs on NSC vs. NDC, NNSC vs. NNDC, NNSR vs. NNDR, and NNSC vs. NNSR in both tasks across three domains, as illustrated in Figure 7.\nFrom the comparison of NSC vs. NDC, and NNSC vs. NNDC in upper Figure 7, it is evident that in all domains, whether in the discriminative or generative task, the average accuracy for NSC is consistently lower than for NDC, and as the same, NNSC is consistently lower than NNDC. As shown in Figure 3, NSC and NNSC involve the edited object and original object belonging to the same concept in the KG, whereas NDC and NNDC involve different concepts. Thus, we conclude that when the edited object and the original object belong to the same concept in the KG, the FPQs generated are more confusing for LLMs, leading to a higher likelihood of factuality hallucination. Similarly, the comparison between NNSR and NNDR in the lower left of Figure 7 reveals that FPQs generated from false triplets where the edited object and original object share the same relation are more likely to induce factuality hallucination in LLMs.\nWe also compare NNSC and NNSR to determine whether the same concept or the same relation editing method has a greater impact on LLMs. The lower right of Figure 7 shows that in the Art domain, the NNSC creates stronger interference than the NNSR, while in the People and Place domains, the NNSR causes greater interference.\nIn conclusion, when the edited object has stronger associations with the original object, the FPQs are more confusing for LLMs, and likely to induce factuality hallucination.\n5.2 Impact of Task Format\nWe analyze the overall performance of each LLM in both discriminative and generative tasks, with the complete results shown in Appendix C.2. This section provides an analysis of the Art domain, with results presented in Figure 8. It is evident that for almost all LLMs, the overall accuracy in the generative task is lower than in the discriminative task, suggesting that LLMs perform worse at generating factual statements than at distinguishing them when faced with FPQs. This highlights that generative FPQs remain a significant challenge for LLMs and warrant further attention.\n5.3 Impact of Knowledge Domain\nFollowing the procedure shown in Figure 4, we first evaluate LLMs on Yes-No format TPQs, with the results presented in Table 12. We propose a hypothesis: From the domain perspective, higher accuracy on TPQs indicates that LLMs are more familiar with the knowledge in that domain, and therefore, the accuracy on FPQs in that domain should also be higher, implying that LLMs are less likely to be misled by FPQs. To verify it, we compare the results of TPQs and FPQs, shown in Figure 9. The average accuracy of TPQs is higher in the People domain compared to Art and Place, whereas the average accuracy of FPQs is highest in the Place domain compared to Art and People. This indicates that the knowledge proficiency of LLMs varies across domains, and that there is no positive correlation between knowledge proficiency and the ability to resist the interference of FPQs.\n5.4 Impact of Model size\nThe evaluated models are classified into 3 categories according to their size: 6B~8B, 13B~14B, and the GPT series. We then calculate the average accuracy for FPQs of each categories across 3 domains, as shown in Figure 10. It can be observed that, regardless of the task format, the average accuracy tends to increase with larger model sizes. This indicates that larger models are more factual in answering FPQs. A similar analysis is conducted for TPQs as presented in Appendix C.4. The GPT series demonstrate the highest performance on TPQs, while the 6B~8B LLMs outperform the 13B~14B LLMs, which is counterintuitive.\nObserving Table 12, we find that the accuracy of the Baichuan2 series is significantly higher than that of other models, and the accuracy of Llama2-13B-Chat is even far below the random guessing probability of 0.5. We undertake a closer examination of these three models, and the results are shown in Figure 15. In most cases, the performance of FPQs for the Baichuan2 series decreases compared to TPQs. By contrast, the accuracy of FPQs for Llama2-13B-Chat significantly increases compared to TPQs. We hypothesize that these models may have an inherent bias that causes them to consistently favor one type of answer when answering Yes-No questions. Despite using repeated questioning and hard voting strategies during evaluation, this tendency remains noticeable, which should be addressed by developers."}, {"title": "6 Conclusion and Discussion", "content": "To evaluate factual hallucination induced by false premise questions in LLMs, we develop an automated and scalable pipeline to construct FPQs by editing the triplets in a KG and utilizing GPTs to generate data. Based on the proposed method we create a comprehensive benchmark, KG-FPQ, offering multiple perspectives for evaluation. Using KG-FPQ, we assess several advanced LLMs. Through extensive experiments, we reach three essential conclusions: (1) FPQs with different levels of confusability have varying degrees of impact on LLMs. (2) LLMs perform worse at generating factual statements than at distinguishing them when faced with FPQs. (3) Knowledge proficiency of LLMs varies across domains, and there is no positive correlation between knowledge proficiency and the ability to resist the interference of FPQs.\nBased on analysis in Section 5.1, we speculate that the internal knowledge storage structures of LLMs may resemble knowledge graphs, which we will explore further in future research. Additionally, FPQs can be exploited as prompt injection attacks, leading LLMs to generate non-factual texts and spread misinformation online. In order to identify and mitigate more potential vulnerabilities, we will expand the variety of FPQs for red teaming LLMs."}, {"title": "Limitations", "content": "We propose a comprehensive FPQ benchmark, based on which we evaluate the FPQ-induced factual hallucinations in several advanced LLMs in both discriminative and generative formats. However, our work still faces limitations and challenges. Firstly, the structured knowledge stored in knowledge graphs is difficult to update in line with developments in the real world, which may lead to misjudgments in some cases. Secondly, as mentioned in Section 5.4, certain models exhibit an inherent bias in the discriminative evaluation, consistently favoring one type of answer when responding to discriminative questions. Although we have taken measures to enhance the robustness of our evaluation, this bias remains unavoidable. Lastly, we fine-tune an evaluator for generative hallucination evaluation, achieving high accuracy in our task. However, this evaluator cannot detect all hallucination in the responses of LLMs, and its generalization performance to other tasks remains to be explored. More precise and comprehensive hallucination detection is still a challenge in the era of LLMs, which we aim to further explore in the future."}, {"title": "A Benchmark", "content": "Filter Rules There are 363 relations in KoPL, and we apply the following rules to select relations for each domain: (1) The relation is associated with corresponding domain. For example, the relation continent is associated with the Place domain but not the Art domain. (2) The relation is informative and does not cause ambiguity. For example, the relation sex or gender is informative and exact, but the relation family are ambiguous. The data selecters are the co-authors. Table 3 shows the representative concepts, relations and subjects in KG-FPQ.\nPrompt Templates for Data Generation Table 4 presents the prompt template used for GPT-3.5 to generate Yes-No questions, and Table 5 is the prompt template used for GPT-4 to generate WH-questions. We prompt GPTs to generate true premise questions based on true triplets and then replace the original object with the edited object from false triplets through string matching. For each domain, we select 3 representative true triplets and manually craft them into demonstrations. During generation in each domain, these three demonstrations remain fixed. The instruction is indicated by the yellow text, the demonstrations are represented by the pink text, and the query data is descripited by the purple text."}, {"title": "B Experiment Settings", "content": "Prompt Templates for Evaluation Table 6 presents the prompts used for evaluation.\nPrompt Templates for Training Data Generation Table 7 presents the prompt template used for GPT-3.5 to generate factual answers, and Table 8 is the prompt template used to generate non-facutal answers. For each domain, we select 3 representative true triplets and manually craft them into demonstrations. During generation in each domain, these three demonstrations remain fixed. The instruction is indicated by the yellow text, the demonstrations are represented by the pink text, and the query data is descripited by the purple text.\nAn Example for Training Data Table 9 shows the examples of training data. This training set includes 13k examples where the answer is a true/false reference answer generated by GPT-3.5. Additionally, it comprises approximately 15k examples where the answer is generated by one of the evaluated models from Section 4.2, with the label derived from human annotation. The goal of FPQ-judge is to evaluate truth for the questions in KG-FPQ only, without the need to generalize to new questions. Therefore, we include as many questions as possible in the training set.\nParameters for Fine-tuning During LoRA fine-tuning, the following parameters are used:\n\u2022 r = 8 (LoRA rank)\n\u2022 lora_alpha = 32 (LoRA scaling factor)\n\u2022 lora_dropout = 0.05 (dropout rate)\n\u2022 learning_rate = 1e - 4\nC Additional Results\nTable 10 presents the evaluation results of all models for FPQs on Yes-No Question Task and WH-Question Task.\nC.1 Impact fo Distance\nIn NSC and NDC, we categorize FPQs into five types based on the number of hops as shown in Table 11, and calculate the accuracy for each category. The formula is as follows:\naccuracy = $\\frac{\\text{correct number in each category}}{\\text{total number in each category}}$\nFigure 11 presents the accuracy of all models in NSC by hops, and Figure 12 presents the accuracy of all models in NDC by hops.\nC.2 Imapct of Task Foramt\nWe calculate the overall performance of each model in the discriminative and the generative task across domains with the following formula:\naccuracy = $\\frac{\\text{correct NSC + ... + correct NNDR}}{6 \\times \\text{total number of FPQs}}$\nFigure 13 presents the results in People and Place domains. It is evident that for almost all LLMs, the overall accuracy in generative task is lower than in discriminative task.\nC.3 Impact of Knowledge Domain\nTable 12 presents the evaluation results of all models for Yes-No format TPQs."}, {"title": "C.4 Impact of Model Size", "content": "Figure 14 compares the average accuracy of TPQs across different model size. The evaluated models are classified into 3 categories according to their size: 6B~8B, 13B~14B, and the GPT series. We calculate the average accuracy of each category by the following formula:\naccuracy = $\\frac{\\sum \\text{acc of each model in the category}}{\\text{total number of models in the category}}$\nWe found that the 6B 8B LLMs outperform the 13B 14B LLMs, which is counterintuitive. Observing Table 12, we find that the performances of the Baichuan2 series and Llama2-13B-Chat are at two extremes. Therefore, we undertake a closer examination of these three models as presented in Figure 15."}, {"title": "A Benchmark", "content": "I want you to act as a fluent #Yes-No question# data generator. I will give you a #Ttriplet#, consisting of (subject, relation, object). Your task is to generate a fluent #Yes-no question# relying solely on the #Ttriplet# and directly output the generated #Yes-no question#.\nHere are some examples:\n#triplet#: [\"Steven Spielberg\", \"spouse\", \"Amy Irving\"]\n#Yes-No question#: Is Steven Spielberg married to Amy Irving?\n#triplet#:\n#Yes-No question#:\n#triplet#:\n#Yes-No question#:\n#triplet#: item[\"Ttriplet\"]\n#Yes-No question#:"}, {"title": "B Experiment Settings", "content": "I want you to act as a fluent #WH-question# data generator. I will give you a #Ttriplet#, consisting of (subject, relation, object). Your task is to generate a fluent #WH-question# relying solely on the #Ttriplet#.\nGeneration rules are as following:\nMake sure the #subject# and #object# of the #Ttriplet# are both exactly contained in the #WH-question# you generate.\nMake sure the #WH-question# you generate is syntactically and semantically sound.\nOutput the #WH-question# directly.\nHere are some examples:\n#triplet#: [\"Steven Spielberg\", \"spouse\", \"Amy Irving\"]\n#WH-question#: When did Steven Spielberg and Amy Irving get married?\n#triplet#:\n#WH-question#:\n#triplet#: ...\n#WH-question#:\n#triplet#: item[\"Ttriplet\"]\n#WH-question#:"}, {"title": "Prompt Template", "content": "Question: item[\"TPQ\"] Respond solely with \"Yes\" or \"No\".\nQuestion: item[\"FPQ\"] Respond solely with \"Yes\" or \"No\".\nQuestion: item[\"FPQ\"] Answer the question with an answer of no more than three sentences."}, {"title": "Prompt Template", "content": "I want you to act as a #factual answer# data generator. I will give you a #query# and some #knowledge# about the #query#.Your task is generate a #factual answer# to the #query# relying on the provided #knowledge#.\nGeneration rules are as following:\nGenerate the #factual answer# relying on the provided #knowledge#.\nMake sure the #factual answer# you generate is limited to 3 sentences.\nOutput the #factual answer# directly.\nHere are some examples:\n#query#: What was the reason behind Walt Disney's citizenship in the Nepal?\n#knowledge#: [\"Walt Disney\", \"country of citizenship\", \"United States of America\"]\n#factual answer#: Walt Disney was not a citizen of Nepal. His country of citizenship was the United States of America. There is no record or reason behind Walt Disney having citizenship in Nepal.\n#query#:\n#knowledge#:\n#factual answer#:\n#query#:\n#knowledge#:\n#factual answer#:\n#query#: WH-question\n#knowledge#: item[\"Ttriplet\"]\n#factual answer#:"}, {"title": "Prompt Template", "content": "I want you to act as a #non-factual answer# data generator. I will give you a #query# and some #knowledge# about the #query#. Your task is generate a #non-factual answer# to the #query# that contradict the given #knowledge#.\nGeneration rules are as following:\nGenerate the #non-factual answer# that contradict the given #knowledge#.\nYour answer is not allowed to start with 'I'm sorry...' or 'I can't ...'.\nMake sure the #non-factual answer# you generate is limited to 3 sentences.\nOutput the #non-factual answer# directly.\nHere are some examples:\n#query#: What was the reason behind Walt Disney's citizenship in the Nepal?\n#knowledge#: [\"Walt Disney\", \"country of citizenship\", \"United States of America\"]\n#non-factual answer#: Walt Disney's citizenship in Nepal was due to his deep fascination with the country's rich cultural heritage and mystical landscapes, which he believed inspired many of his animated films. He often mentioned that the serene environment and the spiritual vibes of Nepal helped him connect with his creative side, leading him to seek citizenship there.\n#query#:\n#knowledge#:\n#non-factual answer#:\n#query#:\n#knowledge#:\n#non-factual answer#:\n#query#: WH-question\n#knowledge#: item[\"Ttriplet\"]\n#non-factual answer#:"}, {"title": "Examples of Training Data for FPQ-Judge", "content": "Instruction: You are given a question and an answer, determine if the answer is factual. If the answer is factual please output only \"Yes\", otherwise please output only \"No\".\nInput: Q:What was the reason behind Walt Disney's citizenship in the Nepal?\nA: Walt Disney was not a citizen of Nepal. His country of citizenship was the United States of America. There is no record or reason behind Walt Disney having citizenship in Nepal. Output:\nOutput: Yes\nInstruction: You are given a question and an answer, determine if the answer is factual. If the answer is factual please output only \"Yes\", otherwise please output only \"No\".\nInput: Q:What was the reason behind Walt Disney's citizenship in the Nepal?\nA: Walt Disney's citizenship in Nepal was due to his deep fascination with the country's rich cultural heritage and mystical landscapes, which he believed inspired many of his animated films. He often mentioned that the serene environment and the spiritual vibes of Nepal helped him connect with his creative side, leading him to seek citizenship there. Output:\nOutput: No"}]}