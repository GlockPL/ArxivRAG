{"title": "SageAttention2 Technical Report:\nAccurate 4 Bit Attention for Plug-and-play Inference Acceleration", "authors": ["Jintao Zhang", "Haofeng Huang", "Pengle Zhang", "Jia Wei", "Jun Zhu", "Jianfei Chen"], "abstract": "Although quantization for linear layers has been widely used, its application to accelerate the attention\nprocess remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication\nwith 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2x speedup\nkernel compared to FlashAttention2. To further enhance the efficiency of attention computation while\nmaintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix mul-\ntiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize\nmatrixes (Q, K) to INT4 in a warp-level granularity and quantize matrixes (P, V) to FP8. Second, we\npropose a method to smooth Q and V, enhancing the accuracy of attention with INT4 QK and FP8\nPV. Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive\nquantization method to ensure the end-to-end metrics over various models. The operations per second\n(OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090,\nrespectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics\nloss across diverse models including those for large language processing, image generation, and video\ngeneration. The codes are available at https://github.com/thu-ml/SageAttention.", "sections": [{"title": "1 Introduction", "content": "Due to the quadratic complexity of attention, efficient implementation becomes increasingly crucial as se-\nquences lengthen in real-world applications (Jiang et al., 2024). Several strategies have been developed to\nmitigate computational demands of attention \u2014such as (1) Linear attention methods (Wang et al., 2020;\nChoromanski et al., 2020; Yu et al., 2022; Katharopoulos et al., 2020) that reduce complexity to O(N) and\n(2) Sparse attention methods (Liu et al., 2021; Chu et al., 2021; Li et al.; Xiao et al., 2023b, 2024; Chen et al.,\n2023; Jiang et al., 2024; Venkataramanan et al., 2023; Gao et al., 2024; Fu et al., 2024) that selectively process\nparts of the context these methods are only suitable for a limited range of models and tasks. Widely used\nattention methods optimize attention by exploiting hardware capacities to enhance the computation speed,\nsuch as FlashAttention (Dao et al., 2022), FlashAttention2 (Dao, 2023), FlashAttention3 (Shah et al., 2024),\nxformers (Lefaudeux et al., 2022), and SageAttention (Zhang et al., 2024). Those works do not omit the\ncomputation of attention over parts of the context and achieve impressive speed and precision performance\nacross various models and tasks.\nFor the two matrix multiplication (Matmul) operations in attention: $Q K^T$ and $PV$, SageAttention\naccelerates them by quantizing the $Q K^T$ to INT8 and uses FP16 Matmul with FP16 accumulators for\n$PV$. Moreover, to keep the accuracy of attention, SageAttention proposes smoothing K by eliminating its\nchannel-wise outliers. SageAttention achieves 2 \u00d7 and 2.7 \u00d7 speedup than FlashAttention2 and xformers\nand is the first quantized attention that incurs negligible end-to-end metrics loss across language, image, and\nvideo generation models. However, SageAttention has two weaknesses. (W1) INT8 Matmul achieves only\nhalf the speed of INT4. (W2) FP16 Matmul with FP16 accumulators is only compatible with RTX4090 and\nRTX3090 GPUs. To leverage the faster INT4 tensor cores for $Q K^T$ and using a method that can generally\naccelerate $PV$, we propose SageAttention2, which quantizes Q, K to INT4 and P, V to FP8.\nChallenges. Quantizing Q, K to INT4 and P, V to FP8 presents significant challenges. For example, when\nonly per-tensor quantizing Q, K to INT4, the text-to-video model CogvideoX will generate a completely\nblurry video (See Figure 2), and Llama2 only achieves a random-guessing-level accuracy of 25% on MMLU.\nAfter investigating deeply, we identified three primary challenges: (C1) The numerical range of INT4, which\nin quantization typically spans from -7 to 7 and totals 15 numbers (Lin et al., 2024), leads to significant\nquantization errors when Q and K have some abnormal values. (C2) In specific layers and timesteps (for\ntext-to-image/video) of some models, quantizing Q and K to INT4 and quantizing P and V to FP8 introduces\nnoticeable errors in attention computation. These errors in worst-case layers/timesteps significantly impact\nthe precision of the end-to-end output. (C3) We discover that the FP32 accumulator designed for FP8\nmatrix multiplication in the tensor core (mma.f32.f8.f8.f32) is actually FP22, specifically with 1 sign bit, 8\nexponent bits, and 13 mantissa bits. This will lead to an accuracy loss of PV.\nOur approach. To address these challenges, we detailedly analyze the reasons and propose two methods.\nFirst, for the significant channel-wise outliers in matrices Q and K,  we adopt smoothing K in SageAttention\nand propose an effective method to remove these outliers in Q. Specifically, we propose subtracting the\naverage value of the channel dimension of Q, referred to as $\\overline{Q}_{m}$. Then, we add $\\overline{Q}_{m}K$ after the QK"}, {"title": "2 Preliminary", "content": null}, {"title": "2.1 FlashAttention", "content": "The computation of self-attention can be formulated as follows: $S = Q K^T / \\sqrt{d}$, $P = \\sigma(S)$, $O = PV$, where\n$\\sigma(S)_{ij} = exp(S_{ij})/\\Sigma_k exp(S_{ik})$ is the softmax operation. The matrices Q, K, and V each have dimensions\n$N \\times d$, while the matrix S, Pare $N \\times N$. While d is typically small, e.g., 64 or 128, N can be thousands if not\nmillions. Therefore, the $N \\times N$ matrices (S, P) are much larger than (Q, K, V), and a naive implementation\nsuffers from the huge amount of global memory IO for (S, P) reads/writes. FlashAttention (Dao, 2023)\nproposes to tile Q, K, and V from the token dimension into blocks ${Q_i}$, ${K_j}$, ${V_j}$ with block size of\n$b_q$, $b_{kv}$, $b_{kv}$, respectively. Then, to avoid the memory IO for (S, P), it uses online softmax (Milakov &\nGimelshein, 2018) to progressively compute each block of O, i.e., $O_i$:\nFirst, for each block of ${K_j}$, ${V_j}$, it computes the following equations iteratively:\n$S_i = Q_i K_j^T/\\sqrt{d}$, $(m_i, P_i) = \\tilde{\\sigma}(m_{i-1}, S_i)$, $1 = exp(m_{i-1} - m_i)1^{-1} + rowsum(P_i)$,\n$\\Omega_i = diag (exp(m_i - m_{i-1})) \\Omega_{i-1}^{-1} + P_iV_i;$\nWhere $m_i$ and $\\Omega_i$ are $b_q \\times 1$ vectors, which are initialized to \u2013$\\infty$ and 0 respectively. The $\\tilde{\\sigma}(\u00b7)$ is an online\nsoftmax operator: $m_i = max\\{m_{i-1}, rowmax(S_i)\\}$, $P_i = exp(S_i - m_i)$.\nFinally, the output $O_i$ can be computed by $O_i = diag(1)^{-1}\\Omega_i$."}, {"title": "2.2 Quantization", "content": "A matrix multiplication $C = AB$ can be accelerated with quantization as:\n$(\\delta_A, \\hat{A}) = \\psi(A)$, $(\\delta_B, \\hat{B}) = \\psi(B)$, $\\hat{C} = \\hat{A}\\hat{B}$, $C = \\psi^{-1}(\\hat{C})$\n$\\psi$ is a quantizer which converts a high-precision (e.g., FP32 or FP16) matrix A to a low-precision format $\\hat{A}$\n(e.g., INT4 or FP8) with a scale $\\delta_A$, and $\\psi^{-1}$ is a dequantizer to convert back to high-precision. We should\nhave $\\psi^{-1}(\\hat{A}) \\approx A$. The actual matrix multiplication $\\hat{A}\\hat{B}$ is carried in low-precision. In modern GPUs, low-\nprecision matrix multiplication is usually multiple times faster than higher-precision ones. Many quantizers\ndepend on the numerical format and granularity, e.g., how many elements share a common scale factor.\nFor example, an INT4 per-tensor quantizer first computes the scale as the maximum absolute value of the\nentire tensor, scales the elements to the maximum representable range of INT4 [-7, +7], and then casts to\nINT4 with rounding: $\\hat{A} = [A/\\delta_A]$, $\\delta_A = max(|A|)/7$. Likewise, per-token quantizer assigns a scale factor for\neach token of a tensor: $\\hat{A}[i, :] = [A[i, :]/\\delta_A[i]]$, $\\delta_A[i] = max(|A[i, :]|)/7$. Also, per-channel quantizer assigns a\nscale factor for each channel of the tensor, i.e., along the channel dimension: $\\hat{A}[:, i] = [A[:, i]/\\delta_A[i]]$, $\\delta_A[i] =$\n$max(|A[:, i]|)/7$. Dequantization process is a element-wise scaling: $\\psi^{-1}(\\hat{A}\\hat{B}) = \\hat{A}\\hat{B} * \\delta_A * \\delta_B$."}, {"title": "2.3 SageAttention", "content": "Based on the block tiling in FlashAttention2 (Dao, 2023), SageAttention (Zhang et al., 2024) quantize Q, K\nto INT8 in a per-block granularity. Moreover, to keep the quantization accuracy, SageAttention proposes to\nsmooth K first:\n$\\overline{K} = K - mean(K)$\n$Q_i = [Q_i / \\delta_Q]$, $\\delta_Q = max(|Q_i|)/127$, $\\hat{K_i} = [K_i / \\delta_K]$, $\\delta_K = max(|K_j|)/127$\n$S_{ij} = Q_i K_j^T * \\delta_Q * \\delta_K$\nWhere $Q_i$, $K_j$ are the tilled block in FlashAttention, and $mean(K)$ is the average value of the channel\ndimension of K. SageAttention keeps PV as FP16 and uses FP16 Matmul with an FP16 accumulator for\nPV. However, FP16 Matmul with an FP16 accumulator only has a speedup effect on RTX4090 and RTX3090\nGPUs."}, {"title": "3 SageAttention-2", "content": null}, {"title": "3.1 Formulation", "content": "Based on the introduction of FlashAttention and quantization presented in Section 2, we now describe the\nquantized attention approach we developed."}, {"title": "3.2 Per-warp INT4 Quantization", "content": "SageAttention uses per-block quantization, which quantizes each block of Q and K for a GPU streaming pro-\ncessor. Such a quantization strategy could achieve an accuracy performance close to per-token quantization\nand avoid the overhead of the dot product of the quantization scale vectors $\\delta_Q$ and $\\delta_K$. However, quantizing\nQ and K to INT4 demands a more accurate quantization granularity. We propose per-warp quantization,"}, {"title": "3.3 Smooth Q", "content": "The representative range for quantization of INT4 is notably restrictive, i.e., only $2^4 \u2013 1 = 15$ values. This\nlimitation significantly degrades performance when quantizing Q, K into INT4 in attention. For example,\nusing INT4 to quantize Q, K can cause the perplexity of Llama3.1 on WikiText to increase by more than\n90%, and the quality of videos generated by CogvideoX decrease about 3x (See Table 1). We analyze\nthe data distribution of Q, K in real models. For example, we find that Llama3.1 and CogvideoX show\ndistinct channel-wised outliers in Q, K (See Figure 4). While per-channel quantization could mitigate the"}, {"title": "3.4 Smooth V", "content": "To avoid the drawback of SageAttention where FP16 Matmul with an FP16 accumulator for PV is only\neffective on GPUs such as the RTX4090, we adopt the approach of quantizing P and V to FP8 to lever-\nage the universal acceleration of FP8 Tensor cores. However, we discover that the accumulator for the\nmma(f32f8f8f32) instruction on the Ada architecture is actually FP22, specifically with 1 sign bit, 8 expo-\nnent bits, and 13 mantissa bits. Specifically, for mma(f32f8f8f32) instruction C = AB + D, where A, B are\ntensors in FP8 data type and C, D are tensors with FP32 data type, we initialize the A, B to zero and vary\nD to test the data type of the accumulator. As shown in Table 7, when D is initialized with 1 sign bit, 8\nexponent bits, and 13 mantissa bits, the value of C matches the result of the mma(f16f16f32) instruction.\nHowever, when D is initialized with more than 13 mantissa bits, the error of C corresponds to the differ-\nence between the results of mma(f32f16f16f32) and mma(f32f8f8f32). Consequently, matrix multiplication\nof matrices P and V, quantized to FP8, incurs a certain degree of precision loss compared to using an FP32\naccumulator. To mitigate this precision loss as much as possible, we propose to smooth V:"}, {"title": "3.5 Quantization for Q, K, P, V", "content": "Quantization for Q, K. We propose $\\psi_Q(Q)$ and $\\psi_K(K)$ in granularity of per-warp. This is first because\nper-channel quantization is not feasible since the scale factors of the inner axis of $Q K^T$ cannot used to\ndo dequantization (Xiao et al., 2023a). Second, as shown in Table 2 and Table 3, we compare the average\nand worst-case accuracies of INT4 quantization at per-token, per-warp, per-block, and per-tensor granularity\nusing real Q, K, V across all layers of CogvideoX. Results indicate that the accuracy of per-warp quantization\nis very close to per-token and outperforms a lot than per-block and per-tensor. Furthermore, per-warp\nquantization incurs less dequantization overhead than per-token as discussed in Section 3.2.\nQuantization for P, V. We choose FP8, specifically the E4M3 data type, for $\\psi_P(P)$ and $\\psi_V(V)$ for two\nreasons. First, most GPUs have tensor cores that support FP8 Matmul operations, which are twice as fast\nas those using FP16. Second, Table 4 and Table 5 show the average and worst accuracy of different data\ntypes used for P, V using real Q, K, V across all layers of CogvideoX. We can see that the accuracy of\nusing E4M3 is very close to that of using FP16 and superior to that of E5M2 and INT8. We propose to\nuse $\\psi_P(P)$ in per-block and $\\psi_V(V)$ in per-channel for three reasons. First, per-channel quantization for P\nand per-token quantization for V are not viable because dequantization requires scale factors of outer axis.\nSecond, $P = exp(S_i - rowmax(S_i))$, where $S_i$ is the Matmul result of a block of Q and $K^T$, the max value in\neach row of P is 1. Hence, we can assign a single static scale s = $\\sqrt{448}$ to a block P, whose accuracy equals\nper-token quantization. Third, per-channel quantization can address the channel-wised outlier of V.\nAccuracy metrics. We use three metrics to assess the accuracy of quantized attention output $O'$ compared\nto attention output in full-precision O: First, we flatten $O'$ and O into vectors in the shape of $1 \\times n$. Then,\nCosine similarity: $CosSim = \\Sigma OO'/\\sqrt{\\Sigma O^2}\\sqrt{\\Sigma O'^2}$, Relative L1 distance: $L1 = \\Sigma |O - O'|/\\Sigma |O|$, Root\nmean square error: $RMSE = \\sqrt{(1/n)\\Sigma (O \u2013 O')^2}$."}, {"title": "3.6 Adaptive Quantization over Layer and Timestep", "content": "Based on the discussion in Section 3.5, we implement two attention kernels (See Table 8) based on a\nchoice: Using INT4 or INT8 quantization for $\\psi_Q(Q)$ and $\\psi_K (K)$. The speed of these kernels is in the"}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Setup", "content": "Models. We validate the effectiveness of SageAttention2 across a diverse set of representative models\nfrom language, image, and video generation. Specifically, we conduct experiments on five models: Llama2\n(7B) (Touvron et al., 2023), Llama3.1 (8B) (Dubey et al., 2024), and GLM4 (9B) (GLM et al., 2024) for\ntext2text, CogvideoX (2B) (Yang et al., 2024) and Open-Sora (Zheng et al., 2024) for text2video, Flux\n(schnell) (Black Forest Labs, 2023) for text2image, and TIMM (Wightman, 2019) for image classification."}, {"title": "4.2 Speed and Accuracy of Kernels", "content": "Speed. We conduct experiments to compare the Speed of SageAttention2 against baselines using configu-\nrations with headdim=64, headdim=128, and headdim=256, both with and without Causal Mask (Vaswani,\n2017). Specifically, Figure 8, Figure 9, and Figure 10 show the speed of SageAttention2 and baselines\nacross varying sequence lengths on RTX4090. These results indicate that SageAttention2 achieves a peak\nof 485 TOPS and is 3.1x faster than FlashAttention2 and 5.4x faster than xformers. Figure 11, Figure 12,\nand Figure 13 illustrate the results on L20 GPU, indicating that SageAttention2 achieves a peak of 288"}, {"title": "4.3 End-to-end Performance", "content": "Metrics loss. We assessed the end-to-end metrics of various models using SageAttention2 compared to\nusing attention in full precision. Detailed evaluation results are presented in Table 11 for Llama2, Llama3.1,\nGLM4, CogvideoX, Open-Sora, Flux, and TIMM, respectively. The results indicate that SageAttn-4b outper-\nforms all baselines and maintains most of the end-to-end accuracy across all models. Additionally, using"}, {"title": "4.4 Ablation Study", "content": "Adaptive quaitzation. To analyze the impact of using different ratio of SageAttn-8b in adaptive quan-\ntization, Table 15 shows the changes in the perplexity of Llama3.1 under various ratio of SageAttn-8b.\nIt can be observed that even if SageAttn-4b is used exclusively, the overall end-to-end representation is\nsufficiently good, and the higher the ratio of SageAttn-8b used, the closer the accuracy approaches that of\nusing full-precision attention."}, {"title": "5 Conclusion and Future Work", "content": "We introduce SageAttention2, an efficient and accurate 4-bit quantization method for attention. First, we\npropose to quantize (Q, K) to INT4 in a warp-level granularity and quantize (P, V) to FP8. Second, we\npropose a method to smooth matrixes Q and V, enhancing the accuracy of attention with INT4 QK and\nFP8 PV. Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive\nquantization method to ensure the end-to-end metrics over various models. Our implementation is faster\nthan FlashAttention2 and xformers by approximately 3.1x and 5.4x on RTX4090, respectively. Extensive\ntesting confirms that our approach maintains end-to-end metrics across various models, including language,\nimage, and video generation.\nFuture Work. We leave the implementation of FP8 MatMul with an FP16 accumulator for PV and\nSageAttention2 on the Hopper architecture for future work."}]}