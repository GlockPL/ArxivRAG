{"title": "Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach", "authors": ["Irina Jurenka", "Markus Kunesch", "Kevin R. McKee", "Daniel Gillick", "Shaojian Zhu", "Sara Wiltberger", "Shubham Milind Phal", "Katherine Hermann", "Daniel Kasenberg", "Avishkar Bhoopchand", "Ankit Anand", "Miruna P\u00eeslar", "Stephanie Chan", "Lisa Wang", "Jennifer She", "Parsa Mahmoudieh", "Aliya Rysbek", "Wei-Jen Ko", "Andrea Huber", "Brett Wiltshire", "Gal Elidan", "Roni Rabin", "Jasmin Rubinovitz", "Amit Pitaru", "Mac McAllister", "Julia Wilkowski", "David Choi", "Roee Engelberg", "Lidan Hackmon", "Adva Levin", "Rachel Griffin", "Michael Sears", "Filip Bar", "Mia Mesar", "Mana Jabbour", "Arslan Chaudhry", "James Cohan", "Sridhar Thiagarajan", "Nir Levine", "Ben Brown", "Dilan Gorur", "Svetlana Grant", "Rachel Hashimshoni", "Laura Weidinger", "Jieru Hu", "Dawn Chen", "Kuba Dolecki", "Canfer Akbulut", "Maxwell Bileschi", "Laura Culp", "Wen-Xin Dong", "Nahema Marchal", "Kelsie Van Deman", "Hema Bajaj Misra", "Michael Duah", "Moran Ambar", "Avi Caciularu", "Sandra Lefdal", "Chris Summerfield", "James An", "Pierre-Alexandre Kamienny", "Abhinit Mohdi", "Theofilos Strinopoulous", "Annie Hale", "Wayne Anderson", "Luis C. Cobo", "Niv Efron", "Muktha Ananda", "Shakir Mohamed", "Maureen Heymans", "Zoubin Ghahramani", "Yossi Matias", "Ben Gomes", "Lila Ibrahim"], "abstract": "A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards maximising the positive impact of gen Al in education.", "sections": [{"title": "1. Introduction", "content": "The roughly 70 year history of Artificial Intelligence (AI) has been one of paradigm shifts: from symbolic systems, to Bayesian approaches, to deep learning, and in the last few years, generative AI (gen AI)-large foundational models trained on huge swaths of media available on the internet to gain an impressive set of general capabilities, whereby they are (most of the time) able to provide a useful response to any user prompt or enquiry. Each paradigm shift brought with it a unique set of hopes, opportunities, and challenges. Yet the current gen AI era is unprecedented: AI is more accessible than ever (because it only requires prompting through natural language), more capable than ever, and appears to be improving faster than ever. Questions naturally arise about how to harness this technology for maximal social benefit."}, {"title": "2. Participatory approach", "content": "This section details the participatory elements that helped shape this project, including the design of our evaluative approach, and our goals in developing LearnLM-Tutor. We firmly believe that responsible development of educational AI systems requires engaging learners, educators, policymakers, and academic researchers [27], to ensure that the resulting systems align with their needs, values, and"}, {"title": "2.1. Participatory workshops: Imagining and critiquing the future of education and AI", "content": "We conducted two participatory workshops in the UK: one with learners, primarily university students coming from diverse academic backgrounds (n = 60), and another with educators, mainly high school teachers specialising in STEM subjects (n = 34). The choice of the participant demographics was dictated by practical considerations. We realise that future work is needed to expand our reach to broader communities, since learners in the UK and other WEIRD countries likely encounter fewer barriers to accessing gen AI tools, and perspectives on Al in education likely differ substantially across cultural contexts.\nFollowing established best practices for participatory workshops [32], we employed structured activities to foster interaction, collaborative learning, and group cohesion (see Section B.1 for more details). Participants were divided into small groups of five to eight individuals and engaged in two key exercises:\n\u2022 Grounding exercise: This activity explored participants' educational experiences, revealing current needs, challenges, and potential areas for improvement regarding gen AI tools.\n\u2022 Speculative design: This exercise encouraged participants to envision a scenario involving a learner facing various challenges. Through collaborative brainstorming, they explored how AI and social factors could exacerbate or mitigate these challenges.\nThese workshops highlighted current challenges in education: learners struggle with time manage- ment, cognitive overload, and demotivation when they perceive their learning materials as irrelevant; while educators struggle to provide personalised attention and feedback in classroom settings.\nPersonalised tutoring, by AI or humans, was valued by both learners and educators. Tutors are especially effective when they have knowledge of the learner and can adapt their approach accordingly. Learners felt more comfortable seeking clarifications from AI tutors than human tutors, perceiving Al tutors as less formal and less likely to induce fears of judgement. A shared limitation of both human and AI tutors was their lack of familiarity with the nuances of particular syllabi or exam board requirements.\nLearners in the workshop were often strong adopters of gen AI. While aware of its limitations, they tended to be happy to work around them. Educators were more sceptical, citing worries about hallucinations, the potential for cheating, and the lack of adaptation to the learner's level and cognitive load in gen Al's \u201cwall-of-text\u201d responses. Both groups saw immediate benefits of gen AI tools, such as from generating practice questions, critiquing and generating ideas, and summarising content.\nA shared vision for the future of education emerged, emphasising the role of personalised AI tutors in enabling flexible, cross-disciplinary, and relevant learning opportunities. Additionally, virtual and augmented reality technologies were seen as beneficial through enhanced immersion. Educators"}, {"title": "2.2. Understanding learning experiences: Initial interviews and Wizard-of-Oz sessions", "content": "To initiate our iterative participatory design process for LearnLM-Tutor, we conducted an exploratory series of user-centred studies involving both learners and educators. We enrolled three adult learners with an intrinsic interest in Python coding into the Codecademy \u201cLearn Python 3\u201d course, to develop a better understanding of the learning experience and needs of potential users. During the first weeks of the course, these learners participated in a series of semi-structured interviews and \u201cWizard-of-Oz\u201d prototyping sessions. During the sessions, members of the research team simulated the role of an Al tutor through a chat interface, engaging in 1:1 interactions with each learner as if they were interacting with a fully functional AI system. In parallel, we conducted individual interviews with six teachers and academics specialising in the intersection of AI and learning science. These interviews aimed to capture educators' perspectives on the potential benefits and challenges of gen Al tutors in educational settings. These participatory design activities provided us with initial insights into user experiences, expectations, and challenges. They informed the key focus areas identified for the early development of LearnLM-Tutor and shaped the design of the turn-based evaluations described in Section 5.2.\nLearners noted several main challenges with online courses: the learners' lack of assumed prerequi- site knowledge, not being able to follow explanations due to missing details or logical steps, difficulty concentrating on long video lectures without doing exercises, and needing more help navigating the course materials. When doing practice problems, learners reported needing help breaking down the task into manageable chunks and diagnosing errors in their solutions; they reported that the tools they used could only point out the error, rather than how to diagnose it. Learners also wanted an AI tutor to have access to the same learning materials as them, use short communications that guide them in small steps, and give them frequent assessments of their knowledge. They did not want the tutor to give away too much information as they reported feeling pride in doing things themselves. They also wanted the tutor to be encouraging and constructive in its feedback, responsive and kind, proactive in soliciting questions from the learners, and always available.\nFrom our conversations with the educators we have derived the following principles that apply to both human and AI tutors (see Section B.2 for additional principles that are only relevant to AI tutors):\n\u2022 Do not give away solutions prematurely. Encourage learners to come up with solutions.\n\u2022 Make explanations easy to understand, for example by making connections to the real world.\n\u2022 Be encouraging. Celebrate learner progress and embrace mistakes as learning opportunities.\n\u2022 Recognise when learners are struggling, and proactively check in with them.\n\u2022 Ask questions to determine learner understanding and misunderstanding.\n\u2022 Explain step-by-step, and deconstruct to teach thought processes."}, {"title": "2.3. Lessons from ShiffBot: Co-design activities", "content": "Another participatory effort that informed the development of LearnLM-Tutor is ShiffBot, an educa- tional Al experiment [33] that uses a \u201cstart with one\" approach, a co-design framework centring on a single person with the goal of developing AI technology that can be impactful for them and their community. It then generalises from that starting point. The \u201cstart with one\u201d approach aligns with participatory practices from contextual inquiry [34] and user-centred design [35], actively including the participant as a partner and stakeholder in the development process. By collaborating with a single participant, the broader research team gained a deep, contextualised understanding of the challenges and needs that can emerge in real-user settings.\nThe participant for the Shiff Bot project was Daniel Shiffman, an educator, NYU professor, and YouTube creator who teaches programming. The ShiffBot project aimed to explore possible ways that gen AI could provide value to learners and educators. Through a set of interviews with Daniel and his students, as well as classroom observations, the ShiffBot team developed the following set of guiding principles for AI development:\n\u2022 Do not just give away the answers. Instead, help the learner discover their own answers. Then help them take their next steps.\n\u2022 Aim to return appropriate credible resources.\n\u2022 Be a safe space to make mistakes.\n\u2022 See what the student sees: screen, code, and error messages.\n\u2022 The bot will not always get it right. We should learn from the mistakes.\nWorking with Daniel made it clear that he valued a tight integration of the AI tutor with his learning materials. In Daniel's case, this involved integrating ShiffBot as a Chrome extension that works inside the web-based p5. js code editor that Daniel uses in the classroom when he teaches and in his YouTube learning videos. Because of the specific syntax of p5.js, it was important to bring retrieval augmented generation (RAG) to ShiffBot to ground its answers on the relevant parts of Daniel's video lectures, and refer his students to those videos instead of directly giving away an answer that relies purely on the underlying knowledge of the Gemini 1.0 model powering ShiffBot. Furthermore, the team worked on making ShiffBot adopt Daniel's particular (successful) teaching style and use an encouraging tone that creates a feeling of safety.\nThe participatory approach resulted in a chatbot that offered helpful suggestions, provided relevant examples, and guided students through coding challenges, all using a teaching style that resembled Daniel's. The iterative development process, informed by input from Daniel and his students, ensured that ShiffBot aligned with the needs and preferences of the target audience, while also identifying the limits of the current technology to inform its future improvements. In the interviews with the research team, his students indicated that ShiffBot provided them with meaningful assistance.\nLearner feedback included: \u201cWhat I like about ShiffBot is that it doesn't disrupt the learning process. Doesn't just give the answer.\u201d [P99]; \u201cShiffBot is useful in understanding other people's code and also useful in cleaning up code.\u201d [P100]; and \u201cHaving used ShiffBot for a few days now, I do think it's quite handy to have it by my side, and actually encourages me to walk myself through my own sketch, and practice how to explain my thinking process more solidly!\u201d [P101]\nLearnLM-Tutor development adopted the guiding principles from the ShiffBot experiment, includ- ing the focus on grounded interactions, with the only exception of trying to copy Daniel's personality and teaching style."}, {"title": "3. Improving Gemini for education", "content": "This section surveys our work on enabling productive pedagogical behaviour in a language-based gen AI model5. We begin by framing our contributions with respect to related prior work in learning science, EdTech and AI research. We then describe a set of fine-tuning datasets we have developed to improve Gemini 1.0 for education, and introduce intermediate model versions trained on different subsets of these datasets showing varying degrees of pedagogical improvements. These models are numbered from earliest to latest in development Mo to M4, where M4 is LearnLM-Tutor. They are used to validate our evaluation methodology introduced in the subsequent sections, which is the primary focus of this report."}, {"title": "3.1. Lack of universal best pedagogical practices: lessons from learning science", "content": "Optimising an Al system for any goal requires a concomitant ability to measure progress. While learning and teaching strategies have been studied across many disciplines, defining (and subsequently quantifying) universal pedagogical principles remains a challenge. As critically noted by Slavin [36], educational research lags behind much of modern science, to the point where at the \u201cdawn of the 21st century, educational research is finally entering the 20th century\".\nOne reason why it has been hard to establish a common set of recommended pedagogical practices is related to the fragmentation of educational research across many disciplines. Even within the same discipline, many studies highlight different interventions or strategies with little overlap\u2014Koedinger et al. [27] synthesised a list of thirty independent instructional principles after reviewing just nine primary sources. The resulting theories are often based on inconclusive evidence [37], and their translation to practice is often difficult or unclear [27, 38, 39]. Furthermore, most cognitive and learn- ing science research tends to be done with small homogeneous populations [27], limited to specific narrow educational contexts, like subject domain, difficulty level, or prior learner knowledge [27], and typically conducted in WEIRD countries [40], which makes the findings hard to generalise. Studied interventions also come with variable implementation parameters (e.g. the time spacing between practices, the ratio of examples to questions) and can be combined in different ways, resulting in a combinatorial explosion in possible, often context-dependant, pedagogical strategies [27] that is hard to explore manually, yet alone measure (see Figure 3, left)."}, {"title": "3.2. Lack of transparency and common evaluation practices: lessons from EdTech", "content": "From the earliest mechanical teaching machines by Pressey (1924) and Skinner (1954) [41], to the first digital Computer Assisted Instruction (CAI) systems [42, 43] and the more modern Intelligent Tutoring Systems (ITSs) [44\u201366], education has always been an important application for the latest computing technology. From the earliest instantiations, these systems tended to follow a similar blueprint. They assume that the learner is interacting with the tutoring system without any assistance from a human teacher, and the tutoring system guides the learner through a pre-defined set of learning materials with some level of adaptation to the learner's progress (e.g., choosing the difficulty of the next practice problem based on how well the learner did on the previous ones), and some level of timely feedback (e.g., at the step or solution level) [41, 44, 48].\nUnder the hood, ITSs tend to be rule-based expert systems [67\u201370]\u2014 the predominant AI paradigm in the 1970-1980s. Although expert systems have many positive qualities, they have largely been replaced by deep learning in recent years due to difficulties with scale and generality inherent in the"}, {"title": "3.3. Generative AI in education", "content": "Deep learning has become the predominant paradigm in Al since the publication of the seminal AlexNet paper [86] in computer vision. It has removed the dependency on humans to provide structured knowledge to AI by enabling AI systems to discover structure from data on their own during training. Over the last 12 years, AI researchers have seen many examples of \u201cthe bitter lesson\u201d-that data and scale tend to trump carefully crafted rules or representations [87]. The latest shift to the gen Al era is a particularly striking demonstration of this lesson. The transformer architecture [88] has reached a level of performance and generality never before seen in AI, mostly through scaling up to more data and compute7. Although there has been a lot of excitement about the potential impact of the recent gen Al technology in education, and a number of gen AI-based tutors have emerged [89\u2013105], the full extent of this potential has not materialised just yet. A recent review of gen Al tutoring systems found that \"dialog tutoring has largely remained unaffected by these advances\" [106].\nOut of the box, gen Al models have a remarkable ability to understand user queries expressed in natural language and generate responses that synthesise relevant information from across the internet (used in the gen AI pre-training) to answer in a helpful and harmless way. However, by default, these models do not typically behave like human tutors. Such default behaviour can be modified in two ways: prompting or fine-tuning (through supervised and/or reinforcement learning). We will discuss the difficulties of both approaches that have affected the pace of progress in gen Al for education, as well as our own efforts in these directions."}, {"title": "3.3.1. Prompting", "content": "Prompting is the easiest and most popular way to adjust the behaviour of gen AI (25/33 papers presented at the recent NeurIPS 23 workshop on Generative AI for Education used prompt engineer- ing [107]). All it requires is for the EdTech designer to write a set of instructions in natural language on what good tutoring behaviours look like, for example: \u201cStart by introducing yourself to the student"}, {"title": "3.3.2. Fine-tuning", "content": "If prompting can be roughly seen as the modern, more capable generalisation of expert systems, its alternative-fine-tuning, which typically includes stages of supervised fine-tuning (SFT), followed by Reinforcement Learning from Human Feedback (RLHF)\u2014brings the full power of the deep learning paradigm, i.e. learning from data, to the table. While far less computationally intensive than the standard pre-training phase, fine-tuning can still be costly to perform on models with many billions of parameters [101], which explains why it is less explored in the gen AI for education literature compared to prompting. However, fine-tuning (RL in particular) may enable AI to capture some of the intuition and reasoning that humans use in effective teaching, leveraging backpropagation to search the vast space of pedagogical possibilities discussed in Section 3.1.\nIn our current work, models M0-M4 are fine-tuned via SFT over all parameters of a base model (PaLM 2.0 [109] for M0-M3 and Gemini 1.0 [10] for M4 of comparable size; see Section E for further implementation details). While reward modeling and RL are crucial (and in our opinion the most promising) ingredients to building high-quality gen AI tutors, we have thus far focused only on SFT (and the requisite creation of behaviour cloning data). Of course, this puts our models at a serious disadvantage in evaluations against the base models, which include both SFT and (non-pedagogical) RL, and we plan to incorporate RL in the future (see Section F for a discussion of the challenges that come with eliciting human preferences to support RL for educational use cases).\nIt is worth mentioning that base models (PaLM 1.0 [110], PaLM 2.0 [109], Gemini 1.0 [10], and now Gemini 1.5 [111]) are improving rapidly. Each new model holds more knowledge, can perform more tasks more accurately, and is more controllable via prompting, so the task of improving them with respect to a particular set of behaviours like pedagogy, is constantly evolving. While M3 far outperformed PaLM 2.0 across many of our metrics, the gap between M4 (which basically differs from"}, {"title": "3.4. Our SFT datasets", "content": "In this section, we describe the datasets we created. Fine-tuning data is often classified as either synthetic (generated by an algorithm) or human (written by a human expert). Synthetic data is often seen as easier to obtain but of worse quality than human data. We believe that the ultimate goal of SFT data is to demonstrate as much of the \u201coptimal pedagogy\u201d from within the high-dimensional space of all possible pedagogical strategies as possible (Figure 3, left). Since such a dataset of perfect tutoring does not exist (even the most talented human teachers are unlikely to demonstrate such perfect behaviour), approximations have to be obtained. These approximations fall on a spectrum between fully synthetic (almost never possible because there is always a human who ultimately designs what good synthetic data should look like, thus injecting human influence) to fully human-created (e.g. recorded conversations between a human learner and human teacher). This section describes the datasets used in each of the milestone models described in this report (see Table 1) and where they fall on this spectrum (see Figure 3, right).\nHuman tutoring We collected a dataset of conversations between human learners and educators by pairing them through a text-based chat interface and paying for their time. Although this data provides demonstrations of human pedagogy, it has a number of limitations. It is not targeted to any specific pedagogical behaviour, contains off-topic discussion related to the task and setting (e.g., \"looks like our time is up\"), and is of uneven quality overall (see Section L for more details).\nGen Al role-play To demonstrate specific pedagogical behaviour, we developed a role-playing framework, in which gen Al models play both tutor and learner. Each was provided with a set of states and strategies relevant to their roles through static prompts, along with dynamic prompting to help them respond to the selected state in the counterpart. For example, when the learner model selects the \"make mistake\" state and generates a flawed solution, this state would be inserted into the tutor prompt to help the tutor model identify and correct the mistake. While the resulting data is synthetic, the hand-engineered framing (human intervention) produced by the dynamic prompting and the injection of privileged information about the internal state of the learner into the tutor resulted in a reasonably consistent (if sometimes stilted) pedagogical dialogue over very long conversations. This was further improved through manual filtering and editing by the researchers.\nGSM8k dialogue Another attempt to create high-quality synthetic data involved converting GSM8k [118] word problems and associated step-by-step solutions (we used the \u201cSocratic\u201d version of the dataset) into learner/tutor conversations, an adaptation of \u201cdialogue in-painting\u201d [119]. Each tutor turn consists of the \u201cSocratic\" version of the next solution step, while a prompted gen AI model"}, {"title": "4. Measuring Pedagogy in Gen AI", "content": "Before evaluating education-specific improvements of LearnLM-Tutor over the prompt tuned Gemini 1.0, we first discuss whether our interventions resulted in any performance regressions in general accuracy. We then provide an overview of existing pedagogical evaluations from the gen Al literature, before describing our own approach to measuring pedagogy in gen Al tutors."}, {"title": "4.1. Accuracy on education-related benchmarks", "content": "We checked whether our fine-tuning interventions resulted in any regressions in accuracy of LearnLM- Tutor compared to base Gemini 1.0. To this end, we ran existing education-related benchmarks including MMLU [120], MATH [121], HellaSwag [122], and HumanEval [123], and safety benchmarks including RealToxicityPrompts [124] and BBQ [125] with LearnLM-Tutor using exactly the same setups that were used for Gemini et al. [10]. The results of LearnLM-Tutor reproduce the performance of Gemini Pro [10], for example an MMLU score of 0.72 and MATH score of 0.33.\nWhile this is a necessary criterion for demonstrating that there are no performance regressions, it is not sufficient as the model might be taken out of the fine-tuning data distribution back into the pre-training distribution of the base model in these few-shot prompting settings. We therefore also evaluated the performance of LearnLM-Tutor and Gemini 1.0 in the pedagogical conversation context by measuring the accuracy of the individual turns produced by these models. We found no"}, {"title": "4.2. Current approaches", "content": "Progress towards building a general purpose gen Al tutor has been slowed by the lack of good measures of progress towards this goal. Most of the evaluation methods from learning science for human tutors are not applicable to AI (e.g., because they rely on self-reports) [98]. Currently, gen Al tutors tend to be evaluated using domain-agnostic metrics which act as a proxy for how coherent and human-like the generated responses are (e.g., BLEU [126], BERTScore [127], Rouge [128], DialogRPT [129]), but which are not designed to measure pedagogy or other education-specific capabilities [89, 98\u2013100, 103, 106]. Such metrics also often assume that there is a ground truth answer that the model response should match. However, there are many ways to respond to the same learner query with potentially equal pedagogical value, so a single \u201coptimal\u201d answer is impossible to define [98, 103, 130]. Many metrics are also easy to trick; for example, always responding with \"Hello\" can score highly [131], and adding a \u201cteacher:\u201d prefix can increase scores [100]. A promising new approach to fast evaluations of gen Al tutors could be to use another gen AI for \u201ccritique\" [132]. Recently, Chevalier et al. [104] proposed using such gen AI critics to evaluate the presentation and correctness of the statements generated by a gen AI tutor. We are not aware of any group using such critics for pedagogical evaluations.\nAn alternative to automatic evaluations described above is using human experts to evaluate pedagogical performance. Interactive human evaluations are known to be important [91, 133, 134] and tend to correlate better with user satisfaction [133]. However, access to pedagogical experts is not easy, so typically studies use either very few experts (<10) [97\u201399] or the evaluation is done by study authors [103], which can both lead to biases. Furthermore, there is no agreed-upon protocol for running pedagogical human evaluations. The most commonly used human evaluation framework (Tack and Piech [98]) asks human raters to compare the responses of two tutors in the context of the same dialogue snippet. The comparison is done along three dimensions: replying like a teacher, understanding of the student, and helpfulness. These dimensions are based on Demszky et al. [135] and are important dimensions to evaluate, but they do not capture the full richness of pedagogy.\nAn important test of any gen AI tutor is whether it actually improves the learning outcomes of real students. Very few studies have run such evaluations, as most of them use paid raters to act as learners [102]. Evaluations with real students are typically done with a small number of participants and in controlled experimental lab settings, which limits their validity [101]. A notable exception is Liu et al. [105], who embedded a gen AI tutor into a CS50 MOOC course and made it available to millions of real students. However, the use of the tutor had to be heavily throttled due to cost considerations, and the results reported so far are limited in scope and come from a small number of on-campus students.\nThe difficulties in evaluating gen AI tutors mean that research groups are evaluating their gen Al tutors using their own metrics [89, 92, 93, 96, 97, 101\u2013105], which makes different approaches hard to compare (the BEA 2023 Shared Task [99] is a notable exception). There is a well-recognised need to develop better evaluation metrics suited to AI in education [79, 99, 100, 106, 107]. However, Tack et al. [99] conclude that we are a long way from achieving the precise, valid, and automated pedagogical evaluations needed for progress in AI for education."}, {"title": "4.3. Our approach", "content": "In this section, we discuss our approach to narrowing down the vast space of all the possible pedagogical strategies (Section 3.1) and translating it into an evaluation rubric. We include discussion of the many pragmatic questions we considered, such as implementation difficulty, cost, validity, and other feasibility concerns."}, {"title": "4.3.1. Pedagogy rubrics", "content": "Alongside the principles described in Section 2, we combined further insights from our participatory sessions with literature reviews to create a high-level pedagogy rubric, which we then translated into measurable tutor behaviours by working together with teachers as expert advisers. The high-level pedagogical principles we prioritised are: encourage active learning (the learner should manipu- late information through discussion, practice, and creation, instead of passively absorbing informa- tion [136\u2013139]), manage cognitive load (the tutor should present information in multiple modalities, structure it well, and segment it into manageable chunks [140]), deepen metacognition (\u201cthinking about thinking\u201d, which enables learners to generalise their skills beyond a single context [141\u2013143]), motivate and stimulate curiosity (as this leads to self-efficacy and lifelong learning [144, 145]), and adapt to learners' goals and needs (by assessing the current state and the goals, and making a plan to bridge the gap [146]). Each high-level pedagogical principle was translated into different measurable items used in different benchmarks (see Table 2 for automatic language model evaluation, Table 10 for conversation-level human evaluation, and Table 13 for turn-level human evaluation). These items took various forms, e.g. differing in the wording of the questions and in the level of granularity at which each high-level principle was broken down, while still designed to measure the same principle. This was to assess whether measuring the same pedagogical capability through different lenses provides a consistent answer, and also due to practical considerations (e.g. a different approach needs to be taken when asking a human or a gen AI critic to assess the same pedagogical principle). This is our first attempt at defining a pedagogical rubric, and we plan to iterate, improve, and expand it in the future."}, {"title": "4.3.2. Pragmatic evaluation taxonomy", "content": "To navigate the large space of practical considerations needed to implement pedagogical evaluations, we designed the taxonomy shown in Figure 2 and used it to compile seven pedagogical benchmarks with different trade-off profiles. We aimed for this set of benchmarks to provide a comprehensive view on the pedagogy performance of AI tutors. They were designed to be diverse and to traverse all nodes of the proposed taxonomy. Future work should do a more systematic investigation of how each node in the taxonomy affects the validity and effectiveness of the resulting benchmark. This taxonomy is described in more detail here:"}, {"title": "5. Human evaluations", "content": "In this section, we present the results of our human evaluations comparing LearnLM-Tutor to base prompt tuned [1] Gemini 1.0. Interactions with human participants represent the gold standard for evaluation in responsible AI development; simulations cannot fully capture the complexities of real-world settings [149\u2013152]. Human participants allow us to observe authentic user behaviour and system responses within the context of dynamic, goal-oriented conversations. They can reveal issues that simulations might miss. Engaging with human participants is also crucial for promoting inclusion and representation in the development process [149]. On the other hand, human evaluations suffer from limited sample sizes due to the expense and slow nature of recruiting pedagogical experts and collecting their judgements using cognitively demanding rubrics. Furthermore, special care needs to be taken to iterate over the rater instructions and the data collection pipelines to ensure the validity, consistency and calibration of the collected human rater judgements. All of these factors tend to lead to limited statistical significance of human evaluation results, which we also found to be the case. However, we see our results as signs of progress towards imbuing the Gemini 1.0 base model with additional pedagogical capabilities. We prioritised responsible design and conduct across all studies, following guidelines from research ethics [153] (see Section I for details of our human evaluation)."}, {"title": "5.1. Unguided conversations: Subjective learner feedback", "content": "Learners first engaged in a 45-minute unguided (open-ended) session with a provided AI tutor through a chat interface. The tutoring session was grounded in an academic YouTube video, which they could select from a list, on maths, CS, biology, chemistry, literature, history or other subjects, like public speaking (see Section J.1 for the data collection details). They were then asked seven questions to assess their perception of the tutor. Learners rated LearnLM-Tutor higher than Gemini 1.0 tutor in most categories (Figure 4). However, we have only achieved statistical significance for one of them: learners felt more confident about applying what they had learnt with LearnLM-Tutor in the future by themselves."}, {"title": "5.2. Turn-level pedagogy: teacher feedback", "content": "We asked expert pedagogical raters to review and rate the unguided conversations from our learner study (Section 5.1). For each tutor turn, they determined whether one of nine suggested pedagogical \"moves\" was appropriate and desired in the conversational context (see Table 13 for the breakdown"}, {"title": "5.3. Conversation-level pedagogy: teacher feedback", "content": "We ran a number of guided conversation-collection experiments in which graduate-level experts interacted with two different tutors, role-playing as learners within their academic domain of expertise (biology, CS, maths, history, English, chemistry, or physics). The conversations with both AI tutors were grounded in the same educational video and a corresponding scenario, which specified the learner's persona, goal in the conversation (e.g. understanding how sound can be a wave, for a physics video on travelling waves), and other details (see Figure 17c). These pairs of conversations were then rated by pedagogical expert raters. First, each individual conversation in the pair was rated against a pedagogy rubric (see Table 10). In all of these rating experiments, the rubric was applied at the conversation level, as opposed to the turn-level ratings described in the previous sections.\nFigure 7 shows the effect sizes of the difference in ratings between pairs of prompted Gemini 1.0 and LearnLM-Tutor conversations on the same scenario. On average, the LearnLM-Tutor conversations were preferred to Gemini"}]}