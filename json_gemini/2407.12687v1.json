{"title": "Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach", "authors": ["Irina Jurenka", "Markus Kunesch", "Kevin R. McKee", "Daniel Gillick", "Shaojian Zhu", "Sara Wiltberger", "Shubham Milind Phal", "Katherine Hermann", "Daniel Kasenberg", "Avishkar Bhoopchand", "Ankit Anand", "Miruna P\u00eeslar", "Stephanie Chan", "Lisa Wang", "Jennifer She", "Parsa Mahmoudieh", "Aliya Rysbek", "Wei-Jen Ko", "Andrea Huber", "Brett Wiltshire", "Gal Elidan", "Roni Rabin", "Jasmin Rubinovitz", "Amit Pitaru", "Mac McAllister", "Julia Wilkowski", "David Choi", "Roee Engelberg", "Lidan Hackmon", "Adva Levin", "Rachel Griffin", "Michael Sears", "Filip Bar", "Mia Mesar", "Mana Jabbour", "Arslan Chaudhry", "James Cohan", "Sridhar Thiagarajan", "Nir Levine", "Ben Brown", "Dilan Gorur", "Svetlana Grant", "Rachel Hashimshoni", "Laura Weidinger", "Jieru Hu", "Dawn Chen", "Kuba Dolecki", "Canfer Akbulut", "Maxwell Bileschi", "Laura Culp", "Wen-Xin Dong", "Nahema Marchal", "Kelsie Van Deman", "Hema Bajaj Misra", "Michael Duah", "Moran Ambar", "Avi Caciularu", "Sandra Lefdal", "Chris Summerfield", "James An", "Pierre-Alexandre Kamienny", "Abhinit Mohdi", "Theofilos Strinopoulous", "Annie Hale", "Wayne Anderson", "Luis C. Cobo", "Niv Efron", "Muktha Ananda", "Shakir Mohamed", "Maureen Heymans", "Zoubin Ghahramani", "Yossi Matias", "Ben Gomes", "Lila Ibrahim"], "abstract": "A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards maximising the positive impact of gen AI in education.", "sections": [{"title": "1. Introduction", "content": "The roughly 70 year history of Artificial Intelligence (AI) has been one of paradigm shifts: from symbolic systems, to Bayesian approaches, to deep learning, and in the last few years, generative AI (gen AI)-large foundational models trained on huge swaths of media available on the internet to gain an impressive set of general capabilities, whereby they are (most of the time) able to provide a useful response to any user prompt or enquiry. Each paradigm shift brought with it a unique set of hopes, opportunities, and challenges. Yet the current gen AI era is unprecedented: AI is more accessible than ever (because it only requires prompting through natural language), more capable than ever, and appears to be improving faster than ever. Questions naturally arise about how to harness this technology for maximal social benefit."}, {"title": "2. Participatory approach", "content": "This section details the participatory elements that helped shape this project, including the design of our evaluative approach, and our goals in developing LearnLM-Tutor. We firmly believe that responsible development of educational AI systems requires engaging learners, educators, policymakers, and academic researchers, to ensure that the resulting systems align with their needs, values, and"}, {"title": "2.1. Participatory workshops: Imagining and critiquing the future of education and AI", "content": "We conducted two participatory workshops in the UK: one with learners, primarily university students coming from diverse academic backgrounds (n = 60), and another with educators, mainly high school teachers specialising in STEM subjects (n = 34). The choice of the participant demographics was dictated by practical considerations. We realise that future work is needed to expand our reach to broader communities, since learners in the UK and other WEIRD countries likely encounter fewer barriers to accessing gen AI tools, and perspectives on Al in education likely differ substantially across cultural contexts.\nFollowing established best practices for participatory workshops we employed structured activities to foster interaction, collaborative learning, and group cohesion . Participants were divided into small groups of five to eight individuals and engaged in two key exercises:\n\u2022 Grounding exercise: This activity explored participants' educational experiences, revealing current needs, challenges, and potential areas for improvement regarding gen AI tools.\n\u2022 Speculative design: This exercise encouraged participants to envision a scenario involving a learner facing various challenges. Through collaborative brainstorming, they explored how AI and social factors could exacerbate or mitigate these challenges.\nThese workshops highlighted current challenges in education: learners struggle with time management, cognitive overload, and demotivation when they perceive their learning materials as irrelevant; while educators struggle to provide personalised attention and feedback in classroom settings.\nPersonalised tutoring, by AI or humans, was valued by both learners and educators. Tutors are especially effective when they have knowledge of the learner and can adapt their approach accordingly. Learners felt more comfortable seeking clarifications from AI tutors than human tutors, perceiving Al tutors as less formal and less likely to induce fears of judgement. A shared limitation of both human and AI tutors was their lack of familiarity with the nuances of particular syllabi or exam board requirements.\nLearners in the workshop were often strong adopters of gen AI. While aware of its limitations, they tended to be happy to work around them. Educators were more sceptical, citing worries about hallucinations, the potential for cheating, and the lack of adaptation to the learner's level and cognitive load in gen Al's \u201cwall-of-text\u201d responses. Both groups saw immediate benefits of gen AI tools, such as from generating practice questions, critiquing and generating ideas, and summarising content.\nA shared vision for the future of education emerged, emphasising the role of personalised AI tutors in enabling flexible, cross-disciplinary, and relevant learning opportunities. Additionally, virtual and augmented reality technologies were seen as beneficial through enhanced immersion. Educators"}, {"title": "2.2. Understanding learning experiences: Initial interviews and Wizard-of-Oz sessions", "content": "To initiate our iterative participatory design process for LearnLM-Tutor, we conducted an exploratory series of user-centred studies involving both learners and educators. We enrolled three adult learners with an intrinsic interest in Python coding into the Codecademy \u201cLearn Python 3\u201d course, to develop a better understanding of the learning experience and needs of potential users. During the first weeks of the course, these learners participated in a series of semi-structured interviews and \u201cWizard-of-Oz\u201d prototyping sessions. During the sessions, members of the research team simulated the role of an Al tutor through a chat interface, engaging in 1:1 interactions with each learner as if they were interacting with a fully functional AI system. In parallel, we conducted individual interviews with six teachers and academics specialising in the intersection of AI and learning science. These interviews aimed to capture educators' perspectives on the potential benefits and challenges of gen Al tutors in educational settings. These participatory design activities provided us with initial insights into user experiences, expectations, and challenges. They informed the key focus areas identified for the early development of LearnLM-Tutor and shaped the design of the turn-based evaluations described in Section 5.2.\nLearners noted several main challenges with online courses: the learners' lack of assumed prerequi-site knowledge, not being able to follow explanations due to missing details or logical steps, difficulty concentrating on long video lectures without doing exercises, and needing more help navigating the course materials. When doing practice problems, learners reported needing help breaking down the task into manageable chunks and diagnosing errors in their solutions; they reported that the tools they used could only point out the error, rather than how to diagnose it. Learners also wanted an AI tutor to have access to the same learning materials as them, use short communications that guide them in small steps, and give them frequent assessments of their knowledge. They did not want the tutor to give away too much information as they reported feeling pride in doing things themselves. They also wanted the tutor to be encouraging and constructive in its feedback, responsive and kind, proactive in soliciting questions from the learners, and always available.\nFrom our conversations with the educators we have derived the following principles that apply to both human and AI tutors (see Section B.2 for additional principles that are only relevant to AI tutors):\n\u2022 Do not give away solutions prematurely. Encourage learners to come up with solutions.\n\u2022 Make explanations easy to understand, for example by making connections to the real world.\n\u2022 Be encouraging. Celebrate learner progress and embrace mistakes as learning opportunities.\n\u2022 Recognise when learners are struggling, and proactively check in with them.\n\u2022 Ask questions to determine learner understanding and misunderstanding.\n\u2022 Explain step-by-step, and deconstruct to teach thought processes."}, {"title": "2.3. Lessons from ShiffBot: Co-design activities", "content": "Another participatory effort that informed the development of LearnLM-Tutor is ShiffBot\u2074, an educational Al experiment that uses a \u201cstart with one\" approach, a co-design framework centring on a single person with the goal of developing AI technology that can be impactful for them and their community. It then generalises from that starting point. The \u201cstart with one\u201d approach aligns with participatory practices from contextual inquiry and user-centred design , actively including the participant as a partner and stakeholder in the development process. By collaborating with a single participant, the broader research team gained a deep, contextualised understanding of the challenges and needs that can emerge in real-user settings.\nThe participant for the Shiff Bot project was Daniel Shiffman, an educator, NYU professor, and YouTube creator who teaches programming. The ShiffBot project aimed to explore possible ways that gen AI could provide value to learners and educators. Through a set of interviews with Daniel and his students, as well as classroom observations, the ShiffBot team developed the following set of guiding principles for AI development:\n\u2022 Do not just give away the answers. Instead, help the learner discover their own answers. Then help them take their next steps.\n\u2022 Aim to return appropriate credible resources.\n\u2022 Be a safe space to make mistakes.\n\u2022 See what the student sees: screen, code, and error messages.\n\u2022 The bot will not always get it right. We should learn from the mistakes.\nWorking with Daniel made it clear that he valued a tight integration of the AI tutor with his learning materials. In Daniel's case, this involved integrating Shiff Bot as a Chrome extension that works inside the web-based p5. js code editor that Daniel uses in the classroom when he teaches and in his YouTube learning videos. Because of the specific syntax of p5.js, it was important to bring retrieval augmented generation (RAG) to Shiff Bot to ground its answers on the relevant parts of Daniel's video lectures, and refer his students to those videos instead of directly giving away an answer that relies purely on the underlying knowledge of the Gemini 1.0 model powering Shiff Bot. Furthermore, the team worked on making ShiffBot adopt Daniel's particular (successful) teaching style and use an encouraging tone that creates a feeling of safety.\nThe participatory approach resulted in a chatbot that offered helpful suggestions, provided relevant examples, and guided students through coding challenges, all using a teaching style that resembled Daniel's. The iterative development process, informed by input from Daniel and his students, ensured that ShiffBot aligned with the needs and preferences of the target audience, while also identifying the limits of the current technology to inform its future improvements. In the interviews with the research team, his students indicated that Shiff Bot provided them with meaningful assistance.\nLearner feedback included: \u201cWhat I like about ShiffBot is that it doesn't disrupt the learning process. Doesn't just give the answer.\u201d ; \u201cShiffBot is useful in understanding other people's code and also useful in cleaning up code.\u201d ; and \u201cHaving used ShiffBot for a few days now, I do think it's quite handy to have it by my side, and actually encourages me to walk myself through my own sketch, and practice how to explain my thinking process more solidly!\u201d\nLearnLM-Tutor development adopted the guiding principles from the ShiffBot experiment, includ-ing the focus on grounded interactions, with the only exception of trying to copy Daniel's personality and teaching style."}, {"title": "3. Improving Gemini for education", "content": "This section surveys our work on enabling productive pedagogical behaviour in a language-based gen AI model. We begin by framing our contributions with respect to related prior work in learning science, EdTech and AI research. We then describe a set of fine-tuning datasets we have developed to improve Gemini 1.0 for education, and introduce intermediate model versions trained on different subsets of these datasets showing varying degrees of pedagogical improvements. These models are numbered from earliest to latest in development Mo to M4, where M4 is LearnLM-Tutor. They are used to validate our evaluation methodology introduced in the subsequent sections, which is the primary focus of this report."}, {"title": "3.1. Lack of universal best pedagogical practices: lessons from learning science", "content": "Optimising an Al system for any goal requires a concomitant ability to measure progress. While learning and teaching strategies have been studied across many disciplines, defining (and subsequently quantifying) universal pedagogical principles remains a challenge. As critically noted by Slavin , educational research lags behind much of modern science, to the point where at the \u201cdawn of the 21st century, educational research is finally entering the 20th century\".\nOne reason why it has been hard to establish a common set of recommended pedagogical practices is related to the fragmentation of educational research across many disciplines. Even within the same discipline, many studies highlight different interventions or strategies with little overlap\u2014Koedinger et al.  synthesised a list of thirty independent instructional principles after reviewing just nine primary sources. The resulting theories are often based on inconclusive evidence , and their translation to practice is often difficult or unclear . Furthermore, most cognitive and learning science research tends to be done with small homogeneous populations , limited to specific narrow educational contexts, like subject domain, difficulty level, or prior learner knowledge , and typically conducted in WEIRD countries , which makes the findings hard to generalise. Studied interventions also come with variable implementation parameters (e.g. the time spacing between practices, the ratio of examples to questions) and can be combined in different ways, resulting in a combinatorial explosion in possible, often context-dependant, pedagogical strategies that is hard to explore manually, yet alone measure (see Figure 3, left)."}, {"title": "3.2. Lack of transparency and common evaluation practices: lessons from EdTech", "content": "From the earliest mechanical teaching machines by Pressey (1924) and Skinner (1954) , to the first digital Computer Assisted Instruction (CAI) systems  and the more modern Intelligent Tutoring Systems (ITSs) , education has always been an important application for the latest computing technology. From the earliest instantiations, these systems tended to follow a similar blueprint. They assume that the learner is interacting with the tutoring system without any assistance from a human teacher, and the tutoring system guides the learner through a pre-defined set of learning materials with some level of adaptation to the learner's progress (e.g., choosing the difficulty of the next practice problem based on how well the learner did on the previous ones), and some level of timely feedback (e.g., at the step or solution level) .\nUnder the hood, ITSs tend to be rule-based expert systems \u2014 the predominant AI paradigm in the 1970-1980s. Although expert systems have many positive qualities, they have largely been replaced by deep learning in recent years due to difficulties with scale and generality inherent in the"}, {"title": "3.3. Generative AI in education", "content": "Deep learning has become the predominant paradigm in Al since the publication of the seminal AlexNet paper in computer vision. It has removed the dependency on humans to provide structured knowledge to AI by enabling AI systems to discover structure from data on their own during training. Over the last 12 years, AI researchers have seen many examples of \u201cthe bitter lesson\u201d-that data and scale tend to trump carefully crafted rules or representations [87]. The latest shift to the gen Al era is a particularly striking demonstration of this lesson. The transformer architecture [88] has reached a level of performance and generality never before seen in AI, mostly through scaling up to more data and compute. Although there has been a lot of excitement about the potential impact of the recent gen Al technology in education, and a number of gen AI-based tutors have emerged , the full extent of this potential has not materialised just yet. A recent review of gen Al tutoring systems found that \"dialog tutoring has largely remained unaffected by these advances\" .\nOut of the box, gen Al models have a remarkable ability to understand user queries expressed in natural language and generate responses that synthesise relevant information from across the internet (used in the gen AI pre-training) to answer in a helpful and harmless way. However, by default, these models do not typically behave like human tutors. Such default behaviour can be modified in two ways: prompting or fine-tuning (through supervised and/or reinforcement learning). We will discuss the difficulties of both approaches that have affected the pace of progress in gen Al for education, as well as our own efforts in these directions."}, {"title": "3.3.1. Prompting", "content": "Prompting is the easiest and most popular way to adjust the behaviour of gen AI (25/33 papers presented at the recent NeurIPS 23 workshop on Generative AI for Education used prompt engineer-ing [107]). All it requires is for the EdTech designer to write a set of instructions in natural language on what good tutoring behaviours look like, for example: \u201cStart by introducing yourself to the student"}, {"title": "3.3.2. Fine-tuning", "content": "If prompting can be roughly seen as the modern, more capable generalisation of expert systems, its alternative-fine-tuning, which typically includes stages of supervised fine-tuning (SFT), followed by Reinforcement Learning from Human Feedback (RLHF)\u2014brings the full power of the deep learning paradigm, i.e. learning from data, to the table. While far less computationally intensive than the standard pre-training phase, fine-tuning can still be costly to perform on models with many billions of parameters , which explains why it is less explored in the gen AI for education literature compared to prompting. However, fine-tuning (RL in particular) may enable AI to capture some of the intuition and reasoning that humans use in effective teaching, leveraging backpropagation to search the vast space of pedagogical possibilities discussed in Section 3.1.\nIn our current work, models M0-M4 are fine-tuned via SFT over all parameters of a base model (PaLM 2.0 for M0-M3 and Gemini 1.0 for M4 of comparable size; see Section E for further implementation details). While reward modeling and RL are crucial (and in our opinion the most promising) ingredients to building high-quality gen AI tutors, we have thus far focused only on SFT (and the requisite creation of behaviour cloning data). Of course, this puts our models at a serious disadvantage in evaluations against the base models, which include both SFT and (non-pedagogical) RL, and we plan to incorporate RL in the future (see Section F for a discussion of the challenges that come with eliciting human preferences to support RL for educational use cases).\nIt is worth mentioning that base models (PaLM 1.0 , PaLM 2.0 , Gemini 1.0 , and now Gemini 1.5 ) are improving rapidly. Each new model holds more knowledge, can perform more tasks more accurately, and is more controllable via prompting, so the task of improving them with respect to a particular set of behaviours like pedagogy, is constantly evolving. While M3 far outperformed PaLM 2.0 across many of our metrics, the gap between M4 (which basically differs from"}, {"title": "4. Measuring Pedagogy in Gen AI", "content": "Before evaluating education-specific improvements of LearnLM-Tutor over the prompt tuned Gemini 1.0, we first discuss whether our interventions resulted in any performance regressions in general accuracy. We then provide an overview of existing pedagogical evaluations from the gen Al literature, before describing our own approach to measuring pedagogy in gen Al tutors."}, {"title": "4.1. Accuracy on education-related benchmarks", "content": "We checked whether our fine-tuning interventions resulted in any regressions in accuracy of LearnLM-Tutor compared to base Gemini 1.0. To this end, we ran existing education-related benchmarks including MMLU , MATH , HellaSwag , and HumanEval , and safety benchmarks including RealToxicityPrompts  and BBQ with LearnLM-Tutor using exactly the same setups that were used for Gemini et al. . The results of LearnLM-Tutor reproduce the performance of Gemini Pro [10], for example an MMLU score of 0.72 and MATH score of 0.33.\nWhile this is a necessary criterion for demonstrating that there are no performance regressions, it is not sufficient as the model might be taken out of the fine-tuning data distribution back into the pre-training distribution of the base model in these few-shot prompting settings. We therefore also evaluated the performance of LearnLM-Tutor and Gemini 1.0 in the pedagogical conversation context by measuring the accuracy of the individual turns produced by these models. We found no"}, {"title": "4.2. Current approaches", "content": "Progress towards building a general purpose gen Al tutor has been slowed by the lack of good measures of progress towards this goal. Most of the evaluation methods from learning science for human tutors are not applicable to AI (e.g., because they rely on self-reports) . Currently, gen Al tutors tend to be evaluated using domain-agnostic metrics which act as a proxy for how coherent and human-like the generated responses are (e.g., BLEU , BERTScore , Rouge , DialogRPT ), but which are not designed to measure pedagogy or other education-specific capabilities [89, 98\u2013100, 103, 106]. Such metrics also often assume that there is a ground truth answer that the model response should match. However, there are many ways to respond to the same learner query with potentially equal pedagogical value, so a single \u201coptimal\u201d answer is impossible to define [98, 103, 130]. Many metrics are also easy to trick; for example, always responding with \"Hello\" can score highly , and adding a \u201cteacher:\u201d prefix can increase scores . A promising new approach to fast evaluations of gen Al tutors could be to use another gen AI for \u201ccritique\u201d . Recently, Chevalier et al. proposed using such gen AI critics to evaluate the presentation and correctness of the statements generated by a gen AI tutor. We are not aware of any group using such critics for pedagogical evaluations.\nAn alternative to automatic evaluations described above is using human experts to evaluate pedagogical performance. Interactive human evaluations are known to be important and tend to correlate better with user satisfaction . However, access to pedagogical experts is not easy, so typically studies use either very few experts (<10)  or the evaluation is done by study authors , which can both lead to biases. Furthermore, there is no agreed-upon protocol for running pedagogical human evaluations. The most commonly used human evaluation framework (Tack and Piech [98]) asks human raters to compare the responses of two tutors in the context of the same dialogue snippet. The comparison is done along three dimensions: replying like a teacher, understanding of the student, and helpfulness. These dimensions are based on Demszky et al.  and are important dimensions to evaluate, but they do not capture the full richness of pedagogy.\nAn important test of any gen AI tutor is whether it actually improves the learning outcomes of real students. Very few studies have run such evaluations, as most of them use paid raters to act as learners . Evaluations with real students are typically done with a small number of participants and in controlled experimental lab settings, which limits their validity . A notable exception is Liu et al. , who embedded a gen AI tutor into a CS50 MOOC course and made it available to millions of real students. However, the use of the tutor had to be heavily throttled due to cost considerations, and the results reported so far are limited in scope and come from a small number of on-campus students.\nThe difficulties in evaluating gen AI tutors mean that research groups are evaluating their gen Al tutors using their own metrics , which makes different approaches hard to compare (the BEA 2023 Shared Task  is a notable exception). There is a well-recognised need to develop better evaluation metrics suited to AI in education . However, Tack et al.  conclude that we are a long way from achieving the precise, valid, and automated pedagogical evaluations needed for progress in AI for education."}, {"title": "4.3. Our approach", "content": "In this section, we discuss our approach to narrowing down the vast space of all the possible pedagogical strategies (Section 3.1) and translating it into an evaluation rubric. We include discussion of the many pragmatic questions we considered, such as implementation difficulty, cost, validity, and other feasibility concerns."}, {"title": "4.3.1. Pedagogy rubrics", "content": "Alongside the principles described in Section 2, we combined further insights from our participatory sessions with literature reviews to create a high-level pedagogy rubric, which we then translated into measurable tutor behaviours by working together with teachers as expert advisers. The high-level pedagogical principles we prioritised are: encourage active learning (the learner should manipu-late information through discussion, practice, and creation, instead of passively absorbing informa-tion ), manage cognitive load (the tutor should present information in multiple modalities, structure it well, and segment it into manageable chunks ), deepen metacognition (\u201cthinking about thinking\u201d, which enables learners to generalise their skills beyond a single context [141\u2013143]), motivate and stimulate curiosity (as this leads to self-efficacy and lifelong learning [144, 145]), and adapt to learners' goals and needs (by assessing the current state and the goals, and making a plan to bridge the gap [146]). Each high-level pedagogical principle was translated into different measurable items used in different benchmarks (see Table 2 for automatic language model evaluation, Table 10 for conversation-level human evaluation, and Table 13 for turn-level human evaluation). These items took various forms, e.g. differing in the wording of the questions and in the level of granularity at which each high-level principle was broken down, while still designed to measure the same principle. This was to assess whether measuring the same pedagogical capability through different lenses provides a consistent answer, and also due to practical considerations (e.g. a different approach needs to be taken when asking a human or a gen AI critic to assess the same pedagogical principle). This is our first attempt at defining a pedagogical rubric, and we plan to iterate, improve, and expand it in the future."}, {"title": "4.3.2. Pragmatic evaluation taxonomy", "content": "To navigate the large space of practical considerations needed to implement pedagogical evaluations, we designed the taxonomy shown in Figure 2 and used it to compile seven pedagogical benchmarks with different trade-off profiles. We aimed for this set of benchmarks to provide a comprehensive view on the pedagogy performance of AI tutors. They were designed to be diverse and to traverse all nodes of the proposed taxonomy. Future work should do a more systematic investigation of how each node in the taxonomy affects the validity and effectiveness of the resulting benchmark. This taxonomy is described in more detail here:\nData collection: Participants To evaluate a gen AI tutor, we need to collect its responses in learning conversations. Who should interact with the tutor in these conversations?"}, {"title": "5. Human evaluations", "content": "In this section, we present the results of our human evaluations comparing LearnLM-Tutor to base prompt tuned Gemini 1.0. Interactions with human participants represent the gold standard for evaluation in responsible AI development; simulations cannot fully capture the complexities of real-world settings . Human participants allow us to observe authentic user behaviour and system responses within the context of dynamic, goal-oriented conversations. They can reveal issues that simulations might miss. Engaging with human participants is also crucial for promoting inclusion and representation in the development process . On the other hand, human evaluations suffer from limited sample sizes due to the expense and slow nature of recruiting pedagogical experts and collecting their judgements using cognitively demanding rubrics. Furthermore, special care needs to be taken to iterate over the rater instructions and the data collection pipelines to ensure the validity, consistency and calibration of the collected human rater judgements. All of these factors tend to lead to limited statistical significance of human evaluation results, which we also found to be the case. However, we see our results as signs of progress towards imbuing the Gemini 1.0 base model with additional pedagogical capabilities. We prioritised responsible design and conduct across all studies, following guidelines from research ethics (see Section I for details of our human evaluation)."}, {"title": "5.1. Unguided conversations: Subjective learner feedback", "content": "Learners first engaged in a 45-minute unguided (open-ended) session with a provided AI tutor through a chat interface. The tutoring session was grounded in an academic YouTube video, which they could select from a list, on maths, CS, biology, chemistry, literature, history or other subjects, like public speaking (see Section J.1 for the data collection details). They were then asked seven questions to assess their perception of the tutor. Learners rated LearnLM-Tutor higher than Gemini 1.0 tutor in most categories (Figure 4). However, we have only achieved statistical significance for one of them: learners felt more confident about applying what they had learnt with LearnLM-Tutor in the future by themselves."}, {"title": "5.2. Turn-level pedagogy: teacher feedback", "content": "We asked expert pedagogical raters to review and rate the unguided conversations from our learner study (Section 5.1). For each tutor turn, they determined whether one of nine suggested pedagogical \"moves\" was appropriate and desired in the conversational context (see Table 13 for the breakdown"}, {"title": "5.3. Conversation-level pedagogy: teacher feedback", "content": "We ran a number of guided conversation-collection experiments in which graduate-level experts interacted with two different tutors, role-playing as learners within their academic domain of expertise (biology, CS, maths, history, English, chemistry, or physics). The conversations with both AI tutors were grounded in the same educational video and a corresponding scenario, which specified the learner's persona, goal in the conversation (e.g. understanding how sound can be a wave, for a physics video on travelling waves), and other details (see Figure 17c). These pairs of conversations were then rated by pedagogical expert raters. First, each individual conversation in the pair was rated against a pedagogy rubric (see Table 10). In all of these rating experiments, the rubric was applied at the conversation level, as opposed to the turn-level ratings described in the previous sections.\nFigure 7 shows the effect sizes of the difference in ratings between pairs of prompted Gemini 1.0 and LearnLM-Tutor conversations on the same scenario. On average, the LearnLM-Tutor conversations were preferred to Gemini 1.0 on all attributes in the pedagogy rubric, except for No Contradiction (\u201cThe tutor does not contradict earlier parts of the conversation\u201d). The differences are statistically significant for Asks Questions (\u201cThe tutor makes the student think by asking questions where appropriate\u201d), and Openings (\u201cThe tutor keeps the conversation going by giving the student openings to engage\u201d), both measures of active learning, further corroborating turn-level teacher feedback which showed that LearnLM-Tutor is better at promoting engagement (Figure 5). Despite the lack of statistical significance, the large effect sizes suggest that LearnLM-Tutor has a better ability to encourage active learning (Active Engagement, Guides to Answer, Asks Questions, Openings), motivate (Stimulates Interest, Adapts to Affect), adapt (Leveling, Unstuck), and manage the learner's cognitive load (Analogies)."}, {"title": "5.4. Side-by-side pedagogy: teacher feedback", "content": "As part of the same study, we also asked raters to rank pairs of conversations with prompted Gemini 1.0 and LearnLM-Tutor that had been elicited with the same scenario. The rankings were according to five broad criteria, including an adapted version of the most widely used human evaluation questions from the GenAI for Education literature [98] (\"In which conversation was the tutor most like an"}, {"title": "5.5. Progress over time", "content": "We also show evidence of progress over time in Table 15 and Figure 19 in the Supplementary Materials, which compare turn-level and conversation-level ratings obtained from pedagogical experts between earlier versions of LearnLM-Tutor, Mo to M3, and the latest version, M4. These results show clear progress in turn-level pedagogy, as well as progress on all of the conversation-level pedagogy criteria with the exception of Manageable Chunks, Guides to Answer (\u201cThe tutor does not give away answers too quickly\u201d), and Expresses Uncertainty. The regression in Guides to Answer is in direct contrast to a significant improvement in Questions Appropriately, which is naturally opposed. Over time we steered"}, {"title": "6. Automatic Evaluations", "content": "While human evaluation is the gold standard for assessing model quality, it suffers from being time-consuming, expensive, and difficult to scale . To address these limitations, we introduce automatic evaluations (auto-evals) as a complementary approach."}, {"title": "6.1. Language Model Evaluations (LME)", "content": "Inspired by the success of large language models (LLMs) as judges in various domains , we propose a framework leveraging LLM-based critics to automatically assess tutor responses across a range of qualitative educational criteria (see Figure 9). Our automatic evaluation framework consists of a task specification (see Table 2 for an overview) and for each task, a dataset of input prompts and a critic LLM conditioned on a task-specific prompt (see Section K for more details)."}, {"title": "6.1.1. Results", "content": "The development of LearnLM-Tutor, from Mo to M4, was primarily guided by iterative improvements based on the automatic evaluation metrics for the pedagogical tasks. To ensure that these improve-"}, {"title": "6.2. Scoring human pedagogy with gen Al tutors", "content": "This section proposes another approach to fast evaluation of pedagogy in gen AI. Unlike the approach described in Section 6.1, which provides a detailed breakdown of the tutor performance along the"}, {"title": "7. Learning from real-world interactions: The ASU Study Hall program", "content": "All of the human- and auto-evaluations described in Sections 5 and 6 provided a consistent signal that LearnLM-Tutor improved over Gemini 1.0 on a number of pedagogical dimensions. To understand how learners would use LearnLM-Tutor in a formal, real-world academic setting, we turned back to a participatory approach and partnered with Arizona State University (ASU) to integrate LearnLM-Tutor into ASU's Study Hall. Study Hall is a partnership between ASU, Crash Course, and YouTube that offers a pathway to college creditand is accessible to learners of all ages and backgrounds. Study Hall, with its open enrollment and no prerequisites, attracts a diverse group of learners from ages 14 to 72, from first-time college students building confidence, to career-minded professionals seeking new skills. The broad appeal and universal access of Study Hall provides a unique opportunity to test"}, {"title": "9. Responsible development", "content": "Our approach to responsible development of LearnLM-Tutor closely follows that of the Gemini family of models  and other releases of Google's AI technology  and is guided by Google's AI principles . Figure 16 shows the structure of our approach. Our starting points are the released Gemini models, which have undergone extensive safety testing and mitigation , but we repeat the entire cycle of responsible development for the specific use-case of an Al tutor. Our participatory and evaluation-driven approach allows us to take a sociotechnical view of the benefits and risks of LearnLM-Tutor; to analyse not only the model itself, but how it might impact learners in a variety of different contexts, and the wider education system. In the remainder of this section, we discuss each step of this process in turn."}, {"title": "9.1. Impact assessment", "content": "Impact assessments were carried out throughout the development, drawing on the participatory workshops with learners and educators described in Section 2.1, and the literature on the benefits and harms of generative AI  and of artificial intelligence for education specifically . All individual studies and products underwent a separate impact assessment; in the case of the ASU HallMate study in Section 7, this was conducted by Google DeepMind's Human Behavioural Research Ethics Committee.\nThrough our participatory research, we have learned that AI tutors can be beneficial to learners by promoting active learning and providing personalised help when explaining concepts or working through problems. An AI tutor can understand the learner's current knowledge, adapt its explanations to the learner's proficiency, and making connections to real-world examples interesting to the learner. An Al tutor can also help with the learners' time management by providing succinct and specific explanations and by highlighting relevant sections in the learning material to study. It can be grounded"}, {"title": "9.2. Policies", "content": "Our safety evaluations and mitigations and launch decisions are guided by policies specifically formulated for LearnLM-Tutor, based on those of Gemini , but tailored to the use case of AI tutoring and contexts such as ASU HallMate (see Section 7). Our policies were informed by our risk assessment and participatory methods. They include areas such anthropomorphism, bias in teaching quality or level, medical and financial advice, neutrality of viewpoint (this is especially important for subjects like history and politics), and how the model should use the grounding material. For example, opinions should not be repeated as fact but should be attributed with a precise reference (e.g., a timestamp in the case of a video lesson)."}, {"title": "9.3. Mitigations", "content": "Mitigations to known risks were applied from the outset, with further mitigations being added to address failure modes discovered during safety evaluations. The first mitigation was careful curation of our SFT data: our \u201cGolden conversations\u201d data was written by pedagogy experts with instructions on style and content, and most of our synthetic fine-tuning data (with the exception of some synthetic data for mathematics) was manually reviewed. Furthermore, we used prompted LLMs to flag turns in the data that might make policy violations more likely and manually reviewed all flagged turns.\nOur main mitigation method was additional safety fine-tuning on top of that of Gemini 1.0. This is necessary to enforce the additional safety policies for LearnLM-Tutor, and mitigate safety issues arising from the customisation of the models for AI tutoring-even non-adversarial customisation can affect safety \u2014 and customise the way the model responds to policy violation-inducing queries. Since a conversation with LearnLM-Tutor has a narrower conversation goal than that of a generalist conversational AI, the handling of most harm-inducing queries can be different: for queries that are unrelated to the learning goal, we aimed for LearnLM-Tutor to give briefer rejections and refocus the conversation on the lesson content. Our safety fine-tuning data consists of harm-inducing conversations and golden responses on lesson material across a wide range of subjects. Queries were either written by the team or taken from failures observed during automatic or human red-teaming. The number and type of training examples was chosen to ensure broad coverage of our model policies and different harm types as well as appropriate dataset size relative to the rest of our fine-tuning data."}, {"title": "9.4. Evaluations", "content": "As a necessary but not sufficient indicator that fine-tuning the model did not lead to safety regressions, we evaluate LearnLM-Tutor on standard safety and bias benchmarks such as RealToxicityPrompts  and BBQ . The results match those of Gemini Pro reported in Gemini et al. . When lesson grounding material is provided, performance on RealToxicityPrompts is further improved significantly as LearnLM-Tutor can easily reject most queries as off-topic. This highlights the limits of standard benchmarks for evaluating context-specific models like LearnLM-Tutor: effective testing of the model has to be specific to the context of an AI tutor and the grounding material provided. In the remainder of this section we describe our custom evaluation methods.\nRed teaming The main goals behind our red teaming efforts were to test adherence of the models to our safety policies (see Section 9.2) and to identify any failure modes. As a side-product, they provided adversarial queries that correspond to current model failures, which made them particularly helpful for the safety fine-tuning data (after writing golden responses) and automatic evaluation prompts. Human red teaming was carried out in collaboration with Google's ProFair  and YouTube's Trust and Safety Team based on our safety policies and followed the structured, sociotechnical approach used by Gemini et al. . Adversarial attacks involved not only the queries, but also the choice of grounding material. This is crucial, as LearnLM-Tutor is trained to stay on topic and our policies cover how LearnLM-Tutor should interact with the grounding material. In addition to this structured red teaming, we organised Google-internal \u201cdogfooding\" programmes and \u201cbug bashes\".\nFurthermore, we used automatic red teaming to find conversations for which LearnLM-Tutor's output maximally violates a specific policy as measured by some approximate scoring function. We do this iteratively by rephrasing LearnLM-Tutor's responses as learner questions, sampling the model multiple times at each stage and retaining only the most policy-violating responses. As scoring function, we use an LLM prompted to quantify the amount of violation of a specific policy. The details of this process are described in Section O. We manually review the resulting conversations, flag any policy-violating ones, and identify failure patterns. An important feature of this process is that it is able to identify failure modes that only arise in multi-turn conversations.\nAutomatic evaluations Our automatic evaluation framework for pedagogy (Section 6) also lent itself well to quantifying and monitoring specific harm types in LearnLM-Tutor. It enabled quick verification of anecdotal reports of policy violations found during dogfooding or human red teaming, quantifying the scale of the problem, and demonstrating successful mitigation (see Tables 6 and 8 for examples). For each metric that should be tracked, we created a dataset of policy-violation inducing queries or conversation histories, sampled model responses, and rated them with a prompted LLM as critic."}, {"title": "9.5. Examples of the evaluation and mitigation process", "content": "We present two examples of our evaluation and mitigation process: failure patterns caused by the customisation of the model for pedagogy, and anthropomorphism as an example of a risk that was identified early on and tracked throughout the entirety of development."}, {"title": "9.5.1. Failure patterns caused by customisation", "content": "Model customisations\u2014even if they are non-adversarial\u2014can result in safety regressions . This is equally true of our pedagogy fine-tuning. For example, the model developed a tendency to praise harm-inducing student questions, such as questions containing harmful premises or asking for help with harmful actions, before rejecting them as off-topic or asking for clarification. Table 6 shows an example of this failure pattern, including an unacceptable and an acceptable response. Clearly, this failure pattern was introduced by the many turns in our fine-tuning data that respond positively to questions from the learner to encourage more questions. Since all safety issues introduced by the fine-tuning affected specific patterns rather than policies, we extended our red-teaming to be informed by patterns in the fine-tuning data, such as identifying mistakes or encouraging questions.\nPattern: Praise for harm-inducing queries when rejecting them as off-topic."}, {"title": "9.5.2. Anthropomorphism", "content": "Perceiving human-like characteristics in non-human systems is known as anthropomorphism [172]. Many technologies have been perceived as human-like by their users , including generative conversational AI systems powered by large language models . Anthropomorphic percep-tions of technologies, including AI, have been demonstrated to have a great impact on how users interact with and form mental models of the systems . While greater trust and acceptance of anthropomorphic systems may have a positive effect on user-system interactions in certain contexts, like customer service , it is important to anticipate downstream harms. For example, users may experience emotional attachments to AI systems, which may give rise to dependence and over-reliance on Al systems .\nIn addition to including harmful anthropomophisms as a target for human and automatic red teaming, we added a family of automatic evaluations to track potentially harmful anthropomorphism in the model. These include directly pretending to be human or the creator of the grounding lesson"}, {"title": "9.6. Deployment", "content": "Launch reviews were performed on LearnLM-Tutor for downstream applications based on the perfor-mance and safety evaluation results, including an analysis of red teaming of the entire pipeline, and the internal model  and system cards. See Section A for the external model card. LearnLM-Tutor should not be used in downstream applications without further evaluation and analysis of the harms specific to this application. Our roll-outs and studies were staged, e.g., via a restricted beta, and we continuously monitor LearnLM-Tutor's performance and user feedback."}, {"title": "10. Discussion", "content": "We are encouraged by the progress described in this report, while remaining conscious of the limitations of our work. Supervised fine-tuning (SFT) with pedagogically informed data mixtures (Figure 3) resulted in an AI tutor more pedagogical than a strong baseline\u2014instruction-tuned Gemini 1.0 prompted with a state-of-the-art externally validated tutor prompt [1]. However, the current version of LearnLM-Tutor (M4) still leaves room for future innovation as we work towards developing true pedagogical mastery.\nOur SFT-based approach requires demonstrations of \u201cgood pedagogy\u201d. It is unknown how many such examples are required to cover a full range of pedagogical behaviours such that a model fine-tuned on them can generalise well, and manual data collection of this type is expensive. It will be useful to additionally explore approaches such as RLHF  in the future.\nThe starting-point benchmarks described in this report come with limitations: gen AI-critics can be unreliable, human evaluations are slow and costly, and there are a number of challenges that come with eliciting accurate feedback from paid raters. Aside from these practical considerations, we believe there is room for continued conceptual iteration to best translate high-level pedagogical principles into tractable auto-eval datasets, critic prompts, and human evaluation rubrics. It will be important to continue to iterate on and adapt these benchmarks so that they remain sensitive to differences between models as gen Al continues to improve."}, {"title": "11. Conclusion", "content": "This report has described our evaluation-driven approach to improving gen Al for education, focusing on conversational tutoring due to its potential for positive impact for both learners and educators. We have put together a multidisciplinary team of AI scientists, engineers, pedagogical experts, safety researchers and cognitive scientists to work together in this direction. Our approach starts and ends with participation, combining direct engagement with learners and educators through interviews and workshops with a thorough literature review of learning science research to identify a set of pedagogical principles and capabilities to prioritise in our development work. These insights were translated into practical steps towards improving the pedagogical abilities of Gemini 1.0 through supervised fine-tuning. Additionally, we created a set of seven diverse pedagogical benchmarks including quantitative, qualitative, human-based and automatic evaluations. These were applied to our best gen Al tutor, LearnLM-Tutor, whose performance we compared to the prompt tuned Gemini 1.0 model, revealing that LearnLM-Tutor outperformed Gemini 1.0 on the majority of measured pedagogical dimensions. This report also describes limitations of our work. We hope that the AI, EdTech, and learning science communities see this report as an invitation to join forces and work together to continue developing and iterating on a set of pedagogical benchmarks that we can all use in our daily research and product development. We strongly believe that having good measures of success is essential for making significant progress towards maximising the potential of gen AI in education."}, {"title": "B. Participatory research details", "content": "This section includes details of the participatory research"}, {"title": "C. Intelligent Tutoring Systems", "content": "This section includes details about Intelligent Tutoring Systems"}, {"title": "D. Challenges with prompting gen AI for pedagogy", "content": "This section includes details about challenges with prompting Gen AI for Pedagogy"}, {"title": "E. Tutor agent", "content": "This section includes details about Tutor agent"}, {"title": "F. Challenges with eliciting human preferences for pedagogy", "content": "This section includes details about eliciting human preferences for pedagogy"}, {"title": "G. Sociotechnical limitations of text-based gen AI", "content": "This section includes details about the Sociotechnical limitations of Text-Based Gen AI"}, {"title": "H. Turn-level human accuracy evaluation details", "content": "This section includes turn-level human accuracy evaluation details"}, {"title": "I. Human evaluations", "content": "This section includes human evaluations"}, {"title": "K. Automatic evaluations: Additional LME details", "content": "This section includes automatic evaluations"}, {"title": "M. Critic prompts used in automatic evaluations", "content": "This section includes critic prompts"}, {"title": "N. Safety AutoEval critic prompts", "content": "This section includes safety autoeval critic prompts"}, {"title": "O. Automatic red teaming algorithm", "content": "This section includes automatic red teaming algorithm"}]}