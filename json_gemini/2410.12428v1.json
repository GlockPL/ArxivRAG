{"title": "Conformity in Large Language Models", "authors": ["Xiaochen Zhu", "Caiqi Zhang", "Tom Stafford", "Nigel Collier", "Andreas Vlachos"], "abstract": "The conformity effect describes the tendency\nof individuals to align their responses with the\nmajority. Studying this bias in large language\nmodels (LLMs) is crucial, as LLMs are increas-\ningly used in various information-seeking and\ndecision-making tasks as conversation partners\nto improve productivity. Thus, conformity to\nincorrect responses can compromise their effec-\ntiveness. In this paper, we adapt psychological\nexperiments to examine the extent of confor-\nmity in state-of-the-art LLMs. Our findings\nreveal that all models tested exhibit varying\nlevels of conformity toward the majority, re-\ngardless of their initial choice or correctness,\nacross different knowledge domains. Notably,\nwe are the first to show that LLMs are more\nlikely to conform when they are more uncer-\ntain in their own prediction. We further explore\nfactors that influence conformity, such as train-\ning paradigms and input characteristics, finding\nthat instruction-tuned models are less suscep-\ntible to conformity, while increasing the natu-\nralness of majority tones amplifies conformity.\nFinally, we propose two interventions-Devil's\nAdvocate and Question Distillation\u2014to miti-\ngate conformity, providing insights into build-\ning more robust language models.", "sections": [{"title": "Introduction", "content": "Although large language models (LLMs) have\nrapidly advanced and exhibit increasingly human-\nlike behavior (Aher et al., 2023; Kasneci et al.,\n2023), they are often affected by biases present in\nthe data they are trained on (Navigli et al., 2023;\nYu et al., 2023). Most biases studied in LLMs tend\nto be overt and domain-specific (e.g., gender, race,\netc.) making them relatively easier to detect and\nmitigate (Gallegos et al., 2024; Ranaldi et al., 2024).\nHowever, in humans, more subtle, meta-cognitive\nbiases exist across different knowledge domains,\nsuch as the Dunning-Kruger Effect (Kruger and\nDunning, 1999), confirmation bias (Mercier and\nSperber, 2017), and the one we focus on in this\nstudy the conformity effect. Conformity refers to\na form of social influence, in which an individual's\nbeliefs or behaviour shift towards being inline with\nthe majority (Asch, 1955; Sowden et al., 2018),\nas illustrated in Figure 1. Extensively studied in\npsychology, conformity is observed not only in sub-\njective or open-ended contexts but also in situations\nwith a clear right answer (Bernheim, 1994; Crutch-\nfield, 1955). For example, Asch (1955) demon-\nstrated that under peer pressure, individuals often\nabandon correct answers to align with an incorrect\nmajority, even in simple perceptual tasks.\nStudying conformity in LLMs is particularly cru-\ncial. The conversational use of LLMs for complex\ntask-solving has been shown to enhance both qual-\nity and productivity, offering a promising future\nfor such systems (Dell'Acqua et al., 2023). How-\never, conformity can significantly degrade the per-\nformance of language models, especially in multi-\nagent systems that utilize LLM ensembles (LLM-\nMAs) or involves human interactions (Guo et al.,\n2024; Hong et al., 2023; Chen et al., 2024; Feng\net al., 2024). When LLMs conform to incorrect an-\nswers, it can undermine the effectiveness of these\nsystems, particularly in tasks such as collective-\nintelligence collaboration or constructive debate\n(Zhang et al., 2024b; Patel et al., 2024; Khan et al.,\n2024), ultimately negating the potential benefits\nthat LLMs offer in these contexts.\nRecent work has acknowledged the existence of\nconformity effect in LLMs. Zhang et al. (2023)\nobserved conformity in general tasks such as chess\nmove validation and multiple-choice question an-\nswering, where models aligned with perceived peer\npressure. Baltaji et al. (2024) extended this in-\nvestigation to cross-cultural collaboration and de-\nbate, highlighting that conformity remains a per-\nsistent issue in more diverse, open-ended discus-\nsions. Additionally, recent research has shown that\nLLMs trained with human preferences exhibit be-\nhaviors similar to the conformity effect, such as ex-\nploiting human judgments and generating outputs\nthat appeal to evaluators regardless of their cor-\nrectness-demonstrating patterns of sycophancy\n(Perez et al., 2023; Sharma et al., 2023). How-\never, these studies primarily focus on identifying\nthe presence of conformity without exploring the\nunderlying factors that drive it. Moreover, there\nis a lack of detailed analysis on the mechanisms\ninfluencing susceptibility to conformity, and no\npotential mitigation strategies have been proposed.\nIn this work, we investigate a range of state-\nof-the-art LLMs (e.g., Llama-3, Qwen2, Gemma-\n2, and Mistral-v0.3) on datasets covering a wider\nrange of tasks, subjective vs. objective question\nanswering, estimation vs. memorisation, mul-\ntiple choice vs. open-ended question answer-\ning (MMLU, BigBenchHard, PopQA, Common-\nsenseQA, Politiscale, OpinionsQA). Our findings\nreveal that conformity is a universal phenomenon\namong LLMs and pervasive across knowledge do-\nmains. To further investigate the factors that trigger\nconformity, we conduct evaluations under vary-\ning conditions, examining both training paradigms\n(e.g., pre-training vs. instruction-tuning) and in-\nput characteristics (e.g., tones, prompt complex-\nity). Our work is the first to show that models\nwith higher initial confidence in their original pre-\ndictions for a question are less likely to conform\n(p < 0.001) when exposed to an incorrect majority.\nWe also propose two simple prompt-based heuris-"}, {"title": "Methodology", "content": "Asch (1956) defined conformity as the phe-\nnomenon where individuals lacks of independence\nin the face of group pressure. The study defined the\nindividuals as critical subjects, who often shifted\ntheir solutions to align with the majority, who are\nreferred to as confederates, regardless of whether\nthe majority's answers were correct. The Asch con-\nformity experiment (Asch, 1951), involves a simple\nvisual perception task. A single participant, the crit-\nical subject, will be asked to give an answer after a\nwrong answer is given unanimously by an increas-\ning number of confederates. The critical subject's\nanswer will then be recorded to examine whether\nit's correct or conformed to the majority. Allen and\nLevine (1969) extended the visual perception task\nto information and opinion items.\nIn our case, we identify the critical subject as\na language model $LM_{\\theta}$. We replace the visual\nperception task with Q&A in the form of a dia-\nlogue. Given a dataset $Q = \\{q_1, q_2, ... q_n\\}$, we\ndefine a prompt function $f(q, p, c; LM_{\\theta})$ that takes\nthe question, $q$, the number of total participants\nin the dialogue, $p$, and an in-domain distractor\nanswer, $c$, to generate a dialogue template that\nprobes for the language model's response. When\n$p > 1$, the model is the $p$th participant to answer\nthe question, with all preceding $p-1$ confederates\nunanimously expressing $c$ as their answer. Oth-\nerwise, if $p = 1$, the language model is the only\nparticipant in the dialogue and it is not affected by\nthe non-existent distractor answer $c$. Firstly, we\nprobe the model's initial answer to the question,\n$a = f(q_i, 1, \\O; LM_{\\theta})$. Then we define the evalu-\nation set $S = \\{(q_i, a_i, c_i) | q_i \\in Q\\}.\nAsch (1951) focuses on the number of correct\nanswers and the frequency of conformity by di-\nrectly comparing the critical subject's answer with\nrespect to its original answer. Similarly, we de-\nfine and monitor the level of conformity $CL_p$, and\nlevel of resistance $RL_p$ of the critical subject model\n$LM_{\\theta}$ with respect to participant number $p$ and the\naugmented evaluation set $S$ from question dataset\n$Q$ as follows:\n$CL_p(S, p; LM_{\\theta}) = \\frac{\\sum_{i=1}^{|S|} 1 (a_i = c_i)}{|S|}$\n$RL_p(S, p; LM_{\\theta}) = \\frac{\\sum_{i=1}^{|S|} 1 (a_i = a_i^*)}{|S|}$\nwhere $a_i = f(q_i, p, c_i; LM_{\\theta})$. We record the pro-\nportion instead of instances for better cross compar-\nison as the size of different Q&A datasets varies.\nObjective vs. Subjective Questions. Allen and\nLevine (1969) reported different patterns of confor-\nmity on information and opinion items from human\nparticipants. Building on this, we examine confor-\nmity of models on both objective and subjective\nquestion. Under our definition, objective questions\nhave clear, fact-based answers that can be verified\nas either correct or incorrect, typically in areas\nlike mathematics, factual knowledge, or natural sci-\nences. In contrast, subjective questions don't have a\nsingle correct answer and often depend on personal\nopinions, interpretations, or perspectives. They are\nmore common in areas like literature, ethics, or\nsocial sciences, where answers can vary based on\nindividual reasoning or experiences.\nEvaluation Strategy. We apply different evalua-\ntion strategies for conformity on objective and sub-\nject questions. Zhang et al. (2024a) point out that\ncurrent models are trained to generate facts even\nwhen such facts are missing from their parametric\nknowledge. For factual questions, when an incor-\nrect answer is provided by the model, it is ambigu-\nous whether the model has memorized an incorrect"}, {"title": "Experiments", "content": ""}, {"title": "Experiment Setup", "content": "Models. We use Llama-3-8B (Meta, 2024),\nMistral-v0.3-7B (Jiang et al., 2023), Qwen2-7B\n(Yang et al., 2024), and Gemma2-9B (Gemma\net al., 2024). For each model, we employ both\nthe instruction-tuned and base versions to inves-\ntigate the effect of instruction tuning. We apply\ngreedy decoding to generate the answers. Details\ncan be found in Appendix A. We use VLLM library\nto serve all models (Kwon et al., 2023).\nDatasets. We evaluate the LLMs on vari-\nous datasets across different knowledge domains.\nThe objective QA datasets we used are MMLU\n(Hendrycks et al., 2021), BigBenchHard (Object\nCounting) (Suzgun et al., 2023), PopQA (Mallen\net al., 2023), and CommonsenseQA (Talmor et al.,\n2019). The Q&A format includes both multiple-\nchoice and open-ended questions. For subjective\nQ&A datasets, we use Politiscale (Conobi, 2018)\nand OpinionsQA (Santurkar et al., 2023)."}, {"title": "Conformity Effect in LLMs", "content": "We first show that the conformity effect is\nwidespread. Figure 2 illustrates that conformity\ninfluences performance across diverse tasks and\nknowledge domains. The resistance level $RL_p$\n(shown in green) gradually decreases, while the\nconformity level $CL_p$ (shown in red) increases as\nthe number of confederates grows. This pattern\nholds across both subjective and objective datasets.\nMore results across different models are in Figure\n14 in Appendix C. For both PopQA and BBH, we\nalso notice a significant number of responses that\nare neither correct nor conforming (shown in blue).\nThis suggests that the conformity setting may also\nmislead the model to select other incorrect answers."}, {"title": "Factors Influencing the Conformity Effect", "content": "We identify two key factors that influence the extent\nof the conformity effect: the tone of confederates\nand whether the models are instruction-tuned.\nTones of Confederates. Figure 4 compares the\nmodel's conformity levels under the Unanimous\nsetting with different tones. We have the following\ntwo findings: (1) Comparing Plain and Neutral,\nwe find that the Neutral setting consistently in-\ncreases the conformity level. The more natural and\nconversational tone, closer to real-life dialogue,\namplifies the tendency to conform. (2) Compar-\ning Neutral, Uncertain and Confident expres-\nsions, we find Confident consistently increases\nconformity. However, the effect of Uncertain\nexpressions on conformity varies across models.\nFor Gemma2 and Llama3, Uncertain expressions\nlead to lower conformity, as expected: if the LLM\nperceives the participants as lacking confidence,\nit relies more on its own beliefs. This indicates\nthat these models are more sensitive to the second\ndifference. In contrast, for Mistral and Qwen2,\nUncertain expressions increase conformity.\nPrevious psychological experiments on humans\nalso studied the factor of confidence in conformity.\nSimmons and Nelson (2006) found that individuals\nwho expressed their opinions with high confidence\nwere significantly more likely to influence the de-\ncisions or opinions of others. Similarly, Moussa\u00efd\net al. (2013) discovered that opinions expressed\nwith high confidence tend to have a greater influ-\nence on the final group decision, as confidence can\nsignal competence or authority.\nInstruction-tuning. As shown in Figure 5, instruc-\ntion tuning reduces conformity across all models.\nFor Gemma2 and Llama-3, it significantly lowers\nthe conformity level. However, for Mistral and\nQwen2, the effect is more limited, as their initial\nconformity levels are already low.\nThe Difficulty of Questions. We observe that ques-\ntion difficulty influences the level of conformity in\nresponses. For instance, in the relatively more chal-\nlenging BBH Object Counting task (Figure 2), we\nfind a higher conformity level compared to sim-\npler tasks. We further analyze the performance\nof Llama-3-8B-Instruct across 57 subjects in the\nMMLU dataset. By identifying tasks with lower ac-\ncuracy as more difficult, we confirm our hypothesis\nthat LLMs are more likely to conform when facing"}, {"title": "Confidence in Conformity", "content": "In this section, we investigate the underlying pat-\nterns of questions that make models prone to con-\nformity. We observe that some questions are never\ninfluenced by the confederates (i.e., they are al-\nways answered correctly). Inspired by the previous\nfinding that lower confidence level in the original\nanswer may lead to more severe conformity (Baron\net al., 1996), we estimate the model's confidence\non these non-conforming questions.\nWe conduct experiments using the MMLU and\nPopQA datasets. For MMLU, we measure con-\nfidence using the log probability of the option,\nand for PopQA, we apply consistency-based uncer-\ntainty estimation via EigV (Lin et al., 2024). First,\nwe select questions that models have never con-\nformed to the majority, regardless of the number of\nconfederates, and compare them to questions where\nconformity occurs at least once. The confidence\ndistributions for these two groups of questions in\nthe vanilla setting (without confederates) are shown\nin Figure 7.\nOur results indicate that the model's initial\nconfidence is a key predictor of whether it will\nconform on a given question. Models with\nhigher initial confidence are less likely to conform,\nwhereas if the initial confidence is low, it is more\nprone to align with the majority. The p-values are\nall smaller than 0.001, indicating a significant dif-"}, {"title": "Eliminating the Conformity Effect", "content": "We propose two methods to eliminate the confor-\nmity effect: Devil's Advocate and Question Distil-\nlation. Figure 8 shows that both approaches effec-\ntively mitigate the conformity effect."}, {"title": "Devil's Advocate (DA)", "content": "Janis (1972) highlighted that assigning someone\nthe role of a devil's advocate can reduce confor-\nmity in decision-making by injecting diversity, thus,\nencouraging independent thinking and mitigating\nthe suppression of alternative viewpoints. This\nis also confirmed by group deliberation research\n(Karadzhov et al., 2024), where diversity in opin-\nions is crucial for improving decision quality. In\nour experiment, we adopted this strategy by having\none extra confederate who provides a different in-\ncorrect answer to inject diversity and examine the\nimpact on language model conformity.\nIn objective questions, the devil's advocate re-\nduces the conformity effect. As shown in Figure\n8 for MMLU, the DA method significantly reduces\nconformity in models that are more susceptible to\nconformity biases (e.g., Gemma2 and Llama3). For\nmodels that are initially more resistant to confor-\nmity (such as Mistral and Qwen2), this effect is less\npronounced. Interestingly, even when the devil's\nadvocate provides an incorrect answer, the mere\npresence of dissent reduces overall conformity, con-\nsistent with the idea that diversity of opinions, even\nwhen inaccurate, can lead to more effective deliber-\nation and independent model outputs. This mirrors\nthe broader observation that diverse groups tend to\noutperform homogeneous ones in decision-making\ntasks (Karadzhov et al., 2024).\nDevil's Advocate is equally effective for subjec-\ntive questions. Allen and Levine (1969) found that\nadditional dissent reduces conformity in factual\ndiscussions but has less impact on opinion-based\nitems. In contrast, our experiments show that the\nDevil's Advocate approach is equally effective for\nLLMs on subjective question datasets, as demon-\nstrated in Figure 9. This divergence suggests that\nLLMs may treat subjective questions similarly to\nobjective ones, as opinions are learned during train-\ning much like factual information, indicating a po-\ntential lack of true subjectivity in their responses."}, {"title": "Question Distillation (QD)", "content": "To analyze which parts of the input prompts con-\ntribute most to the conformity effect, we examine\nthe model's attention distribution of the answer to-\nkens over the input prompt, as presented in Table 2.\nWe find that, instead of focusing on the digit repre-\nsenting the number of participants in the dialogue,\nthe model overemphasizes the repeated answers,\nas shown in the upper part of Table 2, leading to\nconformity. A natural solution is to remove this\nmisplaced attention during the model's inference.\nTherefore, we propose the Question Distillation\n(QD) method to address this issue. QD aims to\nsimplify the prompt, making the task clearer for\nthe model. Rather than listing all confederates' an-\nswers individually, we summarize them into shorter\nprompt (e.g., \"All participants before you have cho-\nsen...\"), as shown in the lower part of Table 5. In\nQD, the model applies less attention to the major-"}, {"title": "Discussion", "content": "In this section, we explore the connection between\nour findings and psychological studies, aiming to\ndeepen the understanding of the relation between\npsychology and NLP. We examine how our study\nextends to other areas of NLP research and, by\ncomparing our results with psychological insights,\nraise important questions about the mechanisms\ndriving LLM conformity and the causality between\nhuman psychology and LLM behavior.\nApplying psychological frameworks to LLMs\nis an emerging approach as these models begin to\ndisplay anthropomorphic traits, including prefer-\nences and social norms. Stereotypes, biases, and\nother human-like behaviors are shaped by the vast\namounts of training data used in pre-training (Ke\net al., 2024; Demszky et al., 2023). Griffin et al.\n(2023) showed that LLMs are susceptible to ex-\nternal input and exhibit psychological shifts remi-\nniscent of human responses. Moreover, Yiu et al.\n(2024) proposed that LLMs imitate human cultural\nand social transmission, suggesting that they repli-\ncate human biases, such as the conformity effect.\nDeutsch and Gerard (1955) expanded on Asch\n(1951) by distinguishing two types of conformity\npressures: normative (conforming to gain approval)\nand informational (believing the group's judgment\nis more accurate). These pressures explain causes\nof conformity via the rational informational weight-\ning framework, where individuals balance their\nown knowledge with external inputs (Sperber et al.,\n2010; Bernard et al., 2015). We interpret this theory\nfor LLMs by equating personal perception to the\nparametric knowledge obtained during pre-training,\nwhile external beliefs are represented by the in-"}, {"title": "Conclusion", "content": "Our study presents the first systematic investiga-\ntion of the conformity effect in LLMs, revealing\nthat leading models display conformity to majority\nopinions. By identifying key triggers and mecha-\nnisms, we distinguish between training paradigms\nand input characteristics influences, finding that\ninitial confidence serves as a critical factor. Addi-\ntionally, we propose two effective, prompt-based\ninterventions\u2014Devil's Advocate and Question Dis-\ntillation to reduce conformity without additional\ntraining. Our findings not only underscore signif-\nicant parallels with human social biases but also\nopen new directions for mitigating subtle biases\nin LLMs, paving the way for more robust and fair\nlanguage models."}, {"title": "Limitations", "content": "The limitations of our work are as follows: First,\nour study focuses exclusively on text-based, single-\nmodal interactions. While this isolates conformity\neffects in language tasks, real-world human-AI in-\nteractions often involve multimodal inputs (e.g.,\nvisual, auditory cues), which may also influence\nconformity. For example, classic Asch experiments\nincluded non-verbal cues like gestures and facial\nexpressions. Future work should incorporate multi-\nmodal frameworks to examine how LLMs conform\nwhen exposed to diverse stimuli, offering deeper\ninsights into cross-modal conformity effects.\nFinally, our use of controlled, artificial Q&A di-\nalogues may not fully capture the complexity of\nreal-world interactions. Human-AI collaboration\ninvolves more nuanced social dynamics, where fac-\ntors such as conversational context, feedback, and\nmulti-turn exchanges could impact conformity dif-\nferently. Future studies should explore more realis-\ntic, open-ended scenarios to assess how conformity\nunfolds in dynamic human-AI interactions."}, {"title": "Ethics Statement", "content": "Our research adheres to strict ethical standards.\nNo human participants were involved in our ex-\nperiments, and no deception or manipulation was\napplied only LLMs were evaluated in the con-\nformity tests. We ensured compliance with the\nlicenses of all datasets and models used. After thor-\nough assessment, we do not anticipate any addi-\ntional ethical concerns or risks related to our work."}, {"title": "Appendix", "content": null}, {"title": "Experiment Details", "content": "We have used Gemma-9B, Llama-3-8B, Mistrial-7B-v0.3, Qwen2-7B and their instruction-tuned versions.\nAll models are publicly available online. All experiments are run on 1 * A100 (80 GB). The experiments\ntakes around 1 hour per setting per model per dataset. In total, all experiments roughly 1 * 8 models\n(including both instruction and base) * 6 datasets * 1 setting + 7 settings * 1 dataset (MMLU) * 8 models\n= 48 + 56 = 104 hours. No hyperparameters is required in our experiments. All models are run with bf16\nprecision and greedy decoding (temperature=0, top-p=1). We use VLLM (v0.5.4) to serve all models."}, {"title": "Prompts", "content": null}, {"title": "Additional Results", "content": null}]}