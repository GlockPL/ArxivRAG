{"title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal", "authors": ["Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Ziyan Lei", "Xiaofei Xie", "Yige Wang", "Chao Shen"], "abstract": "In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by the Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify two critical challenges in generating holistic audit datasets: ensuring audit adequacy and handling knowledge redundancy between forget and retain dataset. To address these challenges, we propose HANKER, an automated framework for holistic audit dataset generation leveraging knowledge graphs to achieve fine-grained coverage and eliminate redundant knowledge. Applying HANKER to the popular MUSE benchmark, we successfully generated over 69,000 and 111,000 audit cases for the News and Books datasets respectively, identifying thousands of knowledge memorization instances that the previous benchmark failed to detect. Our empirical analysis uncovers how knowledge redundancy significantly skews unlearning effectiveness metrics, with redundant instances artificially inflating the observed memorization measurements ROUGE from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have undergone rapid development, demonstrating impressive capabilities across a wide range of applications, from natural language processing to code generation and complex problem-solving (Liu et al., 2023; Satpute et al., 2024). However, these advances have raised concerns about potential risks associated with the vast knowledge stored in these models, e.g., the inadvertent retention of personally identifiable information (PII) (Jang et al., 2022), the propagation of unsafe or biased behaviors (Liu et al., 2024e), and the unauthorized use of copyrighted content (Eldan and Russinovich, 2023). Furthermore, there is an increasing imperative for LLMs to comply with regulatory standards such as the General Data Protection Regulation (GDPR) (Hoofnagle et al., 2019), which enforces the \"Right to be Forgotten\" (Dang, 2021). To address these concerns, researchers are investigating various unlearning techniques (Jia et al., 2024a) to selectively remove specific knowledge from pre-trained LLMs while preserving their general language modeling capabilities, thereby avoiding the substantial computational costs associated with building new models from scratch.\nThe growing significance of LLM unlearning has heightened the importance of rigorous evaluation or audit of unlearing performance. Recent benchmarks like MUSE (Shi et al., 2024) and TOFO (Maini et al., 2024) assess unlearning efficacy across multiple dimensions, ranging from verbatim text retention to embedded knowledge preservation. These pioneering frameworks have advanced the field by establishing standardized datasets, providing pre-trained target models, and introducing multifaceted evaluation metrics. However, their audit suites remain constrained in scope for instance, MUSE employs only 100 test questions to evaluate 0.8M corpora. From an auditing perspective, such limited test coverage may inadequately assess the targeted knowledge removal, potentially compromising the comprehensive evaluation of unlearning effectiveness.\nOur investigation reveals two fundamental challenges in holistic audit dataset synthesis. The primary concern about audit adequacy stems from simply relying on GPT-4 for automated QA generation from forget corpora. While this approach can generate multiple question-answer pairs for each target text, it introduces significant uncertainty in whether the generated questions comprehensively cover all the critical information contained within the source text. The second challenge involves knowledge redundancy between forget and retain corpora. As illustrated in Figure 2, shared knowledge should be preserved during the unlearning process. However, current evaluation methods fail to account for test cases where the information targeted also appears in the retain dataset, as demonstrated in Figure 1.\nIn this paper, we propose HANKER, a novel automated framework for holistic audit dataset generation that leverages knowledge graphs (KGs) to address the aforementioned limitations. Benefiting from advances in named entity recognition and information extraction, various tools now enable efficient conversion of unstructured text into structured entity-relation graphs. HANKER first converts both forget and retain corpora into structural knowledge graphs. By treating each KG edge (i.e., one fact) as a minimal unit, we can explicitly control the coverage of the audit process. Subsequently, by identifying and eliminating identical facts within the forget and retain KGs, we remove redundant knowledge from the forget KG, ensuring a well-defined audit scope. Finally, HANKER utilizes specific facts to guide LLMs in generating high-quality, targeted test questions, guaranteeing comprehensive and accurate auditing. Through this pipeline, HANKER automatically generates large-scale, comprehensive audit datasets for any given forget and retain corpora, thereby providing robust support for LLM unlearning evaluation.\nIn summary, our contributions are as follows:\n\u2022 We introduce HANKER, a novel and automated framework for generating holistic audit datasets for LLM knowledge unlearning, which addresses the challenge of audit adequacy and knowledge redundancy.\n\u2022 We apply HANKER to popular benchmark MUSE, significantly expanding the dataset scale and identifying knowledge memorization cases in unlearned LLMs that exceeded previous findings by three orders of magnitude (103x).\n\u2022 Our experimental results reveal that knowledge redundancy has a substantial impact on the assessment of unlearning effectiveness."}, {"title": "2 Preliminaries and Motivation", "content": "2.1 LLM Unlearning\nLLM unlearning refers to techniques that selectively remove specific behaviors or knowledge from a pre-trained language model while maintaining its overall functionality (Yao et al., 2023). With the proliferation of LLMs, unlearning has gained significant attention due to its broad applications in safety alignment, privacy protection, and copyright compliance (Eldan and Russinovich, 2023; Liu et al., 2024c; Jia et al., 2024b). The evaluation and auditing of LLM unlearning spans from basic verbatim memorization to deeper knowledge memorization (Shi et al., 2024), with this work focusing on the latter. As depicted in Figure 2, LLM unlearning operates as a targeted intervention within the model's knowledge representation framework. Its core objective is the selective removal of specific information while preserving the model's broader knowledge base (e.g, on retain set). This study focuses on the knowledge unlearning auditing that assesses unlearned models' behaviors through comprehensive audit cases. Given access to both forget and retain corpora, we generate a holistic set of test questions with reference answers to thoroughly evaluate whether an unlearned model exhibits any residual knowledge memorization.\n2.2 Knowledge Graph\nA knowledge graph (KG) is a structured multi-relational graph (Bordes et al., 2013), usually representing a collection of facts as a network of entities and the relationships between entities. Formally, a KG $G = (E,R, F)$ could be considered a directed edge-labeled graph (Ji et al., 2021), which comprises a set $E$ of entities (e.g., Harry Potter, Hogwarts School), a set R of relations (e.g., attends), and a set F of facts. A fact is a triple containing the head entity $e_1 \\in E$, the relation $r \\in R$, and the tail entity $e_2 \\in E$ to show that there exists the relation from the tail entity to the head entity, denoted as $(e_1, r, e_2) \\in F$ (Hogan et al., 2021). To illustrate, the fact (Harry Potter, attends, Hogwarts School) shows that there exists the attends relation between Harry Potter and Hogwarts School, which indicates \"Harry Potter attends Hogwarts School\".\n2.3 Motivation\nThis section aims to illustrate why and how we consider employing KG to facilitate the holistic LLM unlearning audit. Two critical factors underpin this task. Audit Adequacy: The Forget Dataset is an extensive, unstructured corpus. Existing approaches typically rely on the LLM's prior knowledge to directly generate QA pairs or segment the corpus and feed these segments to Chat-GPT for automated QA pair generation. Such methods often fail to intuitively reflect or guarantee the sufficiency of the generate dataset. Knowledge Redundancy: A more subtle and easily overlooked issue is that the Retain Dataset and Forget Dataset may contain overlapping knowledge. As illustrated in Figure 2, this overlapping knowledge should be retained by the unlearned model and, therefore not be treated as candidates for the unlearning efficacy audit. Existing evaluation benchmarks like MUSE often neglect this aspect, as evidenced by Figure 1.\nA KG can offer an effective solution to address these two challenges. First, the KG inherently captures the knowledge facts within the Forget Dataset at a fine-grained level, with each edge representing a minimal testable unit. By ensuring coverage of every edge in the KG, one can achieve a more intuitive and relatively comprehensive audit. Moreover, the structured data provided by the KG can facilitate the identification of identical knowledge facts present in both the Retain and Forget Datasets. This capability allows for refinement of the initial forget knowledge graph by removing potentially retained information. Finally, owing to recent advances in KG extraction technology, numerous automated extraction models and pipelines are available to support the automated construction of an audit dataset."}, {"title": "3 Proposed Method", "content": "The core idea behind HANKER is to leverage knowledge graphs to achieve fine-grained and comprehensive test coverage, while rigorously eliminating redundancy between the forgetting and retain objectives. As illustrated in Figure 3, HANKER comprises three sequential stages. During the Knowledge Graph Construction stage, unstructured textual data is systematically transformed into structured knowledge representations. This enables the explicit modeling of atomic knowledge units and their semantic interconnections. Subsequently, the Redundancy Removal stage meticulously identifies and eliminates knowledge facts that are simultaneously present in both forget and retain datasets. This process helps prevent inaccurate assessments by ensuring the audit doesn't mistakenly flag knowledge meant for retain as candidates for removal. Finally, in the Question Synthesis stage, HANKER employs LLMs to generate targeted questions and corresponding reference answers, guided by specific knowledge facts from the pruned knowledge graph. This approach provides an automated and holistic evaluation framework for assessing LLM knowledge unlearning efficacy."}, {"title": "3.1 Stage 1: Knowledge Graph Construction", "content": "Our framework transforms unstructured text corpora into structured knowledge graphs to enable fine-grained knowledge evaluation. This transformation is crucial for capturing semantic relationships and facilitating precise knowledge auditing. Specifically, we construct two distinct knowledge graphs from the forget and retain datasets: $G_{fgt}$ and $G_{ret}$, respectively. Each knowledge graph represents a structured network of entities and their relationships, allowing for systematic analysis of knowledge units. For implementation, following standard practices, we first segment the input text and perform coreference resolution preprocessing (Lee et al., 2017), to ensure accurate entity identification and relationship mapping. We then employ the REBEL-large model (Huguet Cabot and Navigli, 2021), which has been specifically fine-tuned for entity and relation extraction. This model demonstrates robust performance in extracting structured knowledge from natural language text, making it particularly suitable for our knowledge graph construction pipeline."}, {"title": "3.2 Stage 2: Redundancy Removal", "content": "The intricate entanglement of information across retain and forget datasets complicates the identification of specific elements requiring audit. To address this challenge, we implement a graph alignment strategy to detect shared information between $G_{fgt}$ and $G_{ret}$. We identify redundancy through triples that match exactly or share equivalent structures across both graphs. Our method examines each triple $(e_1, r, e_2) \\in G_{fgt}$ to locate its potential counterpart in $G_{ret}$. We express the overlapping edges mathematically as:\n$E_{conf} = E(G_{fgt}) \\cap E(G_{ret}).$\nThe refined test graph is then constructed by removing these intersecting elements:\n$G_{test} = G_{fgt} \\backslash E_{conf}.$\nThis process yields $G_{test}$, which maintains the fundamental structure of $G_{fgt}$ but excludes direct knowledge overlap with $G_{ret}$. The resulting graph provides a clean foundation for assessing selective forgetting performance, preserving crucial network relationships while eliminating redundant elements. It is important to note that this step provides an approximation rather than a perfectly precise identification of redundant knowledge. Even if two facts appear to be identical, their meanings may vary depending on the surrounding context, making exact equivalence challenging to determine. Nevertheless, the distant supervision strategy employed here has been shown to effectively capture the majority of overlapping knowledge (Mintz et al., 2009)."}, {"title": "3.3 Stage 3: Question Synthesis", "content": "Previous benchmarks generate QA pairs by directly feeding entire text segments to LLMs, making it difficult to ensure comprehensive coverage and quality control of the resulting questions. To address this limitation, we adopt a fine-grained, dual-input prompting strategy. Specifically, for each knowledge triple in $G_{test}$, we leverage an LLM to automatically generate targeted test questions. Our dual-input prompting strategy equips LLMs with two complementary information sources: structured knowledge triples and their corresponding source text passages. This approach guides the model to generate fact-anchoring questions while maintaining fidelity to the original context. By anchoring question generation in both structured knowledge and source text, we ensure the generated questions accurately reflect the intended specific facts while preserving contextual relevance. By enumerating each edge in $G_{test}$ and instructing the LLM to generate corresponding QA questions, we can guarantee at least a lower bound on the audit adequacy.\nOur prompt design is based on several key principles. First, we explicitly define the LLM's role as an expert quiz question generator to set clear expectations. Second, by providing structured inputs consisting of both the knowledge triple and its original context, we ensure that the generated questions are firmly grounded in the relevant information. Third, we impose strict criteria on the generated questions: each must be answerable solely from the provided context, specific enough to yield a unique answer, and directly assess the semantic relationship between target entities. To facilitate automated evaluation, we require that each question-answer pair be output in a structured JSON format.\nFurthermore, we adopt the one-shot learning by incorporating carefully selected example question-answer pairs into the prompt. These examples illustrate the desired question format and level of specificity, guiding the LLM toward generating high-quality, targeted questions. This comprehensive prompting strategy ensures that the synthesized questions effectively evaluate selective forgetting while maintaining human interpretability. The specific prompt employed in our experiments is provided in \u00a7 A.1."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nBuilding upon MUSE, a comprehensive benchmark for LLM unlearning that provides extensive datasets and evaluation frameworks (Shi et al., 2024), we integrate HANKER to enhance its capabilities. For question generation, we leverage the state-of-the-art DeepSeek-V3 model (Liu et al., 2024a), which has demonstrated superior performance in recent evaluations. The MUSE framework incorporates two primary data domains\u2014NEWS and BOOKS\u2014and includes a specially adapted LLaMA2-7B model that has undergone thorough training on the complete dataset. This fine-tuned model serves as the input for various unlearning techniques.\nUnlearning Methods. In our evaluation, we investigate three representative unlearning methods, each employing distinct strategies to achieve knowledge removal while preserving model utility.We utilize the default implementations, configurations, and scripts provided in MUSE (Shi et al., 2024). Gradient Ascent(GA) operates by inverting the conventional training objective, maximizing the likelihood loss on forgotten data to discourage the generation of memorized content. Negative Preference Optimization (NPO) reframes the unlearning problem through the lens of preference optimization, treating forgotten knowledge as negative examples. Task Vectors (TV) implements unlearning through a novel weight arithmetic approach. The method first creates a reinforced model by training on forgotten content, then derives a task vector representing the direction of memorization. Unlearning is achieved by subtracting this vector from the original model weights, effectively steering the model away from the memorized information. GA and NPO can be further enhanced with two utility preservation strategies: Gradient Descent on Retain set (GDR) and KL Divergence Regularization (KLR).\nMetrics. We evaluate the effectiveness of unlearning through our generated audit suite by quantifying the number of knowledge memorization cases (KMCs) in the unlearned model. Unlike existing work that assess unlearning based on overall response similarity across the entire dataset, our method applies software testing principles to pinpoint specific failure-revealing test cases\u2014scenarios in which an LLM provider might be liable for disclosing sensitive information. The identification process employs two complementary criteria for judgment. The first criteria uses ROUGE Recall to measure surface-level similarity, requiring model outputs to exceed a strict threshold (Recall=1) compared to reference answers. The second metric leverages an entailment-based approach (Yuan et al., 2024), utilizing a pre-trained NLI model as described in (Sileo, 2024) to verify semantic equivalence between generated and reference answers without logical inconsistencies. A higher frequency of detected memorization cases indicates less successful unlearning, while simultaneously demonstrating the comprehensiveness of our testing methodology."}, {"title": "4.2 Details of Generated Audit Suite", "content": "We applied HANKER to two corpora provided by MUSE, namely the NEWS and BOOKS datasets. The details are summarized in Table 1. For the NEWS dataset, HANKER extracted a knowledge graph (KG) from the forget dataset comprising 24,763 facts. After removing redundant knowledge, a final KG containing 16912 facts was obtained, from which 69,609 QA pairs were generated (On average, one fact corresponds to the generation of 4.11 QA pairs). Similarly, for the BOOKS dataset, HANKER extracted a KG with 41,123 facts from the forget dataset. Following the elimination of redundant knowledge, a final KG comprising 27,254 facts was produced, and 111,855 QA pairs were generated from this KG (on average, one fact corresponds to the generation of 4.10 QA pairs). These results demonstrate the capability of HANKER to automatically extract fine-grained knowledge graphs and generate large-scale audit suites.\nMannual Assessment of the Generated Data. To rigorously assess the quality of HANKER's generated audit dataset, we conducted a detailed manual evaluation on randomly sampled 100 text chunks from each of the NEWS and BOOKS datasets. Our assessment focused on both the accuracy of extracted knowledge triples and the quality of generated QA pairs through four key metrics. Accuracy of Knowledge Fact (AK) measures the precision of knowledge triple extraction from the source text, achieving scores of 0.76 and 0.61 for NEWS and BOOKS respectively. The relatively lower score on BOOKS reflects the inherent challenges in extracting structured knowledge from narrative text compared to more factual NEWS articles. Question-Fact Relevance (QR) evaluates how well generated questions align with both the context and extracted facts. High scores of 0.91 (NEWS) and 0.84 (BOOKS) indicate that our framework effectively translates extracted knowledge into contextually appropriate questions. Question Clarity (QC) assesses the linguistic quality and specificity of generated questions. Near-perfect scores of 0.99 across both domains demonstrate our system's exceptional ability to generate clear, unambiguous, and well-formed questions regardless of source material complexity. Answer-Context Consistency (AC) gauges whether generated reference answers accurately reflect the source context. Strong performance of 0.91 (NEWS) and 0.84 (BOOKS) suggests reliable answer generation that maintains fidelity to the original text. These results demonstrate HANKER's capability in generating high-quality audit datasets, particularly excelling in question generation."}, {"title": "4.3 Evaluation on Unlearning Methods", "content": "Our result reveals a striking disparity in the ability to detect knowledge memorization cases between HANKER's comprehensive audit suite and MUSE's baseline approach. The results paint a concerning picture about the extent of retained knowledge in supposedly unlearned models that were previously undetectable with limited audit sets. On the NEWS dataset, HANKER's detection capability proves remarkably more sensitive: using the ROUGE metric, it identifies over 4,600 memorization cases in the unmodified model, compared to just 33 cases detected by MUSE a 142-fold increase in detection power. This gap widens even further when examining semantic understanding through the Entailment metric, where HANKER detects more than 23,600 cases versus MUSE's 19 cases, representing a dramatic 1,242-fold improvement in identifying retained knowledge. The BOOKS dataset tells an equally compelling story. HANKER's comprehensive evaluation uncovers more than 4,700 memorization cases using ROUGE (compared to MUSE's 25 cases), and a remarkable 38,388 cases using Entailment (versus MUSE's 15 cases). These findings represent average improvements of 188\u00d7 and 1,125\u00d7 respectively in detection capability.\nParticularly noteworthy is how these results persist across different unlearning methods. Even with state-of-the-art approaches like GAKLR and NPOKLR, HANKER consistently reveals significantly more cases where knowledge removal was incomplete. This suggests that current unlearning methods may be less effective than previously thought, with their apparent success potentially being an artifact of insufficient testing rather than genuine knowledge removal.\nThese findings underscore the critical importance of comprehensive testing in evaluating unlearning effectiveness, revealing that the challenge of selective knowledge removal may be substantially more complex than indicated by previous benchmarks."}, {"title": "4.4 Impact of Knowledge Redundancy on Unlearning Effectiveness Audits", "content": "To validate the necessity of knowledge redundancy detection and elimination, we conducted a comprehensive experiment to assess its impact on unlearning evaluation effectiveness. Using the NEWS dataset as our testbed, we compared evaluation outcomes between two scenarios: one using the full dataset (126,224 test cases) and another using our deduplicated dataset (69,609 test cases). Our analysis considered both the number of identified knowledge memorization cases and standard dataset-level metrics (ROUGE and Entailment scores) used in existing evaluations. The results reveal a striking impact of knowledge redundancy on evaluation outcomes. When using our deduplicated audit set, the number of identified knowledge memorization cases decreased substantially: detection rates dropped by 71.3-73.3% under the ROUGE criterion and by 58.3-59.2% under the Entailment criterion. This significant reduction suggests that knowledge redundancy leads to substantial false positives, where retained knowledge is incorrectly flagged as forgetting failures. Furthermore, our analysis of quantitative metrics demonstrates that knowledge redundancy artificially inflates unlearning effectiveness measures. Without deduplication, ROUGE scores showed artificial inflation ranging from 19.7% to 26.1%, while Entailment scores were inflated by 32.4% to 35.2%. These inflated metrics indicate that traditional evaluation approaches may significantly overestimate unlearning effectiveness when redundant knowledge is not properly controlled for.\nThese findings provide compelling evidence for both the effectiveness of our approach and the critical importance of knowledge redundancy elimination in unlearning evaluation. The substantial reductions in false positives and metric inflation demonstrate that rigorous knowledge deduplication is essential for an accurate assessment of unlearning effectiveness."}, {"title": "5 Related Work", "content": "Machine Unlearning for LLMs. Machine unlearning, a technique first established for classification challenges (Bourtoule et al., 2021), has progressively evolved toward applications in large language models. Contemporary research predominantly explores parameter optimization methodologies, achieved through targeted fine-tuning procedures (Yao et al., 2023; Jang et al., 2022; Wang et al., 2024c; Yao et al., 2024; Tian et al., 2024; Liu et al., 2024d; Gu et al., 2024; Jia et al., 2024a) The transparent nature of modifying neural architectures engenders enhanced user trust, despite potential compromises to overall model performance. Beyond parameter-based approaches, researchers have pioneered diverse methodologies including advanced contrastive decoding frameworks (Eldan and Russinovich, 2023; Wang et al., 2024a; Ji et al., 2024; Huang et al., 2024), task-specific vector implementations (Liu et al., 2024e; Dou et al., 2025), contextual learning strategies (Pawelczyk et al., 2024; Muresanu et al., 2024), and sophisticated input processing mechanisms (Gao et al., 2024; Liu et al., 2024b).\nEvaluation of LLM Unlearning. The evaluation unlearning effectiveness of LLM encompasses diverse task scenarios. Early research focused on traditional NLP classification tasks to examine models' prediction (Chen and Yang, 2023). Subsequently, researchers developed specialized datasets to provide standardized evaluation platforms (Eldan and Russinovich, 2023; Shi et al., 2024; Maini et al., 2024). Besides some work has been devoted to focusing on the robustness of unlearning, i.e., adding perturbations or rewrites to the same problem to activate model memory (Joshi et al., 2024).\nKnowledge Graphs for Evaluation. Knowledge graphs offer distinct advantages beyond the completeness and identifiability properties utilized in this study. They serve as effective tools for evaluating both QA systems (Wang et al., 2024b) and LLM unlearning (Wu et al., 2024). Notably, knowledge graphs enable the assessment of model reasoning capabilities through transitive relationships (if a\u2192b and b\u2192c, then testing whether the model infers a\u2192c). The framework we propose in this paper conveniently integrates with these techniques."}, {"title": "6 Conclusion", "content": "In this paper, we introduce HANKER, an automated framework for generating holistic audit datasets to evaluate the effectiveness of LLM unlearning. By leveraging knowledge graphs, HANKER addresses two critical challenges in unlearning evaluation: ensuring audit adequacy and eliminating knowledge redundancy between the forget and retain datasets. Our empirical analysis on the popular MUSE benchmark demonstrates that HANKER can significantly expand the scale of audit datasets, identifying thousands of knowledge memorization cases that previous benchmarks failed to detect, and revealing how knowledge redundancy significantly skews unlearning effectiveness metrics."}, {"title": "Limitations and Ethical Considerations", "content": "Limitations. The primary limitation of our work is that it extends only the dataset provided by MUSE and employs DeepSeek-v3 for question generation. To mitigate this generalization risk, we have released our code and the generated audit suite, allowing researchers to utilize our framework to create additional audit datasets and evaluate their quality. Meanwhile, this is also our future work to extend our framework to other benchmarks.\nEthical Considerations. Machine unlearning can be employed to mitigate risks associated with LLMs in terms of privacy, security, bias, and copyright. Our work is dedicated to providing a comprehensive evaluation framework to help researchers better understand the unlearning effectiveness of LLMs, which we believe will have a positive impact on society."}, {"title": "A Appendix", "content": "A.1 Dataset Details\nBelow, we present the specific prompts used with DeepSeek-V3 for generating audit questions."}]}