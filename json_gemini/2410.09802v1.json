{"title": "EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion Models", "authors": ["Eungbean Lee", "Somi Jeong", "Kwanghoon Sohn"], "abstract": "Exemplar-guided image translation, synthesizing photo-realistic images that conform to both structural control and style exemplars, is attracting attention due to its ability to enhance user control over style manipulation. Previous methodologies have predominantly depended on establishing dense correspondences across cross-domain inputs. Despite these efforts, they incur quadratic memory and computational costs for establishing dense correspondence, resulting in limited versatility and performance degradation. In this paper, we propose a novel approach termed Exemplar-guided Image Translation with Brownian-Bridge Diffusion Models (EBDM). Our method formulates the task as a stochastic Brownian bridge process, a diffusion process with a fixed initial point as structure control and translates into the corresponding photo-realistic image while being conditioned solely on the given exemplar image. To efficiently guide the diffusion process toward the style of exemplar, we delineate three pivotal components: the Global Encoder, the Exemplar Network, and the Exemplar Attention Module to incorporate global and detailed texture information from exemplar images. Leveraging Bridge diffusion, the network can translate images from structure control while exclusively conditioned on the exemplar style, leading to more robust training and inference processes. We illustrate the superiority of our method over competing approaches through comprehensive benchmark evaluations and visual results.", "sections": [{"title": "1 Introduction", "content": "The rising interest in applications of image synthesis has led to a notable surge in demand for image generation capabilities that extend beyond text prompts, emphasizing control through exemplar images or structured inputs. Exemplar- guided image translation task aims to generate photo-realistic images condi- tioned on both a style exemplar image and specific structural controls, such as segmentation masks, edge maps, or pose keypoints."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Controllable Diffusion Models", "content": "Diffusion models [2,8,34,40,41] aim to synthesize images from random Gaussian noise via an iterative denoising process. For customized image generation, recent methods have explored text-guided image generation (T2I) [4,29,33,34,37] and demonstrated extraordinary generative capabilities in modeling the intricacies of complex images. GLIDE [29] aggregated the CLIP texture representations utilizing classifier-free guidance [9]. DALLE-2 [33] proposed a cascade model using the CLIP latent. VQ-Diffusion [4] proposed to learn the diffusion process on the discrete latent space of VQ-VAE [46]."}, {"title": "2.2 Exemplar-guided Image Translation", "content": "The exemplar-guided image translation task involves generating an image based on an input exemplar and structure controls such as an edge, pose, or mask. A major challenge lies in effectively guiding the context within exemplars relative to the input controls. The SPADE [31] framework proposed spatially-adaptive normalization to generate an image from the semantic mask followed by class- adaptive [43] and instance-adaptive [42]. While these approaches have shown promising in global-style translation, they overlooked local details compromising generation quality.\nTo address the local details, significant efforts have been focused on build- ing dense correspondence. Zhang et al. [58] proposed building dense correspon- dence between input semantic and exemplar image. Although their method has shown promising results, their method is limited by many-to-one matching issues and the quadratic computational and memory complexities of dense matching operations, restricting it to capturing only coarse-scale warped features. To al- leviate these issues, recent works introduced effective correspondence learning such as GRU-assisted Patch-Match [61], unbalanced optimal transport [54], bi- level feature alignment strategy [55], multi-scale dynamic sparse attention [20], Cross-domain Feature Fusion Transformer [25] and Masked Adaptive Trans- former [14]. Although they have demonstrated promising results, their matching- based framework still suffers from inherent problems such as sparse matching.\nMeanwhile, recent progress [11, 39, 50, 51] has leveraged diffusion models to bridge the gap between style exemplars and structural controls. Seo et al. [39]"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Diffusion Models", "content": "The general idea of Denoising Diffusion Probabilistic Model (DDPM) [8] is to generate images from Gaussian noise via $T$ steps of an iterative denoising pro- cess. It consists of two processes: the forward process and the reverse process. Given the original data $x_0 \\sim q_{data} (x_o)$, the forward diffusion process maps $x_0$ into noisy latent variables ${x_t}_{t=0}^T$ can be obtained as: $x_t = \\sqrt{\\alpha_t} x_0 + \\sqrt{1 - \\alpha_t} \\epsilon$ where $\\epsilon$ is the Gaussian noise and ${\\alpha_t}_{t=0}^T$ is pre-defined schedule. On the other hand, the corresponding reverse process aims to predict the original data $x_0$ starting from the pure Gaussian noise $x_T \\sim \\mathcal{N}(0, I)$ through iterative denoising processes with pre-defined time steps. It is formulated as another Markov chain as $p_\\theta (x_{t-1} | x_t) := \\mathcal{N} (x_{t\u22121}; \\mu_\\theta (x_t, t), I)$ with learned mean and fixed vari- ance. The denoising network $\\epsilon_\\theta$ is trained to predict the noise by minimizing a weighted mean squared error loss, defined as:\n$L(\\theta) = \\mathbb{E}_{t,x_0,\\epsilon}[||\\epsilon - \\epsilon_\\theta (x_t, t)||_2]$.\t(1)\nSimilarly, the conditional diffusion models [36,38] directly inject the condition $y$ into the training objective (eq. 1), such as $L(\\theta) = \\mathbb{E}_{t,x_0,\\epsilon} ||\\epsilon - \\epsilon_\\theta (x_t, y, t)||_2$."}, {"title": "3.2 Brownian Bridge Diffusion Models", "content": "A Brownian Bridge Diffusion Model (BBDM) [19] is an image-to-image transla- tion framework based on a stochastic Brownian Bridge process. Unlike DDPM that conclude at Gaussian noise $x_T \\sim \\mathcal{N}(0,I)$, BBDM assumes that both end- points of the diffusion process as fixed data points from an arbitrary joint distri- bution, i.e. $(x_T, X_0) \\sim q_{data}(X, Y)$. The BBDM directly learns image-to-image translation $q(x_\\frac{T}{X})$ with boundary distribution $q_{data}(x_0,x_T)$ independent of any conditional process, that enhances the fidelity and diversity of the generated samples. The forward process of the Brownian Bridge forms a bridge between two fixed endpoints at $t = 0$ and $T$:\n$q(x_t|x_0, y) = \\mathcal{N} (x_t; (1 \u2013 m_t) x_0 + m_ty, d_tI)$, where $y \\triangleq x_T$\t(2)"}, {"title": "4 Methodology", "content": "In this section, we delineate our framework based upon discrete-time stochastic Brownian Bridge diffusion process [19] for Exemplar-guided image translation (Fig. 2). Given a control $I_x$ sampled from domain $X$ alongside an exemplar image $I_y$ from domain $Y$, the primary objective is to generate a target image $I_{xy}$ that retains the structure of $I_x$ embodying the style of $I_y$. The key to our method is the infusion of style information from $I_y$ to guide the diffusion trajectory of the target image. To facilitate this, our method integrates three components: a denoising network equipped with an Exemplar Attention Mod- ule, a Global Encoder, and Exemplar Network. The Global Encoder extracts global style information of $I_y$ and the Exemplar Network captures the appear- ance features of $I_y$. The Exemplar Attention Module selectively incorporates appearance information into the denoising process."}, {"title": "4.1 Exemplar-guided Brownian Bridge Diffusion Models", "content": "Denoising Network. Employing the Brownian Bridge diffusion process, our denoising U-Net directly learns the translation from the input controls $I_x$ to images $I_{x\\rightarrow y}$ that preserves the structures of controls. For efficient training and inference, we employ the Stable Diffusion [34] framework. Specifically, given an image $I$, the encoder $\\varepsilon$ maps it into a latent space $z = \\mathcal{E} (I)$, subsequently recon- structed by the decoder $\\hat{I} = \\mathcal{D} (z)$. The denoising U-Net $\\epsilon_\\theta$ learns to establish the bridge from fixed initial point $x_\\tau = z_x$ to the target, $x_0 = z_{xy}$.\nUnlike existing noise-to-image diffusion frameworks [28,57] embed the struc- tural information through intricate frameworks, our approach translates from structural control to images without explicit conditional operation. Consequently, our framework is able to solely focus on exemplar information that fosters en- hanced stable training and inference performance.\nGlobal Encoder. The Global Encoder, utilizing DINOv2 [30], captures the global style information from the exemplar image $I_y$. Specifically, exemplar image $I_y$ is processed through the Global Encoder, subsequently, [CLS] token is extracted and passed through a linear layer to encapsulate global style attributes:\n$T_\\theta(I_y) = Linear (DINO(I_y) [CLS]) \\in \\mathbb{R}^C$,\t(5)\nwhere $c$ denotes the dimension of [CLS] token. The global features are utilized as global style information through a cross-attention mechanism, ensuring that the synthesized output accurately reflects the exemplar's global style.\nIn the context of text-to-image synthesis [8,34], prior works have exten- sively leveraged the CLIP image encoder to convey high-level semantic prompts via cross-attention. This approach, however, primarily focuses on the seman- tic alignment of prompts and images, thereby overlooking the representation of detailed textures. Furthermore, our method does not need textual prompt align- ment. Motivated by recent studies [17,45] that have demonstrated the superior proficiency of DINO [1,30] over CLIP [32] in encapsulating a broader capability of semantic features in images, attributed to its self-supervised learning strategy, our method incorporates the use of a pre-trained DINOv2 encoder to enhance the semantic fidelity of generated images.\nExemplar Network. Notwithstanding the capability of Global Encoder in capturing overarching style information, it is limited to the retention of fine- grained details because it encodes exemplar in low resolution (224$^2$). In contrast, the exemplar-guided image translation tasks require higher fidelity to detail. To this end, we introduce Exemplar Network, referred to $\\psi_\\theta$, of which the objective is to capture the detailed texture information from the exemplar image, thereby compensating for the global information.\nThe Exemplar Network adopts a siamese configuration akin to a denoising U- Net, streamlined by omitting the redundant layers for enhanced efficiency during"}, {"title": "Exemplar Attention module", "content": "training and inference. It encodes the exemplar $z_y$ into a feature maps ${F_l}^N_{l=0}$ across $N$ blocks. Additionally, it processes the global information through cross- attention mechanisms in each block. The exemplar features ${F_l}^N_{l=0}$ are then integrated into the noise prediction branch via Exemplar Attention Module.\nExemplar Attention module. The straightforward approaches to integrate additional features into the denoising network are concatenation [57,60] or ad- dition [28] However, in contrast to existing works in that control features are spatially aligned with the target image, this approach is not suitable for our task because the exemplar image and target control are not spatially aligned. Therefore, we propose an Exemplar Attention Module to integrate the exem- plar features from the Exemplar Network, $F_1 \\in \\mathbb{R}^{C \\times H \\times W}$, into noise prediction features, $F_E \\in \\mathbb{R}^{C \\times H \\times W}$ for each $l$ block. First, these features are concate- nated into a spatial-wise: $F_{in} = concat(F_1, F_2) \\in \\mathbb{R}^{C \\times H \\times 2W}$. Following this, self-attention is applied to compute the spatial attention across the features:\n$\\begin{aligned} Q = \\phi_q(F_{in}), K = \\phi_k(F_{in}), V = \\phi_v(F_{in}) \\\nF_{att} = \\frac{QK^T}{\\sqrt{V}} \\\nF_{EA} = W'\\text{Softmax}(F_{att})V + F_{in},\t(6)\\end{aligned}$ \nwhere $Q, K$ and $V$ represents query, key and value, respectively, $\\phi (\\cdot)$ is layer- specific $1 \\times 1$ convolution operation, and $W'$ is trainable parameter. Subse- quently, exemplar-attended feature $F_{EA} \\in \\mathbb{R}^{C \\times H \\times 2W}$ is segmented, with por- tions corresponding to the denoising features are extracted and forwarded to- ward the output, $F_{out} = Chunk(F_{EA}, 2, dim=0) \\in \\mathbb{R}^{C \\times H \\times W}$. The Exemplar Attention Module computes the region of interest for each query position, a cru- cial step in effectively directing the denoising steps towards the target exemplar style. This approach enables the denoising process to selectively assimilate fea- tures from the Exemplar Network, enhancing the fidelity of the output to the desired stylistic attributes."}, {"title": "Training Objectives", "content": "The training process is performed by optimizing the Evidence Lower Bound (ELBO), following BBDM [19], where the marginal dis- tribution is conditioned on $\\tau$. Thus, the training objective ELBO in eq. 4 can be simplified as:\n$\\mathbb{E}_{x_0,y,I_y,\\epsilon} \\Big[ c_\\text{et} \\bigg|\\Big| m_t (x_T - x_0) + \\sqrt{\\delta_\\tau} \\epsilon - \\epsilon_\\theta (x_t, t, T_\\theta(I_y), \\psi_\\theta(z_y, T_\\theta(I_y)) \\Big|\\Big|_2^2 \\bigg]$\t(7)\nwhere $c_{et}$ is the loss weighting function that develops into 1/t, and $\\delta_t$ denotes the preserved variance schedule, $\\delta_t = 2(m_t \u2013 m_t^2)$."}, {"title": "4.2 Training Strategy", "content": "The training process is unfolded in two stages. In the first stage, denoising U-Net, which utilizes the Global Encoder and cross-attention mechanism, is trained to integrate the global style cues from the exemplar image. Throughout this phase, the Exemplar Network is not engaged, and pre-trained parameters of VAE and"}, {"title": "4.3 Sampling Strategy", "content": "The inference process is similar to BBDM [19] that employs the deterministic ODE sampler [40]. Given a inference timesteps ${t'_s}_{s=1}^S \\sim [1 : T]$, the sampling process is formulated as:\n$x_{t'_{s-1}} = c_{xt} x_{t'_s} + c_{yt} x_T - c_{et}\\epsilon_\\theta (x_t, T_\\theta (I_y), \\psi_\\theta(z_y, T_\\theta (z_y)), t'_s) + \\sqrt{\\delta_t} \\epsilon$ (8)"}, {"title": "5 Experiments", "content": "In this section, we present the experimental results of the proposed method. We conduct three tasks to evaluate our model: Edge-to-photo, mask-to-photo, and pose-to-photo. We perform extensive ablation studies to analyze the effect of each essential component of the proposed method. Also, we provide qualitative and quantitative comparisons with state-of-the-art methods. Implementation details and detailed architecture are described in supplementary material.\nDatasets. We conduct three tasks to evaluate our model: Edge-to-photo, mask- to-photo, and pose-to-photo. For mask-guided and edge-guided image generation tasks, the CelebA-HQ [23] dataset is used and we construct the edge maps using the Canny edge detector following [58,61]. For the pose-guided image generation task, we use deepfashion [22] dataset that consists of 52,712 images with a keypoints annotation. For all tasks, the split of train and validation pairs is consistent with CoCosNet [58] policies."}, {"title": "5.1 Qualitative Evaluation", "content": "We present a comparison of qualitative results (Fig. 3) with existing meth- ods [20, 54, 58] at three tasks. The results demonstrate that our method effec- tively transfers the detailed texture from the exemplar to the target, concurrently preserving the structure of controls. Notably, in pose-to-photo, our approach exhibits superiority in capturing detailed patterns and minor objects, such as a cap, which other methods often overlook due to the limitation of matching frameworks. These advantages show the capability of our proposed method that fully leverages the diffusion framework that ensures a more holistic and precise depiction. On the other hand, in edge-to-photo and mask-to-photo tasks, while"}, {"title": "5.2 Quantitative Evaluation", "content": "Evaluation Metrics. We report the Fr\u00e9chet Inception Distance (FID) [7] and Sliced Wasserstein Distance (SWD) [18] metrics to evaluate the image perceptual quality by reflecting the distance of feature distributions between real images and generated samples. And we also measure LPIPS [59] to evaluate the diversity of translated images. On the other hand, we show the semantic, color, and texture consistency in Tab. 4, also under the same setting as [58].\nImage Quality. Tab. 3 presents a quantitative evaluation against state-of- the-art matching-based methods [20, 39, 54, 58, 61], showing that our method is competitive both on image quality and diversity across various tasks. Addition- ally, in the mask-to-photo task, our method demonstrates superior performance, whereas matching-based methods struggle due to their reliance on cross-domain matching\u2014a notably arduous endeavor when masks offer scant correspondence cues. Conversely, by leveraging diffusion models, our method iteratively trans- lates images from masks via noise prediction. This enables our approach to excel in scenarios with limited direct correspondences, showcasing its robustness and adaptability.\nConsistency. The semantic and style consistency analysis (Tab. 4) evidences that our method either leads or remains competitive in style relevance scores, encompassing color and texture dimensions. In the pose-to-photo domain, de- spite achieving scores comparable to other methods [20,58], a visual assessment"}, {"title": "5.3 Comparison to State-of-the-Arts Diffusion Methods", "content": "We compare our framework against prevalent state-of-the-art (SOTA) diffusion- based techniques, as shown in Fig. 5 and Tab. 2. Based on the Stable Diffusion framework [34], we incorporate the ControlNet [57] and IP-Adapter [52] to fa- cilitate structured and stylistic control, respectively. While the existing SOTA method adeptly captures the control structure and generates photo-realistic im- ages, our method more accurately reflects the style of the exemplar. Notably, diffusion-based approaches, conditioned on multiple information including those derived from ControlNet, textual prompts, and image embeddings, tend to be overly sensitive to hyperparameters such as control and embedding guidance scales. Conversely, our model, predicated on a Brownian Bridge diffusion pro- cess and exclusively conditioned on the exemplar, assures a more effective gen- eration process. Moreover, the capacity of the existing methods for transferring the finer details in exemplar is somewhat constrained by their reliance on CLIP embeddings which often overlook small details. In contrast, our framework, un- derpinned by the Exemplar Network and Exemplar Attention Module, demon- strates superior adeptness in transposing textures from the exemplar."}, {"title": "5.4 Ablation Study", "content": "To validate the efficacy of our proposed architecture, we conduct ablation studies focusing on the following configurations: (1) omitting the Global Encoder, (2) utilizing the baseline model [19] integrated with CLIP, (3) implementing DINOv2, and (4) employing our complete architecture on the edge-to-photo translation task. As illustrated in Fig. 4, our findings reveal that the DINOv2-based Global Encoder surpasses the CLIP in generating images with higher detail fidelity. While CLIP effectively captures the general characteristics of the reference im- age, ensuring a level of resemblance, it does not fully encapsulate the intricacies of the details. Additionally, with our Exemplar Network, inputs with spatial mis- aligned control and exemplar often result in the generation of \"blurry\" images when relying exclusively on features of Global Encoder. In contrast, our com- plete framework demonstrates superior performance across all assessed dimen- sions, highlighting its architectural advantage. Quantitative assessments further underscore the importance of our design choices, as detailed in Tab. 1."}, {"title": "6 Conclusion", "content": "In this study, we presented EBDM, a novel stochastic Brownian bridge diffusion- based approach for exemplar-guided image translation. Our method is structured around three important components: denoising U-Net equipped with Exemplar Attention Module, Global Encoder, and Exemplar Network. By leveraging the Brownian Bridge framework, which translates from fixed data points as struc- tural control to photo-realistic images, our method is exclusively conditioned to the style information, thereby the framework more robust and stable.\nAdditionally, we propose the Exemplar Network and Exemplar Attention Module to selectively incorporate the style information from exemplar images into the denoising process. Our method not only stands competitive or surpasses existing methods across the three distinct tasks. Furthermore, our methods also achieve a significant improvement in visual results not only in photorealism but also in the precise transfer of fine details such as patterns and accessories present in the exemplar images."}]}