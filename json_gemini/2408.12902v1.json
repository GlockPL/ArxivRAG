{"title": "IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities", "authors": ["Bin Wang", "Chunyu Xie", "Dawei Leng", "Yuhui Yin"], "abstract": "In the field of multimodal large language models (MLLMs), common methods typically involve unfreezing the language model during training to foster profound visual understanding. However, the fine-tuning of such models with vision-language data often leads to a diminution of their natural language processing (NLP) capabilities. To avoid this performance degradation, a straightforward solution is to freeze the language model while developing multimodal competencies. Unfortunately, previous works have not attained satisfactory outcomes. Building on the strategy of freezing the language model, we conduct thorough structural exploration and introduce the Inner-Adaptor Architecture (IAA). Specifically, the architecture incorporates multiple multimodal adaptors at varying depths within the large language model to facilitate direct interaction with the inherently text-oriented transformer layers, thereby enabling the frozen language model to acquire multimodal capabilities. Unlike previous approaches of freezing language models that require large-scale aligned data, our proposed architecture is able to achieve superior performance on small-scale datasets. We conduct extensive experiments to improve the general multimodal capabilities and visual grounding abilities of the MLLM. Our approach remarkably outperforms previous state-of-the-art methods across various vision-language benchmarks without sacrificing performance on NLP tasks.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have made substantial progress in recent years, largely attributed to the technique of pre-training and instruction tuning. Building upon this foundation, visual instruction tuning has been proposed to evolve LLMs into Multimodal Large Language Models (MLLMs), thereby endowing them with the capability to interpret and comprehend visual signals. MLLMs prove beneficial in numerous tasks, such as transcribing the text within an image, generating stories and poems based on an image, or converting screenshots of webpages into code. Historically, these tasks have been regarded as challenging for conventional vision-language models. MLLMs exhibit considerable promise in executing these complex, diverse real-world tasks, enabling more natural and human-like interactions. Typically, the operation of a MLLM begins with feeding an image into a visual encoder, such as CLIP or SigLIP, to extract a high-dimensional feature representation. This feature is subsequently transformed through a projection layer to align with the dimension of the large language model. The resulting features, often referred to as image tokens, are concatenated with text tokens and fed into the large language model. This process enables the MLLM to generate responses based on user instructions and input images.\nIn the current common MLLM, when image and text tokens are fed into the large language model, the LLM is typically unfrozen for further training. This strategy has led to significant advancements in the MLLM model. Consequently, it predictably leads to a degradation in the understanding ability of the large language model. To validate this hypothesis, we conduct experiments on the LLaVA-1.5 architecture using the 1.2M-size open-source dataset provided by, which contains a limited amount of plain text data."}, {"title": "Related Work", "content": "Large Language Models. The landscape of Natural Language Processing (NLP) has undergone a revolutionary transformation, driven by the advent and continuous refinement of Large Language Models (LLMs). A pivotal moment in this evolution is the first appearance of the transformer architecture, which serves as a key catalyst, giving rise to pioneering language models like BERT and OPT. These models showcase an unprecedented level of linguistic comprehension, significantly advancing the state-of-the-art in NLP. A critical breakthrough comes with introducing the Generative Pre-trained Transformer (GPT) series, which pioneer an auto-regressive language modeling approach, setting a new standard for language prediction and generation capabilities. Subsequent iterations, including Mixtral, GPT-4, and Llama3, have not only maintained but also amplified this momentum, displaying superior performance on intricate language processing challenges. Moreover, the fusion of LLMs with specialized visual tasks showcases the models' adaptability and broadens their scope, indicating their potential to transcend conventional text-based operations into multimodal interactions. This expansion highlights the transformative role LLMs can assume when incorporated into diverse domains, providing a rich ground for innovation and exploration.\nMultimodal Large Language Models. The advancement of Large Language Models (LLMs) has kindled a growing interest in extending their foundational competencies to incorporate the visual domain, thereby giving birth to multimodal Large Language Models (MLLMs). The works on MLLMS typically follow a tripartite architecture: a visual encoder, a vision-language connector, and a large language model. Notably, BLIP-2 and Flamingo introduce the Q-Former/Resampler as a bridge between vision and language, whereas LLaVA and MiniGPT4 refine this connection via a linear layer. Cambrian-1 proposes a dynamically adaptive connector that integrates high-resolution visual features with LLMs while reducing the number of tokens. To enhance their multimodal performance, contemporary MLLMs mainly fine-tune the LLM and connector using visual instruction tuning data. These models leverage meticulously curated instruction datasets, showcasing an effective strategy that highlights their robust capabilities. However, a common oversight lies in the maintenance of language abilities. Long term multimodal training often leads to degradation of language proficiency. CogVLM seeks to address this by integrating a trainable visual expert into the language model, but still trains the LLM during supervised fine-tuning, resulting in a degradation of language capability. DeekSeek-VL maintains a 70% proportion of language data to preserve the integrity of language knowledge within the model, but incurs a considerable training cost. Departing from these conventional training paradigms of MLLMs, we introduce the inner-adaptor architecture. This design is specifically tailored to preserve the NLP performance of the MLLM while facilitating a seamless augmentation of its multimodal capabilities."}, {"title": "Methodology", "content": "Overview. As illustrated in Figure 2, our approach enables the simultaneous execution of two high-quality workflows post-deployment: one for multimodal interactions and the other for text-only conversations. Both workflows leverage the transformer layers of the large language model. The multimodal interaction workflow encompasses: (1) an image encoder and a projector, utilized for extracting high-quality image features and achieving vision-language alignment, respectively, (2) the transformer layers of the large language model, which remain frozen during training, and (3) the inner-adaptor architecture, which comprises insertion layers, an embedding layer, and a language model head specifically designed for multimodal inputs. Conversely, the text-only conversation workflow solely employs the constituent elements of the original language model, without resorting to the specialized multimodal components.\nImage Encoder and Projector. Following LLava-1.5, we utilize the CLIP ViT-L/14 image encoder with an input resolution of 336px. Subsequently, we employ a vision-language projector composed of a two-layer MLP to integrate the vision features with LLMs.\nLarge Language Model. We employ the Llama3-8B as the base language model throughout the training process.\nInner-Adaptor Architecture. To achieve multimodal comprehension, it is essential to integrate trainable parameters into MLLMs. LLaVA makes the projector and the large language model trainable during visual instruction tuning, but leads to the performance degradation on NLP tasks. Flamingo employs cross-attention with a gating mechanism to introduce image information into the model, facilitating a deep fusion of original image features with text features prior to each layer of the language model. However, this approach requires a considerable volume of pre-training data to train effective cross-attention layers and gating values, which can be computationally costly. Furthermore, the final performance of the model falls short of expectations.\nDrawing insights from recent works , we recognize that the self-attention layer can assimilate image features as prior prompts, thus eliminating the necessity of cross-attention for the obligatory incorporation of image features. In alignment with this perspective, we embark on exploratory research. Referencing Figure 3(a), we are inspired by the prevalent ControlNet architecture. The operation of a specific layer can be succinctly expressed as follows:\n$X_{out} = \\Phi_l(X_{in}) + G(\\Phi_i(X_{in})),$ (1)\nwhere $\\Phi_l$ and $\\Phi_i$ denote the frozen language model (LM) layer and the insertion layer, respectively. Here, $X_{in}$ represents the multimodal input, $X_{out}$ denotes the multimodal output, and $G$ indicates a gating layer initialized at zero. The insertion layer is a transformer decoder layer, comprising the self-attention layer, layer normalization, feed forward network, etc. It is consistent with the parameter scale of a transformer layer in the large language model. For instance, if we target the 22th layer, the initial parameters of the corresponding insertion layer are derived from the 22th language model layer. Nonetheless, the ControlNet-based design did not yield satisfactory performance.\nReferring to Figure 3(b), we endeavor to refine the ControlNet structure. Specifically, we eliminate the feature propagation between insertion layers. Instead, the output of the LM layer serves as the input to the insertion layer. Our expectation is that each frozen LM layer will accommodate multimodal data through a distinct insertion layer and gating layer, with the insertion layer no longer being directly influenced by subsequent layers. Compared to the design in Figure 3(a), the refined architecture shows significant improvements."}, {"title": "Experiments", "content": "In this section, we first describe the training paradigm of our method with the data utilized in the diverse processes. Subsequently, we conduct evaluation on the general multimodal and visual grounding benchmarks to comprehensively assess our models' visual understanding ability. Finally, we detail the ablation experiments of our method.\nTraining Paradigm\nPre-training. During the training process of MLLM, the primary objective of the pre-training phase is to enable MLLM to learn the alignment between visual cues and textual descriptions. This stage, also known as the image-text alignment phase, establishes connections between the vision encoder and LLM. In our architectural design, the image encoder and LLM remain frozen throughout all training phases to preserve the inherent foundational knowledge in both vision and language models. The projector and inner-adapter architecture require training to enhance multimodal capabilities. Our empirical investigations reveal that for the inner-adaptor architecture, applying a high learning rate can lead to overflow in training loss. To alleviate this issue, we devise a dual-stage pre-training procedure.\nIn the first pre-training stage, the model configuration consists of only three components: the image encoder, the projector, and the large language model. The parameters of the image encoder and the large language model are frozen, while a high learning rate of 0.001 is utilized to train a high-quality projector.\nIn the second pre-training stage, the model architecture is expanded to incorporate the inner-adaptor for multimodal tasks. The training parameters now include both the projector and the newly integrated structures. The projector is initialized with the parameters derived from the preceding stage. For this stage, a lower learning rate of 2e-5 is adopted.\nThroughout the pre-training stages, the dataset employed consists of 558k image-text aligned pairs sourced from and an additional 100K pairs from. provides a total of 664K image-text aligned data. We translate the first 100k pairs into Chinese and incorporated them into the training process to fortify the model's understanding of Chinese tasks. Over the course of these stages, we utilize a cumulative total of 658K data pairs.\nInstruction Fine-tuning. We perform instruction fine-tuning based on the model obtained from the second pre-training stage. Throughout this stage, the parameters of the large language model and the image encoder remain frozen. The dataset includes the fine-tuning dataset of 665K samples proposed by, along with additional datasets including DocVQA, VSR, ScienceQA, and an in-house dataset (78.5K). Similar to the pre-training stage, we translate the first 40K entries of the 664K fine-tuning data proposed by into Chinese and incorporate them into the instruction fine-tuning dataset. The aggregate quantity of data utilized in this stage amounts to 865K.\nGrounding Fine-tuning. Building upon the model fine-tuned with instructions, we further train a model specialized in visual grounding. The data used in this stage comprises RefCOCO, COCO, Flickr30k Entities, Objects365, aggregating to approximately 2M data instances. These datasets improves the model's capability of localizing fine-grained visual details. The inclusion of COCO and Objects365 assists the model in improving its ability to localize multiple targets."}, {"title": "Experimental Results", "content": "Main Results on General Multimodal Benchmarks. To assess the multimodal capabilities of our approach, we employ widely recognized benchmarks that are closely related to multimodal tasks: MMEP, MMBench-ENT, MMBench-CNT, and MMMU. These benchmarks are renowned for presenting significant challenges across a diverse range of practical tasks. For evaluation purposes, we adhere to a zero-shot testing protocol, a strict methodology that tests models on unseen data without additional training. Moreover, we categorize comparative methods into two distinct categories: those trained with a frozen language model and those trained with an unfrozen language model. To provide a comprehensive analysis, we show the scale of the data utilized for each method, along with the variations in the image encoders employed. Detailed results of our evaluations are tabulated in Table 2. To ensure a fair and equitable comparison, we choose methods that leverage a base language model with a comparable parameter scale, and the reported metrics for competing methods are based solely on officially published data, avoiding any local testing results.\nOwing to the inherent strengths of our proposed architecture, our method exhibits substantial superiority over those trained with frozen language model. As the current mainstream approach, models trained with unfrozen language models typically achieve better multimodal performance, albeit at the cost of diminished NLP capabilities. We list several state-of-the-art methods adhering to this training paradigm. Compared to Honeybee, Yi-VL, and Deepseek-VL, our method achieves competitive or even superior performance on certain metrics, with an extremely small training data scale. Using the same data scale of 1.2 million, IAA-8 outperforms LLaVA-Llama3. Additionally, IAA-14 with 14 insertion layers achieves better results than IAA-8 with an 8-layer configuration. Furthermore, we compare our approach with LLaVA-Llama3 on NLP benchmarks, including MMLU and C-Eval. The results of NLP benchmarks are summarized in Table 3. Our language model is not impaired in terms of NLP ability, but LLaVA-Llama3 trained on the same data shows deteriorated results on both MMLU and C-Eval. Our method surpasses LLaVA-Llama3 across all metrics, indicating that our architecture is superior to the mainstream LLaVA architecture. The performance of various models on the plain text dialog task is illustrated in Figure 3. It is evident that the text-only workflow of the Inner-Adaptor Architecture (IAA) preserves the original conversational capabilities of the language model. In contrast, open-source multimodal large language models such as LLaVA-Llama3 and LLaVA-v1.5 are more impacted by multimodal data. When queried with the same question, LLaVA-Llama3 and LLaVA-v1.5 produce notably shorter responses. This is directly related to the fact that a large amount of the multimodal training data has shorter text lengths. Fine-tuning the large language model affects its ability to fully understand content and generate more comprehensive responses."}, {"title": "Efficiency in Deployment", "content": "Currently, high-performance multimodal models typically require the unfreezing of the large language model for training. CogVLM highlights the substantial difficulty in developing a model that excels in both multimodal comprehension and visual grounding tasks simultaneously. To address this, it adopts a dual-model strategy, specifically training one model for general multimodal capabilities and another for visual grounding abilities. In this context, deploying a high-quality language model, a multimodal model with outstanding general performance, and a model endowed with proficient visual grounding skills concurrently on a single GPU would demand an estimated 50GB of memory. Our proposed approach, facilitated by the inner-adaptor architecture, ingeniously combines superior general multimodal competencies and robust visual grounding capacities, while concurrently safeguarding the inherent prowess of the original large language model. Specifically, with an 8-layer inner-adaptor configuration, our model exhibits a significantly reduced memory footprint, hovering around 30GB."}, {"title": "Ablation Study", "content": "Structure Analysis. In the exploration of the structure, we furnish quantitative results for validation in Table 5. With an 8-layer insertion scheme as our baseline configuration, we observe that incremental architectural enhancements consistently improve performance metrics across the board. Specifically, the comparison between rows 1, 2, and 4 highlights the benefits of architectural refinement. Moreover, the contrast between rows 3 and 4 demonstrates that the integration of a specialized embedding layer and language model head for multimodal data processing significantly boosts performance.\nComparison of Training Stages. Through empirical evidence detailed in Table 6, we validate the effectiveness of our two-stage pre-training methodology. It can be observed that the model lacking the first stage of alignment training exhibits notably poorer performance. When the projector and insertion layers are engaged in joint pre-training, it is essential to maintain a learning rate of approximately 2e-5 to prevent loss overflow. However, this strategy leads to suboptimal alignment training for the projector, which negatively affects the model's final performance. Furthermore, although the model performs adequately when skipping the second pre-training stage, it ultimately fails to replicate the outstanding results achievable through the complete two-stage pre-training process. This disparity emphasizes the critical significance of the additional pre-training stage in enhancing the model's overall effectiveness.\n$X_{out} = \\Phi_l(f_l(X_{in})).$ (2)\nImpact of Insertion Layer Quantities. We explore the effect of varying numbers of insertion layers, which are presented in Table 7. The experimental results indicate that increasing the number of insertion layers from 8 to 14 yields enhancements in all performance metrics. However, it is imperative to acknowledge that an increase in insertion layers simultaneously impacts the model's efficiency. We advocate that an 8-layer configuration is adequate to effectively address foundational requirements.\nTraining Data Influence Assessment. To delineate the impact of data on model performance, we present comparative results in Table 8. The baseline, outlined in the first row, showcases the performance of LLaVA-Llama3 using the LLaVA architecture and the 1.2 million dataset provided by. Subsequent experimentation, as delineated in the second row, emphasizes the pronounced superiority of our proposed architecture over LLaVA. Additionally, we enrich the training corpus with an extra 0.3 million records, mainly encompassing Chinese data. As a result, our model achieves substantial improvements in all metrics, especially on the Chinese evaluation set MMBench-CNT.\nLimitations The method of extending multimodal capabilities by freezing the language model will introduce certain additional parameters. Compared to the approach of training with an unfrozen language model, the inference speed of the model will be reduced. To mitigate this issue, we extend the key-value cache mechanism to the insertion layers. Based on the MME dataset, compared to the LLaVA architecture, the average inference time of our 8-layer structure increases from 0.103s to 0.124s, which we deem to be within a relatively reasonable range."}, {"title": "Conclusion", "content": "In this paper, we introduce the Inner-Adaptor Architecture, which is designed to enhance the general multimodal and visual grounding capabilities of LLMs. Through a series of architectural exploration experiments, we demonstrate that training with a frozen language model can surpass the multimodal performance of the models with fine-tuned LLMs. Our proposed model has achieved state-of-the-art performance across a multitude of publicly available evaluation datasets. Moreover, after deployment, our approach incorporates dual workflows, thereby preserving the NLP proficiency of the language model. The flexibility of the Inner-Adaptor Architecture provides the potential for extension to additional modalities, which is a direction for future exploration."}, {"title": "The Details of Dateset", "content": "In this section, we introduce the datasets IAA uses at different stages, along with the possible download links for these datasets in detail.\nPre-training\nThroughout the pre-training stages, the dataset employed consists of 558k image-text aligned pairs sourced from LLaVA and an additional 100K pairs from ALLaVA. ALLAVA provides a total of 664K image-text aligned data. We translate the first 100k pairs into Chinese and incorporated them into the training process to fortify the model's understanding of Chinese tasks. Over the course of these stages, we utilize a cumulative total of 658K data pairs.\n558k pairs from LLaVA  https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain\nALLAVA - https://huggingface.co/datasets/\nFreedomIntelligence/ALLaVA-4V\nInstruction Fine-tuning\nWe perform instruction fine-tuning based on the model obtained from the second pre-training stage. Throughout this stage, the parameters of the large language model and the image encoder remain frozen. The dataset includes the fine-tuning dataset of 665K samples proposed by LLaVA, along with additional datasets including DocVQA (50K), VSR (10K), ScienceQA (21K), and an in-house dataset (78.5K). Similar to the pre-training stage, we translate the first 40K entries of the 664K fine-tuning data proposed by ALLaVA into Chinese and incorporate them into the instruction fine-tuning dataset. The aggregate quantity of data utilized in this stage amounts to 865K.\n665K samples from LLaVA  https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K\nDocVQA (50K)  https://huggingface.co/datasets/cmarkea/doc-vqa\nVSR (10K)  https://github.com/cambridgeltl/visual-spatial-reasoning/\nScienceQA (21K) https://github.com/lupantech/ScienceQA\nALLava - https://huggingface.co/datasets/\nFreedomIntelligence/ALLaVA-4V\nGrounding Fine-tuning\nBuilding upon the model fine-tuned with instructions, we further train a model specialized in visual grounding. The data used in this stage comprises RefCOCO, COCO, Flickr30k Entities, Objects365, aggregating to approximately 2M data instances. These datasets improves the model's capability of localizing fine-grained visual details. The inclusion of COCO and Objects365 assists the model in improving its ability to localize multiple targets.\nRefCOCO - https://github.com/lichengunc/refer\nCOCO - https://cocodataset.org/"}, {"title": "Supplementary Display", "content": "Multimodal Capability. Figures 5 and 6 showcase the capabilities of the Inner-Adaptor Architecture (IAA) in encyclopedia question answering, image comprehension, text recognition, and writing.\nGrounding Capability. Figure 7 presents the multi-object detection capability of IAA, while Figure 8 demonstrates its detection capability for fine-grained perception."}]}