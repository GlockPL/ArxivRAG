{"title": "Improving LLM Group Fairness on Tabular Data via In-Context Learning", "authors": ["Valeriia Cherepanova", "Chia-Jung Lee", "Nil-Jana Akpinar", "Riccardo Fogliato", "Martin Andres Bertran", "Michael Kearns", "James Zou"], "abstract": "Large language models (LLMs) have been shown to be effective on tabular prediction tasks in the low-data regime, leveraging their internal knowledge and ability to learn from instructions and examples. However, LLMs can fail to generate predictions that satisfy group fairness, that is, produce equitable outcomes across groups. Critically, conventional debias-ing approaches for natural language tasks do not directly translate to mitigating group un-fairness in tabular settings. In this work, we systematically investigate four empirical ap-proaches to improve group fairness of LLM predictions on tabular datasets, including fair prompt optimization, soft prompt tuning, strate-gic selection of few-shot examples, and self-refining predictions via chain-of-thought rea-soning. Through experiments on four tabu-lar datasets using both open-source and pro-prietary LLMs, we show the effectiveness of these methods in enhancing demographic par-ity while maintaining high overall performance. Our analysis provides actionable insights for practitioners in selecting the most suitable ap-proach based on their specific requirements and constraints.", "sections": [{"title": "1 Introduction", "content": "In recent years, the scope of large language models (LLMs) has broadened significantly beyond tra-ditional natural language processing tasks, with recent research demonstrating their effectiveness in tackling challenges on tabular data, including pre-dictive tasks (Hegselmann et al., 2023; Yin et al., 2020). Typically, structured data is converted into textual format and provided to the language model along with a concise task description and key fea-tures. Notably, it has been shown that language models are particularly beneficial in scenarios with limited training data, as they can utilize internal knowledge about world from pre-training com-bined with textual instructions and few-shot exam-ples to make predictions (Slack and Singh, 2023).\nAlthough considerable research has been de-voted to exploring and addressing issues of stereo-typical bias and fairness in language models ap-plied to natural language tasks, tabular datasets present distinct challenges, particularly in group fairness. It is important to differentiate group fair-ness in the context of tabular data from conven-tional notions of fairness in NLP tasks: group fair-ness in tabular problems hinges on class labels and the representation of various demographic groups within these labels, while stereotypical fairness in NLP has primarily focused on bias in model rep-resentations. Notably, achieving fairness in the typical NLP sense does not automatically ensure group-fair predictions in tabular tasks due to poten-tial disparities in class distributions.\nRecent studies have started exploring how lan-guage models handle group fairness when applied to tabular data, revealing noticeable fairness dis-crepancies among different demographic groups. Liu et al. (2024) and Li et al. (2024) evaluate a few baseline methods for improving group fairness in tabular tasks, including resampled fine-tuning, and few-shot learning with label flipping and find these methods to have limited effectiveness. A re-cent survey paper (Fang et al., 2024) recognizes the challenge of mitigating inherent biases in large lan-guage models through conventional fine-tuning and few-shot learning and highlights the need for more effective strategies to address group unfairness in tabular tasks.\nIn this work we examine four approaches for em-pirically improving demographic parity of LLMs when applied to making predictions on tabular data. These approaches include in-context methods such as prompt optimization, soft prompt tuning, few-shot in-context learning, and self-refining predic-tions to promote fairness. We empirically evaluate"}, {"title": "2 Related Work", "content": "A growing body of work has applied deep learn-ing algorithms to tabular data (Yin et al., 2020; Herzig et al., 2020; Huang et al., 2020; Levin et al., 2022; Zhu et al., 2023). Relevant to our setting, some of these studies have employed LLMs to an-alyze tabular data that is serialized into formatted text. They show that descriptive feature names, well-defined instructions, in-context examples, and chain-of-thought reasoning enhances LLM perfor-mance (Zhao et al., 2023; Marvin et al., 2023; Chen, 2022). Some specifically focus on classifi-cation tasks (Hegselmann et al., 2023; Wang et al., 2024; Liu et al., 2022; Fang et al., 2024; Jaitly et al., 2023), which is also the focus of our work. The prior knowledge of LLMs allows them to per-form better than traditional algorithms such as XG-Boost in low-data regimes (Slack and Singh, 2023; Hegselmann et al., 2023). However, LLM predic-tions can reflect inherent biases, affecting the fair-ness of their outcomes (Hu and Du; Liu et al., 2023). Liu et al. (2023)'s work is closely related to ours: they analyze the accuracy and fairness of LLM predictions, concluding that traditional ML mod-els exhibit fewer disparities. Although in-context learning and finetuning do not fully close the fair-ness gap, label-flipping in in-context examples sig-nificantly reduces biases, albeit at the cost of pre-diction performance. Our work contributes to this literature by introducing four in-context learning approaches for mitigating the demographic parity gap in tabular data predictions, demonstrating their effectiveness across widely-used fairness datasets."}, {"title": "2.2 Bias and Stereotypes in LLMs", "content": "Despite their promising capabilities, language mod-els also exhibit biases and stereotypes (Bolukbasi et al., 2016; Bender et al., 2021; Chu et al., 2024). These biases mostly originate from the training data, which often contain historical and societal prejudices embedded within the text. Biases have been reported with respect to several demographic groups, e.g., gender, race, ethnicity, and socioeco-nomic status (Wan et al., 2023; Haim et al., 2024; Santurkar et al., 2023). With the use of these mod-els becoming more widespread, these biases have the risk to substantially reinforce harmful stereo-types and perpetuate existing inequalities, espe-cially when deployed in high-stakes settings (Zou and Schiebinger, 2018). Addressing these biases is essential, and several mitigation strategies have been proposed for this purpose, including data aug-mentation, prompt tuning and few-shot learning (Zhao et al., 2017; Wang et al., 2021; Sun et al., 2019; Zmigrod et al., 2019; Mattern et al., 2022; Fatemi et al., 2021; Aguirre et al., 2023). How-ever, effectively applying these strategies to the large-scale pretraining corpora remains challeng-ing. Finally, biases can be hard to detect and several datasets and methods have been proposed to help identify them (Caliskan et al., 2017; May et al., 2019; Webster et al., 2020; Kurita et al., 2019; Nan-gia et al., 2020; Blodgett et al., 2021)."}, {"title": "2.3 Fairness on Tabular Data", "content": "Much of the work on classification and algorithmic fairness has focused on tabular datasets (Mehrabi et al., 2021; Caton and Haas, 2024; Pessach and Shmueli, 2023; Fabris et al., 2022; Barocas et al., 2023; Chouldechova and Roth, 2018; Fogliato et al., 2020). Consequently, there is a wide range of research describing the properties and trade-offs of predictive algorithms on this type of data (Dutta et al., 2020; Black et al., 2022; Akpinar et al., 2022). Multiple works have proposed fairness-enhancing techniques for traditional ML algorithms (e.g., lo-gistic regression), which generally work by de-biasing the data, including a fairness constraint in the optimization problem, or post-processing model predictions (Zafar et al., 2017; Dwork et al., 2012; Berk et al., 2017; Lum and Johndrow, 2016; Hardt et al., 2016; Martinez et al., 2020; Akpinar et al., 2024). Our work employs related techniques, although some of them are not directly applica-ble. The formalization of fairness definitions has also been extensively discussed (Castelnovo et al., 2022). Fairness metrics evaluated on tabular data typically measure the equality of some target mea-sure across demographic groups, such as accuracy or recall (Chouldechova, 2017), which fall under the umbrella of group fairness definitions (as op-posed to individual fairness definitions). One such widely-adopted measure, which we also employ in this work, is demographic parity, which ensures that the frequency of positive predictions is approx-imately equal across different demographic groups."}, {"title": "3 Experimental Details", "content": "In our experiments we focus on scenarios with little to no training data available. This is a particularly attractive setting for using language models since they typically outperform classical tabular models in low-data regimes as they can leverage their inherent knowledge for predictions (Hegselmann et al., 2023; Slack and Singh, 2023). To make prediction on a single sample, we prompt the model with task-specific instructions, along with the relevant features of the sample of interest; see Appendix A for more details on prompting templates. Optionally, we may also include fairness-specific instructions and few-shot examples, depending on the method used to improve fairness. We then extract the answer either by directly generating a response to the question (for closed-source models) or by calculating the likelihood of tokens corresponding to labels.\nIn experiments that involve selecting the \"best prompt\" from several iterations, such as in prompt engineering, we utilize a small validation set of 50 labeled examples to assess model accuracy. We then select (empirically) Pareto-optimal prompts, which represent those where any improvement in either accuracy or fairness would necessitate a com-promise in the other metric. Due to its limited size, the validation set is used solely to assess accuracy, while demographic parity is directly evaluated on the test set to identify Pareto-optimal prompts. We additionally compare our methods against a tabular model, specifically, CatBoost implementation of gradient boosted decision trees trained on 50 exam-ples (Prokhorenkova et al., 2018) and fairness con-straint enforced by GridSearch\u00b9 function following the reductions approach by Agarwal et al. (2018)."}, {"title": "3.1 Language Models", "content": "We conduct experiments using a variety of widely used language models that vary in size. Due to the computational demands of some methods, we con-duct computationally intensive experiments with smaller models and reserve methods that require ad-vanced reasoning for larger language models. Our experiments include Llama 3 8B and 70B (Touvron et al., 2023), Mistral 7B (Jiang et al., 2023), Mix-tral 8x7B (Jiang et al., 2024) and Claude Sonnet"}, {"title": "3.2 Datasets", "content": "We explore group fairness of LLMs on a set of publicly available datasets widely used in the algo-rithmic fairness literature. For each of the datasets, we focus on 'gender' as the protected attribute. We briefly introduce the datasets here and point to Ap-pendix C for additional details.\nAdult The Adult Income dataset (Barry Becker, 1996) includes 1994 US Census information to pre-dict whether individuals' yearly income exceeds $50k (1 = yes, 0 = no). In accordance with previ-ous work (Liu et al., 2024; Slack and Singh, 2023), we retain 10 features for prediction, sample 1000 examples for evaluation and use the remainder of the data for generating task-specific instructions.\nGerman credit The German credit dataset (Hof-mann, 1994) is used to predict credit default risk (1 = good, 0 = bad) based on individual attributes. Following previous work (Liu et al., 2024), we re-tain 9 features and split the data set into 50% for evaluation and 50% for task instruction generation.\nACS Income & Coverage The American Com-munity Survey (ACS) data (Ding et al., 2021) is sourced from the US Census. For our experiments, we utilize the income (1 = yearly income >$50k, 0 = else) and coverage (1 = public health coverage, 0 = else) prediction tasks for 2018 data from the state of New York. For each classification task, we sample 1,000 examples for evaluation, and use the remaining data to select 10 features with the highest importance."}, {"title": "3.3 Serialization and prompts", "content": "LLMs require textual input, unlike traditional tabu-lar prediction models. In line with previous work (Slack and Singh, 2023; Hegselmann et al., 2023), we serialize data points by (1) mapping categorical values to the respective strings (e.g. gender = 1 is mapped to gender = male), and (2) consolidating column names and entries into one string per row.\nAlthough we assume little to no training data, it is reasonable to expect that practitioners will pro-vide task-specific instructions to the model to fa-cilitate accurate predictions. For this, we construct instructions using prototype clustering on the train-ing folds of the datasets, as suggested by Slack and Singh (2023). To make instructions more readable, we use GPT-4 to revise prototype information into"}, {"title": "3.4 Metrics", "content": "In this work we focus on optimizing demographic parity (DP) which aims to equalize positive label selection rate across groups, i.e.\n$E[f(X) | G = g] = E[f(X)]$\nfor a binary predictor $f$ and $g \\in \\{male, female\\}$. Constraint violation is reported as ratio between the smallest and largest group level selection rates $E[f(X) | G = g]$ with values closer to 1 indicat-ing better parity. We use DP primarily because it allows to measure fairness on an unlabeled test set directly and does not require labeled training data. Although our primary focus is on demographic parity, the methods we propose can be adapted to other fairness metrics when labeled training data is available as discussed in section 7. Additionally, while our main objective is demographic parity, we also evaluate equalized odds which aims to balance false positive and false negative rates across groups, i.e.\n$E[f(x) | G = g, Y = y] = E[f(x) | Y = y]$\nfor a binary predictor $f$, $Y \\in \\{0,1\\}$, and $g\\in \\{male, female\\}$, and report equalized odds ratio between groups."}, {"title": "4 Methods", "content": "In this work we consider four empirical approaches for improving group fairness of language model predictions on tabular datasets as illustrated in Fig-ure 1. This section provides a detailed overview of each method, with subsequent section delving into experimental results for each approach.\nFair Prompt Optimization. Prompt engineer-ing continues to play an important role in tailoring the capabilities of LLMs to various tasks (Chen et al., 2023; Wang et al., 2023; White et al., 2023). Recently, Tamkin et al. (2023) demonstrated that integrating fairness-specific manually-curated in-structions in the prompt, such as \u201cit is illegal to discriminate\", can attenuate counterfactual biases in model predictions. Additionally, several works have shown that LLMs can act as prompt engi-neers producing performant prompts for down-stream models (Zhou et al., 2022; Yang et al., 2023). In our work we demonstrate the effectiveness of prompt engineering in achieving group fairness in LLMs and show how prompt optimization can be automated. In particular, we propose to optimize a fairness-specific prompt (highlighted in blue on the left panel on Figure 1), appended to the task-specific instructions. We adopt the prompt opti-mization approach following Yang et al. (2023) and employ meta-LLM to iteratively refine fair-ness instructions for the downstream model based\""}, {"title": "Soft Prompt Tuning", "content": "In addition to hard prompt optimization, we explore soft prompt tuning, which optimizes the prompt directly in the embedding space instead of discrete token space, see the second-from-left panel in Figure 1. In traditional tabular methods, standard in-processing fairness in-terventions often involve training machine learning models with a fairness penalty. This encourages the model to equalize selection rates or, depend-ing on the penalty, the error rates across demo-graphic groups (Zafar et al., 2017; Hardt et al., 2016). Drawing inspiration from these techniques and parameter-efficient fine-tuning methods, we propose a similar approach that can be applied to improving group fairness in language models. In particular, rather than optimizing fair prompts in the discrete space of tokens, as done in the previous section, we suggest optimizing a soft prompt by fine-tuning tokens in the embedding space. Contin-uous optimization in the embedding space allows us to incorporate the fairness penalty into objec-tive directly. Specifically, we fine-tune 50 tokens initialized with task-specific instructions in the em-bedding space for 20 epochs. This approach applies a penalty designed to equalize the likelihoods of to-kens corresponding to positive labels across groups within a batch:\n$|P(Y = 1|A = 0) \u2013 P(Y = 1|A = 1)|.$\nTo tune the prompt we use 1000 samples with pseudo-labels obtained by the same language model in the zero-shot setup, simulating a scenario without labeled data. To preserve accuracy and en-sure predictions remain close to the original model outputs, we include the standard cross-entropy loss for the pseudo-label predictions."}, {"title": "Fair Few-Shot Examples", "content": "Prior work (Liu et al., 2024; Hu and Du; Li et al., 2024) has leveraged the in-context learning capabilities of language mod-els for this problem space. They hypothesize that, when selected appropriately, few-shot examples can effectively influence the final predictions to more accurately reflect the desired notion of fair-ness. For instance, it has been demonstrated that flipping the labels of few-shot examples can effec-tively reduce bias, albeit at the expense of signifi-cantly lower classification performance (Liu et al., 2024), while class- and group- balanced selection does not mitigate the bias (Li et al., 2024).\nWith a similar goal, we propose a strategy for constructing fair few-shot examples, which dif-fers from the previous methods in three ways. First, instead of randomly sampling examples from the training data, we apply the nearest neighbor search to select examples that are most similar to a current test instance in the feature space\u00b2. Also, we always select examples that share sen-sitive attribute with the test instance. Secondly, as we assume no access to training data labels, we use the language models' default zero-shot predic-tions as pseudo labels to construct demonstrations"}, {"title": "Self-Refinement", "content": "In addition to in-processing methods, fairness literature also includes a wide array of post-processing techniques (Hardt et al., 2016). These methods work by altering model outputs directly. We propose an LLM-based post-processing method that leverages the reasoning ca-pabilities of language models, along with a chain-of-thought process, to refine their own predictions. The self-refinement approach involves using lan-guage models to identify individuals from both minority and majority groups who are near the \"decision boundary\", and then flipping their la-bels to achieve the desired demographic parity ra-tio. Therefore, the prediction process includes two stages. First, the model makes initial predictions on a batch of data samples. After that, the model assesses demographic parity in a batch and adjusts predictions to attain the desired parity, if necessary. An example prompt used to refine predictions is illustrated in Figure 1 most right panel. Given that self-refinement approach relies on the advanced reasoning capabilities of language models to ana-lyze predictions, compute metrics of interest, and adjust individual predictions, we conduct these ex-"}, {"title": "5 Experimental Results", "content": "In this section we present performance of the pro-posed methods. Since certain fairness metrics are not necessarily aligned with accuracy, models pro-ducing fairer decisions may suffer from accuracy degradation. Therefore, it is important to identify methods resulting in an optimal fairness-accuracy trade-off."}, {"title": "5.1 Fair Prompt Optimization", "content": "To evaluate the effectiveness of prompt engineer-ing for fairness, we, first, manually construct a few prompts which are supposed to improve group fairness, such as (1) \u201cIgnore sex completely when making predictions\u201d, (2) \u201cAssign positive label to females and males at the same rate.\u201d, (3) \u201cAssign positive label to females slightly more often than to males.\", with the last prompt coming from the fact that female group is the one disadvantaged. We include the accuracy and demographic parity for models queried with these prompts for Adult dataset in Figure 3. We observe that, while these prompts can improve demographic parity in some models, finding a universal \u201cfair instruction\u201d that upholds group fairness consistently across multiple models is challenging.\nNext, we experiment with a prompt optimization framework using a meta-LLM, designed to dynami-cally refine fair instructions based on demonstrated\""}, {"title": "5.2 Soft Prompt Tuning", "content": "Soft prompt tuning enables continuous optimiza-tion of a fairness objective by incorporating it di-rectly into the loss function. We tune the soft prompts with a demographic parity fairness reg-ularizer, which aims to equalize the likelihood of positive label predictions across different groups within a batch\u00b3.\nSimilarly to our fair prompt engineering exper-iments, we identify Pareto-optimal points among fine-tuning epochs using the validation set and include results for Pareto-optimal soft prompts achieving at least 0.9 test demographic parity in Table 1. We observe that while tuning soft prompts improves demographic parity across all datasets, it results in suboptimal trade-off with accuracy com-pared to hard prompt optimization approach. This"}, {"title": "5.3 Fair Few-shot Examples", "content": "In this section we present results for our fair few-shot example construction strategy, which selects nearest-neighbors to test instances, uses zero-shot model predictions as pseudo-labels, and adjusts the ratio of positive examples between groups to en-hance fairness. Figure 5 illustrates the impact of varying the ratio of positive examples in the prompt. The x-axis represents the ratio of positive examples for female test instances, while the color indicates the ratio of positive examples used for predictions on male samples. The results are averaged across 3 random seeds, with the band indicating the stan-dard deviation across seeds. We observe that in-creasing the positive ratio for females significantly improves demographic parity, to the extent that the selection rate for females surpasses that for males. Additional figures for other models and datasets are displayed in Appendix E. These results con-firm that adjusting the ratio of positive examples in-context is an effective method for manipulating the prevalence of positive class predictions, and employing different ratios across protected groups can effectively reduce disparities in selection rates. Additionally, we compare our nearest-neighbor selection strategy with a baseline selecting exam-ples randomly while preserving similar label ratios in-context. Figure 6 in Appendix shows that includ-"}, {"title": "5.4 Self-Refinement", "content": "When making predictions in batches, we can utilize the chain-of-thought and self-refinement capabili-ties of language models to apply post-hoc correc-tions to predictions, see the right panel in Figure 1 for an illustration. We make predictions on a batch of 40 samples, and instruct the model to make ad-justments only when the difference in positive rates across groups exceeds 15%. We report the results of the self-refinement approach in Table 2. For all models, refined predictions achieve improved demographic parity across all datasets except for ACS coverage, although this sometimes leads to a notable trade-off in accuracy. In addition, there is no guarantee for similar individuals to receive similar predictions with this method because of the 'correction step' which is at odds with notions of individual fairness (Dwork et al., 2012)."}, {"title": "6 Conclusion", "content": "We systematically explore four empirical methods to improve group fairness of language model pre-dictions on tabular datasets, and discuss the key takeaways for each method below.\nFair Prompt Optimization can improve not only fairness but also classification performance, con-tingent upon the model's \"creativity.\" This method involves an optimization process that requires eval-uating the prompt on a dataset for a number of iterations. Although the resulting instructions are interpretable, the reasons why specific instructions yield fairer results are not always clear.\nSoft prompt tuning is computationally expensive and sensitive to the choice of hyperparameters. While this method does not yield interpretable in-structions, it enables the integration of common fairness regularizers in a differentiable way and may be particularly effective for smaller models.\nFair Few Shot Examples is the most interpretable and predictable method, yielding optimal results across models and datasets when an optimal combi-nation of positive examples ratios is selected. How-ever, it uses a longer context window and may be more computationally expensive for larger datasets because of the number of forward passes needed.\nSelf-refinement requires a model with strong rea-soning capabilities and does not guarantee similar predictions for similar individuals. However, this method offers a computational advantage for larger models, as predictions are made and adjusted in batches, reducing overall processing time.\nWe recommend fair few-shot examples and fair prompt optimization as universal approaches achieving the optimal accuracy tradeoff. Soft"}, {"title": "7 Limitations and Potential Risks", "content": "Our work has several limitations. Firstly, it exclu-sively examines in-context approaches and does not address data pre-processing for bias mitigation or post-hoc methods that modify model outputs di-rectly (Hardt et al., 2016). Additionally, we do not consider model training and fine-tuning strategies other than soft prompt tuning. Finally, we focus on a single notion of fairness, that is demographic parity, since it can be applied in little to no training data regime, the most practical scenario for lan-guage models on tabular datasets. However, most of the discussed methods can be adapted to opti-mize for other fairness notions, such as equalized odds, when labeled training data is available. For example, the prompt optimization procedure can incorporate alternative fairness metrics in the feed-back component of the meta-prompt. Soft prompt tuning can adopt differentiable proxy regularizers to enforce desired fairness criteria, and the few-shot examples approach can demonstrate more exam-ples with ground-truth labels to underrepresented groups.\nWhile the methods explored in this work show promise for improving demographic parity of large language models on tabular prediction tasks, there are also several potential risks to consider. En-hancing group fairness may come at the expense of overall predictive performance, and focusing solely on demographic parity could neglect other important fairness criteria. Moreover, optimizing for group fairness along one dimension could in-advertently amplify or introduce other biases not directly measured. As such, practitioners should carefully evaluate the appropriateness and potential pitfalls of these methods for their specific use case before deploying language models for high-stakes decision making on tabular data."}, {"title": "A Prompt Details", "content": "In this section we provide the prompt templates used in the default setup to obtain predictions from the models. We highlight task instructions in red, and sample for which the prediction is made in blue. When using an optimized fair prompt, if follows the task instructions. For the fair few-shot examples experiments, we provide selected examples before the test sample in the prompt."}, {"title": "A.1 Prompt Template for Adult dataset", "content": "User: You must follow the instructions to predict if income of an individual exceeds $50K/yr. Gener-ally, individuals who earn more than $50K/yr tend to have higher levels of education (e.g., Bachelors or Prof-school), work in managerial or specialty occupations, and are married with a spouse. They also usually work longer hours per week and have higher capital gains. On the other hand, those earn-ing less than or equal to $50K/yr typically have lower levels of education (e.g., HS-grad), work in clerical or craft repair occupations, and may be sin-gle or married without a spouse. They also usually work fewer hours per week and have lower cap-ital gains. You must provide an explanation and include answer enclosed in square brackets at the beginning of your response.\nworkclass: Private; hours-per-week: 40; sex: Male; age: 38; occupation: Craft-repair; capital-loss: 0; education: HS-grad; capital-gain: 0; marital-status: Divorced; relationship: Not-in-family.\nYou must predict if the individual's income exceeds $50K/yr. Answer with one of the following: [less than or equal to 50K] or [greater than 50K].\nAssistant: Based on the information provided the answer is"}, {"title": "A.2 Prompt Template for German Credit dataset", "content": "User: You must follow the instructions to predict the credit risk of a German bank customer based on their profile data. Higher credit amounts and longer durations often indicate a 'bad' classifica-tion, particularly for loans intended for new cars or high-value purchases. A negative or low checking status (either '<0' or '0<=X<200') is also associ-ated with higher risk. Conversely, 'good' classi-fications tend to involve moderate or lower credit amounts, shorter loan durations, and are frequently for consumer goods like radios, TVs, or used cars. Surprisingly, having 'no checking' status tends to lean towards a 'good' classification. More people tend to have good credit risk. You must provide an explanation and include answer enclosed in square brackets at the beginning of your response.\nage: 36.0; sex: male; job: skilled; housing: own; savings status: no known savings; checking status: 0<=X<200; credit amount: 2181.0; duration: 30.0; purpose: new car.\nYou must predict the credit risk of a German bank customer. Answer with one of the following: [good] or [bad].\nAssistant: Based on the information provided the answer is"}, {"title": "A.3 Prompt Template for ACS Income dataset", "content": "User: You must follow the instructions to predict if income of an individual exceeds $50K/yr. Individ-uals who are most likely have income less than or equal to 50K are typically younger, unmarried, with lower levels of education (high school diploma or some college), and engaged in lower-skilled occu-pations such as cashiers, janitors, or administrative assistants. Conversely, individuals who are older, married, with higher levels of education (bachelor's or master's degree), and are employed in profes-sions such as teachers, managers, physicians, or registered nurses are more likely to have income greater than 50K. You must provide an explanation and include answer enclosed in square brackets at the beginning of your response.\nSex: Female; Race: White alone; Educational attainment: Bachelor's degree; Relationship: Roomer or boarder; Age: 35.0; Usual hours worked per week past 12 months: 32.0; Marital status: Never married; Place of birth (Recode): Florida/FL; Class of worker: Employee of a private not-for-profit, tax-exempt, or charitable organiza-tion; Occupation: EDU-Elementary And Middle School Teachers.\nYou must predict if the individual's income exceeds $50K/yr. Answer with one of the following: [less than or equal to 50K] or [greater than 50K].\nAssistant: Based on the information provided the answer is"}, {"title": "A.4 Prompt Template for ACS Coverage dataset", "content": "User: You must follow the instructions to predict whether an individual is covered by public health insurance. Individuals covered by public health in-surance tend to have a regular high school diploma, have never served in the military, and generally have lower income. In contrast, features such as being employed, having educational attainment, higher income (above $20,000) and being married correlate with not being covered by public health insurance. In addition, people with disabilities are more likely to be covered by public health insur-ance. You must provide an explanation and include answer enclosed in square brackets at the begin-ning of your response.\nSex: Female; Race: White alone; Educational attainment: Associate's degree; Military service: Never served in the military; Disability recode: Without a disability; Total person's income: 0.0; Marital status: Never married; Employment status recode: Not in Labor Force; Employment status of parents: N/A (not own child of householder, and not child in subfamily); Gave birth to child within the past 12 months: No.\nYou must predict if the individual is covered by public health insurance. Answer with one of the following: [covered] or [not covered].\nAssistant: Based on the information provided the answer is"}, {"title": "B Additional Experimental Details and Hyperparameters", "content": "Hyperparameters for Soft Prompt Tuning. In the soft prompt tuning experiments, we fine-tune 50 tokens initialized with the task instructions for 20 epochs. We employ a learning rate of 1e - 4 for Llama 8B models and 1e \u2013 5 for Mistral models, allowing the first three epochs for a warm-up with a linear scheduler. During fine-tuning, we use 1000 train samples with pseudo-labels obtained by using the language model in a zero-shot setup, we apply demographic parity regularization with a penalty weight of 0.5. We employ a class-balanced sampler and set the batch size to 60 samples for Mistral and 50 samples for Llama models, which were the largest sizes we could use given the computational constraints."}, {"title": "C Datasets", "content": "We include details on the datasets and features used in our experiments in the Table 3."}, {"title": "D Hardware", "content": "We conducted all experiments using 8 Tesla V100 32GB GPUs through AWS. The soft-prompt tun-ing experiments required approximately 120 GPU hours per model per dataset, resulting in 950 GPU"}, {"title": "E Additional Results", "content": "In Tables 4, 5 we include optimized fair prompts for each dataset each model. In particular, we include Pareto-optimal prompts, which achieve the highest and the lowest demographic parity ratio."}]}