{"title": "ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Framework for Student Learning in Online Education Systems", "authors": ["Hong Qian", "Shuo Liu", "Mingjia Li", "Bingdong Li", "Zhi Liu", "Aimin Zhou"], "abstract": "Cognitive diagnosis models (CDMs) are designed to learn students' mastery levels using their response logs. CDMs play a fundamental role in online education systems since they significantly influence downstream applications such as teachers' guidance and computerized adaptive testing. Despite the success achieved by existing CDMs, we find that they suffer from a thorny issue that the learned students' mastery levels are too similar. This issue, which we refer to as oversmoothing, could diminish the CDMs' effectiveness in downstream tasks. CDMs comprise two core parts: learning students' mastery levels and assessing mastery levels by fitting the response logs. This paper contends that the oversmoothing issue arises from that existing CDMs seldom utilize response signals on exercises in the learning part but only use them as labels in the assessing part. To this end, this paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing response signals in the learning part. Specifically, ORCDF introduces a novel response graph to inherently incorporate response signals as types of edges. Then, ORCDF designs a tailored response-aware graph convolution network (RGC) that effectively captures the crucial response signals within the response graph. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings with the outcome of RGC, allowing for the consideration of response signals on exercises in the learning part. Extensive experiments on real-world datasets show that ORCDF not only helps existing CDMs alleviate the oversmoothing issue but also significantly enhances the models' prediction and interpretability performance. Moreover, the effectiveness of ORCDF is validated in the downstream task of computerized adaptive testing.", "sections": [{"title": "1 INTRODUCTION", "content": "Cognitive diagnosis (CD) [19] serves as the foundational element in online intelligent education systems. It exerts an upstream and fundamental influence on subsequent modules such as computer adaptive testing [42], course recommendation [10, 35] and learning path suggestions [25, 26], among others. Specifically, as illustrated in Figure 1, CD aims to learn students' underlying mastery levels (Mas) by analyzing their historical response logs, thereby providing insights into various attributes of exercises, such as difficulty level (Diff) and discrimination (Disc). In recent years, an array of cognitive diagnosis models (CDMs) have emerged, prominently featuring frameworks such as item response theory (IRT) [8] and the neural cognitive diagnosis model (NCDM) [30]. The two core parts of CDM include learning students' Mas and assessing the learned Mas by fitting the response logs. The function used in the latter part is often referred to as the interaction function (IF). IRT utilizes a latent factor to represent Mas and adopts the logistic function as IF. In contrast, NCDM replaces the traditional IFs with multi-layer perceptrons (MLP) and uses concept-specific vectors (i.e., set the embedding dimension being equal to the number of concepts) to characterize Mas. As embedding-based methods rapidly evolve and gain prominence, there is an increasing trend of representing both students and exercises in a vectorized form, and they are gradually refined by using a variety of advanced techniques [6, 12, 15, 21, 23, 31].\nDespite the success, this paper, for the first time, identifies that existing CDMs share a potential and thorny issue that the learned Mas of students are too similar. We refer to this issue as oversmoothing. Oversmoothing could diminish the CDMs' effectiveness in down-stream tasks. To support the motivation of this paper and reveal the oversmoothing issue, we conduct a pilot study on four real-world datasets collected from the online education systems, ensuring a diverse range of circumstances in the students' response logs. To characterize the degree of oversmoothing, inspired by [14], the mean normalized difference (MND) is proposed to measure the Mas learned by CDMs. Intuitively, the larger the MND value, the bigger the difference among students' Mas that learned by CDMs. Details of MND are elaborated in Section 5.1. As shown in Figure 2, although CDMs such as NCDM [30], CDMFKC [15], KSCD [21] and KaNCD [31] achieve commendable prediction performance, the MND values of Mas they have learned are quite small and hard to distinguish. Since CD is an upstream task, addressing this issue is urgent. For instance, if teachers rely on the outcomes of CD to assist student development, exceedingly subtle distinctions could lead to confusion. Intuitively, if MND is 0.005, it implies that the average difference in Mas for two students in a class on certain concepts is merely 0.005 (e.g., 0.51 and 0.515). Such a small margin could potentially bring difficulty to teachers to accurately assess the cognitive state of entire class. This not only fails to aid students but could also result in misguided instruction. Moreover, for downstream algorithms, a diagnosis result plagued by oversmoothing may lead to erroneous recommendations of learning materials, causing irreversible impacts on students.\nOne straightforward approach is to design a regularization term aimed at amplifying the differences between students. However, achieving a balance between the weight of this regularization term and the binary cross-entropy (BCE) loss during training is challenging. Besides, although this direct approach may help in mitigating the oversmoothing issue, it could compromise the model's prediction performance, since it forcefully amplifies the differences among all students and adversely affects the learning of students' Mas who should, in principle, be closely aligned. In this paper, we contend that the oversmoothing issue arises because existing CDMs seldom utilize response signals in the learning part but only use them as labels in the assessing part. For instance, students with right response on exercises with high difficulty levels should attain higher Mas on corresponding concepts in the learning part. Cooperating response signals in both learning and assessing parts of CDMs can widen the gap among students' Mas as they reserve the unique feature in students' response logs.\nTo this end, this paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing response signals in the learning part. Specifically, ORCDF introduces a novel response graph, which utilizes response logs and a Q-matrix, inherently incorporating response signals as types of edges. Then, ORCDF designs a tailored response-aware graph convolution network (RGC) that effectively captures the crucial response signals within the response graph. We reveal that by utilizing the multiple layers of RGC, we achieve a multi-perspective analysis of student mastery. This is accomplished by combining the outcomes from multiple layers of RGC, leading to a more comprehensive understanding of student learning. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings with the outcome of RGC through the transformation layer, allowing for the consideration of response signals on exercises in the learning part. Nevertheless, ORCDF encounters a new challenge: overemphasizing the role of response signals can exacerbate the guess and slip problem. This problem occurs when students guess in order to answer correctly or make mistakes on exercises they actually master, and could potentially lead models to make unreasonable inference of students' Mas. Different from previous methods that introduce extra parameters for guess and slip probabilities [16], this paper addresses the guess and slip problem in student-exercise interactions by considering them as noise edges in the response graph. Specifically, we flip the student-exercise edge in the response"}, {"title": "2 RELATED WORK", "content": "Cognitive Diagnosis Models. CDMs involve various approaches, such as latent factor models like IRT and MIRT (multidimensional IRT), or concept mastery pattern models like the deterministic input, noisy and gate (DINA) model, to infer students' mastery levels. DINA, a classic CDM, employs binary variables to represent mastery levels where 0 means unmastered and 1 means mastered. However, recent advances in deep learning have led to significant improvements in handling large-scale interactions. Notably, NCDM uses MLP as its IF, treating mastery patterns as continuous variables ranging from 0 to 1. This evolution in approach has been paralleled by diverse methods in analyzing response logs, including MLP based [13, 15, 21], graph attention networks [28] and Bayesian networks [12, 33], each contributing to a more nuanced understanding of student learning patterns. However, as depicted in Figure 2, these advanced CDMs encounter the oversmoothing issue which could potentially hinder the application of CD in downstream tasks of intelligent education, affecting their performance and consequently impacting student learning. To the best of our knowledge, the oversmoothing issue in the field of CD remains unexplored.\nOversmoothing Issue. The oversmoothing issue [14] is a significant problem in graph representation learning (GRL). Many studies have shown that the layers of graph neural network (GNN) deepen, the representations of graph nodes become increasingly smooth, leading to a substantial decrease in accuracy. This has prompted numerous researchers to employ a variety of methods [22] to address this issue, enabling deeper GNN architectures. The same phenomenon is also observed in various fields where graphs are used for data mining. For example, in recommendation systems, graph collaborative filtering (GCF) [34] faces the oversmoothing problem, which arises for the same reasons as in GRL due to the stacking of GNN layers. However, in the context of CD, oversmoothing is not a result of stacking GNN layers, since most CDMs like NCDM, CDM-FKC, KSCD and KaNCD do not utilize GNN. Yet, this issue does exist and is urgent, as shown in Figure 2. Thus, existing solutions to addressing oversmoothing in GRL and GCF are not suitable to resolve the oversmoothing issue in CD."}, {"title": "3 PRELIMINARIES", "content": "This section first introduces the fundamental elements of CD and then introduces the formal problem definition of CD and oversmoothing issue in CDMs. We also give abbreviations for terms in Table 5 at the beginning of the Appendix.\nCognitive Diagnosis Basis. Consider an education scenario which contains three sets: $S = {s_1,..., s_N}$, $E = {e_1, ..., e_M}$, and $C = {c_1,..., c_Z}$. They symbolize students, exercises and knowledge concepts, with respective sizes of N, M and Z. Q represents the relationship between exercises and knowledge concepts, which can be regarded as a binary matrix $Q = (Q_{iz})_{M \\times Z}$, where $Q_{iz} \\in {0,1}$ means whether $e_i$ relates to $c_z$ or not. Students from set S, driven by unique interests and requirements, select exercises from E. The results are documented as response logs. Specifically, these logs can be illustrated as triplets $T = {(s, e, r) | s \\in S, e \\in E, r_{se} \\in {0, 1}}$. $r_{se} = 1$ represents correct and $r_{se} = 0$ represents wrong. In this paper, we treat response logs as interaction matrix $I \\in R^{N \\times M}$. It contains three categorical values (1 means right, 0 means no interaction and -1 means wrong). Finally, we give the formal definition of the CD task and oversmoothing issue in CDMs.\nDefinition 3.1 (Problem Definition). Given interaction matrix $I \\in R^{N \\times M}$, a binary matrix $Q \\in R^{M \\times Z}$, the goal of cognitive diagnosis is to infer $Mas \\in R^{N \\times Z}$, which denotes the latent mastery level of students on each concept.\nDefinition 3.2 (Oversmoothing in CDMs). Given the learned $Mas \\in R^{N \\times Z}$ by CDMs, if the difference in students' Mas is sufficiently small, it indicates the presence of oversmoothing issue in CDMs.\nIn this paper, we utilize the mean normalized difference proposed in Section 5.1 to quantify the degree of oversmoothing."}, {"title": "4 METHODOLOGY: THE PROPOSED ORCDF", "content": "This section introduces the proposed ORCDF. It starts by introducing the proposed novel response graph, then explores the response-aware graph convolution (RGC), a technique designed to capture the rich information embedded in the response graph. Following this, we introduce a consistency regularization loss function. We also discuss the model training and analyze model complexity. An overview of ORCDF is shown in Figure 3.\nResponse Graph. As illustrated in Figure 4(a), focusing on responses, the response graph (ResG), denoted as $G = (V, E)$, comprises three types of nodes and edges. $V = S \\cup E \\cup C$ involves students, exercises, and concepts, $E$ involves interactions between $S$ and $E$ (i.e., \"Right\"), S and E (i.e., \"Wrong\"), E and C (i.e., \"Related\"). Notably, we incorporate the response signal on exercises as the edge types between students' nodes and exercises' nodes. Next, we will introduce how to capture the fruitful response signal information.\n4.1 Response-aware Graph Convolution\nConstruct Embeddings. In CD, the primary data elements are response logs and the Q. It is crucial to deconstruct these complex logs into their fundamental components: students, exercises, and concepts. We encode them with trainable embeddings $H_s \\in R^{N \\times d}$, $H_e \\in R^{M \\times d}$, $H_c \\in R^{Z \\times d}$. For instance, $h_{s_i} \\in R^{1 \\times d}$ denotes the row vector of the i-th student. To facilitate subsequent convolution processes, we stack the aforementioned embeddings to form $H^{(0)} \\in R^{(N+M+Z) \\times d}$.\nRight-Wrong Decomposition. In the ResG, there are two types of response signals existing between student nodes and exercise nodes, as shown in Figure 4(a). To better explore the impact of different response signals on learning Mas, we intuitively decompose"}, {"title": null, "content": "the response graph into a right subgraph and a wrong subgraph. From the perspective of adjacency matrix, this involves splitting the interaction matrix I into $I_{right}$ (1 represents right, 0 represents others) and $I_{wrong}$ (1 represents wrong, 0 represents others). For brevity, in the following sections, we will denote \u201cR\u201d for right and \u201cW\u201d for wrong. Then we construct the right and wrong subgraphs (i.e, $A_R$, $A_W$ as expressed by Eq. (1):\n$A_R =  \\begin{bmatrix}  0 & I_R & 0\\\\  I_R^T & 0 & Q\\\\  0 & Q^T & 0  \\end{bmatrix}$, $A_W =  \\begin{bmatrix}  0 & I_W & 0\\\\  I_W^T & 0 & Q\\\\  0 & Q^T & 0  \\end{bmatrix}$                                                                                                                               (1)\nIn the ResG, the neighbors of a specific exercise node may include students who either answer the exercise right or wrong. However, after disentangling such response signals in the ResG, in each subgraph, the neighbors of a particular exercise node will only consist of students who displayed the same response signals. For example, if both $s_1$ and $s_2$ are connected to $e_1$, indicating that they both answer $e_1$ correctly, there may be some shared information explaining why they both got it right. Such crucial response signals will be propagated during the message-passing mechanism by GCN. This process enables a deeper understanding of the nuances in student responses. In the following part, we will introduce a novel graph convolution approach tailored to capture the information from the two disentangled subgraphs in CD.\nEmbedding Propagation. Considering that in CD, the features of students, exercises, and knowledge concepts are quite simple, consisting only of IDs, we draw inspiration from [7]. As a result, we eliminate linear transformations and nonlinear activation functions, opting to use only the fundamental components of GCN. Hence, the graph embedding propagation layer is designed with the following matrix form\n$H^{(l)} = \\tilde{A}H^{(l-1)}, \\tilde{A} = (D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}})$, (2)\nwhere A can be $A_R$ or $A_W$. The degree matrix D is a diagonal matrix with size $(N+M+Z) \\times (N+M+Z)$, where each entry $D_{ii}$ representing the number of non-zero entries in the i-th row vector of the matrix A. Using Eq. (2), we can obtain the convolution outcomes from the l-th layer of the disentangled subgraphs, specifically $H_R^{(l)}$ and $H_W^{(l)}$. However, right and wrong represent completely opposite response signals, it may be inappropriate to directly plus the results obtained from convolutions performed on the two subgraphs. A sophisticated function capable of aggregating these two types of information is necessary, as the interaction mechanisms between students and exercises are quite intricate. It can be expressed as\n$H_F^{(l)} = \\phi(H_R^{(l)}W_{rc} + H_W^{(l)}W_{wc}),$ (3)\nwhere $H_F^{(l)}$ denotes the final representation of the l-th RGC layer and $\\phi$ denotes arbitrary nonlinear activate function. $W_{rc}, W_{wc} \\in R^{d \\times d}$ are trainable parameters. Intuitively, $H_R^{(l)}W_{rc}$ denotes the right channel which obtain the semantic information from right signal. Conversely, $H_W^{(l)}W_{wc}$ represents the opposite. The ultimate embedding $H_F$ is calculated using a mean pooling operation on the outcomes from each layer of the RGC which can be expressed as\n$H = \\frac{1}{1+L}(H^{(0)} + H_F^{(1)} + ... + H_F^{(l)}+ ... H_F^{(L)})$. (4)\nDiscussion. Here, we explain why RGC can alleviate the oversmoothing issue in existing CDMs. Notably, since our goal is to alleviate the oversmoothing issue in CDMs, and given that shallow layers of RGC already achieve satisfactory experimental results, the"}, {"title": null, "content": "oversmoothing issue caused by deep GNN is not addressed in this work and can be considered for future research. First, we analyse the NCDM which directly set Mas as $H^{(0)} \\in R^{N \\times d}$ and $d = Z$. Firstly, the MND of any two students $s_1$ and $s_2$ in original NCDM is calculated as\n$MND_{s_1, s_2} = ||Mas_{s_1} - Mas_{s_2}||_2 = ||H_{s_1}^{(0)} \u2013 H_{s_2}^{(0)}||_2$. (5)\nClearly, the difference between the learned Mas of $s_1$ and $s_2$ in NCDM reflects the disparity in their individual information. Here, we will use a one-layer RGC incorporated with NCDM as an example. For brevity, we will omit all normalization coefficients, biases and retain only the key components. The MND of students $s_1$ and $s_2$ after utilizing RGC is calculated as\n$MND_{s_1, s_2} = ||Mas_{s_1} - Mas_{s_2}||_2 = ||H_{s_1} \u2013 H_{s_2}||_2$. (6)\nVia Eq. (2), we can derive that $H_{s_1}^{(1)}(R) = \\sum_{e_j \\in N_R(s_1)}H_{e_j}W_{rc}$ where $e_j \\in N_R(s_1)$ represents the j-th exercise $s_1$ practiced correctly and is also the neighbor of $s_1$ in the right subgraph. Consequently, the term $H_{s_1}^{(1)}(R)$ represents the normalized summation exercises where $s_1$ practiced correctly. Similarly, we can derive $H_{s_2}^{(1)}(R), H_{s_1}^{(1)}(W)$ and $H_{s_2}^{(1)}(W)$ following the same logic. Finally, Via Eq. (2) and Eq. (4), the $||H_{s_1} \u2013 H_{s_2}||_2$ can be calculated as $(||H_{s_1}^{(0)} \u2013 H_{s_2}^{(0)} + H_{s_1}^{(1)}(R) \u2013 H_{s_2}^{(1)}(R) + H_{s_1}^{(1)}(W) \u2013 H_{s_2}^{(1)}(W)||)$. Consequently, we can have the following observations:\n*   The first term $H_{s_1}^{(0)} \u2013 H_{s_2}^{(0)}$ is the same as Eq. (5) which reflects the disparity of individual information of $s_1$ and $s_2$.\n*   The second term $H_{s_1}^{(1)}(R) \u2013 H_{s_2}^{(1)}(R) + H_{s_1}^{(1)}(W) \u2013 H_{s_2}^{(1)}(W)$ captures the difference in the exercises that students $s_1$ and $s_2$ practiced correctly and incorrectly.\n*   The final $MND_{s_1,s_2}$ of RA-NCDM is the mean of the first and second terms which can be interpreted as a comparison of the differences between students from the aforementioned perspectives.\nFor instance, if both $s_1$ and $s_2$ have similar accuracy in their exercises, the first term will be quite small due to the monotonicity assumption in CD. However, if the exercises attempted by $s_1$ are more challenging compared to those of $s_2$, the second term will capture this disparity and consequently increase the final difference between $s_1$ and $s_2$. This suggests that the RGC is capable of capturing the differences in the exercises practiced by students, resulting in more distinctive Mas for each student.\nNotably, as the number of RGC layers increases, the perspectives for considering student differences also multiply. For instance, a two-layer RGC would further compare the differences with other students who have similar exercise performance as the current student. Therefore, by incorporating RGC, CDMs can assess student differences from multiple angles, thereby mitigating the oversmoothing issue. We will validate this conclusion in our ablation study in Section 5.3 and give visualizations of the learned Mas by T-SNE [29] in Section 5.4.\n4.2 Consistency Regularization Loss\nAfter the graph convolution by multiple RGC layers, we can get the final representation $H$ via Eq. (4). However, as we disentangle the response signal and capture student differences from various perspectives, it may exacerbate the notorious impact of the guess"}, {"title": null, "content": "and slip problem on CDMs [16, 36]. Previous methods, as referenced in [4], model the guess and slip probabilities for each exercise as fixed parameters. Evidently, this approach is somewhat brute-force and might overlook the individual impact of students. This is because the probability of guessing or slipping is likely to vary for each person across different exercises. Contrary to the aforementioned methods, in this paper, we treat guess and slip as noise edges within the ResG. Specifically, we flip the student-exercise edge type (i.e., from R to W or W to R) with a probability $p_f$ in the ResG. This noised version of the ResG, where some edges are flipped, is referred to as the flipped ResG, as illustrated in the left part of Figure 3. We aim for the representations derived from the original ResG and the flipped ResG to be similar, in order to ensure that the CDMs remain effective even when subject to the disturbances caused by guess and slip problem. It can be formulated as\n$L_{reg} = -\\sum_{s_i \\in S}log(exp(h_i^{\\top}h_i /\\tau))$, (7)\nwhere $h_i$ is the representation derived from flipped ResG, and $h_i$ denotes the similarity score the representation derived from the ResG and flipped ResG. $\\tau$ is the hyperparameter which controls the degree of smoothness utilized in various methods [38-40].\n4.3 Model Training\nGiven input embeddings, existing CDMs predict the performance of students practicing exercises, which can be formulated as\n$\\hat{y_{ij}} = MCD(H_{s_i}, H_{e_j}, H_c)$, (8)\nwhere MCD() denotes the CDMs, and H represents the input embedding that contains the representation of the student, exercises and concepts.\nTransformation Layer. To facilitate the integration of ORCDF with the majority of existing CDMs, we need to transform dimensions to suit the specific type of CDM in use. If the embedding size of CDMs is a latent dimension (e.g., KaNCD), we directly utilize H as the input embedding for incorporated CDMs. Otherwise (e.g., NCDM), we introduce a transformation layer which can be formulated as\n$H_t = HW_t + b_t$, (9)\nwhere $H_t$ will be employed as input embedding for incorporated CDMs and $W_t \\in R^{d \\times Z}$, $b_t \\in R^{(N+M+Z) \\times 1}$ are trainable parameters. As a result, unlike the previous RCD which sets d = Z, we can choose d as a latent dimension (e.g., 64). This significantly reduces the time complexity of graph convolution, a point that will be further analyzed in the subsequent subsection.\nJoint Training. The primary loss employed in CD task is to calculate the BCE loss between the model's predictions and the true response scores in a mini-batch. The aforementioned consistency regularization loss is incorporated jointly optimized with the CD task. The overall loss can be expressed as\n$L_{BCE} = -\\sum_{(s,e,r_{se}) \\in T}[r_{se}log\\hat{y_{se}} + (1-r_{se})log(1-\\hat{y_{se}})],$ (10)\n$L = L_{BCE} + \\lambda_{reg}L_{reg}$. (11)\n$\\lambda_{reg}$ is a hyperparameter that governs the relative importance of the consistency regularization loss."}, {"title": "4.4 Model Complexity Analysis", "content": "Theoretically, we reveal that the graph convolution in ORCDF takes O(4|E|Ld) time complexity. L denotes the number of RGC layers, and d denotes the dimension of embeddings. By leveraging the lightweight backbone and the transformation, our method is significantly lower in time complexity compared with the recent GNN-based approach RCD [6]. Specifically, RCD has the complexity of O(2|E|LZ\u00b2), where Z represents the number of concepts (d < Z). It suggests that ORCDF is more suitable for current online education scenario on ground of the increasing granularity of knowledge concepts. Indeed, ORCDF showcases a notable speed advantage, being up to 18 times faster than RCD on the Assist17 dataset (i.e., Z = 102). This dataset is collected from ASSISTment online tutoring systems and extensively utilized CDMs [12]. This improvement comes along with enhanced performance and lower GPU memory usage. For detailed information, please refer to Appendix A.\n5 EXPERIMENTS\nIn this section, we first describe four real-world datasets and evaluation metrics. Then, through extensive experiments, we aim to verify the superiority of ORCDF, which not only assists existing CDMs in mitigating the oversmoothing issue but also enhances the models' prediction performance and interpretability performance. To ensure the reliability and reproducibility of our experiments, they are independently repeated ten times with different seeds and our code is available at https://github.com/lswhim/ORCDF.\n5.1 Experimental Settings\nDatasets Description. The experiments are conducted using four real-world datasets: Assist17, EdNet-1, Junyi, and XES3G5M. The Assist17 dataset is provided by the ASSISTment web-based online tutoring systems [5] and are widely used for cognitive diagnosis tasks [30]. EdNet-1 [3] is the dataset of all student-system interaction collected over 2 years by Santa, a multi-platform Al web-based tutoring service with more than 780K users in Korea. Junyi [2] is an online math practice log dataset offered by Junyi Academy. XES3G5M [20] is a knowledge tracing benchmark dataset with auxiliary information. For more detailed statistics on these four datasets, please cf. Table 1. Notably, \u201cSparsity\u201d refers to the sparsity of the dataset, which is calculated as |T|/(|S||E|). \"Average Correct Rate\" represents the average score of students on exercises, and \"Q Density\" indicates the average number of concepts per exercise.\nEvaluation Metrics. To assess the efficacy of ORCDF, we utilize both score prediction, interpretability and oversmoothing metrics."}, {"title": "5.2 Student Performance Prediction", "content": "To showcase the effectiveness of ORCDF, we integrate it with various CDMs, as described in the subsequent part.\n*   IRT [8] is a classic model of latent factor CDMs, which uses one dimension $\\theta$ to model Mas and utilize logistic function as IF to predict the student score performance.\n*   MIRT [27] is a representative model of latent factor CDM, which uses multidimensional $\\Theta$ to model Mas.\n*   NCDM [30] is the first recent deep-learning based CDM which utilizes MLP to replace the traditional manually designed IFs."}, {"title": "5.3 Ablation Study", "content": "In this subsection, we scrutinize and evaluate each key individual component of ORCDF to comprehend their respective impacts and significance on the overall performance of the model. The ablation analysis is conducted using the following three versions.\n*   OR-w/o-rgc: This ablation of ORCDF does not integrate the response-aware graph convolution. Instead, it directly perform convolution on the entire response graph without decomposition.\n*   OR-w/o-reg: This ablation of ORCDF does not utilize the proposed consistency regularization loss $L_{reg}$.\n*   OL: It represents the base CDMs, which can be considered as the one without the inclusion of response-aware graph convolution and consistency regularization loss.\nDue to space constraints, we only present the ablation study using OR-NCDM as an example. This choice is motivated by the fact that NCDM is often employed as a classic CDM in downstream tasks. It is worth noting that the results from incorporating other CDMs are generally similar.\nExperimental Results. As indicated in Table 3, the proposed method outperforms the other two versions, suggesting that each component plays a significant role in enhancing the model's overall effectiveness. OR-w/o-rgc performs significantly worse, further validating the superiority of the proposed RGC in capturing the information within the response graph. We empirically find that although the consistency regularization loss is designed to alleviate the guess and slip problem, it not only improves the prediction and interpretability performance but also achieves a higher MND than the original version. This indicates that the guess and slip problem indeed exists in real-world scenarios, and addressing this problem is crucial for the effectiveness of CDMs."}, {"title": "5.4 In-Depth Analysis of ORCDF's Advantages", "content": "In this subsection, we analyze the proposed ORCDF from two perspectives: generalization performance and robustness performance.\nGeneralization Performance. To assess the efficacy of ORCDF in addressing the generalization issue, we conduct experiments on three datasets with varying test ratios $p_t = {10\\%, 20\\%, 30\\%, 40\\%, 50\\%}$. As $p_t$ increases which is consistent with [6], the generalization ability of CDMs is tested more stringently. As depicted in Figure 6 of Appendix B, with an increasing test ratio pt, the number of response logs used for training decreases. However, OR-NCDM consistently outperforms NCDM, illustrating that ORCDF can provide more accurate diagnosis results with fewer student response records. This"}, {"title": "5.6 Hyperparameter Analysis", "content": "Effect of L. As shown in Figure 10 in Appendix D, a larger L decreases the model's training speed, while a smaller L results in poor performance. The recommended values of L are 3 or 4, which can yield relatively good performance. Notably, as L increases, the MND does not continually decrease, a phenomenon that seems different from what is observed in graph representation learning. We contend this could be related to the heterogeneity of the response graph and the complexity of student interactions, which we leave for future work.\nThe Effect of $p_f$. As depicted in Figure 11 in Appendix D, OR-NCDM is influenced by the flip ratio parameter. A too high flip ratio introduces more noise, deteriorating the model's performance. Typically, a $p_f$ =0.15 yields better prediction performance, aligning with the established fact that everyone has a probability of guessing correctly or slipping, neither too high nor too low.\nThe Effect of $\\lambda_{reg}$. As illustrated in Figure 12 in Appendix D, this parameter controls the impact of guess and slip on model training, which varies across different datasets and requires tuning. It is observable that as the number of response logs in the dataset gradually increases, the optimal parameter value decreases. We recommend setting it to 1e-3.\nThe Effect of $\\tau$. As illustrated in Figure 13 in Appendix D, the temperature parameter $\\tau$ affects the similarity between representations learned from the response graph and those from the flipped response graph. As the size of the dataset gradually increases, the better temperature value also gradually increases. Here, we recommend choosing 0.5 when the number of students is small and opting for 3.0 when there is a larger student population."}, {"title": "6 CONCLUSION", "content": "This paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF), where most existing CDMs can be integrated and thus enhanced. We, for the first time, identify the oversmoothing in CD and then address it by learning students' Mas from multiple perspectives, utilizing the proposed response graph and response-aware graph convolution network. Besides, we reformulate the guess and slip problem as noise edges in the response graph and deign a loss function to alleviate the problem. As long as the oversmoothing is addressed in CD, it greatly helps provide distinctive and personalized diagnostic results for students and teachers. However, ORCDF, while effective, is still not sufficiently interpretable enough in the field of intelligent education. More interpretable methods are expected to be developed to mitigate the oversmoothing issue explicably in cognitive diagnosis."}, {"title": "APPENDIX", "content": "The appendix is organized as follows:\n*   Appendix A analyzes the ORCDF's time complexity and compares it with other frameworks.\n*   Appendix B presents the detailed settings of compared baselines and other details about student performance perdition.\n*   Appendix C presents the detailed settings of the downstream tasks", "https": "github.com/lswhim/ORCDF.\nA TIME COMPLEXITY ANALYSIS\nIn this section, we present a detailed time complexity analysis of our proposed model OR-NCDM. We compare our time complexity with that of RCD, as RCD is the only CDM based on GNN.\nTime Complexity Analysis of ORCDF. We take OR-NCDM as an exmaple. In OR-NCDM, we construct a response graph (ResG) G with three node and edge types based on I and Q. Given that we do not employ the non-linear activation and feature transformation usually found in GNNs, the time complexity can be straightforwardly computed as O(2|E|Ld) for RGC, where L denotes the number of RGC' layers. d stands for the size of the embeddings. Due to the need for computing representations through the flipped ResG, the total time complexity amounts to O(4|E|Ld).\nTime Complexity Analysis of RCD. In RCD, it construct three relation maps. Namely, an exercise-concept graph is constructed using Q and a student-exercise graph is formed using I. Given that RCD employs the graph attention network, which necessitates the computation of attention coefficients between every pair of connected nodes, its time complexity belongs to O(2|E|LZ\u00b2). Here"}]}