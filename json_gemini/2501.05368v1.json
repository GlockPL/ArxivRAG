{"title": "Developing a Foundation of Vector Symbolic Architectures Using Category Theory", "authors": ["Nolan Peter Shaw", "P. Michael Furlong", "Jeff Orchard", "Britt Anderson"], "abstract": "At the risk of overstating the case, connectionist approaches to machine learning, i.e. neural net- works, are enjoying a small vogue right now. However, these methods require large volumes of data and produce models that are uninterpretable to humans. An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of algebras on high-dimensional vector repre- sentations. They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform. While machine learning methods have benefited from category theoretical analyses, VSAs have not yet received similar treatment. In this paper, we present a first attempt at applying category theory to VSAs. Specifically, we conduct a brief literature survey demonstrating the lacking intersection of these two topics, provide a list of desiderata for VSAs, and propose that VSAs may be understood as a (division) rig in a category enriched over a monoid in Met (the category of Lawvere metric spaces). This final contribution suggests that VSAs may be generalised beyond current implementations. It is our hope that grounding VSAs in category theory will lead to more rigorous connections with other research, both within and beyond, learning and cognition.", "sections": [{"title": "1 Introduction", "content": "Machine learning has reached sufficient ubiquity that it requires little introduction. However, its adoption in software engineering has largely outpaced the rate at which we understand many of the learning algo- rithms we employ. Many researchers have identified this trend, and set out to better understand learning algorithms. Two main approaches have been taken. The first is to make complex models more \u201cinter- pretable\" to human users. The second, more recent approach, has been to improve our understanding of the mathematical foundations of machine learning. While the mechanics of differentiation and matrix multiplication have been well-understood for a long time, these provide little perspective on the emergent behaviour and structure of large-scale algorithms. Enter category theory.\nIn the past few years, considerable strides have been made in developing a categorical perspective on machine learning and deep learning [1, 16, 50]. A lot of this work has focused on supervised learning, where models learn through exposure to input/output training pairs. Here, lenses have been identified as providing the right structure for explaining the bi-directional flow of forward-propagating computations and backward-propagating errors. Parameterising these lenses yields a means to modify the computation of a model. The combined construction of a parametric lens results in a bicategory, whose 1-cells are composed layers of a network, while the 2-cells are the reparameterisations.\nLess progress has been made on understanding learning (and, more generally, cognition) from the perspective of information storage and association. Artificial feed-forward neural networks do a good job of capturing the hierarchical structures in brains, but brains are very complicated, and neural connectivity is often highly recurrent and seemingly chaotic. This complexity has prompted some researchers to look at neuronal computation at the level of \u201cpopulations\u201d of neurons and the dynamics contained within. Examples include reservoir computing [49, 57], neural engineering frameworks [5], and the focus of this paper: vector symbolic architectures, or \u201cVSAs\u201d.\nAs the name implies, VSAs are fundamentally vector spaces that aim to encode symbols with an associated \u201cmeaning\u201d as vectors. They possess a means of comparing how similar vectors are, and two operations-binding/unbinding and bundling\u2014that produce new vectors corresponding to more sophis- ticated symbols. Maintaining (near-)orthogonality is critical to ensuring that symbols can still be distin- guished from one another. The advantages of VSAs are evident in their ability to store information: given a vector space of dimension d, 2d near-orthogonal vectors can be encoded with a high degree of noise- resistance [23]. Further, for researchers who want to study learning as it occurs in biological brains, VSAs have been implemented in spiking neuron models[6, 5, 3, 14, 35, 21, 28, 54, 2, 47, 10, 9, 11], demonstrating their biological plausibility.\nVSAs also aim to describe cognitive features such as compositionality that are absent from some other connectionist models [7]. In spite of this ambition, discussion of VSAs in categorical terms is largely absent in the literature. This paper is meant to be an initial step in understanding VSAs from a category theoretic perspective. The first contribution of this paper is providing a survey of the literature that highlights the opportunity to apply category theory to VSAs. Our second contribution is a list of desiderata for properties of VSAs, similar to the work done in [1] for supervised learning. The third is to connect each desiderata to a categorical notion. The goal is twofold: to eventually describe a category, VSA, that embodies all the desired properties, such that mappings to and from this category in Cat could illuminate connections to other areas of research that are currently loose analogies (e.g. Hilbert spaces), and to also use known properties of the applicable categorical constructions to better understand how VSAs store and transform information."}, {"title": "2 Background", "content": "Vector Symbolic Architectures\u00b9 arose as an attempt to address the problem in cognitive science of how human behaviour can be characterised by rules and symbolic reasoning, but be implemented in neural networks, which rely on manipulating vector representations and engage in similarity-based reason- ing [51]. At their core, VSAs use randomly generated vectors to represent variables and the values they take on (also called \"slots\u201d and \u201cfillers\u201d). These base symbols are composed into higher-level represen- tations using a fixed set of algebraic operations."}, {"title": "2.1 VSA Definition", "content": "Individual VSAs can be described by the types of vectors that they operate on and corresponding op- erations: similarity, bundling, binding, unbinding, and braiding[17]. Frady et al. note that bundling"}, {"title": "2.3 Example Problem: Composing Functions", "content": "There are a number of ways to compute functions in a VSA. One natural way is to take the set definition of a function, $F = \\{(x, f(x)) | x \\in X\\}$ and encode it as a bundle over the domain and range of the function:\n$F = \\bigoplus_{x\\in X} (x \\otimes f(x))$,\t(1)\n$\\rightarrow f(x) \\approx x^{-1} \\otimes F$.\t(2)"}, {"title": "3 Motivation", "content": "There are good reasons to care about applying categories to VSAs\u2014not only for VSA practitioners, but also for applied category theorists. Here we focus on three motivating drives to formulate VSAs in the language of category theory. First, the intersection of categories and VSAs has not been deeply explored thus far. Exploring something that is unresearched is reason enough to study it, but filling this gap would provide practitioners with a rigorous language to discuss their work in new ways. For category theorists, this adds another \u201cmarble in the bucket\u201d of how people may benefit from category theory. The second motivation is to have a better means of abstracting and unifying VSA models. The benefits to design are clear for engineers, but it should be noted that that there may be some rich structures/construction to be found in VSAs for theoreticians. Third, describing VSAs as a category would allow for researchers to import results to and from other fields by way of functors, adjunctions, etc.\nIn addition to these main drivers, we also want to highlight that VSAs have gained popularity be- cause it is hypothesised that they or something very much like them\u2014may be implemented in the brain [39, 26, 40, 19, 5, 53, 4, 36]. The appeal of this is clear: the possibility that our brains are perform- ing operations in a categorical manner is a great argument for the intrinsic value of category theory\u00b3.\nMoreover, for those who are interested in technological advancements, the capacity to use VSAs as the \"firmware\u201d for neuromorphic hardware is certainly an appealing selling point. Together, we hope that talking about VSAs using category theory will bring the same benefits for neurocognitive models that it has for the theory of programming languages, machine learning, etc."}, {"title": "3.1 Absence in Literature", "content": "We found almost no research on category theory applied to VSAs. We consider our first contribution to be a literature survey that demonstrates this fact. Using a search string of\n(\\vector symbolic architecture\" OR \\vector symbolic algebra\" OR\n\\hyperdimensional computing\")\nAND\n\\category theory\""}, {"title": "3.2 Abstraction and Unifying Models", "content": "There are many choices for similarity measures, as well as binding and bundling operations, when build- ing VSAs. The mix-and-match nature of these various components leads to a daunting number of ar- chitectures for practitioners to choose from. Table 2 emphasises this point, through the size required to comprehensively catalogue the variety amongst VSAs. Notably, we were unable to format this table in such a way that it could be contained in the margins."}, {"title": "3.3 Formalising Analogies and Relationships with Other Fields", "content": "We are also motivated to expand on the many connections that researchers have made between VSAs and other areas of research. First, the bundling operation is often called superposition because simple vector addition acts no differently in VSAs compared to quantum mechanics. This analogy between VSAs and quantum mechanics has been reinforced by noting the similarity between Plate's holographic reduced representations (HRRs) and Hilbert spaces [60, 8, 12, 14]. Quantum probability, which uses an algebra very much like Smolensky's TPR, has been put forward as a modelling tool for cognition (e.g.,[42]). Stewart and Eliasmith observed that the quantum probability model could be implemented using the same kinds of neural processes that implement VSA operations [55]. Furlong and Eliasmith [14] put forward a VSA model of neurosymbolic probability that, in turn, resembles the methods developed in the Kernel Mean Embedding literature [34], which uses the same algebra as quantum probability. More recently, the various work by Widdows mentioned above draw a similar connection. Further, there is already some work on \"quantum natural language processing\" [59] that notes the deep relationship between quantum mechanics, vector-based representations, semantics, syntax, and category theory.\nSince category theory is the proper language for capturing analogy, our hope is that building a stronger categorical foundation of VSAs will make these comparisons more rigorous. Given the vast amount of research on all things quantum, VSA researchers stand to gain a lot from importing results from quantum mechanics/computing to their own field."}, {"title": "4 Desiderata", "content": "As our second contribution, we wish to characterise the fundamental properties that enable VSAs to represent concepts and perform computations.\n1. Similarity: The first necessary property of a VSA is a notion of how \u201csimilar\u201d two vectors are (or aren't). The need to do so is driven by two factors. First, we wish to quantify how closely related concepts are to one another. Second, we wish to have a means of ensuring that a \u201ccollision\" hasn't occurred\u2014that is that two vectors aren't so similar that they are difficult to distinguish. Cosine similarity, Hamming distance, and percent overlap are frequently used in practice.\n2. Two operations: Populating a vector space with a number of vectors with arbitrary associations is not particularly interesting. What we really want to do is perform some type of computation with these vectors. More specifically, we'd like to describe how to create new concepts from pre-existing ones. Starting from the existence of a measure, we'd like to have two operations: one that produces a vector that is similar to its operands, and one that creates a vector that is dissimilar. We've been referring"}, {"title": "5 Applying Categorical Constructions", "content": "We now discuss the categorical notions that capture each of the above desiderata. Our last contribution can be summarised by the following proposition:\nProposition. A VSA is a (division) rig in a category enriched over a monoid in Met, the category of Lawvere metric spaces (which are themselves categories enriched over the poset ([0,\u221e], \u2264)).\nWhat remains of this section is a deconstruction of this proposition, with each relevant notion exam- ined in more detail."}, {"title": "5.1 Enriched Categories", "content": "Enrichment [30] is way to represent more expressive structures about the morphisms of a given category, C, than provided by sets and membership. More specifically, if C is enriched over some underlying category K (which must be monoidal), then $hom_C(a,b) \\in ob(K)$ rather than $hom_C(a,b) \\in Set$ for all objects, a and b, in C. This collection of morphisms is thus called a \u201chom-object\" instead of a \"hom-set.\u201d\nTo model current VSAs, we require that some notion of distance be captured. This leads to selecting an object in Met, and unifies the choice of similarity measure with the choice of domain for the under- lying elements of our \u201cvectors\". Since VSAs fix their chosen metric, each object in Met will be the same for every hom-object in our underlying category. Therefore, it is sufficient to enrich over a single monoid in Met. This indicates that the full generality of enriched categories is unnecessary for capturing similarity. However, this presentation frames similarity as a fundamental structure on our morphisms. Excitingly, this suggests that there may be viable generalisations of VSAs that arise from the choice of which category we enrich over.\nIt is important to note that \"similarity\u201d is not the same thing as a metric. For instance, the cosine similarity function can range from [-1,1] with a value of one indicating two vectors that have the same orientation, rather than a value of zero. However, there is not a unique definition of \u201csimilarity\u201d, in contrast to a metric, and for this reason we focus on enrichment over metric spaces. We suspect that this does not severely hamper our formulation since many similarity measures, such as cosine similarity, can be used to recover a metric. Further, we believe that what is more important for VSAs is that there is some means of identifying some \"best\" candidate vector after performing certain operations. This is because algorithms implemented in VSAs may produce vectors that are not in the specified domain. For instance, after performing a lossy unbinding, we may need to de-noise the computed vector to determine the closest candidate vector. This process is possible when viewed as finding a limit in our chosen metric space.\""}, {"title": "5.2 Rigs and Division Rigs", "content": "The other key construction for this proposed definition is that of the division rig. Rigs are rings where elements don't necessarily have additive inverses (hence dropping the \"n\" from \"ring\"). In the context of VSAs, this corresponds to bundling lacking an inverse operation. Because we want to perform unbinding, we enforce that multiplicative inverses be present, yielding a division rig rather than a rig. We've chosen to necessitate that binding is invertible, as we believe that the distinction between binding and unbinding may be a form of \u201cconceptual obfuscation.\"\nThus we have a bimonoidal structure with operations $\\otimes$ (binding/unbinding) and $\\oplus$ (bundling), where $\\otimes$ distributes over $\\oplus$, 1 is the multiplicative identity, 0 is the additive identity (corresponding to the zero vector in practice), and addition is commutative. We must also impose that \u201cabsorption\" occurs\u2014that is that $0 \\otimes a = 0 = a \\otimes 0$. The intuitive connection to VSAs is clear in this case: bundling a vector with zero should result in a vector that's equivalent to the original.\nFor VSA practitioners, it may seem strange that there are no additive inverses. When bundling, we might expect that you can pull apart the bundle. However, more often what's really important is that you can compare a bundled object with one of its components to determine membership. Our hope is that the presence of a metric preserves this comparative capability without requiring the additional structure. For now we suggest this more general definition, and leave it as an open problem whether the loss of additive inverses leads to representations that are strictly less expressive."}, {"title": "6 Discussion and Future Work", "content": "The first point we wish to discuss is the absence of a specified category in the above proposition. Es- pecially given the name vector symbolic architecture, it is certainly natural to choose Vect as the under- lying category that we're enriching over. This exclusion is intentional and we're optimistic that VSAs may generalise beyond vector representations, especially because current VSAs are almost universally implemented only using vectors from lower dimensional manifolds of a larger vector space. That being said, we are not certain that this is the case; it may be the case that the structure of vectors and linear transformations is necessary to ensure that binding and bundling remain \u201cwell-behaved.\" However, our suggestion that VSAs might be modelled in other categories provides a clear direction for future work to implement VSAs using novel underlying categories.\nAs stated above, it is unclear to us whether enriching over an object in Met is sufficient reason to forego inverses w.r.t. the bundling operation. Building VSAs without additive inverses and attempting to recover the inverse (perhaps through the use of limits) is another way that future work can extend what is present here. Should it be the case that bundling is necessarily invertible, then we can modify the proposed definition of VSAs to be a division ring rather than a division rig.\nNext, there's clear work that needs to be done to better understand the exact relationship between binding, bundling, and similarity. As mentioned in Section 4, binding and bundling should really be de- pendent on our chosen measure. Thus, we hope that future work may be able to simplify the definition of VSAs even further. Perhaps what is needed to really understand VSAs is something like a \u201ctrimonoidal\" structure. Also, it seems to us that bundling is somewhat \u201cdual\u201d to binding: one produces a vector that is similar, while the other produces a vector that is dissimilar. Current intuitions also reflect this \u201cduality\u201d; binding and bundling being treated analogous to AND and OR, or multiplication and addition. Treating these operations as a division rig fails to capture some of this potential duality, and we hope that there is a way to further refine the interplay of these three components. Doing so may be the primary way to improve how principled we are when designing VSAs."}, {"title": "7 Conclusion", "content": "In summary, we've proposed the first description of VSAs explicitly using categorical language, to the best of our knowledge. We are certain that other researchers already have the categorical tools to re- fine our proposed definition and invite comments and critiques of this nature. We also provide many directions for future work in the hopes that this work is just a first step towards a truly comprehen- sive categorical foundation of VSAs. If successful, this would reveal an abundance of possible ways to generalise VSAs."}, {"title": "A Additional Examples of VSA Algorithms", "content": "Here we provide a few more examples of how VSAs can be used to perform computations and solve problems. First, we provide a very simple example of how to construct and deconstruct tuples. Then we present two more examples that also highlight reasons why practitioners may wish to use commutative binding versus non-commutative binding."}, {"title": "A.1 Constructing and Deconstructing Tuples", "content": "It is sometimes the case that individuals construct bundles that have more structure than an unordered set. We can, for example, create a tuple with two roles, first and second. To construct that tuple from two vectors, V1, V2, we would write\nw = (first $\\otimes$ V1) $\\oplus$ (second $\\otimes$ v2).\t(5)\nwhere binding takes precedence over bundling for order of operations (we've added parentheses simply for clarity).\nThen, to recover any element, we would unbind the corresponding role vector:\nv\u2081 $\\approx$ first $\\otimes$ w\nv2 $\\approx$ second $\\otimes$ w"}, {"title": "A.2 An Argument for Commutative Binding: Representing Numbers", "content": "In MAP and (F)HRR, integers and real valued data can be represented through iterative applications of operations, not unlike using a successor operation to construct the naturals. However, they may be more easily represented by simply binding a given vector iteratively. For example, using an initial vector, x, one can represent an integer, n \u2208 Z\u207a, in the (F)HRR algebras by binding x with itself n times, denoted:\n$\\varphi[n] := x \\otimes ... x$\tn\n$= \\bigotimes_{i=1}^n x,$\t(7)\nwhere p[n] denotes the vector representation of n.\nFor VSAs where vectors are their own inverse (e.g., BSC, MAP), the successor encoding is required. Integers are represented by applying the braiding operation an integer number of times to the initial vector:\n$\\varphi[n] := p \\otimes p ... p \\otimes x$\tn\n$= p^n \\otimes x.$\t(9)"}, {"title": "A.3 An Argument Against Commutative Binding: Composing Data Structures", "content": "Data structures can be defined in VSAs. We had previously discussed slot-filler relationships, like with the example of constructing a tuple in Section A.1, but it is also possible to construct data structures that have more detailed ordering, like lists or trees. A list of items (x1,...,xn) can be constructed using bundling and repeated application of a braiding operation, p as follows:\nlist(x1,...,xn) = x1 $\\oplus p \\otimes x2 + p \\otimes p \\otimes x3 + ... p^{n-1} \\otimes xn$\t(10)\nwhere pk indicates k \u2208 Z+ applications of the braiding operation. Individual elements can be selected This construction is made without relying on the binding operation, although such guarding is possible in VSAs with commutative binding operations. This is accomplished by binding with a \u201cguard vector\u201d, h, that has been iteratively bound with itself:\nlist(x1,...,xn) = x\u2081 $\\oplus$ h $\\otimes$ x2 + . . . $\\oplus$h^{n\u22121} $\\otimes$ Xn\u22121,\t(11)\nwhere $h^k$ is shorthand for $\\bigotimes_{i=1}^k h$. Problems with this approach become more apparent when constructing, for example, a binary tree. In this case, one would require two braiding operations, PL and PR, which are typically implemented as two different permutation matrices. Storing data elements in the leaves of a tree with depth 2 would be implemented:\ntree(X1,X2,X3, X4) = PLPLX1 $\\oplus$ PLPRX2$\\oplus$ PRPLX3 $\\oplus$ PRPRX4.\t(12)\nTo construct a tree using guard vectors we would similarity define h\u2081 and hr, and construct the tree:\ntree(X1,X2,X3, X4) = h\u2081 $\\otimes$ h\u2081 $\\otimes$ x1 $\\oplus$ hz $\\otimes$ hr $\\otimes$ x2 $\\oplus$ hr $\\otimes$ h\u2081 $\\otimes$ x3 $\\oplus$ hr $\\otimes$ hr $\\otimes$ X4.\t(13)\nBut if $\\otimes$ commutes, then h\u2081 $\\otimes$ hr = hr $\\otimes$ h\u2081, making it impossible to disambiguate the two middle leaves of the tree. Gayler [17] proposed a braiding operator for precisely this purpose. Plate suggested that multiple non-commutative, similarity preserving binding operators may exist, noting systematically per- muting one of the arguments of a commutative binding operation as one example [41, \u00a73.6.7]. Gosmann and Eliasmith [22] propose the VTB as one non-commutative binding operation, simplifying the number of operations required in the VSA to impose structure."}]}