{"title": "MODELANDO PROCESOS COGNITIVOS DE LA LECTURA NATURAL\nCON GPT-2", "authors": ["Bruno Bianchi", "Alfredo Umfurer", "Juan Kamienkowski"], "abstract": "El avance del campo del Procesamiento del Lenguaje Natural ha permitido desarrollar modelos de len-\nguaje con una gran capacidad de generar texto. Desde hace algunos a\u00f1os, la Neurociencia utiliza estos modelos para\ncomprender mejor los procesos cognitivos. En trabajos previos encontramos que modelos como Ngrams y redes LSTM\nson capaces de modelar parcialmente la Predictibilidad al ser utilizada como co-variable para explicar los movimientos\noculares de lectores. En el presente trabajo profundizamos esta l\u00ednea de investigaci\u00f3n utilizando modelos basados en\nGPT2. Los resultados muestran que \u00e9sta arquitectura logra mejores resultados que sus predecesores.", "sections": [{"title": "1. INTRODUCCI\u00d3N", "content": "El lenguaje es una de las caracter\u00edsticas distintivas del ser humano. Esta capacidad con la que contamos\nnos permite comunicarnos de forma compleja con el fin de expresar pensamientos [1]. A lo largo de la historia\ndiferentes campos cient\u00edficos, como la ling\u00fc\u00edstica, la psicolog\u00eda, y la neurociencia, han abordado el estudio\ndel lenguaje desde diferentes \u00f3pticas.\nEl campo del Procesamiento del Lenguaje Natural (PNL) ha evolucionado paulatinamente hacia algorit-\nmos y modelos que son capaces de replicar el lenguaje humano con una gran fidelidad [2, 3, 4, 5]. A pesar\nde que el objetivo general de \u00e9sta \u00e1rea no es comprender el lenguaje, sino simularlo computacionalmente,\nlos modelos del estado del arte nos brindan una herramienta que puede resultar \u00fatil para comprender el ce-\nrebro. Estas herramientas se suman a los esfuerzos realizados desde la neurociencia cognitiva, la cual aborda\nel estudio del lenguaje con el objetivo general de comprender con detalle los mecanismos cerebrales que lo\nhacen posible. Y, en \u00faltima instancia, generando un ciclo virtuoso en el que el conocimiento del lenguaje y\nlos mecanismos cerebrales asociados aportan al desarrollo de algoritmos [6].\nEn este sentido, se ha estudiado c\u00f3mo diferentes algoritmos del campo del PLN son capaces de estimar la\nPredictibilidad de una palabra. Esta variable se propone como un reflejo de cu\u00e1n probable es para un lector\nsaber una determinada palabra antes de leerla. Se estima mediante un experimento denominado cloze-task,\nen el que se le pide a los participantes que completen sentencias incompletas con una \u00fanica palabra [7], y\nse calcula como la frecuencia con que se responde la palabra originalmente continuaba el texto (cloze-Pred).\nRealizamos su modelado computacional (comp-Pred) mediante el uso de Modelos de Lenguaje Causales.\nEn las \u00faltimas d\u00e9cadas, diferentes investigaciones han modelado la cloze-Pred mediante modelos compu-\ntacionales simples. Hasta el momento todos estos intentos han logrado modelados parciales [8, 9, 10, 11, 12,\n13]. En los \u00faltimos a\u00f1os ha aumentado el inter\u00e9s en realizar estos estudios en base a modelos de aprendizaje\nprofundo. Hofmann y colaboradores [9, 11] entrenaron modelos de N-grams, Topic Models (LDA) y Redes\nNeuronales Recurrentes con textos de Wikipedia y subt\u00edtulos de pel\u00edculas. Al analizar la varianza de los tiem-\npos de fijaci\u00f3n explicada por estas probabilidades, concluyeron que los algoritmos computacionales pueden\nexplicar mejor los movimientos oculares que la cloze-Pred.\nEn estudios previos realizamos un an\u00e1lisis similar al de Hofmanm y colaboradores [10, 13]. Sin embargo,\nno analizamos directamente la varianza explicada por cada Predictibilidad (Cloze o Computacional), ya que\neste an\u00e1lisis puede pasar por alto que alguna de las variables utilizadas est\u00e9 capturando porciones diferentes\nde la variabilidad de los datos de movimientos oculares. En este sentido hemos observado que, a diferencia\nde la cloze-Pred, todas las comp-Pred analizadas (modelos utilizados: N-grams, LSA, word2vec, FastText,\nAWD-LSTM) capturan parte de la varianza explicada originalmente por la Frecuencia L\u00e9xica de las palabras.\nEste efecto se ve levemente reducido al utilizar un modelo de lenguaje basado en redes LSTM entrenado con\nWikipedia en espa\u00f1ol y reentrenado con textos de un dominio similar al de evaluaci\u00f3n (textos narrativos)."}, {"title": "2. M\u00c9TODOS", "content": "2.1. DATOS: TEXTOS, ESTIMACI\u00d3N DE CLOZE-PRED Y MOVIMIENTOS OCULARES\nEn el presente trabajo utilizaremos los datos de cloze-Pred y Movimientos Ocualres publicados previa-\nmente por Bianchi y colaboradores [10]. Se registraron los movimientos oculares (en particular la duraci\u00f3n\nde las fijaciones) de 36 participantes durante la lectura de 8 textos narrativos. Con estos datos se calcul\u00f3 el\nTiempo de Lectura en Primera Pasada (FPRT: suma de todas las fijaciones sobre una palabra en la primera\npasada) para cada palabra le\u00edda por cada sujeto. Resultando en 54,121 elementos totales, 1503 \u00b1 618 por\nsujeto, 6765 \u00b1 3226 por texto, 20 \u00b1 35 vistas por palabras, de 2588 palabras \u00fanicas. Esta variable ser\u00e1 utili-\nzada como variable dependiente en los modelos estad\u00edsticos. Por otro lado, se tomaron las predicciones para\ncada palabra (cloze-Pred) a trav\u00e9s de un experimento online. Se obtuvieron un promedio de 13 respuestas\npara cada palabra (rango: 8\u201337). Este corpus cuenta con cada uno de los 8 textos completamente anotados\ncon variables de inter\u00e9s: Distancia Sacada: cantidad de caracteres recorridos por el ojo previo a la fijaci\u00f3n\nactual; Longitud: cantidad de caracteres de la palabra fijada; Frecuencia: frecuencia l\u00e9xica de la palabra en\nLexEsp; Rel pos: posiciones relativas en la l\u00ednea, el texto o la oraci\u00f3n; Long:Frec: interacci\u00f3n entre Longitud\ny Frecuencia; Pred: predictibilidades Cloze y computacionales. Para m\u00e1s detalles, ver [10].\n2.2. PREDICTIBILIDAD COMPUTACIONAL\nEn el presente trabajo se introdujo la arquitectura de modelo de lenguaje GPT2, y se la compar\u00f3 con\nlas anteriores. Se utiliz\u00f3 el modelo entrenado por el consorcio DeepEsp, disponible en el repositorio de\nHuggingFace 1. Este modelo fue entrenado con 11.5GB de textos en espa\u00f1ol, entre Wikipedia (3.5GB) y\nlibros (8GB) de diferentes dominios (narrativo, historias cortas, teatro, poes\u00eda, ensayos, y popularizaci\u00f3n).\nPara el primer reentrenamiento de este modelo se utiliz\u00f3 el corpus de textos narrativos utilizado en trabajos\nprevios [10]. Al contar con un gran n\u00famero de textos bajo licencia, \u00e9ste corpus no es de acceso p\u00fablico. El\nmismo cuenta con 2082 cuentos (600MB), de variada naturaleza, tanto en el g\u00e9nero literario como en la na-\ncionalidad de los autores. Adicionalmente, realizamos un segundo reentenamiento, independiente del primero\n(es decir, partiendo nuevamente del modelo original), con textos descargados de blogs argentinos (28MB).\nEste \u00faltimo corpus se encuentra en preparaci\u00f3n para ser abierto a acceso p\u00fablico. Ambos reentrenamientos\nfueron realizados sobre todos los par\u00e1metros del modelo original.\n2.3. MODELOS LINEALES MIXTOS\nEn el presente trabajo tomamos como m\u00e9trica los resultados de ajustar Modelos Lineales Mixtos sobre\nel logaritmo de la variable FPRT (lme4 v.3.1-144, R v.3.6.3 [16]), obteniendo un t-valor por co-variable.\nAquellas con $|t - valor| > 2$ se consideran con un efecto significativo sobre la variable dependiente (ver [10]\npara m\u00e1s detalles).\nPartiremos de un modelo base (M0) que presenta variables t\u00edpicamente utilizadas en el estudios de los\nmovimientos oculares [10, 17]. Este modelo nos servir\u00e1 para comprender los efectos de las variables de\npredictibilidad humana y computacional (ver [10, 13] para una discusi\u00f3n de los efectos de estas variables).\nFinalmente, cada uno de los modelos que incluyen comp-Pred ser\u00e1 reanalizado con la cloze-Pred. Para\nesto, luego de ajustar el modelo obtendremos los residuos del mismo, al remover los efectos fijos estimados.\n\u00c9stos residuos ser\u00e1n utilizados en un nuevo modelo lineal mixto con la Cloze-Pred como \u00fanica variable."}, {"title": "3. RESULTADO", "content": "En la Tabla 1 se presentan los resultados de los Modelos Lineales Mixtos realizados para diferentes com-\nbinaciones de variables. Los modelos M0 a M6 representan modelos analizados en trabajos previos. El M0\nes el considerado Modelo Base, que cuenta con todas las variables no relacionadas con la Predictibilidad. El\nM1 cuenta con la cloze-Pred, es decir, es el modelo al que nos queremos acercar con las comp-Pred. El M2\ntiene al modelo Ngram, que es hasta el momento el que mejor ha logrado modelar la Predictibilidad-Cloze.\nEl M3, por su parte, es el mejor resultado obtenido en [13] al entrenar (con Wikipedia) y reentrenar (con un\ngran corpus de textos narrativos) una red de tipo AWD-LSTM [2].\nLos modelos correspondientes a las comp-Pred generadas por los modelos GPT2 (modelo original: M4,\nmodelo reentrenado con cuentos: M5, modelo reentrenado con blogs argentinos: M6) muestran resultados\nmuy similares entre ellos, y prometedores. En los tres modelos podemos ver que el t-valor obtenido es del\norden del t-valor para la comp-Pred de Ngram (M2). Sin embargo, al analizar el impacto de estas comp-Preds\nsobre el resto de las co-variables (comparando con M0) podemos observar que los mismos sufren menos\nvariaciones que en los M2 y M3. En particular, esto se observa en el efecto de la frecuencia l\u00e9xica. Mientras\nque en M2 \u00e9sta pierde significancia, en los modelos M4, M5 y M6 se observan t-valores por encima del\numbral. En este sentido, los modelos reentrenados parecen ser levemente mejor que el modelo original.\nPor su parte, en lo que respecta al an\u00e1lisis de la varianza residual, podemos ver que la cloze-Pred todav\u00eda\nes capaz de capturar varianza suficiente para tener un efecto significativo. En este sentido, el modelos que\nmejor se desempa\u00f1a en capturar esta varianza es el GPT2 original."}, {"title": "4. DISCUSI\u00d3N", "content": "El avance producido en los \u00faltimos a\u00f1os en los modelos del \u00e1rea del Procesamiento de Lenguaje (PLN)\nha permitido que los mismos sean utilizados por la Neurociencia Cognitiva para profundizar el conocimiento\nde los procesos subyacentes del lenguaje [6, 18, 19, 20]. En trabajos anteriores realizamos avances en com-\nprender c\u00f3mo estos tipo de modelos, en particular los modelos de lenguaje causales, son capaces de modelar\nla cloze-Pred como co-variables en modelos estad\u00edsticos utilizados para comprender los movimientos ocu-\nlares [10, 13]. En el presente trabajo profundizamos esta l\u00ednea de investigaci\u00f3n mediante el an\u00e1lisis de los\nresultados de la arquitectura GPT2 [3].\nLos resultados obtenidos en el presente trabajo muestran que el modelo GPT2 entrega las mejores comp-\nPreds hasta el momento. Este resultado fue independiente de si el modelo fue utilizado tal cual se encuentra en\nel repositorio de HuggingFace o si el mismo fue reentrenado con textos del dominio espec\u00edfico o de la variante\ndel espa\u00f1ol de los lectores. Es importante recalcar dos puntos con respecto al reentrenamiento. Por un lado,\nel entrenamiento original del modelo GPT2 utilizado incluy\u00f3 textos narrativos, por lo que el reentrenamiento\npuede no haber agregado informaci\u00f3n novedosa al modelo. Por otro lado, el reentrenamiento con el corpus\nen espa\u00f1ol rioplatense se realiz\u00f3 con un corpus muy chico (28MB) con respecto al corpus original (11GB).\nLa mejor\u00eda de las comp-Pred basadas en GPT2 con respecto a los modelos utilizados previamente (en\nparticular con AWD-LSTM) muestra los beneficios del uso de transformers, que parecer\u00eda permitir capturar\nmayor informaci\u00f3n ling\u00fc\u00edstica. En este sentido, los transformers tienen la ventaja de poder analizar todo el\ntexto de entrada a la vez, sin perder informaci\u00f3n de palabras lejanas. Adem\u00e1s, el aumento en la complejidad\nde estos modelos en los \u00faltimos a\u00f1os tambi\u00e9n ha permitido que la comprensi\u00f3n que logran estas redes sea a\u00fan\nmayor. Esto permitido, principalmente, por un aumento sostenido en la cantidad de par\u00e1metros internos de\nlos modelos. Mientras que nuestro modelo basado en AWD-LSTM cuenta con unos 107 par\u00e1metros, GPT2\ncuenta con 109, es decir dos \u00f3rdenes de magnitud de diferencia.\nEn el futuro cercano nos proponemos aumentar el corpus correspondiente a espa\u00f1ol rioplatense, con el\nobjetivo de profundizar el an\u00e1lisis de este tipo de reentrenamiento. Adem\u00e1s, tambi\u00e9n planificamos profundizar\ny diversificar los an\u00e1lisis correspondientes a utilizar los resultados de \u00e9ste tipo de modelos para mejorar la\ncomprensi\u00f3n de los procesos mentales."}]}