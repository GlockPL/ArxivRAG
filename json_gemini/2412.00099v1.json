{"title": "MIXTURE OF CACHE-CONDITIONAL EXPERTS FOR EFFICIENT MOBILE DEVICE INFERENCE", "authors": ["Andrii Skliar", "Ties van Rozendaal", "Romain Lepert", "Todor Boinovski", "Mart van Baalen", "Markus Nagel", "Paul Whatmough", "Babak Ehteshami Bejnordi"], "abstract": "Mixture of Experts (MOE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or \u201cexperts\u201d for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2\u00d7 speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Mixture of Experts (MoE) models have emerged as a powerful approach in large language models (LLMs), offer-ing advantages in scalability and efficiency (Fedus et al., 2022b;a). By selectively activating a subset of experts for each input, MoEs handle diverse data distributions and cap-ture complex patterns more effectively than traditional dense models. Recent models like GPT-4 (OpenAI et al., 2024),\nGemini (Team et al., 2024), and Mixtral (Jiang et al., 2024) provide evidence for MoEs' success in leveraging special-ized sub-networks to achieve superior performance. Simul-taneously, small language models (SLMs) with MoE archi-tectures, such as OLMOE (Muennighoff et al., 2024), Phi-3.5-MoE (Abdin et al., 2024), and Qwen-MoE (Qwen Team, 2024), have shown promise for deployment on memory-constrained devices due to their ability to maintain high performance with fewer active parameters per token.\nDespite their advantages, deploying MoE models on memory-constrained devices like smartphones and laptops presents significant challenges. One primary issue is that the parallelism techniques enhancing efficiency in server"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "In this section, we provide an overview of Mixture of Ex-perts (MoE) models, followed by a sensitivity analysis that highlights the impact of expert selection on model perfor-mance."}, {"title": "2.1 Preliminaries of MoE", "content": "The sparse MoE layer consists of a set of N expert networks {\u03951, \u03952,..., \u0395N}, and a routing network G which selects a subset of these experts based on their relevance to the input tokens. Let x \u2208 Rd denote the input token. The output of each MoE layer during inference is given by:\ny = \\sum w_i E_i(x) , (1)\nier[:K]\nr = \\text{argsort } w , (2)\nw = \\sigma(z) = \\sigma(G(x)) , (3)\nwhere z = G(x) denotes the logits assigned by the router to the experts, \\sigma denotes the softmax operation, and r is an expert ranking vector based on the weights, of which only the Top-K experts are selected. Note that the weights assigned to each expert might be re-normalized in some cases after the Top-K selection in Equation 1."}, {"title": "2.2 MoE deployment on memory-constrained devices", "content": "Deploying MoEs on memory-constrained devices, such as mobile phones, presents unique challenges due to limited memory resources. As shown in Figure 1 (Left), these devices typically feature a combination of DRAM and flash storage, each with distinct characteristics regarding speed and capacity. DRAM provides high bandwidth but limited capacity, while flash storage offers larger capacity at the expense of slower access speeds.\nIn the context of MoE models, not all parameters can fit into the limited DRAM available. Static parameters, such as attention weights that do not change during inference, can be stored permanently in DRAM. In contrast, given the dynamic nature of expert selection, only a subset of experts is loaded into DRAM at any given time. This subset acts as a cache, enabling faster access to frequently/previously used experts.\nA high cache hit rate is important because it means experts are quickly available from DRAM, reducing the overall la-tency. The cache hit rate measures the proportion of selected experts that are already present in the cache at the time of selection. Let the cache set C denote the indices of the ex-"}, {"title": "2.3 MoEs are less sensitive to dropping experts with lower scores", "content": "The potential for MoE models to become more cache-friendly without sacrificing task performance hinges on their robustness to variations in expert selection. To evaluate this assumption, we conduct a series of analyses on four differ-ent MoE architectures: DeepSeek-V2-Lite (DeepSeek-AI et al., 2024), Qwen1.5-MoE-A2.7B (Qwen Team, 2024), Phi-3.5-MoE (Abdin et al., 2024), and Mixtral-8x7B (Jiang et al., 2024). Our findings demonstrate that these models can tolerate some deviations from the router-selected experts as long as the experts with the highest weights are kept.\nThe left plot in Figure 2 illustrates the impact of completely removing (pruning) experts ranked at or above a specified index. Notably, the removal of the second highest ranked expert leads to a significant performance drop across all four models. However, for models with a larger number of active experts, such as Qwen and DeepSeek, performance quickly recovers, allowing for the pruning of higher-ranked experts with minimal performance loss.\nTo illustrate the significance of expert rank while maintain-ing the total number of activated experts, we replaced the expert ranked k with a randomly selected expert, as shown in the right plot of Figure 2. The results indicate that sub-stituting the most important expert (rank one), which has the highest score, drastically affects model performance. Replacing the second ranked expert also leads to severe degradation, but for the higher ranks, the effect of swapping experts is minimal.\nThis sensitivity analysis of MoEs shows that dropping ex-"}, {"title": "3 CACHE-AWARE EXPERT ROUTING", "content": "In this section, we present our primary techniques designed to enhance the consistency of routing decisions during to-ken generation, which in turn improves the expert cache hit rate and overall model throughput. We propose a general training-free method that can be applied to off-the-shelf MoE models to improve their efficiency for on-device de-ployment. Additionally, we introduce two simple baselines to increase cache hit rates: Max Rank and Cumulative Prob-ability Threshold."}, {"title": "3.1 Max Rank Approach", "content": "In Section 2.3 we demonstrated that MoEs exhibit some ro-bustness to the swapping of their experts with random ones. Building on this insight, we aim to deviate from the Top-K set of experts r[: K], selected by the router and instead promote experts that are already present in the cache C. We first define a general promotion operation that elevates the ranking of all experts in an ordered subset rsubset:\n\\text{promote}(r_{\\text{subset}}; r_{\\text{all}}) := r_{\\text{subset}} \\oplus (r_{\\text{all}} \\backslash r_{\\text{subset}}). (5)\nHere \\oplus denotes the concatenation operation and the set subtraction \\backslash preserves the order of it's left operand.\nA naive solution would be to simply promote all experts currently in the cache, but this approach does not take into account the expert probability assigned by the router. In-stead, we limit the promotion of elements in the cache by a maximum allowed rank M such that the experts with the lowest router probabilities are not chosen:\nr_{\\text{max-rank}} := \\text{promote}(r[: M] \\cap C; r). (6)\nAs observed in Section 2.3, certain Top-J experts are crucial for model performance. To ensure these experts are not overwritten by others in the cache, we always select them, even if they are not in the cache. This can be achieved by adding a second promotion operation:\n\\text{promote} (r[: J]; \\text{promote}(r[: M] \\cap C;r)). (7)\nFinally, the pseudocode for the max-rank algorithm with always keeping the Top-J experts is shown in Algorithm 1."}, {"title": "3.2 Cumulative Probability Threshold Approach", "content": "A limitation of the max-rank routing strategy is that it does not take into account the distribution of all expert probabil-ities G(x). For instance, an input where the most proba-ble expert has a very high probability will likely require a very low max-rank M to maintain good model performance. Conversely, for an input with uniformly distributed router probabilities, there is no strong expert preference, allowing for a very high max-rank M for better cache hit rates.\nWe address this issue in the cumulative probability threshold approach by dynamically choosing the max-rank M for every layer and every input x. We determine M by summing the sorted probabilities of the router outputs G(x) from"}, {"title": "3.3 Cache Prior Reranking", "content": "Although the cumulative threshold routing strategy dynam-ically adjusts the max-rank, it still imposes a hard limit beyond which experts are not promoted. This may be sub-optimal, especially for router distributions with long tails, which may be better suited for making cache-friendly deci-sions.\nTo address this, we use a cache prior that directly manipu-lates the router logits z = G(x) to increase the probability of selecting experts already present in cache. Importantly, we use the manipulated logits z' only to find a new ranking vector r' as defined in Equation 2. We still use the unmod-ified router logits to compute the expert weights used in Equation 1. Figure 3 provides an overview of our proposed cache-aware routing method. Let the bitmask m\u2081 \u2208 {0,1}N denote the state of the cache following the generation of the (t-1)-th token.\nAs with our other methods, we aim to ensure that the Top-J"}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate the effectiveness of our cache-aware routing method (dubbed Prior for simplicity) and compare it to our Pruning, Max-Rank, and Cumulative Sum Thresholding baselines described in Section 3. We imple-ment all methods on four SoTA MoE models: DeepSeek-V2-Lite (DeepSeek-AI et al., 2024), Qwen1.5-MoE-A2.7B (Qwen Team, 2024), Phi-3.5-MoE (Abdin et al., 2024), and Mixtral-8x7B (Jiang et al., 2024). The architectural details of these models are summarized in Table 1.\nThe MoE layers in DeepSeek-V2-Lite include 2 shared ex-perts activated for every token and 64 selectable experts, out of which 6 are activated per token, resulting in 2.4B active parameters per token out of a total of 15.7B parame-ters. Similarly, Qwen1.5-MoE-A2.7B has 4 shared experts and 60 selectable experts, with 4 activated for each token,"}, {"title": "4.1 Implementation Details", "content": "Keeping Top-J Experts As observed in Section 2.3, cer-tain Top-J experts are crucial for model performance. To prevent these experts from being overwritten by others in the cache, we always select them, even if they are not in the cache. In the cases of Phi and Mixtral, where only two ex-perts are activated, we use J = 1. For Qwen and DeepSeek, where the first major perplexity drop occurs at J = 2, we select J = 2.\nWe demonstrate the impact of the specific choice of Top-J in Figure 4. As shown, retaining the Top-J experts is crucial for the performance of baseline methods. Simultaneously, impact to our cache-aware routing method is less clear. How-ever, its effect on our cache-aware routing method is less pronounced. To ensure a fair comparison and simplify the approach, we use the same Top-J configuration across all"}, {"title": "4.2 Language Modeling Evaluation", "content": "In this section, we assess the effect of our proposed cache-aware routing method on the language modeling capabili-ties of MoE models. Figure 4 presents trade-off curves for perplexity versus cache miss rate of our Prior approach com-pared to the Pruning, Max-Rank, and Cumsum-threshold methods with a cache size equal to half the number of ex-perts. The results show that our approach consistently out-performs all baselines across all ranges of perplexity and cache miss rate. Among the baseline methods, Pruning per-forms the worst, indicating that cache-informed expert re-placement is essential for maintaining accuracy. Max-Rank consistently surpasses Pruning, while Cumsum-Threshold consistently outperforms Max-Rank. Ultimately, our Prior method Pareto-dominates all other approaches.\nWe also evaluate language modeling performance with only a quarter of experts cached, as shown in Figure 12 in Ap-pendix, Section B. Our method's robustness to different cache sizes makes it appealing for deployment across a range of devices."}, {"title": "4.3 Downstream Tasks Evaluation", "content": "Figure 5 and Figure 6 present results on the MMLU and GSM8K benchmarks, respectively. Our Prior method's"}, {"title": "4.4 On-Device Deployment", "content": "To evaluate the effectiveness of our cache-aware rout-ing technique in real-world scenarios, we deployed the"}, {"title": "4.5 Temporal Consistency of Expert Selections", "content": "In Table 2, we assess the impact of our method on aver-age cache lifetime, defined as the average number of time steps (tokens generated) an expert remains in memory be-fore being offloaded. Higher cache lifetime indicates better memory efficiency. We calculated this measure over the entire WikiText validation set, averaged across tokens.\nOur approach significantly extends cache lifetimes. For ex-ample, using LRU, each expert in both Qwen and DeepSeek models stays in memory for only 22 tokens, often less than a few seconds on modern devices. With our method, experts remain in memory for 5-9\u00d7 longer, reducing the frequency of weight loading from Flash and improving throughput."}, {"title": "4.6 Ablation Experiments", "content": "In this section, we perform ablations using the DeepSeek model with a cache size of 32, evaluated on Wikitext for the perplexity/cache miss rate trade-off."}, {"title": "5 RELATED WORK", "content": "Speculative Routing and Weight Preloading: Recently, various approaches have been developed to optimize MoE inference through speculative routing and weight preloading (Cui et al., 2023; Xue et al., 2024; Eliseev & Mazur, 2023; Du et al., 2024). These methods primarily focus on expert prefetching strategies to boost cache hit rates. For example, MoE-Infinity (Xue et al., 2024) uses look-ahead routing and caches the most frequently used experts. SiDA-MoE employs an offline-trained hash function to reduce expert selection overhead. However, SiDA-MoE's approximate caching results in a significant drop in perplexity, under-scoring the difficulty of maintaining accuracy. Additionally, (Eliseev & Mazur, 2023) combines lookahead routing and"}, {"title": "6 CONCLUSION", "content": "In this work, we introduced a novel training-free cache-aware expert routing approach for Mixture of Experts (MoE) models, tailored to enhance inference efficiency on memory-constrained hardware. By increasing cache hit rates through consistent routing, our method achieves a significant reduc-tion in latency while preserving model accuracy, demon-strating up to a 2\u00d7 speedup over regular routing with LRU caching techniques on mobile devices.\nOur method is highly adaptable, requiring no model re-training, making it suitable for a wide range of hardware configurations. This adaptability reduces deployment costs and facilitates the practical application of MoE models in real-world, resource-limited environments."}, {"title": "APPENDIX", "content": "A LRU THROUGHPUT\nIn Figure 9 we show on-device performance for various cache sizes. Similar to Section 4.4, we use best LRU perfor-mance as a reference point of 1\u00d7.\nOne can clearly see that increasing cache size beyond 30 in case of 4-bit quantized model on 12GB device (left) and beyond 45 in case of 8-bit quantized model on 16GB device (right) leads to decrease in performance. This is due to the fact that increasing cache size reduces available memory, causing the OS to reload uncached components (e.g., kv-cache, activations) for each token, which significantly slows down inference.\nGiven these results, we use same cache size of 30 for 4B and 45 for 8B in experiments in Section 4.4."}, {"title": "B SMALLER CACHE SIZES", "content": "Throughout all our experiments in the paper, we use a cache size that is 1/2 times the total number of experts. In Figure 12 we demonstrate performance of our approach for models with cache size equal to 1/4 of the total amount. Our Prior method is outperforming all other methods across the board. Note that it is doing so without any changes to method itself making it appealing for final user who might want to deploy it across a range of devices."}, {"title": "C LRU ABLATION", "content": "In Figure 10 we show that our approach (Top-Bottom) for ap-plying the LRU cache, as described in Section 4.6 performs better than Bottom-Top approach. However, the difference is fairly insignificant and thus the LRU strategy should not be considered an important factor."}, {"title": "D LEARNED PRIOR", "content": "In Section 3.3 we present our training-free Cache-Aware routing method. As shown throughout the paper, the pre-sented method is very strong and competitive across a range of scenarios.\nHowever, it is also possible to make the proposed method learnable. To do so, we propose learning the additive prior term using a small cache MLP layer conditioned on the cache state. Specifically, we implement this by employing a two-layer MLP that takes both the cache state and router logits as input, outputting a single bias vector. This bias is then added to the router logits and used for the next step. The loss function is optimized on the softmax outputs. We"}]}