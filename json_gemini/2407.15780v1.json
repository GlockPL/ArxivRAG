{"title": "Explaining Decisions in ML Models:\na Parameterized Complexity Analysis", "authors": ["Sebastian Ordyniak", "Giacomo Paesani", "Mateusz Rychlicki", "Stefan Szeider"], "abstract": "This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models. Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms. We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants. Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, Random Forests, and Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges. This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models. This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems.", "sections": [{"title": "Introduction", "content": "As machine learning (ML) models increasingly permeate essential domains, understanding their decision-making mechanisms has become central. This paper delves into the field of explainable AI (XAI) by examining the parameterized complexity of explanation problems in various ML models. We focus on models with accessible internal mechanisms, shifting away from the traditional black-box paradigm. Our motivation is rooted in establishing a comprehensive theoretical framework that illuminates the complexity of generating explanations for these models,\na task becoming increasingly relevant in light of recent regulatory guidelines that emphasize the importance of transparent and explainable AI [7, 24].\nThe need for transparency and accountability in automated decision-making drives the imperative for explain- ability in AI systems, especially in high-risk sectors. ML models, while powerful, must be demystified to gain trust and comply with ethical and regulatory standards. Formal explanations serve this purpose, providing a structured\nmeans to interpret model decisions [4, 13, 20].\nOur exploration focuses on two types of explanation problems, abductive and contrastive, in local and global contexts [20]. Abductive explanations [15], corresponding to prime-implicant explanations [28] and sufficient reason explanations [10], clarify specific decision-making instances, while contrastive explanations [16, 22], corre- sponding to necessary reason explanations [10], make explicit the reasons behind the non-selection of alternatives. The study of contrastive explanations goes back to the Lipton's work in 1990. Conversely, global explanations\n[15, 27] aim to unravel models' decision patterns across various inputs. This bifurcated approach enables a com- prehensive understanding of model behavior, aligning with the recent emphasis on interpretable ML [19].\nIn contrast to a recent study by Ordyniak et al. [25], who consider the parameterized complexity of finding explanations based on samples classified by a black-box ML model, we focus on the setting where the model\ntogether with its inner workings is available as an input for computing explanations. This perspective, initiated by Barcel\u00f3 et al. [1], is particularly appealing, as it lets us quantify the explainability of various model types based on the computational complexity of the corresponding explanation problems."}, {"title": "Preliminaries", "content": "For a positive integer i, we denote by [i] the set of integers {1, . . ., i}.\nParameterized Complexity (PC). We outline some basic concepts refer to the textbook by Downey and Fellows\n[12] for an in-depth treatment. An instance of a parameterized problem Q is a pair (x, k) where x is the main part and k (usually an non-negative integer) is the parameter. Qis fixed-parameter tractable (FPT) if it can be solved\nin time $f(k)n^c$ where n is the input size of x, c is a constant independent of k, and f is a computable function. If a problem has more then one parameters, then the parameters can be combined to a single one by addition. FPT denotes the class of all fixed-parameter tractable decision problems. XP denotes the class of all parameterized\ndecision problems solvable in time $n^{f(k)}$ where f is again a computable function. An fpt-reduction from one parameterized decision problem Q to another Q' is an fpt-computable reduction that reduces of Q to instances of Q' such that yes-instances are mapped to yes-instances and no-instances are mapped to no-instances. The param- eterized complexity classes W[i] are defined as the closure of certain weighted circuit satisfaction problems under"}, {"title": "Examples and Models", "content": "Challenging the notion of inherent opacity in ML models, our study includes Decision Trees (DTs), Decision Sets (DSs), Decision Lists (DLs), and Ordered Binary Decision Diagrams (OBDDs). Whereas DTs, DSs, and DLs\nare classical ML models, OBDDs can be used to represent the decision, functions of naive Bayes classifiers [5]. We also consider ensembles of all the above ML models; where an ensemble classifies an example by taking the majority classification over its elements. For instance, Random Forests (RFs) are ensembles of DTs.\nEach model presents distinct features affecting explanation generation. For example, the transparent structure\nof DTs and RFs facilitates rule extraction, as opposed to the complex architectures of Neural Networks (NNs) [18,\n27].\nContribution. This paper fills a crucial gap in XAI research by analyzing the complexity of generating expla- nations across different models. Prior research has often centered on practical explainability approaches, but a theoretical understanding still needs to be developed [14, 23]. Our study is aligned with the increasing call for theoretical rigor in AI [6]. By dissecting the parameterized complexity of these explanation problems, we lay the\ngroundwork for future research and algorithm development, ultimately contributing to more efficient explanation\nmethods in AI.\nSince most of the considered explanation problems are NP-hard, we use the paradigm of fixed-parameter\ntractability (FPT), which involves identifying specific parameters of the problem (e.g., explanation size, number of terms/rules, size/height of a DT, width of a BDD) and proving that the problem is fixed-parameter tractable concerning these parameters. By focusing on these parameters, the complexity of the problem is confined, making it more manageable and often solvable in uniform polynomial time for fixed values of the parameters. A significant part of our positive results are based on reducing various model types to Boolean circuits (BCs). This reduction is\ncrucial for the uniform treatment of several model types as it allows the application of known algorithmic results and techniques from the Boolean circuits domain to the studied models. It simplifies the problems and brings them\ninto a well-understood theoretical framework. For ensembles, we consider Boolean circuits with majority gates. In turn, we obtain the fixed-parameter tractability of problems on Boolean circuits via results on Monadic Second Order (MSO). We use extended MSO [2] to handle majority gates, which allows us to obtain efficient algorithmic solutions, particularly useful for handling complex structures.\nOverall, the approach in the manuscript is characterized by a mix of theoretical computer science techniques,\nincluding parameterization, reduction to well-known problems, and the development of specialized algorithms that exploit the structural properties of the models under consideration. This combination enables the manuscript to effectively address the challenge of finding tractable solutions to explanation problems in various machine learning models.\nFor some of the problems, we develop entirely new customized algorithms. We complement the algorithmic\nresults with hardness results to get a complete picture of the tractability landscape for all possible combinations of the considered parameters (an overview of our results are provided in Tables 2, 3, 4).\nIn summary, our research marks a significant advancement in the theoretical understanding of explainability\nin AI. By offering a detailed complexity analysis for various ML models, this work enriches academic discourse\nand responds to the growing practical and regulatory demand for transparent, interpretable, and trustworthy AI\nsystems.\nLet F be a set of binary features. An example $e : F \u2192 {0,1}$ over F is a {0,1}- assignment of the features in F. An example is a partial example (assignment) over F if it is an example over some subset F' of F. We denote by E(F) the set of all possible examples over F. A (binary classification) model\n$M : E(F) \u2192 {0,1}$ is a specific representation of a Boolean function over E(F). We denote by F(M) the set\nof features considered by M, i.e., F(M) = F. We say that an example e is a 0-example or negative example (1-example or positive example) w.r.t. the model M if M(e) = 0 (M(e) = 1). For convenience, we restrict our setting to the classification into two classes. We note however that all our hardness results easily carry over to\nthe classification into any (in)finite set of classes. The same applies to our algorithmic results for non-ensemble\nmodels since one can easily reduce to the case with two classes by renaming the class of interest for the particular explanation problem to 1 and all other classes to 0. We leave it open whether the same holds for our algorithmic results for ensemble models.\nDecision Trees. A decision tree (DT) T is a pair (T, \u03bb) such that T is a rooted binary tree and $\u03bb : V(T) \u2192 FU {0, 1}$ is a function that assigns a feature in F to every inner node of T and either 0 or 1 to every leaf node of\nT. Every inner node of T has exactly 2 children, one left child (or 0-child) and one right-child (or 1-child). The classification function $T : E(F) \u2192 {0,1}$ of a DT is defined as follows for an example $e \u2208 E(F)$. Starting at\nthe root of T one does the following at every inner node t of T. If $e(x(t)) = 0$ one continues with the 0-child of t and if $e(x(t)) = 1$ one continues with the 1-child of t until one eventually ends up at a leaf node l at which e is\nclassified as $\u03bb(l)$. For every node t of T, we denote by $\u03b1_t$ the partial assignment of F defined by the path from the root of T to t in T, i.e., for a feature f, we set $\u03b1_t(f)$ to 0 (1) if and only if the path from the root of T to t\ncontains an inner node t' with $\u03bb(t') = f$ together with its 0-child (1-child). We denote by L(T) the set of leaves of T and we set $L\u266d(T) = {l \u2208 L(T) | \u03bb(l) = b }$ for every $b \u2208 {0,1}$. Moreover, we denote by ||T|| (h(T)) the size (height) of a DT, which is equal to the number of leaves of T (the length of a longest root-to-leaf path in T). Finally, we let MNL(T) = min{|Lo|, |L1|}.\nDecision Sets. A term t over C is a set of literals with each literal being of the form $(f = z)$ where $f \u2208 F$ and $z \u2208 {0,1}$. A rule r is a pair $(t, c)$ where t is a term and $c \u2208 {0,1}$. We say that a rule $(t, c)$ is a c-rule. We say\nthat a term t (or rule (t, c)) applies to (or agrees with) an example e if $e(f) = z$ for every element $(f = z)$ of t. Note that the empty rule applies to any example.\nA decision set (DS) S is a pair (T, b), where T is a set of terms and $b \u2208 {0, 1}$ is the classification of the default rule (or the default classification). We denote by ||S|| the size of S which is equal to $(\\sum_{t \u2208 T} |t|) + 1$; the +1 is for the default rule. The classification function $S : E(F) \u2192 {0,1}$ of a DS S = (T, b) is defined by setting S(e) = b\nfor every example $e \u2208 E(F)$ such that no term in T applies to e and otherwise we set S(e) = 1 \u2212 b.\nDecision Lists. A decision list (DL) L is a non-empty sequence of rules $((r_1 = (t_1, c_1),...,r_\u2113 = (t_\u2113, c_\u2113))$, for some $\\ell \u2265 0$. The size of a DL L, denoted by ||L||, is equal to $\\sum_{i=1}^\u2113(|t_i + 1)$. The classification function"}, {"title": "Considered Problems and Parameters", "content": "We consider the following types of explanations (see Marques-Silva's survey [20]). Let M be a model, e an example over F(M), and let $c \u2208 {0, 1}$ be a classification (class). We consider the following types of explanations\nfor which an example is illustrated in Figure 1.\n\u2022 A (local) abductive explanation (LAXP) for e w.r.t. M is a subset $A \u2286 F(M)$ of features such that $M (e) = M(e')$ for every example e' that agrees with e on A.\n\u2022 A (local) contrastive explanation (LCXP) for e w.r.t. M is a set A of features such that there is an example e' such that $M(e') \u2260 M(e)$ and e' differs from e only on the features in A."}, {"title": "Algorithmic Results", "content": "Challenging the notion of inherent opacity in ML models, our study includes Decision Trees (DTs), Decision Sets (DSs), Decision Lists (DLs), and Ordered Binary Decision Diagrams (OBDDs). Whereas DTs, DSs, and DLs\nare classical ML models, OBDDs can be used to represent the decision, functions of naive Bayes classifiers [5]. We also consider ensembles of all the above ML models; where an ensemble classifies an example by taking the\nmajority classification over its elements. For instance, Random Forests (RFs) are ensembles of DTs.\nEach model presents distinct features affecting explanation generation. For example, the transparent structure\nof DTs and RFs facilitates rule extraction, as opposed to the complex architectures of Neural Networks (NNs) [18,\n27].\nContribution. This paper fills a crucial gap in XAI research by analyzing the complexity of generating expla- nations across different models. Prior research has often centered on practical explainability approaches, but a theoretical understanding still needs to be developed [14, 23]. Our study is aligned with the increasing call for theoretical rigor in AI [6]. By dissecting the parameterized complexity of these explanation problems, we lay the\ngroundwork for future research and algorithm development, ultimately contributing to more efficient explanation\nmethods in AI.\nSince most of the considered explanation problems are NP-hard, we use the paradigm of fixed-parameter tractability (FPT), which involves identifying specific parameters of the problem (e.g., explanation size, number\nof terms/rules, size/height of a DT, width of a BDD) and proving that the problem is fixed-parameter tractable concerning these parameters. By focusing on these parameters, the complexity of the problem is confined, making it more manageable and often solvable in uniform polynomial time for fixed values of the parameters. A significant part of our positive results are based on reducing various model types to Boolean circuits (BCs). This reduction is\ncrucial for the uniform treatment of several model types as it allows the application of known algorithmic results and techniques from the Boolean circuits domain to the studied models. It simplifies the problems and brings them into a well-understood theoretical framework. For ensembles, we consider Boolean circuits with majority gates.\nIn turn, we obtain the fixed-parameter tractability of problems on Boolean circuits via results on Monadic Second Order (MSO). We use extended MSO [2] to handle majority gates, which allows us to obtain efficient algorithmic\nsolutions, particularly useful for handling complex structures.\nOverall, the approach in the manuscript is characterized by a mix of theoretical computer science techniques,\nincluding parameterization, reduction to well-known problems, and the development of specialized algorithms that exploit the structural properties of the models under consideration. This combination enables the manuscript to\neffectively address the challenge of finding tractable solutions to explanation problems in various machine learning\nmodels.\nFor some of the problems, we develop entirely new customized algorithms. We complement the algorithmic\nresults with hardness results to get a complete picture of the tractability landscape for all possible combinations of the considered parameters (an overview of our results are provided in Tables 2, 3, 4).\nIn summary, our research marks a significant advancement in the theoretical understanding of explainability\nin AI. By offering a detailed complexity analysis for various ML models, this work enriches academic discourse\nand responds to the growing practical and regulatory demand for transparent, interpretable, and trustworthy AI\nsystems.\nIn this section, we will present our algorithmic results. We start with some general observations that are independent of a particular model type.\nTheorem 2. Let M be any model type such that M(e) can be computed in polynomial-time for M\u2208 M. M-\nLCXP|| parameterized by xp_size is in XP.\nProof. Let (M, e, k) be the given instance of M-LCXP|| and suppose that A \u2286 F(M) is a cardinality-wise mini- mal local contrastive explanation for e w.r.t. M. Because A is cardinality-wise minimal, it holds the example ea obtained from e by setting $e_A(f) = 1-e(f)$ for every $f \u2208 A$ and $e_A(f) = e(f)$ otherwise, is classified differently\nfrom e, i.e., $M(e) \u2260 M(e_A)$. Therefore, a set A \u2286 F(M) is a cardinality-wise minimal local contrastive explana- tion for e w.r.t. M if and only if $M(e) \u2260 M(e_A)$ and there is no cardinality-wise smaller set A' for which this is\nthe case. This now allows us to obtain an XP algorithm for M-LCXP|| as follows. We first enumerate all possible subsets A \u2286 F(M) of size at most k in time O(|F(M)|k) and for each such subset A we test in polynomial-time if $M(e_A) \u2260 M(e)$. If so, we output that (M, e, k) is a yes-instance and if this is not the case for any of the\nenumerated subsets, we output correctly that (M, e, k) is a no-instance.\nThe remainder of the section is organized as follows. First in Section 5.1, we provide a very general result about Boolean circuits, which will allow us to show a variety of algorithmic results for our models. We then provide our algorithms for the considered models in Subsections 5.2 to 5.4"}, {"title": "A Meta-Theorem for Boolean Circuits", "content": "Here, we present our algorithmic result for Boolean circuits that are allowed to employ majority circuits. In particular, we will show that all considered explanation problems are fixed-parameter tractable parameterized by the so-called rankwidth of the Boolean circuit as long as the Boolean circuit uses only a constant number\nof majority gates. Since our considered models can be naturally translated into Boolean circuits, which require majority gates in the case of ensembles, we will obtain a rather large number of algorithmic consequences from this result by providing suitable reductions of our models to Boolean circuits in the following subsections.\nWe start by introducing Boolean circuits. A Boolean circuit (BC) is a directed acyclic graph D with a unique sink vertex o (output gate) such that every vertex $v \u2208 V(D) \\ {o}$ is either:\n\u2022 an input gate (IN-gate) with no incoming arcs,\n\u2022 an AND-gate with at least one incoming arc,\n\u2022 an OR-gate with at least one incoming arcs,\n\u2022 a majority-gate (MAJ-gate) with at least one incoming arc and an integer threshold $t_v$, or\n\u2022 a NOT-gate with exactly one incoming arc.\nWe denote by IG(D) the set of all input gates of D and by MAJ(D) the set of all MAJ-gates of D. For an assignment $\u03b1: IG(D) \u2192 {0,1}$ and a vertex $v \u2208 V(D)$, we denote by val(v, D, \u03b1) the value of the gate v\nafter assigning all input gates according to \u03b1. That is, val(v, D, \u03b1) is recursively defined as follows: If v is an input gate, then val(v, D, \u03b1) = \u03b1(v), if v is an AND-gate (OR-gate), then $val(v, D, \u03b1) = \\Lambda_{n \u2208 N^-(v)} val(n, D, \u03b1)$ ($val(v, D, \u03b1) = \\bigvee_{n \u2208 N^-(v)} val(n, D, \u03b1)$), and if v is a MAJ-gate, then $val(v, D,\u03b1) = |{n|n \u2208 N^-(v) > val(n, D, \u03b1) = 1 }| > t_v$. Here and in the following $N^-(v)$ denotes the set of all incoming neighbors of v in D. We set O(D, \u03b1) = val(o, D, \u03b1). We say that D is a c-BC if c is an integer and D contains at most e MAJ-gates.\nWe consider Monadic Second Order (MSO\u2081) logic on structures representing BCs as a directed (acyclic) graph with unary relations to represent the types of gates. That is the structure associated with a given BC D has V(D) as its universe and contains the following unary and binary relations over V(D):\n\u2022 the unary relations I, A, O, M, and N containing all input gates, all AND-gates, all OR-gates, all MAJ- gates, and all NOT-gates of D, respectively,\n\u2022 the binary relation Exy containing all pairs x, y \u2208 V(D) such that (x, y) \u2208 A(D).\nWe assume an infinite supply of individual variables and set variables, which we denote by lower case and upper case letters, respectively. The available atomic formulas are Pg (\"the value assigned to variable g is contained\nin the unary relation or set variable P\"), Exy (\u201cvertex x is the head of an edge with tail y\u201d), x = y (equality),"}, {"title": "DTs and their Ensembles", "content": "fpt-reductions. Denoting by P the class of all parameterized decision problems solvable in polynomial time, and\nby paraNP the class of parameterized decision problems that are in NP and NP-hard for at least one instantiation of the parameter with a constant, we have $PC \u2286 FPT \u2286 W[1] \u2286 W[2] \u2286 \u22ef \u2286 XP \u2286 paraNPC paraNP$, where\nall inclusions are believed to be strict. If a parameterized problem is W[i]-hard under fpt-reductions (W[i]-h, for short) then it is unlikely to be FPT. co-C denotes the complexity class containing all problems from C with yes-instances replaced by no-instances and no-instances replaced by yes-instances.\nGraphs, Rankwidth, Treewidth and Pathwidth. We mostly use standard notation for graphs as can be found,\ne.g., in [11]. Let G = (V, E) be a directed or undirected graph. For a vertex subset V' \u2286 V, we denote by G[V'] the graph induced by the vertices in V' and by G \\ V' the graph G[V \\ V']. If G is directed, we denote by $N_G^-(v)$ ($N^-(v)$) the set of all incoming (outgoing) neighbors of the vertex $v \u2208 V$.\nLet G = (V, E) be a directed graph. A tree decomposition of G is a pair T = (T, \u03bb) with T being a tree and $\u03bb : V(T) \u2192 V(G)$ such that: (1) for every vertex $v \u2208 V$ the set { $t \u2208 V(T) | v \u2208 \u03bb(t)$ } is forms a non-empty\nsubtree of T and (2) for every arc e = (u, v) \u2208 E, there is a node $t \u2208 V(T)$ with $u, v \u2208 \u03bb(t)$. The width of T\nis equal to $max_{t\u2208V(T)} |\u03bb(t)| \u2013 1$ and the treewidth of G is the minimum width over all tree decompositions of\nG. T is called a path decomposition if T is a path and the pathwidth of G is the minimum width over all path\ndecompositions of G. We will need the following well-known properties of pathwidth, treewidth, and rankwidth.\nLemma 1 ([8, 26]). Let G = (V, E) be a directed graph and X \u2286 V. The treewidth of G is at most |X|plus the treewidth of G \u2013 X. Furthermore, if G has rankwidth r, pathwidth p and treewidth t, then $r < 3 \\cdot 2^{t-1} < 3 \\cdot 2^{p-1}$.\nHere, we present our algorithms for DTs and their ensembles. We start with a simple translation from DTs to BCs that allow us to employ Theorem 4 for DTs.\nLemma 5. There is a polynomial-time algorithm that given a DT T = (T, \u03bb) and a class c produces a circuit C(T, c) such that:\n(1) for every example e, it holds that T(e) = c if and only if (the assignment represented by) e satisfies C(T, c) and\n(2) rw(C(T,c)) \u2264 3. 2|MNL(T)|\nProof. Let T = (T, \u03bb) be the given DT and suppose that MNL(T) is equal to the number of negative leaves; the construction of the circuit C(T, c) is analogous if instead MNL(T) is equal to the number of positive leaves. We first construct the circuit D such that D is satisfied by e if and only if T(e) = 0. D contains one input gate gf and\none NOT-gate gf, whose only incoming arc is from gf, for every feature in F(T). Moreover, for every $l \u2208 L\u2080(T)$,\nD contains an AND-gate g\u2081, whose incoming arcs correspond to the partial assignment $\u03b1_l$, i.e., for every feature f assigned by $\u03b1_l$, g\u2081 has an incoming arc from gf if $\u03b1_l(f) = 1$ and an incoming arc from gf otherwise. Finally, D contains the OR-gate o, which also serves as the output gate of D, that has one incoming arc from g\u2081 for every\n$l \u2208 L\u2080$. This completes the construction of D and it is straightforward to show that D is satisfied by an example e\nif and only if T(e) = 0. Moreover, using Lemma 1, we obtain that D has treewidth at most |MNL(T)| +1 because\nthe graph obtained from D after removing all gates g\u2081 for every $l \u2208 L\u2080(T)$ is a tree and therefore has treewidth at\nmost 1. Therefore, using Lemma 1, we obtain that D has rankwidth at most $3 \\cdot 2^{|MNL(T)|}$. Finally, C(T, c) can now be obtained from D as follows. If c = 0, then C(T, c) = D. Otherwise, C(T, c) is obtained from D after adding one OR-gate that also serves as the new output gate of C(T, c) and that has only one incoming arc from o.\nWe now provide a translation from DTMAJS to 1-BCs that will allow us to obtain tractability results for DTMAJS.\nLemma 6. There is a polynomial-time algorithm that given a $DT_{MAJ} F$ and a class c produces a circuit C(F, c) such that:\n(1) for every example e, it holds that F(e) = c if and only if (the assignment represented by) e satisfies C(F, c) and"}, {"title": "DSs, DLs and their Ensembles", "content": "(2) rw(C(F,c)) \u22643.2\u2211T\u2208F |MNL(T)|\nProof. We obtain the circuit C(F, c) from the (not necessarily disjoint) union of the circuits C(T, c) for every $T \u2208 F$, which we introduced in Lemma 5, after adding a new MAJ-gate with threshold $[|F|/2] + 1$, which also serves as the output gate of C(F, c), that has one incoming arc from the output gate of C(T, c) for every $T \u2208 F$. Clearly, C(F, c) satisfies (1). Moreover, to see that it also satisfies (2), recall that every circuit C(T, c) has only MNL(T) gates apart from the input gates, the NOT-gates connected to the input gates, and the output gate. Therefore, after removing MNL(T) gates from every circuit C(T, c) inside C(F, c), the remaining circuit is a tree, which together with Lemma 1 implies (2).\nTheorem 7. Let P \u2208 {LAXP, LCXP, GAXP, GCXP}. $DT_{MAJ}-P_\u2081$(ens_size+ mnl_size) and therefore also $DT_{MAJ}- P_\u2081$(ens_size + size_elem) is FPT.\nProof. The theorem follows immediately from Theorem 4 together with Lemma 6.\nWe now give our polynomial-time algorithms for DTs. We start with the following known result for constrastive\nexplanations.\nTheorem 8 ( [1, Lemma 14]). There is a polynomial-time algorithm that given a DT T and an example e outputs a (cardinality-wise) minimum local contrastive explanation for e w.r.t. T or no if such an explanation does not exist. Therefore, DT-LCXP|| can be solved in polynomial-time.\nThe following auxiliary lemma provides polynomial-time algorithms for testing whether a given subset of features A or partial example e' is a local abductive, global abductive, or global contrastive explanation for a given example e or class c w.r.t. a given DT T.\nLemma 9. Let T be a DT, let e be an example and let c be a class. There are polynomial-time algorithms for the following problems:\n(1) Decide whether a given subset $A \u2286 F(T)$ of features is a local abductive explanation for e w.r.t. T.\n(2) Decide whether a given partial example e' is a global abductive/contrastive explanation for c w.r.t. T.\nProof. Let T be a DT, let e be an example and let c be a class. Note that we assume here that T does not have any\ncontradictory path, i.e., a root-to-leaf path that contains that assigns any feature more than once. Because if this was not the case, we could easily simplify T in polynomial-time.\nWe start by showing (1). A subset $A \u2286 F(T)$ of features is a local abductive explanation for e w.r.t. T if and only if the DT $T_{e|A}$ does only contain T(e)-leaves, which can clearly be decided in polynomial-time. Here, $e|A$ is the partial example equal to the restriction of e to A. Moreover, $T_{e'}$, for a partial example e' is the DT obtained from T after removing every 1 \u2013 e'(f)-child from every node t of T assigned to a feature f for which e' is defined.\nSimilarly, for showing (2), observe that partial example (assignment) $\u03c4 : F \u2192 {0,1}$ is a global abductive explanation for c w.r.t. T if and only if the DT T_{\u03c4} does only contain c-leaves, which can clearly be decided in polynomial-time.\nFinally, note that a partial example $\u03c4 : F \u2192 {0,1}$ is a global contrastive explanation for c w.r.t. T if and only if the DT $T_{\u03c4}$ does not contain any c-leaf, which can clearly be decided in polynomial-time.\nUsing dedicated algorithms for the inclusion-wise minimal variants of LAXP, GAXP, and GCXP together with Theorem 8, we obtain the following result.\nTheorem 10. Let P \u2208 {LAXP, LCXP, GAXP, GCXP}. DT-P\u2282 can be solved in polynomial-time.\nProof. Note that the statement of the theorem for DT-LCXPC follows immediately from Theorem 8. Therefore, it suffices to show the statement of the theorem for the remaining 3 problems.\nLet (T, e) be an instance of DT-LAXPC. We start by setting $A = F(T)$. Using Lemma 9, we then test for\nany feature f in A, whether A \\ {f} is still a local abductive explanation for e w.r.t. T in polynomial-time. If so, we repeat the process after setting A to A \\ {f} and otherwise we do the same test for the next feature $f \u2208 A$.\nFinally, if A \\ {f} is not a local abductive explanation for every $f \u2208 A$, then A is an inclusion-wise minimal local abductive explanation and we can output A.\nHere, we present our algorithms for DSs, DLs and their ensembles. Our first algorithmic result is again based on our meta-theorem (Theorem 4) and a suitable translation from DSMAJ and DLMAJ to a Boolean circuit.\nLemma 13. There is a polynomial-time algorithm that given a DS/DL L and a class c produces a circuit C(L, c) such that:"}, {"title": "OBDDs and their Ensembles", "content": "To ensure that every example obtains some classification", "that": "n\u2022 s is a source vertex that can (but does not have to) be equal to to or t\u2081", "\u03c1": "V(D) \\ {t_0", "B": "E(F) \u2192 {0, 1}$ of B is given by setting B(e) = b if P(e) ends in to. We denote by ||B|| the size of B, which is equal to |V(D)|. We say that B is an OBDD if every path in B contains features in the same order. Moreover, B is a complete OBDD if every maximal path contains the same set of features. It is known that every OBDD can be transformed"}]}