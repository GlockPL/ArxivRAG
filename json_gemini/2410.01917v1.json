{"title": "PROVABLY ACCURATE ShapleY VALUE ESTIMATION VIA LEVERAGE SCORE SAMPLING", "authors": ["Christopher Musco", "R. Teal Witter"], "abstract": "Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model pre-dictions to specific input features. However, computing Shapley values exactly is expensive: for a general model with n features, O(2n) model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing Leverage SHAP, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just O(nlogn) model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing leverage score sampling, a powerful regression tool. Beyond theoretical guarantees, we show that Lever-age SHAP consistently outperforms even the highly optimized implementation of Kernel SHAP available in the ubiquitous SHAP library [Lundberg & Lee, 2017].", "sections": [{"title": "INTRODUCTION", "content": "While AI is increasingly deployed in high-stakes domains like education, healthcare, finance, and law, increasingly complicated models often make predictions or decisions in an opaque and un-interpretable way. In high-stakes domains, transparency in a model is crucial for building trust. Moreover, for researchers and developers, understanding model behavior is important for identi-fying areas of improvement and applying appropriate safe guards. To address these challenges, Shapley values have emerged as a powerful game-theoretic approach for interpreting even opaque models (Shapley, 1951; \u0160trumbelj & Kononenko, 2014; Datta et al., 2016; Lundberg & Lee, 2017). These values can be used to effectively quantify the contribution of each input feature to a model's output, offering at least a partial, principled explanation for why a model made a certain prediction.\nConcretely, Shapley values originate from game-theory as a method for determining fair 'payouts' for a cooperative game involving n players. The goal is to assign higher payouts to players who contributed more to the cooperative effort. Shapley values quantify the contribution of a player by measuring how its addition to a set of other players changes the value of the game. Formally, let the value function v : 2[n] \u2192 R be a function defined on sets S \u2286 [n]. The Shapley value for player i is:\n$\\phi_i = \\sum_{S \\subset [n] \\setminus {i}} \\frac{1}{n \\binom{n-1}{|S|}} (v(S \\cup {i}) - v(S))$\n(1)\nThe denominator weights the marginal contribution of player i to set S by the number of sets of size |S, so that the marginal contribution to sets of each size are equally considered. With this weight-ing, Shapley values are known to be the unique values that satisfy four desirable game-theoretic properties: Null Player, Symmetry, Additivity, and Efficiency (Shapley, 1951). For further details on Shapley values and their theoretical motivation, we refer the reader to Molnar (2024).\nA popular way of using Shapley values for explainable AI is to attribute predictions made by a model f: Rn \u2192 R on a given input x \u2208 Rn compared to a baseline input y \u2208 Rn (Lundberg & Lee, 2017). The players are the features and v(S) is the prediction of the model when using the features"}, {"title": "EFFICIENT SHAPLEY VALUE COMPUTATION", "content": "Naively, computing all n Shapley values according to Equation 1 requires O(2n) evaluations of v (each of which involves the evaluation of a learned model) and O(2n) time. This cost can be reduced in certain special cases, e.g. when computing feature attributions for linear models or decision trees (Lundberg et al., 2018; Campbell et al., 2022; Amoukou et al., 2022; Chen et al., 2018).\nMore often, when v is based on an arbitrary model, like a deep neural network, the exponential cost in n is avoided by turning to approximation algorithms for estimating Shapley values, including sampling, permutation sampling, and Kernel SHAP (Strumbelj & Kononenko, 2010; Lundberg &\nLee, 2017; Mitchell et al., 2022). The Kernel SHAP method is especially popular, as it performs well in practice for a variety of models, requiring just a small number of black-box evaluations of v to obtain accurate estimates to $1,..., \u03a6\u03b7. The method is a corner-stone of the ubiquitous SHAP library for explainable AI based on Shapley values (Lundberg & Lee, 2017).\nKernel SHAP is based on an elegant connection between Shapley values and least squares regression (Charnes et al., 1988). Specifically, let [n] denote {1, ..., n}, (\u00d8) denote the empty set, and 1 denote an all 1's vector of length n. The Shapley values \u00a2 = [1,..., \u03a6\u03b7] \u2208 Rn are known to satisfy:\n$\\phi = \\underset{x:(x,1)=v([n])-v(0)}{\\text{arg min}} ||Zx - y||^2,$\n(2)\nwhere Z\u2208 R2\"\u22122\u00d7n is a specific structured matrix whose rows correspond to sets S \u2286 [n] with 0 < |S| < n, and y \u2208 R2\"\u22122 is vector whose entries correspond to values of v(S). (We precisely define Z and y in Section 2.)\nSince solving the regression problem in Equation 2 directly would require evaluating v(S) for all 2n\u22122 subsets represented in y, Kernel SHAP solves the problem approximately via subsampling. Concretely, for a given number of samples m and a discrete probability distribution p\u2208 [0,1]2-2 over rows in Z, consider a sampling matrix S \u2208 Rm\u00d72\"\u22122, where each row of S is 0 except for a single entry 1/\u221aPj in the jth entry with probability pj. The Kernel SHAP estimate is given by\n$\\hat{\\phi} = \\underset{x:(x,1)=v([n])-v(0)}{\\text{arg min}} ||SZx - Sy||^2.$\n(3)\nImportantly, computing this estimate only requires at most m evaluations of the value function v, since Sy can be constructed from observing at most m entries in y. Value function evaluations typically dominate the computational cost of actually solving the regression problem in Equation 3, so ideally m is chosen as small as possible. In an effort to reduce sample complexity, Kernel SHAP does not sample rows uniformly. Instead, the row corresponding to subset S is chosen with proba-bility proportional to:\nw(|S|) = {n \\choose |S|} {|S|(n\u2212|S|)}\n\u22121.\n(4)\nThe specific motivation for this distribution is discussed further in Section 2, but the choice is intu-itive: the method is more likely to sample rows corresponding to subsets whose size is close to 0 or n, which aligns with the fact that these subsets more heavily impact the Shapley values (Equation 1). Ultimately, however, the choice of w(|S|) is heuristic."}, {"title": "OUR CONTRIBUTIONS", "content": "Despite its ubiquity in practice, to the best of our knowledge, no non-asymptotic theoretical accuracy guarantees are known for Kernel SHAP when implemented with m < 2\u03b7 2 row samples (which corresponds to m evaluations of the value function, v). We address this issue by proposing Lever-age SHAP, a light-weight modification of Kernel SHAP that 1) enjoys strong theoretical accuracy guarantees and 2) consistently outperforms Kernel SHAP in experiments.\nLeverage SHAP begins with the observation that a nearly optimal solution to the regression problem in Equation 2 can be obtained by sampling just O(n) rows with probability proportional to their statistical leverage scores, a natural measure for the \u201cimportance\u201d or \u201cuniqueness\u201d of a matrix row (Sarl\u00f3s, 2006; Rauhut & Ward, 2012; Hampton & Doostan, 2015; Cohen & Migliorati, 2017).2 This fact immediately implies that, in principal, we should be able to provably approximate & with a nearly-linear number of value function evaluations (one for each sampled row).\nHowever, leverage scores are expensive to compute, naively requiring at least O(2n) time to write down for a matrix like Z with O(2n) rows. Our key observation is that this bottleneck can be avoided in the case of Shapley value estimation: we prove that the leverage scores of Z have a simple closed form that admits efficient sampling without ever writing them all down. Concretely, we show that the leverage score of the row corresponding to any subset S C [n] is proportional to {n \\choose |S|}\u22121.\nThis suggests a similar, but meaningfully different sampling distribution than the one used by Kernel SHAP (see Equation 4). Since all subsets of a given size have the same leverage score, we can efficiently sample proportional to the leverage scores by sampling a random size s uniformly from {1, ..., n - 1} then selecting a subset S of size s uniformly at random.\nTheorem 1.1. For any e > 0 and constant 8 > 0, the Leverage SHAP algorithm uses m\nO(nlogn + n/\u0454) evaluations of v in expectation\u00b3 and O(mn\u00b2) additional runtime to return es-timated Shapley values & satisfying (6, 1) = v([n]) \u2013 v(\u00d8) and, with probability 1 \u2013 \u03b4,\n||Z \u2013 y||2 \u2264 (1 + \u03b5)||Z\u0444 \u2013 y||2.\n(5)\nIn words, Theorem 1.1 establishes that, with a near-linear number of function evaluations, we can compute approximate Shapley values whose objective value is close to the true Shapley values. We also require O(mn\u00b2) additional runtime to solve the linearly constrained regression problem in Equation 3, although this cost can be reduced in practice using, e.g. iterative methods.\nBy leveraging the fact that Z is a well-conditioned matrix, the bound in Equation 5 also implies a bound on the average squared error, || \u2013 ||2, which is provided in Section 4.\nBeyond our theoretical results, we also show that leverage score sampling can be naturally combined with paired sampling, without sacrificing theoretical guarantees. As for Kernel SHAP, doing so improves its performance in experiments. Moreover, we show that a natural \"without replacement\""}, {"title": "RELATED WORK", "content": "Shapley Values Estimation. As discussed, naively computing Shapley values requires an expo-nential number of evaluations of v. While more efficient methods exist for certain structured value functions (see references in Section 1.1), for generic functions, faster algorithms involve some sort of approximation. The most direct way of obtaining an approximation to is approximate the summa-tion definition of Shapley values (Equation 1), which involves O(2n) terms for each subset of [n], with a random subsample of subsets (Castro et al., 2009; Strumbelj & Kononenko, 2010; \u0160trumbelj\n& Kononenko, 2014). The first methods for doing so use a different subsample for each player i.\nA natural alternative is to try selecting subsets in such a way that allows them to be reused across multiple players (Ill\u00e9s & Ker\u00e9nyi, 2019; Mitchell et al., 2022). However, since each term in the summation involves both v(S) and v(SU {i}), it is difficult to achieve high levels of sample reuse when working with the summation definition. One option is to split the sum into two, and separately estimate Es\u2286[n]\\{i} v(S\u222a{i})/([51) and Es\u2286[n]\\{i} v(S)/([5]). This allows substantial subset reuse, but tends to perform poorly in practice due to high variance in the individual sums Wang & Jia (2023). By solving a global regression problem that determines the entire o vector, Kernel SHAP can be viewed as a more effective way of reusing subsets to obtain Shapley values for all players.\nIn addition to the sampling methods referenced above, Covert & Lee (2021) propose a modified Kernel SHAP algorithm, prove it is unbiased, and compute its asymptotic variance. However, they find that the modified version performs worse than Kernel SHAP. Additionally, we note that some recent work in the explainable AI setting takes advantage of the fact that we often wish to evaluate feature impact for a large number of different input vectors x \u2208 Rn, which each induce their own set function v. In this setting, it is possible to leverage information gathered for one input vector"}, {"title": "BACKGROUND AND NOTATION", "content": "Notation. Lowercase letters represent scalars, bold lowercase letters vectors, and bold uppercase letters matrices. We use the set notation [n] = {1, . . ., n} and () = {}. We let 0 denote the all zeros vector, 1 the all ones vector, and I the identity matrix, with dimensions clear from context. For a vector x, xi is the ith entry (non-bold to indicate a scalar). For a matrix X \u2208 Rp\u00d7n, [X]i \u2208 R1\u00d7n is the ith row. For a vector x, ||x||2 = (\u03a3i x2)1/2 denotes the Euclidean (l2) norm and ||\u00d7||1 = \u2211i | Xi | is the l\u2081 norm. For a matrix X \u2208 Rn\u00d7m, X+ denotes the Moore-Penrose pseudoinverse.\nPreliminaries. Any subset S \u2286 [n] can be represented by a binary indicator vector z \u2208 {0, 1}n, and we use use v(S) and v(z) interchangeably. We construct the matrix Z and target vector y appearing in Equation 2 by indexing rows by all z \u2208 {0,1}n with 0 < ||2||1 < n:\n\u2022 Let Z \u2208 R2\"\u22122\u00d7n be a matrix with [Z]z = \u221aw(||z||1)z.\n\u2022 Let y \u2208 R2\u201d \u22122 be the vector where [y]z = \u221aw(||z||1)(v(z) \u2013 v(0)).\nAbove, w(s) = {n \\choose s} {|S|(n \u2212 s)}\n\u22121 is the same weight function defined in Equation 4.\nAs discussed, the Kernel SHAP method is based on an equaivalence between Shapley values and the solution of a contrained regression problem involving Z and y. Formally, we have:\nLemma 2.1 (Equivalence (Lundberg & Lee, 2017; Charnes et al., 1988)).\n$\\phi = \\underset{x:(x,1)=v(1)-v(0)}{\\text{arg min}} || Zx - y||^2$\n(6)\n= arg min\n\u03a3w(||2||1) . ((z, x) \u2013 (v(z) \u2013 v(0)))2 .\n(7)\nx:(x,1)=v(1)-2(0) 2:0<||2||1<\u03b7\nFor completeness, we provide a self-contained proof of Lemma 2.1 in Appendix F. The form in Equation 7 inspires the heuristic choice to sample sets with probabilities proportional to w(||2||1) in Kernel SHAP, as larger terms in the sum should intutively be sampled with higher probability to reduce variance of the estimate. However, as discussed, a more principaled way to approximately solve least squares regression problems via subsampling is to use probabilities proportional to the statistical leverage scores. Formally, these scores are defined as follows:\nDefinition 2.2 (Leverage Scores). Consider a matrix X \u2208 R\u00ba\u00d7n. For i \u2208 [p], the leverage score of the ith row [X]i \u2208 R1\u00d7n is li := [X]i(XX)+[X]."}, {"title": "LEVERAGE SHAP", "content": "Statistical leverage scores are traditionally used to approximately solve unconstrained regression problems. Since Shapley values are the solution to a linearly constrained problem, we first re-formulate this into an unconstrained problem. Ultimately, we sample by leverage scores of the reformulated problem, which we prove have a simple closed form that admits efficient sampling.\nConcretely, we have the following equivalence:\nLemma 3.1 (Constrained to Unconstrained). Let P be the projection matrix I n 11. Define\nA = ZP and b = y \u2013 Z1 (1)-(0). Then\nargmin ||Zx - y||3 = arg min ||Ax \u2013 b||3 + 10 (1) \u2013 v(0)\n(8)\nx:(x,1)=\u03c5(1)-\u03c5(0) n X n\nFurther, we have that minx:(x,1)=v(1)\u2212v(0) || Zx - y||2 = minx || Ax \u2013 b||2 .\nThe main question is how to build S. Our Leverage SHAP method does so by sampling with prob-abilities proportional to the leverage scores of A. Since naively these scores would be intractable to compute, requiring O(pn\u00b2) time, where p = 2n \u2013 2 is the number of rows in A, our method rests on the derivation of a simple closed form expression for the leverage scores, which we prove below.\nLemma 3.2. Let A be as defined in Lemma 3.1. The leverage score of the row in A with index z \u2208 {0, 1}n, where 0 < ||2||1 < n, is equal to lz = {n \\choose |S|}\u22121.\nThe proof of Lemma 3.2 depends on an explicit expression for the matrix ATA:\nLemma 3.3. Let A be as defined in Lemma 3.1. AT A = n1P.\nThis fact will also be relevant later in our analysis, as it implies that ATA is well-conditioned. Perhaps surprisingly, all of its non-zero singular values are equal to 1/n."}, {"title": "ANALYTICAL FORM OF LEVERAGE SCORES", "content": "Proof of Lemma 3.3. Recall that A = ZP, so ATA = PTZTZP. We start by deriving explicit expressions for the (i, j) entry of ZTZ, denoted by [ZZ](i,j), for all i \u2208 [n] and j\u2208 [n]. In particular, we can check that [ZZ](i,j) = \u2211z\u2208{0,1}n ZiZjW(||z||1). So, when i \u2260 j,\n$\\text{[ZZ]}_{(i,j)} = \\sum_{Z:Z_i,z_j=1}w(||z||_1) = \\sum_{s=2}^{n-1} {n-2 \\choose s-2}^{-1} = c_n,$\n(10)\nLet cn denote the above quantity, which does not depend on i and j. I.e., [ZTZ](i,j) = Cn for i \u2260 j.\nFor the case i = j, let j' be any fixed index not equal to i. We have that\n$\\text{[ZZ]}_{(i,i)} = \\sum_{z:z_i=1} w(||z||_1) = \\sum_{z:z_i=1,z_{j'}=0} w(||z||_1) + \\sum_{z:z_i=1,z_{j'}=1} w(||z||_1)$\n$= \\sum_{s=1}^{n-1} {n-1 \\choose s-1}^{-1} + \\sum_{s=2}^{n-2} {n-2 \\choose s-2}^{-1} = \\frac{1}{n} + c_n.$\n(11)\nThe last equality can be verified via a direct calculation. By Equations 10 and 11, we have that ZTZ = n I + cn11T. So we conclude that:\n$\\text{A}^T \\text{A} = P^T \\text{Z}^T \\text{Z} P = \\frac{1}{n} \\left(I - \\frac{1}{n} 11^T\\right) \\left(\\frac{1}{n} I + c_n 11^T\\right) \\left(I - \\frac{1}{n} 11^T\\right) = \\frac{1}{n} \\left(I - \\frac{1}{n} 11^T\\right).$"}, {"title": "OUR ALGORITHM", "content": "With Lemma 3.2 in place, we are ready to present Leverage SHAP, which is given as Algorithm 1. In addition to leverage score sampling, the algorithm incorporates paired sampling and sampling without replacement, both of which are used in optimized implementations of Kernel SHAP.\nFor paired sampling, the idea is to sample rows with probabilities proportional to the leverage scores, but in a correlated way: any time we sample index z, we also select its complement, z, where zi = 1 - zi for i \u2208 [n]. Note that, by the symmetry of A's leverage scores, lz = lz. Similarly, w(||2||1) = W(||Z||1). Moving forward, let Z denote the set of pairs (z, z) where 0 < ||2||1 < n.\nTo perform paired sampling without replacement, we select indices (z, z) independently at random with probability min(1, c(lz + l\u2082)) = min(1,2clz), where c > 1 is an oversampling parameter. All rows that are sampled are included in a subsampled matrix Z'P and reweighted by the inverse of the probability with which they were sampled. The expected number of row samples in Z'P is equal to \u2211(z,z) min(1, 2clz). We choose e via binary search so that this expectation equals m 2, where m is our target number of value function evaluations (two evaluations are reserved to compute v(1) \u2013 v(0)). I.e., we choose c to solve the equation:\nm - 2 = \\sum_{s=1}^{n-1} {n \\choose s} \\min \\left(1,2c {n-1 \\choose s-1}^{-1} \\right).\n(12)\nNote that our procedure is different from the with-replacement Kernel SHAP procedure described in Section 1.1. Sampling without replacement requires more care to avoid iterating over the exponential"}, {"title": "THEORETICAL GUARANTEES", "content": "As discussed, Leverage SHAP offers strong theoretical approximation guarantees. Our main result is Theorem 1.1 where we show that, with m = O(nlog(n/d) +) samples in expectation, Algorithm 1 returns a solution 6 that, with probability 1 \u03b4, satisfies\n||A \u2013 b||2 \u2264 (1 + \u20ac)||A\u0444 \u2013 b||2.\n(13)\nWe prove this guarantee in Appendix A. The proof modifies the standard analysis of leverage scores for active least squares regression, which requires two components: 1) a subspace embedding guar-antee, proven with a matrix Chernoff bound applied to a sum of rank-1 random matrices, and 2) and an approximate matrix-multiplication guarantee, proven with a second moment analysis. We replace these steps with 1) a matrix Bernstein bound applied to a sum of rank-2 random matrices and 2) a block-approximate matrix multiplication guarantee.\nWhile well-motivated by the connection between Shapley values and linear regression, the objective function in Theorem 1.1 is not intuitive. Instead, we may be interested in the l2-norm error between the true Shapley values and the estimated Shapley values. Fortunately, we can use special properties of our problem to show that Theorem 1.1 implies the following corollary on the l2-norm error."}, {"title": "EXPERIMENTS", "content": "In the experiments, we evaluate Leverage SHAP and Kernel SHAP based on how closely they align with the ground truth Shapley values. We run our experiments on eight popular datasets from the SHAP library (Lundberg & Lee, 2017). We find that Leverage SHAP can even outperform the highly optimized Kernel SHAP in the SHAP library, especially for large n or when access to the set function is noisy.\nImplementation Details. In order to compute the ground truth Shapley values for large values of n, we use a tree-based model for which we can compute the exact Shapley values efficiently using Tree SHAP (Lundberg & Lee, 2017). All of our code is written in Python and can be found in the supplementary material. (We will make a public repository after the review process.) We use the SHAP library for the Optimized Kernel SHAP implementation, Tree SHAP, and the datasets. We use XGBoost for training and evaluating trees, under the default parameters."}, {"title": "CONCLUSION", "content": "We introduce Leverage SHAP, a principled alternative to Kernel SHAP, designed to provide prov-able accuracy guarantees with nearly linear model evaluations. The cornerstone of our approach is leverage score sampling, a powerful subsampling technique used in regression. To adapt this method for estimating Shapley values, we reformulate the standard Shapley value regression problem and analytically compute the leverage scores of this reformulation. Leverage SHAP efficiently uses these scores to produce provably accurate estimates of Shapley values. Our method enjoys strong theoretical guarantees, which we prove by modifying the standard leverage score analysis to incor-porate the empirically-motivated paired sampling and sampling without replacement optimizations. Through extensive experiments on eight datasets, we demonstrate that our algorithm outperforms even the optimized version of Kernel SHAP, establishing Leverage SHAP as a valuable tool in the explainable AI toolkit."}, {"title": "Appendices", "content": ""}, {"title": "PROOF OF APPROXIMATION GUARANTEE", "content": "In this section, we prove the following theorem.\nTheorem A.1. Let m = O(nlog(n/d) + nlog(1/8)/\u0454). Algorithm 1 produces an estimate & such that, with probability 1 \u2013 \u03b4,\n||A \u2013 b||2 \u2264 (1 + \u20ac)||A\u0444 \u2013 b||2.\nBecause of the connection between the constrained and unconstrained regression problems described in Lemma 3.1, the theorem implies the theoretical guarantees in Theorem 1.1. The time complexity bound in Theorem 1.1 follows from Lemma G.1 in Appendix G.\nConsider the following leverage score sampling scheme where rows are selected in blocks. (We use the word \"block\" instead of \u201cpair\" in the formal analysis for generality.) Let be a partition into blocks of equal size (size 2 in our case) where the leverage scores within a block are equal. Block \u0398\u03ae is independently sampled with probability p+ := min(1, c\u2211k\u2208\u04e9; lk) for a constant c. The constant e is chosen so that the expected number of blocks sampled is m, i.e., \u03a3\u2081p+ = m. Let m' be the number of blocks sampled. Let S \u2208 R|ei|m'\u00d7p be the (random) sampling matrix. Each row of S corresponds to a row k from a sampled block i: every entry in the row is 0 except for the kth entry which is 1/1/p+.\nIn order to analyze the solution returned by Algorithm 1, we will prove that the sampling matrix S preserves the spectral norm and Frobenius norm.\nLemma A.2 (Bernoulli Spectral Approximation). Let U \u2208 RP\u00d7n be a matrix with orthonormal columns. Consider the block random sampling matrix S described above with rows sampled ac-cording to the leverage scores of U. When m = \u03a9(nlog(n/\u03b4)/\u03b5\u00b2),\n||I \u2013 USTSU||2 \u2264 \u20ac\n(14)\nwith probability 1 \u2013 \u0431.\nProof of Lemma A.2. We will use the following matrix Bernstein bound (see e.g., Theorem 6.6.1 in Tropp et al. (2015)).\nFact A.3 (Matrix Bernstein). Consider a finite sequence {X} of independent, random, Hermitian matrices with dimension n. Assume that E[Xk] = 0 and ||Xi||2 < L for all i. Define X = \u2211i Xi and let V = ||E[X2]||2 = || \u03a3i E[X]||2. Then for any \u0454 > 0,\nPr (||X||2 \u2265 \u20ac) \u2264 nexp \\left(\\frac{-me^2/2n}{V + Le/3}\\right)\n(15)\nLet U(k,) \u2208 R1\u00d7n be the kth row of U. Similarly, let Ue; \u2208 R|0i|\u00d7n be the matrix with rows U(k,)\nfor k \u2208 \u0398.\nWe will choose X\u2081 = UUe \u2013 UU01[i selected]. Then E[Xi] = 0 and X = I - UTSTSU.\nPi\nFirst, we will upper bound maxi ||Xi ||2. If i is not selected or p = 1, then ||Xi ||2 = 0 so we only need to consider the case where i is selected and p+ = c\u2211\u03ba\u03b5\u03b8; lk < 1. Then\n||Xi||2\u2264 \\frac{1}{\\sqrt{p_i}} + |U_{\\Theta_i} \\sqrt{p_i} U_{\\Theta_i}^T||_2 = \\frac{1}{c\\sum_{k \\in \\Theta_i} \\ell_k} \\sum_{k \\in \\Theta_i} \\ell_k = L\n(16)\nNext, we will upper bound || \u03a3i E[X2]||2. Again, notice that E[X] = 0 if p = 1 so we only need to consider the case where pt < 1. Then\n$ \\sum_i E[X_i^2] = \\sum_{i : p_i^+ < 1} \\left(1 - \\frac{1}{p_i^+} \\right) U_{\\Theta_i} U_{\\Theta_i}^T U_{\\Theta_i} U_{\\Theta_i}^T = \\sum_{i : p_i^+ < 1} \\left(1 - \\frac{1}{p_i^+} \\right) U_{\\Theta_i} U_{\\Theta_i}^T $.\n(17)"}, {"title": "PROOF OF APPROXIMATION COROLLARY", "content": "We will establish several properties specific to our problem and then use these properties to prove Corollary 4.1.\nLemma B.1 (Properties of A, \u0444, and $). The Shapley values \u222e, the matrix A, and the estimated Shapley values & produced by Algorithm 1 satisfy\n||A( \u2013 )||2 = 1|| - ||\n(38)\nand\n|| \u0391\u03c6||= 1|||| \u2013 ((1) \u2013 v(0))2\n(39)\nProof. Even though A does not have n singular values, we can exploit the special structure of A, \u0444, and 6. Let A = UEVT be the singular value decomposition of A. By Lemma 3.3, we know \u03a3\u2208 [][R(n-1)\u00d7(n-1) is a diagonal matrix with on the diagonal and V \u2208 Rn\u00d7(n-1) has n \u2013 1 orthonormal columns that are all orthogonal to 1. Let v1, . . ., Vn-1 \u2208 Rn be these vectors. Then A = =1uiv where u1,..., un\u22121 \u2208 R2\"\u00af\u00b9 are the n-1 orthonormal columns in U.\nBy Lemma 2.1 and 3.1, we can write\n$\\phi = \\sum_{i=1}^{n-1} v_i s_i + \\frac{v(1) - v(0)}{n} 1$ and $\\hat{\\phi} = \\sum_{i=1}^{n-1} v_i \\hat{s}_i + \\frac{v(1) - v(0)}{n} 1$\n(40)\nfor some si and \u015di where i \u2208 [n \u2013 1]. Then (6 \u2013 4) = \u2211i=1 vi(\u00a7i \u2013 si) and\n$\\text{A} (\\hat{\\phi} - \\phi) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n-1} \\frac{n-1}{\\sum_{j=1}} u_i v_i^T (\\hat{s}_j - s_j) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n-1} U_i(\\hat{s}_i - s_i)$\n(41)\nso || A( - ) || = || - ||3.\nSimilarly, we can write\n$A \\phi = \\sum_{i=1}^{n-1} \\frac{1}{\\sqrt{n}} u_i v_j s_j + \\frac{v(1) - v(0)}{n} + \\frac{1}{n} \\sum_{i=1}^{n-1} \\sqrt{n} u_i S_i .$\n(42)\nwhere the second equality follows because v1,..., Vn\u22121 are orthogonal to 1. Then ||A6|| = (||||- ((1)-(0))).\nCorollary 4.1. Suppose & satisfies ||Aq-b||3 < (1+\u20ac)||A\u03c6 \u2013 b||3. Let y = ||A\u00a2-b||3||| \u0391\u03c6||3. Then\n||\u03a6 \u2013 \u03a6|| \u2264 \u03b5\u03b3||||3.\nProof of Corollary 4.1. We have\n||A \u2013 b|3 = ||A\u9b32 \u2013 A\u03c6 + A\u00a2 \u2013 b||3 = ||A \u2013 Ap|| + ||A\u0444 \u2013 b||2\nwhere the second equality follows because A\u0444 \u2013 b is orthogonal to any vector in the span of A.\nThen, by the assumption, we have\n||\u0391\u03a6 \u2013 \u0391\u03c6||3 < \u20ac||\u0391\u03c6 \u2013 b||2.\nBy the definition of y = ||A-bl and Lemma B.1,\n1(v(1) \u2013 \u03c5(0))2`\n\u20ac||\u0391\u03c6 \u2013 b||2 = \u03b5\u03b3||\u0391\u03c6||2 = \u03b5\u03b3 \n(45)"}, {"title": "CONSTRAINED REGRESSION", "content": "Lemma 3.1 (Constrained to Unconstrained). Let P be the projection matrix I n 11. Define\nA = ZP and b = y \u2013 Z1 (1)-(0). Then\nargmin ||Zx - y||3 = arg min ||Ax \u2013 b||3 + 10 (1) \u2013 v(0)\nx:(x", "y||2\nx'+c1": "x'", "c1\nx'": "x',1)=0\n= arg min || ZPx' \u2013 (y \u2013 cZ1)||2 + c1\narg min || ZPx' \u2013 (y \u2013 cZ1)||2 + c12, we have that\nx* = arg min ||Zx - y||2.\nFor Leverage SHAP, we reformulate the problem to an unconstrained regression problem with Lemma 3.1 and solve it using standard least squares.\nIn the standard Kernel SHAP implementation, the constrained problem is solved directly"}]}