{"title": "Ploutos: Towards interpretable stock movement prediction with financial\nlarge language model", "authors": ["Hanshuang Tong", "Jun Li", "Ning Wu", "Ming Gong", "Dongmei Zhang", "Qi Zhang"], "abstract": "Recent advancements in large language models\n(LLMs) have opened new pathways for many\ndomains. However, the full potential of LLMs\nin financial investments remains largely un-\ntapped. There are two main challenges for\ntypical deep learning-based methods for quan-\ntitative finance. First, they struggle to fuse\ntextual and numerical information flexibly for\nstock movement prediction. Second, traditional\nmethods lack clarity and interpretability, which\nimpedes their application in scenarios where\nthe justification for predictions is essential. To\nsolve the above challenges, we propose Ploutos,\na novel financial LLM framework that consists\nof PloutosGen and PloutosGPT. The Ploutos-\nGen contains multiple primary experts that can\nanalyze different modal data, such as text and\nnumbers, and provide quantitative strategies\nfrom different perspectives. Then PloutosGPT\ncombines their insights and predictions and gen-\nerates interpretable rationales. To generate ac-\ncurate and faithful rationales, the training strat-\negy of PloutosGPT leverage rearview-mirror\nprompting mechanism to guide GPT-4 to gener-\nate rationales, and a dynamic token weighting\nmechanism to finetune LLM by detecting and\nemphasizing key tokens in rationales. Extensive\nexperiments show our framework outperforms\nthe state-of-the-art methods on both prediction\naccuracy and interpretability.", "sections": [{"title": "1 INTRODUCTION", "content": "The prospect of predicting stock market trends has\ncontinually intrigued both academics and investors\nfor a long time, and it is still challenging due to the\nvolatile nature of financial markets and multimodal\nfeatures related to stock price (Adam et al., 2016),\nsuch as textual features (e.g., news, tweets) and\ntime-series numerical features (e.g. price sequence,\nvolumes sequence) (Kohara et al., 1997; Neal and\nWheatley, 1998). In practice, researchers focus\nmore on predicting stock movement (whether the\nstock price movement will increase or decrease)\nrather than attempting to predict the precise stock\nprice values, which is widely recognized as a inher-\nently unpredictable task. (Walczak, 2001).\nQuantitative methods can be either traditional\ndeep model based or LLM based. Traditional meth-\nods use LSTM, transformer, or graph models to\ncombine stock texts and prices (Gao et al., 2023;\nSawhney et al., 2020a). However, they are opaque\nand lack of interpretability. LLM based methods\nuse LLM to label sentiment or directly merge news\nand prices into model training (Chen et al., 2023;\nXie et al., 2023b). They also struggle to generate\ninterpretable decision making rationales and fail to\nadaptively consider different strategies for different\nmarkets. To understand and fuse multi-modal fea-\ntures such as stock price and text data in a flexible\nway, we introduce PloutosGen pipeline that consists\nof multiple experts to model stock movement from\ndifferent aspects.\n1. Technical Expert, extracts technical features\nfrom time series based on various alpha for-\nmulas. Then it feeds these features into a\ntime series forecasting model as next-token\nprediction.\n2. Sentiment Expert, captures the correlation be-\ntween news events and their effect on stock\nvaluations, which is well-established but het-\nerogeneous and dependent on the content value\nof the news, ranging from breaking news to\ntrivial commentary. This expert aggregates\nthis diverse news with sophisticated methods.\n3. Human Expert, incorporates human thoughts\ninto the framework, which can enhance its\neffectiveness in real applications.\nHowever, the predictive accuracy of strategy ex-\nperts can vary considerably across different stock"}, {"title": "2 PROBLEM FORMULATION", "content": "The goal of quantitative investment task is to learn\nstock price related information from both social me-\ndia, specifically tweets, and past stock performance\nindicators to predict stock trends. The direction of\na stock's movement is determined by comparing\nits adjusted closing prices over two consecutive\ntrading days. This analysis is structured as a binary\nclassification task, where the prediction revolves\naround whether the price of a particular stock will\nmove up or down."}, {"title": "2.1 Stock Prediction", "content": "For a given stock $s \\in S$, a target prediction date $d$,\nand related historical prices and news data, we de-\nfine stock movement prediction label over a period\nof $T$ days as:\n$Y_d = \\begin{cases}\n0, & P_d^a < P_{d-1}^a\\\\\n1, & P_d^a \\geq P_{d-1}^a\n\\end{cases}$\nHere the $P_d^a$ represents the adjusted closing price\nin the target prediction date $d$ (Xu and Cohen, 2018;\nSawhney et al., 2020a). The label $Y_d$ is categorized\nas 0 if there's a decrease in the adjusted closing\nprice, and as 1 indicate a rise in the adjusted closing\nprice."}, {"title": "3 METHODOLOGY", "content": "In this section, we first give an overview of the\nframework of Ploutos, followed with a detailed\nexplanation of PloutosGen and PloutosGPT."}, {"title": "3.1 PloutosGen Pipeline", "content": "As shown in Fig. 2, the core component of\nPloutsGen pipeline is the diverse expert pool, which\nis designed to provide signals from a wide range\nof experts, including sentiment, technical analysis\nand human analysis, each utilizing a Large Lan-\nguage Model (LLM) to analyze the stock related\ninformation from a specific perspective. By inte-\ngrating insights from these experts, the PloutosGPT\ncan leverage a broad spectrum of knowledge and\ntechniques to inform its predictive capabilities."}, {"title": "3.1.1 Sentiment Analysis Expert", "content": "In this section, we would introduce how we lever-\nage only the sentiment information to predict the\nstock price. Previous research shows social me-\ndia is useful for this task (Si et al., 2013, 2014),\nbut there are two challenges: i) existing large lan-\nguage models (LLMs) are not fine-tuned for stock\nprediction from sentiment perspective; ii) how to\naggregate news or tweets into stock signals, since\ndifferent events affect stock price differently. We ex-\nplore three ways based on whether stock signals are\nused for training: unsupervised, supervised, and\nsupervised&unsupervised sentiment-based stock\nprediction.\nUnsupervised: We use several datasets with differ-\nent assets and news types, such as FIQA-SA (Maia"}, {"title": "3.1.2 Technical Analysis Expert", "content": "We aim to use large language models (LLMs) for\nmulti-features time series forecasting in stock pre-\ndiction, where we leverage multiple alphas (domain-\nspecific time series features) and their explanations\n(text features) to frame the task as next-token predic-\ntion. However, existing methods are often limited"}, {"title": "3.1.3 Human Experts", "content": "The human financial expert encapsulates the wis-\ndom and intuition of seasoned investment profes-\nsionals, including insights from fundamental analy-\nsis, macroeconomic indicators, and market cycles.\nThe key feature of this expert is its ability to con-\ntextualize financial data within a broader economic\nand financial framework, providing an additional\nlayer of natural language analysis that is often dif-\nficult to quantify but critical for making informed\ninvestment decisions. This expert serve as a cus-\ntomizable and optional component in PromptGen\nframework since it's not available in open-source\ndataeset but useful in real applications."}, {"title": "3.2 PloutosGPT Training", "content": "The main objective of PloutosGPT is to develop a\nLarge Language Model (LLM) capable of generat-\ning explainable and accurate rationales for predict-\ning price movements. Explainable AI in finance\nresearch has attracted increasing attention from var-\nious perspectives (Weber et al., 2023). For instance,\nsome approaches aim to build interpretable stock\nprice predictors using neuro-fuzzy rules (Rajab and\nSharma, 2019; Vlasenko et al., 2019). Similarly,\nYang et al. (Yang et al., 2018) leverage attention-\nbased neural networks to assign importance to\ndifferent news. However, the interpretability of\nthese existing methods is largely compromised by\nthe neural network learning phase. To the best of\nour knowledge, PloutosGPT is the first method that\ncan generate natural language rationales for inter-\npretable stock movement prediction. As shown in\nFigure 3, it consists of two steps: rearview-mirror\nbased data construction and dynamic token weight-\ning training strategy. In the first step, as shown in\nthe figure, we prompt a powerful frozen LLM (e.g.\nGPT-4) to summarize both the bullish and bearish\nrationales for the price movement based on diverse\nexperts' input, and generate rationales that is faith-\nful to the given information (Since the ground truth\nis given, the frozen LLM knows which rationale\nmakes the correct prediction). Then, the model"}, {"title": "L", "content": "is prompted to make the final decision based on\nthe trade-off between bullish and bearish rationales.\nThe intuition behind these prompts is to generate\nfaithful rationales by learning weights for each ex-\npert's input based on their accuracy and relevance\nin different market conditions. All of these prompts\nare under the constraint that the model should not\ndirectly use the ground truth as the rationales, which\nechoes our initial motivation in introduction that\nresearchers are better at analyzing the past given\nthe result than predicting the future.\nTo further finetune PloutosGPT, the generated\nrationales and stock movement label are combined\nas the output of the instruction training data, and\nthe input and analysis from diverse experts are com-\nbined as the input. An example of the constructed\ninstruction data is shown in the Table 11 of Ap-\npendix. By leveraging this curated training data,\nPloutosGPT becomes not just a black-box predic-\ntor but a sophisticated decision support tool that\nintegrates the strengths of diverse strategies and\nprovides actionable insights with a clear justifica-\ntion for its predictions. Moreover, though we have\nconstructed bullish and bearish rationales based\non ground truth and diverse experts' output, the\nfine-tuned PloutosGPT still struggles to generate\nkey tokens of rationales. For example, in the MV7"}, {"title": "4 EXPERIMENTS", "content": "indicating an upward trend in short term, the key\nword upward has to be treated carefully. Hence,\nwe propose to dynamically control the weight of\neach token in the rationales of the training data as\nfollows:\n$\\mathcal{L} = \\sum -a_i \\log p(y_i | x, y_{<i})$\nwhere $x$ is input of model, and $y$ is generated content\nconsists of rationales and label. $a_i$ is the weight of\ni-th token in generated content. It can be obtained\nby computing the similarity between a specific type\nembeddings and token embeddings:\n$ \\alpha_i = \\frac{\\exp(cos(t_{y_d}, h_i) / \\tau)}{\\sum_{j=1}^{m} \\exp(cos(t_{y_d}, h_j) / \\tau)}$\nwhere $h_i$ denotes hidden state of $i$-th token from\ntransformer's last layers, $t_{y_d}$ is the type emebdding\nwhich is computed by average pooling of verbal-\nizer tokens embedding and $\\tau$ is temperature which\ncontrols the weight of key tokens. A suitable tem-\nperature could give more importance to key tokens,\nand hence bring higher prediction accuracy and\nhigh quality rationales. The verbalizer tokens are\ngenerated by verbalizer based on label $Y_d$. More\ndetails about the verbalizer can be checked in the\nAppendix C.\nOur experiments are designed to investigate the\nfollowing research questions:\n\u2022 RQ1: How does Ploutos perform compared\nwith current LLMbased and traditional predic-\ntion models?\n\u2022 RQ2: How do the different components in\nPloutos affect its effectiveness?\n\u2022 RQ3: Does the decision making rationales\ninformative and consistent with given infor-\nmation?"}, {"title": "4.1 Experiment Settings", "content": "In this study, we evaluate the proposed method\non two public and classic datasets on stock move-\nment prediction, ACL18 (Xu and Cohen, 2018)\nand CIKM18 (Wu et al., 2018). ACL18 comprises\n88 high-volume trading stocks from the S&P 500\nindex listed on the NYSE and NASDAQ exchanges.\nEach of stock contains price data obtained from"}, {"title": "4.1.1 Data", "content": "Yahoo Finance and related tweets data extracted\nusing regular expression queries based on NAS-\nDAQ ticker symbols for each day (except for week-\nends and public holidays). Then a T-day (T=5)\nrolling window is utilized to produce data candi-\ndates along the series of trading days. Samples\nare labeled as either positive or negative based on\nthe percentage change in closing price, with in-\ncreases of 0.55% or more labeled as positive and\ndecreases of 0.55% or more labeled as negative, in\norder to generate an equitably partitioned dataset\nof 26,614 samples. Following Xu and Chen (Xu\nand Cohen, 2018), we split the dataset into dis-\ntinct sets for training (01/01/2014 to 31/07/2015),\nvalidation (01/08/2015 to 30/09/2015), and test-\ning (01/10/2015 to 01/01/2016) in a 7:1:2 ratio.\nCIKM18 consists of 47 stocks with sufficient tweets\nand price data ranging from January 2017 to Novem-\nber 2017. Similarly, we leverage the same data\nprocessing method as ACL18 to process CIKM18\ndataset, and we follow the same approach as stated\nin the original paper to split the datasets."}, {"title": "4.1.2", "content": "Evaluation Metric\nSince Ploutos aims to predict price movement for a\ngiven target day, i.e., a binary classification problem.\nWe adopt accuracy, F1 score, and the Matthews Cor-\nrelation Coefficient (MCC, sourced from sklearn's\nimplementations\u00b9) for classification performance.\nWe employ MCC due to its robustness and unbias\nevaluation of classifier performance regardless of\ndata imbalance as it considers all quadrants of the\nconfusion matrix."}, {"title": "4.1.3 Baselines", "content": "We compare Ploutos with the below baselines con-\ntaining both traditional model based and LLM based\nmethods. Those baselines are selected based on the\ncriteria that it's either open-source or reproducible.\nTraditional Methods: These methods uses tradi-\ntional deep neural network such as time-series,\ngraph based and others. Including ARIMA (Brown,\n2004), Adv-LSTM (Selvin et al., 2017), Stock-\nNet (Xu and Cohen, 2018), DTML (Nguyen and\nShirai, 2015).\nLLM Based Methods: These methods leverage ei-\nther zero shot or finetuned version of large language\nmodel to predict the stock movement. We select\nLLaMA-2 (Touvron et al., 2023), GPT-4 (Achiam\net al., 2023), FinMA (Xie et al., 2023b) as baselines."}, {"title": "4.1.4", "content": "Training settings\nWe conduct all experiments on 8 Tesla V100 GPU.\nTo get the best performance of Ploutos, we apply\ngrid search for batch size in {64, 128, 256} and\nlearning rate in {1e-6, 2e-06, 3e-6} for each ex-\npert's training. Each experiments run three times\nand we report the average metric. Adam is used\nas the optimizer and we leverage our framework\nto fine-tune from LLaMA-2 for 2 epochs on all\ndatasets. During the rearview-mirror data genera-\ntion stage, we execute the scraping process using\nAzure OpenAI GPT-4 API. We use a temperature\nof 1.0 to generate samples and set the maximum\nnumber of tokens for generation to 2000. Moreover,\nwe set the frequency penalty to zero and top-p to\n1.0. For evaluation, we set the temperature to zero\nfor all models to reduce output randomness and\nensure more focused and deterministic outputs."}, {"title": "4.2", "content": "Performance Comparison (RQ1)\nWe compare the stock movement prediction perfor-\nmance of various methods including both traditional\nand LLM based methods. The evaluation results\nagainst baseline methods are presented in Table 3.\nFrom the figure and table, we draw the following\nconclusions: 1) Our method significantly outper-\nforms both traditional and LLM based methods,\nverifying the superiority of tuning LLMs via our\nproposed training framework. Ploutos successfully\nunlocks the text and numerical data understanding\ncapabilities of LLMs for stock movement prediction\ntask. 2) Traditional deep models such as ARIMA,\nAdv-LSTM and others perform worse than Ploutos,\nimplying the potential of leveraging language model\nto process and understand multi-mode stock re-\nlated data. 3) The zero shot ability of LLaMA-2\nand GPT-4 perform similarly to random guessing\n(ACC\u2248 0.5). However, our Ploutos achieves signif-"}, {"title": "4.3 Ablation Study (RQ2)", "content": "To evaluate the contribution of different compo-\nnents, we compare against several variants:\n\u2022 Ploutoss: Ploutos without the sentiment anal-\nysis expert, which makes decisions just based\non the numerical technical analysis.\n\u2022 Ploutos-\u0442: Ploutos without the technical anal-\nsis expert, which makes decisions just based\non the news sentiment analysis.\n\u2022 Ploutos-r: Ploutos without the rearview-\nmirror instruction data and dynamic token\nweighting mechanism.\nThe results are shown in Table 4. Ploutoss achieves\nhigher prediction accuracy than technical feature\nonly methods such as ALSTM and Adv-LSTM,\nindicating the importance of the number to text\nalignment technique. Similar to the performance\nof Stocknet, Ploutos\u00ac\u0442 has a relatively high Acc.\nbut lower MCC, suggesting texts are particularly\nhelpful to predict on one side of the binary clas-\nsification. By modeling both text data and stock\nrelationships, Ploutos\u00acr reaches a relatively good\nresult. Finally, better performance achieved when\nrearview-mirror instruction data and dynamic token\nweighting are introduced into training. In summary,\nthe ablation studies conclusively demonstrate that\neach component within the Ploutos architecture is\nimportant, and as a result the full model with all\ncomponents achieves the best performance."}, {"title": "4.4 Faithfulness and Informativeness of\nInterpretablity (RQ3)", "content": "To verify the quality of the interpretablity of the\ngenareted rationales, we employ two quantifiable\nmetrics: faithfulness and informativeness. Faithful-\nness measures whether the facts in model's response\nare based on or can be inferred from the given knowl-\nedge. Following the method in HaluEval (Li et al.,\n2023), we design prompt to call GPT-4 to classify\neach fact within the model's response rationales\nas either inferable from the given knowledge or\nnot, and then aggregate these accuracies as the\nmodel's faithfulness score (see Appendix D.1 for\nmore information). Informativeness measures the\namount of information contained in the model's\nresponse, which is equally an important metric for\ninterpretablity, as we found some models might out-\nput generally correct but uninformative rationales\nsuch as \"predict rise in price due to the promising\ninformation.\". To measure the informativeness of\neach rationale, we directly use the informativeness\nmeasurement from TruthfulQA (Lin et al., 2021).\nTable 5 shows the interpretablity score of differ-\nent models, from which we can draw several conclu-\nsions: 1) Compared to other LLM methods, Ploutos\ndemonstrates superior faithfulness and informative-\nness, which indicates that large language model can\nbe trained through specific methods to generate in-\nformative and faithful rationales, thereby enhancing\nmodel's interpretability. 2) The FinMA-7B, which\nadopt similar training datasets compared to Ploutos,\nstruggled to generate informative rationales, which\nindicate the effectiveness of the rearview-mirror\ndata training strategy. 3) FinGPT (Yang et al., 2023)\noutperforms FinMA in terms of both faithfulness\nand informativeness, which can be attributed to its\nchain of thought scraping and training technique\nto enhance reasoning capabilities. However, com-\npared to Ploutos, it does not take technical indicators\nsuch as the interpretability of various alphas into\naccount, leading to a lack of technical analytical\ncapabilities."}, {"title": "ACC", "content": "Moreover, we explored the impact of temperature\nof dynamic token weight on the model's predictive\naccuracy and interpretability. The results are sum-\nmarized in Fig. 4. It can be observed that when the\ntemperature is greater than 0.5, a lower temperature\ncorresponds with higher accuracy, aligning with\nour intuition that a lower temperature increases the\nweight of the loss focus on predicting the stock\nmovement. However, when the temperature is less\nthan 0.5, both accuracy and faithfulness begin to\ndecrease, indicating that overly focusing on the\nmovement itself and neglecting the interpretable\nrationale tokens can actually weaken the model's\nreasoning capabilities. When temperature equals to\n0.5, both faithfulness and accuracy reach their maxi-\nmum, suggesting a synergistic rather than a mutually\nexclusive relationship between predictive accuracy\nand interpretability, which also demonstrates the\neffectiveness of dynamic token weighting."}, {"title": "5 Related Work", "content": "Deep learning approaches have gained prominence\nfor their capacity to model complex patterns in stock\nmarket data. It can be divided into several tracks:\nRNN Based, CNN Based, Transformer Based and\nGraph Based Models. Recurrent neural networks\n(RNNs) are favored for their proficiency in cap-\nturing sequential trends in stocks (Li et al., 2018;\nQin et al., 2017), with advancements like Long\nShort-Term Memory (LSTM) networks enhanced\nby adversarial training demonstrating superior per-\nformance in stock prediction (Feng et al., 2018).\nMoreover, CNN Based Models incorporating stock\ntimes series data as a set of input feature maps (Ho-\nseinzade and Haratizadeh, 2019; Lu et al., 2021).\nFurthermore, the emergence of transformer-based\nmodels, exemplified by StockFormers (Gao et al.,"}, {"title": "5.1 Stock Prediction with Deep Learning", "content": "2023), showcases the effectiveness to extract latent\nrepresentations of long short term stock states and\nasset relations. Recent efforts have also explored\ngraph-based deep learning models or casual ma-\ntrix to analyze the interconnections among stocks,\noutperforming price-only models (Luo et al., 2023;\nSawhney et al., 2020b; Kim et al., 2019)."}, {"title": "5.2 Stock Prediction with Large Language\nModel", "content": "The recent strides in large language model (LLM)\nhave catalyzed the exploration of stock prediction\napplication with LLM. Existing studies consists\nof several directions. Some studies focus solely\non textual information. Lopez-Lira et al. (Lopez-\nLira and Tang, 2023) demonstrate the correlation\nbewteen ChatGPT sentiment score and subsequent\nstock movement. Some researchers focus more on\nthe numerical feature, Alpha-GPT is proposed as\na new alpha mining paradigm by leveraging the\npower of large language models (Wang et al., 2023).\nOthers propose methods combine numerical and\ntextual features, Xie et al. (Xie et al., 2023b,a)\ndirectly prompt and fine-tune LLM to leverage both\nnews and numerical features. Futhormore, Chen\net al. (Chen et al., 2023) explore the relationship\nbetween stock by analyzing the textual information\nby ChatGPT. However, these techniques fall short\nof exploring the interpretablity ability of LLM,\nthereby limiting their application potential."}, {"title": "6 CONCLUSION", "content": "In this work, we explored the feasibility of using\nLLMs to construct a framework that can generate\ninterpretable decision making rationals for stock\nmovement prediction. Specifically, we first explore\ndifferent strategies to analyze the multi-model stock\nrelated data for stock movement prediction from\ndifferent perspectives. Then we propose Ploutos-\nGPT, which introduces an novel training approach\ncalled rearview-mirror prompting and dynamic to-\nken weightin that are designed to dynamically incor-\nporate insights from diverse quantitative strategy\nexperts, encompassing both sentiment and techni-\ncal analysis. Extensive experiments shows that our\nmethod can perform well under different conditions,\nreinforcing its practical applicability."}, {"title": "Limitations", "content": "While our proposed Ploutos framework shows\npromising results in stock movement prediction,"}, {"title": "there are several limitations that need to be ad-\ndressed:", "content": "\u2022 The PloutosGPT is designed to combine in-\nsights and predictions from multiple primary\nexperts. However, the selection and perfor-\nmance of these experts can significantly impact\nthe model's performance. The process of ex-\npert selection and their potential biases need\nto be thoroughly examined.\n\u2022 The computational cost of our model, espe-\ncially when dealing with large-scale data, can\nbe high. Optimizing the model for efficiency\nwithout compromising its predictive accuracy\nis a challenge.\n\u2022 Our model currently focuses on textual and\nnumerical data. Incorporating other types of\ndata, such as visual data, could potentially\nimprove the model's performance."}, {"title": "A Sentiment Experts", "content": "Table 6 shows the comparison of the impact for stock\nmovement prediction task over different datasets.\nAmong all the options, the unsupervised method\ncombining all the datasets performs the best, which\nis reasonable since this method consists of most di-\nversified and robustness data for sentiment analysis.\nBesides, the accuracy of stock market predictions\nusing sentiment scores calculated by this unsuper-\nvised method hovers around 50%, which is not\nparticularly high. This indirectly suggests that\nwhile the sentiment of these news articles does have\na positive correlation with stock prices, there is also\na lot of noise within the news itself. We need to\ndevelop better methods to aggregate this news."}, {"title": "A.1 Choice of unsupervised dataset", "content": "Table 7 shows the prediction results on StockNet\nfor different methods. The results indicate that the\ncombined 'sup & unsup' method performs the best,\nwhich also demonstrates the effectiveness of both\nour supervised and unsupervised approaches."}, {"title": "A.2 Comparison of different training methods", "content": "To evaluate the performance of the number-to-text\nalignment (prompt is shown in Table 9), we care-\nfully choose two popular time series baselines: 1)\nLLMTime (Gruver et al., 2023): a single feature"}, {"title": "B Technical Experts", "content": "time series prediction method. Here we combine\nsimply combine cross feature time series prediction\nsequences into one time series sequence; 2) LLM-\nDate: following prompt in (Xie et al., 2023a) which\nsplit time series features by date, we adopt similar\nprompt showing in appendix. We use the same base\nmodel and hyperparamters to evaluate the perfor-\nmance of the above prompts. As shown in Table 8,\nthe performance of LLMTime is close to random,\nwhich indicates that the LLM is not good at cross\nfeature time series prediction without meticulous\ntask conversion. Besides, we find that N2I-Align\nperforms the best, showing the effectiveness of\nnumber to text alignment."}, {"title": "C Details of Verbalizer", "content": "We define a verbalizer as an injective function that\nmaps each label to a word list from vocabulary of\nlarge language model. For Up label, we map it to\na word list: [up, boost, positive, rise]. For Down\nlabel, we map it to another word list: [down, reduce,\nnegative, drop]"}, {"title": "D Interpretablity Evaluation", "content": "Recent work has shown that LLMs are prone to\nsuffer from hallucination generations across various\napplications, where the generated responses is either\nin conflict with existing fact or cannot be verified by\nthe available knowledge resources. To evaluate how\nfaithful the model response to the given knowledge,\nwe design prompt with help of GPT-4 to verify the\nfaithfulness of model response (prompt is shown\nin Table 12). Unlike the conclusion in HaluEval\nthat existing state of art LLM (GPT-3 at that time)\ncan not distinguish between fact and hallucinated\ncontent, we find that GPT-4 is now capable of handle\nthis job. To evaluate this statement, we samples 500\nexamples from ploutos response and invite human\nlabelers to annotate. For each query and model\nresponse, human labelers will annotate whether the"}, {"title": "D.1 Faithfulness", "content": "response is faithful to the given knowledge resources\n(\u201cYes\u201d or \u201cNo\u201d) and list the corresponding reasons.\nEach response is labeled by three human labelers,\nand we adopt the max-voting strategy to determine\nthe final faithfulness label. At last, we compare\nthe Pearson correlation between the human labeled\nresult and prediction from our proposed method,\nand achieved a Pearson correlation score of 0.826.\nThe result indicates that this method can be used to\nevaluate the faithfulness of model response."}, {"title": "E PloutosGPT Training", "content": "The prompt for rearview-mirror data scraping is\nshown in Table 11."}, {"title": "E.1 Example for Rearview-mirror Instruction\nPrompt", "content": "The prompt for rearview-mirror data scraping is\nshown in Table 11."}]}