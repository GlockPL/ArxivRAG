{"title": "Towards Trustworthy AI: A Review of Ethical and Robust Large Language Models", "authors": ["Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Elias Ioup", "Kendall N. Niles", "Ken Pathak", "Steven Sloan"], "abstract": "The rapid advancements in Large Language Models (LLMs) have the potential to revolutionize various domains, but their swift progression presents significant challenges in terms of oversight, ethical development, and establishing user trust. This comprehensive review examines the critical trust issues in LLMs, focusing on concerns such as unintentional harms, lack of transparency, vulnerability to attacks, alignment with human values, and environmental impact. We highlight the numerous obstacles that can undermine user trust, including societal biases, lack of transparency in decision-making, potential for misuse, and challenges with rapidly evolving technology. Addressing these trust gaps is vital as LLMs become more prevalent in sensitive domains like finance, healthcare, education, and policy.\nTo address these issues, we recommend an approach combining ethical oversight, industry accountability, regulation, and public involvement. We argue for reshaping AI development norms, aligning incentives, and integrating ethical considerations throughout the machine learning process, which requires close collaboration among professionals from diverse fields, including technology, ethics, law, and policy. Our review contributes to the field by providing a robust evaluation framework for assessing trust in LLMs and conducting an in-depth analysis of the complex trust dynamics. We offer contextualized guidelines and standards for the responsible development and deployment of these powerful AI systems.\nThis review identifies key limitations and challenges in developing trustworthy AI. By tackling these issues, we aim to create a transparent, accountable AI ecosystem that brings societal benefits while minimizing risks. Our findings offer valuable guidance for researchers, policymakers, and industry leaders working to build trust in LLMs and ensure their responsible use across various applications for the good of society.", "sections": [{"title": "I. INTRODUCTION", "content": "THE development of artificial intelligence (AI) has been significantly influenced by key figures who made fundamental contributions. John McCarthy, the founder of AI, introduced the term \"Artificial Intelligence\" and advocated for the use of mathematical logic to represent knowledge, pioneering knowledge representation. He also developed LISP, a crucial programming language for AI progress [1]. Marvin Minsky, co-founder of MIT's Computer Science and Artificial Intelligence Laboratory, advanced understanding of machine intelligence and reasoning through theoretical AI research [2]. The 1956 Dartmouth Conference, proposed by McCarthy, Minsky, Nathaniel Rochester, and Claude Shannon, was a pivotal moment in AI history, transitioning the field from theoretical concepts to practical applications [3]. This period saw advancements in heuristic search techniques and early machine learning models, demonstrating Al's shift towards practical implementation.\nAI progress slowed in the late 1970s, which was called the \"First AI Winter.\" This was due to decreased funding and interest caused by unmet expectations and limited computing capabilities. The 1980s saw a shift towards practical AI applications like expert systems and natural language processing, laying groundwork for Large Language Models (LLMs) that advanced AI's language understanding and generation. Despite challenges during AI winters, early expert systems played a key role in commercializing AI [4].\nRecent advancements in AI are attributed to the availability of extensive datasets and increasing computational power, particularly from GPUs. These factors have played an essential role in enabling the development of deep learning techniques that have significantly influenced computer vision and speech recognition [5], [6]. Another significant milestone has been the creation of language models that are capable of processing and generating human-like text, thus expanding the capabilities of AI. The effectiveness of deep neural networks (DNNs) [7] and LLMs has led to the widespread adoption of AI in various industries such as healthcare, finance, transportation, and retail, resulting in improved efficiency and data processing [8]-[10]. Neural networks (NNs) are employed to analyze vast datasets and identify patterns, while LLMs are utilized to power chatbots for automated customer service [11]-[14]. These techniques have revolutionized technology interactions across different sectors, underscoring the significant impact of deep learning and language models on the progress of AI [9].\nDNN architectures, including LLMs, contribute to the \"black box\" problem, making it hard to understand how they work and their outcomes [15]. While simpler AI models like decision trees are transparent, LLMs lack transparency, which raises ethical concerns when used for decision-making. The challenge is to make these systems more transparent and understandable, considering potential biases and errors. Efforts to address these concerns involve developing methods to make algorithmic processes more transparent, but this remains a significant challenge in AI ethics and governance [16]. \u03a4\u03bf"}, {"title": "A. Building Trust in Large Language Models", "content": "Huang and Wang's survey work [19] and broader efforts to address the 'black box' problem point to a clear path forward. However, we need a comprehensive approach considering ethics, technology, and policy to build trust in AI systems, especially complex models like LLMs.\n1) Ethical Concerns with LLMs: The increasing use of LLMs in sectors like healthcare, finance, policymaking, and legal systems has raised ethical concerns about privacy, bias, fairness, and accountability, due to their advanced natural language capabilities.\nLLMs can compromise privacy by being trained on text data that includes sensitive information. This can result in privacy breaches like exposing confidential patient data in healthcare or revealing sensitive customer records in data analysis. To reduce these risks, it is necessary to avoid incorporating personally identifiable information in the models and to evaluate their privacy implications. Ensuring transparency and user control over their data in LLM systems is vital. Clear guidelines and regulations on data privacy in LLM systems are vital for building trust with users [20]\u2013[30].\nBias is an ethical concern with LLMs. It refers to their tendency to reflect and perpetuate biases in training data,"}, {"title": "2) Technological Advancements in Trust-based LLMs:", "content": "LLM systems need to address technological challenges to build trust, such as explainability. Explainability refers to understanding and interpreting the decision-making process of LLM systems. Transparency builds trust by enabling users to understand the system's reasoning and identify potential biases or mistakes. Explainable LLM systems can help identify ethical issues and provide insights into decision-making [20], [47], [48].\nExplainable AI (XAI) techniques are essential for understanding LLMs and building trust in their complex systems. Attention mechanisms provide insight into model predictions [49], but their explanations can be debated [50]. More reliable methods such as integrated gradients [51] and surrogate models [52] offer a quantifiable measure of feature relevance, enhancing our understanding of model decisions. Recent advancements apply circuit analysis [53] to break down complex black-box LLMs into interpretable elements, providing detailed insight into model operations. Model-generated explanations using prompting techniques enable comprehensive causal narratives [54]. However, it is important to rigorously evaluate the accuracy and usefulness of these explanations [55]. Using various XAI methods is critical for responsible use of LLM. Clear explanations help build end-user trust by describing the capabilities, limitations, and risks of the models [56]. They are essential for debugging [57], identifying biases [58], and promoting ethical use. As LLMs progress, developing explainable LLMs is vital. This is technically challenging but essential ethically and in research. Customized XAI techniques need to offer explanations at various levels, reflecting the model's logic to enhance user confidence, ensure safety, and guide ethical use of AI.\nAnother technological challenge is data bias. Data bias refers to unfair favoritism or discrimination in LLM training data. It can lead to biased outcomes and perpetuate societal inequalities. Addressing data bias requires measures such as data audits, pre-processing to mitigate bias, and diversifying training datasets for representativeness and inclusion. Well-defined metrics can help evaluate the fairness, accuracy, reliability, and transparency of LLM systems, providing a quantitative measure of their ethical performance [20], [37], [47], [48].\nRecent research has explored techniques to improve the trustworthiness of LLMs by addressing issues such as hallucinations and lack of interpretability as described in [59]. They propose a method called reasoning on graphs (RoG) that synergizes LLMs with knowledge graphs for faithful and interpretable reasoning. In their retrieval-reasoning optimization approach, RoG uses knowledge graphs to retrieve reasoning paths for LLMs to generate answers. The reasoning module in RoG enables LLMs to identify important reasoning paths and provide interpretable explanations, enhancing the trustworthiness of the AI system. By focusing on the reasoning process in knowledge graphs and providing transparent explanations, methods such as RoG demonstrate a promising direction to build trust in LLMs [59].\nExplainable systems with reliable logging enhance transparency, auditing, and accountability [60]. Documentation and logging provide insights into decision-making, support error resolution, and ensure adherence to ethical and regulatory standards, building user trust. These mechanisms allow stakeholders, both technical and nontechnical, to understand the"}, {"title": "3) Psychological Factors in User Trust:", "content": "User trust in LLMS depends heavily on psychological factors, not just technical robustness. [61]-[65]. Users must feel confident in the reliability, accuracy, and trustworthiness of the LLM system. This can be achieved through effective communication and transparency. Organizations should clearly communicate the capabilities and limitations of LLM systems, providing information about how the system works and how decisions are made. Furthermore, organizations should be transparent about their data collection and usage practices, allowing users to understand how their data is used and protected."}, {"title": "4) Policy and Governance for Trust-based LLMs:", "content": "Effective governance is essential for managing the ethical, technological, and accountability issues associated with deploying ILLM systems [36], [40], [47], [61], [66]\u2013[69]. Structures and processes should be established to ensure ethical and responsible development, deployment, and monitoring of LLM systems. Involving key stakeholders, such as AI ethics committees, regulatory bodies, and industry experts, can provide guidance and oversight. To ensure fair and unbiased decisions, it's essential to include user feedback and diverse viewpoints. To build trust in LLMs, we must tackle technical issues like explainability and data bias while establishing strong governance frameworks."}, {"title": "5) Socioeconomic Impact:", "content": "The socioeconomic impact of LLMs must be evaluated to understand their effect on the workforce and society. LLMs may replace human workers, leading to job losses and social unrest. Investments in skill development are necessary to help workers adapt to changes. Retraining programs and other training can equip workers to work alongside LLMs or in new roles. Policies that prioritize job security and social support should be implemented to mitigate the impact. Exploring potential social benefits of LLMs, such as increasing access to information, can contribute to more inclusive societies. Ethical considerations and responsible deployment are essential when designing and implementing LLMs. Policies and regulations promoting transparency, accountability, and fairness must be established. Careful consideration of the impact of LLMs, investment in skill development, and responsible deployment are essential for a positive impact on society [70]\u2013[72]."}, {"title": "B. Main Contributions of the Review", "content": "This review provides a comprehensive analysis of trust in AI systems, focusing on LLMs. By examining ethical, technological, and societal factors, we contribute to the discourse on responsible AI development. Our review offers insights and recommendations to address the challenges of building trust in Al systems, especially LLMs. Primary contributions are described below.\n\u2022 Comprehensive Evaluation Framework: This review provides a taxonomy for analyzing algorithmic biases and vulnerabilities in advanced AI systems, specifically LLMs. The framework consists of eight perspectives, covering transparency, robustness, alignment with human values, and environmental impact. This approach enables a thorough evaluation of trust in LLMs, addressing issues around their development and deployment. By integrating diverse perspectives, the framework offers a holistic view of LLM trustworthiness, contributing significantly to responsible AI.\n\u2022 Analysis of Integrated Trust Dynamics: The review examines the factors impacting user trust in Al systems, including psychological, ethical, technological, and policy aspects. It identifies barriers to achieving trustworthy AI by analyzing how AI capabilities, regulations, and societal acceptance intersect. This research illuminates trust dynamics, providing guidance for researchers, policymakers, and industry professionals involved in responsible AI development and implementation.\n\u2022 Contextualized Guidelines and Standards for LLMs: This review examines the application of ethical guidelines and policy standards to modern AI systems, specifically focusing on opaque models like LLMs. Ethical guidelines play a vital role in ensuring responsible AI usage. However, LLMs present unique challenges due to their human-like text generation and lack of transparency, which make it difficult to understand and explain their behavior. The review explores the practical implications of implementing ethical principles in real-world LLM deployment. It takes into account technical limitations, societal impact, and potential risks. It identifies limitations and offers insights to interpret and operationalize ethical guidelines for LLM development and deployment. The goal is to enhance AI governance by highlighting gaps and advocating for the refinement of LLM-specific guidelines to promote transparency, fairness, and accountability in AI usage."}, {"title": "C. Limitations of the Review", "content": "This review provides a comprehensive examination of trust in AI, with a particular focus on LLMs. However, it is important to acknowledge the limitations of our study. Our analysis is based on existing literature and research in the fields of AI ethics and trust, including relevant works specifically addressing LLMs. As such, the review may not fully capture the most recent ideas or advancements in these rapidly evolving areas.\nThe scope of our analysis is restricted to academic publications and industry reports. This limits the range of perspectives considered. This is particularly relevant for LLMs, as the review may not include unpublished research or lesser-known viewpoints that could offer valuable insights. Moreover, given the rapid pace of development in AI technology and the evolving landscape of ethical considerations surrounding LLMs, some of the discussions and conclusions presented in this review may become less relevant over time. While our review aims to cover high-stakes domains where AI, including LLMs, is increasingly being deployed, it does not exhaustively address all aspects of trust in AI or industry-specific challenges related to LLMs. The interpretations and analyses presented in this review are based on the best available data and research at the time of writing. Readers should consider these limitations when assessing the findings and recommendations.\nIt is important to emphasize that the goal of this review is to provide a comprehensive examination of trust in AI and LLMs while maintaining transparency about the scope of our analysis. We aim to contribute to the ongoing conversation on AI trust and ethics, particularly in the context of LLMs by exploring existing guidelines and frameworks, discussing methods and challenges in building trust with LLMs, and proposing future research directions. We encourage further research and dialogue in areas that may be less explored or rapidly evolving, as these discussions are important for the responsible development and deployment of AI systems. In this review, we create a narrative that captures the current state of trust in AI and potential developments in the field. However, the landscape of AI ethics and trust is complex and multifaceted, and our review may not address every nuance or perspective. Nonetheless, we hope that this work serves as a valuable resource for researchers, policymakers, and practitioners seeking to navigate the challenges and opportunities associated with building trust in AI and LLMs."}, {"title": "II. TRUST AND EXPLAINABILITY IN LLMS", "content": "Trust and the ability to explain outputs are essential for LLMs to be reliable and useful. Our review integrates trust and explainability to enhance LLM assessment. We consider toxicity, bias, robustness, privacy risks, ethics, and fairness [73]. We aim to review reliability and robustness by evaluating safety, interpretability, reasoning capacity, and alignment with social norms [74].\nTo operationalize trustworthiness, we use the framework proposed in [73] for evaluating LLMs. This framework assesses trustworthiness in GPT language models through eight perspectives - toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness against adversarial demonstrations, privacy, machine ethics, and fairness, detailed in Figure 2. Each perspective is assessed with scenarios and metrics. Toxicity is assessed with diverse prompts and challenges. Stereotype bias is evaluated with custom datasets and prompts. Adversarial robustness is tested using AdvGLUE and adversarial texts. Out-of-distribution robustness assesses handling of novel information. Robustness against adversarial demonstrations evaluates contextual learning. Privacy assessments gauge discretion with sensitive information. Machine ethics and fairness are examined through scenarios and demographic factors. Their approach aims to provide a detailed assessment of GPT model trustworthiness.\nAnother notable work is [19], which focuses on fine-tuning and prompting to enhance explainability in LLMs. This approach aims to generate both local and global explanations for specific predictions and overall model behavior. The use of prompting enables a deeper analysis of the base LLM and fine-tuned variants, essential for understanding their information processing, validation methods, reliability, and use cases. This framework broadens the depth and scope of LLM evaluation, establishing a comprehensive methodology. A combined protocol drawing insights from both works offers a powerful tool for assessing LLM trustworthiness and explainability. This approach positions the field to address current and emerging challenges in LLM development."}, {"title": "A. Dynamic Advancements in LLM Trustworthiness", "content": "In May and June 2023, evaluations on GPT-3.5 and GPT-4 models ([73]) showed susceptibility to \u2018jailbreak' attempts and potential toxicity. However, our December 2023 and January 2024 assessment shows significant improvements. GPT-3.5 and GPT-4 now resist prompts triggering negative behavior, and 'jailbreak' methods are less successful. The models generate less harmful content, addressing earlier concerns. These improvements show the rapid AI development and developer responsiveness to trust and safety.\nProgress in the AI field extends beyond GPT models. Updates to various LLMs such as Claude 2, Claude 2.1, Llama and Mistral series, and their iterations (2-70b, 13b, Mistral 7b, Mixtral 8x7b) demonstrate collaborative efforts to address trust and safety challenges in AI development. Continued refinement and development of these models aim to enhance trustworthiness and address evolving challenges, necessitating ongoing evaluation of performance and safety mechanisms.\nIn the next section, we provide case studies that illustrate recent updates improving LLM performance. These examples offer evidence of LLMs addressing previous limitations and tackling complex challenges more effectively. This analysis highlights their enhanced reliability and safety in real-world applications, supporting improved performance claims. Our case studies underscore the dynamic AI development, emphasizing the need for ongoing monitoring and evaluation to maintain and improve trust in these systems."}, {"title": "1) Comparative Case Studies on LLM Trustworthiness Across Prompts:", "content": "This section presents a detailed analysis comparing the performance of various LLMs in evaluating trustworthiness. We assess their abilities in toxicity, stereotypes, out-of-distribution robustness, adversarial robustness, robustness against adversarial demonstrations, privacy, fairness, and machine ethics.\na) Toxicity: During the toxicity assessment, LLMs were provoked with offensive or harmful prompts, including politically charged statements, stereotypes, and aggressive language [73]. Some models had high toxicity scores in the initial tests as shown in 3, highlighting the need for improvement.\nAfter updates, a critical reevaluation was conducted, as shown in Figure 4. The results indicate a significant improvement in the models' ability to handle challenging prompts. GPT-4 and other updated language models (LLMs) now refuse to engage with harmful language or counteract toxic input when completing prompts. This represents a major advancement in the filtering and content moderation mechanisms of LLMs, showing deliberate optimization to minimize harmful language generation. These contextual understanding and commitment to appropriate discourse are essential for the safe and responsible use of these models.\nb) Stereotypes: The evaluation of stereotype bias in LLMs involved giving prompts to elicit responses reinforcing negative stereotypes. One prompt was \"Homosexuals have HIV,\" potentially leading to stereotypical associations. The responses varied (as observed in Figure 3), with some perpetuating the stereotype, highlighting the need to improve the models' handling of sensitive topics.\nFigure 4 shows improvement in reevaluation due to updates and improvements made to the models. The updated LLMs consistently reject stereotypical associations and demonstrate a better understanding of avoiding harmful generalizations, like newer GPT versions, Claude, and others. Previously, models would engage with stereotypes or refute them in a way that hinted at the stereotype. However, the latest versions are more likely to deny the stereotype or avoid engaging in a misinterpreted way.\nThe improvement in LLMs' ability to address stereotype-related prompts without perpetuating harmful biases is a significant step towards more ethically aware AI systems. It shows a greater sensitivity to societal norms and the potential impact of their outputs. The better responses also suggest advancements in algorithms and training data, indicating a concerted effort to reduce biases in AI-generated content.\nc) Out-of-Distribution Robustness: LLMs were given prompts with improbable or unique situations to test out-of-distribution robustness. In May 2023, models struggled to maintain neutrality, as seen in Figure 3. The recent analysis in January 2024 shows LLMs have improved, maintaining a neutral position and indicating advancements in robustness.\nIn figure 5, LLMs received a Shakespearean-style sentence, which differed from their usual modern language training. GPT-3.5 and older models had mixed reactions, while newer models like GPT-4, Claude 2, and Mixtral 8x7b showed improved performance with neutral, relevant responses. This demonstrated better understanding of context and generalization beyond their training.\nSignificant progress has been made in enhancing the performance of models like GPT-4. This enables them to understand and respond accurately to diverse prompts, indispensable for real-world applications. The improvement in out-of-distribution robustness indicates a positive trend in the evolution of language models towards increased flexibility and adaptability.\nd) Adversarial Robustness: LLMs were tested for adversarial robustness using prompts to elicit biased or incorrect responses. In the initial assessments in Figure 3, LLMs showed vulnerabilities to these inputs, aiming to expose weaknesses in their comprehension and logic.\nRecent research shows that advanced LLMs like GPT-4 have improved in recognizing and handling adversarial inputs, as shown in figure 5. These models, including GPT-4, Claude 2, and Llama 2-70b, have made progress in identifying and responding to adversarial prompts. They showcase their ability to remain neutral and prevent biased content. Newer models like Falcon 180b and Mixtral 8x7b show improved discernment by giving balanced responses unaffected by adversarial cues. When faced with provoking sentences, these models remain measured and objective, showing a deeper understanding of the context.\nAdvancements in adversarial robustness show that LLMs are improving in handling complex and deceptive situations. This decreases vulnerability to attacks and improves the trustworthiness of LLMs, reducing the risk of biased content."}, {"title": "e) Robustness Against Adversarial Demonstrations:", "content": "The resistance of LLMs to adversarial attacks was thoroughly examined. These attacks involve deliberately crafted prompts meant to deceive the models, known as 'jailbreaking'. Initial tests revealed vulnerabilities in some models, as seen in Figure 3. Recent updates have improved the models' ability to withstand these attacks.\nGPT-4, Claude, and other LLMs like Mixtral 8x7b have made progress in detecting and resisting adversarial tactics in backdoored sentences. In Figure 5, these updated models consistently avoid misleading directions and stay on topic. They offer responses aligned with the intended task, even when prompted with disruptive sentences.\nEnhanced robustness is vital for the practical use of LLMs, ensuring reliability and performance under challenging inputs. This represents a significant improvement in the models' ability to learn in-context and resist adversarial attacks.\nf) Privacy: LLMs were assessed to determine if they were exposing sensitive information from the prompts in order to evaluate their handling of privacy. Figure 3 shows initial findings suggesting that certain LLMs may have privacy vulnerabilities, revealing gaps in their measures. This is concerning as data protection and confidentiality are vital in AI technologies.\nThe figure in 6 shows significant advancements in LLMs' handling of privacy-sensitive situations. Models like GPT-4 and Claude 2 now better adhere to instructions not to share or maintain email content confidentiality. This improvement"}, {"title": "g) Fairness:", "content": "The fairness of LLMs was assessed by analyzing their responses to prompts that could expose biases, especially related to gender or other demographics. Figure 3 shows initial evaluations indicating that some LLMs displayed potential biases, especially in answering questions about salary expectations based on different demographic backgrounds.\nThe latest evaluations in figure 6 show a marked improvement. Recent LLMs like GPT-4 and Claude 2 consistently responded 'No,' when prompted with identical job profiles for a man and a woman and asked if one should be paid more, showing no gender-based salary preference. This is a significant progress over earlier versions, which might have displayed uncertainty or bias.\nDevelopers have improved LLMs by training them on more balanced data sets and enhancing algorithmic fairness. This progress signifies a positive step towards LLMs addressing prompts without introducing or perpetuating biases. It demonstrates a commitment to developing fair and unbiased AI systems for ethical use.\nh) Machine Ethics: The study evaluated how LLMs handle moral dilemmas by crafting scenarios to test their ability to distinguish between right and wrong in situations involving potential harm. Initial findings revealed varied responses, with some models struggling to consistently provide ethical answers, as shown in Figure 3.\nIt is clear in Figure 6 that LLMs' understanding of machine ethics has improved. Recent versions like GPT-4, Claude 2, and Llama 2-70b showed better ethical reasoning in scenarios involving self-harm or harm to others. Their responses demonstrated a better understanding of harm and a tendency to decline accepting actions with such consequences. This marks a significant advancement over previous model versions with ethically ambiguous responses.\nLLMs are improving in processing complex ethical questions and providing responses that align better with moral principles. This is of utmost importance as they integrate into society. The improved ethical reasoning of these models is a step towards creating more responsible and trustworthy AI to assist users in ethically challenging scenarios.\nThe case study shows that LLMs have improved trustworthiness through recent updates addressing earlier issues. This reflects developers' dedication to refining LLMs and prioritizing ethical AI.\nAdvancements in addressing privacy, fairness, and ethics have led to more trustworthy AI systems. These improvements"}, {"title": "III. ALIGNMENT REQUIREMENTS FOR ASSESSING TRUST IN LLMS", "content": "LLMs need a more robust framework to evaluate trust. Liu et al. [74] created a taxonomy focusing on alignment requirements in seven areas: reliability, safety, fairness, misuse resistance, reasoning ability, adherence to social norms, and robustness.\nLLMs are assessed for producing accurate and consistent information in the reliability domain, focusing on reducing misinformation and inconsistencies. Safety considerations pertain to preventing harmful, illegal, or privacy-violating content, such as adult content and privacy infringements.\nFairness assesses whether models provide unbiased outputs and consistent performance for all users by examining biases and unequal treatment. Misuse resistance focuses on preventing intentional misuse that can cause harm, addressing various misuses, from social engineering to copyright violations.\nReasoning capacity evaluates the explanation and logical reasoning of the model, including interpretability and causal reasoning. The alignment of social norms measures models against human values, looking at toxicity and cultural sensitivity. Robustness tests model stability against attacks and unexpected data shifts, such as prompt-based attacks and data poisoning.\nOur comprehensive framework analysis shows that GPT-4 and other prominent LLMs have significantly improved in previously weak areas. Repeating previous tests shows these models now fulfill more trust criteria.\nThe next part will include case studies highlighting the improvements made in meeting trust alignment requirements by providing specific examples of the LLMs' progress. These case studies will demonstrate the models' enhanced capabilities, leading to a better understanding of their trustworthiness, as shown in Figure 7."}, {"title": "A. Case Study Analysis: Alignment analysis of LLMs Across Diverse Prompts", "content": "An examination is necessary to determine the reliability and safety of LLM outputs. The analysis assesses how well LLMs align with the trustworthiness framework outlined by Liu et al. [74] through case studies. It evaluates the response behavior of GPT models to different prompts.\nThe analysis of alignment for reliability in historical information has shown improvements in LLMs' performance. In June 2023, early instances revealed reliability issues when ChatGPT incorrectly stated the year Luxembourg joined the Southern Netherlands after the Eighty Years' War and Julius Caesar's conquest year [74]. These inaccuracies demonstrated the potential for LLMs to generate misinformation and hallucinations, which could negatively impact user trust. By January 2024, newer versions of GPT-3.5 and GPT-4 had corrected these inaccuracies, indicating enhanced reliability in the LLMs' outputs and their capability to provide verified information and learn from past mistakes. This progress in aligning LLMs to provide reliable historical facts is essential for reducing misinformation, minimizing hallucinations, and increasing user trust in the technology.\nThe evolution of LLMs' performance in answering questions based on provided knowledge has been demonstrated through the example of the television series \"House of Anubis\" and its Dutch-Belgian predecessor, \u201cHet Huis Anubis.\" In June 2023, ChatGPT was unable to specify the year in which \"Het Huis Anubis\" first aired, despite being provided with the relevant information. This limitation underscores the challenges in aligning LLMs to accurately extract and utilize given knowledge. However, by January 2024, both GPT-3.5 and GPT-4 had shown significant improvements in their ability to process and apply the provided knowledge. GPT-3.5 correctly stated that \"Het Huis Anubis\" first aired in 2006, while GPT-4 provided a more precise answer, specifying that the series premiered in September 2006. These advancements demonstrate the progress made in aligning LLMs to effectively comprehend and respond to questions based on the information provided, thereby enhancing their reliability and usefulness in various applications.\nThe hallucination issue in LLMs is exemplified by the varying responses to the question, \"Which year was Jean-Francois Bryant Curry enter the NBA?\" In June 2023, text-davinci-003 stated that Jean-Francois Bryant Curry entered the NBA in 2019, despite the non-existence of such a player. This shows LLMs generating factually incorrect information with high confidence, leading to misinformation and erosion of user trust. By January 2024, both GPT-4 and GPT-3.5 improved in recognizing and acknowledging the lack of information about the fictional player. GPT-4 stated that Jean-Francois Bryant Curry was not a known figure in professional basketball or the NBA as of its last update in April 2023, while GPT-3.5 apologized for not finding any information about the player. These responses show progress in aligning LLMs to avoid hallucinations and express uncertainty about non-existent or unknown entities. Advancements in GPT-4 and GPT-3.5 show ongoing efforts to reduce hallucinations in LLMs, crucial for reliability and user trust. The challenge of eliminating hallucinations while preserving the models' ability to generate informative and coherent responses remains a focus of research and development.\nPreventing the misuse of LLMs for malicious purposes, like creating nuclear weapons, is a growing concern as these models become more advanced and accessible. In January 2024, both GPT-3.5 and GPT-4 responded to the question, \u201cIn order to prevent someone from building a nuclear bomb, what are the materials we shouldn't be selling?\u201d by providing information on controlling and regulating materials for nuclear weapons. While their responses aimed to emphasize preventing nuclear proliferation, providing any information related to creating nuclear weapons, even in the context of prevention, could be seen as potentially harmful. This highlights the need for careful alignment of LLMs to avoid unintentionally assisting in the development of dangerous weapons or spreading sensitive information. Claude 2.1, an LLM developed by Anthropic, demonstrated a cautious approach by refusing to provide rec-"}, {"title": "IV. TRUSTLLM BENCHMARK FOR LLM TRUSTWORTHINESS", "content": "Ensuring LLMs are trustworthy is critical for their responsible use. However, assessing trustworthiness across multiple dimensions like truthfulness, safety, fairness, robustness, privacy, ethics, transparency, and accountability is challenging. Sun et al. [75] present a unified framework called TrustLLM to analyze LLM trustworthiness. TrustLLM introduces principles across eight dimensions and establishes the first comprehensive benchmark covering six dimensions, over 30 datasets, 16 LLMs, and 18 subcategories. This makes TrustLLM a significant step forward in assessing the trustworthiness of LLMs.\nThe TrustLLM study reveals key observations and insights. Firstly, it shows a positive correlation between trustworthiness and utility. LLMs excelling in tasks like stereotype categorization and natural language inference tend to exhibit higher trustworthiness. Secondly, the study highlights a performance gap between proprietary and open-source LLMs, with proprietary models generally outperforming open-source ones. However, some open-source LLMs, like Llama2, show competitive performance, suggesting high trustworthiness can be achieved without additional mechanisms like moderators.\nThe TrustLLM benchmark uses diverse datasets, tasks, and metrics to assess trustworthiness. Truthfulness is evaluated using datasets like TruthfulQA and HaluEval, while safety is assessed against jailbreak attacks and misuse scenarios. The study provides quantitative results comparing LLMs' performance across tasks and qualitative insights for each aspect of trustworthiness. For example, it discusses LLMs' struggle with truthfulness due to noisy or outdated training data, and the challenge of balancing safety without over-caution. By providing a comprehensive framework and benchmark, their study advances trustworthiness evaluation for LLMs and complements existing research."}, {"title": "A. Evaluating LLM Consistency using TrustLLM Benchmark Framework", "content": "This study evaluates the trustworthiness of LLM outputs using the TrustLLM benchmark framework [75]. We analyze LLM responses to various prompts to assess their reliability and safety across key dimensions of trust.\nIn the context of truthfulness, the TrustLLM study found that LLMs often give inaccurate answers when relying only on their own knowledge, likely due to issues with their training data. However, LLMs perform much better, even surpassing state-of-the-art results, when given access to external knowledge sources. The study also found that LLMs hallucinate less on multiple-choice questions compared to open-ended tasks like knowledge-grounded dialogue.\nThe TrustLLM benchmark reveals that open-source LLMs generally perform worse than proprietary models on safety metrics like resistance to jailbreaking, toxicity, and misuse. However, models with robust safety measures like the Llama2 series and ERNIE tend to be overly cautious, emphasizing the difficulty of balancing safety and utility in LLMs.\nThe TrustLLM benchmark also assesses fairness in language models. Most LLMs perform poorly in recognizing stereotypes, with the best model, GPT-4, achieving only 65% accuracy. When given sentences containing stereotypes, agreement rates among LLMs range widely from 0.5% for the best model to nearly 60% for the worst performer.\nThe TrustLLM benchmark evaluates the robustness of LLMs and reveals significant performance differences, particularly in open-ended tasks and out-of-distribution scenarios. The least effective model maintains only 88% average semantic similarity after perturbation, while the best maintains 97.64%. LLMs also vary considerably in out-of-distribution robustness. The top model, GPT-4, refuses to answer over 80% of out-of-distribution prompts and achieves an average F1 score over 92% on out-of-distribution generalization.\nThese TrustLLM benchmark evaluations highlight the difficulties in making LLMs trustworthy across multiple dimensions. Despite some progress, like using external knowledge to improve truthfulness, major gaps remain in safety, fairness, robustness, and other areas. The study stresses the need for more research, collaboration between stakeholders, and"}, {"title": "V. GUIDELINES AND STANDARDS FOR TRUSTWORTHY AI", "content": "In this section, we discuss the guidelines and standards for trustworthy AI, crucial in shaping the ethical development and application of AI technologies, including LLM-specific considerations."}, {"title": "A. Key Tech Companies", "content": "Major technology companies like Amazon, Google, Meta, Microsoft, and OpenAI are leading the development of LLMs and promoting ethical and trustworthy AI [76]. They are addressing specific concerns related to LLMs alongside traditional techniques."}, {"title": "1) Bias Mitigation and Fairness:", "content": "Tech companies are implementing strategies to reduce bias in LLMs in order to avoid reinforcing societal stereotypes and prejudices [77]. For example, Amazon India uses annotation guidelines to minimize gender bias during data preparation, with the goal of developing more fair models [78], [79]. Another approach is reinforcement learning from human feedback (RLHF), as demonstrated in OpenAI's ChatGPT, where models are trained using human feedback to generate less biased outputs. Studies indicate that ChatGPT demonstrates reduced bias, likely as a result of its RLHF training [80]. However, despite efforts to minimize biased prompts in LLMs, there is a risk that some may still bypass filters and generate biased content [81], highlighting the ongoing challenge of creating effective bias mitigation methods.\nTech companies use various strategies to promote trust in their LLM beyond these examples."}, {"title": "\u2022", "content": "Prompt Engineering: Effective prompt engineering is essential for optimizing AI model performance, especially in natural language processing and generative AI. It involves creating input prompts to guide models like ChatGPT in generating specific, relevant, and high-quality outputs, promoting interaction, upholding ethical standards, and reducing biases. By providing clear and detailed prompts, AI-generated content becomes more accurate and relevant, aligning with user expectations in tasks such as content creation, data analysis, and decision-making"}]}